---
ver: rpa2
title: Attention Consistency for LLMs Explanation
arxiv_id: '2509.17178'
source_url: https://arxiv.org/abs/2509.17178
tags:
- attention
- macs
- tokens
- input
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MACS (Multi-Layer Attention Consistency Score),
  a lightweight method for identifying important input tokens in large language models.
  MACS measures how consistently the strongest attention link from a generated token
  points to each input token across all layers.
---

# Attention Consistency for LLMs Explanation

## Quick Facts
- **arXiv ID:** 2509.17178
- **Source URL:** https://arxiv.org/abs/2509.17178
- **Reference count:** 21
- **Primary result:** MACS achieves mAUC-PR of 0.601 on SQuAD, outperforming attention rollout (0.475) and AtMan (0.550)

## Executive Summary
This paper introduces MACS (Multi-Layer Attention Consistency Score), a lightweight method for identifying important input tokens in large language models. MACS measures how consistently the strongest attention link from a generated token points to each input token across all layers. This approach avoids full attention aggregation, which can produce noisy attributions due to over-squashing and softmax dispersion in deep models. Experiments on question answering tasks show MACS achieves an AUC-PR of 0.601, significantly outperforming attention rollout and AtMan, and matching the faithfulness of the more complex AttnLRP. MACS is also more efficient, using 22% less VRAM and reducing latency by 30%.

## Method Summary
MACS computes attribution scores by extracting attention weights from the last generated token's query to all previous keys, redistributing attention to input tokens, max-pooling across attention heads per layer, applying a floor vector to prevent score collapse, and computing element-wise multiplication (Hadamard product) across layers. The final scores are Z-normalized. This method focuses on attention consistency rather than total information flow, addressing over-squashing in deep models. The implementation requires inference-time access to all attention layers and careful index management to distinguish input tokens from generated tokens.

## Key Results
- MACS achieves mAUC-PR of 0.601 on SQuAD QA task, outperforming attention rollout (0.475) and AtMan (0.550)
- MACS matches AttnLRP's faithfulness (SRG = 0.430 vs 0.435) while being more efficient
- Ablation confirms max-pooling outperforms mean-pooling (0.601 vs 0.570) and min-pooling (0.443)
- Floor vector prevents score collapse (α=1 drops mAUC-PR to 0.546)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Max-pooling across attention heads preserves the strongest attention signal per layer, filtering diffuse attention patterns.
- **Mechanism:** For each layer l, MACS computes `m'_l = max_h(a_R^(l,h))` across H heads. This isolates peak attention to each input token, under the hypothesis that salient contributions manifest as peak signals rather than averaged ones.
- **Core assumption:** Important token contributions are marked by at least one specialized head attending strongly, not uniform attention across heads.
- **Evidence anchors:**
  - [section 3.2.2]: "This step isolates the strongest attention signal directed toward input token i across all heads... filtering out weaker, diffuse attention often encountered in over-mixing scenarios."
  - [section 5.1.1]: Ablation shows max-pooling (0.601) outperforms mean-pooling (0.570) and min-pooling (0.443) on mAUC-PR.
  - [corpus]: Weak direct evidence; related work on attention consistency regularization (arXiv:2601.08891) discusses layer-wise attention alignment but does not specifically validate max-pooling for attribution.
- **Break condition:** If model attention heads are uniformly noisy or no head specializes on relevant tokens, max-pooling will amplify noise.

### Mechanism 2
- **Claim:** Hadamard multiplication across layers measures consistency of strong attention links, yielding clearer attribution than additive aggregation.
- **Mechanism:** MACS computes `c_l = m_l ⊙ c_{l-1}` (element-wise product). A token's final score is high only if it receives consistently strong attention across most/all layers, filtering transient signals.
- **Core assumption:** Tokens that contribute to output maintain sustained, strong attention from the output query across network depth; transient attention is noise.
- **Evidence anchors:**
  - [abstract]: "MACS measures contributions of input tokens based on the consistency of maximal attention."
  - [section 3.2.2]: "The Hadamard product directly measures the consistency of the processed maximal attention link across layers... This contrasts with additive aggregation."
  - [corpus]: No direct external validation of Hadamard vs. additive aggregation for attribution; related work on early-exit networks uses attention consistency regularization but with different objectives.
- **Break condition:** If relevant information is captured in early layers only (deep layers attend elsewhere), Hadamard multiplication will suppress those tokens.

### Mechanism 3
- **Claim:** A floor vector (α-weighted uniform offset) prevents premature score collapse during Hadamard multiplication.
- **Mechanism:** Before multiplication, MACS applies `m_l = α·m'_l + (1-α)·1_N`, ensuring minimum score `(1-α)` per token per layer. This allows tokens relevant only in deeper layers to survive early low-attention layers.
- **Core assumption:** Relevant tokens may not receive strong attention at all layers; a non-zero floor preserves their candidacy until deeper layers reveal relevance.
- **Evidence anchors:**
  - [section 3.2.2]: "This prevents the subsequent Hadamard product from prematurely zeroing out the contribution score for tokens whose relevance might only emerge in deeper layers."
  - [section 5.1.1]: Removing floor (α=1) drops mAUC-PR from 0.601 to 0.546; exact α value has limited impact (0.2→0.599, 0.5→0.599).
  - [corpus]: No external corroboration identified.
- **Break condition:** If α is too low, the floor dominates and differentiation between tokens diminishes; if α=1 (no floor), tokens without early-layer attention are permanently suppressed.

## Foundational Learning

- **Concept:** Decoder-only causal attention (tokens attend only to prior positions).
  - **Why needed here:** MACS extracts attention vectors from the query at position n-1 (predicting token t_k) attending to all prior keys. Understanding caching and causal masking is essential for correct attention extraction.
  - **Quick check question:** During generation step k, which token issues the query, and what keys does it attend to?

- **Concept:** Precision-Recall curves and AUC-PR for token ranking evaluation.
  - **Why needed here:** The paper's primary metric is AUC-PR, measuring how well attribution scores rank ground-truth answer tokens higher than non-answer tokens.
  - **Quick check question:** If an attribution method assigns high scores to 8 of 10 answer tokens but also to 50 of 100 non-answer tokens, would AUC-PR be high or low?

- **Concept:** Faithfulness metrics via perturbation (MIF/LIF/SRG).
  - **Why needed here:** MACS is evaluated on whether removing highly-scored tokens degrades model output more than removing low-scored tokens (Symmetric Relevance Gain).
  - **Quick check question:** What does SRG ≈ 0 indicate about an attribution method's faithfulness?

## Architecture Onboarding

- **Component map:** Attention Extraction -> Max-Pooling -> Floor Vector -> Consistency Score -> Z-Scoring
- **Critical path:** Correct extraction of attention to input tokens (excluding prompt instructions) → max-pooling → floor application → layer-wise multiplication → Z-score normalization. Errors in attention indexing (input vs. generated tokens) will cascade through all steps.
- **Design tradeoffs:**
  - Max-pooling vs. mean-pooling: Max preserves peak signals (better mAUC-PR) but may amplify noise if no head is reliable.
  - Hadamard vs. additive aggregation: Hadamard emphasizes consistency; additive (Rollout) accumulates all paths, risking diffuse attributions.
  - Floor vector α: Too low → weak differentiation; α=1 → premature zeroing. Paper finds robustness across α∈[0.2, 0.8].
- **Failure signatures:**
  - All tokens receiving near-identical scores → check if floor vector is dominating (α too low) or attention extraction is incorrect.
  - Random-level AUC-PR → verify attention is being extracted from correct query position (n-1) and that input indices are correctly isolated.
  - OOM on long contexts → MACS is lightweight (11% VRAM overhead), but if attention matrices are materialized unnecessarily, memory will spike. Ensure per-layer processing without full matrix storage.
- **First 3 experiments:**
  1. **Sanity check:** Apply MACS to a short QA sample; verify Z-scores highlight tokens in the answer span (visual inspection against ground truth).
  2. **Ablation on pooling:** Replace max-pooling with mean-pooling on 50 SQuAD samples; confirm mAUC-PR degradation aligns with paper (0.601 → ~0.570).
  3. **Efficiency benchmark:** Measure VRAM and throughput overhead vs. pure inference on contexts of 500, 1000, 2000 tokens; expect ~11% VRAM increase and ~23% throughput decrease per paper Table 3.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the attention redistribution step significantly improve attribution accuracy in very long-context generation scenarios?
  - **Basis in paper:** [inferred] The ablation study showed removing redistribution had a negligible impact, but the authors hypothesized this was due to the "moderate generation lengths" (max 256 tokens) used in the SQuAD subset.
  - **Why unresolved:** The current experiments did not validate the mechanism's necessity for capturing "distant indirect influences" found in longer contexts where attention decay is more pronounced.
  - **What evidence would resolve it:** Ablation experiments on long-context benchmarks (e.g., documents with 4k+ tokens) showing a statistically significant performance gap between standard MACS and the variant without redistribution.

- **Open Question 2:** How can MACS be theoretically or empirically integrated into hybrid explainability frameworks?
  - **Basis in paper:** [explicit] The authors explicitly state in the Future Work section that "investigating its role as both a standalone diagnostic and as a component in hybrid XAI approaches remains a promising direction."
  - **Why unresolved:** The paper evaluates MACS strictly as a standalone method against other standalone baselines, without exploring potential synergies with gradient or perturbation-based techniques.
  - **What evidence would resolve it:** A proposed framework that combines MACS with methods like Integrated Gradients, demonstrating superior faithfulness or AUC-PR scores compared to either method used in isolation.

- **Open Question 3:** Does the "attention consistency" score correlate mathematically with causal influence in deep networks?
  - **Basis in paper:** [explicit] The authors note in the Limitations section that "understanding the full implications of this consistency measure versus, for example, total information flow, requires careful interpretation."
  - **Why unresolved:** The paper establishes MACS via heuristic motivation (addressing Over-squashing) rather than formal theoretical derivation connecting consistency scores to information theory or causal relevance.
  - **What evidence would resolve it:** A theoretical analysis linking the Hadamard product of max-pooled attention to causal intervention metrics (e.g., causal tracing) in controlled transformer settings.

## Limitations
- Evaluation framework assumes answer tokens are the only "important" inputs, potentially missing context that enables inference
- Perturbation-based faithfulness metrics measure relative changes rather than absolute importance
- Experiments limited to single model (Llama 3.1-8B) and dataset (SQuAD subset), limiting generalizability
- Mechanism lacks formal theoretical derivation connecting consistency scores to information theory or causal relevance

## Confidence

**High Confidence** (supported by direct experimental evidence):
- MACS achieves significantly higher mAUC-PR (0.601) than attention rollout (0.475) and AtMan (0.550) on SQuAD QA task
- MACS matches AttnLRP's faithfulness (SRG = 0.430 vs 0.435) while being more efficient
- Ablation confirms max-pooling outperforms mean-pooling (0.601 vs 0.570) and min-pooling (0.443)
- Floor vector prevents score collapse (α=1 drops mAUC-PR to 0.546)

**Medium Confidence** (plausible mechanism but limited external validation):
- Hadamard multiplication provides cleaner attribution than additive aggregation by emphasizing consistency
- The max-pooling approach reliably isolates specialized head signals across different attention patterns
- Floor vector α=0.8 provides optimal balance between preservation and differentiation

**Low Confidence** (mechanism plausible but unsupported):
- No external validation of Hadamard vs additive aggregation for attribution exists in literature
- No direct evidence that max-pooling is superior for attribution beyond this paper's experiments
- Floor vector mechanism lacks external corroboration

## Next Checks
1. **Cross-Architecture Validation:** Apply MACS to different decoder-only models (Mistral, Gemma) on the same SQuAD task to verify mAUC-PR consistency (target: 0.55-0.65 range across models).
2. **Task Diversity Test:** Evaluate MACS on non-QA tasks like summarization (CNN/DM) and reasoning (GSM8K) to confirm the mechanism generalizes beyond extractive question answering.
3. **Head Specialization Analysis:** For a subset of samples, examine individual attention heads to verify that max-pooling consistently captures specialized attention patterns rather than random noise amplification.