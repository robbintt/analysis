---
ver: rpa2
title: Patience is all you need! An agentic system for performing scientific literature
  review
arxiv_id: '2504.08752'
source_url: https://arxiv.org/abs/2504.08752
tags:
- retrieval
- literature
- question
- term
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an agentic system for performing scientific
  literature reviews using large language models. The system employs keyword-based
  sparse retrieval with LLM-generated query expansion, followed by re-ranking and
  information extraction from full-text articles.
---

# Patience is all you need! An agentic system for performing scientific literature review

## Quick Facts
- **arXiv ID**: 2504.08752
- **Source URL**: https://arxiv.org/abs/2504.08752
- **Reference count**: 40
- **Primary result**: Agentic system achieves 60.4% accuracy and 90% precision on scientific literature review tasks using sparse retrieval with LLM query expansion

## Executive Summary
This paper introduces an agentic system for performing scientific literature reviews using large language models with a purely sparse retrieval architecture. The system employs keyword-based retrieval enhanced by LLM-generated query expansion, followed by re-ranking and information extraction from full-text articles. A verification and diversification step using Chain-of-Verification extends literature coverage. Evaluated on LitQA2 and PubMedQA benchmarks, the system demonstrates comparable performance to state-of-the-art methods while maintaining architectural simplicity by avoiding dense retrieval complexity.

## Method Summary
The system implements a three-stage pipeline: (1) Document retrieval using Elasticsearch with HunFlair NER and LLM-generated query expansion, (2) Re-ranking through BM25L search on chunks generated from 3 LLM-hypothesized answers with reciprocal rank fusion, and (3) Information extraction with chunk summarization and optional Chain-of-Verification for literature expansion. The architecture leverages Claude 3.5 Sonnet throughout, processing PubMed citations and full-text articles to generate verified literature reviews. The approach prioritizes precision over recall, achieving 90% precision but only 50.5% recall of source articles on benchmarks.

## Key Results
- Achieves 60.4% accuracy and 90% precision on LitQA2 benchmark
- 50.5% recall of source articles on LitQA2, with 61.7% in top 200
- Improves literature coverage by 6-25% using Chain-of-Verification for expansion
- Demonstrates comparable performance to state-of-the-art methods without requiring dense retrieval

## Why This Works (Mechanism)
The system's effectiveness stems from its intelligent combination of sparse retrieval with LLM-driven query expansion and re-ranking. By generating multiple hypothetical answers and using reciprocal rank fusion, it compensates for the semantic limitations of keyword-based search. The Chain-of-Verification mechanism systematically expands literature coverage by generating verification questions from draft answers and retrieving additional relevant sources. The chunking strategy with overlap ensures comprehensive coverage of relevant passages while managing computational constraints. The system's design prioritizes precision and verification over exhaustive recall, making it particularly suitable for high-stakes scientific review tasks where accuracy matters more than completeness.

## Foundational Learning

- **Concept: Sparse vs. Dense Retrieval**
  - **Why needed here:** This system's core innovation is its reliance on a purely sparse retrieval architecture. An engineer must understand that sparse retrieval (e.g., BM25 on keywords) is computationally simpler and less infrastructure-intensive than dense retrieval (e.g., vector embeddings), but it lacks built-in semantic understanding, which the paper addresses through LLM-based query expansion.
  - **Quick check question:** What is the key information loss in sparse retrieval that dense retrieval attempts to solve, and how does this paper's approach compensate for it?

- **Concept: Reciprocal Rank Fusion (RRF)**
  - **Why needed here:** RRF is the method used to combine multiple ranked lists (from the original question and LLM-generated answer queries) into a single, more robust re-ranking. It's a critical step in the information pipeline that prioritizes relevant text chunks.
  - **Quick check question:** Why would simply averaging the rank positions from multiple search queries be inferior to a fusion method like RRF?

- **Concept: Chain-of-Verification (CoVe)**
  - **Why needed here:** The system uses CoVe not just for fact-checking, but as a core mechanism for literature expansion. Understanding this multi-step process (plan verification questions -> retrieve -> integrate) is essential to grasp how the system moves from simple Q&A to generating a broader literature review.
  - **Quick check question:** How does the system's use of CoVe differ from its original design goal of merely reducing hallucinations?

## Architecture Onboarding

- **Component map:** Data Sources -> Document Retrieval Agent -> Re-ranking Agent -> Information Extraction Agent -> Verification & Diversification Agent -> (loop back to Retrieval)
- **Critical path:** 1. Question Input & NER 2. LLM Query Expansion & Initial Retrieval 3. Chunking & Re-ranking via LLM-generated answers 4. Summarization & Answer Synthesis 5. (Optional/Parallel) Chain-of-Verification for literature expansion
- **Design tradeoffs:** Speed vs. Quality (10-30 minutes vs. simple chatbots), Simplicity vs. Semantic Depth (sparse retrieval avoids embedding complexity), Recall vs. Precision (90% precision, 50.5% recall)
- **Failure signatures:** Retrieval Failure (mismatched vocabulary), Hallucinated Reranking (incorrect LLM answers lead astray), Topic Drift (verification questions too tangential)
- **First 3 experiments:** 1. Ablation on Reranking: Compare LitQA2 performance with/without LLM-hypothesis-based re-ranking 2. Sparse vs. Dense Baseline: Compare retrieval performance against dense retrieval approach 3. Verification Loop Analysis: Have domain experts evaluate relevance of CoVe-discovered references

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the initial retrieval recall be significantly improved to match state-of-the-art dense methods without introducing the infrastructure complexity associated with dense retrieval? [explicit] The authors state the key area for improvement lies in the initial retrieval stage, noting highest loss in recall occurs here.

- **Open Question 2:** How can the scientific community develop benchmarks that accurately evaluate the "coverage" and quality of long-form literature reviews rather than single-paper fact retrieval? [explicit] The authors argue existing benchmarks are typically answered with a single paper, whereas real-world reviews require synthesizing a broad evidence base.

- **Open Question 3:** To what degree is the observed precision/coverage trade-off in agentic systems determined by the specific "caution" (refusal rate) of the underlying LLM rather than the retrieval architecture? [explicit] The authors hypothesize baseline caution of Claude 3.5 Sonnet leads to reduced coverage compared with PaperQA2.

## Limitations

- Evaluation relies heavily on benchmark datasets (LitQA2, PubMedQA) that may not fully capture real-world literature review breadth
- Performance on open-ended literature review tasks lacks comprehensive quantitative validation, evaluated on only 10 Nature review articles
- Reliance on Claude 3.5 Sonnet throughout introduces potential bias, results may not generalize to other LLMs

## Confidence

- **High Confidence**: Architectural design using sparse retrieval with LLM query expansion is clearly described and technically sound
- **Medium Confidence**: "Simpler architecture" claim is supported by design choices but lacks direct dense retrieval comparisons
- **Low Confidence**: Literature coverage improvement of 6-25% based on evaluation of only 10 review articles

## Next Checks

1. **Benchmark Generalization**: Test the system on additional literature review benchmarks beyond LitQA2 and PubMedQA to validate performance across different scientific domains and question types.

2. **Dense Retrieval Comparison**: Implement a direct comparison between the sparse retrieval approach and a state-of-the-art dense retrieval method (e.g., using sentence transformers) on the same question sets to quantify the "simpler architecture" claim.

3. **Literature Coverage Validation**: Conduct a larger-scale evaluation of the Chain-of-Verification literature expansion mechanism using 50+ review articles across multiple journals to establish confidence intervals for the reported 6-25% coverage improvement.