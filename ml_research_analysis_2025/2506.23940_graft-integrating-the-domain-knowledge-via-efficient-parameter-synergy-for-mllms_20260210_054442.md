---
ver: rpa2
title: 'Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for
  MLLMs'
arxiv_id: '2506.23940'
source_url: https://arxiv.org/abs/2506.23940
tags:
- zhang
- fusion
- arxiv
- wang
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Graft, a dual-gate parameter fusion framework
  for integrating domain-specific multimodal large language models (MLLMs). Graft
  employs both local channel-level gating based on parameter differences and global
  entropy-based weighting to adaptively merge model parameters, ensuring effective
  knowledge integration while minimizing interference.
---

# Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs

## Quick Facts
- arXiv ID: 2506.23940
- Source URL: https://arxiv.org/abs/2506.23940
- Reference count: 19
- Primary result: Dual-gate parameter fusion framework achieves 52.2% on MathVista and 15.9% on HumanEval when merging LoRA-adapted MLLMs

## Executive Summary
Graft introduces a dual-gate parameter fusion framework for integrating domain-specific multimodal large language models (MLLMs). The method combines local channel-wise gating based on parameter differences with global entropy-based weighting to adaptively merge model parameters while minimizing interference. Experiments across diverse multimodal benchmarks demonstrate that Graft significantly outperforms existing merging methods, achieving state-of-the-art performance when fusing specialized MLLMs trained on math, code, medical, and financial domains.

## Method Summary
Graft operates by fusing LoRA adapter parameters from domain-specific MLLMs using a dual-gate mechanism. The local gate processes channel-wise parameter differences through a learnable network to produce attention weights, while the global gate computes weight entropy to generate macro-level fusion guidance. These signals are combined via nonlinear transformations and normalized to produce final fusion coefficients. The framework also includes an activation-based compatibility analysis that predicts fusion success using activation statistics from target data. The method was evaluated on Qwen2-VL-2B backbone with domain datasets sampled at 5K instances each.

## Key Results
- Achieves 52.2% accuracy on MathVista benchmark when fusing LoRA-adapted models
- Reaches 15.9% pass@1 on HumanEval coding benchmark through domain fusion
- Outperforms baseline methods including Task Arithmetic, Ties-Merging, and DARE across MMMU and MME composite scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local channel-wise gating improves fine-grained parameter fusion by prioritizing informative channels
- Mechanism: A learnable network processes the absolute difference between base and graft weights per output channel, producing sigmoid-normalized weights $w_{local}$ that emphasize channels with larger divergence
- Core assumption: Greater parameter divergence at a channel correlates with domain-specific importance for that channel
- Evidence anchors: Abstract mentions "local functional attribution...to guide selective parameter fusion"; section 3.1 defines channel-wise difference and gating
- Break condition: If domains have uniformly low divergence, local gating provides little signal and may overfit noise

### Mechanism 2
- Claim: Global entropy-based weighting guides macro-level fusion by favoring modules with higher parameter information content
- Mechanism: Compute weight entropy $H(W)$ for each module; a normalized scalar $w_{global}$ via arctan of entropy difference biases fusion toward the higher-entropy module
- Core assumption: Higher parameter entropy indicates richer, more transferable representations for fusion
- Evidence anchors: Abstract references "global information-theoretic signals" used for fusion guidance; section 3.2 formalizes entropy computation
- Break condition: When entropy does not correlate with downstream utility, global gating may misguide

### Mechanism 3
- Claim: Dual-gate fusion synergizes local and global signals to reduce interference while preserving domain expertise
- Mechanism: Local and global weights are composed via nonlinear transformations and normalized to produce final fusion coefficients $w_b$, $w_g$; fused weights are convex combinations of base and graft
- Core assumption: Combining fine-grained and coarse signals resolves parameter conflicts better than either alone
- Evidence anchors: Abstract describes "dual-gate parameter fusion framework" and "adaptive" merging; section 3.3 defines dual-gate composition
- Break condition: If local and global signals conflict sharply, the softmax normalization may dilute both

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Graft operates on LoRA adapter parameters for efficient fusion; understanding rank constraints is crucial
  - Quick check question: What happens if LoRA ranks differ between two adapters to be fused?

- Concept: Parameter Entropy
  - Why needed here: Core to global weighting mechanism; informs which module carries more information content
  - Quick check question: Does a higher entropy weight distribution always mean better representation?

- Concept: Activation-Based Compatibility
  - Why needed here: Predicts fusion success using activation statistics; prerequisites include interpreting sparsity and variance
  - Quick check question: If a module has high sparsity on target data, is it likely compatible?

## Architecture Onboarding

- Component map:
  - Local Gate: Learnable MLP φ taking channel-wise difference vector d, outputting w_local
  - Global Gate: Entropy computation H(W) and arctan-based scalar w_global
  - Fusion Module: Combines w_local and w_global via nonlinear functions and softmax to produce w_b, w_g
  - Compatibility Analyzer: Computes ρ'_i from activation mean, sparsity, variance; aggregates to compatibility score

- Critical path:
  1. Load base and graft model/LoRA parameters
  2. Compute channel-wise differences (local) and entropy (global)
  3. Pass through local and global gates to obtain w_local, w_global
  4. Apply dual-gate equations to get final fusion weights w_b, w_g
  5. Fuse weights via convex combination
  6. (Optional) Run compatibility analysis on target dataset to filter candidates

- Design tradeoffs:
  - More bins n in entropy discretization increases granularity but computational cost
  - Aggressive gating (large c) may amplify noise; conservative a limits dynamic range
  - Compatibility analysis requires forward passes on data; reduces scalability for very large candidate pools

- Failure signatures:
  - Performance degradation after fusion suggests incompatible domains (check compatibility score)
  - Uniform fusion weights (w_b ≈ w_g ≈ 0.5) indicate weak divergence/entropy signals
  - NaN or extreme values in w_global if entropy difference is large and c is high

- First 3 experiments:
  1. Ablate local vs. global vs. dual gates on MathVista/HumanEval
  2. Sweep compatibility thresholds and correlate with fusion gains
  3. Compare attention-only vs. MLP-only vs. full-layer fusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the activation-based compatibility metric be refined to reliably predict fusion success for domain pairs with strong baseline performance but low measured compatibility?
- Basis in paper: The authors note an "apparent outlier" where Code+Medical fusion excelled despite a low compatibility score (0.265), and explicitly recommend a "two-factor decision rule" to resolve this discrepancy
- Why unresolved: The current metric fails to capture cases where high standalone proficiency compensates for low feature-space alignment, limiting its utility as a standalone filter
- What evidence would resolve it: A modified scoring function that incorporates baseline proficiency weights, validated across a wider range of asymmetric domain pairings

### Open Question 2
- Question: Why does increasing the number of fused experts (beyond two) lead to performance regression in specific capabilities (e.g., HumanEval) despite the dual-gate mechanism's intent to suppress interference?
- Basis in paper: Table 4 shows HumanEval performance peaking at 15.9% (2 domains) but dropping to 14.6% for 3 and 4 domains, suggesting the gating mechanism struggles to preserve specific skills as expert density increases
- Why unresolved: The paper attributes this to "suppressing interference," but does not explain why interference mitigation fails to maintain the peak performance achieved with fewer experts
- What evidence would resolve it: An analysis of gate weight distributions in 4-domain vs. 2-domain settings to identify if specific coding-related channels are being systematically diluted

### Open Question 3
- Question: How does the quantity and diversity of the calibration data (K samples) impact the variance and reliability of the activation-based compatibility threshold?
- Basis in paper: The methodology relies on selecting a "relatively small value" of K samples to compute variance and sparsity, but provides no analysis of how sample size influences the stability of the resulting compatibility scores
- Why unresolved: Without sensitivity analysis, it is unclear if the compatibility score is robust to random sampling noise or highly dependent on the specific choice of K samples
- What evidence would resolve it: A study plotting the variance of compatibility scores against different values of K (e.g., 10 vs. 100 vs. 1000) on the same dataset

## Limitations
- Gating network architecture underspecification: The learnable gating network φ is not fully specified in terms of depth, hidden dimensions, or activation functions
- Hyperparameter sensitivity: Critical hyperparameters like entropy discretization granularity, arctan scaling, and ceiling constant lack sensitivity analysis
- Limited generalization evidence: Experiments focus on Qwen2-VL-2B and four specific domains; performance on other architectures remains untested

## Confidence
- High confidence: Graft's overall effectiveness (52.2% MathVista, 15.9% HumanEval) compared to baseline methods - supported by comprehensive benchmark suite
- Medium confidence: The dual-gate mechanism's superiority over single-gate approaches - ablation studies would strengthen this claim
- Medium confidence: The compatibility analysis's predictive value for successful fusion - correlation data between predicted scores and actual performance is limited
- Low confidence: The entropy-based weighting's theoretical justification - the paper asserts higher entropy indicates richer representations but provides minimal empirical validation

## Next Checks
1. **Ablation study execution**: Run experiments comparing local-only, global-only, and dual-gate fusion variants on the same datasets to quantify each component's contribution to the 52.2% MathVista score
2. **Hyperparameter sensitivity analysis**: Systematically vary n (entropy bins), a (arctan scaling), and c (ceiling constant) to determine their impact on fusion quality and identify optimal ranges
3. **Cross-architecture validation**: Test Graft's effectiveness when fusing LoRA adapters from different MLLM architectures (e.g., LLaVA-1.5 with Qwen2-VL-2B) to assess generalizability beyond the primary experimental setup