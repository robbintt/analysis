---
ver: rpa2
title: 'Federated Learning in NTNs: Design, Architecture and Challenges'
arxiv_id: '2503.07272'
source_url: https://arxiv.org/abs/2503.07272
tags:
- haps
- clients
- satellites
- communication
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a distributed hierarchical federated learning
  (HFL) framework for non-terrestrial networks (NTNs) using a constellation of high-altitude
  platform stations (HAPS) as intermediate distributed FL servers. The framework integrates
  low-Earth orbit (LEO) satellites and ground clients in the FL training process while
  leveraging geostationary orbit (GEO) and medium-Earth orbit (MEO) satellites as
  relays to exchange global models across HAPS constellations worldwide.
---

# Federated Learning in NTNs: Design, Architecture and Challenges

## Quick Facts
- arXiv ID: 2503.07272
- Source URL: https://arxiv.org/abs/2503.07272
- Reference count: 15
- One-line primary result: Distributed HFL in NTNs achieves up to 17% higher model accuracy than terrestrial-only at the cost of up to 150% higher latency.

## Executive Summary
This paper proposes a distributed hierarchical federated learning (HFL) framework for non-terrestrial networks (NTNs) using a constellation of high-altitude platform stations (HAPS) as intermediate distributed FL servers. The framework integrates low-Earth orbit (LEO) satellites and ground clients in the FL training process while leveraging geostationary orbit (GEO) and medium-Earth orbit (MEO) satellites as relays to exchange global models across HAPS constellations worldwide. Simulation results show that the proposed distributed HFL framework achieves improved model accuracy (up to 17% higher than terrestrial-only), reduced training loss (up to 36% lower), and efficient latency management, with the trade-off of higher cumulative latency (up to 150% higher) justified by substantial gains in model accuracy and reduced training loss.

## Method Summary
The method involves implementing FedAvg with specified hyperparameters (learning rate 0.01, batch size 32, 120 local epochs per round, 10 rounds) on non-IID CIFAR-10 data distributed across simulated clients. Four scenarios are compared: Terrestrial-only (20 ground clients, 1 BS), Satellite-RF (5 LEO, 1 HAPS, RF links), Satellite-FSO (5 LEO, 1 HAPS, FSO links), and Distributed-HAPS (200 ground + 5 LEO, 5 HAPS nodes). The simulation setup uses federated learning frameworks (e.g., Flower, PySyft) with configurable channel delays per scenario and a standard CNN architecture for CIFAR-10.

## Key Results
- Distributed HFL framework achieves up to 17% higher model accuracy than terrestrial-only.
- Training loss reduced by up to 36% compared to terrestrial-only.
- Cumulative latency increased by up to 150% compared to terrestrial-only.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decentralized aggregation via a HAPS constellation improves model accuracy over terrestrial-only FL.
- Mechanism: HAPS nodes act as intermediate FL servers, aggregating local updates from ground and LEO clients within their coverage. They then exchange these cluster-level updates with neighboring HAPS nodes, generating a global model through distributed averaging. This hierarchical structure allows for wider client participation and more robust aggregation than a single terrestrial server.
- Core assumption: Clients in different geographic regions have non-IID data, and aggregating across these diverse regional clusters leads to a more generalized and accurate global model, which outweighs the cost of increased architectural complexity.
- Evidence anchors:
  - [abstract] "...proposes a distributed hierarchical federated learning (HFL) framework... leveraging a high altitude platform station (HAPS) constellation as intermediate distributed FL servers."
  - [abstract] "Simulation results show that the proposed distributed HFL framework achieves improved model accuracy (up to 17% higher than terrestrial-only)...".
  - [section V] "In this framework, local models are first aggregated at the cluster level... These cluster-level updates are then exchanged among neighbouring HAPS nodes...".
  - [corpus] Corpus evidence for this specific architecture is limited. 'Decentralized Trust for Space AI' mentions decentralized FL for LEOs but not the HAPS-centric hierarchical model.
- Break condition: If client data across all regions is largely IID, the added latency and complexity of the HAPS layer may not yield significant accuracy improvements over a simpler centralized system.

### Mechanism 2
- Claim: Integrating LEO satellites as FL clients diversifies the training data pool.
- Mechanism: LEO satellites, acting as FL clients, perform local training on their unique on-orbit data (e.g., Earth observation). They upload model updates via high-capacity FSO links to the HAPS layer, injecting this data diversity into the global model without requiring raw data downlink.
- Core assumption: The data collected by LEO satellites is valuable for the learning task and their intermittent connectivity can be managed effectively by the scheduling and aggregation logic of the HAPS servers.
- Evidence anchors:
  - [abstract] "Our framework integrates both low-Earth orbit (LEO) satellites and ground clients in the FL training process...".
  - [section V.A] "...LEO satellites perform low-power computing tasks... making them potential FL clients...".
  - [section V.H] "To leverage the strong LoS links between HAPS and satellites, we use the FSO band for communication...".
  - [corpus] 'Satellite Federated Fine-Tuning for Foundation Models' supports the concept of LEO-based federated learning, though in a different architectural context.
- Break condition: If the computational or energy cost for LEO satellites to perform local training is too high, or if connectivity is too sporadic, their contributions may be too delayed (stale) to benefit the global model.

### Mechanism 3
- Claim: GEO/MEO satellites act as global relays to synchronize models between dispersed HAPS constellations.
- Mechanism: A HAPS constellation handles regional learning. To achieve global consistency, GEO or MEO satellites receive the aggregated global model from one HAPS constellation and relay it to others worldwide. This decouples global synchronization from the lower-latency regional training loop.
- Core assumption: The higher latency of the GEO/MEO relay link is acceptable for global model distribution, which can occur less frequently than intra-constellation updates.
- Evidence anchors:
  - [abstract] "...utilizing geostationary orbit (GEO) and medium-Earth orbit (MEO) satellites as relays to exchange FL global models across other HAPS constellations worldwide".
  - [section III.A.2] "GEO and MEO satellites... act primarily as relays... enabling a ubiquitous FL system on a global scale."
  - [corpus] Evidence from corpus is weak on this specific three-tier relay model; related work focuses on satellite-to-ground or inter-satellite FL.
- Break condition: If an alternative, lower-latency network (e.g., terrestrial fiber) is available to connect the regional HAPS constellations, using the satellite relay becomes an unnecessary bottleneck.

## Foundational Learning
- Concept: **Federated Learning (FL)**
  - Why needed here: This is the core technique the paper aims to deploy in a challenging environment. Understanding the basic cycle of local training and global aggregation is essential.
  - Quick check question: What is the primary advantage of Federated Learning over centralized machine learning in terms of data privacy?

- Concept: **Non-Terrestrial Networks (NTNs)**
  - Why needed here: The paper's contribution is a specialized architecture for FL within the unique constraints (mobility, latency, tiers) of NTNs.
  - Quick check question: What are the three primary tiers that constitute a Non-Terrestrial Network?

- Concept: **Hierarchical Aggregation**
  - Why needed here: The proposed solution is a hierarchical framework. One must understand that aggregation happens at multiple levels (client -> HAPS -> global) rather than just once at a central server.
  - Quick check question: In this hierarchical model, what is the specific role of a HAPS node that distinguishes it from a LEO satellite or a GEO relay?

## Architecture Onboarding
- Component map:
  - **Terrestrial:** Ground Clients (data gen/local training), Ground BSs (relays/aggregators).
  - **Aerial:** HAPS Constellation (Distributed FL Servers), UAVs (relays).
  - **Satellite:** LEOs (FL Clients), GEO/MEOs (Global Relays).
- Critical path:
  1. **Broadcast:** HAPS broadcasts global model to ground & LEO clients.
  2. **Local Training:** Clients train locally on private data.
  3. **Regional Upload:** Clients send model updates to their serving HAPS (directly, via BS, or UAV).
  4. **Regional Aggregation:** Each HAPS aggregates its cluster's updates.
  5. **Intra-Constellation Sync:** HAPS nodes exchange their regional models with neighbors.
  6. **Global Relay Sync:** GEO/MEO satellites relay models between distant HAPS constellations.
- Design tradeoffs:
  - **Latency for Accuracy:** The framework accepts up to 150% higher cumulative latency to achieve up to 17% higher model accuracy by enabling more client participation.
  - **Scalability vs. Complexity:** Using a distributed HAPS constellation avoids a central bottleneck, improving scalability, but introduces complex coordination and synchronization challenges between HAPS nodes.
- Failure signatures:
  - **Model Staleness:** Updates from LEO clients may arrive late due to orbital dynamics, degrading model quality if not handled asynchronously.
  - **Cluster Isolation:** Loss of a HAPS node or its GEO/MEO backhaul link can isolate an entire regional cluster from the global learning process.
  - **Resource Drain:** FL tasks could deplete the limited energy of LEO satellites or battery-powered ground clients.
- First 3 experiments:
  1. **Terrestrial Baseline:** Run a standard centralized FL simulation with ground clients only to establish baseline metrics for accuracy and latency.
  2. **Single-HAPS Test:** Add one HAPS as an FL server for a cluster of ground and LEO clients to measure the impact of the intermediate aggregation layer.
  3. **Multi-HAPS Distributed Test:** Simulate multiple HAPS nodes exchanging models to measure the model accuracy gain and latency trade-off of the fully distributed architecture.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can dynamic schedulers be designed to minimize interference among concurrent free-space optical (FSO) transmissions while adapting to variable atmospheric conditions?
- Basis in paper: [explicit] Section VII, "Efficient Scheduling for FSO Transmissions," states that further research is required to create schedulers that ensure reliable communication for distributed learning.
- Why unresolved: The paper notes that FSO links suffer from interference during concurrent transmissions (e.g., HAPS to multiple LEOs), and atmospheric variability makes static scheduling ineffective.
- What evidence would resolve it: A proposed scheduling algorithm demonstrating maintained throughput and reliability under simulated atmospheric turbulence and high-concurrency scenarios.

### Open Question 2
- Question: What hierarchical synchronization protocols can effectively coordinate model updates across heterogeneous tiers with varying latencies and node capabilities?
- Basis in paper: [explicit] Section VII, "Coordination and Synchronization," identifies coordination across tiers as challenging and suggests exploring Gossip-based algorithms or consensus mechanisms.
- Why unresolved: The diverse latency profiles and computational capabilities of terrestrial, aerial, and satellite nodes make standard synchronization methods prone to failure or inefficiency.
- What evidence would resolve it: Simulation results showing a specific protocol maintaining model consistency and convergence speed despite high latency variance between ground and satellite nodes.

### Open Question 3
- Question: What is the optimal placement strategy for HAPS nodes to balance geographic coverage with low-latency communication for Federated Learning tasks?
- Basis in paper: [explicit] Section VII, "HAPS Constellation Design," highlights the need to investigate optimal placement and scalable architectures to meet dynamic network needs.
- Why unresolved: While the benefits of HAPS are clear, the specific trade-offs between coverage area, latency, and FL aggregation efficiency in a constellation topology are not yet defined.
- What evidence would resolve it: An optimization model defining HAPS placement and empirical data showing improved FL convergence rates compared to ad-hoc or grid-based placements.

## Limitations
- Simulation results are not validated on real-world NTN deployments.
- The proposed architecture introduces significant computational and communication overhead, particularly in the GEO/MEO relay links.
- The model assumes idealized channel conditions and does not account for potential node failures or adversarial attacks on the federated learning process.

## Confidence
- High: The hierarchical FL framework's design and its potential to improve model accuracy and reduce training loss in NTNs are well-supported by the simulation results.
- Medium: The latency trade-off is justified by the accuracy gains, but the real-world impact of the higher cumulative latency is not fully explored.
- Low: The assumption that GEO/MEO satellites can effectively relay global models without introducing significant delays or bottlenecks is not empirically validated.

## Next Checks
1. Conduct a sensitivity analysis on the non-IID data distribution method to determine its impact on model accuracy and convergence.
2. Perform a scalability test to evaluate the framework's performance as the number of HAPS nodes and clients increases.
3. Implement a fault-tolerance mechanism to assess the system's resilience to node failures and link disruptions.