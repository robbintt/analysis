---
ver: rpa2
title: 'NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible
  Deployment'
arxiv_id: '2505.22327'
source_url: https://arxiv.org/abs/2505.22327
tags:
- linguistics
- computational
- pages
- association
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey maps NLP applications for social good (NLP4SG) to global
  goals and risks, finding uneven research distribution across nine domains. It analyzes
  ACL Anthology trends, revealing that AI harms and inclusion attract the most research,
  while poverty, peacebuilding, and environmental protection remain underexplored.
---

# NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment

## Quick Facts
- arXiv ID: 2505.22327
- Source URL: https://arxiv.org/abs/2505.22327
- Reference count: 40
- This survey maps NLP applications for social good to global frameworks, finding uneven research distribution with AI harms and inclusion dominating while poverty, peacebuilding, and environment remain underexplored.

## Executive Summary
This survey examines how NLP can address pressing societal challenges by mapping research applications to UN Sustainable Development Goals and Global Risks. Through bibliometric analysis of ACL Anthology papers, the authors reveal that while AI harms and inclusion attract the most research, domains like poverty, peacebuilding, and environmental protection remain severely understudied despite their severity rankings. The paper identifies critical challenges including data scarcity, bias, misaligned evaluation metrics, and infrastructure gaps, while outlining opportunities through participatory design, human-AI collaboration, and improved explainability.

## Method Summary
The authors conducted a large-scale analysis of 47,078 ACL Anthology papers (2019-2024) using zero-shot classification with GPT-4.1-mini to categorize papers into 9 NLP4SG domains. They created taxonomies of 45 tasks and 49 methods using Gemini-2.5-flash and ChatGPT, then analyzed publication volumes and co-occurrence patterns. The mapping to SDGs and Global Risks was performed through systematic literature review and expert consultation, with results visualized in heatmaps and trend analyses.

## Key Results
- NLP research shows severe uneven distribution: AI harms and inclusion dominate with ~6000 papers each while poverty, peacebuilding, and environment protection remain near baseline
- Domain-specific data challenges fundamentally constrain NLP deployment: healthcare faces privacy/sensitivity issues, peacebuilding struggles with euphemism and low-resource languages, poverty lacks direct indicators
- The authors call for participatory design with affected communities, joint benchmarks with diverse data, and human-centered evaluation frameworks beyond accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping NLP applications to global frameworks (SDGs, Global Risks) reveals systematic research gaps that bibliometric analysis alone would miss.
- **Mechanism:** The framework operationalizes "social good" through two complementary lenses: SDGs provide development-oriented goals while Global Risks capture emerging technological and societal threats. This dual mapping exposes not just where research exists, but where it's misaligned with pressing needs—e.g., AI harms and inclusion attract the most research, while poverty, peacebuilding, and environment protection remain underexplored despite their severity rankings in Global Risks.
- **Core assumption:** The authors assume SDG and Global Risk frameworks capture what constitutes "social good" and that alignment with these frameworks indicates potential for positive impact. This assumption is acknowledged as subjective in the Limitations section.
- **Evidence anchors:**
  - [abstract] "This paper examines the role of NLP in addressing pressing societal challenges. Through a cross-disciplinary analysis of social goals and emerging risks..."
  - [section] Figure 1 maps nine NLP4SG domains to specific SDGs and Global Risks; Figure 2 shows publication volume disparities across domains (AI Harms peaks at ~6000 papers while Environment Protection and Poverty remain near baseline).
  - [corpus] Related work like "Good Intentions Beyond ACL" confirms similar patterns: ~20% of ACL papers address social good topics, but distribution remains uneven across SDG alignment.
- **Break condition:** This mechanism fails when (1) SDGs/Global Risks frameworks don't reflect community priorities in specific contexts, or (2) research misalignment reflects legitimate technical barriers rather than incentive gaps.

### Mechanism 2
- **Claim:** Domain-specific data characteristics create divergent NLP deployment barriers that require tailored approaches rather than unified solutions.
- **Mechanism:** Each domain presents distinct data challenges: healthcare has scarcity, sensitivity, and privacy constraints; peacebuilding faces euphemistic language, low-resource languages, and temporal volatility; poverty lacks direct indicators, forcing reliance on noisy proxies. These characteristics don't just affect data collection—they determine which NLP approaches are viable. For example, the paper notes that LLMs "lack the stability, contextual grounding, and ethical guarantees required for autonomous clinical or therapeutic decision-making" in healthcare, while conflict prediction suffers from domain drift as "protests, ceasefires, or escalations outpace model adaptation."
- **Core assumption:** The authors assume domain characteristics are fundamental constraints rather than engineering challenges that unified methods could overcome.
- **Evidence anchors:**
  - [section] Healthcare challenges: "data... is scarce, sensitive, and often biased, with limited language coverage and marginalized group representation raising ethical and privacy concerns"
  - [section] Peacebuilding challenges: "threats and violations are often euphemistic, coded, or strategically disguised... Data is fragmented across low-resource languages and dialects"
  - [corpus] "A Survey on Responsible LLMs" confirms domain-specific risk patterns differ significantly, requiring targeted mitigation strategies.
- **Break condition:** This mechanism fails if transfer learning or foundation models successfully generalize across domain-specific constraints, making domain-agnostic approaches viable.

### Mechanism 3
- **Claim:** Participatory design with affected communities and domain experts can bridge evaluation gaps that technical metrics cannot capture.
- **Mechanism:** Traditional NLP evaluation relies on accuracy-based metrics, but social good applications require assessing real-world impact, fairness, and alignment with community values. The paper proposes participatory approaches—collaborative data collection with cultural experts, co-designed fairness goals, human-in-the-loop evaluation—as a mechanism to operationalize impact measurement. For education, this means "aligning with expert-annotated pedagogical traces and grounding evaluations in curriculum outcomes"; for inclusion, it means "participatory approaches centering marginalized voices, including co-designed fairness goals."
- **Core assumption:** The authors assume affected communities can articulate evaluation criteria and that participatory processes produce valid, generalizable standards. This assumes resources and infrastructure for participation exist.
- **Evidence anchors:**
  - [section] "Future work should focus on specific groups such as teachers (Du et al., 2024), students (Shen and Cui, 2024), and other professionals... to obtain different perspectives"
  - [section] "Participatory design and evaluation ensure that systems are co-developed with affected communities"
  - [corpus] Related corpus lacks strong experimental validation of participatory evaluation effectiveness; this represents a gap requiring empirical investigation.
- **Break condition:** This mechanism fails when (1) communities lack capacity for meaningful participation, (2) participation becomes extractive without benefit sharing, or (3) participatory criteria conflict with technical feasibility.

## Foundational Learning

- **Concept: UN Sustainable Development Goals (SDGs) and Global Risks Framework**
  - **Why needed here:** The paper's entire analytical framework depends on mapping NLP work to these normative structures. You cannot evaluate "social good" alignment or identify research gaps without understanding what these frameworks prioritize and how they differ (SDGs are development-focused; Global Risks emphasize emerging threats).
  - **Quick check question:** Can you name three SDGs and explain how NLP research in misinformation would map to both an SDG and a Global Risk category?

- **Concept: Domain-Specific Data Constraints in NLP**
  - **Why needed here:** The paper argues that effective NLP4SG requires understanding why poverty, peacebuilding, and environment domains remain underexplored—not just that they are. Data scarcity, sensitivity, and proxy reliability fundamentally shape what's technically possible.
  - **Quick check question:** If you were building an NLP system for human rights violation detection, what three data characteristics would most constrain your approach, and how would they differ from a hate speech detection system?

- **Concept: Participatory Design and Human-Centered Evaluation**
  - **Why needed here:** The paper's primary recommendation for bridging the deployment gap is not technical—it's methodological. You need to understand how participatory approaches differ from standard benchmark-driven evaluation and why they're proposed as essential for NLP4SG.
  - **Quick check question:** What is one concrete way participatory evaluation would change your development process for an educational NLP tool, compared to standard accuracy-based evaluation?

## Architecture Onboarding

- **Component map:** Domain selection → Data constraint analysis → Partnership establishment → Method selection → Participatory evaluation design → Deployment with human oversight
- **Critical path:** Domain selection → Data constraint analysis → Partnership establishment → Method selection → Participatory evaluation design → Deployment with human oversight. Note: Partnership establishment should precede technical development, not follow it.
- **Design tradeoffs:**
  1. **Generalist vs. Specialist Models:** Generalist LLMs offer broad coverage but may lack domain grounding; specialist models require more resources but align better with specific needs
  2. **Automation vs. Human-in-the-loop:** Full automation risks harm in high-stakes domains; human oversight limits scalability but provides safety
  3. **Benchmark-driven vs. Impact-driven Evaluation:** Benchmarks enable comparison but may not capture real-world value; impact evaluation is harder to measure but more aligned with social good goals
- **Failure signatures:**
  1. **Deployment-Research Gap:** System performs well on benchmarks but fails in real-world conditions due to distributional shift (common in peacebuilding, healthcare)
  2. **Extractive Participation:** Community involvement in design without benefit sharing or ongoing engagement
  3. **Proxy Misalignment:** Using available data (e.g., income proxies) that don't reflect actual target outcomes (e.g., poverty experience)
  4. **Overreliance Effects:** Users trust system outputs beyond demonstrated reliability, particularly in healthcare counseling scenarios
- **First 3 experiments:**
  1. **Domain Mapping Exercise:** Select one underexplored domain (poverty, peacebuilding, or environment), identify three existing NLP papers addressing it, and map their approaches to the challenges outlined in the survey. Assess whether proposed solutions address core data constraints.
  2. **Participatory Evaluation Design:** For a selected NLP4SG application, design a participatory evaluation protocol including: stakeholder identification, evaluation criteria co-design process, and metrics that capture community-defined values beyond accuracy.
  3. **Partnership Feasibility Assessment:** Identify three domain experts or civil society organizations relevant to your chosen domain, research their existing AI/NLP engagement, and outline what a collaboration structure would require (time, resources, data sharing agreements, benefit-sharing mechanisms).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can joint benchmarks be developed featuring multilingual, culturally diverse, and socially grounded data for NLP4SG?
- Basis in paper: [explicit] The authors explicitly call on the community to "Develop joint benchmarks featuring multilingual, culturally diverse, and socially grounded data" in Section 11.
- Why unresolved: Current benchmarks are fragmented across domains, with poverty, peacebuilding, and environment protection remaining underexplored; cross-linguistic and cross-cultural generalizability is limited.
- What evidence would resolve it: Collaborative benchmark creation with affected communities and domain experts, demonstrating improved model performance across diverse linguistic and cultural contexts.

### Open Question 2
- Question: How can poverty-related indicators be reliably inferred from text when ground-truth data is scarce and rarely shared?
- Basis in paper: [explicit] Section 4 states: "A major challenge is the fact that poverty data are often scarce or incomplete. Indicators like income or poverty status are rarely shared, making it hard to infer them from text."
- Why unresolved: Privacy concerns and data sensitivity limit availability; proxies like mean income often do not accurately reflect true poverty levels.
- What evidence would resolve it: Datasets from data donations or user surveys labeled with income/socioeconomic status, and demonstrated success extracting poverty indicators via NLP methods.

### Open Question 3
- Question: How can NLP systems be evaluated beyond accuracy to reflect fairness, contextual understanding, empathy, and human-centered values?
- Basis in paper: [explicit] Section 2 states evaluation frameworks "must go beyond accuracy to reflect fairness, contextual understanding, human-centered values (e.g. empathy), and ensure reproducibility." Section 10 notes "lack of standardized practices for measuring real-world impact."
- Why unresolved: No consensus on holistic metrics; domain-specific values are difficult to operationalize; long-term user impact is understudied.
- What evidence would resolve it: Validated evaluation frameworks co-designed with domain experts and communities, showing correlation with real-world outcomes.

### Open Question 4
- Question: How do compounded intersectional forms of discrimination manifest in NLP systems, and how can they be detected and mitigated?
- Basis in paper: [explicit] Section 7 states "compounded forms of discrimination—arising at the intersection of race, gender, social status, and disability—remain underexplored."
- Why unresolved: Simplifying assumptions (binary gender, coarse proxies) dominate; limited intersectional frameworks exist.
- What evidence would resolve it: Intersectional bias benchmarks, compositional identity modeling, and demonstrated mitigation across multiple demographic dimensions simultaneously.

## Limitations

- The analysis is limited to ACL Anthology data, potentially underrepresenting industry and non-ACL academic contributions
- Mapping to SDGs and Global Risks reflects subjective judgments about what constitutes "social good" that may not align with all stakeholder perspectives
- The paper identifies research gaps but cannot definitively prove these gaps reflect priority misalignments rather than legitimate technical barriers

## Confidence

- **High Confidence:** Domain-specific data challenges (healthcare sensitivity, peacebuilding euphemism, poverty proxy limitations) are well-documented across multiple sources and represent fundamental technical constraints
- **Medium Confidence:** The claim that participatory design can bridge evaluation gaps is supported by theoretical arguments but lacks robust empirical validation in the corpus
- **Low Confidence:** The assertion that uneven research distribution reflects priority misalignment versus technical feasibility remains speculative without intervention studies

## Next Checks

1. **Cross-venue validation:** Replicate the bibliometric analysis using multiple NLP conference datasets (EMNLP, NAACL, ICML, ICLR) to assess whether ACL-specific patterns generalize

2. **Stakeholder alignment study:** Survey domain experts, civil society organizations, and affected communities to compare their priority rankings against the SDG/Global Risk mappings used in this analysis

3. **Participatory evaluation trial:** Design and execute a controlled experiment comparing standard benchmark evaluation versus participatory evaluation for a specific NLP4SG application, measuring differences in system development and deployment outcomes