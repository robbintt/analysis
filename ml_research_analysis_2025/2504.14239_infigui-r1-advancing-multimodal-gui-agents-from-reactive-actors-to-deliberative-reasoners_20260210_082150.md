---
ver: rpa2
title: 'InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative
  Reasoners'
arxiv_id: '2504.14239'
source_url: https://arxiv.org/abs/2504.14239
tags:
- reasoning
- arxiv
- agent
- agents
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving reasoning capabilities
  in Multimodal Large Language Model (MLLM) based GUI agents. Current approaches often
  rely on manually designed templates or lack sufficient depth for complex tasks requiring
  planning and error recovery.
---

# InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners

## Quick Facts
- **arXiv ID**: 2504.14239
- **Source URL**: https://arxiv.org/abs/2504.14239
- **Reference count**: 40
- **Primary result**: State-of-the-art 87.5% average ScreenSpot grounding accuracy with a 3B model, outperforming larger baselines.

## Executive Summary
InfiGUI-R1 addresses the fundamental limitations of reactive GUI agents by transforming them into deliberative reasoners. The paper proposes the Actor2Reasoner framework that moves beyond simple perception-action loops to enable planning, reflection, and error recovery. By employing a two-stage training approach combining Spatial Reasoning Distillation and Reinforcement Learning with Sub-goal Guidance, InfiGUI-R1-3B achieves state-of-the-art performance on both grounding and complex trajectory tasks while using a significantly smaller model size than previous approaches.

## Method Summary
The paper proposes transforming MLLM-based GUI agents from reactive Perception→Action models to deliberative Perception→Reasoning→Action systems through the Actor2Reasoner framework. This two-stage approach first injects spatial reasoning capabilities via Spatial Reasoning Distillation using teacher models (QwQ-32B, Qwen2.5-VL-32B), then enhances deliberation through Reinforcement Learning with Sub-goal Guidance and Error Recovery Scenario Construction. The training uses Qwen2.5-VL-3B-Instruct as the base model with a diverse dataset including AndroidControl trajectories, GUI grounding data, and MathV360K, achieving significant performance gains on ScreenSpot and AndroidControl-High benchmarks.

## Key Results
- Achieves state-of-the-art 87.5% average grounding accuracy on ScreenSpot across multiple platforms
- Delivers 71.1% success rate on complex AndroidControl-High trajectory tasks
- Outperforms larger models (Qwen2.5-VL-7B, Qwen2.5-VL-72B) despite using only a 3B parameter model
- Demonstrates strong generalization capabilities on unseen application scenarios

## Why This Works (Mechanism)
The framework succeeds by explicitly teaching GUI agents to reason about spatial relationships and plan multi-step actions rather than just reacting to immediate visual inputs. Spatial Reasoning Distillation injects foundational cross-modal reasoning capabilities by training the model to generate detailed spatial descriptions and logical step-by-step solutions. The Reinforcement Learning stage then refines these capabilities through sub-goal guided trajectories and error recovery scenarios, enabling the agent to handle complex tasks with planning and self-correction. This two-stage approach addresses the core limitation of current GUI agents: their inability to plan ahead and recover from failures in complex, multi-step tasks.

## Foundational Learning
- **Spatial Reasoning Distillation**: Converting visual GUI elements into structured spatial descriptions and logical reasoning steps; needed because GUI agents must understand spatial relationships to ground actions accurately; quick check: compare spatial description quality between baseline and distilled models.
- **Reinforcement Learning with Sub-goal Guidance**: Training agents to decompose complex tasks into manageable sub-goals and follow intermediate reasoning steps; needed because complex GUI tasks require planning beyond immediate perception; quick check: measure sub-goal adherence rates during task execution.
- **Error Recovery Scenario Construction**: Creating synthetic failure scenarios to teach agents recovery strategies; needed because real-world GUI interactions frequently encounter unexpected errors; quick check: evaluate recovery success rate on injected failure scenarios.

## Architecture Onboarding
- **Component Map**: Base Model (Qwen2.5-VL-3B) -> Spatial Reasoning Distillation (Stage 1) -> Reinforcement Learning with Sub-goal Guidance (Stage 2) -> InfiGUI-R1-3B Agent
- **Critical Path**: Reasoning Injection (SFT) → Deliberation Enhancement (RL) → Performance Evaluation
- **Design Tradeoffs**: Smaller 3B model with advanced reasoning framework vs larger models with simpler architectures; computational efficiency vs raw parameter count
- **Failure Signatures**: Incorrect spatial descriptions, failure to generate sub-goals, inability to recover from specified error types
- **First Experiments**: 1) Test base model performance on reasoning bottleneck samples, 2) Evaluate spatial reasoning quality after Stage 1 training, 3) Measure sub-goal adherence and error recovery success after Stage 2 training

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability and generalization of the Actor2Reasoner framework. It questions whether the reasoning capabilities learned through teacher model distillation can generalize beyond the specific patterns demonstrated during training, or if the model is merely mimicking the teacher's stylistic approaches. Additionally, the framework's effectiveness on base models significantly larger than 3B parameters remains unexplored, raising questions about whether the computational overhead of RL-based deliberation enhancement provides proportional benefits for models that already possess stronger implicit reasoning capabilities.

## Limitations
- Relies heavily on teacher model quality for spatial reasoning distillation, potentially limiting the reasoning ceiling
- Training on constructed recovery scenarios may lead to overfitting to specific error types rather than fostering generalizable recovery strategies
- Computational intensity of the two-stage training process, particularly the RL phase with multiple rollouts per sample
- Limited evaluation on extremely long and complex trajectories beyond the tested AndroidControl-High benchmark

## Confidence
- **State-of-the-art performance claims**: High - Well-validated against established benchmarks with clear performance improvements
- **Two-stage training methodology**: Medium - Sound theoretical foundation but limited ablation studies on individual components
- **Generalizability to unseen environments**: Low - Primary evaluation focuses on controlled benchmark scenarios
- **Scalability to larger base models**: Low - Only validated on 3B parameter model, scalability remains theoretical

## Next Checks
- Verify reasoning bottleneck sample identification by testing base model with and without sub-goal hints on the training dataset
- Evaluate spatial description quality by comparing generated descriptions against ground truth for GUI elements
- Test error recovery generalization by injecting novel failure modes into previously successful trajectories