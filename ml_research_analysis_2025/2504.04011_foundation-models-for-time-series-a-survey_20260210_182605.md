---
ver: rpa2
title: 'Foundation Models for Time Series: A Survey'
arxiv_id: '2504.04011'
source_url: https://arxiv.org/abs/2504.04011
tags:
- time
- series
- data
- forecasting
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive taxonomy of transformer-based
  foundation models for time series analysis, addressing the growing complexity in
  this rapidly evolving field. The authors systematically categorize models across
  multiple dimensions including architecture (encoder-only, decoder-only, encoder-decoder,
  and non-transformer), patching strategies (patch-based vs non-patch-based), objective
  functions (MSE, NLL, Huber), and task scope (univariate vs multivariate, probabilistic
  vs deterministic).
---

# Foundation Models for Time Series: A Survey

## Quick Facts
- arXiv ID: 2504.04011
- Source URL: https://arxiv.org/abs/2504.04011
- Authors: Siva Rama Krishna Kottapalli; Karthik Hubli; Sandeep Chandrashekhara; Garima Jain; Sunayana Hubli; Gayathri Botla; Ramesh Doddaiah
- Reference count: 40
- Primary result: Comprehensive taxonomy of transformer-based foundation models for time series analysis, categorizing 17 models across architecture, patching, objectives, and task scope

## Executive Summary
This survey provides a systematic taxonomy of transformer-based foundation models for time series analysis, addressing the growing complexity in this rapidly evolving field. The authors categorize models across multiple dimensions including architecture (encoder-only, decoder-only, encoder-decoder, and non-transformer), patching strategies (patch-based vs non-patch-based), objective functions (MSE, NLL, Huber), and task scope (univariate vs multivariate, probabilistic vs deterministic). The survey analyzes 17 prominent models including Tiny Time Mixers, TimeGPT, Chronos, and others, detailing their specific characteristics and performance. A key contribution is the structured framework that enables systematic comparison of these models, highlighting trade-offs between computational efficiency and predictive accuracy. The survey identifies critical challenges including model scalability, interpretability, and generalization across diverse time series datasets, while outlining promising directions for future research such as hybrid architectures and domain-specific adaptations.

## Method Summary
The survey systematically reviews transformer-based foundation models for time series analysis through comprehensive literature analysis. Models are categorized by architecture type (encoder-only, decoder-only, encoder-decoder, non-transformer), patching strategy (patch-based vs non-patch), objective functions (MSE, NLL, Huber), and task scope (univariate vs multivariate, probabilistic vs deterministic). The authors analyze 17 prominent models including Tiny Time Mixers, TimeGPT, Chronos, and others, documenting their specific characteristics, implementation details, and performance profiles. The methodology involves extracting key architectural decisions, training procedures, and evaluation metrics from each model's original paper, then organizing these findings into a unified taxonomy that enables systematic comparison and identifies research gaps.

## Key Results
- The taxonomy reveals distinct architectural trade-offs: decoder-only models excel at autoregressive generation but suffer from error accumulation, while encoder-only models better handle representation tasks but lack generative flexibility
- Patching strategies significantly impact computational efficiency, with patch-based approaches reducing quadratic complexity from O(n²) to O((n/p)²) where p is patch size, at the cost of potentially losing fine-grained temporal patterns
- Pretraining on heterogeneous time series data shows promise for cross-domain transfer, though the extent of generalization varies significantly based on pretraining data composition and target domain similarity

## Why This Works (Mechanism)

### Mechanism 1: Self-Attention Enables Long-Range Dependency Modeling
- Claim: If a model uses self-attention over recurrent processing, it may better capture dependencies across distant time steps.
- Mechanism: Self-attention computes pairwise relationships between all time steps simultaneously via Query-Key-Value matrices, allowing the model to focus on relevant observations regardless of temporal distance, rather than propagating information sequentially.
- Core assumption: Temporal dependencies in time series data span variable distances and are not confined to local windows.
- Evidence anchors:
  - [abstract] "classifying models by their architecture design, distinguishing between those leveraging patch-based representations and those operating directly on raw sequences"
  - [section 2.2.1] "The self-attention mechanism in Transformers computes pairwise relationships between the time steps, allowing the model to focus on relevant inputs from both distant and recent time points"
  - [corpus] Weak direct corpus evidence; related papers focus on positional encoding and interpretability rather than attention mechanics specifically.
- Break condition: If time series exhibits only short-term dependencies with negligible long-range patterns, recurrent models may achieve comparable performance with lower computational cost.

### Mechanism 2: Patching Reduces Sequence Complexity While Preserving Local Semantics
- Claim: If time series are segmented into fixed-length patches before attention, the model may achieve better computational efficiency while retaining local pattern information.
- Mechanism: Patching aggregates adjacent time steps into tokens (e.g., non-overlapping windows), reducing sequence length from n to n/patch_size, which lowers self-attention's quadratic complexity while creating semantically meaningful local representations.
- Core assumption: Local temporal patterns within patches carry meaningful structure that benefits from being processed as unified tokens.
- Evidence anchors:
  - [abstract] "distinguishing between those leveraging patch-based representations and those operating directly on raw sequences"
  - [section 4.2.1] "Patch-based time series Transformers segment the input sequence into fixed-length 'windows' or 'patches', which are treated as individual tokens. This approach allows the model to capture local temporal patterns within each segment before learning global dependencies"
  - [corpus] No direct corpus comparison; neighbor papers do not address patching trade-offs.
- Break condition: If the optimal pattern length varies significantly across datasets or time, fixed patch sizes may miss critical boundaries or introduce artificial segmentations.

### Mechanism 3: Pretraining on Heterogeneous Data Enables Cross-Domain Transfer
- Claim: If a model is pretrained on diverse time series domains, it may generalize to unseen datasets with minimal fine-tuning.
- Mechanism: Pretraining exposes the model to varied temporal patterns (seasonality, trends, noise structures) across domains, learning generalizable representations that transfer via fine-tuning, reducing task-specific data requirements.
- Core assumption: Time series from different domains share underlying structural properties that can be captured in unified representations.
- Evidence anchors:
  - [abstract] "highlights differences between lightweight architectures and large-scale foundation models"
  - [section 2.4.1] "Pretraining: Foundation models are trained on vast datasets to capture generalized patterns, structures, and dependencies... encapsulate temporal dependencies, multivariate interactions, and periodic patterns across diverse datasets"
  - [corpus] "Advancing Financial Engineering with Foundation Models" (arXiv:2507.18577) discusses generalization capabilities of FMs in financial contexts, supporting cross-domain transfer claims.
- Break condition: If target domain exhibits fundamentally different temporal dynamics (e.g., irregular sampling, different frequency regimes) not represented in pretraining data, transfer may degrade or require substantial fine-tuning.

## Foundational Learning

- Concept: Self-Attention Mechanics (Q/K/V computation, scaled dot-product)
  - Why needed here: Understanding how attention weights are computed from Query-Key similarity and applied to Values is essential for debugging attention patterns and interpreting model focus areas.
  - Quick check question: Given a sequence of 100 time steps, how does self-attention differ from RNN processing in terms of which past steps influence the current output?

- Concept: Pretraining-Fine-Tuning Paradigm
  - Why needed here: Foundation models operate in two stages; understanding what transfers from pretraining (general patterns) versus what fine-tuning adapts (domain specifics) informs data collection and training strategies.
  - Quick check question: If you have only 1,000 labeled samples for a new forecasting task, should you train from scratch or fine-tune a pretrained model? What factors determine the answer?

- Concept: Probabilistic vs. Deterministic Forecasting
  - Why needed here: Choosing between point forecasts (MSE) and distribution forecasts (NLL) depends on whether downstream decisions require uncertainty quantification.
  - Quick check question: For inventory management where stockouts are costly but overstocking is cheap, would you prefer probabilistic or deterministic forecasts? Why?

## Architecture Onboarding

- Component map: Input Time Series → [Patching (optional)] → Token Embedding → Positional Encoding → Transformer Layers (Encoder/Decoder/Both) → Task Head (Forecasting/Classification/Anomaly) → Loss Function (MSE/NLL/Huber) → Output

- Critical path:
  1. Determine task type (forecasting horizon, classification, anomaly detection)
  2. Assess data characteristics (univariate vs. multivariate, sequence length, sampling regularity)
  3. Select architecture family (encoder-only for representation learning, decoder-only for autoregressive generation, encoder-decoder for seq2seq)
  4. Choose patching strategy based on sequence length and computational budget
  5. Select objective function aligned with output requirements (probabilistic vs. deterministic)

- Design tradeoffs:
  | Dimension | Choice A | Choice B | Trade-off |
  |-----------|----------|----------|-----------|
  | Architecture | Decoder-only | Encoder-only | Decoder enables autoregressive generation; encoder better for representation tasks |
  | Patching | Patch-based | Non-patch | Patching reduces O(n²) complexity but may lose fine-grained patterns |
  | Output | Probabilistic | Deterministic | Probabilistic provides uncertainty but requires distributional assumptions |
  | Scale | Large (1B+ params) | Lightweight (<50M) | Large models generalize better; lightweight deployable on edge |

- Failure signatures:
  - Model fails to capture seasonality: Likely insufficient context length or positional encoding issue
  - Good training loss, poor validation: Overfitting; reduce model capacity or increase data augmentation
  - Erratic long-horizon forecasts: Error accumulation in autoregressive decoding; consider teacher forcing or shorter horizons
  - Poor transfer to new domain: Pretraining data mismatch; collect domain-specific fine-tuning data

- First 3 experiments:
  1. **Baseline comparison**: Run a patch-based model (e.g., PatchTST-style) vs. non-patch model on a standard benchmark (e.g., ETTh1) with identical hyperparameters to isolate patching impact.
  2. **Context length ablation**: Train with varying lookback windows (96, 336, 512 steps) to identify the minimum context needed for your target horizon without performance degradation.
  3. **Fine-tuning data budget**: Fine-tune a pretrained model (e.g., MOMENT or TimesFM) with varying percentages of domain data (10%, 30%, 100%) to quantify transfer efficiency and determine acceptable data collection thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications can enable point-forecasting models (e.g., TTM, TimesFM, Timer) to provide probabilistic outputs with uncertainty quantification without significant computational overhead?
- Basis in paper: [explicit] The survey notes multiple models "only generate point forecasts and doesn't facilitate probabilistic forecasts as it does not use distribution head in its architecture" and that TimesFM "does not explicitly support probabilistic forecasting, limiting its ability to capture and convey forecasting uncertainty."
- Why unresolved: Adding distribution heads changes training objectives and inference complexity; the trade-off between probabilistic capability and efficiency remains unexplored across architectures.
- What evidence would resolve it: Comparative benchmarks of modified architectures with distribution heads measuring both uncertainty calibration metrics and inference latency across standardized datasets.

### Open Question 2
- Question: How can foundation models handle variable context lengths and multi-resolution inputs without requiring separate model instances for each configuration?
- Basis in paper: [explicit] "Tiny Time Mixers only generates point forecasts and doesn't facilitate probabilistic forecasts... Another limitation of TTM is the need to train different models for different context length settings. Due to its non-Transformer-based architecture, TTM is sensitive to context lengths." Also, "Timer-XL doesn't incorporate multi-resolution patches for input and output series."
- Why unresolved: Current tokenization and patching strategies assume fixed configurations; flexible mechanisms would require architectural innovations in positional encoding and attention.
- What evidence would resolve it: A single model architecture demonstrating consistent performance across varying context lengths (e.g., 96, 192, 336, 720 time steps) and multiple input resolutions on benchmark datasets.

### Open Question 3
- Question: What mechanisms can effectively integrate multi-modal inputs (tabular data, textual context, exogenous variables) into time series foundation models while preserving temporal modeling fidelity?
- Basis in paper: [explicit] "Future work should also focus on broadening LLMs' capabilities, not just for time series, but across a variety of other complex tasks" and regarding MOIRAI: "the future integration of multi-modal inputs, such as tabular or textual data, would further unlock the potential." Also, TimeGPT's "reliance solely on historical load data restricts its ability to incorporate external factors, such as Numerical Weather Prediction (NWP) data."
- Why unresolved: Cross-modal fusion architectures for time series remain underexplored; aligning embeddings across modalities with different temporal granularities is non-trivial.
- What evidence would resolve it: Benchmark results on datasets with paired multi-modal inputs (e.g., financial news + stock prices, weather data + energy demand) showing improved forecasting accuracy over unimodal baselines.

### Open Question 4
- Question: Can iterative error accumulation in autoregressive decoder-only architectures be mitigated through architectural or training innovations for long-horizon forecasting?
- Basis in paper: [explicit] "Timer-XL necessitates iterative generation for long-term forecasting, which may lead to error accumulation and inflexibility in the output length."
- Why unresolved: Autoregressive generation inherently compounds prediction errors; direct multi-step alternatives sacrifice the generative flexibility that decoder architectures provide.
- What evidence would resolve it: Comparative error analysis across forecast horizons (short vs. long-term) measuring error propagation rates in autoregressive versus non-autoregressive variants on long-sequence benchmarks.

## Limitations
- The survey captures a snapshot of rapidly evolving field that may quickly become outdated as new models emerge
- Performance comparisons between different architectures are limited to qualitative descriptions rather than comprehensive empirical benchmarking across diverse datasets
- Focus on transformer-based approaches may overlook emerging non-transformer architectures that could offer competitive performance with lower computational requirements

## Confidence
- **High confidence**: The taxonomy framework and categorization methodology (Medium-High) - The systematic classification of models by architecture, patching strategy, objective functions, and task scope is well-supported by the literature and represents a sound analytical approach.
- **Medium confidence**: Claims about specific model performance characteristics (Medium) - While the survey accurately describes the architectural features of individual models like Tiny Time Mixers, TimeGPT, and Chronos, the performance claims are largely based on original paper results rather than independent validation across standardized benchmarks.
- **Low confidence**: Generalization and transfer learning claims (Low-Medium) - The assertion that pretraining on heterogeneous data enables effective cross-domain transfer is theoretically sound but lacks comprehensive empirical validation across diverse real-world scenarios.

## Next Checks
1. **Benchmark replication study**: Implement and compare 3-5 representative models from different architectural families (e.g., patch-based vs non-patch, encoder-only vs decoder-only) on standardized time series benchmarks (ETTh1, Weather, Traffic) to empirically validate the performance trade-offs described in the taxonomy.

2. **Transfer learning validation**: Design a controlled experiment testing cross-domain generalization by pretraining on one time series domain (e.g., energy consumption) and fine-tuning on a structurally different domain (e.g., financial data), measuring performance degradation and required fine-tuning data volumes.

3. **Computational efficiency analysis**: Systematically measure the computational trade-offs between patch-based and non-patch approaches across varying sequence lengths and hardware constraints, validating the claimed O(n²) complexity benefits against actual training/inference times.