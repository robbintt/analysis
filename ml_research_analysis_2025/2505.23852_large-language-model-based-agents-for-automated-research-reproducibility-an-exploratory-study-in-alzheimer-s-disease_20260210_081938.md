---
ver: rpa2
title: 'Large Language Model-Based Agents for Automated Research Reproducibility:
  An Exploratory Study in Alzheimer''s Disease'
arxiv_id: '2505.23852'
source_url: https://arxiv.org/abs/2505.23852
tags:
- agents
- data
- studies
- methods
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the use of Large Language Model (LLM) agents
  to automate reproducibility of published Alzheimer's Disease research. Using GPT-4o
  agents with defined roles (Planner, Engineer, Scientist, Critic, Executor), the
  system attempted to reproduce findings from five highly cited NACC-based studies
  using only study abstracts, methods, and dataset descriptions.
---

# Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease

## Quick Facts
- arXiv ID: 2505.23852
- Source URL: https://arxiv.org/abs/2505.23852
- Reference count: 40
- Primary result: LLM agents reproduced approximately 53.2% of key findings from five Alzheimer's Disease studies using only abstracts and methods sections

## Executive Summary
This exploratory study investigated whether LLM-based autonomous agents could reproduce published Alzheimer's Disease research findings. Using GPT-4o agents with defined roles (Planner, Engineer, Scientist, Critic, Executor), the system attempted to reproduce findings from five highly cited NACC-based studies using only study abstracts, methods, and dataset descriptions. On average, LLM agents reproduced 53.2% of key findings per study. While numeric values and statistical methods often differed from originals, agents successfully replicated some study trends and significances. The results demonstrate both the potential and limitations of LLM agents for automating scientific reproducibility assessments, highlighting challenges in exact replication while suggesting promise for scalable evaluation of research rigor.

## Method Summary
The study employed GPT-4o agents within the Autogen v2 framework to reproduce published AD research findings. Agents were assigned specific roles: Planner (strategy development), Engineer (code writing), Scientist (method validation), Critic (review), and Executor (code execution). The system used only abstract and methods text from studies, along with 15-30 pre-selected NACC dataset columns. A user-in-the-loop approach required approval at key decision points. The workflow involved iterative code writing and execution, with error feedback loops allowing self-correction. Success was measured by alignment with abstract-stated findings, either through numeric values within 1.0 or Boolean matches.

## Key Results
- LLM agents reproduced 53.2% of key findings across 35 total findings from five studies
- Python code executions failed in 26/55 attempts (47.2%), but all errors were eventually corrected through iteration
- Agents successfully replicated study trends and statistical significance in some cases, despite numeric differences
- Implementation flaws and missing methodological information caused failures in 6/14 method comparisons

## Why This Works (Mechanism)

### Mechanism 1: Iterative Feedback Loops
- **Claim:** Iterative feedback loops allow LLM agents to self-correct code execution errors that would otherwise halt a static analysis script.
- **Mechanism:** The Executor agent attempts to run Python code generated by the Engineer. If the code fails (raises an exception), the error traceback is fed back into the conversation history. The Engineer then analyzes the error and revises the code. This cycle repeats until successful execution or a token limit is reached.
- **Core assumption:** The LLM possesses sufficient coding proficiency to interpret standard Python tracebacks and generate valid syntax corrections.
- **Evidence anchors:** LLM agents were tasked with writing and executing code to dynamically reproduce findings; Python code executions failed in 26/55 attempts but all were eventually corrected; Computational Reproducibility suggests code often fails due to environment or documentation issues.
- **Break condition:** The error lies in logical reasoning rather than syntax, or the model enters a "repair loop" where it repeatedly fails to fix the same error.

### Mechanism 2: Role-Based Decomposition
- **Claim:** Role-based decomposition improves task alignment by separating planning, coding, and critique into distinct contexts.
- **Mechanism:** The system assigns specific personas (Planner, Engineer, Scientist, Critic) rather than using a single monolithic prompt. This reduces cognitive load on any single generation step, allowing the Planner to set strategy before the Engineer writes code, while the Critic reviews adherence to the plan.
- **Core assumption:** The Manager agent can effectively route conversation flow to the appropriate agent based on the current problem state.
- **Evidence anchors:** Five agent roles were declared with specific responsibilities; an agent manager facilitates agent-to-agent communication and chooses the next appropriate agent; the Aleks system similarly utilizes multi-agent systems to handle complex data analysis.
- **Break condition:** Role instructions are too vague, causing agents to hallucinate capabilities.

### Mechanism 3: Explicit Context Injection
- **Claim:** Performance relies on explicit injection of data schemas and methodological excerpts into the prompt context (grounding).
- **Mechanism:** Agents do not "know" the dataset; they rely on manually selected variables from the NACC data dictionary and full text of the Abstract/Methods provided in the prompt. This constrains the search space for column names and statistical parameters.
- **Core assumption:** The provided text contains sufficient information to reconstruct statistical logic, and the data dictionary accurately reflects the dataset structure.
- **Evidence anchors:** The prompt included full Abstract and Methods sections and manually selected variables from the NACC data dictionary; agents failed because of implementation flaws or missing methodological information; lack of documentation is a primary barrier in computational reproducibility.
- **Break condition:** The original study references external methods without defining them in the provided text, causing the agent to default to general approximations.

## Foundational Learning

- **Concept: Agent Orchestration (Autogen)**
  - **Why needed here:** You must understand how the Manager agent selects which agent speaks next to debug why the system might be stuck in a loop or ignoring the Critic.
  - **Quick check question:** In the Autogen framework, does the Executor agent run code locally or send it to an external API, and how does the error message return to the Engineer?

- **Concept: Statistical Fidelity vs. Trend Matching**
  - **Why needed here:** The study distinguishes between reproducing a "numeric value" vs. a "Boolean assertion" or trend. You need this to interpret the 53.2% success metric correctly.
  - **Quick check question:** If an agent reproduces the correct "trend" (e.g., Group A > Group B) but the p-value differs significantly due to using unadjusted vs. adjusted regression, does the paper classify this as a "reproduced finding"?

- **Concept: Context Window Limitations**
  - **Why needed here:** The study limits input to "Abstract + Methods + 15-30 columns." Understanding token limits explains why they couldn't feed the full dataset schema or referenced papers.
  - **Quick check question:** Why did the authors exclude figures and tables from the prompt, and how might this affect the agent's ability to reproduce results described only in a table legend?

## Architecture Onboarding

- **Component map:** Admin agent -> Manager -> [Planner, Engineer, Scientist, Critic, Executor] -> NACC dataset -> Results
- **Critical path:**
  1. Data Prep: Manual selection of relevant NACC data columns and extraction of Abstract/Methods text
  2. Prompting: Admin agent sends initial task + context to the Manager
  3. Planning: Planner proposes a stepwise analysis plan
  4. Execution: Engineer writes code -> Executor runs code -> Error/Success returned
  5. Validation: Agents compare output to Abstract assertions; Manager decides to terminate or retry

- **Design tradeoffs:**
  - **Generality vs. Accuracy:** Using general-purpose agents (GPT-4o) allows handling various study types but leads to "methodological drift" (e.g., defaulting to standard regression when domain-specific adjustments are needed)
  - **Autonomy vs. Control:** The system allows user-in-the-loop approvals, but this slows down the process compared to fully autonomous runs
  - **Prompt Size:** Limiting input to Abstract/Methods avoids context overflow but omits critical details often buried in Results tables or supplementary materials

- **Failure signatures:**
  - **Methodological Approximation:** Agent uses a "standard" statistical test when a specialized one was required but not fully detailed in the text
  - **Reference Shift:** Agent inverts comparison groups, leading to numerically inverted results
  - **Hallucinated Columns:** Agent attempts to use a column name that sounds plausible but does not exist in the provided dictionary

- **First 3 experiments:**
  1. **Baseline Run:** Attempt to reproduce a single study from the paper using only the Abstract and a schema file to verify the "47% error rate" and self-correction loop in your local environment
  2. **Method Injection:** Manually add the specific statistical formula to the Scientist agent's system message and measure if numeric precision improves
  3. **Critic Ablation:** Remove the Critic agent from the team and observe if the Engineer converges on incorrect code faster or if the error rate increases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does LLM agent performance in reproducing research generalize to other biomedical domains or larger study cohorts?
- **Basis in paper:** The authors state in the Limitations section that expanded evaluations across different disciplines are necessary because this study was restricted to only five studies within Alzheimer's Disease.
- **Why unresolved:** The observed 53.2% reproduction rate may be specific to the NACC dataset structure or the specific statistical methods common in AD research.
- **What evidence would resolve it:** Executing the same agent-based protocol on a diverse sample of studies from fields like oncology or cardiology.

### Open Question 2
- **Question:** To what extent does providing structured context, such as tables, figures, or referenced papers, improve the accuracy of automated reproducibility?
- **Basis in paper:** The authors note that excluding results section tables/figures and failing to include referenced methodological details likely hindered the agents' ability to reproduce specific domain methods.
- **Why unresolved:** The current study design strictly limited input to text from Abstracts and Methods sections, creating a "lack of contextual memory" and missing key data definitions.
- **What evidence would resolve it:** Ablation studies comparing the reproducibility success rate of agents given text-only inputs versus those provided with multi-modal prompts including tables and figures.

### Open Question 3
- **Question:** Would biomedical-specific fine-tuning or alternative agent architectures improve methodological fidelity compared to general models?
- **Basis in paper:** The Limitations section highlights that the study relied on a single model (GPT-4o) and a predefined set of agent roles which may not be optimal for complex research tasks.
- **Why unresolved:** It is unclear if the failure to implement domain-specific methods was due to the general-purpose nature of the model or the agent configuration.
- **What evidence would resolve it:** Benchmarking the reproducibility task using specialized biomedical LLMs or dynamic agent role configurations against the current baseline.

## Limitations
- The study's reliance on abstract and methods sections alone creates significant knowledge gaps for reproducing complex statistical analyses
- The 47.2% code execution failure rate, while eventually corrected through iteration, suggests substantial fragility in the automated pipeline
- The evaluation metric of "53.2% reproducibility" conflates exact numeric replication with trend matching, making it unclear whether agents truly reproduce findings or merely approximate their direction

## Confidence
- **High confidence:** The iterative error-correction mechanism works for syntactic coding errors (evidence: all 26 failed executions eventually succeeded)
- **Medium confidence:** Role-based decomposition improves task alignment (evidence: qualitative improvement over single-prompt approaches, but Critic agent often failed to catch errors)
- **Low confidence:** The 53.2% success rate accurately represents research reproducibility capability (evidence: metric conflates different types of alignment; no human baseline comparison provided)

## Next Checks
1. **Human benchmark comparison:** Have domain experts attempt to reproduce the same findings using only abstract/methods text, measuring if LLM performance matches or exceeds human capabilities
2. **Method injection test:** For one study, provide the exact statistical formulas and covariate adjustment procedures to the Scientist agent and measure improvement in numeric precision vs. the original approach
3. **Critical ablation study:** Remove the Critic agent from the team and measure changes in convergence speed and error rate to quantify the value of redundant validation