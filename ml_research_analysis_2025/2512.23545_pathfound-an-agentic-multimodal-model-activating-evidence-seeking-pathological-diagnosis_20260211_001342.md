---
ver: rpa2
title: 'PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological
  Diagnosis'
arxiv_id: '2512.23545'
source_url: https://arxiv.org/abs/2512.23545
tags:
- diagnosis
- diagnostic
- cell
- information
- further
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PathFound, an agentic multimodal model designed
  for evidence-seeking pathological diagnosis. Unlike traditional models that perform
  one-time inference on whole-slide images, PathFound iteratively refines diagnoses
  by proactively acquiring targeted visual and clinical evidence.
---

# PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis

## Quick Facts
- arXiv ID: 2512.23545
- Source URL: https://arxiv.org/abs/2512.23545
- Reference count: 40
- Primary result: PathFound achieves state-of-the-art accuracy (e.g., 92.28% balanced accuracy on RCC subtyping) by iteratively refining diagnoses through targeted evidence acquisition.

## Executive Summary
PathFound introduces an agentic multimodal model that transforms pathological diagnosis from static inference to iterative, evidence-driven reasoning. Unlike traditional models that make one-time predictions on whole-slide images, PathFound mimics clinical workflows by proactively seeking visual and clinical evidence to refine diagnoses. The system combines three specialized modules—slide highlighter, vision interpreter, and diagnostic reasoner—to generate differential diagnoses, select diagnostically relevant regions, and integrate external examination results. Across renal cell carcinoma subtyping, prostate adenocarcinoma grading, and lymphovascular invasion detection, PathFound significantly outperforms existing approaches by leveraging targeted evidence acquisition.

## Method Summary
PathFound operates through a three-stage protocol: initial diagnosis generation, evidence-seeking refinement, and final decision consolidation. The slide highlighter uses morphological prototypes to identify diagnostically relevant regions at multiple magnifications, while the vision interpreter translates visual features into medical descriptions. The diagnostic reasoner, trained via reinforcement learning with verifiable rewards, generates differential diagnoses and decides when to trigger evidence-seeking. The system employs prototype-based region selection, instruction-tuned vision-language models, and GRPO-trained reasoning to iteratively refine diagnoses based on targeted visual evidence and simulated external examination results.

## Key Results
- PathFound achieves 92.28% balanced accuracy on renal cell carcinoma subtyping, outperforming baseline models by 30-40%
- The evidence-seeking protocol corrects initial diagnostic errors, boosting accuracy from 49.58% to 92.28% on challenging cases
- Combining visual re-observation (+23.57%) and external examination results (+28.91%) yields optimal performance across all tested pathology tasks

## Why This Works (Mechanism)

### Mechanism 1: Hypothesis-Driven Iterative Refinement
Formulating differential diagnoses before seeking evidence improves final diagnostic accuracy compared to single-pass prediction. The diagnostic reasoner first generates ranked hypotheses and an action plan, then re-triggers visual modules with task-specific goals, and finally consolidates evidence. This creates an outer loop where slide observation serves diagnostic reasoning rather than a fixed prediction objective.

### Mechanism 2: Task-Specific Toolkit-Guided Region Selection
Prototypes from vision foundation models enable semantic retrieval of diagnostically relevant regions without task-specific retraining. The slide highlighter maps WSI patches to morphological prototypes via cosine similarity, then selects RoIs using either region-level semantic grounding (top-k + random sampling) or entity-level localization. Different toolkits are activated based on diagnostic stage.

### Mechanism 3: Reinforcement Learning with Verifiable Rewards for Tool-Calling Behavior
RLVR trains the diagnostic reasoner to appropriately call re-observation tools and request examinations. GRPO optimizes a compound reward including rank-sensitive diagnostic accuracy (Rd), examination consistency (Re), and tool-calling bonus/penalty (Rt). The model learns when to seek more evidence vs. when to conclude.

## Foundational Learning

- **Vision-Language Models (VLMs) with modality adapters**: The vision interpreter must translate visual features into medical descriptions; understanding how visual encoders connect to LLM decoders explains RoI-to-text conversion. *Quick check*: Can you explain the difference between CLIP-style contrastive pretraining and generative VLM instruction tuning?

- **Reinforcement Learning from Human Feedback / GRPO**: The diagnostic reasoner uses RLVR with group-relative policy optimization; understanding policy gradients and reward shaping is essential for debugging training. *Quick check*: What is the difference between a format reward, a task reward, and a consistency reward in RLVR?

- **Whole-Slide Image Pyramids and Magnification Levels**: The slide highlighter operates at different magnifications (5×, 10×, 20×) for different tasks; understanding multi-resolution pathology is critical for toolkit design. *Quick check*: Why might invasion detection favor 5×/10× while nuclear grading requires 20×?

## Architecture Onboarding

- **Component map**: WSI → [Slide Highlighter + Toolkit Library] → RoIs → [Vision Interpreter + ICL] → Visual Findings → [Diagnostic Reasoner (GRPO-trained)] → Action Plan / Diagnosis → Re-trigger Slide Highlighter (specific toolkit) → Targeted RoIs → Vision Interpreter ↘→ External Request → Simulated/Ground-Truth Examination Results

- **Critical path**: The diagnostic reasoner's ability to generate actionable tool calls (e.g., `tool-ccRCC`, `tool-Nuclear`) determines whether the system enters meaningful evidence-seeking. If the initial differential is poor, subsequent tool calls are misdirected.

- **Design tradeoffs**: More RoIs improve coverage but increase context length and latency; ablation shows saturation at ~8 RoIs. ICL with reference images improves fine-grained judgment but requires prototype images per entity; scaling to new entities needs new prototypes. Separate VLM + LLM (7B + 32B) enables explicit reasoning but adds complexity vs. single unified model.

- **Failure signatures**: Low PEMR (Proactive Evidence Mentioning Rate) → model not triggering evidence-seeking appropriately. Long differential lists with high DDx accuracy but low final accuracy → lack of confidence, not genuine reasoning. High recall / low precision on invasion detection → over-predicting without sufficient evidence.

- **First 3 experiments**:
  1. Replicate RCC subtyping with one-pass vs. evidence-seeking protocols on TCGA-RCC subset; verify ~30%+ gap as reported.
  2. Ablate the tool-call reward (Rt) during GRPO training; measure PEMR change to confirm RLVR drives evidence-seeking behavior.
  3. Test ICL with varying numbers of reference images (1, 5, 10) on nuclear grading; verify saturation point reported at ~5-10 samples.

## Open Questions the Paper Calls Out

- Can PathFound generalize to completely unseen disease entities with only minimal prior information (few reference images and brief descriptions) without requiring pre-defined toolkits?
- Can the current modular architecture (separate VLM + diagnostic reasoner) be unified into a single vision-language model with sufficient complex reasoning capabilities?
- How does PathFound perform when receiving real clinical laboratory results versus the LLM-simulated examination results used in evaluation?
- Can proactive evidence-seeking behavior trained on specific organ systems transfer effectively to entirely new diagnostic contexts without explicit tool-calling training?

## Limitations

- The prototype-based region selection approach may not scale well to rare entities or morphological variants not represented in the prototype library.
- The evidence-seeking protocol assumes synchronous tool execution and ground-truth examination results, which doesn't reflect real clinical workflows where test results arrive asynchronously.
- The clinical reasoning fidelity claim relies on qualitative observations rather than quantitative comparison with actual pathologist behavior.

## Confidence

- **High Confidence**: The iterative refinement mechanism demonstrably improves diagnostic accuracy over one-pass inference, with ablation studies clearly showing component contributions.
- **Medium Confidence**: The prototype-based region selection generalizes well within tested morphological space, but robustness to outliers remains unproven.
- **Low Confidence**: The claim that PathFound "closely mimics" expert workflows relies on qualitative observations rather than quantitative comparison with actual pathologist behavior.

## Next Checks

1. **Prototype Robustness Test**: Evaluate PathFound on intentionally held-out rare entities or morphological variants not present in the prototype library to quantify generalization limits.

2. **Asynchronous Examination Simulation**: Modify the evidence-seeking protocol to handle delayed and uncertain external examination results (10-60 minute latencies, probabilistic confidence scores) and measure impact on diagnostic accuracy.

3. **Human-AI Protocol Comparison**: Conduct a controlled study where expert pathologists solve the same cases with and without PathFound's evidence-seeking interface, comparing diagnostic accuracy, time-to-diagnosis, and evidence utilization patterns.