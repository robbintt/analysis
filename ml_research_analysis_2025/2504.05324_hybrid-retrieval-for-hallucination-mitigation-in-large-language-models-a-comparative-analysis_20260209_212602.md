---
ver: rpa2
title: 'Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A
  Comparative Analysis'
arxiv_id: '2504.05324'
source_url: https://arxiv.org/abs/2504.05324
tags:
- retrieval
- arxiv
- hybrid
- query
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the impact of hybrid retrieval on reducing
  hallucinations in Large Language Models (LLMs) by combining sparse (BM25) and dense
  (semantic) retrieval with query expansion and dynamic weighting. Using the HaluBench
  dataset, the proposed hybrid retriever (RetHyb-RRF) achieved significantly higher
  retrieval performance with MAP@3 of 0.897 and NDCG@3 of 0.915, compared to sparse
  (0.724, 0.732) and dense (0.768, 0.783) retrievers.
---

# Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis

## Quick Facts
- arXiv ID: 2504.05324
- Source URL: https://arxiv.org/abs/2504.05324
- Reference count: 40
- This study evaluates hybrid retrieval for hallucination mitigation in LLMs using BM25 and semantic retrieval with query expansion and dynamic weighting.

## Executive Summary
This paper presents a hybrid retrieval approach (RetHyb-RRF) that combines sparse (BM25) and dense (semantic) retrieval with query expansion and dynamic weighting to reduce hallucinations in large language models. Using the HaluBench dataset, the method achieves significantly higher retrieval performance with MAP@3 of 0.897 and NDCG@3 of 0.915, compared to sparse (0.724, 0.732) and dense (0.768, 0.783) retrievers. In hallucination mitigation, the hybrid approach delivered the highest accuracy (up to 92%), lowest hallucination rate (as low as 4%), and lowest rejection rate (as low as 2%) across multiple datasets.

## Method Summary
The method implements query expansion using WordNet to add top-2 synonyms, then runs parallel BM25 sparse retrieval and dense semantic retrieval using all-mpnet-base-v2 embeddings. A dynamic weighting mechanism calculates query specificity via TF-IDF average to set weights (specific queries favor BM25, generic favor dense). The hybrid results are fused using weighted Reciprocal Rank Fusion (RRF) with dynamic weights, retrieving top-3 documents. These documents are passed to LLaMA-3-8B-Instruct with zero-shot prompting for generation, targeting intrinsic hallucinations where answers contradict retrieved context.

## Key Results
- Hybrid retriever achieved MAP@3 of 0.897 and NDCG@3 of 0.915, significantly outperforming sparse (0.724, 0.732) and dense (0.768, 0.783) retrievers
- Highest accuracy up to 92%, lowest hallucination rate as low as 4%, and lowest rejection rate as low as 2% across multiple datasets
- The dynamic weighting mechanism based on query specificity effectively balanced lexical and semantic retrieval strengths

## Why This Works (Mechanism)

### Mechanism 1
Fusing sparse and dense retrieval via weighted Reciprocal Rank Fusion (RRF) appears to mitigate hallucinations by improving the relevance and ranking of retrieved context compared to single-method retrieval. The system runs parallel sparse (BM25) and dense (semantic) retrievers. Instead of averaging scores, it uses RRF (based on rank positions) with dynamic weights. If a query is "specific" (high TF-IDF), it weights the sparse retriever higher; if "generic," it weights the dense retriever higher. This attempts to capture both exact term matches and semantic intent. Core assumption: The "specificity" of a query (calculated via TF-IDF) reliably predicts whether lexical (sparse) or semantic (dense) retrieval will yield better results. Break condition: If a query is lexically specific but requires semantic understanding (e.g., complex technical jargon where keywords mislead), the dynamic weighting may over-prioritize the sparse retriever, degrading context quality.

### Mechanism 2
Query expansion using lexical databases likely reduces the "lexical chasm," increasing the probability that the retriever finds relevant documents even when user terminology differs from the corpus. Before retrieval, the system expands the query using WordNet to add top-2 synonyms for query terms. This broadens the search scope, aiming to bridge the gap between user vocabulary and document vocabulary. Core assumption: General-purpose synonyms from WordNet align with the domain-specific context of the query without introducing noise or semantic drift. Break condition: Polysemous words may cause the expansion to add irrelevant synonyms, retrieving off-topic documents and increasing hallucination risk.

### Mechanism 3
Higher retrieval precision (MAP@3/NDCG@3) correlates with lower hallucination rates, suggesting that providing the LLM with highly relevant context constrains the model to generate answers grounded in that context rather than fabricating information. By maximizing the relevance of the top 3 documents, the LLM (Llama-3) has sufficient "grounding" evidence. This reduces "intrinsic hallucinations" (where the answer contradicts the context) and rejection rates (where the model refuses to answer due to lack of context). Core assumption: The LLM possesses the reasoning capability to utilize the provided context correctly if the context is relevant. Break condition: If the LLM is not instruction-tuned to prioritize context over its internal weights, it may ignore even highly relevant retrieved documents, leading to persistence of hallucinations.

## Foundational Learning

- **Concept: Reciprocal Rank Fusion (RRF)**
  - **Why needed here:** You cannot simply add the scores of BM25 (sparse) and Cosine Similarity (dense) because they operate on different scales. RRF allows you to merge ranked lists using rank positions.
  - **Quick check question:** If Document A is rank 1 in Sparse and rank 10 in Dense, and Document B is rank 2 in both, which one does RRF likely favor? (Answer: Document B, as RRF sums the reciprocals of ranks).

- **Concept: Sparse vs. Dense Retrieval**
  - **Why needed here:** The system relies on the distinct strengths of these methods. Sparse (BM25) is fast and good for exact keywords; Dense (Embeddings) is good for synonyms and concepts. You need to know why both are running.
  - **Quick check question:** Why would a query for "The bank rate" be difficult for a pure sparse retriever compared to a dense retriever? (Answer: "Bank" is ambiguous—financial vs. river—dense embeddings capture context better).

- **Concept: Intrinsic vs. Extrinsic Hallucination**
  - **Why needed here:** The paper explicitly targets *intrinsic* hallucinations (answers contradicting the context). It does not claim to fix *extrinsic* hallucinations (answers that contradict world facts but agree with context).
  - **Quick check question:** If the retrieved document says "The moon is made of cheese" and the LLM answers "The moon is cheese," is this an intrinsic hallucination? (Answer: No, it is faithful to the context, even if factually false).

## Architecture Onboarding

- **Component map:** User Query -> Query Expansion (WordNet) -> Dual Retrieval (BM25 Sparse + Dense Semantic) -> Dynamic Weighting (TF-IDF Specificity) -> Weighted RRF Fusion -> Top-3 Documents -> LLaMA-3-8B-Instruct Generation

- **Critical path:** The path from Raw Query to Dynamic Weighting. If the specificity score is calculated incorrectly or the query expansion drifts the meaning, the fusion weights will be wrong, and the wrong documents will be retrieved, causing the LLM to hallucinate.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Running dual retrievers and query expansion increases latency compared to a single sparse index.
  - **Static vs. Dynamic Weights:** The system uses heuristics (TF-IDF) for weights. A learned router (using a classifier) might be more adaptive but adds training complexity.

- **Failure signatures:**
  - **High Rejection Rate:** Indicates the "Dynamic Weighting" or "Query Expansion" failed to find relevant context.
  - **Domain Failure:** Poor performance on FinanceBench/CovidQA suggests general-purpose WordNet expansion and generic embeddings struggle with highly specialized terminology.

- **First 3 experiments:**
  1. **Ablation on Fusion:** Run the pipeline with *only* RRF (no dynamic weights, fixed 50/50) vs. the proposed Dynamic RRF to isolate the contribution of the weighting logic.
  2. **Expansion Noise Test:** Manually inspect retrieval results for queries with polysemous terms to verify if WordNet adds confusing synonyms.
  3. **Sensitivity Analysis:** Vary the `k` value in retrieval (Top-1 vs Top-3 vs Top-5) to see if hallucination rates increase with more context (potential "lost in the middle" phenomenon).

## Open Questions the Paper Calls Out
1. How do advanced re-ranking algorithms compare to Reciprocal Rank Fusion (RRF) in refining the top-k retrieved documents for hallucination mitigation? The conclusion states future work should explore "incorporating advanced re-ranking algorithms to further refine the selection of retrieved documents."
2. To what extent does the proposed hybrid retrieval method generalize to Large Language Models with different parameter sizes or architectures? The conclusion proposes to "investigate the impact of the proposed method on other types of LLMs."
3. Does the evaluation of hallucination mitigation hold under multi-annotator verification? Page 8 notes that the HaluBench small subset was "annotated by a single human annotator due to time constraints."

## Limitations
- The dynamic weighting based on TF-IDF query specificity assumes the same threshold works across all domains without cross-domain validation
- The paper does not address computational overhead from running dual retrievers with query expansion or report latency measurements
- The claim that WordNet-based query expansion universally improves retrieval without introducing noise is weakly supported with no systematic evaluation of expansion quality

## Confidence

- **High Confidence:** Retrieval performance improvements (MAP@3, NDCG@3) are well-supported by experimental results on HaluBench
- **Medium Confidence:** Reduction in hallucination rates is supported by data but relies on human annotation that introduces potential subjectivity
- **Low Confidence:** The claim that WordNet expansion improves retrieval without noise is weakly supported without systematic evaluation

## Next Checks
1. **Cross-domain Sensitivity Test:** Apply the hybrid retriever with identical hyperparameters (α=1, top-2 expansion) to an unseen domain like legal or technical documentation to verify if TF-IDF-based dynamic weighting remains effective without domain-specific tuning.
2. **Latency Benchmarking:** Measure end-to-end response time for the hybrid pipeline (including dual retrieval, query expansion, and RRF fusion) versus single-method baselines to quantify the performance-accuracy tradeoff.
3. **Expansion Quality Audit:** For a sample of 50 queries with polysemous terms (e.g., "bank," "crane," "java"), manually inspect the expanded query terms and retrieved documents to verify that WordNet expansion adds relevant synonyms rather than introducing semantic drift.