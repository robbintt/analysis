---
ver: rpa2
title: Enhancing Multi-modal Models with Heterogeneous MoE Adapters for Fine-tuning
arxiv_id: '2503.20633'
source_url: https://arxiv.org/abs/2503.20633
tags:
- experts
- expert
- multi-modal
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a heterogeneous mixture-of-experts (MoE)
  adapter for parameter-efficient fine-tuning of multi-modal models. The key innovation
  is extending traditional PEFT methods to support cross-modal expert combinations,
  enabling better information interaction between modalities while maintaining low
  computational overhead.
---

# Enhancing Multi-modal Models with Heterogeneous MoE Adapters for Fine-tuning

## Quick Facts
- **arXiv ID:** 2503.20633
- **Source URL:** https://arxiv.org/abs/2503.20633
- **Reference count:** 39
- **Primary result:** Achieves competitive performance on 8 downstream tasks using only 5-8% of parameters fine-tuned, outperforming existing PEFT methods

## Executive Summary
This paper introduces a heterogeneous mixture-of-experts (HMMoE) adapter for parameter-efficient fine-tuning of multi-modal models. The key innovation is extending traditional PEFT methods to support cross-modal expert combinations, enabling better information interaction between modalities while maintaining low computational overhead. The method modifies affine linear expert design to facilitate efficient modal fusion in low-rank space. Experiments across eight downstream tasks (visual-audio and text-visual) show that the approach achieves competitive performance with only 5-8% of parameters fine-tuned, outperforming existing PEFT methods.

## Method Summary
The HMMoE framework extends mixture-of-experts to multi-modal settings by introducing three heterogeneous expert types: single-modal adapters that preserve modality-specific features, cross-attention experts that capture global inter-modal dependencies in low-rank space, and channel-attention experts that perform per-channel modulation. The architecture employs hierarchical routing with a global router selecting expert groups and local routers performing top-k expert selection within each group. The method is inserted after feed-forward layers in transformer encoders, freezing pre-trained weights while training only the adapter modules and classification heads.

## Key Results
- Achieves 0.6-0.8 percentage point improvements over LoRA, LoRA-FA, and Series/Parallel-Adapter baselines across 8 tasks
- Maintains competitive performance while training only 5-8% of parameters (r=32 low-rank dimension)
- Ablation studies show cross-attention (+0.6 NLVR) and channel-attention (+1.2 NLVR) experts provide additive gains
- Expert utilization analysis reveals balanced usage at lower layers, with multi-modal experts favored at higher layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Low-rank projection enables efficient cross-modal fusion with minimal parameters while preserving representational capacity.
- **Mechanism:** Input features are down-projected to dimension r via learnable matrices, cross-modal attention operates in this compressed space, then up-projected. The residual connection preserves original feature information.
- **Core assumption:** Cross-modal dependencies can be captured in lower-dimensional subspaces without significant information loss.
- **Evidence anchors:** Low-rank design shown in Section III-C equations; TT-LoRA MoE and DR-LoRA papers support low-rank MoE structures; no direct cross-modal fusion evidence in corpus.
- **Break condition:** Tasks requiring fine-grained high-dimensional cross-modal correlations may lose critical signals.

### Mechanism 2
- **Claim:** Heterogeneous expert types capture distinct interaction patterns better than homogeneous experts.
- **Mechanism:** Three expert types operate in parallel: single-modal adapters preserve modality-specific features, cross-attention experts capture global inter-modal dependencies, and channel-attention experts compute modulated scaling factors per channel dimension.
- **Core assumption:** Different cross-modal tasks require different fusion granularities—some need global semantic alignment, others need channel-wise feature recalibration.
- **Evidence anchors:** Heterogeneous design described in abstract; Table IV ablation shows cross-attention (+0.6 NLVR) and channel-attention (+1.2 NLVR) improvements; MoA paper validates heterogeneous adapters for LLMs.
- **Break condition:** Predominantly uni-modal tasks or those requiring only one fusion type show diminishing returns.

### Mechanism 3
- **Claim:** Hierarchical routing (global group-level + local expert-level) enables adaptive expert selection across transformer layers.
- **Mechanism:** Global router computes softmax weights over expert groups. Local router computes expert probabilities and selects top-k per group. Lower layers show balanced expert usage; higher layers favor multi-modal experts.
- **Core assumption:** Different transformer layers require different fusion strategies—early layers preserve modality structure, later layers integrate cross-modal semantics.
- **Evidence anchors:** Two-tier routing described in Section III-E; Figure 3 shows expert utilization distribution across layers; no direct corpus evidence for hierarchical routing in multi-modal PEFT.
- **Break condition:** If routing collapses to always selecting the same expert type, MoE capacity is underutilized.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** HMMoE builds on PEFT principles—freezing pretrained weights while training small adapter modules. Understanding LoRA, adapter bottlenecks, and the parameter/compute tradeoff is essential.
  - **Quick check question:** Can you explain why PEFT methods typically use bottleneck architectures (down-project → nonlinearity → up-project)?

- **Concept: Mixture of Experts with Sparse Routing**
  - **Why needed here:** The method extends MoE to multi-modal settings with heterogeneous experts. Understanding top-k routing, load balancing, and expert capacity is prerequisite.
  - **Quick check question:** What is the difference between token-based routing and expert-based routing, and why does top-k matter?

- **Concept: Cross-Modal Attention Mechanisms**
  - **Why needed here:** Cross-attention and channel-attention experts are core to the heterogeneous design. You need to understand how queries/keys/values enable cross-modal alignment.
  - **Quick check question:** In cross-modal attention, which modality should provide the query vs. key/value for an audio-to-visual fusion task?

## Architecture Onboarding

- **Component map:** Visual/Audio Backbone → Feed-Forward Layer → [HMMoE Module] → Normalization Layer → Global Router (group weights) → Single-Modal Experts (bottleneck adapters), Cross-Attention Experts (Q-K-V in low-rank space), Channel-Attention Experts (channel-wise gating) → Local Router (top-k per group) → Weighted sum → Output features

- **Critical path:**
  1. Implement single-modal adapter as baseline
  2. Add cross-attention expert with low-rank projection
  3. Add channel-attention expert
  4. Implement global router and local router
  5. Insert HMMoE after feed-forward layers in transformer encoder

- **Design tradeoffs:**
  - **Rank r:** Lower r reduces parameters but risks information loss. Paper shows r=32 is optimal; r=8 underperforms, r>32 has diminishing returns
  - **Number of experts per group:** More experts improve performance but increase overhead. Paper uses 2 experts per group as efficiency/performance balance
  - **Expert type mix:** Task-dependent—audio-visual tasks may favor channel-attention; text-visual tasks favor cross-attention

- **Failure signatures:**
  - Routing collapse: One expert type dominates (>80% selection) across all layers
  - Performance plateau: Adding experts beyond 2-3 per group shows <0.2% improvement
  - Cross-modal degradation: Removing multi-modal experts drops performance significantly

- **First 3 experiments:**
  1. **Sanity check:** Implement single-modal expert only, verify performance matches baseline adapter on one task (e.g., AVE)
  2. **Component ablation:** Add cross-attention expert, measure improvement; then add channel-attention expert, confirm additive gains
  3. **Hyperparameter sweep:** Vary rank r ∈ {8, 16, 32, 64} and experts per group ∈ {1, 2, 3, 4} on a validation task to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed pairwise cross-attention experts scale effectively to scenarios involving three or more modalities (e.g., audio-visual-text fusion)?
- **Basis in paper:** The paper evaluates only bi-modal tasks and utilizes cross-attention experts designed explicitly for pairs of inputs, leaving the handling of combinatorial complexity in higher-modal settings unaddressed.
- **Why unresolved:** The current architecture fuses specific pairs; extending this to N modalities may require an explosion of expert types or a new fusion strategy not defined here.
- **What evidence would resolve it:** Experimental results on tri-modal benchmarks comparing HMMoE against multi-modal PEFT baselines.

### Open Question 2
- **Question:** Does the HMMoE architecture introduce inference latency or computational FLOPs that outweigh its parameter efficiency benefits?
- **Basis in paper:** The paper focuses on parameter reduction (5-8%) but does not report inference throughput or latency metrics.
- **Why unresolved:** MoE models require routing computations and managing multiple expert branches, which can increase actual execution time compared to simpler PEFT methods like LoRA despite having fewer active parameters.
- **What evidence would resolve it:** A comparative analysis of inference time and FLOPs between HMMoE and standard adapters on the tested downstream tasks.

### Open Question 3
- **Question:** Is the specific combination of Cross-Attention and Channel-Attention experts universally optimal, or does it require manual tuning for different task types?
- **Basis in paper:** The paper proposes a specific "heterogeneous" mix of experts but does not provide a mechanism for automatically searching for the best expert types.
- **Why unresolved:** The choice of experts appears heuristic; it is unclear if other attention mechanisms or different ratios would perform better on distinct tasks.
- **What evidence would resolve it:** An ablation study varying expert types beyond the proposed two, or an automated architecture search over the expert space.

## Limitations

- The heterogeneous expert design lacks rigorous theoretical justification for why specific expert combinations outperform alternatives
- The low-rank assumption for cross-modal fusion may not generalize to tasks requiring fine-grained temporal or spatial alignment
- The hierarchical routing mechanism appears novel but is not thoroughly validated against simpler alternatives

## Confidence

**High Confidence Claims:**
- Parameter efficiency (5-8% parameters) is directly measurable and validated across all tasks
- Outperformance over baseline PEFT methods is empirically demonstrated with statistical significance
- The low-rank projection framework is mathematically well-defined and reproducible

**Medium Confidence Claims:**
- The heterogeneous expert design provides meaningful task-specific advantages (supported by ablation but not thoroughly analyzed)
- The routing mechanism effectively adapts to different layers and tasks (based on utilization patterns rather than controlled ablation)
- Cross-modal fusion via low-rank space preserves semantic information (assumed but not explicitly verified)

**Low Confidence Claims:**
- The specific combination of cross-attention and channel-attention experts is optimal (no comparison with other expert types)
- Hierarchical routing is necessary rather than over-engineering (no ablation against single-tier routing)
- The method generalizes to arbitrary multi-modal tasks beyond the tested visual-audio/text-visual domains

## Next Checks

1. **Expert Type Ablation Study:** Remove one expert type at a time on all eight tasks to quantify individual contributions and identify which tasks benefit most from each type.

2. **Routing Mechanism Analysis:** Replace the two-tier routing with simpler alternatives (single-tier global routing or fixed expert assignment) to determine whether hierarchical routing provides measurable benefits.

3. **Low-Rank Sensitivity Test:** Systematically vary the rank parameter r across a wider range (8, 16, 32, 64, 128) on a subset of tasks to identify the breaking point where performance degradation becomes significant.