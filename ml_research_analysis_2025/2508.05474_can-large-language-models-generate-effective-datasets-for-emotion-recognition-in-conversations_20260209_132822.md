---
ver: rpa2
title: Can Large Language Models Generate Effective Datasets for Emotion Recognition
  in Conversations?
arxiv_id: '2508.05474'
source_url: https://arxiv.org/abs/2508.05474
tags:
- datasets
- data
- dataset
- emotion
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether large language models can generate
  effective datasets for emotion recognition in conversations (ERC). The authors address
  ERC data scarcity and bias by employing a small, resource-efficient LLM to synthesize
  novel datasets with diverse properties, specifically generating six new datasets
  tailored to enhance three widely used ERC benchmarks.
---

# Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?

## Quick Facts
- **arXiv ID:** 2508.05474
- **Source URL:** https://arxiv.org/abs/2508.05474
- **Reference count:** 40
- **Primary result:** Small LLMs can generate high-quality ERC datasets that improve classifier performance on three benchmarks

## Executive Summary
This paper investigates whether large language models can generate effective datasets for emotion recognition in conversations (ERC). The authors address ERC data scarcity and bias by employing a small, resource-efficient LLM to synthesize novel datasets with diverse properties, generating six new datasets tailored to enhance three widely used ERC benchmarks. Using structured prompt engineering to generate both dialogues and corresponding emotion labels simultaneously, the approach improves consistency and reliability compared to traditional annotation methods. Experimental results demonstrate that ERC classifier models trained on these generated datasets consistently outperform their counterparts trained on original datasets, with statistically significant improvements. The study concludes that small LLMs can effectively generate high-quality ERC data, offering a scalable, affordable solution for improving ERC performance.

## Method Summary
The method employs Vicuna 1.5 (13B) to generate synthetic ERC dialogues using structured prompt engineering. The LLM generates speaker names, utterances, and emotion labels simultaneously through a three-part prompt: task definition, logic reasoning mapping emotions to numeric symbols, and structured output formatting. Two generation strategies are used: "Balanced" datasets ensure each emotion appears at least once per dialogue, while "Natural" datasets allow unconstrained generation. Generated data is then used to pre-train ERC classifiers (CoMPM, EmoOne-RoBERTa, TODKAT), which are subsequently fine-tuned on original benchmark training data. Performance is evaluated using the "Train on Synthetic, Test on Real" (TSTR) paradigm, measuring Weighted F1-score on original benchmark test sets.

## Key Results
- ERC classifiers trained on generated datasets consistently outperformed models trained only on original datasets across all three benchmarks
- Balanced datasets yielded highest scores on MELD, while natural datasets excelled on IEMOCAP
- Synthetic data generation achieved statistically significant improvements (p < 0.05) as confirmed by Friedman rank sum tests
- Small LLMs (13B) achieved comparable results to larger models (33B) while requiring significantly less computational resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Simultaneous generation of dialogue utterances and emotion labels improves dataset consistency compared to separate annotation pipelines
- **Mechanism:** The LLM generates speaker names, utterances, and emotion labels in a single pass, eliminating subjectivity and disagreement inherent in human annotation
- **Core assumption:** The LLM's internal representation of emotion is sufficiently aligned with human emotion perception that self-labeled outputs are reliable for training downstream classifiers
- **Evidence anchors:** [abstract] "structured prompt engineering to generate both dialogues and corresponding emotion labels simultaneously, improving consistency and reliability"
- **Break condition:** If the LLM's emotion schema diverges significantly from the target benchmark's emotion taxonomy

### Mechanism 2
- **Claim:** Numeric symbol assignment for emotion labels constrains LLM output consistency through logic reasoning prompting
- **Mechanism:** Each emotion label is assigned a numeric symbol (e.g., 1=Neutral, 2=Joy) to reduce hallucination and format inconsistency
- **Core assumption:** Numeric symbols reduce token-level ambiguity and help the LLM maintain format discipline across long outputs
- **Evidence anchors:** [section III-B] "assign each emotion label with a number as symbols and ask the LLM to provide one of them for each utterance"
- **Break condition:** If prompt complexity exceeds the LLM's context-tracking capacity, the numeric constraint may be ignored

### Mechanism 3
- **Claim:** Label distribution strategy (balanced vs. natural) has dataset-specific optimal configurations for downstream ERC performance
- **Mechanism:** Balanced datasets are generated by instructing the LLM to include at least one utterance with a specific target emotion per dialogue, iterating through all labels
- **Core assumption:** The benefit of label balancing depends on the original benchmark's imbalance severity and the inherent difficulty of emotion categories
- **Evidence anchors:** [section IV-B] "Balanced datasets yielded the highest scores on the MELD dataset across all classifiers"
- **Break condition:** If a benchmark's difficulty stems from factors other than label imbalance (e.g., low inter-annotator agreement)

## Foundational Learning

- **Concept: Emotion Recognition in Conversation (ERC)**
  - **Why needed here:** The entire methodology targets ERC-specific challenges: multi-party dialogue context, emotion shifts across utterances, and speaker-dependent emotional expression
  - **Quick check question:** Can you explain why ERC differs from single-utterance emotion classification, and what additional context signals ERC models must track?

- **Concept: Train-on-Synthetic-Test-on-Real (TSTR) Evaluation**
  - **Why needed here:** The paper uses TSTR as its primary validation framework. Understanding this paradigm is essential to interpret why testing on original benchmark test splits validates synthetic data quality
  - **Quick check question:** Why is testing on real data necessary after training on synthetic data, rather than testing on held-out synthetic data?

- **Concept: Class Imbalance Effects in Classification**
  - **Why needed here:** The balanced vs. natural dataset comparison directly engages with how label skew affects classifier learning
  - **Quick check question:** Why might a severely imbalanced dataset benefit more from balanced synthetic augmentation than a moderately imbalanced one?

## Architecture Onboarding

- **Component map:** LLM Generator (Vicuna 1.5) -> Prompt Engineering Layer -> Data Processing -> Downstream Classifiers (CoMPM, EmoOne-RoBERTa, TODKAT) -> Evaluation Pipeline
- **Critical path:** 1) Design prompt template matching target benchmark's emotion taxonomy 2) Generate dialogues with fixed parameters 3) Parse structured outputs 4) Split generated data 5) Pre-train classifiers on generated data 6) Fine-tune on original benchmark 7) Evaluate on original test splits
- **Design tradeoffs:** Model size vs. consistency (7B repetition issues, 13B optimal balance), Balanced vs. Natural (accuracy vs. realism), API vs. Local LLM (performance vs. reproducibility)
- **Failure signatures:** 7B model repetition, prompt forgetting, format inconsistency, emotion schema drift
- **First 3 experiments:** 1) Baseline validation on original MELD/IEMOCAP/EmoryNLP, 2) Natural dataset transfer on MELD, 3) Label distribution ablation comparing balanced vs. natural variants

## Open Questions the Paper Calls Out
None

## Limitations
- The mechanism by which numeric symbol assignment improves LLM consistency lacks direct empirical validation within the paper
- The "train-on-synthetic-test-on-real" paradigm assumes synthetic data distributions capture essential features of real emotional dynamics without systematic bias
- The study focuses exclusively on textual ERC without multimodal considerations, potentially limiting generalizability

## Confidence
- **High Confidence:** Empirical finding that synthetic datasets improve ERC classifier performance compared to training on original datasets alone
- **Medium Confidence:** Numeric symbol assignment improves label consistency, plausible but lacking direct ERC context validation
- **Low Confidence:** Mechanism explaining why simultaneous generation improves consistency over separate annotation pipelines is theoretically reasonable but unproven

## Next Checks
1. Conduct a human evaluation study comparing label consistency between simultaneously generated synthetic data and separately generated dialogue/emotion pairs aligned post-hoc
2. Test whether synthetic data generated for one benchmark provides similar benefits when used to train classifiers for different benchmarks
3. Systematically vary LLM size and generation parameters to quantify the relationship between model capacity, generation quality, and downstream classifier performance