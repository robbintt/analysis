---
ver: rpa2
title: LLM-Guided Search for Deletion-Correcting Codes
arxiv_id: '2504.00613'
source_url: https://arxiv.org/abs/2504.00613
tags:
- functions
- priority
- code
- function
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose LLM-guided evolutionary search to find deletion-correcting
  codes. We represent the problem as constructing large independent sets in graphs
  where vertices are binary sequences and edges connect sequences sharing a subsequence.
---

# LLM-Guided Search for Deletion-Correcting Codes

## Quick Facts
- arXiv ID: 2504.00613
- Source URL: https://arxiv.org/abs/2504.00613
- Reference count: 40
- Primary result: Discovers maximum-size deletion-correcting codes for lengths 6-11 and matches VT code sizes up to length 25

## Executive Summary
This paper proposes using LLM-guided evolutionary search to find deletion-correcting codes by formulating the problem as constructing large independent sets in graphs. Vertices represent binary sequences and edges connect sequences sharing subsequences. Using FunSearch with LLM-generated priority functions, the method greedily constructs codes by iteratively adding highest-priority sequences while maintaining independence constraints. The approach discovers optimal codes for single-deletion correction up to length 11, matches conjectured-optimal VT codes for lengths up to 25, and establishes new best-known sizes for two-deletion correction at lengths 12, 13, and 16.

## Method Summary
The method represents deletion-correcting codes as maximum independent sets in graphs where vertices are binary sequences and edges connect sequences sharing subsequences of length ≥ n-s. Rather than searching directly for codes, it searches for priority functions f(v,G) that assign scores to vertices. An evolutionary search with 5 independent islands iteratively samples functions, generates new ones via LLM (StarCoder2-15B), evaluates their performance on graph instances, and updates the database. The greedy algorithm constructs codes by repeatedly adding highest-priority vertices while removing neighbors. Deduplication removes functionally identical priority functions to improve prompt effectiveness. The search evaluates on code lengths n∈[6,11] and scores based on independent set size at n=11.

## Key Results
- Discovers maximum-size codes for single-deletion correction at lengths 6-11
- Rediscovers VT codes in equivalent form and matches their sizes for lengths up to 25
- Achieves zero sequence overlap with VT codes for odd lengths (n=7,9,11,13)
- Establishes new best-known sizes for two-deletion correction: n=12 (34), n=13 (41), n=16 (204)
- Demonstrates generalization capability beyond evaluation ranges

## Why This Works (Mechanism)

### Mechanism 1: Greedy Construction with Priority Functions
Searching for priority functions rather than directly for codes enables generalization beyond evaluation ranges. The method searches for compact priority functions f(v,G) that assign scores to vertices, then uses a greedy algorithm to iteratively select highest-priority vertices while removing neighbors. This avoids the exponentially large space of all 2^n binary sequences. The approach assumes good priority functions capture structural regularities that transfer across code lengths and deletion counts. Evidence shows optimal functions discovered for n∈[6,11] work for longer lengths, though overfitting to specific evaluation lengths remains a risk.

### Mechanism 2: Evolutionary Search with Island-Based Diversity
Parallel island populations with periodic resetting prevent premature convergence and maintain diverse solution candidates. Functions are stored in separate "islands" that evolve independently, with clusters grouping functions by performance scores. Temperature-controlled sampling shifts from exploration to exploitation, and periodic island resets share information by reinitializing poor islands with best functions from survivors. The core assumption is that diverse function representations increase the probability of discovering generalizable solutions. While island-based diversity is claimed as important, the paper provides no ablation studies comparing single-island vs. multi-island performance.

### Mechanism 3: Deduplication Improves Sample Efficiency
Filtering syntactically different but functionally identical priority functions improves prompt quality and reduces wasted iterations. Two functions are considered duplicates if they produce identical priority scores across all sequences (hash-based comparison). This ensures few-shot prompts contain genuinely different examples rather than minor syntactic variations. The approach assumes LLMs generate more novel solutions when prompted with functionally diverse examples. Without deduplication, approximately 20% of generated functions are duplicates, wasting computational resources.

## Foundational Learning

- **Independent Sets in Graphs**: The core formulation represents deletion-correcting codes as independent sets where no two vertices (sequences) share an edge (common subsequence). Quick check: Given a graph with 4 vertices {A, B, C, D} and edges {(A,B), (B,C), (C,D)}, what is the maximum independent set size?

- **Varshamov-Tenengolts (VT) Codes**: VT codes are the benchmark for single-deletion correction. The paper shows LLM-guided search rediscovers and matches VT codes, validating the approach. Quick check: For VT codes defined by Σᵢ i·vᵢ ≡ a mod (n+1), why does partitioning into n+1 residue classes guarantee deletion-correcting capability?

- **Evolutionary Search Temperature Schedules**: The sampling temperature T_j controls exploration vs. exploitation. Understanding softmax sampling over cluster scores is essential for tuning convergence. Quick check: If temperature T→∞, what happens to the sampling distribution p_i over clusters? What about T→0?

## Architecture Onboarding

- **Component map**: Program Database (stores functions, handles sampling, prompt construction, island resets) -> LLM Workers (generate priority functions from few-shot prompts) -> Evaluator Workers (execute functions, compute independent set sizes) -> Message Queue (asynchronous communication via AMQP)

- **Critical path**: LLM inference → Function execution → Score computation → Database update. Evaluators are the bottleneck; optimal LLM-to-evaluator ratio is ~1:20.

- **Design tradeoffs**: Evaluation range: Larger n provides stronger training signal but exponential graph growth limits feasibility (paper uses n∈[6,11]). Scoring function: Using only largest n finds optimal functions faster than aggregate scoring, but may reduce generalization. Prompt engineering: Removing graph input (Prompt 3) biases toward sequence-only logic; including graph enables structure-aware functions but may limit generalization.

- **Failure signatures**: High duplicate ratio (>30%): Indicates temperature too low or prompt lacks diversity signals. Low executable function rate (<15%): LLM hyperparameters (repetition penalty, max tokens) need adjustment. Stagnating score trajectory: Islands converged; reduce temperature reset period P or increase initial temperature T.

- **First 3 experiments**:
  1. **Baseline replication**: Run with T=0.1, P=30K, R=1.2K, n∈[6,11], s=1. Verify optimal function discovery within 400K processed.
  2. **Ablation on deduplication**: Run identical configuration without deduplication. Compare functions processed until optimal and duplicate ratio.
  3. **Generalization test**: Take priority functions from Experiment 1, evaluate on n∈[12,16] and s∈{1,2}. Measure gap to VT code sizes and best-known bounds.

## Open Questions the Paper Calls Out

### Open Question 1
Can the computational scalability of the evaluator be improved to handle code lengths beyond n ≈ 16-25? The paper notes this as a key limitation since the number of sequences grows exponentially with code length, making evolutionary search infeasible for moderate to large lengths. Demonstration of the method successfully finding optimal or near-optimal codes for n ≥ 30 without exponential memory growth would resolve this.

### Open Question 2
Why do discovered alternative maximum-size codes with zero VT overlap occur only for odd code lengths? The paper documents that new codes matching VT code sizes are only found for odd lengths, with zero overlap achieved for n = 7, 9, 11, 13. A characterization of structural differences between odd and even length deletion graphs, or proof that alternative optimal codes cannot exist for even n, would resolve this.

### Open Question 3
Can LLM-guided search close the gap between lower and upper bounds for two-deletion-correcting codes? While the paper establishes improved lower bounds (n=12: 34 vs. previous 32; n=16: 204 vs. previous 201), significant gaps to upper bounds remain. Discovery of priority functions achieving code sizes matching or exceeding the best-known upper bounds would resolve this.

### Open Question 4
Can the approach generalize to other error models (insertions, substitutions, combinations) beyond deletions? The paper claims the method applies to any error type or combination thereof as long as the distinguishability constraint is well-defined, but only demonstrates deletion-correcting codes. Successful application to insertion-correcting codes, edit-distance codes, or combined error models would resolve this.

## Limitations
- **Computational scalability**: Exponential evaluator growth makes the approach infeasible for moderate to large code lengths
- **Overfitting risk**: No rigorous analysis of generalization boundaries beyond evaluation ranges
- **Island architecture validation**: Lacks ablation studies comparing multi-island vs. single-island performance
- **Hash-based deduplication**: May discard useful diversity for longer sequences

## Confidence
- **High Confidence**: Discovery of maximum-size codes for lengths 6-11 and rediscovery of VT codes in equivalent forms
- **Medium Confidence**: Claims about matching VT code sizes for n≤25 and achieving zero overlap for odd lengths
- **Low Confidence**: Claims about establishing new best-known sizes for s=2 codes at lengths 12, 13, and 16

## Next Checks
1. **Scalability analysis**: Implement time/memory profiling for evaluator on graphs with n=11, 12, 13. Measure actual growth rate and identify bottlenecks. Compare against theoretical exponential predictions to quantify practical limits.

2. **Generalization boundary study**: Take priority functions discovered for n∈[6,11], evaluate performance systematically on n∈[12,15]. Plot performance degradation curve to identify where generalization fails. Test whether functions that match VT codes for n≤11 continue matching for n>11.

3. **Ablation on evolutionary components**: Run controlled experiments removing deduplication, island architecture, and temperature scheduling. Measure impact on functions processed before finding optimal, duplicate ratios, and final code sizes. This would validate which components are essential vs. optional.