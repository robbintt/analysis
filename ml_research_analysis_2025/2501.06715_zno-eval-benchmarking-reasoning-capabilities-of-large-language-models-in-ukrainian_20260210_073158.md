---
ver: rpa2
title: 'ZNO-Eval: Benchmarking reasoning capabilities of large language models in
  Ukrainian'
arxiv_id: '2501.06715'
source_url: https://arxiv.org/abs/2501.06715
tags:
- language
- ukrainian
- reasoning
- tasks
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZNO-Eval, a comprehensive benchmark for evaluating
  the reasoning capabilities of large language models in the Ukrainian language. Built
  from real exam tasks in Ukraine's standardized testing system, the benchmark covers
  Ukrainian language, mathematics, history, and geography, using various question
  formats including single-answer, multiple-choice, matching, and open-ended questions.
---

# ZNO-Eval: Benchmarking reasoning capabilities of large language models in Ukrainian

## Quick Facts
- arXiv ID: 2501.06715
- Source URL: https://arxiv.org/abs/2501.06715
- Reference count: 9
- This paper introduces ZNO-Eval, a comprehensive benchmark for evaluating the reasoning capabilities of large language models in the Ukrainian language.

## Executive Summary
This paper introduces ZNO-Eval, a comprehensive benchmark for evaluating the reasoning capabilities of large language models in the Ukrainian language. Built from real exam tasks in Ukraine's standardized testing system, the benchmark covers Ukrainian language, mathematics, history, and geography, using various question formats including single-answer, multiple-choice, matching, and open-ended questions. Evaluation of several state-of-the-art models showed GPT-4o achieved the highest overall performance, particularly in language tasks, while Gemini-1.5 Pro and GPT-4 Turbo excelled in arithmetic problems. All models performed near-maximum on common knowledge tasks but showed gaps in Ukrainian language and math, highlighting the need for specialized benchmarks in underrepresented languages. The ZNO-Eval benchmark provides a valuable tool for assessing model reasoning capabilities and limitations in Ukrainian, with plans to extend it to multimodal tasks.

## Method Summary
The ZNO-Eval benchmark was constructed using real exam tasks from Ukraine's standardized testing system (ZNO) for 2024, covering Ukrainian language, mathematics, history, and geography. The dataset includes multiple question formats: single-answer, multiple-choice, matching, and open-ended questions. Four state-of-the-art language models were evaluated: GPT-4o, Gemini-1.5 Pro, GPT-4 Turbo, and LLaMA 3.1 70B. Performance was measured across task categories, with particular attention to reasoning capabilities and language-specific challenges.

## Key Results
- GPT-4o achieved the highest overall performance across all task categories, particularly excelling in language tasks
- Gemini-1.5 Pro and GPT-4 Turbo performed best on arithmetic problems
- All models showed near-maximum performance on common knowledge tasks but demonstrated significant gaps in Ukrainian language and mathematics

## Why This Works (Mechanism)
The benchmark leverages real standardized exam content, ensuring authentic reasoning challenges across multiple domains. The variety of question formats tests different cognitive abilities, from factual recall to complex problem-solving. The use of actual Ukrainian educational materials provides a culturally and linguistically relevant evaluation framework that existing English-centric benchmarks cannot offer.

## Foundational Learning
- Ukrainian language processing: Essential for evaluating linguistic reasoning; quick check: verify model can parse complex Ukrainian grammatical structures
- Mathematical reasoning: Critical for assessing logical problem-solving; quick check: confirm model can handle multi-step arithmetic operations
- Historical knowledge: Important for testing factual recall and contextual understanding; quick check: ensure model recognizes Ukrainian historical events and figures
- Geographical reasoning: Tests spatial and contextual knowledge; quick check: validate model's understanding of Ukrainian geography
- Multiple-choice strategy: Evaluates elimination and inference skills; quick check: assess model's ability to identify correct answers among distractors
- Open-ended response generation: Measures comprehensive reasoning and expression; quick check: verify model can construct coherent, relevant answers

## Architecture Onboarding
Component map: Exam questions -> Model input -> Reasoning process -> Answer generation -> Evaluation metric
Critical path: Question comprehension → Information retrieval/extraction → Reasoning/calculation → Answer formulation → Verification
Design tradeoffs: Single-answer vs. open-ended questions balance precision with comprehensive reasoning assessment
Failure signatures: Model struggles with Ukrainian linguistic nuances, complex multi-step math problems, and context-specific historical/geographical knowledge
First experiments:
1. Evaluate model performance on simple vs. complex Ukrainian grammatical structures
2. Test arithmetic reasoning with varying levels of computational complexity
3. Assess model's ability to handle context-dependent historical and geographical questions

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on single year's exam content (2024) limits generalization to future test designs
- No inter-annotator agreement or quality control checks reported for human-coded answer keys
- Potential biases in exam content (regional or cultural specificity) not addressed
- Absence of detailed error analysis by question type or reasoning step

## Confidence
- Confidence in GPT-4o being the best overall performer: High
- Confidence in all models performing near-maximum on common knowledge: Medium
- Confidence in need for specialized benchmarks for underrepresented languages: High

## Next Checks
1. Conduct a split-half reliability test by randomly dividing the 2024 exam questions into two sets and re-evaluating model performance to assess consistency
2. Expand the benchmark to include at least two additional years of exam content to test generalization and robustness over time
3. Perform a detailed error analysis by reasoning step (e.g., comprehension, calculation, inference) to identify specific failure modes and guide targeted model improvements