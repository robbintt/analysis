---
ver: rpa2
title: Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning
  and Experience
arxiv_id: '2506.18928'
source_url: https://arxiv.org/abs/2506.18928
tags:
- llms
- strategic
- randomization
- game
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can recognize
  when randomization is strategically optimal and deliberately choose to randomize.
  The authors design a novel zero-sum game inspired by the Tian Ji Horse Race, where
  the Nash equilibrium corresponds to a maximal entropy strategy.
---

# Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning and Experience
## Quick Facts
- arXiv ID: 2506.18928
- Source URL: https://arxiv.org/abs/2506.18928
- Reference count: 4
- Primary result: Strong LLMs can recognize when randomization is strategically optimal and deliberately choose to randomize, while weaker models remain deterministic regardless of prompts.

## Executive Summary
This paper investigates whether large language models can recognize when randomization is strategically optimal and deliberately choose to randomize. The authors design a novel zero-sum game inspired by the Tian Ji Horse Race, where the Nash equilibrium corresponds to a maximal entropy strategy. To isolate the cognitive decision to randomize from the mechanical generation of randomness, the study provides system-generated random choices and evaluates whether models use them. Five LLMs are tested across three prompt styles (framed, neutral, and hinted) in competitive multi-tournament gameplay.

Results show that weaker models remain deterministic regardless of prompts, while stronger models exhibit increased randomization under explicit hints. When facing weaker models, strong LLMs adopt deterministic strategies to exploit biases, but converge toward equilibrium play when facing peers. The study reveals meaningful variation in LLMs' strategic reasoning capabilities and highlights opportunities for improvement in abstract reasoning and adaptive learning.

## Method Summary
The study employs a round-robin tournament across five LLMs using a Tian Ji Horse Race variant with N=7 horses per player. The game is specifically designed so uniform randomization is the Nash equilibrium. Each tournament consists of 7 rounds where players simultaneously select horses from their remaining pool. System-generated random choices are provided to models, who can either use them or deviate. Three prompt variants are tested: framed (narrative context), neutral (abstract rules), and hinted (explicit Nash equilibrium guidance). Models compete in K=10 tournaments per pair, with outcomes measured through win/loss matrices and log Bayes factors comparing randomized vs. deterministic strategy hypotheses.

## Key Results
- Stronger models exhibit increased randomization under explicit hints while weaker models remain deterministic regardless of prompts
- Strong models dynamically shift between exploitation and equilibrium play based on opponent capability
- There exists a capability threshold below which strategic randomization cannot be elicited even with explicit theoretical guidance

## Why This Works (Mechanism)

### Mechanism 1: Prompting-Guided Strategic Randomization
- Claim: Explicit game-theoretic hints enable stronger models to adopt randomized strategies that weaker models cannot access regardless of prompting.
- Mechanism: The hinted prompt provides theoretical grounding (Nash equilibrium = maximal entropy), which stronger models can operationalize by choosing to use system-provided random choices rather than defaulting to deterministic heuristics.
- Core assumption: Models possess latent game-theoretic knowledge that can be activated through appropriate framing (assumed, not directly tested).
- Evidence anchors:
  - [abstract] "stronger models exhibit increased randomization under explicit hints"
  - [section 5.2] "Stronger models exhibit high Bayes factors under hinted prompts, confirming deliberate randomization"
  - [corpus] Related work on LLM bounded rationality (arXiv:2506.09390) suggests LLMs deviate from rationality similarly to humans
- Break condition: If models were simply following instructions without understanding, they would randomize equally against all opponents, but they show opponent-dependent behavior.

### Mechanism 2: Opponent-Adaptive Strategy Switching
- Claim: Strong models dynamically shift between exploitation (deterministic) and equilibrium play (randomized) based on inferred opponent capability.
- Mechanism: When facing weaker opponents with predictable biases, strong models abandon randomization to exploit patterns; when facing peers, they converge to Nash equilibrium strategies.
- Core assumption: Models maintain some internal representation of opponent behavior across tournament rounds (assumed from results, not directly measured).
- Evidence anchors:
  - [abstract] "When facing weaker models, strong LLMs adopt deterministic strategies to exploit biases, but converge toward equilibrium play when facing peers"
  - [section 5.1] Win/loss matrices show strong models consistently outperform weak ones
  - [corpus] LLM-Stackelberg Games (arXiv:2507.09407) shows LLMs can model opponent reasoning in sequential games
- Break condition: If adaptation were purely from prompt context, strategy would not vary by opponent identity within the same prompt condition.

### Mechanism 3: Capability Threshold for Abstract Game-Theoretic Reasoning
- Claim: There exists a model capability threshold below which strategic randomization cannot be elicited even with explicit theoretical guidance.
- Mechanism: Weaker models lack the reasoning infrastructure to connect game-theoretic concepts (Nash equilibrium, maximal entropy) to concrete action selection, defaulting instead to simple heuristics.
- Core assumption: The observed failure is due to reasoning limitations rather than prompt interpretation failure (not directly disentangled).
- Evidence anchors:
  - [abstract] "weaker models remain deterministic regardless of prompts"
  - [section 5.2] "doubao-1-5-pro-256k-250115 consistently exhibit low log Bayes factors"
  - [section 5.3] Case study shows weak model "failed to adapt or learn from repeated gameplay, regardless of prompt type"
  - [corpus] CaRT (arXiv:2510.08517) addresses related question of when models "know enough" to act
- Break condition: If this were purely about instruction-following rather than reasoning, stronger prompts would eventually succeed.

## Foundational Learning

- Concept: **Nash Equilibrium in Zero-Sum Games**
  - Why needed here: The game is specifically designed so uniform randomization is the equilibrium; understanding this is essential to interpret why randomization is "optimal."
  - Quick check question: Can you explain why a mixed strategy (randomizing) might outperform any deterministic strategy in a repeated zero-sum game?

- Concept: **Bayes Factor Analysis**
  - Why needed here: The paper uses log Bayes factors to quantify the degree to which models follow randomized vs. deterministic strategies; interpreting results requires understanding this metric.
  - Quick check question: Given two hypotheses H1 (randomized) and H2 (deterministic), what does a positive log Bayes factor indicate about observed choices?

- Concept: **Mixed vs. Pure Strategies**
  - Why needed here: The core question is whether LLMs can recognize when mixed strategies (probability distributions over actions) are superior to pure strategies (single deterministic actions).
  - Quick check question: In rock-paper-scissors, why is playing each option with 1/3 probability the Nash equilibrium?

## Architecture Onboarding

- Component map: Game engine -> Random choice provider -> LLM interface -> Metrics calculator
- Critical path:
  1. Initialize game state (N=7 horses per player)
  2. Per round: Generate system random choice → Prompt LLM → Log action → Update state
  3. Post-tournament: Determine winner → Compute Bayes factor for action sequences
  4. Aggregate across K=10 tournaments per model pair
- Design tradeoffs:
  - Providing random choices vs. LLM generation: Decouples cognitive decision from mechanical randomness but introduces artificial scaffolding
  - Fixed N=7: Enables controlled comparison but limits generalization to other game sizes
  - No reinforcement learning: Isolates pre-trained reasoning but prevents studying learning dynamics
- Failure signatures:
  - Low Bayes factors across all prompt conditions: Model cannot recognize optimal randomization
  - High variance in Bayes factors across opponents: Model is adapting strategically (expected for strong models)
  - Consistent heuristic choices (e.g., always fastest horse first): Model lacks strategic flexibility
- First 3 experiments:
  1. Replicate the framed vs. neutral vs. hinted prompt comparison with your target model to establish baseline randomization behavior.
  2. Extend N (number of horses) to test whether randomization recognition scales with game complexity or breaks down.
  3. Introduce asymmetric information (hide opponent's remaining horses) to probe whether randomization persists under imperfect information.

## Open Questions the Paper Calls Out
- How do variations in game complexity, such as increasing the number of agents or introducing imperfect information, affect LLMs' ability to adopt strategic randomization?
- Can reinforcement learning frameworks effectively train LLMs to recognize optimal randomization strategies through experience rather than relying on explicit prompting?
- What specific architectural or training improvements are required to enable weaker LLMs to override deterministic heuristics and adopt maximal entropy strategies?

## Limitations
- The experimental design provides system-generated random choices, which may overestimate true strategic reasoning capabilities by introducing artificial scaffolding
- Models are assumed to maintain opponent representations across tournament rounds without direct measurement of this cognitive process
- The fixed game size (N=7 horses) limits generalizability to other strategic contexts

## Confidence
- **High confidence**: The finding that stronger models exhibit increased randomization under explicit hints while weaker models remain deterministic regardless of prompts
- **Medium confidence**: The claim that strong models dynamically shift between exploitation and equilibrium play based on opponent capability
- **Medium confidence**: The existence of a capability threshold below which strategic randomization cannot be elicited

## Next Checks
1. Test whether the same randomization patterns emerge when models generate their own random choices rather than being provided system-generated options, to validate that the cognitive decision to randomize persists without scaffolding
2. Conduct ablation studies varying game complexity (N values) to determine whether the observed capability threshold is fixed or scales with problem difficulty
3. Implement direct measurement of opponent modeling by having models explicitly predict opponent moves in early rounds, then correlate prediction accuracy with subsequent strategy adaptation