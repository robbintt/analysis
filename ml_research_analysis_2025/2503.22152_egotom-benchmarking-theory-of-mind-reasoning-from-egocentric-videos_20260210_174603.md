---
ver: rpa2
title: 'EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos'
arxiv_id: '2503.22152'
source_url: https://arxiv.org/abs/2503.22152
tags:
- belief
- video
- goal
- actions
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EgoToM, a new benchmark for evaluating Theory
  of Mind (ToM) reasoning from egocentric videos. Using a causal ToM model, the authors
  generate multi-choice questions about goals, beliefs, and next actions from Ego4D
  video narrations.
---

# EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos
## Quick Facts
- arXiv ID: 2503.22152
- Source URL: https://arxiv.org/abs/2503.22152
- Authors: Yuxuan Li; Vijay Veerabadran; Michael L. Iuzzolino; Brett D. Roads; Asli Celikyilmaz; Karl Ridgeway
- Reference count: 15
- Over 1,000 multi-choice questions generated from 785 Ego4D video clips to test Theory of Mind reasoning

## Executive Summary
This paper introduces EgoToM, a new benchmark for evaluating Theory of Mind (ToM) reasoning from egocentric videos. Using a causal ToM model, the authors generate multi-choice questions about goals, beliefs, and next actions from Ego4D video narrations. The benchmark contains over 1,000 questions across 785 video clips. Evaluation shows that while models perform close to humans on goal inference (over 80% accuracy), they struggle significantly on belief and action inference (only 50-60% accuracy). Even large multimodal models with 100B+ parameters fall short of human performance on these harder inference tasks, highlighting the challenge of inferring internal mental states from visual behavior.

## Method Summary
The authors developed EgoToM by first collecting egocentric videos from the Ego4D dataset with rich narrations describing human activities. They then applied a causal Theory of Mind model to analyze these narrations and automatically generate multi-choice questions targeting three types of mental state inferences: goals (what the person wants), beliefs (what the person thinks is true), and next actions (what the person will do next). The questions were structured as multiple choice with four options each. The benchmark includes 1,046 questions spanning 785 video clips, providing a comprehensive evaluation framework for assessing how well AI systems can reason about human mental states from first-person visual perspectives.

## Key Results
- Models achieve over 80% accuracy on goal inference tasks, performing close to human level
- Performance drops significantly to 50-60% accuracy on belief and action inference tasks
- Even 100B+ parameter multimodal models cannot match human performance on the more challenging ToM tasks
- Clear performance gap demonstrates the difficulty of inferring internal mental states from visual behavior

## Why This Works (Mechanism)
The benchmark leverages causal ToM reasoning to generate questions that probe different levels of mental state understanding. By using actual egocentric video data with narrations, the questions are grounded in realistic human behavior scenarios. The multi-choice format provides clear evaluation metrics while the automatic generation process enables scaling to large question sets. The three-tier structure (goals, beliefs, actions) systematically tests increasingly complex levels of ToM reasoning.

## Foundational Learning
- **Theory of Mind**: The ability to attribute mental states (beliefs, intents, desires) to oneself and others - needed to understand what the benchmark measures; quick check: can you explain false-belief tasks?
- **Egocentric vision**: First-person video understanding from wearable cameras - needed to grasp the visual data domain; quick check: can you distinguish egocentric from third-person video?
- **Causal reasoning**: Understanding cause-effect relationships in observed behavior - needed to follow how questions are generated; quick check: can you trace how observed actions lead to inferred mental states?
- **Multimodal learning**: Combining visual and textual information processing - needed to understand how models process both video and narration; quick check: can you name two modalities used in this work?

## Architecture Onboarding
- **Component map**: Ego4D videos -> Causal ToM model -> Question generation -> Multi-choice format -> Model evaluation
- **Critical path**: Video narration → Causal ToM analysis → Question generation → Model inference → Accuracy measurement
- **Design tradeoffs**: Automatic question generation enables scale but may introduce quality variations; multiple-choice format simplifies evaluation but may allow guessing
- **Failure signatures**: Poor performance on belief/action questions suggests difficulty with mental state inference; large models underperforming humans indicates fundamental reasoning limitations
- **3 first experiments**: 1) Evaluate model performance on each question type separately, 2) Test different model sizes on same question sets, 3) Compare automatic generation quality with human-written questions

## Open Questions the Paper Calls Out
The authors acknowledge several limitations that affect confidence in their findings. The benchmark generation process relies on automatic question generation from video narrations, which may introduce bias or inconsistency in question quality. The evaluation uses a relatively small number of human participants (21 for goal inference, 9 for belief and action inference), limiting statistical power. Additionally, the causal ToM model's reasoning process is not fully transparent, making it difficult to verify the quality of generated questions.

## Limitations
- Automatic question generation from narrations may introduce bias or inconsistency in question quality
- Small human evaluation sample sizes (21 for goal inference, 9 for belief/action inference) limit statistical power
- Causal ToM model reasoning process lacks transparency, making question quality verification difficult
- Benchmark relies on existing Ego4D data rather than collecting purpose-built ToM datasets

## Confidence
*High confidence*: The finding that MLLMs perform near human level on goal inference but struggle on belief and action inference. This is supported by direct experimental results across multiple model sizes.

*Medium confidence*: The claim that EgoToM represents a novel and valuable benchmark for ToM reasoning from egocentric videos. While the benchmark appears unique, its long-term utility depends on community adoption and further validation.

*Medium confidence*: The assertion that even 100B+ parameter models cannot fully solve the ToM tasks. The evaluation shows performance gaps, but the ceiling is unknown since no perfect baseline exists.

## Next Checks
1. Expand human evaluation with larger sample sizes to improve statistical reliability of human performance estimates
2. Test additional MLLMs beyond the five models evaluated to determine if results generalize across the model landscape
3. Conduct ablation studies removing the causal ToM model to assess whether question generation quality directly impacts downstream model performance