---
ver: rpa2
title: 'Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence
  Functions Reveal About Robust Generalization'
arxiv_id: '2601.20172'
source_url: https://arxiv.org/abs/2601.20172
tags:
- symmetry
- influence
- learning
- data
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new influence-based diagnostic for assessing
  whether neural PDE emulators have learned symmetry properties of the underlying
  solution operator. The method measures cross-influence between symmetry-related
  states via the metric-weighted overlap of loss gradients along group orbits, going
  beyond forward-pass equivariance tests by probing the local geometry of the learned
  loss landscape and whether training dynamics couple physically equivalent configurations.
---

# Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization

## Quick Facts
- arXiv ID: 2601.20172
- Source URL: https://arxiv.org/abs/2601.20172
- Reference count: 40
- The paper introduces a new influence-based diagnostic for assessing whether neural PDE emulators have learned symmetry properties of the underlying solution operator.

## Executive Summary
This paper presents a novel influence-based diagnostic to evaluate whether neural PDE emulators have learned physical symmetry properties. By measuring cross-influence between symmetry-related states via metric-weighted overlap of loss gradients along group orbits, the method goes beyond forward-pass equivariance tests to probe the local geometry of the learned loss landscape. Applied to autoregressive fluid-flow emulators, the results demonstrate that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry-compatible basin.

## Method Summary
The method computes an influence function defined as the metric-weighted (Neural Tangent Kernel) overlap of loss gradients: $I(x, gx) = (\nabla_\theta \mathcal{L}(gx))^\top \chi^{-1} (\nabla_\theta \mathcal{L}(x))$. High positive influence indicates that training on $x$ improves performance on $g \cdot x$, signaling that the model couples these states on the loss landscape. The approach was applied to UNet (13M params) and ViT (5M params) architectures trained on PDEGym datasets for 2D compressible Euler and Navier-Stokes flows, using autoregressive training with SMSE loss and distributed training on A100 GPUs.

## Key Results
- Orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations
- Data-induced anisotropies can cause optimization to converge to symmetry-breaking basins
- Architecture-dependent gradient allocation: UNets show uniform influence across orbits while ViTs exhibit non-uniform, resonant coupling

## Why This Works (Mechanism)

### Mechanism 1: Orbit-Wise Gradient Coherence as a Symmetry Diagnostic
If a model has internalized a physical symmetry, the gradient updates induced by a specific input $x$ will constructively influence (lower the loss of) its symmetry-transformed counterparts $g \cdot x$. The method computes an influence function defined as the metric-weighted (Neural Tangent Kernel) overlap of loss gradients. High positive influence indicates that training on $x$ improves performance on $g \cdot x$, signaling that the model couples these states on the loss landscape. If influence values approach zero or turn negative for specific group elements, it indicates a decoupling of learning dynamics, meaning the model is likely memorizing patterns rather than learning the symmetry.

### Mechanism 2: Data-Induced Anisotropy and Basin Selection
When training data exhibits directional biases (anisotropies), optimization can converge to a basin that minimizes training loss but possesses a geometry that actively breaks symmetry. The optimizer follows the path of steepest descent dictated by the data. If the training distribution heavily samples specific orientations, the loss landscape geometry elongates or warps to favor these directions. Gradient signals from under-sampled symmetry transformations become subleading perturbations that fail to accumulate coherently.

### Mechanism 3: Architecture-Dependent Gradient Allocation
If the model architecture enforces rigid inductive biases (e.g., convolutions), it promotes uniform gradient coupling across orbits; conversely, flexible architectures (e.g., Transformers) allow for non-uniform, "resonant" influence allocation. UNets (CNNs) possess built-in translation equivariance, enforcing a smoother, more uniform influence profile across translations. ViTs lack this constraint, allowing them to concentrate influence on "privileged" group elements to optimize faster, potentially at the cost of heterogeneous symmetry adherence.

## Foundational Learning

- **Influence Functions & The Neural Tangent Kernel (NTK)**: The paper relies on a metric-weighted definition of influence rather than standard Euclidean gradients. Understanding that the NTK defines the geometry of parameter space is crucial for interpreting the diagnostic. *Quick check*: Why does the paper use the inverse NTK ($\chi^{-1}$) to weigh the overlap of gradients rather than a simple dot product?
- **Equivariance vs. Invariance**: The study focuses on PDE solution operators which must be equivariant (output transforms with input), not just invariant. The diagnostic measures this specific property. *Quick check*: If a model is equivariant under a rotation group $G$, what is the expected relationship between $f(x)$ and $f(g \cdot x)$?
- **Group Orbits**: The diagnostic evaluates coherence *along* an orbitâ€”the set of all states reachable by symmetry transformations of a single input. *Quick check*: What does it imply for the "mechanism of learning" if the influence between two points on the same orbit is near zero?

## Architecture Onboarding

- **Component map**: PDEGym dataset -> UNet (13M params) and ViT (5M params) backbones -> Autoregressive training -> Influence computation via Krylov solvers
- **Critical path**: 1) Train emulator on PDE trajectories (standard autoregressive MSE loss) 2) Select test mini-batches and apply symmetry transformations 3) Compute loss gradients $\nabla_\theta \mathcal{L}$ for original and transformed states 4) Solve for influence: Calculate $I(x, gx)$ using the NTK metric to assess gradient coherence
- **Design tradeoffs**: UNet offers high symmetry adherence (uniform influence) but potential optimization rigidity; ViT provides fast convergence and high peak accuracy but risks "idiosyncratic" coupling where some symmetry transformations are ignored
- **Failure signatures**: Suppressed Cross-Influence ($I(x, gx) \approx 0$ while $I(x, x) \gg 0$) indicates the model treats $x$ and $gx$ as unrelated tasks; Resonance Artifacts (periodic spikes in influence/error plots) indicate architectural biases interfering with continuous symmetry learning
- **First 3 experiments**: 1) Translation Baseline: Compute influence matrix for horizontal translations on trained UNet to verify uniform profile 2) Dihedral Stress Test: Train ViT on Navier-Stokes dataset and plot equivariance error vs. influence for D4 elements to reproduce suppressed influence correlation 3) Architectural Ablation: Vary ViT patch size and observe change in periodicity of translation influence landscape to confirm link between patching and axis-dependent anisotropy

## Open Questions the Paper Calls Out

### Open Question 1
Can influence-based diagnostics be used to formally determine minimal data augmentation strategies for symmetry learning? The paper demonstrates that low influence correlates with high equivariance error, but does not test whether actively augmenting low-influence data points closes the generalization gap efficiently. An ablation study where augmentation is applied only to group elements exhibiting sub-threshold cross-influence would resolve this.

### Open Question 2
How does the influence-based diagnostic perform when assessing approximate symmetries, such as Galilean boosts, which generic architectures can only satisfy approximately? The current study focuses on discrete transformations where exact equivariance is possible; the framework's sensitivity to approximate or "soft" symmetries remains untested. Application to models trained on data with Galilean invariance would resolve this.

### Open Question 3
Does orbit-wise gradient coherence function as a causal mechanism for symmetry learning during early training, or is it merely a property of the final converged basin? The paper establishes a correlation between final coherence and generalization but lacks temporal data to prove that coherent gradients drive the model into symmetry-compatible basins during the optimization trajectory. Tracking the evolution of cross-influence matrices from initialization to convergence would resolve this.

## Limitations
- Exact computation of the NTK-based metric is computationally expensive and implementation details from companion paper are needed
- Data-induced bias attribution is plausible but not definitively isolated from other factors like architectural bias
- Diagnostic validation relies on forward-pass equivariance error, which is a local test that may not predict long-range generalization

## Confidence

- **High Confidence**: The mechanism linking orbit-wise gradient coherence to symmetry generalization is well-supported by empirical correlation across architectures and datasets
- **Medium Confidence**: The diagnosis of data-induced anisotropy as the cause of suppressed influence is plausible but not definitively proven
- **Low Confidence**: The exact numerical values of influence across group elements are highly sensitive to metric construction and solver parameters

## Next Checks

1. **Metric Sensitivity Analysis**: Reproduce influence vs. equivariance error plots while systematically varying the damping parameter $\lambda$ in the metric construction to quantify sensitivity to numerical implementation choices.

2. **Architectural Ablation with Explicit Symmetry**: Train a ViT with modified patch size and a known discrete symmetry (group convolution layer for 90-degree rotations) to isolate the effect of architectural bias on gradient coherence.

3. **Data Augmentation Intervention**: Train a UNet on NS dataset with aggressive data augmentation (random rotations and reflections) and measure the influence landscape and equivariance error before and after augmentation to directly test the hypothesis that data-induced anisotropy drives suppressed cross-influence.