---
ver: rpa2
title: 'Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for
  Multilingual Fact-Checked Claim Retrieval'
arxiv_id: '2505.12616'
source_url: https://arxiv.org/abs/2505.12616
tags:
- tf-idf
- retrieval
- languages
- fact
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a TF-IDF-based retrieval system for multilingual
  fact-checked claim retrieval, experimenting with vector dimensions and tokenization
  strategies. The best configuration used word-level tokenization with a 15,000-feature
  vocabulary, achieving an average success@10 score of 0.78 on the development set
  and 0.69 on the test set across ten languages.
---

# Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval

## Quick Facts
- arXiv ID: 2505.12616
- Source URL: https://arxiv.org/abs/2505.12616
- Reference count: 5
- Best configuration achieved 0.78 average S@10 on dev and 0.69 on test across 10 languages

## Executive Summary
This paper presents a TF-IDF-based retrieval system for multilingual fact-checked claim retrieval, experimenting with vector dimensions and tokenization strategies. The best configuration used word-level tokenization with a 15,000-feature vocabulary, achieving an average success@10 score of 0.78 on the development set and 0.69 on the test set across ten languages. The system performed better on higher-resource languages but lagged behind the top-ranked system's 0.96 average score. The findings suggest that properly optimized traditional methods like TF-IDF remain competitive baselines, particularly in resource-constrained scenarios, though advanced neural architectures still outperform them overall.

## Method Summary
The system uses TF-IDF with TfidfVectorizer from scikit-learn, experimenting with vocabulary sizes (5K, 10K, 15K, 20K, 25K) and tokenization strategies (word-level vs character-level). Text preprocessing concatenates post text and metadata, then applies TF-IDF vectorization. Cosine similarity ranks fact-checks against each post, returning top-10 predictions. The approach is implemented in Python with a simple pipeline that processes CSV files and outputs JSON predictions.

## Key Results
- Word-level tokenization with 15,000 features achieved 0.78 S@10 on dev and 0.69 on test
- Character-level analyzers underperformed at ~0.45 average S@10 across all vocabulary sizes
- TF-IDF with proper optimization outperformed unoptimized neural baselines (XLM-R: 0.0984, DistilBERT: 0.3212) but fell short of top-ranked system (0.96)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing vocabulary size from default to 15,000 features improves retrieval performance.
- Mechanism: A larger `max_features` parameter retains more unique terms, increasing the probability that query terms match fact-check terms in the TF-IDF vector space. The 15K vocabulary captured domain-specific terminology that smaller vocabularies excluded.
- Core assumption: Important retrieval terms are among the top 15,000 by frequency; lower-frequency terms are noise rather than signal.
- Evidence anchors:
  - [abstract] "best-performing configuration used word-level tokenization with a vocabulary size of 15,000 features, achieving an average success@10 score of 0.78 on the development set"
  - [section] Table 2 shows Base (0.5513 avg) vs 10K (0.7630 avg) vs 15K (0.7757 avg)
  - [corpus] Weak comparative evidence—corpus papers focus on neural approaches rather than TF-IDF hyperparameter tuning.
- Break condition: Vocabulary expansion yields diminishing returns when additional terms are predominantly noisy or when computational constraints dominate.

### Mechanism 2
- Claim: Word-level tokenization outperforms character-level approaches for multilingual claim retrieval.
- Mechanism: Word analyzers capture semantic units directly, while character n-grams fragment meaning across sub-word sequences. For languages with complex morphology (German, Arabic), character analyzers produced noisy representations that degraded similarity matching.
- Core assumption: Claims and posts share vocabulary terms; lexical overlap correlates with semantic relevance.
- Evidence anchors:
  - [section] "character-based TF-IDF approaches underperformed, such as TF-IDF 15K (Char WB Analyzer) with an average score of 0.4528"
  - [section] "Character-based analyzers underperformed, more so in German and Arabic, mainly due to their complex word forms"
  - [corpus] No direct corpus evidence on tokenization strategy comparisons.
- Break condition: Word-level tokenization fails when languages lack whitespace delimiters or when morphological variants prevent exact matches.

### Mechanism 3
- Claim: Properly optimized TF-IDF can outperform unoptimized neural embedding models on this task.
- Mechanism: TF-IDF's sparse representations matched the lexical nature of claim-post pairs, while neural models (XLM-RoBERTa, DistilBERT) without task-specific fine-tuning produced generic embeddings that failed to capture domain-specific relevance signals.
- Core assumption: The neural models tested were not fine-tuned; performance reflects zero-shot behavior, not neural model capacity.
- Evidence anchors:
  - [section] Table 1: TF-IDF-B (0.5513) vs XLM-R (0.0984) vs DistilBERT (0.3212) on dev
  - [section] "fine-tuning these models on a more extensive and diverse set of fact-checking data could help them better capture domain-specific features"
  - [corpus] Neighbor papers (e.g., "fact check AI at SemEval-2025 Task 7") report higher scores using fine-tuned bi-encoder models, supporting the fine-tuning hypothesis.
- Break condition: This advantage disappears when neural models are properly fine-tuned on domain data, as evidenced by the top-ranked system's 0.96 score.

## Foundational Learning

- Concept: TF-IDF (Term Frequency-Inverse Document Frequency)
  - Why needed here: Core retrieval mechanism; understanding how term importance weighting affects similarity is essential for debugging and optimization.
  - Quick check question: If a term appears in 90% of documents, will TF-IDF give it high or low weight? Why?

- Concept: Success@K (S@10) metric
  - Why needed here: Evaluation metric determines optimization direction; S@10 rewards systems that retrieve at least one relevant result in top 10.
  - Quick check question: A system retrieves 5 relevant documents at rank 15-19. What is its Success@10 score?

- Concept: Sparse vs. dense retrieval representations
  - Why needed here: Paper compares sparse (TF-IDF) against dense (E5, T5, XLM-R) approaches; understanding tradeoffs guides model selection.
  - Quick check question: What is the memory implication of storing 15,000-dimensional sparse vectors vs. 768-dimensional dense vectors for 150K documents?

## Architecture Onboarding

- Component map: Data loader -> Preprocessor -> Vectorizer -> Similarity computer -> Ranker -> Output generator
- Critical path: The `max_features` and `analyzer` parameters in TfidfVectorizer directly control retrieval quality. Incorrect settings cascade to all downstream similarity computations.
- Design tradeoffs:
  - Larger vocabulary (15K) improves recall but increases memory and computation
  - Word-level tokenization is simpler but fails on morphologically rich languages
  - TF-IDF is computationally cheap but cannot capture semantic similarity beyond lexical overlap
- Failure signatures:
  - Low scores on English (0.452) despite high training data proportion (55.76%) suggests lexical mismatch between posts and fact-checks
  - Character analyzers scoring ~0.45 avg indicates tokenization strategy failure
  - Neural models scoring below TF-IDF suggests insufficient fine-tuning or domain adaptation
- First 3 experiments:
  1. Reproduce the 15K word-level baseline on the dev split to validate pipeline correctness (target: ~0.78 S@10).
  2. Ablate vocabulary size (5K, 10K, 20K, 25K) to identify the performance cliff point.
  3. Implement a hybrid approach: use TF-IDF for first-stage retrieval, then re-rank top-100 with E5 embeddings to test the reviewers' suggestion.

## Open Questions the Paper Calls Out

None

## Limitations

- Lack of per-language performance analysis makes it difficult to assess generalization across all ten languages
- Neural model comparisons are unfair since baselines were not fine-tuned, creating apples-to-oranges benchmarking
- Statistical significance testing is absent for the claimed performance differences between tokenization strategies

## Confidence

**High Confidence**: The claim that TF-IDF with 15K word-level vocabulary achieves 0.78 S@10 on dev set and 0.69 on test set. This is directly reported from the competition results with specific, verifiable scores.

**Medium Confidence**: The mechanism that larger vocabulary improves performance by retaining more unique terms. While supported by the score progression (0.5513 → 0.7630 → 0.7757), the evidence is correlational rather than causal, and the optimal point is not demonstrated.

**Low Confidence**: The claim that TF-IDF can outperform unoptimized neural models. This comparison is fundamentally flawed since the neural models weren't fine-tuned, making it an unfair benchmark. The top-ranked system achieving 0.96 S@10 suggests that properly optimized neural approaches dominate.

## Next Checks

1. **Per-language performance analysis**: Request the raw per-language S@10 scores from the competition organizers to determine if the 0.78/0.69 averages mask significant performance variation across languages, particularly for low-resource languages where TF-IDF might fail.

2. **Statistical significance testing**: Perform paired t-tests or bootstrap confidence intervals on the 15K word-level vs. character-level comparisons to determine if the ~0.15-0.20 score differences are statistically significant given the evaluation set size.

3. **Vocabulary size ablation study**: Re-run the TF-IDF pipeline with vocabulary sizes at 5K, 10K, 15K, 20K, and 25K on the dev set to identify the precise performance curve and determine whether 15K is truly optimal or if performance plateaus earlier.