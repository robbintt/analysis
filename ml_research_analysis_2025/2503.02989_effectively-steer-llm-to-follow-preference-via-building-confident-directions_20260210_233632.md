---
ver: rpa2
title: Effectively Steer LLM To Follow Preference via Building Confident Directions
arxiv_id: '2503.02989'
source_url: https://arxiv.org/abs/2503.02989
tags:
- steering
- direction
- user
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a theoretical framework to understand and quantify
  model steering methods for aligning LLM outputs with user preferences. The framework
  characterizes effective steering directions that can accurately represent user preferences
  and generate outputs matching Bayesian optimal predictors.
---

# Effectively Steer LLM To Follow Preference via Building Confident Directions

## Quick Facts
- arXiv ID: 2503.02989
- Source URL: https://arxiv.org/abs/2503.02989
- Reference count: 26
- Authors: Bingqing Song; Boran Han; Shuai Zhang; Hao Wang; Haoyang Fang; Bonan Min; Yuyang Wang; Mingyi Hong
- Primary result: CONFST outperforms mean steering baselines in topic and style shift tasks, achieving higher success rates in steering outputs across diverse preferences without explicit instructions.

## Executive Summary
This paper introduces CONFST, a method for steering LLM outputs toward user preferences by constructing "confident" steering directions from high-confidence activations. The approach uses logistic regression to classify activations into preference categories, then builds steering vectors from only those activations that exceed a confidence threshold. This theoretically justified method enables steering toward multiple preferences without requiring explicit instructions, head selection, or layer tuning. Experiments on GPT-2XL, Mistral, and Gemma demonstrate superior performance compared to mean steering baselines across topic shift (news categories) and style shift (emotional tone, verbosity, helpfulness, safety) tasks.

## Method Summary
CONFST extracts Multi-Head Attention activations from a fixed shallow layer of the LLM, trains a logistic regression classifier to distinguish user preferences, and constructs steering vectors by averaging only high-confidence activations (those exceeding threshold β). During inference, these vectors are added to the residual stream at a specified layer and position to bias generation toward the target preference. The method supports additive steering for combining multiple preferences and works without explicit user instructions. Experiments use various datasets including AgNews for topic shift and HelpSteer/oasst2 for style shift, with success rates measured by classification accuracy or LLM-as-judge scoring.

## Key Results
- CONFST achieves higher success rates than mean steering baselines in topic shift tasks across AgNews dataset with 4 classes
- The method demonstrates effective additive steering, successfully combining topic and style preferences (e.g., sports + conciseness)
- Performance is sensitive to the confidence threshold β, with optimal values varying by dataset and preference type
- The approach works across different model architectures (GPT-2XL, Mistral, Gemma) with minimal hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1: High-Confidence Activation Selection
The method selects activations that receive high classification probability from a logistic regression model, filtering out ambiguous samples. This creates a steering vector that more accurately represents the target preference direction. The core assumption is that high classification probability correlates with proximity to the true latent preference vector. If the classifier generalizes poorly or β is too high for sparse data, the steering vector becomes zero or biased, causing steering failure.

### Mechanism 2: Shallow-Layer Residual Intervention
Steering vectors are added to the residual stream at a fixed shallow layer (typically layer 0 or 1), shifting internal representations processed by subsequent layers. The assumption is that early layers encode high-level semantic features that can be pivoted without destroying deeper grammatical processing. If α is excessive, outputs become incoherent; if the layer is too deep, the semantic direction may be ignored or conflict with positional features.

### Mechanism 3: Bayesian Approximation via Filtering
The filtering process theoretically approximates a Bayesian optimal predictor by maximizing the posterior probability of the preference given the steering vector, minimizing KL divergence between steered and optimal distributions. This assumes confident activations satisfy the theoretical requirement of separability. If user history contains mixed preferences, the theoretical optimality guarantee degrades.

## Foundational Learning

- **Concept: Residual Stream in Transformers**
  - **Why needed here:** The method injects steering vectors directly into the residual stream, which accumulates information across layers
  - **Quick check question:** How does adding a vector to the residual stream at layer 1 affect the input to the Attention block at layer 2?

- **Concept: Logistic Regression for Probing**
  - **Why needed here:** The algorithm uses a linear classifier to "probe" activations and determine confidence scores
  - **Quick check question:** Why might a linear classifier be sufficient to separate high-level concepts like "sports" vs. "business" in a large model's activation space?

- **Concept: KL Divergence and Distribution Alignment**
  - **Why needed here:** The theoretical framework uses KL divergence to prove that the steered model is closer to the "optimal" preference predictor
  - **Quick check question:** If the KL divergence between the steered distribution and the target distribution is high, what does that imply about the effectiveness of the steering vector?

## Architecture Onboarding

- **Component map:** Activation Extractor -> Preference Classifier -> Vector Constructor -> Inference Engine
- **Critical path:** The offline training of the Preference Classifier is the critical dependency. Without accurate classification of activations into preference classes, the subsequent selection of "confident" directions is random, and the steering vector will be noisy.
- **Design tradeoffs:**
  - Layer Selection (ℓ): Fixed shallow layer simplifies implementation vs. potential loss of precision compared to layer-wise optimization
  - Confidence Threshold (β): Higher β ensures purity of direction vs. risk of empty vectors for sparse preferences
- **Failure signatures:**
  - No effect: Steering coefficient α is too low, or threshold β is too strict (empty vector)
  - Incoherent output: α is too high, perturbing the residual stream beyond the model's robustness
  - Wrong direction: Classifier is poorly trained or user history contains conflicting labels
- **First 3 experiments:**
  1. Baseline Comparison (AgNews): Run CONFST on GPT-2XL using AgNews dataset, compare success rate against Mean Steering
  2. Ablation on Threshold (β): Run on Emotion dataset while sweeping β from 0.1 to 0.9, verify performance degradation at extremes
  3. Additive Steering (Topic + Style): Construct vectors for "Sports" and "Conciseness", add them, and verify short sports summaries

## Open Questions the Paper Calls Out

### Open Question 1
Can the confident direction steering method effectively scale to align hundreds of distinct user preferences or styles simultaneously?
- **Basis:** The conclusion states the need to develop methods applicable to hundreds of styles and topics
- **Why unresolved:** Experiments only validate on small numbers of classes (4 topics in AgNews)
- **Evidence needed:** Successful application on datasets containing >100 distinct preference categories

### Open Question 2
How does the performance of additive steering degrade when combining more than two distinct preference directions?
- **Basis:** The conclusion identifies the need to align more preference directions beyond the two-trait co-steering demonstrated
- **Why unresolved:** Paper demonstrates additive steering primarily on two traits
- **Evidence needed:** Empirical results showing successful simultaneous steering of 3+ orthogonal preferences

### Open Question 3
How can the confidence threshold β be determined adaptively to maximize steering success rates without manual tuning?
- **Basis:** Remark 4 notes success rates don't always increase with β due to data noise
- **Why unresolved:** The method treats β as a hyperparameter requiring dataset-specific tuning
- **Evidence needed:** A mechanism that dynamically adjusts β based on signal-to-noise ratio of training activations

### Open Question 4
Is a single fixed shallow layer universally optimal for steering across different model architectures and preference types?
- **Basis:** While the abstract claims "no need to determine which layer," the algorithm requires a target layer input
- **Why unresolved:** The paper doesn't provide theoretical justification for why shallow layers suffice universally
- **Evidence needed:** Ablation studies across diverse architectures showing shallow layers consistently contain most separable preference information

## Limitations
- Theoretical framework relies on assumptions about unique ground-truth preference vectors that may not hold with nuanced or contradictory preferences
- Empirical validation limited to GPT-2XL, Mistral, and Gemma models, with no testing on more capable models like GPT-4 or Claude
- Steering effectiveness sensitive to hyperparameters (β threshold, α coefficient) requiring dataset-specific tuning
- Additive steering mechanism demonstrated but not theoretically justified—no guarantee that linear superposition produces coherent intermediate preferences

## Confidence

- **High Confidence**: Basic mechanism of filtering high-confidence activations and adding them to residual streams works empirically for discrete preference shifts. Ablation studies on β threshold and success rate metrics are straightforward to reproduce.
- **Medium Confidence**: Theoretical claims about Bayesian optimality and KL divergence minimization are mathematically sound but their practical relevance depends on whether "confident direction" selection approximates the theoretical ideal in high-dimensional activation spaces.
- **Low Confidence**: Generalizability to complex, continuous preference spaces and effectiveness on state-of-the-art models remains unproven. Robustness to adversarial preferences or preferences requiring semantic understanding is unknown.

## Next Checks

1. **Classifier Validation Before Steering**: Verify the logistic regression classifier's validation accuracy on held-out preference data before running steering experiments. If accuracy is below 80-85%, the subsequent "confident direction" selection will be unreliable.

2. **Steering Vector Dimensionality Analysis**: For each preference class, compute and report the L2 norm of the steering vector v and the number of activations selected (those above β threshold). Compare across different β values and preference classes to quantify how sparse/representative the selected directions are.

3. **Cross-Model Transferability Test**: Apply CONFST trained on GPT-2XL activations to steer Mistral or Gemma models (or vice versa), holding all hyperparameters constant. This tests whether steering directions are model-specific or capture more general preference representations.