---
ver: rpa2
title: Improving Model Alignment Through Collective Intelligence of Open-Source LLMS
arxiv_id: '2505.03059'
source_url: https://arxiv.org/abs/2505.03059
tags:
- data
- alignment
- llms
- performance
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Mixture of Agents Alignment (MoAA), a method
  for improving large language model alignment by leveraging multiple open-source
  LLMs to generate high-quality synthetic training data. The approach employs a two-stage
  process: first using a mixture of agents (MoA) framework to produce diverse, high-quality
  supervised fine-tuning (SFT) data, then employing MoA as a reward model for direct
  preference optimization (DPO).'
---

# Improving Model Alignment Through Collective Intelligence of Open-Source LLMS

## Quick Facts
- arXiv ID: 2505.03059
- Source URL: https://arxiv.org/abs/2505.03059
- Reference count: 40
- This paper introduces Mixture of Agents Alignment (MoAA), a method for improving large language model alignment by leveraging multiple open-source LLMs to generate high-quality synthetic training data.

## Executive Summary
This paper introduces Mixture of Agents Alignment (MoAA), a method for improving large language model alignment by leveraging multiple open-source LLMs to generate high-quality synthetic training data. The approach employs a two-stage process: first using a mixture of agents (MoA) framework to produce diverse, high-quality supervised fine-tuning (SFT) data, then employing MoA as a reward model for direct preference optimization (DPO). The method is evaluated on LLaMA-3.1-8B-Instruct and Gemma-2-9B-it, showing substantial improvements—win rates increase from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2. The results demonstrate that MoAA-generated data outperforms single-model alternatives and even surpasses GPT-4o in some benchmarks.

## Method Summary
MoAA is a two-stage alignment method. First, a Mixture of Agents (MoA) architecture generates high-quality SFT data: proposer models generate responses, and an aggregator synthesizes them into unified outputs. Second, the trained model uses this MoA as a reward model for DPO, where multiple responses are evaluated with criteria filtering and the aggregator decides preferences. The method is evaluated on LLaMA-3.1-8B-Instruct and Gemma-2-9B-it, showing substantial improvements on alignment benchmarks.

## Key Results
- MoAA-generated data outperforms single-model alternatives and even surpasses GPT-4o in some benchmarks
- Win rates increase from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2
- The strongest model in the MoA ensemble improves significantly when fine-tuned on MoAA-generated data, highlighting its potential to advance open-source LLMs without reliance on stronger external supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model iterative refinement produces higher-quality synthetic data than single-model generation.
- Mechanism: Proposer models generate diverse initial responses; aggregator models synthesize these into unified outputs across multiple layers, reducing individual model biases and gaps.
- Core assumption: Aggregator models can effectively identify and combine correct information while filtering errors from proposer outputs.
- Evidence anchors:
  - [abstract] "MoAA-generated data outperforms single-model alternatives and even surpasses GPT-4o in some benchmarks"
  - [section 4.2, Table 3] MoA-SFT achieves 43.77 AlpacaEval2 score vs. "Combined 5" (27.23) and "Random 5" (25.30), demonstrating architecture matters beyond mere multi-model access
  - [corpus] Related work on LLM aggregation shows consistent benefits (FMR 0.58 in routing/aggregation studies), but corpus lacks direct comparisons of layered proposer-aggregator vs. simple ensemble
- Break condition: If aggregator model lacks sufficient capability to discriminate quality, synthesized outputs may amplify rather than reduce errors.

### Mechanism 2
- Claim: MoA-based preference annotation provides more robust reward signals than single reward models for DPO.
- Mechanism: Multiple proposer models evaluate response pairs with criteria filtering; aggregator synthesizes judgments, reducing position bias through dual evaluation with switched order.
- Core assumption: Collective judgment averages out individual evaluator biases and blind spots.
- Evidence anchors:
  - [abstract] Two-stage process using "MoA as a reward model for direct preference optimization"
  - [section 4.3, Table 6] MoA reward model achieves 57.23 AlpacaEval2 vs. individual models (55.35-56.81), though ArmoRM scores 57.79
  - [section 3.2.3, Table 7] Criteria filtering improves Safety (88.1→90.6) and Reasoning (85.6→87.7) on RewardBench
  - [corpus] Weak direct evidence; corpus papers focus on coordination rather than reward modeling
- Break condition: If all proposer models share systematic biases (e.g., length preference), aggregation won't correct them.

### Mechanism 3
- Claim: Strong models can improve beyond their initial capabilities when trained on MoA-generated data containing their own outputs.
- Mechanism: Models learn from refined combinations of collective responses, including their own contributions processed through the aggregator, capturing patterns they couldn't consistently produce independently.
- Core assumption: The aggregation process produces outputs that exceed what any individual model could reliably generate alone.
- Evidence anchors:
  - [abstract] "models finetuned on MoA-generated data surpass their own initial capabilities"
  - [section 4.2, Table 4] Gemma-2-9B-it fine-tuned on MoA-Small-Scale achieves 54.19 AlpacaEval2 vs. original 48.54, outperforming itself as the strongest component
  - [corpus] Insufficient evidence; self-improvement loops in multi-agent systems remain underexplored in corpus
- Break condition: If aggregator simply selects best proposer response without synthesis, self-improvement potential is limited to distillation.

## Foundational Learning

- Concept: **Mixture of Agents (MoA) Architecture**
  - Why needed here: Core infrastructure for both data generation and reward modeling stages
  - Quick check question: Can you explain why a two-layer MoA with distinct proposer/aggregator roles outperforms combining five models' outputs directly?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Stage 2 alignment method that uses MoA-generated preference pairs
  - Quick check question: How does DPO differ from PPO-based RLHF in its reward model requirements?

- Concept: **Length-Controlled Win Rates**
  - Why needed here: Primary evaluation metric that neutralizes length bias in AlpacaEval2
  - Quick check question: Why might a model with higher raw win rates have lower LC win rates?

## Architecture Onboarding

- Component map: Input → [Proposers: WizardLM-8x22B, Qwen2-72B, Gemma-2-27B, LLaMA-3.1-70B] → [Aggregator: Qwen1.5-110B] → Synthetic SFT Data → Target Model → Sample 5 responses → [MoA Reward Model with Criteria Filtering] → Chosen/Rejected pairs → DPO Training

- Critical path: Proposer/aggregator selection dominates outcome quality; criteria filtering in Stage 2 provides measurable Safety/Reasoning gains.

- Design tradeoffs:
  - Two layers balance quality vs. cost (Table 11: MoAA achieves 90.6% MoA performance at 5.4% cost)
  - Ultrafeedback + Ultrachat mix trades dataset scale for multi-turn capability
  - On-policy response generation for DPO prevents distribution mismatch (Appendix J)

- Failure signatures:
  - SFT regression on reasoning tasks (Table 12: MMLU 0.7089→0.6854) requires DPO recovery
  - "Combined 5" approach (simply concatenating data) underperforms MoA by 16+ points
  - Off-policy MoA generation creates distribution mismatch, reducing DPO effectiveness

- First 3 experiments:
  1. Replicate MoAA-SFT with 2 proposers + 1 aggregator on 10K Ultrafeedback subset to validate architecture vs. random teacher baseline
  2. Ablate criteria filtering on 500 held-out examples to measure Safety/Reasoning delta
  3. Test self-improvement hypothesis: fine-tune your strongest available model on MoA data where it served as proposer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated discrete optimization methods identify Mixture of Agents (MoA) architectures that outperform manually selected configurations?
- Basis in paper: [explicit] Appendix B states, "Note that a more explicit and intelligent search method can be used to find potentially better architecture. We leave this interesting exploration to future work."
- Why unresolved: The current study relies on manual selection and limited empirical sweeps to determine the layers, proposers, and aggregators.
- What evidence would resolve it: Implementation of an automated search algorithm (e.g., using BFGS) that identifies architectures exceeding the performance of the manually defined "MoA-Data-Generator" baseline.

### Open Question 2
- Question: How does the structural placement of previous turns' context (before vs. after the MoA prompt) affect the quality of synthetic multi-turn alignment data?
- Basis in paper: [explicit] Section 3.1.2 notes, "We can decide whether to put the previous turns' context before or after the MoA prompt. We leave a more exhaustive search of optimal structure to future work."
- Why unresolved: The authors acknowledge this design choice variability but do not evaluate the impact of different context injection points on model performance.
- What evidence would resolve it: An ablation study comparing models fine-tuned on data generated with varying context placement strategies, evaluated on multi-turn benchmarks like MT-Bench.

### Open Question 3
- Question: Does integrating explicit reasoning data into the MoAA training mixture mitigate the performance drop in math and coding tasks observed during the SFT stage?
- Basis in paper: [explicit] Appendix D observes a slight decrease in reasoning metrics and states, "Composing a more balanced dataset mixture with reasoning data is a nice direction of future work."
- Why unresolved: The current instruction dataset (Ultrafeedback/Ultrachat) prioritizes general alignment, potentially neglecting the specialized skills needed for MATH and HumanEval tasks.
- What evidence would resolve it: Evaluation of MoAA models trained on a modified dataset containing reasoning-heavy instructions to see if they recover or surpass the base model's reasoning scores.

## Limitations

- Proposer quality ceiling: The method's effectiveness depends heavily on having access to models substantially stronger than the target for data generation, creating a chicken-and-egg problem for initial adoption.
- Scale vs. quality tradeoff: While the paper shows MoAA achieves 90.6% of full MoA performance at 5.4% cost, the optimal point on this curve remains unclear.
- Self-improvement saturation: The observation that models can surpass their own initial capabilities is compelling but the paper provides limited analysis of when or why this effect plateaus.

## Confidence

**High confidence**: The empirical improvements on standard benchmarks (AlpacaEval2 LC win rates from 19.5→48.3, Arena-Hard from 22.33→57.23) are well-documented and robust across different target models. The architecture choices (proposer/aggregator separation, criteria filtering) are clearly specified and validated.

**Medium confidence**: The claim that MoAA-generated data outperforms single-model alternatives is supported by strong results (43.77 vs 27.23-25.30 on AlpacaEval2), but the comparison doesn't control for dataset size or diversity beyond the architectural differences. The self-improvement mechanism is observed but not fully explained.

**Low confidence**: The assertion that MoAA can advance open-source LLMs "without reliance on stronger external supervision" is overstated. The method fundamentally requires access to significantly stronger models (70B-110B parameter models) for the proposer/aggregator roles, which may not be practically available to many research groups.

## Next Checks

1. **Minimum viable configuration test**: Validate whether the architecture provides benefits when using smaller proposer/aggregator models (e.g., 7B-13B range) that are more widely accessible, and determine the point at which MoAA stops providing meaningful gains.

2. **Position bias validation**: Systematically test whether the reward MoA shows systematic preference for response position by evaluating the same pairs with switched order across 1000+ examples, measuring consistency of rankings.

3. **Saturation point analysis**: Conduct a systematic study of self-improvement limits by training the same model multiple times on increasingly large amounts of MoAA-generated data, measuring performance gains per unit of compute to identify when returns diminish.