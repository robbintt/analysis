---
ver: rpa2
title: Improving the Reusability of Conversational Search Test Collections
arxiv_id: '2503.09899'
source_url: https://arxiv.org/abs/2503.09899
tags:
- systems
- https
- test
- retrieval
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incomplete relevance judgments
  in conversational search test collections, which limits their reusability for evaluating
  new systems. The authors propose using large language models (LLMs) to automatically
  fill the "holes" in test collections by leveraging existing human judgments.
---

# Improving the Reusability of Conversational Search Test Collections
## Quick Facts
- arXiv ID: 2503.09899
- Source URL: https://arxiv.org/abs/2503.09899
- Reference count: 0
- This paper proposes using large language models to automatically fill incomplete relevance judgments in conversational search test collections, improving their reusability for evaluating new systems.

## Executive Summary
This paper addresses the challenge of incomplete relevance judgments in conversational search test collections, which limits their reusability for evaluating new systems. The authors propose using large language models (LLMs) to automatically fill the "holes" in test collections by leveraging existing human judgments. They experiment with both commercial (GPT-3.5) and open-source (Llama-3.1) models using zero-shot, few-shot, and fine-tuning approaches on TREC iKAT 23 and TREC CAsT 22 datasets.

## Method Summary
The authors propose a methodology for filling incomplete relevance judgments in conversational search test collections using large language models. They employ three approaches: zero-shot prompting, few-shot prompting, and fine-tuning of LLMs. The models (GPT-3.5 and Llama-3.1) are tasked with predicting relevance judgments for unjudged document-query pairs in conversational search collections. The evaluation uses binary agreement metrics and rank correlation measures to compare LLM-generated judgments against human assessors' judgments.

## Key Results
- Fine-tuned Llama-3.1 achieves 71.8% binary agreement with human assessors on the iKAT 23 dataset
- Few-shot prompting of GPT-3.5 achieves 0.862 Kendall's tau rank correlation with human-created pools on the iKAT 23 test set
- Conversational search collections become less reusable at deeper conversation turns due to increased system divergence

## Why This Works (Mechanism)
The methodology works because large language models can leverage patterns learned from existing human relevance judgments to predict missing judgments for unjudged document-query pairs. By fine-tuning on human-annotated data and using few-shot prompting, LLMs can capture the nuanced decision-making processes that human assessors employ when determining document relevance in conversational contexts. The approach exploits the consistency in human judgment patterns to fill gaps where manual annotation would be prohibitively expensive or time-consuming.

## Foundational Learning
The work builds on established principles from information retrieval evaluation, where test collections with complete relevance judgments are essential for fair system comparison. It extends previous research on automated relevance judgment by applying modern large language models to the specific challenges of conversational search, where the sequential nature of queries and the need for context-aware relevance assessment create unique evaluation requirements. The methodology also draws from transfer learning principles, where models trained on existing judgments can generalize to predict missing relevance assessments.

## Architecture Onboarding
The architecture involves three main components: the conversational search test collection with incomplete judgments, the large language model (either commercial like GPT-3.5 or open-source like Llama-3.1), and the evaluation framework. The LLM processes document-query pairs through prompt engineering or fine-tuning, generating predicted relevance judgments. These predictions are then evaluated against human judgments using binary agreement metrics and rank correlation measures. The system architecture is designed to be modular, allowing different LLM models and prompting strategies to be easily substituted.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of LLM-based hole-filling across different conversational search domains, the impact of temporal changes in document relevance on LLM predictions, and how different types of judgment errors might affect downstream system evaluation outcomes. Additionally, it raises questions about the methodology's performance on collections with non-English content, domain-specific terminology, or varying conversational structures.

## Limitations
- The methodology's performance on collections with different conversational structures, domain-specific terminology, or non-English content remains unknown
- The relatively high binary agreement (71.8%) still represents a substantial error rate that could affect system evaluation outcomes
- The claim that hole-filling enables "fairer comparison" assumes LLM-generated judgments adequately approximate human judgment quality
- The approach may not capture the full complexity of human relevance assessment, particularly for nuanced or context-dependent judgments
- The methodology's effectiveness across different LLM architectures and sizes has not been systematically explored

## Confidence
- **High Confidence**: The observation that conversational search test collections become less reusable at deeper conversation turns is well-supported by the analysis of system divergence patterns
- **Medium Confidence**: The effectiveness of few-shot prompting for GPT-3.5 and fine-tuning for Llama-3.1 is demonstrated on specific test collections but requires validation on other conversational search scenarios
- **Low Confidence**: The assumption that LLM-generated judgments enable fair comparison between systems may not hold across all system types or query contexts

## Next Checks
1. Apply the hole-filling methodology to conversational search collections from different domains (e.g., medical, technical support) to assess generalizability beyond TREC datasets
2. Evaluate whether LLM-generated judgments remain consistent over time as models are updated or when applied to collections collected across different time periods
3. Systematically assess how different types of LLM judgment errors affect downstream system evaluation metrics and rankings to quantify the practical impact of the 28.2% disagreement rate