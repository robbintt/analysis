---
ver: rpa2
title: Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model
arxiv_id: '2505.18674'
source_url: https://arxiv.org/abs/2505.18674
tags:
- image
- diffusion
- restoration
- images
- iide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an internal detail-preserving diffusion model
  for high-fidelity restoration of real-world degraded images. The method addresses
  the challenge of restoring old photographs and low-resolution images with complex,
  mixed degradations like scratches, color fading, and noise, while also providing
  object-level colorization control.
---

# Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model

## Quick Facts
- **arXiv ID:** 2505.18674
- **Source URL:** https://arxiv.org/abs/2505.18674
- **Authors:** Peng Xiao; Hongbo Zhao; Yijun Wang; Jianxin Lin
- **Reference count:** 39
- **Primary result:** Achieves 53.6% improvement in CLIPIQA score and 29.5% improvement in MUSIQ score compared to existing methods for old photo restoration.

## Executive Summary
This paper introduces an Internal Detail Enhancement (IIDE) technique for high-fidelity restoration of real-world degraded images. The method fine-tunes Stable Diffusion 2.1 using a ControlNet architecture while freezing the base model, allowing it to leverage pretrained generative priors for perceptual quality. By injecting degradation operations into the diffusion denoising process and enforcing self-consistency constraints, IIDE preserves essential structural details while achieving state-of-the-art results on old photo restoration tasks. The approach also enables text-guided restoration with object-level color control, demonstrating significant improvements in both qualitative and perceptual quantitative evaluations.

## Method Summary
The method operates entirely in Stable Diffusion's latent space, encoding degraded inputs via VAE and applying diffusion denoising with IIDE constraints. At each diffusion step, the model estimates a "clean" latent from the noisy state using DDIM and enforces self-regularization between conditions from the original degraded input and the estimated version. Only the ControlNet module is trained while the SD backbone remains frozen, preserving generative quality. The approach supports multiple restoration tasks including old photo restoration (with scratch masks) and super-resolution (with null text prompts), achieving superior perceptual metrics at the expense of some pixel-level fidelity.

## Key Results
- Achieves 53.6% improvement in CLIPIQA score and 29.5% improvement in MUSIQ score compared to existing methods for old photo restoration
- Demonstrates significant perceptual quality improvements while maintaining reasonable pixel-level fidelity (with PSNR trade-off of -0.62)
- Enables text-guided restoration with object-level color control, allowing semantic similarity adjustments based on textual prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The IIDE technique creates a self-regularization constraint that forces the diffusion model to preserve structural details from degraded inputs without requiring explicit degradation modeling.
- **Core assumption:** DDIM-estimated x̃₀ at intermediate timesteps provides structurally meaningful degraded approximations, and the Markov transition dynamics preserve sufficient content information for the self-consistency constraint to be meaningful.
- **Evidence anchors:** [abstract], [section III.B], and corpus work on proxy-based restoration evaluation.
- **Break condition:** If DDIM estimates at early timesteps produce high-variance or semantically inconsistent degraded versions, the self-consistency constraint may enforce preservation of incorrect or noisy details rather than meaningful structure.

### Mechanism 2
- **Claim:** Operating in latent diffusion space allows the method to leverage pretrained generative priors while avoiding the computational cost of pixel-space diffusion.
- **Core assumption:** The noise schedule α̅_t is calibrated such that large-t latents are sufficiently content-agnostic for degraded inputs to share distributions with clean images, while mid-t latents retain recoverable structural information.
- **Evidence anchors:** [section III.A], [section III.B], and corpus work on latent-space restoration methods.
- **Break condition:** If real-world degradations involve structured artifacts that persist even at high noise levels, the content-agnostic assumption fails and the pretrained prior may not generalize.

### Mechanism 3
- **Claim:** Fine-tuning only a ControlNet while freezing the base Stable Diffusion model preserves generative quality while enabling conditional control for restoration tasks.
- **Core assumption:** Zero convolution initialization ensures training begins at SD's baseline generative behavior, and the frozen backbone provides sufficient representation capacity for restoration without architectural modification.
- **Evidence anchors:** [section III.C], [section IV.A], and comparisons with StableSR and DiffIR methods.
- **Break condition:** If degradation characteristics require modification to the UNet's attention mechanisms or early-layer feature extraction, the frozen constraint limits adaptation capacity.

## Foundational Learning

- **Concept:** Denoising Diffusion Implicit Models (DDIM)
  - **Why needed here:** IIDE relies critically on DDIM's deterministic sampling to estimate x̃₀ from intermediate z_t. Understanding Eq. 3-4 is essential for implementing the self-regularization constraint and debugging convergence issues.
  - **Quick check question:** Given z_t at step t, can you derive how DDIM directly estimates x̃₀ without running the full reverse sampling process?

- **Concept:** Latent Diffusion and VAE Geometry
  - **Why needed here:** The method operates entirely in Stable Diffusion's latent space. Degradation patterns, detail preservation, and the self-consistency constraint all manifest in latent geometry rather than pixel space.
  - **Quick check question:** Why might a scratch artifact in pixel space have a different (potentially more diffuse) representation in the VAE latent space, and how does this affect the ControlNet's ability to condition on it?

- **Concept:** Zero Convolution and Training Dynamics
  - **Why needed here:** ControlNet uses zero-initialized 1×1 convolutions to connect to the frozen UNet. This architectural choice determines initial training behavior and gradient flow characteristics.
  - **Quick check question:** At initialization, what is the output of a zero convolution layer, and why is this critical for preserving the pretrained model's behavior at the start of fine-tuning?

## Architecture Onboarding

- **Component map:** Input I_lq -> VAE Encoder E -> z_lq (latent) -> ControlNet -> conditional features -> Frozen SD 2.1 UNet -> DDIM Sampling (T→0) with IIDE constraint -> z_0 -> VAE Decoder D -> Restored I_re

- **Critical path:** 1) VAE encoding of degraded input to latent space, 2) ControlNet extraction of degradation-aware conditioning features, 3) DDIM reverse sampling with IIDE self-regularization applied at each step, 4) VAE decoding of final clean latent to pixel space

- **Design tradeoffs:**
  - p_iide = 0.5 mix-up probability: Lower values increase stability but reduce detail preservation effectiveness
  - Frozen backbone: Preserves generative priors but limits adaptation to severe or unusual degradations (reported PSNR tradeoff of -0.24 vs. StableSR)
  - Perceptual vs. fidelity metrics: Method explicitly optimizes CLIPIQA/MUSIQ at the expense of PSNR/SSIM (53.6% CLIPIQA improvement but 0.62 PSNR reduction vs. BrOldPho)
  - Task-specific conditions: Old photo restoration adds scratch masks; super-resolution uses null text prompts

- **Failure signatures:**
  - Detail hallucination without IIDE: Removing IIDE causes the model to generate plausible but incorrect textures and lose fine details
  - Color drift with conflicting prompts: Text prompts can override learned color distributions, requiring careful prompt engineering
  - Over-smoothing with high p_iide: Excessive self-regularization may suppress legitimate detail recovery
  - Scratch mask quality dependency: For old photos, scratch detection failures propagate directly to restoration quality

- **First 3 experiments:**
  1. **Ablate IIDE mechanism:** Train with p_iide ∈ {0.0, 0.5, 1.0} on DIV2K synthetic old photos. Measure CLIPIQA, MUSIQ, and PSNR to quantify the perceptual-fidelity tradeoff curve.
  2. **Degradation robustness test:** Create controlled degradation combinations (noise+blur, scratches+fading, compression+noise) and compare restoration quality against task-specific baselines. Assess generalization to unseen degradation mixtures.
  3. **ControlNet capacity analysis:** Vary ControlNet depth/width and measure convergence speed plus final quality. Determine whether the current architecture is under- or over-parameterized.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Internal Image Detail Enhancement (IIDE) mechanism be modified to close the performance gap in pixel-level fidelity metrics (PSNR/SSIM) while maintaining its superior perceptual quality?
- **Basis in paper:** [explicit] Section IV explicitly states the method exhibits a "slight trade-off in traditional quality metrics, such as PSNR and SSIM."
- **Why unresolved:** The current implementation prioritizes generative prior for perceptual realism, which inherently alters pixel distributions.
- **What evidence would resolve it:** A modified loss function or architectural adjustment that achieves comparable PSNR/SSIM scores to discriminative models without degrading perceptual scores.

### Open Question 2
- **Question:** Does the reliance on Stable Diffusion's latent space encoder limit the restoration of high-frequency textural details that are lost during the autoencoding process?
- **Basis in paper:** [inferred] The method operates in the latent space of a pre-trained autoencoder, and latent diffusion models are known to lose high-frequency information during compression.
- **Why unresolved:** The IIDE technique enforces consistency in the latent space, but if the initial encoding discards fine details, the reverse diffusion process might hallucinate textures rather than retrieve original data.
- **What evidence would resolve it:** A comparative analysis of frequency-domain spectra between restored images and ground truths, specifically targeting high-frequency recovery error.

### Open Question 3
- **Question:** To what extent can the model disentangle text-guided semantic edits from the content preservation constraints when the text prompt implies a structural change?
- **Basis in paper:** [inferred] The Introduction identifies "object-level color nuances" as an unsolved task, and the paper demonstrates color control.
- **Why unresolved:** The IIDE self-regularization may overly constrain the model's ability to perform structural edits guided by text.
- **What evidence would resolve it:** Experiments using text prompts that specifically target structural alterations, measuring the failure rate of the semantic edit versus preservation of original structure.

## Limitations
- **Training hyperparameters unspecified:** Critical parameters like learning rate, batch size, and epochs are not provided, limiting reproducibility.
- **Pixel-level fidelity trade-off:** The method shows significant perceptual improvements but exhibits decreased PSNR/SSIM scores compared to some state-of-the-art methods.
- **Scratch mask dependency:** For old photo restoration, the quality of scratch detection directly impacts restoration results, though the extraction method is not detailed.

## Confidence
- **High confidence** in the IIDE mechanism's theoretical validity and the general approach of using self-regularization in latent diffusion space
- **Medium confidence** in the claimed quantitative improvements, pending verification of evaluation protocols and baseline implementations
- **Medium confidence** in the method's generalization to unseen degradation patterns, as the self-consistency assumption may not hold for all real-world degradation types

## Next Checks
1. **Ablation study:** Train with p_iide ∈ {0.0, 0.5, 1.0} on DIV2K synthetic old photos to verify the perceptual-fidelity tradeoff curve and confirm Table I claims about IIDE's impact on detail preservation versus hallucination
2. **Degradation robustness test:** Create controlled degradation combinations (noise+blur, scratches+fading, compression+noise) and compare restoration quality against task-specific baselines to assess generalization to unseen degradation mixtures
3. **ControlNet capacity analysis:** Vary ControlNet depth/width and measure convergence speed plus final quality to determine whether the current architecture is under- or over-parameterized for the restoration task