---
ver: rpa2
title: 'Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific
  Papers'
arxiv_id: '2511.18036'
source_url: https://arxiv.org/abs/2511.18036
tags:
- system
- text
- evaluation
- diagrams
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first large-scale benchmark for automatically
  generating system architecture diagrams from scientific papers, addressing the lack
  of standardized evaluation methods in this domain. The benchmark includes 3,000
  paper-diagram pairs and a three-tiered evaluation framework assessing semantic accuracy,
  layout coherence, and visual quality.
---

# Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers

## Quick Facts
- **arXiv ID**: 2511.18036
- **Source URL**: https://arxiv.org/abs/2511.18036
- **Authors**: Ziyi Guo; Zhou Liu; Wentao Zhang
- **Reference count**: 40
- **Primary result**: Introduces the first large-scale benchmark for generating structured, editable system architecture diagrams from scientific papers, achieving a composite score of 69.0 on a curated test set.

## Executive Summary
This paper addresses the challenge of automatically generating structured, editable system architecture diagrams from scientific papers. The authors propose Paper2SysArch, a multi-agent system that converts papers into a strict 3-level hierarchical graph (Module, Tool/Data, Component) represented as JSON and rendered as PPTX. The system enforces strict topological constraints to prevent illogical connections and uses parallel agent drafting to handle long documents. A comprehensive evaluation framework assesses semantic accuracy, layout coherence, and visual quality across 3,000 paper-diagram pairs, establishing a foundational benchmark for reproducible research in scientific visualization.

## Method Summary
The Paper2SysArch pipeline consists of four phases: (1) Analysis, where an Analyst Agent extracts paper summaries and an Architect Agent builds top-level modules; (2) Drafting, where parallel Designer Agents populate internal details for each module followed by serial refinement; (3) Regularization, where a symbolic pruner removes illegal cross-module edges; and (4) Rendering, where the Eclipse Layout Kernel (ELK) computes coordinates and python-pptx generates the final diagram. The system uses GPT-4o throughout and enforces a strict hierarchical graph structure to ensure logical consistency. Evaluation employs a three-tiered metric combining semantic node consistency (using Sentence-BERT matching), layout penalties for overlaps and crossings, and visual quality assessed by VLM agents.

## Key Results
- The system achieved a composite score of 69.0 on a manually curated test set of 108 papers, with strong performance in visual quality (87.3) and layout (83.9) but notably lower semantic accuracy (29.8).
- The code-based baseline (GPT-4o + GraphViz) outperformed the agent-based approach in semantic accuracy (42.0 vs 29.8), highlighting the challenge of capturing logical relationships.
- The enforced hierarchical constraints successfully prevented structural hallucinations while maintaining editability through vector-based PPTX output.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing strict topological constraints via a hierarchical graph representation significantly reduces structural hallucinations (e.g., illogical connections) common in pixel-based diffusion models.
- Mechanism: The system enforces a "Three-level Hierarchical Graph" (Module, Entity, Component) where directed edges are strictly prohibited from crossing module boundaries to connect internal components directly. A "Hybrid Constraint Enforcement" step uses an LLM to identify errors and a symbolic program to delete illegal edges, decoupling connectivity logic from visual rendering.
- Core assumption: System architecture can be effectively modeled as a tree-like containment hierarchy where data flow strictly follows sibling relationships at specific abstraction levels.
- Evidence anchors:
  - [abstract] "structure-constrained system architecture generation... converts papers into structured, editable diagrams."
  - [section 4.1] "This principle forces cross-module interactions to be represented at a higher level... prevents structural ambiguity by design."
  - [corpus] "SketchAgent: Generating Structured Diagrams..." supports the viability of structured diagram generation over free-form pixel synthesis.
- Break condition: If the input paper describes a non-hierarchical, mesh-like network where every node connects to every other node regardless of module containment, this hierarchical constraint would sever necessary logical connections.

### Mechanism 2
- Claim: Decomposing the generation task into parallel, isolated sub-graph drafting followed by sequential revision mitigates the context window limitations and attention degradation of LLMs when processing full scientific papers.
- Mechanism: The system assigns independent "Designer Agents" to populate internal details (L2/L3) for each high-level module in parallel. A subsequent "Serial Revision" phase aggregates these drafts into a Global Context to align IDs and interfaces, preventing the "lost in the middle" phenomenon where LLMs forget constraints from distant sections of a paper.
- Core assumption: The logical dependencies between modules can be effectively resolved post-hoc (in the revision phase) rather than requiring global awareness during the initial drafting phase.
- Evidence anchors:
  - [abstract] "leverages multi-agent collaboration to convert papers..."
  - [section S1.1.3] "To ensure context isolation and generation efficiency... assigns an independent Designer Agent to each L1 module."
  - [corpus] "Paper2Code: Automating Code Generation..." and "AutoP2C" similarly employ multi-agent frameworks to manage the complexity of translating long-form papers into structured code.
- Break condition: If a paper's modules are deeply recursively dependent (e.g., Module A requires details from Module B to define its own inputs, and vice versa), strict isolation during drafting may create irreconcilable interface mismatches during revision.

### Mechanism 3
- Claim: Separating evaluation into orthogonal layers (Semantic, Layout, Visual) allows for precise diagnosis of model failures, specifically isolating logical misunderstanding from aesthetic rendering artifacts.
- Mechanism: The evaluation framework calculates a "Node Consistency" score using a two-stage matching algorithm (text similarity followed by structural propagation) rather than pixel-wise comparison. This allows the system to penalize logical errors (missing nodes) independently of visual errors (blurry text).
- Core assumption: Semantic fidelity can be approximated by aligning node/edge sets between generated and ground-truth graphs via text and structural similarity.
- Evidence anchors:
  - [abstract] "three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality."
  - [section 3.4.2] "This process encompasses the accurate representation of system components... device a two-stage iterative matching algorithm."
  - [corpus] "Can Multimodal Foundation Models Understand Schematic Diagrams?" (MISS-QA) highlights the difficulty of evaluating diagram interpretation, reinforcing the need for structured benchmarks like this one.
- Break condition: If the generated diagram is semantically correct but uses different terminology or granularities (e.g., splitting one complex node into two simpler ones), the rigid node-matching algorithm may penalize it as a semantic error rather than a valid abstraction.

## Foundational Learning

- Concept: **Hierarchical Graph Modeling**
  - Why needed here: The core data structure of this system is not an image but a strict 3-level hierarchy (Module -> Tool/Data -> Component). Understanding containment vs. connection is prerequisite to defining the JSON schema.
  - Quick check question: Can you distinguish between a "connection" (edge between nodes) and "containment" (node inside a parent module) in a standard CNN architecture diagram?

- Concept: **LLM Context Window & Attention**
  - Why needed here: The paper explicitly uses a multi-agent parallel strategy to handle long documents. Understanding why a single LLM fails on 30-page PDFs (attention dilution) explains the architectural split between "Analyst" (global) and "Designer" (local) agents.
  - Quick check question: Why would asking an LLM to extract details for "Module A" be more accurate if you hide the text for "Module B" compared to showing the whole paper at once?

- Concept: **Vector vs. Raster Rendering**
  - Why needed here: The paper critiques Diffusion models for outputting "flat" pixels. The system outputs PPTX (vector) via `python-pptx`. Understanding editable coordinate systems is necessary for the "Layout and Rendering" phase.
  - Quick check question: If you change the text of a node from "CNN" to "Convolutional Neural Network," why does a vector format (PPTX) adjust the layout automatically while a raster format (PNG) requires regenerating the image?

## Architecture Onboarding

- Component map:
  - Input PDF Paper (Text + Abstract) -> Analysis (Analyst Agent + Architect Agent) -> Drafting (Parallel Designer Agents + Global Context Aggregator + Serial Refiner) -> Regularization (LLM Checker + Symbolic Pruner) -> Rendering (ELK Layout Engine + Icon Generator + python-pptx) -> Evaluation (VLM-based Agents)

- Critical path: The **Semantic Parsing (Phase 1)** to **Serial Revision (Phase 2)** link. If the Architect Agent fails to identify the correct main modules in the paper, the downstream Designer Agents will operate on garbage, and the Revision phase cannot fix fundamental categorization errors.

- Design tradeoffs:
  - **Editability vs. Aesthetics**: The system prioritizes editable PPTX using the ELK algorithm, but the paper admits this results in "inflexibility" and lower aesthetic quality compared to human diagrams or free-form generative models.
  - **Constraint vs. Expressiveness**: The strict "sibling-only" edge constraint guarantees logical layout but may fail to represent complex cross-module data flows visually, potentially oversimplifying the architecture.

- Failure signatures:
  - **Low Semantic Score (29.8)**: The system identifies the visual "look" of a diagram but misses the core logical validity (e.g., connecting the wrong layers).
  - **Hallucinated Components**: The Case Study (Figure 6) shows baseline models generating "nonsensical components" or garbled text; the agent system may still suffer from "invented" nodes if the paper text is ambiguous.
  - **Rigid Layouts**: Over-reliance on the ELK algorithm produces "blocky" or standardised layouts that fail to capture unique diagrammatic styles in the source papers.

- First 3 experiments:
  1. **Module Ablation**: Run the pipeline using a single "Monolithic Agent" vs. the "Parallel Designer Agents" on a 15-page paper to measure the drop in node consistency (validates the scalability claim).
  2. **Constraint Relaxation**: Disable the "Symbolic Pruner" (allow cross-hierarchy edges) and measure the increase in "Line Crossings" detected by the LayoutExamineAgent (validates the structural constraint necessity).
  3. **Visual Ablation**: Compare the "IconExamineAgent" scores when using generic icons vs. text-to-image generated icons to determine if generated visual metaphors actually aid semantic understanding or add noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the semantic accuracy of the Paper2SysArch pipeline be improved to surpass the current code-based baseline given that the agent-based approach scored significantly lower (29.8) than the GPT-4o + GraphViz baseline (42.0)?
- Basis in paper: [explicit] The authors explicitly state in the results that "our method (29.8) also failed to outperform the code-based baseline (42.0) in this regard" and that "accurately capturing and reproducing the core logical structure... remains a key challenge."
- Why unresolved: The current multi-agent decomposition strategy focuses on parallel drafting of modules but appears to lose logical fidelity compared to direct code generation methods.
- What evidence would resolve it: A modified version of Paper2SysArch achieving a semantic score greater than 42.0 on the benchmark test set.

### Open Question 2
- Question: Can learned layout optimization techniques replace the rigid Eclipse Layout Kernel (ELK) RectPacking algorithm to resolve the layout inflexibility identified in the conclusion?
- Basis in paper: [explicit] The authors conclude that "our agentâ€™s reliance on pre-defined open-source algorithms for layout generation can lead to inflexibility."
- Why unresolved: The current implementation uses a fixed algorithm which constrains the arrangement of geometric coordinates, potentially limiting the natural arrangement of complex hierarchies.
- What evidence would resolve it: A comparative study showing that a deep-learning-based layout model produces higher user preference ratings or layout scores than the current ELK implementation.

### Open Question 3
- Question: What specific visual refinement techniques are required to close the "considerable gap" in aesthetic quality between agent-generated diagrams and human-authored ones?
- Basis in paper: [explicit] The conclusion notes that "a considerable gap in aesthetic quality remains when compared to human-authored diagrams" and identifies this as a key avenue for future research.
- Why unresolved: While the system scores well on objective metrics like text legibility and icon relevance, it lacks the subjective design intuition required to match professional human standards.
- What evidence would resolve it: A human evaluation study where participants cannot statistically distinguish between generated diagrams and ground-truth diagrams based on aesthetic appeal alone.

## Limitations

- The system achieved a notably low semantic score (29.8/100), indicating significant challenges in accurately capturing logical relationships between system components.
- The enforced hierarchical structure and ELK-based layout algorithm produce standardized diagrams that lack the creative flexibility of human-generated architectures.
- The node matching algorithm for semantic evaluation relies on specific similarity weights and thresholds that are not fully specified, potentially affecting reproducibility.

## Confidence

- **High Confidence**: The structural constraints effectively prevent logical connection errors (hallucinations). The multi-agent parallel drafting approach demonstrably scales to long documents by avoiding context window limitations.
- **Medium Confidence**: The evaluation framework's orthogonal separation (Semantic/Layout/Visual) provides useful diagnostic granularity. The claim that pixel-based diffusion models produce "flat" outputs requiring vector rendering is well-supported.
- **Low Confidence**: The absolute semantic accuracy scores may be overly conservative due to the strict node-matching algorithm's sensitivity to terminology variations and granularities.

## Next Checks

1. **Semantic Robustness Test**: Evaluate the system on papers with intentionally ambiguous or sparse system descriptions to measure its tolerance for incomplete information and propensity for hallucination.
2. **Cross-Architecture Generalization**: Test the system on non-AI domains (e.g., mechanical engineering or biology) to assess whether the hierarchical constraint assumption holds for fundamentally different system types.
3. **Human Preference Validation**: Conduct a user study comparing human-rated diagram quality between the system's output and free-form generative models to quantify the claimed trade-off between editability and aesthetic quality.