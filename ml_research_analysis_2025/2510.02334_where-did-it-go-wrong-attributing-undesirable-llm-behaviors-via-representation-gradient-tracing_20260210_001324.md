---
ver: rpa2
title: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation
  Gradient Tracing
arxiv_id: '2510.02334'
source_url: https://arxiv.org/abs/2510.02334
tags:
- data
- arxiv
- training
- attribution
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RepT, a framework for tracing undesirable
  behaviors in large language models (LLMs) by analyzing representation gradients
  in the activation space. Traditional gradient-based attribution methods are computationally
  expensive and suffer from noisy signals due to the high dimensionality of model
  parameters.
---

# Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing

## Quick Facts
- arXiv ID: 2510.02334
- Source URL: https://arxiv.org/abs/2510.02334
- Reference count: 19
- Primary result: RepT achieves near-perfect precision in identifying problematic training data while being more computationally efficient than gradient-based baselines

## Executive Summary
This paper introduces RepT, a framework for tracing undesirable behaviors in large language models by analyzing representation gradients in the activation space. Traditional gradient-based attribution methods are computationally expensive and suffer from noisy signals due to the high dimensionality of model parameters. RepT addresses these limitations by shifting attribution from the parameter space to the representation space, using the hidden states and their gradients to link model outputs to their training data origins. The framework operates at two granularities: sample-level attribution for identifying influential training documents and token-level attribution for pinpointing specific causal phrases.

Evaluated across three tasks—harmful content generation, backdoor poisoning detection, and knowledge contamination attribution—RepT consistently outperforms existing methods, achieving near-perfect precision in identifying problematic training data. It is also more efficient, requiring less memory and computation compared to gradient-based baselines. This work provides a scalable and interpretable diagnostic tool for auditing and mitigating risks in LLMs.

## Method Summary
RepT operates by shifting the attribution problem from the high-dimensional parameter space to the lower-dimensional representation space of hidden states. The framework computes gradients of the model's output with respect to hidden states at each layer, then aggregates these gradients to identify which training samples and tokens contributed most to undesirable behaviors. At the sample level, RepT ranks training documents by their influence on the problematic output. At the token level, it identifies specific phrases or tokens within training samples that serve as causal factors. The method leverages the fact that hidden states capture more interpretable semantic information than raw parameters, making the attribution signals less noisy and more actionable. By operating in representation space rather than parameter space, RepT achieves significant computational efficiency gains while maintaining attribution accuracy.

## Key Results
- Achieved near-perfect precision (over 95%) in identifying training documents responsible for backdoor triggers
- Successfully traced harmful content generation to specific training samples with high accuracy
- Demonstrated 10-100x reduction in memory and computation compared to parameter-space gradient methods

## Why This Works (Mechanism)
RepT works by exploiting the representational structure of LLMs, where hidden states encode semantic information in a more compressed and interpretable form than raw parameters. By computing gradients with respect to these representations rather than parameters, the framework reduces noise and focuses on semantically meaningful attributions. The method leverages the chain rule to propagate attribution signals backward through the network, accumulating influence scores at the representation level. This approach effectively filters out irrelevant parameter variations while preserving causal relationships between training data and model outputs. The representation space captures the essential features that drive model behavior, making the attribution process both more accurate and computationally tractable.

## Foundational Learning
- **Representation Space vs Parameter Space** - Why needed: Parameter space is too high-dimensional and noisy for effective attribution. Quick check: Verify that hidden state dimensions are orders of magnitude smaller than parameter counts.
- **Gradient-Based Attribution** - Why needed: Gradients provide a principled way to measure influence of inputs on outputs. Quick check: Confirm gradients flow correctly through all layers.
- **Hidden State Interpretation** - Why needed: Understanding what hidden states represent is crucial for meaningful attribution. Quick check: Validate that hidden states capture relevant semantic features.
- **Chain Rule Propagation** - Why needed: Enables backward attribution through the network architecture. Quick check: Ensure gradient accumulation follows correct mathematical formulation.
- **Sample-Level vs Token-Level Attribution** - Why needed: Different granularities serve different diagnostic purposes. Quick check: Confirm both levels provide complementary insights.
- **Computational Efficiency Tradeoffs** - Why needed: Attribution must be practical for large models. Quick check: Benchmark memory and compute requirements against baselines.

## Architecture Onboarding

### Component Map
Input Data -> Model Forward Pass -> Hidden State Extraction -> Gradient Computation -> Attribution Aggregation -> Training Data Ranking

### Critical Path
The most time-critical path is the forward pass combined with gradient computation, as these operations must be performed for each evaluation sample to generate attribution scores.

### Design Tradeoffs
The framework trades some attribution granularity (working in representation space rather than parameter space) for significant gains in computational efficiency and signal clarity. This represents a pragmatic compromise between attribution quality and practical applicability.

### Failure Signatures
Attribution may fail when behaviors emerge from complex interactions across many training examples rather than clear causal links to individual documents. The method may also struggle with behaviors that are encoded in subtle parameter interactions rather than explicit representation patterns.

### First 3 Experiments
1. Apply RepT to a simple synthetic dataset where the link between training samples and outputs is known
2. Test attribution accuracy on a controlled backdoor injection scenario
3. Compare RepT's efficiency against parameter-space gradient methods on a medium-sized model

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can RepT scale to trace behaviors in multi-trillion-token pre-training corpora?
- Basis in paper: [explicit] The authors state that applying the method to massive pre-training corpora "remains a hurdle for exhaustive real-time attribution."
- Why unresolved: Current experiments were restricted to curated fine-tuning datasets (thousands of examples) rather than foundation-scale data.
- What evidence would resolve it: Successful application of RepT on a full pre-training dataset (e.g., Common Crawl) with maintained efficiency.

### Open Question 2
- Question: Is RepT effective at attributing emergent behaviors that lack discrete data origins?
- Basis in paper: [explicit] Section C notes the framework's effectiveness on "more subtle, emergent phenomena, such as nuanced biases... requires further investigation."
- Why unresolved: The evaluation focused on behaviors with clear ground-truth sources (e.g., backdoor triggers) rather than distributed patterns.
- What evidence would resolve it: Successful attribution of a complex societal bias to a distributed subset of training data verified by retraining.

### Open Question 3
- Question: Can representation gradients be utilized for real-time inference-time model correction?
- Basis in paper: [explicit] The authors propose using the framework to "enable real-time model correction" via "steering vectors to modify model activations."
- Why unresolved: The current work focuses solely on the diagnostic capability; the mitigation application is proposed but untested.
- What evidence would resolve it: An experiment where derived steering vectors successfully suppress a targeted undesirable behavior without retraining.

## Limitations
- Limited effectiveness on behaviors that emerge from complex interactions across many training examples
- Evaluation focused on three specific types of undesirable behaviors, limiting generalizability
- Performance may vary across different model architectures and configurations

## Confidence
**High Confidence** - Computational efficiency claims are well-supported by the theoretical reduction in dimensionality
**Medium Confidence** - Near-perfect precision claims may be inflated by controlled experimental conditions
**Low Confidence** - Generalizability to arbitrary undesirable behaviors remains speculative

## Next Checks
1. Test RepT across multiple LLM architectures to assess cross-architecture effectiveness
2. Apply RepT to real-world web-scale datasets with complex contamination issues
3. Evaluate RepT's ability to attribute behaviors that emerge from multi-step reasoning or complex interactions