---
ver: rpa2
title: 'DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization'
arxiv_id: '2507.12933'
source_url: https://arxiv.org/abs/2507.12933
tags:
- quantization
- diffusion
- scaling
- arxiv
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quantizing diffusion models
  for efficient deployment by tackling outliers in activation distributions. The authors
  propose DMQ, a post-training quantization framework combining Learned Equivalent
  Scaling (LES) with Adaptive Timestep Weighting and channel-wise Power-of-Two Scaling
  (PTS).
---

# DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization

## Quick Facts
- arXiv ID: 2507.12933
- Source URL: https://arxiv.org/abs/2507.12933
- Authors: Dongyeun Lee; Jiwan Hur; Hyounguk Shon; Jae Young Lee; Junmo Kim
- Reference count: 40
- This paper proposes DMQ, a post-training quantization framework that addresses activation outliers in diffusion models through Learned Equivalent Scaling, Adaptive Timestep Weighting, and Power-of-Two Scaling, achieving state-of-the-art FID/sFID scores at low bit-widths.

## Executive Summary
This paper addresses the challenge of quantizing diffusion models for efficient deployment by tackling outliers in activation distributions. The authors propose DMQ, a post-training quantization framework combining Learned Equivalent Scaling (LES) with Adaptive Timestep Weighting and channel-wise Power-of-Two Scaling (PTS). LES learns channel-wise scaling factors to redistribute quantization difficulty between weights and activations, while PTS applies power-of-two scaling to handle extreme outliers in layers like skip connections. An adaptive timestep weighting scheme prioritizes critical early denoising steps that significantly impact final output quality. Extensive experiments demonstrate DMQ's superiority, particularly at low bit-widths (W4A6, W4A8), achieving state-of-the-art FID and sFID scores across multiple datasets including FFHQ, LSUN-Bedroom/Church, ImageNet, and MS-COCO.

## Method Summary
DMQ is a post-training quantization framework for diffusion models that addresses activation outliers through three key components. First, Learned Equivalent Scaling (LES) introduces channel-wise scaling factors τ to redistribute quantization difficulty between weights and activations, maintaining mathematical equivalence while compacting activation ranges. Second, Adaptive Timestep Weighting prioritizes early denoising steps using a focal-loss-inspired approach where steps with lower accumulated quantization error receive higher weights during optimization. Third, Power-of-Two Scaling (PTS) applies bit-shifting to handle extreme outliers in specific layers like skip connections, with a voting algorithm ensuring reliable factor selection even with small calibration sets. The method is validated across multiple diffusion model architectures and datasets, demonstrating significant improvements in FID and sFID scores at low bit-widths.

## Key Results
- DMQ achieves state-of-the-art FID scores at W4A6 and W4A8 bit-widths across FFHQ, LSUN-Bedroom/Church, ImageNet, and MS-COCO datasets
- Learned Equivalent Scaling (LES) reduces weight quantization error from 0.0694 to 0.0058 compared to SmoothQuant while improving FID from 36.08 to 30.37
- Adaptive timestep weighting with α=20-25 improves FID from 33.46 (uniform) to 31.83 by prioritizing early denoising steps
- PTS voting algorithm with κ=0.85 reliably selects scaling factors for skip connections, achieving FID of 30.37 vs 32.55 with MSE-based selection

## Why This Works (Mechanism)

### Mechanism 1: Learned Equivalent Scaling (LES)
Channel-wise scaling factors can redistribute quantization difficulty between weights and activations, reducing overall error. Introduces τ such that Y = (X/τ)(τ^T ⊙ W). Scaling down outlier activation channels while inversely scaling corresponding weights maintains mathematical equivalence while compacting activation ranges. τ is learned by minimizing ||XW - Q(X̂)Q(Ŵ)||² per layer.

### Mechanism 2: Adaptive Timestep Weighting
Early denoising steps disproportionately impact final output quality despite lower quantization error magnitude. Focal-loss-inspired weighting: λ_t = (1 - Λ_t/ΣΛ)^α where Λ_t is running accumulated loss. Lower accumulated loss → higher weight. Prioritizes early steps where small errors compound through subsequent denoising.

### Mechanism 3: Power-of-Two Scaling (PTS) with Voting Algorithm
Power-of-two channel-wise scaling handles extreme outliers with near-zero hardware overhead via bit-shifting. For each channel k, select δ_k from {2^0, 2^1, ..., 2^D} via voting: compute optimal δ per calibration sample, take mode, require agreement ratio r_k > κ. Apply as bit-shift on weights: W̃_shifted = W̃ << δ_k.

## Foundational Learning

- **Uniform Quantization with Scale Factors**: Core to understanding how LES fuses τ into scales: Q(X) = s · clamp(⌊X/s⌉, l, u). Why needed: Explains mathematical equivalence in LES. Quick check: Why can't per-element quantization use efficient integer GEMM?

- **Diffusion Denoising Dynamics**: Explains why early-step errors compound; denoising iteratively refines noise toward data distribution. Why needed: Understanding error propagation across timesteps. Quick check: At timestep t=800 vs t=50, which has higher activation variance but potentially more impact on final quality?

- **Equivalent Scaling Mathematics**: Y = XW = (X/τ)(τ^T ⊙ W); τ can fuse into weights or activation scales. Why needed: Mathematical foundation for LES. Quick check: If τ=2 for channel k, how must weights in column k adjust to preserve Y?

## Architecture Onboarding

- **Component map**: Calibration Data Generation -> LES Factor Learning -> Adaptive Weighting Optimization -> PTS Voting -> Model Deployment

- **Critical path**:
  1. Calibration data generation: 256 samples × T steps (e.g., 5120 for 20-step DDIM)
  2. Learn τ per layer (4000-6000 iterations, batch 32)
  3. Compute Λ_t during training; update λ_t dynamically
  4. Run voting algorithm for PTS on skip-connection layers
  5. Deploy: fuse τ into scales/weights; custom kernel for PTS bit-shift

- **Design tradeoffs**:
  - α (20-25): Higher = more early-step emphasis; risk of missing layer-specific patterns
  - κ (0.85): Higher = more conservative; fewer channels scaled but safer
  - D (3): Range of power-of-two candidates; larger = more flexibility but more computation
  - Calibration size vs generalization: Voting designed for N≈5000-10000

- **Failure signatures**:
  - FID suddenly > 100 at W4A6 → outliers not handled; check PTS coverage
  - Weight quantization error > 0.05 → τ factors too large; check LES learning
  - Performance varies with different seeds → voting κ too low; increase to 0.9
  - Slow inference → bit-shift not fused; verify kernel implementation

- **First 3 experiments**:
  1. Baseline LES only: Apply MinMax quantization, then add LES; expect FID improvement ~36→33 on FFHQ W4A8
  2. Timestep weighting ablation: Compare uniform/linear/quadratic vs adaptive; adaptive should achieve best FID
  3. PTS voting validation: Compare MSE-based selection vs voting with small calibration (N=1000); voting should show better generalization on held-out samples

## Open Questions the Paper Calls Out
- Can the DMQ framework be effectively extended to extreme low-bit widths (e.g., 2-bit or 3-bit representations) by prioritizing salient features using mixed-precision weights?
- How does DMQ perform when applied to downstream tasks requiring efficient fine-tuning, such as personalized text-to-image generation?
- Can the quantization accuracy of DMQ be further improved by integrating orthogonal noise correction techniques or timestep-specific parameters?

## Limitations
- The method's effectiveness depends heavily on the quality and representativeness of the calibration set for learning LES factors and PTS scaling
- Computational overhead from PTS bit-shifting requires careful implementation to ensure efficiency gains are not negated
- Limited analysis of performance on models with different architectures, such as those using attention mechanisms or recurrent layers

## Confidence
- **High Confidence**: The core mechanism of LES for redistributing quantization difficulty is well-supported by the mathematical framework and empirical evidence
- **Medium Confidence**: The adaptive timestep weighting scheme is logically sound but has limited corpus support
- **Medium Confidence**: The PTS voting algorithm is practical but reliability depends on the choice of agreement threshold

## Next Checks
1. **Generalization Test**: Apply DMQ to a diffusion model trained on a dataset with significantly different characteristics (e.g., medical imaging) to assess robustness of learned LES factors
2. **Ablation on κ Threshold**: Systematically vary the PTS voting threshold (κ) from 0.7 to 0.95 and evaluate trade-off between outlier handling and quantization error
3. **Architectural Stress Test**: Apply DMQ to a diffusion model incorporating attention mechanisms and analyze performance on layers with different activation distributions