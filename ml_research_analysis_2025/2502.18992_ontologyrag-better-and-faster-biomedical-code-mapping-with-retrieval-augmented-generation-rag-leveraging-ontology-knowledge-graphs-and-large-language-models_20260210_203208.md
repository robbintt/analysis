---
ver: rpa2
title: 'OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented
  Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models'
arxiv_id: '2502.18992'
source_url: https://arxiv.org/abs/2502.18992
tags:
- mapping
- code
- level
- ontology
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OntologyRAG, a retrieval-augmented generation
  (RAG) pipeline that leverages ontology knowledge graphs and large language models
  (LLMs) to improve biomedical code mapping. The method addresses the challenge of
  manual refinement in code mapping by automatically indexing ontology data into knowledge
  graphs, retrieving relevant subgraphs using natural language queries, and providing
  interpretable mapping proximity assessments with reasoning.
---

# OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models

## Quick Facts
- arXiv ID: 2502.18992
- Source URL: https://arxiv.org/abs/2502.18992
- Reference count: 0
- Achieves 87.13% accuracy for mapping level predictions using GPT-4 with chain-of-thought prompting

## Executive Summary
OntologyRAG presents a retrieval-augmented generation pipeline that combines ontology knowledge graphs with large language models to improve biomedical code mapping efficiency. The system automatically indexes ontology data into knowledge graphs, retrieves relevant subgraphs using natural language queries, and provides interpretable mapping proximity assessments with reasoning. Evaluation on self-curated gold datasets demonstrates that coding experts can achieve better and faster results by focusing on ambiguous cases rather than all mappings. The approach offers a scalable solution that eliminates the need for frequent LLM retraining while providing interpretable results for domain experts.

## Method Summary
OntologyRAG leverages retrieval-augmented generation (RAG) by indexing ontology data into knowledge graphs and using natural language queries to retrieve relevant subgraphs. The system combines this retrieval mechanism with large language models, specifically using GPT-4 with chain-of-thought prompting to achieve 87.13% accuracy in mapping level predictions. The pipeline provides interpretable results through mapping proximity assessments and reasoning, allowing domain experts to focus their effort on ambiguous cases rather than all mappings. The approach is designed to be scalable and eliminates the need for frequent LLM retraining.

## Key Results
- GPT-4 achieves 87.13% accuracy for mapping level predictions using chain-of-thought prompting
- Open-source models like Meta-Llama-3-8B show competitive performance, though specific quantitative comparisons are not provided
- Coding experts can achieve better and faster code mapping by focusing on ambiguous cases rather than all mappings

## Why This Works (Mechanism)
The system works by leveraging the structured nature of ontology knowledge graphs to provide context-rich information retrieval that complements the reasoning capabilities of large language models. By retrieving relevant subgraphs based on natural language queries, the system can provide domain-specific context that improves the accuracy of code mapping predictions. The interpretability through proximity assessments and reasoning allows domain experts to understand and validate the mapping decisions, creating a collaborative human-AI workflow that enhances both speed and quality.

## Foundational Learning
- Ontology knowledge graphs: Why needed - provide structured representation of biomedical concepts and relationships; Quick check - verify graph connectivity and completeness
- Retrieval-augmented generation: Why needed - combines information retrieval with language model reasoning; Quick check - measure retrieval precision and recall
- Chain-of-thought prompting: Why needed - improves reasoning quality by breaking down complex tasks; Quick check - compare with standard prompting on validation set
- Biomedical code mapping: Why needed - translates clinical documentation into standardized codes; Quick check - measure inter-annotator agreement on gold dataset
- Knowledge graph indexing: Why needed - enables efficient subgraph retrieval; Quick check - benchmark query response times
- Interpretability mechanisms: Why needed - builds trust and enables expert validation; Quick check - conduct user studies on explanation clarity

## Architecture Onboarding

Component map: Ontology data -> Knowledge graph indexing -> Natural language query processing -> Subgraph retrieval -> LLM reasoning -> Mapping proximity assessment -> Expert review

Critical path: The critical path involves processing natural language queries to retrieve relevant subgraphs, which are then fed to the LLM for reasoning and mapping decisions. The system's performance depends on the quality of subgraph retrieval and the LLM's ability to reason with the retrieved context.

Design tradeoffs: The system trades off between retrieval precision and coverage, balancing the depth of subgraph retrieval against computational efficiency. Using open-source models versus proprietary models involves tradeoffs between cost, performance, and customization flexibility. The interpretability mechanisms add computational overhead but provide essential validation capabilities for domain experts.

Failure signatures: Poor subgraph retrieval leads to incomplete context for the LLM, resulting in inaccurate mappings. Ambiguous queries may retrieve multiple relevant subgraphs, causing confusion in the mapping process. The system may struggle with novel concepts not well-represented in the ontology knowledge graph.

First experiments:
1. Test subgraph retrieval accuracy on a small ontology subset with known query-answer pairs
2. Evaluate LLM performance on code mapping tasks with and without retrieved subgraphs
3. Measure expert review time and accuracy improvements when using OntologyRAG versus traditional methods

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Relies on self-curated gold datasets rather than established benchmark standards, limiting external validation
- Lacks confidence intervals or statistical significance testing for the reported 87.13% accuracy
- Does not address potential biases in the retrieval mechanism or handling of conflicting information across different ontologies

## Confidence
High: The core concept of using RAG with ontology knowledge graphs for biomedical code mapping is technically sound and addresses a real-world challenge.
Medium: The claimed improvements in speed and quality of code mapping by domain experts, based on the authors' evaluation methodology without independent verification.
Low: The assertion that the approach eliminates retraining needs, as this depends on ontology stability and system adaptability to novel concepts over time.

## Next Checks
1. Conduct independent replication studies using established benchmark datasets for biomedical code mapping to verify reported accuracy and performance claims.
2. Perform longitudinal testing to evaluate the system's performance as ontologies evolve and new concepts emerge, specifically testing the retraining elimination claim.
3. Design user studies with coding experts to measure actual time savings and quality improvements in real-world workflows, comparing traditional methods against OntologyRAG implementation.