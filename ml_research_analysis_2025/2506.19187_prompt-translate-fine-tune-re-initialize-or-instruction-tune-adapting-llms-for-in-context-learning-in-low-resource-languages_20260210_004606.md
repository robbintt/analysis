---
ver: rpa2
title: Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting
  LLMs for In-Context Learning in Low-Resource Languages
arxiv_id: '2506.19187'
source_url: https://arxiv.org/abs/2506.19187
tags:
- language
- outputs
- languages
- adaptation
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates five adaptation methods for cross-lingual
  transfer to low-resource languages in in-context learning: few-shot prompting, translate-test,
  language-adaptive fine-tuning (LAFT), FOCUS (embedding re-initialization + LAFT),
  and language-adaptive instruction tuning (LAIT). Across three base models, five
  languages, and seven tasks, prompting-based methods consistently outperformed gradient-based
  methods.'
---

# Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages

## Quick Facts
- **arXiv ID**: 2506.19187
- **Source URL**: https://arxiv.org/abs/2506.19187
- **Reference count**: 33
- **Key outcome**: Few-shot prompting and translate-test outperform gradient-based methods (LAFT, FOCUS, LAIT) across models, languages, and tasks; trained models suffer catastrophic forgetting of both linguistic ability and task alignment

## Executive Summary
This study systematically compares five adaptation methods for cross-lingual in-context learning in low-resource languages: few-shot prompting, translate-test, language-adaptive fine-tuning (LAFT), FOCUS (embedding re-initialization + LAFT), and language-adaptive instruction tuning (LAIT). Across three base models, five languages, and seven tasks, prompting-based methods consistently outperformed gradient-based methods. The novel Valid Output Recall (VOR) metric revealed that gradient-based approaches suffer from catastrophic forgetting, losing both linguistic ability and task alignment. This suggests that while prompting and translate-test maintain robust cross-lingual transfer, fine-tuning degrades model performance post-training.

## Method Summary
The paper evaluates five adaptation strategies on cross-lingual in-context learning: (1) few-shot prompting with hand-crafted examples in the target language, (2) translate-test where inputs are translated to English before prompting, (3) language-adaptive fine-tuning (LAFT) with language-specific data, (4) FOCUS which re-initializes embeddings before LAFT, and (5) language-adaptive instruction tuning (LAIT) on instruction datasets. The evaluation spans three base models (Flan-T5, GPT-2, OPT), five low-resource languages (Catalan, Estonian, Finnish, Latvian, Romanian), and seven tasks covering generation, summarization, and classification. A novel Valid Output Recall (VOR) metric was introduced to measure both task alignment and linguistic capability.

## Key Results
- Few-shot prompting and translate-test consistently outperformed gradient-based methods (LAFT, FOCUS, LAIT) across all evaluated models, languages, and tasks
- Gradient-based methods exhibited catastrophic forgetting, losing both linguistic ability and task alignment as measured by the novel VOR metric
- FOCUS (embedding re-initialization + LAFT) did not prevent the performance degradation observed in standard LAFT

## Why This Works (Mechanism)
Assumption: The superior performance of prompting-based methods likely stems from preserving the original model's pre-trained cross-lingual capabilities while gradient-based methods overwrite these representations during fine-tuning, leading to catastrophic forgetting of both linguistic features and task-specific alignment patterns.

## Foundational Learning
Unknown: The paper does not explicitly discuss how pre-training objectives or data distribution affects the observed catastrophic forgetting in gradient-based methods versus the stability of prompting-based approaches.

## Architecture Onboarding
Unknown: The paper does not detail specific architectural considerations for adapting models to low-resource languages beyond the fine-tuning strategies examined.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's evaluation scope is relatively narrow, covering only five languages and seven tasks, which may not capture the full diversity of low-resource scenarios
- The novel VOR metric, while innovative, has not been externally validated against established cross-lingual benchmarks
- The catastrophic forgetting phenomenon observed may be task-specific and not generalize across all types of cross-lingual tasks
- The paper does not explore alternative fine-tuning strategies (such as gradual unfreezing or regularization techniques) that might mitigate catastrophic forgetting

## Confidence
- **High confidence**: Prompting-based methods consistently outperforming gradient-based approaches across the evaluated settings
- **Medium confidence**: Catastrophic forgetting conclusion, pending independent validation of the VOR metric
- **Low confidence**: Generalizability claims about gradient-based methods being unsuitable for cross-lingual transfer across all low-resource scenarios

## Next Checks
1. Validate the VOR metric against established cross-lingual benchmarks like XNLI or PAWS-X to confirm its sensitivity to linguistic degradation and task misalignment
2. Conduct ablation studies testing whether alternative fine-tuning strategies (gradual unfreezing, regularization techniques, or different learning rate schedules) can reduce catastrophic forgetting in gradient-based methods
3. Expand experimental scope to include additional language families (e.g., non-Indo-European languages) and task types (particularly generation-heavy tasks) to assess generalizability of prompting advantage across diverse low-resource scenarios
4. Investigate whether pre-training data composition or model architecture influences susceptibility to catastrophic forgetting in cross-lingual fine-tuning scenarios