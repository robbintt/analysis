---
ver: rpa2
title: Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases
arxiv_id: '2508.15796'
source_url: https://arxiv.org/abs/2508.15796
tags:
- islamic
- prompt
- inheritance
- reasoning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks state-of-the-art LLMs on Islamic inheritance
  reasoning using the QIAS 2025 dataset. Base and fine-tuned models were evaluated
  for accuracy in identifying heirs and computing shares in Arabic inheritance scenarios.
---

# Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases

## Quick Facts
- arXiv ID: 2508.15796
- Source URL: https://arxiv.org/abs/2508.15796
- Reference count: 9
- Primary result: Proprietary models (GPT-o3: 92.3%, Gemini Flash 2.5: 91.5%) significantly outperform open-source Arabic models (24-39%) on Islamic inheritance reasoning task

## Executive Summary
This study benchmarks state-of-the-art LLMs on Islamic inheritance reasoning using the QIAS 2025 dataset. Base and fine-tuned models were evaluated for accuracy in identifying heirs and computing shares in Arabic inheritance scenarios. While open-source Arabic models showed limited performance, proprietary models like GPT-o3 and Gemini Flash 2.5 achieved up to 92.3% and 91.5% accuracy respectively. Fine-tuning improved some models but degraded others, likely due to data and adapter size mismatches. A majority voting ensemble of three top base models achieved 92.7% accuracy, placing third in the challenge.

## Method Summary
The study evaluates base LLMs and fine-tuned variants on Islamic inheritance scenarios from the QIAS 2025 dataset (9,446 train / 1,000 val / 1,000 test examples). Models are assessed using two prompt templates: a minimal prompt and a Chain-of-Thought style prompt. Open-source Arabic models (Falcon3, Fanar, Allam) are compared against proprietary models (GPT-o3, Gemini variants). Fine-tuning experiments use LoRA adapters for Llama 4 and OpenAI/Vertex AI fine-tuning for GPT-4o and Gemini Flash 2.5. A majority voting ensemble combines predictions from GPT-o3, Gemini Flash 2.5, and Gemini Pro 2.5. All evaluations use temperature=0, top_p=1.

## Key Results
- Proprietary models achieved 88-92% accuracy, while open-source Arabic models scored only 24-39%
- Fine-tuning GPT-4o improved accuracy from 70.1% to 84.6%, but degraded Gemini Flash 2.5 from 91.5% to 74.6%
- Majority voting ensemble of three top base models achieved 92.7% accuracy (third place)
- Prompt sensitivity varied dramatically: GPT-4o showed 12.6% improvement with CoT prompting, while Gemini Flash 2.5 improved only 0.8%

## Why This Works (Mechanism)

### Mechanism 1: Proprietary Model Reasoning Advantage
- Claim: State-of-the-art proprietary LLMs substantially outperform open-source Arabic LLMs on complex legal reasoning tasks.
- Mechanism: Larger pre-training corpora and advanced reasoning optimization (e.g., chain-of-thought training) enable proprietary models to handle multi-step legal inference that requires identifying heirs, applying rules, and computing fractional shares.
- Core assumption: The performance gap reflects underlying model capability differences rather than merely prompt engineering or evaluation artifacts.
- Evidence anchors:
  - [abstract]: "open-source Arabic models showed limited performance, proprietary models like GPT-o3 and Gemini Flash 2.5 achieved up to 92.3% and 91.5% accuracy respectively"
  - [section]: Table 1 shows open-source models (Falcon3: 24.2%, Fanar: 31.7%, Allam: 38.8%) vs. proprietary (GPT-o3: 92.3%)
  - [corpus]: Related QIAS 2025 papers report similar patterns with Arabic models; no contradictory evidence found.
- Break condition: If evaluation data overlaps with proprietary model pre-training data, accuracy inflation may occur. Also, single-run results without variance reporting (noted as limitation by authors).

### Mechanism 2: Asymmetric Fine-Tuning Response
- Claim: Fine-tuning improves generalist models but degrades reasoning-optimized models when training data lacks reasoning chains.
- Mechanism: Generalist models (GPT-4o) benefit from domain knowledge injection. Reasoning-optimized models (Gemini Flash 2.5) lose their chain-of-thought structure when fine-tuned on label-only data, causing reasoning-data misalignment.
- Core assumption: The degradation stems from data structure mismatch rather than hyperparameter issues alone.
- Evidence anchors:
  - [section]: "fine-tuning GPT-4o...accuracy improved significantly, reaching over 84%...Gemini Flash 2.5 dropped after fine-tuning (91.5% → 74.6%)"
  - [section]: "If the fine-tuning dataset has only final labels and lacks detailed reasoning chains, the model may lose its reasoning structure"
  - [corpus]: QU-NLP at QIAS 2025 used LoRA fine-tuning on Fanar-1-9B with similar challenges noted; corpus evidence on reasoning-chain alignment is limited.
- Break condition: Adapter size experiments were incomplete (adapter size 1 vs. 8 tested, but authors note need for further validation). Dataset size (7,000 examples) may be insufficient for larger adapters.

### Mechanism 3: Ensemble Diversity Bonus
- Claim: Majority voting across top-performing base models yields higher accuracy than any single model.
- Mechanism: Different models make different errors; when top models disagree, the majority vote filters idiosyncratic failures, assuming uncorrelated error patterns.
- Core assumption: Error patterns across GPT-o3, Gemini Flash 2.5, and Gemini Pro 2.5 are sufficiently independent.
- Evidence anchors:
  - [abstract]: "majority voting solution, leveraging three base models...achieves up to 92.7% accuracy"
  - [section]: Individual test accuracies (88.4%, 88.1%, 87.9%) combine to 92.7% via majority voting
  - [corpus]: No corpus papers report contradictory ensemble results for this task; ensemble benefits are commonly observed but task-specific validation is limited.
- Break condition: If models share systematic biases (e.g., common training data, similar failure modes on edge cases), ensemble gains diminish.

## Foundational Learning

- Concept: **Islamic Inheritance Law (Ilm al-Mawārīth)**
  - Why needed here: The task requires identifying rightful heirs and computing fractional shares under Qur'anic rules; understanding the domain prevents misinterpreting model outputs.
  - Quick check question: Can you explain why inheritance calculation is considered "structured, rule-based, and context-sensitive"?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Prompt 2 (CoT-style) significantly improved GPT-4o accuracy (57.5% → 70.1%); understanding CoT helps explain prompt sensitivity differences.
  - Quick check question: Why might a reasoning-optimized model perform well without explicit CoT prompts while a generalist model benefits more from them?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Fine-tuning experiments used LoRA with varying adapter sizes; understanding this helps diagnose why adapter size 8 degraded performance for Gemini Flash 2.
  - Quick check question: What is the tradeoff between larger adapter size (more trainable parameters) and dataset size?

## Architecture Onboarding

- Component map:
  Input Layer: Arabic inheritance scenarios (text) → prompt template (Prompt 1: minimal; Prompt 2: CoT-style)
  Model Layer: Base LLMs (open-source Arabic: Falcon3, Fanar, Allam; proprietary: GPT-o3, Gemini variants) OR fine-tuned variants (LoRA adapters)
  Ensemble Layer: Majority voting across top-3 base model predictions
  Output Layer: Multiple-choice answer (A-F) + optional reasoning justification

- Critical path:
  1. Select model tier (open-source Arabic vs. proprietary API) based on accuracy requirements
  2. If fine-tuning: verify dataset includes reasoning chains for CoT-optimized models
  3. If fine-tuning: match adapter size to dataset scale (smaller adapter for limited data)
  4. For highest accuracy: deploy ensemble of 3+ top base models with majority voting

- Design tradeoffs:
  - Proprietary vs. Open-source: 2.4-3.8x accuracy improvement (proprietary) vs. cost/control (open-source)
  - Fine-tuning: Can improve generalist models (+16.5% for GPT-4o) but risks degrading reasoning-optimized models (-17% for Gemini Flash 2.5)
  - Ensemble vs. Single model: +4-5% accuracy gain but 3x inference cost and latency
  - Prompt complexity: CoT prompts help some models (GPT-4o: +12.6%) but show minimal effect on others (Gemini Flash 2.5: +0.8%)

- Failure signatures:
  - Open-source Arabic models: Low accuracy across all prompt configurations (22-39%) indicates domain/reasoning gap
  - Fine-tuned reasoning models with small adapters: Sudden accuracy drops suggest reasoning-data misalignment
  - Fine-tuned models with large adapters on small datasets: Performance degradation indicates overfitting/insufficient data
  - High prompt sensitivity (large accuracy swings between prompts): Signals weak internal reasoning

- First 3 experiments:
  1. **Baseline comparison**: Run all available open-source Arabic and proprietary models on validation set (1,000 examples) using identical prompt templates (both Prompt 1 and Prompt 2) to establish performance tiers and prompt sensitivity.
  2. **Fine-tuning data structure ablation**: Fine-tune GPT-4o and Gemini Flash 2.5 on (a) label-only data and (b) data with reasoning chains (if available via dataset v2) to test the reasoning-alignment hypothesis.
  3. **Ensemble composition analysis**: Test majority voting with different model combinations (2-model vs. 3-model ensembles; same-family vs. cross-family) to measure diversity bonus and identify optimal cost-accuracy tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal adapter size for fine-tuning reasoning-optimized models like Gemini Flash 2.5 on specialized legal domains without causing performance degradation?
- Basis in paper: [explicit] Authors state: "To confirm our observation, we may consider fine-tuning the same model with larger adapter sizes" and "To validate our observation, we may consider fine-tuning the same model with smaller adapter sizes."
- Why unresolved: Fine-tuning Gemini Flash 2.5 degraded performance from 91.5% to 74.6% with adapter size 1, and Flash 2 degraded with adapter size 8, suggesting a mismatch between adapter size and dataset scale that was not systematically tested.
- What evidence would resolve it: Ablation studies varying adapter sizes (e.g., 1, 2, 4, 8, 16) across multiple reasoning-optimized models with statistical significance testing.

### Open Question 2
- Question: Does training on datasets with explicit reasoning chains (Chain-of-Thought) improve fine-tuning outcomes for reasoning-optimized models compared to label-only datasets?
- Basis in paper: [explicit] The limitation section states: "in the original dataset, answer choices were labeled without detailed reasoning for each option, which complicated the fine-tuning of reasoning-based models. To solve this, a second version of the dataset was released recently, including the reasoning behind each selected answer."
- Why unresolved: The paper hypothesizes that reasoning-optimized models like Flash 2.5 may lose their reasoning structure when fine-tuned on label-only data, but this was not empirically tested with the new dataset version.
- What evidence would resolve it: Comparative fine-tuning experiments using both dataset versions on the same models, measuring accuracy on held-out test sets.

### Open Question 3
- Question: What is the true performance variance between top proprietary models (GPT-o3 vs. Gemini Flash 2.5) when accounting for statistical uncertainty?
- Basis in paper: [explicit] "The small gap in accuracy may be due to the results being based on a single run. Therefore, for future work, we should run each model multiple times and compute the average and variance."
- Why unresolved: The 0.8% difference between GPT-o3 (92.3%) and Gemini Flash 2.5 (91.5%) may not be statistically significant, but no variance estimates exist from single-run evaluations.
- What evidence would resolve it: Multiple inference runs (e.g., n≥10) with different random seeds, reporting mean accuracy, standard deviation, and confidence intervals.

## Limitations

- Results are based on single-run experiments without reported variance or statistical significance testing
- Fine-tuning experiments showed incomplete adapter size testing (only 1 vs 8 tested)
- Dataset's reasoning-chain content and prompt templates used for evaluation are not fully specified
- Test set labels are withheld (competition setup), preventing independent verification of ensemble accuracy claims

## Confidence

- **High Confidence**: Proprietary models (GPT-o3, Gemini Flash 2.5) substantially outperform open-source Arabic models (24-39% vs 88-92% accuracy) on Islamic inheritance reasoning tasks
- **Medium Confidence**: Fine-tuning improves generalist models (GPT-4o: +16.5%) but degrades reasoning-optimized models (Gemini Flash 2.5: -17%) due to data-structure misalignment
- **Low Confidence**: Majority voting ensemble of top base models achieves 92.7% accuracy (third place)

## Next Checks

1. **Statistical Significance Testing**: Re-run the base model evaluations (GPT-o3, Gemini Flash 2.5, Gemini Pro 2.5) on the validation set multiple times to establish confidence intervals and test whether the 92.7% ensemble accuracy is statistically distinguishable from individual model performance.

2. **Fine-tuning Data Structure Experiment**: Fine-tune GPT-4o and Gemini Flash 2.5 on both label-only data and reasoning-chain-enriched data (if available via dataset v2) to definitively test whether the performance degradation stems from reasoning-data misalignment rather than adapter size or hyperparameter issues.

3. **Ensemble Composition Analysis**: Systematically test majority voting with different model combinations (2-model vs. 3-model; same-family vs. cross-family) on the validation set to measure the actual diversity bonus and identify the optimal cost-accuracy tradeoff, particularly comparing ensembles of proprietary vs. open-source models.