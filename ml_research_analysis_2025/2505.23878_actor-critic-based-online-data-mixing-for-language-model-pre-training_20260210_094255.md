---
ver: rpa2
title: Actor-Critic based Online Data Mixing For Language Model Pre-Training
arxiv_id: '2505.23878'
source_url: https://arxiv.org/abs/2505.23878
tags:
- data
- training
- domain
- domains
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient data mixing for large
  language model pretraining by developing an actor-critic based online data mixing
  (AC-ODM) method. The core method uses reinforcement learning with actor-critic networks
  to dynamically adjust domain weights during pretraining, considering intra-domain
  interactions through a gradient alignment-based reward function.
---

# Actor-Critic based Online Data Mixing For Language Model Pre-Training

## Quick Facts
- **arXiv ID:** 2505.23878
- **Source URL:** https://arxiv.org/abs/2505.23878
- **Reference count:** 9
- **One-line primary result:** AC-ODM improves LLM pretraining efficiency by 27.5% on MMLU and 2.23x on HumanEval compared to ODM.

## Executive Summary
This paper introduces an actor-critic based online data mixing (AC-ODM) method for efficient LLM pretraining. The core innovation is using reinforcement learning with DDPG to dynamically adjust domain weights during training, considering intra-domain interactions through a gradient alignment-based reward function. To address computational efficiency, the method transfers a converged sampling policy learned by a small proxy model (410M parameters) to pretrain a larger target model (1B parameters). Experimental results demonstrate significant improvements in perplexity reduction speed and downstream task performance compared to existing online data mixing methods.

## Method Summary
The method frames data mixing as a continuous control problem using DDPG. An actor network outputs domain weights based on a state vector containing training dynamics, while a critic network evaluates the quality of these weights using a gradient alignment reward. The reward function $W_i = \langle \nabla \ell_i, \sum \nabla \ell_j \rangle$ captures intra-domain interactions by measuring how learning one domain contributes to learning others. The approach trains a proxy model (410M) with AC networks, then transfers the converged actor to guide the target model (1B) training, bypassing the expensive gradient alignment computation on the large model.

## Key Results
- AC-ODM-410M reaches optimal validation perplexity 71% faster than ODM
- Improves zero-shot MMLU accuracy by 27.5% and HumanEval pass@1 by 2.23x compared to ODM
- Achieves 20.7% lower perplexity than ODM baseline with 41,667 training steps
- Demonstrates superior perplexity across all 22 domains in the Pile dataset

## Why This Works (Mechanism)

### Mechanism 1
Prioritizing domains based on gradient alignment accelerates pretraining by exploiting shared knowledge structures (intra-domain interactions). Instead of minimizing loss per domain, the method rewards domains where the gradient $\nabla \ell_i$ aligns with the average gradient of all domains. This prioritizes data that contributes to the "general" learning direction, potentially acting as a form of implicit curriculum learning. The core assumption is that domains share lexical, syntactic, or semantic features; therefore, optimizing for alignment helps the model learn transferable representations earlier.

### Mechanism 2
Transferring a converged sampling policy from a small proxy model to a large target model maintains efficacy while drastically reducing computational overhead. The "environment" (training dynamics) is learned by the Actor-Critic networks using a cheap 410M parameter model. Once the Actor learns to map states (losses, steps) to weights, this policy is frozen and used to drive the data sampling for the expensive 1B model. The core assumption is that the relative utility of data domains during pretraining follows a similar dynamic across different model scales (scaling consistency).

### Mechanism 3
Using DDPG allows for fine-grained, continuous control over domain weights, improving upon discrete bandit methods. Data mixing is framed as a continuous control problem. DDPG allows the Actor to output precise floating-point weights $\alpha$ rather than discrete selections, enabling nuanced trade-offs between domains. The core assumption is that the optimal data mixture is a continuous function of the training state, not a sequence of discrete choices.

## Foundational Learning

- **Concept: Actor-Critic (RL) & DDPG**
  - Why needed here: To understand how the system separates the "policy" (Actor: deciding data weights) from the "value estimation" (Critic: predicting future reward/alignment) and why continuous action spaces require DDPG rather than standard REINFORCE or DQN.
  - Quick check question: How does the Critic use the gradient alignment reward to guide the updates of the Actor's weights?

- **Concept: Gradient Alignment / Multi-task Learning**
  - Why needed here: The core innovation is the reward function $W_i$. Understanding gradient alignment (e.g., how cosine similarity of gradients indicates conflicting or synergistic updates) is essential to grasp why this selects "generalizable" data.
  - Quick check question: If two domains have conflicting gradients (negative alignment), how would the reward function penalize them compared to domains with positive alignment?

- **Concept: Proxy-based Transfer Learning**
  - Why needed here: The efficiency gain relies on the hypothesis that small models can teach large models how to learn (data ordering). Understanding the limitations of this transfer is critical for architecture decisions.
  - Quick check question: What are the failure modes if the proxy model is significantly smaller than the target model (e.g., 70M vs 1B) as hinted in the analysis?

## Architecture Onboarding

- **Component map:** Environment (LLM) -> State (losses, steps, weight norms) -> Actor Network (outputs domain weights) -> Critic Network (evaluates weights) -> Reward (gradient alignment) -> Replay Buffer (stores experience tuples)

- **Critical path:**
  1. Train Proxy LLM (410M) + AC Networks
  2. Compute Reward $W$ using gradient alignment (expensive step)
  3. Update Actor via DDPG until convergence
  4. **Checkpoint:** Freeze the Actor network
  5. Train Target LLM (1B) using the frozen Actor to determine data sampling weights at each step. *Do not* update AC networks in this phase

- **Design tradeoffs:**
  - **Proxy Size vs. Transferability:** A larger proxy (410M) yields a better policy but costs more to train. A smaller proxy (70M) is cheap but may fail to generalize (evidenced by AC-ODM-70M performance)
  - **Reward Computation Cost:** Calculating the full gradient alignment matrix $W$ requires significant memory/compute. The authors mitigate this by only using selected layers (indexes 12, 14, 16) for the reward calculation

- **Failure signatures:**
  - **Policy Collapse:** Actor outputs zero weights for all but one domain (over-exploitation)
  - **Unstable Alignment:** Reward $W$ fluctuates wildly; check learning rate or increase moving average coefficient $\xi$
  - **Negative Transfer:** Target model perplexity diverges or stalls despite Proxy convergence; indicates proxy was too small to model target dynamics

- **First 3 experiments:**
  1. **Sanity Check (Proxy Overfit):** Train the AC-ODM on the proxy model and verify it can overfit/solve a tiny dataset with known optimal mixing to ensure the RL loop is functional
  2. **Reward Ablation:** Compare "Loss-only reward" (standard ODM) vs. "Gradient Alignment reward" on the proxy model to isolate the gain from intra-domain interaction modeling
  3. **Scale Transfer Test:** Train a 160M proxy, transfer policy to 1B target, and compare against 1B trained from scratch (AC-ODM) to measure the "transfer gap" and wall-clock savings

## Open Questions the Paper Calls Out

- **Question:** What is the precise scaling relationship (extrapolation law) required between the proxy model and the target model to ensure effective transfer of the data mixing strategy?
  - **Basis in paper:** [Explicit] Page 7 states, "We will study if there would be extrapolation relationship between the sizes of the proxy and target LLMs in the future." Additionally, the conclusion notes the authors "do not investigate the optimal size of proxy LLM."
  - **Why unresolved:** The authors observed that a 70M proxy failed while 410M succeeded for a 1B target, but the specific ratio or function governing this transfer across larger scales (e.g., training a 70B target) remains undefined.
  - **What evidence would resolve it:** Empirical data plotting target model performance against various proxy sizes for models significantly larger than the 1B parameter model tested in the paper.

- **Question:** Can the computational overhead of the gradient alignment reward function be reduced to support direct application on large-scale target models without necessitating a proxy?
  - **Basis in paper:** [Inferred] Page 4 highlights that the storage burden of gradients $\nabla \ell(\theta_M)$ grows exponentially, causing high Memory Access Costs (MAC). The current solution relies entirely on transferring the policy from a small proxy to bypass this cost.
  - **Why unresolved:** The method's efficiency relies on the proxy being small; if the "optimal size" of the proxy is found to be a significant fraction of the target model size (e.g., 10%), the computational cost of the gradient alignment reward might become prohibitive.
  - **What evidence would resolve it:** Demonstration of an approximation method for the gradient alignment reward that maintains policy quality while reducing memory/access overheads, allowing AC-ODM to run directly on a 7B+ parameter model.

- **Question:** How sensitive is the AC-ODM framework to the manual selection of specific layers used for state representation and reward calculation?
  - **Basis in paper:** [Inferred] The paper manually selects specific features for the state, such as "transformers indexed with 12, 14 and 16" for reward calculation (Page 6) and L2 norms of layers with even indexes for the state (Page 4).
  - **Why unresolved:** The selection of these specific indices appears heuristic or tailored to the 1B Pythia model; it is unclear if this configuration generalizes to different architectures or if a learned/adaptive selection method would yield superior results.
  - **What evidence would resolve it:** Ablation studies showing the variance in perplexity and downstream performance when different layer indices or state features are used for the Actor-Critic training.

## Limitations
- **Gradient alignment reward computation:** The computational overhead of calculating full gradient alignments remains significant despite layer selection, and the approach assumes gradient alignment is a reliable proxy for domain contribution across architectures.
- **Transfer consistency:** The assumption that proxy model training dynamics transfer to larger models is not universally validated; the 410M→160M performance degradation suggests transfer quality is sensitive to scale gaps.
- **Evaluation scope:** While perplexity improvements are measured across all 22 Pile domains, downstream task generalization (MMLU, HumanEval) is only reported for a subset, limiting claims about overall robustness.

## Confidence
- **High Confidence:** The DDPG-based actor-critic framework for continuous domain weight control is technically sound and the reported perplexity improvements on the Pile dataset are verifiable through the described methodology.
- **Medium Confidence:** The gradient alignment reward function effectively captures intra-domain interactions for the Pile dataset domains tested, though generalization to other corpora requires validation.
- **Low Confidence:** The proxy transfer mechanism will scale consistently to much larger model sizes (e.g., 8B→70B) without significant performance degradation or need for fine-tuning the transferred policy.

## Next Checks
1. **Architecture Ablation Test:** Train AC-ODM with the gradient alignment reward removed (using only loss-based rewards) on the same proxy model to quantify the specific contribution of intra-domain interaction modeling versus general AC optimization.

2. **Scale Transfer Gap Analysis:** Systematically vary the proxy size (70M, 160M, 410M) and measure the exact perplexity degradation when transferring policies to the same target size (1B), creating a transfer efficiency curve to identify the minimum viable proxy size.

3. **Gradient Alignment Sensitivity:** Test the stability of the reward function by randomly permuting the selected layers (12, 14, 16) used for gradient computation and measuring the variance in learned policies and final performance, establishing whether the choice of layers is critical or robust.