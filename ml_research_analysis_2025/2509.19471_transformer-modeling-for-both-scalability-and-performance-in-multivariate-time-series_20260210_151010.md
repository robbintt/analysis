---
ver: rpa2
title: Transformer Modeling for Both Scalability and Performance in Multivariate Time
  Series
arxiv_id: '2509.19471'
source_url: https://arxiv.org/abs/2509.19471
tags:
- deltaformer
- attention
- forecasting
- transformer
- delegate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability bottleneck in transformer
  modeling for multivariate time series (MTS) data, where variable count limits model
  applicability. The authors propose DELTAformer, which constrains inter-variable
  modeling through delegate tokens that aggregate information from patches at each
  position.
---

# Transformer Modeling for Both Scalability and Performance in Multivariate Time Series

## Quick Facts
- arXiv ID: 2509.19471
- Source URL: https://arxiv.org/abs/2509.19471
- Authors: Hunjae Lee; Corey Clark
- Reference count: 40
- Primary result: DELTAformer achieves state-of-the-art performance on MTS forecasting benchmarks, reducing MSE by 41% vs. Crossformer on long-term tasks

## Executive Summary
This paper addresses the scalability bottleneck in transformer modeling for multivariate time series (MTS) data, where variable count limits model applicability. The authors propose DELTAformer, which constrains inter-variable modeling through delegate tokens that aggregate information from patches at each position. These delegate tokens then undergo unconstrained inter-temporal attention, enabling the model to learn selective cross-variable relationships. This approach acts as an implicit regularizer, reducing noise accumulation while maintaining linear scaling with variable count. Experimental results demonstrate that DELTAformer achieves state-of-the-art performance across benchmarks, showing 41% average MSE reduction against Crossformer on long-term forecasting tasks. The model exhibits superior attention allocation efficiency and robustness to noise compared to standard transformers, validating its effectiveness in leveraging domain-specific challenges in MTS data.

## Method Summary
DELTAformer addresses scalability in MTS transformers by introducing delegate tokens that aggregate information across variables at each patch position, then apply unconstrained temporal attention. The architecture consists of funnel-in attention (O(C) per position) to create delegate tokens, standard self-attention across positions, and funnel-out attention to propagate information back. This achieves O(C×(L/P) + (L/P)²) complexity versus O(C²(L/P)²) for standard transformers, enabling linear scaling with variable count. The model uses variate-wise patching with L=96 look-back and P=16 patch length, trained with MSE loss and RevIN preprocessing. Key design choices include expansion factors of 1.0-1.5 for delegate tokens and 2-layer depth.

## Key Results
- Achieves 41% average MSE reduction against Crossformer on long-term forecasting benchmarks
- Demonstrates linear memory scaling with variable count versus quadratic scaling of standard transformers
- Shows superior attention allocation efficiency and noise robustness compared to standard transformer architectures

## Why This Works (Mechanism)

### Mechanism 1: Constrained Inter-variable Bottleneck via Delegate Tokens
- Claim: Limiting cross-variable information flow through compressed delegate tokens reduces noise accumulation while preserving predictive signals in MTS forecasting.
- Mechanism: At each patch position, delegate tokens aggregate information from all C variables via attention (O(C) complexity). The bottleneck forces selective aggregation—only information deemed useful for temporal modeling propagates forward.
- Core assumption: Informative inter-variable signals in MTS are sparse; most cross-variable interactions are noise rather than signal.
- Evidence anchors:
  - [abstract] "Delegate tokens act as an implicit regularizer that forces the model to be highly selective about what inter-variable information is allowed to propagate through the network."
  - [Page 2] "a growing consensus in the field points to indiscriminate inter-variable mixing as a potential source of significant noise accumulation and performance degradation."
  - [corpus] Weak direct evidence—neighbor papers discuss inter-variable dependencies but do not test bottleneck mechanisms.
- Break condition: If your MTS data has dense, highly informative cross-variable correlations (e.g., tightly coupled physical systems), the delegate token bottleneck may underrepresent critical dependencies.

### Mechanism 2: Decoupled Scaling Through Spatial-Temporal Factorization
- Claim: Separating variable aggregation (O(C)) from temporal attention (O((L/P)²)) achieves linear scaling with variable count while retaining full temporal modeling capacity.
- Mechanism: Funnel-in and funnel-out attention operate per-position across variables (linear in C). Delegate token attention operates across positions only (quadratic in L/P, but L/P ≪ C in typical MTS). Total complexity: O(C×(L/P) + (L/P)²).
- Core assumption: The number of patch positions (L/P) is substantially smaller than the number of variables (C) in target applications.
- Evidence anchors:
  - [Page 3] "Funnel-in attention for each Mi and Di achieves O(C) complexity. Over all patch positions, funnel-in attention's total complexity can be characterized as O(C×(L/P)) where it typically holds that L/P ≪ C."
  - [Page 8, Figure 5] Memory scaling visualization shows linear growth for DELTAformer vs. quadratic for standard transformers.
  - [corpus] Not directly tested in neighbor papers—scaling claims are DELTAformer-specific.
- Break condition: If your application has L/P comparable to or larger than C, the complexity advantage diminishes; standard full-attention may be preferable.

### Mechanism 3: Attention-Based Funneling Preserves Variable-Specific Characteristics
- Claim: Using attention (vs. MLP/linear) for funnel-in/out operations enables explicit, learnable control over variable-specific information preservation.
- Mechanism: Attention weights during funnel-out determine how much temporal/inter-variable context each patch receives, allowing the model to preserve heterogeneity (magnitude, distribution) across variables.
- Core assumption: Variable-specific characteristics are important for accurate forecasting and should not be erased during aggregation.
- Evidence anchors:
  - [Page 4] "Because funnel-out attention propagates information to each patch explicitly, it is able to learn to preserve variable-specific information such as data magnitudes and statistical distribution for each patch."
  - [Page 8, Table 2] Ablation shows attention-based funneling achieves up to 26-27% MSE reduction over MLP/linear alternatives, especially on high-dimensional datasets (Traffic, ECL).
  - [corpus] No direct corroboration—neighbor papers do not compare funneling mechanisms.
- Break condition: If variables are homogeneous (similar magnitude, distribution), simpler aggregation may suffice; attention-based funneling adds unnecessary overhead.

## Foundational Learning

- **Variate-wise Patching**
  - Why needed here: DELTAformer assumes variate-wise patches as input tokens. Understanding this tokenization is prerequisite to understanding how delegate tokens aggregate across variables.
  - Quick check question: Given MTS data of shape (C=100, L=96) with patch length P=16, how many total patches exist, and how many delegate tokens will DELTAformer create?

- **Self-Attention Complexity in Transformers**
  - Why needed here: The paper's scalability claims rest on comparing O(C²) or O(C²(L/P)²) against DELTAformer's O(C(L/P) + (L/P)²). You need to derive these to evaluate applicability.
  - Quick check question: For a dataset with C=862 variables, L=96, P=16, compute the theoretical FLOP ratio between full transformer and DELTAformer.

- **Attention as Learned Weighting vs. Fixed Aggregation**
  - Why needed here: The paper argues attention-based funneling outperforms MLP/linear because it enables dynamic, per-patch weighting. Understanding this distinction clarifies why the ablation results favor attention.
  - Quick check question: If you replace funnel-in attention with mean pooling, what information is definitively lost that attention could preserve?

## Architecture Onboarding

- **Component map:**
  - Input: Variate-wise patches → shape (C, d_patch, L/P)
  - Funnel-in Attention: Per-position aggregation → (L/P) delegate tokens
  - Delegate Token Attention: Full self-attention across positions → temporally-conditioned delegate tokens
  - Funnel-out Attention: Propagation back to patches → (C, d_patch, L/P)
  - Output: Linear decoder for forecasting

- **Critical path:**
  1. Verify patch dimensions match expected (L/P ≪ C)
  2. Initialize delegate tokens as learnable parameters
  3. Ensure expansion factor (delegate token dimension multiplier) is configured per dataset

- **Design tradeoffs:**
  - Expansion factor: Larger delegate tokens increase capacity but reduce regularization effect; ablation shows diminishing/no returns beyond 1.5×
  - Patch length: Larger P reduces temporal resolution for delegate token attention; P=16 performed best in ablation
  - Layers: Paper uses 2 layers; deeper models not tested

- **Failure signatures:**
  - Degraded performance on low-dimensional datasets (e.g., ETTh1 with C=7) suggests bottleneck over-constrains when C is small
  - Underperformance on short-term forecasting (PEMS) indicates temporal modeling may be misaligned for rapid pattern shifts
  - If attention weights concentrate on single variables during funnel-in, check for initialization issues or extreme heterogeneity

- **First 3 experiments:**
  1. **Sanity check on synthetic data**: Create sparse-signal MTS with known cross-variable correlations; verify delegate tokens attend to informative variables.
  2. **Scaling validation**: Train on Traffic (C=862) and measure peak memory against iTransformer/Timer-XL; confirm linear growth pattern matches Figure 5.
  3. **Ablation on funnel mechanism**: Replace funnel-in/out attention with MLP on your target dataset; quantify performance gap to assess whether your data benefits from attention-based variable preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DELTAformer perform in domains characterized by dense inter-variable correlations rather than sparse signals?
- Basis in paper: [explicit] The authors state in Section C.5 (Limitations) that "DELTAformer may struggle in datasets with dense inter-variable information" due to the bottleneck created by the funneling operations.
- Why unresolved: The model is designed assuming informative signals in MTS are sparse; dense systems may exceed the representative capacity of the constrained delegate tokens.
- Evidence: Evaluation on synthetic datasets with controlled signal density or real-world benchmarks known for high cross-variable coupling (e.g., certain physics-based systems).

### Open Question 2
- Question: What is the formal theoretical basis for the delegate token's effectiveness as an implicit regularizer?
- Basis in paper: [explicit] Section C.5 notes that the design relies on empirical observations and states it would be useful to "develop more rigorous theoretical basis for DELTAformer’s robust performance."
- Why unresolved: It is currently unclear mathematically why restricting information flow via delegate tokens consistently improves accuracy rather than simply degrading representation power.
- Evidence: A theoretical framework (potentially relating to Information Bottleneck theory) defining the necessary and sufficient capacity of delegate tokens for various noise levels.

### Open Question 3
- Question: Can the architecture be adapted to improve performance on short-term forecasting tasks?
- Basis in paper: [inferred] Section 4.2 and Table 9 show that while DELTAformer dominates long-term benchmarks, it is outperformed by the MLP-based TimeMixer on short-term PEMS datasets.
- Why unresolved: The model's strength in modeling "temporally distant dependencies" via global attention may hinder its ability to isolate immediate, sudden changes required for short-term predictions.
- Evidence: Architectural modifications that weight local temporal features more heavily in the delegate attention phase, closing the performance gap with MLP models on horizons < 48 steps.

## Limitations

- The bottleneck created by delegate tokens may underrepresent critical dependencies in domains with dense, highly informative cross-variable correlations
- Performance degrades on low-dimensional datasets (C < 10) where the inter-variable constraint becomes over-regularizing
- Underperforms on short-term forecasting tasks (< 48 steps), suggesting misalignment between delegate token design and rapid pattern recognition needs

## Confidence

- **High confidence**: The O(C×(L/P) + (L/P)²) complexity analysis and memory scaling patterns shown in Figure 5 are mathematically sound and empirically validated on Traffic dataset
- **Medium confidence**: The 41% MSE improvement over Crossformer is robust across long-term benchmarks, but the mechanism explaining this through noise reduction via constrained inter-variable mixing remains largely theoretical without ablation testing on synthetic controlled datasets
- **Low confidence**: Claims about attention-based funneling preserving variable-specific characteristics lack comparative validation against simpler aggregation methods beyond the MLP/linear ablations presented

## Next Checks

1. **Synthetic signal validation**: Create controlled MTS datasets with known sparse vs. dense cross-variable correlations to test whether delegate tokens correctly identify and propagate informative signals while suppressing noise
2. **Scaling boundary testing**: Evaluate DELTAformer on datasets where L/P approaches or exceeds C to quantify when the complexity advantage disappears and identify crossover points
3. **Short-term adaptation study**: Modify delegate token design to include per-position temporal context during funnel-in (not just across variables) and test whether short-term forecasting performance improves on PEMS benchmarks