---
ver: rpa2
title: 'VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions'
arxiv_id: '2509.25818'
source_url: https://arxiv.org/abs/2509.25818
tags:
- image
- captions
- evaluation
- long
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VELA, an automatic evaluation metric designed
  to assess long image captions generated by multimodal Large Language Models (MLLMs).
  Unlike existing metrics primarily suited for short captions, VELA employs a novel
  LLM-Hybrid-as-a-Judge framework that integrates visual and textual information through
  two branches: the R2C-LLM branch, which uses a lightweight non-autoregressive LLM
  to capture linguistic relationships between candidates and references, and the I2C-Align
  branch, which uses Long-CLIP to compute visual alignment without relying on slow
  MLLM inference.'
---

# VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions

## Quick Facts
- arXiv ID: 2509.25818
- Source URL: https://arxiv.org/abs/2509.25818
- Authors: Kazuki Matsuda; Yuiga Wada; Shinnosuke Hirano; Seitaro Otsuki; Komei Sugiura
- Reference count: 40
- VELA achieves superhuman performance in long image caption evaluation, significantly outperforming existing metrics including LLM-as-a-Judge approaches

## Executive Summary
This paper introduces VELA, an automatic evaluation metric designed to assess long image captions generated by multimodal Large Language Models (MLLMs). Unlike existing metrics primarily suited for short captions, VELA employs a novel LLM-Hybrid-as-a-Judge framework that integrates visual and textual information through two branches: the R2C-LLM branch, which uses a lightweight non-autoregressive LLM to capture linguistic relationships between candidates and references, and the I2C-Align branch, which uses Long-CLIP to compute visual alignment without relying on slow MLLM inference. VELA evaluates captions from three distinct perspectives—Descriptiveness, Relevance, and Fluency—preventing important criteria from being overlooked.

The authors construct LongCap-Arena, a benchmark with 7,805 images, human-provided long reference captions, long candidate captions, and 32,246 human judgments collected from 1,020 annotators. Experimental results demonstrate that VELA significantly outperforms existing metrics, including LLM-as-a-Judge approaches, achieving superhuman performance. On the test set, VELA achieves Kendall's τc scores of 56.4, 40.0, and 57.4 for Descriptiveness, Relevance, and Fluency, respectively, while maintaining much faster inference speeds than competing LLM-based methods.

## Method Summary
VELA introduces a hybrid evaluation framework that combines both visual and textual information processing to assess long image captions. The core innovation is the LLM-Hybrid-as-a-Judge approach, which uses two specialized branches: R2C-LLM for capturing linguistic relationships between candidate and reference captions using a lightweight non-autoregressive LLM, and I2C-Align for computing visual alignment using Long-CLIP. This architecture enables efficient evaluation without the computational overhead of full MLLM inference. The metric evaluates captions across three dimensions—Descriptiveness (how comprehensively the image is described), Relevance (how well the caption relates to the image content), and Fluency (the linguistic quality of the caption). The authors construct LongCap-Arena, a benchmark dataset with 7,805 images and extensive human judgments, to validate their approach against existing evaluation metrics.

## Key Results
- VELA achieves Kendall's τc scores of 56.4, 40.0, and 57.4 for Descriptiveness, Relevance, and Fluency on the test set, respectively
- VELA significantly outperforms existing metrics including LLM-as-a-Judge approaches while maintaining faster inference speeds
- VELA demonstrates superhuman performance in long image caption evaluation compared to human annotators

## Why This Works (Mechanism)
VELA's effectiveness stems from its hybrid approach that combines efficient visual alignment with linguistic relationship modeling. The R2C-LLM branch uses a lightweight non-autoregressive LLM to capture nuanced linguistic relationships between candidate and reference captions without the computational cost of full autoregressive generation. The I2C-Align branch leverages Long-CLIP to compute visual alignment efficiently, avoiding the slow inference times of full MLLM evaluation. By evaluating captions across three distinct dimensions (Descriptiveness, Relevance, and Fluency), VELA captures a more comprehensive assessment than single-score metrics, ensuring that no critical evaluation criteria are overlooked.

## Foundational Learning
- **Non-autoregressive LLMs**: Used in R2C-LLM branch for efficient linguistic comparison without sequential generation overhead. Why needed: Traditional autoregressive LLMs are too slow for evaluation tasks requiring rapid comparison of multiple caption pairs.
- **Visual alignment with Long-CLIP**: Enables efficient comparison of image-caption pairs without full MLLM inference. Why needed: MLLM inference is computationally expensive and slow, making it impractical for large-scale evaluation.
- **Three-branch evaluation framework**: Separates Descriptiveness, Relevance, and Fluency assessments. Why needed: Long captions require nuanced evaluation across multiple dimensions that a single metric cannot adequately capture.
- **Kendall's τc correlation**: Standard metric for comparing ranked predictions against human judgments. Why needed: Provides a statistically sound measure of how well automated metrics align with human evaluation preferences.
- **LLM-Hybrid-as-a-Judge paradigm**: Combines multiple specialized models rather than relying on a single MLLM. Why needed: Balances evaluation quality with computational efficiency for practical deployment.

## Architecture Onboarding

**Component Map**: Image + Candidate Caption + Reference Caption -> I2C-Align (Long-CLIP) + R2C-LLM (Non-autoregressive LLM) -> Descriptiveness Score + Relevance Score + Fluency Score -> VELA Final Score

**Critical Path**: The critical evaluation path involves processing the image through Long-CLIP for visual alignment while simultaneously processing the candidate and reference captions through the R2C-LLM branch. These two branches produce intermediate scores that are combined to generate the final VELA scores for each evaluation dimension.

**Design Tradeoffs**: The hybrid approach trades some potential accuracy from full MLLM evaluation for significant gains in computational efficiency. Using a non-autoregressive LLM reduces inference time but may miss some contextual nuances that autoregressive models capture. The three-branch evaluation ensures comprehensive assessment but adds complexity compared to single-score metrics.

**Failure Signatures**: Performance degradation may occur when candidate and reference captions have very different styles or when images contain complex scenes that exceed Long-CLIP's representation capabilities. The metric may also struggle with captions that are grammatically correct but semantically incorrect.

**Three First Experiments**:
1. Compare VELA scores against human judgments across all three evaluation dimensions to establish baseline correlation
2. Test computational efficiency by measuring inference time on varying numbers of caption pairs
3. Conduct ablation studies removing each evaluation branch to quantify individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark scale is relatively small with 7,805 images and 32,246 human judgments, potentially limiting generalizability across diverse caption evaluation scenarios
- The three-branch framework lacks comprehensive ablation studies to empirically validate that all branches are necessary for optimal performance
- The evaluation relies heavily on Kendall's τc correlation with human judgments without exploring alternative correlation measures or robustness checks across different statistical methods

## Confidence
- **High confidence**: The architectural design of VELA (R2C-LLM and I2C-Align branches) is technically sound and well-motivated by the need for efficient long caption evaluation
- **Medium confidence**: The superior performance claims relative to existing metrics, as these are based on correlation with human judgments from a single benchmark
- **Medium confidence**: The claim that existing metrics are "primarily suited for short captions" - while generally true, the paper could provide more systematic evidence of metric failure on long captions

## Next Checks
1. Test VELA on additional long caption datasets from different domains (medical imaging, satellite imagery, artistic photography) to assess domain generalization
2. Conduct ablation studies removing each evaluation branch individually to quantify their individual contributions and verify the claim that all three are necessary
3. Compare VELA's performance against human consensus using inter-annotator agreement metrics to better contextualize the "superhuman" claims and assess whether VELA truly captures the aspects humans value in long captions