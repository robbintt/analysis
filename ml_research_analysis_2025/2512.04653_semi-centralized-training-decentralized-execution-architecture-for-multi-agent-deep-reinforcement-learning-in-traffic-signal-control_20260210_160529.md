---
ver: rpa2
title: Semi Centralized Training Decentralized Execution Architecture for Multi Agent
  Deep Reinforcement Learning in Traffic Signal Control
arxiv_id: '2512.04653'
source_url: https://arxiv.org/abs/2512.04653
tags:
- uni00000013
- regional
- uni00000048
- intersection
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEMI-CTDE, a region-based MARL architecture
  for adaptive traffic signal control that balances coordination and scalability.
  The method partitions a traffic network into tightly coupled regions, performs centralized
  training within each region with parameter sharing, and executes decentralized control
  at intersections.
---

# Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control

## Quick Facts
- arXiv ID: 2512.04653
- Source URL: https://arxiv.org/abs/2512.04653
- Reference count: 24
- Primary result: Region-based MARL with parameter sharing and composite states reduces average waiting time vs. decentralized and actuated baselines in 5×5 traffic grid

## Executive Summary
This paper introduces SEMI-CTDE, a region-based MARL architecture for adaptive traffic signal control that balances coordination and scalability. The method partitions a traffic network into tightly coupled regions, performs centralized training within each region with parameter sharing, and executes decentralized control at intersections. Key innovations include composite state/reward designs that jointly encode local and regional traffic dynamics, and a flexible architecture transferable across policy backbones. Two implementations—RegionWide (whole-region summaries) and OneHop (bounded neighborhood summaries)—are evaluated on a 5×5 grid network under varying traffic demands.

## Method Summary
SEMI-CTDE partitions a traffic network into regions using fuzzy α-cut clustering, then trains a shared DDQN (Regional Agent) for all intersections within each region. Each intersection agent encodes local and regional traffic features via a Topology and Location Encoder (TLE), passes them to the shared Regional Agent, and uses an Action Mapping Module (AMM) to select valid actions based on local topology. The method uses composite state and reward formulations that jointly encode local and regional information, with two receptive field designs: RegionWide (full region view) and OneHop (bounded neighborhood view). Training uses experience replay with parameter sharing across intersections in each region.

## Key Results
- SEMI-CTDE models consistently outperform fully decentralized and actuated baselines across all traffic demands
- OneHop implementation excels under light flows (AWT reduction to 28.27s from 34.31s)
- RegionWide implementation dominates under heavy congestion (AWT of 67.61s vs. 91.33s for actuated in W4)
- Regional parameter sharing and tailored state/reward design are critical for superior performance across diverse traffic conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Regional parameter sharing stabilizes learning in heterogeneous networks by increasing sample efficiency.
- **Mechanism:** A single DDQN serves all intersections within a tightly coupled region. By pooling experiences from diverse intersection topologies into a shared replay memory, the agent learns a generalizable policy faster than independent agents.
- **Core assumption:** Intersections within a defined region share sufficient traffic dynamics such that a single policy can represent optimal actions for all.
- **Evidence anchors:** Section 6.3 notes parameter sharing increases sample efficiency and stabilizes learning compared to fully decentralized baselines in light flows. Section 4.4.5 describes how pooling heterogeneous experiences promotes coordinated behavior.
- **Break condition:** If regions are partitioned incorrectly (containing unrelated dynamics), the shared policy will fail to specialize, potentially degrading performance vs. independent learners.

### Mechanism 2
- **Claim:** Composite state/reward designs enable agents to balance local throughput with regional coordination.
- **Mechanism:** The architecture decomposes state and reward into local components (immediate queues/delay) and regional components (aggregated neighbor stats). This provides the agent with a "multi-scale" view, theoretically allowing it to select actions that optimize immediate flow while preventing spillback or downstream starvation.
- **Core assumption:** The hand-crafted aggregation of regional features correlates strongly with the global objective of minimizing average travel time.
- **Evidence anchors:** Section 4.2 & 4.3 details the modular concatenation of local and regional state components. Abstract states the architecture uses "composite state and reward formulations that jointly encode local and regional information."
- **Break condition:** If the regional summary dimensions are too high or noisy, the "curse of dimensionality" may return, or the agent may ignore the regional signal, reverting to greedy local behavior.

### Mechanism 3
- **Claim:** Performance is contingent on matching the regional receptive field to traffic density.
- **Mechanism:** The paper distinguishes between "OneHop" (neighborhood view) and "RegionWide" (full region view). OneHop focuses on immediate neighbors, which is sufficient for light traffic where interactions are local. RegionWide captures global imbalances, which becomes necessary under heavy congestion where spillback propagates across the region.
- **Core assumption:** Heavy congestion creates long-range dependencies that bounded-neighborhood methods cannot capture.
- **Evidence anchors:** Abstract reports "OneHop excels under light flows... while RegionWide dominates under heavy congestion." Section 6.3 observes that in heavy flows, OneHop "reacts shortsightedly to local symptoms rather than the region-scale structure of queues."
- **Break condition:** Applying OneHop to saturated networks may lead to "gridlock" ignorance; applying RegionWide to sparse networks may dilute the learning signal with irrelevant noise.

## Foundational Learning

- **Concept: Centralized Training Decentralized Execution (CTDE)**
  - **Why needed here:** This is the baseline paradigm the paper modifies ("SEMI-CTDE"). You must understand that standard CTDE uses a global critic during training but local execution, whereas this paper proposes *regional* centralization.
  - **Quick check question:** How does SEMI-CTDE differ from standard CTDE regarding the scope of the "critic" or training server?

- **Concept: Double Deep Q-Networks (DDQN)**
  - **Why needed here:** The Regional Agents are implemented as DDQNs. Understanding how target networks and the decoupling of action selection/evaluation mitigate overestimation is crucial for debugging training instability.
  - **Quick check question:** Why does the DDQN target formula separate the action selection (online network) from the value evaluation (target network)?

- **Concept: Action Masking in MARL**
  - **Why needed here:** The architecture uses an "Action Mapping Module" (AMM) to handle heterogeneous intersections. You need to understand how masking invalid actions (e.g., phasing impossible at a T-junction) allows a single global policy network to operate safely.
  - **Quick check question:** How does the AMM ensure that a shared Regional Agent does not select an invalid phase for a specific intersection topology?

## Architecture Onboarding

- **Component map:** Simulation -> TLE -> Regional Agent -> AMM -> Action Execution
- **Critical path:**
  1. **Simulation:** Generate raw traffic data (queues, phase)
  2. **TLE:** Encode raw data -> s_enc (handles heterogeneity)
  3. **Inference:** Regional Agent predicts Q(s_enc)
  4. **AMM:** Mask invalid actions -> Execute valid action
  5. **Learning:** Store transition in regional replay buffer; update shared DDQN parameters

- **Design tradeoffs:**
  - **RegionWide vs. OneHop:** Choose RegionWide for high-density/congestion scenarios (requires more bandwidth/computation for aggregation) vs. OneHop for light traffic (lower latency/computation)
  - **Capacity vs. Generalization:** A single shared network (SEMI-CTDE) generalizes better but may have lower capacity than Fully Decentralized (one network per intersection)

- **Failure signatures:**
  - **Instability in Heavy Traffic (OneHop):** Queue lengths stabilize initially but explode during peak demand due to lack of global visibility
  - **Stagnation in Light Traffic (RegionWide):** Learning plateaus or oscillates because the regional aggregation washes out the weak signal from sparse local events
  - **Invalid Actions:** If the AMM is misconfigured, the agent may attempt to activate non-existent phases, causing simulation crashes

- **First 3 experiments:**
  1. **Topology Validation:** Implement the TLE and AMM for a mixed network (T-junction + Cross). Verify that a single shared network can output valid actions for both without crashing.
  2. **Receptive Field A/B Test:** Train two models (OneHop vs. RegionWide) on a "W4" (heavy Weibull) flow. Confirm that RegionWide yields lower AWT than OneHop.
  3. **Ablation on Parameter Sharing:** Compare "Partially SEMI-CTDE" (sharing parameters, local info only) against "Fully Decentralized." Verify if sample efficiency improves even without regional state info.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SEMI-CTDE maintain performance stability and convergence speeds when applied to continuous action spaces or more complex phasing schemes?
- **Basis in paper:** [explicit] The authors explicitly state that the implemented models use a "restricted discrete action space" and suggest that "more complex phasing schemes, as well as continuously variable green durations" should be explored in future work.
- **Why unresolved:** The current study validates the architecture only on a discrete action set of four phase logics with two durations, leaving the efficacy of continuous control or high-dimensional phase selections untested.
- **What evidence would resolve it:** Experimental results comparing the current discrete DDQN implementation against policy backbones capable of continuous action outputs (e.g., actor-critic methods) within the SEMI-CTDE framework.

### Open Question 2
- **Question:** Do learned feature extractors, such as Graph Neural Networks (GNNs) or LSTMs, outperform the hand-crafted spatial and temporal aggregates currently used in the state design?
- **Basis in paper:** [explicit] The conclusion notes the state design relies on "hand-crafted spatial and temporal aggregates" and identifies the integration of "learned feature extractors" as a "natural extension" to capture finer-grained dynamics.
- **Why unresolved:** The paper relies on manually designed features for interpretability, but it remains unknown if automated feature learning could better capture latent traffic dynamics to improve policy efficiency.
- **What evidence would resolve it:** Ablation studies replacing the manual feature blocks with GNNs (for spatial structure) and LSTMs (for temporal dynamics) to measure performance deltas in complex traffic scenarios.

### Open Question 3
- **Question:** How does SEMI-CTDE scale when deployed on larger networks or real-world urban topologies with irregular road structures?
- **Basis in paper:** [explicit] The conclusion lists exploring "larger-scale or real-world networks" as a "promising direction for future research."
- **Why unresolved:** The entire experimental validation is limited to a synthetic 5×5 grid network; thus, the architecture's robustness to the irregular topologies and scale of actual city road networks is unverified.
- **What evidence would resolve it:** Empirical evaluations of SEMI-CTDE on standard large-scale traffic benchmarks (e.g., Hangzhou or Manhattan datasets) or real-world city maps.

## Limitations

- **Unknown sensitivity to α threshold:** The paper does not evaluate how sensitive performance is to the choice of α for fuzzy graph region formation
- **Limited topology validation:** All experimental validation is confined to a single 5×5 grid network; scaling behavior to larger, more heterogeneous topologies is unknown
- **Unverified backbone transferability:** The claimed transferability across policy backbones is asserted but only validated with DDQN; other backbones may behave differently

## Confidence

- **Performance claims (AWT reductions):** High - supported by multiple metrics and flows
- **Mechanism claims (regional parameter sharing, composite states):** Medium - supported by ablation results but lacking ablations on α-cut or alternative regional representations
- **Density-dependent receptive field hypothesis:** Medium - clear trend reversals observed but exact density thresholds not quantified

## Next Checks

1. Perform sensitivity analysis on α-cut clustering: sweep α values and measure impact on AWT across all traffic flows
2. Test adaptive region re-partitioning during training: compare static vs. dynamic regions on heavy congestion scenarios
3. Validate policy backbone transferability: reimplement SEMI-CTDE with PPO or SAC and compare performance to DDQN on the same 5×5 grid