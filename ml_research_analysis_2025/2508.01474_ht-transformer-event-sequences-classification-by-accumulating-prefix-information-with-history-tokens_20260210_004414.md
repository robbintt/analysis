---
ver: rpa2
title: 'HT-Transformer: Event Sequences Classification by Accumulating Prefix Information
  with History Tokens'
arxiv_id: '2508.01474'
source_url: https://arxiv.org/abs/2508.01474
tags:
- history
- tokens
- sequence
- event
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HT-Transformer addresses the limitation of standard Transformers
  in sequence classification by introducing history tokens that accumulate contextual
  information during pretraining via next-token prediction. The method employs a custom
  attention mask allowing history tokens to gather past information while event tokens
  selectively attend to them.
---

# HT-Transformer: Event Sequences Classification by Accumulating Prefix Information with History Tokens

## Quick Facts
- arXiv ID: 2508.01474
- Source URL: https://arxiv.org/abs/2508.01474
- Authors: Ivan Karpukhin; Andrey Savchenko
- Reference count: 7
- State-of-the-art sequence classification performance on finance, healthcare, and e-commerce benchmarks

## Executive Summary
HT-Transformer addresses the limitation of standard Transformers in sequence classification by introducing history tokens that accumulate contextual information during pretraining via next-token prediction. The method employs a custom attention mask allowing history tokens to gather past information while event tokens selectively attend to them. This design enables history tokens to serve as effective sequence embeddings for downstream tasks. Evaluated across finance, healthcare, and e-commerce benchmarks, HT-Transformer achieves state-of-the-art results after supervised fine-tuning, outperforming standard Transformers by up to 0.96% ROC AUC on the Churn dataset. The approach is particularly effective for future-oriented prediction tasks and demonstrates strong adaptability to global classification problems when fine-tuned.

## Method Summary
HT-Transformer extends standard Transformers by introducing history tokens that accumulate contextual information during pretraining. The architecture uses a custom attention mask that allows history tokens to gather information from all previous tokens while restricting event tokens to attend only to history tokens. During pretraining, history tokens are updated through next-token prediction, enabling them to capture comprehensive sequence context. For downstream tasks, the final history token state serves as the sequence embedding for classification. The model supports various history token positioning strategies (uniform, bias-end) and demonstrates particular effectiveness for future-oriented prediction tasks. Supervised fine-tuning adapts the pretrained embeddings to specific classification objectives.

## Key Results
- Achieves state-of-the-art performance on finance, healthcare, and e-commerce benchmarks after supervised fine-tuning
- Outperforms standard Transformers by up to 0.96% ROC AUC on the Churn dataset
- Demonstrates strong adaptability to global classification problems when fine-tuned, though requires supervised adaptation for optimal performance

## Why This Works (Mechanism)
The architecture works by creating dedicated history tokens that serve as information accumulators during pretraining. These tokens are strategically positioned and updated through the next-token prediction objective, allowing them to capture comprehensive sequence context. The custom attention mask ensures history tokens receive information from all previous tokens while event tokens can selectively attend to these history tokens, creating an efficient information flow. This design enables the final history token state to encode rich sequence-level information that can be directly used for classification tasks. The approach is particularly effective for future-oriented prediction because the history tokens naturally accumulate information relevant to predicting subsequent events.

## Foundational Learning

**Attention Mechanisms** - Why needed: Core to how tokens interact and information flows in Transformers; quick check: verify mask shapes and attention patterns match intended design

**Next-Token Prediction** - Why needed: Pretraining objective that drives history token accumulation; quick check: confirm loss computation uses correct token positions

**Sequence Classification Embeddings** - Why needed: Need to represent entire sequences as fixed-size vectors for downstream tasks; quick check: verify history token extraction for classification head

## Architecture Onboarding

**Component Map**: Input Sequence -> History Token Insertion -> Custom Attention Mask -> Next-Token Prediction Loss -> History Token State -> Classification Head

**Critical Path**: Event tokens → History tokens (through attention) → Next-token prediction → History token accumulation → Final history token → Classification

**Design Tradeoffs**: The addition of history tokens increases model complexity but provides more effective sequence embeddings. The custom attention mask adds implementation complexity but enables targeted information flow. The approach requires supervised fine-tuning for optimal performance, adding a training stage.

**Failure Signatures**: Poor performance on global classification tasks with frozen embeddings, suboptimal history token positioning leading to degraded downstream accuracy, and increased computational overhead during pretraining and inference.

**3 First Experiments**:
1. Verify attention mask implementation by visualizing attention patterns between event and history tokens
2. Test different history token positioning strategies (uniform vs. bias-end) on a validation set
3. Compare frozen embedding performance against supervised fine-tuning across different task types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the attention mechanism be specifically optimized to exploit the sparsity of HT-Transformer's custom attention masks?
- Basis in paper: [explicit] The authors state in the "Limitations and Future Work" section that the current reliance on standard PyTorch multi-head attention is suboptimal for the custom masks and that exploiting sparsity could reduce computational cost.
- Why unresolved: The current implementation uses a general-purpose attention module that does not leverage the specific sparse structure where only a small subset of tokens participates in full self-attention.
- What evidence would resolve it: Implementation of a custom kernel optimized for these specific sparse masks and subsequent benchmarks showing reduced training time and resource consumption compared to conventional causal Transformers.

### Open Question 2
- Question: What are the optimal policies for history token positioning and sampling beyond the "Uniform" and "Bias-End" strategies?
- Basis in paper: [explicit] The "Limitations and Future Work" section calls for a "more detailed analysis of token positioning and sampling policies," noting that the ablation studies showed placement has a significant impact on downstream performance.
- Why unresolved: The study primarily compared static strategies (uniform vs. biased towards the end) and did not explore dynamic, learned, or content-aware placement strategies.
- What evidence would resolve it: A comparative analysis of adaptive positioning algorithms demonstrating consistent performance gains or training stability across the evaluated finance, healthcare, and e-commerce benchmarks.

### Open Question 3
- Question: Can the architecture be modified to better capture global sequence properties in the unsupervised embedding stage without relying on supervised fine-tuning?
- Basis in paper: [inferred] The paper notes that HT-Transformer underperforms compared to contrastive learning (CoLES) on the AgePred dataset (a global task) when using frozen embeddings, as the next-token prediction objective prioritizes recent context over global attributes.
- Why unresolved: While supervised fine-tuning (SFT) bridges this performance gap, the fundamental inability of the pretraining objective to capture global characteristics without labels remains a limitation of the proposed method.
- What evidence would resolve it: A modification to the pretraining objective or history token accumulation mechanism that improves performance on global classification tasks to rival contrastive baselines without requiring labeled data for fine-tuning.

## Limitations
- Custom attention mask and history token mechanisms increase implementation complexity and computational overhead
- Requires supervised fine-tuning for optimal performance, adding an additional training stage
- Evaluation focuses on classification accuracy metrics without comprehensive analysis of computational trade-offs or inference efficiency

## Confidence
High confidence in the core architectural contribution and its effectiveness for sequence classification tasks
Medium confidence in generalizability across diverse domains due to limited evaluation scope
Medium confidence in computational efficiency claims due to limited complexity analysis

## Next Checks
1. Conduct ablation studies removing history tokens to quantify their specific contribution versus standard Transformer performance
2. Evaluate computational complexity and memory requirements during both pretraining and fine-tuning phases
3. Test the approach on sequence classification tasks with longer temporal dependencies and larger event vocabularies to assess scalability