---
ver: rpa2
title: Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising
arxiv_id: '2503.17198'
source_url: https://arxiv.org/abs/2503.17198
tags:
- domain
- unauthorized
- authorized
- data
- jailntl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JailNTL, the first black-box attack method
  against Non-Transferable Learning (NTL) models. NTL protects model IP by creating
  a "non-transferable barrier" that restricts model performance from authorized to
  unauthorized domains.
---

# Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising

## Quick Facts
- arXiv ID: 2503.17198
- Source URL: https://arxiv.org/abs/2503.17198
- Authors: Yongli Xiang; Ziming Hong; Lina Yao; Dadong Wang; Tongliang Liu
- Reference count: 40
- Key outcome: First black-box attack method against Non-Transferable Learning models, achieving up to 55.7% accuracy increase in unauthorized domains using only 1% authorized samples

## Executive Summary
This paper introduces JailNTL, a novel black-box attack method that breaks Non-Transferable Learning (NTL) models by disguising unauthorized domain inputs to appear authorized. Unlike existing white-box attacks that modify model weights, JailNTL operates at test time by training a disguising network that transforms unauthorized data to bypass the non-transferable barrier. The method uses adversarial domain translation combined with content preservation and model-guided statistics alignment to achieve unprecedented attack success without requiring model access beyond black-box predictions.

## Method Summary
JailNTL trains a disguising network to transform unauthorized domain inputs into representations that appear authorized to NTL models. The method operates in two levels: data-intrinsic disguising (DID) eliminates domain discrepancy through adversarial learning while preserving class-related content via bidirectional reconstruction, and model-guided disguising (MGD) minimizes output-level statistics differences by aligning prediction confidence and class balance distributions. Using only 1% of authorized samples, the approach achieves up to 55.7% accuracy improvement in unauthorized domains, exceeding existing white-box attacks and demonstrating effectiveness across multiple domain shift scenarios.

## Key Results
- Achieves up to 55.7% accuracy increase in unauthorized domains using only 1% authorized samples
- Outperforms existing white-box attacks in black-box settings
- Ablation shows both DID and MGD components are essential for maximum effectiveness
- Successfully transfers to multiple NTL baselines including MMLD, SIM, and NLLDM

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Domain Translation for Authorization Bypass
Modifying unauthorized domain inputs to appear statistically similar to authorized inputs can bypass the non-transferable barrier, as NTL models primarily rely on distributional discrepancies to restrict generalization. JailNTL trains a disguising network using adversarial learning to minimize domain discrepancy between disguised unauthorized data and real authorized data. A domain discriminator tries to distinguish disguised data from authorized data, while the disguising model tries to fool it—creating increasingly realistic translations. Core assumption: NTL models enforce the non-transferable barrier primarily through statistical differences in feature representations between domains rather than semantic content understanding.

### Mechanism 2: Content Preservation via Bidirectional Feedback
A feedback network that reconstructs original inputs from disguised versions ensures class-related content is preserved during domain translation, maintaining prediction accuracy. An inverse disguising model maps disguised data back to the original domain, with L1 loss constraining reconstruction fidelity. Bidirectional training further stabilizes the mapping and prevents information loss. Core assumption: Domain-specific style information can be separated from class-related content, and the disguising network learns this disentanglement implicitly through reconstruction constraints.

### Mechanism 3: Model-Guided Statistics Alignment
Aligning output-level statistics (prediction confidence and class balance) between disguised and authorized data improves attack effectiveness by matching the behavioral signatures the NTL model expects from authorized inputs. Using black-box access to NTL model outputs, JailNTL minimizes: (1) entropy difference between confidence distributions, and (2) entropy difference between class balance distributions. Zero-order gradient estimation enables optimization without backpropagation through the NTL model. Core assumption: NTL models exhibit systematic statistical differences between authorized and unauthorized domain predictions that can be measured and matched.

## Foundational Learning

- **Adversarial Training in GANs**: Why needed here: The disguising network uses generator-discriminator adversarial dynamics. Understanding how generators improve by fooling discriminators is essential for debugging convergence issues. Quick check question: Can you explain why the discriminator must be updated in alternation with the generator rather than simultaneously?

- **Domain Adaptation vs. Domain Translation**: Why needed here: JailNTL performs test-time domain translation (modifying data) rather than domain adaptation (modifying model). This distinction clarifies why model weights remain untouched. Quick check question: If you had white-box access to model gradients, would domain translation still be preferable to fine-tuning? Why or why not?

- **Zero-Order Gradient Estimation**: Why needed here: Black-box constraints prevent backpropagation through the NTL model. Finite difference approximation enables gradient-based optimization using only forward passes. Quick check question: What is the computational cost tradeoff between zero-order estimation and standard backpropagation for a model with N parameters?

## Architecture Onboarding

- **Component map**: Unauthorized data -> Disguising Model (fd) -> Disguised data -> NTL Model (fntl) -> Predictions -> Domain Discriminators (fc, ˆfc) for adversarial training, Inverse Disguising Model (ˆfd) for reconstruction

- **Critical path**: 1) Sample mini-batches Bu (unauthorized) and Ba (authorized) 2) Forward through fd to generate disguised unauthorized data 3) Compute adversarial loss via discriminators fc and ˆfc 4) Reconstruct via ˆfd, compute L1 consistency loss 5) Query NTL model for logits on Ba and fd(Bu) 6) Compute confidence and class balance losses using zero-order gradients 7) Update fd and ˆfd (minimize), fc and ˆfc (maximize) via Adam (lr=0.0002)

- **Design tradeoffs**: Bidirectional vs. unidirectional training doubles computation but stabilizes mappings and improves content preservation; PatchGAN vs. full-image discriminator is computationally efficient but may miss global inconsistencies; Model-guided loss inclusion improves accuracy but requires logit access; Batch size of 5 limits class balance estimation reliability but may be necessary for memory constraints

- **Failure signatures**: Mode collapse where disguised samples converge to limited variations; content drift where reconstruction loss increases over training; statistics mismatch where class balance loss fails to converge; domain gap too large for certain dataset pairs

- **First 3 experiments**: 1) Reproduce baseline on CIFAR10→STL10 with NTL model using full JailNTL (DID+MGD) with 1% authorized data (500 images); target 51.6% improvement 2) Ablate model-guided components by running JailNTL* (DID only) to isolate MGD contribution; expect 49.8-55.0% improvement 3) Test with 0.5% authorized data (250 images) to evaluate robustness to limited supervision; expect 44.8-54.0% improvement

## Open Questions the Paper Calls Out
1. How can Non-Transferable Learning models be effectively defended against test-time data disguising attacks?
2. What is the query efficiency of JailNTL given its reliance on zero-order gradient estimation?
3. How robust is JailNTL when attacking models under stricter black-box settings where only hard prediction labels are available?

## Limitations
- Focus on classification tasks with clear domain shifts leaves unclear how well the method generalizes to more complex task types
- Reliance on logit access for model-guided statistics alignment presents a practical constraint, though these can be omitted with limited impact
- Black-box constraint significantly restricts optimization methods, requiring computationally expensive zero-order gradient estimation

## Confidence
- **High confidence**: The fundamental adversarial domain translation mechanism and bidirectional content preservation approach are well-supported by ablation studies and theoretical grounding in GAN literature
- **Medium confidence**: The model-guided statistics alignment shows measurable improvements but lacks direct corpus validation, and its contribution varies significantly across datasets
- **Medium confidence**: The claim of exceeding white-box attacks with only 1% authorized data is well-supported on tested datasets but may not generalize to all domain shift scenarios

## Next Checks
1. Test on real-world domain shift scenarios such as medical imaging datasets (different hospitals/scanners) or autonomous driving datasets with sensor variation
2. Measure robustness to black-box constraints by systematically evaluating performance when only hard labels are available and when different zero-order gradient estimation methods are used
3. Analyze transferability to other model architectures by testing whether disguising models trained on one NTL architecture transfer to different NTL implementations