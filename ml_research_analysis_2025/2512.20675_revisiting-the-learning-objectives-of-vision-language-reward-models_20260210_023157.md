---
ver: rpa2
title: Revisiting the Learning Objectives of Vision-Language Reward Models
arxiv_id: '2512.20675'
source_url: https://arxiv.org/abs/2512.20675
tags:
- reward
- view
- learning
- language
- objectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates learning objectives for vision-language
  reward models, aiming to learn generalizable reward functions without human supervision
  by leveraging contrastive vision-language models (VLMs). The authors systematically
  compare recent learning objectives under a unified framework, controlling for model
  architecture, training data, and evaluation environments.
---

# Revisiting the Learning Objectives of Vision-Language Reward Models

## Quick Facts
- arXiv ID: 2512.20675
- Source URL: https://arxiv.org/abs/2512.20675
- Authors: Simon Roy; Samuel Barbeau; Giovanni Beltrame; Christian Desrosiers; Nicolas Thome
- Reference count: 8
- Primary result: Simple triplet loss outperforms complex temporal contrastive objectives for vision-language reward modeling on Meta-World tasks

## Executive Summary
This paper systematically compares learning objectives for vision-language reward models under a unified framework, controlling for model architecture, training data, and evaluation environments. Using Meta-World tasks, the authors evaluate reward model performance by measuring consistency with ground truth reward and correlation with expert progress. Surprisingly, a simple triplet loss outperforms more complex state-of-the-art methods, suggesting that recent improvements may be attributed to differences in data and model architectures rather than the learning objectives themselves.

## Method Summary
The authors compare four learning objectives (Triplet, TCN, VIP, and LIV) for vision-language reward models using a unified framework with identical SigLIP2 backbone, LoRA adapters, and Meta-World training data. All methods learn to map language goal descriptions and image observations to similarity scores that indicate task progress. The models are trained on 45 Meta-World tasks with 3 camera views per trajectory, then evaluated on held-out tasks using consistency accuracy against ground truth rewards and Value-Order Correlation (VOC) with expert progress. The key difference lies in the loss functions: triplet uses simple margin-based ranking, while TCN, VIP, and LIV employ more complex temporal contrastive objectives.

## Key Results
- Triplet loss achieves 68.88% average accuracy across held-out tasks, outperforming R3M (66.40%) and VIP variants (56.10%)
- All methods show high accuracy on seen tasks (>80%) but performance drops on held-out tasks, with triplet maintaining the highest overall accuracy
- VIP and LIV objectives show instability during training, requiring 10× lower learning rate (1e-5 vs 1e-4)
- All methods fail on compositional multi-step tasks like door-open, with VOC scores near 10%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple ranking-based objectives can outperform complex temporal contrastive objectives for vision-language reward modeling.
- Mechanism: The triplet loss treats language goal embedding as anchor, later-frame image embedding as positive, and earlier-frame embedding as negative. This directly encodes the ordinal assumption that task progress correlates with increased similarity to the goal description, without requiring temporal distance modeling.
- Core assumption: Expert trajectories monotonically approach the language-described goal state.
- Evidence anchors:
  - [abstract] "a simple triplet loss outperforms state-of-the-art methods"
  - [section 4, Table 1] Triplet achieves 68.88% average accuracy vs. 66.40% for R3M and 56.10% for VIP variants
  - [corpus] No direct corpus validation; related work (RoboReward) explores VLM reward models but doesn't compare triplet losses
- Break condition: Tasks with non-monotonic progress (e.g., retreat-and-approach strategies) or subgoals contradicting final goal description would violate the ordinal assumption.

### Mechanism 2
- Claim: Complex multi-component losses (TCN+language, VIP+language+InfoNCE) introduce optimization conflicts that degrade reward quality.
- Mechanism: LIV combines L_VIP + L_VIP^text + InfoNCE, each pulling representations in different directions. VIP pushes temporally adjacent frames apart while pulling distant frames together—counterintuitive for progress estimation. The paper notes LIV required 10× lower learning rate (1e-5 vs 1e-4) due to instability.
- Core assumption: Multi-objective optimization yields representations that satisfy all constraints simultaneously.
- Evidence anchors:
  - [section 2] VIP "separates embeddings of temporally adjacent images while bringing together embeddings of images that are farther apart"
  - [appendix A.1] "LIV objectives suffered from instability and required a learning rate of 1e-5"
  - [corpus] VARP paper notes "subtle misalignments or reward hacking" in complex reward design, consistent with optimization conflict hypothesis
- Break condition: If tasks require fine-grained temporal reasoning (e.g., distinguishing sub-second motion phases), single-objective triplet loss may underfit temporal structure.

### Mechanism 3
- Claim: Pre-trained VLM backbone quality dominates downstream reward model performance more than objective function choice.
- Mechanism: SigLIP2 provides strong vision-language alignment from web-scale pretraining. Fine-tuning with LoRA adapts representations while preserving semantic knowledge. All methods shared identical backbone, yet performance variance was ~16 percentage points (52-69% accuracy), suggesting objective matters but backbone quality is foundational.
- Core assumption: Pre-trained VLM encodes task-relevant visual concepts transferable to robotic manipulation.
- Evidence anchors:
  - [abstract] "much of the improvements in recent approaches could be attributed to differences in data and architectures"
  - [section 4] Frozen SigLIP2 baseline achieves ~47% accuracy (below random), confirming finetuning necessity
  - [corpus] FedVLMBench and domain generalization surveys show backbone quality correlates with transfer performance across domains
- Break condition: Novel visual domains (infrared, surgical scenes) lacking representation in VLM pretraining data would require domain-adaptive pretraining before reward modeling.

## Foundational Learning

- Concept: Contrastive learning fundamentals (anchor/positive/negative sampling, margin-based separation)
  - Why needed here: Understanding why triplet loss works requires grasping how embedding space geometry encodes similarity rankings
  - Quick check question: Can you explain why increasing margin α in triplet loss would affect false-negative tolerance?

- Concept: Vision-language model architectures (dual-encoder vs fused, embedding alignment objectives)
  - Why needed here: SigLIP2 uses sigmoid loss instead of contrastive; understanding this informs why LoRA fine-tuning is appropriate
  - Quick check question: What's the difference between CLIP's contrastive pretraining and SigLIP's sigmoid-based approach?

- Concept: Reward model evaluation in RL (dense vs sparse rewards, policy alignment metrics)
  - Why needed here: VOC metric measures correlation between predicted rewards and expert progress—different from accuracy on random pairs
  - Quick check question: Why might high accuracy on random pairs fail to translate to good policy learning signals?

## Architecture Onboarding

- Component map:
  - SigLIP2 ViT backbone (frozen) -> LoRA adapters (r=16, α=32) -> Text/vision encoders -> Cosine similarity function -> Triplet/TCN/VIP/LIV loss computation

- Critical path:
  1. Load SigLIP2 weights → 2. Initialize LoRA adapters (r=16, α=32) → 3. Sample trajectory batches with language labels → 4. Compute embeddings for [I_i, I_j, l] → 5. Apply triplet loss → 6. Update LoRA weights only

- Design tradeoffs:
  - LoRA rank (r=16): Lower risks underfitting; higher risks catastrophic forgetting of VLM semantics
  - Negative sampling strategy (k=-3): Paper uses "Negatives (all methods) −3"—assumed to mean sampling negatives from 3 timesteps prior; more aggressive negatives increase hardness but may violate semantic similarity
  - Multi-view averaging: Improves robustness (+1-2% accuracy) but increases inference cost 3×

- Failure signatures:
  - Negative VOC scores (e.g., VIP on button-press View 1: -21.03%): Indicates reward decreases along expert trajectories—objective is inverted
  - High variance across views: Suggests overfitting to camera-specific features rather than task-relevant semantics
  - Near-50% accuracy on held-out tasks: Model learned dataset-specific shortcuts, not transferable progress indicators

- First 3 experiments:
  1. Reproduce triplet vs TCN comparison on single held-out task (button-press) with frozen random seed—verify 75.36% vs 69.63% accuracy gap
  2. Ablate margin α ∈ {0.1, 0.3, 0.5, 0.7} to find sensitivity—paper uses 0.3 without justification
  3. Test on compositional task (door-open) with checkpoint analysis—determine where all methods collapse (VOC ~10%) and whether this is architecture or data limited

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ranking-based objectives like triplet loss maintain their advantage when scaled to larger, more diverse video datasets beyond the 50k samples used in this study?
- Basis in paper: [explicit] The authors state in future work: "scaling to larger datasets" as a key direction, noting their experiments used only 50k samples from 132 trajectories across 47 Meta-World tasks.
- Why unresolved: The controlled experiments were conducted on a relatively small dataset; it remains unclear whether the triplet loss simplicity advantage persists with orders of magnitude more training data.
- What evidence would resolve it: Systematic comparison of learning objectives on datasets like Ego4D or Epic-Kitchens with controlled architectures.

### Open Question 2
- Question: Why do all evaluated objectives fail on compositional multi-step tasks like door-open, and what architectural or objective modifications could address this limitation?
- Basis in paper: [explicit] The authors note: "All methods collapse on door-open, indicating a need for methods more adapted to multi-step tasks." Door-open requires navigation followed by manipulation.
- Why unresolved: The paper identifies the failure mode but does not investigate whether this stems from temporal reasoning limitations, credit assignment, or representation capacity.
- What evidence would resolve it: Analysis of embedding trajectories on multi-step tasks and evaluation of hierarchical or temporally-extended objectives.

### Open Question 3
- Question: What is the relative contribution of pre-training data versus model architecture versus learning objective to reward model performance?
- Basis in paper: [inferred] The paper concludes "much of the improvements in recent approaches could be attributed to differences in data and architectures," but did not directly vary these factors to quantify their contributions.
- Why unresolved: The unified framework held data and architecture constant to isolate objective effects, leaving the attribution to data/architecture as a hypothesis rather than a tested claim.
- What evidence would resolve it: Factorial experiments systematically varying backbone size, pre-training datasets, and objectives while measuring downstream performance.

## Limitations
- All methods fail on compositional multi-step tasks like door-open, with VOC scores near 10%, suggesting fundamental limitations in handling complex task structures
- Negative sampling strategy lacks precise specification ("Negatives (all methods) −3"), creating ambiguity in loss optimization
- VOC metric may not translate to effective RL policy learning despite measuring correlation with expert progress

## Confidence
- **High confidence**: Triplet loss outperforms complex methods on held-out Meta-World tasks (supported by direct experimental comparison across identical conditions)
- **Medium confidence**: Objective function choice matters less than backbone architecture/data quality (extrapolated from controlled ablation showing ~16 percentage point variance)
- **Low confidence**: Simple ranking-based objectives generalize to non-sequential or compositional tasks (door-open task performance <10% suggests severe limitations not fully characterized)

## Next Checks
1. Replicate VOC correlation analysis on non-sequential tasks (e.g., pick-and-place with random intermediate states) to test ordinal assumption violations
2. Implement cross-view generalization test: train on View 1, evaluate on held-out Views 2-3 to quantify camera-specific overfitting
3. Compare reward model pretraining on domain-specific data (surgical/industrial) versus web-scale VLM to quantify backbone quality hypothesis