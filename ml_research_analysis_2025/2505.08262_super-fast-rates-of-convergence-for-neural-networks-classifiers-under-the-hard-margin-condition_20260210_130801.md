---
ver: rpa2
title: Super-fast rates of convergence for Neural Networks Classifiers under the Hard
  Margin Condition
arxiv_id: '2505.08262'
source_url: https://arxiv.org/abs/2505.08262
tags:
- neural
- which
- sign
- networks
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Super-fast rates of convergence for Neural Networks Classifiers under the Hard Margin Condition

## Quick Facts
- arXiv ID: 2505.08262
- Source URL: https://arxiv.org/abs/2505.08262
- Authors: Nathanael Tepakbong; Ding-Xuan Zhou; Xiang Zhou
- Reference count: 10
- Key outcome: Establishes excess risk convergence rates of $\mathcal{O}(n^{-\alpha})$ for arbitrarily large $\alpha > 0$ for DNN classifiers under hard margin conditions

## Executive Summary
This paper proves that deep neural network classifiers can achieve super-fast convergence rates under the hard margin condition, a strong form of Tsybakov's low-noise condition. The authors develop a novel excess risk decomposition and show that with appropriate architecture scaling (width $W_n = \tilde{\mathcal{O}}(n^{\alpha d/2s})$ and depth $L_n = O(\log L_0)$) and $\ell_p$ regularization, DNNs can achieve excess risk bounds of $\mathcal{O}(n^{-\alpha})$ for arbitrarily large $\alpha$. This improves upon prior results that were limited to $O(n^{-1})$ rates for DNNs. The key assumptions are that the regression function $\eta$ satisfies the hard margin condition and is sufficiently smooth.

## Method Summary
The paper analyzes binary classification with square loss surrogate and $\ell_p$ penalty on weights. It uses fully connected ReLU networks with clipped outputs, minimizing empirical risk with square loss. The theoretical analysis employs a novel decomposition of excess risk into approximation, penalty, and statistical error terms. The key technical contributions include deriving covering number bounds for the network hypothesis space using a Lipschitz constant of the realization map, and establishing that under hard margin and smoothness assumptions, the approximation error can decrease exponentially fast with network size. The architecture is explicitly scaled with sample size $n$ and smoothness $s$ to balance approximation and estimation errors.

## Key Results
- Establishes excess risk convergence rates of $\mathcal{O}(n^{-\alpha})$ for arbitrarily large $\alpha > 0$ under hard margin conditions
- Proves these super-fast rates using a novel decomposition of excess risk
- Shows explicit scaling laws for network width and depth: $W_n = \tilde{\mathcal{O}}(n^{\alpha d/2s})$, $L_n = O(\log L_0)$
- Demonstrates that hard margin condition (limit of Tsybakov's low-noise as $q \to \infty$) enables the super-fast rates

## Why This Works (Mechanism)

### Mechanism 1: Novel Excess Risk Decomposition
The paper introduces a decomposition of excess risk into approximation error, penalty error, and statistical error. This allows separate analysis of each term under specific regularity and margin conditions. The statistical error term is bounded using covering numbers and a Lipschitz constant of the network's realization map, yielding an exponential bound. This decomposition is the key to proving super-fast rates and is explicitly shown in the proof of Theorem 1.

### Mechanism 2: Hard Margin and Smoothness Enable Super-Fast Rates
The hard margin condition ensures data points are a positive distance from the decision boundary, combined with smoothness of the regression function $\eta$. This allows the approximation error to decrease rapidly with network size. The paper shows that by scaling network depth and width appropriately with $n$, the overall excess risk can decrease at an arbitrarily fast polynomial rate. The convergence exponent $\alpha$ is proportional to smoothness $s$, leading to the super-fast rate.

### Mechanism 3: Architecture Scaling and Regularization
The theoretical rates are achieved by explicitly scaling the neural network's width and depth with sample size $n$ and applying $\ell_p$ regularization. The proof constructs networks with width $W_n = \tilde{\mathcal{O}}(n^{\alpha d/2s})$ and depth $L_n = O(\log L_0)$. This scaling ensures approximation error and statistical error balance to yield the super-fast rate. The $\ell_p$ penalty on weights, along with clipping network outputs, controls function space complexity.

## Foundational Learning

- **Concept: Tsybakov's Low-Noise / Hard Margin Condition**
  - Why needed here: This is the central structural assumption about data distribution, quantifying how "separated" the two classes are. The "hard margin" is the idealized limit of this condition and is key to unlocking super-fast convergence rates.
  - Quick check question: Can you explain the difference between the general Tsybakov low-noise condition with exponent $q$ and the hard margin condition ($q \to \infty$)? How does this relate to the distance of data points from the decision boundary?

- **Concept: Excess Risk and Convergence Rates**
  - Why needed here: This is the primary performance metric. Understanding the difference between standard rate (e.g., $O(n^{-1/2})$), "fast" rate (e.g., $O(n^{-1})$), and "super-fast" rates proved here ($O(n^{-\alpha})$ for any $\alpha$) is essential to grasp the paper's contribution.
  - Quick check question: Define the excess risk of a classifier. What does it mean for a classifier's risk to converge at a rate of $\mathcal{O}(n^{-\alpha})$? Why is a larger $\alpha$ considered "faster"?

- **Concept: Approximation vs. Estimation Error Trade-off**
  - Why needed here: The core of the proof involves balancing these two errors. Approximation error (from using finite network to represent $\eta$) decreases as network grows, while estimation error (from learning from finite data) can increase with model complexity.
  - Quick check question: In the context of learning theory, what is the trade-off between approximation error and estimation error? How does increasing the network size affect each of these terms?

## Architecture Onboarding

- **Component map**: Fully connected ReLU networks with $\ell_p$ weight penalty -> Clipping Layer (applied at output to bound supremum norm) -> Realization Map $F_\sigma$ (function mapping network parameters to final classifier, Lipschitz constant is key complexity measure) -> Covering Number (used to measure complexity of hypothesis class for generalization bounds)

- **Critical path**: Assume hard margin and smooth $\eta$ -> choose appropriate network architecture (depth $L_n$, width $W_n$) based on sample size $n$ and smoothness $s$ -> perform $\ell_p$-regularized Empirical Risk Minimization with square loss -> minimum-norm solution has excess risk bound of $\mathcal{O}(n^{-\alpha})$

- **Design tradeoffs**: Primary tradeoff is between required network size and achieved convergence rate. Larger desired exponent $\alpha$ (faster convergence) requires larger width $W_n$, increasing model complexity and computational cost. Choice of $p$ in $\ell_p$ penalty affects norm bound but main results hold for any $0 < p < \infty$.

- **Failure signatures**:
  - Slow/no convergence: If data doesn't satisfy hard margin condition (e.g., high label noise near boundary), theoretical guarantees vanish and expect slower convergence
  - Overfitting on small data: If network is too large for given sample size $n$ compared to smoothness $s$, estimation error term could dominate, leading to poor generalization
  - Optimization failure: Theory assumes optimizer finds minimum-norm global minimizer. In practice, gradient-based methods might fail to find such solution in non-convex loss landscape

- **First 3 experiments**:
  1. **Synthetic Data Validation**: Generate 2D binary classification dataset with clear hard margin and smooth decision boundary. Train fully connected ReLU networks of increasing depth/width per paper's scaling laws. Plot excess risk vs sample size $n$ on log-log plot to verify slope corresponds to predicted super-fast rate.
  2. **Ablation on Margin**: Using same setup, inject controlled label noise near decision boundary to violate hard margin condition. Compare convergence rate to "clean" case to demonstrate degradation in performance, now bounded by noise exponent $q$.
  3. **Loss Function Comparison**: On dataset satisfying hard margin, compare performance of square loss (as used in paper) against standard cross-entropy loss. Tests paper's conjecture that square loss is theoretically well-motivated for this regime and compares to common practice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can super-fast convergence rates be established for DNN classifiers when using hinge loss or cross-entropy loss instead of square loss?
- Basis in paper: [explicit] The authors state their analysis "crucially relies on the properties of the square loss surrogate" and extending these results to losses like hinge loss or cross-entropy is "an interesting avenue of future research."
- Why unresolved: Current theoretical decomposition and proof techniques depend specifically on mathematical properties of square loss.
- What evidence would resolve it: A proof of convergence rates of order $\mathcal{O}(n^{-\alpha})$ under hard-margin conditions for classifiers trained with non-square surrogate losses.

### Open Question 2
- Question: Can these super-fast rates be generalized to sparse architectures, such as deep Convolutional Neural Networks (CNNs)?
- Basis in paper: [explicit] The authors note state-of-the-art results often use sparse architectures like CNNs and explicitly ask if similar rates can be established for them as their approximation theory becomes better understood.
- Why unresolved: Paper focuses solely on fully connected neural networks, leaving application to function spaces and complexity measures of sparse networks unexplored.
- What evidence would resolve it: Derivation of excess risk bounds for CNN hypothesis spaces that demonstrate same $\mathcal{O}(n^{-\alpha})$ dependency.

### Open Question 3
- Question: Is Assumption (A5), which requires depth scaling to be $O(\sqrt{s})$ rather than $O(s^2)$, necessary to achieve super-fast rates?
- Basis in paper: [inferred] Authors introduce Assumption (A5) to replace worse dependency found in standard approximation theorems, describing it as requiring regression function to be "nicer" than worst-case $C^s$ function.
- Why unresolved: It is unstated whether super-fast rates hold for general $C^s$ functions or only for those satisfying this specific, tighter depth-approximation condition.
- What evidence would resolve it: A proof that achieves super-fast rates utilizing standard $O(s^2)$ depth scaling, or a counter-example showing such rates are impossible without stricter assumption.

## Limitations
- Theoretical assumptions (hard margin, smoothness of Î·, well-separation of minimizers) are rarely verifiable in practice and may not hold on real-world data
- The theory assumes the optimization algorithm finds a minimum-norm global minimizer, which is not addressed for practical gradient-based methods
- No empirical results are provided to demonstrate that theoretical rates can be achieved in practice

## Confidence
- **High Confidence**: Mathematical derivation of novel excess risk decomposition and subsequent bound under stated assumptions; construction of network architecture and theoretical justification
- **Medium Confidence**: Claim that hard margin condition is key to unlocking super-fast rates; well-supported by theory but requires strong assumptions
- **Low Confidence**: Practical applicability of results; without experiments or discussion of optimization gap, unclear if rates are achievable in practice

## Next Checks
1. **Synthetic Data Experiment**: Generate synthetic data satisfying hard margin condition and smoothness assumption. Train networks with prescribed architecture scaling and verify convergence rate empirically matches theoretical prediction.
2. **Assumption Violation Study**: Repeat experiment with controlled amount of label noise near decision boundary (violating hard margin). Quantify degradation in convergence rate to understand impact of key assumption.
3. **Optimization Robustness**: Implement method to approximate minimum-norm solutions (e.g., early stopping with small regularization). Test if theoretical rates are robust to choice of optimizer and its hyperparameters.