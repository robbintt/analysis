---
ver: rpa2
title: 'ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations
  with LLMs'
arxiv_id: '2510.26178'
source_url: https://arxiv.org/abs/2510.26178
tags:
- legal
- case
- retrieval
- reasoning
- issues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of legal case retrieval (LCR)
  by proposing ReaKase-8B, a framework that integrates legal entity relations and
  reasoning into case embeddings. The method extracts legal facts, issues, judgments,
  and relation triplets, then generates legal reasoning via an LLM to produce structured,
  knowledge-enhanced case representations.
---

# ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs

## Quick Facts
- **arXiv ID:** 2510.26178
- **Source URL:** https://arxiv.org/abs/2510.26178
- **Reference count:** 40
- **Key result:** Achieves state-of-the-art NDCG@5 scores of 55.6 and 52.5 on COLIEE 2022/2023 datasets

## Executive Summary
ReaKase-8B introduces a novel framework for legal case retrieval that integrates legal entity relations and reasoning into case embeddings. The method extracts structured legal elements (facts, issues, judgments, relation triplets) and generates explicit legal reasoning via an LLM, then fine-tunes an embedding model using contrastive learning with hard negative samples. Experiments on COLIEE 2022 and 2023 datasets demonstrate significant performance improvements over existing baselines, with NDCG@5 scores reaching 55.6 and 52.5 respectively.

## Method Summary
The framework extracts legal facts (summarized by GPT-5), issues (identified via placeholder heuristics), judgments, and relation triplets using open-source NER tools. An LLM generates ~100-word legal reasoning from these elements. The contextualized case encoding combines facts, issues, triplets, and reasoning (excluding judgment) into a structured prompt for the embedding model. Contrastive learning fine-tunes the Qwen3-Embedding-8B model with positive pairs, easy negatives, and hard negatives (high BM25 scores but not ground-truth relevant), using LoRA for efficient adaptation.

## Key Results
- Achieves NDCG@5 scores of 55.6 on COLIEE 2022 and 52.5 on COLIEE 2023 datasets
- Outperforms existing baselines including LLM-based and traditional legal retrieval models
- Demonstrates Micro-F1 of 27.0 and 28.7 on COLIEE 2022 and 2023 respectively
- Shows significant improvement through ablation studies confirming contribution of each component

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating structured legal entity relations into case embeddings enhances retrieval accuracy by capturing latent structural information absent from raw text.
- **Mechanism:** The framework extracts legal relation triplets (head entity, relation, tail entity) from facts and issues using open-source NER and relation extraction tools. These triplets are concatenated with other case elements into a structured prompt for the embedding model, providing explicit semantic and structural signals that improve case distinctiveness and similarity matching.
- **Core assumption:** The extracted triplets accurately reflect meaningful legal relationships and that their inclusion in the prompt leads to better semantic representations for retrieval.
- **Evidence anchors:**
  - [abstract] "...integrates legal entity relations and reasoning into case embeddings."
  - [section 3.2] "...yield the triplet (claimant, filed, an appeal). Formally, a case is represented as a set of relational triplets..."
  - [corpus] Weak direct evidence; neighbor papers discuss structured legal knowledge in RAG but not this specific triplet-mechanism for retrieval embeddings.
- **Break condition:** The mechanism's effectiveness is undermined if the underlying NER/relation extraction tools produce noisy or irrelevant triplets, or if the base embedding model cannot effectively leverage the structured prompt format.

### Mechanism 2
- **Claim:** Generating and embedding explicit legal reasoning (the inferential path from facts to judgment) improves retrieval by aligning case representations with judicial decision-making patterns.
- **Mechanism:** An LLM (GPT-5) is prompted with extracted facts, issues, and judgment to generate a ~100-word legal reasoning. This reasoning text is included in the contextualized prompt for the embedding model, forcing the encoder to represent the logical pathway of the case, thereby prioritizing cases with similar reasoning patterns over mere textual or statute overlap.
- **Core assumption:** The generated reasoning is a faithful and useful proxy for the actual judicial reasoning process, and its presence in the prompt guides the embedding model to create more relevant representations.
- **Evidence anchors:**
  - [abstract] "...integrates... the crucial reasoning process that uncovers how legal facts and legal issues can lead to judicial decisions."
  - [section 4.4 Case Study] "This shows that reasoning alignment captures substantive relevance while filtering out statute-only matches."
  - [corpus] Related work (e.g., CLaw, LegalSearchLM) underscores the importance of legal reasoning in LLM tasks, supporting the premise.
- **Break condition:** The mechanism fails if the LLM-generated reasoning is hallucinated, overly generic, or inconsistent with the case's actual legal logic, potentially misleading the embedding model.

### Mechanism 3
- **Claim:** Fine-tuning an embedding model with contrastive learning, using hard negative samples, creates a more discriminative case representation space.
- **Mechanism:** The model is trained with a contrastive loss that pulls embeddings of query-positive pairs closer and pushes query-negative pairs apart. Critically, hard negatives are cases with high BM25 scores but are not ground-truth relevant, explicitly teaching the model to distinguish superficially similar but legally irrelevant cases.
- **Core assumption:** The hard negative mining strategy effectively surfaces challenging and informative examples that refine the model's decision boundary.
- **Evidence anchors:**
  - [abstract] "A contrastive learning approach is used to fine-tune an embedding model, resulting in improved semantic understanding."
  - [section 3.4] "...hard negative samples are introduced by retrieving cases with high BM25 relevance scores that are not labelled as ground-truth matches..."
  - [corpus] Contrastive learning is a standard technique in embedding model fine-tuning, with neighbor papers applying similar principles in legal domains.
- **Break condition:** The mechanism may fail if the hard negatives are not truly "hard" (i.e., are easily distinguishable), providing limited learning signal, or if the mining process is biased by the lexical retriever (BM25) in a way that conflicts with the desired semantic understanding.

## Foundational Learning

- **Concept: Contrastive Learning**
  - **Why needed here:** This is the core training paradigm used to fine-tune the embedding model. Understanding how positive and negative pairs shape the embedding space is essential to grasp how ReaKase-8B learns to separate relevant from irrelevant cases.
  - **Quick check question:** In the contrastive loss equation, what is the effect of increasing the temperature parameter τ?

- **Concept: Legal Element Extraction**
  - **Why needed here:** The performance of the entire framework hinges on the quality of extracted elements (facts, issues, triplets). Understanding the extraction logic and its limitations is key to diagnosing system failures.
  - **Quick check question:** According to the paper, how are legal issues identified in the COLIEE datasets?

- **Concept: In-Context Learning / Prompt Engineering**
  - **Why needed here:** The method relies heavily on structured prompts to (1) generate legal reasoning with an LLM and (2) contextualize the input for the embedding model. The prompt design directly influences the quality of downstream components.
  - **Quick check question:** What components are excluded from the contextualized case encoding prompt for the embedding model, and why?

## Architecture Onboarding

- **Component map:**
  Preprocessing -> Legal Element Extraction (Facts, Issues, Judgments, Triplets) -> Legal Reasoning Generation -> Contextualized Encoder (Qwen3-Embedding-8B) -> Contrastive Fine-tuning

- **Critical path:** Quality of Extracted Elements (Facts/Issues/Triplets) → Quality of Generated Reasoning → Quality of Contextualized Prompt → Quality of Learned Embedding → Retrieval Accuracy.

- **Design tradeoffs:**
  - **Generation Cost vs. Quality:** Using GPT-5 for fact summarization and reasoning generation may improve quality but introduces dependency on a proprietary model and associated costs/latency.
  - **Hard Negative Mining:** Relying on BM25 for hard negatives is computationally cheaper than embedding-based mining but may inherit lexical biases. The choice of `m` (number of hard negatives) trades off training difficulty with computational load.
  - **Prompt Complexity:** The contextualized prompt is designed to be information-rich but must fit within the model's 2048-token limit, potentially truncating very long cases.

- **Failure signatures:**
  - **Noisy Extractions:** Failure of the triplet extraction tools leads to corrupted structural information in the prompt, degrading embedding quality.
  - **Superficial Hard Negatives:** If BM25 retrieves cases that are semantically very different (i.e., not "hard"), the contrastive learning signal is weakened.
  - **Reasoning Hallucination:** The GPT-5 generated reasoning may contain logical errors or irrelevancies, actively harming the embedding's representation of the case's true logic.

- **First 3 experiments:**
  1. **Component Ablation:** Re-run the full pipeline with one component removed at a time (e.g., no triplets, no reasoning) to validate each contribution on a held-out set. This directly replicates the paper's RQ2 (Section 4.2).
  2. **Prompt Sensitivity Analysis:** Test the impact of the prompt template by comparing the default prompt against variations (e.g., Prompt 1 and Prompt 2 from Section 4.3) to ensure robustness and identify optimal prompting strategies.
  3. **Hard Negative Analysis:** Vary the number (`m`) and selection strategy (e.g., BM25 vs. random vs. embedding-based) of hard negatives to understand their impact on training convergence and final performance, informing the tradeoff around this critical design choice.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can ReaKase-8B effectively generalize to civil law jurisdictions or languages other than English without extensive re-engineering?
  - **Basis in paper:** [explicit] The paper states: "Beyond English, ReaKase-8B can be readily extended to multilingual legal systems by integrating domain-specific information extraction pipelines..."
  - **Why unresolved:** The current experiments are restricted to the COLIEE datasets (Federal Court of Canada), and the method relies on common law reasoning structures.
  - **What evidence would resolve it:** Evaluation of the framework on legal case retrieval benchmarks from civil law jurisdictions (e.g., Japanese, Chinese, or German law) or non-English datasets.

- **Open Question 2:** How robust is the retrieval performance when the LLM-generated legal reasoning contains hallucinations or logical errors?
  - **Basis in paper:** [inferred] The framework relies on GPT-5 to generate reasoning (`c_Reason`), but provides no analysis on the quality of this synthetic data or the model's sensitivity to noise.
  - **Why unresolved:** While the paper shows that adding reasoning helps, it does not investigate whether incorrect reasoning hurts performance more than the absence of reasoning.
  - **What evidence would resolve it:** An ablation study where varying degrees of synthetic noise or logical contradictions are injected into the generated reasoning text to observe performance degradation.

- **Open Question 3:** Is the extraction pipeline transferable to legal corpora that lack the specific structural artifacts used to identify issues?
  - **Basis in paper:** [inferred] The extraction of legal issues (`c_Issue`) explicitly relies on the presence of the `[PH]` placeholder (e.g., `FRAGMENT_SUPPRESSED`).
  - **Why unresolved:** The method depends on a dataset-specific convention for identifying cited precedents in the analysis section, which may not exist in other legal document formats.
  - **What evidence would resolve it:** Testing the extraction module on raw legal documents from jurisdictions that do not use such explicit citation placeholders for issue identification.

## Limitations

- The framework's performance depends critically on proprietary LLM components (GPT-5) that are not publicly available, introducing uncertainty about reproducibility with alternative models.
- The contrastive learning framework relies on BM25 for hard negative mining, which may introduce lexical retrieval biases that conflict with the desired semantic understanding.
- The 2048-token limit for the contextualized prompt may truncate very long legal cases, potentially affecting embedding quality, though the paper does not quantify this impact.

## Confidence

**High Confidence:** The architectural framework combining structured legal element extraction, LLM-generated reasoning, and contrastive fine-tuning is well-specified and theoretically sound. The core experimental design and evaluation metrics are clearly defined.

**Medium Confidence:** The reported performance improvements over baselines (NDCG@5 scores of 55.6/52.5 and Micro-F1 of 27.0/28.7) are credible given the SOTA results, but the dependence on GPT-5 for key components introduces reproducibility uncertainty. The ablation study results showing contribution margins for each component are convincing but may not generalize if component quality differs with alternative tools.

**Low Confidence:** The specific hyperparameter choices (learning rate, weight decay, temperature τ, similarity function) are not fully specified, and the paper does not provide error analysis on when the method fails or how sensitive results are to prompt variations.

## Next Checks

1. **Component Ablation Validation:** Reproduce the ablation study by systematically removing each component (triplets, reasoning, contrastive fine-tuning) from the pipeline and measuring performance degradation on a held-out validation set. This validates whether the claimed contribution margins (10-20% NDCG improvement) hold with alternative LLM implementations.

2. **Hard Negative Mining Robustness:** Conduct controlled experiments varying the hard negative selection strategy (BM25 vs. embedding-based vs. random) and the number of hard negatives per query (m=1, 3, 5) to quantify the impact on training stability and final performance, addressing the tradeoff between computational cost and learning signal quality.

3. **Prompt Sensitivity Analysis:** Test the robustness of the embedding model to prompt variations by systematically modifying the contextualized prompt template (e.g., reordering components, omitting specific elements) and measuring the impact on retrieval performance, ensuring the reported results are not artifacts of a particular prompt structure.