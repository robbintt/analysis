---
ver: rpa2
title: A Survey on Multi-Turn Interaction Capabilities of Large Language Models
arxiv_id: '2501.09959'
source_url: https://arxiv.org/abs/2501.09959
tags:
- multi-turn
- llms
- language
- association
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of multi-turn interaction
  capabilities of large language models (LLMs). It systematically reviews the current
  landscape of how LLMs maintain context and engage users across multiple dialogue
  turns.
---

# A Survey on Multi-Turn Interaction Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2501.09959
- Source URL: https://arxiv.org/abs/2501.09959
- Authors: Chen Zhang; Xinyi Dai; Yaxiong Wu; Qu Yang; Yasheng Wang; Ruiming Tang; Yong Liu
- Reference count: 40
- Primary result: Comprehensive survey of multi-turn interaction capabilities of large language models (LLMs)

## Executive Summary
This paper provides a comprehensive survey of multi-turn interaction capabilities of large language models (LLMs). It systematically reviews the current landscape of how LLMs maintain context and engage users across multiple dialogue turns. The survey covers four key aspects: how multi-turn interactions are evaluated, the core model capabilities essential for effective interactions, algorithms used to enhance these capabilities, and potential future research directions. The authors categorize and analyze various model-specific skills such as multi-turn instruction following, context memory management, planning, and reasoning. They also examine different evaluation frameworks and benchmarks used to assess these capabilities, including MT-Bench, MT-Eval, and specialized benchmarks for math and code reasoning.

## Method Summary
The paper conducts a comprehensive literature review of multi-turn interaction capabilities in LLMs. It synthesizes findings from various research papers, benchmarks, and evaluation frameworks. The survey examines evaluation methods including LLM-as-a-Judge frameworks, core capabilities like context memory and planning, and enhancement algorithms such as ArCHer (hierarchical RL) and MTPO (multi-turn preference optimization). The authors analyze multiple benchmarks including MT-Bench, MT-Bench-101, Multi-IF, MathChat-Bench, API-Bank, MINT, and AgentBench. The methodology focuses on categorizing and comparing existing approaches rather than presenting original experimental results.

## Key Results
- Multi-turn RL algorithms like ArCHer and MTPO extend single-turn RLHF by optimizing over complete conversation trajectories
- LLM-as-a-Judge has become the standard approach for scalable multi-turn evaluation, showing high correlation with human assessments
- Current challenges include long-term memory retention, strategic reasoning across multiple turns, and reliable multi-turn preference data
- Key capabilities identified include multi-turn instruction following, context memory management, planning, and reasoning

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Credit Assignment in Multi-Turn RL
- Claim: Separating high-level conversation-value estimation from low-level token generation enables more effective long-horizon optimization than single-turn RL
- Mechanism: ArCHer uses hierarchical structure with high-level off-policy value-based RL to aggregate sparse rewards across multiple turns, while low-level policy gradient trains token-level policies within each turn
- Core assumption: Long-term conversation utility can be learned as value function over dialogue states, and this signal can guide token-level generation meaningfully
- Evidence anchors: "ArCHer employs a hierarchical RL strategy with two parallel algorithms: a high-level off-policy value-based RL to aggregate rewards across multiple utterances, and a low-level policy gradient RL that leverages the high-level value function to train token-level policies within each turn"

### Mechanism 2: Multi-Turn Preference Optimization (MTPO)
- Claim: Optimizing over complete conversation trajectories captures long-term planning dependencies
- Mechanism: MTPO extends RLHF by using preference-based Q-function that evaluates actions based on contribution to entire conversation outcomes, not just immediate turns
- Core assumption: Reliable multi-turn preference data exists or can be synthesized, and Q-function can accurately model how single turn influences eventual conversation success
- Evidence anchors: "MTPO utilizes a preference-based Q-function that accounts for the long-term effects of individual actions, incorporating preferences over entire conversations rather than just single-turn feedback"

### Mechanism 3: LLM-as-a-Judge for Scalable Evaluation
- Claim: Strong LLMs can approximate human judgment for multi-turn dialogue evaluation, enabling scalable benchmarking
- Mechanism: Capable LLM (e.g., GPT-4) prompted to rate or compare multi-turn dialogues, replacing expensive human evaluation
- Core assumption: Judge LLM's internal preference model aligns sufficiently with human notions of conversation quality
- Evidence anchors: "They report high correlations between the LLM-generated judgments and human assessments. Since then, 'LLM-as-a-Judge' has become a standard approach for evaluating user-LLM interactions"

## Foundational Learning

- Concept: **Credit Assignment Problem in RL**
  - Why needed here: Central to understanding why multi-turn RL is harder than single-turn; must grasp how algorithm attributes final reward back through sequence of actions to update policy effectively
  - Quick check question: An agent receives reward only after 10-turn conversation. How does it determine which specific turns contributed positively or negatively to that outcome?

- Concept: **Q-Function (Action-Value Function)**
  - Why needed here: Essential for Mechanism 1 & 2; Q-function Q(s, a) estimates expected future reward of taking action a in state s; core tool for long-term planning in these algorithms
  - Quick check question: If you have perfect Q-function for dialogue agent, how would you use it to select next response during conversation?

- Concept: **Reward Modeling & RLHF**
  - Why needed here: Provides foundation for how we train models to align with human preferences; mechanisms in this paper are advanced extensions of this core concept
  - Quick check question: In standard RLHF, reward model trained on human preference data. What is key limitation of applying this single-turn reward model directly to multi-turn conversation?

## Architecture Onboarding

- Component map: Agent Core -> Context/Memory Module -> Multi-Turn Reward/Evaluator -> Optimization Engine
- Critical path: 1) Define interaction patterns and collect multi-turn trajectory data 2) Get trajectory-level rewards from evaluator 3) Use multi-turn RL algorithm to update Agent Core 4) Evaluate on multi-turn benchmarks using LLM-as-a-Judge
- Design tradeoffs:
  - External vs. Internal Memory: External (RAG) is interpretable/modular but adds latency; Internal is faster but harder to train/debug
  - Judge Power vs. Cost: Stronger judge (GPT-4) is more accurate but expensive; weaker judge may introduce noise and bias
  - Algorithm Complexity: SFT is simple but short-sighted; Multi-turn RL is powerful but complex and can be unstable
- Failure signatures:
  - Reward Hacking: Agent learns to exploit judge biases rather than improving genuine conversation quality
  - Catastrophic Forgetting: Optimizing for multi-turn tasks degrades model's core language capabilities or single-turn performance
  - Context Drift: Agent loses track of information from earlier turns, leading to inconsistent or repetitive responses
- First 3 experiments:
  1. Baseline & Judge Calibration: Evaluate base LLM on MT-Bench-101 using GPT-4 as judge; manually review sample to identify judge biases
  2. Ablate Memory: Compare performance of base model with and without simple external memory module on long-context task
  3. Implement SFT on Multi-Turn Data: Fine-tune model on curated dataset of high-quality multi-turn dialogues before attempting complex RL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can high-quality data for multi-turn instruction following be systematically extracted and filtered from real user-LLM interaction logs?
- Basis in paper: Conclusion notes current datasets are often synthetic and biased, suggesting future research focus on "effectively mining and filtering high-quality data from real user-LLM interaction data"
- Why unresolved: Existing works rely heavily on prompting advanced LLMs for data curation, which fails to capture diverse linguistic styles and interaction patterns of real users
- What evidence would resolve it: Data pipeline that successfully mines diverse interaction patterns from large-scale real-user logs (e.g., WildChat) and yields models with superior generalization to real-world usage

### Open Question 2
- Question: What specific methods are required to calibrate LLM-based evaluators to mitigate biases and ensure robust evaluation of multi-turn interactions?
- Basis in paper: Section 5 states while "LLM-as-a-Judge" is standard, it introduces biases, necessitating research into "fine-tuning them with human-annotated data and implementing bias mitigation strategies"
- Why unresolved: Advanced LLM judges are known to favor certain models or response styles, potentially compromising fairness and accuracy of multi-turn benchmarks
- What evidence would resolve it: Development of evaluator that demonstrates reduced specific biases (e.g., length, positional) and higher correlation with human judgments compared to standard GPT-4 baselines

### Open Question 3
- Question: How can explicit and implicit user or environmental feedback be most effectively integrated into LLM's reflective process to guide next-turn generation?
- Basis in paper: Section 5 explicitly identifies "how to effectively incorporate user or environmental feedback to guide generation of next-turn response" as "intriguing research question"
- Why unresolved: Current models often struggle to recognize implicit cues like user confusion or dissatisfaction and lack robust mechanisms to adjust subsequent responses dynamically
- What evidence would resolve it: Agent architecture that dynamically adapts strategy mid-dialogue upon receiving negative feedback, leading to measurable improvements in task completion rates in long-horizon interactions

## Limitations

- Primary limitation is scope as literature review rather than presenting original experimental results
- Claims about algorithm performance are derived from cited papers rather than original experiments
- Rapid pace of LLM development means some discussed approaches may already be superseded
- Does not provide direct empirical validation of effectiveness of multi-turn RL algorithms

## Confidence

- **High Confidence**: Existence and descriptions of evaluation benchmarks (MT-Bench, MT-Bench-101, AgentBench) are well-established and widely used in community
- **Medium Confidence**: Proposed mechanisms for multi-turn RL (ArCHer, MTPO) are theoretically sound based on cited papers, but practical effectiveness depends heavily on implementation details and data quality
- **Low Confidence**: Claims about future research directions and unsolved challenges are inherently speculative and may not materialize as predicted

## Next Checks

1. Implement basic LLM-as-a-Judge pipeline using MT-Bench and verify correlation with human judgments on sample set to assess bias and reliability
2. Run controlled ablation studies comparing single-turn RLHF vs. multi-turn RL approaches on simple dialogue task to empirically validate credit assignment problem
3. Test memory retention across multiple turns by designing dialogue task that requires recalling information from turn 1 in turn 10, comparing base LLM vs. models with memory augmentation