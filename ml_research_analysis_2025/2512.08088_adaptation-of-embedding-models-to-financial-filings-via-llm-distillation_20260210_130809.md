---
ver: rpa2
title: Adaptation of Embedding Models to Financial Filings via LLM Distillation
arxiv_id: '2512.08088'
source_url: https://arxiv.org/abs/2512.08088
tags:
- queries
- retrieval
- positive
- training
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving embedding models
  for specialized domains like finance, where general-purpose models underperform.
  The core method introduces an iterative pipeline that uses an LLM to mine positive
  and negative examples from an unlabeled corpus, progressively refining a student
  retrieval model through contrastive learning.
---

# Adaptation of Embedding Models to Financial Filings via LLM Distillation

## Quick Facts
- arXiv ID: 2512.08088
- Source URL: https://arxiv.org/abs/2512.08088
- Reference count: 25
- Primary result: 27.7% improvement in MRR@5, 44.6% improvement in mean DCG@5 on 21,800 query-document pairs for financial filings retrieval

## Executive Summary
This paper introduces a novel iterative distillation pipeline that adapts general-purpose embedding models to specialized domains like financial filings. The method uses an LLM teacher to generate synthetic queries and judge retrieved passages, progressively mining harder positive and negative examples to fine-tune a student retriever through contrastive learning. By constraining training triples to within-document passages, the approach optimizes for the common RAG task of finding specific sections within long documents. Experiments demonstrate significant improvements in retrieval accuracy for 14 financial filing types, with most gains realized in the first iteration.

## Method Summary
The approach employs an iterative student-teacher loop where a base embedding model retrieves candidate passages for synthetic queries, an LLM evaluates these candidates to identify hard positives and negatives, and the student model is fine-tuned using triplet loss on the mined contrastive triples. Training is constrained to passages from the same document to optimize within-document retrieval accuracy. The pipeline iterates by using the updated student model to mine increasingly challenging examples. Synthetic queries are generated via InPars-style prompting with few-shot examples, and relevance is scored on a 1-4 ordinal scale by the LLM teacher.

## Key Results
- 27.7% average improvement in MRR@5 and 44.6% improvement in mean DCG@5 across 14 financial filing types
- Improved NDCG on 3 of 4 classes in the FinanceBench benchmark
- Gains primarily attributed to better handling of tables, domain-specific terminology, and semantic understanding
- Most benefits realized in the first iteration, with diminishing returns in subsequent iterations

## Why This Works (Mechanism)

### Mechanism 1
The iterative student-teacher loop progressively hardens the training signal by using an improving retriever to mine increasingly challenging contrastive examples. The pipeline starts with a base embedding model and in each iteration retrieves candidate passages for synthetic queries, with an LLM identifying hard positives and negatives to create new training triples. This differs from one-shot fine-tuning by interleaving retrieval-based mining with iterative retraining. If the student model's retrieval degrades catastrophically, it will fail to surface relevant candidates, starving the LLM judge of good examples and causing the training loop to collapse.

### Mechanism 2
Training with contrastive triples constrained to the same document forces the model to learn fine-grained, within-document discrimination. The method constructs triples where both chunks are sampled from the same source document, increasing the difficulty of the discrimination task as distractors are semantically proximate. This directly optimizes for the common RAG task of finding a specific section within a long document. For queries requiring synthesis across multiple documents, this constraint may limit the model's ability to learn cross-document relevance.

### Mechanism 3
Relevance distillation from an LLM encodes domain-specific semantic relationships absent from general-purpose pretraining. A large LLM assigns ordinal relevance scores to retrieved passages, allowing training with nuanced signals including "hard positives" (partially relevant) and "hard negatives" (somewhat relevant but not answers). This captures complex domain semantics like connecting "raising capital" to "notes payable." The LLM may introduce its own biases or hallucinations into the relevance judgments, which the authors attempt to mitigate by using a different commercial LLM for final evaluation.

## Foundational Learning

- **Bi-Encoder (Dense Retrieval)**: Encodes queries and passages into a shared vector space for fast similarity search. Quick check: How does a bi-encoder differ from a cross-encoder in terms of computational cost and accuracy, and why is a bi-encoder preferred for the initial retrieval step in a RAG pipeline?

- **Contrastive Learning with Triplet Loss**: The core learning objective builds (query, relevant_chunk, irrelevant_chunk) triples and uses triplet loss to push the query embedding closer to the relevant chunk. Quick check: Given the triplet loss function L_triplet = max(0, α + d(q, c_rel) - d(q, c_irrel)), what happens to the loss when the distance between the query and the relevant chunk is already much smaller than the distance to the irrelevant chunk by at least the margin α?

- **Knowledge Distillation**: The overall approach transfers knowledge from a large "teacher" LLM to a smaller "student" retrieval model through relevance judgments. Quick check: In this paper, what specific "knowledge" is being distilled from the teacher LLM to the student bi-encoder? Is it raw text, probability distributions, or something else?

## Architecture Onboarding

- **Component map**: Unlabeled Corpus -> Synthetic Query Generator -> Example Miner -> Student Retriever -> LLM Judge
- **Critical path**: 1) Initialize base embedding model 2) Generate synthetic queries 3) Iteration loop: a) Retrieve top-K passages with current student b) LLM scores relevance c) Form contrastive triples d) Fine-tune updated student 4) Evaluate with held-out test set
- **Design tradeoffs**: Within-document vs. global negatives (sharpens within-document precision but may miss cross-document distractors), synthetic vs. human queries (avoids annotation costs but risks LLM biases), iterative vs. one-shot (more complex but designed for progressively harder examples)
- **Failure signatures**: Training collapse (student retrieval becomes so poor it fails to surface relevant passages), mode collapse (model maps all queries to single point in embedding space), overfitting to synthetic queries (excels on synthetic evaluation but fails on human benchmarks), catastrophic forgetting (forgets general knowledge while learning domain specifics)
- **First 3 experiments**: 1) Establish baseline with non-fine-tuned embedding model on manually labeled domain queries 2) Pilot single iteration with small synthetic query set and LLM judging 3) Ablate on negative sampling comparing global vs. within-document negatives

## Open Questions the Paper Calls Out

### Open Question 1
Can query expansion using knowledge graphs (e.g., GraphRAG) or external knowledge bases improve retrieval for compound or terminologically dense queries? The paper identifies that FinanceBench 10-K queries requiring unpacking (e.g., "quick ratio") are not adequately represented in current synthetic queries. What evidence would resolve it: An ablation study comparing baseline synthetic queries against knowledge-augmented query expansion on FinanceBench 10-K, measuring reduction in demoted relevant passages.

### Open Question 2
Why do aggregate metric gains plateau after the first iteration, and what modifications could enable sustained improvement? The paper observes most benefits come from a single iteration but doesn't diagnose whether saturation stems from exhausting learnable patterns, overfitting, or limitations in hard negative mining. What evidence would resolve it: Analysis tracking diversity and difficulty distribution of mined triples across iterations, combined with experiments varying negative sampling strategies or curriculum learning schedules.

### Open Question 3
Can "agentic" models with reasoning capabilities generate more effective anchor queries for bi-encoder training than current few-shot prompting? The paper proposes this as an alternative to current query generation but hasn't tested whether agentic planning yields queries with better semantic space coverage. What evidence would resolve it: A controlled comparison measuring t-SNE cluster diversity and downstream MRR@5 when queries are generated via agentic planning versus current few-shot method.

## Limitations
- Dependency on LLM-generated queries and judgments may not reflect real user behavior or contain systematic biases
- Performance gains concentrated on 10-K filings with less improvement on other financial filing types in FinanceBench
- Within-document constraint may limit cross-document reasoning capabilities essential for complex financial analysis
- Exact prompt templates and parameter choices (K for retrieval, decay constant) are underspecified, limiting exact reproduction

## Confidence

- **High Confidence**: The iterative contrastive learning mechanism with within-document constraints produces measurable improvements in MRR and DCG metrics. The ablation showing gains from same-document negatives vs. global negatives is direct evidence.
- **Medium Confidence**: The LLM distillation approach works as described, but the exact prompt templates and parameter choices are underspecified, limiting exact reproduction.
- **Low Confidence**: Claims about semantic understanding improvements are primarily supported by qualitative examples rather than systematic error analysis. The performance degradation on non-10-K filings raises questions about generalizability.

## Next Checks

1. Conduct detailed error analysis categorizing retrieval failures on FinanceBench by query type (simple lookup vs. complex reasoning) to identify where the model struggles with human-written queries
2. Test the model's ability to retrieve information requiring synthesis across multiple documents to quantify the tradeoff from the within-document constraint
3. Implement a lightweight version of the pipeline with fixed, published parameters to verify the magnitude of gains independently and test sensitivity to the margin parameter and triplet sampling strategy