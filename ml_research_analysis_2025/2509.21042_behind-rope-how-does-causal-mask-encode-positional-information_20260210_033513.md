---
ver: rpa2
title: 'Behind RoPE: How Does Causal Mask Encode Positional Information?'
arxiv_id: '2509.21042'
source_url: https://arxiv.org/abs/2509.21042
tags:
- layer
- bias
- positional
- causal
- recency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that causal self-attention, when combined with
  LayerNorm, induces a recency bias in attention scores, favoring closer query-key
  pairs over more distant ones. Theoretical analysis proves that even without learnable
  parameters or causal input dependencies, LayerNorm transforms stacked causal self-attention
  layers into a mechanism that exhibits recency bias.
---

# Behind RoPE: How Does Causal Mask Encode Positional Information?

## Quick Facts
- arXiv ID: 2509.21042
- Source URL: https://arxiv.org/abs/2509.21042
- Reference count: 40
- Primary result: Causal self-attention with LayerNorm induces recency bias, favoring nearby query-key pairs over distant ones, even without learnable parameters or causal input dependencies.

## Executive Summary
This paper demonstrates that causal self-attention, when combined with LayerNorm, induces a recency bias in attention scores, favoring closer query-key pairs over more distant ones. Theoretical analysis proves that even without learnable parameters or causal input dependencies, LayerNorm transforms stacked causal self-attention layers into a mechanism that exhibits recency bias. This occurs because LayerNorm normalizes hidden states such that nearby tokens become more similar in representation, leading to higher attention scores for recent tokens. Empirical simulations confirm this effect across layers, with recency bias becoming more pronounced in deeper layers. The study also examines how residual connections and anisotropic input embeddings modulate this bias, showing that anisotropy amplifies the effect while residual connections dampen it.

## Method Summary
The study uses synthetic token embeddings x_i^(0) ~ N(0, I_d/d) with n=10 tokens and d=64 hidden dimension. Anisotropic inputs are modeled as x_i^(0) = ε_i + sqrt(α/(1-α))·v where ε_i, v ~ N(0, I_d/d). The method employs parameter-free Transformer decoder architecture (no learnable weights, no FFN) with Pre-LN architecture. Ablations include ±LayerNorm, ±residual connections, and varying anisotropy levels α ∈ {0.0, 0.2, 0.4, 0.5, 0.6, 0.8}. The simulation runs 100,000 times averaged per configuration, analyzing attention patterns across layers and h(j) curves for varying α.

## Key Results
- LayerNorm combined with causal self-attention induces recency bias in stacked layers, favoring recent tokens over earlier ones
- Anisotropy in input embeddings amplifies the recency bias, making it more pronounced in deeper layers
- Residual connections moderate the magnitude of the induced recency bias
- The positional bias from causal masking differs from typical relative positional encodings like RoPE, encoding absolute proximity rather than pairwise distances

## Why This Works (Mechanism)

### Mechanism 1: LayerNorm Induces Recency Bias
Stacking causal self-attention layers with LayerNorm induces recency bias, whereas attention layers without LN tend to favor earlier tokens or uniform distributions. Causal masking restricts information flow to past tokens. LN normalizes hidden states such that the self-attention score S_ij^(2) in the second layer becomes strictly increasing with respect to the key index j for a fixed query i. This effectively reverses the "primacy bias" observed in stacks without LN. The effect holds when inputs follow standard normal distribution and hidden size d >> 1.

### Mechanism 2: Anisotropy Amplifies Bias
High anisotropy (high cosine similarity) in input token embeddings strengthens the recency bias, making it more pronounced in deeper layers. Real-world embeddings often share a common direction vector v. When anisotropy level α > 0, the function h(j) governing attention score growth increases rapidly with position index j, amplifying the score difference between adjacent and distant tokens. This effect holds regardless of anisotropy level.

### Mechanism 3: Residual Dampening
Residual connections moderate the magnitude of the induced recency bias. Residual connections add the original input x^(0) to the transformed output. Since components of x^(0) are not fully shared between positions, this dilutes the "shared component" accumulation that drives the high recency scores, reducing the bias intensity.

## Foundational Learning

**Concept: Layer Normalization (LN) Geometry**
- Why needed here: The paper relies on how LN projects vectors onto a sphere, changing dot product dynamics. You must understand that ⟨LN(x), LN(y)⟩ differs significantly from ⟨x, y⟩.
- Quick check question: If vectors a and b are orthogonal, are LN(a) and LN(b) necessarily orthogonal?

**Concept: Causal vs. Positional Bias**
- Why needed here: To distinguish between explicit positional encodings (like RoPE) and implicit biases arising from the mask structure itself.
- Quick check question: Does a causal mask provide absolute, relative, or no positional information by itself without weights?

**Concept: Anisotropy in Embeddings**
- Why needed here: The paper assumes a specific distribution (isotropic vs. anisotropic) for inputs to prove the bias strength.
- Quick check question: In a highly anisotropic embedding space, do tokens generally occupy a narrow cone or a wide sphere?

## Architecture Onboarding

**Component map:** Input -> [LayerNorm -> Causal Self-Attention (+ Residual)] -> [LayerNorm -> FFN (+ Residual)]

**Critical path:** The interaction occurs primarily at the Attention Score calculation (S) following LayerNorm. Ensure you instrument the attention maps at Layer 2 specifically, as Theorem 1 targets this layer.

**Design tradeoffs:**
- *LayerNorm:* Essential for training stability but introduces implicit recency bias. If long-range dependency is required, this implicit bias may fight against explicit long-range positional encodings.
- *Residuals:* Stabilize training and dampen recency bias, potentially allowing access to older context.

**Failure signatures:**
- Models failing to capture long-range dependencies in deep layers (excessive recency bias)
- Unexpectedly uniform attention in early layers if input anisotropy is low

**First 3 experiments:**
1. **Baseline Replication:** Implement a parameter-free causal self-attention stack with LayerNorm (as per Eq. 2) using Gaussian inputs. Verify the emergence of recency bias in Layer 2 attention maps.
2. **Anisotropy Ablation:** Inject a controlled "common direction" vector into inputs to vary anisotropy (α). Plot h(j) (Eq. 41) to verify amplification of bias.
3. **Residual Ablation:** Toggle residual connections in the simulation. Quantify the reduction in attention score variance across positions to confirm the dampening effect.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does the recency bias induced by LayerNorm and causal self-attention interact with explicit positional encodings like RoPE?
- Basis in paper: The authors state in the Limitations section: "Although most modern Transformer decoder-based models use RoPE... for positional encoding, its interaction with the positional information induced by causal self-attention has not been examined."
- Why unresolved: The theoretical and empirical analysis in the paper focuses on architectures without explicit positional encodings to isolate the specific contribution of the causal mask and LayerNorm.
- What evidence would resolve it: A theoretical or empirical analysis of attention score distributions in models that combine the LayerNorm-induced bias with explicit encodings like RoPE or ALiBi.

**Open Question 2**
- Question: What is the relationship between the induced recency bias and the downstream task performance or generalization capabilities of the model?
- Basis in paper: The Limitations section notes that "the relationship between recency bias and overall model performance is not evaluated."
- Why unresolved: The paper focuses on proving the existence of the bias and analyzing its modulation by residual connections, but does not link the magnitude of this bias to metrics like perplexity, accuracy, or length generalization.
- What evidence would resolve it: Experiments correlating the strength of the recency bias (modulated by anisotropy or residual connections) with performance on benchmarks requiring varying degrees of positional awareness.

**Open Question 3**
- Question: How do feed-forward networks (FFNs), learnable parameters, and multi-head attention mechanisms modulate the recency bias theoretically derived for simplified architectures?
- Basis in paper: The authors explicitly limit their scope: "We do not analyze the effects of feed-forward networks, other learnable parameters, or multi-head attention within Transformer decoder layers."
- Why unresolved: The theoretical proofs rely on a simplified single-head attention model without FFNs to ensure mathematical tractability regarding the LayerNorm interaction.
- What evidence would resolve it: Extending the theoretical derivation to include FFNs or conducting ablation studies in full-scale models to observe if these components dampen, amplify, or distort the LayerNorm-induced recency bias.

## Limitations

- The analysis assumes parameter-free causal self-attention with LayerNorm, limiting direct applicability to full-scale trained models where learnable matrices and FFN layers can modulate the effect
- The claim that LayerNorm is "essential" for inducing recency bias may not generalize to other normalization variants or model architectures
- The anisotropy analysis assumes a specific embedding structure (isotropic noise plus shared vector) that may not capture all real-world embedding distributions

## Confidence

**High Confidence:** The theoretical proof that LayerNorm combined with causal masking induces recency bias in stacked self-attention layers is mathematically sound under the stated assumptions (Gaussian inputs, large d, pre-LN). The empirical simulation results align with this prediction.

**Medium Confidence:** The claim that anisotropy amplifies the bias is well-supported by both theory and simulation, but the exact relationship in real-world embeddings requires further validation. The dampening effect of residuals is observed but the mechanism is less rigorously analyzed.

**Low Confidence:** The assertion that this effect is distinct from typical relative positional encodings like RoPE, while plausible from the analysis, lacks direct empirical comparison to such encodings in the same framework.

## Next Checks

1. **Real Embedding Validation:** Replace synthetic Gaussian embeddings with pre-trained token embeddings (e.g., from GPT-2) and measure the induced recency bias in attention scores across layers. Verify that the bias magnitude correlates with embedding anisotropy.

2. **Positional Encoding Interaction:** Implement a standard Transformer with RoPE and compare the attention patterns to the parameter-free model. Quantify how explicit positional encodings interact with or override the implicitly induced recency bias from causal masking.

3. **Ablation on Normalization:** Replace LayerNorm with other normalization schemes (e.g., RMSNorm, InstanceNorm) in the causal self-attention stack and measure the presence or absence of recency bias. This tests whether the effect is unique to LayerNorm or a broader property of normalization + masking.