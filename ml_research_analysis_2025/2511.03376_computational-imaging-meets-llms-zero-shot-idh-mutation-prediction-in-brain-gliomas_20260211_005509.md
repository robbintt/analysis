---
ver: rpa2
title: 'Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in Brain
  Gliomas'
arxiv_id: '2511.03376'
source_url: https://arxiv.org/abs/2511.03376
tags:
- gpt-5
- gpt-4o
- features
- were
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a zero-shot framework combining large language
  models with computational image analytics to predict IDH mutation status in brain
  gliomas from MRI data. Structured visual attributes and quantitative features from
  multi-parametric MRI and tumor segmentations are serialized in JSON and queried
  to GPT-4o and GPT-5 without fine-tuning.
---

# Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in Brain Gliomas

## Quick Facts
- arXiv ID: 2511.03376
- Source URL: https://arxiv.org/abs/2511.03376
- Reference count: 0
- Primary result: Zero-shot LLM framework achieves 91.87-92.15% accuracy predicting IDH mutation status in brain gliomas from MRI features

## Executive Summary
This study presents a zero-shot framework combining large language models with computational image analytics to predict IDH mutation status in brain gliomas from MRI data. Structured visual attributes and quantitative features from multi-parametric MRI and tumor segmentations are serialized in JSON and queried to GPT-4o and GPT-5 without fine-tuning. Evaluated on six public cohorts (N=1427), the approach achieved high accuracy and balanced classification, with GPT-5 outperforming GPT-4o in sensitivity and F1 scores. Volumetric measures were the strongest predictors, supplemented by subtype-specific imaging markers and clinical features.

## Method Summary
The framework preprocesses multi-parametric MRI (FLAIR, T1, T1-CE, T2) through rigid registration to MNI-152 space, resampling to 1mm³, and N4 bias correction. Tumor segmentations are generated with BrainSegFounder when ground-truth is unavailable. Five feature categories are extracted: location features via atlas registration, T2-FLAIR mismatch metrics, mass effect measurements, tumor morphology parameters, and volumetric measures from BraTS segmentation regions. These features plus clinical metadata are serialized as JSON and queried to GPT-4o or GPT-5 via API (temperature=0.0), with responses parsed via regex to extract classification labels and reasoning.

## Key Results
- Overall accuracy of 91.87-92.15% across six cohorts (N=1427)
- GPT-5 achieved higher sensitivity (77.88%) and F1 score (84.74%) than GPT-4o
- Volumetric measures were strongest predictors, with ablation showing 15% drop in geometric mean recall when removed
- Framework generalizes across heterogeneous cohorts including those without ground-truth segmentations

## Why This Works (Mechanism)

### Mechanism 1
Structured semantic feature serialization enables LLMs to perform zero-shot medical classification without domain-specific training. Raw MRI volumes are transformed into ~50 interpretable quantitative and semantic features, serialized as JSON key-value pairs. This structured representation maps tumor phenotype onto language tokens the LLM has seen during pretraining, enabling the model to apply reasoning patterns learned from medical literature without gradient updates. Break condition: If feature extraction produces noisy features from poor segmentations, prediction degrades. Ablation shows removing volumetric features drops geometric mean recall from 0.83 to 0.68.

### Mechanism 2
Volumetric ratios encode the primary diagnostic signal for IDH genotype discrimination. IDH-mutant tumors characteristically show larger non-enhancing tumor proportions relative to enhancing core, while IDH-wildtype gliomas have larger enhancing/necrotic components. Volumetric fractions capture this distribution without requiring the LLM to reason about spatial patterns directly. Break condition: If segmentation quality degrades, particularly distinguishing NET from ET, volumetric fractions become unreliable. The paper notes 34% of subjects lacked ground-truth segmentations, using automated BrainSegFounder instead.

### Mechanism 3
Subtype-specific imaging markers provide orthogonal signal that corrects volumetric biases for edge cases. T2-FLAIR mismatch sign is highly specific for IDH-mutant astrocytomas but has poor sensitivity. Morphological features help distinguish oligodendrogliomas. The LLM integrates these with base volumetric signal via context fusion in the prompt. Break condition: If the prompt does not clearly structure feature relationships, the LLM may apply features uniformly rather than conditionally, reducing calibration.

## Foundational Learning

- **Zero-shot inference via prompt engineering**: Why needed: The entire framework depends on eliciting correct classification from frozen LLM weights through structured prompts. Understanding how to design prompts that activate relevant pretraining knowledge is essential. Quick check: Can you explain why the prompt specifies "we do not have information on the necrosis component" and how omission statements might affect LLM reasoning?

- **IDH mutation biology and imaging correlates**: Why needed: The feature extraction pipeline is designed around known radiologic-genomic relationships. Without this domain knowledge, you cannot validate whether extracted features are diagnostically meaningful or debug ablation results. Quick check: Why would T2-FLAIR mismatch be highly specific but poorly sensitive for IDH status, and how does this explain the ablation results?

- **BraTS tumor segmentation protocol**: Why needed: All volumetric and morphological features derive from segmentation subregions (WT, NET, ET, ED). Understanding what these labels represent is prerequisite to interpreting feature definitions. Quick check: If a segmentation model misclassifies necrotic tissue as enhancing tumor, which volumetric fractions would be most affected?

## Architecture Onboarding

- **Component map**:
```
Input: mpMRI volumes (FLAIR, T1, T1-CE, T2) + segmentation mask
  ↓
Preprocessing (ANTsPy): Rigid registration to MNI-152, resampling 1mm³, N4 bias correction
  ↓
Computational Imaging Toolbox (Python):
  ├─ Location Features (4 atlases via ANTsPy registration)
  ├─ T2-FLAIR Mismatch (intensity ratios, binary flags)
  ├─ Mass Effect (midline crossing, ventricular asymmetry)
  ├─ Tumor Morphology (sphericity, boundary sharpness, rim architecture)
  └─ Volumetric Measures (subregion volumes, fractional burdens)
  ↓
JSON Serialization (~50 features + clinical metadata)
  ↓
LLM Query (GPT-4o/GPT-5 via OpenAI API, temp=0.0)
  ↓
Response Parsing (regex extraction of class label + reasoning)
```

- **Critical path**: Preprocessing → Segmentation quality → Volumetric feature extraction → LLM inference. Ablation shows volumetric features dominate performance; these depend entirely on accurate segmentation.

- **Design tradeoffs**:
  - Automated vs. manual segmentation: BrainSegFounder enables processing unlabeled data but introduces segmentation error propagation (unquantified in this study)
  - Feature breadth vs. prompt length: 50+ features maximize information but risk LLM attention dilution—the paper does not ablate prompt length effects
  - GPT-4o vs. GPT-5: GPT-5 costs 50% less ($0.002 vs $0.004/case) with better sensitivity, but availability/access may differ

- **Failure signatures**:
  - Low sensitivity on minority class (IDH-mutant): Check if volumetric features are dominated by majority-class patterns
  - High variance across cohorts: Check preprocessing alignment and atlas registration quality
  - Disagreement between GPT-4o/GPT-5: Inspect for high-grade aggressive tumors (GPT-4o favored) vs. low-grade non-enhancing lesions (GPT-5 favored)

- **First 3 experiments**:
  1. **Reproduce baseline on single cohort**: Run pipeline on UCSF-PDGM (n=443, has ground-truth segmentations) to validate end-to-end flow; target accuracy ~94%
  2. **Ablate volumetric features**: Remove all volumetric measures from JSON and measure recall drop on GBM cases; should replicate ~50% recall degradation
  3. **Test segmentation robustness**: Replace BrainSegFounder with alternative segmentation model on subset without ground-truth; compare volumetric feature distributions and downstream accuracy

## Open Questions the Paper Calls Out
- **Segmentation algorithm robustness**: The authors state "a more extensive analysis is needed to assess the impact of different automated segmentation algorithms" as a key limitation, having evaluated only BrainSegFounder for generating automatic segmentations.

- **Open-weight LLM performance**: The authors note they "evaluated only proprietary, API-accessible LLMs; behavior may differ for open-weight models" and plan to "benchmark the pipeline on high-performing open-source medical LLMs" for clinical deployment considerations.

- **Fine-tuning vs. zero-shot trade-offs**: The authors acknowledge "The study focused on zero-shot LLM-based inference without fine-tuning, which may have limited optimal model performance" and that this "limited interpretability beyond the prompt-defined reasoning framework."

## Limitations
- Performance heavily dependent on segmentation quality, with 34% of cohort using automated BrainSegFounder without validation against ground-truth
- Zero-shot approach lacks direct validation of whether LLM truly "understands" radiogenomic associations versus pattern matching on feature names
- GPT-5 model (gpt-5-chat-latest) used is not publicly available, limiting reproducibility
- Moderate sensitivity (72.97-77.88%) for IDH-mutant cases may limit clinical utility for detecting rarer genotype

## Confidence
- **High confidence**: Overall accuracy metrics and comparative performance between GPT-4o and GPT-5 across multiple cohorts
- **Medium confidence**: Volumetric features as primary predictors (supported by ablation but limited to one direction of testing)
- **Medium confidence**: Zero-shot capability through structured JSON serialization (demonstrated but not validated against alternative prompt formats)
- **Low confidence**: Clinical generalizability across institutions (performance differences across cohorts suggest protocol dependence not fully characterized)

## Next Checks
1. **Segmentation robustness validation**: Run the same pipeline on UCSF-PDGM cohort using both BrainSegFounder and a second automated segmentation model (e.g., nnUNet), comparing volumetric feature distributions and classification accuracy to quantify segmentation-induced performance variance

2. **Feature semantic ablation**: Create control JSON prompts with randomized feature names (e.g., "feature_1" instead of "sphericity") but identical numerical values, comparing GPT-5 performance to validate whether the LLM leverages semantic context or treats features as abstract numerical vectors

3. **Cross-protocol generalization test**: Acquire a small external cohort with different MRI protocols (e.g., 3T vs 1.5T, different field-of-view) to measure performance degradation and identify protocol-dependent feature sensitivities