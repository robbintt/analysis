---
ver: rpa2
title: 'Child vs. machine language learning: Can the logical structure of human language
  unleash LLMs?'
arxiv_id: '2502.17304'
source_url: https://arxiv.org/abs/2502.17304
tags:
- plural
- german
- language
- llms
- form
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that human language learning is driven by logical
  structures that are not naturally present in current LLM architectures, leading
  to sub-optimal learning of certain linguistic patterns. Specifically, the authors
  predict that LLMs will struggle with German plural formation due to the logical
  structure of the default -s ending, which requires negating the four other classes
  of plural endings.
---

# Child vs. machine language learning: Can the logical structure of human language unleash LLMs?

## Quick Facts
- **arXiv ID**: 2502.17304
- **Source URL**: https://arxiv.org/abs/2502.17304
- **Authors**: Uli Sauerland; Celia Matthaei; Felix Salfner
- **Reference count**: 3
- **Primary result**: LLMs struggle with German plural formation due to logical structures not naturally present in their architectures

## Executive Summary
This paper investigates whether the logical structures underlying human language learning are present in current large language models (LLMs). The authors argue that human language learners have an inherent cognitive bias for logical negation—when learning output A, the negative counterpart Ā becomes cognitively available—a mechanism absent in standard LLM architectures. Testing seven current LLMs on German plural formation using nonce nouns reveals consistent failures, particularly with the default -s ending which requires negating four other plural classes. The results suggest that incorporating logical structures more explicitly into LLM architectures could improve their linguistic performance.

## Method Summary
The study adapted the Marcus et al. (1995) nonce noun pluralization task to test German plural formation in seven LLMs. Researchers queried each model with 24 German nonce nouns (e.g., Bral, Klot, Fneik, Pnähf, Kach, Bneik), half of which rhymed with real German nouns. Each LLM was queried in a fresh session with "Was wäre der Plural von XXX?" and optionally followed up with clarification or challenge prompts. The resulting plural forms were mapped to one of five German plural endings (-e, -n, -er, -∅, -s) or marked as impossible, then assigned plausibility ratings from the original Marcus et al. human judgment data (0–4.3 scale, threshold of 3.0 for "plausible").

## Key Results
- All seven tested LLMs produced forms with lower human plausibility ratings than human benchmarks
- LLMs consistently struggled with nonce nouns requiring the default -s plural ending
- Performance was particularly poor for nonce nouns that rhymed with real German nouns
- Models frequently produced morphologically impossible forms (e.g., "Klötze" for "Klot" instead of valid "Klote" or default "Klot-s")

## Why This Works (Mechanism)

### Mechanism 1: Human Logical Negation Bias
Human language learners spontaneously generate access to logical negations (¬A) after learning output A, a cognitive bias not present in standard LLM architectures. When humans learn a form A in certain contexts, the negative counterpart Ā becomes cognitively available without explicit instruction. This drives pattern formation like German children producing "mit ohne" (lit. "with without") despite never hearing it. This negation bias is a domain-general cognitive property, not learned from linguistic input alone.

### Mechanism 2: Default Rules via Implicit Exclusion Logic
Morphological defaults (like German plural -s) operate via implicit negation: "if NOT in classes {A, B, C, D}, then apply default"—a structure humans acquire naturally but LLMs struggle to induce from distributional data. The default -s applies when all other class memberships are negated. Humans learn this exclusion logic readily; LLMs must infer it statistically from sparse exemplars. The low frequency of German -s plurals prevents statistical learning from converging on the correct generalization.

### Mechanism 3: Statistical Pattern Matching Without Algebraic Bias
LLMs learn surface statistical patterns without architectural bias toward logical/algebraic relationships, producing different generalization errors than humans. Neural networks infer concepts independently from co-occurrence statistics. Without built-in bias for complementary distribution (A ↔ Ā), learning "if not-X then Y" requires substantially more data than pattern-based generalizations. This is an architectural constraint, not merely a data scaling issue.

## Foundational Learning

- **Concept: Default/Elsewhere Rules (Pāṇini's Principle)**
  - Why needed here: Understanding that morphological systems contain rules that apply only when all more specific rules fail—the "elsewhere" case.
  - Quick check question: Given that English irregular plurals (children, oxen) block the regular -s, what does this tell you about how defaults work?

- **Concept: Complementary Distribution in Neural Representations**
  - Why needed here: Recognizing that implementing A and ¬A requires either inverse weight coupling or explicit logical layers—neither is native to transformers.
  - Quick check question: If neuron A outputs "plural-e" with probability 0.8, how would a network represent "definitely NOT plural-e"?

- **Concept: nonce word (Wug) Testing**
  - Why needed here: This paradigm isolates productive rule application from memorized forms—critical for assessing true generalization.
  - Quick check question: Why does testing plural formation on made-up words like "Bneik" reveal more about underlying knowledge than testing real words?

## Architecture Onboarding

- **Component map**: German noun stem → phonological feature extraction → feature-based class membership evaluation (Classes 1-4: -e, -n, -r, -∅) → Gap: No explicit "if none match → default -s" logic → Output based on highest activation pattern match

- **Critical path**: 
  1. Noun stem → phonological feature extraction
  2. Feature-based class membership evaluation (Classes 1-4: -e, -n, -er, -∅)
  3. **Gap**: No explicit "if none match → default -s" logic
  4. Output based on highest activation pattern match

- **Design tradeoffs**:
  - **Explicit logical layers vs. emergent reasoning**: Adding neurosymbolic components provides guarantees but increases complexity
  - **Curriculum-based RL training vs. architectural modification**: DeepSeek-R1 style logical reasoning RL may transfer but hasn't been tested on implicit morphology
  - **Frequency weighting vs. rule extraction**: Resampling training data to over-represent -s defaults might help but doesn't address the underlying architectural gap

- **Failure signatures**:
  - Producing morphologically impossible forms (e.g., "Klötze" for "Klot"—umlaut + suffix combination that doesn't exist for this stem)
  - Over-applying frequent patterns to rhyme-matched nonce words instead of applying defaults
  - Inconsistency under re-prompting ("Are you sure?" causing corrections in both directions)
  - Hallucinating that nonce words are real words from other languages/dialects

- **First 3 experiments**:
  1. **Nonce noun pluralization battery**: Administer the 24-item Marcus et al. (1995) nonce noun set to target LLM; score outputs against human plausibility ratings; isolate rhyme vs. non-rhyme conditions
  2. **Default-s frequency manipulation**: Fine-tune a model on German data with artificially increased -s plural frequency; test whether this closes the gap or merely shifts bias
  3. **Logical reasoning transfer test**: Take a model trained with explicit logical reasoning exercises (à la DeepSeek-R1) and test on the nonce plural task to assess whether explicit logic training transfers to implicit morphological defaults

## Open Questions the Paper Calls Out

### Open Question 1
Can explicit architectural incorporation of logical structures (e.g., negation operations) enable LLMs to learn morphological defaults without massive training data? The authors propose that adding logical negation mechanisms could help, but require "substantial changes to the network architecture." The paper demonstrates the problem but does not implement or test any modified architecture. Evidence would come from comparing standard LLMs to modified architectures with explicit logical operators on the same nonce noun pluralization task.

### Open Question 2
Would a linguistic curriculum including implicit logical exercises (analogous to the mathematical reasoning curricula used in RL training) improve LLM performance on tasks like German plural formation? The authors propose "it might be fruitful to compile a more extensive set of exercises including linguistic ones for the training of LLMs – we might call this a Curriculum for LLMs." Reinforcement learning with reasoning exercises has improved explicit logical reasoning, but whether this transfers to implicit linguistic logic remains untested. Evidence would come from training LLMs with a curriculum of morphological exercises involving defaults across multiple languages and testing on held-out nonce word tasks.

### Open Question 3
Do the findings generalize to other linguistic phenomena requiring logical negation beyond German plural formation (e.g., complementizer distribution, polarity items, or default verb conjugations)? The paper focuses on one phenomenon; the authors note morphological systems "almost always involve defaults" and that negation is present even in prelinguistic infants. The study tested only German plural formation using 24 nonce nouns from one prior study. Evidence would come from testing LLMs across diverse linguistic structures requiring default logic in multiple languages and linguistic domains.

## Limitations

- The logical structure of default rules (negation-based exclusion) is inferred rather than directly validated as absent from LLM architectures
- Marcus et al. (1995) human plausibility ratings may not fully capture modern German grammaticality, and exact coding rules for mapping LLM outputs are not fully specified
- The study tests only seven LLMs without controlling for training data differences, making it difficult to isolate whether failures are truly architectural or due to scale/exposure

## Confidence

- **High confidence**: LLMs produce forms rated as implausible by human speakers on German pluralization tasks, especially for nonce nouns requiring the default -s ending or rhyming with real nouns
- **Medium confidence**: The logical structure of default rules (negation-based exclusion) is not naturally present in current LLM architectures and may explain the observed failures
- **Low confidence**: Incorporating explicit logical structures into LLM architectures will necessarily and substantially improve their linguistic performance; the study does not demonstrate this, only speculates on the mechanism

## Next Checks

1. **Explicit logic transfer experiment**: Test whether models trained with explicit logical reasoning (e.g., DeepSeek-R1 style) outperform standard models on the German nonce plural task, isolating architectural from statistical factors

2. **Corpus frequency manipulation**: Fine-tune a model on German data with artificially increased -s plural frequency and retest; determine if scaling alone closes the gap or if architectural change is needed

3. **Controlled prompting study**: Systematically vary prompt complexity and follow-up questions to assess whether prompting alone (not model architecture) can elicit more human-like default pluralization