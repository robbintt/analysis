---
ver: rpa2
title: 'Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian
  Argument Mining Model over LLM Augmentation'
arxiv_id: '2511.20872'
source_url: https://arxiv.org/abs/2511.20872
tags:
- argument
- english
- mining
- cross-lingual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study tackles the challenge of argument mining in low-resource
  languages, specifically Persian, by leveraging cross-lingual transfer from English.
  The authors compare three training scenarios: (1) zero-shot transfer using only
  English data, (2) LLM-augmented English-only training, and (3) a cross-lingual model
  combining English and manually translated Persian data.'
---

# Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian Argument Mining Model over LLM Augmentation

## Quick Facts
- arXiv ID: 2511.20872
- Source URL: https://arxiv.org/abs/2511.20872
- Authors: Ali Jahan; Masood Ghayoomi; Annette Hautli-Janisz
- Reference count: 40
- Key outcome: Cross-lingual model trained on English+Persian outperforms zero-shot transfer and LLM augmentation, achieving 74.8% F1 on Persian test set

## Executive Summary
This study addresses the challenge of argument mining in low-resource languages, specifically Persian, by leveraging cross-lingual transfer from English. The authors compare three training scenarios: zero-shot transfer using only English data, LLM-augmented English-only training, and a cross-lingual model combining English and manually translated Persian data. Their results show that while LLM augmentation improves performance over zero-shot transfer, the cross-lingual model—trained on both languages but evaluated only on Persian—achieves the best results with an F1 score of 74.8%. This outperforms the strongest LLM-augmented variant by 5.5 percentage points and the baseline by 24.6 percentage points.

## Method Summary
The study employs a multi-task learning approach using XLM-R-base as a shared encoder with two separate linear classification heads for stance classification (pro/con) and relation detection (support/rebuttal/undercut/example). Three training scenarios are compared: (1) zero-shot transfer using only English Microtext corpus, (2) LLM-augmented English-only training with synthetic minority-class examples generated by GPT-4/Claude/Gemini/DeepSeek, and (3) cross-lingual training combining English data with manually translated Persian sentences. The model is trained with AdamW optimizer (lr=5e-6, batch size=16) for up to 100 epochs with early stopping on validation loss. Maximum sequence length is 128 tokens.

## Key Results
- Cross-lingual model (EN+FA) achieves 74.8% F1 on Persian test set, outperforming strongest LLM-augmented variant by 5.5 percentage points
- LLM augmentation improves over zero-shot transfer but falls short of cross-lingual approach
- Manual translation of 112 Persian documents provides more effective training signal than large-scale synthetic augmentation
- Severe class imbalance in baseline (pro:con = 451:125) causes near-zero recall on minority class (7.4% recall)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Representation Alignment via Shared Encoder
Multilingual transformer encoders (XLM-R) enable transfer by mapping semantically similar argumentative structures across languages into aligned vector spaces. The shared encoder processes both English and Persian inputs through identical transformer layers, allowing argument patterns learned from English to activate for Persian inputs when fine-tuned on combined data. Core assumption: Argumentative discourse patterns share structural regularities across languages that multilingual pre-training captures.

### Mechanism 2: Class Imbalance Correction via Synthetic Minority Augmentation
LLM-generated synthetic examples for the minority class (con) improve model calibration by exposing the classifier to balanced training distributions. Synthetic "con" ADUs generated by multiple LLMs are appended to training data, raising minority class frequency and reducing pro-dominance bias. Core assumption: LLM-generated synthetic ADUs preserve semantic validity and argumentative structure of real counter-arguments.

### Mechanism 3: Target-Language Grounding via Manual Translation
Even modest amounts of human-translated target-language data provide stronger grounding than synthetic augmentation alone. Concatenating manually translated Persian sentences with English training data exposes the model to authentic Persian syntax, discourse markers, and argumentative phrasing, enabling better generalization to Persian test instances. Core assumption: Translation quality preserves argumentative structure and stance labels accurately.

## Foundational Learning

- **Argument Mining (AM) subtasks**: The paper decomposes AM into two tasks—ADU stance classification (pro/con) and relation detection (support/rebuttal/undercut/example). Understanding this separation is essential for interpreting the multi-task architecture. *Quick check: Can you explain why stance classification and relation detection might require different classification heads?*

- **Zero-shot Cross-Lingual Transfer**: The baseline scenario assumes a model trained only on English will generalize to Persian without target-language exposure. Understanding zero-shot limitations motivates the augmentation and cross-lingual approaches. *Quick check: Why does zero-shot transfer struggle with minority class (con) retrieval?*

- **Class Imbalance in Sequence Labeling**: The paper's central problem is severe pro/con imbalance (451 vs. 125). Understanding how imbalance affects precision-recall trade-offs clarifies why augmentation helps but doesn't fully solve the problem. *Quick check: If a model achieves 87.3% F1 on "pro" but 13.3% F1 on "con", what does this indicate about the training distribution?*

## Architecture Onboarding

- **Component map**: Input tokenization -> XLM-R shared encoder -> [CLS] extraction -> Stance head (linear classifier) and Relation head (linear classifier on concatenated ADU pairs)

- **Critical path**: 1) EDU/ADU tokenization → XLM-R encoding → [CLS] extraction 2) For stance: [CLS] → linear classifier → softmax → pro/con label 3) For relations: ADU pair concatenation → [CLS] of combined sequence → linear classifier → relation type 4) Joint training with cross-entropy loss for both tasks

- **Design tradeoffs**: Multi-task vs. separate models (shared encoder reduces inference cost but may introduce task interference); LLM augmentation vs. manual translation (LLM is cheaper but noisier; manual translation is costly but provides cleaner signal with 5.5% F1 gain); augmentation scale (paper balanced classes to 665 each; optimal ratio not explored)

- **Failure signatures**: Minority class collapse (near-zero recall on "con" stance indicates imbalance problem); relation type drift (LLM augmentation "tends to mistype undercut as rebuttal"); precision-recall gap (large gaps signal overconfident predictions on majority class)

- **First 3 experiments**: 1) Replicate baseline zero-shot: Train XLM-R on English Microtext only; evaluate on both English and Persian test sets to confirm ~50% F1 and diagnose minority class recall 2) Ablate augmentation scale: Test GPT augmentation at 25%, 50%, 100% of paper's synthetic volume to find saturation point where noise outweighs signal 3) Measure translation quality impact: Compare machine-translated vs. manual Persian data to quantify translation quality effect

## Open Questions the Paper Calls Out

### Open Question 1
Can semi-automatic back-translation effectively scale the Persian training data to improve performance without sacrificing the quality achieved by manual translation? The conclusion explicitly proposes "enlarging the Persian portion with semi-automatic back-translation" as a direction for future study. Experiments comparing F1 scores of models trained on manual translation versus back-translated data would resolve this.

### Open Question 2
Does injecting discourse-level prompts into LLMs yield more diverse and effective synthetic examples for the minority "con" class? The authors suggest "injecting discourse-level prompts to elicit more diverse minority-class arguments" to address data imbalance. Comparative analysis of lexical diversity and model performance between standard prompts and discourse-level prompts would resolve this.

### Open Question 3
Can parameter-efficient multilingual fine-tuning close the performance gap between direct transfer and fully supervised systems? Section 8 lists "exploring parameter-efficient multilingual fine-tuning" as a method to potentially "tighten the gap between direct transfer and fully supervised systems." Benchmarks showing PEFT techniques achieving comparable or superior F1 scores would resolve this.

### Open Question 4
Do the advantages of the cross-lingual model persist when applied to longer, more complex argumentative texts rather than short microtexts? The study relies exclusively on the "Microtext" corpus, and generalization to longer texts is not addressed. Evaluation on a Persian corpus of full-length essays or debate transcripts would resolve this.

## Limitations

- Core mechanism explanations (shared encoder alignment, class imbalance correction) remain theoretical assumptions without empirical validation
- Key implementation details missing (exact train/val/test splits, prompt templates, negative sampling strategy) block exact reproduction
- Results apply to Persian-English transfer only; generalizability to other language pairs untested
- Small Persian corpus (112 documents) raises questions about scalability of observed gains

## Confidence

- **High confidence**: Empirical finding that cross-lingual training (EN+FA) outperforms zero-shot transfer and LLM augmentation on Persian test data (74.8% F1)
- **Medium confidence**: Claim that manual translation is more effective than LLM augmentation for low-resource argument mining (supported by 5.5% F1 difference but assumes translation quality)
- **Low confidence**: Mechanism explanations for why cross-lingual transfer works and why augmentation helps (remain theoretical assumptions without validation)

## Next Checks

1. Probe cross-lingual alignment: Use semantic similarity analysis or probing classifiers to test whether XLM-R's Persian and English representations for argumentative discourse markers are truly aligned

2. Validate synthetic ADU quality: Implement automatic evaluation metrics for LLM-generated ADUs (coherence scores, stance consistency checks) and compare their distribution to real data

3. Test augmentation scaling: Systematically vary the amount of synthetic data (25%, 50%, 100% of paper's augmentation) and measure the point where additional augmentation degrades performance due to noise