---
ver: rpa2
title: Domain specific ontologies from Linked Open Data (LOD)
arxiv_id: '2505.22550'
source_url: https://arxiv.org/abs/2505.22550
tags:
- ontology
- knowledge
- wikidata
- terms
- en66es
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a domain-agnostic pipeline for building domain-specific
  ontologies from Linked Open Data (LOD), focusing on IT operations. The approach
  leverages Wikidata's structure and extends it with domain-specific glossaries using
  semantic embeddings.
---

# Domain specific ontologies from Linked Open Data (LOD)

## Quick Facts
- **arXiv ID**: 2505.22550
- **Source URL**: https://arxiv.org/abs/2505.22550
- **Reference count**: 0
- **Primary result**: Domain-agnostic pipeline for building domain-specific ontologies from LOD, demonstrated on IT operations

## Executive Summary
This paper presents a three-stage pipeline for building domain-specific ontologies from Linked Open Data (LOD), focusing on IT operations. The approach leverages Wikidata's structure and extends it with domain-specific glossaries using semantic embeddings. The pipeline extracts explicit LOD based on seed concepts, adds implicit LOD using Wikipedia categories and a classifier, and extends with domain-specific glossary terms via similarity-based attachment to parent concepts. The ITOPS ontology was evaluated for IT domain coverage and conciseness, achieving 44.4% coverage improving to 51.7% for top-3 positions when prioritizing terms near non-leaf entities.

## Method Summary
The method consists of three sequential modules: S1 extracts a Wikidata subgraph from seed concepts using subclass (P279) and instance (P31) relations, mapping entities to a General Library of Objects (GLO) via subConceptOf. S2 expands coverage by retrieving Wikipedia categories for S1 entities and using a binary classifier trained on structural features (normalized class count, ISA path length, overlap metrics) to filter domain-relevant categories. S3 attaches glossary terms by propositionalizing S2 entities into definition-like text, encoding both entities and terms with Universal Sentence Encoder, and finding top-5 similar entities to attach terms via subConceptOf. The approach was evaluated against Wikidata for coverage, conciseness metrics, and terminology ranking performance using the TechQA dataset.

## Key Results
- ITOPS-S1 achieved 44.4% coverage, improving to 51.7% for top-3 positions when prioritizing terms near non-leaf entities
- Manual evaluation of glossary term attachment achieved 50% correct placement and 20% related placement
- Automated validation showed 60% of leaves attached within 2 hops of ground truth
- Extended version (S1+S2+S3) showed better conciseness with shorter term lists

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Seed-based subgraph extraction from Wikidata produces a domain-focused ontology skeleton with measurable coverage improvements.
- Mechanism: Starting from positive seed concepts (and optional negative concepts), the pipeline extracts a Wikidata subgraph by traversing `wdt:P279` (subclass of) and `wdt:P31` (instance of) relations, mapping entities to a General Library of Objects (GLO) via `subConceptOf`. The paper reports ITOPS-S1 achieved 44.4% coverage, improving to 51.7% for top-3 positions when prioritizing terms near non-leaf entities.
- Core assumption: Wikidata's IS-A hierarchy reliably reflects domain semantics, and "ISA overloading" (conflating subclass and instance relations) does not significantly degrade downstream query quality for dynamic ontologies.
- Evidence anchors:
  - [abstract]: "ITOPS-S1 achieved 44.4% coverage, improving to 51.7% for top-3 positions when prioritizing terms near non-leaf entities."
  - [section]: "ITOPS S1 has 47,300 entities, of which 2,000 of them have subconcepts of their own, plus 212 domain-specific relations."
  - [corpus]: Weak direct corpus support for this specific seed-extraction mechanism; related papers focus on neuro-symbolic integration rather than LOD subgraph extraction.
- Break condition: If seed concepts are too generic or sparse, the extracted subgraph will lack discriminative power, yielding low precision in domain term ranking.

### Mechanism 2
- Claim: A binary classifier trained on Wikipedia category structural features can identify domain-relevant categories to expand coverage beyond explicit Wikidata content.
- Mechanism: For each S1 entity, associated Wikipedia categories are retrieved. Features include normalized class count, average ISA path length between member classes, class/entity overlap with S1, and heterogeneity metrics. A classifier trained on manually labeled positive (domain-specific) and negative (generic) categories filters relevant ones. This step added 4,752 entities after filtering.
- Core assumption: Domain-relevant categories exhibit structural signatures (higher coherence, greater overlap with existing domain entities) that distinguish them from generic categories.
- Evidence anchors:
  - [abstract]: "Adding implicit LOD using Wikipedia categories and a classifier."
  - [section]: "This step, S2, identified 5,589 new entities. After eliminating low scored entities, 4,752 entities were added to S1."
  - [corpus]: No strong corpus corroboration for this exact category-classification approach; neighbor papers focus on ontology matching and knowledge extraction from text.
- Break condition: If Wikipedia categories for the target domain are sparse or structurally heterogeneous, classifier precision will drop, introducing noise.

### Mechanism 3
- Claim: Predicate-based propositionalization combined with sentence embeddings enables reasonably accurate attachment of glossary terms to an existing ontology.
- Mechanism: S2 entities are converted into definition-like propositional representations, then encoded into 500-dimensional vectors using Universal Sentence Encoder (USE). Glossary terms are similarly encoded. For each term, the top-K most similar entities are identified, and the glossary term is attached via `subConceptOf`. Manual evaluation achieved 50% correct placement and 20% "reasonably related" placement; automated validation showed 60% of leaves attached within 2 hops of ground truth.
- Core assumption: Propositionalized entity representations capture sufficient semantic content for similarity comparison with glossary definitions; embedding similarity correlates with taxonomic proximity.
- Evidence anchors:
  - [abstract]: "Extending with domain-specific glossary terms via similarity-based attachment to parent concepts. Manual evaluation of glossary term attachment achieved 50% correct placement and 20% related placement."
  - [section]: "We achieve an inter-annotator agreement of 0.71 with Fleiss' Kappa... Our approach performs well with more than 60% of the leaves attached within 2 hops away."
  - [corpus]: Corpus papers reference "Language Models as Ontology Encoders" (arXiv:2507.14334), which supports the broader premise of embedding-based ontology extension but does not validate this specific pipeline.
- Break condition: If glossary terms lack definitions, or if ontology entities have sparse propositional representations, embedding similarity will be unreliable, resulting in misplaced attachments.

## Foundational Learning

- **OWL RL Profile and Semantic Web Standards (RDF, Turtle, SPARQL)**
  - Why needed here: ITOPS is published in OWL RL dialect; understanding reasoning profiles, serialization formats, and query semantics is prerequisite for extending or querying the ontology.
  - Quick check question: Can you explain why OWL RL enables "efficiently computable" queries compared to OWL DL?

- **Knowledge Graph T-Box vs. A-Box and ISA Hierarchies**
  - Why needed here: The pipeline builds a T-Box (concept hierarchy) from Wikidata, conflating `subclassOf` and `instanceOf` into `subConceptOf`; understanding this design choice is critical for interpreting results.
  - Quick check question: What practical problems might arise from "ISA overloading" in a dynamic ontology?

- **Sentence Embeddings for Semantic Similarity**
  - Why needed here: S3 relies on USE to encode propositionalized entities and glossary definitions for similarity-based attachment.
  - Quick check question: How does predicate-based propositionalization differ from directly embedding entity labels, and what are the tradeoffs?

## Architecture Onboarding

- **Component map**:
  GLO (General Library of Objects) -> S1 Module (Explicit LOD Extraction) -> S2 Module (Implicit LOD via Wikipedia Categories) -> S3 Module (Glossary Extension) -> Evaluation Layer

- **Critical path**:
  1. Define positive/negative seed concepts for target domain
  2. Run S1 extraction to produce initial subgraph
  3. Train S2 category classifier on manually labeled sample (requires SME input)
  4. Run S2 expansion; validate new entities for domain relevance
  5. Encode S2 entities and glossary terms with USE; run S3 attachment
  6. Evaluate coverage, conciseness, and downstream task performance (e.g., terminology ranking)

- **Design tradeoffs**:
  - **ISA overloading vs. strict T-Box**: Simplifies dynamic ontology maintenance but may reduce reasoning precision
  - **topK=5 vs. topK=10 for glossary attachment**: Paper found similar performance; lower K reduces noise but may miss valid parents
  - **Automated vs. manual evaluation**: Manual evaluation provides ground truth but doesn't scale; automated validation using leaf deletion assumes sub-tree structure is representative

- **Failure signatures**:
  - **Low coverage in S1**: Seed concepts may be too narrow or Wikidata may lack domain entities; consider expanding seed list or using alternative LOD sources
  - **High NotReachable rate in S3 (>30%)**: Sub-trees with height â‰¤2 may lack representative information after leaf deletion; consider restructuring or enriching propositionalizations
  - **Poor inter-annotator agreement**: Indicates ambiguous annotation guidelines or domain complexity; refine SME instructions

- **First 3 experiments**:
  1. **Seed sensitivity analysis**: Vary the number and specificity of seed concepts; measure impact on S1 coverage and downstream ranking performance
  2. **Classifier ablation**: Train S2 classifier with different feature subsets (e.g., only overlap metrics vs. full feature set); evaluate precision/recall on held-out categories
  3. **Embedding model comparison**: Replace USE with alternative sentence encoders (e.g., SBERT, domain-tuned models); measure impact on S3 attachment accuracy and distance-from-ground-truth metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the domain-agnostic pipeline maintain comparable coverage and conciseness when applied to specialized domains other than IT, such as oil and gas or finance?
- Basis in paper: [inferred] The authors state the pipeline was designed to be "domain-agnostic so it can be leveraged in other domains" (Page 2), but all evaluation results presented are specific to the IT Operations (ITOPS) ontology.
- Why unresolved: The pipeline's reliance on Wikipedia categories and Wikidata may perform differently in domains with sparser linked data or different structural characteristics than IT.
- What evidence would resolve it: Application of the pipeline to a distinct domain (e.g., finance) with resulting metrics for coverage, conciseness, and attachment accuracy similar to the ITOPS evaluation.

### Open Question 2
- Question: How can the S2 extraction phase be refined to ensure critical parent concepts are retained, reducing the high rate of glossary terms labelled as -1 (missing parent)?
- Basis in paper: [explicit] The evaluation notes that "a considerable number of terms are labelled as -1... because their parent concepts are not present in S2" (Page 3), causing the attachment algorithm to default to the root node.
- Why unresolved: The current extraction method (S1/S2) fails to capture specific mid-level concepts (e.g., `database_keyword`) required to correctly ground glossary terms.
- What evidence would resolve it: A modified extraction logic that successfully captures these missing parents, resulting in a lower percentage of terms assigned the -1 label in manual evaluation.

### Open Question 3
- Question: Does enriching the semantic information of nodes in shallow subtrees reduce the percentage of "NotReachable" entities during automated validation?
- Basis in paper: [explicit] The authors hypothesize that the "NotReachable" error rate (~30%) occurs because "subtrees with the tree height being <= 2... might not have enough representative information" (Page 3).
- Why unresolved: The paper identifies the lack of representative information as a possible cause but does not implement or test a solution to verify if adding descriptions or context fixes the path-finding issue.
- What evidence would resolve it: An experiment where semantic descriptions are enhanced for nodes in shallow subtrees, showing a statistically significant reduction in the "NotReachable" metric.

## Limitations
- Key implementation details are unspecified, including exact seed concepts, classifier architecture, and propositionalization templates
- Evaluation relies heavily on Wikidata coverage and conciseness metrics with limited external validation
- Effectiveness across diverse domains is untested; performance may degrade with sparse LOD structure

## Confidence
- **High**: Extracting a domain-focused subgraph from Wikidata using seed concepts and ISA relations (S1)
- **Medium**: Binary classifier filtering Wikipedia categories for domain relevance (S2)
- **Medium**: Embedding-based glossary term attachment (S3)

## Next Checks
1. **Seed sensitivity analysis**: Systematically vary the number and specificity of seed concepts; measure impact on S1 coverage and downstream terminology ranking performance
2. **Classifier ablation study**: Train S2 classifier with different feature subsets; evaluate precision/recall on held-out categories to isolate feature contributions
3. **Embedding model comparison**: Replace Universal Sentence Encoder with alternative sentence encoders; measure impact on S3 attachment accuracy and distance-from-ground-truth metrics