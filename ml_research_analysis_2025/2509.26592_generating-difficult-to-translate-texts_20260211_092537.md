---
ver: rpa2
title: Generating Difficult-to-Translate Texts
arxiv_id: '2509.26592'
source_url: https://arxiv.org/abs/2509.26592
tags:
- translation
- mt-breaker
- source
- human
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MT-breaker uses an LLM to iteratively modify source texts and probe
  a machine translation model for weaknesses, mimicking human experts. This produces
  harder-to-translate examples that are more challenging for the targeted MT model
  while maintaining naturalness and diversity compared to baseline generation methods.
---

# Generating Difficult-to-Translate Texts

## Quick Facts
- arXiv ID: 2509.26592
- Source URL: https://arxiv.org/abs/2509.26592
- Reference count: 27
- Primary result: MT-breaker iteratively modifies source texts using an LLM and probes an MT model for weaknesses, creating harder-to-translate examples that transfer across models and languages while maintaining naturalness and diversity.

## Executive Summary
MT-breaker is a method for generating difficult-to-translate texts by iteratively modifying source texts through an LLM that probes a target machine translation model for weaknesses. The process starts with a seed text, translates it, and uses the translation to guide the LLM in modifying the source to increase difficulty. This creates examples that are more challenging for the targeted MT model while maintaining naturalness and diversity compared to baseline generation methods. Human evaluation confirms the difficulty, and the method demonstrates transferability across different MT models and languages.

## Method Summary
MT-breaker generates difficult-to-translate texts through an iterative feedback loop between an LLM and a target MT model. Starting with 100 English seed texts from WMT 2024, the method runs 10 iterative steps where the target MT model translates the current source, then the LLM (Gemini 2.5-pro) modifies the source based on the translation and conversation history. After N steps, quality estimation scores (MetricX-24-QE + LLM-as-QE) are computed for all intermediate translations, and the source with the lowest QE score is selected as the final output. The method is tested against multiple MT models including Google Translate, Gemini 2.0/2.5-flash, and Gemma 3-27b-it, with target languages including Czech, German, Chinese, Vietnamese, and Polish.

## Key Results
- MT-breaker produces examples with significantly lower QE scores than baselines, indicating higher difficulty for target MT models
- Seeded variants maintain high naturalness (>82) and diversity while achieving strong difficulty, outperforming seedless variants which sacrifice naturalness for difficulty
- Generated examples demonstrate transferability, remaining difficult for MT models other than the one used during generation

## Why This Works (Mechanism)

### Mechanism 1
Iterative feedback between an LLM and target MT model systematically increases translation difficulty. The process starts with a seed text, translates it, and uses the LLM to identify potential failure points in the translation to guide source text modifications. This loop accumulates errors or ambiguities that the specific MT model is vulnerable to. Core assumption: the LLM has sufficient metacognitive ability to analyze translations and generate modifications that exploit identified weaknesses.

### Mechanism 2
Quality Estimation (QE) scores provide a viable proxy signal for difficulty, enabling automated selection of the hardest example from generation trajectories. After N iterative steps, a QE metric scores each intermediate translation, and the source text corresponding to the lowest QE score is selected. This filters out random walks that don't increase difficulty. Core assumption: the chosen QE metric is robust enough to rate adversarially-generated texts and correlates with human perception of difficulty.

### Mechanism 3
Constraining generation with natural "seed" texts preserves data diversity and naturalness better than synthesis from scratch. Instead of prompting the LLM to "write a difficult sentence," the system forces modifications upon existing natural text, anchoring the output in realistic domains and linguistic structures. This prevents the model from drifting into repetitive or unnatural adversarial patterns. Core assumption: natural corpora contain a wider and more representative distribution of topics and styles than generic "be difficult" prompts.

## Foundational Learning

**Quality Estimation (QE) Metrics**: Used to rank translations without ground-truth references. Critical because the entire selection process depends on trusting that low QE scores imply hard or bad translations. Quick check: If the QE metric is biased against short sentences, how would that affect selection of "difficult" examples?

**LLM-as-a-Judge / Critique**: The core engine requires the LLM to act as a critic of MT output, identifying errors to guide modifications. Quick check: What happens if the LLM judge is miscalibrated and rates a perfect translation as poor? How does that impact the next generation step?

**Adversarial Examples in NLP**: MT-breaker is a form of adversarial attack. Understanding that models have brittle decision boundaries helps explain why iterative, small modifications can cause catastrophic failure (difficulty spikes). Quick check: Is the goal to create imperceptible perturbations or any input that causes failure, even if semantically distinct?

## Architecture Onboarding

**Component map**: Seed Selector -> Target MT Model -> LLMstep -> QE Scorer -> Orchestrator (loop N times, select arg min QE)

**Critical path**: The prompt engineering within LLMstep, where translation errors must be interpreted. If the LLM ignores translation errors or modifies text randomly, difficulty will not increase iteratively.

**Design tradeoffs**:
- Seeded vs. Seedless: Seeded maintains naturalness and diversity; seedless maximizes raw difficulty but produces unnatural, lower-diversity data
- In-loop QE vs. Post-hoc Selection: Showing QE score during generation can lower variance but may prematurely narrow the search space

**Failure signatures**:
- Collapse: Generated text becomes gibberish or ungrammatical (Naturalness drops)
- Stagnation: QE score plateaus or increases after step 3-4, indicating LLM has run out of ideas
- Metric Hacking: Text includes weird punctuation or formatting that hurts QE score but doesn't make translation task semantically harder

**First 3 experiments**:
1. **Ablation on Loop Length**: Run MT-breaker with N=5 vs N=10 vs N=20. Plot QE score curve to find point of diminishing returns.
2. **Transfer Check**: Generate difficult examples using Model A and evaluate on Model B. Quantify how much "difficulty" transfers using table format.
3. **Error Pattern Analysis**: Take the 5 lowest-scoring examples and manually inspect them. Verify if low QE score is due to genuine semantic difficulty or superficial artifacts.

## Open Questions the Paper Calls Out

**Human vs Machine Difficulty**: Do texts generated by MT-breaker pose disproportionate difficulty for human translators compared to machine translation models? The paper hypothesizes some texts may be difficult for humans, not just MT models, but hasn't measured relative translation effort or accuracy for humans.

**Hillclimbing for Robustness**: Can difficult examples be effectively used for fine-tuning to improve model robustness? The paper suggests using generated datasets to find weaknesses for hillclimbing during model development, but hasn't demonstrated this utility as a training signal.

**Domain Conditioning Impact**: Does domain conditioning in the Zeroshot baseline significantly improve its diversity and difficulty? The paper excluded this feature for comparability with prior work, leaving open whether the baseline was handicapped.

## Limitations
- Human evaluation scalability is uncertain due to subjective, expensive nature and unspecified inter-annotator agreement
- QE metric reliability is questionable when evaluating adversarially-generated text that may contain artifacts designed to fool the metric
- Transferability ambiguity exists as the paper doesn't quantify how much difficulty transfers or explore whether examples are hard for fundamental vs model-specific reasons

## Confidence
- **High confidence**: The iterative feedback loop between LLM and MT model is a valid mechanism for generating harder examples with clear algorithmic description
- **Medium confidence**: Claim that MT-breaker produces "more challenging" examples is supported by QE scores and human naturalness judgments, but methodology is underspecified
- **Low confidence**: Transferability claim lacks quantitative depth; showing examples hard for one model are also hard for another is necessary but not sufficient

## Next Checks
1. **QE Robustness Test**: Compare human judgments of actual difficulty between MT-breaker examples and examples optimized for low QE scores through superficial means to validate MT-breaker's difficulty isn't an artifact of QE metric gaming.

2. **Transfer Difficulty Quantification**: For each target language, generate difficult examples using one model and evaluate on multiple other models. Compute correlation between difficulty scores and report average degradation when transferring.

3. **Error Pattern Analysis**: Manually inspect the 10 lowest-scoring examples per language to categorize error types (idioms, ambiguity, rare words, syntactic complexity) and assess whether they align with known MT failure modes.