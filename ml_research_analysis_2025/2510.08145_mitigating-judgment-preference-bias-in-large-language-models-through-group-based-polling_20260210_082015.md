---
ver: rpa2
title: Mitigating Judgment Preference Bias in Large Language Models through Group-Based
  Polling
arxiv_id: '2510.08145'
source_url: https://arxiv.org/abs/2510.08145
tags:
- judgment
- genii
- response
- preference
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Genii, a group-based polling optimization framework
  that mitigates judgment preference bias in large language models (LLMs) used as
  automatic evaluators. The core idea is to integrate multiple LLM-based judgment
  models into a multi-agent system where agents interact through a client-server polling
  mechanism, optimizing each model unsupervisedly by aggregating group consistency
  scores to curate preference pairs.
---

# Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling

## Quick Facts
- arXiv ID: 2510.08145
- Source URL: https://arxiv.org/abs/2510.08145
- Reference count: 12
- Core result: Unsupervised group-based polling framework outperforms supervised models on judgment bias mitigation without human annotations

## Executive Summary
This paper addresses the critical challenge of judgment preference bias in Large Language Models (LLMs) used as automatic evaluators. The authors propose Genii, a novel group-based polling optimization framework that leverages multiple LLM-based judgment models in a multi-agent system. Through a client-server polling mechanism that aggregates group consistency scores, Genii unsupervisedly optimizes each model by curating preference pairs, effectively mitigating the tendency of models to favor their own outputs.

The framework demonstrates significant improvements in judgment accuracy across multiple benchmarks while eliminating the need for expensive human-labeled annotations. Notably, Genii shows consistent performance improvements even when weaker models serve as server agents, and successfully reduces models' self-preference bias, yielding more reliable and objective judgments in preference evaluation tasks.

## Method Summary
Genii implements a multi-agent system where multiple LLM-based judgment models interact through a client-server polling mechanism. Each agent acts as both client and server in different interactions, generating preference judgments that are aggregated through group consistency scores. These scores are then used to unsupervisedly optimize the judgment models by curating preference pairs. The framework iteratively refines the models' evaluation capabilities by leveraging the collective judgment of the group, effectively reducing individual biases through collaborative learning. The approach operates without requiring human-annotated training data, instead relying on the internal consistency of the agent group to drive model improvement.

## Key Results
- Genii outperforms supervised models trained on annotated judgment data across multiple benchmarks
- Framework achieves significant improvements in judgment accuracy without requiring human-labeled annotations
- Consistently improves performance across different client agents during polling, even with weaker models as servers
- Effectively reduces models' tendency to favor their own incorrect answers, yielding more reliable judgments

## Why This Works (Mechanism)
The mechanism works by creating a collaborative multi-agent system where individual LLM biases are balanced through group consensus. When models make judgments, their outputs are compared against the collective consistency of the group rather than absolute standards. This approach exploits the diversity of different models to identify and correct individual biases, as models that consistently deviate from group consensus are iteratively adjusted. The polling mechanism ensures that no single model's bias dominates the evaluation process, while the unsupervised optimization through group consistency scores provides a scalable way to improve judgment accuracy without human intervention.

## Foundational Learning
- **Multi-agent systems**: Why needed - to aggregate diverse judgments and reduce individual biases; Quick check - verify that agents can communicate effectively and maintain consistent states
- **Unsupervised optimization**: Why needed - to eliminate dependency on expensive human-labeled data; Quick check - ensure consistency scores accurately reflect group agreement
- **Group consistency scoring**: Why needed - to measure collective judgment reliability; Quick check - validate that scores correlate with actual judgment quality
- **Preference pair curation**: Why needed - to create training signals from collective judgments; Quick check - confirm curated pairs capture meaningful preference distinctions
- **Client-server polling mechanism**: Why needed - to structure agent interactions and information flow; Quick check - ensure polling rounds converge to stable judgments
- **Bias mitigation through diversity**: Why needed - to counteract individual model limitations; Quick check - verify that ensemble performance exceeds individual model capabilities

## Architecture Onboarding

**Component Map**: Client Models -> Polling Server -> Group Consistency Engine -> Preference Pair Curator -> Model Optimizer -> Updated Client Models

**Critical Path**: Judgment Generation → Consistency Aggregation → Preference Curation → Model Update → Next Polling Round

**Design Tradeoffs**: The framework trades computational overhead of multiple model evaluations against the benefit of reduced bias and improved accuracy. While simpler ensemble methods might be more efficient, they lack the iterative refinement capability that makes Genii effective. The unsupervised approach eliminates annotation costs but may be slower to converge than supervised alternatives.

**Failure Signatures**: Poor performance when agent diversity is low, indicating insufficient variation to identify and correct biases. Oscillation in consistency scores suggests unstable polling dynamics. Failure to improve on simple baselines may indicate ineffective preference pair curation or inadequate optimization signals.

**First 3 Experiments**:
1. Test baseline performance with homogeneous agent group to verify diversity requirement
2. Measure convergence rate of consistency scores across polling rounds
3. Evaluate sensitivity to group size by varying the number of participating agents

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation relies primarily on benchmark datasets with limited real-world deployment assessment
- Unsupervised approach may inherit biases present in constituent LLM training data
- Scalability to larger groups of diverse models and computational efficiency compared to simpler methods remains unclear

## Confidence

**High confidence**: The core mechanism of using group consistency scores to optimize judgment models is technically sound and the reported improvements over baseline models are statistically significant across multiple benchmarks

**Medium confidence**: The claim that Genii consistently improves performance regardless of client-server agent strength requires further validation across broader model families and task types

**Medium confidence**: The assertion that the approach reduces self-preference bias needs additional testing on more diverse and challenging preference judgment scenarios

## Next Checks
1. Conduct ablation studies to quantify the contribution of each component in the polling mechanism and test sensitivity to different group sizes and model combinations
2. Evaluate performance on out-of-distribution evaluation tasks and real-world preference judgment scenarios to assess generalizability beyond curated benchmarks
3. Perform robustness testing by introducing adversarial preference pairs and measuring how well the framework maintains consistent judgments across different agent configurations