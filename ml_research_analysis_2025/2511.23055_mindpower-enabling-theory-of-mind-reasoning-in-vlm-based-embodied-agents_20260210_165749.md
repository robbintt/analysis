---
ver: rpa2
title: 'MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents'
arxiv_id: '2511.23055'
source_url: https://arxiv.org/abs/2511.23055
tags:
- reasoning
- action
- object
- mindpower
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MindPower, a framework enabling Theory-of-Mind
  (ToM) reasoning in vision-language embodied agents. It addresses the gap in current
  agents' inability to reason about human mental states and their own perspective,
  which limits autonomous decision-making.
---

# MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents

## Quick Facts
- arXiv ID: 2511.23055
- Source URL: https://arxiv.org/abs/2511.23055
- Reference count: 40
- Primary result: MindPower enables ToM reasoning in embodied agents, outperforming GPT-4o by 12.77% in decision accuracy and 12.49% in action generation on a 590-scenario benchmark.

## Executive Summary
MindPower introduces a framework that enables vision-language embodied agents to perform Theory-of-Mind (ToM) reasoning, addressing the gap in current agents' inability to reason about human mental states and their own perspective. The framework integrates Perception, Mental Reasoning, Decision Making, and Action through a Robot-Centric hierarchy, allowing agents to infer beliefs, desires, and intentions of both themselves and humans. Evaluated on 590 interactive home scenarios, MindPower achieves substantial improvements in ToM-grounded embodied reasoning through its structured BDI reasoning hierarchy and Mind-Reward reinforcement optimization.

## Method Summary
MindPower is a two-stage trained model on Qwen2.5-VL-7B-Instruct. Stage 1 uses Supervised Fine-Tuning (SFT) for 5 epochs on annotated data with 6-layer hierarchical outputs. Stage 2 employs Group Relative Policy Optimization (GRPO) with Mind-Reward, computing rewards from ROUGE-based atomic action sequence overlap (R_atomic, R_local, R_global) plus Format-Reward for hierarchy compliance. The framework processes 32-frame video inputs with optional dialogue, producing structured outputs from Perception through Action layers, with each layer conditioning the next. Mind-Reward aligns intermediate reasoning traces with executable atomic actions.

## Key Results
- Achieves 15.40% action correctness with SFT+Mind-Reward vs. 10.48% with SFT-only
- Outperforms GPT-4o by 12.77% in decision accuracy and 12.49% in action generation
- Maintains high BDI and Perspective Consistency scores across benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1: Structured BDI Reasoning Hierarchy
The MindPower Hierarchy decomposes embodied reasoning into six sequential layers—<Perception>, <Belief>, <Desire>, <Intention>, <Decision>, <Action>—creating a causal chain from sensory input to motor output. Each layer produces structured text that conditions subsequent layers. Removing this hierarchy caused GPT-4o's action accuracy to drop from 2.91% to 0.82%.

### Mechanism 2: Robot-Centric Dual-Perspective Modeling
At the <Belief> layer, the agent generates second-order beliefs: "I believe Alice thinks the apple is on the table, but I believe it is in the refrigerator." This perspective separation allows the agent to detect belief contradictions and plan corrective actions.

### Mechanism 3: Mind-Reward GRPO Optimization
Mind-Reward computes R = α₁R_atomic + α₂R_local + α₃R_global (weighted ROUGE scores over atomic action sequences) plus a Format-Reward for hierarchy compliance. GRPO samples multiple outputs, computes advantages via group normalization, and updates policy to maximize reward.

## Foundational Learning

- **Belief-Desire-Intention (BDI) Framework**
  - Why needed here: The entire MindPower architecture is built on BDI as the cognitive scaffold for ToM reasoning.
  - Quick check question: Can you explain how a desire differs from an intention in the BDI model?

- **Second-Order False Belief**
  - Why needed here: The benchmark's False-Belief Correction task requires reasoning about what someone else believes about a third party's beliefs.
  - Quick check question: In the Sally-Anne task, what does Anne believe about Sally's belief?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Mind-Reward uses GRPO for fine-tuning; understanding advantage computation is essential for debugging training.
  - Quick check question: How does GRPO's advantage calculation differ from standard PPO?

## Architecture Onboarding

- **Component map:**
  Input -> Perception layer -> Mental Reasoning (Belief/Desire/Intention) -> Decision -> Action
  Video frames (32) + optional dialogue text -> text description of scene -> structured text with agent/human tags -> natural language plan -> atomic action sequence: `action(object)`

- **Critical path:**
  1. Perception must capture action sequences and object locations (not just scene descriptions)
  2. Belief layer must correctly attribute beliefs to specific agents
  3. Action layer must use only allowed atomic verbs from the unified action set

- **Design tradeoffs:**
  - Open-ended generation vs. constrained action vocabulary (paper chooses hybrid: natural language reasoning + atomic action output)
  - Explicit hierarchy increases token count but improves interpretability
  - Simulator-limited environments (VirtualHome, ThreeDWorld) may not transfer directly to real-world robotics

- **Failure signatures:**
  - Stereotypical predictions (e.g., "kitchen → cleaning" regardless of actual human behavior)
  - Perspective confusion (attributing robot's knowledge to human)
  - Non-executable actions (e.g., `identify()` instead of `pick()`)
  - Missing hierarchy tags in output (Format-Reward = 0)

- **First 3 experiments:**
  1. Reproduce the ablation: compare full hierarchy vs. direct Decision/Action output on a 50-sample subset using GPT-4o.
  2. Validate Mind-Reward components: ablate R_atomic, R_local, R_global individually to measure contribution.
  3. Test cross-simulator generalization: train on VirtualHome scenarios, evaluate on ThreeDWorld held-out set.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can implicit mental-state modeling be developed to reduce the token length of the reasoning hierarchy while maintaining interpretability?
- **Open Question 2**: Does the MindPower framework maintain its decision-making accuracy when deployed on physical robotic platforms?
- **Open Question 3**: How does the Robot-Centric perspective scale when extended to scenarios requiring complex multi-agent coordination?

## Limitations
- Performance gains measured in controlled simulator environments with pre-defined atomic action sets
- Mind-Reward assumes n-gram overlap correlates with true ToM reasoning quality
- 590-scenario benchmark represents limited sample of household interaction diversity

## Confidence
- **High confidence**: The structured BDI hierarchy improves over direct output generation (supported by clear ablation results)
- **Medium confidence**: Dual-perspective modeling provides meaningful advantage beyond command-following (limited direct evidence, relies on related work)
- **Medium confidence**: Mind-Reward optimization meaningfully aligns reasoning with actions (reward design is plausible but correlation with true ToM is untested)

## Next Checks
1. Conduct cross-simulator transfer: train on VirtualHome, evaluate on held-out ThreeDWorld scenarios to test generalization beyond training distribution.
2. Perform human evaluation on action execution feasibility: have human raters assess whether generated atomic action sequences are actually executable in the described scenes.
3. Test robustness to perception noise: systematically corrupt Perception layer outputs and measure degradation in downstream ToM reasoning quality.