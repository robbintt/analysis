---
ver: rpa2
title: 'Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language
  Models'
arxiv_id: '2510.07642'
source_url: https://arxiv.org/abs/2510.07642
tags:
- access
- setting
- control
- user
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how large language models can enforce role-based
  access control when answering database queries. The authors extend the Spider and
  BIRD text-to-SQL datasets with realistic PostgreSQL RBAC policies at table and column
  levels, creating a benchmark for role-conditioned refusals.
---

# Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2510.07642
- **Source URL**: https://arxiv.org/abs/2510.07642
- **Reference count**: 24
- **Primary result**: Fine-tuning with role-permission supervision achieves the strongest balance between safety and utility for access control in LLM-based text-to-SQL systems.

## Executive Summary
This paper evaluates how large language models can enforce role-based access control when answering database queries. The authors extend the Spider and BIRD text-to-SQL datasets with realistic PostgreSQL RBAC policies at table and column levels, creating a benchmark for role-conditioned refusals. They compare three strategies: zero/few-shot prompting, a two-step generator-verifier pipeline, and LoRA fine-tuning. Across multiple model families, explicit verification in the two-step pipeline improves refusal precision and lowers false permits, while fine-tuning achieves the strongest balance between safety and utility, especially in execution accuracy. Performance declines with longer and more complex policies.

## Method Summary
The authors create Spider-ACL and BIRD-ACL datasets with PostgreSQL RBAC policies at table and column levels. Each database has four hierarchical roles (unrestricted to minimal visibility). Three settings are compared: (1) single-model prompting with policy in prompt, (2) two-step generator-verifier pipeline where SQL is generated then verified against policy, and (3) LoRA fine-tuning with role-conditioned supervision. Models evaluated include LLaMA-3.1-8B, Mistral-7B-Instruct, GPT-4o-mini, and DeepSeek-R1-Distill-Qwen-14B. Ground truth labels are generated via deterministic policy engine checking table/column visibility against user roles.

## Key Results
- Explicit verification (Setting 2) improves refusal precision and lowers false permits compared to single-model reasoning
- Fine-tuning achieves the strongest balance between safety and utility, particularly in execution accuracy
- Performance degrades significantly with longer and more complex policies (F1 drops from ~0.90 to ~0.69 as policy length increases)

## Why This Works (Mechanism)

### Mechanism 1: Generator-Verifier Separation
Decoupling SQL generation from policy enforcement improves refusal precision by preventing the generator from conflating utility with safety. A generator produces SQL without policy awareness, then a separate verifier checks the query against the user's role permissions. This specialization prevents models from trying to optimize both correctness and safety simultaneously.

### Mechanism 2: Role-Conditioned Supervision via LoRA
Fine-tuning with role-permission supervision enables models to internalize access boundaries. Training examples pair (schema, question, role, policy) with either SQL output (PERMIT) or "Access Denied" (DENY). LoRA adapters modify attention projections to encode permission-aware behavior without full model retraining.

### Mechanism 3: Chain-of-Thought Policy Grounding
Explicit step-by-step reasoning about required schema elements and policy constraints improves access decisions. Models first identify which tables/columns a question requires, then explicitly check these against the user's allowed permissions before deciding. This forces policy-text-to-schema mapping rather than pattern matching.

## Foundational Learning

- **Concept: Role-Based Access Control (RBAC) in Databases**
  - Why needed: The paper assumes familiarity with GRANT statements, table/column-level permissions, and role hierarchies. Without this, the policy encoding and verification logic are opaque.
  - Quick check question: Given `GRANT SELECT (name, dept) ON employees TO role_analyst`, can a user with `role_analyst` query `SELECT name, salary FROM employees`?

- **Concept: Text-to-SQL Task Structure**
  - Why needed: The entire evaluation framework builds on Spider/BIRD benchmarks. Understanding schema linking, cross-domain generalization, and execution accuracy metrics is prerequisite to interpreting results.
  - Quick check question: Why might a model generate syntactically valid SQL that fails execution accuracy evaluation?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed: Setting 3 relies on LoRA adapters for permission-aware training. Understanding rank, alpha, and target modules is necessary to reproduce or extend the approach.
  - Quick check question: If LoRA rank is increased from 16 to 64, what trade-offs in memory, training time, and overfitting risk should you expect?

## Architecture Onboarding

- **Component map**: Spider/BIRD datasets -> RBAC-Extended datasets (Spider-ACL, BIRD-ACL) -> Policy Engine (ground truth labeling) -> Setting 1/2/3 modules -> Evaluation metrics

- **Critical path**: Dataset preparation (convert SQLite→PostgreSQL, define 4 roles per DB, generate GRANT statements) -> Ground-truth labeling (run policy engine to tag PERMIT/DENY for each query-role pair) -> Model configuration (select backbone, choose Setting 1/2/3, configure prompts) -> Evaluation (refusal F1, execution accuracy, leakage rate)

- **Design tradeoffs**: Setting 2 adds inference latency (two model calls) but improves refusal precision by 8–15 F1 points; Setting 1 is faster but has higher false-permit rates. Fine-tuning achieves best Spider performance but requires training data and may not generalize to new schemas.

- **Failure signatures**: Mixed-mode refusals where models output "Access Denied" followed by SQL anyway (common in smaller models), schema hallucination in large BIRD databases causing false permits, policy-length degradation as policies exceed 10k characters, and over-refusal where legitimate queries are blocked.

- **First 3 experiments**: (1) Reproduce Setting 2 (GPT→GPT zero-shot) on Spider-ACL subset to validate pipeline; target refusal F1 > 0.85, (2) Ablate verifier model by swapping GPT-4o-mini for LLaMA-3.1 and measure precision/recall tradeoff, (3) Test fine-tuned Mistral-7B (Setting 3) on held-out BIRD-ACL split to assess cross-dataset generalization; target F1 > 0.60.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do current refusal strategies generalize to conversational or retrieval settings where access permissions are implicit and context accumulates over multiple turns? The authors state future work should extend evaluation beyond text-to-SQL to conversational and retrieval settings.

- **Open Question 2**: Can LLMs reliably enforce access control when user privileges must be inferred implicitly from metadata (e.g., estimated user age) rather than explicit role assignments? The Conclusion notes plans to explore settings where access control may need to be inferred implicitly.

- **Open Question 3**: How does the reliability of generator-verifier pipelines degrade when faced with conflicting, overlapping, or dynamically changing organizational policies? The Limitations section specifies that current evaluation uses deterministic policies and does not model edge cases such as conflicting or conditional roles.

## Limitations

- The deterministic policy engine assumes perfectly encoded RBAC rules without real-world policy ambiguities or exceptions
- Cross-dataset generalization from Spider to BIRD is notably weak (F1 drops 28-30 points)
- Policy-length degradation indicates fundamental scalability limitations as policies exceed 10k characters

## Confidence

- **High**: Generator-Verifier pipeline improves refusal precision over single-model approaches (consistent F1 gains across all model families in Table 2)
- **Medium**: LoRA fine-tuning achieves the best safety-utility balance (execution accuracy on Spider is strong, but BIRD generalization is limited)
- **Low**: Chain-of-thought reasoning is essential for access control (ablations show impact, but no direct corpus support for CoT in RBAC contexts)

## Next Checks

1. **Policy-Length Stress Test**: Evaluate Setting 2 with GPT-4o-mini on synthetic queries requiring access to >15 tables/columns, measuring refusal F1 as policy length increases from 5k to 25k characters to confirm the observed degradation trend.

2. **Real-World Policy Transfer**: Apply the fine-tuned Mistral-7B model (Setting 3) to a held-out subset of BIRD-ACL with schemas and policies completely disjoint from Spider training data, measuring both refusal F1 and execution accuracy to quantify true generalization.

3. **Verifier Over-Refusal Analysis**: Systematically measure false-refusal rates across different verifier models (GPT-4o-mini, LLaMA-3.1, Mistral-7B) on a balanced test set of permitted/denied queries, identifying the precision-recall tradeoff point where execution accuracy begins to suffer.