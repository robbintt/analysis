---
ver: rpa2
title: Free Argumentative Exchanges for Explaining Image Classifiers
arxiv_id: '2502.12995'
source_url: https://arxiv.org/abs/2502.12995
tags:
- agents
- faxs
- image
- classifiers
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Free Argumentative Exchanges (FAXs), a novel
  multi-agent framework for explaining image classifiers through debates between two
  artificial agents. The method addresses the opacity problem in deep learning by
  representing explanations as argumentative debates, where agents argue for and against
  predicted classes using class-specific features as arguments.
---

# Free Argumentative Exchanges for Explaining Image Classifiers

## Quick Facts
- arXiv ID: 2502.12995
- Source URL: https://arxiv.org/abs/2502.12995
- Reference count: 40
- Primary result: Novel multi-agent debate framework using argumentative exchanges between artificial agents to explain image classifiers with high completeness (0.96-1.00) across datasets

## Executive Summary
This paper introduces Free Argumentative Exchanges (FAXs), a novel multi-agent framework for explaining image classifiers through debates between two artificial agents. The method addresses the opacity problem in deep learning by representing explanations as argumentative debates, where agents argue for and against predicted classes using class-specific features as arguments. Unlike previous approaches, FAXs allow agents to interpret argument relationships differently, creating "free" argumentative exchanges that reveal the reasoning process of the underlying classifier.

## Method Summary
The method trains N class-specific codebooks using Gumbel-softmax quantization with distillation loss to discretize continuous features from a pretrained classifier. N agent sequence models (GRUs) are trained with modulator networks and policy heads using REINFORCE reinforcement learning plus stance loss. At deployment, the top-2 predicted classes are selected and their corresponding agents engage in a debate until resolution or information plateau. Arguments are discrete codebook entries representing image regions, and agents exchange attacks/supports that may be interpreted differently by the receiving agent. The resulting exchange BAF captures not just "which regions" but "how regions relate competitively."

## Key Results
- FAXs achieve high completeness (0.96-1.00) across all settings, indicating quantized classifiers faithfully approximate original models during debates
- Consensus and persuasion rates vary by model certainty - higher for fair/biased models (0.24-0.90) than random models (0.09-0.45)
- FAXs outperform standard baselines like GradCAM and LIME in faithfulness metrics while providing more interpretable, dynamic explanations through agent interactions
- The argumentative structure successfully captures class-discriminative reasoning, with agents consistently resolving debates when meaningful differences exist

## Why This Works (Mechanism)

### Mechanism 1: Bipolar Argumentation with Free Interpretation
When two agents exchange arguments with "free" interpretation of attack/support relations, the resulting debate structure exposes the reasoning process of the underlying classifier more faithfully than static attribution methods. Each agent maintains a private Bipolar Argumentation Framework (BAF) containing arguments (image regions/features) connected by attack or support relations to the explanandum. Crucially, when Agent A contributes a relation, Agent B may internalize it with the opposite polarity. This asymmetry forces explicit reasoning about why features matter differently for each class.

### Mechanism 2: Dialectically Monotonic Evaluation
Constraining agents to interpret incoming arguments such that attackers never increase and supporters never decrease their confidence in the explanandum yields explanations that align with human intuitions about argument strength. At each timestep, when receiving an argument z, an agent queries its private classifier qáµ¢ on existing attackersâˆªsupportersâˆª{z} vs. existing attackersâˆªsupporters alone. If the difference is negative, z is recorded as an attack; if non-negative, as a support.

### Mechanism 3: Quantized Surrogate Distillation
A quantized classifier, trained to distill the original classifier's predictions over discrete class-specific codebooks, provides a faithful proxy that enables argument generation and evaluation without querying the original model at debate time. Continuous features z = f(x) are projected onto class-specific codebooks Äˆ_Å· via Gumbel-softmax sampling. A quantized classifier q is trained with a distillation loss (quantization commitment + cross-entropy toward the original classifier's predicted class).

## Foundational Learning

- **Bipolar Argumentation Frameworks (BAFs)**
  - Why needed here: BAFs provide the formal language for agents to represent and exchange arguments with both attack and support relations. Understanding that an argument's strength depends recursively on its attackers and supporters is essential for following the evaluation method Ïƒ.
  - Quick check question: Given a BAF with arguments {a, b, c} where b supports a and c attacks b, if c is strengthened, should a's strength increase, decrease, or stay the same under dialectical monotonicity?

- **Vector Quantization / Codebook Learning**
  - Why needed here: FAX operates on discrete "arguments" derived from continuous CNN features. Without understanding how codebooks discretize latent space and how commitment losses work, the connection between image regions and debate arguments is opaque.
  - Quick check question: If a codebook has 64 entries per class and images produce 256 feature vectors, how does the system map each vector to a codebook entry, and what does "feature leakage" mean in this context?

- **Policy Gradient (REINFORCE)**
  - Why needed here: Agents learn which arguments to contribute via reinforcement learning, not supervised labels. The reward is based on stance maintenance and persuasion. Without grasping REINFORCE, the training dynamics (variance, baselines, exploration) are difficult to debug.
  - Quick check question: In REINFORCE, why is a baseline bâ‚œáµ¢ subtracted from the reward râ‚œáµ¢, and what happens to training if the baseline is always zero?

## Architecture Onboarding

- **Component map:** Pretrained feature extractor f -> Class-specific codebooks -> Quantized classifier q -> Agent sequence models (GRUs) -> Modulator networks -> Policy networks -> Private classifiers qáµ¢
- **Critical path:** 1) Train fâ€¢g (original classifier) â†’ freeze f. 2) Train codebooks and q jointly via quantization + distillation loss. 3) Initialize N agents (one per class) with shared q, random policy networks. 4) Train agents via REINFORCE with stance loss. 5) Deploy: for each test image, select top-2 classes, run debate between corresponding agents until resolution or information plateau.
- **Design tradeoffs:** Codebook size (larger â†’ finer arguments but underutilized entries), debate length (longer â†’ more thorough but potential redundancy), two-agent vs. multi-class (simpler but may miss nuances), faithfulness vs. interpretability (completeness scores quantify fidelity gap).
- **Failure signatures:** Low consensus + low persuasion suggests agents cannot find common ground despite feature leakage; high completeness but low correctness indicates quantized classifier matches g but g itself is inaccurate; stuck debates may indicate no conflict between top classes; repetitive arguments suggest policy collapse.
- **First 3 experiments:** 1) Ablation on codebook size (16, 32, 64, 128 entries) to identify capacity sweet spot. 2) Baseline faithfulness comparison on CIFAR-10 to confirm FAX completeness remains high (>0.90). 3) Human evaluation of argument interpretability with 5 raters assessing agreement between argument visualizations and agent assignments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FAXs effectively uncover shortcuts and spurious correlations in image classifiers?
- Basis in paper: "We plan to investigate whether FAXs can uncover shortcuts in classifiers."
- Why unresolved: The current work focused on demonstrating FAXs' faithfulness and argumentative quality metrics, but did not test whether the debate structure can identify when classifiers rely on dataset artifacts or biases rather than meaningful features.

### Open Question 2
- Question: How do humans interpret and evaluate FAX explanations compared to traditional heatmap-based methods?
- Basis in paper: The paper conducts only quantitative evaluation using automated metrics (completeness, consensus, persuasion rate) without human subject studies to assess actual interpretability.
- Why unresolved: While FAXs show higher faithfulness metrics than GradCAM, DeepLIFT, DeepSHAP, and LIME, the authors acknowledge the need for "deeper and more dynamic explanations" but do not empirically validate that humans find argumentative exchanges more useful.

### Open Question 3
- Question: Can FAXs scale to settings with more than two agents arguing for multiple classes simultaneously?
- Basis in paper: The framework definition allows for |ð’œ| = m â‰¥ 2 agents, but implementation restricts to strictly interleaved FAXs with exactly two agents.
- Why unresolved: Multi-class debates may reveal more nuanced reasoning but require new coordination protocols, reward structures, and evaluation metrics beyond the binary consensus/persuasion measures defined.

### Open Question 4
- Question: Does replacing vector quantization with grounded object-centric methods improve the human understandability of FAX arguments?
- Basis in paper: "We also plan to explore the use of object identification methods such as grounded slot attention, instead of quantization, to improve the human understandability of arguments in our FAXs."
- Why unresolved: Current arguments are discrete latent features from codebooks that may not align with semantically meaningful concepts humans can interpret.

## Limitations
- The method's reliance on discrete codebook arguments introduces a fidelity gap between the original classifier's reasoning and the debate representation, with completeness scores indicating this gap exists.
- The bipolar argumentation framework assumes that meaningful class-discriminative features can be extracted as discrete units, which may not hold for datasets with significant class overlap or subtle decision boundaries.
- The assertion that free interpretation of attack/support relations uniquely benefits explanation quality is plausible but not empirically isolated from other factors like quantization quality or policy learning.

## Confidence
- **High confidence:** The formal definitions of FAXs, dialectical monotonicity, and quantization loss are clearly specified and theoretically sound. Quantitative results are internally consistent across datasets and classifier types.
- **Medium confidence:** The claim that FAXs provide more interpretable explanations than static attribution methods is supported by the argumentative structure but lacks direct human evaluation of interpretability.
- **Low confidence:** The assertion that free interpretation of attack/support relations uniquely benefits explanation quality is plausible but not empirically isolated from other factors like quantization quality or policy learning.

## Next Checks
1. **Human Interpretability Test:** Conduct a user study where participants rate the clarity and helpfulness of FAX debates versus GradCAM/DeepLIFT explanations on a subset of images.
2. **Codebook Capacity Sensitivity:** Systematically vary codebook sizes (e.g., 16, 32, 64, 128 entries per class) and measure the impact on completeness, consensus, and argument diversity.
3. **Cross-Dataset Generalization:** Apply FAX to a new dataset (e.g., CIFAR-10) and verify that completeness remains high (>0.90) and that consensus/persuasion patterns align with classifier fidelity.