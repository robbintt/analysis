---
ver: rpa2
title: A Large-Scale Benchmark for Vietnamese Sentence Paraphrases
arxiv_id: '2502.07188'
source_url: https://arxiv.org/abs/2502.07188
tags:
- paraphrase
- sentence
- dataset
- paraphrases
- nguyen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViSP, a large-scale Vietnamese paraphrase
  dataset with 1.2M original-paraphrase pairs. It was constructed using a hybrid approach
  combining automatic paraphrase generation with manual evaluation to ensure high
  quality.
---

# A Large-Scale Benchmark for Vietnamese Sentence Paraphrases

## Quick Facts
- **arXiv ID:** 2502.07188
- **Source URL:** https://arxiv.org/abs/2502.07188
- **Reference count:** 29
- **Primary result:** ViSP is a 1.2M Vietnamese paraphrase dataset built via hybrid automatic generation and manual verification, with BARTpho-wordlarge achieving BLEU-4 of 72.06 and ROUGE-2 of 76.06 on the validation set.

## Executive Summary
This paper introduces ViSP, a large-scale Vietnamese paraphrase dataset containing 1.2 million original-paraphrase pairs. The dataset was constructed using a hybrid approach that combines automatic paraphrase generation techniques (EDA and back-translation) with manual evaluation to ensure quality. The resulting dataset serves as a valuable resource for Vietnamese NLP research and applications.

The authors evaluate traditional methods, baseline models (BART, T5), and large language models (GPT-4o, Gemini-1.5, etc.) on this benchmark. BARTpho-wordlarge emerges as the best-performing model, achieving state-of-the-art results with BLEU-4 of 72.06 and ROUGE-2 of 76.06 on the validation set. The paper provides a comprehensive analysis of paraphrase generation for Vietnamese and establishes a foundation for future research in this area.

## Method Summary
The dataset construction employs a hybrid methodology combining automatic paraphrase generation with manual evaluation. The authors used existing Vietnamese corpora as source material and applied automatic generation techniques including EDA (Easy Data Augmentation) and back-translation to create initial paraphrase candidates. These candidates were then manually verified by Vietnamese native speaker annotators to ensure quality and semantic equivalence.

For model evaluation, the authors tested traditional methods (EDA, back-translation), baseline transformer models (BART, T5), and large language models (GPT-4o, Gemini-1.5, etc.). The evaluation used standard metrics including BLEU, ROUGE, and BERTScore to compare model performance on the paraphrase generation task. The BARTpho-wordlarge model achieved the best overall performance across these metrics.

## Key Results
- ViSP dataset contains 1.2 million Vietnamese paraphrase pairs constructed via hybrid automatic generation and manual verification
- BARTpho-wordlarge achieves state-of-the-art performance with BLEU-4 of 72.06 and ROUGE-2 of 76.06 on the validation set
- Large language models (GPT-4o, Gemini-1.5) show competitive performance but don't surpass the best baseline models on standard metrics

## Why This Works (Mechanism)
The hybrid approach of combining automatic generation with manual verification addresses the scalability challenge of building large paraphrase datasets while maintaining quality. Automatic methods like EDA and back-translation can efficiently generate large volumes of candidate paraphrases, but they often produce noisy or semantically divergent outputs. Manual verification by native speakers ensures that only valid paraphrases are included, creating a high-quality dataset suitable for training and evaluation.

The success of BARTpho-wordlarge suggests that transformer-based architectures pre-trained on Vietnamese text are particularly effective for paraphrase generation in this language. The model's ability to capture both lexical and syntactic variations while maintaining semantic equivalence indicates that it has learned robust representations of Vietnamese language structure and meaning.

## Foundational Learning

**Automatic paraphrasing techniques (EDA, back-translation)** - These methods provide scalable ways to generate paraphrase candidates by applying simple transformations or leveraging bilingual models. Why needed: Manual paraphrase generation is too slow for large datasets. Quick check: Verify that generated paraphrases maintain semantic meaning while showing lexical/syntactic variation.

**Vietnamese language processing challenges** - Vietnamese has unique linguistic features including tonal system, lack of morphological inflection, and specific syntactic structures. Why needed: Models must be adapted to handle these language-specific characteristics. Quick check: Evaluate model performance on Vietnamese-specific linguistic phenomena like tonal variations and compound word structures.

**Manual quality verification procedures** - Human evaluation ensures that automatically generated paraphrases meet quality standards for semantic equivalence and naturalness. Why needed: Automatic metrics alone cannot fully capture paraphrase quality. Quick check: Assess inter-annotator agreement rates and establish clear guidelines for paraphrase acceptance/rejection.

## Architecture Onboarding

Component map: Source corpora -> Automatic generation (EDA, back-translation) -> Manual verification -> ViSP dataset -> Model training/evaluation

Critical path: The most important pathway is Source corpora -> Automatic generation -> Manual verification -> Model training. Each step must maintain quality to ensure the final dataset is useful for training robust paraphrase models.

Design tradeoffs: The paper balances dataset size against quality by using automatic generation for scalability but manual verification for quality control. This creates a practical compromise between the two competing objectives, though it may introduce some selection bias in what types of paraphrases are included.

Failure signatures: Poor paraphrase quality would manifest as low inter-annotator agreement, high metric scores but poor human evaluation results, or models that generate semantically divergent outputs. The paper's focus on metric evaluation without human validation is a potential weakness.

First experiments to run:
1. Compare automatic metric scores against human judgments of paraphrase quality to validate the correlation
2. Test model performance on out-of-domain Vietnamese text to assess generalization
3. Evaluate the dataset's impact on downstream Vietnamese NLP tasks like machine translation or text simplification

## Open Questions the Paper Calls Out
None

## Limitations
- The manual verification process lacks detailed documentation of annotator training, expertise levels, and quality control procedures, making it difficult to assess the actual quality of the 1.2M pairs
- Dataset construction details are sparse regarding source corpora characteristics, domains, and potential biases, limiting understanding of dataset representativeness
- Evaluation focuses heavily on metric-based comparisons rather than human evaluation of paraphrase quality, with no evidence that generated paraphrases capture true semantic equivalence

## Confidence

Dataset construction methodology: Medium confidence - The approach is described but lacks critical implementation details
Model performance claims: Medium confidence - Results are metric-based without human validation of paraphrase quality
Dataset utility claims: Low confidence - No downstream task evaluations or real-world application demonstrations provided

## Next Checks

1. Conduct human evaluation study with Vietnamese linguists to assess the quality and semantic equivalence of automatically generated paraphrases from the top-performing models, comparing against human-written paraphrases

2. Perform dataset analysis to characterize the source corpora domains, text styles, and potential biases, including statistical analysis of paraphrase types (lexical, syntactic, phrasal) and their distributions

3. Implement and evaluate the dataset on downstream NLP tasks beyond paraphrasing (e.g., machine translation, text simplification, question answering) to assess its broader utility and generalizability

Required section headers to keep/add exactly:
- ## Method Summary
- ## Key Results
- ## Why This Works (Mechanism)
- ## Foundational Learning
- ## Architecture Onboarding
- ## Open Questions the Paper Calls Out
- ## Limitations
- ## Confidence
- ## Next Checks

Rules:
- Keep all required section headers exactly unchanged
- Preserve optional sections (for example: Quick Facts, Executive Summary) if present
- Make minimal necessary edits only
- Do not add YAML frontmatter
- Remove markdown code fences if present
- For weak sections, expand with concise, cautious content derived from existing report context
- Use line-level qualifiers like "Assumption:" or "Unknown:" when needed
- Never replace a required section with a single placeholder sentence