---
ver: rpa2
title: 'Sample, Don''t Search: Rethinking Test-Time Alignment for Language Models'
arxiv_id: '2504.03790'
source_url: https://arxiv.org/abs/2504.03790
tags:
- distribution
- arxiv
- reward
- language
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes QAlign, a test-time alignment method that samples
  from the optimal aligned distribution without retraining. The method uses Markov
  chain Monte Carlo with a language model-based proposal to generate aligned responses.
---

# Sample, Don't Search: Rethinking Test-Time Alignment for Language Models

## Quick Facts
- arXiv ID: 2504.03790
- Source URL: https://arxiv.org/abs/2504.03790
- Reference count: 40
- Primary result: QAlign, a test-time alignment method that samples from optimal aligned distribution without retraining, outperforms best-of-n, majority voting, and weighted majority voting on mathematical reasoning benchmarks using task-specific reward models, and outperforms direct preference optimization (DPO) and other methods across five diverse datasets using a general reward model.

## Executive Summary
This paper introduces QAlign, a test-time alignment method for language models that uses Markov chain Monte Carlo (MCMC) sampling to generate responses aligned with a reward model without requiring model retraining or logit access. The method constructs a Markov chain via Metropolis-Hastings with a language model-based proposal, accepting or rejecting proposed text modifications based on reward differences and a temperature parameter β. QAlign converges to sampling from the optimal aligned distribution π(y|x) ∝ pLM(y|x)·exp(r(y,x)/β) as test-time compute increases, avoiding the reward over-optimization that degrades best-of-n performance.

## Method Summary
QAlign is a test-time alignment method that uses MCMC sampling to generate responses from the optimal aligned distribution π(y|x) ∝ pLM(y|x)·exp(r(y,x)/β) without retraining the base model or requiring logit access. The method employs Metropolis-Hastings MCMC with a QUEST proposal (uniformly sampling an index and generating suffixes from the language model), accepting/rejecting proposals based on reward differences and a temperature parameter β. The acceptance criterion depends only on reward differences and text lengths, making it suitable for black-box LLM APIs. For aggregation, QAlign uses Minimum Bayes Risk selection: majority voting for tasks with exact answers and ROUGE for open-ended tasks.

## Key Results
- QAlign outperforms best-of-n, majority voting, and weighted majority voting on GSM8K and GSM-Symbolic benchmarks using a task-specific reward model
- With the general TÜLU3-8B-RM, QAlign outperforms DPO, best-of-n, majority voting, and weighted majority voting across five diverse datasets including GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA
- QAlign avoids the degradation seen in best-of-n as compute scales, maintaining improvement trajectory while BoN shows inflection points

## Why This Works (Mechanism)

### Mechanism 1
QAlign converges to sampling from the optimal aligned distribution π(y|x) as test-time compute increases, avoiding reward over-optimization. The method constructs a Markov chain via Metropolis-Hastings with a language model proposal (QUEST). The chain accepts/rejects proposed text modifications based on reward differences and a temperature β, ensuring the stationary distribution equals π(y|x) ∝ pLM(y|x)·exp(r(y,x)/β). The Markov chain is ergodic (irreducible and aperiodic), which holds for the QUEST proposal per Appendix C.

### Mechanism 2
QAlign avoids the degradation seen in best-of-n (BoN) and weighted majority voting (WMV) as compute scales, because it samples from a fixed target distribution rather than progressively shifting toward higher reward. BoN selects the max-reward sample, which implicitly targets a distribution that shifts with n (β decreases with n per Eq. 10), leading to over-optimization of imperfect rewards. QAlign fixes β, targeting a consistent distribution.

### Mechanism 3
QAlign can be applied without logit access or model retraining, making it suitable for black-box LLM APIs. The acceptance criterion depends only on reward differences and text lengths, not on pLM logits, because the likelihood ratio cancels the partition function and the proposal likelihood ratio simplifies to a length ratio.

## Foundational Learning

### Concept: Markov Chain Monte Carlo (MCMC) and Metropolis-Hastings
- Why needed here: QAlign is fundamentally an MCMC sampler; understanding acceptance probabilities, stationarity, and mixing is essential
- Quick check question: Given a current state yt, a proposed state y, and rewards r(yt), r(y), write the acceptance probability for β=1

### Concept: Reward Models and RLHF as Bayesian Inference
- Why needed here: The paper frames alignment as posterior inference π(y|x) ∝ pLM(y|x)·exp(r(y,x)/β); understanding this links QAlign to RLHF theory
- Quick check question: Explain why the partition function Zβ(x) cancels in the Metropolis-Hastings acceptance ratio

### Concept: Over-optimization and Reward Hacking
- Why needed here: The core motivation for QAlign is avoiding over-optimization of imperfect reward models, a known failure mode in RLHF and BoN
- Quick check question: Why does selecting the maximum-reward sample (BoN) risk degradation as n increases?

## Architecture Onboarding

### Component map
Base Language Model (pLM) -> Proposal Sampler -> Acceptor -> Aggregator

### Critical path
For each MCMC step: sample index → generate continuation → score with reward model → compute acceptance probability → accept/reject. Repeat for T steps, then aggregate samples via MBR.

### Design tradeoffs
- β selection: Controls exploration-exploitation; too low → low acceptance, slow mixing; too high → rewards have little effect
- Proposal design: QUEST is simple and logit-free but may mix slowly; more complex proposals (e.g., self-refine) require logits and complicate acceptance
- Compute vs. latency: QAlign is sequential, increasing latency compared to parallel sampling methods like BoN

### Failure signatures
- Low acceptance rate (<10%): β may be too low or reward scale too large; increase β or normalize rewards
- No improvement over majority voting: Reward model may be uninformative or β too high; inspect reward distributions and tune β
- Degradation on distribution shift (e.g., GSM-Symbolic): Reward model may be overfitted; consider more general RMs or ensemble approaches

### First 3 experiments
1. Reproduce GSM8K results with LLAMA-3.1-8B-INSTRUCT and the provided task-specific RM, sweeping β to achieve ~50% acceptance. Compare QAlign vs. BoN vs. MV at matched FLOPs
2. Test on GSM-Symbolic to assess robustness; analyze reward distributions and acceptance rates under distribution shift
3. Apply QAlign to TÜLU3-8B-SFT with TÜLU3-8B-RM on a diverse benchmark (GSM8K, MATH500, IFEval). Compare to DPO-MV at matched test-time compute to validate general alignment claims

## Open Questions the Paper Calls Out
- Can more sophisticated proposal distributions (e.g., based on self-refinement) be developed to improve the effective sample size of QAlign without requiring logit access or suffering from model hallucination?
- How does QAlign perform when applied to closed-source, state-of-the-art models via API, where the proposal distribution is limited to the provided generation interface?
- How can the inherent sequential latency of the MCMC chain be mitigated to support real-time applications while maintaining alignment quality?

## Limitations
- Convergence and mixing efficiency depend on the proposal's mixing speed, which is not thoroughly analyzed
- Reward model generalization is limited, with task-specific models showing significant degradation on distribution-shifted tasks
- Performance is sensitive to the temperature parameter β, which lacks a systematic selection method across diverse tasks

## Confidence

**High Confidence**: The core mechanism of QAlign (MCMC with Metropolis-Hastings, QUEST proposal, logit-free acceptance) is well-specified and reproducible. The theoretical framing is sound, and the empirical comparison to baselines on diverse benchmarks is convincing.

**Medium Confidence**: The empirical results are promising but depend on reward model quality and β choice. The task-specific RM's degradation on GSM-Symbolic raises generalization concerns, though the general TÜLU3-8B-RM mitigates this in some cases.

**Low Confidence**: The paper lacks convergence diagnostics, mixing time analysis, and a principled method for β selection across tasks. Scalability to very long sequences or multimodal reward landscapes is not explored, and sequential sampling may limit practical applicability for latency-sensitive applications.

## Next Checks

1. **Convergence Analysis**: For GSM8K and MATH500, run QAlign with increasing numbers of MCMC steps (e.g., 1K, 4K, 16K accepted samples) and plot accuracy vs. effective sample size or autocorrelation time. Compare to BoN's parallel sampling efficiency to quantify the trade-off between sample quality and computational cost.

2. **Reward Model Ablation**: Train multiple reward models for GSM8K with varying data sizes (e.g., 16, 64, 256 preference pairs) and test QAlign's performance on both GSM8K and GSM-Symbolic. Analyze how reward model quality and distribution shift affect QAlign's robustness, and explore ensemble or adaptive RM selection strategies.

3. **Beta Sensitivity Sweep**: For TÜLU3-8B-RM, sweep β over a wide range (e.g., 0.1 to 2.0) on GSM8K, MATH500, and TruthfulQA. Plot accuracy and acceptance rate vs. β to identify optimal ranges and quantify the sensitivity of QAlign's performance to this hyperparameter.