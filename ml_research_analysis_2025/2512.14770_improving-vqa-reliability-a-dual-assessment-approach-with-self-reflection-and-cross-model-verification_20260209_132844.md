---
ver: rpa2
title: 'Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection
  and Cross-Model Verification'
arxiv_id: '2512.14770'
source_url: https://arxiv.org/abs/2512.14770
tags:
- reliability
- bilibili
- answer
- performance
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of unreliable answers from vision-language
  models (VLMs) in Visual Question Answering (VQA) tasks, which often produce overconfident
  yet incorrect responses due to hallucinations. The authors propose Dual-Assessment
  for VLM Reliability (DAVR), a framework combining Self-Reflection and Cross-Model
  Verification to comprehensively estimate uncertainty.
---

# Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification

## Quick Facts
- arXiv ID: 2512.14770
- Source URL: https://arxiv.org/abs/2512.14770
- Authors: Xixian Wu; Yang Ou; Pengchao Tian; Zian Yang; Jielei Zhang; Peiyi Li; Longwen Gao
- Reference count: 5
- Primary result: Achieves Φ100 score of 39.64 and 100-AUC of 97.22 on Reliable VQA Challenge 2025, securing first place

## Executive Summary
This paper tackles unreliable answers from vision-language models (VLMs) in Visual Question Answering (VQA) tasks, which often produce overconfident yet incorrect responses due to hallucinations. The authors propose Dual-Assessment for VLM Reliability (DAVR), a framework combining Self-Reflection and Cross-Model Verification to comprehensively estimate uncertainty. The method uses a dual-pathway architecture: one pathway fuses VLM latent features with QA embeddings via dual selector modules to assess response reliability, while the other uses external reference models for factual cross-checking. Evaluated on the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading Φ100 score of 39.64 and 100-AUC of 97.22, demonstrating its effectiveness in enhancing VLM trustworthiness.

## Method Summary
DAVR employs a dual-pathway architecture for uncertainty estimation in VQA. The self-reflection pathway extracts last-layer hidden states from the primary VLM and combines them with CLIP-encoded QA embeddings, processed through two independent selector modules (MLP and Transformer) that classify discretized VQA accuracy. The cross-model verification pathway uses an external Qwen2.5-VL-7B-Inst model fine-tuned in two configurations (projector-only and projector+language) to independently assess answer correctness. Confidence scores from all components are progressively aggregated using equal weighting (0.5/0.5 at each step), enabling the system to determine whether to provide an answer or abstain. The framework is trained on a combination of 13K public samples and 17K additional samples collected via LLM feedback.

## Key Results
- Achieved first place on Reliable VQA Challenge 2025 with Φ100 score of 39.64 and 100-AUC of 97.22
- MLP+T+P+L configuration (progressive aggregation of all components) outperforms individual components
- Test-time augmentation (TTA) provides marginal improvement (82.97% → 83.14% accuracy)
- Single-epoch fine-tuning sufficient for external verifier models

## Why This Works (Mechanism)

### Mechanism 1: Self-Reflection via Internal Representation Fusion
Fusing VLM hidden states with CLIP-based QA embeddings enables more robust internal uncertainty estimation than either signal alone. The approach extracts last-layer hidden states during inference and combines them with separate CLIP embeddings of the question and answer. Two architectures (MLP selector and Transformer selector) independently learn to classify discretized VQA accuracy. At inference, confidence is computed as the expected accuracy under the predicted categorical distribution. This relies on the assumption that VLM hidden states reliably encode uncertainty signals that correlate with answer correctness, and class imbalance can be addressed via focal loss.

### Mechanism 2: Cross-Model Verification via Differentiated Fine-Tuning
External reference models with strategically varied fine-tuning scope can detect hallucinations that self-reflection misses. The method uses Qwen2.5-VL-7B-Inst as an external verifier, fine-tuned in two configurations: projector-only and projector+language. Both versions assess whether answers are correct/wrong/uncertain. Confidence is derived from p("correct" | input, VLM answer). This relies on the assumption that disagreement between self-reflection and cross-model verification signals potential hallucination, and the external model's probability distribution is calibrated enough for threshold-based decisions.

### Mechanism 3: Progressive Ensemble Aggregation with Equal Weighting
Sequential, equally-weighted aggregation of diverse confidence estimators yields monotonic improvement in reliability metrics. Confidence scores are combined progressively: MLP + Transformer, then + Projector-tuned VLM, then + Language-tuned VLM. Each step uses 0.5 weight for the accumulated score and 0.5 for the new component. This assumes each successive model contributes orthogonal uncertainty information and equal weighting is near-optimal without requiring learned combination weights.

## Foundational Learning

- **Selective Prediction with Abstention**
  - Why needed: The framework operates in a selective prediction paradigm—models must not only answer but also assess when to abstain. Understanding abstention thresholds and coverage metrics (Cov@k%) is essential.
  - Quick check: Can you explain why a global abstention threshold might be preferred over per-sample adaptive thresholds in safety-critical applications?

- **Focal Loss for Class Imbalance**
  - Why needed: The paper notes "relatively low proportion of negative samples" and uses focal loss with class weighting αc and focusing parameter γ. Understanding how these interact with calibration is critical.
  - Quick check: If γ is set too high, what happens to the model's confidence estimates for easy vs. hard examples?

- **Hidden State Semantics in VLMs**
  - Why needed: The self-reflection pathway relies on the claim that last-layer hidden states "encode rich multimodal interactions." Without this foundation, the approach lacks theoretical grounding.
  - Quick check: Why might last-layer hidden states be more informative than intermediate layers for uncertainty estimation, and under what conditions might this assumption fail?

## Architecture Onboarding

- **Component map**: VLM -> Hidden states + CLIP QA embeddings -> MLP Selector + Transformer Selector; VLM -> External Verifiers (VLM-P, VLM-L) -> Confidence scores; Progressive aggregation of all confidence scores

- **Critical path**: 1) VLM inference → extract hidden states + generate answer; 2) CLIP encoding of Q+A → concatenate with hidden states; 3) Parallel scoring via MLP and Transformer selectors; 4) External verification via both fine-tuned VLM variants; 5) Progressive aggregation → final confidence score; 6) Threshold comparison → answer or abstain

- **Design tradeoffs**: Classification vs. regression chosen for stability (loses granularity but easier to optimize); equal vs. learned weighting (simple but may not be optimal under correlated outputs); single-epoch SFT (prevents overfitting but may underutilize verification data); TTA (slight accuracy gain at inference cost)

- **Failure signatures**: Systematic overconfidence if all models share training data biases; threshold sensitivity to distribution shift; hidden state collapse under certain prompts or adversarial inputs

- **First 3 experiments**: 1) Ablate aggregation order (test whether adding VLM-L before VLM-P changes Φ100); 2) Threshold calibration analysis (plot confidence distributions for correct vs. incorrect answers on held-out data); 3) Correlation analysis (compute pairwise correlations between all four component confidence scores)

## Open Questions the Paper Calls Out
None

## Limitations
- Equal-weight progressive aggregation assumes component independence, but this assumption lacks validation through correlation analysis
- Single-epoch fine-tuning for external verifiers is efficient but may underfit verification data, potentially limiting detection capability
- Strong performance on a single benchmark raises questions about generalizability across domains, VQA datasets, and VLM architectures

## Confidence
- **High Confidence**: Self-reflection mechanism leveraging VLM hidden states and CLIP embeddings is well-grounded in multimodal representation learning theory
- **Medium Confidence**: Progressive ensemble aggregation achieves strong empirical results but equal weighting assumption lacks theoretical justification
- **Medium Confidence**: Cross-model verification framework is technically sound but single-epoch fine-tuning may limit detection capability

## Next Checks
1. **Component Correlation Analysis**: Compute pairwise Pearson correlation coefficients between all four component confidence scores (MLP, Transformer, VLM-P, VLM-L). Correlations above 0.7 would indicate redundant components.

2. **Distribution Shift Calibration Test**: Plot confidence score distributions for correct versus incorrect answers on held-out validation data. Test whether these distributions remain stable when evaluated on test data.

3. **Aggregation Order Ablation**: Systematically test alternative aggregation orders (e.g., adding VLM-L before VLM-P) to verify the claimed component orthogonality.