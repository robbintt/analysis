---
ver: rpa2
title: Expert or not? assessing data quality in offline reinforcement learning
arxiv_id: '2510.12638'
source_url: https://arxiv.org/abs/2510.12638
tags:
- policy
- offline
- random
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of assessing data quality in
  offline reinforcement learning without training an agent. The authors propose the
  Bellman-Wasserstein Distance (BWD), a value-aware optimal transport metric that
  measures how dissimilar a dataset's behavioral policy is from a random reference
  policy.
---

# Expert or not? assessing data quality in offline reinforcement learning

## Quick Facts
- arXiv ID: 2510.12638
- Source URL: https://arxiv.org/abs/2510.12638
- Reference count: 25
- One-line primary result: BWD achieves up to 0.95 Pearson correlation with oracle performance across D4RL MuJoCo tasks.

## Executive Summary
This paper introduces Bellman-Wasserstein Distance (BWD), a value-aware optimal transport metric for assessing offline reinforcement learning dataset quality without training an agent. BWD measures the dissimilarity between a dataset's behavioral policy and a random reference policy using a behavioral critic and state-conditional optimal transport formulation. The method requires no environment interaction or full policy optimization, making it computationally efficient for quality assessment. Across D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance score that aggregates multiple offline RL algorithms, achieving up to 0.95 Pearson correlation. The method is also shown to improve policy performance when used as a regularizer during training, demonstrating its practical utility for both dataset evaluation and policy optimization in offline RL settings.

## Method Summary
The method computes BWD by first training a behavioral critic Qβ using SARSA-style updates on the static dataset, then evaluating a state-conditional optimal transport distance between behavioral and random policies. The cost function combines Q-values with Euclidean action distances to capture both value and action dissimilarity. The dual formulation of the entropic optimal transport problem is solved using potential networks to compute BWD efficiently. For regularization, BWD is added to the policy optimization objective, encouraging the learned policy to diverge from random behavior while staying close to the dataset distribution.

## Key Results
- BWD achieves up to 0.95 Pearson correlation with oracle performance scores across D4RL MuJoCo tasks
- BWD computation requires only 10,000 training steps and 10 minutes on a single GPU
- BWD regularization improves IQL performance on Walker2d and HalfCheetah tasks by 17-19% in normalized returns
- The method successfully ranks dataset quality without requiring full agent training or environment interaction

## Why This Works (Mechanism)

### Mechanism 1: Value-Aware Optimal Transport Cost Design
- Claim: The proposed BWD metric correlates with dataset quality by measuring dissimilarity between behavioral and random policies through a Q-informed transport cost.
- Mechanism: The cost function $c((s,a), (s,a')) = Q_\beta(s, a') - \|a' - a\|_2^2$ couples two signals: (1) Q-values penalize random actions that appear high-value under the behavioral critic, and (2) Euclidean distance rewards action divergence. The dual formulation (Eqs. 15-16) optimizes potential functions $g, f$ to maximize this transport objective. Higher BWD indicates the behavioral policy is both value-superior and action-distant from random behavior.
- Core assumption: Random policies represent a valid lower-bound baseline for quality; the behavioral Q-function can be reliably estimated from offline data alone.
- Evidence anchors:
  - [abstract] "BWD strongly correlates with an oracle performance score that aggregates multiple offline RL algorithms, achieving up to 0.95 Pearson correlation."
  - [Section 4, Eq. 14-16] Defines the cost function and dual formulation explicitly.
  - [corpus] Related work "Density-Ratio Weighted Behavioral Cloning" addresses corrupted dataset quality but uses density ratios rather than OT-based metrics; limited direct corpus validation of the OT-for-quality-assessment mechanism.
- Break condition: If the behavioral Q-function is poorly estimated (insufficient data, high variance), or if random policies do not meaningfully represent low-quality behavior for the task domain.

### Mechanism 2: Behavioral Critic Estimation Without Policy Optimization
- Claim: A behavioral critic $Q_\beta$ trained via SARSA-style updates on the static dataset provides sufficient signal for quality assessment without full agent training.
- Mechanism: The critic is trained by minimizing mean squared Bellman error over transitions $(s, a, r, s') \sim D$ where the next action $a'$ is sampled from the dataset distribution (Eq. 5), not from a learned policy. This yields $Q_\beta$ that reflects the behavioral policy's value structure. Training requires only 10,000 steps with batch size 256 on a single GPU.
- Core assumption: The dataset's action distribution adequately represents the behavioral policy; SARSA updates converge sufficiently for quality estimation purposes.
- Evidence anchors:
  - [Section 2.2, Eq. 5] "These approaches approximate the behavioral Qβ function using the provided dataset of experience."
  - [Section 5, Settings] "Post-training, we randomly sampled 20,000 transitions... to compute the scores."
  - [corpus] "Using Non-Expert Data to Robustify Imitation Learning" leverages offline RL for robust policy learning but does not address critic-only quality estimation.
- Break condition: Sparse reward environments where Q-values provide poor signal; highly non-stationary behavioral policies within the dataset.

### Mechanism 3: BWD as Policy Regularization
- Claim: Incorporating BWD as a regularization term $J_{reg} = J_{orig} + \text{BWD}(\pi_{actor}, \pi_{rand})$ during IQL training improves returns by pushing learned policies away from random behavior.
- Mechanism: During policy optimization, the regularized objective adds the BWD term computed between the actor's actions and random actions sampled from $\pi_{rand}$. The potentials $g_\psi, f_\phi$ are optionally updated during training. This explicit dissimilarity pressure biases the policy toward higher-quality action regions.
- Core assumption: Random-action regions in state-action space are systematically lower-value; pushing away from them is broadly beneficial across dataset qualities.
- Evidence anchors:
  - [Section 6, Eq. 17] "The regularized objective can be expressed as: $J_{reg}(\pi_{actor}) = J_{orig}(\pi_{actor}) + \text{BWD}(\pi_{actor}, \pi_{rand})$."
  - [Table 2] Walker2d shows improvement from 81.149 → 98.545 normalized score at 1000K steps; HalfCheetah improves from 82.159 → 86.525.
  - [corpus] "Adaptive Scaling of Policy Constraints" addresses constraint scaling but not OT-based regularization specifically.
- Break condition: If random actions occasionally encode useful exploration behaviors; if the regularization weight $\lambda_{BWD}$ is improperly tuned relative to the base objective.

## Foundational Learning

- Concept: **Q-Function and Bellman Equation**
  - Why needed here: The entire BWD framework depends on understanding how $Q(s,a)$ estimates expected cumulative reward and how SARSA-style updates differ from Q-learning.
  - Quick check question: Given a transition $(s, a, r, s')$, what is the SARSA target versus the Q-learning target for updating $Q(s,a)$?

- Concept: **Optimal Transport and Wasserstein Distance**
  - Why needed here: BWD is built on the dual formulation of entropically-regularized Kantorovich OT; understanding cost functions and transport plans is essential.
  - Quick check question: In the Kantorovich formulation, what does the transport plan $\gamma \in \Pi(\mu, \nu)$ represent, and how does adding entropic regularization change the optimization?

- Concept: **Offline RL Distribution Shift**
  - Why needed here: The paper's motivation rests on why dataset quality matters—BC fails on mixed-quality data because of distributional issues; offline RL algorithms address this via policy constraints or value regularization.
  - Quick check question: Why does standard off-policy RL often fail in offline settings, and what role do BC regularization terms (as in TD3+BC) play?

## Architecture Onboarding

- Component map:
  Behavioral Critic -> Potential Networks -> Random Policy Sampler -> BWD Computer -> Optional IQL Integration

- Critical path:
  1. Load offline dataset $D$
  2. Train $Q_\beta$ for 10K steps (SARSA-style, batch 256)
  3. Sample $B$ state-action pairs from $D$; sample $K$ random actions per state
  4. Compute costs $c_{i,k}$ using $Q_\beta$ and action distances
  5. Optimize $g_\psi, f_\phi$ via gradient ascent on dual objective
  6. Evaluate BWD on held-out batch (higher = better quality)

- Design tradeoffs:
  - **Potential network size vs. compute**: Paper uses minimal architecture (256 units, 1 layer); larger networks may improve OT approximation but increase runtime.
  - **Batch size $B$ and negatives $K$**: Larger values improve gradient estimates but cost more Q-function evaluations.
  - **Entropic weight $\varepsilon$**: Controls transport plan smoothness; too low causes optimization instability, too high over-smooths the distance.
  - **Critic training steps**: 10K is sufficient for quality estimation; regularization use may benefit from longer training.

- Failure signatures:
  - **BWD near-zero across all datasets**: Critic $Q_\beta$ may not have converged; check training loss curves.
  - **Negative BWD values**: Cost function implementation error or potential networks not converged.
  - **Poor correlation with oracle**: May indicate task-specific reward scaling issues; verify normalization.
  - **Regularization causing instability**: BWD gradients may dominate base objective; reduce $\lambda_{BWD}$.

- First 3 experiments:
  1. **Sanity check on D4RL random/expert splits**: Compute BWD on HalfCheetah-random vs. HalfCheetah-expert; verify BWD(expert) >> BWD(random). Should reproduce Table 1 ordering.
  2. **Ablation on critic training steps**: Train $Q_\beta$ for 1K, 5K, 10K, 20K steps; plot BWD-oracle correlation vs. training steps. Validate 10K sufficiency claim.
  3. **IQL+BWD on single environment**: Run IQL and IQL+BWD on Walker2d-medium-replay for 3 seeds; compare final normalized scores. Should show improvement consistent with Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative, domain-specific cost functions enhance the Bellman-Wasserstein Distance's (BWD) predictive accuracy or regularization utility beyond the proposed Q-weighted Euclidean distance?
- Basis in paper: [explicit] The "Future work" section explicitly suggests that the general optimal transport problem allows for arbitrary cost functions and encourages the search for "better RL specific cost functions."
- Why unresolved: The current study utilizes a specific cost formulation $c((s, a),(s, a'))$ defined in Equation 14, leaving the potential performance of other cost functionals unexplored.
- What evidence would resolve it: Ablation studies comparing the correlation and downstream performance of BWD when using alternative cost functions against the proposed baseline.

### Open Question 2
- Question: Is a random policy always the optimal reference baseline, or does this assumption limit the metric's effectiveness on datasets containing adversarial or systematically sub-random behaviors?
- Basis in paper: [inferred] The method is founded on the premise that a "random policy represents the lowest performance" (Section 4), but this may not hold for all wild datasets which could contain destructive noise.
- Why unresolved: The paper does not validate the robustness of the "random" reference point against alternative low-quality baselines in diverse data regimes.
- What evidence would resolve it: Experiments measuring BWD sensitivity and correlation quality when the reference policy is swapped for a "worst-case" or noise-injected policy.

### Open Question 3
- Question: Does BWD maintain high correlation with agent performance in environments with sparse rewards or high-dimensional visual state spaces?
- Basis in paper: [inferred] The experiments are strictly confined to D4RL MuJoCo locomotion tasks, which typically feature dense rewards and low-dimensional state vectors.
- Why unresolved: BWD relies on accurately estimating a behavioral critic $Q_\beta$, a task known to be unstable in sparse reward settings or high-dimensional spaces like Atari.
- What evidence would resolve it: Evaluation of BWD's predictive power on standard offline RL benchmarks with sparse rewards (e.g., AntMaze) or image-based observations.

## Limitations
- Strong dependence on accurate Q-function estimation; noisy or sparse reward data could degrade BWD quality signal
- Assumes random policies provide a meaningful lower-bound baseline, which may not hold in all domains
- Regularization experiments show modest improvements; the method may not generalize to non-IQL algorithms or very poor datasets

## Confidence
- **High**: BWD correlation with oracle scores on D4RL MuJoCo (empirical results are directly measured)
- **Medium**: BWD as a general dataset quality metric (requires validation on non-MuJoCo domains)
- **Low**: Regularization benefits (only shown on IQL; regularization weight sensitivity not fully explored)

## Next Checks
1. Test BWD on non-MuJoCo datasets (Atari, Box2D, custom continuous control) to assess domain generalization
2. Compare BWD correlation with oracle when using Q-learning vs. SARSA critic training
3. Run BWD regularization ablation with varying λ_BWD values (0.01, 0.1, 1.0, 10.0) to identify optimal scaling