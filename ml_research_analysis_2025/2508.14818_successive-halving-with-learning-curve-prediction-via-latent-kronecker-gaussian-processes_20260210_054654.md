---
ver: rpa2
title: Successive Halving with Learning Curve Prediction via Latent Kronecker Gaussian
  Processes
arxiv_id: '2508.14818'
source_url: https://arxiv.org/abs/2508.14818
tags:
- performance
- learning
- values
- data
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using learning curve predictions via Latent
  Kronecker Gaussian Processes (LKGPs) to guide the Successive Halving (SH) algorithm
  in hyperparameter optimization. SH typically makes promotion decisions based on
  current performance values, which can lead to prematurely pruning slow-starting
  candidates that might eventually become optimal.
---

# Successive Halving with Learning Curve Prediction via Latent Kronecker Gaussian Processes

## Quick Facts
- **arXiv ID:** 2508.14818
- **Source URL:** https://arxiv.org/abs/2508.14818
- **Reference count:** 1
- **Primary result:** Latent Kronecker Gaussian Processes (LKGPs) can guide Successive Halving hyperparameter optimization but are not Pareto optimal compared to simply increasing computational resources for standard SH with current performance values.

## Executive Summary
This paper investigates using Latent Kronecker Gaussian Processes to predict future performance values from partially observed learning curves, with the goal of making more informed candidate promotion decisions in the Successive Halving (SH) algorithm. Standard SH relies on current performance values, which can prematurely prune slow-starting candidates that might become optimal. LKGPs offer a scalable probabilistic approach by factorizing correlations across hyperparameter space and time. The authors conducted large-scale experiments on the Criteo 1TB click prediction dataset using three neural network architectures, finding that while LKGPs achieve competitive performance, they are not Pareto optimal due to the computational cost of requiring fully observed learning curves as training data.

## Method Summary
The method combines Successive Halving with Latent Kronecker Gaussian Processes for learning curve prediction. SH operates with reduction factor η=2, promoting the top N/η^(s-1) candidates at each rung after a 10% grace period. LKGPs use product kernels that factorize across hyperparameters and time, enabling scalable GP regression. The model is trained on fully observed learning curves using Adam optimization (lr=0.1, 100 iterations) with standardized inputs and outputs. Candidates are ranked by expected pairwise wins computed from the GP posterior, integrating both mean predictions and uncertainty estimates. The study compares SH using current performance versus LKGP-guided SH across various configurations of final candidates F and training curve counts C.

## Key Results
- LKGPs achieve competitive performance but are not Pareto optimal compared to standard SH with increased computational resources
- The primary limitation stems from requiring fully observed learning curves as training data, adding significant upfront compute cost
- The non-Pareto result holds across three neural network architectures (Factorization Machines, Deep & Cross Networks, Mixture of Experts) on the Criteo 1TB dataset
- Authors suggest the computational drawback could be mitigated by leveraging existing learning curve data from similar architectures

## Why This Works (Mechanism)

### Mechanism 1: Product Kernel Factorization for Scalable GP Regression
- Claim: LKGPs enable scalable learning curve prediction by factorizing the covariance structure across hyperparameter space and time.
- Mechanism: The model uses a product kernel k((x,t), (x',t')) = k_X(x,x') × k_T(t,t') that decomposes correlations across hyperparameters and timesteps independently. This Kronecker structure allows the GP to scale to large datasets while capturing how similar hyperparameter configurations produce similar learning curve dynamics.
- Core assumption: Hyperparameter similarity correlates with learning curve similarity, and temporal patterns are reasonably consistent across configurations.
- Evidence anchors:
  - [section 2]: "LKGPs model f using a product kernel k((x,t), (x',t')) = k_X(x,x')k_T(t,t') and make nonparametric predictions based on correlations with observed training data"
  - [corpus]: Related paper "Scalable Gaussian Processes with Latent Kronecker Structure" provides theoretical foundation for this approach.

### Mechanism 2: Probabilistic Ranking via Pairwise Expected Wins
- Claim: Ranking candidates by expected pairwise wins accounts for prediction uncertainty better than point-estimate sorting.
- Mechanism: For each candidate i, compute E[wins(i)] = (1/(n-1)) × Σ_{j≠i} P[perf(i) > perf(j)] using the cumulative distribution function Φ. This integrates both predicted means μ_i and variances σ²_i, so candidates with uncertain but potentially high performance are appropriately weighted.
- Core assumption: The GP posterior is well-calibrated; variance estimates meaningfully reflect true uncertainty.
- Evidence anchors:
  - [section 3]: "The ranking is obtained by calculating and sorting the expected number of wins per candidate in a pairwise comparison" with formula using Φ((μ_i - μ_j)/√(σ²_i + σ²_j))

### Mechanism 3: Training Data Transfer as Compute Amortization
- Claim: The primary limitation—requiring fully observed curves as training data—could be overcome by reusing curves from prior experiments.
- Mechanism: If learning curves from previous SH runs or architecture searches are available, the "training cost" of LKGPs can be amortized across multiple optimization campaigns, potentially achieving Pareto optimality.
- Core assumption: Learning curves from prior experiments transfer meaningfully to new hyperparameter distributions or datasets.
- Evidence anchors:
  - [abstract]: "This downside could be mitigated by leveraging existing learning curve data"
  - [section 4]: "This downside could be negated by leveraging existing learning curves, which is straightforward for the same neural network architecture and motivates future work to identify ways to transfer between architectures"

## Foundational Learning

- Concept: **Gaussian Process Regression**
  - Why needed here: LKGPs are fundamentally GP models; understanding posterior inference, kernel functions, and marginal likelihood optimization is prerequisite to modifying or debugging the prediction pipeline.
  - Quick check question: Can you explain why a GP provides both mean predictions and uncertainty estimates, and how the kernel function determines correlation structure?

- Concept: **Successive Halving / Hyperband Algorithms**
  - Why needed here: SH is the base algorithm being augmented; understanding rung structure, reduction factor η, and the tradeoff between number of candidates F and compute budget is essential.
  - Quick check question: Given N=256 candidates and η=2, how many rungs are needed to reduce to F=16 finalists?

- Concept: **Kronecker Product Matrix Structure**
  - Why needed here: The scalability of LKGPs depends on exploiting Kronecker structure for efficient matrix operations; this is the "secret sauce" enabling GP inference on large learning curve datasets.
  - Quick check question: If K_X is n×n and K_T is t×t, what is the size and computational complexity advantage of the Kronecker product K_X ⊗ K_T versus a dense kernel matrix?

## Architecture Onboarding

- Component map: Training Data (fully observed curves) → GP Training (marginal likelihood opt) → Partially Observed Candidates → GP Posterior Inference → Mean/Variance per candidate → Pairwise Win Computation → Ranking → Successive Halving Scheduler ← Rung Promotions (top N/η^(s-1) candidates)

- Critical path: The GP training step (kernel hyperparameter optimization via Adam) is the computational bottleneck and determines prediction quality. The 100 Adam iterations with lr=0.1 are a key configuration point.

- Design tradeoffs:
  - **Training curves C vs. compute**: More training curves improve prediction quality but linearly increase upfront compute cost. Paper tested C ∈ {8, 16, 32, 64}.
  - **Final candidates F vs. regret**: Larger F reduces risk of discarding the best candidate but increases total compute. Standard SH with F≥32 achieved zero regret in all trials.
  - **Grace period (10%) vs. early detection**: Delaying SH reduces noise but postpones resource savings.

- Failure signatures:
  - All candidates converge to similar predicted performance: May indicate insufficient kernel expressiveness or poorly conditioned covariance matrix.
  - Rankings highly volatile across rungs: Suggests predictions are not informative; grace period may be too short or training data insufficient.
  - LKGP regret curve never crosses standard SH curve: Confirms non-Pareto-optimality finding; consider if training data reuse is feasible.

- First 3 experiments:
  1. Reproduce the non-Pareto result: Run SH with current performance vs. LKGP on a small hyperparameter grid (e.g., 32 candidates, 2 architectures). Verify that investing compute in larger F outperforms LKGP with training data cost.
  2. Ablate training data quality: Compare LKGP performance when training curves are selected uniformly at random vs. stratified across hyperparameter space. Hypothesis: better coverage improves prediction on novel regions.
  3. Test transfer assumption: Train LKGP on curves from architecture A, evaluate prediction accuracy on architecture B. This probes the "future work" direction and establishes baseline for cross-architecture transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learning curve data be effectively transferred between different neural network architectures to eliminate the compute overhead of collecting fully observed training curves?
- Basis in paper: [explicit] The authors state that leveraging existing learning curves "is straightforward for the same neural network architecture and motivates future work to identify ways to transfer between architectures."
- Why unresolved: The paper only tested LKGPs using training curves from the same architecture being optimized. Cross-architecture transfer was not investigated.
- What evidence would resolve it: Experiments showing that LKGPs trained on learning curves from one architecture (e.g., Factorization Machines) can successfully guide SH for a different architecture (e.g., Mixture of Experts) without significant performance degradation.

### Open Question 2
- Question: What is the optimal strategy for selecting training curves for LKGPs rather than uniform random sampling?
- Basis in paper: [explicit] The authors acknowledge "we selected the training data for LKGPs uniformly at random, which is most likely suboptimal."
- Why unresolved: Random selection may miss diverse curve shapes or hyperparameter regions that would improve prediction generalization.
- What evidence would resolve it: A comparison of selection strategies (e.g., diversity-based, performance-stratified, active learning) showing measurable regret reduction compared to random selection.

### Open Question 3
- Question: Can specialized kernels or feature engineering (input warping, embeddings) improve LKGP prediction quality enough to achieve Pareto optimality over standard SH?
- Basis in paper: [explicit] The authors suggest "the prediction quality of LKGPs could potentially be improved by considering specialized kernels and more elaborate feature engineering."
- Why unresolved: The study used standard squared exponential kernels without architecture-specific adaptations that might better capture learning curve dynamics.
- What evidence would resolve it: Experiments with domain-specific kernels or learned embeddings demonstrating that improved predictions reduce the training data requirements or improve regret enough to surpass the Pareto frontier of standard SH.

## Limitations
- The primary limitation is the computational overhead of generating fully observed learning curves for training the LKGP model, preventing Pareto optimality compared to standard SH with more candidates
- The assumption that learning curves from prior experiments can effectively transfer to new optimization campaigns remains speculative without empirical validation
- The study uses only the Criteo click prediction dataset, limiting generalizability to other domains with different learning curve characteristics

## Confidence

- **High confidence**: The empirical finding that LKGP-guided SH is not Pareto optimal compared to standard SH with more candidates is well-supported by the experimental results across multiple architectures.
- **Medium confidence**: The mechanism by which product kernel factorization enables scalable GP regression is theoretically sound but depends on the assumption that hyperparameter similarity correlates with learning curve similarity.
- **Low confidence**: The claim that training data reuse could mitigate the computational drawback is presented as future work without empirical validation in the paper.

## Next Checks

1. **Transfer learning experiment**: Train LKGP on learning curves from architecture A, evaluate prediction accuracy and SH performance on architecture B to validate the transfer assumption.
2. **Kernel structure ablation**: Compare LKGP performance using different kernel structures (e.g., additive vs. product kernels, non-stationary kernels) to assess whether the current design is optimal.
3. **Early stopping sensitivity analysis**: Vary the grace period (10% training delay) and analyze its impact on LKGP prediction quality versus standard SH performance to identify optimal timing for promotion decisions.