---
ver: rpa2
title: Training Diagonal Linear Networks with Stochastic Sharpness-Aware Minimization
arxiv_id: '2503.11891'
source_url: https://arxiv.org/abs/2503.11891
tags:
- page
- issn
- then
- isbn
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the landscape and training dynamics of diagonal
  linear networks under stochastic sharpness-aware minimization (S-SAM), where small
  isotropic normal noise is added to network parameters. The main contributions include:
  Regularization Analysis: The noise-induced regularization is characterized, showing
  that it forces balancing of weight matrices at a fast rate along the descent trajectory.'
---

# Training Diagonal Linear Networks with Stochastic Sharpness-Aware Minimization

## Quick Facts
- arXiv ID: 2503.11891
- Source URL: https://arxiv.org/abs/2503.11891
- Reference count: 40
- One-line primary result: Stochastic Sharpness-Aware Minimization with isotropic noise induces balancing of weight matrices in diagonal linear networks while forcing a shrinkage-thresholding operator on the true parameters

## Executive Summary
This paper analyzes the landscape and training dynamics of diagonal linear networks under stochastic sharpness-aware minimization (S-SAM), where small isotropic normal noise is added to network parameters. The main contributions include characterizing how noise-induced regularization forces weight matrices to balance at a fast rate along the descent trajectory, and showing that this regularization minimizes average sharpness and the trace of the Hessian matrix among all possible factorizations of the same matrix. The critical points of the regularized loss are shown to result from a shrinkage-thresholding operator applied to the true parameter, with the noise level explicitly regulating both the shrinkage factor and the threshold.

## Method Summary
The method trains diagonal linear networks of depth L where each weight matrix Wáµ¢ is diagonal. S-SAM adds isotropic Gaussian noise Î¾áµ¢,áµ¢áµ¢ ~ ğ’©(0, Î·Â²) to each diagonal element during gradient computation. The regularized loss includes a sharpness-aware term that penalizes all partial products of weight matrices weighted by powers of Î·Â². The analysis assumes whitened data (nâ»Â¹âˆ‘Xáµ¢Xáµ¢áµ€ = I) and focuses on the theoretical properties of the resulting optimization landscape.

## Key Results
- The noise-induced regularization forces weight matrices to balance across layers at a fast rate, minimizing both average sharpness and Hessian trace
- Critical points take the form of a shrinkage-thresholding operator applied to true parameters, with noise level regulating both shrinkage factor and threshold
- The S-SAM recursion converges to critical points of the regularized loss under suitable step sizes
- The gradient descent dynamics rapidly balance weight matrices, minimizing sharpness along the trajectory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding isotropic Gaussian noise to network parameters induces an explicit regularizer that forces weight matrices to balance across layers.
- Mechanism: The expected gradient under noise perturbation equals âˆ‡ğ’ªáµ£ = âˆ‡ğ’ª + âˆ‡R, where R penalizes all partial products of weight matrices weighted by powers of Î·Â². For diagonal networks, this creates the constraint WÂ²áµ¢ = WÂ²áµ¢â‚Šâ‚ at critical points, which minimizes both average sharpness and the Hessian trace among all factorizations of the same product matrix.
- Core assumption: Diagonal weight matrices and whitened data (nâ»Â¹Â·âˆ‘Xáµ¢Xáµ¢áµ€ = I).
- Evidence anchors:
  - [abstract]: "the noise changes the expected gradient to force balancing of the weight matrices at a fast rate along the descent trajectory"
  - [Section 3, Theorem 3.1(a)]: Critical set satisfies âˆ‡áµ¥áµ¢â‚Šâ‚R(W)Wáµ¢â‚Šâ‚ = âˆ‡áµ¥áµ¢R(W)Wáµ¢, entailing WÂ²áµ¢ = WÂ²â‚˜
  - [Section 3, Lemma 3.2]: Balancing condition minimizes Savg and Tr(âˆ‡Â²ğ’ª) over all factorizations
  - [Corpus]: Limited direct evidence; related work shows SAM variants (GCSAM, LSAM) improve generalization but mechanism differs
- Break condition: Non-diagonal weight matrices; non-whitened data where nâ»Â¹âˆ‘Xáµ¢Xáµ¢áµ€ â‰  I; layers with different dimensions (width varies).

### Mechanism 2
- Claim: The noise level Î· explicitly regulates a shrinkage-thresholding operator applied to true parameters, determining which components survive.
- Mechanism: Critical points take form Wáµ¢,â‚•â‚• = sáµ¢,â‚• Â· Î»â‚• Â· |W*|â‚/Ë¡á´´á´´ where shrinkage factor Î»â‚• âˆˆ (0,1) solves Î»Â²â‚• - Î»â‚-Â¹/â½Ë¡â»Â¹â¾â‚• + Î·Â²/|W*|Â²/Ë¡á´´á´´ = 0. Components with |W*á´´á´´| < threshold (depends on Î·á´¸ and L) are forced to zero, creating an implicit bias toward low-rank solutions.
- Core assumption: The teacher parameter W* exists and the student model is overparameterized to capture it.
- Evidence anchors:
  - [abstract]: "the noise forces the gradient descent iterates towards a shrinkage-thresholding of the underlying true parameter, with the noise level explicitly regulating both the shrinkage factor and the threshold"
  - [Section 3, Theorem 3.1(b)]: Explicit threshold formula for non-zero solutions; for L=2, threshold is Î·Â²
  - [Corpus]: No direct corroboration; dropout regularization shows similar thresholding but with different threshold structure
- Break condition: Non-isotropic noise distributions; varying noise levels per layer; loss functions without polynomial structure.

### Mechanism 3
- Claim: The regularized loss ğ’ªáµ£ is coercive, ensuring bounded trajectories and convergence to critical points under suitable step sizes.
- Mechanism: The regularizer upper-bounds parameter norm via âˆ‘â€–Wáµ¢â€–Â² â‰¤ (Î·Â²(L-1))â»Â¹Â·R â‰¤ (Î·Â²(L-1))â»Â¹Â·ğ’ªáµ£. Combined with Åojasiewicz-type arguments, this guarantees gradient descent iterates converge to critical points of ğ’ªáµ£ when step sizes satisfy strong descent conditions (Î±â‚– < 2(1-Î´)Â·(âˆšLÂ·7âˆšL+2/Î·Â²Â·ğ’ªáµ£â»Â¹)).
- Core assumption: Analytic loss function (holds for polynomial ğ’ªáµ£); step sizes satisfy âˆ‘Î±â±¼ = âˆ and âˆ‘Î±Â²â±¼ < âˆ for stochastic case.
- Evidence anchors:
  - [Section 4.1, Eq. 16]: â€–W(t)â€–â‚‚ â‰¤ (Î·Ë¡â»Â¹)â»Â¹Â·âˆšğ’ªáµ£(initial), proving coercivity
  - [Section 4.2, Theorem 4.2]: Convergence to critical point when step sizes bounded by local Lipschitz estimates
  - [Section 4.3, Theorem 4.4]: Projected S-SAM converges when projection radius r â‰¥ max{1, âˆšL/2Î·Ë¡â»Â¹}Â·â€–W*â€–
  - [Corpus]: Convergence analysis in "Sharpness-Aware Minimization: General Analysis and Improved Rates" provides complementary non-convex convergence results
- Break condition: Step sizes exceeding local Lipschitz bounds; non-projection with unbounded noise; non-analytic loss functions.

## Foundational Learning

- Concept: Matrix factorization via deep linear networks
  - Why needed here: The paper models W = Wâ‚—Â·Â·Â·Wâ‚ as product of diagonal matrices; understanding how different factorizations relate to the same product is essential for grasping why balancing matters.
  - Quick check question: Given W = 6, can you identify three different factorizations Wâ‚‚Wâ‚ = 6? Which has minimal âˆ‘WÂ²áµ¢?

- Concept: Sharpness measures (Savg, Smax, Hessian trace)
  - Why needed here: S-SAM minimizes average sharpness Savg = ğ”¼[ğ’ª(W+Î·Î¾)] - ğ’ª(W), which connects to generalization; understanding this distinction from max sharpness clarifies why isotropic noise is used.
  - Quick check question: For a quadratic loss ğ’ª(w) = wÂ², compute Savg(w, Î·) assuming Î¾ ~ ğ’©(0,1).

- Concept: PAC-Bayes generalization bounds
  - Why needed here: The paper connects the regularizer R to generalization via PAC-Bayes theory, showing that minimizing average sharpness plus parameter norm yields non-vacuous bounds.
  - Quick check question: What two quantities does the PAC-Bayes bound in Lemma 2.3 depend on, and why does controlling both matter?

## Architecture Onboarding

- Component map:
```
Input: X âˆˆ â„áµˆ (whitened, so nâ»Â¹âˆ‘Xáµ¢Xáµ¢áµ€ = I)
  â†“
Layer 1: Wâ‚ (diagonal dÃ—d) â†’ output: Wâ‚X
  â†“
Layer 2: Wâ‚‚ (diagonal dÃ—d) â†’ output: Wâ‚‚Wâ‚X
  â†“
...
  â†“
Layer L: Wâ‚— (diagonal dÃ—d) â†’ output: Wâ‚—Â·Â·Â·Wâ‚X
  â†“
Loss: ğ’ªáµ£ = â€–W* - Wâ‚—Â·Â·Â·Wâ‚â€–Â² + Tr(âˆ(WÂ²áµ¢ + Î·Â²I) - âˆWÂ²áµ¢)
```
Noise injection: áº†áµ¢,áµ¢áµ¢ = Wáµ¢,áµ¢áµ¢ + Î¾áµ¢,áµ¢áµ¢ where Î¾áµ¢,áµ¢áµ¢ ~ ğ’©(0, Î·Â²) applied during each gradient computation.

- Critical path:
  1. Initialize Wâ‚,...,Wâ‚— (paper uses small initialization near balanced)
  2. Sample data point (Xâ‚–, Yâ‚–) and noise Î¾â‚–
  3. Compute noisy gradient âˆ‡áº†â‚–ğ’ª(áº†â‚,...,áº†â‚—)
  4. Update Wáµ¢ â† Wáµ¢ - Î±â‚–Â·âˆ‡áµ¥áµ¢ğ’ªáµ£ - Î±â‚–Â·(noisy - expected gradient)
  5. (Optional) Project onto ball of radius r if using projected S-SAM
  6. Repeat until convergence; balancing occurs at rate exp(-4Î·Â²Lâ»Â²Â·t)

- Design tradeoffs:
  | Choice | Effect | Guideline |
  |--------|--------|-----------|
  | Î· (noise level) | Larger Î· â†’ stronger regularization, more shrinkage, faster balancing | Start with Î· ~ 0.1-0.5; increase if overfitting |
  | L (depth) | Deeper â†’ more non-convexity, higher threshold for non-zero solutions | Threshold âˆ Î·á´¸ for L > 2; avoid excessive depth |
  | Step size Î±â‚– | Must satisfy Î±â‚– < O(Î·Â²/ğ’ªáµ£(initial)) for strong descent | Use Î±â‚– = Î±/(k+1) or small constant Î± |
  | Projection radius r | Must satisfy r â‰¥ âˆšL/(2Î·Ë¡â»Â¹)Â·â€–W*â€– to avoid boundary issues | Estimate â€–W*â€– from data or use large r |

- Failure signatures:
  - Exploding gradients / divergent trajectories: Step sizes too large relative to local Lipschitz constant; reduce Î±â‚– or increase Î·.
  - No convergence to balanced solution: Initialization too unbalanced with too small Î· or Î±â‚–; increase noise or use longer training.
  - All parameters collapse to zero: Î· too large relative to â€–W*â€–; signals below threshold; reduce Î·.
  - Oscillatory behavior without convergence: Constant step size with large Î· without projection; use decaying step sizes or projection.

- First 3 experiments:
  1. **Sanity check - 1D case with L=2**: Implement two-layer scalar network (wâ‚‚wâ‚ â‰ˆ w*). Plot loss landscape for different Î· values. Verify that critical points converge to balanced solutions (|wâ‚| = |wâ‚‚|) and shrinkage factor matches theoretical Î» = âˆš(1 - Î·Â²/|w*|) for |w*| > Î·Â². This confirms basic mechanism before scaling.
  2. **Balancing rate verification**: Initialize Wâ‚,...,Wâ‚— deliberately unbalanced (e.g., Wâ‚ = 2I, Wâ‚‚ = I/2, others = I). Track â€–WÂ²áµ¢ - WÂ²áµ¢â‚Šâ‚â€– over iterations. Fit exponential decay rate and compare to theoretical exp(-4Î·Â²Lâ»Â²Â·t). Vary Î· and L to confirm scaling.
  3. **Shrinkage-thresholding test**: Generate synthetic W* with entries of varying magnitudes. Train with different Î· values. Plot recovered product â€–Wâ‚—Â·Â·Â·Wâ‚â€– vs |W*| entries. Identify threshold behaviorâ€”entries below threshold should be zeroed, above should be shrunk by factor Î»â‚•. Compare to theoretical threshold formula.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the S-SAM regularization inducing balancing and shrinkage-thresholding generalize to non-diagonal linear networks?
- Basis in paper: [explicit] The paper explicitly states that general linear networks are a natural generalization and asks if an analog of Theorem 3.1 can be proven, noting the loss of commutativity in non-diagonal settings.
- Why unresolved: The proofs heavily rely on diagonal matrices being symmetric and commuting, which simplifies the gradient and regularizer analysis; this structure breaks down for general rectangular or non-diagonal square matrices.
- What evidence would resolve it: A theoretical derivation of the regularizer for general matrix factorization showing whether the "balancing" constraint $W_l^\top W_{l+1} = W_l W_{l+1}^\top$ holds, or a counter-example showing divergent behavior.

### Open Question 2
- Question: What is the stationary distribution of the S-SAM recursion under constant step-sizes?
- Basis in paper: [explicit] The authors explicitly identify characterizing the behavior of S-SAM as a diffusion in the parameter space for constant step-sizes as a "natural follow-up question."
- Why unresolved: The current analysis focuses on convergence for diminishing step-sizes ($\sum \alpha_k^2 < \infty$), whereas constant step-sizes typically converge to a stationary distribution rather than a single point.
- What evidence would resolve it: A characterization of the invariant measure of the Markov chain defined by the S-SAM update, potentially relating it to the local geometry of the loss landscape similar to Stochastic Gradient Langevin Dynamics (SGLD).

### Open Question 3
- Question: Can adaptive, geometry-aware noise distributions yield better generalization guarantees than isotropic noise?
- Basis in paper: [explicit] Section 5 discusses that the choice of isotropic normal noise may be sub-optimal and suggests that adapting the noise to the local geometry (higher-order information) could optimize PAC-Bayes bounds.
- Why unresolved: The paper analyzes a specific noise model (Gaussian) and its induced regularizer, but does not compare it against data-dependent or adaptive posterior distributions.
- What evidence would resolve it: Deriving a PAC-Bayes bound for an adaptive noise scheme and empirically verifying if it leads to tighter bounds or better generalization performance on standard tasks.

### Open Question 4
- Question: How does the regularizer structure change when the input data is not whitened (i.e., covariance $\Sigma \neq I$)?
- Basis in paper: [inferred] The core theoretical results (e.g., Lemma 2.2 and Theorem 3.1) rely on the assumption that $n^{-1}\sum \mathbf{X}_i \mathbf{X}_i^\top = I_d$ (whitened data).
- Why unresolved: Real-world data rarely has an identity covariance matrix. The interaction between the data covariance and the weight matrices $W_l$ in the gradient computation (Lemma 2.1) suggests the resulting regularizer would be significantly more complex.
- What evidence would resolve it: Extending the derivation of Lemma 2.2 to a general covariance matrix $\Sigma$ and analyzing if the "balancing" property is distorted or if new constraints emerge.

## Limitations
- The analysis is limited to diagonal linear networks with whitened data, which may not capture the full complexity of practical deep learning scenarios.
- The convergence guarantees rely on specific conditions (analytic loss functions, bounded step sizes) that may be challenging to verify in practice.
- The theoretical analysis assumes exact balancing conditions that may not hold with finite-precision computations or non-ideal initialization schemes.

## Confidence
- **High Confidence**: The regularization mechanism (Mechanism 1) connecting isotropic noise to weight balancing and Hessian minimization, supported by explicit gradient computations and multiple theorems (Theorem 3.1(a), Lemma 3.2).
- **Medium Confidence**: The shrinkage-thresholding operator characterization (Mechanism 2), as it relies on specific algebraic conditions that may be sensitive to noise distribution and network depth assumptions.
- **Medium Confidence**: The convergence proofs (Mechanism 3), particularly the Åojasiewicz arguments and step size conditions, which depend on local Lipschitz constants that are difficult to estimate in practice.

## Next Checks
1. **Robustness to Initialization**: Test whether the balancing mechanism still operates when initializing W matrices far from the balanced state, and quantify the effect of initial imbalance on convergence rates.

2. **Non-Diagonal Generalization**: Extend the noise-regularization analysis to non-diagonal networks by examining whether similar balancing occurs in off-diagonal elements, and characterize the modified regularizer structure.

3. **Practical Generalization Bounds**: Implement the PAC-Bayes bound (Lemma 2.3) to verify whether the theoretically minimized average sharpness translates to improved test performance on real-world datasets with appropriate data whitening preprocessing.