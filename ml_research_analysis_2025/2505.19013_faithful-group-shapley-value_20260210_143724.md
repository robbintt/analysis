---
ver: rpa2
title: Faithful Group Shapley Value
arxiv_id: '2505.19013'
source_url: https://arxiv.org/abs/2505.19013
tags:
- data
- group
- shapley
- fgsv
- valuation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Faithful Group Shapley Value addresses the problem of strategic
  manipulation in group data valuation through shell company attacks, where unfair
  players can inflate valuations by splitting data into smaller subgroups. The authors
  propose Faithful Group Shapley Value (FGSV) as the sum of individual Shapley values,
  which uniquely satisfies a set of axioms including faithfulness that prevents such
  manipulation.
---

# Faithful Group Shapley Value

## Quick Facts
- arXiv ID: 2505.19013
- Source URL: https://arxiv.org/abs/2505.19013
- Reference count: 40
- Primary result: FGSV uniquely satisfies faithfulness axiom, preventing shell company attacks in data valuation

## Executive Summary
Faithful Group Shapley Value (FGSV) addresses strategic manipulation in group data valuation where unfair players can inflate valuations by splitting data into smaller subgroups (shell company attacks). The authors propose FGSV as the sum of individual Shapley values, which uniquely satisfies a set of axioms including faithfulness that prevents such manipulation. They develop an efficient approximation algorithm based on key mathematical observations that a small subset of terms dominate FGSV's formula, achieving significantly lower approximation error and faster convergence than state-of-the-art approaches.

## Method Summary
FGSV defines group value as the sum of individual Shapley values: ν(S₀) = Σᵢ∈S₀ SV(i). The approximation algorithm exploits concentration of measure for hypergeometric distributions to approximate contributions using derivatives at the mean intersection size rather than summing over all subsets. For small subsets (s < s̄), direct Monte Carlo estimation is used via Eq. (8) and (10). For large subsets (s ≥ s̄), paired Monte Carlo sampling estimates utility differences directly to reduce variance. The method requires O(n[1 + (α₀(1 − α₀))²(log n)³]) utility evaluations for (ϵ, δ)-approximation.

## Key Results
- FGSV achieves lowest Area Under the Convergence Curve (AUCC) across all problem sizes in experiments
- The method demonstrates stable valuations that resist manipulation through data partitioning
- FGSV achieves (ϵ, δ)-approximation with significantly fewer utility evaluations than baseline approaches
- Applications to copyright attribution in generative AI show FGSV produces faithful valuations resistant to shell company attacks

## Why This Works (Mechanism)

### Mechanism 1: Summation Invariance via Axiom of Faithfulness
The shell company attack exploits the fact that standard Group Shapley Value treats groups as atomic units, causing combinatorial weights to shift when groups are split. FGSV defines group value as the sum of individual Shapley values, making the total value invariant to how data is partitioned. This neutralizes the strategic incentive to split groups since the sum of individual values remains constant regardless of grouping.

### Mechanism 2: Concentration of Measure for Efficient Approximation
FGSV's formula can be approximated efficiently by exploiting the observation that intersection sizes between random subsets and target groups concentrate tightly around their mean. This allows the algorithm to use Taylor expansion approximations at the mean intersection size rather than summing over all possible intersection sizes, dramatically reducing computational complexity.

### Mechanism 3: Variance Reduction via Paired Sampling
The algorithm uses paired sampling to estimate utility differences directly, which is statistically more efficient than estimating individual utilities and subtracting. By evaluating U(S ∪ {i₁}) - U(S ∪ {i₂}) on the same base set S, the variance from the randomness of S is cancelled out, isolating the marginal value of group membership and improving convergence speed.

## Foundational Learning

- **Shapley Value Axioms (Efficiency, Symmetry, Null Player)**: Understanding why standard Group Shapley Value fails requires knowing that GSV satisfies these axioms at the group level but fails the new "Faithfulness" axiom because it ignores internal group structure. Quick check: Does satisfying "Efficiency" guarantee that a subgroup's value equals the sum of its members' values?

- **Hypergeometric Distribution**: The approximation algorithm relies on the probability distribution of how many items from a target group appear in a random subset. Quick check: If you draw k items from a jar of N items containing K red items, what is the expected number of red items in your hand?

- **Algorithmic Stability (Deletion/Second-order)**: The algorithm's speed relies on the utility function not changing drastically if one or two data points are swapped. Quick check: If a model retrains from scratch with a small data change, does its accuracy typically change smoothly or discontinuously?

## Architecture Onboarding

- **Component map**: Input Dataset D, Target Group S₀, Threshold s̄, Utility Oracle U -> Sampler (generates base sets S and paired indices) -> Evaluator (queries U for utility pairs) -> Aggregator (sums weighted utility differences into T(s) terms and accumulates into FGSV score)

- **Critical path**: The calculation of Δμ estimate (Eq 11) for large s is where the "faithful" value is actually approximated. If the sampling distribution is wrong here, the defense against shell attacks fails mathematically.

- **Design tradeoffs**: Setting threshold s̄ too low forces expensive Monte Carlo estimation of full grids for more terms. Setting it too high risks approximation error if the Taylor expansion logic doesn't hold for smaller subsets. For very small groups, the large s approximation may never trigger effectively.

- **Failure signatures**: Inconsistent valuation occurs if splitting group B into B₁, B₂ changes total value (FGSV(B) ≠ FGSV(B₁) + FGSV(B₂)), indicating implementation issues with paired sampling logic or threshold s̄. High variance suggests increasing sample size m₂ for dominant terms.

- **First 3 experiments**:
  1. **Sanity Check (SOU Game)**: Run FGSV on Sum-of-Unanimity game where exact Shapley values are known to verify estimated FGSV matches sum of closed-form SVs.
  2. **Attack Resilience Test**: Construct dataset with groups A and B, compute values, split B into 5 sub-groups, re-compute, and plot total valuation under GSV (should spike) vs. FGSV (should remain flat).
  3. **Stability Profile**: Run FGSV on Diabetes dataset with increasing subsample sizes m, plotting convergence of valuation error to establish required budget for target precision ε.

## Open Questions the Paper Calls Out

- **Can a principled (axiom-based) defense be developed against the "copier attack," where groups duplicate high-value data from other groups to inflate their own valuations?** The paper acknowledges this remains an open problem, with only ad-hoc remedies like preprocessing to detect and remove duplicates.

- **Is there a theoretically grounded alternative to the ad-hoc non-informative data augmentation strategy for handling utility functions that are ill-defined for small subset sizes?** The current augmentation approach is heuristic and lacks theoretical analysis of how injected non-informative data affects approximation guarantees.

- **Does Assumption 2 (second-order algorithmic stability) hold for broader classes of utility functions beyond SGD and Influence Functions?** Only two specific cases are verified in the paper, leaving many modern ML settings unexamined.

## Limitations

- The method's foundational mechanism of summing individual Shapley values breaks down if the utility function is only meaningful at the group level (e.g., requires minimum batch size).
- Theoretical guarantees rely on second-order algorithmic stability, which may not hold for discontinuous or highly non-convex utility surfaces common in modern ML models.
- The copyright attribution and explainable AI applications involve complex real-world utility functions where stability assumptions may be violated.

## Confidence

- **High confidence**: Theoretical framework for FGSV's faithfulness property and efficient approximation algorithm's asymptotic guarantees are mathematically rigorous and well-supported.
- **Medium confidence**: Empirical validation demonstrates clear advantages over baseline methods in controlled experiments, but real-world applications may violate stability assumptions.
- **Medium confidence**: Paired sampling variance reduction shows strong empirical performance, but theoretical analysis assumes specific noise correlation properties that may not generalize.

## Next Checks

1. **Robustness to Utility Function Class**: Test FGSV on utility functions with known discontinuities or non-smooth behavior (e.g., decision trees, ReLU networks) to verify stability assumptions hold empirically.

2. **Scaling Analysis Beyond Toy Problems**: Implement FGSV on larger-scale datasets (n > 1000) and more complex utility functions to validate whether the claimed O(n[1 + (α₀(1 − α₀))²(log n)³]) scaling holds in practice.

3. **Shell Company Attack Resilience Under Noise**: Systematically evaluate FGSV's resistance to manipulation when utility evaluations are noisy, simulating realistic conditions where exact Shapley computation is infeasible.