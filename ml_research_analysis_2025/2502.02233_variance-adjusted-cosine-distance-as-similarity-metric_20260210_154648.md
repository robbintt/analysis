---
ver: rpa2
title: Variance-Adjusted Cosine Distance as Similarity Metric
arxiv_id: '2502.02233'
source_url: https://arxiv.org/abs/2502.02233
tags:
- data
- cosine
- space
- distance
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the limitations of using traditional cosine
  similarity for data with significant variance and correlation. It proposes a variance-adjusted
  cosine distance that transforms data from a random variable space to a Euclidean
  space using inverse Cholesky factors of covariance matrices.
---

# Variance-Adjusted Cosine Distance as Similarity Metric

## Quick Facts
- arXiv ID: 2502.02233
- Source URL: https://arxiv.org/abs/2502.02233
- Authors: Satyajeet Sahoo; Jhareswar Maiti
- Reference count: 0
- Primary result: Variance-adjusted cosine distance using inverse Cholesky factors of covariance matrices improves KNN classification accuracy on Wisconsin Breast Cancer Dataset from 91.21% to 92.44% (LOOCV), with 100% accuracy achievable when population covariance matrices are known.

## Executive Summary
This paper addresses the fundamental limitation that traditional cosine similarity assumes data lies on a spheroid, which fails when data exhibits significant variance and correlation structure. The authors propose a variance-adjusted cosine distance that transforms data from random variable space to Euclidean space using inverse Cholesky factors of covariance matrices. This whitening transformation decorrelates features and normalizes variances, making angular distance a valid similarity metric. The method is validated using K-Nearest Neighbors classification on the Wisconsin Breast Cancer Dataset, demonstrating that accounting for data distribution characteristics significantly improves classification performance, especially when population covariance matrices are known.

## Method Summary
The method transforms data vectors X_Random to Euclidean space using X_Euclidean = Λ⁻¹X_Random, where Λ comes from the Cholesky decomposition of the covariance matrix Σ = ΛΛᵀ. For classification, three scenarios are tested: (1) raw cosine distance on untransformed data, (2) class-specific Cholesky transformations using population covariance matrices, and (3) an expected-value transformation E(W⁻¹) = p·W_B⁻¹ + (1-p)·W_M⁻¹ using sample covariances and class priors p. The approach leverages the connection between Mahalanobis distance and Euclidean distance in transformed space, generalizing this to cosine similarity.

## Key Results
- Raw cosine similarity on Wisconsin Breast Cancer Dataset achieves 91.21% LOOCV accuracy
- Variance-adjusted cosine using expected Cholesky factors achieves 92.44% LOOCV accuracy
- Class-specific population covariance transformations achieve 100% test accuracy (though implementation details suggest potential label leakage)
- The improvement is most pronounced when population covariance matrices are known and class labels are available for test transformation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transforming data from random variable space to Euclidean space via inverse Cholesky factors makes cosine distance a valid similarity metric when data exhibits covariance structure.
- **Mechanism:** The population covariance matrix Σ is decomposed as Σ = ΛΛᵀ. Applying Λ⁻¹ to raw data vectors performs Mahalanobis-style whitening—decorrelating features and normalizing variances—so the transformed data satisfies the spheroidal distribution assumption underlying cosine similarity. The transformed vector X_Euclidean = Λ⁻¹X_Random lives in a space where Euclidean geometry operations are valid.
- **Core assumption:** The original "random variable space" has non-spheroidal distribution due to covariance; after whitening, the space becomes Euclidean where angular distance accurately reflects similarity.
- **Evidence anchors:**
  - [abstract] "transforms data from a random variable space to a Euclidean space using inverse Cholesky factors of covariance matrices"
  - [Section 3, equations 1-4] Derives Mahalanobis distance as Euclidean distance in transformed space: "Hence Mahalanobis distance is essentially Euclidean distance between the data vectors in a new vector space"
  - [corpus] MA-DPR paper similarly notes embeddings "lie on lower-dimensional, non-linear manifold" and standard cosine fails—consistent with the geometric mismatch argument.
- **Break condition:** When covariance matrix is singular or near-singular, Cholesky decomposition fails or produces numerically unstable Λ⁻¹.

### Mechanism 2
- **Claim:** Class-specific covariance matrices enable perfect classification when population parameters are known.
- **Mechanism:** Each class has its own Σ_y and corresponding Λ_y⁻¹. Training points are transformed using their true class's transformation matrix. This aligns each class's internal geometry with Euclidean assumptions, so within-class similarity is correctly measured by cosine distance.
- **Core assumption:** Population covariance matrices for each class are known and accurately estimated; class labels for training data are correct.
- **Evidence anchors:**
  - [Section 4] "In the second case the population covariance matrix of all datapoints classed 'B' and 'M' were calculated separately"
  - [Section 5, Fig. 4] Shows 100% test accuracy when using class-specific Λ⁻¹ transformations
  - [corpus] Weak direct evidence; neighbor papers don't replicate this class-specific transformation approach.
- **Break condition:** When classes have highly overlapping covariance structures, the transformation provides minimal discriminative benefit; when sample sizes are small, sample covariance poorly estimates population covariance.

### Mechanism 3
- **Claim:** Using expected value E(Λ⁻¹) as transformation matrix improves accuracy over raw cosine even when population covariance and class membership are unknown.
- **Mechanism:** For test data with unknown class, the paper proposes E(Λ⁻¹) = p·Λ_y=1⁻¹ + (1-p)·Λ_y=-1⁻¹, where p is the prior class probability from training data. This creates a "pseudo-Euclidean" space that partially corrects for covariance structure without requiring class labels.
- **Core assumption:** The weighted average of class-specific transformations approximates a useful shared transformation; training class proportions estimate true priors well.
- **Evidence anchors:**
  - [Section 3, equations 8-10] Derives E(Λ⁻¹) and p estimation from training data
  - [Section 5, Table 1] Shows E(W⁻¹) adjusted cosine achieves 0.9244 (LOOCV) vs 0.9121 for raw cosine
  - [corpus] No direct corroboration; this expected-value approach appears novel to this paper.
- **Break condition:** When class covariance structures differ substantially, the linear interpolation between Λ⁻¹ matrices may produce a transformation that poorly fits either class.

## Foundational Learning

- **Concept: Cholesky Decomposition**
  - Why needed here: This is the computational engine for obtaining Λ⁻¹ from covariance matrix Σ. Understanding that Σ = ΛΛᵀ for positive definite matrices, and that Λ is lower triangular, is essential for implementing the transformation.
  - Quick check question: Given a 2×2 covariance matrix with diagonal [4, 9] and off-diagonal 3, can you compute the Cholesky factor Λ?

- **Concept: Mahalanobis Distance**
  - Why needed here: The variance-adjusted cosine is built on the same principle—whitening data via Σ⁻¹/² so that Euclidean distance becomes meaningful. The paper explicitly cites this connection.
  - Quick check question: Why is Mahalanobis distance scale-invariant while Euclidean distance is not?

- **Concept: Cosine Similarity vs Euclidean Distance Relationship**
  - Why needed here: The paper generalizes from "Euclidean distance valid in transformed space" to "cosine distance also valid." Understanding that cosine measures angle while Euclidean measures magnitude helps parse why normalization matters.
  - Quick check question: For normalized vectors, what is the mathematical relationship between cosine similarity and Euclidean distance?

## Architecture Onboarding

- **Component map:** Covariance Estimator -> Cholesky Decomposer -> Matrix Inverter -> Transform Module -> Similarity Computer -> KNN Classifier

- **Critical path:** Covariance estimation → Cholesky decomposition → Matrix inversion → Vector transformation → Cosine computation. The decomposition and inversion steps are O(p³) for p features; numerical stability depends on condition number of Σ.

- **Design tradeoffs:**
  - **Known vs unknown population covariance:** Using population Σ gives 100% accuracy in this study but is rarely available in practice; sample covariance with E(W⁻¹) is more realistic but achieves only marginal improvement (~1%)
  - **Class-specific vs shared transformation:** Class-specific requires knowing labels at transform time; expected-value approach works for unlabeled test data but sacrifices accuracy
  - **Regularization:** Paper does not address regularization of covariance estimates (e.g., shrinkage), which could help with small samples or near-singular matrices

- **Failure signatures:**
  - Cholesky decomposition fails with error → Σ is not positive definite (check for multicollinearity or insufficient samples)
  - Accuracy degrades below baseline → Covariance estimates are unreliable; try shrinkage estimators or increase training data
  - E(W⁻¹) performs worse than raw cosine → Classes have very different covariance structures; expected transformation is a poor compromise

- **First 3 experiments:**
  1. **Reproduce 3-case comparison** on Wisconsin Breast Cancer Dataset: (a) raw cosine, (b) class-specific Λ⁻¹ with population Σ, (c) E(W⁻¹) with sample Σ. Verify 100% accuracy in case (b) and marginal improvement in case (c).
  2. **Ablation on regularization:** Add shrinkage regularization (Ledoit-Wolf) to sample covariance before Cholesky decomposition. Test whether this closes the gap between case (b) and (c).
  3. **Stress test on synthetic data:** Generate data with controlled covariance structure (varying condition number of Σ, class overlap). Characterize when the transformation helps vs. when it degrades performance due to estimation noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the estimation of the population covariance matrix be improved to increase the accuracy of the variance-adjusted cosine distance when the true population parameters are unknown?
- **Basis in paper:** [explicit] The Conclusion states, "future course of work will involve finding a better estimate of the population covariance matrix so that more accurate transformation of the raw data can be carried out."
- **Why unresolved:** The current proposed solution uses the expected value of Cholesky factors from the sample covariance, which yields lower accuracy (92.44% LOOCV) compared to the 100% accuracy achieved when population covariance is known.
- **What evidence would resolve it:** A new estimation technique that consistently achieves classification accuracy statistically indistinguishable from the "known covariance" baseline on the same dataset.

### Open Question 2
- **Question:** How can the transformation logic be generalized for multi-class classification problems involving more than two labels?
- **Basis in paper:** [inferred] The methodology defines the transformation for test data using a binary probability $p$ and $1-p$ (Equations 8-10), specifically balancing transformations for $y=1$ and $y=-1$.
- **Why unresolved:** The formula for the "pseudo-Euclidean" transformation relies on a binary toggle, making the current mathematical derivation incompatible with datasets having three or more classes.
- **What evidence would resolve it:** A generalized weighting scheme for $E(W^{-1})$ that accommodates $k$ classes and successful validation on a multi-class dataset.

### Open Question 3
- **Question:** Is the method robust to singular covariance matrices found in high-dimensional, low-sample-size data?
- **Basis in paper:** [inferred] The method relies on Cholesky decomposition, which requires the covariance matrix $\Sigma$ to be positive definite. The paper only validates this on a dataset where $n > d$.
- **Why unresolved:** The paper suggests the method is useful for text mining, yet text data is often high-dimensional and sparse (leading to singular covariance matrices), which would cause the standard Cholesky decomposition to fail.
- **What evidence would resolve it:** Demonstrating the metric's performance on high-dimensional data using regularization or pseudo-inverse techniques to handle singularity.

## Limitations

- **Data leakage concerns:** The 100% accuracy result using population covariance matrices may involve transforming test points with their true labels, which is unrealistic in practice and represents potential data leakage.
- **Marginal practical improvement:** The expected-value transformation improves accuracy by only ~1% over raw cosine, raising questions about whether the added computational complexity is justified in real applications.
- **Unaddressed numerical stability:** The paper does not discuss regularization strategies for near-singular covariance matrices, which is critical for datasets with multicollinearity or limited samples.

## Confidence

- **100% accuracy claim (population covariance):** Medium confidence - results appear to involve label leakage during test transformation
- **Expected-value transformation improvement:** Medium confidence - methodology is sound but improvement is marginal (~1%)
- **General theoretical framework:** High confidence - the connection between Mahalanobis distance and variance-adjusted cosine is mathematically sound

## Next Checks

1. **Data Leakage Verification**: Replicate the 100% accuracy result from case 2 using a strict train-test protocol where test points are transformed using only training information (either their true labels if available at test time, or E(W⁻¹) otherwise).

2. **Regularization Impact Study**: Implement covariance matrix regularization (e.g., shrinkage estimators) and measure how it affects the performance gap between class-specific and expected-value transformations.

3. **Synthetic Data Stress Test**: Generate synthetic datasets with controlled covariance properties (varying condition numbers, class overlap) to characterize when the variance-adjusted cosine transformation provides benefits versus when estimation noise degrades performance.