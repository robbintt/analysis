---
ver: rpa2
title: 'Putting It All into Context: Simplifying Agents with LCLMs'
arxiv_id: '2505.08120'
source_url: https://arxiv.org/abs/2505.08120
tags:
- code
- solve
- context
- files
- lclms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether complex agent scaffolding is necessary
  for software engineering tasks like SWE-bench. The authors propose a simplified
  approach using long-context language models (LCLMs) that directly process the entire
  codebase without specialized tools or pipelines.
---

# Putting It All into Context: Simplifying Agents with LCLMs

## Quick Facts
- **arXiv ID**: 2505.08120
- **Source URL**: https://arxiv.org/abs/2505.08120
- **Reference count**: 40
- **Primary result**: Gemini-1.5-Pro achieves 38% solve rate on SWE-bench using direct codebase prompting, competitive with complex agent scaffolds (32%)

## Executive Summary
This paper challenges the necessity of complex agent scaffolding for software engineering tasks by demonstrating that long-context language models (LCLMs) can achieve competitive performance through direct codebase processing. The authors propose a simplified approach where LCLMs process entire repositories without specialized tools or pipelines, using code restatement and chain-of-thought prompting techniques. Their results show that Gemini-1.5-Pro reaches 38% solve rate on SWE-bench, comparable to carefully engineered agent scaffolds, while Gemini-2.5-Pro attains 50.8%. The work suggests that as LCLM capabilities improve, simpler monolithic approaches may replace complex agent scaffolding for many tasks.

## Method Summary
The paper proposes a simplified approach using long-context language models to handle software engineering tasks without complex agent scaffolding. The method involves prompting LCLMs with the entire repository context and using two key techniques: code restatement for in-context retrieval and chain-of-thought prompting. The authors also introduce a two-stage approach that combines LCLM compression with short-context LM repair, achieving 48.6% solve rate. The approach is evaluated on SWE-bench, demonstrating that LCLMs can directly process codebases and generate solutions without the need for specialized tools or pipelines.

## Key Results
- Gemini-1.5-Pro achieves 38% solve rate on SWE-bench using direct codebase prompting
- Gemini-2.5-Pro attains 50.8% solve rate, outperforming complex agent scaffolds
- Two-stage approach (LCLM compression + short-context LM repair) achieves 48.6% solve rate
- Results are competitive with carefully engineered agent scaffolds (32% solve rate)

## Why This Works (Mechanism)
The success of this approach stems from LCLMs' ability to process entire codebases in context, eliminating the need for specialized retrieval and tool-use mechanisms. By providing the full repository context directly to the model, LCLMs can leverage their understanding of code relationships and patterns without intermediate steps. The code restatement technique helps the model retrieve relevant information from the large context window, while chain-of-thought prompting enables systematic problem-solving. This monolithic approach reduces complexity and potential failure points compared to multi-agent systems with specialized components.

## Foundational Learning
- **Long-context language models**: Models capable of processing thousands of tokens, enabling direct codebase analysis
  - Why needed: Traditional models have limited context windows, requiring complex retrieval systems
  - Quick check: Verify model context window size and effective utilization

- **Code restatement prompting**: Technique for retrieving relevant information from large contexts
  - Why needed: Helps models identify and focus on relevant code sections in extensive repositories
  - Quick check: Test retrieval accuracy with and without code restatement

- **Chain-of-thought prompting**: Structured reasoning approach for systematic problem-solving
  - Why needed: Enables step-by-step analysis of complex software engineering tasks
  - Quick check: Compare success rates with and without chain-of-thought

- **Two-stage compression approach**: Combining LCLM processing with short-context refinement
  - Why needed: Leverages strengths of both long and short-context models
  - Quick check: Measure improvement from adding short-context refinement stage

## Architecture Onboarding

Component map: LCLM -> Codebase Processing -> Solution Generation

Critical path: Input repository → LCLM processing → Code restatement → Chain-of-thought reasoning → Solution generation

Design tradeoffs: Monolithic LCLM approach vs. complex agent scaffolding with specialized tools
- Monolithic: Simpler, fewer failure points, but requires large context windows
- Agent scaffolding: More modular, but introduces complexity and potential failure modes

Failure signatures: Context window overflow, retrieval failures, incomplete reasoning chains

First experiments to run:
1. Baseline test: LCLM with direct codebase prompting (no special techniques)
2. Code restatement ablation: Compare with and without code restatement
3. Chain-of-thought comparison: Test different prompting strategies for reasoning

## Open Questions the Paper Calls Out
None

## Limitations
- All experiments focus on SWE-bench, limiting generalizability to other software engineering tasks
- Comparison methodology may be unfair as it uses aggregated agent scaffold results rather than individual frameworks
- Ablation studies only tested Gemini-1.5-Pro, leaving uncertainty about technique effectiveness for other models

## Confidence

**High confidence**: LCLMs can achieve competitive SWE-bench performance without complex scaffolding

**Medium confidence**: Code restatement and chain-of-thought prompting are key contributors to success

**Medium confidence**: The two-stage approach provides consistent improvements

## Next Checks
1. Test the proposed approach on additional software engineering benchmarks (e.g., HumanEval, MBPP) to assess generalizability beyond SWE-bench
2. Conduct ablation studies for all model configurations (including Gemini-2.5-Pro and two-stage) to isolate the impact of each prompting technique
3. Compare against the full range of individual agent scaffolds referenced in [4] rather than a single aggregated number to ensure fair evaluation