---
ver: rpa2
title: Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification
  Without Augmentation
arxiv_id: '2510.03598'
source_url: https://arxiv.org/abs/2510.03598
tags:
- training
- loss
- accuracy
- epoch
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates the Hierarchical Reasoning Model (HRM) for
  small natural-image classification without data augmentation, comparing it to a
  simple CNN baseline. HRM uses two Transformer-style modules (fL, fH), DEQ-style
  one-step training, deep supervision, rotary position embeddings, and RMSNorm.
---

# Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation

## Quick Facts
- arXiv ID: 2510.03598
- Source URL: https://arxiv.org/abs/2510.03598
- Authors: Alexander V. Mantzaris
- Reference count: 35
- HRM without augmentation achieves ~98% on MNIST but only 65.0% on CIFAR-10 and 29.7% on CIFAR-100, while training ~30x slower than CNN baseline

## Executive Summary
This study evaluates the Hierarchical Reasoning Model (HRM) for small natural-image classification without data augmentation, comparing it against a simple CNN baseline. HRM employs two Transformer-style modules with DEQ-style one-step training, deep supervision, rotary position embeddings, and RMSNorm. While HRM performs strongly on MNIST (~98% accuracy), it struggles on CIFAR datasets, achieving only 65.0% on CIFAR-10 and 29.7% on CIFAR-100 despite showing stable optimization curves. The CNN baseline outperforms HRM significantly, training approximately 30x faster per epoch and achieving 77.2% accuracy on CIFAR-10 and 45.3% on CIFAR-100. The study concludes that HRM lacks sufficient image-specific inductive bias in the small-image, no-augmentation regime.

## Method Summary
The Hierarchical Reasoning Model uses two Transformer-style modules (f_L, f_H) with DEQ-style one-step training approach. The architecture incorporates rotary position embeddings for sequence modeling and RMSNorm for normalization. Deep supervision is employed throughout training. The model is evaluated on MNIST, CIFAR-10, and CIFAR-100 without any data augmentation. Performance is benchmarked against a simple CNN baseline that uses standard convolutional layers. Training dynamics and optimization stability are analyzed through loss curves and accuracy metrics across multiple epochs.

## Key Results
- HRM achieves ~98% test accuracy on MNIST, demonstrating strong performance on simple datasets
- On CIFAR-10, HRM reaches only 65.0% test accuracy after 25 epochs, while CNN baseline achieves 77.2% accuracy and trains ~30x faster per epoch
- On CIFAR-100, HRM attains 29.7% test accuracy with 91.5% train accuracy (indicating overfitting), while CNN achieves 45.3% test accuracy with 50.5% train accuracy

## Why This Works (Mechanism)
None

## Foundational Learning
- **DEQ-style one-step training**: A fixed-point iteration approach that reduces computational complexity compared to traditional deep equilibrium models; needed for efficient training of recursive architectures; quick check: verify convergence within specified iteration limits
- **Rotary position embeddings**: Encode positional information in sequence models by rotating queries and keys based on their positions; needed for capturing spatial relationships in images; quick check: ensure rotation matrices are correctly applied to embedding dimensions
- **RMSNorm**: Root mean square normalization that stabilizes training by normalizing activations; needed for consistent gradient flow in deep networks; quick check: verify normalization statistics are computed across correct dimensions
- **Deep supervision**: Provides intermediate supervision at multiple layers to accelerate training and improve gradient flow; needed for training very deep or recursive models; quick check: confirm gradient contributions from all supervision points

## Architecture Onboarding

**Component Map**: Input -> Convolutional Encoder -> f_L (Transformer-style) -> f_H (Transformer-style) -> Classification Head

**Critical Path**: Image input flows through initial convolutional layers for feature extraction, then passes through two hierarchical Transformer-style modules that process features at different scales, with rotary position embeddings providing spatial context at each stage.

**Design Tradeoffs**: The use of DEQ-style training reduces memory requirements but increases per-epoch computation time. Rotary position embeddings provide spatial awareness but may not capture local image patterns as effectively as convolutional operations. The hierarchical structure allows multi-scale processing but may introduce optimization challenges without sufficient image-specific inductive bias.

**Failure Signatures**: Poor performance on complex datasets without augmentation, significant train-test accuracy gap indicating overfitting, and substantially slower training times compared to CNN baselines suggest the model lacks adequate spatial feature extraction capabilities for small natural images.

**First Experiments**:
1. Evaluate HRM performance with standard data augmentation on CIFAR datasets
2. Replace rotary position embeddings with learned positional encodings to test their impact
3. Add convolutional layers before the Transformer modules to increase image-specific inductive bias

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to small-scale image datasets (MNIST, CIFAR-10, CIFAR-100) without augmentation
- Performance gap between HRM and CNN baseline may not generalize to larger or more complex datasets
- One-step DEQ-style training results in significantly slower per-epoch training times
- Overfitting interpretation for CIFAR-100 results lacks exploration of regularization techniques

## Confidence

**High Confidence**: HRM's strong performance on MNIST (~98% accuracy), the training speed comparison showing CNN is ~30x faster per epoch, and the stable optimization curves for HRM.

**Medium Confidence**: The overfitting interpretation for CIFAR-100 results and the conclusion that HRM lacks image-specific inductive bias, as these could potentially be addressed through architectural modifications.

**Low Confidence**: The generalizability of these findings to larger datasets or different image classification tasks, as the study is limited to small-resolution images without augmentation.

## Next Checks
1. Evaluate HRM with standard data augmentation techniques on CIFAR datasets to determine if the performance gap with CNN baselines narrows.
2. Conduct ablation studies to isolate the impact of individual HRM components (rotary position embeddings, RMSNorm, deep supervision) on performance.
3. Test HRM on larger-scale datasets (e.g., ImageNet-32x32 or Tiny ImageNet) to assess scalability beyond the small-image regime studied here.