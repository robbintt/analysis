---
ver: rpa2
title: 'PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness
  with Theory of Mind'
arxiv_id: '2504.15313'
source_url: https://arxiv.org/abs/2504.15313
tags:
- game
- name
- card
- chips
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PolicyEvol-Agent is a large language model (LLM)-based framework
  for multi-agent games with incomplete information, featuring policy evolution via
  environment perception and self-awareness with Theory of Mind. The agent dynamically
  adjusts its behavioral policy by integrating game experiences and iteratively updating
  strategies without parameter tuning.
---

# PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind

## Quick Facts
- arXiv ID: 2504.15313
- Source URL: https://arxiv.org/abs/2504.15313
- Authors: Yajie Yu; Yue Feng
- Reference count: 36
- Primary result: Outperforms RL-based and Suspicion-Agent baselines in Leduc Hold'em, winning significantly more chips after 100 games.

## Executive Summary
PolicyEvol-Agent is a large language model (LLM)-based framework for multi-agent games with incomplete information, featuring policy evolution via environment perception and self-awareness with Theory of Mind. The agent dynamically adjusts its behavioral policy by integrating game experiences and iteratively updating strategies without parameter tuning. It generates multifaceted beliefs about both the environment and self through ToM reasoning, enabling more rational decision-making. In experiments on Leduc Hold'em, PolicyEvol-Agent outperformed traditional RL-based methods and the state-of-the-art Suspicion-Agent baseline, winning significantly more chips after 100 games.

## Method Summary
PolicyEvol-Agent uses a four-module architecture built around an LLM with Theory of Mind capabilities. The system converts low-level game states into readable text observations, maintains game memory, and periodically evolves its policy by reflecting on past mistakes through structured LLM prompts. Belief generation is split into environment perception (opponent modeling) and self-awareness (hand strength evaluation), both conditioned on the updated policy patterns. Plan recommendation generates candidate actions with associated win rates, and the action with highest expected chip gain is selected. The entire framework operates through prompt-based inference without any parameter training.

## Key Results
- Won significantly more chips than traditional RL methods (DQN, NFSP, DMC, CFR) and Suspicion-Agent baseline after 100 games
- Policy evolution mechanism effectively improved performance over time
- Ablation studies confirmed importance of belief generation and planning modules
- Demonstrated strategic human-like behaviors including bluffing and flexible folding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy evolution through memory reflection corrects biased behavioral probabilities
- Mechanism: Agent maintains policy P(am|cn) and compares it against detection probability P_detect from history. When inconsistencies found, LLM with ToM reasoning generates revised joint probability P(am,cn|Reflection), creating calibrated P_new
- Core assumption: LLMs can perform Bayesian-like updates on policy when prompted with structured historical data and reflective task
- Evidence anchors: Abstract states "uses policy evolution to iteratively refine decision-making based on game history"; section 3.2 describes fine-tuning P(am|cn) through reflection on irrational decisions
- Break condition: Fails if LLM's reflection is hallucinated or misidentifies loss causes, leading to policy degradation

### Mechanism 2
- Claim: Multifaceted belief generation via Theory of Mind improves decision grounding
- Mechanism: Belief_Env infers opponent's hidden card and strategy from actions and opponent pattern PattEnv; Belief_Self evaluates agent's own hand strength and position conditioned on Belief_Env and PattSelf
- Core assumption: Explicitly generating and conditioning on beliefs about opponent and self provides richer state representation than single belief state
- Evidence anchors: Abstract mentions "generates multifaceted beliefs about opponents and oneself"; section 3.3 describes belief perception consisting of self-awareness and environmental reasoning
- Break condition: Fails if initial opponent model is flawed, cascading into incorrect beliefs

### Mechanism 3
- Claim: Plan recommendation with win-rate calculation guides action selection
- Mechanism: Using generated beliefs and calibrated policy, LLM generates multiple action plans with expected win rates, selecting plan with highest expected chip gain
- Core assumption: LLMs can perform reliable expected value calculations over candidate plans when explicitly prompted
- Evidence anchors: Section 3.4 describes LLM evaluating actions by calculating expectation of potential chip gains; abstract mentions "generates plans with associated win rates"
- Break condition: Fails if LLM's arithmetic or probabilistic reasoning is flawed

## Foundational Learning

**Theory of Mind (ToM)**
- Why needed: Core cognitive primitive for inferring hidden information and intentions from observable actions
- Quick check: Explain difference between first-order ("I think he has a King") and second-order ("I think he thinks I have a King") ToM inference

**Imperfect Information Games**
- Why needed: Framework designed for settings where agent doesn't know full game state (opponent's private cards)
- Quick check: Why is Minimax search inadequate for Leduc Hold'em?

**Prompt Engineering for State Representation**
- Why needed: Agent's performance depends on translating game states into LLM-understandable textual descriptions
- Quick check: What are risks of dumping raw JSON state into LLM prompt without structured observation interpretation?

## Architecture Onboarding

**Component map:**
Observation Interpretation -> Game Memory -> Policy Evolution Module (Detection -> Evaluation -> Revision) -> Multifaceted Belief Generation Module -> Plan Recommendation Module -> Action Selector

**Critical path:** State -> Observation Interpretation -> Memory Update -> Policy Evolution (periodic) -> Belief Generation -> Plan Recommendation -> Action

**Design tradeoffs:**
- LLM Calls vs. Latency/Cost: Multiple LLM calls per decision introduce significant latency and API cost
- Generalization vs. Game-Specificity: Prompts highly tailored to Leduc Hold'em, requiring substantial engineering for new games
- Probabilistic Rigor vs. LLM Intuition: "Soft" probabilistic approach trading mathematical guarantees for flexibility

**Failure signatures:**
- Cascading Hallucination: Wrong Belief_Env leads to sub-optimal plans
- Policy Drift: Flawed policy evolution causes degradation over games
- Inconsistent Reasoning: Plan contradicts its own Belief_Self
- Arithmetic Errors: LLM miscalculates expected chip gain

**First 3 experiments:**
1. Replicate Leduc Hold'em baselines and verify performance against reported results
2. Ablation on belief components to measure individual contribution to win rate
3. Track P_new values over long series of games to visualize policy convergence

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on LLM's ability to perform reliable probabilistic reasoning and self-reflection
- Lack of explicit validation for LLM's internal reasoning processes
- High sensitivity to prompt engineering quality
- Limited demonstration of generalizability to other imperfect information games

## Confidence

**Major Uncertainties:**
Core claims rely on LLM's ability to perform reliable probabilistic reasoning and self-reflection, which lacks direct validation. Framework's sensitivity to prompt engineering quality is not thoroughly explored.

**Confidence Labels:**
- High Confidence: Overall architecture and experimental setup are well-specified; implementation details clear
- Medium Confidence: Policy evolution mechanism's effectiveness depends on LLM's reflection accuracy
- Low Confidence: Generalizability to other games is not demonstrated

## Next Checks
1. Probe LLM reasoning by analyzing generated reflection texts against ground-truth game outcomes
2. Test policy stability through extended simulations (1000+ games) against fixed opponent
3. Implement framework on different imperfect information game (e.g., Kuhn Poker) to measure performance degradation