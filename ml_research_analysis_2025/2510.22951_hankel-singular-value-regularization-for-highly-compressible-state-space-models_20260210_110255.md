---
ver: rpa2
title: Hankel Singular Value Regularization for Highly Compressible State Space Models
arxiv_id: '2510.22951'
source_url: https://arxiv.org/abs/2510.22951
tags:
- singular
- hankel
- which
- state
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to make state space models (SSMs)
  used in deep learning more compressible by regularizing their Hankel singular values
  during training. The core idea is that systems with Hankel singular values that
  decay quickly can be compressed effectively using standard model reduction techniques.
---

# Hankel Singular Value Regularization for Highly Compressible State Space Models

## Quick Facts
- arXiv ID: 2510.22951
- Source URL: https://arxiv.org/abs/2510.22951
- Authors: Paul Schwerdtner, Jules Berman, Benjamin Peherstorfer
- Reference count: 40
- Primary result: Enables up to 10x more compression while maintaining accuracy compared to standard training methods

## Executive Summary
This paper addresses the challenge of compressing state space models (SSMs) used in deep learning by introducing a novel training-time regularization approach. The key insight is that models with rapidly decaying Hankel singular values can be compressed more effectively using standard model reduction techniques. The authors propose a block-structured parameterization of SSMs that enables efficient computation of Hankel singular values during training, making this regularization practical. Experiments on Long Range Arena benchmarks demonstrate that this approach achieves significantly better compression rates while maintaining high accuracy compared to standard training methods.

## Method Summary
The method introduces Hankel Singular Value Regularization (HSVR) into SSM training by adding a nuclear-norm regularizer that penalizes the sum of Hankel singular values. This is made computationally tractable through a block-diagonal rotation parameterization of the system matrix A, which reduces the Lyapunov equation complexity from O(n³) to O(n²). A custom associative scan operator enables efficient state sequence computation without explicit matrix powers. After training, balanced truncation is applied to compress the models. The approach is implemented in the Mamba codebase and tested on various Long Range Arena tasks.

## Key Results
- Up to 10x more compression achievable while maintaining accuracy on Long Range Arena benchmarks
- HSVR achieves higher accuracy than standard training at the same compression ratio across all tested tasks
- Models trained with HSVR can be compressed to 80-90% of their original size with less than 2% accuracy drop
- Regularization enables effective compression where standard training methods fail to achieve meaningful size reduction

## Why This Works (Mechanism)

### Mechanism 1: Hankel Singular Value Decay Controls Approximability
Regularizing the sum of Hankel singular values during training produces models whose input-output maps can be accurately approximated by lower-order systems. The error bound for reduced-order approximation is directly proportional to the sum of truncated HSVs, so minimizing this bound during training creates naturally compressible models.

### Mechanism 2: Block-Diagonal Rotation Parameterization Reduces Gramian Computation Cost
Structuring the system matrix A as block-diagonal rotation matrices reduces Lyapunov equation complexity from O(n³) to O(n²), making HSV regularization tractable during training. This decomposition enables independent solution of smaller Lyapunov equations rather than solving large coupled systems.

### Mechanism 3: Parallel Scan with Rotation Operator Composition Avoids Explicit Matrix Products
A custom associative scan operator enables O(log ns) parallel computation of state sequences without materializing powers of A. This operator composition tracks only parameter vectors rather than full matrices, dramatically reducing computational overhead for long sequences.

## Foundational Learning

- **Hankel Operators and Singular Values in LTI Systems**
  - Why needed: HSV decay rate determines how well a system can be approximated by lower-order models
  - Quick check: Given HSVs [10, 1, 0.1, 0.01, 0.001], if you truncate to order 2 with ∥u∥ℓ2 = 1, what's the worst-case approximation error bound?

- **Lyapunov Equations and Gramians**
  - Why needed: HSVs are computed from solutions to Lyapunov equations, which arise from controllability/observability analysis
  - Quick check: For what class of systems are Lyapunov equations AP A^⊤ − P + BB^⊤ = 0 guaranteed to have unique solutions? What property of A is required?

- **Balanced Truncation and Error Bounds**
  - Why needed: The compression method uses balanced truncation because it provides provable error bounds tied to HSV sum
  - Quick check: Why must a system be balanced before truncating states? What happens if you truncate without balancing?

## Architecture Onboarding

- **Component map:** Input sequence → Encoder → [SSM Layer × L] → Decoder → Classifier
- **Critical path:** Implement rotation-block parameterization → Implement block-structured Lyapunov solver → Verify HSV computation → Integrate regularizer → Implement balanced truncation
- **Design tradeoffs:** Rotation blocks vs. diagonal A (expressivity vs. efficiency), regularization magnitude selection, unidirectional vs. bidirectional scan
- **Failure signatures:** Accuracy collapse at moderate truncation, training divergence, no compression benefit despite regularization, inference errors after compression
- **First 3 experiments:**
  1. Sanity check: Train single SSM layer (n=64) on toy linear system identification task, verify HSV decay with regularization
  2. Ablation on regularization strength: Sweep regularizer magnitude on sMNIST, plot HSV decay curves and accuracy vs. truncation ratio
  3. Comparison to pruning baseline: Replicate comparison on IMDB between HSVR and LAST pruning at various truncation ratios

## Open Questions the Paper Calls Out

### Open Question 1
Can nonlinear transformations be developed to induce Hankel singular value compressibility in pre-trained state space models without requiring regularization during initial training? The paper notes that linear transformations cannot change Hankel singular values, so new nonlinear mechanisms are needed for post-hoc compression of existing models.

### Open Question 2
How can the proposed regularization framework be extended to state space models with time-varying system matrices, such as those found in MAMBA architecture? The current approach relies on Lyapunov equations that assume time-invariant system matrices, requiring different theoretical treatment for time-varying systems.

### Open Question 3
Can regularizers be formulated to enable rigorous nonlinear compression of state space layers, rather than being restricted to linear balanced truncation? While linear compression provides error bounds, nonlinear reduction methods might yield smaller or more efficient models, but suitable regularizers are currently undefined.

## Limitations
- Currently restricted to linear time-invariant systems; time-varying systems require different theoretical treatment
- Limited to linear compression methods (balanced truncation); nonlinear compression remains future work
- No experiments validating expressivity of rotation-block parameterization compared to unconstrained or diagonal alternatives

## Confidence
- High: Theoretical foundation of HSV regularization and error bounds
- Medium: Computational efficiency claims for block-structured Lyapunov solver
- Medium: Expressivity of rotation-block parameterization for complex tasks
- Low: Performance claims without extensive ablation studies on regularization hyperparameters

## Next Checks
1. Verify the O(n²) complexity claim for the block-structured Lyapunov solver by comparing runtime against standard O(n³) solver on systems of increasing size
2. Confirm that the rotation-block parameterization maintains universal approximation capability by testing on a known benchmark requiring complex dynamics
3. Validate the accuracy claims by reproducing the 10× compression result on at least one Long Range Arena task with reported metrics matching the paper's figures