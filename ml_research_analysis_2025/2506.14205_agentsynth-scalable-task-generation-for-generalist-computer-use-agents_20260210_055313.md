---
ver: rpa2
title: 'AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents'
arxiv_id: '2506.14205'
source_url: https://arxiv.org/abs/2506.14205
tags:
- task
- tasks
- uni00000013
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentSynth, a scalable pipeline for synthesizing
  high-quality computer-use tasks and trajectory datasets for generalist agents. By
  leveraging information asymmetry, AgentSynth constructs simple subtasks that are
  easy to generate but challenging when composed into long-horizon tasks.
---

# AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents

## Quick Facts
- **arXiv ID:** 2506.14205
- **Source URL:** https://arxiv.org/abs/2506.14205
- **Reference count:** 40
- **Primary result:** State-of-the-art agents drop from 18% to 4% success across difficulty levels 1-6, with $0.6 cost per trajectory.

## Executive Summary
AgentSynth introduces a scalable pipeline for synthesizing high-quality computer-use tasks and trajectory datasets for generalist agents. By leveraging information asymmetry, it constructs simple subtasks that are easy to generate but challenging when composed into long-horizon tasks. The pipeline uses six LLM-based agents to propose, execute, verify, revise, and summarize tasks, achieving controllable difficulty through subtask chaining. Experiments show state-of-the-art LLM agents experience steep performance degradation across difficulty levels, demonstrating the benchmark's discriminative power while maintaining low generation costs.

## Method Summary
AgentSynth generates tasks through a six-agent pipeline: task proposer, executor, verifier, reviser, follow-up proposer, and summarizer. The executor uses a two-stage ReAct-style approach with GPT-4.1 for planning and computer-use-preview for visual grounding. Tasks are built by chaining simple subtasks—each subtask is proposed, executed, and verified before the next is generated. The summarizer composes the first n subtasks into a composite task at difficulty level n. This exploits information asymmetry: tasks are easy to generate forward but hard to solve backward. A reviser ensures task descriptions match actual execution, and a verifier uses WebJudge-style screenshot analysis to validate trajectories.

## Key Results
- State-of-the-art agents drop from 18% to 4% success across difficulty levels 1-6
- Pipeline achieves $0.6 cost per trajectory, orders of magnitude cheaper than human annotations
- Generates over 6,000 diverse and realistic tasks in the OSWorld environment
- Human evaluation shows 89% verifier accuracy in detecting task success

## Why This Works (Mechanism)

### Mechanism 1: Information Asymmetry for Task Generation
The pipeline exploits information asymmetry by breaking long-horizon tasks into simple sequential subtasks. It's easier to solve tasks step-by-step in the forward direction than to infer entire solutions from scratch. A task proposer generates an initial subtask, an executor completes it while logging the trajectory, and follow-up proposers iteratively build on prior states. This makes tasks easy to generate but hard to solve at test time.

### Mechanism 2: Controllable Difficulty via Subtask Chaining
Difficulty is controlled by composing the first n subtasks into a single high-level task. Increasing n adds steps, applications, and planning requirements, systematically increasing difficulty. The paper defines difficulty level n as the summary of the first n subtasks.

### Mechanism 3: Multi-Agent Pipeline with Verification and Revision
Six LLM-based agents work in sequence to improve trajectory reliability. The verifier judges success and completion percentage, and if incomplete, the reviser rewrites the task description to match actual progress. This prevents cascading errors and ensures summaries reflect reality.

## Foundational Learning

- **Concept: Information Asymmetry in Generation vs. Evaluation**
  - Why needed here: The core insight is that generating tasks forward (step-by-step) is easier than solving them backward (from goal to plan). Understanding this helps grasp why the pipeline can produce hard tasks from easy generation.
  - Quick check question: Why is generating a long-horizon task by chaining subtasks easier than asking an agent to solve the same task from scratch?

- **Concept: ReAct-Style Agents and Visual Grounding**
  - Why needed here: The executor uses a ReAct-style loop where a planner (GPT-4.1) proposes high-level actions and a grounding model (computer-use-preview) maps them to pixel-level coordinates. This separation is critical for understanding pipeline behavior and failure modes.
  - Quick check question: What is the role of the two-stage executor, and why is visual grounding separated from planning?

- **Concept: Automated Verification in Agent Pipelines**
  - Why needed here: Verification determines which trajectories are valid and shapes downstream task descriptions via the reviser. Understanding verification approaches (e.g., WebJudge-style filtering) is key to diagnosing quality issues.
  - Quick check question: How does the verifier reduce token usage and improve accuracy over naive screenshot-by-screenshot review?

## Architecture Onboarding

- **Component map:** Task Proposer → Executor → Verifier → (if incomplete) Reviser → Follow-up Proposer → Executor (loop n times) → Summarizer

- **Critical path:** Task Proposer → Executor → Verifier → (if incomplete) Reviser → Follow-up Proposer → Executor (loop n times) → Summarizer. The verifier and reviser are essential to prevent misaligned trajectories from corrupting the final dataset.

- **Design tradeoffs:**
  - Two-stage executor improves grounding but adds latency and cost
  - Downsampling screenshots for verification reduces tokens but may lose fine details
  - Persona-driven task continuity enhances diversity but may produce unrealistic tasks if personas are poorly matched to the environment

- **Failure signatures:**
  - Cascading grounding errors: Repeated misclicks or lost focus, visible in logs as repeated similar coordinates without progress
  - Verifier false positives: Tasks marked as successful despite incomplete evidence (e.g., missing save/submit action)
  - Non-compositional subtasks: Difficulty does not increase with level; token count grows but success rate plateaus

- **First 3 experiments:**
  1. Baseline without reviser: Run the pipeline with the reviser disabled and measure trajectory misalignment rate (compare final summaries to actual executed actions)
  2. Difficulty calibration: Evaluate whether success rate decreases monotonically with difficulty level across multiple models; analyze per-domain breakdown to identify where horizon length vs. intrinsic complexity dominates
  3. Verifier stress test: Introduce controlled errors (e.g., intentionally incomplete trajectories) and measure verifier accuracy; compare against human annotations to quantify false positive/negative rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LLM used in the task generation pipeline systematically affect task diversity, realism, and difficulty calibration?
- Basis in paper: [explicit] "Different LLM models might generate tasks with systematically different complexity, realism, or meaningfulness, and it remains an open and interesting research question how model choice affects generated task properties."
- Why unresolved: The authors used only GPT-4.1 for generation and did not compare across models.
- What evidence would resolve it: Run AgentSynth with multiple LLM architectures and compare generated task properties using standardized metrics.

### Open Question 2
- Question: Can intrinsic task complexity be disentangled from task novelty in the current difficulty framework based on subtask chaining?
- Basis in paper: [explicit] "Future analyses could systematically separate intrinsic task complexity from novelty or lack of exposure."
- Why unresolved: Current difficulty levels confound horizon length with intrinsic difficulty; a short task may be intrinsically hard.
- What evidence would resolve it: Design controlled experiments comparing agent performance on short-but-novel vs. long-but-familiar tasks at matching difficulty levels.

### Open Question 3
- Question: Does fine-tuning or reinforcement learning on AgentSynth trajectories improve visual grounding and long-horizon planning in LLM agents?
- Basis in paper: [explicit] "Fine-tuning on AgentSynth-generated trajectories may help models acquire better visual grounding and long-term planning skills. Due to computational limitations, we leave this direction to future work."
- Why unresolved: AgentSynth was used only for evaluation; training experiments were not conducted.
- What evidence would resolve it: Train agents on AgentSynth trajectories using curriculum learning and measure transfer performance on held-out tasks.

### Open Question 4
- Question: Would adding safe, sandboxed authentication-based tasks (e.g., dummy email accounts) significantly improve dataset realism without safety trade-offs?
- Basis in paper: [inferred] The authors exclude login tasks due to privacy concerns but note this "limits our exposure to realistic and practically important tasks."
- Why unresolved: No experiments were conducted with sandboxed authentication environments.
- What evidence would resolve it: Create sandboxed accounts, generate authenticated tasks, and compare agent performance gaps to real-world human baselines.

## Limitations
- The paper does not rigorously validate whether difficulty scaling reflects true intrinsic complexity versus merely increased horizon length
- The evaluation focuses on three specific models, limiting generalizability to other agent architectures and environments
- The pipeline relies heavily on the accuracy of LLM-based verification, which may introduce systematic biases

## Confidence

- **High Confidence**: AgentSynth achieves low-cost trajectory generation ($0.60/trajectory) and produces a large, diverse dataset (6,000+ tasks). The multi-agent pipeline with verification and revision is technically sound and demonstrably improves data quality.
- **Medium Confidence**: The claim that difficulty scales linearly with subtask count is plausible but not rigorously validated. The paper shows monotonic success rate decline but does not isolate horizon length from intrinsic complexity.
- **Medium Confidence**: The paper's evaluation on three models (GPT-4.1, OSWorld baseline, Mind2Web) demonstrates discriminative power, but the generalizability to other environments and agent architectures is untested.

## Next Checks

1. **Difficulty Calibration Validation**: Run the pipeline with the reviser disabled and measure the rate of trajectory misalignment (i.e., summaries that do not reflect actual execution). This will isolate the impact of the reviser on data quality.
2. **Difficulty Scaling Analysis**: Conduct a per-domain breakdown of success rates across difficulty levels to determine whether horizon length or intrinsic complexity dominates. For example, compare success rates on simple vs. complex subtasks at each level.
3. **Verifier Stress Test**: Introduce controlled errors (e.g., incomplete trajectories) and measure verifier accuracy against human annotations. This will quantify false positive/negative rates and identify systematic biases.