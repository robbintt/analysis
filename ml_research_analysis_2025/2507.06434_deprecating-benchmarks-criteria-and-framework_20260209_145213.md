---
ver: rpa2
title: 'Deprecating Benchmarks: Criteria and Framework'
arxiv_id: '2507.06434'
source_url: https://arxiv.org/abs/2507.06434
tags:
- deprecation
- benchmarks
- benchmark
- arxiv
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of outdated or flawed AI benchmarks
  by proposing a deprecation framework and criteria to determine when benchmarks should
  be deprecated or updated. The authors identify seven key criteria for deprecation,
  including saturation, contamination, statistical bias, high annotation error rates,
  task obsolescence, invalidated assumptions, and semantic drift.
---

# Deprecating Benchmarks: Criteria and Framework

## Quick Facts
- arXiv ID: 2507.06434
- Source URL: https://arxiv.org/abs/2507.06434
- Reference count: 21
- Primary result: Framework to determine when AI benchmarks should be deprecated or updated, with seven criteria and three-phase workflow

## Executive Summary
This paper addresses the critical problem of outdated or flawed AI benchmarks by proposing a systematic framework for determining when benchmarks should be deprecated or updated. The authors identify seven key criteria including saturation, contamination, statistical bias, annotation errors, task obsolescence, invalidated assumptions, and semantic drift. Their three-phase framework (assessment, reporting, notification) provides structured guidance for benchmark maintainers and governance actors to ensure evaluation quality. The work aims to prevent "safety-washing" and maintain robust AI evaluation practices by enabling transparent decisions about benchmark retirement.

## Method Summary
The authors propose a deprecation framework consisting of three phases: assessment, reporting, and notification. The assessment phase evaluates seven criteria to determine whether a benchmark should be partially or fully deprecated. The reporting phase documents the deprecation rationale, timeline, and alternatives in a formal deprecation report. The notification phase communicates the decision through original publication channels with visual indicators. The framework allows both benchmark developers and governance actors to initiate deprecation, with different transparency requirements for third-party actions. The authors recommend version control with DOIs and maintaining deprecation lists to track retired benchmarks.

## Key Results
- Seven criteria for benchmark deprecation identified: saturation, contamination, statistical bias, annotation errors, task obsolescence, invalidated assumptions, and semantic drift
- Three-phase framework (assessment, reporting, notification) provides systematic approach to benchmark retirement
- Dual-initiator model allows both developers and governance actors to deprecate benchmarks with appeals processes
- Framework addresses "safety-washing" by enabling removal of flawed benchmarks that enable inflated capability claims

## Why This Works (Mechanism)

### Mechanism 1: Multi-Criteria Deprecation Triggers
- Claim: Structured criteria reduce reliance on ad-hoc judgments about when benchmarks become inadequate.
- Mechanism: Seven distinct criteria serve as heuristics that function like case law—accumulated community experience informs decisions rather than rigid binary rules.
- Core assumption: Benchmark failure modes are detectable through systematic review and can be categorized into meaningful patterns.
- Evidence anchors: Abstract states "we propose criteria to decide when to fully or partially deprecate benchmarks"; Section 3 groups criteria into quantitative and qualitative categories.
- Break condition: If criteria thresholds cannot be operationalized, the framework remains guidance without enforcement teeth.

### Mechanism 2: Three-Phase Deprecation Workflow (Assess → Report → Notify)
- Claim: Sequential phases create accountability and prevent sudden evaluation ecosystem disruption.
- Mechanism: Assessment evaluates impact and component validity; Reporting documents rationale, timelines, and alternatives; Notification ensures affected users receive direct communication and visual indicators distinguish deprecated benchmarks.
- Core assumption: Transparency and documentation increase trust and compliance with deprecation decisions.
- Evidence anchors: Section 4 describes three-phase framework; Section 4.3 specifies notification should appear in same channels as original publication.
- Break condition: If notification channels are fragmented or users ignore communications, deprecated benchmarks persist in practice.

### Mechanism 3: Dual-Initiator Model (Developers + Governance Actors)
- Claim: Allowing both benchmark creators and external governance actors to initiate deprecation addresses abandoned benchmarks and conflicts of interest.
- Mechanism: Developers possess intimate benchmark knowledge but may resist deprecation due to reputational/commercial incentives; governance actors can issue third-party deprecation with appeals processes and review boards.
- Core assumption: External actors have sufficient expertise and legitimacy to evaluate technical benchmarks.
- Evidence anchors: Section 4 permits governance actor deprecation; Section 1 notes AI labs have little incentive to deprecate benchmarks supporting their models.
- Break condition: If governance actors lack technical credibility or appeals processes are perceived as unfair, legitimacy collapses.

## Foundational Learning

- **Concept: Benchmark saturation and contamination**
  - Why needed here: Saturation (performance ceiling) and contamination (training data leakage) are two of seven core deprecation criteria; understanding them is prerequisite to applying the assessment phase.
  - Quick check question: Can you explain why a model scoring 99% on a benchmark might indicate a problem with the benchmark rather than exceptional capability?

- **Concept: Dataset versioning with DOIs**
  - Why needed here: The framework recommends Digital Object Identifiers for version control; without understanding how versioning enables deprecation tracking, implementation fails.
  - Quick check question: How would a DOI help distinguish deprecated v1.0 from updated v2.0 of a benchmark?

- **Concept: Safety-washing via benchmark gaming**
  - Why needed here: The paper explicitly warns that flawed benchmarks enable "safety-washing"—inflated capability claims that obscure actual risks.
  - Quick check question: Why might a company resist deprecating a benchmark on which their model achieves state-of-the-art results?

## Architecture Onboarding

- **Component map:**
  - Assessment layer: Criteria evaluation (quantitative: saturation/contamination/bias; qualitative: errors/obsolescence/assumptions/drift)
  - Decision layer: Partial (update) vs. full deprecation determination
  - Documentation layer: Deprecation reports (rationale, timeline, alternatives, appeals process)
  - Communication layer: Notification channels, visual indicators, deprecation lists

- **Critical path:**
  1. Monitor benchmark performance curves and literature critiques
  2. Apply criteria to identify failure modes
  3. Determine deprecation level (partial vs. full)
  4. Generate deprecation report with alternatives
  5. Issue notifications through original publication channels
  6. Maintain appeals process for contested decisions

- **Design tradeoffs:**
  - Flexibility vs. enforceability: Criteria are "soft guidance" adaptable to context but lack automatic triggers
  - Developer-led vs. governance-led: Developer initiation leverages expertise; governance initiation addresses abandonment but requires external expertise
  - Full vs. partial deprecation: Partial preserves valid components but complicates version tracking

- **Failure signatures:**
  - Deprecated benchmarks continue appearing in papers without acknowledgment
  - Appeals process backlogged or perceived as rubber-stamp
  - Deprecation lists created without transparent methodology
  - No alternatives identified, leaving evaluation gaps

- **First 3 experiments:**
  1. Apply the seven criteria to one saturated benchmark (e.g., MMLU) and document which criteria trigger and at what thresholds
  2. Draft a sample deprecation report following Appendix B format for a benchmark with known annotation errors
  3. Simulate a governance-actor deprecation scenario where the original developer is unresponsive; test the appeals process design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific quantitative thresholds (e.g., error rates, saturation points) should trigger the deprecation of a benchmark?
- Basis in paper: Explicit (Section 3: "determining concrete thresholds for these criteria is beyond the scope of this paper").
- Why unresolved: The authors propose criteria as "soft" heuristics rather than rigid binary rules, leaving the specific tipping points for action undefined.
- What evidence would resolve it: Empirical studies correlating specific benchmark metric values (e.g., >90% saturation or >10% annotation error) with a measurable loss of evaluation validity.

### Open Question 2
- Question: How can governance actors effectively assess and deprecate benchmarks when the original developers are unavailable or uncooperative?
- Basis in paper: Explicit (Section 4: "inclusion requires additional transparency... when it is not done by the benchmark developers themselves").
- Why unresolved: While the framework allows third-party deprecation, it acknowledges that external actors lack the "crucial knowledge" developers possess, potentially lowering the quality of the assessment.
- What evidence would resolve it: Case studies of successful third-party audits where deprecation decisions were validated by independent review boards without developer input.

### Open Question 3
- Question: To what extent does the proposed deprecation framework mitigate "safety-washing" compared to the risk of simply replacing deprecated benchmarks with equally flawed new ones?
- Basis in paper: Inferred (Abstract: risks "safety-washing"; Section 1: "better alternatives [prevented] from gaining traction").
- Why unresolved: The paper assumes that removing bad benchmarks improves the ecosystem, but does not verify if the recommended "alternatives" are inherently more robust or if they will be adopted.
- What evidence would resolve it: Longitudinal analysis of evaluation quality in model cards before and after the adoption of deprecation lists.

## Limitations

- Framework remains conceptual guidance without operational thresholds for most deprecation criteria
- Dual-initiator model presents practical challenges around governance actor expertise and potential conflicts of interest
- Effectiveness depends heavily on user compliance and ecosystem adoption, which cannot be guaranteed through framework design alone

## Confidence

- **High Confidence**: The identification of seven systematic deprecation criteria is well-supported by benchmark evaluation literature and represents a meaningful contribution to addressing benchmark stagnation in AI evaluation practices.
- **Medium Confidence**: The three-phase framework structure (assess-report-notify) provides logical workflow for deprecation processes, though implementation details and enforcement mechanisms require further specification.
- **Medium Confidence**: The dual-initiator model addressing both developer and governance actor roles is theoretically sound but faces practical challenges around governance actor legitimacy and expertise that may limit real-world applicability.

## Next Checks

1. **Operational Thresholds Validation**: Apply the seven criteria to a saturated benchmark (e.g., MMLU) and document specific saturation levels, contamination severity metrics, and error rates that would trigger deprecation decisions, testing whether criteria can be operationalized beyond conceptual guidance.

2. **Governance Actor Capability Assessment**: Design and conduct a simulation where a governance panel (e.g., academic consortium) attempts to deprecate an abandoned benchmark, measuring the time required to gather sufficient expertise, document justification, and manage appeals compared to developer-initiated deprecation.

3. **User Compliance Tracking**: Implement the notification system for a deprecated benchmark and track downstream usage in published papers over six months, measuring whether visual indicators and direct communication reduce continued citation of deprecated versions.