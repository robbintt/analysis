---
ver: rpa2
title: 'Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse'
arxiv_id: '2410.22598'
source_url: https://arxiv.org/abs/2410.22598
tags:
- features
- feature
- explanations
- responsive
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce feature responsiveness scores to address
  limitations of standard feature attribution methods like SHAP and LIME, which can
  highlight features that do not provide recourse. Their approach measures the probability
  that a decision subject can attain a target prediction by intervening on a specific
  feature, considering actionability constraints and downstream effects.
---

# Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse

## Quick Facts
- arXiv ID: 2410.22598
- Source URL: https://arxiv.org/abs/2410.22598
- Authors: Harry Cheon; Anneke Wernerfelt; Sorelle A. Friedler; Berk Ustun
- Reference count: 40
- One-line primary result: Responsiveness scores achieve 100% responsive features in explanations compared to 0.2% for SHAP-AW on heloc dataset, while reducing cases lacking actionable features from 94% to 8%.

## Executive Summary
Standard feature attribution methods like SHAP and LIME can highlight features that do not provide recourse for decision subjects. This paper introduces feature responsiveness scores that measure the probability a decision subject can attain a target prediction by intervening on a specific feature, considering actionability constraints and downstream effects. The approach addresses the "reasons without recourse" problem by computing scores over the entire feasible intervention set rather than local gradients or single counterfactual points.

The authors develop efficient methods to compute these scores for any model, including sampling and enumeration algorithms. Experiments on three lending datasets show that standard methods often highlight unresponsive or immutable features, while their approach effectively identifies responsive features and flags fixed predictions. The method successfully identifies when predictions are truly immutable and cannot be changed by any reasonable intervention.

## Method Summary
The approach computes feature responsiveness scores that measure the probability of attaining a target prediction by intervening on specific features while respecting actionability constraints. For each feature, it generates a "reachable set" of possible intervention values using mixed-integer programming (for discrete features) or sampling (for continuous features). The score is calculated as the proportion of reachable points that achieve the target prediction. The method requires explicit actionability constraints specifying which features are mutable, their directional relationships, and any joint constraints between features.

## Key Results
- On the heloc dataset, responsiveness method achieved 100% responsive features in explanations versus 0.2% for SHAP-AW
- The approach reduced proportion of cases lacking actionable features from 94% to 8% by including additional intervention information
- Standard attribution methods highlight unresponsive features in 77-92% of cases across datasets
- The method successfully identified fixed predictions where no recourse exists for 22.2% of HELOC applicants

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Intervention Scoring
Feature responsiveness scores provide actionable recourse by measuring the probability of success rather than just contribution to the prediction. Unlike SHAP or LIME, which compute feature importance, this method computes the probability over the entire feasible intervention set for a feature, not just a local gradient or single counterfactual point.

### Mechanism 2: Hard Constraint Filtering via Reachable Sets
The method eliminates "reasons without recourse" by strictly enforcing actionability constraints during set generation. It generates a "reachable set" by solving mixed-integer programs or sampling, explicitly excluding points that violate logical or physical constraints. If the reachable set contains no positive predictions, the feature score is 0.

### Mechanism 3: Fixed Prediction Detection
The approach protects users from futile attempts by identifying "fixed predictions" where no recourse exists. If the responsiveness score is 0 for all features, the system withholds the explanation or flags the prediction as fixed, preventing harm from explaining impossible changes.

## Foundational Learning

- **Concept:** Actionability Constraints (Separable vs. Joint)
  - **Why needed here:** You cannot compute responsiveness without defining what a user can change. This paper relies heavily on distinguishing between simple bounds and causal links.
  - **Quick check question:** Can you identify a feature that is mutable but "unresponsive"?

- **Concept:** Reachable Sets (R_j(x))
  - **Why needed here:** This is the core data structure. Instead of a single counterfactual point, you must understand the set of all possible states reachable by altering feature j.
  - **Quick check question:** Why does sampling from the reachable set allow us to estimate the probability of recourse?

- **Concept:** Feature Attribution vs. Feature Responsiveness
  - **Why needed here:** To understand the gap the paper fills. Attribution answers "Why did this happen?"; Responsiveness answers "Can I fix it?"
  - **Quick check question:** Why does a high SHAP score for "Age" fail to provide recourse?

## Architecture Onboarding

- **Component map:** Constraint Encoder -> Reachable Set Generator -> Feasibility Checker -> Scorer
- **Critical path:** Defining the Actionability Constraints. If constraints are wrong, the reachable set is invalid, rendering the scores meaningless.
- **Design tradeoffs:**
  - Enumeration (Exact): Use for discrete features. Guarantees precision but computationally expensive.
  - Sampling (Approximate): Use for continuous features or large spaces. Requires setting sample size N. Trades exactness for speed.
- **Failure signatures:**
  - "Reasons without Recourse": Explanation lists immutable features (SHAP failure mode)
  - "Fixed Prediction" explanation: Explaining a decision that is mathematically impossible to change
  - Over-optimism: High responsiveness scores due to missing causal constraints
- **First 3 experiments:**
  1. Baseline Comparison: Implement SHAP-AW vs. RESP on HELOC dataset. Verify RESP yields significantly fewer "All Features Unresponsive" cases.
  2. Fixed Prediction Audit: Run RESP to calculate "% No Recourse" population. Check if these are truly fixed by model logic or constraint limitations.
  3. Sample Size Sensitivity: Run sampling algorithm with N=100 vs N=500. Measure variance in scores to validate confidence intervals.

## Open Questions the Paper Calls Out
- How can actionability constraints be effectively elicited from individual decision subjects rather than using global, indisputable constraints?
- How well do responsiveness scores generalize to high-stakes domains beyond consumer lending (e.g., employment, healthcare, criminal justice)?
- What is the computational cost of computing responsiveness scores at scale compared to standard attribution methods like SHAP?

## Limitations
- Computational scalability concerns for high-dimensional datasets with many jointly constrained features
- Burden of requiring explicit actionability constraints that must be provided by domain experts
- Assumes 1-D recourse (single feature changes) is sufficient, cannot capture multi-feature intervention scenarios

## Confidence
- **High Confidence:** Core mechanism of using probability over reachable sets is well-defined and empirically validated
- **Medium Confidence:** Claim about protecting users from "reasons without recourse" is supported but depends on correct constraint specification
- **Low Confidence:** Scalability claims are based on datasets with modest dimensionality; performance on high-dimensional data remains untested

## Next Checks
1. Constraint Sensitivity Analysis: Systematically vary constraint specifications to measure how sensitive responsiveness scores are to constraint errors
2. Multi-Feature Recourse Extension: Implement n-D recourse check for cases where 1-D responsiveness is zero but multi-feature interventions might work
3. Computational Benchmarking: Measure runtime and memory usage across datasets of increasing dimensionality to establish scalability boundaries