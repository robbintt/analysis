---
ver: rpa2
title: Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers
arxiv_id: '2510.22555'
source_url: https://arxiv.org/abs/2510.22555
tags:
- graph
- trigger
- learning
- backdoor
- triggers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of backdoor attacks on graph neural
  networks (GNNs) that can transfer across different learning paradigms (graph supervised
  learning, contrastive learning, and prompt learning). The key idea is to use graph
  prompt learning (GPL) to optimize a set of condensed subgraph triggers, making them
  generalizable across diverse paradigms.
---

# Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers

## Quick Facts
- **arXiv ID:** 2510.22555
- **Source URL:** https://arxiv.org/abs/2510.22555
- **Reference count:** 40
- **Primary result:** CP-GBA achieves state-of-the-art cross-paradigm transferability with 94.9-100% ASR across paradigms and 40.4% speedup.

## Executive Summary
This paper introduces CP-GBA, a graph backdoor attack method that enables trigger transferability across graph learning paradigms (GSL, GCL, GPL) by leveraging graph prompt learning. The key innovation is optimizing condensed subgraph triggers through GPL, which forces them to learn representations that map consistently across different downstream paradigms. Experiments on Cora, Pubmed, Facebook, and OGB-arxiv demonstrate superior attack success rates compared to single-paradigm approaches, with triggers remaining stealthy against multiple defense strategies.

## Method Summary
CP-GBA constructs a structured trigger repository using K-means clustering on target-class subgraphs, then optimizes these triggers via graph prompt learning using a bi-level optimization framework. The method freezes a pre-trained GNN encoder and trains learnable prompts along with a surrogate classifier to optimize trigger transferability. At inference, triggers are selected based on cosine similarity to target node embeddings and injected into the graph. The approach claims 40.4% speedup compared to existing methods while maintaining high attack success rates across different GNN architectures and learning paradigms.

## Key Results
- Achieves 94.9-100% ASR across GSL, GCL, and GPL paradigms on benchmark datasets
- Maintains 40.4% speedup compared to existing backdoor attack methods
- Successfully evades four defense strategies (RIGBD, Prune, OD, SACS) while preserving attack effectiveness
- Requires only ~20 condensed triggers to achieve optimal attack performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph Prompt Learning (GPL) enables cross-paradigm trigger transfer because it optimizes triggers to operate within a surjective representation space rather than paradigm-specific feature distributions.
- **Mechanism:** GPL uses a frozen pre-trained encoder with learnable prompts, forcing triggers to learn generalizable representations that map consistently across different downstream paradigms. The surjective property (Theorem 1) ensures any target embedding can be reached from some trigger-augmented graph.
- **Core assumption:** The pre-trained GNN encoder has sufficient representational capacity to map diverse graph structures to a continuous feature space.
- **Evidence anchors:** Theorems 1-2 establish surjective mapping; GPL-trained triggers achieve 94.9-100% ASR vs. 83.7-87.8% for GSL-trained triggers.

### Mechanism 2
- **Claim:** Condensed subgraph triggers improve attack effectiveness by preserving class-awareness, structural fidelity, and feature richness simultaneously.
- **Mechanism:** K-means clustering on subgraph embeddings selects K representative triggers that cover the diversity of the target class, avoiding distributional mismatch from shallow MLPs.
- **Core assumption:** The target class has sufficient structural diversity that K representative subgraphs can capture its essential patterns.
- **Evidence anchors:** K-means initialization equips triggers with category-aware properties; ASR plateaus at ~20 triggers indicating sufficient coverage.

### Mechanism 3
- **Claim:** Trigger selection via cosine similarity between target node and trigger pool embeddings ensures stealthiness while maintaining effectiveness.
- **Mechanism:** For each target node, the trigger with highest average cosine similarity to its embedding is selected, keeping injected triggers in-distribution relative to the target node's neighborhood.
- **Core assumption:** Cosine similarity in embedding space correlates with structural/feature compatibility in the original graph.
- **Evidence anchors:** Similarity-based selection formally defined in Eq. 7; t-SNE visualization shows GPL triggers remain close to original nodes.

## Foundational Learning

- **Concept: Graph Neural Network message-passing**
  - **Why needed here:** Understanding how GNNs aggregate neighborhood information is critical to grasping why subgraph triggers can manipulate node representations.
  - **Quick check question:** Can you explain why a 5-node trigger affects the embedding of nodes beyond those 5 nodes?

- **Concept: Graph learning paradigms (GSL vs. GCL vs. GPL)**
  - **Why needed here:** The entire contribution hinges on why triggers trained under one paradigm fail under others—requires understanding their optimization objectives.
  - **Quick check question:** What is the fundamental difference between how GSL and GPL optimize their objectives?

- **Concept: Backdoor attack threat model (gray-box)**
  - **Why needed here:** The method assumes access to training data but not target model architecture/parameters—shapes what information the trigger can exploit.
  - **Quick check question:** Why does gray-box access constrain trigger design differently from white-box access?

## Architecture Onboarding

- **Component map:** Clean GCN encoder (pre-trained, frozen) → Trigger constructor (K-means clustering) → Prompt module (learnable prompts) → Surrogate classifier (2-layer MLP) → Trigger repository (K condensed subgraphs) → Attack executor (similarity-based selection + injection)

- **Critical path:** 1. Train clean encoder on full graph → freeze 2. Extract target-class subgraphs → cluster → initialize trigger repository 3. Bi-level optimization: inner loop (prompts + classifier), outer loop (triggers) 4. At inference: compute target node embedding → select best trigger → inject

- **Design tradeoffs:**
  - Trigger size (n nodes): Larger triggers increase ASR but may create anomalous structures (plateau at 5 nodes)
  - Repository size (K triggers): More triggers improve coverage but increase storage; experiments show K=20 sufficient
  - Prompt format: Subgraph prompts outperform token prompts for structural triggers

- **Failure signatures:**
  - Low ASR with high clean accuracy → triggers may be out-of-distribution; check stealthiness loss weight λ
  - High ASR with low clean accuracy → model collapsed to target class; reduce poison budget Δp
  - Defense methods reducing ASR → trigger patterns detected; increase structural diversity

- **First 3 experiments:**
  1. **Sanity check:** Train triggers on GPL surrogate, test on GCN (GSL) on Cora—should see ASR >90% with CA within 2% of clean baseline
  2. **Ablation:** Remove K-means initialization (random trigger selection) on Pubmed—expect ASR drop of 5-15%
  3. **Defense robustness:** Apply Prune defense on Facebook dataset—ASR should remain >90% if stealthiness loss is working

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can CP-GBA be extended to simultaneous node and graph classification tasks (multi-task settings) without semantic conflicts in the trigger?
- **Basis in paper:** Section VI explicitly lists "Cross Task Attack" as a future direction toward a "more universal attack."
- **Why unresolved:** Current method optimizes triggers specifically for node classification, but multi-task learning introduces conflicting gradients and semantic spaces.
- **What evidence would resolve it:** Successful attack success rates on multi-task benchmarks where a single trigger simultaneously manipulates node class and graph class predictions.

### Open Question 2
- **Question:** How can CP-GBA be adapted to operate in zero-shot or extremely limited data access scenarios where the attacker cannot query the training graph?
- **Basis in paper:** Section VI highlights "Limited Data Access" as a critical missing capability for real-world threats.
- **Why unresolved:** Current upstream optimization and K-means selection both require access to node features and structural information from the target graph.
- **What evidence would resolve it:** Demonstration of effective trigger generation using only a surrogate dataset distinct from the target graph, maintaining >80% ASR without target data access.

### Open Question 3
- **Question:** Does the theoretical guarantee of surjective mapping (Theorem 1) fail in deep GNNs due to over-smoothing, and how does this limit trigger transferability?
- **Basis in paper:** Theorem 1 assumes injectivity while Section VI-A notes that "large triggers" are inefficient due to limited propagation.
- **Why unresolved:** Over-smoothing in deep GNNs causes node representations to converge, potentially violating the injectivity assumption required for the "bridge graph" to exist effectively.
- **What evidence would resolve it:** Analysis of ASR degradation relative to GNN depth, comparing representation similarity matrices of trigger nodes in deep vs. shallow architectures.

## Limitations
- Dependency on high-quality pre-trained encoders—poor pre-training breaks surjective mapping assumption
- Assumes target class has sufficient structural diversity for K-means clustering to work effectively
- Bi-level optimization introduces hyperparameter sensitivity, particularly the balance between transferability and stealthiness loss
- Claim of "40.4% speedup" lacks clear methodological detail for independent verification

## Confidence
- **High confidence:** Core mechanism of using GPL for cross-paradigm trigger optimization is sound and well-supported by Theorems 1-2 and ablation studies
- **Medium confidence:** Claim of superior stealthiness across defenses relies on comparisons with limited set of defenses; generalizability uncertain
- **Low confidence:** Assertion that CP-GBA works "universally" across any GNN architecture is overstated—experiments focus on structurally similar architectures

## Next Checks
1. **Encoder Robustness Test:** Reproduce experiment using poorly pre-trained encoder (10% of GRACE epochs) to quantify how pre-training quality affects cross-paradigm transferability
2. **Class Homogeneity Stress Test:** Apply CP-GBA to dataset with highly homogeneous class and measure whether ASR degrades compared to structurally diverse classes
3. **Defense Generalization Benchmark:** Evaluate CP-GBA against two additional defenses not mentioned in paper (e.g., GNNGuard, GNNScope) on Facebook dataset to test broad defense evasion claims