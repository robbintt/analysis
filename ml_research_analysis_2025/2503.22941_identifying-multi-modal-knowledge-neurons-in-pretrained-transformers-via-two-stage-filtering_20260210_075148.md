---
ver: rpa2
title: Identifying Multi-modal Knowledge Neurons in Pretrained Transformers via Two-stage
  Filtering
arxiv_id: '2503.22941'
source_url: https://arxiv.org/abs/2503.22941
tags:
- image
- knowledge
- neurons
- activation
- shows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to identify knowledge neurons in multimodal
  large language models (MLLMs) by leveraging activation differences and GradCAM filtering.
  The approach focuses on neurons in the feed-forward network (FFN) layers of MiniGPT-4,
  hypothesizing that neurons with significant activation differences when specific
  objects are removed from images are likely associated with the knowledge of those
  objects.
---

# Identifying Multi-modal Knowledge Neurons in Pretrained Transformers via Two-stage Filtering

## Quick Facts
- arXiv ID: 2503.22941
- Source URL: https://arxiv.org/abs/2503.22941
- Reference count: 30
- Primary result: Method identifies knowledge neurons in MLLMs achieving 0.77 suppression effectiveness and 0.73 knowledge retention

## Executive Summary
This paper introduces a two-stage filtering approach to identify knowledge neurons in multimodal large language models (MLLMs) that encode specific visual concepts. The method leverages activation differences from inpainting and GradCAM-based filtering to isolate neurons in feed-forward network (FFN) layers that causally represent target knowledge. Applied to MiniGPT-4 on MS COCO 2017, the approach successfully identifies neurons that can suppress target knowledge while preserving other information when noise is applied, outperforming existing methods on multiple evaluation metrics.

## Method Summary
The method operates in two stages: First, it computes activation differences between original and inpainted images at FFN activation outputs, filtering neurons with large differences as candidates. Second, it applies GradCAM to eliminate neurons activated by spurious correlations unrelated to the target knowledge. The approach requires successful caption generation for both original and inpainted images, with the target concept appearing in the original caption but not the inpainted version. Identified neurons are validated through noise perturbation experiments measuring suppression effectiveness and knowledge retention rates.

## Key Results
- Achieved suppression effectiveness (Sse) of 0.77 and knowledge retention rate (Sre) of 0.73
- Outperformed existing methods on BLEU, ROUGE, and BERTScore metrics for knowledge suppression and retention
- Demonstrated effective identification of knowledge neurons across 9 target objects (bed, bear, banana, giraffe, pizza, bus, etc.) in MS COCO 2017
- Showed that identified neurons effectively suppress target knowledge while retaining other knowledge when noise is applied

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neurons with large activation differences between original and inpainted images are strongly associated with the removed knowledge.
- **Mechanism:** When an object representing knowledge k is removed via inpainting, neurons encoding k exhibit reduced activation at image token positions. Computing O′_o - O′_i and thresholding isolates candidate neurons with causal sensitivity to k.
- **Core assumption:** Inpainting selectively removes only the target knowledge while preserving contextual features; model must correctly caption the original image and fail to mention k after inpainting.
- **Evidence anchors:**
  - [abstract] "extract knowledge neurons through two stages: activation differences filtering using inpainting"
  - [section II-C0a] "Since the inpainting operation is restricted to the region containing the object associated with knowledge k... neurons with larger activation differences are considered more strongly associated with knowledge k."
  - [corpus] Weak direct support; corpus focuses on evaluation benchmarks and unlearning, not activation-difference mechanisms.
- **Break condition:** If inpainting removes non-target knowledge or introduces artifacts, activation differences reflect confounding factors rather than k-specific relevance.

### Mechanism 2
- **Claim:** GradCAM filtering removes neurons activated by spurious correlations unrelated to target knowledge.
- **Mechanism:** For each candidate neuron from Stage 1, GradCAM computes gc = O′_o × (∂y_c/∂O′_o), where y_c is the logit for token c associated with k. Neurons with low gradient contribution to predicting c are discarded, yielding final set Nk.
- **Core assumption:** Gradients with respect to the target token's logit accurately reflect causal influence on knowledge expression; the generated caption correctly contains token c.
- **Evidence anchors:**
  - [abstract] "gradient-based filtering using GradCAM"
  - [section II-C0b] "This step aims to eliminate neurons activated by factors unrelated to knowledge k."
  - [corpus] Weak support; corpus does not validate GradCAM-based neuron selection in MLLMs.
- **Break condition:** If the model's gradient path is noisy or token c appears in captions for unrelated reasons (e.g., linguistic priors), GradCAM may retain irrelevant neurons.

### Mechanism 3
- **Claim:** Feed-forward network (FFN) layers in Transformers store factual knowledge as key-value pairs, making them the appropriate target for neuron-level analysis.
- **Mechanism:** Prior work (Geva et al., 2020) showed FFN layers function as key-value memories: W_in rows act as keys, W_out columns as values. When a key matches input, the corresponding value is retrieved and mixed into representations via residual connections.
- **Core assumption:** This key-value interpretation of FFNs extends to multimodal settings where visual features are projected into the LLM's embedding space.
- **Evidence anchors:**
  - [section II-B] "Geva et al. [15] revealed that the FFN layer also plays an important role in storing knowledge and memory... each row of the first weight matrix, W^l_in, acts as a key, and the columns of W^l_out serve as values."
  - [section II-B] "factual knowledge is primarily stored in the FFN layers of early model layers" (citing Meng et al.)
  - [corpus] "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers" provides cross-domain support for FFN-based information encoding.
- **Break condition:** If multimodal knowledge is distributed across attention layers or cross-modal projections rather than FFNs, neuron analysis restricted to FFNs will miss relevant representations.

## Foundational Learning

- **Concept: Vision-Language Projectors in MLLMs**
  - **Why needed here:** The paper restricts analysis to MLP-based projectors (like MiniGPT-4's) because activation heatmaps require clear correspondence between image tokens and image patches. Q-Former architectures break this mapping.
  - **Quick check question:** Can you explain why attention-based projectors (Q-Former, Perceiver Resampler) prevent direct activation-to-image-patch mapping?

- **Concept: Activation Functions as Neuron Outputs**
  - **Why needed here:** The paper defines "activation value" O as the output of the FFN's activation function σ, not the post-projection output. This determines what is thresholded and visualized.
  - **Quick check question:** Given O^l = σ(W^l_in(a^l + h^{l-1})), what dimensions does O have for a model with L layers, T tokens, and FFN intermediate dimension d_f?

- **Concept: Inpainting for Controlled Feature Removal**
  - **Why needed here:** The method's validity hinges on inpainting removing only the target object. Understanding inpainting limitations (edge artifacts, background filling) is critical for interpreting results.
  - **Quick check question:** What three dataset constraints does the paper impose to ensure inpainting produces valid activation differences?

## Architecture Onboarding

- **Component map:** Image + text prompt → ViT encoder → features F ∈ R^(P×d_h) → Linear projection → F′ ∈ R^(P×h) → LLM (Llama-2-7B) → Stacked Attention + FFN layers → Output caption

- **Critical path:**
  1. Verify caption generation works (original contains target token, inpainted does not)
  2. Extract activations for both images at FFN activation function output
  3. Compute differences, apply per-knowledge threshold (see Appendix Table IV)
  4. For each candidate neuron, compute GradCAM w.r.t. target token logit
  5. Validate with noise perturbation experiments (Table II metrics)

- **Design tradeoffs:**
  - **Data-driven vs. model-driven:** This approach requires specific input pairs (original/inpainted), unlike gradient-only methods that work on single inputs. Benefit: captures causal sensitivity. Cost: requires successful inpainting and captioning.
  - **Threshold selection:** Currently manual per knowledge instance (Table IV). Automatic thresholding would improve scalability but risks over/under-selection.
  - **Architecture constraint:** Only works with MLP-based projectors; attention-based architectures (BLIP-2, Flamingo) lack direct image-token-to-patch correspondence.

- **Failure signatures:**
  - **Low suppression effectiveness (S_se):** Identified neurons are not causally linked to target knowledge. Check: does original caption even contain target token?
  - **Low retention rate (S_re):** Neurons are too broadly active. GradCAM threshold may be too permissive.
  - **Ungrammatical noise-perturbed captions:** Excessive noise scaling or selection of neurons involved in syntactic processing. See Table I examples (Schwettmann's "RRRRRR..." output).

- **First 3 experiments:**
  1. **Reproduce layer distribution (Figure 2):** For a single knowledge (e.g., "bear"), identify neurons using both stages and histogram their layer positions. Expect middle-to-late layer concentration.
  2. **Noise perturbation validation (Table II):** Apply Gaussian noise (n=80) to identified neurons and compute S_se and S_re. Target: S_se > 0.7, S_re > 0.65.
  3. **Ablate one filtering stage:** Remove GradCAM filtering and compare suppression/retention metrics to quantify its contribution to precision.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the thresholds for activation differences and GradCAM be determined automatically rather than manually?
- **Basis in paper:** [explicit] Section V states that threshold values were "determined manually" and suggests that "future improvements should explore methods for automatic threshold determination."
- **Why unresolved:** Manual threshold tuning limits the scalability of the method across diverse datasets and knowledge types, introducing subjectivity and resource overhead.
- **What evidence would resolve it:** An algorithm or heuristic capable of dynamically setting optimal thresholds ($threshold_a$ and $threshold_g$) while maintaining the reported suppression effectiveness and knowledge retention rates.

### Open Question 2
- **Question:** Does the identification method generalize to architectures with non-MLP vision-language projectors, such as Q-Former or Perceiver Resamplers?
- **Basis in paper:** [explicit] Section V notes that the method is currently constrained to MLP-based projectors because scaling activation values to image size depends on a clear correspondence between input tokens and image patches, which is absent in Q-Former architectures like BLIP-2.
- **Why unresolved:** The spatial dependency required for heatmap generation and the current filtering logic break down when image tokens are aggregated or fused before entering the LLM.
- **What evidence would resolve it:** Successful identification and visualization of knowledge neurons in models like BLIP-2 or Flamingo, potentially requiring an adaptation of the gradient or activation mapping strategy.

### Open Question 3
- **Question:** Can the identified knowledge neurons be effectively utilized for targeted knowledge editing rather than just suppression?
- **Basis in paper:** [inferred] The abstract mentions the "potential for future knowledge editing and control," and the introduction highlights the ability to "modify or remove specific knowledge." However, experiments are limited to noise perturbation for suppression, not factual modification.
- **Why unresolved:** Demonstrating that a neuron corresponds to knowledge requires showing that manipulating it can change the output to a specific target (editing), not just erase it (suppression).
- **What evidence would resolve it:** Experiments where modifying the weights or activations of identified neurons results in a predictable change in the generated caption (e.g., changing "bear" to "dog") without degrading grammatical fluency.

## Limitations

- Method requires MLP-based projectors with direct image token-to-patch correspondence, excluding Q-Former architectures like BLIP-2 and Flamingo
- Manual threshold selection per knowledge instance lacks systematic justification and limits scalability
- Experiments use only 90 samples from MS COCO, limiting generalizability across diverse knowledge types
- Claims about FFN layers storing multimodal knowledge rely on single prior work without cross-modal validation

## Confidence

**High confidence:** The two-stage filtering mechanism (activation differences followed by GradCAM) is clearly specified and mathematically well-defined. The method's logic for identifying causally relevant neurons is internally consistent.

**Medium confidence:** Claims about FFN layers storing multimodal knowledge are supported by single prior work (Geva et al.) and an unrelated speech processing paper. Cross-modal validation is absent.

**Medium confidence:** Quantitative results (S_se=0.77, S_re=0.73) appear robust within the experimental setup, but the small sample size (90 instances) and lack of statistical significance testing reduce confidence in generalizability.

**Low confidence:** Claims about GradCAM effectively removing spurious correlations lack direct validation - the paper doesn't test what happens when GradCAM filtering is removed entirely or compare against alternative filtering methods.

## Next Checks

1. **Architecture generalization test:** Apply the method to a Q-Former based model (e.g., BLIP-2) and document whether the direct image token-to-patch correspondence assumption breaks down. This would validate the MLP-architectures-only constraint.

2. **Threshold sensitivity analysis:** Systematically vary threshold_a and threshold_g values across their reported ranges (0.8-2.5 and 0.0001-0.002 respectively) and measure how suppression effectiveness and retention rates change. This would quantify the method's robustness to parameter selection.

3. **GradCAM ablation study:** Run the complete pipeline with and without GradCAM filtering, then compare suppression effectiveness and retention rates. This would directly measure GradCAM's contribution to filtering spurious correlations and validate Mechanism 2's claimed importance.