---
ver: rpa2
title: Benchmarking In-context Experiential Learning Through Repeated Product Recommendations
arxiv_id: '2511.22130'
source_url: https://arxiv.org/abs/2511.22130
tags:
- product
- products
- agent
- persona
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a benchmark to evaluate in-context experiential
  learning in recommendation tasks, where agents must adapt their strategies over
  multiple interactions with customers. The BELA dataset combines real Amazon products,
  diverse user personas, and an LLM simulator to create multi-turn, interactive recommendation
  environments.
---

# Benchmarking In-context Experiential Learning Through Repeated Product Recommendations

## Quick Facts
- arXiv ID: 2511.22130
- Source URL: https://arxiv.org/abs/2511.22130
- Authors: Gilbert Yang; Yaqin Chen; Thomson Yen; Hongseok Namkoong
- Reference count: 40
- Primary result: State-of-the-art models fail to improve product recommendations across episodes when interacting with the same persona, revealing fundamental limitations in in-context experiential learning.

## Executive Summary
This paper introduces BELA, a benchmark designed to evaluate in-context experiential learning in recommendation tasks. The benchmark requires agents to interact with customers over multiple episodes, asking questions to uncover preferences and recommending products based on free-form feedback. Experiments with state-of-the-art models like GPT-4o and Claude show these models struggle to improve performance across episodes and fail to calibrate uncertainty from past experiences. The paper reveals that current agentic systems cannot effectively leverage accrued experiences to reason through uncertainty in multi-turn recommendation scenarios.

## Method Summary
BELA combines 71K real Amazon products with 1M diverse user personas, using GPT-4o and Gemini-1.5-Pro to score product-persona compatibility (0-100). An LLM simulator (GPT-4o) interacts with agents in character, providing natural language feedback after each episode. Agents must ask questions to uncover latent preferences before recommending products, with performance measured by regret (difference between recommended product score and best available score). The benchmark tests whether agents can learn from experience across multiple episodes with the same persona, using three feedback types: regret, star ratings, or free-form text.

## Key Results
- All tested models (GPT-4o, Claude-Opus-4, Gemini-2.5-Pro) significantly underperform oracle performance and fail to improve across episodes
- Models show declining question counts over episodes despite stagnant regret, indicating premature convergence
- No statistically significant performance difference across feedback types (regret, stars, or free-form text)
- Manual questioning demonstrates better trajectories are achievable, highlighting current models' inability to leverage experience effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real products paired with diverse personas expose models' inability to transfer insights across episodes
- Mechanism: BELA combines 71K Amazon products with 1M persona descriptions, scoring each product-persona pair (0-100) via GPT-4o/Gemini-1.5-Pro. The LLM-powered simulator answers questions in character and gives free-form feedback, forcing the agent to infer latent preferences from natural language—not explicit reward signals
- Core assumption: Persona descriptions and LLM-based scoring produce coherent, learnable preference structures that agents should, in principle, be able to infer over multiple episodes
- Evidence anchors: [abstract]: combines rich real-world products from Amazon, diverse user personas, and an LLM user simulator; [section 4.1–4.2]: 71K products, 2K choice sets; preference scoring by GPT-4o/Gemini-1.5-Pro with average score variation 4.1 vs. within-category SD 25.4
- Break condition: If persona scoring or simulator responses become inconsistent (low alignment with persona), learning signals may be too noisy to differentiate agent capability from benchmark noise

### Mechanism 2
- Claim: Free-form natural language feedback tests whether agents can perform implicit belief updates without scalar rewards
- Mechanism: At episode end, the simulator provides regret, star ratings, or free-form text feedback. Agents must interpret language-only signals to update beliefs about latent preferences—mirroring real-world recommendation settings where exact utilities are unavailable
- Core assumption: Agents can extract meaningful learning signals from natural language feedback and carry those insights forward to future episodes
- Evidence anchors: [abstract]: rewards primarily encoded in free-form natural language responses; models struggle to calibrate uncertainty from past experiences; [section 4.2, 5.1]: three feedback types tested; no statistically significant performance difference across types; models do not improve across episodes
- Break condition: If models successfully use explicit regret or star feedback but still fail with text feedback, the bottleneck is language interpretation, not cross-episode learning itself

### Mechanism 3
- Claim: Multi-episode settings reveal whether agents strategically explore versus prematurely converge
- Mechanism: Agents interact with the same persona across episodes with different choice sets (personalization) or with different personas for the same category (choice-set learning). Performance is measured via regret; question counts track exploration vs. exploitation
- Core assumption: Agents have sufficient context window and reasoning capacity to maintain and leverage multi-episode history
- Evidence anchors: [abstract]: models fail to meaningfully improve across episodes; manual questioning shows better trajectories are achievable; [section 5.1, Figure 5]: all models significantly underperform the oracle; number of questions declines over episodes despite stagnant regret
- Break condition: If models with longer context windows or explicit memory modules improve across episodes, failure may be due to context management, not core reasoning

## Foundational Learning

- Concept: POMDP (Partially Observable Markov Decision Process)
  - Why needed here: The agent must maintain a belief over latent user preferences and select actions (questions) to reduce uncertainty before a final recommendation
  - Quick check question: Can you explain why the optimal policy in a POMDP depends on the belief state rather than the current observation alone?

- Concept: Expected Value of Information (EVI)
  - Why needed here: Strategic questioning should maximize information gain about the latent preference; EVI quantifies the benefit of each possible question
  - Quick check question: How would you compute the expected value of asking about "budget range" versus "style preference" in a product recommendation setting?

- Concept: Calibration and Uncertainty Quantification
  - Why needed here: The paper tests whether models can accurately estimate their confidence; poor calibration indicates inability to reason about what they don't know
  - Quick check question: If a model reports 90% confidence that a product is top-1 but achieves only 30% accuracy, is it overconfident, underconfident, or well-calibrated?

## Architecture Onboarding

- Component map: Amazon Product DB -> Category Hierarchy -> Choice Set Generator -> Preference Scorer (GPT-4o/Gemini) -> Persona Engine -> User Simulator (GPT-4o) -> Recommender Agent -> Evaluation Loop
- Critical path: 1. Sample persona and choice set; ensure max score >60 (viability filter); 2. Agent asks questions; simulator responds in character; 3. Agent makes recommendation; simulator provides feedback; 4. Log regret, questions, and confidence; repeat for N episodes
- Design tradeoffs: Realism vs. scalability (real Amazon products increase ecological validity but limit coverage; simulated personas scale but may not match real user distributions); Feedback richness (free-form text is realistic but harder to interpret; regret/stars are cleaner but less representative of real interactions); Context vs. overload (full episode histories provide information but may exceed context windows or overwhelm reasoning)
- Failure signatures: Declining question count with flat or increasing regret (premature convergence); Repetitive or irrelevant questions despite available history (no effective belief update); High confidence with high regret (poor calibration)
- First 3 experiments: 1. Baseline evaluation: Run GPT-4o, Claude-Opus-4, Gemini-2.5-Pro over 10 episodes with free-form text feedback; log regret and questions per episode; 2. Feedback ablation: Compare regret/stars/text feedback for a fixed model; test whether richer feedback improves cross-episode learning; 3. Planning prompt test: Use POMDP-style planning prompt vs. baseline prompt; evaluate whether explicit EVI reasoning reduces regret or question count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do state-of-the-art models reduce question-asking behavior over episodes despite failing to minimize regret, and how can this be corrected?
- Basis in paper: [explicit] Section 5.1 ("Do Models Learn From Experiences?") observes that models ask fewer questions in later episodes even when performance stagnates, a behavior the authors highlight as a "major deficiency"
- Why unresolved: The paper identifies the failure to leverage experience for exploration but does not isolate whether this stems from context window limitations, implicit bias toward brevity, or a lack of intrinsic uncertainty-driven motivation
- What evidence would resolve it: An ablation study showing that explicit reward signals or prompting interventions for exploration can reverse the trend of declining questions and subsequently lower regret

### Open Question 2
- Question: Can models be trained to perform "posterior updates" to calibrate uncertainty based on natural language feedback rather than scalar rewards?
- Basis in paper: [explicit] Section 5.2 ("Can Models Comprehend Their Uncertainties?") notes that models "struggle to estimate uncertainties" and fail to calibrate confidence based on past interactions
- Why unresolved: The authors note that current training paradigms focus on knowledge distillation rather than reasoning through ambiguity; the mechanism for converting text-based feedback into calibrated probabilistic beliefs remains undefined
- What evidence would resolve it: Demonstrating a model that improves its Expected Calibration Error (ECE) on BELA specifically after fine-tuning on trajectories containing free-form text feedback

### Open Question 3
- Question: To what extent does the use of LLM-based user simulators limit the generalizability of experiential learning benchmarks?
- Basis in paper: [inferred] While Section 4.2 validates simulator consistency, the authors rely on LLMs (GPT-4o/Gemini) to generate preferences and simulate responses. This methodology assumes the simulator accurately mirrors the complexity of human latent factors
- Why unresolved: If the simulator's responses are too predictable or linguistically distinct from human dialogue, models might learn to exploit simulator artifacts rather than general experiential reasoning
- What evidence would resolve it: A comparative evaluation showing that agents trained on BELA's LLM simulator perform equally well when evaluated against human annotators acting as customers

## Limitations

- The benchmark relies on LLM-simulated personas and preferences, which may not fully capture real user behavior or preferences
- The paper assumes the simulator provides consistent and meaningful learning signals, but this may not hold if responses become noisy or inconsistent
- The focus on product recommendation may limit generalizability to other domains requiring experiential learning

## Confidence

- **High confidence**: The benchmark design is sound, the experimental methodology is rigorous, and the observation that models fail to improve across episodes is robust
- **Medium confidence**: The interpretation that this failure stems from inability to reason through uncertainty and adapt from interactions, rather than implementation details or context limitations
- **Medium confidence**: The conclusion that current LLMs cannot perform effective in-context experiential learning, given the controlled nature of the experiments

## Next Checks

1. **Real User Validation**: Run the same experimental protocol with actual human participants rather than LLM simulators to verify whether the observed limitations persist in real-world interactions
2. **Extended Context Testing**: Evaluate whether models with longer context windows (or explicit memory modules) show improved cross-episode learning, to isolate whether the failure is due to reasoning capacity versus context management
3. **Alternative Feedback Formats**: Test whether providing structured preference representations (rather than free-form text) enables better learning, to determine whether the bottleneck is language interpretation or experiential learning itself