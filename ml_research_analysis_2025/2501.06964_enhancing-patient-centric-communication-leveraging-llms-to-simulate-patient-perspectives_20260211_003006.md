---
ver: rpa2
title: 'Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient
  Perspectives'
arxiv_id: '2501.06964'
source_url: https://arxiv.org/abs/2501.06964
tags:
- llms
- discharge
- language
- human
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  simulate diverse patient personas to improve comprehension of discharge summaries.
  Using role-play prompting, GPT-4 was guided to adopt perspectives of individuals
  with different educational backgrounds, genders, and healthcare visit frequencies.
---

# Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives

## Quick Facts
- arXiv ID: 2501.06964
- Source URL: https://arxiv.org/abs/2501.06964
- Reference count: 40
- Key outcome: LLMs simulated patient personas with 54.97% alignment to human responses for discharge summary comprehension questions

## Executive Summary
This paper investigates whether large language models can simulate diverse patient perspectives to improve healthcare communication. Using GPT-4 with role-play prompting, the researchers guided the model to adopt perspectives of individuals with different educational backgrounds, genders, and healthcare visit frequencies. The LLM's responses were compared against real human subjects answering questions about understanding discharge instructions. While LLMs showed promise in simulating patient comprehension (54.97% accuracy vs. 26.7% random guessing), they systematically overestimated understanding, particularly for longer summaries and struggled to express uncertainty like humans do.

## Method Summary
The researchers used GPT-4 (specifically gpt-4-0613 via Azure OpenAI API) with in-context impersonation prompts to simulate eight different patient personas based on education level, gender, and healthcare visit frequency. The model responded to 10 questions per discharge summary (4 total summaries, 8 information-based and 2 perception-based questions each) using zero-shot reasoning with role-play prompting ("If you were a {persona}..."). Responses were constrained to single letters (A-E) and compared against human subject responses (N=96) to calculate alignment rates across different question types and persona attributes.

## Key Results
- LLMs achieved 54.97% alignment with human responses overall, significantly above random guessing (26.7%)
- Performance was strongest for information-based questions (58.38% alignment) and higher-education personas (85.4% alignment)
- LLMs struggled with perception-based questions (51.56% alignment) and personas with lower education or high ER visit frequency
- The model systematically overestimated comprehension for longer discharge summaries and never selected "I don't know" despite human respondents doing so

## Why This Works (Mechanism)

### Mechanism 1: In-Context Impersonation via Persona Priming
Providing role-specific prompts like "If you were a {persona}" modulates LLM outputs to approximate different human subpopulation responses by injecting contextual cues that shift the conditional token distribution. The core assumption is that training data contains sufficient stratified examples of language use correlated with demographic attributes. Evidence shows 88% accuracy for educational background priming, but performance drops to 40.63% for ER visit frequency, suggesting this mechanism fails when attributes lack distinctive training patterns.

### Mechanism 2: Task-Type Divergence in Simulation Fidelity
LLMs better simulate factual information extraction than perception or uncertainty expression because information-based questions map to deterministic pattern-matching operations while perception questions require modeling subjective cognitive states. Human responses to factual questions exhibit lower variance within demographic groups than perception-based responses. The LLM achieved 58.38% alignment on information-based tasks versus 51.56% on perception-based tasks, and never selected "I don't know" despite human respondents doing so.

### Mechanism 3: Comprehension Overestimation via Length-Complexity Decoupling
LLMs systematically overestimate human comprehension of longer medical texts because they process text without human-equivalent cognitive load. LLMs have constant per-token processing capacity and no working memory limits, preventing them from simulating cumulative cognitive burden. For longer summaries DS2 and DS3, 30% of human respondents chose difficult comprehension categories while the LLM allocated no responses to these categories.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The entire persona-simulation approach relies on ICL—understanding that models can adapt behavior from prompt context without gradient updates.
  - **Quick check question:** Can you explain why ICL differs from fine-tuning, and what this implies for the stability of persona adoption across prompt variations?

- **Concept: Distributional Alignment Metrics**
  - **Why needed here:** The paper measures success via alignment rates between LLM and human response distributions; understanding what these metrics capture (and obscure) is essential for interpreting results.
  - **Quick check question:** If an LLM achieves 85% alignment but never selects "I don't know," is it truly simulating the persona or just the confident subset?

- **Concept: Stereotype Amplification in Persona Prompting**
  - **Why needed here:** The paper explicitly warns that LLMs can "misportray and flatten identity groups"—a critical failure mode for deployment.
  - **Quick check question:** What safeguards would detect whether a persona prompt is eliciting out-group stereotypes rather than in-group authentic responses?

## Architecture Onboarding

- **Component map:** [Discharge Summary Corpus] → [Persona Prompt Template] → [GPT-4 (gpt-4-0613 via Azure)] → [Alignment Evaluation Module] ← [Human Subject Survey Data (n=96)]

- **Critical path:** Prompt template design → Persona attribute injection → Response sampling → Human response collection → Distribution comparison. The prompt template ("If you were a {persona}...") is the highest-leverage component.

- **Design tradeoffs:**
  - Simple persona injection vs. multi-attribute prompting: Adding attributes beyond education can degrade performance below random chance
  - Constrained output (letter only) vs. free text: Constraining reduces noise but eliminates uncertainty expression
  - Real human comparison vs. synthetic evaluation: Human data is resource-intensive but essential for detecting misportrayal

- **Failure signatures:**
  - Concentrated response distributions (LLM selects 1-2 categories vs. human spread across 5)
  - Zero "uncertainty" responses despite human baseline >0%
  - Alignment drops when persona attribute is weakly correlated with training data patterns

- **First 3 experiments:**
  1. **Ablate persona specificity:** Test whether "You are a patient" vs. "You are a patient with a high school education who visits the ER monthly" changes alignment
  2. **Calibrate uncertainty expression:** Modify prompts to explicitly permit/encourage "I don't know" responses
  3. **Cross-validate on held-out summaries:** Reserve 2 of 4 discharge summaries for development, test on remaining 2

## Open Questions the Paper Calls Out

- **Open Question 1:** How can LLMs be optimized to simulate human uncertainty and cognitive diversity rather than producing concentrated, overconfident responses?
  - Basis: LLMs fail to express uncertainty (never selecting "I don't know") leading to misportrayal of identity groups
  - What evidence would resolve it: Demonstration of prompting strategy yielding response distributions statistically indistinguishable from human subject distributions

- **Open Question 2:** Can LLM-simulated patient personas reliably replace human subjects in reinforcement learning (RLHF) for training medical communication models?
  - Basis: Strong role-playing capabilities "could allow these models to be supplementary to real human feedback in key processes, such as RLHF"
  - What evidence would resolve it: Comparative results showing models trained via LLM-simulated feedback perform equivalently to those trained on human feedback across diverse demographic groups

- **Open Question 3:** Why does the inclusion of specific demographic variables like gender or ER visit frequency cause LLM alignment to drop below random chance?
  - Basis: Performance "significantly drops, falling below random chance levels" when other information is included beyond education
  - What evidence would resolve it: Ablation study analyzing internal model states to determine if these demographic tokens trigger spurious correlations or stereotypes

## Limitations

- Small human subject sample (n=96) with limited demographic diversity may not capture full response variability
- LLM performance depends entirely on whether training corpus contained sufficient examples of language patterns correlated with tested demographic attributes
- Complete inability to express uncertainty represents fundamental limitation in simulating authentic patient cognition

## Confidence

**High confidence** (based on direct empirical evidence):
- LLMs show measurable alignment with human responses for information-based questions (58.38% accuracy)
- Education level is the most reliably simulated demographic attribute (77.5% alignment)
- LLMs struggle with perception-based questions (51.56% alignment) and cannot express uncertainty

**Medium confidence** (based on statistical patterns but with mechanistic gaps):
- In-context impersonation works through distributional shifts in token probabilities
- Comprehension overestimation is primarily driven by length-complexity decoupling
- Performance degrades when simulating attributes weakly correlated with training data patterns

**Low confidence** (requires further validation):
- Extent to which findings generalize to other medical document types beyond ICU discharge summaries
- Whether fine-tuning on human difficulty ratings could correct comprehension overestimation bias
- Potential for stereotype amplification when simulating intersectional personas

## Next Checks

1. **Ablate persona specificity:** Systematically test whether adding demographic attributes beyond education degrades alignment below random chance

2. **Calibrate uncertainty expression:** Modify prompts to explicitly permit/encourage "I don't know" responses and measure whether this reduces systematic overestimation bias

3. **Cross-validate on held-out summaries:** Reserve 2 of 4 discharge summaries for development, test on remaining 2 to assess generalization before deployment consideration