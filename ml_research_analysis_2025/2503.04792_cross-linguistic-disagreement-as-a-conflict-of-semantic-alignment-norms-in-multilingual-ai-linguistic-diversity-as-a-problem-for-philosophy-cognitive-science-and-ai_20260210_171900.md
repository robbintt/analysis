---
ver: rpa2
title: Cross-linguistic disagreement as a conflict of semantic alignment norms in
  multilingual AI~Linguistic Diversity as a Problem for Philosophy, Cognitive Science,
  and AI~
arxiv_id: '2503.04792'
source_url: https://arxiv.org/abs/2503.04792
tags:
- linguistic
- llms
- such
- japanese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines how linguistic diversity creates cross-linguistic
  disagreements in multilingual LLMs, where concepts like "knowledge-how" elicit different
  truth conditions across languages. Through testing four leading conversational AIs
  (ChatGPT-4o, Claude 3, Copilot, Gemini) with philosophical cases in English and
  Japanese, the study finds significant inconsistencies: while Copilot aligns with
  folk judgments, other models prioritize cross-linguistic consistency, demonstrating
  conceptual crosslingual knowledge barriers.'
---

# Cross-linguistic disagreement as a conflict of semantic alignment norms in multilingual AI~Linguistic Diversity as a Problem for Philosophy, Cognitive Science, and AI~

## Quick Facts
- **arXiv ID:** 2503.04792
- **Source URL:** https://arxiv.org/abs/2503.04792
- **Reference count:** 0
- **Key outcome:** LLMs exhibit cross-linguistic disagreement on concepts like "knowledge-how," with models prioritizing either universal consistency or language-specific norms depending on developer alignment policies.

## Executive Summary
This paper investigates how linguistic diversity creates cross-linguistic disagreements in multilingual LLMs, where concepts like "knowledge-how" elicit different truth conditions across languages. Through testing four leading conversational AIs (ChatGPT-4o, Claude 3, Copilot, Gemini) with philosophical cases in English and Japanese, the study finds significant inconsistencies: while Copilot aligns with folk judgments, other models prioritize cross-linguistic consistency, demonstrating conceptual crosslingual knowledge barriers. The research reveals conflicting developer policies between CL-consistency and Folk-consistency, with no current LLM acknowledging cross-linguistic disagreement as legitimate. These findings expose a novel qualitative limitation in crosslingual transfer and raise normative questions about whether AI should prioritize universal concepts or respect linguistic diversity.

## Method Summary
The study employed a philosophical methodology using the "Method of Cases" to test multilingual LLMs. Researchers presented conversational AIs with specific scenarios (Ski Case, Karaoke Case) in both English and Japanese, asking whether characters possessed "knowledge-how" to perform certain actions. The models' binary responses (Yes/No) were analyzed for consistency across languages. Four commercial models were tested: ChatGPT-4o, Claude 3, Copilot, and Gemini. The study also examined whether models could recognize and acknowledge cross-linguistic disagreement when explicitly challenged about inconsistencies.

## Key Results
- Copilot aligns with folk judgments (English: "No," Japanese: "Yes"), while other models prioritize cross-linguistic consistency
- No tested LLM acknowledges cross-linguistic disagreement as legitimate, attempting to "correct" inconsistencies instead
- The observed disagreements reflect genuine semantic divergence rather than cultural or psychological factors
- Different alignment policies (CL-consistency vs. Folk-consistency) produce systematically different outputs despite using similar base models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-linguistic disagreement arises when accurate translation preserves lexical form but fails to preserve truth conditions due to intrinsic semantic divergence.
- **Mechanism:** A concept (e.g., "knowledge-how") in a source language (English) maps to a construction in a target language (Japanese) that has a different extension. The model translates the sentence correctly but inherits the target language's truth conditions, resulting in "faultless disagreement."
- **Core assumption:** The observed disagreement is linguistic rather than cultural or psychological.
- **Evidence anchors:** [abstract] Mentions "disagreements purely due to semantic differences about a relevant concept." [section] Section 1-1 shows the "Ski Case" where English speakers deny knowledge (19%) but Japanese speakers affirm it (68%) despite referencing the same scenario. [corpus] Paper 113895 corroborates that models encode "language-dependent memories" where beliefs are unevenly distributed across linguistic spaces.
- **Break condition:** If the disagreement were driven solely by cultural values rather than semantic norms, this mechanism would not explain the specific divergence in truth conditions for concepts like "knowledge."

### Mechanism 2
- **Claim:** Divergent responses among LLMs are likely driven by post-training alignment policies (RLHF/fine-tuning) rather than pre-training architecture.
- **Mechanism:** The base model may possess capacities for both universal and language-specific reasoning. The "alignment norm" prioritized by the developer—Cross-Linguistic (CL)-consistency vs. Folk-consistency—acts as a filter during response generation.
- **Core assumption:** Commercial conversational AIs reflect the intentional or implicit ideals of their developers.
- **Evidence anchors:** [abstract] Identifies a conflict between "alignment policies of their developers." [section] Section 2.3 shows that while both Copilot and ChatGPT-4o use GPT-4o, Copilot is Folk-consistent while ChatGPT-4o is CL-consistent. [corpus] Corpus evidence for the specific RLHF mechanism is weak; related papers focus on representation alignment rather than policy conflicts.
- **Break condition:** If different base model versions were used unknowingly, the attribution of differences to "policy" would be invalid.

### Mechanism 3
- **Claim:** Models may resolve semantic conflict by deferring to an "English-centric" internal representation during intermediate processing.
- **Mechanism:** Non-English inputs are internally converted to English-centric representations for reasoning before generating a response. This forces CL-consistency that aligns with English semantic norms, potentially overwriting language-specific nuances.
- **Core assumption:** Intermediate layers in current LLMs do not form truly language-neutral "universal" concepts but rather English-dominant ones.
- **Evidence anchors:** [section] Methodological Notes cites Zhao et al. (2024), stating "in the intermediate layers... English-centric processing takes place." [corpus] Paper 47224 supports the view that activation patterns and performance vary significantly across languages, suggesting non-uniform internal processing.
- **Break condition:** If a model processes reasoning natively in the target language's representation space without English intermediation, this bias would disappear.

## Foundational Learning

- **Concept:** **CL-Consistency vs. Folk-Consistency**
  - **Why needed here:** This is the central tension defining the paper's experimental design. One cannot interpret the LLM failures without understanding that "CL-consistency" demands universal answers, while "Folk-consistency" demands answers that match local human intuitions.
  - **Quick check question:** If an LLM answers "Yes" in English and "No" in Japanese to the same scenario, is it broken (failing CL) or culturally aligned (passing Folk)?

- **Concept:** **The Method of Cases**
  - **Why needed here:** The paper utilizes philosophical "cases" (e.g., the Ski Case, Karaoke Case) rather than large-scale benchmarks. Understanding this qualitative, scenario-based approach is necessary to replicate the evaluation.
  - **Quick check question:** Why might testing a model on abstract logic puzzles fail to reveal the specific semantic misalignments found in the "Ski Case"?

- **Concept:** **Faultless Disagreement**
  - **Why needed here:** The paper argues that cross-linguistic disagreements are not errors but situations where "opposing but equally valid judgments" exist. This frames the "failure" of LLMs as a philosophical limitation rather than a bug.
  - **Quick check question:** Can two agents contradict each other (A says P, B says not-P) without one of them being wrong?

## Architecture Onboarding

- **Component map:** Input (Cross-lingual probes) -> Processing Core (LLM intermediate layers) -> Alignment Layer (RLHF policy) -> Output (Binary judgments)

- **Critical path:** The alignment layer is the critical point of failure. Even if the base model understands the semantic nuance, the alignment policy (e.g., OpenAI's vs. Microsoft's) forces a choice between consistency and diversity.

- **Design tradeoffs:**
  - **Prioritizing CL-Consistency:** Maximizes usability for cross-lingual transfer and logical coherence, but risks imposing English-centric concepts on other languages (linguistic imperialism).
  - **Prioritizing Folk-Consistency:** Respects linguistic diversity and local norms, but creates "Self-Inconsistency" where the model contradicts itself across languages.

- **Failure signatures:**
  - **Level 3 Inconsistency:** The model denies a fact in English that it asserts in Japanese (or vice versa) without acknowledging the linguistic basis for the difference.
  - **Meta-Inconsistency:** When challenged, the model flip-flops or corrects itself in a way that contradicts its previous corrections, indicating a lack of a fixed policy on linguistic diversity.

- **First 3 experiments:**
  1. **Replicate the "Ski Case":** Ask the target model if a protagonist who read a book "knows how to ski" in English and Japanese. Check for CL-consistency (same answer) vs. Folk-consistency (Japanese=Yes, English=No).
  2. **Level 3 Stress Test:** Explicitly point out the discrepancy in answers between languages to see if the model admits "faultless disagreement" or attempts to "correct" the inconsistency to force alignment.
  3. **Base vs. Fine-tuned Comparison:** If access permits, compare the raw base model outputs against the RLHF-aligned version to isolate where the alignment norm (CL vs. Folk) is introduced.

## Open Questions the Paper Calls Out

The paper raises several normative questions about AI development priorities: Should multilingual AI systems prioritize universal concepts that ensure cross-linguistic consistency, or should they respect linguistic diversity by producing language-specific responses that match local semantic norms? The research suggests that current commercial models reflect developer preferences rather than principled approaches to handling semantic divergence, leaving open questions about whether technical improvements can overcome these philosophical challenges or whether they require fundamental changes in how we approach multilingual AI development.

## Limitations

- The study's focus on Japanese-English pairs limits claims about universal applicability across language families
- The analysis assumes that LLMs encode language-specific "memories" in ways comparable to human semantic knowledge
- The mechanism by which alignment policies select between CL-consistency and Folk-consistency remains under-specified
- The philosophical methodology may not generalize to all concept types beyond "knowledge-how"

## Confidence

- **High Confidence:** The empirical observation that leading LLMs produce inconsistent responses across languages for the same scenarios, and that different models prioritize different alignment norms (CL vs. Folk).
- **Medium Confidence:** The characterization of this as "faultless disagreement" arising from semantic divergence rather than error, pending broader cross-linguistic validation.
- **Low Confidence:** The specific mechanism by which alignment policies override base model capabilities, and whether English-centric intermediate processing is the primary driver of CL-consistency.

## Next Checks

1. Replicate the Ski and Karaoke cases across additional language pairs (e.g., Mandarin-Japanese, Spanish-French) to test whether semantic divergence patterns hold across different language families.
2. Conduct controlled ablation studies comparing base model outputs against fine-tuned versions to isolate where alignment policies introduce CL-consistency preferences.
3. Test whether models can explicitly acknowledge linguistic diversity when prompted, distinguishing between genuine semantic disagreement and model uncertainty about cross-linguistic norms.