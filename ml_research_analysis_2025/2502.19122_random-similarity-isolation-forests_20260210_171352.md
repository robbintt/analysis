---
ver: rpa2
title: Random Similarity Isolation Forests
arxiv_id: '2502.19122'
source_url: https://arxiv.org/abs/2502.19122
tags:
- data
- random
- outlier
- similarity
- isolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Random Similarity Isolation Forest (RSIF),
  a novel outlier detection algorithm designed to handle datasets with mixed data
  types, including numerical features, complex objects like graphs and time series,
  and multi-modal data. RSIF combines the concepts of isolation trees and similarity-based
  projections, enabling it to process arbitrary data types without requiring data
  transformation or fusion of multiple models.
---

# Random Similarity Isolation Forests

## Quick Facts
- arXiv ID: 2502.19122
- Source URL: https://arxiv.org/abs/2502.19122
- Reference count: 35
- Outperforms five state-of-the-art competitors on 47 benchmark datasets, achieving best average rank (2.81) and statistically significant improvements in AP and AUC

## Executive Summary
Random Similarity Isolation Forest (RSIF) introduces a novel outlier detection algorithm that handles datasets with mixed data types including numerical features, complex objects like graphs and time series, and multi-modal data. The method combines isolation trees with similarity-based projections, processing arbitrary data types without requiring data transformation or fusion of multiple models. RSIF uses distance-based projections to create dynamic features and random splits to isolate outliers, with a focus on selecting optimal reference objects for projections.

## Method Summary
RSIF builds upon Isolation Forest's tree structure and scoring mechanism but replaces axis-aligned splits with distance-based projections. For each feature, it selects reference objects using a two-step process (random → furthest → furthest), then projects all points onto a one-dimensional axis using the difference of distances to these references. Random splits are applied to these projections, with outliers isolated in fewer steps due to their relative separation from the main distribution. The algorithm tunes distance measures on a validation split and maintains the same scoring formula as Isolation Forest, where shorter average path lengths indicate higher anomaly scores.

## Key Results
- Achieves best average rank (2.81) among six competitors on 47 benchmark datasets
- Statistically significantly outperforms five state-of-the-art methods including Isolation Forest, LOF, and Similarity Forest
- Demonstrates effectiveness across mixed data types including graphs, time series, and numerical features
- Shows consistent improvements in both AP and AUC metrics across diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distance-based projections convert arbitrary data types into a common dynamic feature space, enabling isolation without data transformation.
- Mechanism: For any feature F_k with a defined distance measure δ, RSIF projects examples onto a one-dimensional axis using reference objects x_q and x_r via: P(x_i^k) = δ(x_r^k, x_i^k) - δ(x_q^k, x_i^k). This creates a scalar "dynamic feature" that preserves relative similarity relationships, allowing random splits to separate points regardless of original data modality.
- Core assumption: A meaningful distance measure δ exists for each data type, and this distance preserves the structure necessary to distinguish outliers from inliers.
- Evidence anchors:
  - [abstract] "Our method combines the notions of isolation and similarity-based projection to handle datasets with mixtures of features of arbitrary data types."
  - [section 3.2, p.5] "The essential component making our approach multi-modal is the use of distance-based projections as dynamic features."
  - [corpus] Weak direct evidence; related work on Random Forest-based outlier detection (RFOD) focuses on tabular data rather than multi-modal projections.
- Break condition: If the selected distance measure is uninformative (e.g., constant for all pairs) or inappropriate for the data type, projections collapse to uniform values, and isolation fails.

### Mechanism 2
- Claim: Two-step reference object selection (random seed → furthest → furthest) maximizes projection spread, improving outlier separability.
- Mechanism: The algorithm selects x_u^k randomly, then x_q^k = argmax δ(x_i^k, x_u^k), then x_r^k = argmax δ(x_i^k, x_q^k). This heuristic approximates finding two maximally distant points along a promising direction, ensuring projection values are well-distributed rather than clustered. Well-separated reference objects prevent many points from receiving identical projection values.
- Core assumption: Outliers reside at distribution extremes and benefit from projections that spread points across the full value range.
- Evidence anchors:
  - [section 3.3, p.6] "When the reference objects are close to each other, many examples become indiscernible because they get the same projection value."
  - [section 3.3, p.7] "We propose a two-step approach that selects a random object x_i^k, then the furthest from it x_q^k and the furthest from that x_r^k."
  - [corpus] No direct corpus evidence; this is a novel contribution of RSIF compared to supervised Similarity Forests.
- Break condition: If all objects are equidistant (e.g., identical feature values), the two-step procedure cannot find separated references, and splits become meaningless.

### Mechanism 3
- Claim: Random splits on projected features isolate outliers with shorter expected path lengths, inherited from Isolation Forest's scoring logic.
- Mechanism: After projection, RSIF applies a random threshold split uniformly sampled from [min(P), max(P)]. Outliers, being few and different, are separated from the main distribution with fewer splits on average. The anomaly score f(x) = 2^(-E(h(x))/c) inversely relates to average path depth.
- Core assumption: Outliers are susceptible to isolation—i.e., they occupy sparse regions separable by random axis-aligned cuts in projection space.
- Evidence anchors:
  - [section 3.2, p.5] "After all isolation trees are created, for an example x an anomaly score f(x) is calculated in the same way as in Isolation Forests."
  - [abstract] "Experiments performed on 47 benchmark datasets demonstrate that Random Similarity Isolation Forest outperforms five state-of-the-art competitors."
  - [corpus] Extending Decision Predicate Graphs paper confirms Isolation Forest's isolation mechanism remains a foundation for interpretability.
- Break condition: If outliers are not susceptible to isolation (e.g., clustered anomalies requiring many splits, or inliers scattered sparsely), path length no longer correlates with outlierness.

## Foundational Learning

- Concept: **Isolation Forest fundamentals**
  - Why needed here: RSIF inherits tree structure, subsampling, path-length scoring, and hyperparameters (t, ψ) directly from Isolation Forest.
  - Quick check question: Can you explain why shorter path lengths indicate higher anomaly scores in Isolation Forest?

- Concept: **Distance/similarity metrics for diverse data types**
  - Why needed here: RSIF's flexibility depends entirely on selecting appropriate distance functions per feature (Euclidean for numeric, DTW for time series, NetLSD for graphs, etc.).
  - Quick check question: Given a dataset with a graph feature and a histogram feature, which distance measures would you consider?

- Concept: **Projection-based feature construction**
  - Why needed here: Understanding how relative distances to reference points create interpretable 1D projections is essential for debugging and customizing RSIF.
  - Quick check question: If P(x_i) = δ(x_r, x_i) - δ(x_q, x_i), what does a negative projection value indicate about x_i's position relative to x_q and x_r?

## Architecture Onboarding

- Component map: Forest builder -> Tree builder -> Reference selector -> Distance registry -> Scorer
- Critical path: Distance measure selection → Reference object selection → Projection computation → Random split → Path length accumulation → Score normalization. Errors in distance computation propagate through all downstream steps.
- Design tradeoffs:
  - More trees (t): Better score stability but higher compute; paper shows saturation around t=100
  - Larger subsample (ψ): More data per tree but deeper trees; default ψ=256 balances depth and diversity
  - Reference pool ratio (m): Restricting candidate references to m<1.0 speeds distance precomputation but may miss optimal projections; paper uses m=0.5
  - Distance measure selection: Paper validates on a holdout portion; domain expertise is recommended over blind search
- Failure signatures:
  - All projections identical → Check if reference objects have zero distance (identical feature values) or if distance measure returns constants
  - Scores uniform across all points → Verify tree depth is sufficient (not prematurely hitting max_depth) and that splits are non-trivial
  - Poor performance on specific modality → Likely inappropriate distance measure; consult domain literature for alternatives
  - Training too slow → Reduce m (reference pool), precompute distance matrices, or decrease t/ψ
- First 3 experiments:
  1. Validate on pure numeric data with Euclidean distance: Confirm RSIF reproduces Isolation Forest behavior; compare scores and rankings
  2. Test single complex feature (e.g., graphs with NetLSD): Verify projections are non-degenerate and outliers receive higher scores; visualize projection distributions
  3. Mixed-type dataset with manual distance assignment: Construct a small dataset with one numeric and one categorical feature; test multiple distance combinations and observe impact on AP/AUC

## Open Questions the Paper Calls Out

- Can the distance-based projection approach used in RSIF effectively generalize statistical outlier detection methods like HBOS and ECOD?
  - Basis: The discussion section states applying projected features to statistical models like HBOS and ECOD "constitutes a promising line of future research."
  - Why unresolved: The paper only validates the projection mechanism within the Isolation Forest framework; it is unknown if statistical models can effectively interpret the resulting dynamic feature histograms.
  - What evidence would resolve it: Modified versions of HBOS or ECOD utilizing distance projections, demonstrating competitive performance on mixed-type datasets.

- Can the selection of optimal distance measures be automated in a purely unsupervised manner without relying on a labeled validation set?
  - Basis: The authors note there is "no best default set of distance measures" and experimentally relied on a "30% validation portion" to select measures, implying a lack of a fully unsupervised selection heuristic.
  - Why unresolved: Using a validation set requires labels, which contradicts the typical constraints of unsupervised outlier detection tasks.
  - What evidence would resolve it: A heuristic or meta-learning approach that selects appropriate distance measures based solely on intrinsic data properties, matching the performance of validation-based selection.

- What are the specific advantages and limitations of applying RSIF to real-world multi-modal case studies, such as predictive maintenance?
  - Basis: The authors conclude it "would be worthwhile to investigate the pros and cons of using multi-modal representations of data in a set of case studies, for example, involving predictive maintenance tasks."
  - Why unresolved: The current study relies on benchmark datasets (often modified classification sets), which may not reflect the complexity of true industrial data fusion (e.g., sensor noise, synchronization).
  - What evidence would resolve it: Analysis of RSIF performance on raw industrial datasets containing sensor readings, images, and logs, identifying specific failure modes or benefits.

## Limitations

- Distance measure selection heavily impacts performance but lacks clear guidelines for novel data types
- Computational complexity scales poorly with complex data types due to pairwise distance calculations
- Performance gains over competitors are statistically significant but modest in some cases

## Confidence

- High Confidence: Isolation Forest scoring mechanism inheritance, two-step reference object selection heuristic, projection formula derivation
- Medium Confidence: Overall AP/AUC improvements on benchmark datasets, statistical significance claims
- Low Confidence: Generalizability to entirely novel data types without validated distance measures, scalability claims for extremely large complex datasets

## Next Checks

1. Test RSIF on a dataset with deliberately uninformative distance measures (e.g., constant values) to verify the algorithm gracefully handles degenerate cases
2. Implement a systematic comparison of different distance measure selection strategies (validation vs. domain expertise) across multiple modalities
3. Profile computational performance on increasingly large graph datasets to quantify scaling behavior and identify practical limits