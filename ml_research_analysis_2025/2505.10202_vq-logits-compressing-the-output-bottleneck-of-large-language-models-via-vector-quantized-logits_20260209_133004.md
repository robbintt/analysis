---
ver: rpa2
title: 'VQ-Logits: Compressing the Output Bottleneck of Large Language Models via
  Vector Quantized Logits'
arxiv_id: '2505.10202'
source_url: https://arxiv.org/abs/2505.10202
tags:
- output
- vq-logits
- codebook
- softmax
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes VQ-Logits, a method that uses vector quantization
  to compress the output projection layer of large language models. Instead of projecting
  to the full vocabulary, the model predicts over a small codebook and then scatters
  the results to the full vocabulary space.
---

# VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits

## Quick Facts
- arXiv ID: 2505.10202
- Source URL: https://arxiv.org/abs/2505.10202
- Reference count: 18
- Achieves up to 99% reduction in output layer parameters and up to 6× speedup in logit computation

## Executive Summary
VQ-Logits introduces a method to compress the output projection layer of large language models by replacing the full vocabulary-sized output embedding matrix with a small shared codebook. Instead of predicting over the entire vocabulary, the model predicts over a small set of prototype logits and then scatters these to the full vocabulary space. This approach significantly reduces parameters and computational cost while maintaining reasonable perplexity performance, making it particularly valuable for deploying LLMs on resource-constrained hardware.

## Method Summary
VQ-Logits replaces the traditional output projection matrix W_out ∈ R^(d_model × V) with a small codebook C ∈ R^(K × d_model) where K ≪ V, and a fixed mapping M that assigns each vocabulary token to a codebook index. During inference, the model computes logits over the K codebook entries instead of the full V vocabulary, then scatters these K logits to the full V-dimensional space using M. The approach can be combined with parameter tying (input embeddings to codebook) for further compression. Codebook vectors can be initialized via k-means clustering on pre-trained output embeddings and optionally fine-tuned.

## Key Results
- Up to 99% reduction in output layer parameters compared to full softmax baselines
- Up to 6× speedup in logit computation time
- 4% increase in perplexity compared to full softmax baselines
- k-means initialization outperforms random initialization by ~0.6 perplexity points

## Why This Works (Mechanism)

### Mechanism 1
Replacing a full vocabulary-sized output projection with a small shared codebook preserves most modeling capacity while drastically reducing parameters. The output projection W_out ∈ R^(d_model × V) is replaced by a codebook C ∈ R^(K × d_model) where K ≪ V. Tokens share codebook vectors via a fixed mapping M, so the model predicts over K prototype logits instead of V individual logits. The semantic overlap among many tokens (e.g., synonyms, functional equivalents) means a single prototype can proxy for multiple vocabulary items without catastrophic information loss.

### Mechanism 2
Computing logits over a small codebook and scattering to full vocabulary yields substantial inference speedups. Logit computation changes from h · W_out with complexity O(B · S · d_model · V) to h · C^T with complexity O(B · S · d_model · K). Since K ≪ V, the matrix multiplication is substantially faster. The scatter operation (copying K logits to V positions via mapping M) is memory-bound but cheap relative to the matmul savings.

### Mechanism 3
Initializing the codebook via k-means on pre-trained output embeddings and fine-tuning yields better perplexity than random initialization. K-means clusters the original V output embeddings into K centroids, which seed the codebook C. The cluster assignments define M. Fine-tuning adapts C to the task while preserving the semantic structure from the original embedding space.

## Foundational Learning

- **Concept:** Vector Quantization (VQ)
  - **Why needed here:** VQ-Logits applies VQ to the output embedding space; understanding discrete codebooks, assignment, and reconstruction is essential.
  - **Quick check question:** Can you explain how mapping continuous vectors to a discrete codebook induces quantization error, and why that error might be acceptable in this context?

- **Concept:** Softmax and Cross-Entropy over Large Vocabularies
  - **Why needed here:** The method modifies how logits are computed before softmax; understanding the baseline bottleneck clarifies what's being optimized.
  - **Quick check question:** Why does the size of V linearly scale both the parameter count and computational cost of the standard output layer?

- **Concept:** K-means Clustering
  - **Why needed here:** Codebook and mapping initialization use k-means on embeddings; understanding convergence behavior and centroid sensitivity informs debugging.
  - **Quick check question:** If k-means is run on output embeddings with very uneven cluster sizes, what might happen to the representation of rare tokens?

## Architecture Onboarding

- **Component map:**
  - Hidden state h -> Codebook projection h · C^T -> K logits -> Scatter via M -> V logits -> Softmax -> Probabilities

- **Critical path:**
  1. Hidden state h (B × S × d_model) is reshaped to (B·S × d_model)
  2. Project to codebook logits: L_c = h · C^T → (B·S × K)
  3. Scatter to vocab logits: L_v[..., i] = L_c[..., M(i)] for all vocab indices i
  4. Apply softmax over V dim; compute cross-entropy loss

- **Design tradeoffs:**
  - **K vs. perplexity:** Smaller K gives more compression and speed but higher PPL; K=1024-4096 is a practical range for V~267k
  - **Fixed vs. learned C:** Learned C improves PPL (19.2 vs. 20.1) but requires fine-tuning the output layer
  - **Tied embeddings:** Tying input embeddings to codebook vectors reduces params further but increases PPL (20.4 vs. 19.2)
  - **Assumption:** Mapping M is fixed during fine-tuning for stability; learning M requires non-differentiable assignment

- **Failure signatures:**
  - **PPL significantly above baseline (>10% degradation):** K may be too small; increase codebook size
  - **No speedup observed:** Scatter operation may be unoptimized; verify gather implementation and memory access patterns
  - **Training instability after VQ-Logits conversion:** Check that learning rate for C is appropriate; consider freezing C initially and fine-tuning only the LLM body
  - **Semantic confusion in generation (e.g., wrong day of week):** Inspect M for problematic clusterings; consider increasing K or post-hoc reassignment for critical token groups

- **First 3 experiments:**
  1. **Baseline calibration:** Train a full-softmax model on your target dataset; record PPL, output layer params, and logit computation time
  2. **Codebook size sweep:** Convert the baseline to VQ-Logits with K ∈ {512, 1024, 2048}; fine-tune with learned C and fixed M; plot PPL vs. K and speedup vs. K
  3. **Initialization ablation:** Compare k-means initialization from pre-trained output embeddings vs. random initialization for a fixed K (e.g., 1024); measure PPL gap after equal fine-tuning steps

## Open Questions the Paper Calls Out

- Can end-to-end differentiable learning of the vocabulary-to-codebook mapping (M) improve performance over fixed k-means assignments?
- Does the efficiency-accuracy trade-off of VQ-Logits persist when scaling to models with 100B+ parameters?
- Can dynamic or hierarchical codebook structures improve the representation capacity without increasing the static codebook size K?

## Limitations
- Evaluation limited to WikiText-103 dataset with 768M parameter GPT-2 model
- Fixed mapping M strategy cannot adapt to task-specific token relationships during fine-tuning
- Semantic clustering analysis presented qualitatively rather than quantitatively
- Hardware-specific results not provided, so speedup claims may vary across deployment targets

## Confidence

- **High Confidence:** The core mechanism of replacing V×d_model output projections with K×d_model codebooks and achieving parameter reduction is mathematically sound and empirically validated. The speedup claim (up to 6×) is supported by direct measurements on WikiText-103.
- **Medium Confidence:** The k-means initialization advantage (19.2 vs 19.8 PPL) is demonstrated but only within a single experimental setup. The claim about semantic clustering being beneficial lacks quantitative validation and may not hold across all token types or domains.
- **Low Confidence:** The generalization of results to different model scales (beyond 768M parameters), domains (beyond WikiText-103), and hardware platforms (beyond the tested GPU configuration) is not established.

## Next Checks
1. **Domain Generalization Test:** Apply VQ-Logits to models trained on code (e.g., GitHub Python corpus) or multi-lingual data (e.g., mC4) and measure the PPL degradation curve vs. compression ratio.
2. **Hardware Profiling:** Measure the actual wall-clock speedup on different hardware targets (GPU, CPU, mobile NPU) and profile the scatter operation overhead.
3. **Learned Mapping Exploration:** Implement a differentiable version of the mapping M (e.g., using Gumbel-softmax or other relaxation techniques) and compare perplexity against the fixed mapping approach.