---
ver: rpa2
title: 'Forecasting NCAA Basketball Outcomes with Deep Learning: A Comparative Study
  of LSTM and Transformer Models'
arxiv_id: '2508.02725'
source_url: https://arxiv.org/abs/2508.02725
tags:
- lstm
- team
- transformer
- brier
- predictive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses predicting NCAA basketball tournament outcomes
  using deep learning models, comparing LSTM and Transformer architectures. The study
  employs advanced feature engineering including Elo ratings, GLM-based team quality
  metrics, and seed differences to capture team strengths and dynamics.
---

# Forecasting NCAA Basketball Outcomes with Deep Learning: A Comparative Study of LSTM and Transformer Models

## Quick Facts
- arXiv ID: 2508.02725
- Source URL: https://arxiv.org/abs/2508.02725
- Authors: Md Imtiaz Habib
- Reference count: 40
- Key outcome: Transformer model with BCE achieves AUC 0.8473 (highest), LSTM model with Brier loss achieves Brier score 0.1589 (lowest)

## Executive Summary
This paper addresses predicting NCAA basketball tournament outcomes using deep learning models, comparing LSTM and Transformer architectures. The study employs advanced feature engineering including Elo ratings, GLM-based team quality metrics, and seed differences to capture team strengths and dynamics. Both models are trained with two loss functions: Binary Cross-Entropy (BCE) for classification and Brier loss for probability calibration. Results show that the Transformer model with BCE achieves the highest AUC (0.8473), indicating strong discriminative power, while the LSTM model with Brier loss yields the lowest Brier score (0.1589), demonstrating superior probabilistic calibration. These findings highlight the importance of matching model architecture and loss function to the specific needs of the prediction task—whether prioritizing ranking or calibrated probability estimates.

## Method Summary
The study uses Kaggle March Machine Learning Mania 2025 competition data covering NCAA Division 1 men's and women's games from 2003-2024. For each game, two records are created (original and inverted team order) with normalized statistics adjusted for overtime. Features include seed difference, Elo ratings (K=100, baseline 1000), GLM-derived team quality, and season-averaged box scores. LSTM (32 hidden units, dropout 0.5, L2 regularization) and Transformer (2-head attention d=64, 64-unit feedforward) architectures are trained with BCE and Brier loss separately. Temporal validation uses 2024 season data, with early stopping and learning rate reduction applied.

## Key Results
- Transformer model with BCE achieves highest AUC of 0.8473, indicating superior discriminative power
- LSTM model with Brier loss achieves lowest Brier score of 0.1589, demonstrating best calibration
- GLM-based team quality and Elo rating features emerge as most critical in ablation study
- Model performance varies significantly based on loss function choice, not just architecture

## Why This Works (Mechanism)

### Mechanism 1
Brier loss acts as a regularizer for probability estimates, reducing overconfidence compared to Binary Cross-Entropy (BCE). BCE penalizes incorrect binary labels using log-loss, which pushes probabilities toward 0 or 1 aggressively to minimize loss. In contrast, Brier loss (Mean Squared Error on probabilities) applies a quadratic penalty that is less severe for "slightly wrong" predictions but heavily penalizes extreme errors. This forces the model to remain uncertain when evidence is weak, resulting in better calibration. The dataset contains sufficient noise and overlap such that pure discrimination leads to miscalibration on unseen tournament data.

### Mechanism 2
Self-attention in Transformers captures non-sequential interactions between team features more effectively than recurrent gating in LSTMs for ranking tasks. The input is a 2-vector sequence (Team 1 and Team 2). Transformers use multi-head attention to weigh the relationship between all features of Team 1 and Team 2 simultaneously. This allows the model to identify complex interactions (e.g., a high Elo difference interacting with a specific seed differential) that LSTMs must process sequentially or via fixed hidden states, providing higher discriminative power (AUC). The predictive signal lies in the complex, non-linear interactions between the two teams' metrics rather than just the magnitude of differences.

### Mechanism 3
GLM-based quality metrics and Elo ratings condense historical performance into high-signal priors, reducing the learning burden on the deep learning model. Raw box-score data is high-dimensional and noisy. By pre-computing Elo (dynamic strength) and GLM quality scores (strength accounting for opposition), the feature space is reduced to dense, highly predictive signals. The deep model then learns residual interactions rather than basic strength definitions, preventing overfitting to noisy regular-season statistics. Team strength is a latent variable that can be effectively summarized by historical point differentials and win/loss quality prior to the tournament.

## Foundational Learning

- **Concept: Calibration vs. Discrimination**
  - Why needed here: The paper explicitly trades off these two metrics. Discrimination (AUC) ranks winners correctly (useful for brackets), while Calibration (Brier) ensures the probability estimates are accurate (useful for betting/risk).
  - Quick check question: If a model predicts a 90% win chance for a team that loses, did the model fail? (Answer: Not necessarily—it may have just been the 10% outcome. Calibration checks if "90%" predictions win 90% of the time over many games).

- **Concept: Elo Rating System**
  - Why needed here: It is the primary "Hard" feature driving performance. Understanding that it updates dynamically based on opponent strength is key to understanding why it outperforms raw win/loss records.
  - Quick check question: If Team A beats a weak Team B, does Team A's Elo rating increase by the same amount as beating a strong Team C? (Answer: No, Elo updates are weighted by the expected score).

- **Concept: Binary Cross-Entropy vs. Brier Loss**
  - Why needed here: The choice of loss function is shown to be as impactful as the model architecture.
  - Quick check question: Which loss function penalizes a prediction of 0.9 for a true label of 1.0 less severely than a prediction of 0.99? (Answer: Brier loss penalizes quadratic distance (0.01 vs 0.81), whereas BCE penalizes logarithmic distance).

## Architecture Onboarding

- **Component map:**
  Input (2×d tensor + features) -> Backbone (LSTM/Transformer) -> Dense -> Sigmoid (Output: Win Probability)

- **Critical path:**
  1. Imputation: Handle missing seeds/Elo (Median/1000)
  2. Symmetrization: Duplicate and invert dataset to prevent position bias
  3. Loss Selection: Choose Brier if probabilities matter, choose BCE if ranking matters

- **Design tradeoffs:**
  - Transformer: Higher AUC (better ranking) and faster parallel training, but prone to overconfidence and slightly harder to calibrate
  - LSTM: Lower parameter count and better natural calibration (with Brier loss), but lower discriminative ceiling (AUC)
  - Features: "Hard" features (Elo/GLM) are computationally expensive to engineer but yield ~50% of the predictive power

- **Failure signatures:**
  - Overconfidence: Model predicts 0.99 for favorites who lose frequently. Fix: Switch to Brier loss or apply Temperature Scaling
  - Stagnant Loss: Validation loss doesn't improve. Fix: Check for data leakage or reduce learning rate
  - Gender Bias: Model fails on Women's tournament. Fix: Check if Coach stats (Men-only) are leaking into the general feature set

- **First 3 experiments:**
  1. Sanity Check: Train a simple Logistic Regression model on "Seed Diff" only. If this baseline isn't beaten by deep models, the feature pipeline is broken.
  2. Ablation Study: Train the LSTM with only "Easy" features (Seeds) vs. "Hard" features (Elo). Confirm that the "Hard" features provide the majority of the performance lift.
  3. Loss Sensitivity: Train the Transformer with BCE vs. Brier. Verify that BCE gives higher AUC but lower Calibration (higher Brier score) to validate the paper's central trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
Can ensembling the LSTM (Brier-optimized) and Transformer (BCE-optimized) models improve generalization by combining their specific strengths in calibration and discrimination? The study evaluates architectures independently; the interaction between the LSTM's superior calibration and the Transformer's superior ranking ability remains untested. Experimental results showing that a meta-learner or averaged ensemble achieves lower variance and better overall metrics than either model alone would resolve this.

### Open Question 2
Can Graph Neural Networks (GNNs) outperform the current sequence models by explicitly encoding the relational structures of tournaments and team interactions? The current LSTM and Transformer models process matchups as isolated or sequential pairs without explicitly modeling the network of team relationships. A comparative analysis where a GNN-based model processes the team interaction graph and outperforms the Transformer's AUC of 0.8473 would resolve this.

### Open Question 3
Does incorporating dynamic, time-series representations of recent team form (e.g., "momentum" or late-season surges) improve accuracy over static, season-averaged statistics? Section 7.3.3 highlights the limitation of using "static, season-aggregated statistics," noting that the models fail to capture temporal dynamics like "recent form" or "late-season momentum." An ablation study replacing season averages with rolling-window statistics to quantify the predictive value of short-term performance trends would resolve this.

## Limitations
- Validation methodology uncertainty: Paper mentions using 2024 season data but doesn't specify exact train/val split boundaries or whether both regular season and tournament games are included
- Feature engineering dependency: Claims that GLM and Elo features provide ~50% of predictive power are based on ablation study but could vary with different engineering choices
- Limited architectural comparison: Only compares LSTM and Transformer, missing potential benefits from other architectures like Graph Neural Networks

## Confidence
- High confidence: Core architectural comparison and general observation that model choice should match the task are well-supported
- Medium confidence: Specific performance metrics depend on unknown validation setup and could vary with different temporal splits
- Medium confidence: Claim about GLM and Elo features providing ~50% of predictive power based on ablation study

## Next Checks
1. **Temporal Validation Verification**: Re-run experiments with explicit year-by-year temporal splits (e.g., 2003-2023 train, 2024 validate) to confirm the reported metrics are stable across different validation windows.

2. **Loss Function Impact Isolation**: Train the Transformer with BCE and Brier loss on identical validation sets to verify the claimed trade-off (higher AUC with BCE vs better calibration with Brier) holds consistently.

3. **Feature Ablation Replication**: Perform the same feature ablation study (removing Easy vs Hard features) to confirm that Elo and GLM metrics contribute the claimed majority of predictive performance.