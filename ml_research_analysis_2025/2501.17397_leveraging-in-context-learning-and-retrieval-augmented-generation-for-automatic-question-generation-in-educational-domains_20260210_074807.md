---
ver: rpa2
title: Leveraging In-Context Learning and Retrieval-Augmented Generation for Automatic
  Question Generation in Educational Domains
arxiv_id: '2501.17397'
source_url: https://arxiv.org/abs/2501.17397
tags:
- generation
- question
- u1d458
- questions
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of generating contextually relevant
  and pedagogically sound questions in educational domains. The authors propose a
  hybrid approach combining In-Context Learning (ICL) with Retrieval-Augmented Generation
  (RAG) to overcome limitations of existing methods.
---

# Leveraging In-Context Learning and Retrieval-Augmented Generation for Automatic Question Generation in Educational Domains

## Quick Facts
- **arXiv ID:** 2501.17397
- **Source URL:** https://arxiv.org/abs/2501.17397
- **Reference count:** 40
- **Primary result:** Hybrid ICL-RAG approach achieves highest human evaluation scores for educational question generation

## Executive Summary
This paper addresses the challenge of generating contextually relevant and pedagogically sound questions in educational domains. The authors propose a hybrid approach combining In-Context Learning (ICL) with Retrieval-Augmented Generation (RAG) to overcome limitations of existing methods. The hybrid model first retrieves relevant documents from an external corpus, then uses GPT-4 with few-shot examples to generate questions. Evaluated on the EduProbe dataset, the hybrid model achieved the highest scores in human evaluations across grammaticality (4.84), appropriateness (4.74), relevance (4.25), and complexity (4.02), outperforming both baseline models and standalone ICL and RAG approaches.

## Method Summary
The study implements three main approaches: ICL using GPT-4 with varying numbers of few-shot examples (k=3,5,7), RAG using BART-large fine-tuned on EduProbe with FAISS retrieval from an NCERT corpus, and a hybrid model combining both retrieval and few-shot learning. The hybrid approach constructs prompts that include both retrieved documents and demonstration examples for GPT-4. Baseline models include T5-large and BART-large fine-tuned on EduProbe. The EduProbe dataset contains 3,502 question-answer pairs across five school-level subjects. Evaluation uses both automated metrics (BLEU-4, ROUGE-L, METEOR, ChRF, BERTScore) and human evaluation on a 1-5 scale for five pedagogical qualities.

## Key Results
- ICL with k=7 examples achieves highest automated metric scores (BLEU-4: 22.69, ROUGE-L: 55.95, METEOR: 34.62)
- Hybrid model achieves highest human evaluation scores (grammaticality: 4.84, appropriateness: 4.74, relevance: 4.25)
- RAG shows lower answerability scores (2.90) compared to ICL (3.31) and hybrid (3.20) due to retrieval noise
- Standalone BART-large and T5-large baselines underperform both ICL and RAG approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-Context Learning with sufficient few-shot examples (k=7) produces questions that achieve higher automated metric scores than parameter-updating approaches.
- **Mechanism:** Providing the model with multiple demonstration pairs (passage, question) establishes a pattern of question formulation that the model replicates without gradient updates. The examples implicitly encode task structure, pedagogical style, and question complexity norms.
- **Core assumption:** The few-shot examples are representative of the desired output distribution across subjects.
- **Evidence anchors:**
  - [abstract] "ICL with 7 examples performs best in automated metrics (BLEU-4: 22.69, ROUGE-L: 55.95, METEOR: 34.62, BERTScore: 75.92)"
  - [Section 8] "ICL with k = 7 excels in ROUGE-L, METEOR, CHrF, and BERTScore... This advantage can be attributed to ICL's ability to effectively leverage multiple examples"
  - [corpus] Related work "Beyond In-Context Learning: Aligning Long-form Generation" notes ICL underperforms in long-form tasks but remains effective for QA/QG.

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation enriches context but introduces retrieval noise that can reduce answerability.
- **Mechanism:** FAISS retrieves top-k semantically similar documents from an educational corpus; these are concatenated with the input passage before BART-large generates the question. External context enables broader questions but may drift from the source passage.
- **Core assumption:** Retrieved documents are relevant and non-redundant.
- **Evidence anchors:**
  - [Section 5.2] "The retrieval module enriches the context, allowing the model to generate questions that are better aligned with the content"
  - [Section 8] "RAG (k = 5) shows weaker performance, particularly in answerability... reliance on external document retrieval, which may introduce content that is less directly tied to the passage"
  - [corpus] "Retrieval-Augmented Generation as Noisy In-Context Learning" provides theoretical grounding for retrieval noise effects.

### Mechanism 3
- **Claim:** The hybrid model (RAG + ICL) achieves highest human evaluation scores because retrieval provides breadth while few-shot examples constrain pedagogical quality.
- **Mechanism:** First, retrieve k=5 documents to enrich context. Then, provide m=5 few-shot examples to GPT-4, which generates using both the enriched passage and the demonstration patterns. Retrieval enables depth/breadth; ICL constrains output style and complexity.
- **Core assumption:** GPT-4 can integrate both retrieved context and few-shot patterns without interference.
- **Evidence anchors:**
  - [Section 5.3] "combines the retrieval-based context enrichment of RAG with the few-shot learning mechanism of ICL using GPT-4"
  - [Section 8] "Hybrid Model achieves the highest scores in most human evaluation metrics... integration of both retrieval and few-shot learning techniques, which allows it to create questions that are well-aligned with the passage and involve deeper reasoning"

## Foundational Learning

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - Why needed here: Understanding how LLMs generalize from demonstrations without weight updates is essential for designing effective prompts and selecting appropriate k values.
  - Quick check question: If you increase few-shot examples from 3 to 7 and performance improves on automated metrics but plateaus on human evaluation, what might this indicate about the metric-task alignment?

- **Concept: Dense Retrieval with FAISS**
  - Why needed here: The retrieval module's quality directly impacts RAG and hybrid performance; understanding embedding similarity and top-k selection is critical for debugging retrieval failures.
  - Quick check question: What happens to question quality if the retrieved documents are semantically similar to each other but collectively irrelevant to the target passage?

- **Concept: Evaluation Metric Limitations for Generation**
  - Why needed here: The paper shows ICL wins on automated metrics while hybrid wins on human evaluation—understanding this gap prevents over-reliance on BLEU/ROUGE for pedagogical quality assessment.
  - Quick check question: Why might a question with high ROUGE-L overlap still receive low "answerability" scores from human evaluators?

## Architecture Onboarding

- **Component map:**
  - Input: Educational passage P
  - ICL path: P → few-shot prompt construction (k examples) → GPT-4 API → question Q
  - RAG path: P → FAISS retrieval (top-k from NCERT corpus) → concatenate P + retrieved docs → BART-large (fine-tuned on EduProbe) → question Q
  - Hybrid path: P → FAISS retrieval → construct prompt with P + retrieved docs + m few-shot examples → GPT-4 API → question Q

- **Critical path:**
  1. Few-shot example selection (ICL/Hybrid): Must be curriculum-aligned and representative
  2. Retrieval quality (RAG/Hybrid): Index construction, embedding model, k selection
  3. Prompt engineering: Token budget management when combining retrieval + few-shot

- **Design tradeoffs:**
  - ICL: Best automated metrics, lowest latency, but depends on example quality; no external knowledge
  - RAG: External knowledge access, but retrieval noise and lower answerability
  - Hybrid: Best human-rated quality, but highest complexity and cost (GPT-4 API + retrieval)

- **Failure signatures:**
  - Questions that cannot be answered from the passage → likely retrieval noise introduced irrelevant content
  - Questions that are generic or template-like → few-shot examples may be too similar to each other
  - Off-topic questions → check retrieval relevance or prompt construction

- **First 3 experiments:**
  1. Replicate ICL k={3,5,7} on held-out EduProbe subjects not used in few-shot examples to test generalization.
  2. Ablate retrieval: replace FAISS retrieval with random document selection to quantify retrieval contribution vs. noise.
  3. Prompt length analysis: measure how question quality degrades as retrieved documents + few-shot examples approach token limits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hybrid model's advantage in human evaluation scores (particularly grammaticality, appropriateness, relevance, and complexity) persist across educational domains beyond the five subjects tested (History, Geography, Economics, Environmental Studies, Science)?
- Basis in paper: [explicit] The authors state: "In future work, we plan to expand evaluations to diverse datasets and educational domains to better assess the generalizability of our models."
- Why unresolved: The current study only evaluates on the EduProbe dataset covering five school-level subjects; it remains unclear whether the hybrid approach generalizes to STEM-heavy domains, vocational education, or higher education contexts.
- What evidence would resolve it: Running the same ICL, RAG, and hybrid experiments on datasets from additional domains (e.g., medicine, engineering, language arts) and comparing automated and human evaluation rankings.

### Open Question 2
- Question: Would alternative state-of-the-art LLMs (e.g., Gemini, Llama-2-70B) produce similar patterns where ICL excels in automated metrics while hybrid approaches excel in human evaluations?
- Basis in paper: [explicit] The authors note: "In addition, we plan to explore other state-of-the-art LLMs, such as Gemini and Llama-2-70B, for in-context learning."
- Why unresolved: Only GPT-4 was tested for ICL and the hybrid model; the observed performance patterns may be specific to GPT-4's architecture and training rather than inherent to the ICL/hybrid paradigm.
- What evidence would resolve it: Replicating the experimental setup using Gemini, Llama-2-70B, and other LLMs for both ICL and hybrid configurations, then comparing metric patterns.

### Open Question 3
- Question: Can retrieval noise (irrelevant or redundant documents) be reduced through improved retrieval mechanisms to enhance RAG's answerability scores, which currently lag behind ICL and hybrid models?
- Basis in paper: [inferred] The paper notes that "RAG's performance in answerability scores lower (2.90) compared to both ICL (3.31) and Hybrid (3.20)" and attributes this to "reliance on external document retrieval, which may introduce content that is less directly tied to the passage." The authors also acknowledge RAG "can sometimes retrieve irrelevant or redundant documents."
- Why unresolved: The FAISS-based retrieval with top-5 documents may include noise; no ablation was conducted on retrieval quality thresholds or alternative retrieval methods.
- What evidence would resolve it: Implementing retrieval quality filtering, testing different numbers of retrieved documents (k values), or using dense passage retrieval methods, then measuring answerability improvements.

## Limitations
- The study only evaluates on NCERT textbooks for grades 6-12, limiting generalizability to other educational systems or higher education
- Human evaluation shows smaller performance gaps between models compared to automated metrics, suggesting metrics may not fully capture pedagogical quality
- The hybrid model's success depends heavily on prompt engineering choices not systematically explored

## Confidence
- **High Confidence:** ICL with 7 examples outperforming baseline models on automated metrics (BLEU-4: 22.69, ROUGE-L: 55.95) - directly supported by quantitative data and controlled experiments
- **Medium Confidence:** Hybrid model achieving highest human evaluation scores (grammaticality: 4.84, appropriateness: 4.74) - supported by human ratings but evaluation methodology lacks detail on rater selection
- **Medium Confidence:** Retrieval-Augmented Generation's effectiveness despite retrieval noise - theoretically sound but limited empirical evidence distinguishing retrieval benefits from noise

## Next Checks
1. Evaluate the hybrid model on question generation tasks from educational domains outside the EduProbe dataset to assess cross-domain performance
2. Measure the computational cost (API calls, retrieval latency, fine-tuning resources) of each approach and quantify the trade-off between performance gains and resource requirements
3. Systematically vary retrieval quality by using controlled document sets to quantify how retrieval noise specifically impacts question answerability and pedagogical value