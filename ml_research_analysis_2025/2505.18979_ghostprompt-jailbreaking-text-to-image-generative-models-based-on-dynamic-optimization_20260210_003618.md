---
ver: rpa2
title: 'GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic
  Optimization'
arxiv_id: '2505.18979'
source_url: https://arxiv.org/abs/2505.18979
tags:
- safety
- prompt
- adversarial
- filters
- bypass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GhostPrompt addresses the challenge of bypassing multimodal safety
  filters in text-to-image (T2I) generation models, which are increasingly ineffective
  against semantic-level adversarial attacks. The method introduces a dynamic optimization
  framework that combines feedback-driven prompt rewriting with adaptive safety indicator
  injection, leveraging reinforcement learning to guide benign visual cue embedding.
---

# GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization

## Quick Facts
- **arXiv ID**: 2505.18979
- **Source URL**: https://arxiv.org/abs/2505.18979
- **Reference count**: 40
- **Primary result**: Achieves 99.0% text filter bypass and 75.5% image filter bypass while maintaining semantic alignment (CLIP score 0.2762)

## Executive Summary
GhostPrompt introduces a dynamic optimization framework that bypasses multimodal safety filters in text-to-image generation models through feedback-driven prompt rewriting and adaptive safety indicator injection. The system combines reinforcement learning with large language model (LLM) guidance to generate adversarial prompts that evade both text and image safety filters while preserving semantic alignment with target NSFW content. It achieves state-of-the-art performance on open-source models and demonstrates transferability to closed-source systems like DALL·E 3.

## Method Summary
GhostPrompt employs an iterative optimization loop where an LLM rewrites blocked prompts based on feedback from text safety filters and CLIP-guided semantic preservation. The framework uses in-context learning with rejection reasons and semantic scores to generate adversarial prompts. A multi-armed bandit approach optimizes the injection of benign visual cues (safety indicators like logos) to bypass image filters. The method balances exploration and exploitation to find effective visual tricks while maintaining semantic fidelity through CLIP score monitoring.

## Key Results
- 99.0% text filter bypass rate on ShieldLM-7B-internlm2
- 75.5% image filter bypass rate on InternVL2-2B
- 0.2762 CLIP score for semantic alignment
- 4.2× reduction in query cost compared to baselines
- 84.0% bypass rate on closed-source DALL·E 3

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Driven Semantic Rephrasing
The system bypasses semantic-level text filters by iteratively refining prompts based on rejection signals. An LLM generates adversarial variants, and when a text safety filter rejects a prompt, the rejection reason is fed back into the LLM's context window. This allows semantic surgery—altering the presentation of unsafe concepts while preserving underlying meaning—until the filter's detection logic fails. This works because text filters have semantic consistency with specific blind spots that LLM can discover via trial and error.

### Mechanism 2: Benign Visual Cue Injection via Bandits
Image filters are evaded by optimizing prompts to include instructions for benign visual overlays (e.g., logos). The addition of safety indicators is formulated as a Multi-Armed Bandit problem where the agent selects an indicator to append to the prompt. The image filter presumably weighs the presence of these safety markers heavily, classifying images as benign despite unsafe content. This exploits heuristic shortcuts in image classifiers that associate safety markers with benign content.

### Mechanism 3: CLIP-Guided Semantic Anchoring
Dynamic rephrasing risks semantic drift, which is mitigated by explicitly maximizing CLIP similarity between generated images and original target prompts. During optimization, the system computes CLIP scores and treats attempts as failures if scores fall below threshold, forcing LLM rewrites to restore semantic alignment. This works because CLIP embedding space provides a reliable proxy for human judgment regarding semantic equivalence of unsafe content.

## Foundational Learning

- **Multi-Armed Bandits (MAB)**: Used to optimize safety indicator injection strategy by balancing exploration (trying new logos) vs. exploitation (using proven logos). Quick check: If "Green Checkmark" works 80% of the time, how does softmax policy ensure occasional testing of "Copyright Symbol"?

- **Chain-of-Thought (CoT) & In-Context Learning (ICL)**: Text attack relies on feeding failure examples back to LLM. Understanding how LLMs process these demonstrations to adjust output logic is critical. Quick check: Why include specific rejection reason in ICL context rather than just "try again"?

- **CLIP (Contrastive Language-Image Pre-training)**: Serves as semantic referee for preservation. Need to understand its limitations—it matches text to images but may struggle with fine-grained nuances. Quick check: If adversarial prompt generates stylistically different "nude" image, would CLIP score increase or decrease?

## Architecture Onboarding

- **Component map**: Orchestrator -> LLM Rewriter -> Text Filter -> T2I Generator -> CLIP Critic -> MAB Agent -> Image Filter
- **Critical path**: Feedback loop between Text Filter output and LLM Rewriter input. If parser extracting rejection reasons fails, LLM cannot correct course and system relies on random guessing.
- **Design tradeoffs**: Efficiency vs. Fidelity (CLIP threshold too high ensures perfect images but may reduce bypass rate); Query Cost vs. Success (system is query-heavy, problematic in rate-limited APIs).
- **Failure signatures**: Oscillation (alternating between prompts blocked for different reasons); Semantic Collapse (bypasses filter but image is irrelevant); Indicator Overfit (specific logo works for one filter but flags another).
- **First 3 experiments**: 1) Text Feedback Ablation (generic vs. granular feedback); 2) Indicator Transferability (train on proxy filter, test on robust target); 3) Semantic Drift Analysis (plot CLIP score trajectory over iterations).

## Open Questions the Paper Calls Out

- How can adversarial prompt generation be adapted to effectively bypass dynamic or adaptive safety interventions in closed-source models?
- Which specific defense mechanisms, such as adversarial training or semantic drift detection, are most effective at neutralizing feedback-driven optimization attacks?
- To what extent does the dynamic optimization framework transfer to multilingual inputs or cross-domain generative tasks?

## Limitations

- Evaluation limited to single text filter (ShieldLM-7B-internlm2) and single image filter (InternVL2-2B), restricting generalizability claims
- Proprietary NSFW-200 dataset contents not publicly available, requiring synthetic alternatives
- Incomplete specification of critical implementation details like safety indicator prompt formatting and ShieldLM output parsing logic
- Claims about DALL·E 3 transferability lack independent verification due to closed-source nature

## Confidence

**High Confidence (8/10)**: Core technical framework is well-specified with clear equations and pseudocode for iterative optimization, in-context learning, and multi-armed bandit approaches.

**Medium Confidence (6/10)**: Reported performance metrics likely achievable but lack baseline comparisons and require independent verification due to proprietary evaluation dataset.

**Low Confidence (3/10)**: DALL·E 3 results cannot be verified without access to closed-source systems or detailed methodology for adapting approach to black-box APIs.

## Next Checks

1. **Baseline Comparison Implementation**: Reproduce methodology on common benchmark datasets and compare against TokenProber, Reason2Attack, and Multimodal Prompt Decoupling Attack to validate "state-of-the-art" claims.

2. **Cross-Filter Transferability Study**: Test GhostPrompt-generated prompts against multiple text and image safety filters to assess whether bypass patterns are filter-specific or demonstrate broader vulnerabilities.

3. **CLIP Score Sensitivity Analysis**: Systematically vary the CLIP threshold (e.g., 0.20, 0.26, 0.30) to document trade-off curves between semantic alignment quality and bypass success rate.