---
ver: rpa2
title: Community Detection on Model Explanation Graphs for Explainable AI
arxiv_id: '2510.27655'
source_url: https://arxiv.org/abs/2510.27655
tags:
- modules
- module
- stability
- community
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of missing meso-scale structure
  in local model explanations by introducing Modules of Influence (MoI), a framework
  that constructs a model explanation graph from per-instance attributions and applies
  community detection to find feature modules that jointly affect predictions. Core
  idea: transform SHAP/LIME attributions into a co-influence graph, detect communities
  as modules, and quantify their synergy, redundancy, bias exposure, and stability.'
---

# Community Detection on Model Explanation Graphs for Explainable AI

## Quick Facts
- arXiv ID: 2510.27655
- Source URL: https://arxiv.org/abs/2510.27655
- Reference count: 30
- The paper introduces Modules of Influence (MoI), a framework that applies community detection to model explanation graphs to uncover meso-scale feature groups affecting predictions.

## Executive Summary
This paper addresses a critical gap in local model explanations by introducing Modules of Influence (MoI), a framework that constructs a model explanation graph from per-instance attributions and applies community detection to find feature modules that jointly affect predictions. The framework transforms SHAP/LIME attributions into a co-influence graph, detects communities as modules, and quantifies their synergy, redundancy, bias exposure, and stability. Results show MoI recovers planted modules in synthetic data (ARI/NMI up to 0.78/0.71) and aligns with semantically coherent groups in real datasets, while enabling fairness improvements and dimensionality reduction without sacrificing predictive performance.

## Method Summary
The MoI framework operates by first collecting per-instance attribution scores from explainers like SHAP or LIME across a dataset. These attributions are then transformed into a model explanation graph where nodes represent features and weighted edges capture their co-occurrence in influencing predictions. Community detection algorithms (Louvain or Leiden) are applied to this graph to identify modules—groups of features that tend to influence predictions together. The framework quantifies module properties including synergy (how well features work together), redundancy (feature overlap), bias exposure (disparity in feature importance across demographic groups), and stability (consistency across subsamples). These modules can then be used for interpretability, fairness auditing, and as compressed feature representations for downstream modeling.

## Key Results
- Recovers planted modules in synthetic data with ARI/NMI scores up to 0.78/0.71
- Aligns detected modules with semantically coherent groups in real datasets
- High-BEImodule scores localize demographic disparities and reduce demographic parity gaps when used as constraints
- Module aggregation preserves predictive performance (AUROC ≈ 0.91) while reducing dimensionality from 128 to 18 features
- Stability metrics correlate with utility, suggesting robust module detection

## Why This Works (Mechanism)
MoI works by bridging the gap between local explanation granularity and global model understanding. By aggregating per-instance attributions into a co-influence graph, it captures meso-scale patterns that individual explanations miss. Community detection then identifies feature modules that jointly influence predictions, revealing functional groupings that align with domain semantics. The framework's metrics (synergy, redundancy, bias exposure) provide interpretable measures of module quality, while stability analysis ensures reliability across data samples.

## Foundational Learning
- **Attribution aggregation**: Converting per-instance SHAP/LIME scores into global feature relationships. Why needed: Individual explanations lack meso-scale context. Quick check: Verify attribution scores are normalized and complete.
- **Graph construction**: Building weighted edges based on co-occurrence of feature influences. Why needed: Captures joint feature behavior. Quick check: Ensure edge weights reflect meaningful co-influence patterns.
- **Community detection**: Applying Louvain/Leiden algorithms to identify feature modules. Why needed: Reveals functional groupings missed by individual feature analysis. Quick check: Test multiple resolution parameters for optimal modularity.
- **Bias exposure indexing**: Measuring demographic disparities in module importance. Why needed: Enables fairness-aware module selection. Quick check: Validate BEI scores against known demographic disparities.
- **Stability metrics**: Quantifying module consistency across data subsamples. Why needed: Ensures reliable module detection. Quick check: Compare stability scores across different sampling strategies.

## Architecture Onboarding

Component Map:
Input Attributions -> Graph Construction -> Community Detection -> Module Analysis -> Downstream Applications

Critical Path:
Attributions are collected from SHAP/LIME, aggregated into a co-influence graph, communities are detected, modules are analyzed for properties (synergy, redundancy, bias, stability), and results are used for interpretation or as compressed features.

Design Tradeoffs:
Louvain offers speed but may produce unstable partitions; Leiden provides more stable communities at computational cost. The choice of attribution method (SHAP vs LIME) affects graph structure and module detection quality. Graph resolution parameters control module granularity vs. interpretability.

Failure Signatures:
If modules lack semantic coherence, check attribution quality and graph construction. High instability scores indicate noisy attributions or inappropriate community detection parameters. Low synergy scores suggest modules aren't functionally meaningful.

First Experiments:
1. Run MoI on synthetic data with planted modules to verify detection accuracy
2. Compare Louvain vs Leiden stability on a small real dataset
3. Test module stability across different attribution aggregation windows

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the limitations and confidence sections.

## Limitations
- MoI performance depends heavily on attribution quality and granularity, but the impact of attribution noise remains untested
- Community detection algorithms may introduce instability in small or noisy graphs, though stability metrics are provided
- The correlation between BEImodule scores and fairness metrics is observed but not causally proven

## Confidence
- Synthetic data recovery (ARI/NMI up to 0.78/0.71): High confidence
- Semantic alignment in real datasets: Medium confidence (qualitative validation)
- Demographic parity gap reduction via BEImodule constraint: Medium confidence (correlational evidence)
- Stability-utility correlation: Medium confidence (empirical observation)

## Next Checks
1. Test MoI robustness to attribution noise by adding Gaussian noise to SHAP values and measuring module stability
2. Compare Louvain vs. Leiden detection stability on sparse vs. dense graphs to quantify algorithmic sensitivity
3. Validate causality between BEImodule scores and fairness improvements via ablation studies (e.g., random vs. constrained module selection)