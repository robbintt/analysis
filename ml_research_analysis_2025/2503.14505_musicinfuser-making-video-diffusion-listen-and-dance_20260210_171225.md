---
ver: rpa2
title: 'MusicInfuser: Making Video Diffusion Listen and Dance'
arxiv_id: '2503.14505'
source_url: https://arxiv.org/abs/2503.14505
tags:
- dance
- video
- music
- prompt
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MusicInfuser aligns pre-trained text-to-video diffusion models
  to generate high-quality, synchronized dance videos driven by music. Instead of
  training from scratch, it injects zero-initialized cross-attention modules into
  selected DiT layers, guided by a constructive influence function to preserve denoising
  capability and prior knowledge.
---

# MusicInfuser: Making Video Diffusion Listen and Dance

## Quick Facts
- arXiv ID: 2503.14505
- Source URL: https://arxiv.org/abs/2503.14505
- Authors: Susung Hong; Ira Kemelmacher-Shlizerman; Brian Curless; Steven M. Seitz
- Reference count: 40
- Primary result: Zero-initialized cross-attention modules injected into pre-trained DiT layers, guided by constructive influence function, enable high-quality music-driven dance video generation with generalization to unseen music and subjects

## Executive Summary
MusicInfuser introduces a novel approach to music-driven video generation by injecting zero-initialized cross-attention (ZICA) modules into pre-trained text-to-video diffusion transformers. Rather than training from scratch, it leverages the implicit choreographic knowledge in existing models through principled layer selection using a constructive influence function and Beta-Uniform noise scheduling for stable adaptation. The method achieves state-of-the-art performance in style alignment, beat synchronization, and choreography complexity while maintaining generalization to novel subjects including animals and unseen musical content.

## Method Summary
MusicInfuser injects zero-initialized cross-attention modules into selected DiT layers of a pre-trained text-to-video model (Mochi). It uses a constructive influence function to identify layers where audio conditioning will most effectively preserve denoising capability and prior knowledge. A Beta-Uniform noise scheduling strategy gradually transitions from low-noise to uniform sampling during training. The approach includes Wav2Vec 2.0 for audio encoding, LoRA adapters for domain adaptation, and is trained on combined AIST and in-the-wild YouTube dance data (1:1 ratio) for 4000 steps.

## Key Results
- Outperforms baselines on style alignment, beat alignment, body representation, choreography complexity, and video quality
- Achieves superior realism and music-dance synchronization in human studies
- Generalizes to unseen music, longer sequences, and novel subjects (including animals)
- Produces results on a single GPU in under a day without motion capture data

## Why This Works (Mechanism)

### Mechanism 1: Zero-Initialized Cross-Attention (ZICA) for Stable Modality Injection
Zero-initializing the output projection matrix in cross-attention modules enables stable integration of audio conditioning into pre-trained video diffusion models without disrupting prior knowledge. Cross-attention blocks are inserted into selected DiT layers, but W_O is initialized to zero, making the block act as an identity function at training start. As W_O gradually moves from zero, audio features are progressively integrated rather than injected abruptly. This preserves the implicit choreographic knowledge in pre-trained models.

### Mechanism 2: Constructive Influence Function for Principled Layer Selection
Selecting adaptation layers based on their positive influence as guidance signals preserves denoising capability better than heuristic selection strategies. Each layer's influence is measured by using it for guidance during inference with the pre-trained model. Layers more connected to structural/perceptual quality show greater guidance improvement, identifying layers where modulation can effectively influence motion without deviating from the learned denoising manifold.

### Mechanism 3: Beta-Uniform Noise Scheduling for Gradual Adaptation
Training with a noise distribution that transitions from low-noise concentration to uniform improves body representation and movement realism during adapter training. Initial Beta(1, β) distribution with β > 1 concentrates sampling on smaller noise scales (high-frequency components). As β decays toward 1, distribution flattens toward uniform. This causes the model to first learn task-specific fine components before structural dance movements, preserving pre-trained physics knowledge.

## Foundational Learning

- **Diffusion Transformer (DiT) Architecture**
  - Why needed here: MusicInfuser injects cross-attention into DiT blocks; understanding token flow and layer structure is prerequisite for modification
  - Quick check question: Can you explain how video tokens flow through a DiT block and where temporal information is processed?

- **Classifier-Free Guidance (CFG)**
  - Why needed here: The constructive influence function builds on guidance concepts; understanding CFG is necessary to interpret how layer-skip guidance works
  - Quick check question: How does CFG combine conditional and unconditional denoiser outputs, and what does the guidance scale γ control?

- **Cross-Attention for Multimodal Conditioning**
  - Why needed here: ZICA is fundamentally a cross-attention variant; understanding Q/K/V projections and attention patterns is essential
  - Quick check question: In cross-attention between audio tokens A and video tokens V, which modality provides queries and which provides keys/values?

## Architecture Onboarding

- **Component map**: Wav2Vec 2.0 -> Audio tokens -> MLP -> Downsampling -> Audio conditioning -> ZICA blocks -> DiT layers -> Video generation

- **Critical path**:
  1. Pre-compute layer adaptability using guidance-based influence (requires inference-only passes)
  2. Select top-k adaptable layers for ZICA insertion
  3. Initialize all ZICA output projections to zero
  4. Train with Beta-Uniform scheduling (β: 3 → 1), learning rate 1e-4, 4000 steps
  5. Inference with CFG scale 6.0

- **Design tradeoffs**:
  - ZICA vs. Feature Addition: Direct feature addition (ControlNet-style) underperforms ZICA (Table 1: 8.14 vs 7.92 overall average)
  - LoRA rank: Higher rank (64) benefits temporal/multimodal adaptation vs. typical image ranks (8-16)
  - In-the-wild data inclusion: Trades some style capture for generalization (Table 4: style capture drops from 7.80 to 6.80 without wild data)

- **Failure signatures**:
  - Fine details (fingers, faces) fail with fast movements (inherited from base model)
  - Silhouette confusion causes body part merging/swapping
  - Random initialization of cross-attention causes severe quality degradation

- **First 3 experiments**:
  1. Layer selection ablation: Compare influence-based selection vs. evenly-distributed/first/middle/last strategies on same validation set
  2. Zero-init validation: Train with random vs. zero-initialized W_O on small subset, measure video quality metric divergence
  3. Noise schedule sweep: Test β ∈ {1.5, 2, 3, 4} initial values, evaluate body representation and movement realism scores

## Open Questions the Paper Calls Out
None

## Limitations
- Performance generalization: Claims about generalization to unseen music and novel subjects need systematic validation across diverse, held-out distributions
- Motion quality inheritance: Inherited limitations from base model constrain performance on complex, high-speed choreography
- Computational efficiency claims: Single-GPU training time and memory requirements need verification across different hardware configurations

## Confidence
- **High Confidence**: Core architectural innovations (ZICA, constructive influence function, Beta-Uniform scheduling) are well-documented and ablation studies provide strong evidence
- **Medium Confidence**: Claims about generalization to unseen music and novel subjects are supported by qualitative examples but lack systematic quantitative validation
- **Medium Confidence**: Comparative performance claims require more comprehensive benchmarking against established music-to-video generation methods

## Next Checks
1. Distributional generalization test: Evaluate on systematically curated music genres and dance styles not present in training data (e.g., classical Indian dance, contemporary ballet, breakdancing)
2. Extended baseline comparison: Conduct comprehensive benchmarking against established music-to-video generation methods using standardized evaluation metrics
3. Motion complexity stress test: Create controlled evaluation set with progressively complex choreographic sequences to systematically identify model boundaries