---
ver: rpa2
title: 'Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation
  for Islamic Knowledge Question Answering'
arxiv_id: '2509.23793'
source_url: https://arxiv.org/abs/2509.23793
tags:
- retrieval
- knowledge
- islamic
- arabic
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a hybrid retrieval-augmented generation system
  for Arabic Islamic knowledge QA. It combines BM25 sparse retrieval, dense semantic
  retrieval using Matryoshka embeddings, and cross-encoder reranking.
---

# Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering

## Quick Facts
- arXiv ID: 2509.23793
- Source URL: https://arxiv.org/abs/2509.23793
- Reference count: 6
- Key outcome: Hybrid RAG system achieves 45% accuracy on Islamic Inheritance subtask and 80% on General Knowledge

## Executive Summary
This paper presents a hybrid retrieval-augmented generation system for Arabic Islamic knowledge question answering. The system combines sparse BM25 retrieval, dense semantic retrieval with Matryoshka embeddings, and cross-encoder reranking to improve accuracy on two Arabic MCQ tasks. Evaluated on QIAS 2025, the approach achieves 45% accuracy on Islamic Inheritance reasoning and 80% on general Islamic knowledge, representing up to 25% improvement over baseline methods.

## Method Summary
The system employs a four-component pipeline: (1) Arabic preprocessing using CAMeL Tools with two tiers - full preprocessing for BM25 and light preprocessing for dense retrieval; (2) Three-stage retrieval combining BM25 (top-1000), dense embeddings (top-200), and cross-encoder reranking (top-5); (3) Context integration with domain-specific persona prompt and 2 few-shot examples; (4) LLM inference using either Fanar or Mistral-saba-24b. The approach processes 32,000 fatwas and 25 classical Islamic books, chunking text into 200-token segments with 20-token overlap.

## Key Results
- Subtask 1 (Islamic Inheritance): 45% accuracy (baseline improvement of 1-4%)
- Subtask 2 (Islamic General Knowledge): 80% accuracy (up to 25% improvement)
- System handles both structured inheritance reasoning and general knowledge questions
- Demonstrates effectiveness of hybrid retrieval approach for Arabic Islamic QA

## Why This Works (Mechanism)
The hybrid retrieval approach addresses the challenge of Islamic knowledge QA by combining multiple retrieval strategies. BM25 provides keyword-based retrieval for exact matches, while dense embeddings capture semantic similarity for nuanced queries. Cross-encoder reranking then refines the top candidates, improving precision. The domain-specific persona prompt guides the LLM to provide contextually appropriate answers grounded in Islamic scholarship.

## Foundational Learning
- **Arabic NLP preprocessing**: Why needed - Islamic texts contain complex morphology and diacritics; Quick check - Verify CAMeL Tools v1.2.0 installation and preprocessing output consistency
- **Retrieval-augmented generation**: Why needed - LLMs need external knowledge for domain-specific questions; Quick check - Confirm context integration and prompt structure
- **Multilingual embeddings**: Why needed - Arabic text requires specialized semantic representations; Quick check - Load Arabic-Triplet-Matryoshka-V2 and verify embedding dimensions
- **Cross-encoder reranking**: Why needed - Improves retrieval precision by re-scoring candidate passages; Quick check - Verify mmarco-mMiniLMv2-L12-H384-v1 model loading and inference

## Architecture Onboarding

**Component Map:** BM25 retrieval -> Dense embedding retrieval -> Cross-encoder reranking -> Context integration -> LLM inference

**Critical Path:** Query preprocessing → BM25 retrieval (top-1000) → Dense retrieval (top-200) → Cross-encoder reranking (top-5) → Prompt construction → LLM inference

**Design Tradeoffs:** Fixed retrieval parameters (n=1000, m=200, k=5) vs. dynamic selection; dual preprocessing tiers for different retrieval methods; 200-token chunks with 20-token overlap balance context and efficiency

**Failure Signatures:** 
- Retrieval mismatch between indexing and query time causes degraded performance
- Context window overflow from too many retrieved passages
- Semantic similarity between questions with different answers hurts dense retrieval precision

**First Experiments:**
1. Verify preprocessing consistency between indexing and query time with spot-checking
2. Test context window limits by logging token counts per prompt
3. Analyze per-difficulty accuracy to identify retrieval limitations

## Open Questions the Paper Calls Out
- How can structured reasoning or symbolic modules be integrated into RAG frameworks for multi-step fractional calculations in Islamic Inheritance?
- To what extent does domain-specific fine-tuning of retrieval and generation components improve performance compared to zero-shot implementation?
- Can dynamic context selection strategies outperform fixed three-stage retrieval parameters?

## Limitations
- QIAS 2025 dataset not publicly available, requiring organizer access
- Exact prompt template and few-shot examples not fully specified in paper
- Risk of context window overflow with top-5 retrieved passages not explicitly addressed

## Confidence
- High Confidence: Overall RAG pipeline and use of established tools (CAMeL Tools, Matryoshka embeddings, cross-encoders)
- Medium Confidence: Preprocessing strategy details and implementation nuances may vary
- Medium Confidence: Reported accuracy gains and final scores without full dataset access

## Next Checks
1. Verify CAMeL Tools v1.2.0 preprocessing consistency between indexing and query time
2. Test context window limits by logging total token count per prompt
3. Analyze per-difficulty accuracy to identify retrieval limitations and potential failure modes