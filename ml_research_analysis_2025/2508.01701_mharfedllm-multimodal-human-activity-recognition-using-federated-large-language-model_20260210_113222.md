---
ver: rpa2
title: 'MHARFedLLM: Multimodal Human Activity Recognition Using Federated Large Language
  Model'
arxiv_id: '2508.01701'
source_url: https://arxiv.org/abs/2508.01701
tags:
- data
- federated
- learning
- attention
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Human Activity Recognition
  (HAR) in multimodal settings, where data from depth cameras, pressure mats, and
  accelerometers must be effectively fused. The authors propose FedTime-MAGNET, a
  novel federated learning framework that combines multimodal sensor and image data.
---

# MHARFedLLM: Multimodal Human Activity Recognition Using Federated Large Language Model

## Quick Facts
- **arXiv ID:** 2508.01701
- **Source URL:** https://arxiv.org/abs/2508.01701
- **Reference count:** 10
- **Primary result:** FedTime-MAGNET achieves F1 scores of 0.934 (centralized) and 0.881 (federated) on multimodal HAR using depth cameras, pressure mats, and accelerometers

## Executive Summary
This paper addresses the challenge of Human Activity Recognition (HAR) in multimodal settings, where data from depth cameras, pressure mats, and accelerometers must be effectively fused. The authors propose FedTime-MAGNET, a novel federated learning framework that combines multimodal sensor and image data. At its core is the Multimodal Adaptive Graph Neural Expert Transformer (MAGNET), which integrates graph attention and mixture of experts to capture complex inter-modal relationships. To process time series sensor data, the framework uses a customized T5 encoder enhanced with LoRA, while DART-CNN handles image modalities with spatial and channel attention mechanisms. Experiments on the MEx dataset show that FedTime-MAGNET achieves a centralized F1 score of 0.934 and a federated F1 score of 0.881, outperforming baseline models in both settings. These results demonstrate the effectiveness of the proposed architecture in building accurate and robust HAR systems that preserve privacy through federated learning.

## Method Summary
The method combines a customized T5 encoder with LoRA for time-series sensor data and DART-CNN for image modalities, fused through MAGNET. The T5 encoder uses channel-dependent patching to process multivariate sensor sequences without destroying cross-channel correlations. DART-CNN processes depth camera and pressure mat images with spatial and channel attention mechanisms. MAGNET employs graph attention networks with dynamic and learnable adjacency matrices, combined with a mixture of experts (4 experts, top-2 routing) for adaptive inter-modal relationship learning. The federated learning framework uses FedAvg with 10 global rounds, 5 local epochs per round, and samples 9 of 21 clients per round. Training uses AdamW optimizer with learning rate 1e-4 and weight decay 1e-4.

## Key Results
- Centralized F1 score of 0.934 outperforms baselines on MEx dataset
- Federated F1 score of 0.881 demonstrates effective privacy-preserving learning with only ~5% degradation
- MAGNET fusion significantly outperforms DART-CNN alone in federated setting (0.881 vs 0.651 F1)
- Ablation shows depth camera modality is most critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Channel-dependent per-timestep patching enables transformer encoders to process multivariate sensor sequences without destroying cross-channel correlations.
- **Mechanism:** Each timestep's full multivariate vector (all d channels) is embedded as a single token via linear projection, rather than patching consecutive timesteps per channel independently. Sinusoidal positional encodings preserve temporal ordering. The T5 encoder's self-attention then operates on this sequence of timestep tokens.
- **Core assumption:** Temporal patterns in HAR emerge from relationships between complete system states at each timestep, not from independent channel subseries.
- **Evidence anchors:**
  - [abstract] "customized T5 encoder only architecture" for "time series sensor data"
  - [section] "We adopt a channel dependent patching strategy based on individual time steps, as opposed to the channel independent temporal patching used in the PatchTST architecture"
  - [corpus] Limited direct corpus support; neighboring papers focus on LLM prompting strategies rather than patching mechanisms.
- **Break condition:** If sensor channels operate at vastly different timescales or have misaligned sampling, per-timestep aggregation may introduce noise.

### Mechanism 2
- **Claim:** Combining graph attention with mixture-of-experts enables adaptive inter-modal relationship learning while routing inputs to specialized processing pathways.
- **Mechanism:** Modalities are treated as graph nodes. Dynamic adjacency (cosine similarity) captures data-driven relationships; learnable adjacency captures task-specific patterns. The combined adjacency weights multi-head attention. MoE then routes through sparse experts (top-k selection) with entropy-based load balancing to prevent expert collapse.
- **Core assumption:** Different activity classes and modalities benefit from specialized processing, and the optimal fusion topology varies per sample.
- **Evidence anchors:**
  - [abstract] "graph attention and a Mixture of Experts to generate unified, discriminative embeddings"
  - [section] "Af inal = Adynamic ⊙ Alearn + 0.5 · I" and "sparse MoE implementation uses top-k routing with entropy-based load balancing"
  - [corpus] GraMFedDHAR uses graph-based multimodal fusion with differential privacy, suggesting graph structures are tractable for this domain.
- **Break condition:** If modalities are highly redundant, the graph attention may over-weight spurious correlations.

### Mechanism 3
- **Claim:** LoRA decomposition reduces trainable parameters while maintaining federated communication efficiency and client-specific adaptation capacity.
- **Mechanism:** Weight matrices W = W₀ + BA where W₀ is frozen random initialization and BA is low-rank trainable component. Only BA parameters are communicated in FL rounds and updated during local training, reducing both bandwidth and client compute.
- **Core assumption:** The adaptation manifold for HAR tasks is low-rank relative to the full transformer parameter space.
- **Evidence anchors:**
  - [abstract] "lightweight T5 encoder only architecture is customized and adapted"
  - [section] "rank r = 16 with α = 32, targeting query, key, and value projection matrices"
  - [corpus] Corpus shows "Fdlora: personalized federated learning of large language model via dual lora tuning" as related work, supporting this approach.
- **Break condition:** If client data distributions diverge significantly (high non-IID), shared frozen weights may bottleneck personalization.

## Foundational Learning

- **Concept:** Self-attention and positional encoding in transformers
  - **Why needed here:** Understanding how T5 processes sequences of timestep tokens requires grasping Q/K/V projections and why position information matters for temporal data.
  - **Quick check question:** Can you explain why adding sinusoidal positional encodings to patch embeddings preserves order information for the attention mechanism?

- **Concept:** Graph attention networks (GAT)
  - **Why needed here:** MAGNET treats modalities as nodes; understanding attention-weighted message passing is essential for debugging fusion behavior.
  - **Quick check question:** Given node embeddings E₁ and E₂, how would you compute attention coefficient α₁₂ in a GAT layer?

- **Concept:** Federated averaging (FedAvg)
  - **Why needed here:** The federated framework relies on client weight aggregation; understanding local update vs. global aggregation tradeoffs is critical.
  - **Quick check question:** If 9 of 21 clients participate per round with 5 local epochs each, how many gradient descent steps occur before aggregation?

## Architecture Onboarding

- **Component map:** Sensor Data → Channel-dependent patching → T5 encoder + LoRA ─┐
                                                                      ├→ MAGNET: GAT + MoE → Classifier
              Image Data (DC/PM) → DART-CNN + dual attention + BiRNN stack ──┘
                                        ↓
                              Learnable modality weights wₘ

- **Critical path:** T5 patch embedding dimension (D=512) → modality-specific FFNN projection → MAGNET expects consistent embedding dimensions across all modalities before fusion. Mismatch here breaks the graph attention computation.

- **Design tradeoffs:**
  - Centralized (F1=0.934) vs. Federated (F1=0.881): ~5% performance gap for privacy preservation
  - 4 experts with top-2 routing balances specialization vs. compute; increasing experts may not help given MEx dataset scale (30 subjects, 7 classes)
  - Depth camera ablation shows largest performance drop; prioritize DC modality quality over others if resource-constrained

- **Failure signatures:**
  - MoE expert collapse: if validation shows one expert receives >80% of samples, increase entropy regularization λ (currently 0.01)
  - Federated divergence: if global validation loss oscillates without convergence, reduce local epochs or learning rate
  - Channel attention saturation: if DART-CNN spatial/channel attention weights become uniform, check reduction ratio r=16 may be too aggressive

- **First 3 experiments:**
  1. **Modality ablation:** Train with {act, pm, dc} only (paper's best: F1=0.927 centralized, 0.872 federated) to establish upper bound for reduced-sensor deployments.
  2. **Patching strategy comparison:** Implement PatchTST-style channel-independent patching vs. paper's channel-dependent approach on same T5 backbone; expect ~2-4% F1 difference if paper's assumption holds.
  3. **LoRA rank sweep:** Test r ∈ {4, 8, 16, 32} while tracking communication cost and F1; paper uses r=16 but provides no ablation—identify elbow point for your compute budget.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can differential privacy or homomorphic encryption be integrated into FedTime-MAGNET to prevent gradient-based leakage while maintaining competitive F1 scores?
- **Basis in paper:** [explicit] The conclusion states: "information can still be leaked through model updates, such as adversarial attacks. This risk can be mitigated by integrating techniques like differential privacy or homomorphic encryption."
- **Why unresolved:** Privacy-preserving techniques like differential privacy typically degrade model utility, and the trade-off in multimodal HAR settings is unexplored.
- **What evidence would resolve it:** Experiments showing F1 scores under varying privacy budgets (ε values) or homomorphic encryption overhead measurements.

### Open Question 2
- **Question:** Would other time series foundation models (e.g., Time-LLM, Chronos) outperform the custom T5 encoder in the federated multimodal HAR setting?
- **Basis in paper:** [explicit] The conclusion proposes: "Considering other time series foundation models, along with differential privacy, could be a good future direction."
- **Why unresolved:** The current architecture uses a customized T5 trained from scratch; comparative evaluation against pre-trained time series foundation models is absent.
- **What evidence would resolve it:** Ablation studies replacing T5 with other foundation models, reporting both centralized and federated F1 scores.

### Open Question 3
- **Question:** What causes the severe performance degradation of DART-CNN in the federated setting (F1: 0.874 → 0.651), and can architectural modifications mitigate this?
- **Basis in paper:** [inferred] Table 1 shows DART-CNN federated F1 drops to 0.651 while MAGNET maintains 0.881, suggesting modality-specific federated learning challenges.
- **Why unresolved:** The paper does not analyze why image-based modalities struggle more in federated settings than sensor modalities.
- **What evidence would resolve it:** Layer-wise analysis of weight divergence across clients, or experiments with modality-specific federated aggregation strategies.

## Limitations
- Exact dimensions for MAGNET's modality-specific FFNNs and final hierarchical classifier layers are unspecified
- Class imbalance handling through weighted cross-entropy lacks specification of the weights used
- Initialization scheme for learnable modality weights (w_act, w_acw, w_dc, w_pm) is not detailed

## Confidence
- **Centralized performance claims (F1=0.934):** High confidence - based on direct experimentation on MEx dataset with clear methodology
- **Federated performance claims (F1=0.881):** Medium confidence - federated results show reasonable gap from centralized, but hyperparameter sensitivity and non-IID client distributions could affect real-world deployment
- **Architectural innovations (channel-dependent patching, MAGNET fusion):** Medium confidence - mechanisms are well-described but lack comparative ablation studies to isolate individual contributions
- **LoRA effectiveness for federated learning:** High confidence - LoRA is a well-established technique with appropriate parameter choices (r=16, α=32)

## Next Checks
1. **Patching strategy ablation:** Implement and compare both PatchTST-style channel-independent patching and the paper's channel-dependent approach on the same T5 backbone to quantify the claimed 2-4% F1 improvement.

2. **Expert utilization monitoring:** During federated training, track MoE expert routing entropy and utilization rates. If any expert consistently receives >80% of samples, implement stronger entropy regularization (λ > 0.01) and measure impact on performance.

3. **Communication cost analysis:** For the federated setup, measure actual parameter transfer sizes with LoRA (r=16) versus full fine-tuning across 21 clients. Verify that the reported 5% performance gap from centralized to federated is worth the privacy preservation benefits in your deployment context.