---
ver: rpa2
title: 'CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on
  Short-Video Platforms'
arxiv_id: '2510.10095'
source_url: https://arxiv.org/abs/2510.10095
tags:
- query
- knowledge
- rewriting
- search
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CardRewriter, an LLM-based framework that
  enhances long-tail query rewriting on short-video platforms by incorporating domain-specific
  knowledge through generated knowledge cards. For each query, the system aggregates
  multi-source knowledge (videos, similar queries, and open-domain documents), summarizes
  it into a concise knowledge card, and uses this card to guide the LLM to produce
  more effective rewrites.
---

# CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms

## Quick Facts
- **arXiv ID**: 2510.10095
- **Source URL**: https://arxiv.org/abs/2510.10095
- **Reference count**: 40
- **Primary result**: LLM-based framework using knowledge cards improves long-tail query rewriting on short-video platforms, achieving +1.853% LVR, +3.729% CTR, and -2.630% IQRR in online A/B tests on Kuaishou.

## Executive Summary
CardRewriter addresses the challenge of long-tail query rewriting on short-video platforms where LLMs struggle with proprietary content like videos, live streams, and micro dramas. The framework introduces knowledge cards—concise, query-relevant summaries of multi-source knowledge (videos, similar queries, and external documents)—to guide LLM-based rewriting. By filtering noise from raw retrieved content and preserving domain-specific entity knowledge, the system achieves significant improvements in both semantic relevance and retrieval performance. Trained via a two-stage pipeline (SFT followed by GRPO) with a tailored reward balancing relevance and retrieval effectiveness, CardRewriter has been deployed since September 2025 and serves hundreds of millions of users daily on Kuaishou.

## Method Summary
CardRewriter employs a two-stage training pipeline to generate knowledge cards and rewrite queries. First, a card generator (Qwen2.5-VL-7B-Instruct) summarizes multi-source knowledge (top-k videos, similar queries via Q2Q and embedding matching, and open-domain documents) into concise cards. This card is then used by a query rewriter (Qwen3-8B) to generate more effective rewrites. Training involves supervised fine-tuning (SFT) on high-quality examples followed by group relative policy optimization (GRPO) with a reward system combining relevance (RRel) and system preference (RSys). The reward model uses Bradley-Terry preference modeling to approximate system preferences from pairwise comparisons. Knowledge cards address the noise sensitivity of naive RAG injection, while the two-stage training optimizes the tradeoff between expanding retrieval coverage and preserving user intent.

## Key Results
- Online A/B tests on Kuaishou show +1.853% long-view rate (LVR), +3.729% click-through rate (CTR), and -2.630% initiative query reformulation rate (IQRR) for covered traffic.
- Offline experiments demonstrate significant improvements in semantic relevance (QR-Rel) and retrieval performance (Hitrate@300, Increment).
- Ablation studies confirm the effectiveness of multi-source knowledge aggregation: removing related-videos drops Hitrate@300 from 51.36% to 47.16%, while removing external docs drops it to 50.31%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Summarizing multi-source knowledge into concise knowledge cards improves rewriting quality compared to naive RAG injection
- Mechanism: Raw retrieved knowledge contains structural inconsistencies and noise. The card generation model acts as a denoising filter, extracting only query-relevant signals. This prevents the rewriting model from drifting toward irrelevant retrieved content while preserving domain-specific entity knowledge.
- Core assumption: The summarization model can reliably distinguish signal from noise in retrieved content.
- Evidence anchors: [abstract] "summarizes it into an informative and query-relevant knowledge card"; [section 4.3.1] "directly injecting such knowledge reduces semantic alignment... arises from retrieval-augmented rewriting, which lengthens queries and introduces noise that drifts from the original intent"
- Break condition: If retrieved videos are overwhelmingly irrelevant to user intent, even summarized cards may amplify incorrect signals.

### Mechanism 2
- Claim: Two-stage training (SFT + GRPO) with a reward balancing relevance and retrieval effectiveness produces higher-quality rewrites than SFT alone
- Mechanism: SFT establishes basic rewriting capability from filtered high-quality examples. GRPO then optimizes using ROverall = RSys + RRel, where RSys (learned via Bradley-Terry reward model) captures system preference (hitrate/increment), and RRel ensures semantic fidelity. The combination explicitly trades off between expanding retrieval coverage and preserving user intent.
- Core assumption: The reward model accurately approximates true system preferences from pairwise comparison data.
- Evidence anchors: [abstract] "two-stage training pipeline: supervised fine-tuning followed by group relative policy optimization, with a tailored reward system balancing query relevance and retrieval effectiveness"; [section 4.3.3 / Figure 4] "using only the relevance reward... fails to expand coverage... relying solely on the system reward retrieves many previously inaccessible videos yet often drifts from the original intent"
- Break condition: If reward model training data has systematic biases (e.g., over-indexing on popular content), GRPO may optimize for wrong objectives.

### Mechanism 3
- Claim: Multi-source knowledge aggregation (original query videos + similar queries + external documents) provides complementary coverage for long-tail queries
- Mechanism: Long-tail queries often retrieve low-relevance videos directly. Similar high-supply queries (via Q2Q lexical overlap and embedding similarity) surface videos that better match user intent. External documents fill gaps when platform content is sparse.
- Core assumption: Similar queries identified by lexical and embedding overlap share relevant retrieved content with the original query.
- Evidence anchors: [abstract] "aggregates multi-source knowledge (videos, similar queries, and open-domain documents)"; [section 4.3.2 / Table 2] Ablation shows removing related-videos drops Hitrate@300 from 51.36% to 47.16%; removing external docs drops to 50.31%
- Break condition: If similar query matching returns semantically divergent queries, aggregated knowledge may introduce conflicting signals.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG) noise sensitivity
  - Why needed here: Understanding why naive RAG fails (noise drift) motivates the card summarization step.
  - Quick check question: Can you explain why adding more retrieved context might decrease rewriting quality?

- **Concept**: Bradley-Terry preference modeling
  - Why needed here: The reward model uses BT formulation to convert pairwise preferences into scalar rewards.
  - Quick check question: Given preferences A>B and B>C, how does BT compute P(A>C)?

- **Concept**: Group Relative Policy Optimization (GRPO)
  - Why needed here: GRPO estimates advantages relative to group samples rather than a separate value function, reducing memory overhead.
  - Quick check question: How does GRPO differ from PPO in advantage estimation?

## Architecture Onboarding

- **Component map**: Knowledge Collection Module → Card Generator (Cθ) → Query Rewriter (Gθ) → KV Cache
- **Critical path**: Query → Knowledge Collection → Card Generation → Query Rewriting → KV Lookup/Cache
- **Design tradeoffs**:
  - Near-line vs. online: Near-line caching trades freshness for latency (<100ms requirement)
  - Coverage vs. precision: Targeting only 15-20% of traffic (long-tail with poor performance) maximizes ROI
  - Card length: ≤200 chars balances information density vs. context window consumption
- **Failure signatures**:
  - Low QC-Rel but high QR-Rel: Card summarization is noisy but rewriter is robust
  - High Increment but low Hitrate: Rewriter expands recall but retrieves irrelevant content
  - Spike in IQRR: Rewrites may be drifting from user intent
- **First 3 experiments**:
  1. **Ablate knowledge sources**: Remove each source (vision, text, similar queries, external docs) and measure Hitrate@300 impact to validate Table 2 results on your traffic distribution.
  2. **Reward component analysis**: Train with RSys-only vs. RRel-only vs. ROverall on a held-out set to verify the tradeoff in Figure 4.
  3. **Card quality baseline**: Compare prompt-based vs. SFT vs. SFT+GRPO card generation (per Figure 5) to determine training investment ROI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CardRewriter be adapted for real-time online inference while meeting the latency requirements of production search systems?
- Basis in paper: [explicit] The paper states: "Due to the large parameter size and the auto-regressive nature of LLMs, directly deploying CardRewriter under Kuaishou's stringent low-latency search requirements is infeasible," necessitating a near-line deployment strategy instead.
- Why unresolved: The current architecture relies on two-stage LLM inference (card generation + query rewriting), which exceeds acceptable latency thresholds for real-time serving.
- What evidence would resolve it: Demonstrating model distillation, quantization, or architectural modifications that achieve sub-100ms inference while maintaining rewriting quality.

### Open Question 2
- Question: How can the coverage of query rewriting be expanded beyond the current 15-20% of daily search traffic?
- Basis in paper: [explicit] The selection criteria target queries with "average daily searches between 5 and 300," excluding both very rare and very popular queries, limiting the system's reach.
- Why unresolved: The near-line deployment strategy imposes practical constraints on which queries can be processed, and it remains unclear whether the approach generalizes to queries outside these thresholds.
- What evidence would resolve it: Experiments showing consistent performance gains when applying CardRewriter to head queries or extremely rare queries, or alternative deployment architectures.

### Open Question 3
- Question: How robust is CardRewriter when multi-source knowledge contains strongly conflicting evidence?
- Basis in paper: [inferred] Figure 6 shows a case where videos and documents conflict (MBTI personality vs. game skin), which prompt-based card generation alone could not resolve, requiring SFT+GRPO training.
- Why unresolved: The case study suggests that knowledge card quality depends on training data distribution, and edge cases with multiple plausible interpretations may not be well-covered.
- What evidence would resolve it: Systematic evaluation on adversarially constructed queries with deliberately conflicting retrieved evidence.

### Open Question 4
- Question: Does CardRewriter generalize to other short-video platforms with different content distributions and user query patterns?
- Basis in paper: [inferred] The paper evaluates exclusively on Kuaishou, and the training data (search logs, query sets) is platform-specific; no cross-platform transfer experiments are reported.
- Why unresolved: Domain-specific knowledge cards may encode platform-specific conventions that do not transfer, and reward models trained on one platform's user behavior may not align with another's.
- What evidence would resolve it: Zero-shot or few-shot transfer experiments applying CardRewriter to platforms like TikTok or Xiaohongshu without platform-specific retraining.

## Limitations
- **Model availability**: The paper relies on proprietary Qwen models (Qwen3-235B-A22B, Qwen3-8B) that are not publicly released, creating a fundamental barrier to reproduction.
- **Platform-specific data**: The multi-source knowledge collection depends on Kuaishou's proprietary search logs, video retrieval index, and user behavior patterns that are not accessible to external researchers.
- **Deployment validation**: While online A/B test results are reported, the lack of detailed methodology and potential platform-specific optimizations limits generalizability to other short-video platforms.

## Confidence

- **High**: The two-stage SFT → GRPO training pipeline with combined relevance-system reward is technically sound and well-supported by ablation studies (Figure 4, Table 2).
- **Medium**: The knowledge card summarization mechanism effectively addresses RAG noise issues, though the exact denoising capability depends on the quality of retrieved multi-source knowledge.
- **Medium**: The claim that similar queries provide complementary coverage is supported by ablation results but assumes reliable query similarity matching.
- **Low**: Online performance gains (LVR +1.853%, CTR +3.729%, IQRR -2.630%) cannot be independently verified due to platform-specific deployment details.

## Next Checks

1. **Knowledge source ablation replication**: Independently replicate the ablation study removing each knowledge source (vision, text, similar queries, external docs) on a different short-video platform to validate the 51.36% → 47.16% → 50.31% Hitrate@300 pattern.
2. **Reward component sensitivity**: Systematically vary the weighting between RSys and RRel in the combined reward to map out the full tradeoff curve and verify that ROverall indeed balances relevance and retrieval effectiveness.
3. **Card quality threshold analysis**: Conduct human evaluation studies comparing prompt-based, SFT-only, and SFT+GRPO card generation quality to determine the actual ROI of the full training pipeline versus simpler alternatives.