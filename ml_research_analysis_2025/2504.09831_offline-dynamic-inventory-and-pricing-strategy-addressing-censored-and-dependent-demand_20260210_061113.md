---
ver: rpa2
title: 'Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent
  Demand'
arxiv_id: '2504.09831'
source_url: https://arxiv.org/abs/2504.09831
tags:
- policy
- censoring
- where
- demand
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the offline sequential pricing and inventory
  control problem where demand is censored (unobservable when exceeding inventory)
  and dependent on past demand levels. The main challenge is learning an optimal policy
  from historical data containing censored sales, prices, ordering quantities, inventory
  levels, covariates, and censored sales, while maximizing long-term profit.
---

# Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand

## Quick Facts
- **arXiv ID:** 2504.09831
- **Source URL:** https://arxiv.org/abs/2504.09831
- **Reference count:** 40
- **Primary result:** Proposed Censored Fitted Q-Iteration (C-FQI) and Pessimistic Censored Fitted Q-Iteration (PC-FQI) algorithms achieve finite-sample regret bounds and converge to optimal policies in offline pricing/inventory control with censored, history-dependent demand.

## Executive Summary
This paper addresses the offline dynamic inventory and pricing problem where demand is both dependent on past demand and censored (unobservable when exceeding inventory). The authors develop two algorithms that incorporate survival analysis for reward imputation and offline reinforcement learning for policy estimation. The key innovation is approximating the observed process as a high-order MDP characterized by consecutive censoring instances, allowing stationary policy recovery despite violations of the Markov property. The method handles the dual challenges of censored rewards and demand dependency through Kaplan-Meier-based imputation and specialized Q-iteration with censoring coverage constraints.

## Method Summary
The method works by first estimating the maximum consecutive censoring instances in the offline data, then partitioning the data into subsets based on censoring history length. For each partition, a Kaplan-Meier estimator is used to impute missing rewards during censored periods, creating surrogate rewards that satisfy modified Bellman equations. Fitted Q-Iteration is then applied separately to each partition to learn distinct Q-functions. The policy construction incorporates a "censoring parity" constraint that forces selection of a known safe action when the maximum censoring depth is reached, preventing dangerous extrapolation into unobserved regions of the state space.

## Key Results
- Numerical experiments show C-FQI converges to optimal policies as sample size increases
- C-FQI performs better under uniform behavior policies with good exploration
- PC-FQI performs better under near-optimal behavior policies with limited coverage
- Theoretical finite-sample regret bounds are established under censoring coverage assumptions
- Implementations are available at https://github.com/gundemkorel/Inventory_Pricing_Control

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The observed process, which violates the Markov property due to censoring and demand dependency, can be approximated as a High-Order MDP to recover a stationary optimal policy.
- **Mechanism:** Instead of relying on the current state alone, the system defines the state space by the number of consecutive censoring instances ($n_{K,b}$). This transforms the partially observed trajectory into a high-order Markov chain where the transition depends on the history up to the last uncensored point.
- **Core assumption:** Assumption 3 (Regularity): The probability of observing a non-censored state under the optimal policy increases as the number of consecutive censoring instances increases.
- **Evidence anchors:**
  - [abstract]: "...approximating the optimal policy via a high-order MDP characterized by consecutive censoring instances..."
  - [section 3.2]: "This dependency is a result of censoring, which influences how the observed process inherits the Markov property..."
  - [corpus]: *FreshRetailNet-50K* highlights the challenge of "stockout-annotated censored demand," validating the need for specialized state representations like the High-Order MDP to handle data gaps.
- **Break condition:** If the demand dependency spans further back than the maximum observed consecutive censoring window in the offline data ($n_{K,b}$ is too small), the Markov approximation fails.

### Mechanism 2
- **Claim:** Missing profit information (rewards) during censored periods can be recovered to satisfy Bellman equations via survival analysis imputation.
- **Mechanism:** When demand is censored, the true stock-out cost is unknown. The method uses the Kaplan-Meier estimator to calculate a conditional survival function, allowing the estimation of expected demand conditional on the inventory cap. This "surrogate reward" acts as a statistically valid replacement for the missing true reward in the Q-learning updates.
- **Core assumption:** Assumption 4 (Conditional Non-Informative Censoring): Demand $D_t$ is independent of inventory $Y_t$ given features and actions.
- **Evidence anchors:**
  - [abstract]: "...solve these equations from offline data with imputed rewards via survival analysis."
  - [section 3.4]: "...it is sufficient to estimate $\tilde{R}_t$ ... relying on which we can recover the optimal policy."
  - [corpus]: *Dynamic Pricing with Adversarially-Censored Demands* addresses censoring in an online/adversarial setting; this mechanism adapts imputation specifically for the offline, non-parametric setting.
- **Break condition:** If inventory levels actively influence demand (informative censoring), the Kaplan-Meier estimator will be biased, leading to incorrect reward imputation.

### Mechanism 3
- **Claim:** A "Censoring Parity" constraint (or pessimism penalty) prevents the learned policy from extrapolating into unobserved high-censoring territories, ensuring safety in offline deployment.
- **Mechanism:** The algorithm estimates the maximum consecutive censoring instances present in the offline data ($\hat{n}_{K,b}$). It forces the policy to select a specific "uncensored pathway" action ($a_{max}$) when this limit is reached, or applies a pessimism penalty (PC-FQI) to the Q-value to discourage out-of-distribution actions.
- **Core assumption:** Assumption 2: A known action $a_{max}$ exists that can guarantee an uncensored state almost surely (e.g., ordering excessive stock or raising prices).
- **Evidence anchors:**
  - [abstract]: "...incorporate censoring parity constraints and ensure learned policies avoid excessive censoring."
  - [section 4]: "The term... quantifies the insufficient censoring coverage... if the offline data distribution has full censoring coverage... this term becomes zero."
  - [corpus]: *Spatial Supply Repositioning with Censored Demand Data* deals with networked censored demand but does not explicitly define a "Censoring Parity" mechanism to bound policy extrapolation as done here.
- **Break condition:** If the optimal policy fundamentally requires more consecutive censoring than the offline data contains (insufficient censoring coverage), the "uncensored pathway" constraint may force a suboptimal action.

## Foundational Learning

- **Concept: Kaplan-Meier Estimator**
  - **Why needed here:** To estimate the conditional survival function of demand when data is right-censored by inventory limits. It is the non-parametric backbone for the reward imputation mechanism.
  - **Quick check question:** Can you explain why right-censoring in inventory data is mathematically similar to survival analysis in clinical trials?

- **Concept: Fitted Q-Iteration (FQI)**
  - **Why needed here:** The base offline reinforcement learning algorithm. The paper extends FQI to handle the specialized Bellman equations and the imputed rewards derived from the High-Order MDP.
  - **Quick check question:** How does the max-operator in standard FQI lead to overestimation errors when offline data coverage is poor?

- **Concept: High-Order Markov Decision Process (MDP)**
  - **Why needed here:** Standard MDPs assume the next state depends only on the current state. Here, demand dependency and censoring history require looking back $n$ steps to satisfy the Markov property.
  - **Quick check question:** What happens to the state space size as the order $n$ of the MDP increases?

## Architecture Onboarding

- **Component map:** Data Pre-processor -> Partitioner -> Q-Learner (C-FQI/PC-FQI) -> Policy Constructor
- **Critical path:** The estimation of the conditional survival function in Step 1. If this probability distribution is inaccurate, the imputed rewards poison the Q-functions, leading to systematically biased pricing/inventory decisions.
- **Design tradeoffs:**
  - **C-FQI vs. PC-FQI:** C-FQI is standard FQI adapted for this structure (higher variance, exploits exploration). PC-FQI subtracts an uncertainty quantifier (pessimistic) to ensure safety but may be overly conservative if data is sparse but the model is correct.
  - **Iteration Count ($K$):** A larger $K$ reduces approximation error but amplifies any residual estimation errors; the theoretical bound suggests $K$ should scale with $\log(NT)$.
- **Failure signatures:**
  - **Runaway Censoring:** If the deployed policy triggers more consecutive censoring than $\hat{n}_{K,b}$, the Q-function has no training data to guide the decision, likely leading to random or disastrous actions.
  - **Imputation Bias:** If stockouts cause customers to permanently leave (informative censoring), the imputed demand will be over-optimistic, causing over-pricing.
- **First 3 experiments:**
  1. **Baseline Recovery:** Run C-FQI on synthetic data where true demand is known. Compare the "Regret" against the Oracle policy (Eq 3) to verify convergence claims.
  2. **Coverage Ablation:** Artificially truncate the offline dataset to reduce $\hat{n}_{K,b}$. Verify if PC-FQI outperforms C-FQI (as predicted for low-coverage data).
  3. **Imputation Validation:** Isolate the reward imputation module. Compare the Kaplan-Meier estimated demand distribution against the ground truth demand distribution for censored time-steps only.

## Open Questions the Paper Calls Out
None

## Limitations
- The method critically depends on the assumption that demand is independent of inventory given features (Assumption 4)
- Performance is fundamentally limited by the maximum consecutive censoring instances observed in the offline data
- The algorithms may be sensitive to hyperparameter choices like kernel bandwidth and regularization parameters

## Confidence
- **High Confidence:** The theoretical framework (High-Order MDP construction, Kaplan-Meier imputation, regret bounds) is mathematically sound under stated assumptions
- **Medium Confidence:** The empirical results on synthetic data demonstrate convergence, but real-world validation is needed to confirm robustness to violations of Assumption 4
- **Low Confidence:** The relative performance of C-FQI vs. PC-FQI in real-world settings (beyond the synthetic study) is unclear, as the paper does not provide extensive results on the impact of censoring coverage in practical datasets

## Next Checks
1. **Assumption 4 Violation Test:** Run the algorithm on a synthetic dataset where demand is explicitly dependent on inventory (informative censoring). Quantify the degradation in performance to validate the importance of this assumption.
2. **Censoring Coverage Ablation:** Use a real-world dataset (e.g., FreshRetailNet-50K) and systematically truncate the data to reduce the observed censoring coverage. Measure how the performance gap between C-FQI and PC-FQI evolves.
3. **Hyperparameter Sensitivity Analysis:** Conduct a grid search over the Kernel Ridge Regression regularization parameter and discount factor. Determine if the algorithm's performance is robust or highly sensitive to these choices.