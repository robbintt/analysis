---
ver: rpa2
title: Multitask finetuning and acceleration of chemical pretrained models for small
  molecule drug property prediction
arxiv_id: '2510.12719'
source_url: https://arxiv.org/abs/2510.12719
tags:
- kermt
- data
- multitask
- performance
- kpgt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores multitask finetuning of chemical pretrained
  models for drug property prediction. The authors benchmark three pretrained models
  (KERMT, KPGT, and MoLFormer) and compare them against Chemprop in single-task and
  multitask settings across internal and public ADMET datasets.
---

# Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction

## Quick Facts
- arXiv ID: 2510.12719
- Source URL: https://arxiv.org/abs/2510.12719
- Reference count: 40
- Primary result: KERMT multitask outperforms other methods on internal ADMET data, with performance gains increasing as training data size grows

## Executive Summary
This study benchmarks three pretrained chemical models (KERMT, KPGT, MoLFormer) for drug property prediction across single-task and multitask settings. The authors find that multitask finetuning with KERMT significantly outperforms other approaches on internal AstraZeneca ADMET datasets, particularly at larger data sizes. Counterintuitively, multitask benefits increase rather than decrease with more training data. The work also releases two new multitask ADMET dataset splits and provides an accelerated KERMT implementation enabling efficient industrial-scale pretraining and finetuning workflows.

## Method Summary
The authors evaluate pretrained chemical models on ADMET property prediction tasks through systematic benchmarking. They compare three pretrained models (KERMT, KPGT, MoLFormer) against Chemprop in both single-task and multitask settings across internal and public datasets. The study includes both small molecule and peptide subsets, examines the impact of training data size, and implements an accelerated KERMT version for improved computational efficiency. The benchmarking framework evaluates performance across multiple ADMET endpoints with correlation metrics.

## Key Results
- KERMT multitask significantly outperforms other methods on internal data, with the largest performance gap at the highest data sizes
- Performance improvements from multitask finetuning increase rather than decrease with more training data, contradicting conventional wisdom about pretrained models
- Internal pretraining with peptide data did not improve performance on peptide subsets, suggesting current pretraining tasks may be insufficient for this modality

## Why This Works (Mechanism)
The performance advantage of multitask finetuning appears to stem from improved feature learning across related ADMET tasks. When multiple properties are predicted simultaneously, the model learns shared representations that capture underlying chemical patterns relevant to multiple endpoints. The counterintuitive finding that multitask benefits increase with data size suggests that larger datasets provide more examples of these shared patterns, allowing the model to better exploit task correlations. This challenges the traditional view that pretraining primarily benefits low-data regimes.

## Foundational Learning
1. **Chemical pretraining** - Models trained on large molecular datasets using self-supervised objectives
   - Why needed: Provides rich chemical representations without requiring labeled data
   - Quick check: Model can generate meaningful embeddings for unseen molecules

2. **Multitask learning** - Simultaneous prediction of multiple related properties
   - Why needed: Exploits correlations between related ADMET endpoints
   - Quick check: Model learns shared representations across tasks

3. **Transformer-based molecular representations** - Using attention mechanisms to capture molecular structure
   - Why needed: Handles complex molecular features and long-range dependencies
   - Quick check: Model maintains performance across different molecular sizes

4. **Self-supervised pretraining objectives** - Masked token prediction and property prediction
   - Why needed: Creates useful representations without labeled data
   - Quick check: Pretrained model performs well on downstream tasks

5. **ADMET property prediction** - Absorption, Distribution, Metabolism, Excretion, and Toxicity endpoints
   - Why needed: Critical for drug discovery and development
   - Quick check: Model predictions correlate with experimental measurements

## Architecture Onboarding

**Component Map:** Molecular inputs -> Pretrained encoder -> Task-specific heads -> Property predictions

**Critical Path:** Data loading → Pretraining/finetuning → Multitask optimization → Property prediction

**Design Tradeoffs:** Larger models (KERMT-large) overfit on smaller datasets vs. base models, but scale better with more data; multitask learning adds computational overhead but improves generalization

**Failure Signatures:** Performance degradation when pretraining tasks poorly align with downstream properties; overfitting on small datasets with large models; loss of multitask benefits with insufficient task correlation

**First Experiments:**
1. Benchmark single-task vs. multitask performance on a small internal dataset
2. Test model generalization from small molecules to peptides
3. Compare training times between standard and accelerated KERMT implementations

## Open Questions the Paper Calls Out

### Open Question 1
Can extended pretraining tasks specifically designed for peptide representations improve downstream performance on this modality?
- Basis in paper: The authors state in Section 2.4 and the Discussion that future work should investigate extended pretraining tasks for peptides, as current models trained primarily on small molecules show no significant performance difference on peptide subsets.
- Why unresolved: Current pretrained models are trained largely on small molecules (ChEMBL/ZINC), and internal pretraining with more peptide data surprisingly did not improve performance.
- What evidence would resolve it: Benchmarking performance on peptide datasets after pretraining with objectives specifically tailored to peptide structures and properties.

### Open Question 2
How does the correlation between specific pretraining tasks and multitask finetuning performance vary across different architectures?
- Basis in paper: The Discussion section notes that future ablation studies should rigorously determine these correlations to explain why KERMT benefits significantly from multitasking while KPGT often suffers performance degradation.
- Why unresolved: The difference is hypothesized to stem from the specific pretraining tasks (e.g., predicting chemical environments vs. masked descriptors), but this has not been empirically isolated.
- What evidence would resolve it: Ablation studies that swap or modify pretraining objectives between KERMT and KPGT architectures to observe changes in multitasking efficacy.

### Open Question 3
How do encoder parameter size and partial freezing strategies affect downstream performance in small versus large data regimes?
- Basis in paper: The Discussion identifies the need to investigate parameter size and partial freezing, noting that the larger KERMT model overfits on smaller datasets compared to the base model.
- Why unresolved: While the study establishes that the base model outperforms the large model as data decreases, the effect of freezing encoder layers to mitigate overfitting was not tested.
- What evidence would resolve it: Benchmarking finetuning performance with varying degrees of encoder freezing (layer-wise learning rates or frozen weights) across different training data sizes.

## Limitations
- Performance comparisons rely heavily on proprietary internal AstraZeneca dataset, limiting external validation
- Three public datasets (Caco-2, CYP3A4, P-gp) represent a narrow slice of drug property prediction tasks
- Analysis focuses on correlation metrics without reporting other important measures like precision, recall, or calibration curves

## Confidence
- KERMT multitask performance superiority (High confidence): Demonstrated across multiple data sizes and task types
- Multitask benefits increase with data size (High confidence): Consistent pattern across benchmarks
- Accelerated KERMT implementation claims (Medium confidence): Supported by timing data but needs broader hardware validation

## Next Checks
1. Evaluate these models across a broader range of ADMET endpoints and public datasets to assess generalizability
2. Conduct ablation studies to understand which components of the multitask approach drive performance improvements
3. Test model performance at different data size thresholds to precisely characterize when multitask advantages emerge and whether the relationship is monotonic or shows diminishing returns