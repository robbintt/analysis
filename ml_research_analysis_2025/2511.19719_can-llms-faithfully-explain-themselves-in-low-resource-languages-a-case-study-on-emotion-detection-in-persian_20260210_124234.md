---
ver: rpa2
title: Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study
  on Emotion Detection in Persian
arxiv_id: '2511.19719'
source_url: https://arxiv.org/abs/2511.19719
tags:
- words
- text
- output
- emotion
- influential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the faithfulness of self-generated explanations\
  \ from large language models (LLMs) for emotion classification in Persian, a low-resource\
  \ language. The research compares two prompting strategies\u2014Predict-then-Explain\
  \ (P-E) and Explain-then-Predict (E-P)\u2014to assess how the order of explanation\
  \ and prediction affects explanation quality."
---

# Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian

## Quick Facts
- arXiv ID: 2511.19719
- Source URL: https://arxiv.org/abs/2511.19719
- Reference count: 20
- This study investigates the faithfulness of self-generated explanations from large language models (LLMs) for emotion classification in Persian, a low-resource language.

## Executive Summary
This study investigates the faithfulness of self-generated explanations from large language models (LLMs) for emotion classification in Persian, a low-resource language. The research compares two prompting strategies—Predict-then-Explain (P-E) and Explain-then-Predict (E-P)—to assess how the order of explanation and prediction affects explanation quality. Using a Persian emotion-labeled corpus, the study evaluates faithfulness through confidence scores derived from token-level log-probabilities, calibrated using temperature scaling to improve reliability. The models, including GPT-4 variants, Llama3.3-70B, and DeepSeek-V3, achieved strong classification performance but often produced explanations that diverged from human reasoning, with greater agreement among themselves than with human annotations. Results indicate that P-E consistently yields more accurate classifications, faithful explanations, and closer alignment with human reasoning. However, no model achieved high faithfulness metrics, suggesting limitations in current explanation strategies or evaluation metrics. The study highlights the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.

## Method Summary
The study evaluates LLM faithfulness in emotion classification for Persian using two prompting paradigms: Predict-then-Explain (P-E) and Explain-then-Predict (E-P). The ARMANEMO dataset subset (300 evaluation samples, 210 calibration samples) was used with 6 emotion classes. For each sample, models classified emotion, extracted top-5 influential words, and re-classified using only those words or with words replaced by placeholder. Confidence scores were computed from token-level log-probabilities and calibrated using temperature scaling (grid search [0.1, 21.0]) to minimize Expected Calibration Error (ECE). Faithfulness metrics (Comprehensiveness, Sufficiency, DFTopKRemoved, DFTopKOnly) and agreement metrics (Feature Agreement, IoU) were computed against human annotations. Prompt templates were provided for all model configurations.

## Key Results
- Predict-then-Explain (P-E) paradigm consistently outperformed Explain-then-Predict (E-P) across all models in classification accuracy, faithfulness metrics, and alignment with human reasoning
- All models achieved strong classification performance (F1 scores 0.76-0.88) but produced explanations with low faithfulness metrics, indicating explanations often diverged from human reasoning
- Models showed higher agreement with each other than with human annotations, suggesting reliance on shared statistical patterns rather than semantic understanding
- Temperature scaling significantly reduced calibration error (ECE from 0.03-0.06 to 0.01-0.02), improving confidence score reliability

## Why This Works (Mechanism)
The Predict-then-Explain paradigm allows models to first establish a contextual understanding of the full text before identifying influential words, leading to more coherent and faithful explanations. Temperature scaling calibrates token-level log-probabilities to produce reliable confidence scores, which are essential for computing meaningful faithfulness metrics. The comparison between model-generated explanations and human annotations reveals whether models capture semantic understanding or rely on superficial keyword patterns.

## Foundational Learning
- **Expected Calibration Error (ECE)**: Measures the difference between predicted confidence and actual accuracy; needed to ensure confidence scores reflect true model reliability. Quick check: Verify calibration improves when ECE decreases after temperature scaling.
- **Token-level log-probabilities**: Raw model outputs used to compute confidence scores; needed as foundation for faithfulness metrics. Quick check: Ensure log-probabilities are correctly extracted for label tokens only.
- **Faithfulness metrics (Comprehensiveness/Sufficiency)**: Quantify how well explanations capture model reasoning; needed to evaluate explanation quality beyond classification accuracy. Quick check: Verify metric calculations match formulas in Table 4.
- **Temperature scaling**: Calibration technique that adjusts model outputs to improve confidence reliability; needed to ensure faithfulness metrics are based on accurate confidence estimates. Quick check: Confirm grid search finds optimal temperature that minimizes ECE on calibration set.
- **Feature agreement metrics**: Measure overlap between model and human identified influential words; needed to assess alignment with human reasoning. Quick check: Compare IoU scores across models to identify patterns.
- **Low-resource language challenges**: Persian has limited training data compared to high-resource languages; needed context for interpreting results and generalizability concerns. Quick check: Consider whether findings might differ in high-resource language settings.

## Architecture Onboarding
- **Component Map**: ARMANEMO Dataset -> Prompt Templates -> LLM API (GPT-4, Llama3.3-70B, DeepSeek-V3) -> Token Log-probabilities -> Temperature Scaling -> Faithfulness Metrics -> Human Annotation Comparison
- **Critical Path**: Dataset → P-E/E-P prompting → Log-probability extraction → Calibration → Faithfulness computation → Agreement evaluation
- **Design Tradeoffs**: Single language focus (Persian) provides depth but limits generalizability; token-level confidence analysis provides fine-grained evaluation but requires API access; balanced dataset ensures fair comparison but may not reflect real-world distribution.
- **Failure Signatures**: Non-compliant outputs (<1% discarded), consistent misclassification of Surprise emotion, overconfident predictions skewing faithfulness metrics, low agreement with human annotations despite high classification performance.
- **First Experiments**: 1) Verify temperature scaling calibration improves ECE on calibration set; 2) Compare faithfulness metrics across P-E and E-P paradigms; 3) Analyze per-class performance differences, particularly for Surprise emotion.

## Open Questions the Paper Calls Out
- **Open Question 1**: Do the findings regarding the superiority of the Predict-then-Explain paradigm and explanation faithfulness generalize to other low-resource languages and diverse tasks? The conclusion states that future work should "expand research to diverse datasets, languages, and tasks for more robust evaluations." The study is limited to a single case study: emotion detection in Persian.
- **Open Question 2**: Can alternative explanation strategies or refined faithfulness metrics bridge the gap between model-generated rationales and human reasoning? The conclusion highlights the "limitations of current explanation methods and metrics" and calls for work to "explore alternative explanation strategies and refine faithfulness metrics." Current metrics indicated that no model achieved high faithfulness, suggesting current evaluation approaches may be inadequate.
- **Open Question 3**: What specific factors drive the convergence of LLM reasoning strategies towards keyword-based heuristics rather than the semantic understanding used by humans? The authors note models show higher agreement with each other than humans, likely due to shared "statistical regularities or keyword-based heuristics" versus human "semantic understanding." The study identifies this divergence but does not isolate the training data features or architectural components causing it.

## Limitations
- The evaluation dataset (300 samples across 6 emotion classes) is relatively small for assessing low-resource language capabilities
- All models show low faithfulness metrics despite high classification performance, suggesting either fundamental limitations in current self-explanation approaches or inadequacies in the evaluation metrics themselves
- The study's focus on Persian limits broader applicability claims and generalizability to other low-resource languages and tasks

## Confidence
- **High confidence**: The experimental methodology for computing faithfulness metrics and the observed pattern that P-E outperforms E-P across all models are well-supported by the data presented. The calibration procedure and its impact on ECE reduction are clearly demonstrated.
- **Medium confidence**: The conclusion that current explanation strategies are inadequate in low-resource languages is supported by the results, but could benefit from additional analysis of whether the faithfulness metrics themselves are appropriate for the task.
- **Low confidence**: The generalizability of findings to other low-resource languages and tasks beyond emotion classification.

## Next Checks
1. **Temperature Scaling Verification**: Reproduce the calibration procedure by implementing the grid search over temperature [0.1, 21.0] on the 210-sample calibration set, comparing pre/post calibration ECE with Table 3 values to verify the reported calibration improvements.

2. **Faithfulness Metric Sensitivity**: Test alternative thresholds for influential word selection (beyond top-5) and examine how faithfulness metrics change, particularly focusing on whether current metrics appropriately capture explanation quality or if they may be too stringent for the task.

3. **Cross-Lingual Generalization**: Apply the same methodology to another low-resource language emotion classification dataset (when available) to assess whether the P-E advantage and faithfulness challenges persist across languages, helping determine if findings are specific to Persian or indicative of broader LLM limitations.