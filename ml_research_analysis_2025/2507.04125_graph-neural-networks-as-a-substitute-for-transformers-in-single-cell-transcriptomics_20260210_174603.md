---
ver: rpa2
title: Graph Neural Networks as a Substitute for Transformers in Single-Cell Transcriptomics
arxiv_id: '2507.04125'
source_url: https://arxiv.org/abs/2507.04125
tags:
- gnns
- positions
- relative
- single-cell
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares Graph Neural Networks (GNNs) and Transformers
  in single-cell transcriptomics, where positional information is inherently absent.
  The authors theoretically and empirically demonstrate that when relative positions
  are not important, GNNs can achieve competitive performance to Transformers while
  using significantly fewer computational resources.
---

# Graph Neural Networks as a Substitute for Transformers in Single-Cell Transcriptomics

## Quick Facts
- **arXiv ID**: 2507.04125
- **Source URL**: https://arxiv.org/abs/2507.04125
- **Authors**: Jiaxin Qi; Yan Cui; Jinli Ou; Jianqiang Huang; Gaogang Xie
- **Reference count**: 40
- **Primary result**: GNNs match Transformer accuracy with 1/4 to 1/2 the computational cost and 1/8 the memory usage in single-cell transcriptomics

## Executive Summary
This paper challenges the prevailing assumption that Transformers are always the optimal architecture for single-cell transcriptomics by demonstrating that Graph Neural Networks (GNNs) can achieve competitive performance while using significantly fewer computational resources. The key insight is that when relative positions are not important—as is the case in single-cell transcriptomics where positional information is inherently absent—GNNs can match Transformer performance. Through both theoretical analysis and extensive empirical validation on large-scale transcriptomic datasets, the authors show GNNs offer a more efficient alternative architecture for this domain.

## Method Summary
The authors conducted synthetic experiments and extensive trials on a large-scale transcriptomic dataset to compare GNNs and Transformers in single-cell transcriptomics applications. They specifically focused on scenarios where relative positional information is not meaningful, leveraging the inherent graph structure of single-cell data. The study examined computational costs (including both processing time and memory usage) while maintaining comparable accuracy metrics. Theoretical analysis complemented empirical results to explain why GNNs perform competitively to Transformers when positional encoding is unnecessary.

## Key Results
- GNNs match Transformer accuracy in single-cell transcriptomics tasks
- GNNs use 1/4 to 1/2 the computational cost compared to Transformers
- GNNs require only 1/8 the memory usage of Transformers

## Why This Works (Mechanism)
The mechanism underlying GNNs' competitive performance lies in the nature of single-cell transcriptomics data, where positional information is inherently absent or meaningless. Unlike natural language processing or computer vision tasks where relative positions carry important information, single-cell data represents molecular measurements where the ordering of cells or genes doesn't convey meaningful biological information. GNNs naturally operate on graph structures without requiring positional encodings, making them well-suited for this domain where Transformers would be spending computational resources on encoding irrelevant positional information.

## Foundational Learning
- **Graph Neural Networks**: Why needed - to process data with inherent graph structure; Quick check - can you explain message passing between nodes?
- **Transformers**: Why needed - to understand the baseline architecture being compared; Quick check - what role does positional encoding play?
- **Single-cell transcriptomics**: Why needed - to understand the domain-specific context; Quick check - why is positional information absent in this data?
- **Computational complexity analysis**: Why needed - to evaluate efficiency claims; Quick check - can you distinguish between computational cost and memory usage?
- **Relative positional information**: Why needed - central to understanding when GNNs can substitute for Transformers; Quick check - when is positional information important vs. unimportant?

## Architecture Onboarding

**Component Map**: Single-cell data -> Graph Construction -> GNN layers -> Output layer; Single-cell data -> Positional encoding -> Transformer layers -> Output layer

**Critical Path**: Data preprocessing -> Graph construction (for GNN) OR positional encoding (for Transformer) -> Model layers -> Classification/regression output

**Design Tradeoffs**: GNNs sacrifice explicit positional encoding capabilities but gain efficiency when positional information is irrelevant; Transformers maintain flexibility for positional data but incur unnecessary computational overhead in position-agnostic domains

**Failure Signatures**: GNNs may underperform when positional relationships become important; Transformers may waste computational resources encoding meaningless positional information

**First Experiments**: 1) Test on synthetic data with controlled positional relevance; 2) Compare memory usage across different batch sizes; 3) Measure accuracy degradation when adding irrelevant positional information

## Open Questions the Paper Calls Out
None

## Limitations
- The findings are specific to single-cell transcriptomics where positional information is absent, limiting generalizability to other domains
- Theoretical analysis assumes specific graph structures that may not fully capture real-world biological network complexity
- The study doesn't extensively explore trade-offs in biological interpretability of results

## Confidence
- **Core finding that GNNs can match Transformer performance**: High confidence
- **Computational efficiency claims (1/4 to 1/2 cost, 1/8 memory)**: High confidence
- **Generalizability beyond single-cell transcriptomics**: Low confidence

## Next Checks
1. Test GNN performance across multiple single-cell technologies (scRNA-seq, scATAC-seq, multi-omics) to verify domain-specific advantages hold across modalities
2. Conduct ablation studies varying graph construction methods and neighborhood sizes to determine optimal configurations for different transcriptomic tasks
3. Compare biological interpretability by analyzing feature importance and pathway enrichment between GNN and Transformer models to ensure efficiency gains don't compromise biological insights