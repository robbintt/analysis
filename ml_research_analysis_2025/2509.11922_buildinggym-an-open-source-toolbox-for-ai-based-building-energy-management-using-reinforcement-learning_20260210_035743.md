---
ver: rpa2
title: 'BuildingGym: An open-source toolbox for AI-based building energy management
  using reinforcement learning'
arxiv_id: '2509.11922'
source_url: https://arxiv.org/abs/2509.11922
tags:
- control
- energy
- building
- buildinggym
- cooling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BuildingGym is an open-source framework that integrates EnergyPlus
  with reinforcement learning to enable flexible, real-time control of building energy
  systems. It addresses the lack of a unified environment for training RL algorithms
  across various building control problems.
---

# BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning

## Quick Facts
- **arXiv ID:** 2509.11922
- **Source URL:** https://arxiv.org/abs/2509.11922
- **Reference count:** 18
- **Primary result:** BuildingGym integrates EnergyPlus with RL for flexible, real-time control of building energy systems.

## Executive Summary
BuildingGym is an open-source framework that addresses the gap in unified environments for training reinforcement learning algorithms for building energy management. By integrating EnergyPlus as its core simulator, the framework enables direct interaction between RL agents and high-fidelity physics simulations, supporting both system-level and room-level control. Key innovations include acceptance of external signals for grid-interactive applications, easy configuration of RL algorithms and building models, and support for both on-policy (PPO, A2C) and off-policy (DQN, TD3) methods. The framework was validated on cooling load management tasks, with TD3 achieving median control errors of 0.5% for constant reduction and 1% for dynamic targets.

## Method Summary
BuildingGym employs a modular architecture that separates configuration, training, and algorithm modules to enable flexible RL-based building control. The framework uses EnergyPlus State API to create a closed loop where the simulation pauses at each timestep to pass observation data to a Python agent, which then returns action commands. External signals can be injected as observations to support grid-interactive control. The unified configuration interface allows users to easily swap between different RL algorithms and building models without rewriting environment logic. The system supports both on-policy and off-policy RL methods and includes a reward function definition that balances energy reduction with comfort constraints.

## Key Results
- TD3 achieved median control errors of 0.5% for constant cooling load reduction and 1% for dynamic target tracking
- Framework successfully handled both system-level and room-level control scenarios
- External signal integration enabled grid-interactive control capabilities
- EnergyPlus integration maintained simulation fidelity while supporting real-time agent interaction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct API integration with EnergyPlus allows RL agents to interact with high-fidelity physics simulations in a stepwise manner.
- **Mechanism:** Uses EnergyPlus State API and Python callback functions to create a closed loop where simulation pauses, passes observations, receives actions, and resumes.
- **Core assumption:** Computational overhead of running detailed physics engine is acceptable or fidelity gain outweighs speed loss.
- **Evidence anchors:** Abstract mentions integration with EnergyPlus for both system and room-level control; Section 2.3.1 describes using EnergyPlus State API in Python environment.
- **Break condition:** Fails if simulation timestep required for stable control is significantly smaller than EnergyPlus zone timestep or Python-to-EnergyPlus communication introduces latency.

### Mechanism 2
- **Claim:** Accepting external signals as observations allows RL agent to internalize grid or community-level constraints.
- **Mechanism:** Architecture includes `ext_obs_bool` flag and user-defined function interface to inject external time-series data into agent's observation vector at runtime.
- **Core assumption:** External signals are predictive of future constraints or rewards, and agent has sufficient memory to associate delayed rewards with immediate external signals.
- **Evidence anchors:** Abstract states ability to accept external signals instead of treating building as stand-alone entity; Section 4.2.1 defines external signal function for demand response.
- **Break condition:** Breaks if external signal is non-stationary beyond agent's exploration capacity or interface timing desynchronizes data stream from simulation clock.

### Mechanism 3
- **Claim:** Standardizing interface between environment and agent via unified configuration module allows rapid decoupling of physics modeling from algorithm selection.
- **Mechanism:** "Configuration Module" separates problem definition from "Algorithm Module" (PPO, TD3, etc.) by mapping observations and actions to generic vectors.
- **Core assumption:** Action and observation spaces are compatible across different algorithm types or wrapper functions can handle translation seamlessly.
- **Evidence anchors:** Abstract mentions easy configuration of RL algorithms and building models; Section 2.2 describes unified configuration interface for testing different RL algorithms.
- **Break condition:** Fails when algorithm-specific requirements are not adequately handled by action function wrapper, leading to shape mismatches or invalid control commands.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - **Why needed here:** Paper relies on framing building control as MDP; understanding reward function definition is critical because agent blindly optimizes reward and will game metrics if not properly defined.
  - **Quick check question:** If I set reward to maximize energy reduction without penalty for temperature deviation, what will agent likely do during cooling demand response event?

- **Concept: On-Policy vs. Off-Policy Learning**
  - **Why needed here:** Paper compares algorithms (PPO/A2C vs. DQN/TD3); off-policy methods handle continuous control and sample efficiency differently than on-policy methods.
  - **Quick check question:** Why might off-policy algorithm (like TD3) be preferred over on-policy algorithm (like PPO) when training on slow, high-fidelity simulator where data collection is expensive?

- **Concept: Thermal Dynamics and Action Latency**
  - **Why needed here:** Buildings have thermal mass meaning actions have delayed effects on target variable; paper notes 2-hour lag in typical day control.
  - **Quick check question:** If simulation timestep is 10 minutes but building thermal mass creates 2-hour lag, how does discount factor influence agent's ability to connect current actions to future rewards?

## Architecture Onboarding

- **Component map:**
  1. Configuration Module: Parses User Input (IDF model, Weather file) → Defines `building_env` (Observations, Actions, Rewards)
  2. Training Module: Initializes EnergyPlus API → Establishes Callbacks → Runs simulation loop
  3. Algorithm Module: Contains Agent (e.g., TD3) → Receives Observation → Outputs Action → Updates Policy via Buffer
  4. Output: Saved Agent (`.pt` file) + Time-series Results (`.csv`)

- **Critical path:**
  1. Verify EnergyPlus installation and `.idf` file compatibility
  2. Define `building_env` class, mapping EnergyPlus sensor keys to observation space
  3. Define Reward Function (Eq. 20: `c - |error| * 10` is good starting point)
  4. Execute training script with built-in algorithm (e.g., TD3)

- **Design tradeoffs:**
  - Fidelity vs. Speed: Uses EnergyPlus (Detailed) rather than RC-models (Reduced-order), improving accuracy but increasing training time
  - Discrete vs. Continuous Control: Paper notes DQN (discrete) and TD3 (continuous) had similar trends, but TD3 was more precise; continuous control preferred for setpoint adjustment

- **Failure signatures:**
  - Simulation Instability: Drastic setpoint changes too quickly may cause EnergyPlus to fail convergence, crashing training loop
  - Reward Hacking: Reward function only targeting energy reduction may cause agent to set temperatures to extremes to minimize load
  - Observation Normalization: Unnormalized inputs may cause neural network gradients to explode, preventing learning

- **First 3 experiments:**
  1. Baseline Validation: Run provided demo case (Small Office in Miami) using Rule-Based Control to establish baseline energy profile
  2. Static Target Control: Train TD3 agent to maintain constant cooling load (e.g., 17kW) using Eq. 20 reward function
  3. External Signal Injection: Enable `ext_obs_bool` and feed synthetic dynamic price signal; modify reward to minimize `Energy * Price`

## Open Questions the Paper Calls Out

- **Open Question 1:** How effectively do control agents trained in BuildingGym transfer to real-world BMS with live data?
  - Basis in paper: Authors state "it is interesting to adapt BuildingGym API to real building BMS... to further test performance of RL in real building control"
  - Why unresolved: Current validation restricted to EnergyPlus simulation engine, which may not capture noise, latency, and faults of physical hardware
  - What evidence would resolve it: Empirical results from deploying BuildingGym-trained agents in physical building using BMS interface

- **Open Question 2:** Can BuildingGym facilitate robust multi-agent control for multi-source energy communities involving stochastic distributed storage like EVs?
  - Basis in paper: Authors highlight "urgent need for standardized environment that can facilitate integration of these multi-source energy communities"
  - Why unresolved: Framework supports external signals but hasn't validated coordination strategies for complex communities with stochastic elements
  - What evidence would resolve it: Successful simulations of community-level coordination between buildings and EV batteries handling variable connection times

- **Open Question 3:** Does superior performance of TD3 algorithm generalize to diverse building types and climate zones?
  - Basis in paper: Paper notes "case studies may not provide definitive comparisons of algorithm performance" and evaluation was limited to single small office building in Miami
  - Why unresolved: Algorithm selection may be highly sensitive to specific thermal dynamics and variability of environment
  - What evidence would resolve it: Comprehensive benchmark of built-in algorithms across varied building archetypes and weather conditions

## Limitations
- Scalability concerns for large-scale simulations and long-term seasonal training due to computational overhead of EnergyPlus API integration
- External signal integration assumes perfect temporal alignment with simulation timesteps, lacking validation under timing mismatches or signal noise
- Algorithm performance generalization beyond the single building type and climate zone evaluated in current validation

## Confidence

- **High Confidence:** Claim that BuildingGym integrates EnergyPlus with RL through State API is well-supported by codebase and experimental validation showing TD3 achieving median control errors of 0.5% and 1%
- **Medium Confidence:** Claim about standardizing interfaces for algorithm flexibility is architecturally sound but lacks extensive empirical validation across diverse algorithm types
- **Low Confidence:** Assertion that framework readily supports grid-interactive control through external signals is theoretically demonstrated but not validated with real-world grid data

## Next Checks

1. **Scalability Test:** Evaluate training performance and wall-clock time when scaling from single-building to multi-building simulations, measuring impact on convergence rates and computational requirements

2. **Robustness to Signal Noise:** Introduce synthetic timing delays and noise into external signal stream to assess whether RL agent maintains performance or learns to exploit spurious correlations

3. **Algorithm Generalization:** Train same building control task across at least five different RL algorithms (including both on-policy and off-policy variants) to empirically validate claim of easy algorithm swapping through unified configuration interface