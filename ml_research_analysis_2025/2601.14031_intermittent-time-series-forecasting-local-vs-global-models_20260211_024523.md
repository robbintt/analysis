---
ver: rpa2
title: 'Intermittent time series forecasting: local vs global models'
arxiv_id: '2601.14031'
source_url: https://arxiv.org/abs/2601.14031
tags:
- time
- series
- distribution
- d-linear
- feed-forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares local and global models for probabilistic forecasting
  of intermittent time series. It evaluates feed-forward neural networks (FNN), D-Linear,
  DeepAR, and transformers with different distribution heads (negative binomial, hurdle-shifted
  negative binomial, Tweedie) against local models (iETS, TweedieGP, in-sample quantiles).
---

# Intermittent time series forecasting: local vs global models

## Quick Facts
- arXiv ID: 2601.14031
- Source URL: https://arxiv.org/abs/2601.14031
- Reference count: 40
- Primary result: D-Linear outperforms local models and provides best accuracy among global models while maintaining low computational costs

## Executive Summary
This paper systematically compares local and global models for probabilistic forecasting of intermittent time series. Through extensive experiments on five real-world datasets containing over 40,000 time series, the study demonstrates that the D-Linear model consistently outperforms both local models (iETS, TweedieGP) and other global architectures (DeepAR, Transformers, FNN). The research particularly emphasizes the importance of distribution heads, showing that Tweedie distributions excel at capturing extreme quantiles while negative binomial distributions provide the best overall performance.

## Method Summary
The study evaluates five global models (FNN, D-Linear, DeepAR, Transformers) and two local models (iETS, TweedieGP) on five intermittent time series datasets. Models are trained using negative log-likelihood loss with Adam optimization and early stopping. Key innovations include proper scaling by non-zero mean values and comparison of three distribution heads (negative binomial, hurdle-shifted negative binomial, Tweedie). The D-Linear architecture uses moving average decomposition to separate trend from remainder signals before linear transformation.

## Key Results
- D-Linear consistently outperforms local models (iETS, TweedieGP) across all datasets
- Transformers-based architectures are computationally expensive and less accurate than simpler global models
- Tweedie distribution heads provide best estimates for highest quantiles (0.99)
- Negative binomial offers overall best performance with lower parameter count
- D-Linear achieves best accuracy among global models while maintaining low computational requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: D-Linear outperforms complex architectures by separating stable trend from sparse remainder signals
- Mechanism: Moving-average kernel decomposes each series into trend and remainder components; each passes through a linear layer before recombination. This reduces noise sensitivity and helps the model focus on the mean signal, critical when observations are mostly zeros
- Core assumption: Intermittent series contain a stable underlying demand component that can be isolated via decomposition
- Evidence anchors: Abstract shows D-Linear best accuracy; section explains denoising effect of moving average kernel; limited corpus support
- Break condition: When series are extremely short or have no stable trend component, decomposition may not add value

### Mechanism 2
- Claim: Transformers underperform because sparse context windows provide insufficient signal for attention mechanisms
- Mechanism: Self-attention relies on relationships across positions; with many zeros and few positive spikes, signal-to-noise ratio is low, leading to unstable training
- Core assumption: Attention mechanisms require sufficient non-zero structure within context window to capture meaningful dependencies
- Evidence anchors: Abstract shows transformers computationally expensive and less accurate; section explains context window contains little signal
- Break condition: When datasets are large and dense, transformers become more stable and competitive

### Mechanism 3
- Claim: Negative binomial provides most reliable overall distribution head, while Tweedie better captures extreme quantiles
- Mechanism: Negative binomial handles overdispersion and zero mass; Tweedie provides heavier tails suitable for extreme-demand spikes
- Core assumption: Predictive distribution must simultaneously model zero-inflation and long upper tails
- Evidence anchors: Abstract shows Tweedie best for highest quantiles, negative binomial overall best; Table 4 shows consistent performance patterns
- Break condition: If target series is real-valued rather than count-valued, Tweedie is required

## Foundational Learning

- Concept: Local vs. global models
  - Why needed here: Understanding when pooling data across series (global) outperforms fitting individual series (local) is central to architectural selection and data requirements
  - Quick check question: Given 500 intermittent series with 20-50 observations each, would you expect a global model to outperform 500 independent local models?

- Concept: Probabilistic forecasting with proper scoring rules
  - Why needed here: Paper evaluates models using quantile loss and RMSSE; must understand why probabilistic outputs matter for inventory decisions
  - Quick check question: Why is quantile loss preferred over MAE for evaluating probabilistic forecasts at high quantiles (0.95, 0.99)?

- Concept: Distribution heads for zero-inflated, overdispersed data
  - Why needed here: Selecting and tuning distribution head (NegBin, Tweedie, HSNB) directly affects tail behavior and calibration
  - Quick check question: A demand series has 70% zeros and occasional large spikes; which distribution property (zero mass, overdispersion, heavy tail) should you prioritize?

## Architecture Onboarding

- Component map: Preprocessing -> Encoder (D-Linear/FNN/DeepAR/Transformers) -> Distribution head -> Training loop -> Evaluation
- Critical path:
  1. Implement correct scaling (mean of non-zero values, not overall mean)
  2. Integrate distribution head with automatic differentiation (PyTorch distributions)
  3. Tune context length (start at 2-4× horizon; performance stabilizes beyond threshold)
- Design tradeoffs:
  - NegBin vs. Tweedie: NegBin has fewer parameters and more stable training; Tweedie for extreme quantiles or real-valued targets
  - Shallow (D-Linear) vs. Deep (DeepAR, Transformers): D-Linear offers lower cost, higher stability, better accuracy
  - Autoregressive sampling vs. multi-horizon output: Autoregressive is costlier (200-10,000 samples needed); multi-horizon is faster but less expressive
- Failure signatures:
  - High variance across training runs → likely transformer; switch to D-Linear
  - Overestimation at high quantiles → distribution tail too light; try Tweedie
  - Poor performance on short series → insufficient data for global model; consider local TweedieGP or in-sample quantiles
  - Scaling artifacts (non-integer predictions from discrete distributions) → incorrect scaling logic
- First 3 experiments:
  1. Baseline comparison: D-Linear with NegBin head vs. in-sample quantiles and TweedieGP on held-out validation split
  2. Distribution head ablation: Compare NegBin, Tweedie, HSNB on D-Linear
  3. Context length sensitivity: Test D-Linear with context lengths at 2×, 4×, 6×, and 8× forecast horizon

## Open Questions the Paper Calls Out

- Question: How do LLM-based forecasting models compare to the global and local models tested in this study for intermittent time series forecasting?
  - Basis: Authors state in Introduction they leave comparison with LLM-based forecasting models for future research
  - Why unresolved: LLM-based forecasting approaches were explicitly excluded from current experiments
  - What evidence would resolve it: Benchmark study comparing state-of-the-art LLM forecasting methods against D-Linear and local baselines on same datasets

- Question: Can extensive hyper-parameter tuning and complex regularization enable deep architectures (Transformers, DeepAR) to outperform the simple D-Linear model in terms of accuracy?
  - Basis: Conclusion notes that complex regularization and hyper-parameter selection could enhance performance of large neural architectures but requires massive computational overhead
  - Why unresolved: Study applied models "off-the-shelf with minimal hyper-parameter tuning"
  - What evidence would resolve it: Experimental results showing accuracy gains of Transformers and DeepAR after rigorous hyper-parameter optimization

- Question: Does the performance of D-Linear model and Tweedie distribution head generalize to real-valued intermittent time series?
  - Basis: Section 2.4 states Tweedie distribution can be only suitable distribution head if intermittent time series is real-valued
  - Why unresolved: Experiments conducted exclusively on count-valued data
  - What evidence would resolve it: Evaluation of D-Linear with Tweedie head on datasets containing continuous non-negative intermittent observations

## Limitations
- Hyperparameter selection process is underspecified, making exact replication challenging
- Transformer architectures tested are standard implementations without modifications specific to intermittent data handling
- Distribution head implementations (particularly HSNB and Tweedie) rely on external references that may have subtle implementation differences
- Evaluation focuses on point forecasts through quantile loss without examining calibration or coverage properties

## Confidence

- High confidence: D-Linear's superiority over local models and computational efficiency relative to transformers (consistent results across five diverse datasets)
- Medium confidence: Mechanism explanation for D-Linear's success through trend decomposition (intuitive but not isolated through controlled experiments)
- Low confidence: Definitive characterization of transformers as "not recommended" for intermittent series (based on single standard implementation)

## Next Checks

1. Ablation study on D-Linear: Implement D-Linear variants without moving average kernel to quantify exact contribution of decomposition mechanism to performance improvements

2. Distribution head calibration: Beyond quantile loss, evaluate probabilistic calibration (coverage rates, reliability diagrams) for NegBin vs Tweedie across all datasets

3. Alternative transformer architectures: Test transformer variants designed for sparse data (sparse attention mechanisms, learned embeddings for zero patterns) to determine if poor performance is inherent to transformers or specific to implementation used