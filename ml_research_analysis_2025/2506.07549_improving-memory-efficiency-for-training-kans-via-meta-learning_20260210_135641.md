---
ver: rpa2
title: Improving Memory Efficiency for Training KANs via Meta Learning
arxiv_id: '2506.07549'
source_url: https://arxiv.org/abs/2506.07549
tags:
- kans
- metakans
- function
- parameters
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory inefficiency of Kolmogorov-Arnold
  Networks (KANs), which have superior function approximation capabilities but require
  significantly more parameters than traditional MLPs due to learnable activation
  functions. The authors propose MetaKANs, a meta-learning framework that generates
  KAN weights through a smaller meta-learner rather than directly optimizing all parameters.
---

# Improving Memory Efficiency for Training KANs via Meta Learning

## Quick Facts
- **arXiv ID:** 2506.07549
- **Source URL:** https://arxiv.org/abs/2506.07549
- **Reference count:** 40
- **Primary result:** MetaKANs reduce KAN trainable parameters by 1/3 to 1/9 while maintaining or improving performance on symbolic regression, PDE solving, and image classification

## Executive Summary
This paper addresses the memory inefficiency of Kolmogorov-Arnold Networks (KANs), which require significantly more parameters than traditional MLPs due to learnable activation functions. The authors propose MetaKANs, a meta-learning framework that generates KAN weights through a smaller meta-learner rather than directly optimizing all parameters. The method uses learnable prompts to identify activation functions and a meta-learner to predict their weights, achieving parameter counts comparable to MLPs while preserving KANs' interpretability and expressiveness. Extensive experiments demonstrate competitive accuracy with substantially fewer parameters across multiple domains.

## Method Summary
MetaKANs replace direct optimization of KAN activation function weights with a meta-learner architecture. Each activation function is identified by a learnable prompt, and a small MLP meta-learner generates the weights based on these prompts. The approach shifts trainable parameters from direct weight optimization to meta-learner parameters and prompts, reducing memory usage by 1/3 to 1/9. For deep networks, layer clustering groups similar layers and assigns dedicated meta-learners to each cluster. The framework is model-agnostic and extends to various KAN variants including FastKAN, ConvKAN, and WavKAN, achieving parameter counts comparable to MLPs while preserving KANs' function approximation capabilities.

## Key Results
- Parameter reduction: 1/3 to 1/9 of original KAN parameters while maintaining accuracy
- Competitive performance: Comparable or superior results on symbolic regression, PDE solving, and image classification
- Memory efficiency: Significant memory savings, particularly for larger networks and higher grid resolutions
- Model-agnostic: Framework extends to FastKAN, ConvKAN, and WavKAN variants

## Why This Works (Mechanism)

### Mechanism 1: Shared Functional Class Exploitation
KAN activation functions belong to common functional families and can share a learned weight generation rule rather than independent optimization. The meta-learner $M_\theta$ maps learnable prompts to activation weights, shifting trainable parameters from direct optimization to meta-learner parameters. This works because univariate functions learned by KANs share sufficient structure that a compact meta-learner can capture their distribution.

### Mechanism 2: Learnable Prompts as Task Identifiers
Scalar or low-dimensional prompt vectors uniquely identify each activation function, enabling the meta-learner to produce function-specific weights. Each edge receives a prompt that the meta-learner conditions on to generate weights. During training, both prompts and meta-learner parameters are optimized jointly, allowing the prompt space to encode meaningful distinctions between activation functions.

### Mechanism 3: Layer Clustering for Distribution Shift
Deep KAN layers exhibit different task distributions; assigning dedicated meta-learners to layer clusters improves capacity while maintaining efficiency. K-means clustering on channel dimensions partitions layers into groups, with each cluster having its own meta-learner. This addresses the mismatch in task distributions across layers while keeping the total number of meta-learners manageable.

## Foundational Learning

- **Kolmogorov-Arnold Representation Theorem:** KANs are built on this theorem, which states any multivariate continuous function decomposes into sums of univariate functions. This explains why KANs use learnable activation functions on edges rather than at nodes like MLPs. Quick check: Can you explain why KANs place learnable functions on edges rather than at nodes like MLPs?

- **B-Spline Basis Functions:** Standard KANs parameterize activation functions as B-splines with learnable coefficients. The grid size $G$ and spline order $k$ directly determine parameter count. Quick check: If $G=10$ and $k=3$, how many learnable coefficients does each activation function require?

- **Hypernetworks/Meta-Learning:** MetaKANs are a form of hypernetwork where a small network generates weights for a larger network. Understanding weight generation vs. direct optimization is central. Quick check: What is the key difference between training weights directly and training a network to generate weights?

## Architecture Onboarding

- **Component map:** Input x → [KAN Layer 0: Φ_{Z^(0),θ}^(0)] → ... → [KAN Layer L-1] → Output, with Meta-learner(s) M_θ and Prompts Z^(0) to Z^(L-1) conditioning each layer's activations

- **Critical path:** 1) Initialize prompts Z randomly, 2) Initialize meta-learner θ randomly, 3) Forward: generate weights w_α^(l) = M_θ(z_α^(l)), compute activations, 4) Compute loss, backpropagate through both KAN and meta-learner, 5) Update θ and Z jointly via gradient descent

- **Design tradeoffs:** Larger d_hidden improves meta-learner capacity but increases fixed cost; more clusters C improves accuracy but adds parameters; scalar prompts minimize parameters while higher dimensions improve accuracy on complex tasks

- **Failure signatures:** Small KANs may have MetaKAN parameters exceed original KAN due to fixed overhead; very deep networks with C=1 show degraded performance; training instability if learning rates are poorly balanced

- **First 3 experiments:** 1) Fit 2D function f(x₁,x₂) = x₁x₂ with [2,2,1] structure, compare MSE and parameters between KAN and MetaKAN with d_hidden=16, 2) On MNIST with [784,32,10], sweep d_hidden and record accuracy vs. parameter count to find knee point, 3) Train 8-layer ConvKAN on CIFAR-10 with C∈{1,3,5,7} meta-learners, plot accuracy vs. C to validate clustering benefit

## Open Questions the Paper Calls Out

### Open Question 1
Can the learned weight prediction rules (meta-learner) transfer to improve memory efficiency for training *novel* KAN tasks (meta-transfer)? The current framework trains the meta-learner for specific tasks but doesn't investigate if a meta-learner trained on one set of functions can generalize to different domains without retraining. Evidence would require experiments showing pre-trained meta-learners accelerate convergence on target KAN tasks.

### Open Question 2
Does the layer clustering strategy based on channel dimensions capture the optimal functional groupings for meta-learners in very deep architectures? While ablation confirms multiple clusters outperform a single shared meta-learner, it doesn't validate if KMeans on channel widths is the best way to group layers. Comparative analysis against alternative grouping strategies would resolve this question.

### Open Question 3
To what extent does the implicit weight generation via the meta-learner affect the *symbolic interpretability* of the resulting KAN compared to direct optimization? While visually cleaner, the indirect mapping might break the strict correspondence required for reliable symbolic regression. A quantitative study measuring complexity and fidelity of symbolic formulas extracted from MetaKANs vs. standard KANs would provide evidence.

## Limitations

- Performance depends heavily on hyperparameter tuning (d_hidden, C, prompt dimension) with limited ablation studies
- Clustering approach for deep networks requires careful calibration to avoid losing efficiency gains
- Generalization to highly diverse activation functions not extensively validated

## Confidence

- **High confidence:** Core memory reduction mechanism is well-specified and mathematically sound
- **Medium confidence:** Performance claims are supported but limited by hyperparameter sensitivity and lack of extensive ablations
- **Low confidence:** Generalization to other functional architectures and highly diverse function families not extensively validated

## Next Checks

1. **Prompt Collapse Detection:** Monitor prompt distribution statistics during training to verify they remain diverse and don't converge to similar values

2. **Memory Profiling Validation:** Measure actual peak GPU memory during training for both KAN and MetaKAN implementations across different sizes

3. **Functional Diversity Test:** Apply MetaKAN to datasets requiring highly diverse activation functions to test break conditions where meta-learner cannot capture sufficient functional diversity