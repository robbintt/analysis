---
ver: rpa2
title: 'Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced
  Hallucination in Large Language Models'
arxiv_id: '2505.00557'
source_url: https://arxiv.org/abs/2505.00557
tags:
- tarot
- hallucination
- elements
- table
- periodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates prompt-induced hallucination in large language
  models (LLMs) by introducing Hallucination-Inducing Prompts (HIPs) that fuse semantically
  distant concepts (e.g., periodic table and tarot) and Hallucination Quantifying
  Prompts (HQPs) to evaluate the resulting outputs. Controlled experiments across
  six LLMs revealed significant variation in hallucination susceptibility, with DeepSeek-R1
  and DeepSeek showing the highest scores (8.2 and 7.73 out of 10) while Gemini2.5Pro
  demonstrated the lowest (3.07 out of 10).
---

# Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models

## Quick Facts
- arXiv ID: 2505.00557
- Source URL: https://arxiv.org/abs/2505.00557
- Authors: Makoto Sato
- Reference count: 0
- Primary result: DeepSeek-R1 and DeepSeek showed highest hallucination susceptibility (8.2 and 7.73/10) while Gemini2.5Pro showed lowest (3.07/10)

## Executive Summary
This study systematically investigates prompt-induced hallucination in large language models by introducing Hallucination-Inducing Prompts (HIPs) that fuse semantically distant concepts and Hallucination Quantifying Prompts (HQPs) to evaluate outputs. Across six LLMs, the research reveals significant variation in hallucination susceptibility, with DeepSeek-R1 scoring highest (8.2/10) and Gemini2.5Pro scoring lowest (3.07/10). The experiments demonstrate that conceptual fusion without logical grounding acts as a key trigger for hallucination, with reasoning-oriented models exhibiting distinct profiles compared to general-purpose ones.

## Method Summary
The study introduces three prompt types: HIPc (confusing conceptual fusion), HIPn (null prompt - non-fusion control), and TIPcs (transition-inducing prompt with logical fusion). Six LLMs were tested using disposable sessions to prevent context leakage. Each model received HIPc/HIPn/TIPcs prompts, with responses evaluated by GPT-o3 using HQP1 scoring (0-10 scale). The experimental design included three independent HIPc responses per model with five HQP evaluations each, plus two independent HIPn/TIPcs responses with five evaluations each. Statistical comparisons used one-tailed Welch's t-tests (p < 0.05).

## Key Results
- HIPs consistently produced more hallucinated responses than null-fusion controls across all tested models
- DeepSeek-R1 and DeepSeek showed highest hallucination susceptibility (8.2 and 7.73/10)
- Gemini2.5Pro demonstrated lowest susceptibility (3.07/10) with epistemic conservatism
- Reasoning-oriented models exhibited distinct hallucination profiles compared to general-purpose ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically incompatible conceptual fusion without grounding triggers hallucination in LLMs
- Mechanism: When prompts structurally fuse unrelated domains (e.g., "periodic table × tarot divination"), LLMs lack cognitive filtering mechanisms to reject illegitimate blends. Instead of discarding illogical combinations, models proceed with speculative or fabricated reasoning to complete the fusion task
- Core assumption: Hallucination arises from the model's attempt to resolve representational conflicts introduced by structurally misleading prompts
- Evidence anchors:
  - [abstract] "HIPs synthetically fuse semantically distant concepts (e.g., periodic table and tarot divination) to stress-test semantic stability"
  - [section] "HIPc (confusing conceptual fusion)" vs "HIPn (Null Prompt - Non-Fusion Type)" showed significant differences (p < 0.01)
  - [corpus] Neighbor paper "The Way We Prompt" operationalizes Conceptual Blending Theory similarly; corpus alignment is moderate (avg FMR=0.35)
- Break condition: If models were tested with fusion prompts that have established domain bridges (e.g., chemistry ↔ alchemy historically), the hallucination differential may diminish or disappear

### Mechanism 2
- Claim: Architecture-specific self-regulation mechanisms—not reasoning orientation alone—determine hallucination resistance
- Mechanism: Models with stricter thresholds for speculative generation (e.g., Gemini2.5Pro) exhibit epistemic conservatism, refusing or qualifying impossible requests. Other architectures (e.g., DeepSeek-R1) may have reasoning enhancements that paradoxically increase susceptibility by generating elaborate justifications for illegitimate fusions
- Core assumption: Hallucination resistance emerges from internal filtering/gating mechanisms rather than reasoning capacity per se
- Evidence anchors:
  - [abstract] "reasoning-oriented models (e.g., GPT-o3, Gemini2.5Pro) showed lower susceptibility than general-purpose ones"
  - [section] Gemini2.5Pro "explicitly framed as non-scientific" with score of 2; DeepSeek-R1 scored 9 with "almost every step fuses an evidence-based scientific system with a mystical tool without offering a verifiable causal bridge"
  - [corpus] Limited direct corpus evidence on architecture-specific filtering; related work focuses on structured reasoning (KDCM) rather than internal gating
- Break condition: If reasoning-oriented models universally reduced hallucination, DeepSeek-R1 would not outscore DeepSeek general-purpose; the observed inversion (p ≈ 0.048) suggests the mechanism is architecture-specific, not category-wide

### Mechanism 3
- Claim: Logically grounded conceptual fusion acts as a stabilizing scaffold, reducing hallucination below even non-fusion baselines
- Mechanism: Prompts that blend concepts with technical/logical coherence (e.g., "aperiodic tilings integrated with traditional crafts") provide representational structure that guides generation toward factually aligned reasoning, reducing semantic conflict
- Core assumption: Hallucination is triggered by the type of fusion and its structural coherence—not merely by the presence of multiple concepts
- Evidence anchors:
  - [abstract] "logically grounded fusions (TIPcs) elicited fewer hallucinations than unstructured ones"
  - [section] "TIPcs responses showed even lower hallucination scores than HIPn (the null-fusion version of HIP)"
  - [corpus] "Mitigating Prompt-Induced Hallucinations via Structured Reasoning" supports structured prompting reducing hallucination
- Break condition: If logical grounding were sufficient, all technically coherent fusions would be safe; however, the paper only tests one TIPcs example, limiting generalization

## Foundational Learning

- Concept: Conceptual Blending Theory (Fauconnier & Turner, 2002)
  - Why needed here: The paper explicitly builds on this cognitive science framework to explain how LLMs fail at reconciling semantically distant domains where humans might succeed through selective projection
  - Quick check question: Can you distinguish between a conceptual blend that produces emergent meaning vs. one that creates epistemic instability?

- Concept: Prompt-induced hallucination (PIH) as distinct subtype
  - Why needed here: This paper proposes PIH as qualitatively different from fact-based hallucinations (misattributed events, fabricated citations); PIH involves fluent speculation lacking domain legitimacy evaluation
  - Quick check question: Given an LLM output that is metaphorically coherent but lacks empirical grounding, would you classify it as PIH or factual hallucination?

- Concept: Stateless/disposable session methodology
  - Why needed here: The experimental design requires decoupling HIP and HQP phases and eliminating session history effects; understanding this is critical for reproducibility
  - Quick check question: Why might reusing a chat session contaminate hallucination measurements?

## Architecture Onboarding

- Component map: HIP (Hallucination-Inducing Prompt) -> HQP (Hallucination Quantifying Prompt) -> Evaluator model (GPT-o3)
- Critical path:
  1. Design HIPc/HIPn/TIPcs with matched token counts (~29-30), sentence structure (2 sentences)
  2. Submit HIP to target model in disposable session (fresh chat, no prior context)
  3. Extract response, insert into HQP template
  4. Submit HQP to evaluator model (GPT-o3) in separate disposable session
  5. Repeat 5× per condition; aggregate scores, compute Welch's t-test
- Design tradeoffs:
  - Using same model family (GPT-o3) for all HQP evaluations ensures consistency but introduces platform dependency
  - 0-10 scoring provides granularity but requires subjective interpretation; human validation not yet performed
  - Single fusion pair (periodic table × tarot) enables controlled comparison but limits domain generalization
- Failure signatures:
  - High variance in HQP scores across trials suggests evaluator instability
  - Evaluator model produces scores without justified examples → prompt may need refinement
  - Target model refuses to answer (as Gemini2.5Pro sometimes did) → score as baseline (2) but document behavior
- First 3 experiments:
  1. Replicate HIPc/HIPn comparison on a new model not in original study (e.g., Claude, Llama) to test framework portability
  2. Test new fusion pairs with varying semantic distance (e.g., "law × mythology" vs. "physics × engineering") to map instability gradients
  3. Add human evaluation alongside HQP scoring on a subset (n=20) to calibrate automatic vs. human judgment alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does prompt-induced hallucination (PIH) generalize across different semantic fusion domains beyond "periodic table × tarot"?
- Basis in paper: [explicit] Discussion Section 1 states: "While 'periodic table × tarot' served as a compelling test case, future work should explore other high-instability combinations—such as 'law and mythology' or 'physics and literature'—to examine whether PIH generalizes across domains"
- Why unresolved: Only one fusion pair was systematically tested; domain-specific semantic distances and conflict structures may yield different hallucination profiles
- What evidence would resolve it: Replicate HIP/HQP experiments across systematically varied fusion pairs spanning scientific, humanistic, and pseudoscientific domain combinations

### Open Question 2
- Question: Can internal model signals (logit distributions, attention patterns, semantic entropy) predict or correlate with externally observed hallucination scores?
- Basis in paper: [explicit] Discussion Section 4 proposes: "capturing logit distributions and attention patterns during generation" to compute "semantic entropy, confidence variance, or token-level perplexity" and correlate with HIP/HQP scores
- Why unresolved: Current study relies solely on external behavioral measurements (HQP evaluations); no internal neural signatures have been examined
- What evidence would resolve it: Deploy locally hosted LLMs to capture internal activations during HIP responses and statistically correlate with HQP-derived hallucination scores

### Open Question 3
- Question: Do HQP-derived hallucination scores align with human expert judgments of hallucination?
- Basis in paper: [explicit] Discussion Section 4 states: "validating HQP scores against human judgments will be essential to establish their external interpretability and trustworthiness"
- Why unresolved: GPT-o3 served as the sole HQP evaluator; whether LLM-based scoring reflects human perception of hallucination remains unverified
- What evidence would resolve it: Collect human expert ratings on HIP responses and correlate with GPT-o3 HQP scores to establish convergent validity

### Open Question 4
- Question: Which specific dimensions of conceptual conflict (semantic distance, logical incompatibility, domain authority asymmetry) most strongly drive hallucination severity?
- Basis in paper: [explicit] Discussion Section 1 states: "Varying fusion pairs systematically may also help identify which dimensions of conceptual conflict most strongly drive hallucination"
- Why unresolved: HIPc vs. HIPn comparison shows fusion matters, but the structural properties of fusion that trigger instability remain underspecified
- What evidence would resolve it: Design factorial experiments varying semantic distance, epistemic authority, and logical compatibility independently while measuring hallucination scores

## Limitations
- Limited domain generalizability: The study tests only one conceptual fusion pair (periodic table × tarot), leaving uncertainty about whether results hold across diverse semantic distances and domains
- Single evaluator dependency: Using GPT-o3 for all HQP evaluations introduces potential systematic bias and limits reproducibility without access to identical model versions
- Undefined generation parameters: Temperature, top_p, and other generation settings were not specified, potentially affecting hallucination susceptibility across models
- Architecture-specific findings: DeepSeek-R1's higher hallucination score than DeepSeek general-purpose (p ≈ 0.048) suggests model-specific behaviors that may not generalize to other model families

## Confidence
- **High confidence**: HIPs consistently produce more hallucinations than non-fusion controls across all tested models; DeepSeek-R1 and DeepSeek show highest hallucination susceptibility
- **Medium confidence**: Reasoning-oriented models show distinct hallucination profiles; Gemini2.5Pro demonstrates lowest susceptibility with epistemic conservatism
- **Low confidence**: TIPcs universally reduce hallucination below non-fusion baselines; semantic distance directly correlates with hallucination magnitude

## Next Checks
1. **Cross-domain validation**: Test HIP framework with 3-5 new fusion pairs spanning different semantic distances (e.g., law × mythology, physics × engineering, chemistry × poetry) to establish instability gradients
2. **Human evaluator calibration**: Run parallel human evaluation on 20 randomly selected responses to compare against HQP scores and establish inter-rater reliability
3. **Generation parameter sensitivity**: Systematically vary temperature (0.1, 0.5, 0.9) and top_p (0.5, 0.9, 1.0) across models to quantify their impact on hallucination susceptibility