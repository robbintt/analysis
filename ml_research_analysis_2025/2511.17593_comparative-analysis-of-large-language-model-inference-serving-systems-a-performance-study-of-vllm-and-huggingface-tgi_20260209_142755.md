---
ver: rpa2
title: 'Comparative Analysis of Large Language Model Inference Serving Systems: A
  Performance Study of vLLM and HuggingFace TGI'
arxiv_id: '2511.17593'
source_url: https://arxiv.org/abs/2511.17593
tags:
- vllm
- memory
- throughput
- latency
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides a comprehensive performance comparison of two
  leading LLM serving frameworks, vLLM and HuggingFace TGI, across multiple model
  sizes and workload patterns. Using LLaMA-2 models (7B, 13B, 70B) on NVIDIA A100
  GPUs, the research evaluates throughput, latency, GPU utilization, and memory efficiency
  under realistic deployment scenarios.
---

# Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI

## Quick Facts
- arXiv ID: 2511.17593
- Source URL: https://arxiv.org/abs/2511.17593
- Authors: Saicharan Kolluru
- Reference count: 10
- Primary result: vLLM achieves 2-24x higher throughput than TGI under high-concurrency workloads due to PagedAttention mechanism

## Executive Summary
This study provides a comprehensive performance comparison of two leading LLM serving frameworks, vLLM and HuggingFace TGI, across multiple model sizes and workload patterns. Using LLaMA-2 models (7B, 13B, 70B) on NVIDIA A100 GPUs, the research evaluates throughput, latency, GPU utilization, and memory efficiency under realistic deployment scenarios. The findings demonstrate that vLLM's novel PagedAttention mechanism enables superior throughput and memory efficiency, while TGI excels at time-to-first-token for interactive applications.

## Method Summary
The study benchmarks vLLM v0.6.1 and TGI v2.3.0 using LLaMA-2 models (7B, 13B, 70B) in FP16 on 4x NVIDIA A100 80GB GPUs with CUDA 12.1 and PyTorch 2.1.0. Workloads use ShareGPT dataset prompts with Poisson-distributed lengths (mean=512 for prompts, 256 for outputs), evaluated across concurrency levels 1-200. Performance metrics include throughput (tokens/sec), TTFT, TPOT, latency percentiles, and GPU memory/utilization, with 100 warmup and 1,000 measurement requests per run, median of three runs.

## Key Results
- vLLM achieves 2-24x higher throughput than TGI under high-concurrency workloads through PagedAttention mechanism
- TGI shows 1.3-2x lower TTFT for interactive scenarios but suffers from earlier throughput saturation
- vLLM demonstrates 19-27% lower memory consumption and 85-92% GPU utilization versus TGI's 68-74%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PagedAttention's non-contiguous memory allocation enables 19-27% memory savings, which directly translates to larger batch sizes and 2-24x higher throughput under high concurrency.
- Mechanism: KV cache blocks are allocated on-demand in non-contiguous pages (similar to OS virtual memory), eliminating pre-allocation for maximum sequence length. This removes internal fragmentation when actual sequences are shorter than the maximum, and enables KV cache sharing across requests with common prefixes.
- Core assumption: Memory fragmentation, not compute, is the primary bottleneck under concurrent load—a claim supported by TGI's GPU utilization capping at 68-74% despite available compute capacity.
- Evidence anchors:
  - [abstract] "vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism"
  - [Section 5.1.1] "Eliminating fragmentation: Non-contiguous memory allocation means no wasted space between variable-length sequences"
  - [corpus] FlashInfer paper addresses customizable attention engines, suggesting ongoing optimization of non-standard memory access patterns; corpus evidence on PagedAttention specifically is limited.
- Break condition: If prompt lengths are uniformly long (near max context) with little variance, fragmentation benefits diminish. If compute-bound rather than memory-bound (e.g., smaller models on H100s), throughput gains may narrow.

### Mechanism 2
- Claim: Continuous batching (iteration-level scheduling) maximizes GPU utilization but introduces per-request latency variance; batch-level scheduling with timeout windows produces more predictable TTFT at the cost of throughput.
- Mechanism: vLLM adds new requests to the batch immediately as slots free (iteration-level), keeping GPU fed. TGI uses configurable timeout windows (batch-level), which introduces deliberate waiting to batch more predictably but leaves GPU underutilized during wait periods.
- Core assumption: The scheduling overhead of continuous batching is negligible compared to the utilization gains—a claim conditionally supported by vLLM's 85-92% utilization figures.
- Evidence anchors:
  - [Section 5.1.2] "vLLM: Higher throughput, variable per-request latency; TGI: More consistent latency, lower throughput"
  - [Section 4.2] TGI exhibits 1.3-2x lower TTFT at p50, but vLLM shows 1.5-1.7x better p99 total latency
  - [corpus] AugServe and EWSJF papers address adaptive scheduling for mixed workloads, confirming scheduling strategy as an active research area with no single optimal solution.
- Break condition: If request arrival is bursty rather than sustained, timeout-based batching may actually outperform continuous batching by amortizing scheduling overhead. If strict SLAs require bounded p99 TTFT, vLLM's aggressive batching may violate constraints.

### Mechanism 3
- Claim: Memory management strategy determines throughput saturation point under increasing concurrency—vLLM scales linearly to 100-150 concurrent requests; TGI saturates at 50-75.
- Mechanism: As concurrency increases, KV cache memory demand grows. vLLM's efficient allocation delays memory exhaustion, extending linear scaling. TGI's pre-allocation exhausts memory earlier, forcing requests to queue and throughput to plateau.
- Core assumption: The workload has sufficient request density to reach these concurrency levels, and the model fits in memory with room for KV cache—both conditions met in the experimental setup but may not hold in resource-constrained deployments.
- Evidence anchors:
  - [Section 4.4] "vLLM: Throughput increases linearly up to 100-150 concurrent requests before plateauing; TGI: Shows earlier saturation (50-75 concurrent requests)"
  - [Section 4.3.2] Peak memory: LLaMA-2-7B uses 24.3GB (vLLM) vs 31.7GB (TGI) at 50 concurrent requests
  - [corpus] Joint Encoding of KV-Cache Blocks paper addresses KV cache scaling, implicitly confirming memory as the scalability bottleneck; corpus does not directly replicate the saturation curves.
- Break condition: With tensor parallelism across more GPUs, memory pressure decreases and saturation points shift right. With quantization (INT8/INT4), both systems would show different saturation patterns—these conditions were not tested.

## Foundational Learning

- Concept: **KV Cache and its growth dynamics**
  - Why needed here: The entire performance comparison hinges on how each system manages the KV cache, which stores attention states for previously generated tokens and grows linearly with sequence length. Without understanding this, PagedAttention's benefits are opaque.
  - Quick check question: For a 70B model generating 2048 tokens, the paper states the KV cache requires ~140GB per request. Can you explain why this scales with sequence length but not model depth?

- Concept: **Autoregressive generation and its batching implications**
  - Why needed here: LLM requests complete at different times due to varying generation lengths, unlike vision models where batching is straightforward. This explains why "continuous batching" is meaningful—requests can join mid-generation.
  - Quick check question: Why does autoregressive generation make traditional static batching inefficient for variable-length outputs?

- Concept: **Time-to-First-Token (TTFT) vs. Time-per-Output-Token (TPOT)**
  - Why needed here: The paper's key finding is that TGI wins on TTFT while vLLM wins on throughput and tail latency. These metrics capture different user experiences—initial responsiveness vs. streaming speed.
  - Quick check question: A chatbot user sends a prompt and waits. Which metric (TTFT or TPOT) most directly affects their perception of "the AI started answering quickly"?

## Architecture Onboarding

- Component map:
  - **vLLM**: PagedAttention kernel (non-contiguous KV cache access) → Continuous batching scheduler → Block manager (virtual memory-style allocation) → Custom CUDA kernels optimized for scattered memory access
  - **TGI**: Contiguous KV cache pre-allocation → Dynamic batching with timeout windows → Quantization support (GPTQ, AWQ, EETQ) → Production monitoring and distributed inference layer
  - **Shared**: Model loader, tokenizer, HTTP/gRPC API layer, CUDA runtime

- Critical path: For vLLM, the PagedAttention kernel and block manager are the performance-defining components—any regression here directly impacts the 19-27% memory advantage. For TGI, the batching scheduler and its timeout configuration determine the latency/throughput trade-off profile.

- Design tradeoffs:
  - Memory efficiency (vLLM) vs. deployment simplicity and ecosystem integration (TGI)
  - Maximum throughput under load (vLLM) vs. predictable p50 TTFT for interactive use (TGI)
  - Non-contiguous memory access requiring custom kernels (vLLM) vs. quantization breadth (TGI)
  - Linear scaling under high concurrency (vLLM) vs. fairness and latency consistency (TGI)

- Failure signatures:
  - vLLM shows high throughput but p99 TTFT violations under mixed short/long request workloads (aggressive batching starves late arrivals)
  - TGI shows early throughput saturation with GPU utilization stuck at ~70% despite queue depth (memory fragmentation limiting batch size)
  - Both systems show degraded performance with 70B model if tensor parallelism is misconfigured

- First 3 experiments:
  1. **Replicate the memory saturation curve**: Run LLaMA-2-7B with both frameworks at concurrency levels [10, 25, 50, 75, 100, 150], plotting throughput and GPU memory. Expect vLLM to continue scaling past 75 while TGI plateaus. This validates your setup matches the paper.
  2. **TTFT vs. throughput trade-off sweep**: For TGI, vary the batch timeout window (e.g., 10ms, 50ms, 100ms, 500ms) and measure both TTFT and throughput. This maps the configurable trade-off space not fully explored in the paper.
  3. **KV cache sharing microbenchmark**: Send requests with shared prompt prefixes (e.g., few-shot examples) to vLLM and measure memory consumption vs. unique prompts. This tests the "KV cache sharing" claim in Section 5.1.1, which the paper asserts but does not directly benchmark.

## Open Questions the Paper Calls Out

- **Quantization Impact**: The paper explicitly states it did not evaluate quantized models (INT8, INT4), which significantly affect memory and performance, and notes TGI includes extensive quantization support (GPTQ, AWQ, EETQ).

- **Architecture Generalization**: The authors acknowledge focusing on LLaMA-2 models and note performance characteristics may differ for other architectures (Mistral, Falcon, GPT variants).

- **Long-Context Scenarios**: The paper identifies long-context scenarios (32K+ tokens) and structured generation (JSON, code) as warranting separate analysis.

- **Hybrid Deployment Strategies**: Section 5.3 briefly suggests hybrid strategies (request routing, time-based switching, model-specific choices) but provides no evaluation of their effectiveness.

## Limitations

- Benchmark focuses exclusively on FP16 precision, omitting quantization formats that would significantly alter memory consumption patterns
- Workload characterization using Poisson-distributed prompt and output lengths may not reflect real-world power-law distributions
- Experimental setup uses homogeneous 4x A100 80GB nodes, while real deployments frequently mix GPU types and generations

## Confidence

- **High Confidence**: Throughput measurements and GPU utilization comparisons are reproducible given the specified hardware and software versions
- **Medium Confidence**: Memory efficiency claims (19-27% savings) and saturation points (50-75 concurrent requests for TGI, 100-150 for vLLM) depend on specific workload characteristics
- **Low Confidence**: Architectural attribution of performance differences to PagedAttention's non-contiguous memory allocation is mechanistically plausible but not definitively proven

## Next Checks

1. **Quantization Impact Study**: Replicate the benchmark with LLaMA-2-7B using INT8 and INT4 quantization in both frameworks to validate whether memory efficiency advantage persists when both systems leverage reduced precision

2. **Request Pattern Sensitivity Analysis**: Modify benchmark to use power-law distribution for prompt lengths (matching typical production workloads) and measure impact on throughput saturation points and TTFT distributions

3. **Mixed Precision Stress Test**: Configure both frameworks to use mixed precision (FP16 with FP8 attention) and measure memory consumption, throughput, and accuracy degradation to determine if memory fragmentation advantage is fundamental or artifact of FP16-only design