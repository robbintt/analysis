---
ver: rpa2
title: 'Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer'
arxiv_id: '2512.14585'
source_url: https://arxiv.org/abs/2512.14585
tags:
- nepali
- language
- training
- text
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addressed the lack of effective generative NLP models
  for Nepali, a low-resource language with complex grammar and limited high-quality
  corpora. A GPT-2-based language model was developed using a custom 16k Byte-Pair
  Encoding (BPE) tokenizer trained exclusively on Nepali text to improve segmentation
  and input representation.
---

# Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer

## Quick Facts
- **arXiv ID**: 2512.14585
- **Source URL**: https://arxiv.org/abs/2512.14585
- **Reference count**: 21
- **Primary result**: GPT-2-based model trained on Nepali text achieves perplexity of 21.80 using custom 16k BPE tokenizer and FlashAttention

## Executive Summary
This study addresses the challenge of developing effective generative NLP models for Nepali, a low-resource language with complex grammar and limited high-quality corpora. The researchers developed a GPT-2-based language model using a custom 16k Byte-Pair Encoding (BPE) tokenizer trained exclusively on Nepali text, combined with FlashAttention integration for memory efficiency. After training on a 10.75GB cleaned corpus for two epochs, the model achieved a training loss of 3.168177, validation loss of 3.081982, and perplexity of 21.80, demonstrating its ability to generate coherent Nepali news-style text.

## Method Summary
The researchers assembled a 10.75GB cleaned corpus combining the NepBERTa dataset with web-scraped Nepali news articles from four major news sites. They trained a custom 16k BPE tokenizer using SentencePiece on an 8-million character sample to ensure consistent Nepali subword segmentation. The GPT-2 model (12 layers, 12 heads, 768 embedding dimension, ~98M parameters) was trained with micro-batches of 8, gradient accumulation over 64 steps (effective batch 524,288 tokens), bfloat16 precision, AdamW optimizer, and FlashAttention for memory efficiency. The training used a 715-step warmup followed by cosine decay learning rate schedule with weight decay of 0.1.

## Key Results
- Achieved training loss of 3.168177 and validation loss of 3.081982 after two epochs
- Final perplexity of 21.80 indicates strong text generation capability
- Demonstrated stable training without overfitting (validation loss < training loss)
- Generated coherent Nepali news-style text with appropriate grammar and syntax

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific BPE tokenizer improves subword segmentation for morphologically complex, low-resource languages
- Mechanism: Training a 16k vocabulary exclusively on Nepali text produces subword units aligned with Nepali morphology and Devanagari script, reducing fragmentation compared to multilingual tokenizers
- Core assumption: Nepali text contains consistent subword patterns that BPE can capture, and monolingual vocabulary suffices for target domains
- Evidence anchors: Custom BPE tokenizer trained on 8M character sample; smaller vocabulary decreases embedding matrix size and computational overhead

### Mechanism 2
- Claim: FlashAttention enables stable training with large effective batch sizes under GPU memory constraints
- Mechanism: FlashAttention computes attention in tiled blocks rather than materializing full attention matrices, reducing memory from O(n²) while preserving numerical equivalence
- Core assumption: The IO-aware tiling strategy provides sufficient numerical precision for Nepali language modeling at the specified scale
- Evidence anchors: FlashAttention integrated to reduce memory usage and stabilize training; significantly reduces memory usage and increases throughput

### Mechanism 3
- Claim: GPT-3-inspired training schedules stabilize convergence for low-resource language pretraining
- Mechanism: 715-step warmup prevents early gradient instability; cosine decay provides smooth learning rate reduction; weight decay (0.1) regularizes against overfitting on the limited 10.75GB corpus
- Core assumption: Hyperparameters transfer from English-centric GPT-3 settings to Nepali without extensive tuning
- Evidence anchors: Learning rate warm-up applied for first 715 steps; weight decay set to 0.1 to prevent overfitting; model achieved stable learning without signs of overfitting

## Foundational Learning

- **Concept: Byte-Pair Encoding (BPE) tokenization**
  - Why needed here: Understanding how subword units are learned from corpus statistics and why vocabulary size affects both coverage and efficiency
  - Quick check question: Given a Nepali corpus, would a 16k vocabulary likely over-segment rare words or under-segment common words compared to a 50k vocabulary?

- **Concept: Gradient accumulation for effective batch size**
  - Why needed here: The paper uses micro-batches of 8 with 64 accumulation steps to simulate large-batch training—essential for understanding how loss is computed and scaled
  - Quick check question: If gradient accumulation steps are increased from 64 to 128 without adjusting learning rate, what happens to training dynamics?

- **Concept: Perplexity as language model evaluation**
  - Why needed here: The paper reports perplexity of 21.80 as the primary quality metric; understanding its relationship to cross-entropy loss is necessary to interpret results
  - Quick check question: If perplexity decreases from 232 to 21.8, what does this indicate about the model's probability distribution over tokens?

## Architecture Onboarding

- **Component map**: SentencePiece BPE tokenizer → tokenized NumPy shards → GPT-2 decoder-only model → AdamW optimizer → FlashAttention computation → bfloat16 training

- **Critical path**: 
  1. Corpus cleaning (regex filtering, deduplication) → 10.75GB clean text
  2. Tokenizer training (8M character sample, 16k vocab)
  3. Corpus tokenization → 87 shards
  4. Model training (2 epochs, 3300 steps, warmup + cosine decay)
  5. Evaluation (loss/perplexity tracking)

- **Design tradeoffs**:
  - Vocabulary size (16k vs 50k): Smaller vocab reduces parameters but may increase sequence length for rare words
  - Corpus composition: NepBERTa provides scale; web-scraped news adds contemporary vocabulary but introduces cleaning complexity
  - Training duration: 2 epochs limit overfitting risk but may underutilize data for generalization

- **Failure signatures**:
  - Validation loss exceeds training loss significantly → overfitting (not observed: 3.082 val < 3.168 train)
  - Perplexity plateaus early → learning rate too high or data quality issues
  - Tokenizer produces excessive single-character tokens → vocabulary too small or corpus insufficient

- **First 3 experiments**:
  1. Replicate baseline training (2 epochs, current hyperparameters) and verify loss curve matches reported values (3.168 train, 3.082 val)
  2. Ablate tokenizer vocabulary size (8k, 32k, 64k) while holding other factors constant; measure impact on perplexity and average tokens per sentence
  3. Extend training to 4-6 epochs with early stopping; monitor for overfitting and assess whether perplexity continues to improve or stabilizes

## Open Questions the Paper Calls Out

- **Question**: Does fine-tuning the pretrained model on instruction-based datasets significantly improve its ability to perform specific, task-oriented generation?
  - Basis in paper: The conclusion states that "fine-tuning the model on instruction-based datasets may improve its ability to follow specific instructions and generate task-oriented Nepali text more effectively"
  - Why unresolved: The current study focuses exclusively on pretraining and calculating perplexity; no experiments were conducted using instruction-tuning datasets to verify this hypothesized improvement
  - What evidence would resolve it: A comparative evaluation of the base model against an instruction-tuned variant on a Nepali instruction-following benchmark or via human evaluation of task completion

- **Question**: To what extent does scaling the corpus beyond news articles improve the model's understanding of diverse terminologies and non-news contexts?
  - Basis in paper: The authors note in the conclusion that "further enhancements can be achieved by using larger and more diverse datasets," specifically to help the model "understand varied terminologies and contexts"
  - Why unresolved: The training data primarily comprised the NepBERTa corpus and web-scraped news, potentially limiting the model's effectiveness in technical, colloquial, or literary domains
  - What evidence would resolve it: Training the model on an expanded corpus including non-news domains and evaluating performance on domain-specific perplexity or downstream tasks

- **Question**: Does the reported low perplexity (21.80) correlate with superior performance on downstream NLP tasks compared to existing encoder models?
  - Basis in paper: The evaluation section relies heavily on cross-entropy loss and perplexity to demonstrate "strong performance," but does not benchmark the model against existing baselines (like NepBERTa) on specific downstream tasks
  - Why unresolved: While low perplexity indicates coherent text generation, it does not necessarily guarantee competitive performance on practical applications like sentiment analysis, named entity recognition, or question answering without explicit fine-tuning and testing
  - What evidence would resolve it: Fine-tuning the model on standard downstream tasks and comparing its accuracy/F1 scores against existing monolingual and multilingual baselines

## Limitations

- Web-scraped news corpus composition and scraping methodology are not detailed, raising questions about temporal coverage and potential domain bias
- FlashAttention's specific contribution to training stability is assumed but not experimentally isolated
- Hyperparameter choices (particularly learning rate schedule and weight decay) follow GPT-3 patterns without Nepali-specific tuning
- The 2-epoch training duration appears arbitrary with no analysis justifying this choice versus longer training with early stopping

## Confidence

- **High confidence**: Custom BPE tokenizer training methodology is clearly specified and reproducible; 16k vocabulary size decision is justified by memory/compute constraints
- **Medium confidence**: Training loss and perplexity metrics are verifiable from the described procedure; reported values appear internally consistent
- **Low confidence**: Claims about FlashAttention's specific contribution to training stability cannot be verified without ablation studies; 2-epoch choice lacks empirical justification

## Next Checks

1. **Ablation study on FlashAttention**: Train the model with and without FlashAttention while holding all other variables constant. Measure GPU memory usage, training time per step, and final perplexity to isolate FlashAttention's contribution to the reported performance gains.

2. **Extended training duration analysis**: Continue training from the reported checkpoint for 2-4 additional epochs with early stopping based on validation loss. Compare perplexity trajectories and text generation quality to determine if 2 epochs was optimal or suboptimal.

3. **Tokenizer vocabulary size sensitivity**: Retrain the BPE tokenizer with vocabulary sizes of 8k, 32k, and 64k using identical corpus samples. Measure average tokens per sentence, OOV rates on held-out text, and downstream perplexity to establish the sensitivity of performance to tokenizer capacity.