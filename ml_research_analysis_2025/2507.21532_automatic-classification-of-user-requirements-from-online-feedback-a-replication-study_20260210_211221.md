---
ver: rpa2
title: Automatic Classification of User Requirements from Online Feedback -- A Replication
  Study
arxiv_id: '2507.21532'
source_url: https://arxiv.org/abs/2507.21532
tags:
- baseline
- dataset
- replication
- reviews
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This replication study evaluated the reproducibility of deep learning
  models for classifying user requirements from online feedback, extending prior work
  by testing generalizability on external datasets and comparing performance with
  GPT-4o zero-shot classification. Using the baseline dataset and an additional external
  dataset, the study found varying reproducibility levels across models, with Naive
  Bayes achieving perfect reproducibility while BERT and others showed mixed results.
---

# Automatic Classification of User Requirements from Online Feedback -- A Replication Study

## Quick Facts
- arXiv ID: 2507.21532
- Source URL: https://arxiv.org/abs/2507.21532
- Reference count: 26
- Primary result: Replication study evaluating reproducibility of deep learning models for classifying user requirements from online feedback, finding varying reproducibility levels across models with BERT and ELMo showing good generalization on external datasets

## Executive Summary
This replication study investigates the reproducibility of deep learning models for classifying user requirements from online feedback, extending prior work by testing generalizability on external datasets and comparing performance with GPT-4o zero-shot classification. Using both the original baseline dataset and an additional external dataset, the study found varying reproducibility levels across models, with Naive Bayes achieving perfect reproducibility while BERT and others showed mixed results. The research also introduces replication study ID-cards to enhance transparency and replicability in software engineering research.

## Method Summary
The study employed a multi-faceted approach to evaluate model reproducibility and generalizability. Researchers first replicated the original study's experimental setup using the baseline dataset, then tested each model's performance on an external dataset to assess generalizability. They compared traditional machine learning approaches (Naive Bayes, SVM, Random Forest) with deep learning models (BERT, ELMo) and evaluated GPT-4o's zero-shot classification performance. The study created replication study ID-cards documenting environment setup and methodology to address transparency gaps in the original research.

## Key Results
- Naive Bayes achieved perfect reproducibility across all experimental conditions
- BERT and ELMo demonstrated good generalization capabilities on external dataset, outperforming traditional ML approaches
- GPT-4o showed performance comparable to baseline machine learning models but did not surpass fine-tuned deep learning models

## Why This Works (Mechanism)
None

## Foundational Learning

**Naive Bayes Classification**
- Why needed: Probabilistic approach for text classification that provides baseline performance and perfect reproducibility in this study
- Quick check: Verify probability calculations and independence assumptions hold for requirement classification

**BERT (Bidirectional Encoder Representations from Transformers)**
- Why needed: Deep learning model that captures contextual information from user feedback, enabling better generalization to external datasets
- Quick check: Confirm attention mechanisms properly weight different aspects of user requirements

**Zero-shot Classification with GPT-4o**
- Why needed: Evaluates large language model capabilities without fine-tuning, providing comparison point for specialized models
- Quick check: Test different prompt formulations to optimize classification performance

## Architecture Onboarding

**Component Map**
Baseline dataset -> Model training -> External dataset validation -> GPT-4o comparison -> ID-card documentation

**Critical Path**
Data preparation -> Model training/replication -> Performance evaluation on external dataset -> Zero-shot GPT-4o evaluation -> Documentation creation

**Design Tradeoffs**
- Traditional ML (Naive Bayes) vs Deep Learning (BERT, ELMo): Simplicity and reproducibility versus contextual understanding and generalization
- Fine-tuned models versus zero-shot GPT-4o: Specialized performance versus general language understanding
- Single external dataset versus multiple datasets: Manageable scope versus comprehensive generalizability assessment

**Failure Signatures**
- Poor reproducibility indicates missing implementation details or environment dependencies
- Weak external dataset performance suggests overfitting to baseline dataset characteristics
- GPT-4o underperformance may indicate requirement-specific language patterns not captured by general models

**First Experiments**
1. Replicate original study using baseline dataset to verify reproducibility claims
2. Test each model on external dataset to assess generalization capabilities
3. Evaluate GPT-4o with various prompt engineering approaches to optimize zero-shot performance

## Open Questions the Paper Calls Out
None

## Limitations
- Modest size of external dataset may not fully represent diversity of online feedback across software domains
- GPT-4o performance evaluation did not explore prompt engineering variations that could improve results
- ID-card approach for transparency was innovative but not empirically validated for improving replicability

## Confidence
High: Perfect reproducibility achieved with Naive Bayes
Medium: Mixed reproducibility results for BERT and other deep learning models
Low: GPT-4o performance claims without prompt engineering optimization

## Next Checks
1. Conduct additional experiments using multiple external datasets from diverse software domains to strengthen generalizability claims and validate observed performance differences between model types.

2. Perform systematic prompt engineering experiments with GPT-4o and other large language models to determine optimal zero-shot classification performance and establish whether prompt variations could close the performance gap with fine-tuned models.

3. Implement longitudinal testing by collecting new online feedback data over time to assess model performance stability and potential concept drift, validating the long-term utility of proposed classification approaches.