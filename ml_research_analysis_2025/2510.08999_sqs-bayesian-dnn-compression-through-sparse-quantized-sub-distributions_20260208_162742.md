---
ver: rpa2
title: 'SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions'
arxiv_id: '2510.08999'
source_url: https://arxiv.org/abs/2510.08999
tags:
- weights
- weight
- compression
- quantization
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SQS, a Bayesian framework for simultaneous
  pruning and quantization of deep neural networks via variational learning. The key
  innovation is a spike-and-slab prior combined with a Gaussian Mixture Model (GMM)
  to induce sparsity and enable low-bit precision.
---

# SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions

## Quick Facts
- **arXiv ID:** 2510.08999
- **Source URL:** https://arxiv.org/abs/2510.08999
- **Reference count:** 40
- **Primary result:** SQS achieves up to 32× compression rates with <1.3% accuracy drop on ResNet and <1.66% F1 drop on BERT-base

## Executive Summary
SQS introduces a Bayesian framework for simultaneous pruning and quantization of deep neural networks through variational learning. The method employs a spike-and-slab prior combined with Gaussian Mixture Models (GMMs) to induce sparsity while enabling low-bit precision quantization. By optimizing these objectives jointly rather than sequentially, SQS achieves higher compression rates than traditional "prune-then-quantize" approaches while maintaining comparable accuracy. The framework is validated across diverse architectures including ResNet, BERT-base, Llama3.2-1B, and Qwen2.5-0.5B models.

## Method Summary
SQS treats DNN compression as a Bayesian inference problem where weights are modeled as random variables following a spike-and-GMM variational distribution. The "spike" component (Dirac delta at zero) induces sparsity through Bernoulli gates, while the GMM "slab" enables quantization by forcing non-zero weights toward discrete modes. The method optimizes an approximate ELBO that combines log-likelihood, KL divergence for sparsity, and KL divergence for quantization. Key innovations include outlier-aware windowing for long-tailed distributions in LLMs and layer-wise quantization with K-means initialization of GMM parameters.

## Key Results
- Achieves up to 32× compression rates on ResNet-18 with only 0.73% accuracy drop
- Outperforms baselines like DGMS, AWQ, and TTQ on compression-accuracy trade-offs
- Maintains <1.66% F1 score drop on BERT-base while achieving 16× compression
- Effective across diverse architectures: ResNet, BERT, Llama3.2-1B, and Qwen2.5-0.5B

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Unified distribution modeling enables simultaneous pruning and quantization, achieving higher compression rates than sequential pipelines.
**Mechanism:** The spike-and-GMM variational distribution combines a Bernoulli gate (spike) that forces weights toward exact zero and a GMM (slab) that forces remaining weights toward discrete modes. Joint optimization avoids error accumulation from separate pruning and quantization steps.
**Core assumption:** Weight importance and optimal quantization centroids are interdependent; determining them in isolation yields sub-optimal solutions.
**Evidence anchors:** Abstract states the spike-and-GMM approach; Eq. 6 defines the marginal variational distribution combining Bernoulli gate and GMM.
**Break condition:** Fails if gradient signal cannot effectively update sparse gating parameters due to conflicting optimization pressures.

### Mechanism 2
**Claim:** Tractable variational inference is achieved by upper-bounding the intractable KL divergence between GMM slab and prior.
**Mechanism:** The exact KL between complex GMM variational posterior and simple Gaussian prior is intractable. SQS uses an approximate ELBO that bounds the KL term using the dominant Gaussian component and Bernoulli KL.
**Core assumption:** GMM components are sufficiently sharp and separated that the dominant component characterizes local geometry for regularization.
**Evidence anchors:** Eq. 8 shows the approximated objective; Appendix A.2 derives the approximation using Jensen's inequality.
**Break condition:** High temperature causes GMM components to overlap significantly, making the dominant component approximation unstable.

### Mechanism 3
**Claim:** Outlier-aware windowing preserves critical weight magnitudes in LLMs, preventing accuracy collapse in long-tailed distributions.
**Mechanism:** Standard quantization clusters most weights in the center, ignoring significant outliers. SQS uses IQR-based windowing to isolate outliers into separate groups before applying GMM, ensuring high-magnitude weights are preserved as distinct centroids.
**Core assumption:** High-magnitude weights act as critical "salient features" or activation peaks that cannot be rounded down without significant performance loss.
**Evidence anchors:** Section 3.1 describes the 1.5×IQR windowing; Figure 2 shows outlier-aware window better matches full-precision tail.
**Break condition:** If distribution is Gaussian or outliers are noise, this strategy may allocate bits inefficiently, reducing compression density.

## Foundational Learning

- **Variational Inference & ELBO:** SQS frames compression as Bayesian inference minimizing divergence between variational distribution and true posterior, balanced by ELBO. Why needed: This is not standard backprop but a Bayesian approach. Quick check: Why approximate the KL divergence term rather than computing it directly? (Answer: No closed-form solution exists for KL between mixture of Gaussians and spike-and-slab prior).

- **Spike-and-Slab Priors:** Mathematical engine of sparsity using discrete probability mass/Bernoulli variable to induce exact zeros. Why needed: To distinguish from L1 regularization that only produces small weights. Quick check: How does the spike component differ from L1? (Answer: Uses discrete probability mass inducing exact zeros rather than just small weights).

- **Gaussian Mixture Models for Quantization:** Treats quantization as mixture of continuous distributions where GMM means become quantization grid. Why needed: To understand quantization as Bayesian averaging rather than simple rounding. Quick check: How does GMM differ from K-Means for weight quantization? (Answer: GMM provides variance estimates and allows Bayesian averaging during inference).

## Architecture Onboarding

- **Component map:** Prior (Spike-and-Slab) -> Variational Family (Spike-and-GMM) -> Objective (Approximate ELBO) -> Inference (Deterministic pruning + Bayesian averaging)
- **Critical path:** 1) Initialize GMM parameters via K-means per layer, 2) Forward pass samples weights from variational distribution, 3) Backward pass computes gradients of approximate ELBO, 4) Apply non-zero rate threshold to learned λ values for pruning
- **Design tradeoffs:** Number of components K increases bit-precision but reduces compression; temperature τ affects GMM sharpness and training stability; sparsity target saves memory but may remove critical outliers
- **Failure signatures:** Posterior collapse (all gates → 0 or → 1), mode collapse (all GMM components converge to same mean), numerical instability (exploding gradients from variances approaching zero)
- **First 3 experiments:** 1) Reproduce ResNet-32 results on CIFAR-10 comparing spike-and-slab vs Gaussian prior, 2) Compare Bayesian averaging vs greedy decoding on validation set, 3) Apply outlier-aware windowing to single attention layer and visualize weight histogram

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How does SQS performance and computational overhead scale for Large Language Models exceeding 1B parameters, specifically addressing the authors' hardware limitations with models like Llama3.1-8B?
**Basis in paper:** "Due to hardware limitations, we cannot run very large-scale LLMs, which are Llama3.1-8B and Qwen2.5-7B."
**Why unresolved:** Experimental validation currently limited to 0.5B-1B models; larger model behaviors may differ.
**What evidence would resolve it:** Benchmarks on 7B+ models with runtime analysis and memory footprint comparisons.

### Open Question 2
**Question:** Can the theoretical convergence guarantees for SQS be extended to cover modern architectures like Transformers and CNNs?
**Basis in paper:** "The theoretical analysis mainly considers an L-hidden layer fully connected NN... while our method SQS is empirically validated on a variety of models such as ResNets... and LLMs."
**Why unresolved:** Proof relies on specific structures of fully connected networks; unclear if "mild conditions" hold for attention mechanisms.
**What evidence would resolve it:** Theoretical extension or empirical analysis showing convergence properties match theory in non-fully connected layers.

### Open Question 3
**Question:** What is the sensitivity of SQS to the fine-tuning requirement, and can the method be adapted to perform well without extensive fine-tuning?
**Basis in paper:** "We find that omitting the fine-tuning step significantly degrades the performance of SQS."
**Why unresolved:** Reliance on fine-tuning limits applicability where training data is scarce or compute is restricted.
**What evidence would resolve it:** Ablation studies on zero-shot or few-shot compression performance without pre-compression fine-tuning.

## Limitations
- Approximation of KL divergence using dominant GMM component lacks quantitative bounds on ELBO fidelity
- Outlier-aware windowing strategy is empirically motivated but lacks theoretical justification and ablation studies
- Statistical significance of performance drops (<1.3% accuracy, <1.66% F1) is not established
- Compression rate claims relative to hardware (SQ-format) are speculative without empirical validation
- Number of Monte Carlo samples for Bayesian averaging is unspecified, affecting computational overhead

## Confidence

- **High Confidence:** Spike-and-GMM approach for joint sparsity and quantization is mathematically sound; GMM initialization via K-means and layer-wise quantization are standard and well-justified
- **Medium Confidence:** Outlier-aware windowing is supported by visual evidence but lacks ablation studies; KL approximation is common in VI but not quantitatively validated
- **Low Confidence:** Hardware efficiency claims are speculative without empirical validation; statistical significance of performance drops is not established

## Next Checks

1. **Quantify KL Approximation Error:** Measure difference between approximated ELBO (Eq. 8) and true ELBO (Eq. 7) on held-out validation set to assess dominant-component approximation fidelity

2. **Ablate Outlier-Aware Windowing:** Run experiments with standard equal-sized windows on ResNet and BERT to quantify performance cost on non-heavy-tailed distributions

3. **Validate Hardware Alignment:** Implement minimal SQ-format compression scheme and compare actual memory savings and inference speed with reported compression rates to validate hardware efficiency claims