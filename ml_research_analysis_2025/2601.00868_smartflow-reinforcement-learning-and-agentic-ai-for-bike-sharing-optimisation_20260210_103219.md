---
ver: rpa2
title: SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation
arxiv_id: '2601.00868'
source_url: https://arxiv.org/abs/2601.00868
tags:
- agent
- learning
- rebalancing
- smartflow
- bike
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SmartFlow is a multi-layered framework that integrates Reinforcement
  Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing
  services. Its architecture separates strategic, tactical, and communication functions
  for clarity and scalability.
---

# SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation

## Quick Facts
- **arXiv ID:** 2601.00868
- **Source URL:** https://arxiv.org/abs/2601.00868
- **Reference count:** 26
- **Primary result:** SmartFlow reduces bike-sharing network imbalance by over 95% with minimal fleet travel and high truck utilization

## Executive Summary
SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New York's Citi Bike network, learns robust rebalancing policies by modeling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimizes multi-leg journeys and schedules just-in-time dispatches to minimize fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlow's high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilization. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

## Method Summary
SmartFlow integrates a DQN agent trained on historical Citi Bike data to generate rebalancing strategies, a deterministic tactical module for route optimization and scheduling, and a grounded LLM for human-readable dispatch instructions. The DQN learns policies in a simulated environment where states encode station inventories and time-of-day, and actions specify bike transfers between stations. Strategic plans are converted into multi-leg journeys via greedy chaining and scheduled using just-in-time dispatch logic. The Gemma-2b-it LLM, constrained by grounded prompts, generates actionable Markdown reports, with a Python formatter as fallback. The system was evaluated on NYC's top 30 busiest stations using data from 2015-2017, with three random seeds for training runs.

## Key Results
- Network imbalance reduced by over 95%
- Total fleet travel distance approximately 37 km
- Truck utilization rate around 66%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A DQN agent learns to reduce network imbalance by formulating rebalancing as a sequential decision problem where optimal transfers maximize cumulative reward.
- **Mechanism:** The agent observes station inventory vectors plus time-of-day, selects discrete transfer actions (i, j), and receives shaped rewards proportional to need satisfied minus penalties for infeasible moves. Experience replay and target networks stabilize learning across ~1M timesteps.
- **Core assumption:** Bike demand patterns are sufficiently regular and time-correlated that a learned policy generalizes beyond training episodes.
- **Evidence anchors:**
  - [abstract] "Deep Q-Network (DQN) agent... learns robust rebalancing policies by modelling the challenge as a Markov Decision Process"
  - [section 3.5] Formal MDP tuple M = (S, A, P, R, γ) with shaped reward function
  - [corpus] Weak direct evidence—corpus focuses on multi-agent RL and flow prediction, not single-agent DQN for rebalancing
- **Break condition:** If demand becomes highly non-stationary (e.g., major events, policy changes) without retraining, learned Q-values may misallocate bikes.

### Mechanism 2
- **Claim:** Decoupling strategic station selection from tactical route optimization yields efficient, interpretable dispatches with high truck utilization.
- **Mechanism:** RL outputs only *which* transfers are needed; a deterministic module chains transfers into multi-leg journeys using greedy nearest-deficit routing, then schedules just-in-time dispatches working backwards from urgency.
- **Core assumption:** The set of strategic transfers is small enough that chaining does not introduce significant suboptimality versus joint optimization.
- **Evidence anchors:**
  - [abstract] "high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys"
  - [section 3.7] Multi-leg journey optimization and just-in-time scheduling algorithms described
  - [corpus] Hierarchical Adaptive Grouping (HAG-PS) paper supports hierarchical decomposition for mobility resource allocation
- **Break condition:** If transfer count is very high or spatially scattered, chaining may produce circuitous routes; consider constrained VRP solver as fallback.

### Mechanism 3
- **Claim:** An LLM with grounded prompts reliably translates structured plans into actionable human instructions without hallucination.
- **Mechanism:** The Gemma-2b-it model receives the JSON journey plan plus explicit constraints to use *only* provided data. A deterministic Python formatter serves as fallback on LLM failure.
- **Core assumption:** Prompt constraints are sufficient to suppress hallucination; LLM output quality is acceptable for operational use.
- **Evidence anchors:**
  - [abstract] "grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions"
  - [section 3.6] Grounded prompt engineering with persona, data, and format rules; fallback formatter
  - [corpus] No direct corpus evidence for grounded LLM dispatch reporting in mobility systems
- **Break condition:** If prompts leak ambiguity or edge cases (e.g., incomplete data, conflicting transfers), LLM may generate unverifiable instructions; enforce deterministic fallback and human review loop.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** The paper models rebalancing as MDP tuples (S, A, P, R, γ); understanding state transitions and reward shaping is prerequisite to interpreting agent behavior.
  - **Quick check question:** Given a state vector of station inventories and time, can you explain why time-of-day is included as an observation?

- **Concept: Deep Q-Networks and Experience Replay**
  - **Why needed here:** DQN with replay buffer and target networks is the core learning algorithm; the reported 95% imbalance reduction depends on stable convergence.
  - **Quick check question:** Why does experience replay break temporal correlations, and what could happen without a target network?

- **Concept: Grounded Prompt Engineering for LLMs**
  - **Why needed here:** The communication layer constrains LLM output to verifiable data; understanding grounding prevents misinterpreting "interpretable output" as "trustworthy output."
  - **Quick check question:** If the prompt omits a transfer in the JSON, what should the LLM do, and how does the framework enforce this?

## Architecture Onboarding

- **Component map:**
  1. SmartFlow-Prep Pipeline: Raw trip data → cleaned, enriched dataset (weather, elevation, temporal features)
  2. Gymnasium Environment: State = inventory + hour; Action = (source, dest) transfer; step() simulates demand and returns reward
  3. DQN Agent: MLP (2×128 hidden), ε-greedy exploration, replay buffer 50K, target network periodic update
  4. Tactical Module: Greedy chaining → multi-leg journeys; just-in-time scheduler → dispatch times
  5. Agentic AI Layer: Gemma-2b-it with grounded prompt → Markdown report; Python fallback formatter

- **Critical path:**
  Data ingestion → Environment initialization → RL policy training (offline) → Strategic plan generation → Tactical route chaining → LLM dispatch report → Human review

- **Design tradeoffs:**
  - RL vs MILP: Scalability and adaptability vs guaranteed optimality
  - Single-agent vs multi-agent: Simplicity vs fleet coordination scalability
  - LLM vs template: Naturalness vs reliability; mitigated by fallback formatter

- **Failure signatures:**
  - **Training divergence:** Reward curve flat or oscillating after 500K steps—check learning rate, reward scale, replay buffer integrity
  - **Infeasible actions dominate:** High penalty counts—validate state/action space constraints and reward shaping
  - **LLM hallucination:** Dispatch instructions reference stations not in JSON—audit prompt constraints and enforce fallback
  - **Poor chaining efficiency:** Low truck utilization (<40%)—review greedy routing heuristic; consider inserting distance thresholds

- **First 3 experiments:**
  1. **Baseline validation:** Run environment with random action selection (no RL) to quantify imbalance reduction without learning—establishes lower bound.
  2. **Ablation on reward shaping:** Remove need-scaled positive reward; train agent with only feasibility penalties—compare convergence speed and final imbalance.
  3. **Prompt robustness test:** Intentionally omit 10% of transfers from JSON input to LLM; verify fallback formatter activates and output remains consistent with available data.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a Multi-Agent Reinforcement Learning (MARL) framework effectively scale SmartFlow to city-wide networks while maintaining the >95% imbalance reduction achieved by the single-agent DQN?
- **Basis in paper:** [explicit] Section 6.2 proposes transitioning to MARL to address scalability limitations and distribute computational load.
- **Why unresolved:** The current study validates a centralized single-agent architecture; decentralized coordination logic and training stability in a multi-agent setting remain untested.
- **What evidence would resolve it:** Performance metrics (imbalance reduction, convergence speed) comparing the current DQN against a cooperative MARL setup on a significantly larger station network.

### Open Question 2
- **Question:** How does the framework perform when deployed with live data feeds and real-time traffic stochasticity compared to the historical simulation?
- **Basis in paper:** [explicit] Section 6.2 identifies "Live Data Integration" as a primary goal, and Section 3.9 cites the "simulation-to-reality gap" as a practical deployment challenge.
- **Why unresolved:** Evaluation relies on a deterministic simulation of historical data (2015–2017 Citi Bike) without real-world API latencies or dynamic traffic patterns.
- **What evidence would resolve it:** Results from a physical pilot deployment or a rigorous "sim-to-real" transfer experiment incorporating stochastic real-time traffic and demand shocks.

### Open Question 3
- **Question:** Can the agent's reward function be enriched to optimize for financial costs and environmental sustainability without degrading network balance?
- **Basis in paper:** [explicit] Section 6.2 suggests augmenting the reward signal with penalties for operational costs and incentives for sustainability.
- **Why unresolved:** The current reward function is shaped primarily for inventory balance and efficiency metrics (distance, utilization), ignoring complex externalities.
- **What evidence would resolve it:** Training runs showing the trade-off curves between the current 95.47% imbalance reduction and new metrics like fuel consumption or service equity.

## Limitations

- Reward function precision is unspecified, making it difficult to verify whether 95% imbalance reduction stems from optimal shaping or over-penalization.
- Scalability assumptions untested: performance may degrade with larger networks or non-stationary demand.
- LLM reliability under edge conditions unverified: no evidence on how system behaves with incomplete or inconsistent input data.

## Confidence

- **High:** The decoupling of strategic (RL) and tactical (deterministic routing) modules is well-defined and supported by the cited HAG-PS work on hierarchical decomposition.
- **Medium:** The DQN's 95% imbalance reduction is credible given the MDP formulation and evaluation across three random seeds, but the lack of precise reward function details limits full verification.
- **Low:** The LLM's role in generating actionable human instructions is plausible, but without systematic testing of hallucination under degraded input, confidence in operational reliability is low.

## Next Checks

1. **Reward function ablation:** Train two DQN variants—one with need-scaled positive rewards, one with only feasibility penalties. Compare imbalance reduction and convergence curves to quantify reward shaping impact.
2. **Scalability simulation:** Extend the environment to 50–100 stations and measure whether the learned policy maintains >90% imbalance reduction and acceptable truck utilization.
3. **LLM robustness audit:** Systematically corrupt 10–30% of the input JSON (missing transfers, duplicate entries) and verify that the fallback formatter activates, outputs remain consistent, and no hallucinated data appears in the final report.