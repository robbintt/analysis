---
ver: rpa2
title: 'PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning
  for Complex Problem Solving'
arxiv_id: '2507.07495'
source_url: https://arxiv.org/abs/2507.07495
tags:
- plan
- planning
- reasoning
- math
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PLAN-TUNING, a post-training method that
  improves smaller LLMs' reasoning by teaching them to generate structured planning
  trajectories. The approach distills step-by-step decomposition plans from large-scale
  models and fine-tunes smaller models via supervised fine-tuning and reinforcement
  learning.
---

# PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving

## Quick Facts
- arXiv ID: 2507.07495
- Source URL: https://arxiv.org/abs/2507.07495
- Reference count: 40
- Key outcome: PLAN-TUNING improves smaller LLMs' reasoning by 7% on GSM8k and 20% on MATH through step-by-step planning trajectory distillation

## Executive Summary
PLAN-TUNING introduces a post-training method that teaches smaller language models to generate structured planning trajectories before solving complex mathematical problems. The approach distills step-by-step decomposition plans from large-scale models and fine-tunes smaller models via supervised fine-tuning and reinforcement learning. On GSM8k and MATH benchmarks, plan-tuned models outperform strong baselines significantly. The method demonstrates that incorporating explicit planning during training enhances mathematical reasoning capabilities in smaller open-source LLMs.

## Method Summary
PLAN-TUNING works by first generating candidate plans using a large LLM (Gemini-2.0-Flash), then filtering these through Plan and Answer Verifiers to ensure quality. The filtered (problem, plan, execution, answer) tuples train smaller models via supervised fine-tuning (M1: joint plan+answer; M2: plan-only) and reinforcement learning (GRPO with combined plan similarity and answer correctness rewards). This two-stage approach teaches models to decompose problems into subgoals before execution, improving reasoning accuracy on mathematical benchmarks.

## Key Results
- SFT models trained with planning trajectories outperform baselines by 7% on GSM8k and 20% on MATH
- Plan-tuned models show better generalization on out-of-domain datasets (OlympiadBench, AIME 2024) with 10-12% improvements
- Two-stage filtering (plan verification + answer verification) improves GSM8K accuracy from 83.32% to 87.11% and AIME 2024 from 22.72% to 32.05%
- GRPO with combined rewards further enhances performance, though small models show inconsistent results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit planning structures during training reduce reasoning errors by guiding decomposition and execution separately
- Mechanism: Models learn to generate planning trajectories (sequence of decomposition operators) before deriving solutions, constraining reasoning into human-interpretable subgoals
- Core assumption: Decomposition into subgoals aligns with valid solution construction and can be distilled from large LLMs
- Evidence: Abstract mentions distilling "synthetic task decompositions" from large LLMs; section 3.1 formalizes planner as product of decomposition operators
- Break condition: If planning trajectories are low-quality or incoherent, model learns incorrect decomposition patterns

### Mechanism 2
- Claim: Jointly optimizing for plan quality and answer correctness via RL reinforces better planning habits beyond simple imitation
- Mechanism: GRPO uses reward combining plan similarity and answer correctness to amplify trajectories scoring well on both
- Core assumption: Reward functions align with genuine reasoning quality and aren't gameable
- Evidence: Abstract mentions using "supervised and reinforcement-learning objectives designed to mimic these planning processes"; section 3.3.2 shows modified GRPO objective
- Break condition: If reward is misspecified, policy may optimize for superficial plan similarity without improving correctness

### Mechanism 3
- Claim: High-quality, filtered distillation data ensures models learn from coherent, verified planning trajectories
- Mechanism: Two-stage filtering process using Plan Verifier (threshold ≥ 80) and Answer Verifier to retain only high-quality, correct trajectories
- Core assumption: Verification agents reliably identify high-quality plans that generalize
- Evidence: Section 3.2 describes filtering retaining only trajectories with verifier score ≥ 80 and correct answers; Table 3 shows two-stage filtering improves accuracy
- Break condition: If verifier thresholds are misaligned with downstream tasks, filtered data may be too narrow or miss useful diversity

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT)**
  - Why needed here: SFT provides initial alignment to planning trajectory distribution before RL refinements
  - Quick check question: Can you differentiate between training a model to output (Plan + Execution + Answer) jointly vs. only Plan? Which would you expect to be more data-efficient?

- Concept: **Policy Gradient Methods (GRPO)**
  - Why needed here: GRPO is the RL algorithm used to optimize policy using rewards for planning quality and answer correctness
  - Quick check question: Why is a combined reward R_plan + R_ans potentially better than using just one component?

- Concept: **Data Distillation & Verification**
  - Why needed here: Method's efficacy hinges on quality of synthetic trajectories distilled from large LLM and filtered via verification agents
  - Quick check question: What potential biases might be introduced by relying on a large LLM for both synthesis and verification of plans?

## Architecture Onboarding

- Component map:
  Large-Scale LLM (Teacher) -> Plan Verifier -> Plan Executor + Answer Verifier -> Training Corpus -> Policy Model (Student) -> GRPO Reward Functions (R_plan + R_ans)

- Critical path:
  1. Generate 5 candidate plans per problem using Large LLM (N=5, temperature=0.7)
  2. Pass each through Plan Verifier; retain those with score ≥ 80
  3. Execute retained plans; keep only those yielding correct final answer
  4. Train Policy Model using SFT (M1/M2) or GRPO with combined reward
  5. Evaluate on in-domain (GSM8K, MATH) and out-of-domain (OlympiadBench, AIME 2024) datasets

- Design tradeoffs:
  - M1 vs. M2 (SFT): M1 (joint) may better integrate execution while M2 (plan-only) focuses on planning quality
  - SFT vs. GRPO: SFT is simpler and more stable but limited to imitation; GRPO can exceed imitation but is more sensitive to reward design
  - Single-Dataset vs. Mixed-Dataset Training: Training on GSM8K and MATH together can degrade performance (~2% on GSM8K)

- Failure signatures:
  - Low GRPO performance on small models (Gemma-3-1B-It, Qwen3-4B) indicating RL instability
  - Performance drop on AIME with Qwen-3 GRPO suggesting reward misalignment
  - High-quality plan but wrong answer indicating execution errors
  - Degradation with mixed datasets signaling conflicting planning patterns

- First 3 experiments:
  1. Baseline SFT Comparison: Train baseline SFT model (no planning trajectories) vs. M1 and M2 on GSM8K and MATH
  2. Ablation on Filtering Stages: Compare single-stage vs. two-stage filtered data on GSM8K and AIME 2024 accuracy
  3. GRPO Reward Ablation: Train GRPO models with R_ans only vs. R_plan + R_ans to quantify planning reward contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does PLAN-TUNING generalize to non-mathematical domains such as code synthesis or commonsense reasoning without substantial adaptation?
- Basis in paper: Authors state in Limitations section that "it remains to be validated whether PLAN-TUNING generalizes to other problem types (e.g., commonsense, code synthesis)"
- Why unresolved: Current experiments restricted to mathematical reasoning benchmarks
- What evidence would resolve it: Successful application to benchmarks like HumanEval (code) or StrategyQA (commonsense) showing comparable improvements

### Open Question 2
- Question: Can objective-function evaluations like ROSCOE replace LLM-based verification for planning rewards in GRPO to improve stability?
- Basis in paper: Authors note "For GRPO, we used only LLM-based verification for planning reward, we will certainly explore more objective-function evaluations such as ROSCOE as future work"
- Why unresolved: LLM-based rewards may be noisy or biased; alternative reward signals haven't been tested
- What evidence would resolve it: Comparative study of GRPO convergence speed and final accuracy using ROSCOE metrics vs. current Gemini-based similarity scorer

### Open Question 3
- Question: How can negative interference observed when training on mixed datasets (e.g., GSM8k + MATH) be mitigated to create a unified planning model?
- Basis in paper: Figure 4 and analysis show mixing datasets degrades performance (~2% drop), suggesting model struggles with inconsistent reasoning styles
- Why unresolved: Paper identifies issue but doesn't propose solution for multi-domain training
- What evidence would resolve it: Demonstrating techniques like curriculum learning or domain-specific adapters allow single model to learn from both datasets without accuracy drop

## Limitations

- Effectiveness relies heavily on quality of large LLM (Gemini-2.0-Flash) used for plan synthesis, creating potential dependency on model-specific biases
- Plan Verifier implementation details are partially specified, and scoring threshold (α=80) may not generalize across different problem distributions
- Combined reward function in GRPO assumes proper alignment between plan similarity and reasoning quality, but study doesn't fully explore potential reward misspecification

## Confidence

- **High confidence**: GSM8k and MATH benchmark improvements with SFT (M1/M2 variants showing 7-20% gains over baselines)
- **Medium confidence**: Out-of-domain generalization improvements (10-12% on OlympiadBench/AIME), given potential domain-specific plan patterns
- **Low confidence**: GRPO's contribution to overall performance, particularly on smaller models where results are inconsistent

## Next Checks

1. Conduct ablation study isolating Plan Verifier's impact by testing different scoring thresholds and comparing filtered vs. unfiltered data quality
2. Implement reward ablation experiment testing GRPO with only answer correctness reward vs. combined reward to quantify planning reward contribution
3. Evaluate plan diversity and coherence metrics across filtered dataset to assess whether two-stage filtering maintains sufficient reasoning variety for generalization