---
ver: rpa2
title: The Best Instruction-Tuning Data are Those That Fit
arxiv_id: '2502.04194'
source_url: https://arxiv.org/abs/2502.04194
tags:
- data
- arxiv
- training
- grape
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GRAPE improves instruction-tuning by selecting responses aligned\
  \ with the base model\u2019s pretrained distribution. It gathers multiple candidate\
  \ responses per instruction, ranks them by normalized probability under the target\
  \ model, and selects the highest-scoring one."
---

# The Best Instruction-Tuning Data are Those That Fit

## Quick Facts
- **arXiv ID**: 2502.04194
- **Source URL**: https://arxiv.org/abs/2502.04194
- **Authors**: Dylan Zhang; Qirun Dai; Hao Peng
- **Reference count**: 40
- **Primary result**: GRAPE improves instruction-tuning by selecting responses aligned with the base model's pretrained distribution, achieving gains up to 13.8% over strongest generators and 17.3% over 3× more data.

## Executive Summary
GRAPE addresses the fundamental challenge of distribution shift in supervised fine-tuning by selecting responses that maximize the probability under the target base model rather than using responses from stronger teacher models. The method gathers multiple candidate responses per instruction, ranks them by length-normalized probability under the target model, and selects the highest-scoring one. This approach enables more efficient training with significantly less data while preserving base model capabilities. Experiments demonstrate substantial performance gains across diverse model families and task domains, with Llama3.1-8B surpassing Tulu3-SFT using only 1/3 of the data and half the epochs.

## Method Summary
GRAPE is a data selection algorithm for supervised fine-tuning that operates by computing the length-normalized log-probability of multiple candidate responses for each instruction using the target base model itself. For each instruction, GRAPE gathers candidate responses from diverse sources, calculates the average negative log-likelihood per token for each response using the base model, and retains only the response with the highest probability (lowest perplexity). The selected responses are then used for standard SFT training with typical hyperparameters (1 epoch, learning rate 1e-5, batch size 256). The method assumes access to a quality-controlled candidate pool containing multiple responses per instruction, generated by various teacher models or datasets.

## Key Results
- GRAPE achieves gains up to 13.8% over the strongest generator and 17.3% over training on 3× more data on UltraInteract-SFT
- On Tulu3/Olmo-2 mixtures, GRAPE outperforms strong baselines with 4.5× more data by up to 6.1%
- Llama3.1-8B surpasses Tulu3-SFT using 1/3 of the data and half the epochs with GRAPE
- GRAPE prevents model collapse from self-distillation while maintaining training efficiency

## Why This Works (Mechanism)

### Mechanism 1: Minimizing Distribution Shift via Probability Alignment
Standard SFT often uses responses from stronger "teacher" models that can lie outside the base model's pretrained distribution (high perplexity). GRAPE ranks candidate responses by normalized probability (equivalent to ranking by lowest perplexity). By selecting the response the base model assigns the highest probability to, the supervision signal stays closer to the model's existing manifold, potentially reducing "spurious correlations" and optimization conflicts. The core assumption is that the base model's pretrained distribution already contains the latent capabilities required for the task, and these are best elicited by supervision that is semantically and syntactically familiar to the model.

### Mechanism 2: Filtering Out "Model Collapse" Catalysts
The paper notes that self-distillation (training on self-generated outputs) often degrades performance due to narrowing diversity. Conversely, using outputs from much stronger models can be "out-of-distribution." GRAPE acts as a filter that selects for external diversity while enforcing internal consistency. It prioritizes responses that the model "almost" knows how to generate, providing a learning signal that is challenging but reachable. High perplexity in a candidate response indicates a reasoning step or style that is too distant for the student model to effectively emulate without distorting its weights.

### Mechanism 3: Target-Specific Customization (vs. Universal Quality)
Standard datasets use a "one-size-fits-all" response (usually from the strongest available model). GRAPE customizes the dataset per model. Different base models (e.g., Qwen vs. Llama) select different responses for the same instruction. This suggests the mechanism relies on finding the "path of least resistance" for that specific architecture. Optimization speed and final performance are higher when the gradient updates follow the natural curvature of the specific base model's loss landscape.

## Foundational Learning

- **Concept: Length-Normalized Log-Probability (Perplexity)**
  - **Why needed here:** GRAPE uses this specific metric to rank responses. Raw log-probabilities favor shorter responses; normalization is essential to compare responses of varying lengths fairly.
  - **Quick check question:** Why is raw log-probability insufficient for ranking candidate responses of different lengths?

- **Concept: Off-Policy vs. On-Policy Learning**
  - **Why needed here:** The paper frames standard SFT (using teacher responses) as "off-policy" and GRAPE as a move toward "on-policy" alignment. Understanding this RL analogy helps explain why distribution mismatch hurts performance.
  - **Quick check question:** How does using a response generated by a 405B model to train a 7B model constitute an "off-policy" training signal?

- **Concept: Model Collapse / Degradation Loops**
  - **Why needed here:** Section 5.5 explicitly warns against naive self-distillation. Understanding why recursive training on synthetic data narrows diversity is key to appreciating why GRAPE uses an external candidate pool but an internal selection metric.
  - **Quick check question:** Why does selecting the highest-probability response from an *external* pool avoid the "narrowing" problem of self-distillation?

## Architecture Onboarding

- **Component map:** Candidate Pool Aggregator -> Base Model (Frozen) -> Ranker -> SFT Trainer
- **Critical path:** The unique dependency is the Scoring Pass. You cannot pre-compute the GRAPE dataset without the specific base model checkpoint you intend to fine-tune. The dataset is model-dependent.
- **Design tradeoffs:**
  - **Efficiency vs. Quality:** You must generate/store multiple candidates per instruction (inflating storage) to have a selection pool, even though you only train on a fraction of them.
  - **Standardization vs. Customization:** You cannot create a single "GRAPE dataset" for the community; every team must run the scoring step on their specific base model.
- **Failure signatures:**
  - **Confirmation Bias:** If the base model prefers incorrect but confident hallucinations, GRAPE will amplify these. Monitor correctness of selected candidates.
  - **Loss of Diversity:** If the candidate pool is too small or similar, "highest probability" might just select the "safest" but least informative response.
  - **Length Bias:** If normalization is implemented incorrectly, the selector might still bias toward extremely short or long responses.
- **First 3 experiments:**
  1. **Loss Curve Validation:** Reproduce Figure 2. Train three small models on the same instructions paired with Best (GRAPE), Random, and Worst (lowest prob) responses. Verify that the "Best" run converges faster/lower.
  2. **Cross-Family Transfer:** Check if GRAPE-selected data for Llama-3 works well for Mistral (it shouldn't work as well as Mistral-specific GRAPE data, validating the "customization" hypothesis).
  3. **Pool Size Ablation:** Vary the number of candidates (N) per instruction (e.g., N=2 vs N=10) to find the point of diminishing returns for the compute spent on scoring.

## Open Questions the Paper Calls Out

- **Question 1:** Can GRAPE be effectively combined with instance-level data selection algorithms (e.g., LESS, S2L) to simultaneously optimize both instruction coverage and response distribution alignment?
- **Question 2:** How robust is GRAPE when applied to candidate pools containing significant noise or low-quality responses, rather than the curated, verified pools used in the experiments?
- **Question 3:** What are the performance limits and convergence properties of iterative GRAPE application (Multi-Round GRAPE)?
- **Question 4:** Does the effectiveness of GRAPE degrade for base models with extremely limited inherent capabilities or distinct architectural inductive biases?

## Limitations

- **Candidate Pool Quality Assumption:** The method assumes the candidate pool contains at least one response that is both high-quality and high-probability for the base model, which may not hold in noisy real-world settings.
- **Model-Specific Dataset Generation:** Unlike standard SFT datasets that can be shared, GRAPE requires running the selection process with each specific base model, making dataset sharing impractical.
- **Generalization to Non-Technical Tasks:** All experimental results focus on coding, math, and reasoning tasks, with no validation on creative writing or open-ended dialogue domains.

## Confidence

- **High Confidence:** Claims about computational efficiency gains (4.5× less data achieving comparable results) and the basic mechanism of probability-based selection are well-supported by controlled experiments and ablation studies.
- **Medium Confidence:** Claims about avoiding distribution shift and preventing model collapse are supported by experimental evidence but rely on the untested assumption that the candidate pool always contains acceptable high-probability responses.
- **Low Confidence:** Claims about GRAPE's effectiveness across diverse model families beyond the tested ones and across all task domains remain speculative without broader empirical validation.

## Next Checks

1. **Candidate Pool Robustness Test:** Systematically evaluate GRAPE's performance when the candidate pool contains only low-quality responses, only high-quality but low-probability responses, or a mix where quality and probability are inversely correlated.

2. **Cross-Domain Generalization Study:** Apply GRAPE to a dataset of creative writing or open-ended dialogue tasks, comparing performance against standard SFT and evaluating whether the probability-alignment mechanism remains beneficial when "correctness" is subjective.

3. **Long-Term Capability Retention Analysis:** After GRAPE fine-tuning, evaluate base model capabilities on tasks unrelated to the SFT domain (e.g., general knowledge, common sense reasoning) to verify that distribution alignment prevents catastrophic forgetting over extended use.