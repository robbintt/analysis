---
ver: rpa2
title: Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization
arxiv_id: '2503.18130'
source_url: https://arxiv.org/abs/2503.18130
tags:
- reward
- policy
- uni00000013
- bspo
- behavior-supported
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reward over-optimization in RLHF, where LLMs
  exploit reward model extrapolation errors on out-of-distribution (OOD) responses.
  The core method, Behavior-Supported Policy Optimization (BSPO), uses the next-token
  distribution from reward training data (behavior policy) to detect OOD responses
  and regularize value functions to penalize OOD values without affecting in-distribution
  ones.
---

# Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization

## Quick Facts
- **arXiv ID**: 2503.18130
- **Source URL**: https://arxiv.org/abs/2503.18130
- **Reference count**: 40
- **Primary result**: BSPO outperforms baselines across three proxy model scales, achieving higher gold rewards and reducing OOD response generation.

## Executive Summary
This paper addresses reward over-optimization in RLHF, where LLMs exploit reward model extrapolation errors on out-of-distribution (OOD) responses. The authors propose Behavior-Supported Policy Optimization (BSPO), which uses the next-token distribution from reward training data to detect OOD responses and regularize value functions to penalize OOD values without affecting in-distribution ones. The method demonstrates theoretical guarantees of monotonic improvement and empirical superiority over baselines like KL penalty and CPPO across multiple proxy model scales.

## Method Summary
BSPO introduces a behavior-supported Bellman operator that applies standard Bellman updates for supported actions (those with non-zero probability in the behavior policy β) but sets Q-values to a penalty constant for unsupported actions. The behavior policy β is derived from the next-token distribution in the reward training dataset. During RL training, a dual-head ScoreLM model predicts both the behavior distribution and reward, enabling detection of OOD responses. The critic is trained with the behavior-supported Bellman operator, while the actor is optimized via PPO using advantages from these regularized values.

## Key Results
- BSPO achieves higher gold rewards than baselines across all three proxy model scales (774M, 1.1B, 2.7B parameters)
- The method maintains performance advantages even at larger KL divergence distances
- BSPO significantly reduces the generation of behavior-unsupported actions compared to baselines
- Proxy model accuracy drops from 75.91% on supported responses to 58.10% on unsupported responses, validating the OOD detection approach

## Why This Works (Mechanism)

### Mechanism 1: OOD Detection via Behavior Policy
The behavior policy β(a|s) from the preference dataset identifies OOD responses by checking if β(a|s) = 0. Since reward models are trained on this data, unsupported actions indicate extrapolation risk. The method assumes pretrained LLMs can accurately predict in-distribution next-token probabilities for the preference dataset.

### Mechanism 2: Behavior-Supported Bellman Operator
The operator T^π_β applies standard Bellman updates for supported actions but sets Q(s,a) = Q_min for unsupported actions. This creates value estimates that discourage OOD action selection while maintaining unbiased estimates for in-distribution actions.

### Mechanism 3: Monotonic Convergence
Policy iteration with behavior-supported values converges monotonically to the optimal behavior-supported policy because Q^π_β provides unbiased estimates for ID actions, allowing greedy policy improvement to select optimal supported actions.

## Foundational Learning

- **Reward Over-Optimization (Reward Hacking)**: Why needed: BSPO directly addresses this phenomenon where proxy reward increases while true reward decreases. Quick check: Can you explain why maximizing a proxy reward model can hurt actual performance?

- **Bellman Operators and Fixed Points**: Why needed: The paper introduces a modified Bellman operator and proves it contracts to a unique fixed point. Quick check: What does it mean for an operator to be a γ-contraction, and why does this guarantee convergence?

- **In-Distribution vs. Out-of-Distribution in Reward Models**: Why needed: The core insight is that reward models fail on OOD responses, causing over-optimization. Quick check: Why would a reward model trained on preference data give unreliable scores for responses outside that distribution?

## Architecture Onboarding

- **Component map**: ScoreLM (LM head + score head) -> Critic (behavior-supported V-operator) -> Actor (PPO with advantages)
- **Critical path**: Train ScoreLM with preference + supervised loss → generate responses → compute rewards → compute behavior-supported values → update policy
- **Design tradeoffs**: ScoreLM integration adds minimal overhead but requires joint training; ε_β threshold determines OOD sensitivity; Q_min penalty must balance suppression vs. stability
- **Failure signatures**: Gold reward plateauing while proxy reward increases (ε_β too low); gold reward consistently lower than baselines (β too restrictive); training instability (V_min incorrect)
- **First 3 experiments**: 1) Validate behavior policy OOD detection by comparing proxy accuracy on supported vs. unsupported responses 2) Ablate ε_β threshold across values [1e-3, 1e-4, 1e-5, 1e-6, 1e-7] 3) Compare against KL-penalty baseline at multiple KL distances

## Open Questions the Paper Calls Out

1. **Real-world human evaluation**: How does BSPO compare to baselines when using actual human annotators rather than synthetic gold-reward models? The paper acknowledges this limitation due to cost constraints and notes that human preferences exhibit greater variability.

2. **OOD prompts in RL phase**: How can BSPO handle distribution shifts in the prompt domain during RL, in addition to OOD responses? The current method focuses on action/response validity but doesn't address input prompt distribution shifts.

3. **Multi-objective RLHF extension**: How can the behavior-supported regularization method be adapted for scenarios involving multiple reward models? The paper identifies this as a promising direction for future research to mitigate over-optimization of each individual reward model.

## Limitations

- The method assumes the optimal policy exists within the behavior-supported policy set Π_β, which may not hold for all alignment tasks
- Effectiveness depends on the behavior policy having sufficient coverage of valid responses; poor dataset coverage can lead to suboptimal policies
- The paper relies on synthetic gold-reward models rather than real human evaluation, limiting real-world applicability claims

## Confidence

- **High Confidence**: Empirical demonstration that reward models exhibit lower accuracy on unsupported responses and that BSPO achieves higher gold rewards than baselines across multiple proxy scales
- **Medium Confidence**: Theoretical guarantees of monotonic convergence given the strong assumptions about MDP structure and the requirement that the optimal policy exists within Π_β
- **Low Confidence**: The assumption that pretrained LLMs can accurately predict behavior distributions for the preference dataset, as this is not directly validated in the experiments

## Next Checks

1. **Validate Behavior Policy Coverage**: Test whether BSPO's performance degrades when the reward training dataset is intentionally made to have poor coverage of high-quality responses to validate robustness to dataset coverage limitations.

2. **Ablate Hyperparameter Sensitivity**: Systematically test BSPO's performance across a wider range of ε_β values (e.g., 1e-2 to 1e-8) and V_min values to quantify the method's sensitivity to these critical hyperparameters.

3. **Test on Diverse Alignment Tasks**: Evaluate BSPO on alignment tasks beyond the helpfulness-focused UltraFeedback dataset, particularly tasks where the optimal policy might require responses that are unsupported by the behavior policy.