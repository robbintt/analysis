---
ver: rpa2
title: Mitigating Forgetting in Low Rank Adaptation
arxiv_id: '2512.17720'
source_url: https://arxiv.org/abs/2512.17720
tags:
- lalora
- forgetting
- learning
- accuracy
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaLoRA applies a Laplace approximation to the LoRA weights to estimate
  their importance for source-domain performance, then uses this as a regularizer
  during fine-tuning to mitigate forgetting. It computes a precision matrix from surrogate
  source data and penalizes updates to high-curvature (critical) LoRA parameters while
  allowing low-curvature ones to adapt.
---

# Mitigating Forgetting in Low Rank Adaptation

## Quick Facts
- arXiv ID: 2512.17720
- Source URL: https://arxiv.org/abs/2512.17720
- Reference count: 40
- LaLoRA achieves up to 4 percentage points higher source accuracy while maintaining target performance compared to LoRA when fine-tuning Llama-3.2-3B for math reasoning.

## Executive Summary
LaLoRA addresses catastrophic forgetting in LoRA fine-tuning by applying a Laplace approximation to estimate parameter importance for source-domain performance. The method computes a precision matrix from surrogate source data and uses this as a regularizer during target fine-tuning, constraining updates to high-curvature (critical) parameters while allowing low-curvature ones to adapt. Evaluated on Llama-3.2-3B fine-tuned for math reasoning, LaLoRA significantly improves the Pareto frontier of the learning-forgetting trade-off compared to baselines like MIGU, MILORA, and L2 regularization.

## Method Summary
LaLoRA operates in two stages: first, it computes a precision matrix from the empirical Fisher of source proxy data to identify critical LoRA parameters; second, it fine-tunes with a Laplace regularizer that penalizes updates to high-curvature parameters. The regularizer r = ½(∆W - μ)ᵀΣ⁻¹(∆W - μ) preserves source knowledge by constraining updates in directions where small changes sharply increase source loss. The regularization strength λ provides direct control over the learning-forgetting trade-off. The method remains lightweight by applying Laplace approximation only to LoRA adapters (not full weights) and demonstrates effectiveness across different LoRA ranks, training lengths, and curvature approximations.

## Key Results
- LaLoRA achieves up to 4 percentage points higher source accuracy while maintaining target performance compared to LoRA
- Benefits persist across LoRA ranks, training lengths, and curvature approximations (diagonal, block-diagonal K-FAC, and block tri-diagonal K-FAC)
- Using just 1-2 batches of proxy source data suffices for effective regularization
- LaLoRA improves the Pareto frontier over baselines like MIGU, MILORA, and L2 regularization

## Why This Works (Mechanism)

### Mechanism 1: Precision-Guided Weight Protection
- Claim: Constraining updates to high-curvature LoRA parameters preserves source-domain knowledge.
- Mechanism: The Laplace approximation yields a precision matrix Σ⁻¹ from the Hessian of the source loss. High-precision entries correspond to directions where small changes sharply increase source loss. The regularizer r = ½(∆W - μ)ᵀΣ⁻¹(∆W - μ) penalizes deviations proportional to this precision, effectively freezing critical weights while permitting adaptation along flat directions.
- Core assumption: Local curvature around the pre-trained weights meaningfully captures parameter importance for source tasks.
- Evidence anchors:
  - [abstract] "constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning"
  - [Section 5.2, Figure 3a] Shows "important" weights (top 10% precision) have mean change 1.32×10⁻⁴ vs 0.0249 for "flexible" weights under LaLoRA; baseline updates all groups uniformly (~0.02)
  - [corpus] Weak direct validation; neighbor papers on continual LoRA (C-LoRA, AltLoRA) address forgetting but use different mechanisms (gating, projection)
- Break condition: If source and target domains require overlapping high-curvature directions, regularization will block necessary adaptation, degrading target performance.

### Mechanism 2: Surrogate Data Sufficiency for Curvature Estimation
- Claim: A small number of batches from proxy source datasets provides sufficient curvature signal for effective regularization.
- Mechanism: Rather than requiring full pre-training data, LaLoRA estimates precision from 1-2 mini-batches per proxy dataset (e.g., WinoGrande, ARC, HellaSwag). The empirical diagonal Fisher requires only gradient computations: D = (θ log p(y|x,∆W)/θθ)² accumulated across batches.
- Core assumption: Proxy datasets sample loss curvature similar to true pre-training distribution; curvature signal saturates quickly.
- Evidence anchors:
  - [Section 5.2, Figure 4] Two batches per source dataset optimizes trade-off; one batch is nearly as effective
  - [Section 5.2, Figure 3b] Using only WinoGrande for LA improves forgetting on all three source datasets (+2.7 to +7.8 pp benefit)
  - [corpus] No direct validation; related work (EWC, OSLA) similarly uses surrogate data but not specifically analyzed for batch sufficiency
- Break condition: If proxy data distribution diverges significantly from pre-training distribution, estimated curvature misidentifies critical directions.

### Mechanism 3: Low-Rank Factorization Limits Regularization Scope
- Claim: Applying Laplace approximation only to LoRA adapters (not full weights) keeps computation tractable while still protecting source knowledge.
- Mechanism: LoRA restricts updates to ∆W = BA with B∈ℝ^(Dout×r), A∈ℝ^(r×Din). The precision matrix is computed only for these 2r(Din+Dout) parameters rather than Din×Dout full weights. This reduces Hessian approximation from O(D²) to O(r²(Din²+Dout²)) for K-FAC variants.
- Core assumption: Protecting adapter weights indirectly protects the effective update ∆W = BA sufficiently; full-weight curvature is unnecessary.
- Evidence anchors:
  - [Section 3] "By applying the Laplace approximation only to the LoRA weights, the method remains lightweight"
  - [Section D.1] Shows diagonal LA on adapters corresponds to a specific covariance structure on ∆W
  - [corpus] Weak; Yang et al. (2024) apply LA post-hoc to LoRA for uncertainty quantification (different goal)
- Break condition: If critical source knowledge is encoded in directions inaccessible through low-rank updates, adapter-only regularization cannot protect it.

## Foundational Learning

- Concept: **Fisher Information Matrix as Curvature Proxy**
  - Why needed here: LaLoRA uses the empirical diagonal Fisher to approximate the Hessian without second-order derivatives. Understanding why F = E[∇log p · ∇log pᵀ] ≈ Hessian requires knowing the connection between score function curvature and parameter sensitivity.
  - Quick check question: Why does the Fisher approximate the Hessian specifically at the MAP estimate (not arbitrary points)?

- Concept: **Bayesian Interpretation of L2 Regularization**
  - Why needed here: The LaLoRA regularizer is a Gaussian prior with learned covariance. Standard L2 is the special case with identity precision. Understanding this frames why curvature-weighted regularization outperforms uniform L2 (Table 1: L2 stability achieves 18.4% target accuracy vs 24.0% for LaLoRA at similar source protection).
  - Quick check question: What prior does λ||∆W-μ||²₂ correspond to, and how does Σ⁻¹ change this?

- Concept: **Stability-Plasticity Trade-off in Continual Learning**
  - Why needed here: LaLoRA directly controls this trade-off via λ. High λ prioritizes source retention (stability); low λ prioritizes target learning (plasticity). Figure 2b shows this Pareto frontier explicitly.
  - Quick check question: On the Pareto frontier in Figure 2b, what does moving right-to-left vs left-to-right represent for λ?

## Architecture Onboarding

- Component map:
Stage I (Pre-training proxy → Precision Matrix):
  Source proxy batches → Forward pass → Loss gradients → Empirical Fisher → Accumulated precision Σ⁻¹

Stage II (Target fine-tuning with regularization):
  Target batch → Standard forward pass → Loss L_target
               → LaLoRA regularizer: r = ½·vec(∆W-μ)ᵀ Σ⁻¹ vec(∆W-μ)
               → Combined: L_reg = L_target + λr
               → Backprop on LoRA weights only

- Critical path:
  1. Initialize LoRA: A ~ N(0, σ²), B = 0 (so BA = 0 initially)
  2. Compute precision: For each proxy dataset, accumulate diagonal Fisher over Ns batches
  3. Fine-tune with regularizer added to loss; μ remains at initialization

- Design tradeoffs:
  - **DIAG vs B-K-FAC vs B-TRI-K-FAC**: Paper finds diagonal sufficient; K-FAC adds ~40MB memory (Table 15) with no accuracy gain in their experiments
  - **Ns (batches per proxy)**: 1 batch is default; 2 batches marginally better but not worth cost (Figure 4)
  - **λ selection**: Requires validation sweep; paper uses geometric grid {1, 10, 10², ..., 10⁶}

- Failure signatures:
  - Target accuracy drops significantly while source barely improves → λ too high (over-regularization)
  - Source accuracy drops like baseline → λ too low or proxy data mismatch
  - Training diverges → precision matrix has negative/NaN entries (gradient computation bug)
  - OOM during precision computation → batch size too large for LA estimation

- First 3 experiments:
  1. **Baseline reproduction**: Run vanilla LoRA on GSM-8K with Llama-3.2-3B, measure source (WinoGrande/ARC/HellaSwag) and target accuracy to establish forgetting baseline
  2. **DIAG LaLoRA with single proxy**: Add diagonal regularizer using only WinoGrande (Ns=1 batch), sweep λ ∈ {10², 10³, 10⁴}, plot forgetting curve (Figure 2a pattern)
  3. **Ablation: proxy dataset coverage**: Compare using WinoGrande-only vs all three proxies for precision estimation at fixed λ=10³, quantify forgetting benefit gap (expect +2-3 pp from broader coverage per Figure 3b)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can regularization strength λ be set automatically without manual tuning?
- Basis in paper: [explicit] "Setting λ to achieve the optimal learning–forgetting trade-off requires tuning, which can be challenging in real-world continual learning or fine-tuning scenarios. Future work could focus on developing methods for automatically setting this hyperparameter."
- Why unresolved: Currently, practitioners must run validation sweeps to select λ for their specific stability-plasticity trade-off preference.
- What evidence would resolve it: A meta-learning or gradient-based approach that dynamically adjusts λ during training, evaluated across multiple domains without requiring validation data.

### Open Question 2
- Question: Does debiasing the stochastic Laplace approximation improve LaLoRA's performance?
- Basis in paper: [explicit] "Additionally, recent work by Tatzel et al. (2025) demonstrated an efficient way to improve the Laplace approximation's accuracy by debiasing its stochastic estimates."
- Why unresolved: LaLoRA uses mini-batch empirical Fisher estimates which may introduce bias; debiasing has been shown to help in other Laplace applications but not yet tested for forgetting mitigation.
- What evidence would resolve it: Comparison of debiased vs. standard empirical Fisher on source-domain accuracy and target-domain learning metrics.

### Open Question 3
- Question: Why don't more expressive curvature approximations (B-KFAC, B-TRI-KFAC) outperform the diagonal approximation?
- Basis in paper: [inferred] The paper finds B-KFAC provides no improvement over diagonal approximation and notes "this may be at least a partial explanation" related to low-rank structure collapsing, but the mechanism remains unclear.
- Why unresolved: Intuitively, capturing intra-layer and inter-layer parameter interactions should better identify critical directions, yet experiments show otherwise.
- What evidence would resolve it: Theoretical analysis of the induced covariance structure on the full parameter space, or experiments with different LoRA initialization schemes where cross-parameter correlations may be more informative.

### Open Question 4
- Question: Can LaLoRA be extended effectively to other PEFT methods beyond LoRA?
- Basis in paper: [explicit] The method "serves as a drop-in for standard LoRA pipelines and can be easily adapted to other PEFT methods," but only LoRA was evaluated.
- Why unresolved: Different PEFT methods (e.g., prefix tuning, adapter layers) have different parameter structures and may require modifications to the Laplace approximation approach.
- What evidence would resolve it: Evaluation on methods like DoRA, AdaLoRA, or prompt tuning, measuring whether the curvature-based regularization provides comparable forgetting benefits.

## Limitations

- The method's effectiveness depends on proxy datasets adequately representing the pre-training distribution, which is not rigorously validated
- The theoretical guarantee that protecting adapter weights indirectly protects source knowledge is not established
- Computational overhead of precision matrix computation is not fully characterized beyond noting K-FAC variants increase memory by ~40MB

## Confidence

- **High Confidence**: The mechanism by which LaLoRA constrains high-curvature parameter updates to protect source knowledge is well-supported by empirical evidence showing differential weight changes between "important" and "flexible" parameters. The Pareto frontier results demonstrating consistent improvement over baselines are robust.
- **Medium Confidence**: The claim that 1-2 batches of proxy data suffice for effective regularization is supported by Figure 4, but the analysis is limited to the specific datasets and model used. The diagonal Fisher approximation appears sufficient, but the paper doesn't thoroughly explore whether this holds across different model architectures or task domains.
- **Low Confidence**: The theoretical guarantee that protecting adapter weights indirectly protects source knowledge is not established. The paper asserts this follows from the low-rank structure but provides limited empirical validation of this claim.

## Next Checks

1. **Proxy Data Generalization**: Test LaLoRA with proxy datasets that are deliberately mismatched from the pre-training distribution (e.g., use code datasets as proxies for a language model) to quantify degradation in regularization effectiveness.

2. **High-Curvature Interference Analysis**: Design an experiment where source and target domains share critical high-curvature directions, then measure whether LaLoRA's regularization prevents necessary target adaptation compared to baseline LoRA.

3. **Adapter-Only Protection Validation**: Compare LaLoRA's source retention against a variant that applies Laplace approximation to full weights (not just adapters) to test whether adapter-only protection is truly sufficient for preserving source knowledge.