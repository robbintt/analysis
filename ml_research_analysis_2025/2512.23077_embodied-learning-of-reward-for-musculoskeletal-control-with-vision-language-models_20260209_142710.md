---
ver: rpa2
title: Embodied Learning of Reward for Musculoskeletal Control with Vision Language
  Models
arxiv_id: '2512.23077'
source_url: https://arxiv.org/abs/2512.23077
tags:
- reward
- control
- motion
- musculoskeletal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Motion from Vision-Language Representation
  (MoVLR), a framework that uses vision-language models (VLMs) to automatically design
  reward functions for high-dimensional musculoskeletal control. MoVLR iteratively
  optimizes control policies, evaluates resulting motion videos using a VLM, and refines
  reward functions through language model feedback, enabling discovery of biomechanically
  grounded rewards from natural language task descriptions.
---

# Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models

## Quick Facts
- arXiv ID: 2512.23077
- Source URL: https://arxiv.org/abs/2512.23077
- Authors: Saraswati Soedarmadji; Yunyue Wei; Chen Zhang; Yisong Yue; Yanan Sui
- Reference count: 14
- Primary result: MoVLR framework uses VLMs to automatically design reward functions for high-dimensional musculoskeletal control, achieving up to 2.76m walking distance on rough terrain

## Executive Summary
MoVLR introduces a framework that uses vision-language models to automatically design reward functions for high-dimensional musculoskeletal control systems. The approach iteratively optimizes control policies, evaluates resulting motion videos using a VLM, and refines reward functions through language model feedback. This enables discovery of biomechanically grounded rewards from natural language task descriptions without manual reward engineering. The method outperforms state-of-the-art language-based approaches across locomotion and manipulation tasks, demonstrating strong generalization across environments and morphologies.

## Method Summary
MoVLR uses an iterative loop where MPC2 policy optimization generates control trajectories from current reward functions, which are then rendered as videos and evaluated by a VLM against task descriptions. The VLM produces structured feedback that an LLM uses to refine the reward function code. This process repeats until convergence, with the framework maintaining the best reward found. The approach uses a modular architecture with separate VLM (Gemini 2.0 Flash) for visual assessment and LLM (Qwen2.5-Coder-32B-Instruct) for reward code generation, leveraging the strengths of each model type.

## Key Results
- Locomotion performance reaches up to 2.76m walking distance on rough terrain, outperforming text-only methods
- Lower position/orientation errors in manipulation tasks compared to baselines
- Strong generalization across environments and morphologies, with rewards transferring effectively between different musculoskeletal structures
- Learned rewards show hierarchical progression from global stability to fine-grained coordination

## Why This Works (Mechanism)

### Mechanism 1
- Iterative VLM-based visual feedback enables reward refinement that text-only methods cannot achieve
- The VLM observes rendered motion videos and produces structured biomechanical feedback that identifies postural errors, coordination failures, and stability issues, which is converted into explicit reward term modifications by an LLM
- Core assumption: VLMs possess sufficient understanding of biomechanically plausible motion to diagnose coordination failures from video
- Evidence: MoVLR shows iterative improvement through visual-grounded feedback, while HARMON (Jiang et al., 2025) using image feedback shows degraded performance

### Mechanism 2
- Reward refinement follows a hierarchical progression from global stability to fine-grained coordination
- Early iterations prioritize coarse objectives (height, balance, velocity) to establish motion feasibility, while later iterations focus on localized descriptors (foot placement, joint alignment, gait symmetry)
- Core assumption: The reward space admits a structured decomposition where global stability prerequisites enable finer coordination learning
- Evidence: Refinement heatmaps show early concentration on global stability terms shifting to localized biomechanical descriptors

### Mechanism 3
- Separating VLM feedback generation from LLM code synthesis improves reward quality compared to unified multimodal models
- The framework uses a VLM for visual assessment and a separate LLM for reward code generation, allowing each model to specialize in its strength
- Core assumption: Current vision-language models lack compositional reasoning to simultaneously evaluate motion quality and generate valid executable code
- Evidence: Ablation study shows unified variant produces substantially degraded performance with invalid or incomplete reward code

## Foundational Learning

- **High-dimensional musculoskeletal dynamics**: Musculoskeletal systems have 206 joints and 700 muscle-tendon actuators with highly nonlinear coupling, where multiple muscle activations can produce identical joint motions (redundancy), making reward design non-trivial
  - Quick check: Can you explain why musculoskeletal systems are "over-actuated" and how this affects reward design?

- **Model Predictive Control (MPC) with sampling-based planning**: MoVLR uses MPC2 as the policy optimizer, solving planning in a reduced posture space rather than full actuator space, enabling rapid iteration cycles (minutes vs. hours for RL)
  - Quick check: How does the hierarchical decomposition in MPC2 reduce optimization dimensionality from H·d_u to d_z?

- **Reward shaping and linear reward decomposition**: The framework represents rewards as r = w · r with weighted terms, where understanding how individual reward terms combine helps interpret refinement heatmaps
  - Quick check: Given Equation 3, how would increasing w_balance affect the tradeoff between stability and forward progress?

## Architecture Onboarding

- **Component map**: Task description → Initial reward r(0) → MPC2 optimization → Rollout ζ → Video render → VLM evaluation → Feedback F → LLM code generation → New reward r(i+1) → repeat
- **Critical path**: Task description flows through iterative loop of policy optimization, video rendering, VLM evaluation, and LLM reward refinement
- **Design tradeoffs**: MPC2 vs. RL (faster iterations vs. potential suboptimal asymptotic performance), full video vs. sparse frames (temporal dynamics vs. API costs), reward complexity vs. interpretability
- **Failure signatures**: Invalid code generation, feedback drift, reward collapse, stagnation
- **First 3 experiments**:
  1. Sanity check on flat terrain locomotion with minimal reward (forward velocity only)
  2. Ablation on VLM feedback quality comparing full video vs. sparse frame extraction
  3. Cross-morphology transfer test training on human model, applying to ostrich model

## Open Questions the Paper Calls Out

### Open Question 1
- Can unified vision-language model (VLM) architectures be improved to successfully perform both visual evaluation and reward code synthesis simultaneously?
- The paper explicitly states that a unified configuration failed due to current VLMs lacking compositional or programmatic reasoning required for both tasks
- Demonstration of a single VLM handling both feedback interpretation and code generation without performance degradation would resolve this

### Open Question 2
- Can the MoVLR framework be effectively adapted for reinforcement learning (RL) policy optimization given its current reliance on fast, training-free model-based control?
- The paper notes that RL requires hours to days of training, which limits the number of possible reward learning iterations
- Successful application to an RL pipeline where sample efficiency compensates for longer training times would resolve this

### Open Question 3
- Do the learned biomechanical rewards generalize effectively to physical robotic systems or biological tissues, or are they overfitted to simulation dynamics?
- All experiments were conducted in MuJoCo simulator without validation on physical hardware or real biomechanical data
- Transfer learning results showing maintained performance on physical systems would resolve this

## Limitations
- Requires significant computational resources for video rendering and VLM queries, with each iteration taking minutes
- Performance heavily depends on VLM assessment quality and consistency
- Modular architecture adds complexity that may be unnecessary if future VLMs improve compositional reasoning
- Validated primarily on bipedal and quadrupedal morphologies, with uncertain performance on more exotic musculoskeletal structures

## Confidence
- Locomotion performance claims: **High** - multiple quantitative benchmarks with clear improvement metrics
- Reward generalization claims: **Medium** - demonstrated across morphologies but limited environment diversity
- Hierarchical refinement mechanism: **Medium** - theoretically sound but not explicitly validated through ablation studies
- VLM feedback consistency: **Low** - single VLM model used, no robustness testing against different assessment models

## Next Checks
1. Test reward stability across multiple VLM models to verify feedback consistency isn't dependent on specific model biases
2. Conduct systematic ablation on video resolution and frame rate to quantify their impact on performance vs. computational cost
3. Evaluate transfer performance on significantly different musculoskeletal morphologies (e.g., multi-limbed or exoskeletal systems) to stress-test generalization claims