---
ver: rpa2
title: Enhancing Adversarial Transferability through Block Stretch and Shrink
arxiv_id: '2511.17688'
source_url: https://arxiv.org/abs/2511.17688
tags:
- adversarial
- input
- transferability
- attacks
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of enhancing adversarial transferability
  in deep neural networks by improving the diversity of attention heatmaps while preserving
  global semantic integrity. The authors propose Block Stretch and Shrink (BSS), a
  novel input transformation method that divides images into blocks and applies stretch
  and shrink operations to enrich attention heatmaps while maintaining global semantics.
---

# Enhancing Adversarial Transferability through Block Stretch and Shrink

## Quick Facts
- arXiv ID: 2511.17688
- Source URL: https://arxiv.org/abs/2511.17688
- Reference count: 40
- Primary result: Block Stretch and Shrink (BSS) outperforms existing input transformation methods in generating highly transferable adversarial examples while maintaining global semantic integrity.

## Executive Summary
This paper addresses the challenge of enhancing adversarial transferability in deep neural networks by introducing Block Stretch and Shrink (BSS), a novel input transformation method that applies block-level stretch and shrink operations to diversify attention heatmaps while preserving global semantic content. The method outperforms existing transformation-based attacks across various CNN and vision transformer architectures on ImageNet, demonstrating that semantically preserving transformations with diverse attention patterns yield more robust adversarial perturbations. The paper also introduces the concept of "number scale" as a standardized metric for fair comparison of transformation-based attack methods.

## Method Summary
BSS divides images into blocks using constrained random segmentation, then applies stretch and shrink operations to each block with bilinear interpolation, followed by concatenation. The method operates sequentially along height and width dimensions, with parameters M=2 segmentation points, db=35 boundary distance, dp=40 inter-point distance, and r=1 range-to-center ratio. BSS is integrated with MI-FGSM, where gradients are averaged across N transformed variants. The approach aims to balance attention heatmap diversity with semantic preservation to improve cross-model transferability.

## Key Results
- BSS achieves higher attack success rates than baseline methods (BSR, TIM, DIM) across CNN and ViT architectures on ImageNet
- Constrained segmentation significantly outperforms unconstrained random points in ablation studies
- Transferability consistently improves with higher number scales (N), validating the need for standardized evaluation metrics
- BSS maintains effectiveness against defense mechanisms including adversarial training and gradient-based purification

## Why This Works (Mechanism)

### Mechanism 1: Attention Heatmap Diversification Through Block-Level Scale Variation
Stretch and shrink operations at the block level generate diverse attention distributions across transformed inputs, correlating with improved cross-model transferability. By sampling scale factors from a uniform distribution and applying them to randomly segmented blocks, different spatial regions receive varying emphasis across transformations, forcing the surrogate model to optimize perturbations against a broader set of attention patterns rather than overfitting to specific localized features.

### Mechanism 2: Semantic Preservation via Constrained Random Segmentation
Enforcing minimum distance constraints on segmentation points preserves sufficient semantic content within each block to maintain global structure during deformation. The constrained point selection rejects candidate points that are too close to image boundaries or adjacent points, ensuring each block contains meaningful semantic regions rather than arbitrary fragments, enabling the surrogate model to leverage invariant features that transfer across architectures.

### Mechanism 3: Number Scale as a Confounding Variable
Transferability improvements should be evaluated under unified number scales to separate transformation quality from computational budget effects. Number scale N determines how many transformed inputs are generated per optimization step, directly scaling model evaluations. Without standardization, methods using higher N appear more effective purely due to increased diversity exposure, not superior transformation design.

## Foundational Learning

- **Concept: Adversarial Transferability (White-box → Black-box)**: Why needed here: The entire paper is predicated on generating perturbations on a known surrogate model that fool unknown target models. Quick check question: Can you explain why a perturbation optimized on ResNet-18 might also fool Inception-v3, and why this transfer is not guaranteed?

- **Concept: Attention Heatmaps (Grad-CAM/CAM)**: Why needed here: The paper's core hypothesis links transferability to attention heatmap diversity. Quick check question: If two transformed inputs produce identical Grad-CAM heatmaps, would you expect them to improve transferability? Why or why not?

- **Concept: Input Transformation-based Attacks (DIM, TIM, SIM, BSR)**: Why needed here: BSS is positioned as an improvement over prior transformation operators. Quick check question: What is the key tradeoff that existing methods like SIM and BSR fail to balance simultaneously?

## Architecture Onboarding

- **Component map:** Input + Perturbation -> Constrained Random Segmentation -> Block Partition -> Block-Level Stretch/Shrink -> Bilinear Interpolation -> Concatenation -> Dual-Dimension Processing -> Ensemble Gradient -> MI-FGSM Update

- **Critical path:** Constrained segmentation → Block stretch/shrink → Dual-dimension processing → Gradient averaging → Perturbation update. The ablation shows the full pipeline substantially outperforms 1D variants, indicating both dimensions contribute synergistically.

- **Design tradeoffs:**
  - db/dp constraints: Larger values preserve semantics but reduce block count and diversity; optimal found at db=35, dp=40
  - Range-to-center ratio r: Controls stretch/shrink intensity; r=1 balances diversity vs. semantic preservation
  - Number of segmentation points M: More points create finer-grained variations; M=2 (3 blocks) optimal
  - Number scale N: Higher N improves transferability but linearly increases compute; paper uses N=21-30 for experiments

- **Failure signatures:**
  - Semantic fragmentation: If db or dp too small, blocks contain insufficient context; attention patterns become noisy
  - Insufficient diversity: If r too small or M too low, transformed inputs produce similar attention maps, limiting transferability gains
  - Architecture mismatch: Transfer to ViTs is consistently lower than CNNs, suggesting architectural priors matter

- **First 3 experiments:**
  1. Reproduce ablation (Table 3): Compare 1D-BSS, 2D-BSS-RP, and full BSS on ResNet-18 → ViT transfer to validate constrained + dual-dimension synergy.
  2. Number scale sweep: Replicate Figure 4 on a smaller model subset to confirm that BSS maintains advantage at both low (N=10) and high (N=100) number scales.
  3. Attention visualization: Generate Grad-CAM heatmaps for BSS vs. BSR transformed inputs to qualitatively verify that BSS produces more diverse attention patterns while preserving object structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Block Stretch and Shrink method maintain high transferability when applied to dense prediction tasks like object detection or semantic segmentation?
- Basis in paper: The introduction explicitly identifies object detection and semantic segmentation as security-critical domains vulnerable to adversarial attacks, yet all experimental validation is restricted to image classification on ImageNet.
- Why unresolved: The paper does not provide experimental results or theoretical analysis regarding the impact of block-level geometric distortions on localization tasks or pixel-wise predictions.
- What evidence would resolve it: Experiments applying BSS-generated perturbations to standard detection (e.g., COCO) or segmentation datasets to measure mAP or IoU degradation.

### Open Question 2
- Question: Can the transformation parameters (e.g., segmentation points, range-to-center ratio) be optimized dynamically per image rather than set globally?
- Basis in paper: The ablation study identifies specific fixed parameters that optimize average success rates, implying that image-specific or adaptive tuning might yield higher transferability but remains unexplored.
- Why unresolved: The current implementation uses fixed hyperparameters and a random seed for all inputs, potentially limiting the optimality of the transformation for diverse image textures or layouts.
- What evidence would resolve it: A study comparing the performance of fixed versus learned/adapted transformation parameters across the ImageNet validation set.

### Open Question 3
- Question: How does the Block Stretch and Shrink method perform against defenses specifically designed to detect or reverse geometric transformations?
- Basis in paper: The defense experiments show BSS bypassing standard adversarial training and gradient-based purification, but do not test defenses explicitly designed to handle the geometric invariance exploited by BSS.
- Why unresolved: While BSS succeeds against current standard defenses, its reliance on noticeable block resizing creates visual artifacts that specialized geometric-defense mechanisms might detect or smooth out.
- What evidence would resolve it: Evaluating attack success rates against defense methods that explicitly normalize or detect spatial transformation artifacts.

## Limitations

- Semantic preservation assumption lacks quantitative validation beyond success rate improvements
- Attention heatmap diversity correlation with transferability is not explicitly validated
- Defense evaluation scope is limited to specific defenses without comprehensive testing against all mentioned in related work

## Confidence

**High Confidence Claims**:
- BSS outperforms baseline transformation methods under unified number scales in transferability metrics
- Number scale standardization is necessary for fair comparison of transformation methods
- Ablation results showing constrained segmentation and dual-dimension processing are statistically significant

**Medium Confidence Claims**:
- Attention heatmap diversification is the primary mechanism for improved transferability
- Semantic preservation through constrained segmentation is necessary
- BSS effectiveness generalizes across different model architectures

**Low Confidence Claims**:
- Exact contribution ratio between semantic preservation and attention diversity to overall performance
- Claims about BSS being "far more effective" against defenses without comprehensive defense testing
- Generalization of results to full ImageNet scale beyond the 1,000-image subset

## Next Checks

1. **Quantitative Attention Diversity Validation**: Measure correlation between attention heatmap diversity metrics (e.g., entropy, spatial variance) and actual transferability success rates across different transformation methods, including BSS variants.

2. **Comprehensive Defense Evaluation**: Test BSS against a broader range of defenses including fixed transformations (JPEG compression, random resizing/cropping) and adaptive defenses that could potentially mitigate block-level deformations.

3. **Semantic Coherence Analysis**: Conduct controlled experiments varying the db and dp constraints to empirically determine the threshold where semantic fragmentation begins to negatively impact transferability, validating the semantic preservation hypothesis quantitatively.