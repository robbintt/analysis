---
ver: rpa2
title: 'Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What
  Works?'
arxiv_id: '2601.20598'
source_url: https://arxiv.org/abs/2601.20598
tags:
- supervised
- reid
- language-aligned
- training
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Supervised ReID models dominate in-domain performance but catastrophically
  fail on cross-domain datasets, with models like OSNet dropping from 83% mAP on Market-1501
  to just 3% on MSMT17. Language-aligned models like SigLIP2 achieve moderate performance
  across all domains (5-14% mAP) and outperform supervised models on in-the-wild datasets
  like CelebReID (14.2% vs 5.8%).
---

# Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?

## Quick Facts
- arXiv ID: 2601.20598
- Source URL: https://arxiv.org/abs/2601.20598
- Reference count: 20
- Key outcome: Supervised models excel in-domain but catastrophically fail cross-domain; language-aligned models provide moderate robustness; self-supervised models struggle in zero-shot ReID.

## Executive Summary
This study evaluates 11 models across 9 datasets to compare supervised, self-supervised, and language-aligned paradigms for person Re-Identification (ReID). The key finding is a fundamental trade-off: supervised models like OSNet achieve peak performance on their training domain (83% mAP on Market-1501) but collapse on cross-domain data (3% mAP on MSMT17). Language-aligned models like SigLIP2 show moderate, consistent performance across domains (5-14% mAP) and outperform supervised models on in-the-wild datasets like CelebReID. Self-supervised models like DINOv2 perform poorly in zero-shot ReID (0.3-4.7% mAP). Model size does not correlate with better performance. The study concludes that no single paradigm excels everywhere, recommending supervised for in-domain, language-aligned for diverse scenarios.

## Method Summary
The paper evaluates three paradigms: supervised (OSNet, CLIP-ReID), self-supervised (DINOv2, Perception Encoder), and language-aligned (CLIP, SigLIP2). Models are trained on MSMT17 (for supervised) or used zero-shot (for foundation models). Evaluation is conducted on 9 datasets including Market-1501, DukeMTMC-reID, CUHK03, GRID, CelebReID, PKU-ReID, LasT, and IUSReID. Performance is measured using mAP and Rank-k accuracy (CMC). Zero-shot models are evaluated without ReID-specific fine-tuning, while CLIP-ReID is fine-tuned on MSMT17. The study isolates training data to assess cross-domain generalization.

## Key Results
- Supervised models dominate in-domain performance but catastrophically fail on cross-domain datasets (OSNet: 83% → 3% mAP).
- Language-aligned models like SigLIP2 achieve moderate performance across all domains (5-14% mAP) and outperform supervised models on in-the-wild datasets (14.2% vs 5.8% on CelebReID).
- Self-supervised models like DINOv2 perform poorly in zero-shot ReID (0.3-4.7% mAP).
- Model size does not correlate with better performance - PE-Core (671M parameters) underperforms smaller models.
- No single paradigm excels everywhere: choose supervised for in-domain, language-aligned for diverse/unknown scenarios.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised ReID models exploit dataset-specific correlations rather than learning transferable identity representations, causing catastrophic cross-domain failure.
- Mechanism: Supervised training with identity classification + triplet loss optimizes for discriminative power within a fixed distribution. Models learn to associate identities with low-level cues (camera viewpoint, background, illumination patterns) that are consistent within the training domain but incoherent elsewhere. Smaller capacity models (OSNet: 2.5M parameters) exhibit sharper overfitting due to limited representational capacity.
- Core assumption: The performance drop reflects learned dataset bias rather than fundamental task difficulty.
- Evidence anchors:
  - [abstract] "supervised models dominate their training domain but crumble on cross-domain data"
  - [section 6.1 Finding 1.1] "OSNet achieves 83.57% mAP on Market-1501, its performance drops sharply to 3.37% mAP on MSMT17... model has over-specialized to dataset-specific characteristics"
  - [corpus] DynaMix (arxiv:2511.19067) addresses similar generalization issues by combining labeled multi-camera data with pseudo-labeled single-camera data, confirming that pure supervised approaches lack cross-domain robustness.
- Break condition: If supervised models trained with explicit domain randomization or massive dataset diversity maintain cross-domain performance, the mechanism shifts from "dataset bias exploitation" to "insufficient training diversity."

### Mechanism 2
- Claim: Language-aligned models (SigLIP2) achieve cross-domain robustness through semantic attribute learning rather than pixel-level pattern matching.
- Mechanism: Training on web-scale image-text pairs with compositional descriptions (e.g., "woman wearing a red evening gown") forces the model to learn disentangled semantic attributes—personhood, clothing categories, accessories, spatial relations. These representations transfer because semantic concepts (color, texture, garment types) are domain-invariant, whereas low-level pixel statistics are not. SigLIP2's sigmoid loss and additional dense captioning objectives further enhance this effect compared to vanilla CLIP's contrastive softmax.
- Core assumption: The web-scale pretraining data contains sufficient attribute diversity to support ReID-relevant matching.
- Evidence anchors:
  - [abstract] "Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so"
  - [section 6.2 Finding 2.3] "language-aligned models match individuals based on high-level semantic appearance rather than brittle, low-level visual patterns"
  - [corpus] Find Them All (arxiv:2508.06908) explores MLLMs for ReID, suggesting that multi-modal semantic understanding is an emerging direction for generalizable ReID—consistent with the semantic transfer mechanism.
- Break condition: If language-aligned models trained on datasets without human/clothing descriptions still show cross-domain robustness, the mechanism may involve generic visual feature quality rather than semantic attribute learning.

### Mechanism 3
- Claim: Self-supervised vision models lack the discriminative structure needed for zero-shot ReID because pure visual self-supervision does not enforce identity-level separation.
- Mechanism: DINOv2's teacher-student framework with augmentation invariance learns general-purpose visual features optimized for dense prediction tasks. Without explicit identity labels or text anchoring, the embedding space lacks the fine-grained instance discrimination needed to separate visually similar individuals. The model captures local patterns but not the relational identity structure.
- Core assumption: The failure is due to supervision type, not model capacity or architecture.
- Evidence anchors:
  - [section 6.1 Finding 1.3] "DINOv2 exhibits limited effectiveness in the zero-shot ReID setting, achieving only 0.3%-4.7% mAP... purely visual self-supervised pretraining lacks the explicit semantic and relational structure required for reliable identity matching"
  - [section 4.2] "self-supervised models learn strong general-purpose visual features, they lack explicit identity-level supervision"
  - [corpus] YOLO11-JDE (arxiv:2501.13710) incorporates a self-supervised Re-ID branch into detection, but requires joint training with detection—suggesting self-supervised ReID needs task-specific integration, contradicting zero-shot viability.
- Break condition: If self-supervised models with explicit instance-discriminative objectives (e.g., contrastive learning with hard negative mining) succeed at zero-shot ReID, the mechanism shifts from "missing supervision type" to "insufficient pretraining task design."

## Foundational Learning

- Concept: **Distribution Shift and Domain Generalization**
  - Why needed here: The entire paper centers on why models trained on one distribution (surveillance datasets) fail on others (in-the-wild). Understanding that correlation does not imply causation in learned features is essential.
  - Quick check question: If a model trained on campus surveillance achieves 80% mAP, what assumptions must hold for it to maintain performance in a subway station?

- Concept: **Metric Learning and Embedding Spaces**
  - Why needed here: ReID is fundamentally about learning embeddings where same-identity images are closer than different-identity images. The loss functions (triplet, contrastive) directly shape this geometry.
  - Quick check question: In a well-learned ReID embedding space, what should happen to the distance between two images of the same person wearing different clothing vs. two different people wearing similar clothing?

- Concept: **Zero-Shot Transfer vs. Fine-Tuning**
  - Why needed here: The paper compares zero-shot foundation models (CLIP, SigLIP2, DINOv2) against fine-tuned models (CLIP-ReID). The performance gap reveals what pretraining alone can and cannot achieve.
  - Quick check question: Why might a model with strong zero-shot performance on CelebReID underperform on surveillance datasets, and what does fine-tuning actually change?

## Architecture Onboarding

- Component map:
  - **Image encoder** (ViT-B/16, ViT-L/14, or Omni-Scale Net): Maps input crops to embedding vectors
  - **Training head** (identity classifier + metric learning): Supervised models use both; language-aligned models use contrastive/sigmoid image-text loss; self-supervised use teacher-student distillation
  - **Similarity function**: Cosine similarity or L2 distance for ranking gallery images against queries
  - **Evaluation pipeline**: mAP and CMC computation across query-gallery splits

- Critical path:
  1. Input: Person crop (assumes prior detection)
  2. Encoder produces embedding z ∈ R^d
  3. During training: Loss computed (identity cross-entropy + triplet, or image-text contrastive, or self-supervised distillation)
  4. During inference: Similarity scores rank gallery; mAP/CMC computed
  5. Cross-domain test: Same pipeline on out-of-distribution dataset

- Design tradeoffs:
  - **Supervised (OSNet)**: Peak in-domain performance, minimal compute, catastrophic cross-domain failure
  - **Language-aligned zero-shot (SigLIP2)**: Moderate everywhere, no task-specific training required, higher compute
  - **Hybrid fine-tuned (CLIP-ReID)**: Best overall, requires labeled ReID data, risk of semantic prior forgetting
  - **Model scale**: Larger models (PE-Core: 671M) do not guarantee better ReID—data alignment matters more

- Failure signatures:
  - **Supervised cross-domain**: >90% relative mAP drop (e.g., 83% → 3%)
  - **Self-supervised zero-shot**: 0.3-4.7% mAP across all datasets
  - **Vanilla CLIP zero-shot**: 0.1-2.7% mAP (sigmoid vs. softmax loss matters)
  - **Scale without alignment**: PE-Core (671M) underperforms smaller models

- First 3 experiments:
  1. **Reproduce the supervised domain gap**: Train OSNet on MSMT17, evaluate on Market-1501 and CelebReID. Expect 60%+ mAP on MSMT17, <10% on CelebReID. Confirms overfitting mechanism.
  2. **Zero-shot SigLIP2 vs. CLIP on CelebReID**: Extract embeddings from both without fine-tuning, compute mAP. Expect SigLIP2: ~14%, CLIP: <1%. Confirms sigmoid loss + dense captioning advantage.
  3. **Fine-tuning ablation with CLIP-ReID**: Fine-tune on MSMT17 with/without freezing text encoder, evaluate cross-domain retention. Expect frozen text encoder retains more semantic prior but lower in-domain peak. Defines the forgetting-accuracy tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hybrid models be designed to combine supervised discriminative power with language-aligned semantic robustness without suffering from catastrophic forgetting?
- Basis in paper: [explicit] The Future Research Directions section states the need for "advanced hybrid approaches that effectively combine supervised discriminative power with language-aligned semantic robustness, avoiding the catastrophic forgetting of semantic priors."
- Why unresolved: The study establishes a clear trade-off where supervised models overfit to domains (Finding 1.1) while language-aligned models offer stable but lower performance (Finding 1.2). Current fine-tuning methods like CLIP-ReID bridge the gap but lose the zero-shot robustness of the original foundation model.
- What evidence would resolve it: A model that matches OSNet's in-domain performance (e.g., >80% mAP on Market-1501) while retaining SigLIP2's cross-domain stability (e.g., >10% mAP on CelebReID) without requiring target-domain training data.

### Open Question 2
- Question: What architectural or training modifications are required to make self-supervised vision models viable for zero-shot person ReID?
- Basis in paper: [explicit] Finding 1.3 concludes that "purely visual self-supervised pretraining lacks the explicit semantic and relational structure required for reliable identity matching."
- Why unresolved: Despite their success in other vision tasks, models like DINOv2 and Perception Encoder fail to identify individuals (0.3–4.7% mAP), indicating that standard aggregation or loss functions in self-supervision do not capture identity-level features.
- What evidence would resolve it: A self-supervised method achieving significant zero-shot performance (e.g., >15% mAP) on diverse datasets like LasT or IUSReID using only visual pre-training.

### Open Question 3
- Question: How can future ReID systems integrate robustness to clothing changes and temporal reasoning while maintaining generalization?
- Basis in paper: [explicit] Section 7 lists "robustness to clothing changes through gait or body shape modeling" and "long-term tracking" as "critical challenges" for real-world deployment.
- Why unresolved: The current study focuses on static appearance matching using standard benchmarks, which do not fully account for subjects reappearing in different outfits or over extended time horizons.
- What evidence would resolve it: Evaluation results on temporal datasets showing sustained performance when subjects change clothes, implying the model relies on non-clothing biometrics.

## Limitations
- **Training hyperparameters for supervised models** (learning rate, batch size, triplet loss weight λ) are not specified in the paper.
- **Zero-shot evaluation protocol** for foundation models (CLIP, SigLIP2, DINOv2) is not fully detailed—specifically, whether exact preprocessing matches the paper's implementation.
- **No ablation of data augmentation strategies** that could affect cross-domain generalization, leaving open the question of whether simple augmentation changes could reduce supervised model overfitting.

## Confidence
- **High confidence:** Supervised models dominate in-domain but fail catastrophically cross-domain; language-aligned models show moderate, consistent performance; self-supervised zero-shot models perform poorly.
- **Medium confidence:** No correlation between model size and ReID performance (PE-Core vs. smaller models); model size does not predict better cross-domain generalization.
- **Low confidence:** Specific performance numbers (e.g., SigLIP2 achieving exactly 14.2% on CelebReID) due to potential unreported implementation details.

## Next Checks
1. **Cross-domain gap reproduction:** Train OSNet on MSMT17 and evaluate on Market-1501 and CelebReID. Expect >60% mAP on MSMT17, <10% on CelebReID.
2. **SigLIP2 vs. CLIP zero-shot on CelebReID:** Extract embeddings from both models without fine-tuning. Expect SigLIP2 ~14% mAP, CLIP <1% mAP.
3. **CLIP-ReID fine-tuning ablation:** Fine-tune on MSMT17 with/without freezing the text encoder. Measure cross-domain retention and in-domain peak to quantify the forgetting-accuracy tradeoff.