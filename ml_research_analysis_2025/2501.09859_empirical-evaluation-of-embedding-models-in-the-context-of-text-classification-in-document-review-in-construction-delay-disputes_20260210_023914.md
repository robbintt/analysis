---
ver: rpa2
title: Empirical Evaluation of Embedding Models in the Context of Text Classification
  in Document Review in Construction Delay Disputes
arxiv_id: '2501.09859'
source_url: https://arxiv.org/abs/2501.09859
tags:
- text
- embedding
- dataset
- delay
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates four text embedding models\u2014Bag-of-Words,\
  \ Sentence Transformers (MiniLM-L6-v2, nli-mpnet-base-v2), and NVIDIA NeMo\u2019\
  s LLM-based embeddings\u2014for classifying construction delay-related text snippets\
  \ in legal document review. Using KNN and Logistic Regression on datasets from two\
  \ real-world construction delay disputes, the experiments show that embedding models\
  \ outperform Bag-of-Words in cross-matter scenarios, capturing semantic similarities\
  \ across datasets."
---

# Empirical Evaluation of Embedding Models in the Context of Text Classification in Document Review in Construction Delay Disputes

## Quick Facts
- arXiv ID: 2501.09859
- Source URL: https://arxiv.org/abs/2501.09859
- Reference count: 6
- Key outcome: Embedding models outperform bag-of-words for cross-matter legal text classification, with NVIDIA embeddings achieving highest accuracy

## Executive Summary
This study evaluates four text embedding models—Bag-of-Words, Sentence Transformers (MiniLM-L6-v2, nli-mpnet-base-v2), and NVIDIA NeMo's LLM-based embeddings—for classifying construction delay-related text snippets in legal document review. Using KNN and Logistic Regression on datasets from two real-world construction delay disputes, the experiments show that embedding models outperform Bag-of-Words in cross-matter scenarios, capturing semantic similarities across datasets. NVIDIA embeddings consistently achieved the best performance across all settings. Logistic Regression performed well with sufficient training data, while Bag-of-Words lagged in cross-matter tests. The study highlights the potential of embedding models to enhance accuracy in legal text classification, though computational costs remain a challenge.

## Method Summary
The study used two labeled datasets from real construction delay cases: Dataset A (3,240 snippets) and Dataset B (32,303 snippets). Four embedding methods were evaluated: Bag-of-Words/TF-IDF, all-MiniLM-L6-v2 (384-dim), nli-mpnet-base-v2 (768-dim), and nv-embed-v2 (4096-dim). Two classifiers were tested: KNN (K=3) and Logistic Regression. Performance was evaluated using 3-fold cross-validation within matters and cross-matter validation (training on one matter, testing on another). Results were visualized using precision-recall curves, with all experiments executed on an Azure Cloud GPU server.

## Key Results
- Embedding models consistently outperformed Bag-of-Words, especially in cross-matter validation
- NVIDIA nv-embed-v2 achieved the highest performance across all experiments and settings
- Bag-of-Words + Logistic Regression performed nearly as well as NVIDIA embeddings in within-matter validation
- Bag-of-Words + KNN showed poor performance in cross-matter scenarios

## Why This Works (Mechanism)

### Mechanism 1
Embedding models capture semantic relationships that generalize across document collections from different organizations. Embeddings encode text into continuous vector spaces where semantically similar concepts cluster together regardless of exact word overlap. Bag-of-words captures only syntactic similarity (word frequency), which varies significantly when different matters use different terminology for similar delay concepts. This assumes delay-related snippets from different legal matters share underlying semantic patterns but may differ in surface vocabulary and phrasing.

### Mechanism 2
KNN benefits more from rich embeddings than Logistic Regression because KNN's distance-based decisions depend directly on vector similarity quality. KNN classifies based on nearest neighbors in embedding space—performance is bounded by how well the embedding captures task-relevant similarity. Logistic Regression learns weighted decision boundaries and can adaptively emphasize discriminative features, extracting signal even from sparse bag-of-words representations when training data is sufficient. This assumes sufficient labeled training data enables LR to learn meaningful feature weights.

### Mechanism 3
Higher-dimensional embeddings from larger models capture more nuanced semantic distinctions relevant to specialized legal domains. nv-embed-v2 produces 4096-dimensional vectors vs. MiniLM's 384 dimensions—higher capacity enables encoding of finer-grained semantic distinctions, particularly valuable where subtle language differences carry legal significance. This assumes legal/technical documents contain semantically dense information requiring higher-dimensional representation for optimal discrimination.

## Foundational Learning

- Concept: **Precision-Recall Tradeoff in Imbalanced Classification**
  - Why needed here: Dataset is highly imbalanced (~12% positive class: 4,357 delay vs. 31,186 non-delay). PR curves, not accuracy, are the appropriate evaluation metric.
  - Quick check question: If a model predicts "not delay" for all 35,543 snippets, what is its accuracy? (Answer: ~88%, but recall = 0%)

- Concept: **Cross-Matter (Domain Transfer) Validation**
  - Why needed here: The paper's key contribution is testing generalization—training on one legal matter, testing on another—rather than within-matter memorization.
  - Quick check question: Why is cross-matter performance typically lower than within-matter cross-validation? (Answer: Different vocabulary, organizations, document types, and delay event patterns)

- Concept: **Text Embeddings as Fixed Feature Extractors**
  - Why needed here: Understanding that embeddings are pre-trained and frozen (not fine-tuned here) clarifies why semantic knowledge transfers but domain-specific nuances may be missed.
  - Quick check question: What happens if a delay concept uses terminology not in the embedding model's pre-training corpus? (Answer: Representation quality degrades; may cluster incorrectly)

## Architecture Onboarding

- Component map: Text Input (emails, PDFs, Office files) → Embedding Layer (4 options: BoW, MiniLM-384d, MPNet-768d, nv-embed-4096d) → Classifier (KNN-k=3 or LR) → Binary Output (delay/not-delay) → Infrastructure: Azure Cloud GPU server required for embedding computation

- Critical path: 1. Extract and preprocess text snippets from source documents, 2. Generate embeddings (model selection drives accuracy/cost tradeoff), 3. Train classifier on labeled snippets, 4. Evaluate via precision-recall curves on held-out data, 5. Deploy for snippet triage in active matters

- Design tradeoffs: Accuracy vs. Speed (nv-embed-v2 highest accuracy but requires much longer processing times than bag-of-words), In-matter vs. Cross-matter (if same-matter training data exists, BoW+LR may suffice; for transfer scenarios, embedding models essential), Dimensionality vs. Storage/Compute (4096-dim embeddings require ~10x storage vs. 384-dim MiniLM)

- Failure signatures: BoW+KNN performs poorly in cross-matter (Figures 3-4)—avoid this pairing for transfer scenarios, MPNet struggles at low recall thresholds (Figure 6, <20% recall)—may indicate poor calibration for high-precision regimes, Asymmetric transfer: Model trained on smaller dataset (A: 3,240) tested on larger (B: 32,303) underperforms reverse direction

- First 3 experiments: 1. Establish baseline: BoW + LR with in-matter 3-fold cross-validation to set computational/accuracy floor, 2. Quantify semantic transfer: MiniLM (fastest embedding) on cross-matter A↔B to measure minimum embedding benefit, 3. Measure ceiling: nv-embed-v2 on cross-matter to determine maximum achievable performance vs. computational cost ratio

## Open Questions the Paper Calls Out

- Do the evaluated embedding models retain their performance advantage over bag-of-words when applied to information retrieval and question answering tasks? (The authors state future research will expand scope to these tasks)

- How do the four embedding models compare when integrated into a Retrieval-Augmented Generation (RAG) framework for handling query-response scenarios? (The paper notes ongoing work exploring RAG applications)

- What is the practical impact of these embedding models when applied to business-specific use cases beyond academic classification metrics? (The authors plan to extend evaluation to business-specific use cases)

## Limitations

- Results depend on specific legal domain vocabulary and may not transfer to other document types or legal contexts
- Computational cost of NVIDIA embeddings (4096 dimensions) represents a practical barrier for real-time applications
- Performance differences between embedding models may be overstated given the limited sample size (two legal matters)

## Confidence

- High confidence: Embedding models outperform bag-of-words in cross-matter scenarios (supported by consistent results across multiple experiments and clear mechanism through semantic generalization)
- Medium confidence: NVIDIA embeddings consistently achieve best performance (supported by results but limited to two datasets and no ablation studies on dimensions)
- Medium confidence: KNN benefits more from rich embeddings than Logistic Regression (mechanistically sound but limited corpus support for this specific algorithm-interaction claim)

## Next Checks

1. Implement the exact pipeline with public legal/construction text corpora to verify if embedding advantages persist across different matter pairs and document types
2. Measure actual deployment costs for each embedding model in production, including GPU requirements, inference latency, and storage needs relative to classification accuracy gains
3. Fine-tune sentence transformer models on the combined dataset to determine if embedding quality improvements offset the increased computational costs of nv-embed-v2