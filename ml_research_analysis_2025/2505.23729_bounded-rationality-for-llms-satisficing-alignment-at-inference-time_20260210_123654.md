---
ver: rpa2
title: 'Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time'
arxiv_id: '2505.23729'
source_url: https://arxiv.org/abs/2505.23729
tags:
- alignment
- reward
- satisficing
- decoding
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SITAlign, an inference-time alignment framework
  that applies satisficing principles from bounded rationality to multi-faceted LLM
  alignment. Instead of maximizing all objectives simultaneously, SITAlign optimizes
  a primary objective while enforcing threshold-based constraints on secondary criteria,
  using duality theory to estimate Lagrange multipliers without iterative optimization.
---

# Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time

## Quick Facts
- arXiv ID: 2505.23729
- Source URL: https://arxiv.org/abs/2505.23729
- Authors: Mohamad Chehade; Soumya Suvra Ghosal; Souradip Chakraborty; Avinash Reddy; Dinesh Manocha; Hao Zhu; Amrit Singh Bedi
- Reference count: 40
- Primary result: Inference-time alignment framework that achieves 22.3% improvement in GPT-4 win-tie rate while maintaining threshold-based constraints on secondary objectives

## Executive Summary
This paper introduces SITAlign, an inference-time alignment framework that applies satisficing principles from bounded rationality to multi-faceted LLM alignment. Rather than maximizing all objectives simultaneously, SITAlign optimizes a primary objective while enforcing threshold-based constraints on secondary criteria. The framework uses duality theory to estimate Lagrange multipliers without iterative optimization, enabling efficient inference-time alignment. Experiments on three benchmarks demonstrate that this satisficing approach outperforms state-of-the-art multi-objective decoding strategies while maintaining computational efficiency.

## Method Summary
SITAlign reformulates multi-objective LLM alignment as a constrained optimization problem where a primary reward is maximized subject to threshold constraints on secondary rewards. The key innovation is using duality theory to compute Lagrange multipliers via a closed-form quadratic approximation, eliminating the need for iterative optimization during inference. For each token, the method estimates Transfer Q* values through trajectory sampling from pre-aligned baseline policies, computes λ* using gradient and Hessian information at λ=0, and generates token distributions that satisfy the constraints while optimizing the primary objective.

## Key Results
- Achieves 22.3% improvement in GPT-4 win-tie rate for helpfulness reward on PKU-SafeRLHF benchmark
- Maintains harmlessness thresholds with ≥50% win-tie rate on constrained reward
- Ablation study validates that setting appropriate thresholds is sufficient rather than continuous maximization
- Outperforms state-of-the-art multi-objective decoding strategies across all three benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Threshold-Sufficient Alignment
The paper demonstrates that setting acceptable thresholds on secondary objectives is sufficient for satisfying human preferences, rather than continuous maximization of all rewards. This works because reward scores correlate with preference satisfaction, showing diminishing returns once thresholds are met. For harmlessness on PKU-SafeRLHF, responses with scores above -12 (on a [-36, 0] scale) were judged 90%+ harmless by GPT-4.

### Mechanism 2: Closed-Form Lagrange Multiplier Estimation
Lagrange multipliers can be computed via quadratic approximation without iterative optimization, enabling inference-time alignment. The dual objective is approximated using a second-order Taylor expansion of Zλ around λ=0, yielding a closed-form solution that estimates λ* using only gradient and Hessian information at λ=0.

### Mechanism 3: Transfer Q* for Tractable Action-Value Estimation
Optimal action-value functions Q* can be estimated at inference time using pre-aligned trajectory-level baseline policies. TQ* (Transfer Q*) estimates Q* by sampling future trajectories from a baseline policy ρBL_i that was pre-aligned to each reward, avoiding the need to compute Q* directly during decoding.

## Foundational Learning

- **Lagrangian Duality and KKT Conditions**
  - Why needed here: The framework reformulates constrained alignment as a min-max Lagrangian problem. Understanding why strong duality holds (convex objective + linear constraints) is essential.
  - Quick check question: Given the optimization in Eq. 2, why does strong duality apply, and what does λi = 0 indicate about constraint i?

- **Markov Decision Processes for Token Generation**
  - Why needed here: Decoding is modeled as an MDP where states are [prompt, partial response] and actions are next tokens. The action-value function Qπ guides token selection.
  - Quick check question: At step t with state st = [x, y<t], what is the transition function, and why is it deterministic?

- **KL-Divergence Regularization**
  - Why needed here: The β1 DKL term controls deviation from the reference policy, ensuring the aligned policy doesn't drift too far and maintaining strong convexity.
  - Quick check question: How does increasing β1 affect the tradeoff between reward optimization and policy conservatism?

## Architecture Onboarding

- **Component map:**
  - Token-level baseline policy πBL (e.g., Zephyr-7B-β) -> Trajectory-level baseline policies ρBL_i (one per reward) -> Reward models r1 (primary), r2...rN (secondary) -> TQ* estimator (samples trajectories from ρBL_i to compute expected rewards) -> Lagrange multiplier estimator (computes λ* via quadratic approximation) -> Token distribution calculator (produces π*Alg using modified softmax)

- **Critical path:**
  1. At each step t, sample top-k tokens from πBL
  2. For each candidate z, compute TQ*_i(st, z) for all N rewards by sampling trajectories
  3. Compute ∇Zλ and ∇²Zλ at λ=0
  4. Solve closed-form for λ* (Eq. 7)
  5. Compute final token probabilities: π*Alg ∝ πBL · exp(Σ λi · TQ*_i / β1)

- **Design tradeoffs:**
  - k (top-k sampling): Higher k = better exploration but more TQ* computations
  - β1 (KL coefficient): Controls conservatism; too low → incoherent outputs; too high → constrained optimization
  - Thresholds β2...βN: Primary tuning knobs; higher values enforce stricter constraints but may hurt primary objective

- **Failure signatures:**
  - Constraint violations (secondary rewards below threshold) → thresholds too aggressive or λ estimation error
  - Poor primary reward → β1 too large (overly conservative) or TQ* estimates noisy
  - Incoherent/overly safe outputs → constraints too tight or k too small

- **First 3 experiments:**
  1. **Threshold calibration:** Replicate Section 3.1 analysis on your reward models—plot GPT-4 win-rate vs. reward score bins to identify plateau points.
  2. **Single-constraint validation:** Test with one primary (helpfulness) and one secondary (harmlessness) objective; verify λ* activates only when constraint is at risk.
  3. **Compute/quality sweep:** Measure win-tie rates and latency across k ∈ {5, 10, 20} to find practical operating point.

## Open Questions the Paper Calls Out

### Open Question 1
Can satisficing alignment mitigate reward overoptimization in conventional alignment approaches? The paper states this as an important question but doesn't specifically analyze reward hacking or overoptimization failure modes. Evidence would come from experiments comparing satisficing vs. traditional alignment on benchmarks known to exhibit reward overoptimization.

### Open Question 2
Can thresholds be learned adaptively at inference-time rather than requiring a priori estimation? The current approach requires manual threshold selection before deployment. Evidence would come from development and evaluation of an online threshold-learning mechanism that adjusts thresholds based on real-time feedback.

### Open Question 3
Does satisficing alignment generalize effectively across diverse cultural and linguistic contexts? The paper evaluates only on English benchmarks without testing cross-cultural applicability of threshold-based constraints. Evidence would come from multilingual and cross-cultural evaluations.

### Open Question 4
How does satisficing alignment scale to settings with more than two competing objectives? All experiments use only two objectives, though the formulation supports N constraints. Evidence would come from experiments with 3+ objectives analyzing performance and computational overhead.

## Limitations

- Baseline policy specification lacks training procedure details, creating ambiguity in reproduction
- Experimental scope limited to 7B parameter models without testing larger model generalization
- Computational overhead and inference latency not reported, making practical utility uncertain

## Confidence

- **High Confidence:** Theoretical framework is mathematically sound and well-specified. Core claim that threshold-based alignment can outperform continuous maximization is supported by ablation studies.
- **Medium Confidence:** Experimental results show consistent improvements across benchmarks, but absence of detailed computational analysis and limited model size coverage reduces confidence in real-world applicability.
- **Low Confidence:** Practical implementation details (baseline policy training, trajectory sampling parameters, numerical stability measures) are underspecified, creating uncertainty in faithful reproduction.

## Next Checks

1. **Threshold Calibration Validation:** Replicate the correlation analysis between reward scores and GPT-4 evaluations on your specific reward models to verify that satisficing thresholds align with preference plateaus.

2. **Computational Efficiency Benchmark:** Measure inference latency and compare win-tie rates across different trajectory sample sizes (K=8, 16, 32) to determine the practical operating point that balances quality and speed.

3. **Constraint Violation Analysis:** Systematically test constraint violations by varying threshold values (β) and monitoring Lagrange multiplier behavior (λᵢ) to validate the closed-form estimation accuracy and identify failure modes.