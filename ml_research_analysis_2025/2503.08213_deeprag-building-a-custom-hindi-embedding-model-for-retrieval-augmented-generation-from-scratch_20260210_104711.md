---
ver: rpa2
title: 'DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation
  from Scratch'
arxiv_id: '2503.08213'
source_url: https://arxiv.org/abs/2503.08213
tags:
- hindi
- embeddings
- embedding
- semantic
- deeprag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepRAG, a custom Hindi text embedding model
  built from scratch for Retrieval Augmented Generation (RAG) systems. The authors
  developed a comprehensive framework including corpus collection (2.7M samples),
  specialized tokenizer training with SentencePiece, custom transformer architecture
  with Hindi-specific attention mechanisms, and training with contrastive learning
  techniques.
---

# DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation from Scratch

## Quick Facts
- arXiv ID: 2503.08213
- Source URL: https://arxiv.org/abs/2503.08213
- Reference count: 23
- Custom Hindi embedding model achieves 23% improvement in retrieval precision over multilingual baselines

## Executive Summary
This paper presents DeepRAG, a custom Hindi text embedding model built from scratch for Retrieval Augmented Generation (RAG) systems. The authors developed a comprehensive framework including corpus collection (2.7M samples), specialized tokenizer training with SentencePiece, custom transformer architecture with Hindi-specific attention mechanisms, and training with contrastive learning techniques. Their model significantly outperforms multilingual alternatives, achieving a 23% improvement in retrieval precision on Hindi semantic similarity tasks. The methodology provides a roadmap for creating domain-specific embeddings for low-resource languages where general-purpose multilingual models fall short.

## Method Summary
DeepRAG uses a custom SentencePiece Unigram tokenizer trained on 2.7M Hindi texts with 50K vocabulary and 87.3% morphological segmentation accuracy. The model employs a 12-layer transformer encoder with 768-dimensional embeddings, rotary positional embeddings, and multi-resolution attention. Weighted pooling accounts for Hindi's SOV word order, and training uses a mixed loss function (0.5 MSE + 0.3 contrastive + 0.2 triplet) with 300K hard negatives and 450K synthetic pairs. The system achieves P@1 of 0.79 and semantic similarity of 0.81 on Hindi benchmarks.

## Key Results
- 23% improvement in retrieval precision over multilingual models
- P@1 of 0.79 vs 0.67 for mE5-base on Hindi benchmarks
- 87.3% morphological segmentation accuracy with 4.2% OOV rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Custom SentencePiece tokenizer improves embedding quality by preserving morphological boundaries
- Mechanism: Unigram language model with 50K vocabulary segments Hindi compound words into semantically meaningful units rather than arbitrary subword fragments
- Core assumption: Hindi's agglutinative morphology benefits from linguistically-informed segmentation
- Evidence anchors: 87.3% morphological segmentation accuracy, 4.2% OOV rate; 9% MRR improvement when using custom tokenizer vs multilingual
- Break condition: Heavy English-Hindi code-mixing or technical jargon not represented in training samples

### Mechanism 2
- Claim: Weighted pooling accounting for SOV word order outperforms standard pooling
- Mechanism: Learned weighting function combines token position sensitivity, attention-derived importance, and layer-wise contextual significance
- Core assumption: Position-aware pooling captures syntactic structure relevant to semantic similarity
- Evidence anchors: 9.3% improvement in semantic similarity tasks vs standard mean pooling; MRR drops from 0.86 to 0.81 without weighted pooling
- Break condition: Very short queries (<5 tokens) or highly informal text with non-standard word order

### Mechanism 3
- Claim: Mixed loss function produces more robust embeddings than single-objective training
- Mechanism: Combined loss L = 0.5×MSE + 0.3×contrastive + 0.2×triplet balances absolute similarity prediction, relative ranking, and margin-based separation
- Core assumption: Fixed weight ratios are near-optimal across Hindi semantic tasks
- Evidence anchors: Mixed loss outperforms individual loss functions; MRR drops from 0.86 to 0.80 using only MSE
- Break condition: Retrieval tasks with extreme class imbalance or binary relevance only

## Foundational Learning

- Concept: **SentencePiece Unigram tokenization**
  - Why needed here: Uses Unigram language model rather than BPE/WordPiece for Hindi
  - Quick check question: Given a new Hindi compound word, can you predict whether the tokenizer will split it and where?

- Concept: **Contrastive learning with hard negatives**
  - Why needed here: Training pipeline uses 300K "challenging negative pairs with subtle semantic differences"
  - Quick check question: Why would including easy negatives hurt contrastive learning effectiveness?

- Concept: **Rotary Position Embeddings (RoPE)**
  - Why needed here: Uses RoPE instead of learned positional embeddings for better handling of Hindi's free word order
  - Quick check question: How does RoPE's relative position encoding differ from absolute position embeddings?

## Architecture Onboarding

- Component map: Raw Hindi text → [Cleaning pipeline] → [Custom SentencePiece tokenizer] → Token IDs → [12-layer Transformer encoder with RoPE + Multi-resolution attention] → Hidden states → [Weighted pooling] → [L2 normalization] → 768-dim embedding

- Critical path: The tokenizer is the highest-impact single component. Ablation shows removing it drops MRR by 0.09—more than any other single change. Start here when debugging.

- Design tradeoffs:
  - Vocabulary size (50K): Larger vocab showed diminishing returns; smaller increased OOV
  - Model dimensionality (768): Balanced capacity vs inference speed
  - Training from scratch vs fine-tuning: Ground-up approach enables language-specific optimization but requires ~2.7M cleaned samples

- Failure signatures:
  - High OOV rate on domain-specific text indicates tokenizer coverage gap
  - Retrieval returns semantically unrelated documents → check if weighted pooling weights are being learned
  - Training loss plateaus early → hard negatives may be too difficult or data augmentation too aggressive

- First 3 experiments:
  1. **Tokenizer validation**: Run trained tokenizer on 100 held-out Hindi documents from target domain. Measure OOV rate and inspect morphological segmentation quality. Target: <5% OOV, meaningful subword units.
  2. **Pooling ablation on your data**: Compare weighted pooling vs mean pooling on small retrieval benchmark (100-500 queries). If improvement is <5%, SOV-aware weighting may not transfer to your text style.
  3. **Loss function sensitivity**: Train 6-layer variant with MSE-only vs mixed loss on subset of data. Verify mixed loss advantage before full training run.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can DeepRAG methodology be effectively transferred to other Indic languages while preserving language-specific optimizations?
  - Basis: Future work could explore expanding to other Indic languages while preserving optimizations
  - Why unresolved: Only validated for Hindi; other Indic languages have different morphological patterns
  - Evidence needed: Applying same methodology to another Indic language and measuring comparable gains

- **Open Question 2**: What is the impact of synthetic training data quality on embedding performance?
  - Basis: Used 450,000 LLM-generated pairs but no analysis separates contribution of synthetic vs human-curated pairs
  - Why unresolved: Doesn't evaluate whether synthetic pairs introduce artifacts, biases, or domain gaps
  - Evidence needed: Ablation study comparing models with and without synthetic pairs on out-of-domain retrieval tasks

- **Open Question 3**: What are the scaling laws for Hindi embedding models?
  - Basis: Corpus is orders of magnitude smaller than English models; performance ceiling unknown
  - Why unresolved: No experiments with larger data scales reported
  - Evidence needed: Training experiments with progressively larger Hindi corpora (10M, 50M, 100M samples)

## Limitations

- **Reproducibility Barriers**: Lacks critical implementation details for hard negative mining, synthetic data generation, and multi-resolution attention
- **Generalization Scope**: 23% improvement measured on curated Hindi benchmarks; not validated on real-world RAG deployments or code-mixed text
- **Computational Requirements**: Training from scratch requires 4× A100 GPUs and ~2.7M cleaned samples; 75% size reduction claim lacks quantified performance degradation data

## Confidence

- **High Confidence**: Custom tokenizer architecture, transformer encoder specifications, and mixed loss function formulation
- **Medium Confidence**: 23% retrieval precision improvement supported by ablation studies but lacks external validation
- **Low Confidence**: Multi-resolution attention and morphology-aware feed-forward layers lack sufficient architectural detail

## Next Checks

1. **Tokenizer Transferability Test**: Apply trained DeepRAG tokenizer to 100 Hindi documents from domain not in original corpus. Measure OOV rate and conduct expert linguistic evaluation of morphological segmentation. Target: <7% OOV rate and preservation of meaningful morphological boundaries.

2. **Cross-Domain Retrieval Performance**: Evaluate DeepRAG embeddings on Hindi RAG benchmark using queries from different source than training data. Compare retrieval MRR against multilingual baselines. Document performance degradation patterns.

3. **Ablation on Loss Components**: Train 6-layer variant using only MSE loss, only contrastive loss, and only triplet loss on 10% subset of training data. Quantify individual contribution of each loss component to validate claimed optimal weight ratios.