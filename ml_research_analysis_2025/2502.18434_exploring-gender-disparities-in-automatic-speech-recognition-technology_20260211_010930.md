---
ver: rpa2
title: Exploring Gender Disparities in Automatic Speech Recognition Technology
arxiv_id: '2502.18434'
source_url: https://arxiv.org/abs/2502.18434
tags:
- training
- gender
- women
- speech
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores gender bias in automatic speech recognition
  (ASR) systems by examining how training data composition affects performance across
  genders. Using the LibriSpeech dataset and the Whisper small model, the research
  investigates the impact of gender distribution in training data, speech content
  readability, semantic similarity, and pitch variability on ASR accuracy.
---

# Exploring Gender Disparities in Automatic Speech Recognition Technology

## Quick Facts
- arXiv ID: 2502.18434
- Source URL: https://arxiv.org/abs/2502.18434
- Authors: Hend ElGhazaly; Bahman Mirheidari; Nafise Sadat Moosavi; Heidi Christensen
- Reference count: 4
- One-line primary result: Optimal fairness and performance in ASR occur at 60-70% women representation in training data, not at 50-50 splits.

## Executive Summary
This study investigates gender bias in automatic speech recognition systems by systematically varying the gender composition of training data. Using the LibriSpeech dataset and Whisper small model, the research demonstrates that optimal fairness-performance tradeoffs occur at specific gender distributions rather than simple 50-50 splits. The findings reveal that pitch variability significantly influences ASR accuracy and that fine-tuning pre-trained models on balanced data can substantially reduce gender bias.

## Method Summary
The study creates 11 training subsets from LibriSpeech with women's representation ranging from 0% to 100% while maintaining constant total audio files. The Whisper small model is fine-tuned on each subset using fixed hyperparameters (3000 steps, lr=5e-5, batch=16), then evaluated on the TestOther set. WER is computed per gender and WER gap (men minus women) is analyzed. The research also examines pitch distributions, text readability, and semantic similarity across subsets to understand their impact on ASR performance.

## Key Results
- Optimal fairness-performance tradeoffs occur at 60-70% women representation, not 50-50 splits
- Fine-tuning reduced gender bias by up to 97% while improving overall WER
- Pitch variability significantly influences ASR accuracy, with more balanced training sets covering broader pitch distributions
- The smallest positive WER gap (0.09%) occurred at 60% women representation

## Why This Works (Mechanism)

### Mechanism 1
Training data gender composition affects WER asymmetrically—increasing women's representation improves female speaker accuracy but can degrade male speaker performance, with equilibrium at 60-70% women. Core assumption: observed non-linearity reflects underlying acoustic feature distributions correlated with gender.

### Mechanism 2
Training sets with broader pitch distribution coverage better match test set pitch characteristics, enabling models to handle wider acoustic ranges. Core assumption: pitch diversity, not speaker count, drives generalization benefit.

### Mechanism 3
Fine-tuning pre-trained ASR models on domain-specific data reduces pre-existing gender bias through exposure to balanced speaker representations. Core assumption: bias reduction stems from representation, not hyperparameter changes.

## Foundational Learning

- Concept: **Word Error Rate (WER) and WER Gap as Fairness Metrics**
  - Why needed here: Core evaluation; WER measures transcription accuracy, WER gap quantifies bias
  - Quick check question: If Model A has 8% WER for women and 7% for men, and Model B has 10% WER for both, which is "fairer" by the gap metric?

- Concept: **Fundamental Frequency (F0) and Pitch Distribution**
  - Why needed here: Pitch extraction (50-600Hz range) analyzes acoustic diversity; understanding pitch distributions is essential for Section 3.3
  - Quick check question: Why might a training set with only low-pitch speakers fail on a test set with high-pitch speakers, even if gender-balanced?

- Concept: **Fine-tuning vs. Training from Scratch**
  - Why needed here: Experiments use pre-trained Whisper small fine-tuned on LibriSpeech subsets; understanding fixed hyperparameters is crucial
  - Quick check question: What is the risk of comparing fine-tuning results across datasets with different hyperparameter settings?

## Architecture Onboarding

- Component map: Whisper small model -> 11 training subsets with varying gender ratios -> LibriSpeech dev set validation -> TestOther evaluation
- Critical path: 1) Extract pitch distributions using Parselmouth 2) Create 11 training subsets 3) Fine-tune Whisper small on each subset 4) Evaluate on TestOther 5) Compute WER and WER gap 6) Analyze correlations
- Design tradeoffs: Small model chosen due to compute constraints; results may not generalize to larger models; binary gender metadata limits analysis
- Failure signatures: WER gap increasing despite balanced data → check pitch distribution; high TestOther WER → verify noise robustness; inconsistent patterns → inspect text content
- First 3 experiments: 1) Replicate 11-subset fine-tuning on target model 2) Control for pitch with matched diversity subsets 3) Cross-dataset validation on Common Voice

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed optimal gender distributions (60–70% women) generalize across different ASR architectures, model sizes, and languages? Only Whisper small and English were tested; larger models and other languages may exhibit different trade-offs.

### Open Question 2
How do other speaker demographics (age, accent, race) interact with pitch variability and gender to affect ASR fairness? LibriSpeech only provides gender metadata, limiting intersectional analysis.

### Open Question 3
Does the nonlinear relationship between training-set gender ratio and WER hold for spontaneous conversational speech, not just read audiobook content? LibriSpeech comprises read speech, which differs from spontaneous speech acoustically and prosodically.

## Limitations

- Findings specific to Whisper small model and LibriSpeech corpus; may not generalize to larger models or other datasets
- Binary gender classification excludes non-binary speakers; results limited to female/male categories
- Optimal ratio (60-70% women) may be model- and corpus-dependent; 97% bias reduction claim lacks independent validation

## Confidence

**High Confidence**: Methodological approach of systematically varying training data gender composition is sound; correlation between pitch variability and ASR performance is well-established.

**Medium Confidence**: Specific optimal ratio of 60-70% women representation may be model- and corpus-dependent; 97% bias reduction figure needs independent verification.

**Low Confidence**: Cross-linguistic generalization claims unsupported without testing on non-English datasets; assumption that pitch variability is primary driver requires further validation.

## Next Checks

1. **Cross-Model Validation**: Replicate gender distribution experiments using Whisper medium/large models and other ASR architectures to determine if the 60-70% optimal ratio persists.

2. **Controlled Pitch Experiment**: Create training subsets with matched pitch distribution diversity but varying gender ratios to isolate whether pitch or representation drives observed effects.

3. **Cross-Corpus Testing**: Apply the 60-70% women optimal ratio to a different corpus (e.g., Common Voice) to test whether the ratio generalizes beyond LibriSpeech.