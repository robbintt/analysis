---
ver: rpa2
title: 'ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic
  Dynamical Systems'
arxiv_id: '2601.01982'
source_url: https://arxiv.org/abs/2601.01982
tags:
- reasoning
- systems
- chaotic
- logical
- chaos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChaosBench-Logic, a benchmark that evaluates
  large language models' (LLMs) logical and symbolic reasoning on chaotic dynamical
  systems using a first-order logic ontology. The benchmark includes 621 questions
  across 30 systems and seven reasoning task families.
---

# ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems

## Quick Facts
- arXiv ID: 2601.01982
- Source URL: https://arxiv.org/abs/2601.01982
- Reference count: 28
- Key outcome: LLMs achieve high per-item accuracy (91-94%) but fail completely on compositional questions and exhibit fragile multi-turn dialogue coherence (53.1-75.5%).

## Executive Summary
This paper introduces ChaosBench-Logic, a benchmark that evaluates large language models' logical and symbolic reasoning on chaotic dynamical systems using a first-order logic ontology. The benchmark includes 621 questions across 30 systems and seven reasoning task families. Experiments show that frontier LLMs achieve high per-item accuracy but fail completely on compositional questions and exhibit fragile multi-turn dialogue coherence. The work reveals that current models can make locally correct judgments but struggle to maintain globally consistent reasoning under formal logical constraints.

## Method Summary
ChaosBench-Logic evaluates LLMs on 30 chaotic dynamical systems using a first-order logic ontology with 11 semantic predicates. The benchmark includes 621 questions across seven task families including atomic QA, multi-hop implications, cross-system analogies, counterfactuals, bias probes, multi-turn dialogues, and compositional reasoning. Models are evaluated using zero-shot and chain-of-thought prompting with temperature=0. Responses are normalized and evaluated for per-item accuracy (Acc), dialogue accuracy (DlgAcc), contradiction counts, and axiom-violation counts under 13 explicit FOL axioms. The dataset and evaluation code are available on HuggingFace and GitHub.

## Key Results
- LLMs achieve high per-item accuracy (91-94%) on single-item questions
- Dialogue-level accuracy is significantly lower (53.1% to 75.5%), showing fragile multi-turn coherence
- Compositional questions score 0% across all models due to converse fallacy failures
- Chain-of-thought prompting degrades logical coherence (e.g., GPT-4: -5.8pp Acc, -16.3pp DlgAcc)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High single-item accuracy coexists with fragile multi-turn coherence because models lack explicit global belief-state maintenance.
- Mechanism: LLMs produce locally correct judgments via pattern matching, but each turn is effectively independent—no persistent symbolic state enforces consistency under FOL axioms.
- Core assumption: Per-turn accuracy approximates p≈0.92; dialogue-level accuracy compounds as p^T, and correlated drift worsens this further.
- Evidence anchors: [abstract] dialogue accuracy ranges from 53.1% to 75.5%; [Section 5.1] models can produce correct local judgments but struggle to maintain consistent global world models.

### Mechanism 2
- Claim: Compositional failure (0% across all models) stems from converse fallacy—heuristic inversion of one-way implications.
- Mechanism: The axiom system contains only forward implications (e.g., Chaotic ⇒ Sensitive), but models implicitly treat these as bidirectional. When compositional items require rejecting "Sensitive ⇒ Chaotic," models fail.
- Core assumption: The failure is systematic, not random—models overgeneralize correlational patterns from training data.
- Evidence anchors: [Section 5.2] converse assumptions are a key failure mode; [Section 3.4] reverse implications are not assumed to avoid overspecification.

### Mechanism 3
- Claim: Chain-of-thought prompting degrades logical coherence by introducing unsupported intermediate assumptions.
- Mechanism: CoT increases output length and apparent rigor, but also injects plausible-yet-unjustified inferences (e.g., conflating "random-looking" with "random"), which propagate into contradictions across turns.
- Core assumption: Longer reasoning traces amplify, rather than correct, implicit misconceptions.
- Evidence anchors: [Section 5.4] CoT reduces both accuracy and dialogue-level coherence; [Table 5] GPT-4 shows -5.8pp Acc and -16.3pp DlgAcc drops.

## Foundational Learning

- **First-Order Logic (Horn-style implications)**
  - Why needed here: The benchmark's ground truth is computed via forward chaining over Φ; without understanding implication directionality, you cannot debug model failures or extend the axiom system.
  - Quick check question: Given ∀s: Chaotic(s) ⇒ Sensitive(s), can you validly infer ¬Chaotic(s) from ¬Sensitive(s)? (Answer: Yes, contrapositive. But cannot infer Chaotic(s) from Sensitive(s).)

- **Chaos Theory Semantics (determinism vs. randomness vs. sensitivity)**
  - Why needed here: The predicate vocabulary (Chaotic, Random, Deterministic, PosLyap, Sensitive) encodes domain distinctions; misreading these produces invalid questions.
  - Quick check question: Is a chaotic system random? (Answer: No—chaos is deterministic but exhibits sensitive dependence on initial conditions.)

- **Forward Chaining / Logical Closure**
  - Why needed here: Ground-truth answers are derived via Cl_Φ(A_s)—the least fixed point of repeated implication application.
  - Quick check question: If A_s = {Chaotic(s)}, what else is entailed under Φ? (Answer: Deterministic, PosLyap, Sensitive, PointUnpredictable, StatPredictable, ¬Random.)

## Architecture Onboarding

- **Component map:** System Encoder -> Template Generator -> Human Curator -> Evaluator -> Axiom Checker
- **Critical path:** System encoding → Template-based generation → Human curation → Final filtering → Evaluation. Human curation is the bottleneck; template generation is automatable but requires constraint enforcement.
- **Design tradeoffs:** Lightweight symbolic (current) vs. numerical simulation (future); strict dialogue metric (all-or-nothing) vs. partial credit scoring.
- **Failure signatures:** Contradiction (asserting both Random(s) and Deterministic(s)); Axiom violation (committing A then ¬B where A ⇒ B in Φ); Belief drift (per-turn answers correct in isolation but mutually inconsistent); Converse fallacy (inferring Chaotic from Sensitive or PosLyap).
- **First 3 experiments:** 1) Baseline audit: Run all 621 items through target model; compute Acc per task family and DlgAcc. 2) Axiom-constrained decoding: Implement post-hoc filter rejecting responses violating Φ given prior commitments. 3) Ablate CoT vs. zero-shot: Compare Acc and DlgAcc for same model under both prompting regimes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Chain-of-Thought (CoT) prompting significantly degrade logical accuracy and dialogue coherence on ChaosBench-Logic?
- Basis in paper: Section 5.4 and Table 5 show CoT reduces accuracy (e.g., GPT-4 accuracy drops by 5.8 pp) and dialogue coherence (DlgAcc drops by 16.3 pp).
- Why unresolved: Authors hypothesize CoT may introduce unsupported assumptions or "converse fallacies" but do not isolate the specific failure mechanism.
- Evidence to resolve: Ablation study analyzing intermediate reasoning steps in CoT outputs to identify where logical validity breaks down compared to zero-shot baselines.

### Open Question 2
- Question: Can neuro-symbolic architectures overcome the complete failure (0% accuracy) on compositional reasoning items?
- Basis in paper: Section 6.2 suggests "symbolic verifiers" and "logic-guided decoding" as potential solutions to observed failures in global constraint satisfaction.
- Why unresolved: Current study only evaluates standalone LLMs, which fail to integrate multiple constraints simultaneously.
- Evidence to resolve: Benchmarking a hybrid system using the ChaosBench-Logic axiom system Φ as an external constraint checker to validate or guide LLM outputs.

### Open Question 3
- Question: How does model performance vary when logical reasoning must be integrated with tool-verified numeric diagnostics?
- Basis in paper: Section 6.3 lists "hybrid items that combine symbolic logic with tool-verified numeric diagnostics (e.g., Lyapunov estimation)" as a necessary future extension.
- Why unresolved: Current benchmark separates logical reasoning from numerical simulation, leaving interaction between quantitative estimation and qualitative deduction untested.
- Evidence to resolve: Evaluating models on new task family where they must first calculate/simulate a system property before applying logical ontology.

### Open Question 4
- Question: Can LLMs maintain logical consistency when systems are presented across multiple parameter regimes rather than as static entities?
- Basis in paper: Section 6.3 notes limitation of using "canonical regime labels" and proposes extending benchmark to include "multiple parameter regimes per system."
- Why unresolved: Paper acknowledges current items treat systems as static (e.g., "Lorenz is chaotic"), whereas real-world reasoning requires handling bifurcations where system might become periodic.
- Evidence to resolve: Performance metrics on dataset extension where same system is queried under different parameter conditions (e.g., varying r in logistic map) that alter its predicate assignments.

## Limitations
- The benchmark assumes a single canonical regime per system, which may not capture regime-dependent dynamics in some chaotic systems
- Reproducibility is limited by unspecified prompt templates for each task family and exact API versions of tested models
- The strict all-or-nothing dialogue metric may underrepresent partial competence in multi-turn reasoning

## Confidence

- **High confidence:** The observed degradation of dialogue coherence (DlgAcc: 53.1-75.5%) is robust, given that this is a strict all-or-nothing metric directly computed from model responses and axiom violations.
- **Medium confidence:** The claim that CoT degrades logical coherence (ΔAcc: -5.8pp, ΔDlgAcc: -16.3pp for GPT-4) is supported by the data but may be model-dependent.
- **Medium confidence:** The assertion that compositional questions score 0% due to converse fallacy is plausible given the axiom structure, but evidence is indirect.

## Next Checks
1. Implement axiom-constrained decoding that rejects any response violating Φ given prior commitments; measure reduction in contradiction/axiom-violation counts to validate the mechanism behind dialogue fragility.
2. Generate and test regime-specific items for systems known to have multiple dynamical regimes; compare performance against the canonical-regime baseline to assess this architectural limitation.
3. Test the benchmark on smaller, logic-trained models (e.g., models fine-tuned on formal reasoning datasets) to determine whether the converse fallacy is a fundamental limitation or an artifact of general-purpose pretraining.