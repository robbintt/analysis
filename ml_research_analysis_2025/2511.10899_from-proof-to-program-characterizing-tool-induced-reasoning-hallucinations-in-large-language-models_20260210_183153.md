---
ver: rpa2
title: 'From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations
  in Large Language Models'
arxiv_id: '2511.10899'
source_url: https://arxiv.org/abs/2511.10899
tags:
- reasoning
- tool
- code
- solution
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates Tool-Induced Myopia (TIM), a hallucination
  where tool-augmented language models (TaLMs) substitute tool outputs for mathematical
  reasoning, even when tools are used correctly. Focusing on the Code Interpreter,
  it introduces PYMATH, a benchmark of 1,679 competition-level math problems where
  code is helpful but not sufficient.
---

# From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models

## Quick Facts
- **arXiv ID:** 2511.10899
- **Source URL:** https://arxiv.org/abs/2511.10899
- **Reference count:** 40
- **Primary result:** Tool-augmented LLMs achieve up to 19.3 percentage point gains in final-answer accuracy but consistently exhibit degraded reasoning quality, with non-tool LLMs winning up to 41.5% more pairwise comparisons of reasoning process.

## Executive Summary
This study investigates Tool-Induced Myopia (TIM), a hallucination where tool-augmented language models (TaLMs) substitute tool outputs for mathematical reasoning, even when tools are used correctly. Focusing on the Code Interpreter, it introduces PYMATH, a benchmark of 1,679 competition-level math problems where code is helpful but not sufficient. Using a multi-dimensional evaluation suite, it shows TaLMs achieve up to 19.3 percentage point gains in final-answer accuracy but consistently exhibit degraded reasoning, with non-tool LLMs winning up to 41.5% more pairwise comparisons of reasoning process. The degradation intensifies with tool call frequency and shifts errors from arithmetic to global reasoning failures, with TIM present in ~55% of high-risk cases. Two mitigation strategies are proposed: a prompting intervention and a preference-optimization framework. Fine-tuning with DPO improves final-answer accuracy (+0.6%) and reasoning depth, surpassing both vanilla TaLM and Base LLM.

## Method Summary
The study uses PYMATH, a benchmark of 1,679 competition-level math problems where code is helpful but not sufficient. It evaluates TaLMs and Base LLMs using four metrics: Final-Answer Accuracy, Win Rate (pairwise comparisons), Miss Rate (missing gold steps), and PRM Accuracy. Two mitigation strategies are tested: a prompting intervention ("treat code snippets as hints") and DPO fine-tuning on GPT-4.1 with chosen/rejected pairs contrasting reasoning-rich vs tool-over-reliant solutions.

## Key Results
- TaLMs achieve up to 19.3 percentage point gains in final-answer accuracy but lose up to 41.5% more pairwise reasoning comparisons to non-tool LLMs
- Tool call frequency correlates with reasoning degradation: higher Miss Rate and lower Win Rate as tool usage increases
- DPO fine-tuning improves Win Rate (58.2% vs 45.6%) and Miss Rate (46.6% vs 49.9%) while maintaining accuracy
- TIM is present in approximately 55% of high-risk cases where reasoning is shallow but final answers are correct
- Error type shifts from arithmetic to global reasoning failures as tool usage increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Access to a Code Interpreter can cause models to substitute tool outputs for mathematical reasoning, even when tool execution is correct.
- **Mechanism:** Tool availability biases the model's generation process toward computation-heavy shortcuts (e.g., empirical checks, enumeration) rather than step-by-step derivation. The model treats the tool as a reasoning replacement rather than an aid.
- **Core assumption:** The model's training distribution or prior behavior makes tool calls a low-cost pathway to plausible outputs, which it defaults to under uncertainty.
- **Evidence anchors:** [abstract] "TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification." [Section 2] TIM is defined as a failure mode where "tool access suppresses internal reasoning and induces tool-driven shortcuts."

### Mechanism 2
- **Claim:** Increased tool call frequency correlates with degraded reasoning quality (higher Miss Rate, lower Win Rate).
- **Mechanism:** Each additional tool call creates more opportunities for the model to offload reasoning steps to the tool, accumulating skipped derivations and shallow inferences across the solution trajectory.
- **Core assumption:** Tool calls are not being used strictly for verification; they are interwoven into the reasoning chain as surrogate logic.
- **Evidence anchors:** [Section 5.2] "As tool usage increases... Miss Rate generally rises... PRM Accuracy typically declines." [Section 5.5] Thinking models invoke tools more frequently (49.7% more problems) and thus face higher TIM risk.

### Mechanism 3
- **Claim:** Preference-optimization-based fine-tuning (DPO) can realign TaLMs to use tools as assistive evidence rather than substitutes.
- **Mechanism:** DPO shapes the model's policy by contrasting chosen (reasoning-rich, tool-assisted) and rejected (tool-dependent, shallow) trajectories, reinforcing behaviors where tools support rather than replace derivation.
- **Core assumption:** The preference dataset captures meaningful distinctions between deep and shallow tool use, and the model can internalize this preference without overfitting.
- **Evidence anchors:** [Section 6.3] DPO-tuned GPT-4.1 achieves higher Win Rate (58.2% vs. 45.6% vanilla TaLM) and lower Miss Rate (46.6% vs. 49.9%).

## Foundational Learning

- **Concept: Tool-Augmented Reasoning (TAR)**
  - **Why needed here:** Central to understanding TaLMs and why tools can introduce new hallucination modes like TIM.
  - **Quick check question:** What is the key difference between a tool being *helpful* vs. *sufficient* for solving a problem?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** The paper uses DPO as a mitigation strategy; understanding how it works is critical for implementing and troubleshooting alignment-based fixes.
  - **Quick check question:** In DPO, what does the model learn from the contrast between "chosen" and "rejected" samples?

- **Concept: Process-Based vs. Outcome-Based Evaluation**
  - **Why needed here:** TIM is invisible to outcome metrics (e.g., final-answer accuracy); multi-dimensional process evaluation is required to detect it.
  - **Quick check question:** Why might a model with a correct final answer still be considered to have failed the reasoning process?

## Architecture Onboarding

- **Component map:** PYMATH Dataset -> Evaluation Suite (Final-Answer Accuracy, Win Rate, Miss Rate, PRM Accuracy) -> TaLM Under Test (Code Interpreter access) -> Base LLM Counterpart (no tool access) -> Mitigation Modules (Prompting injector, DPO fine-tuning pipeline)

- **Critical path:**
  1. Curate or load PYMATH problems
  2. Generate solutions from both TaLM and Base LLM
  3. Compute all four evaluation metrics
  4. Analyze tool call frequency vs. reasoning degradation
  5. Apply and benchmark mitigation strategies (prompting, DPO)

- **Design tradeoffs:**
  - Prompting is lightweight and requires no training but yields modest accuracy recovery; DPO requires dataset creation and compute but achieves better Win Rate and Miss Rate
  - PRM-based evaluation may be unreliable for long, tool-heavy traces from thinking models (per Section 5.1)
  - Focusing only on Code Interpreter controls for tool-execution confounds but limits generalizability to other tool types

- **Failure signatures:**
  - High Miss Rate with correct final answers
  - Win Rate <50% (Base LLM preferred over TaLM)
  - Linguistic cues in outputs: "numerical check shows," "systematic search," "let's verify programmatically"
  - Error type shift from arithmetic to logic/assumption/creativity errors (Figure 4)

- **First 3 experiments:**
  1. **Replicate benchmark on a single model:** Run PYMATH eval split through a TaLM and its Base variant; verify the TIM pattern (higher accuracy, lower Win Rate/Miss Rate)
  2. **Test prompting intervention:** Apply the "treat outputs as hints" prompt to the TaLM and compare Win Rate and Miss Rate against vanilla TaLM
  3. **Analyze tool call frequency impact:** Bin problems by number of tool calls (0–3, 4–7, 8–11, 12+) and plot Miss Rate/Win Rate per bin to confirm degradation trend

## Open Questions the Paper Calls Out
- Does Tool-Induced Myopia manifest in tool environments beyond the Code Interpreter, such as web search or domain-specific APIs? (The authors list this as a limitation and call for future work extending the framework to broader tools.)
- Can the proposed preference-optimization mitigation strategy effectively reduce TIM in "thinking" models or architectures other than GPT-4.1? (The study focused exclusively on GPT-4.1 due to computational constraints.)
- Can the identified linguistic precursor phrases (e.g., "one checks numerically") be utilized for real-time intervention to prevent TIM during generation? (The paper identifies these as indicators but does not test active mitigation using them.)

## Limitations
- The study focuses exclusively on Code Interpreter, limiting generalizability to other tool types like web search or knowledge retrieval
- DPO mitigation was validated only on GPT-4.1 with a single epoch of training, so effectiveness on other models or with longer training is uncertain
- PRM-based reasoning quality metrics may be unreliable for long, tool-heavy traces from thinking models, potentially understating TIM severity

## Confidence
- **High Confidence**: The existence of Tool-Induced Myopia (TIM) as a distinct failure mode is well-supported by consistent patterns across metrics—TaLMs achieve higher final-answer accuracy but lose pairwise comparisons to Base LLMs (up to 41.5% more wins for Base), with degraded reasoning quality (Miss Rate up to 49.9%, Win Rate down to 45.6%).
- **Medium Confidence**: The DPO fine-tuning mitigation's effectiveness is moderately supported: GPT-4.1 with DPO achieves improved Win Rate (58.2% vs 45.6%) and Miss Rate (46.6% vs 49.9%), with modest final-answer accuracy gains (+0.6%).
- **Low Confidence**: The automatic detection of TIM via linguistic cues ("numerical check shows," "systematic search") is plausible but not validated—these patterns could be false positives or change with model versions.

## Next Checks
1. **Test DPO Generalization:** Apply the same DPO fine-tuning procedure to at least two additional model families (e.g., Claude, Llama) and compare Win Rate, Miss Rate, and final-answer accuracy to assess robustness across architectures.

2. **Validate TIM Detection:** Manually annotate a random sample of 100 tool-augmented solutions for TIM presence using the linguistic cues, then compare against automatic detection to measure precision/recall of the heuristic.

3. **Cross-Tool Generalizability:** Repeat the PYMATH evaluation with a different tool type (e.g., web search, knowledge base query) to determine if TIM manifests similarly beyond Code Interpreter.