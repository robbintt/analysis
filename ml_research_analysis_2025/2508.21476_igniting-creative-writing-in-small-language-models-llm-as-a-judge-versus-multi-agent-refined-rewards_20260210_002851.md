---
ver: rpa2
title: 'Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus
  Multi-Agent Refined Rewards'
arxiv_id: '2508.21476'
source_url: https://arxiv.org/abs/2508.21476
tags:
- reward
- positive
- arxiv
- greeting
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates enhancing creative writing in small language
  models using two AI-driven reward strategies within a reinforcement learning framework.
  The first employs a refined reward model trained on preference data curated by a
  multi-agent rejection sampling framework.
---

# Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards

## Quick Facts
- arXiv ID: 2508.21476
- Source URL: https://arxiv.org/abs/2508.21476
- Reference count: 40
- This paper investigates enhancing creative writing in small language models using two AI-driven reward strategies within a reinforcement learning framework.

## Executive Summary
This paper investigates enhancing creative writing in small language models using two AI-driven reward strategies within a reinforcement learning framework. The first employs a refined reward model trained on preference data curated by a multi-agent rejection sampling framework. The second, more novel approach utilizes a principle-guided LLM-as-a-Judge with an adversarially optimized reward function. Experiments on generating Chinese greetings using a 7B-parameter model show that both strategies significantly improve creative output over baselines, but the principle-guided LLM-as-a-Judge yields superior generation quality, training efficiency, and reduced dependency on human-annotated data, making it a more scalable path toward creative SLMs.

## Method Summary
The paper proposes two reward strategies for enhancing creative writing in small language models: a refined reward model trained on preference data curated by multi-agent rejection sampling, and a principle-guided LLM-as-a-Judge with adversarially optimized rewards. Both approaches are integrated into a reinforcement learning framework to fine-tune a 7B-parameter model on the task of generating Chinese greetings. The principle-guided LLM-as-a-Judge approach is designed to be more scalable and less dependent on human-annotated data compared to traditional reward models.

## Key Results
- Both the refined reward model and principle-guided LLM-as-a-Judge significantly improve creative output over baselines
- Principle-guided LLM-as-a-Judge yields superior generation quality compared to multi-agent refined rewards
- Principle-guided LLM-as-a-Judge demonstrates better training efficiency and reduced dependency on human-annotated data

## Why This Works (Mechanism)
The mechanism leverages reinforcement learning to fine-tune small language models for creative writing tasks. The multi-agent rejection sampling framework curates high-quality preference data to train a refined reward model, while the principle-guided LLM-as-a-Judge approach uses adversarial optimization to align rewards with creative writing principles. Both methods provide more nuanced and task-specific feedback than traditional reward models, enabling the SLMs to generate more creative and diverse outputs.

## Foundational Learning
- **Reinforcement Learning**: Needed to fine-tune the language model based on reward signals. Quick check: Ensure the RL algorithm (e.g., PPO) is properly implemented and converges.
- **Reward Modeling**: Critical for providing feedback on generated text quality. Quick check: Validate the reward model's ability to distinguish between high and low-quality creative writing.
- **Preference Data Curation**: Essential for training the refined reward model. Quick check: Assess the diversity and quality of the curated preference data.
- **Adversarial Optimization**: Used to align the LLM-as-a-Judge with creative writing principles. Quick check: Monitor the adversarial training process for stability and convergence.

## Architecture Onboarding
- **Component Map**: Language Model -> RL Fine-tuning -> Reward Strategy (Refined or Principle-guided LLM-as-a-Judge)
- **Critical Path**: The reward strategy provides feedback to the RL fine-tuning process, which updates the language model's parameters to improve creative writing performance.
- **Design Tradeoffs**: Refined reward model requires more human-annotated data but may generalize better; principle-guided LLM-as-a-Judge is more scalable but relies on carefully crafted principles.
- **Failure Signatures**: Poor creative output, mode collapse, or unstable training may indicate issues with the reward strategy or RL implementation.
- **3 First Experiments**: 1) Evaluate the baseline model's creative writing performance; 2) Compare the two reward strategies on a held-out validation set; 3) Conduct human evaluations to assess the quality and diversity of generated text.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are based on a single task (Chinese greetings) and a single 7B-parameter model, limiting generalizability.
- Comparison between reward strategies lacks ablation studies on the multi-agent rejection sampling components.
- The principle-guided LLM-as-a-Judge approach relies on carefully crafted principles that may not transfer to other languages or creative domains without significant adaptation.

## Confidence
- **High Confidence**: The experimental results showing both reward strategies outperform baselines on the Chinese greetings task.
- **Medium Confidence**: The claim that principle-guided LLM-as-a-Judge yields superior generation quality and training efficiency.
- **Low Confidence**: The scalability claim regarding reduced dependency on human-annotated data.

## Next Checks
1. Replicate experiments across multiple creative writing tasks (poetry, short stories, dialogue generation) to assess generalizability of the reward strategies.
2. Conduct ablation studies on the multi-agent rejection sampling framework to isolate which components contribute most to refined reward quality.
3. Perform cost-benefit analysis comparing computational requirements and inference latency between principle-guided LLM-as-a-Judge and multi-agent refined rewards approaches.