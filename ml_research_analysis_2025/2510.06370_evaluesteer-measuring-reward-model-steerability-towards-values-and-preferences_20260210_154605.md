---
ver: rpa2
title: 'EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences'
arxiv_id: '2510.06370'
source_url: https://arxiv.org/abs/2510.06370
tags:
- style
- value
- values
- user
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EVALUESTEER introduces a benchmark to measure how well reward models
  can adapt to diverse user values and style preferences. It synthetically generates
  165,888 preference pairs, systematically varying responses along four value and
  four style dimensions.
---

# EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences

## Quick Facts
- **arXiv ID**: 2510.06370
- **Source URL**: https://arxiv.org/abs/2510.06370
- **Reference count**: 40
- **Primary result**: EVALUESTEER reveals that reward models exhibit strong intrinsic biases toward secular values and verbose style, struggle to use user context for steering, and prioritize style over values when preferences conflict.

## Executive Summary
EVALUESTEER introduces a benchmark to measure how well reward models can adapt to diverse user values and style preferences. It synthetically generates 165,888 preference pairs, systematically varying responses along four value and four style dimensions. The benchmark tests whether models can select responses aligned with given user profiles. Across six models and 11 prompting conditions, the best models achieve less than 75% accuracy when given full user context, compared to over 99% when only relevant information is provided. The study reveals that reward models exhibit secular value leanings, strong stylistic biases favoring verbose and confident language, and a tendency to prioritize style over values when preferences conflict. These findings highlight significant limitations in current reward models' ability to adapt to pluralistic user preferences.

## Method Summary
EVALUESTEER synthetically generates 165,888 preference pairs by systematically varying responses along four value dimensions (traditional/secular, survival/self-expression) and four style dimensions (verbosity, reading difficulty, confidence, warmth). User profiles are created from 18 World Values Survey participants, each with defined value and style preferences. Value-laden responses are generated using GPT-4o with value-control prompts, then rewritten into 16 style variants. The benchmark evaluates six reward models (including GPT-4.1-Mini, Gemini-2.5-Flash, and Llama-3.1-8B) across 11 prompting conditions to measure steerability toward user preferences.

## Key Results
- Reward models achieve less than 75% accuracy when given full user context to select aligned responses
- Models show strong intrinsic biases toward secular values and verbose, confident style even without context
- When value and style preferences conflict, models prioritize style preferences over values ~67% of the time
- Models struggle to identify and use relevant user profile information, with 25% accuracy drop when given full context vs. only relevant information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The EVALUESTEER benchmark measures RM steerability toward diverse user values and style preferences through controlled pairwise preference evaluation.
- Mechanism: A user profile (values + style) and two candidate responses are presented to an RM. One response is value/style-aligned with the profile; the other is misaligned. The RM's selection accuracy reveals its ability to steer toward the target preference. The task isolates steerability by holding all linguistic/semantic factors constant except the controlled value and style signals.
- Core assumption: Synthetic, human-grounded user profiles (from WVS) and controlled value/style-laden responses validly proxy real pluralistic human preferences.
- Evidence anchors:
  - [abstract] "We synthetically generated 165,888 preference pairs – systematically varying pairs along 4 value dimensions... and 4 style dimensions... We use EVALUESTEER to evaluate whether, given a user profile and a pair of candidate value-laden and style-laden responses, LLMs and RMs are able to select the output that aligns with the user's preferences."
  - [Section 3] "Since all other linguistic and semantic factors remain constant, EVALUESTEER reveals whether models can be steered toward the correct joint outcome under systematically varied prompt conditions."
  - [corpus] Related work (e.g., "Reward Models Inherit Value Biases from Pretraining") corroborates the importance of evaluating RM alignment, though EVALUESTEER is novel for RM steerability.
- Break condition: If synthetic profiles or controlled responses do not reflect real user value/style preferences; if task conflates value/style signals with other artifacts.

### Mechanism 2
- Claim: RMs have strong intrinsic biases toward secular, self-expression values and verbose/confident style, limiting their steerability to diverse user preferences.
- Mechanism: Measured via "no context" condition, where no user profile is given. The RM's preference rates for specific values (e.g., secular over traditional) and styles (e.g., verbose over concise) reveal its priors. These priors emerge from pretraining/alignment data and interfere with steering to contrary user preferences.
- Core assumption: No-context preference rates accurately reflect intrinsic RM priors.
- Evidence anchors:
  - [abstract] "The study reveals that reward models exhibit secular value leanings, strong stylistic biases favoring verbose and confident language, and a tendency to prioritize style over values when preferences conflict."
  - [Section 5.2] "We find in Figure 3... that models systematically prefer responses that support Secular-Self-expression value profiles. RMs show strong secular tendencies..."
  - [Section 5.2] "Figure 4 shows that all RMs have stylistic preferences, favoring responses that are more verbose, cold / formal, higher in reading difficulty and, specifically for GPT-4.1-Mini, more confident."
  - [corpus] "Reward Models Inherit Value Biases from Pretraining" supports the idea that RMs inherit biases from pretraining.
- Break condition: If no-context rates are confounded by prompt/response artifacts unrelated to priors.

### Mechanism 3
- Claim: RMs struggle to identify and use relevant user profile information for steering, especially when values and styles conflict, defaulting to style over value.
- Mechanism: In the "neutral full context" condition (full profile, no priority guidance), when value and style preferences conflict (value-aligned but style-misaligned vs. style-aligned but value-misaligned), RMs prefer the style-aligned response ~67% of the time. This suggests a "style-over-substance" bias, possibly because style signals are more salient than implicit value reasoning.
- Core assumption: The conflict scenario validly isolates the RM's preference between value vs. style steering.
- Evidence anchors:
  - [abstract] "...a tendency to prioritize style over values when preferences conflict."
  - [Section 5.3] "Overwhelmingly, we find that models steer to the user's style preferences over values by 32.83% across the 6 RMs." (Note: context indicates ~67% style preference, ~33% value preference.)
  - [Section 5.3] "We conduct an additional analysis study... under value and style conflict conditions... we again see significant bias in steering towards responses that feature the preferred styles over values..."
  - [corpus] Limited direct corpus evidence for "style-over-substance" bias in RMs; EVALUESTEER contributes new evidence.
- Break condition: If conflict scenarios are unrepresentative of real user preference conflicts.

## Foundational Learning

- Concept: **Reward Models (RMs) and LLM-as-Judge**
  - Why needed here: EVALUESTEER evaluates both fine-tuned RMs and LLM-as-Judge. Understanding their role in RLHF and alignment is essential for interpreting benchmark results.
  - Quick check question: Can you explain the difference between a fine-tuned reward model and an LLM prompted as a judge?

- Concept: **Inglehart-Welzel Cultural Map / World Values Survey**
  - Why needed here: EVALUESTEER's value profiles are grounded in the WVS. Understanding the traditional/secular and survival/self-expression axes is crucial for interpreting RM value biases.
  - Quick check question: What are the two main axes of the Inglehart-Welzel cultural map, and what do they represent?

- Concept: **Controlled Evaluation / Synthetic Data Generation**
  - Why needed here: EVALUESTEER relies on synthetically generated preference pairs to isolate value and style signals. Understanding the tradeoffs (control vs. ecological validity) is important for assessing the benchmark's generalizability.
  - Quick check question: What is a key advantage of controlled, synthetic evaluation over "in-the-wild" data for testing RM steerability?

## Architecture Onboarding

- Component map:
  - Benchmark Generator -> Evaluation Harness -> RM Interface -> Metrics Calculator
  - User profiles (WVS) -> Synthetic response pairs -> Controlled evaluation -> Accuracy/bias metrics

- Critical path:
  1. Define user value profiles using WVS-based selection
  2. Select PRISM prompts and match to WVS items
  3. Generate value-laden responses using GPT-4o with value-control prompts
  4. Apply style augmentations to generate response variants
  5. Filter for quality using Oracle Setting (GPT-4.1 with only relevant context)
  6. Construct preference pairs based on value/style combinations
  7. Evaluate RMs across prompting conditions and compute accuracy/bias metrics

- Design tradeoffs:
  - Synthetic control vs. ecological validity
  - Breadth of coverage (288 profiles, 24 prompts) vs. computational cost
  - Oracle filtering for quality vs. potential loss of challenging cases

- Failure signatures:
  - RM accuracy near random (50%) in full-context settings → severe steerability limitation
  - Strong preference for specific styles/values in no-context settings → intrinsic bias
  - Significant drop in accuracy when full profile provided vs. only relevant context → difficulty identifying relevant information

- First 3 experiments:
  1. **Baseline steerability**: Evaluate RMs in no-context, value-only, style-only, and combined context conditions to establish baseline accuracy and context impact
  2. **Intrinsic bias probing**: In no-context setting, measure preference rates for each value (e.g., traditional vs. secular) and style (e.g., verbose vs. concise) dimension to characterize RM priors
  3. **Value vs. Style conflict**: In neutral full-context setting, present value/style conflict pairs to quantify style-over-substance bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explicit value reasoning capabilities be incorporated directly into RM training, rather than only at inference time via chain-of-thought prompting?
- Basis in paper: [explicit] The authors state "Future work can investigate how to incorporate this while training RMs" after finding CoT provides modest gains.
- Why unresolved: The paper tests CoT only as an inference intervention; training-time integration of structured value reasoning is unexplored.
- What evidence would resolve it: Training RMs with auxiliary value-reasoning objectives and measuring steerability gains on EVALUESTEER.

### Open Question 2
- Question: What specific architectural or training factors cause reward models to systematically prioritize style over values when both are present?
- Basis in paper: [explicit] The authors note "Attributing these limitations to specific factors in RM capabilities signifies key next steps" after finding a 32.83% style-over-value preference.
- Why unresolved: The paper quantifies the bias but does not isolate whether it stems from pre-training corpora, RLHF data, model scale, or other factors.
- What evidence would resolve it: Ablation studies varying each factor while holding others constant, paired with interpretability analyses of RM attention to value vs. style tokens.

### Open Question 3
- Question: How can user value profiles be modeled to improve RM prediction of downstream preferences in realistic, noisy contexts?
- Basis in paper: [explicit] The authors call for "a more targeted analysis" to understand how to improve user value profile modeling for better preference prediction.
- Why unresolved: Current profiles use ~200 WVS statements; it is unclear whether shorter, more dynamic, or context-adaptive profiles would narrow the 25% oracle gap.
- What evidence would resolve it: Experiments comparing profile representations (compact vs. verbose, static vs. query-adaptive) on steering accuracy across contexts.

## Limitations

- The synthetic preference generation process may not fully capture the complexity and nuance of authentic user preferences
- Intrinsic bias measurements could be confounded by prompt artifacts or training data distribution shifts
- The "style-over-substance" bias finding, while novel, requires validation across different cultural contexts and real-world preference conflicts

## Confidence

- **High Confidence**: The benchmark's construction methodology (synthetic generation, systematic variation, quality filtering) and its basic measurement framework are well-documented and reproducible
- **Medium Confidence**: The specific numerical findings on RM accuracy (less than 75% full-context) and bias patterns require validation across different RM architectures and generation parameters
- **Low Confidence**: The causal attribution of observed biases to pretraining data versus alignment procedures, and the generalizability of style-over-substance bias to non-controlled preference conflicts

## Next Checks

1. **Ecological Validity Test**: Evaluate RMs on authentic user preference data (e.g., from actual preference datasets) to verify whether synthetic-controlled findings transfer to real-world steerability challenges

2. **Bias Attribution Analysis**: Conduct ablation studies varying pretraining data composition and alignment procedures to isolate whether observed value/style biases stem from pretraining, RLHF, or synthetic generation artifacts

3. **Cross-Cultural Robustness Check**: Test whether RM biases toward secular values persist across different cultural contexts by evaluating steerability for profiles drawn from non-Western WVS samples