---
ver: rpa2
title: 'Metaphor identification using large language models: A comparison of RAG,
  prompt engineering, and fine-tuning'
arxiv_id: '2509.24866'
source_url: https://arxiv.org/abs/2509.24866
tags:
- metaphor
- metaphorical
- llms
- identification
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models (LLMs) can effectively
  automate metaphor identification in full texts. We compare retrieval-augmented generation
  (RAG), prompt engineering, and fine-tuning methods, finding that fine-tuning achieves
  the highest accuracy with a median F1 score of 0.79.
---

# Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning

## Quick Facts
- arXiv ID: 2509.24866
- Source URL: https://arxiv.org/abs/2509.24866
- Authors: Matteo Fuoli; Weihang Huang; Jeannette Littlemore; Sarah Turner; Ellen Wilding
- Reference count: 6
- Primary result: Fine-tuning achieves highest accuracy with median F1 score of 0.79 for full-text metaphor identification

## Executive Summary
This study demonstrates that large language models can effectively automate metaphor identification in full texts through three distinct approaches: retrieval-augmented generation (RAG), prompt engineering, and fine-tuning. The research finds that fine-tuning a model on 80% of a manually annotated corpus of 94 IMDb film reviews yields the highest accuracy with a median F1 score of 0.79. Notably, chain-of-thought prompting with reasoning models achieves comparable performance (median F1 0.76) without requiring extensive training data. The study also reveals systematic errors in LLM outputs that align with known theoretical challenges in metaphor research, suggesting these models can semi-automate metaphor annotation and enhance scalability for more robust, generalizable metaphor analyses across diverse discursive domains.

## Method Summary
The study compares three approaches for metaphor identification: RAG using a codebook with task prompts, prompt engineering (zero-shot, few-shot with 4/8 examples, and chain-of-thought with reasoning models), and fine-tuning on 80% of an annotated corpus. The corpus consists of 94 IMDb film reviews (2,828 sentences, 59,147 words) manually annotated with 2,599 metaphorical expressions using a phraseological approach that captures multi-word spans. Five runs were conducted for each configuration using various models including Llama 3.1/3.2, DeepSeek R1, GPT-4.1 series, and o3 reasoning models. Token-level precision, recall, and F1 scores were calculated against human gold standard annotations, with mixed-effects beta regression testing for significance.

## Key Results
- Fine-tuning achieved the highest accuracy with median F1 score of 0.79
- Chain-of-thought prompting with reasoning models performed comparably with median F1 score of 0.76
- RAG performed significantly worse than both fine-tuning and prompt engineering approaches
- Analysis revealed systematic errors in LLM outputs tied to theoretical challenges in metaphor research

## Why This Works (Mechanism)

### Mechanism 1: Fine-tuning for phraseological alignment
Fine-tuning achieves highest accuracy by aligning model predictions with specific phraseological annotation boundaries through supervised weight updates on gold standard data. This reduces ambiguity present in prompt-only instructions by internalizing the specific MIP/VIP protocol for span annotation rather than isolated words.

### Mechanism 2: Chain-of-thought reasoning scaffolding
Chain-of-thought prompting with reasoning models approximates fine-tuning performance by explicitly generating reasoning traces that force semantic incongruity resolution before labeling. This scaffolding process mirrors human MIP procedures by forcing models to identify "basic" vs. "contextual" meanings.

### Mechanism 3: RAG limitations with abstract rules
RAG performs worse due to inability to consistently apply abstract codebook rules across long contexts. While providing explicit rules, models struggle to generalize complex conditional guidelines (e.g., distinguishing metonymy from metaphor) over full texts without fine-tuning's parameter updates.

## Foundational Learning

- **Phraseological vs. Word-level Annotation**: The study uses a phraseological approach capturing multi-word spans rather than single words. Why needed: To correctly interpret F1 scores and understand evaluation methodology. Quick check: For "He was bathed in soft light," should the model tag "bathed" or "bathed in soft light"?

- **Conventionality in Metaphor Theory**: Models struggle with "dead" or highly conventional metaphors (e.g., "near future"). Why needed: To diagnose false negatives and understand failure modes. Quick check: Why is "spend time" harder to classify as metaphorical than "a heart of stone"?

- **Reasoning Models vs. Standard LLMs**: Reasoning models specifically benefit from Chain-of-Thought prompting. Why needed: To understand why o3 achieves parity with fine-tuned standard models. Quick check: Does adding reasoning steps improve a standard LLM as much as a reasoning model?

## Architecture Onboarding

- **Component map**: Raw text (IMDb reviews) -> Knowledge Source (Codebook with guidelines) -> Processing (RAG, Prompting, or Fine-tuning) -> XML tagged text (`<Metaphor>...</Metaphor>`) -> Evaluation (Token-level F1, Precision, Recall)

- **Critical path**: Chain-of-Thought construction using GPT-4o to generate reasoning explanations for few-shot examples enabled reasoning models to compete with fine-tuning.

- **Design tradeoffs**:
  - Fine-tuning: High accuracy (0.79 F1) but high friction (requires 80% data, model re-training)
  - CoT Prompting: High accuracy (0.76 F1) with low friction (no training) but expensive reasoning model inference
  - RAG: Lower accuracy but strictly grounded in provided rules for auditability

- **Failure signatures**:
  - "Twice-True" Ambiguity: Missing metaphors in domains where language is both literally and metaphorically true
  - Span Drift: Tagging only key words instead of entire human-annotated scenes
  - Personification Blindness: False negatives on personification due to blurred lines with literal agency

- **First 3 experiments**:
  1. Zero-shot prompt baseline on held-out text to measure inherent metaphorical reasoning (~0.73 F1 for o3)
  2. Span alignment comparison: token-level F1 vs. span-exact-match to verify phraseological capture
  3. "Twice-True" test: Feed texts with high literal/metaphorical overlap to see if model defaults to literal labels

## Open Questions the Paper Calls Out

- **Integration multiplier effect**: Does combining RAG, prompt engineering, and fine-tuning yield multiplicative performance gains? The authors explicitly state future studies could combine methods to assess integration effects.

- **Phraseological intuitiveness**: Is the phraseological approach inherently more intuitive for LLMs than word-level procedures like MIP(VU)? The authors query whether improved results stem from techniques or the annotation approach itself.

- **Metaphor labeling extension**: Can LLMs effectively perform metaphor labeling by identifying source and target domains, not just identification? The authors identify this as an "equally important and challenging task for future LLM evaluation."

## Limitations
- Single annotated corpus (IMDb reviews) constrains generalizability across domains
- Phraseological annotation approach creates evaluation challenges when comparing against word-level metaphor detection models
- Inherent subjectivity in metaphor annotation with potential human annotator discrepancies affecting model performance metrics

## Confidence

- **High Confidence**: Fine-tuning achieving highest accuracy (median F1 = 0.79) - Directly supported by quantitative results across multiple model configurations and statistical validation
- **Medium Confidence**: Chain-of-thought prompting achieving comparable performance - F1 scores close (0.76 vs 0.79) but reliance on expensive reasoning models and reasoning trace quality introduces uncertainty
- **Medium Confidence**: Systematic errors tied to theoretical challenges - Qualitative analysis based on theoretical frameworks with ongoing debates in metaphor research

## Next Checks

1. **Cross-Domain Validation**: Test fine-tuned model and chain-of-thought prompting on texts from different domains (academic writing, news articles, social media) to assess generalizability beyond film reviews.

2. **Human Agreement Study**: Conduct inter-annotator agreement analysis on subset of texts to quantify subjectivity in metaphor annotation and understand how much model-human discrepancy stems from inherent annotation uncertainty.

3. **Error Type Analysis**: Systematically categorize model errors (false positives, false negatives, span boundaries) to identify clustering around specific linguistic phenomena and whether patterns differ between fine-tuning and prompting approaches.