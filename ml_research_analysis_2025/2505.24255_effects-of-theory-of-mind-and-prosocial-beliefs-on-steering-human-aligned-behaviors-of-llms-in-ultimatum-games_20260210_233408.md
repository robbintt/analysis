---
ver: rpa2
title: Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors
  of LLMs in Ultimatum Games
arxiv_id: '2505.24255'
source_url: https://arxiv.org/abs/2505.24255
tags:
- reasoning
- responder
- game
- player
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explored how different reasoning methods, including\
  \ theory-of-mind (ToM) reasoning at multiple levels, influence the behavioral alignment\
  \ of large language models (LLMs) in ultimatum games. By initializing agents with\
  \ prosocial beliefs (Greedy, Fair, Selfless) and applying reasoning strategies such\
  \ as chain-of-thought (CoT) and varying ToM orders, the research examined how agents\u2019\
  \ negotiation behaviors align with human norms."
---

# Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games

## Quick Facts
- **arXiv ID**: 2505.24255
- **Source URL**: https://arxiv.org/abs/2505.24255
- **Reference count**: 40
- **One-line primary result**: ToM reasoning improves LLM behavioral alignment in ultimatum games, with Fair-Fair beliefs and role-specific reasoning (first-order ToM for proposers, combined ToM for responders) showing the strongest effects.

## Executive Summary
This study investigates how theory-of-mind (ToM) reasoning at multiple levels, combined with prosocial beliefs (Greedy, Fair, Selfless), influences the behavioral alignment of large language models (LLMs) in ultimatum games. Across 2,700 simulations using diverse models including o3-mini and DeepSeek-R1, results show that ToM reasoning enhances behavioral alignment, decision-making consistency, and negotiation outcomes compared to vanilla or chain-of-thought approaches. Fair-Fair belief combinations produce the highest alignment, while role-specific reasoning strategies (first-order ToM for proposers, combined ToM for responders) prove most effective in matching human expectations.

## Method Summary
The study simulates multi-round ultimatum games between LLM agents initialized with belief-specific system prompts and various reasoning methods (Vanilla, CoT, Zero-order ToM, First-order ToM, Both ToM). Agents generate intermediate reasoning before decisions, with proposals constrained to specific amounts and responses limited to Accept/Reject. The evaluation uses deviation scores (DS) measuring how far agent behavior deviates from heuristic human expectations for each belief type, alongside acceptance rates and negotiation outcomes. Experiments were conducted across 6 LLMs with 9 belief combinations × 5 reasoning methods × 10 games each.

## Key Results
- ToM reasoning significantly improves behavioral alignment compared to vanilla or CoT approaches (β=-0.164 to -0.2534, p<0.01)
- Fair-Fair belief combinations achieve the highest alignment and acceptance rates across all models
- First-order ToM for proposers and combined ToM for responders show optimal alignment performance
- Built-in reasoning models (o3-mini, DeepSeek-R1) show limited behavioral alignment compared to ToM-prompted standard models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ToM reasoning improves behavioral alignment with human norms in negotiation tasks, but effectiveness depends on the game role and decision type.
- Mechanism: ToM prompts (following the Belief-Desire-Intention model) cause the model to explicitly generate representations of mental states—either its own (zero-order), the other agent's (first-order), or both—before making decisions. This intermediate reasoning step appears to constrain the action space toward more socially coherent choices.
- Core assumption: The deviation scores (DS) used as evaluation metrics validly capture "human-aligned" behavior based on heuristics from prior ultimatum game literature.
- Evidence anchors:
  - [abstract] "Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes."
  - [section 5.2] "Models usually achieve the least deviation when utilizing a first-order (β=-0.164, p<0.01)" for proposers; "Models utilizing both ToM (β=-0.2534, p<0.01) has generally show the least deviation" for responder acceptance.
  - [corpus] Related work (arXiv:2501.15355) proposes "ToM-agent" paradigm for LLM generative agents with counterfactual reflection, suggesting convergent interest in ToM-augmented agent architectures, though mechanisms remain underexplored.
- Break condition: If the task requires rejecting offers rather than accepting them, complex ToM reasoning degrades alignment—zero-order or CoT performed better (β=0.047, p>0.05 for zero-order vs. β=-0.208, p<0.05 for both ToM).

### Mechanism 2
- Claim: Fair-Fair belief combinations produce the highest behavioral alignment and acceptance rates across models.
- Mechanism: When both agents are initialized with "Fair" beliefs, their expected behaviors converge on the 50-50 split that matches established human norms. This reduces the negotiation search space and minimizes deviation from expected outcomes.
- Core assumption: The expected behavior heuristics (Fair = 50%, Greedy ≥70% proposer/≥60% responder, Selfless ≤30%/≤40%) accurately represent human behavioral distributions.
- Evidence anchors:
  - [abstract] "Fair-fair belief combinations led to the highest alignment"
  - [section 5.1] "In general the Fair-Fair belief combination achieves the highest overall AC, closely followed by the Fair-Selfless combination. [...] Fair-Fair is the only belief combination that shows the closest rational behavior."
  - [corpus] Corpus lacks direct replication of this specific finding; related ToM papers focus on single-agent reasoning rather than belief-matching in multi-agent games.
- Break condition: Asymmetric belief combinations (e.g., Selfless Proposer vs. Fair Responder) show high deviation scores (up to DS=6), indicating the mechanism fails when agent expectations diverge significantly.

### Mechanism 3
- Claim: Built-in reasoning models (o3-mini, DeepSeek-R1) show limited behavioral alignment compared to models with explicit ToM prompting.
- Mechanism: Assumption—The implicit reasoning in these models may not spontaneously generate the BDI-structured representations that the ToM prompts elicit, leading to less predictable alignment with human norms.
- Core assumption: The comparison is fair despite different architectures; the authors explicitly note this as consistent with previous findings.
- Evidence anchors:
  - [abstract] "Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning"
  - [section 5.2] o3-mini showed high rejection deviation (β=1.08, p<0.01), indicating poor alignment for that decision type despite being a reasoning model.
  - [corpus] No corpus papers directly compare built-in reasoning models vs. ToM-prompted standard models; this finding requires further validation.
- Break condition: The paper does not fully explain why reasoning models underperform—this remains an open question warranting caution in generalization.

## Foundational Learning

- Concept: **Ultimatum Game (UG)**
  - Why needed here: This is the experimental environment. Understanding that proposers make offers, responders accept/reject, and that human behavior deviates from game-theoretic rationality is essential for interpreting alignment metrics.
  - Quick check question: Why do human responders reject positive offers in UG?

- Concept: **Belief-Desire-Intention (BDI) Model**
  - Why needed here: This cognitive architecture frames the ToM reasoning prompts. The zero-order, first-order, and combined ToM methods all structure reasoning around these three mental state categories.
  - Quick check question: What is the difference between zero-order and first-order ToM in the BDI framework?

- Concept: **Behavioral Alignment Metrics (Deviation Scores)**
  - Why needed here: The study's central claim depends on these metrics. DS measures how far agent behavior deviates from heuristic human expectations for each belief type.
  - Quick check question: Why is it significant that Fair-Fair combinations have lower DS than Greedy-Selfless?

## Architecture Onboarding

- Component map:
  - Agent Initialization -> Reasoning Module -> Decision Module -> Game Loop -> Evaluation Layer

- Critical path:
  1. Initialize both agents with belief-specific system prompts
  2. Proposer generates ToM reasoning → proposes split
  3. Responder sees proposal → generates ToM reasoning → accepts/rejects
  4. If rejected, loop back to step 2 (max 5 rounds)
  5. Compute deviation scores against expected behavior

- Design tradeoffs:
  - **Single-stake vs. multi-stake**: Authors chose single-stake for comparability to human literature
  - **Multi-round vs. single-round**: Multi-round enables ToM reasoning but increases complexity
  - **Predefined strategies vs. open-ended**: Constraining action space improves reproducibility but limits ecological validity
  - **Heuristic expectations vs. empirical human data**: Authors note this as a limitation

- Failure signatures:
  - **High DS with Selfless beliefs**: Models struggle to maintain consistent selfless behavior (e.g., Case 1: Selfless proposer offered $8/$2, DS=6)
  - **Low acceptance with mismatched beliefs**: Fair-Greedy combinations show variable AC (10-100% depending on model)
  - **Reasoning-decision inconsistency**: Models sometimes generate ToM reasoning that doesn't match their subsequent decision (see sample transcripts)

- First 3 experiments:
  1. Replicate Fair-Fair baseline across your target models to establish alignment ceiling before testing other belief combinations.
  2. Compare first-order ToM vs. CoT for proposer-only role to validate role-specific reasoning benefits before full game simulation.
  3. Test sensitivity of deviation scores to the expected behavior thresholds (as authors do in Appendix F) to understand robustness before drawing conclusions from new model families.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ToM reasoning effectively steer aligned behaviors in negotiation scenarios outside of the ultimatum game, such as in real-world or multi-issue negotiations?
- Basis in paper: [explicit] "Future work should investigate ToM reasoning in other economic games or negotiation scenarios to improve generalizability..."
- Why unresolved: The study focused solely on the ultimatum game as a controlled environment, leaving the transferability of these findings untested.
- What evidence would resolve it: Replication of the study in diverse negotiation contexts (e.g., salary negotiation, prisoner's dilemma) showing consistent alignment improvements.

### Open Question 2
- Question: How do additional agentic factors, specifically emotion and trust, interact with ToM reasoning to influence behavioral alignment?
- Basis in paper: [explicit] "Future work should... explore other agentic factors such as emotion and trust..."
- Why unresolved: The study focused on prosocial beliefs and reasoning methods but excluded emotional and trust dynamics which are inherent to human negotiation.
- What evidence would resolve it: Experiments integrating emotion and trust variables into the agent framework to measure their individual and combined effects on ToM reasoning performance.

### Open Question 3
- Question: How does the behavioral alignment of ToM-enabled LLMs compare to actual human participant data rather than heuristic expectations?
- Basis in paper: [inferred] "...we did not utilize parallel experimental data from human participants, which might differ from that of the agent-based simulations."
- Why unresolved: The evaluation relies on heuristic expectations of human behavior rather than a direct, controlled comparison with human subjects.
- What evidence would resolve it: Conducting parallel experiments with human participants to generate a ground-truth baseline for the LLM agents' deviation scores.

## Limitations
- The study relies on heuristic thresholds for expected human behavior rather than empirical human data, creating potential circularity in alignment evaluation.
- The comparison between built-in reasoning models and ToM-prompted standard models lacks mechanistic explanation for the performance gap.
- The single-stake design may not capture the full complexity of human decision-making in variable-stakes scenarios.

## Confidence
- **High confidence**: Fair-Fair belief combinations produce the highest behavioral alignment and acceptance rates (supported by robust statistical evidence across models)
- **Medium confidence**: Role-specific reasoning (first-order ToM for proposers, combined ToM for responders) improves alignment (statistically significant but requires careful interpretation)
- **Low confidence**: General claim that ToM reasoning "improves behavioral alignment with human norms" across all conditions (effectiveness depends critically on role, decision type, and belief combination)

## Next Checks
1. **Validate heuristic thresholds against empirical human data**: Replicate the study using observed human behavior distributions from controlled ultimatum game experiments rather than predefined thresholds for expected behavior.
2. **Test reasoning model architecture sensitivity**: Conduct ablation studies comparing different reasoning model configurations (temperature, top_p, max_tokens) to determine if the underperformance of built-in reasoning models is due to architectural differences or implementation choices.
3. **Cross-cultural validation**: Run the same experiments with different stake values and cultural contexts to test whether the deviation score metrics and alignment findings generalize beyond the single-stake, Western-normative framework used here.