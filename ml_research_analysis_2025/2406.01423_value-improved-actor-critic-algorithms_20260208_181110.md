---
ver: rpa2
title: Value Improved Actor Critic Algorithms
arxiv_id: '2406.01423'
source_url: https://arxiv.org/abs/2406.01423
tags:
- policy
- greedification
- operator
- improvement
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Value-Improved Actor Critic (VIAC), which decouples
  the evaluated policy from the acting policy and applies greedification operators
  independently to both. This approach addresses the tradeoff between greedification
  and stability in Actor Critic algorithms.
---

# Value Improved Actor Critic Algorithms

## Quick Facts
- arXiv ID: 2406.01423
- Source URL: https://arxiv.org/abs/2406.01423
- Reference count: 40
- Primary result: VIAC decouples evaluated and acting policies, achieving state-of-the-art performance in DeepMind control environments with negligible additional cost.

## Executive Summary
This paper introduces Value-Improved Actor Critic (VIAC), which addresses the tradeoff between greedification and stability in Actor Critic algorithms by decoupling the policy evaluated by the critic from the acting policy. The authors propose applying greedification operators independently to both policies, allowing the critic to evaluate a greedier policy while maintaining a stable, gradually improving acting policy. The approach is theoretically grounded with convergence guarantees for finite-horizon MDPs and demonstrates significant empirical improvements when incorporated into TD3 and SAC across DeepMind control environments.

## Method Summary
VIAC modifies standard actor-critic algorithms by decoupling the policy evaluated by the critic from the acting policy, applying greedification operators independently to both. The critic uses value improvement operators (either explicit multi-step gradient updates or implicit expectile loss) to evaluate a greedier policy, while the actor continues gradual gradient-based improvement. The expectile loss with τ>0.5 provides implicit value improvement with negligible computational overhead compared to explicit operators. The method is implemented by replacing the standard MSE loss on the critic with the asymmetric expectile loss L²_τ, using the current online policy (not target policy) to calculate expectile targets.

## Key Results
- VI-TD3 and VI-SAC outperform or match their respective baselines in all tested DeepMind control environments
- Incorporating value improvement into TD3 and SAC significantly improves performance with negligible additional cost
- Empirical results show monotonic performance gains as evaluated-policy greedification increases (pg=1 to 20) before plateauing/decaying
- Implicit expectile-based VI-TD3 matches or exceeds explicit gradient-based VI-TD3 performance

## Why This Works (Mechanism)

### Mechanism 1
Decoupling the evaluated policy from the acting policy enables independent control of their greedification rates, improving sample efficiency without destabilizing gradient-based actor updates. Standard actor-critic methods constrain both policies to improve at the same (slow) gradient-based rate. VIAC separates them: the critic evaluates a greedier policy (e.g., via argmax-style or implicit operators) to generate larger bootstrap targets, while the actor remains stable with gradual gradient updates.

### Mechanism 2
Implicit greedification via asymmetric loss functions (e.g., expectile loss) provides value improvement with negligible computational overhead compared to explicit multi-step gradient operators. The expectile loss Lτ₂ (τ>0.5) trains the critic to track higher-value outcomes, implicitly estimating the value of a greedier policy without explicitly constructing that policy.

### Mechanism 3
Sufficient greedification operators (lower-bounded or limit-sufficient) guarantee convergence of generalized policy iteration to optimal policies in finite-horizon MDPs, even with stochastic policies. Theorem 3 shows that if operators ensure the policy sequence converges to a greedy policy with respect to the limiting Q-function (or improves by at least ε per step), the overall GPI process converges.

## Foundational Learning

- **Policy Improvement Theorem**: Why needed: VIAC's theoretical justification relies on the guarantee that greedification (increasing expected Q-value) yields improved policies. Quick check: Can you explain why argmax over Qπ yields a policy with Vπ′ ≥ Vπ?

- **Actor-Critic Architecture (TD3/SAC)**: Why needed: VIAC modifies existing off-policy AC algorithms; understanding their update rules (policy gradient, target networks, entropy bonuses) is essential. Quick check: What is the role of target policy smoothing in TD3, and why does SAC include entropy regularization?

- **Greedification Operators**: Why needed: VIAC introduces operator classes (deterministic, limit-sufficient, lower-bounded); distinguishing them is critical for correct implementation. Quick check: Given a Q-function q(s,a) and policy π, does the operator I(π,q)(s)=softmax(q(s,·)) satisfy the sufficient greedification condition? Why or why not?

## Architecture Onboarding

- **Component map**: Actor network πθ -> produces actions; Critic network(s) qϕ -> estimates Q-values; Explicit I₂ -> additional gradient steps on temporary policy copy; Implicit I₂ -> expectile loss on critic targets; Replay buffer B -> stores transitions

- **Critical path**: 1. Sample transition (s,a,r,s′) from B; 2. Apply I₂ to produce greedier next-action distribution; 3. Compute target y = r + γqϕ(s′, a′) with a′ from greedier policy; 4. Update critic with Lτ₂ loss (or MSE if explicit); 5. Update actor with standard policy gradient (I₁)

- **Design tradeoffs**: Explicit vs. implicit I₂: explicit (pg>1) gives fine-grained control but costs more compute; implicit (τ>0.5) is cheap but couples greedification with loss asymmetry. Higher τ/pg: faster learning up to a point, then overestimation/instability risk.

- **Failure signatures**: Performance plateaus early: τ too low (no value improvement) or pg=0 (baseline). Performance degrades after initial gains: τ>0.8 or excessive pg steps (overestimation, target instability). No convergence in toy MDPs: operator I does not satisfy sufficient conditions.

- **First 3 experiments**: 1. Ablation on τ (implicit): Run VI-TD3/SAC with τ∈{0.5,0.6,0.7,0.75,0.8,0.9} on hopper-hop. 2. Explicit vs. implicit comparison: For matched compute budget, compare VI-TD3 with pg=5 vs. τ=0.75. 3. Convergence validation in tabular MDP: Implement Algorithm 2 with Igmz in small finite-horizon gridworld.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the convergence of Value-Improved Generalized Policy Iteration (Algorithm 2) hold for infinite-horizon MDPs? The authors explicitly state their analysis is restricted to finite-horizon MDPs, relying on finite trajectory length to guarantee convergence from terminal states backwards.

- **Open Question 2**: Can the convergence guarantees for "sufficient greedification" operators be extended to continuous action spaces? The analysis assumes finite action spaces, with continuous spaces allowing infinitesimal updates that may never reach the argmax policy required for optimality.

- **Open Question 3**: To what extent is the empirical performance gain of VIAC attributable to controlled overestimation bias versus the direct acceleration of greedification? While benefits are "not limited to" overestimation, results show strong correlation between expectile parameter τ, performance, and overestimation bias.

## Limitations

- The empirical evaluation is limited to DeepMind Control environments, lacking comparisons on more diverse domains like robotic manipulation or contact-rich locomotion tasks.
- The theoretical convergence guarantees depend on operator choice and k≥1 Bellman updates, with practical implications for continuous control under function approximation not fully validated across broader task sets.
- The implicit expectile-based improvement mechanism's robustness to data distribution shifts and its bias-variance tradeoff at high τ values are not thoroughly characterized.

## Confidence

- **High Confidence**: The core claim that decoupling evaluated/acting policies enables independent greedification rates is well-supported by empirical results showing monotonic improvements with increasing τ/pg (Figure 2), and the mechanism is theoretically grounded in generalized policy iteration.
- **Medium Confidence**: The convergence proof for finite-horizon MDPs with sufficient operators is rigorous, but its practical implications for continuous control with function approximation remain to be validated across broader task sets and operator designs.
- **Low Confidence**: The implicit expectile-based improvement mechanism's robustness to data distribution shifts and its bias-variance tradeoff at high τ values are not thoroughly characterized.

## Next Checks

1. **τ Sensitivity Across Tasks**: Systematically vary τ in VI-SAC across 3+ diverse DM Control tasks (e.g., hopper-hop, humanoid-stand, fish-swim) and plot return vs. τ to confirm the existence of a consistent sweet spot (expected: 0.7–0.8) and quantify variance across seeds.

2. **Implicit vs. Explicit Tradeoff**: For matched compute budgets, compare VI-TD3 with pg=5 (explicit) vs. τ=0.75 (implicit) on a standard task (e.g., walker-walk). Measure both final return and wall-clock time to determine if implicit VIAC's computational efficiency translates to practical advantages.

3. **Convergence in Tabular MDP**: Implement Algorithm 2 with Igmz (Gumbel MuZero operator) in a small finite-horizon gridworld. Track Q-value and policy convergence over iterations to empirically validate Theorem 3's claim that sufficient operators guarantee convergence to optimal policies.