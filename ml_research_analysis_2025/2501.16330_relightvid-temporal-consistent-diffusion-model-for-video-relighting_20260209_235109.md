---
ver: rpa2
title: 'RelightVid: Temporal-Consistent Diffusion Model for Video Relighting'
arxiv_id: '2501.16330'
source_url: https://arxiv.org/abs/2501.16330
tags:
- video
- relighting
- diffusion
- editing
- illumination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of video relighting with temporal
  consistency, which is difficult due to the lack of paired video relighting datasets
  and the randomness inherent in diffusion models. The proposed RelightVid framework
  leverages a pre-trained image relighting diffusion model and extends it to video
  by incorporating temporal attention layers and training on a carefully curated dataset
  called LightAtlas.
---

# RelightVid: Temporal-Consistent Diffusion Model for Video Relighting

## Quick Facts
- arXiv ID: 2501.16330
- Source URL: https://arxiv.org/abs/2501.16330
- Authors: Ye Fang, Zeyi Sun, Shangzhan Zhang, Tong Wu, Yinghao Xu, Pan Zhang, Jiaqi Wang, Gordon Wetzstein, Dahua Lin
- Reference count: 13
- Primary result: Achieves 18.79 PSNR, 0.7832 SSIM, 1.458 motion smoothness for video relighting with temporal consistency

## Executive Summary
This paper addresses the challenge of video relighting with temporal consistency, which is difficult due to the lack of paired video relighting datasets and the randomness inherent in diffusion models. The proposed RelightVid framework leverages a pre-trained image relighting diffusion model and extends it to video by incorporating temporal attention layers and training on a carefully curated dataset called LightAtlas. LightAtlas includes both real-world videos with augmentation and 3D-rendered data under extreme lighting conditions. RelightVid supports multiple relighting conditions including background videos, text prompts, and HDR environment maps. The method achieves high-quality, temporally consistent video relighting while preserving the illumination priors of its image backbone.

## Method Summary
RelightVid extends the IC-Light image relighting diffusion model to video by inflating its 2D U-Net to 3D and adding temporal attention layers initialized from AnimateDiff-V2. The spatial layers remain frozen to preserve learned illumination editing capabilities, while only temporal layers are fine-tuned. The model is trained on LightAtlas, a dataset containing 200K in-the-wild video pairs and 1M 3D-rendered pairs. It supports multi-modal relighting conditions through cross-attention: background video latents, CLIP-encoded text prompts, and HDR environment maps processed through a 5-layer MLP encoder. For inference, an Illumination-Invariant Ensemble (IIE) averages predictions across brightness-augmented inputs to improve albedo preservation.

## Key Results
- Quantitative improvements: 18.79 PSNR, 0.7832 SSIM, 1.458 motion smoothness compared to baseline methods
- Significant gains in temporal consistency as measured by motion smoothness
- User studies show superior video smoothness, lighting rationality, text alignment, and ID-preservation
- Flexible relighting control supporting background videos, text prompts, and HDR environment maps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal attention layers added to a frozen image relighting backbone can extend image-to-video relighting while preserving spatial priors.
- Mechanism: The 2D diffusion model (IC-Light) is inflated to 3D U-Net. Spatial layers remain frozen, retaining learned illumination editing capabilities. Temporal attention layers (initialized from AnimateDiff-V2) are trained to model frame-to-frame dependencies. This separation prevents catastrophic forgetting of image-level relighting quality.
- Core assumption: The image backbone's relighting priors are sufficiently general to transfer frame-by-frame, with only temporal coordination needed.
- Evidence anchors: [abstract] "incorporating temporal attention layers into the diffusion model while preserving the image backbone's relighting priors"; [section 3.2.1] "During training, the spatial layers are kept frozen, while only the temporal layers are fine-tuned. This strategy preserves the in-the-wild editing capability of IC-Light"

### Mechanism 2
- Claim: Joint training on multi-modal conditions (background video, text, HDR maps) via cross-attention enables flexible relighting control.
- Mechanism: Background video latents are concatenated with noisy input. Text is encoded via CLIP and injected through cross-attention. HDR environment maps pass through a 5-layer MLP encoder, decomposed into LDR/HDR components, then fused with text embeddings before cross-attention injection. The joint loss optimizes all conditioning paths simultaneously.
- Core assumption: Different conditioning modalities provide complementary illumination signals that can be learned together without interference.
- Evidence anchors: [abstract] "accept background video, text prompts, or environment maps as relighting conditions"; [section 3.2.2] Equation 1 shows joint training objective with all encoded conditions

### Mechanism 3
- Claim: Illumination-Invariant Ensemble (IIE) improves albedo preservation by averaging predictions across brightness-augmented inputs.
- Mechanism: For inference, the input video is augmented with N brightness scaling factors (e.g., 0.5-1.5). Each augmented version produces a noise prediction; these are averaged before denoising. The rationale: the correct relit output should be invariant to input brightness, so averaging cancels brightness-dependent artifacts.
- Core assumption: Relighting output should be illumination-invariant; variations due to input brightness are noise to be suppressed.
- Evidence anchors: [section 3.2.3] "the relighted foreground Vrel should ideally be fixed...regardless of any brightness augmentation applied to the original input"; [table 3] 5-Aug-Input achieves 19.07 PSNR vs. 18.79 single input

## Foundational Learning

- Concept: Diffusion model inflation (2D → 3D U-Net)
  - Why needed here: Understanding how spatial convolutions/attention are extended with temporal dimensions without retraining from scratch.
  - Quick check question: Can you explain what changes when a 2D convolution (H×W) becomes a 3D convolution (T×H×W)?

- Concept: Cross-attention conditioning in diffusion models
  - Why needed here: Multi-modal control relies on cross-attention to inject external signals (text, HDR) into the denoising process.
  - Quick check question: In a cross-attention layer, which tensor is the query and which is the key/value when conditioning on text?

- Concept: Temporal consistency metrics (Motion Smoothness)
  - Why needed here: Evaluating video relighting requires metrics beyond image quality; motion smoothness quantifies frame-to-frame coherence.
  - Quick check question: Why would per-frame PSNR be high but motion smoothness low?

## Architecture Onboarding

- Component map: Input video → VAE encode → noise addition → concatenate with conditions → 3D U-Net denoising (cross-attention injection) → VAE decode → optional IIE averaging

- Critical path: Input video → VAE Encoder: Compresses video to latents (z_rel, z_bg) → 3D U-Net: Spatial layers (frozen, from IC-Light/SD-1.5) + Temporal layers (trainable, from AnimateDiff-V2) → HDR Encoder: 5-layer MLP → LDR/HDR decomposition → Cross-attention: Fuses HDR+text with spatial features → VAE Decoder: Latents → output video → IIE (optional): Ensemble across brightness augmentations

- Design tradeoffs:
  - Frozen spatial layers: Preserves IC-Light priors but limits adaptation to video-specific lighting dynamics
  - Joint multi-modal training: Flexible control but potential condition conflicts
  - IIE ensemble: Better albedo preservation but increased inference cost (N× forward passes) and risk of blurring

- Failure signatures:
  - Flickering output: Temporal layers undertrained or learning rate too high
  - Albedo drift (color shift): IIE not applied or insufficient augmentations
  - Ignored text condition: Cross-attention weights too small or text-HDR fusion insufficient
  - Blurry output: IIE with too many augmentations (N>5)

- First 3 experiments:
  1. Ablate temporal layers: Compare full RelightVid vs. per-frame IC-Light on Motion Smoothness metric to quantify temporal contribution.
  2. Test IIE sensitivity: Run inference with N=1, 3, 5 augmentations; measure PSNR and visual sharpness tradeoff.
  3. Condition conflict test: Provide contradictory inputs (text="night" + background=daylit scene); analyze which condition dominates and document failure modes.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The paper does not specify critical training hyperparameters including batch size, video resolution, and frame count, which are essential for faithful reproduction.
- The temporal attention configuration details (frame window size, stride) are only vaguely referenced as "AnimateDiff-V2 initialization."
- The effectiveness of the Illumination-Invariant Ensemble (IIE) method is demonstrated only on this dataset without external validation.
- The 3D-rendered training data, while addressing extreme lighting conditions, may not fully represent real-world complexity.

## Confidence
- **High confidence**: The core architecture of extending IC-Light via temporal attention layers (Mechanism 1) and the quantitative improvements over baselines
- **Medium confidence**: The effectiveness of multi-modal conditioning (Mechanism 2) due to lack of ablation studies isolating each condition's contribution
- **Medium confidence**: The IIE ensemble method (Mechanism 3) shows quantitative gains but lacks ablation on ensemble size sensitivity and visual quality tradeoffs

## Next Checks
1. **Temporal ablation study**: Compare full RelightVid vs. per-frame IC-Light processing on motion smoothness to quantify the exact contribution of temporal layers
2. **IIE sensitivity analysis**: Systematically test ensemble sizes (N=1, 3, 5, 7) to identify the optimal tradeoff between albedo preservation and visual sharpness
3. **Condition conflict experiment**: Provide contradictory conditioning inputs (e.g., "night" text with daylight background) to analyze which modality dominates and document failure modes.