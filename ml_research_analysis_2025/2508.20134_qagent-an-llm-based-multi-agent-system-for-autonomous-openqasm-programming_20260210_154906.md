---
ver: rpa2
title: 'QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming'
arxiv_id: '2508.20134'
source_url: https://arxiv.org/abs/2508.20134
tags:
- quantum
- coder
- qagent
- pass
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QAgent is a hierarchical LLM-powered multi-agent system that automates
  OpenQASM programming for NISQ quantum devices. It combines dynamic few-shot learning
  with tool-augmented planning, supported by retrieval-augmented generation, in-context
  learning, and chain-of-thought reasoning for iterative refinement.
---

# QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming

## Quick Facts
- **arXiv ID:** 2508.20134
- **Source URL:** https://arxiv.org/abs/2508.20134
- **Reference count:** 40
- **Primary result:** Improves OpenQASM generation accuracy by up to 71.6% over static LLM baselines across five fundamental quantum algorithms

## Executive Summary
QAgent is a hierarchical LLM-powered multi-agent system that automates OpenQASM programming for NISQ quantum devices. It combines dynamic few-shot learning with tool-augmented planning, supported by retrieval-augmented generation, in-context learning, and chain-of-thought reasoning for iterative refinement. The system routes tasks between two specialized coders: one mimicking canonical patterns for short, structured algorithms, and another leveraging predefined tools for complex, parameterized circuits. Evaluations show QAgent improves OpenQASM generation accuracy by up to 71.6% over static LLM baselines across five fundamental quantum algorithms, with Qwen-235B achieving the highest pass rates. While performance remains strong for multi-algorithm tasks involving two algorithms, accuracy declines for higher-complexity decompositions.

## Method Summary
QAgent employs a hierarchical multi-agent architecture with dual-path task routing. For quantum programming tasks, it first attempts generation using a "Dynamic-few-shot Coder" that relies on pattern mimicry and in-context learning from retrieved examples. If this fails (hitting reflection round limits), the system escalates to a "Tools-augmented Coder" that uses explicit planning and Python tool execution for parameterized complexity. A dedicated Reflection Agent provides iterative debugging via chain-of-thought reasoning, while a Prompt Agent performs retrieval-augmented generation to ground outputs in verified examples. The system validates outputs using Qiskit simulation before final delivery.

## Key Results
- QAgent achieves up to 71.6% improvement in OpenQASM generation accuracy over static LLM baselines
- Qwen-235B model shows highest pass rates among tested LLMs
- System maintains strong performance for two-algorithm tasks but accuracy declines for three or more algorithm decompositions
- Iterative reflection significantly enhances code quality for parameterized circuits

## Why This Works (Mechanism)

### Mechanism 1: Dual-Path Task Routing
The system routes quantum programming tasks between two specialized coders based on structural complexity. It first attempts generation using a "Dynamic-few-shot Coder," which relies on pattern mimicry and is computationally cheaper. If this fails (e.g., hits a reflection round limit), the task is escalated to a "Tools-augmented Coder" that uses explicit planning and function calls to handle parameterized complexity. The core assumption is that short, structured quantum algorithms are best solved by imitating examples, while complex, parameter-heavy algorithms require explicit planning and tool execution.

### Mechanism 2: Iterative Debugging via CoT Reflection
Chain-of-Thought (CoT) based reflection allows the system to repair functional errors in quantum code that static generation misses. A dedicated "Reflection Agent" receives failed code, test logs, and previous prompts. It generates a natural language diagnosis (CoT) and a revision strategy, which is fed back to the Coding/Execution Agent. This loop allows the model to correct specific issues like miscounted gate repetitions, assuming the base LLM possesses sufficient reasoning capability to diagnose quantum syntax errors and logical bugs from execution logs.

### Mechanism 3: Context-Grounded Generation (RAG)
Retrieval-Augmented Generation (RAG) stabilizes output quality by grounding the LLM in verified, algorithm-specific examples rather than relying solely on parametric memory. A "Prompt Agent" queries a database for relevant QASM code snippets, which are parsed and injected into the prompt, providing the LLM with immediate syntactic templates required by OpenQASM. The effectiveness depends on the retrieval database containing high-quality, syntactically correct examples that match the target algorithm's structure.

## Foundational Learning

- **Concept: OpenQASM Syntax & Quantum Gates**
  - **Why needed here:** The system generates OpenQASM directly. Understanding low-level primitives (qubits, classical bits, gates like `h`, `cx`, `mcx`) is required to interpret error logs.
  - **Quick check question:** Can you identify a syntax error in a QASM snippet where a gate is applied to a qubit index that hasn't been declared?

- **Concept: In-Context Learning (Few-Shot)**
  - **Why needed here:** The "Dynamic Coder" relies entirely on the LLM's ability to recognize patterns from examples in the prompt without weight updates.
  - **Quick check question:** How does providing 3 examples of Grover's algorithm in a prompt help an LLM generate a 4th variant?

- **Concept: Agent Tool Use**
  - **Why needed here:** The "Tools-augmented Coder" does not just output text; it calls Python functions (tools) to construct circuits.
  - **Quick check question:** Why might a "tool-augmented" approach be safer for calculating precise rotation angles (e.g., `ry(pi/4)`) than pure text generation?

## Architecture Onboarding

- **Component map:** Orchestrator -> Dynamic Coder (Prompt Agent -> Coding Agent -> Test Agent -> Reflection Agent) OR Tools Coder (Plan Agent -> Execution Agent -> Reflection Agent) -> Environment (Qiskit simulator)
- **Critical path:** The Test Agent is the bridge between generation and refinement. Without accurate test feedback (syntax + functional verification), the Reflection Agent cannot form a valid CoT diagnosis.
- **Design tradeoffs:**
  - Dynamic vs. Tools: Dynamic is fast/cheap but brittle for parameters; Tools is robust but requires expensive planning and LLM reasoning
  - Reflection Rounds: More rounds improve "Tools" accuracy but linearly increase latency and token cost
- **Failure signatures:**
  - Infinite Loop: Reflection Agent suggests fixes that pass syntax but fail function, repeating until budget exhaustion
  - Tool Mismatch: Plan Agent selects a tool for "Phase Estimation" but passes wrong parameters, leading to valid code but wrong quantum behavior
  - Decomposition Error: In multi-algorithm tasks, the system fails to separate sub-tasks, generating jumbled code
- **First 3 experiments:**
  1. Baseline Validation: Run "Static" vs. "Dynamic" coder on 5 fundamental algorithms (Level-1) to replicate 71.6% improvement metric
  2. Ablation on Reflection: Disable Reflection Agent on "Tools" coder for Phase Estimation and W-state to confirm performance drop
  3. Stress Test: Feed Level-3 multi-algorithm task to Hybrid system to observe decomposition failure mode

## Open Questions the Paper Calls Out

1. Can proactive task complexity estimation improve Coder routing accuracy compared to the current sequential fallback strategy?
2. What architectural improvements enable reliable decomposition of user queries involving three or more quantum algorithms?
3. Does QAgent-generated OpenQASM produce equivalent correctness when executed on real NISQ hardware versus classical simulators?
4. How does the Tools-augmented Coder's tightly coupled pipeline contribute to success, and can it be modularized for interpretability?

## Limitations
- Scalability of decomposition strategy is unverified beyond three-algorithm tasks
- Tool accuracy and test suite completeness are not characterized
- Retrieval database quality and coverage are not reported
- Performance on real NISQ hardware versus simulators is untested

## Confidence

**High confidence:** The dual-path routing mechanism and its basic effectiveness for single-algorithm tasks (71.6% improvement metric well-supported)
**Medium confidence:** Iterative reflection mechanism's ability to fix complex errors, particularly for parameterized circuits
**Low confidence:** System's ability to scale to real-world quantum programming tasks beyond five tested algorithms

## Next Checks
1. Test suite validation: Run system on algorithms with known ground truth quantum states, verify actual quantum behavior on simulator versus syntactic tests
2. Retrieval failure analysis: Systematically remove examples from RAG database, measure accuracy degradation to identify sensitive algorithm types
3. Decomposition stress test: Create Level-4 and Level-5 multi-algorithm tasks (4-5 algorithm compositions) to determine exact failure point and test alternative decomposition strategies