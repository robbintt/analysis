---
ver: rpa2
title: 'Thunder-NUBench: A Benchmark for LLMs'' Sentence-Level Negation Understanding'
arxiv_id: '2506.14397'
source_url: https://arxiv.org/abs/2506.14397
tags:
- negation
- shot0
- zero-shot0
- sentence
- after
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Thunder-NUBench introduces a novel benchmark to evaluate sentence-level
  negation understanding in large language models. The benchmark goes beyond surface-level
  negation detection by distinguishing between standard negation, local negation,
  contradiction, and paraphrase through carefully curated sentence pairs and multiple-choice
  tasks.
---

# Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding

## Quick Facts
- arXiv ID: 2506.14397
- Source URL: https://arxiv.org/abs/2506.14397
- Reference count: 40
- Models still struggle to differentiate standard negation from closely related alternatives, especially local negation variants.

## Executive Summary
Thunder-NUBench is a novel benchmark for evaluating sentence-level negation understanding in large language models (LLMs). Unlike previous work that focuses on negation detection, this benchmark challenges models to distinguish between standard negation, local negation, contradiction, and paraphrase in a multiple-choice format. Through experiments across diverse model families and sizes, the authors demonstrate that while larger and instruction-tuned models generally perform better, all models struggle with the core challenge of identifying standard negation when local negation options are present. The benchmark provides a robust diagnostic tool for assessing semantic reasoning in language models and highlights the need for further research in this area.

## Method Summary
Thunder-NUBench consists of a sentence-negation pair training set (3,772 pairs) and a multiple-choice test set (1,261 sentences). For each original sentence, four options are provided: standard negation, local negation, contradiction, and paraphrase. The benchmark is constructed from HoVer and Wikipedia Summary datasets, with manual creation of standard and local negation pairs and LLM generation of distractors reviewed by humans. Evaluation uses two modes (completion-based and option-selection) and two instruction formats (definition and detailed). Models are evaluated zero-shot and with few-shot examples, and some undergo supervised fine-tuning using LoRA.

## Key Results
- Larger models generally perform better on negation understanding tasks
- Instruction tuning improves performance across most model families
- Models still struggle to differentiate standard negation from local negation, especially in compound sentences and relative clauses

## Why This Works (Mechanism)

### Mechanism 1
The benchmark's multi-distractor multiple-choice design forces models to perform semantic scope resolution rather than rely on shallow negation-cue detection. By presenting standard negation alongside local negation, contradiction, and paraphrase, the task penalizes models that detect "not" or similar markers without understanding which proposition is negated. Success requires identifying the main predicate and applying truth-functional negation to it.

### Mechanism 2
Model scale and instruction tuning jointly improve performance by enhancing the capacity for comparative reasoning over semantically close alternatives. Larger models and instruction-tuned variants show higher accuracy, particularly in option-selection settings. This suggests that increased parameter count and training on instruction-following data support both better semantic representations for each option and more effective comparison of these representations against the task definition.

### Mechanism 3
Supervised fine-tuning on sentence-negation pairs improves task-specific accuracy but does not fully resolve the confusions between standard and local negation. Fine-tuning on explicitly constructed negation pairs provides direct signal for the truth-reversal operation. However, residual errors—especially misclassifying local negation as standard negation—indicate that the models learn a pattern for negation-application without robustly internalizing the logical scope constraints defined in the benchmark's typology.

## Foundational Learning

- **Concept**: Truth-functional negation and logical scope
  - Why needed here: The benchmark is explicitly grounded in sentential logic, defining standard negation as an operation that flips the truth value of the main predicate. Understanding De Morgan's laws for compound sentences and the distinction between clausal and subclausal negation is essential for interpreting model errors and designing future evaluations.
  - Quick check question: Given the sentence "Alice called Bob and he answered," what is its standard negation? (Answer: "Alice did not call Bob or he did not answer.")

- **Concept**: Distributional semantic limitations
  - Why needed here: The paper cites the limitation that distributional models often assign similar representations to "X" and "not X" because they appear in similar contexts. This explains why LLMs may rely on surface cues and struggle with the semantic impact of negation.
  - Quick check question: In distributional semantics, why might a word and its negation (e.g., "good" and "not good") have similar vector representations?

- **Concept**: Instruction tuning vs. supervised fine-tuning
  - Why needed here: The experiments separately evaluate the effects of instruction tuning (general alignment) and SFT (task-specific training). Distinguishing these helps diagnose whether performance gains stem from better instruction-following or from learning the negation task directly.
  - Quick check question: If a model improves after instruction tuning but not after SFT on a specific dataset, what does that suggest about the nature of the task?

## Architecture Onboarding

- **Component map**: Sentence extraction from HoVer and Wikipedia Summary datasets -> Grammar correction and sentence splitting/merging -> Manual creation of standard negation pairs -> LLM generation of distractors -> Human review and quality control -> Multiple-choice dataset compilation -> Model evaluation

- **Critical path**:
  1. Sentence selection and cleaning determines the complexity and domain of test cases
  2. Accurate construction of standard and local negation is the core bottleneck; human involvement is required because LLMs were observed to produce incorrect negations
  3. Quality control of contradiction/paraphrase distractors ensures they do not overlap semantically with negation options
  4. Evaluation protocol (completion-based vs. option-selection, prompt variants) directly affects measured performance

- **Design tradeoffs**:
  - Manual vs. automated generation: Manual creation of negation pairs ensures logical correctness but is time-intensive; contradictions/paraphrases are automated but require human oversight
  - Dataset source balance: Using HoVer (fact-centric) and Wikipedia (encyclopedic) provides complex sentences but may not cover informal language or dialogue negation
  - Distractor difficulty: The choice to use semantically close distractors makes the benchmark diagnostic but lowers overall accuracy; this highlights model weaknesses more clearly

- **Failure signatures**:
  - Local negation confusion: Models systematically select local negation (choice B) instead of standard negation, especially for compound sentences and relative/participle clauses
  - Paraphrase misclassification: In option-selection settings, models sometimes choose paraphrase (choice D), suggesting an inability to distinguish meaning reversal from meaning preservation
  - Instruction sensitivity: Performance varies significantly between "definition" and "detailed" prompts, indicating fragile task conceptualization

- **First 3 experiments**:
  1. Baseline evaluation: Run zero-shot evaluation on the multiple-choice test set using a set of pretrained and instruction-tuned models under both prompt variants to establish performance ranges and identify primary error modes
  2. Ablation on distractors: Create a variant of the benchmark where options include only standard negation and local negation (removing contradiction and paraphrase). This isolates the core negation-scope challenge
  3. Fine-tuning and generalization: Fine-tune a mid-sized model on the sentence-negation pair training set. Evaluate on the held-out test set and on a small set of out-of-distribution sentences to test generalization of the learned negation operation

## Open Questions the Paper Calls Out

- **Open Question 1**: Do LLMs' negation understanding capabilities generalize across diverse linguistic typologies and cross-lingual syntactic structures?
  - Basis in paper: [Explicit] The "Limitations" section states that the benchmark is exclusively in English and findings may not generalize to other languages, prompting a need for future cross-linguistic evaluation.
  - Why unresolved: Negation manifests differently across languages; it is unknown if models rely on language-specific cues or universal semantic logic.
  - Evidence: Translating Thunder-NUBench into typologically diverse languages (e.g., Japanese, Arabic) and observing performance consistency.

- **Open Question 2**: How do LLMs handle complex negation phenomena, specifically double negation and negative polarity items (NPIs), which are excluded from the current benchmark?
  - Basis in paper: [Explicit] The "Limitations" section explicitly identifies "double negation and negative polarity items (NPIs)" as important phenomena not directly addressed in the current work.
  - Why unresolved: The current benchmark focuses solely on standard sentence-level negation; model performance on these nuanced logical constructions remains untested.
  - Evidence: Expanding the dataset to include NPI licensing environments and double negation scenarios to measure error rates.

- **Open Question 3**: Why does instruction tuning occasionally degrade performance on logical negation tasks in specific model families compared to their pretrained counterparts?
  - Basis in paper: [Inferred] The results section notes that "some Qwen3 models exhibit comparable or higher performance in their pretrained versions," but the paper does not analyze the cause of this regression.
  - Why unresolved: The trade-off between optimizing for instruction following versus maintaining logical truth-value reasoning is not understood.
  - Evidence: A comparative mechanistic analysis of attention heads or activation patterns between base and instruction-tuned variants when processing negation cues.

## Limitations

- Negligible generalization data: The paper does not evaluate model performance on out-of-domain sentences or negation types not covered in the benchmark (e.g., conditional or double negation), limiting claims about robust negation understanding.
- SFT training details unclear: The exact learning rate for supervised fine-tuning is unspecified, which may affect reproducibility and comparability of results.
- No negative controls: Absence of control tasks to rule out superficial heuristics (e.g., memorization of syntactic negation patterns) leaves open the possibility that high accuracy is not due to semantic reasoning.
- Limited linguistic diversity: Benchmark sources (HoVer and Wikipedia) may not represent negation use in dialogue, informal language, or other real-world contexts.

## Confidence

- **High confidence**: Claims about model performance on the Thunder-NUBench test set and error distribution (especially local negation confusion) are directly supported by the experimental results.
- **Medium confidence**: Claims about the mechanism by which instruction tuning and scale improve performance are plausible but not directly proven; they rely on correlation rather than ablation or diagnostic experiments.
- **Low confidence**: Claims that supervised fine-tuning improves robust negation understanding are undermined by persistent confusion with local negation and lack of generalization tests.

## Next Checks

1. **Ablation on distractor types**: Remove contradiction and paraphrase options, leaving only standard vs. local negation. Measure if model accuracy improves, which would isolate the core negation-scope discrimination challenge.

2. **Out-of-distribution generalization test**: Evaluate fine-tuned models on a small set of sentences with conditional negation, double negation, or other clause types not present in the benchmark. This tests whether SFT yields robust or dataset-specific learning.

3. **Control task experiment**: Design a variant where options include non-negation semantic transformations (e.g., antonym replacement, quantifier change). Compare accuracy to the standard benchmark to detect if models rely on shallow negation cues.