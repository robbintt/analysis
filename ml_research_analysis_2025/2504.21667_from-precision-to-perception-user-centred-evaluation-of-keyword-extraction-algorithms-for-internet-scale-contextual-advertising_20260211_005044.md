---
ver: rpa2
title: 'From Precision to Perception: User-Centred Evaluation of Keyword Extraction
  Algorithms for Internet-Scale Contextual Advertising'
arxiv_id: '2504.21667'
source_url: https://arxiv.org/abs/2504.21667
tags:
- keyword
- keywords
- task
- https
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates keyword extraction algorithms for contextual\
  \ advertising through a human-in-the-loop framework combining quantitative precision\
  \ metrics with qualitative user perception surveys. Three algorithms\u2014TF-IDF,\
  \ KeyBERT, and Llama 2\u2014were tested on news articles with feedback from 855\
  \ participants across four experiments."
---

# From Precision to Perception: User-Centred Evaluation of Keyword Extraction Algorithms for Internet-Scale Contextual Advertising

## Quick Facts
- **arXiv ID:** 2504.21667
- **Source URL:** https://arxiv.org/abs/2504.21667
- **Reference count:** 40
- **Primary result:** KeyBERT outperforms TF-IDF and Llama 2 in user preference while maintaining computational efficiency, though all lag behind human-selected gold-standard keywords.

## Executive Summary
This study evaluates keyword extraction algorithms for contextual advertising through a human-in-the-loop framework combining quantitative precision metrics with qualitative user perception surveys. Three algorithms—TF-IDF, KeyBERT, and Llama 2—were tested on news articles with feedback from 855 participants across four experiments. Results show KeyBERT outperforms others in user preference and computational efficiency, while still lagging behind human-selected gold-standard keywords. Notably, state-of-the-art LLMs like DeepSeek achieve higher precision benchmarks but do not translate to superior user satisfaction. The study highlights the gap between algorithmic precision and user perception, emphasizing the need for human-in-the-loop evaluation methods in real-world applications where speed and cost are critical.

## Method Summary
The study evaluated three keyword extraction algorithms (TF-IDF, KeyBERT, Llama 2) on five news articles from The Guardian. A human-in-the-loop approach was employed where 855 participants on Prolific rated keyword sets using a 7-point Likert scale across four properties: comprehensiveness, representativeness, distinctiveness, and reasonableness. The "gold standard" keywords were established through participant annotations in a preliminary experiment. Algorithms were compared against this gold standard using precision, recall, cosine similarity, and edit distance metrics, while also measuring processing time to assess computational efficiency for real-time bidding constraints.

## Key Results
- KeyBERT consistently outperformed TF-IDF and Llama 2 in user preference ratings, scoring on average 5.8% higher across all evaluation properties
- State-of-the-art LLMs like DeepSeek achieved higher precision benchmarks but failed to translate this into superior user satisfaction
- Llama 2 exhibited significantly higher computational latency (exceeding the ~100ms RTB constraint), making it impractical for real-time advertising despite its generative capabilities
- There is a clear misalignment between algorithmic benchmark performance and user ratings, revealing a gap between precision-focused metrics and user-perceived efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a keyword extraction method uses document-level embeddings (e.g., KeyBERT), it aligns better with user perception than statistical frequency (TF-IDF) or unconstrained generative models (Llama 2) for news content.
- **Mechanism:** KeyBERT generates keywords by measuring the cosine similarity between the document embedding and token embeddings. This selects terms that are semantically central to the document's "concept," mirroring how humans generalize context, whereas TF-IDF relies on isolated term frequency and LLMs may "drift" or hallucinate.
- **Core assumption:** Users prefer keywords that capture the *semantic gist* of an article over those that merely appear frequently or are creatively generated.
- **Evidence anchors:**
  - [abstract] KeyBERT achieves an effective balance between user preferences and computational efficiency compared to the other algorithms.
  - [section 4.4] KeyBERT consistently outperforms the others [TF-IDF and Llama 2], scoring on average 5.8% higher... The trend is more pronounced than in the preliminary experiment.
  - [corpus] Related work on *Improving Ad matching via Cluster-Adaptive Keyword Expansion* supports the shift from token-based matching to semantic expansion to maintain relevance.
- **Break condition:** This mechanism breaks if the specific domain requires highly technical or distinct entities (e.g., medical codes) where frequency (TF-IDF) is a proxy for importance, or if the LLM is heavily fine-tuned on the specific corpus.

### Mechanism 2
- **Claim:** High precision against a gold standard does not causally imply high user satisfaction; a misalignment exists between algorithmic benchmarks and human perception.
- **Mechanism:** Precision metrics often penalize synonyms or semantically equivalent phrases that differ from the exact "gold standard" tokens. Users, however, rate based on "comprehensiveness" and "reasonableness," allowing for semantic variety not captured by exact-match metrics.
- **Core assumption:** The "Gold Standard" derived from participant annotations is a valid proxy for "ideal" keywords, but the *metric* used to compare algorithms to it (precision) is flawed.
- **Evidence anchors:**
  - [abstract] State-of-the-art LLMs like DeepSeek achieve higher precision benchmarks but do not translate to superior user satisfaction.
  - [section 4.6] There is a clear misalignment between algorithmic benchmark performance and user ratings... This reveals a long-overlooked gap between traditional precision-focused metrics and user-perceived algorithm efficiency.
  - [corpus] Corpus signals indicate ongoing research into *Multi-Objective* keyword generation, implying single-objective optimization (like precision) is insufficient for complex ad tasks.
- **Break condition:** This mechanism breaks if the downstream task is machine-centric (e.g., indexing for a strictly keyword-based search engine) rather than human-centric (e.g., reading comprehension or ad targeting).

### Mechanism 3
- **Claim:** In high-frequency ad auctions, computational efficiency acts as a hard gating mechanism; an algorithm is only viable if it completes inference within the latency window (approx. 100ms).
- **Mechanism:** Real-Time Bidding (RTB) systems require responses before the ad opportunity expires. Even if an LLM produces superior keywords, its inference time violates the timing constraints of the auction loop, forcing a trade-off toward lighter models like KeyBERT.
- **Core assumption:** The system architecture involves synchronous processing where keyword extraction happens on-demand during the ad request lifecycle.
- **Evidence anchors:**
  - [section 2.3] Since the automated auctions used to sell online advertising space must be completed within approximately 100 ms, the algorithms involved must operate swiftly and efficiently.
  - [section 4.7] TF-IDF is the least computationally intensive, followed by KeyBERT, whereas Llama 2 is significantly more time-consuming, operating on a completely different scale.
  - [corpus] *Efficient Continual Learning in Keyword Spotting* highlights the necessity of resource-efficient models in real-time environments.
- **Break condition:** This mechanism breaks in asynchronous or "warm-up" contexts where keywords can be pre-computed and cached before the auction.

## Foundational Learning

- **Concept: Cosine Similarity in Vector Space**
  - **Why needed here:** KeyBERT relies entirely on vector embeddings and cosine similarity to rank keywords. You cannot debug KeyBERT without understanding that it prioritizes semantic closeness over raw frequency.
  - **Quick check question:** If a document mentions "canine" frequently but the embedding for "dog" is closer to the document vector, which will KeyBERT likely prefer?

- **Concept: Human-in-the-Loop (HITL) Evaluation**
  - **Why needed here:** The paper's core thesis is that automated metrics (F1, Precision) are insufficient. You must understand how to structure surveys (Likert scales, distinctiveness vs. representativeness) to capture "perception."
  - **Quick check question:** Why does the paper argue that "distinctiveness" received notably low ratings from participants, and what does that imply about how users read summaries?

- **Concept: The RTB Latency Constraint**
  - **Why needed here:** This is the business justification for not using SOTA LLMs. You must understand the ~100ms budget to advocate for simpler models like KeyBERT.
  - **Quick check question:** If an LLM takes 2 seconds to generate perfect keywords, why is it useless for the contextual advertising system described?

## Architecture Onboarding

- **Component map:** Ingest (News Articles) -> Extract (TF-IDF, KeyBERT, Llama 2 pipelines) -> Benchmark (Gold Standard comparison) -> Evaluate (Survey Interface) -> Deploy (Matching Logic -> Ad Selection)

- **Critical path:** The **Preliminary Experiment** phase is critical. You cannot evaluate algorithms without first establishing the "Gold Standard" keywords via crowd-sourcing. If this ground truth is noisy, all comparative metrics (Mechanism 2) are invalid.

- **Design tradeoffs:**
  - **Latency vs. Nuance:** KeyBERT offers a middle ground (semantic understanding + low latency), whereas LLMs offer nuance at high latency cost, and TF-IDF offers speed with zero semantic understanding.
  - **Metric vs. Perception:** Optimizing for Precision (Machine metric) often degrades alignment with user Reasonableness (Human metric).

- **Failure signatures:**
  - **The "Exact Match" Trap:** An algorithm achieves high Precision scores but users rate it poorly. *Diagnosis:* The model is likely missing synonyms or paraphrasing that users find "reasonable."
  - **The "Drift" Failure:** LLM output is rated high on "reasonableness" but low on "representativeness." *Diagnosis:* The model hallucinated a connection or focused on a minor detail.

- **First 3 experiments:**
  1. **Latency Profiling:** Run TF-IDF, KeyBERT, and a small LLM (Llama 2 7B) on a local CPU against a 5-article batch to reproduce the processing time gap. Verify that LLMs exceed the 100ms threshold significantly.
  2. **Gold Standard Validation:** Replicate the "Preliminary Experiment" with a small group (n=10). Have them select keywords for a single article. Calculate inter-rater agreement (consensus) to see if a clear Gold Standard emerges.
  3. **Precision-Perception Gap Test:** Generate keywords for an article using KeyBERT and a strong LLM. Measure Precision against your Gold Standard. Then, blindly ask a user which set they prefer. Confirm if the higher Precision set loses the popularity vote.

## Open Questions the Paper Calls Out

- **How can incorporating goal-oriented instructions on specific keyword properties (e.g., reasonableness) into LLM prompts affect user satisfaction compared to general prompting strategies?**
  - The authors note Llama 2 was prompted without explicit instructions on specific properties and suggest future work explore "alternative prompting strategies that incorporate more specific and goal-oriented instructions."

- **How can the evaluation property of "distinctiveness" be operationalized to reduce participant confusion and better distinguish high-quality keyword sets?**
  - The authors observe that distinctiveness received notably low ratings and confused participants, suggesting "clarifying the measurement of distinctiveness could be a valuable area for future research."

- **To what extent does domain-specific fine-tuning of keyword extraction algorithms improve the alignment between quantitative precision metrics and qualitative user perception?**
  - The authors acknowledge a limitation that they used standard implementations without domain-specific data, noting that future work should explore how domain adaptation influences performance and potential bias.

## Limitations

- The study relies on a small sample of 5 news articles, raising concerns about generalizability to other content types and domains
- Participant demographics (all Prolific users) may introduce sampling bias that affects the validity of user preference findings
- Exact Llama 2 prompt configuration and Word2Vec model parameters remain unspecified, creating reproducibility gaps

## Confidence

- **High Confidence:** The computational efficiency rankings (TF-IDF fastest, Llama 2 slowest) and the fundamental observation that user perception differs from precision metrics are well-supported by the data and methodology
- **Medium Confidence:** The specific performance percentages for KeyBERT (5.8% higher than alternatives) and the claim that SOTA LLMs don't translate to superior user satisfaction require cautious interpretation due to the small sample size
- **Low Confidence:** The generalization of findings to other content types, languages, or advertising contexts beyond news articles and English-language content

## Next Checks

1. **Dataset Expansion Validation:** Test the three algorithms across 50+ articles from diverse news sources and other domains (technical, medical, entertainment) to verify if KeyBERT's performance advantage holds across different content types

2. **RTB Constraint Verification:** Implement a simulated real-time bidding system with 100ms latency constraints to empirically confirm whether Llama 2 consistently exceeds this threshold in practical deployment scenarios

3. **Prompt Sensitivity Analysis:** Systematically vary the Llama 2 prompt structure and instruction specificity to determine if the reported user perception gap can be reduced through better prompt engineering