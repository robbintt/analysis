---
ver: rpa2
title: 'TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language
  Analysis'
arxiv_id: '2510.06063'
source_url: https://arxiv.org/abs/2510.06063
tags:
- time
- series
- data
- anomaly
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TelecomTS, a large-scale multi-modal observability
  dataset derived from a 5G telecommunications network, containing over 1 million
  observations across 18 heterogeneous KPIs with categorical and numerical data types.
  TelecomTS supports diverse downstream tasks including anomaly detection, root-cause
  analysis, and multimodal question-answering.
---

# TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis

## Quick Facts
- arXiv ID: 2510.06063
- Source URL: https://arxiv.org/abs/2510.06063
- Reference count: 40
- Primary result: State-of-the-art models struggle with TelecomTS's abrupt, noisy dynamics and fail to leverage scale information effectively, with anomaly detection F1 scores ranging from 0.133 to 0.711.

## Executive Summary
TelecomTS is a large-scale multi-modal observability dataset derived from a 5G telecommunications network, containing over 1 million observations across 18 heterogeneous KPIs with categorical and numerical data types. The dataset supports diverse downstream tasks including anomaly detection, root-cause analysis, and multimodal question-answering. Benchmarking shows that current state-of-the-art models struggle with the dataset's abrupt, noisy dynamics and fail to effectively leverage scale information, highlighting the need for scale-aware foundation models for real-world observability applications.

## Method Summary
The TelecomTS dataset was constructed from 5G network KPIs (18 channels, 10Hz) and control plane logs, with samples aligned via histogram-matching to correct time offsets. Synthetic anomalies were injected using physics-aware functional transformations of KPIs, validated by expert review. The dataset includes 100ms resolution samples of 128 timesteps each, with 80-20 train-test splits. Models were benchmarked using frozen backbones with trained classification heads, Adam optimizer (lr=0.0001), batch size 64, and 10 epochs, with cross-entropy for classification and MSE for forecasting tasks.

## Key Results
- Anomaly detection F1 scores for top models range from 0.133 to 0.711, with Mantis achieving the highest performance by embedding scale statistics
- Question-answering MAE varies from 0.020 to over 1500, demonstrating significant challenges in multi-modal reasoning
- Standard normalization practices obscure critical magnitude information, limiting use for tasks beyond forecasting
- Models exhibit high false positive rates when interpreting normal bursts as anomalies without contextual priors

## Why This Works (Mechanism)

### Mechanism 1: Scale-Aware Feature Embedding
Preserving the absolute scale of covariates improves performance on observability tasks compared to normalized representations. By embedding scale statistics (mean/std) directly into input patches, models can anchor their representations to physical reality rather than just relative shape. Mantis, which embeds scale information, achieves the highest F1 score (0.711) in anomaly detection, significantly outperforming other architectures.

### Mechanism 2: Contextual Disambiguation of Erratic Noise
Raw observability data is too stochastic for purely statistical anomaly detection; contextual priors (natural language descriptions of "normal" behavior) are required to reduce false positives. Observability data is inherently "bursty" (zero-inflated). Models tend to interpret normal bursts as anomalies. Providing a "context" window allows the model to adjust its prior probability of an anomaly based on operational semantics.

### Mechanism 3: Synthetic Grounding via Functional Transformation
Real-world anomalies are too rare for robust training; principled synthetic anomaly generation effectively augments the learning signal. Instead of random noise injection, the dataset applies physics/protocol-aware functions (linear growth, sinusoidal fluctuation) to specific KPIs to simulate failures. This ensures the model learns causal symptom patterns rather than just generic outliers.

## Foundational Learning

- **Concept: Zero-Inflation and Sparsity**
  - **Why needed here:** Observability data is often zero (idle states) punctuated by extreme spikes. Standard time series models often fail here because they assume continuity.
  - **Quick check question:** Can you explain why a standard smoothing filter might remove the most "informative" parts of a zero-inflated network metric?

- **Concept: Scale-Aware vs. Scale-Invariant Modeling**
  - **Why needed here:** Knowing a value is "90" (high load) vs "10" (low load) is critical, whereas standard datasets often normalize everything to [0,1].
  - **Quick check question:** If you normalize all KPIs to z-scores, what specific information required for "Root Cause Analysis" might be lost?

- **Concept: Multi-Modal Alignment (Time Series + Text)**
  - **Why needed here:** The dataset requires reasoning over both the time series plot and a textual "troubleshooting ticket."
  - **Quick check question:** How would you structure a training sample that pairs a time window of `UL_SNR` drops with the text "User experienced connectivity loss due to jamming"?

## Architecture Onboarding

- **Component map:** 5G Network KPIs + Control Plane logs -> Histogram-matching alignment -> Synthetic Anomaly Injection + Ticket Generation -> Benchmark tasks
- **Critical path:** The De-anonymization and Alignment step is the bottleneck. If the time offset between the traces isn't corrected via KL-divergence minimization, the "multi-modal" link between user activity and KPI spikes breaks.
- **Design tradeoffs:** Real vs. Synthetic Anomalies (authenticity vs. volume); Scale Preservation (enables new tasks vs. complicates convergence)
- **Failure signatures:** High False Positives (flags every traffic burst as anomalous); Magnitude Blindness (predicts correct trend but wrong magnitude)
- **First 3 experiments:**
  1. Baseline Context Test: Run a Zero-Shot LLM on anomaly detection with vs. without the "context" prompt
  2. Scale Ablation: Train a simple forecaster on normalized data vs. raw data to measure degradation in MAE
  3. Root Cause Transfer: Test if a model trained only on "Synthetic" anomalies can successfully detect the "Real" Jamming anomalies

## Open Questions the Paper Calls Out

### Open Question 1
How can time series foundation models be architected to natively encode and leverage absolute scale information without relying on normalization? The conclusion emphasizes the "need for foundation time series models that natively leverage scale information," noting that Mantis performed best specifically because it embedded scale statistics.

### Open Question 2
How can time series model architectures be redesigned to effectively integrate heterogeneous categorical variables alongside numerical metrics? The "Overall Discussions" section states that "categorical variables, commonly present in observability systems, remain underexplored in the design of time series model architectures, which predominantly focus on numerical variates."

### Open Question 3
How can models be improved to effectively fuse temporal data with textual context for multi-modal reasoning in observability settings? The Q&A results analysis underscores a "critical gap in current models' ability to perform multi-modal reasoning," specifically in linking engineering concepts to time series data.

## Limitations

- The evaluation relies heavily on proprietary API calls for LLMs and reasoning models, introducing potential variability in results
- The synthetic anomaly generation method, while validated by experts, may not capture all real-world failure modes, creating a simulation-to-reality gap
- The dataset's scale and heterogeneity make it challenging to establish ground truth for some tasks, particularly root cause analysis

## Confidence

- **High**: Claims about dataset scale (1M+ observations, 18 KPIs) and basic task definitions
- **Medium**: Claims about model performance limitations and importance of scale-aware modeling
- **Low**: Claims about superiority of specific synthetic anomaly generation methods over alternative approaches

## Next Checks

1. Independently verify a sample of anomaly labels and root cause annotations against raw network logs to assess labeling accuracy
2. Test whether models trained on synthetic anomalies can successfully detect the real jamming anomalies, quantifying the simulation-to-reality gap
3. Systematically compare model performance on normalized vs. raw KPI values across all tasks to measure the true impact of scale preservation