---
ver: rpa2
title: Probing the topology of the space of tokens with structured prompts
arxiv_id: '2503.15421'
source_url: https://arxiv.org/abs/2503.15421
tags:
- token
- dimension
- tokens
- distribution
- material
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to recover the token subspace of a
  large language model (LLM) using structured prompts, without access to internal
  embeddings. The approach treats LLMs as nonlinear autoregressive processes and uses
  mathematical transversality theory to prove that collecting response token probabilities
  from single-token queries can embed the token subspace into response probability
  space.
---

# Probing the topology of the space of tokens with structured prompts

## Quick Facts
- arXiv ID: 2503.15421
- Source URL: https://arxiv.org/abs/2503.15421
- Reference count: 38
- Key outcome: Recovers LLM token subspace topology using structured prompts and output probabilities, without accessing internal embeddings

## Executive Summary
This paper presents a novel method to recover the topology of a large language model's token subspace using only output probability distributions from structured prompts, without requiring access to internal token embeddings. The approach treats LLMs as nonlinear autoregressive dynamical systems and applies transversality theory to prove that collecting response token probabilities from single-token queries can embed the token subspace into response probability space. The method was demonstrated on Llemma-7B, successfully recovering its token subspace topology with a fiber bundle structure separating semantic variability from noise.

## Method Summary
The method queries an LLM with a fixed prefix plus a single variable token, then collects the top-k probabilities for m next tokens in the response. By concatenating these probability vectors for all possible query tokens, the method constructs a map from the latent token space to a response probability space. Transversality theory guarantees this mapping preserves topological invariants like dimension and clustering up to homeomorphism. The approach uses delay embedding theory analogous to Takens' theorem, requiring the observation window m and probability dimension ℓ to satisfy 2d < mℓ, where d is the latent subspace dimension.

## Key Results
- Successfully recovered token subspace topology from Llemma-7B using only output probabilities
- Recovered embedding preserves topological features like dimension and clustering, though not exact distances
- Token subspace exhibits fiber bundle structure with base stratum (semantic variability, dim 5-10) and fiber stratum (noise)
- Data collection took 13 hours and dimension estimation 3 hours on MacBook M3

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating an LLM as a nonlinear autoregressive dynamical system allows recovery of its token subspace topology from output probabilities alone.
- **Mechanism:** The method uses delay embedding theory (analogous to Takens' theorem). By querying with fixed prefix and variable token x, observing probabilities of next m response tokens constructs a map from latent token space to response probability space.
- **Core assumption:** LLM's transformer block is smooth function f, token subspace on compact manifold Z of dimension d.
- **Evidence anchors:** [abstract] "...treats LLMs as nonlinear autoregressive processes and uses mathematical transversality theory..." [section 3] "The correctness of Algorithm 1 is justified by Theorem 1..."
- **Break condition:** If 2d > m min{ℓ, dim X}, the map fails to be injective immersion.

### Mechanism 2
- **Claim:** Recovered topology separates semantic structure ("base") from model noise ("fiber") via fiber bundle structure.
- **Mechanism:** Local dimension estimation reveals stratified manifold with lower-dimensional "base" stratum (dim 5-10) capturing semantic variability and higher-dimensional "fiber" stratum capturing noise.
- **Core assumption:** Probability sampling process is stable enough to resolve "base" dimension despite "fiber" noise.
- **Evidence anchors:** [results] "The token subspace seems to have the structure of a fiber bundle..." [section 4] "Figure 3 shows... corners present in the curve..."
- **Break condition:** Excessive sampling error causes "bleed through" from fiber to base, obscuring stratification.

### Mechanism 3
- **Claim:** Transversality guarantees generic measurement functions preserve topological invariants.
- **Mechanism:** Theorem 1 uses transversality theory to prove for "generic" choices of model f and measurement g, mapping from latent token space to observed probability space is homeomorphism.
- **Core assumption:** Specific LLM and probability collection method fall within "generic" (good) set defined in proof.
- **Evidence anchors:** [section 3] "...there is a residual subset V of C^∞(X, Y) such that if g ∈ V..." [abstract] "...provides strong theoretical justification..."
- **Break condition:** If output embedding is pathological (e.g., rank-deficient), theoretical guarantees void.

## Foundational Learning

- **Concept: Takens' Theorem / Delay Embedding**
  - **Why needed here:** Mathematical bedrock allowing dynamical system's state space reconstruction from time-series observations. Paper extends this from continuous dynamics to discrete autoregressive LLMs.
  - **Quick check question:** If latent manifold has dimension d=14, what minimum bound for observation window m (given ℓ=3) ensures embedding? (Paper uses 2d < mℓ).

- **Concept: Homeomorphism vs. Isometry**
  - **Why needed here:** Method recovers topology up to homeomorphism, not isometry. Means local connectivity and dimension preserved, but exact distances and angles not.
  - **Quick check question:** Can you use recovered coordinates to calculate exact Euclidean distance between two tokens in original embedding space?

- **Concept: Stratified Manifolds**
  - **Why needed here:** Token subspace is not uniform manifold; has regions of different dimensions (strata). Required to interpret fiber bundle results and "corners" in dimension plots.
  - **Quick check question:** In context of this paper, does "corner" in log-log radius plot indicate broken token or transition between semantic and noise strata?

## Architecture Onboarding

- **Component map:** Input prefix + Query Token → LLM (Transformer f) → Output Probability Estimator (g) → Vector concatenation → Local Dimension Estimator

- **Critical path:** Stability of probability estimation (Step 5 in Algorithm 1). High variance directly obscures "base" dimension, causing recovered topology to reflect sampling noise rather than semantic structure.

- **Design tradeoffs:**
  - Option 1 (m=30, ℓ=3): Low computational cost (R^90). Best for efficient dimension estimation.
  - Option 3 (m=1, ℓ=32016): High cost. Theoretically valid but practically intractable for post-processing.
  - Constraint: Must satisfy 2d < mℓ. Increasing ℓ more expensive than increasing m if vocabulary is large.

- **Failure signatures:**
  - Dimension Collapse: Estimated dimensions cluster around single median value (approx 10) regardless of true local dimension (Figure 5 bias).
  - Loss of Stratification: Log-log plots (Figure 3) appear linear without "corners," indicating fiber/base structure indistinguishable.

- **First 3 experiments:**
  1. Baseline Reproduction: Run Algorithm 1 on Llemma-7B with Option 1 (m=30, ℓ=3). Measure wall-clock time for 100 tokens to estimate 13-hour scaling claim.
  2. Sensitivity Analysis: Vary number of query repetitions for probability estimation. Plot variance of recovered dimension estimate against sampling count to find "stable" plateau.
  3. Topology Verification: Select tokens known to be "isolated" (low true dimension) and compare recovered dimension against high-density semantic tokens to verify method's discriminative power.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fiber bundle structure with specific "definite radius" sphere observed in Llemma-7B generalize to other large language models?
- Basis in paper: [explicit] Authors note in Section 4 that definite radius of sphere at each token "appears to be characteristic of Llemma-7B; other LLMs do not seem to exhibit this structure as clearly."
- Why unresolved: Experimental results focused exclusively on Llemma-7B, and preliminary checks suggest this specific geometric feature may not be universal.
- What evidence would resolve it: Applying Algorithm 1 to diverse model architectures (e.g., Llama, GPT families) and analyzing resulting volume-radius plots for similar strata.

### Open Question 2
- Question: Does recovered embedding preserve complex topological invariants beyond local dimension, such as persistent homology or exact clustering?
- Basis in paper: [explicit] Section 3 states that "checking whether two spaces are homeomorphic is extremely difficult" and explicitly notes that "Other verifications remain as future work."
- Why unresolved: Authors restricted validation to dimension estimation because verifying full homeomorphism is computationally and mathematically complex.
- What evidence would resolve it: Computing persistent homology on recovered coordinates and comparing persistence diagrams to those derived from original token embeddings.

### Open Question 3
- Question: Can this probing method be utilized to systematically detect or characterize "glitch tokens" (under-trained or anomalous tokens)?
- Basis in paper: [explicit] Section 5 observes that data collection process "may be finding interesting glitch tokens" because semantically meaningless queries resulted in coherent responses.
- Why unresolved: Identification of glitch tokens was incidental observation of method's output, not systematic study or primary focus of current work.
- What evidence would resolve it: Study correlating tokens identified as isolated or low-dimension by this method with known sets of glitch tokens or adversarial inputs.

## Limitations
- Computational scalability is prohibitive for models with large vocabularies (32K+ tokens), making method impractical for most modern LLMs
- Method's ability to distinguish semantic "base" from noise "fiber" depends critically on stable probability estimation, which may not hold in all operating regimes
- Theoretical genericity assumptions rely on smoothness conditions that may not be satisfied by discrete token spaces and learned embeddings

## Confidence
- **High Confidence:** Mathematical framework using transversality theory is internally consistent and provides theoretical justification; recovered topology preserves local dimension and clustering structure; method successfully recovers structured token subspace from Llemma-7B data
- **Medium Confidence:** Recovered token subspace has fiber bundle structure with semantic base and noise fiber strata; base dimension typically falls in 5-10 range for semantic variability; dimension estimation method reliably captures true subspace dimensions
- **Low Confidence:** Specific numerical values for base/fiber dimensions are universally applicable across different LLMs; method can reliably separate semantic structure from model noise in all operating regimes; computational time estimates will hold for other models and hardware configurations

## Next Checks
1. **Genericity Validation:** Systematically test method across multiple LLMs with different architectures to empirically verify transversality assumptions hold in practice. Compare recovered dimensions against ground truth when available.
2. **Sampling Sensitivity Analysis:** Conduct controlled experiments varying query repetitions and measurement dimensions to establish quantitative thresholds for when fiber obscures base structure. Determine minimum sampling requirements for reliable base dimension recovery.
3. **Cross-Model Topology Comparison:** Apply method to multiple LLMs (different sizes, training approaches, vocabularies) and compare recovered base dimensions and fiber structures to test whether 5-10 base dimension range is universal property or model-specific.