---
ver: rpa2
title: Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?
arxiv_id: '2511.00808'
source_url: https://arxiv.org/abs/2511.00808
tags:
- rlvr
- reward
- duration
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting the duration of
  public transit incidents from unstructured text alerts. It proposes adapting Reinforcement
  Learning from Verifiable Rewards (RLVR) to this noisy, continuous forecasting task
  by introducing a tolerance-based, shaped reward function that grants partial credit
  within a continuous error margin.
---

# Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?

## Quick Facts
- arXiv ID: 2511.00808
- Source URL: https://arxiv.org/abs/2511.00808
- Reference count: 15
- Key outcome: RLVR significantly outperforms specialized math-reasoning LLMs and classical regressors at tight accuracy bands, achieving a 35% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline.

## Executive Summary
This paper addresses the problem of predicting the duration of public transit incidents from unstructured text alerts. It proposes adapting Reinforcement Learning from Verifiable Rewards (RLVR) to this noisy, continuous forecasting task by introducing a tolerance-based, shaped reward function that grants partial credit within a continuous error margin. The method was evaluated on a curated dataset of NYC MTA service alerts. Results show that RLVR significantly outperforms specialized math-reasoning LLMs and classical regressors at tight accuracy bands, achieving a 35% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline, demonstrating that RLVR can be successfully adapted to real-world, noisy forecasting when the reward design reflects the continuous nature of the problem.

## Method Summary
The method uses RLVR (DAPO or GRPO) to train LLMs to predict transit incident durations from unstructured GTFS-rt service alerts. A tolerance-based shaped reward function (R2) grants partial credit within a continuous error margin, replacing binary rewards that cause instability in continuous forecasting. The best configuration uses DAPO with Llama-3.1-8B-Instruct, P2 prompt (category list), and δ=10, α=2 parameters. Training runs for 100 steps with batch size 64, learning rate 1e-6, and 8 rollouts per prompt at temperature 1.0. The primary metric is Acc@5 (accuracy within ±5 minutes).

## Key Results
- RLVR with shaped reward R2 achieves 47.0% Acc@5, outperforming math-specialized models by 8.5% relative and classical regressors by 35% relative
- P2 prompt (category list without statistics) ultimately achieves highest Acc@5 despite lower initial zero-shot performance compared to P3
- Binary reward R1 causes catastrophic -10.6% drop in Acc@5, confirming shaped rewards are essential for stable training

## Why This Works (Mechanism)

### Mechanism 1: Gradient Smoothing via Tolerance-Based Rewards
Applying a tolerance-based, shaped reward function (R2) stabilizes policy optimization for continuous forecasting tasks where exact matches are rare, unlike binary rewards (R1). The R2 reward $r_{shp} = [1 - (e/\delta)^\alpha]^+$ creates a Lipschitz-smooth gradient based on the distance to the ground truth, preventing the high-variance updates caused by sparse binary signals. This allows the LLM to learn "directionally" correct reasoning from partial credit. If the prediction errors are dominated by extreme outliers (heavy tails), a bounded smooth reward might under-penalize large deviations compared to a robust loss like Huber loss.

### Mechanism 2: Generalist Semantic Anchoring over Specialist Formalism
General instruction-tuned LLMs outperform math-specialized models because the task requires robust semantic parsing of noisy text rather than formal symbolic derivation. Math-specialized models are optimized for formal logic where syntax is strict, while transit alerts contain "noisy, continuous labels" and inconsistent formatting. General models leverage broader semantic priors to handle this ambiguity, whereas math models "hallucinate" formal structure where none exists. If the transit alerts were re-formatted into structured, equation-like logic puzzles, the math-specialized models would likely regain dominance.

### Mechanism 3: Exploration-Prior Prompts (P2) vs. Anchoring-Prior Prompts (P3)
Prompts that omit explicit statistical priors (P2) allow the RLVR policy to explore and learn underlying features more effectively than prompts that provide statistical tables (P3), despite P3 having superior zero-shot performance. P3 encourages the model to "memorize/anchor" to the provided mean values, causing a premature collapse in entropy. P2 provides structural guidance without numerical shortcuts, forcing the model to learn the mapping from text to duration via the shaped reward, preserving exploration. If the training budget is very low, the exploration phase of P2 might not converge, making the "free lunch" of P3's statistical priors the better choice.

## Foundational Learning

- **Concept: Reinforcement Learning from Verifiable Rewards (RLVR)**
  - Why needed here: This is the core training paradigm replacing Supervised Fine-Tuning (SFT). Unlike RLHF which uses a learned reward model, RLVR uses a deterministic function (the verifier) to check the answer.
  - Quick check question: Does the reward come from a neural network judging the answer (RLHF), or a code snippet checking the answer (RLVR)? (Answer here: A "Verifier" checks the prediction vs. ground truth).

- **Concept: Tolerance-based Accuracy (Acc@k)**
  - Why needed here: Standard regression metrics (MAE) failed to capture the utility of the RLVR models, which made occasional large errors but were more often "close enough."
  - Quick check question: If a transit delay lasts 12 minutes, is a prediction of 13 minutes considered "correct"? (Answer: Yes, under Acc@5, $|13-12|=1 \le 5$).

- **Concept: KL Divergence / Reference Models**
  - Why needed here: To prevent the model from generating gibberish to hack the reward, the optimization penalizes deviation from a frozen reference model ($\pi_{ref}$).
  - Quick check question: Why is the term $\beta \text{KL}(\pi || \pi_{ref})$ necessary in the loss function?

## Architecture Onboarding

- **Component map:** GTFS-rt Text Alerts -> Tokenized Prompt (P1/P2/P3) -> LLM (Policy) generates Duration -> Verifier parses output and computes Error -> Applies Shaped Reward (R2) -> DAPO/GRPO updates Policy using Verifier Reward + KL constraint

- **Critical path:** The implementation of the **Shaped Reward (R2)** is the single most critical failure point. If implemented as a simple binary check (R1) or raw MAE (R0), the paper states training is unstable or degrades.

- **Design tradeoffs:**
  - RLVR vs. Classical Regressors: You trade off low MAE (Regressors win) for high Acc@5 (RLVR wins). Choose RLVR if "tight correctness" matters more than "average closeness."
  - Prompt Strategy: Choose P3 if you have **no training budget** (Zero-shot). Choose P2 if you **can train** (RLVR) to maximize performance.

- **Failure signatures:**
  - Entropy Collapse: If training loss drops to 0 but accuracy stagnates, the model has overfitted to a specific token sequence (check Section 6.1.1 entropy plots).
  - R1 Instability: If accuracy oscillates wildly, ensure you are using the Shaped Reward (R2), not Binary (R1).
  - Math-Model Drift: If the model starts outputting latex formatting or proofs, you likely loaded a Math-specialized model instead of an Instruction-tuned one.

- **First 3 experiments:**
  1. **Reward Validation:** Train two small models (1 epoch) comparing R1 (Binary) vs. R2 (Shaped) on a fixed seed. Confirm R1 is unstable (Section 5.7).
  2. **Prompt Ablation:** Run zero-shot inference with P3 (Statistics) to establish the baseline "statistical anchor." Then run RLVR training with P2 to observe the "P2 takes longer but wins" dynamic (Section 6.1.3).
  3. **Generalization Check:** Evaluate the trained model on a hold-out set of "Passenger Incidents" (common) vs. "Station Related" (rare) to see if the shaped reward handles class imbalance better than Category-Mean heuristics (Table 3/4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the learned linguistic and operational patterns from the NYC MTA dataset generalize to transit systems in other cities or with different modal mixes (e.g., bus-dominant networks)?
- Basis in paper: [explicit] Section 7.2 states, "Future work will investigate cross-city transferability... [to] reveal whether the learned linguistic and operational patterns are universal or city-specific."
- Why unresolved: The study evaluated data exclusively from NYC MTA, which has specific alert phrasing and a subway-dominant network structure, potentially limiting the universality of the findings.
- What evidence would resolve it: Testing the zero-shot or fine-tuned performance of the trained models on out-of-distribution service alert datasets from geographically distinct transit agencies.

### Open Question 2
- Question: What are the full performance capabilities and training dynamics of this RLVR approach when scaled up beyond the 100-step limit used in this study?
- Basis in paper: [explicit] Section 6.2 notes, "It remains an open question for the full potential of RLVR finetuned LLMs... [due to] limited training scale."
- Why unresolved: Training was capped at 100 steps/6 hours; some prompt strategies (like P2) showed low initial accuracy but high final performance, suggesting longer training might yield different or stronger results.
- What evidence would resolve it: Training convergence curves and final performance metrics (Acc@5, MAE) from experiments allowing for significantly longer training durations (e.g., 1,000+ steps).

### Open Question 3
- Question: Can this temporal prediction framework be effectively extended to model spatial impacts, such as identifying affected network segments and quantifying delay propagation?
- Basis in paper: [explicit] Section 7.2 identifies "Spatio-Temporal Impact" as a critical next step: "extend this framework to predict the spatial impact... and quantifying the delay propagation."
- Why unresolved: The current model predicts a single scalar value (duration) from text, lacking the integration of network topology required to estimate how a delay ripples through specific routes or stations.
- What evidence would resolve it: A multi-task model architecture that outputs both duration and affected downstream stations, validated against realized delay propagation data.

## Limitations

- The shaped reward function parameters (δ=10, α=2) are empirically tuned to this specific dataset without strong theoretical justification, potentially limiting generalizability.
- The category extraction and statistics table generation procedure is described but not fully specified, creating potential reproducibility issues.
- The paper doesn't address how the model handles events with durations exceeding 2 hours (the dataset maximum), which could represent edge cases in real-world deployment.

## Confidence

- **High Confidence:** The comparative results showing RLVR with shaped reward R2 outperforming classical regressors and math-specialized models on Acc@5 metrics. The ablation studies demonstrating R1's instability and R2's necessity are well-supported by the data presented.
- **Medium Confidence:** The mechanism explanations regarding why general instruction-tuned models outperform math-specialized models. While the evidence from ablation studies is strong, the semantic parsing explanation is somewhat speculative without direct experimental validation of the proposed mechanism.
- **Medium Confidence:** The claim that P2 ultimately outperforms P3 despite P3's superior zero-shot performance. The training curves support this, but the exact prompt formulations and category extraction procedures have some ambiguity that could affect reproducibility.

## Next Checks

1. **Reward Function Sensitivity Analysis:** Systematically vary δ and α parameters in the shaped reward R2 (e.g., δ ∈ {5, 10, 20}, α ∈ {1, 2, 3}) and measure Acc@5 performance across the full range. This would quantify how sensitive the method is to reward parameterization and whether the chosen values are optimal or dataset-specific.

2. **Cross-Dataset Generalization Test:** Apply the trained RLVR model to a completely different public transit dataset (e.g., another city's GTFS-rt alerts or a different transportation mode) without fine-tuning. Measure whether the Acc@5 performance degrades significantly, which would indicate whether the approach generalizes beyond the NYC MTA dataset or is overfit to its specific characteristics.

3. **Math-Specialized Model Retraining Experiment:** Retrain Qwen-Math (or another math-specialized model) using the same RLVR pipeline with shaped reward R2 and P2 prompt, then compare performance to the instruction-tuned baseline. This would determine whether the poor performance of math models is due to their pre-training or whether they can be successfully adapted to this task with appropriate RLVR training.