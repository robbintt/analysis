---
ver: rpa2
title: 'Advancing Academic Chatbots: Evaluation of Non Traditional Outputs'
arxiv_id: '2512.00991'
source_url: https://arxiv.org/abs/2512.00991
tags:
- llama
- human
- evaluation
- llms
- academic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This research evaluates whether large language models (LLMs) can\
  \ generate high-quality non-traditional academic outputs such as slide decks and\
  \ podcast scripts, and compares two retrieval strategies\u2014Advanced RAG (hybrid\
  \ keyword-semantic search) and Graph RAG (structured knowledge graph)\u2014for question\
  \ answering (Q&A). A prototype combining Meta\u2019s LLaMA 3.3 70B and OpenAI\u2019\
  s GPT-4o-mini was tested using both human ratings and LLM-as-a-judge evaluation."
---

# Advancing Academic Chatbots: Evaluation of Non Traditional Outputs

## Quick Facts
- arXiv ID: 2512.00991
- Source URL: https://arxiv.org/abs/2512.00991
- Reference count: 0
- Primary result: GPT-4o-mini with Advanced RAG outperformed Graph RAG and LLaMA 3.3 70B for academic Q&A accuracy and structured output generation.

## Executive Summary
This research evaluates large language models' ability to generate non-traditional academic outputs such as slide decks and podcast scripts, comparing two retrieval strategies: Advanced RAG (hybrid keyword-semantic search) and Graph RAG (structured knowledge graph). A prototype combining Meta's LLaMA 3.3 70B and OpenAI's GPT-4o-mini was tested using human ratings and LLM-as-a-judge evaluation. GPT-4o-mini with Advanced RAG consistently produced the most accurate responses and excelled in structured outputs, while Graph RAG showed limited improvements and higher hallucination rates. Human evaluators proved essential for detecting stylistic and layout flaws, emphasizing the need for combined human-LLM evaluation in academic contexts.

## Method Summary
The study tested two retrieval-augmented generation configurations on 10 academic papers from a university archive. Advanced RAG combined BM25 keyword matching with BERT embeddings and Named Entity Recognition in ChromaDB, while Graph RAG used manually curated knowledge graphs with NetworkX/igraph and MiniLM embeddings. Two models were evaluated: LLaMA 3.3 70B Instruct and GPT-4o-mini. Outputs included Q&A responses, summaries, podcast scripts, and slide decks. Quality was assessed across 11 dimensions using human raters and LLM judges (Claude, DeepSeek), with reference-free evaluation grounded in source documents.

## Key Results
- GPT-4o-mini with Advanced RAG produced the most accurate responses and highest scores in usefulness, reasoning, and overall satisfaction.
- Graph RAG offered limited improvements and led to more hallucinations due to structural complexity and manual setup.
- GPT-4o-mini excelled in structured outputs like slides, while LLaMA 3.3 70B demonstrated strength in narrative coherence for podcasts.
- Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human-LLM evaluation.

## Why This Works (Mechanism)

### Mechanism 1
Hybrid lexical-semantic retrieval (Advanced RAG) outperforms knowledge graph-based retrieval (Graph RAG) for academic Q&A accuracy. Advanced RAG combines BM25 keyword matching with BERT embeddings and Named Entity Recognition, allowing both precise lexical hits and semantic similarity. Graph RAG's structural complexity introduces failure points during manual entity-relationship construction. The academic domain benefits more from flexible keyword-semantic matching than from manually curated graph structures.

### Mechanism 2
Model selection should be task-dependent—GPT-4o-mini excels at structured outputs while LLaMA 3.3 70B produces more coherent narrative content. GPT-4o-mini's distillation-based training yields concise, well-organized outputs suited for slides and Q&A. LLaMA 3.3 70B's larger parameter count and training data support longer-form narrative coherence in podcast dialogues.

### Mechanism 3
Combined human-LLM evaluation captures both quantitative consistency and qualitative nuance better than either method alone. LLM judges apply uniform scoring logic with low variance, detecting hallucinations systematically. Human evaluators show higher variance but identify layout flaws, tonal appropriateness, and engagement that LLMs overlook.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: All tested configurations ground outputs in source documents. Understanding chunking, embedding, and retrieval scoring is prerequisite.
  - Quick check question: Can you explain how BM25 differs from semantic embedding retrieval?

- Concept: Knowledge Graphs and Community Detection
  - Why needed here: Graph RAG uses NetworkX, igraph, and Leiden algorithm. Understanding entity-relationship structures is required for debugging.
  - Quick check question: What is the Leiden algorithm's purpose in graph-based retrieval?

- Concept: LLM-as-a-Judge Evaluation
  - Why needed here: External models (Claude, DeepSeek) evaluate outputs using prompted criteria. Understanding pointwise vs pairwise evaluation formats is essential.
  - Quick check question: What biases might an LLM exhibit when evaluating another LLM's outputs?

## Architecture Onboarding

- Component map: Document ingestion -> Chunking with metadata -> Dual retrieval pipeline construction -> Model-specific prompt engineering -> Output generation -> Parallel human/LLM evaluation

- Critical path: Document ingestion → Chunking with metadata → Dual retrieval pipeline construction → Model-specific prompt engineering → Output generation → Parallel human/LLM evaluation

- Design tradeoffs: Advanced RAG offers faster setup and lower hallucination rates vs Graph RAG's potentially richer semantic relationships; GPT-4o-mini provides structured, pedagogical outputs vs LLaMA's narrative depth; LLM evaluation scales but misses stylistic nuance; human evaluation captures nuance but doesn't scale.

- Failure signatures: High hallucination rates associated with Graph RAG + LLaMA combination; check entity extraction quality; low completeness scores for LLaMA Graph RAG; verify retrieval coverage; evaluation divergence signals stylistic vs factual assessment mismatch.

- First 3 experiments:
  1. Replicate GPT-4o-mini + Advanced RAG baseline on your document corpus; measure Q&A accuracy and hallucination rate using reference-free evaluation
  2. A/B test slide generation with GPT-4o-mini vs LLaMA 3.3 70B on identical source documents; use human evaluators for layout/structure assessment
  3. Compare LLM judge (Claude) scores against human evaluators on 20 outputs; calculate standard deviation and identify systematic blind spots in automated evaluation

## Open Questions the Paper Calls Out

- Can a hybrid architecture combining Advanced RAG and Graph RAG outperform individual retrieval methods for academic tasks? The study evaluated the retrieval systems in isolation, finding Advanced RAG superior in accuracy while Graph RAG struggled with hallucinations, but did not test a combined approach.

- How effective are automated entity and relationship extraction systems in constructing knowledge graphs for domain-specific academic content? The authors manually curated the knowledge graph to isolate structural performance, leaving the noise and errors inherent in automated extraction untested.

- Do LLM-generated slides and podcasts demonstrate pedagogical value in longitudinal classroom settings? The current evaluation relied on immediate human and LLM ratings of quality, rather than measuring actual learning outcomes or student engagement over time.

## Limitations

- Small sample size (10 academic papers) may not scale to broader domains
- Manual Graph RAG construction doesn't reflect real-world automated extraction
- Exact prompt templates for different output types remain unspecified

## Confidence

- High Confidence: GPT-4o-mini with Advanced RAG consistently outperforming other configurations in Q&A accuracy and structured outputs
- Medium Confidence: Task-specific model selection (GPT-4o-mini for structured outputs, LLaMA 3.3 70B for narrative content)
- Medium Confidence: Necessity of combined human-LLM evaluation based on observed discrepancies

## Next Checks

1. Replicate the Advanced RAG + GPT-4o-mini configuration on a larger corpus (minimum 50 documents) to verify scalability and consistency of performance gains
2. Implement automated entity extraction for Graph RAG to eliminate manual setup complexity and compare hallucination rates against the original manual approach
3. Conduct cross-validation with domain experts evaluating the same outputs independently to quantify the reliability of current human evaluation findings