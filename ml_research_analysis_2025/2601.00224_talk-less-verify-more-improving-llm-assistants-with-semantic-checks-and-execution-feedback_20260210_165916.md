---
ver: rpa2
title: 'Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and
  Execution Feedback'
arxiv_id: '2601.00224'
source_url: https://arxiv.org/abs/2601.00224
tags:
- systems
- feedback
- verification
- assistants
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two complementary verification mechanisms,
  Q and Feedback+, to improve the reliability of LLM-based assistants in enterprise
  settings. Q performs reverse translation and semantic matching between generated
  code and user intent, while Feedback+ incorporates execution feedback to guide iterative
  code refinement.
---

# Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback

## Quick Facts
- arXiv ID: 2601.00224
- Source URL: https://arxiv.org/abs/2601.00224
- Authors: Yan Sun; Ming Cai; Stanley Kok
- Reference count: 0
- One-line primary result: Two verification mechanisms (Q* and Feedback+) reduce error rates and completion time in LLM-based assistants, with semantic matching excelling on SQL tasks and execution feedback improving mathematical reasoning

## Executive Summary
This paper introduces two complementary verification mechanisms, Q* and Feedback+, to improve the reliability of LLM-based assistants in enterprise settings. Q* performs reverse translation and semantic matching between generated code and user intent, while Feedback+ incorporates execution feedback to guide iterative code refinement. Evaluated on three benchmark datasets (Spider, Bird, GSM8K), both mechanisms reduced error rates and task completion time compared to a baseline iterative correction approach. Q* achieved up to 93.50% accuracy in semantic matching on a dedicated dataset, while Feedback+ showed stronger performance on code generation tasks, particularly for mathematical reasoning (GSM8K accuracy increased from 29.00% to 41.00%). Results indicate that semantic matching is well-handled by modern LLMs, but reverse translation remains a bottleneck, especially for tasks involving abstract logic.

## Method Summary
The study employs an Actor-Critic iterative correction framework where CodeLlama-7B generates code and GPT-3.5-turbo evaluates it. Q* mechanism translates generated code back to natural language and performs semantic matching with the original query. Feedback+ mechanism executes the generated code and feeds execution results (errors or outputs) back to the model for correction. Both approaches use up to 10 correction rounds with early stopping if discrimination scores exceed 0.99 or fail to improve for 3 consecutive rounds. The methods were evaluated on Spider and Bird text-to-SQL datasets (400 samples each) and GSM8K mathematical reasoning tasks (500 samples).

## Key Results
- Q* achieved 93.50% accuracy on semantic matching tasks
- Feedback+ improved GSM8K accuracy from 29.00% to 41.00%
- Runtime reduced from 29.0 hours (baseline) to 13.5-18.5 hours depending on method
- Q* excelled on SQL tasks but performed worse than baseline on mathematical reasoning (22.00% vs 29.00%)

## Why This Works (Mechanism)

### Mechanism 1: Q* (Reverse Translation & Semantic Matching)
- **Claim**: Translating generated code back into natural language allows the system to verify semantic alignment with user intent, reducing "hidden errors" in data retrieval tasks.
- **Mechanism**: An Actor generates code (e.g., SQL). A Critic performs **reverse translation** to convert this code back into a natural language question (Q*). The Critic then performs **semantic matching** between Q* and the original user query Q, assigning a probability score to the "Yes" token. High scores allow the code to proceed; low scores trigger iterative refinement.
- **Core assumption**: Modern LLMs (like GPT-3.5) are sufficiently capable of semantic comparison (93.50% accuracy noted) that the primary failure point is the fidelity of the reverse translation step rather than the comparison itself.
- **Evidence anchors**:
  - [abstract] Q* performs reverse translation and semantic matching... identifying reverse translation as a key bottleneck.
  - [Page 6] Details the two-stage prompt: "Given the database schema and SQL below, generate the corresponding question..."
  - [Page 10] Shows Q* improves SQL tasks (Spider/Bird) but fails on GSM8K (accuracy drop to 22.00% vs Baseline 29.00%).
- **Break condition**: Effectiveness drops significantly for tasks requiring abstract multi-step logic (e.g., mathematical reasoning) where surface-level semantic alignment does not guarantee logical correctness.

### Mechanism 2: Feedback+ (Execution-Grounded Correction)
- **Claim**: Incorporating runtime execution results (errors or outputs) into the LLM prompt provides a "ground-truth" signal that enables more robust self-correction than semantic review alone.
- **Mechanism**: The system executes the generated code (SQL or Python). The execution result—including success status, error messages, or output—is fed back into the prompt. The Actor uses this explicit feedback to repair "Buggy SQL" or flawed logic in the next iteration.
- **Core assumption**: The LLM can interpret technical error messages and trace them back to logical flaws in the code within a limited context window.
- **Evidence anchors**:
  - [abstract] Feedback+ incorporates execution feedback... showed stronger performance on code generation tasks (GSM8K accuracy increased from 29.00% to 41.00%).
  - [Page 7] Describes the Feedback+ prompt template including the "Execution Result: {}" placeholder.
  - [corpus] Related work "Trust, But Verify" supports the value of verifiable rewards/feedback in reducing "superficial self-reflection."
- **Break condition**: If the execution environment is unavailable, sandboxed, or too slow, this mechanism cannot function. It also struggles if error messages are cryptic or uninformative.

### Mechanism 3: Generator-Discriminator Loop with Early Stopping
- **Claim**: Replacing linear generation with an iterative loop managed by a discriminator reduces task completion time by filtering low-quality candidates early.
- **Mechanism**: The system operates in rounds (up to 10). A Critic (Discriminator) scores candidates. The loop stops early if the discrimination score exceeds 0.99 or fails to improve for 3 rounds, preventing infinite regression.
- **Core assumption**: The Discriminator's scoring heuristic correlates strongly with actual task success.
- **Evidence anchors**:
  - [Page 5] Describes the Actor-Critic setup and the shift from linear pipelines to iterative verification.
  - [Page 12] Shows runtime reduction from 29.0 hours (Baseline) to 13.5 hours (Q*) or 18.5 hours (Feedback+) on the Spider dataset.
  - [corpus] "Monitor-Generate-Verify (MGV)" formalizes similar iterative reasoning cycles, though the paper notes prior work lacks the specific semantic/execution checks introduced here.
- **Break condition**: If the Critic is misaligned (e.g., "misjudges plan alignment"), valid plans may be rejected early, leading to cascading errors.

## Foundational Learning

- **Concept**: **Actor-Critic Architecture**
  - **Why needed here**: The entire framework relies on separating the "generator" (Actor) from the "evaluator" (Critic). Without this distinction, the system cannot perform internal verification before user delivery.
  - **Quick check question**: Can you explain the difference between the model generating the code and the model evaluating the code's intent?

- **Concept**: **Reverse Translation (Code-to-NL)**
  - **Why needed here**: This is the core novelty of the Q* mechanism. Engineers must understand that "verifying code" often requires translating it back to the user's language to check intent.
  - **Quick check question**: If an LLM generates a SQL query, how would you prompt it to explain what question that query answers?

- **Concept**: **Semantic vs. Logical Correctness**
  - **Why needed here**: The paper demonstrates that semantic alignment (Q*) works for SQL, while logical execution (Feedback+) is needed for Math. Understanding this distinction is critical for task-specific deployment.
  - **Quick check question**: Why might a mathematically correct Python script be flagged as "semantically wrong" if it solves a different problem than the user asked?

## Architecture Onboarding

- **Component map**:
  - User Query + Schema -> Actor (CodeLlama-7B) -> Candidate Code
  - Code -> Critic (GPT-3.5-turbo) -> Score
  - Execution Environment (Python/ SQL) -> Feedback+ (execution results)

- **Critical path**:
  1. User Query + Schema -> Actor -> Candidate Code
  2. **Parallel/Divergent Path**:
     - *Path A (Q*)*: Code -> Critic (Reverse Translate) -> NL Question -> Critic (Match) -> Score
     - *Path B (Feedback+)*: Code -> Execution Env -> Result/Error -> Critic -> Score
  3. If Score < Threshold: Append Feedback/Reasoning to Prompt -> Return to Step 1
  4. If Score > Threshold: Return Code to User

- **Design tradeoffs**:
  - **Q***: Lower latency (faster convergence on SQL tasks) but brittle for complex logic (fails on GSM8K)
  - **Feedback+**: Higher accuracy on reasoning tasks but higher latency per iteration (O((n + p_exec) * d)) and requires sandboxed execution

- **Failure signatures**:
  - **Q* Hallucination**: The reverse translation invents a question that matches the user query, but the underlying code does not actually answer that question (the "semantic matching is easy, translation is hard" problem)
  - **Feedback+ Loop**: The model repeatedly generates the same syntax error despite error messages, requiring the 3-round early stop

- **First 3 experiments**:
  1. **Baseline Reproduction**: Implement the simple iterative correction loop (Baseline) on Spider to verify the 44.30% accuracy benchmark
  2. **Q* Isolation**: Implement only the Reverse Translation + Semantic Matching module. Run on Spider vs. GSM8K to replicate the performance divergence (SQL improves, Math degrades)
  3. **Feedback+ Integration**: Add the execution result to the prompt context. Measure the accuracy lift on GSM8K (targeting the ~41% range) and compare wall-clock time against Q*

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the fidelity of reverse translation be improved for tasks involving abstract logic, such as mathematical reasoning?
- **Basis in paper**: [explicit] The authors state, "Future efforts to enhance Q* should focus less on the semantic module and more on the reverse translation component" because it remains a bottleneck in domains like GSM8K.
- **Why unresolved**: While semantic matching is accurate (93.50%), Q* performance dropped significantly on GSM8K (29% to 22%) because translating structured code back into natural language fails to capture implicit logical steps.
- **What evidence would resolve it**: A modified Q* mechanism that improves GSM8K accuracy by successfully translating and verifying abstract logic steps, rather than just surface-level intent.

### Open Question 2
- **Question**: Do the verification gains observed in Q* and Feedback+ persist when applied to larger, real-world enterprise datasets?
- **Basis in paper**: [explicit] The conclusion notes, "Future work should... evaluate performance on larger enterprise datasets."
- **Why unresolved**: The current study used controlled benchmark subsets (400-500 samples) selected due to resource constraints, leaving scalability unproven.
- **What evidence would resolve it**: Evaluation results showing consistent error reduction and time savings on full-scale, complex industrial data warehouses.

### Open Question 3
- **Question**: Is the performance gap between Feedback+ and Q* consistent across different underlying LLM architectures?
- **Basis in paper**: [inferred] The experiments were limited to a specific setup (CodeLlama-7B as Actor, GPT-3.5-turbo as Critic).
- **Why unresolved**: Feedback+ significantly outperformed Q* on code generation, but it is unclear if this is inherent to the verification method or a result of the specific models' reasoning capabilities versus their semantic understanding.
- **What evidence would resolve it**: Ablation studies varying the Actor/Critic models to determine if Q* becomes competitive with Feedback+ under different model configurations.

## Limitations
- Q* mechanism fails dramatically on mathematical reasoning tasks (22% accuracy vs 29% baseline), suggesting fundamental limitations for abstract logic
- Both mechanisms require specific infrastructure (execution environment for Feedback+, high-quality semantic matching for Q*) that may not be available in all enterprise settings
- Performance depends heavily on the quality of reverse translation, which remains a bottleneck for complex logical reasoning tasks

## Confidence

**High confidence**: Q* accuracy on SQL tasks (93.50% semantic matching), runtime reduction metrics (13.5-18.5 hours vs 29.0 baseline), and the general observation that semantic matching is easier than reverse translation for LLMs

**Medium confidence**: Cross-task generalizability claims, as results show Q* performs well on SQL but poorly on GSM8K, suggesting task-specific rather than universal effectiveness

**Medium confidence**: Early stopping effectiveness, as the 0.99 threshold and 3-round improvement criteria are heuristic and may not generalize to different task distributions

## Next Checks

1. **Task-specific mechanism validation**: Test Q* on non-SQL code generation tasks (e.g., API calls, configuration files) to determine if the semantic matching advantage extends beyond database queries

2. **Error analysis**: Perform detailed analysis of Q* failures on GSM8K to determine if reverse translation consistently loses logical steps or if other failure modes exist

3. **Resource consumption profiling**: Measure the trade-off between Feedback+ execution overhead and iteration count reduction across different task types to establish when each mechanism is cost-effective