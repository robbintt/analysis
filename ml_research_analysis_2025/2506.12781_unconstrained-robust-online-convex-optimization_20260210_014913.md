---
ver: rpa2
title: Unconstrained Robust Online Convex Optimization
arxiv_id: '2506.12781'
source_url: https://arxiv.org/abs/2506.12781
tags:
- algorithm
- online
- lemma
- robust
- unconstrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses online convex optimization with corrupted
  gradients in the unconstrained setting, where the algorithm only receives potentially
  corrupted gradients instead of the true gradients. The main challenge is that unconstrained
  algorithms can grow exponentially fast, making them highly sensitive to corruption.
---

# Unconstrained Robust Online Convex Optimization

## Quick Facts
- arXiv ID: 2506.12781
- Source URL: https://arxiv.org/abs/2506.12781
- Authors: Jiujia Zhang; Ashok Cutkosky
- Reference count: 40
- Primary result: Achieves regret bound ||u||G(√T + k) with known Lipschitz constant, or ||u||G(√T + k) + (√(||u||² + G²)k) with unknown Lipschitz

## Executive Summary
This paper develops the first robust online convex optimization algorithms for the unconstrained setting, where gradients can be corrupted. The key insight is combining gradient clipping with carefully designed regularization that includes both Huber and quadratic components. This approach allows the algorithm to tolerate up to √T corruptions without degrading regret asymptotically compared to the uncorrupted setting. The method achieves a regret bound of ||u||G(√T + k) when the Lipschitz constant G is known, and a bound with an extra penalty of (√(||u||² + G²)k) when G is unknown.

## Method Summary
The method combines gradient clipping with adaptive regularization. For known Lipschitz constant G, the algorithm clips gradients at threshold G and uses Huber-type regularization with scaling c = kG to cancel corruption error while maintaining low bias. For unknown G, it employs a FILTER mechanism that maintains an adaptive threshold h_t that doubles only after observing k+1 gradients exceeding the current threshold. The regularization combines sparse quadratic components (activated when truncation is detected) with attenuated corruption-penalizing terms that decrease over epochs. The key insight is that the CORRECTION term from regularization cancels the ERROR from corruptions while the BIAS term grows only as ||u||.

## Key Results
- Achieves regret ||u||G(√T + k) in the unconstrained setting with known Lipschitz constant
- For unknown Lipschitz constant, achieves regret ||u||G(√T + k) + (√(||u||² + G²)k)
- Tolerates up to √T corruptions without degrading regret asymptotically compared to uncorrupted setting
- Uses only O(1) space for the unknown Lipschitz case, versus O(k) for prior bounded-domain approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient clipping at an appropriate threshold bounds the corruption error introduced by corrupted gradients, preventing them from causing unbounded regret in the unconstrained setting.
- Mechanism: The algorithm computes a clipped gradient $\tilde{g}_t^c = \tilde{g}_t / \|\tilde{g}_t\| \cdot \min(h_t, \|\tilde{g}_t\|)$ at each round. When the threshold $h_t = G$ (the Lipschitz constant), this ensures $\|\tilde{g}_t^c - g_t\| \leq \|\tilde{g}_t - g_t\|$, making the clipped gradient no more corrupted than the original. The ERROR term in the regret decomposition is then bounded by $kG \max_t \|w_t\|$ rather than growing unboundedly.
- Core assumption: A known or estimated bound $G \geq \max_t \|g_t\|$ on the true gradients (Lipschitz constant); corruption level $k$ satisfies the cumulative deviation bound in Equation (4).
- Evidence anchors:
  - [abstract]: "combines gradient clipping with regularization"
  - [Section 4, Equation 5]: Formal definition of the clipping operation $\tilde{g}_t^c$
  - [Section 5.1]: Analysis showing ERROR ≤ kG max_t ||w_t|| when h_t = G
  - [corpus]: Van Erven et al. (2021) uses similar clipping in bounded domains; this paper extends to unconstrained setting
- Break condition: If $h_t$ is set significantly below $G$, the clipping operation introduces truncation error where even uncorrupted gradients are altered: $\|g_t - \tilde{g}_t^c\| > \|g_t - \tilde{g}_t\|$. This is addressed in Section 6.2 via the FILTER mechanism.

### Mechanism 2
- Claim: Huber-type regularization with carefully chosen scaling cancels the corruption error while maintaining bias proportional to the comparator norm $\|u\|$.
- Mechanism: The regularizer $r_t(w) = f_t(w; c, p, \alpha)$ defined in Equation (7) uses a piecewise structure: polynomial ($\|w\|^p$) near $\|w_t\|$ and linear elsewhere. With $c = kG$ and $p = \ln T$, the CORRECTION term $\sum_t r_t(w_t)$ grows as $\tilde{\Omega}(kG \max_t \|w_t\|)$, which cancels the ERROR term. Meanwhile, the BIAS term $\sum_t r_t(u)$ grows only as $\tilde{O}(kG\|u\|)$, preserving comparator adaptivity.
- Core assumption: Corruption level $k$ and Lipschitz constant $G$ are both known for parameter tuning.
- Evidence anchors:
  - [Section 5.1, Equation 7]: Definition of $f_t$ with scaling factor $c = kG$
  - [Lemma B.1]: Bounds showing $\sum_t f_t(w_t) \geq c(\max_t \|w_t\| - \alpha)$ and $\sum_t f_t(u) \leq \tilde{O}(c\|u\|)$
  - [Theorem 5.1]: Final regret bound $\tilde{O}(\|u\|G(\sqrt{T} + k))$ achieved through error-bias cancellation
  - [corpus]: Zhang & Cutkosky (2022) introduced this Huber-type structure for unconstrained learning without corruption
- Break condition: If the scaling constant $c$ is set below $kG$ (e.g., underestimating corruption or Lipschitz constant), CORRECTION becomes insufficient to cancel ERROR, leading to residual linear regret growth. If $c$ is excessively large, BIAS dominates.

### Mechanism 3
- Claim: When $G$ is unknown, a combination of adaptive thresholding (FILTER), sparse quadratic regularization, and attenuated Huber regularization achieves robustness with an additive penalty $(\|u\|^2 + G^2)k$.
- Mechanism: (1) **FILTER** (Algorithm 3) maintains $h_t$ that doubles only after observing $k+1$ gradients exceeding the current threshold, ensuring $h_t \leq O(G)$ always. This limits truncation error to $\tilde{O}(k)$ rounds. (2) **Sparse quadratic regularization** $\alpha_t\|w\|^2$ activates only when $h_t$ doubles (truncation detected), controlling truncation error via the identity $\|g_t - \tilde{g}_t^c\|\|w_t\| - \alpha_t\|w_t\|^2 \leq \|g_t - \tilde{g}_t^c\|^2 / 4\alpha_t$. (3) **Attenuated $\beta_t$** for corruption error applies quadratic regularization only at "epoch boundaries" where $\|w_t\|$ doubles, with $\beta_t$ decreasing as $1/(1 + \text{count of doublings})$. (4) A small Huber regularizer handles residual corruption error.
- Core assumption: Corruption bound $k$ is known; the problem structure allows epoch-based error decomposition.
- Evidence anchors:
  - [Section 6.2, Algorithm 3]: FILTER mechanism with $k$-lag thresholding
  - [Lemma F.2]: Properties guaranteeing $h_t \leq \max(\tau, 4G)$ and $|\bar{P}| \leq \tilde{O}(k)$
  - [Equations 11-12]: Definitions of $\alpha_t$ and attenuated $\beta_t$
  - [Theorem 6.1, Corollary 6.2]: Final regret with $(\|u\|^2 + G^2)k$ penalty
  - [corpus]: Van Erven et al. (2021) uses similar filtering for bounded domains but requires $O(k)$ space; this paper achieves $O(1)$ space
- Break condition: If corruption level $k$ is underestimated, FILTER may double $h_t$ prematurely (causing excessive truncation error) or fail to reach adequate thresholds (causing clipping to be too aggressive). The regret bound degrades proportionally to actual vs. estimated corruption.

## Foundational Learning

- **Online Convex Optimization (OCO) and Regret**:
  - Why needed here: The entire paper builds on the OCO framework where a learner outputs $w_t$, receives gradient $g_t$, and incurs regret $R_T(u) = \sum_t \langle g_t, w_t - u \rangle$ relative to comparator $u$. Understanding why regret (not just loss) is the metric is essential.
  - Quick check question: Explain why the paper aims for regret scaling with $\|u\|$ rather than a fixed diameter $D$. What does this imply for the algorithm's behavior when $u$ is far from the origin?

- **Composite Loss and Mirror Descent**:
  - Why needed here: The base algorithm (Algorithm 2) must handle losses $\langle \tilde{g}_t^c, w \rangle + r_t(w)$ where $r_t$ is known ahead of time. Standard online learning treats losses as opaque; composite losses require the algorithm to exploit the known regularizer structure.
  - Quick check question: In Equation (14), why must $f_t$ be known before computing $w_{t+1}$? How does Lemma C.1's two-step decomposition help?

- **Huber Loss and Piecewise Regularization**:
  - Why needed here: The regularizer $f_t$ in Equation (7) has a non-standard form: polynomial near $\|w_t\|$, linear elsewhere. Understanding this geometry is crucial for seeing why it provides strong correction at the iterates while maintaining low bias at arbitrary $u$.
  - Quick check question: Sketch the behavior of $\sigma_t(w; p, \alpha)$ for $p = 3$ when $\|w\|$ varies from $0$ to $10\|w_t\|$. Why does the linear tail matter for the BIAS bound?

## Architecture Onboarding

- **Component map**:
  Input: Corrupted gradient ˜g_t, corruption bound k, [optional: Lipschitz G]

  [FILTER] ──► h_t (threshold) ──► [Clipping] ──► ˜g_t^c
                                      │
  [TRACKER] ◄── w_t ──► z_t (magnitude estimate) ──► β_t
      │
      └───────────────────────────────────────────────┐
                                                      ▼
  [Regularizer Composer] ◄── α_t from FILTER ◄─── r_t(w) = f_t(w) + a_t||w||²
              │                              ▲
              └── c, p, α parameters ────────┘
                      │
                      ▼
  [Base Learner A] ◄── ˜g_t^c, r_t
      │
      ├── Known G: Algorithm 2 (Centered Mirror Descent)
      └── Unknown G: Algorithm 5 (Epigraph-based + Aw + Ay)
                      │
                      ▼
                  w_{t+1}

- **Critical path**:
  1. At each round $t$, FILTER receives $\tilde{g}_t$ and outputs both the clipped gradient $\tilde{g}_t^c$ and updated threshold $h_{t+1}$.
  2. TRACKER (unknown-G case only) receives $w_t$ and outputs $z_{t+1}$ for computing $\beta_t$.
  3. Regularizer composer computes $r_t$ based on whether $h_t$ or $z_t$ changed (indicating truncation or epoch boundary).
  4. Base learner A receives $\tilde{g}_t^c$ and $r_t$, outputs $w_{t+1}$ via either mirror descent (known G) or epigraph projection (unknown G).
  5. Key invariant: ERROR − CORRECTION ≤ $\tilde{O}(kG^2)$ must hold for the regret decomposition to work.

- **Design tradeoffs**:
  - **Known G vs. Unknown G**: Known G achieves optimal $\|u\|G(\sqrt{T} + k)$ regret; unknown G adds $(\|u\|^2 + G^2)k$ penalty but requires no hyperparameter knowledge. Choose based on whether Lipschitz constant can be reliably estimated offline.
  - **Safety at origin vs. performance for large $\|u\|$** (Corollary 6.2 vs. 6.3): Setting $c = k\tau_G, \gamma_\beta = k$ yields better regret for large comparators; setting $c = \tau_G, \gamma_\beta = k^2$ guarantees constant regret at $u = 0$ regardless of $k$ but pays $k^2$ penalty elsewhere.
  - **Space complexity**: FILTER uses $O(1)$ space (counter only), vs. van Erven et al.'s $O(k)$ approach for bounded domains.

- **Failure signatures**:
  - **Exponential iterate growth**: If FILTER's $k$ is underestimated and corruptions are frequent, $h_t$ may never reach $G$, causing clipping to truncate true gradients repeatedly. Symptom: $\|w_t\|$ grows as $2^{t}$ (Lemma E.3). Fix: Increase $k$ estimate or add a hard bound on $\|w_t\|$.
  - **Regret at origin exceeds $\epsilon$**: If base learner's composite regret handling fails (e.g., incorrect mirror map), $R_A^T(0) > \epsilon$. Check: Verify Algorithm 2's $\psi_t$ definition and bisection solver correctness.
  - **Quadratic bias dominates**: If $\alpha_t$ or $\beta_t$ accumulate beyond $\tilde{O}(k)$, the BIAS term explodes. Check: Ensure $\gamma_\alpha = O(1)$ and $\gamma_\beta = O(k)$ with proper attenuation in Equation (12).

- **First 3 experiments**:
  1. **Validate known-G algorithm on synthetic 1D problem**: Set $\ell_t(w) = |w - u^*|$ with $u^* = 1$, $G = 1$, $T = 400$. Inject $k = 20$ corruptions at rounds 300-319 by setting $\tilde{g}_t = -g_t$. Compare Algorithm 1 + Algorithm 2 against KT-bettor baseline. Expected: Regret ratio (corrupted/clean) ≈ $(\sqrt{T} + k)/\sqrt{T} \approx 2$; KT-bettor should show exponential deviation per Figure 1.
  
  2. **Stress-test FILTER with varying corruption levels**: Fix $G = 1$, vary $k \in \{10, 20, 40, 80\}$ with $T = k^2$. Measure (a) final threshold $h_{T+1}$, (b) number of clipping rounds $|\bar{P}|$, (c) regret at $u = 0$ and $u = 10$. Expected: $h_{T+1} \leq 4G$, $|\bar{P}| \leq \tilde{O}(k)$, regret at origin ≈ $\epsilon$.
  
  3. **Compare unknown-G parameter settings**: Using Corollary 6.2 settings ($c = k\tau_G, \gamma_\beta = k$) vs. Corollary 6.3 settings ($c = \tau_G, \gamma_\beta = k^2$), plot regret vs. $\|u\|$ for $\|u\| \in [0.1, 100]$. Expected: Corollary 6.3 should show flat regret near origin; Corollary 6.2 should show lower regret for $\|u\| \gg 1$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the extra additive penalty term $(\|u\|^2 + G^2)k$ incurred in the unknown Lipschitz setting optimal?
- Basis in paper: [explicit] The conclusion states, "it is unclear whether the multiplicative dependence on $k$ is optimal in the presence of corruption" referring to the unknown $G$ result.
- Why unresolved: The paper provides a matching lower bound for the known $G$ case (Theorem 5.2) but does not prove whether the specific error terms introduced to handle unknown $G$ are unavoidable in the corrupted setting.
- What evidence would resolve it: A lower bound proof in the unknown Lipschitz setting that necessitates the $(\|u\|^2 + G^2)k$ dependency.

### Open Question 2
- Question: Can the algorithm be made adaptive to the corruption level $k$ without requiring it as an input parameter?
- Basis in paper: [inferred] Section 2 assumes the algorithm is provided with a number $k$ satisfying corruption bounds, and Algorithm 3 uses this input to set the clipping threshold.
- Why unresolved: The current "FILTER" and regularization mechanisms rely on prior knowledge of $k$ to function, preventing a fully parameter-free robust solution.
- What evidence would resolve it: An algorithmic variant that estimates corruption on-the-fly or adapts without $k$ input while maintaining regret guarantees of similar order.

### Open Question 3
- Question: Is it possible to achieve the regret bound for unknown $G$ without the logarithmic factors hidden in the $\tilde{O}(\cdot)$ notation?
- Basis in paper: [inferred] While Theorem E.5 provides a lower bound with log factors for the known $G$ case, the paper does not analyze if the log factors in the unknown $G$ setting (Corollary 6.2) are tight.
- Why unresolved: The "epigraph-based regularization" technique used for unknown $G$ introduces logarithmic dependencies that may be an artifact of the analysis method rather than the problem difficulty.
- What evidence would resolve it: A refined analysis or new algorithm for unknown $G$ that removes the logarithmic factors, or a problem-specific lower bound establishing their necessity.

## Limitations

- The main theorems assume the Lipschitz constant $G$ is known, which may not hold in practice. While the unknown-$G$ variant exists, it incurs an additional $(\|u\|^2 + G^2)k$ penalty.
- The algorithm requires knowing the corruption bound $k$ as an input parameter, which may be difficult to estimate accurately in real applications.
- The analysis focuses on worst-case regret bounds but doesn't provide empirical validation of practical performance or compare against non-robust baselines in corrupted settings.

## Confidence

- **High**: The regret bound derivation for the known-$G$ case (Theorem 5.1) and the core mechanism of error-bias cancellation through Huber regularization.
- **Medium**: The FILTER mechanism's correctness and the unknown-$G$ regret bounds (Theorem 6.1), which involve more complex epoch-based analysis.
- **Medium**: The claim that $O(1)$ space suffices for the unknown-$G$ case, which relies on specific properties of the doubling threshold strategy.

## Next Checks

1. Implement the known-$G$ algorithm (Algorithm 1 + 2) and validate the regret scaling on a synthetic problem with varying corruption levels and comparator norms.
2. Test FILTER's behavior with underestimated corruption bounds to confirm the doubling mechanism prevents threshold underflow.
3. Compare the unknown-$G$ algorithm's performance under both Corollary 6.2 and 6.3 parameter settings to verify the trade-off between origin safety and large-comparator performance.