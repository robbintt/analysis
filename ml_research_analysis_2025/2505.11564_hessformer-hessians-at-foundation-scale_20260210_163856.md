---
ver: rpa2
title: 'HessFormer: Hessians at Foundation Scale'
arxiv_id: '2505.11564'
source_url: https://arxiv.org/abs/2505.11564
tags:
- arxiv
- hessian
- eigenvalues
- figure
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HessFormer introduces a distributed Hessian-vector product (HVP)
  framework that scales to billion-parameter transformer models by preserving gradients
  in the autograd graph and overlapping HVP contractions with forward passes. This
  enables stochastic Lanczos quadrature on multi-GPU nodes without modifying user
  code.
---

# HessFormer: Hessians at Foundation Scale

## Quick Facts
- **arXiv ID:** 2505.11564
- **Source URL:** https://arxiv.org/abs/2505.11564
- **Reference count:** 11
- **Primary result:** First empirical Hessian spectral densities at foundation model scale (70B+ parameters)

## Executive Summary
HessFormer introduces a distributed Hessian-vector product (HVP) framework that scales to billion-parameter transformer models by preserving gradients in the autograd graph and overlapping HVP contractions with forward passes. This enables stochastic Lanczos quadrature on multi-GPU nodes without modifying user code. Applied to DeepSeek-LLM-70B, the method reveals the first empirical Hessian spectral densities at foundation model scale, including heavy-tailed eigenvalue decay, near-zero rank degeneracy, and large negative outliers suggesting atypical loss-surface geometry.

## Method Summary
HessFormer implements distributed HVP computation for billion-parameter transformers using double backward passes (Pearlmutter's trick) with gradients preserved in the autograd graph. The framework uses HuggingFace's `device="auto"` for layerwise model splitting, enabling efficient computation across multiple GPUs. Stochastic Lanczos quadrature with 10-20 iterations estimates spectral density without storing the full Hessian. The method requires disabling Flash SDP and careful memory management to avoid OOM errors while maintaining numerical stability.

## Key Results
- First empirical Hessian spectral densities for 70B-parameter foundation models
- Heavy-tailed eigenvalue decay with concentration near zero suggesting near-rank degeneracy
- Large negative outliers (-100 to -600) indicating atypical loss-surface geometry
- Spectral properties persist across different datasets and subsampling rates

## Why This Works (Mechanism)
The distributed HVP computation works by keeping the gradient computation attached to the autograd graph during the first backward pass (`create_graph=True`), enabling a second backward pass for the Hessian-vector product. This approach, combined with layerwise model splitting and vector scattering across GPUs, allows the framework to scale to models that would otherwise exceed single-GPU memory limits. The stochastic Lanczos quadrature transforms the massive Hessian into a small, tractable tridiagonal matrix whose spectrum can be efficiently computed.

## Foundational Learning

- **Concept: Autograd and Computational Graphs**
  - **Why needed here:** HessFormer's core innovation is keeping the gradient computation attached to the graph (`create_graph=True`) to enable a second backward pass. Without this, distributed HVP computation is not possible.
  - **Quick check question:** What PyTorch argument must be set during the first backward pass to enable gradient-of-gradient computation?

- **Concept: Stochastic Trace Estimation**
  - **Why needed here:** Directly computing the trace of a massive matrix (like a 70B-parameter Hessian) is intractable. Stochastic trace estimation allows for approximating it using Hutchinson's method with random vectors, which is the foundation for estimating spectral density.
  - **Quick check question:** Why is the trace of a matrix equal to the expected value of a random vector's quadratic form?

- **Concept: Krylov Subspaces and Lanczos Algorithm**
  - **Why needed here:** The Lanczos algorithm builds an orthonormal basis for the Krylov subspace. This process is what transforms the massive Hessian into a small, tractable tridiagonal matrix whose spectrum can be easily computed.
  - **Quick check question:** What is the primary advantage of the Lanczos algorithm over direct diagonalization for very large matrices?

## Architecture Onboarding

- **Component Map:** HVP Worker (forward pass + double backward) -> Lanczos Driver (orthogonalization + tridiagonal construction) -> Spectral Analyzer (diagonalization of tridiagonal matrix)

- **Critical Path:**
  1. Initialization: Random seed vector `v` scattered across GPUs, model in eval mode
  2. HVP Iteration: For each Lanczos iteration, HVP Worker called with current vector `v`, returns `H*v`
  3. Orthogonalization: Result orthogonalized against previous two vectors via three-term recurrence
  4. Analysis: After `k` iterations, tridiagonal matrix diagonalized to produce spectral density plot

- **Design Tradeoffs:**
  - Memory vs. Iterations: Avoiding full Q-matrix storage saves memory but limits iteration count before "ghost" eigenvalues appear; recommended `k_max < 20`
  - Compute vs. Spectral Resolution: More iterations yield better spectrum but cost more compute; 4-10 iterations typical for massive models
  - Data Subsampling: Necessary for feasibility but too much subsampling alters spectral shape, increasing noise and outlier prominence

- **Failure Signatures:**
  - Memory Overflow (OOM): Model plus gradient buffers exceed combined GPU memory
  - Ghost Eigenvalues: Spurious duplicate eigenvalues clustered near largest magnitude ones indicate too many Lanczos iterations
  - Numerical Instability: Spectral peaks very close to zero (e.g., `1e-6`) difficult to distinguish from true rank degeneracy

- **First 3 Experiments:**
  1. Sanity Check on Small Model: Run pipeline on single-GPU model (e.g., 1.5B Qwen) and compare distributed vs single-GPU spectral results
  2. Orthogonalization Ablation: Run Lanczos with full orthogonalization vs three-term recurrence on small model to quantify ghost eigenvalue error
  3. Dataset Subsampling Test: Run pipeline on 12B Pythia with varying subsampling (1%, 10%, 100%) to observe spectral shape changes

## Open Questions the Paper Calls Out

- **Open Question 1:** Is the observed concentration of eigenvalues near zero indicative of a truly low-rank Hessian, or does the matrix simply possess a massive number of numerically flat directions?
  - **Basis in paper:** [explicit] The authors state on Page 7, "Further work could look a multiplicative noise model to settle the question as to whether the Hessian really is low rank, or just has very many nearly flat directions."
  - **Why unresolved:** The stochastic Lanczos quadrature method provides only a moment-matched approximation, and standard additive noise models cannot distinguish between exact zeros and values smaller than machine epsilon ($10^{-7}$).
  - **What evidence would resolve it:** A theoretical derivation and empirical validation of a multiplicative noise model applied to the Hessian at foundation scale.

- **Open Question 2:** Do Hessian spectral properties such as heavy-tailed decay and block-level heterogeneity persist, flatten, or fundamentally change when scaling models to 1 trillion parameters or applying Mixture-of-Experts (MoE) architectures?
  - **Basis in paper:** [explicit] Page 2 explicitly lists as unknown: "Whether any of these properties persist, flatten, or split in the 70 B to 1 T range is unknown" and questions if heavy tails "become plateaus at mixture-of-experts boundaries."
  - **Why unresolved:** Current empirical knowledge stops at dense 70B parameter models; computational cost prevents analyzing trillion-parameter or MoE models.
  - **What evidence would resolve it:** Extending HessFormer framework to dense models >100B parameters and MoE architectures to compare spectral densities against observed heavy-tailed distributions.

- **Open Question 3:** Can the Hessian of a foundation model be proven to be banded (or sparse within machine precision), and can this band size be efficiently estimated?
  - **Basis in paper:** [explicit] Page 8 suggests that since checking for the minimum band is NP-hard, "potentially a sampling based RCM [Reverse Cuthill-McKee] method could be used to see what the band size could be capped at."
  - **Why unresolved:** Determining exact bandwidth requires $O(P^2)$ calculations, which is computationally infeasible for billion-parameter models.
  - **What evidence would resolve it:** Development of a sampling-based permutation algorithm that can verify bandedness without requiring the full dense matrix.

## Limitations

- Code and exact checkpoint identifiers are not released, making exact reproduction difficult
- Memory-optimized Lanczos without full orthogonalization introduces spectral estimation errors
- Dataset subsampling methodology not precisely specified, potentially affecting spectral density estimates
- Interpretation of negative Hessian eigenvalues as "atypical loss-surface geometry" lacks rigorous theoretical justification

## Confidence

- **High confidence:** Distributed HVP framework using double backward passes is technically sound and builds on established Pearlmutter's method
- **Medium confidence:** Spectral density estimation via stochastic Lanczos quadrature is valid in principle, but memory optimizations introduce accuracy uncertainties
- **Low confidence:** Interpretation of negative eigenvalues as indicative of atypical loss-surface geometry is not well-supported by theoretical justification

## Next Checks

1. **Code Release Verification:** Obtain and run HessFormer codebase on multi-GPU setup with 1.5B Qwen model to confirm distributed HVP implementation works as described
2. **Orthogonalization Ablation Benchmark:** Systematically compare spectral estimates with and without full re-orthogonalization across varying iteration counts (k=4, 8, 12, 16) on 12B parameter model
3. **Data Scale Sensitivity Analysis:** Replicate spectral density estimation on 12B Pythia with controlled dataset subsampling (1%, 5%, 10%, 100%) while keeping all other hyperparameters fixed