---
ver: rpa2
title: 'OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling'
arxiv_id: '2506.20512'
source_url: https://arxiv.org/abs/2506.20512
tags:
- mid-training
- data
- arxiv
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why different base language model families
  exhibit divergent behaviors during reinforcement learning, particularly on reasoning
  tasks. The authors find that Llama models tend to predict answers prematurely and
  produce repetitive outputs during RL training, unlike more RL-friendly Qwen models.
---

# OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling

## Quick Facts
- arXiv ID: 2506.20512
- Source URL: https://arxiv.org/abs/2506.20512
- Reference count: 28
- Primary result: Mid-training transforms Llama into an RL-compatible foundation model, matching Qwen2.5 performance at same scale

## Executive Summary
This paper investigates why Llama models struggle with reinforcement learning for mathematical reasoning while Qwen models excel. Through systematic mid-training interventions, the authors identify that high-quality mathematical web corpora establish reasoning priors that prevent policy collapse during RL. They develop a two-stage mid-training strategy (Stable-then-Decay) that transforms Llama into an RL-compatible foundation model. Their OctoThinker models achieve performance on par with Qwen2.5 at the same scale, effectively narrowing the gap between model families and providing actionable insights for developing RL-scalable foundation models.

## Method Summary
The OctoThinker approach applies two-stage mid-training to Llama base models before RL training. The Stable phase trains on 200B tokens of high-quality mathematical web data (MegaMath-Web-Pro) with constant learning rate to establish reasoning priors. The Decay phase introduces a 20B-token mixture of QA data and CoT data with cosine learning rate decay to refine behavioral patterns. Models are then fine-tuned using GRPO with a progressive length scheduler and complex prompt templates. The method is evaluated across multiple scales (1B, 3B, 8B) and compared against Qwen2.5 baselines.

## Key Results
- MegaMath-Web-Pro significantly outperforms lower-quality math corpora (FineMath-4plus) for RL readiness
- Two-stage mid-training with Stable-then-Decay learning rate schedule improves both base and RL performance
- OctoThinker models match Qwen2.5 performance at 3B/8B scales on math reasoning benchmarks
- Long-CoT data improves reasoning depth but requires stability interventions (complex templates, length scheduling) to prevent RL collapse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-quality mathematical web corpora establish a latent reasoning manifold that prevents policy collapse during RL exploration, whereas lower-quality math data fails to provide this stability.
- **Mechanism:** Mid-training on reasoning-dense web text shapes the base model's prior distribution. When RL (GRPO) subsequently searches for reasoning paths, a high-quality prior ensures gradient updates find coherent chains of thought rather than falling into repetitive or premature-answer local minima.
- **Core assumption:** The divergence between Llama and Qwen is primarily data-driven rather than architecture-driven, and can be closed via data intervention.
- **Evidence anchors:**
  - [abstract]: "high-quality mathematical corpora... significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so."
  - [section 3.2]: "MegaMath-Web-Pro and MegaMath-Web-Pro-Max bring significant gains... while Finemath-4plus yields only marginal improvements... [FineMath] typically begins with '\boxed{}' and devolves into repetitive 'Solution' statements."
  - [corpus]: Related work *Open-Reasoner-Zero* supports the viability of scaling RL on base models, but this paper isolates the *data* as the causal lever for stability.

### Mechanism 2
- **Claim:** Long-CoT data induces reasoning depth but creates RL instability (verbosity/collapse), which must be counteracted by "behavioral anchors" like instruction-following data and specific prompt formatting.
- **Mechanism:** Pure Long-CoT training pushes the model toward high-entropy, lengthy outputs. Without the "anchoring" effect of instruction data (which teaches constraint adherence) or structured prompts (which define boundaries), the RL policy exploits the length reward loop, leading to runaway response lengths.
- **Core assumption:** Instability is a function of poorly constrained output distributions during the initial RL phase, not a fundamental failure of the learning algorithm.
- **Evidence anchors:**
  - [abstract]: "while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training."
  - [section 3.4]: "incorporating instruction-following data... helps stabilize response length... [and] this addition still fails to prevent the overall decline... without [complex templates/length scheduling]."
  - [corpus]: *SPIRAL* suggests self-play can incentivize reasoning without explicit human data, implying the instability here comes from the specific distribution shift of Long-CoT distillation.

### Mechanism 3
- **Claim:** A "Stable-then-Decay" learning rate schedule acts as a strategic plasticity switch, separating "knowledge accumulation" (Stable phase) from "behavioral specialization" (Decay phase).
- **Mechanism:** A constant high learning rate (Stable phase) allows the model to ingest massive volumes of reasoning data without forgetting. Dropping the learning rate (Decay phase) reduces update variance, allowing the model to precisely integrate specific QA/CoT distributions and "lock in" reasoning behaviors before RL begins.
- **Core assumption:** The optimal data mixture for base performance (knowledge) differs from the optimal mixture for RL readiness (behavioral format).
- **Evidence anchors:**
  - [section 4]: "decaying the learning rate in the second stage amplifies the effect of injected data... [and] reduces the overall training cost."
  - [section 4.2]: "We observe a consistent trend: increasing the QA data ratio leads to improved RL performance... [facilitated by the Decay stage]."
  - [corpus]: *ReMiT* explores bidirectional training, reinforcing the idea that distinct training phases (mid-training vs. post-training) require distinct optimization regimes.

## Foundational Learning

- **Concept:** **Mid-training (vs. Pre-training/Post-training)**
  - **Why needed here:** The paper defines this as a distinct intermediate stage. Understanding this distinction is critical because the authors argue you cannot just "post-train" or "pre-train" your way to RL compatibility; the *intervention* happens in this specific middle window.
  - **Quick check question:** Can you explain why the authors use a constant learning rate for 200B tokens here rather than the standard cosine decay used in pre-training?

- **Concept:** **R1-Zero Style RL (Base-to-RL)**
  - **Why needed here:** The entire experimental framework relies on applying GRPO directly to base models without SFT. You must understand what "Zero RL" means (no intermediate supervised fine-tuning) to grasp why the base model's "priors" are the bottleneck.
  - **Quick check question:** What specific failure mode does Llama-3.2-3B-Base exhibit when subjected to this specific training regime compared to Qwen?

- **Concept:** **Learning Rate Scheduling (WSD/Decay)**
  - **Why needed here:** The "Stable-then-Decay" strategy is the core architectural contribution. You need to understand how learning rate decay affects model "plasticity" to see why the second stage is necessary for integrating QA data.
  - **Quick check question:** Why does the author decay the learning rate specifically when introducing the QA/CoT data mixture?

## Architecture Onboarding

**Component Map:**
*   Data Engine: Curates MegaMath-Web-Pro-Max (Web), OpenR1-Math (Long-CoT), and TULU3 (Instructions)
*   Mid-Trainer: Nanotron framework running a 2-Stage Schedule (200B Constant LR -> 20B Decayed LR)
*   RL Engine: verl framework running GRPO
*   Stability Controller: A Progressive Length Scheduler (2k -> 4k -> 8k) and "Complex Template" prompt wrapper

**Critical Path:**
1. Stable Phase: Train Llama-3.2-Base on 200B tokens of high-quality math web data (Const LR)
2. Branching (Decay Phase): Fork training into 3 branches (Short, Long, Hybrid) using specific QA mixes + LR Decay
3. RL Phase: Apply GRPO with the Progressive Length Scheduler to the branched base models

**Design Tradeoffs:**
*   Short vs. Long CoT Branch: Short branches are stable but may lack depth; Long branches offer higher ceiling but require aggressive stability interventions (Section 3.4)
*   QA Data Ratio: <30% QA limits RL ceiling; >40% QA causes diminishing returns/redundancy (Section 4.2)
*   Prompt Complexity: "Simple" templates fail for Long-CoT models; "Complex" templates are mandatory for stability (Figure 9)

**Failure Signatures:**
*   The "Repetition Loop": Model outputs `\boxed:{}` followed by repetitive "Solution" text (indicates low-quality math data or insufficient instruction anchoring)
*   Length Explosion: Response length spikes to 4096 tokens instantly (indicates missing Progressive Length Scheduler or "Simple" template usage)

**First 3 Experiments:**
1. Data Quality Ablation: Swap MegaMath-Web-Pro for FineMath-4plus in a 20B-token run. Verify that the model enters the repetition loop upon RL initiation (replicating Figure 5)
2. Stability Fix Validation: Train a Long-CoT branch with the "Simple" template and fixed max length. Confirm training collapse/length spike. Then apply the "Complex Template" to verify stabilization
3. Branch Efficiency Test: Run RL on both OctoThinker-Short and OctoThinker-Long. Compare sample efficiency (steps to convergence) vs. final reasoning depth to select the optimal production branch

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: What are the individual contributions of QA data format versus reasoning content in mid-training?
- Basis in paper: [explicit] The authors explicitly state in the Future Work section a need for "disentangling QA format and content to better understand their individual contributions."
- Why unresolved: Current experiments mix format (question-answer pairs) with reasoning content (Chain-of-Thought), making it difficult to isolate which factor drives the improvements in RL scaling.
- What evidence would resolve it: Ablation studies using mid-training datasets that control for reasoning complexity while varying formats, or vice versa, to measure isolated effects on RL stability.

**Open Question 2**
- Question: Can RL-compatible base models be developed using open recipes without relying on distillation from proprietary models?
- Basis in paper: [explicit] The Future Work section lists "designing RL-friendly base models using open recipes without distillation from those powerful long CoT reasoning models" as a key goal.
- Why unresolved: The current study relies on datasets like OpenR1-Math-220K, which are derived from powerful models (e.g., R1), leaving the efficacy of purely synthetic or human-curated long-CoT data unproven.
- What evidence would resolve it: Demonstrating successful OctoThinker-style training using a long-CoT corpus generated entirely without distillation from advanced proprietary models.

**Open Question 3**
- Question: How can the instability and verbosity induced by long-CoT data be fully mitigated without relying on length schedulers?
- Basis in paper: [inferred] The authors note that while prompt templates and progressive length schedulers stabilize training, "performance across evaluation benchmarks still deteriorates during the later stages of RL training" (Page 10).
- Why unresolved: Current fixes (progressive schedulers) are external constraints rather than intrinsic solutions to the model's tendency towards repetition and length exploitation when exposed to long-CoT data.
- What evidence would resolve it: Identifying a data formatting or regularization technique that allows long-CoT mid-training without requiring manual, staged intervention on response length.

## Limitations
- Data Quality Dependence: The paper heavily emphasizes MegaMath-Web-Pro's superiority, but the evaluation of "low-quality" math corpora is based on limited examples with underspecified filtering criteria.
- RL Scaling Ceiling: Performance matching at 3B/8B scales doesn't establish whether this approach scales effectively to 70B+ parameter models.
- Stability Attribution: The paper attributes RL instability to Long-CoT data but doesn't fully isolate whether instability stems from data distribution shift or interaction with GRPO algorithm.

## Confidence
- **High Confidence** - The core finding that MegaMath-Web-Pro significantly outperforms FineMath-4plus for RL readiness is well-supported by quantitative results (Section 4.2, Table 1).
- **Medium Confidence** - The claim that instruction-following data stabilizes RL training has strong supporting evidence (Section 3.4), but the exact mechanism could be more precisely characterized.
- **Low Confidence** - The assertion that this approach "narrows the gap between model families" is primarily validated through performance matching rather than mechanistic understanding of why Qwen was inherently more RL-friendly.

## Next Checks
1. **Architecture Sensitivity Test** - Apply the OctoThinker mid-training pipeline to a different base architecture (e.g., Mistral or Gemma) to determine whether improvements are architecture-agnostic or specific to Llama's pretraining characteristics.
2. **Long-CoT Stability Analysis** - Conduct ablation studies removing individual stability interventions (Complex Template, Progressive Length Scheduler, instruction data) to quantify their relative contributions to preventing RL collapse.
3. **Scale Boundary Test** - Implement OctoThinker mid-training on a 30B+ parameter Llama model and evaluate whether performance gains and stability benefits observed at 3B/8B scales persist at frontier sizes.