---
ver: rpa2
title: 'RiM: Record, Improve and Maintain Physical Well-being using Federated Learning'
arxiv_id: '2505.06384'
source_url: https://arxiv.org/abs/2505.06384
tags:
- data
- learning
- client
- sleep
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RiM is a mobile app that uses federated learning to help students
  improve physical well-being by analyzing lifestyle habits such as steps, sleep,
  and meals. The app uses a pre-trained MLP model fine-tuned with user data via FedAvg
  and FedPer algorithms to ensure privacy by keeping raw data local.
---

# RiM: Record, Improve and Maintain Physical Well-being using Federated Learning

## Quick Facts
- arXiv ID: 2505.06384
- Source URL: https://arxiv.org/abs/2505.06384
- Authors: Aditya Mishra; Haroon Lone
- Reference count: 0
- Primary result: RiM uses federated learning to predict sleep and distance deficits with 60.71% accuracy via FedAvg

## Executive Summary
RiM is a mobile wellness app that uses federated learning to help students improve physical well-being by analyzing lifestyle habits including steps, sleep, and meals. The system employs a pre-trained multilayer perceptron (MLP) model fine-tuned with user data via FedAvg and FedPer algorithms, ensuring privacy by keeping raw data local on devices. The app provides personalized recommendations based on predicted deficits and rule-based risk scoring, achieving 60.71% accuracy and 0.91 MAE on the FedAvg variant.

## Method Summary
RiM employs a two-stage federated learning approach: first pre-training an MLP on simulated data using domain-appropriate statistical distributions, then fine-tuning on real user data collected from Android devices. The system uses the Flower framework for federated coordination, with clients training locally on 8+ days of lifestyle data (steps, sleep, meals, demographics) before sharing model weights with a central server. The pre-trained model predicts continuous deficit values for sleep and distance, which are combined with rule-based severity checks to generate personalized recommendations.

## Key Results
- FedAvg variant achieves 60.71% accuracy and 0.91 MAE on deficit prediction
- FedAvg outperforms FedPer (46.34% accuracy, 1.19 MAE) for non-ideal behavior detection
- System successfully generates personalized recommendations while preserving data privacy through federated learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated learning enables privacy-preserving model personalization without centralizing raw user data.
- Mechanism: Client devices train locally on their own data, share only model weights with a central server, which aggregates updates via weighted averaging (FedAvg) or partial-layer sharing (FedPer), then redistributes the improved global model—raw behavioral data never leaves the device.
- Core assumption: Model weight updates contain sufficiently less sensitive information than raw time-series lifestyle data, and aggregating many clients' updates obscures individual patterns.
- Evidence anchors:
  - [abstract] "The federated learning approach guarantees differential privacy by exclusively sharing model weights rather than raw data."
  - [section] "Each client then fine-tunes the pre-trained model using its own real-world data. The data remains on the client side, ensuring that users' data stays isolated."
  - [corpus] Related work (Arivazhagan et al.) describes FedPer personalization layers; corpus evidence supports FL privacy properties but does not independently validate this specific app's privacy guarantees.
- Break condition: If adversaries can reconstruct user data from gradient updates (gradient inversion attacks), or if client devices are compromised during local training, privacy claims weaken. The paper does not implement formal differential privacy mechanisms like gradient clipping or noise injection.

### Mechanism 2
- Claim: Pre-training on simulated data provides useful initialization for fine-tuning on limited real-world samples.
- Mechanism: A synthetic dataset is generated using domain-appropriate statistical distributions (negative binomial for steps, log-normal for distance, normal for sleep), allowing the MLP to learn general deficit-prediction patterns before encountering sparse real user data.
- Core assumption: Simulated distributions approximate real student lifestyle patterns sufficiently that learned representations transfer.
- Evidence anchors:
  - [abstract] "Our approach involves pre-training a multilayer perceptron (MLP) model on a large-scale simulated dataset to generate personalized recommendations."
  - [section] Table 2.1 maps features to distributions; Figure 2.1 visualizes simulated feature distributions.
  - [corpus] Weak direct corpus support; synthetic pre-training for federated wellness apps is not extensively validated in retrieved neighbors.
- Break condition: If real student data deviates significantly from simulation assumptions (e.g., non-stationary behavior during exams, cultural dietary differences), pre-trained representations may harm rather than help fine-tuning.

### Mechanism 3
- Claim: Hybrid ML + rule-based recommendation prioritizes actionable guidance by filtering low-confidence predictions through domain heuristics.
- Mechanism: The MLP outputs continuous deficit predictions (sleep deficit, distance deficit); a rule engine applies thresholds, weights violations, checks inter-parameter interactions (e.g., sleep + meal skipping), and returns only top-N highest-risk recommendations.
- Core assumption: Rule-based heuristics capture safety-critical or clinically relevant patterns that the ML model may miss or under-predict.
- Evidence anchors:
  - [abstract] "The system provides personalized recommendations based on predicted deficits and rule-based risk scoring."
  - [section] "We employ a two-stage approach that (i) predicts lifestyle deficits via a pretrained MLP regression model and (ii) applies rule-based severity and interaction checks."
  - [corpus] Corpus does not provide comparative evidence on hybrid vs. pure ML recommendation quality for wellness apps.
- Break condition: If rules are poorly calibrated or conflict with model predictions for edge cases, users may receive redundant or contradictory advice. The paper does not report user studies validating recommendation quality.

## Foundational Learning

- **Federated Averaging (FedAvg)**
  - Why needed here: The core aggregation algorithm enabling decentralized training; understanding it is essential to interpret why FedAvg outperforms FedPer for non-ideal behavior detection.
  - Quick check question: Can you explain why averaging client weights might dilute personalization for users with unusual (e.g., all-ideal) data patterns?

- **Multi-Layer Perceptron (MLP) for Regression**
  - Why needed here: The model predicts continuous deficit values (not classes); understanding regression outputs and MAE is necessary to interpret reported metrics.
  - Quick check question: Why might MAE be more informative than accuracy for deficit prediction tasks?

- **Personalization vs. Generalization Trade-off in FL**
  - Why needed here: The paper's key finding is that FedAvg and FedPer excel on opposite user types; grasping this trade-off is critical for architecture decisions.
  - Quick check question: Under what data distribution conditions would you choose FedPer over FedAvg?

## Architecture Onboarding

- **Component map:**
  Mobile app (React Native, Android ≤13) -> MLP model (5 hidden layers: 64→32→16→8→4) -> Federated server (Flower framework) -> Recommendation engine

- **Critical path:**
  1. User installs app → grants permissions → demographic input
  2. App collects 8+ days of local data (steps, sleep, meals)
  3. Client fine-tunes MLP locally (10 epochs), sends weight updates to server
  4. Server aggregates across 10 clients, broadcasts updated global model
  5. App receives updated weights, generates recommendations via hybrid engine

- **Design tradeoffs:**
  - FedAvg (60.71% acc, 0.91 MAE) vs. FedPer (46.34% acc, 1.19 MAE): FedAvg better for heterogeneous/non-ideal users; FedPer preserves zero-deficit patterns for ideal-behavior users.
  - Simulated pre-training vs. from-scratch: Enables cold-start but risks distribution mismatch.
  - Top-2 recommendations vs. exhaustive list: Reduces notification fatigue but may miss secondary issues.

- **Failure signatures:**
  - Low accuracy for clients with predominantly ideal data (Clients 2, 4, 7 in Table 4.1): Global model dilutes local zero-deficit bias.
  - High variance across clients (25–81% accuracy range): Non-IID data distribution not fully addressed by FedAvg.
  - Attrition (18 enrolled → 10 evaluable): Sensor malfunctions and engagement dropouts limit deployment viability.

- **First 3 experiments:**
  1. **Reproduce FedAvg vs. FedPer comparison** on the paper's 10-client dataset; verify reported accuracy/MAE gaps and per-client variance patterns.
  2. **Ablate pre-training** by training from scratch on real data only; quantify performance degradation to validate transfer learning contribution.
  3. **Stress-test recommendation engine** with synthetic edge cases (e.g., severe sleep deficit + normal distance, or all-ideal inputs); verify rule thresholds and prioritization logic behave as expected.

## Open Questions the Paper Calls Out

- Can advanced bi-level optimization algorithms like Ditto outperform FedAvg in balancing global aggregation with per-client personalization for the RiM framework?
- What are the computational and latency impacts of integrating the MLP model directly into the mobile application for offline inference?
- Can a hybrid aggregation strategy be developed to simultaneously capture "ideal" zero-deficit patterns and non-ideal lifestyle behaviors?

## Limitations

- Small evaluation cohort (10 clients) and Android 13 exclusivity limit real-world applicability
- Simulated pre-training may not capture complex student lifestyle patterns during academic stress
- Privacy claims lack formal differential privacy mechanisms like gradient clipping or noise injection

## Confidence

- **High Confidence**: Core federated learning implementation and FedAvg vs. FedPer accuracy comparison are well-documented and reproducible
- **Medium Confidence**: Simulated data generation and hybrid recommendation engine logic are described sufficiently for implementation
- **Low Confidence**: Privacy guarantees and real-world effectiveness of recommendations lack independent validation

## Next Checks

1. **Gradient Privacy Audit**: Implement gradient inversion attack tests on the trained model to verify that weight updates cannot reconstruct sensitive user data patterns
2. **Distribution Mismatch Analysis**: Collect real student lifestyle data across different academic periods and compare statistical properties against simulated distributions
3. **User Engagement Study**: Deploy the app to a larger, diverse student population across multiple institutions, measuring both prediction accuracy and actual behavior change through longitudinal tracking