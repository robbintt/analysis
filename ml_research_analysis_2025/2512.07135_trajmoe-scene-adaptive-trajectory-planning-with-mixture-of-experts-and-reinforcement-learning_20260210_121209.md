---
ver: rpa2
title: 'TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement
  Learning'
arxiv_id: '2512.07135'
source_url: https://arxiv.org/abs/2512.07135
tags:
- trajectory
- stage
- different
- experts
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitations of fixed trajectory priors
  in end-to-end autonomous driving systems, specifically their inability to adapt
  to varying driving scenarios and lack of policy-driven refinement in trajectory
  evaluation. The proposed TrajMoE model introduces a Mixture of Experts (MoE) transformer
  that routes trajectory tokens to different experts based on scene context, enabling
  scenario-specific trajectory processing.
---

# TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.07135
- Source URL: https://arxiv.org/abs/2512.07135
- Reference count: 12
- Authors: Zebin Xing; Pengxuan Yang; Linbo Wang; Yichen Zhang; Yiming Hu; Yupeng Zheng; Junli Wang; Yinfeng Gao; Guang Li; Kun Ma; Long Chen; Zhongpu Xia; Qichao Zhang; Hangjun Ye; Dongbin Zhao
- Primary result: 51.08 EPDMS on NavSim ICCV benchmark (3rd place)

## Executive Summary
TrajMoE addresses limitations in end-to-end autonomous driving systems by introducing a Mixture of Experts (MoE) transformer that routes trajectory tokens to scenario-specific experts based on scene context. The model also employs reinforcement learning via Group Relative Policy Optimization (GRPO) to fine-tune trajectory scoring mechanisms, improving alignment with ground-truth supervision. By integrating perception backbones like V2-99, VGGT, and DINOv3-L, and employing ensemble methods, TrajMoE achieves 51.08 EPDMS on the NavSim ICCV benchmark, securing third place. The approach demonstrates improved trajectory planning by combining MoE, RL fine-tuning, and multi-backbone integration.

## Method Summary
TrajMoE uses a two-stage training approach. Stage 1 applies supervised learning with a GTRS baseline and V2-99 backbone. Stage 2 applies GRPO fine-tuning on scoring heads. The MoE transformer replaces the standard FFN with N private experts plus one shared expert, using top-k routing with load balancing loss. GRPO samples N candidates from a Gaussian distribution, computes normalized rewards, and optimizes a clipped surrogate objective with KL regularization. The final model ensembles multiple backbone variants (V2-99, VGGT, DINOv3-L) via weighted averaging of trajectory predictions.

## Key Results
- Achieves 51.08 EPDMS on NavSim ICCV benchmark (3rd place)
- Single-model variants score between 41.7-47.0 EPDMS
- Ensemble method (GTRS+TrajMoE) achieves 51.08 EPDMS
- MoE-only variant scores 43.8 EPDMS on Stage 2
- GRPO variant scores 42.7 EPDMS on Stage 2

## Why This Works (Mechanism)

### Mechanism 1
Routing trajectory tokens through scene-conditioned experts improves trajectory-scoring accuracy by specializing processing to scenario types. A Sparse MoE layer with router distribution and top-k selection enables different trajectory tokens to be processed by different experts conditioned on scene features. Assumes distinct driving scenarios exhibit sufficiently different trajectory prior distributions. Evidence: abstract claims MoE applies "trajectory priors tailored to different scenarios," equations formalize routing and load balancing, GEMINUS and ARTEMIS show convergent exploration but limited empirical validation.

### Mechanism 2
Reinforcement learning fine-tuning via GRPO improves alignment between predicted driving scores and ground-truth supervision by enabling policy-driven exploration beyond one-stage supervised learning. Each scoring head outputs Gaussian-parameterized scores, samples N candidates, computes normalized rewards, and uses relative advantages to guide a clipped surrogate objective with KL regularization. Assumes treating score prediction as stochastic policy enables exploration that improves upon deterministic supervised predictions. Evidence: abstract mentions "policy-driven refinement," section II-C provides full GRPO formulation, CoMoCAVs applies multi-policy RL but direct GRPO-for-scoring evidence is sparse.

### Mechanism 3
Ensembling models with different perception backbones (V2-99, VGGT, DINOv3-L) via weighted trajectory averaging improves planning by combining complementary perceptual strengths. Features from different backbones are extracted and integrated, with final trajectories from multiple model variants ensembled through weighted averaging. Assumes different backbones capture distinct perceptual invariances with sufficiently uncorrelated errors. Evidence: abstract mentions "integrate models with different perception backbones," Table I shows GTRS+TrajMoE ensemble achieves 51.0 EPDMS vs. single-model ranges of 41.7-47.0, but no direct corpus evidence on backbone ensembling for trajectory planning.

## Foundational Learning

- **Concept: Mixture of Experts (Sparse Routing)**
  - Why needed: Understanding router assignment, load-balancing losses, and top-k selection is essential to diagnose routing failures and tune expert capacity.
  - Quick check: Can you explain why a shared expert (e₀) is included alongside private experts, and what failure mode the load-balancing loss prevents?

- **Concept: Policy Gradient Methods with KL Regularization**
  - Why needed: GRPO adapts PPO-style clipped objectives with KL penalties; understanding advantage estimation, reward normalization, and reference policies is required to modify or debug the RL fine-tuning stage.
  - Quick check: What does the KL divergence term in Eq. 9 constrain, and why is a reference policy π_ref necessary?

- **Concept: Transformer-Based Trajectory Scoring**
  - Why needed: The architecture fuses camera features F_cam with trajectory vocabulary embeddings F_traj via cross-attention; understanding token interactions is critical for debugging fusion quality.
  - Quick check: How do trajectory tokens from the vocabulary interact with scene features in the transformer, and what role does the scoring head play?

## Architecture Onboarding

- **Component map:** Sensor Input (3-camera concat) → Backbone (V2-99 / DINOv3-L / VGGT) → BEV Encoder → F_cam → MoE Transformer (Router + N experts + shared expert) → Scoring Heads (Imi, DAC, DDC, NC) → GRPO Fine-tuning → Ensemble (weighted avg across backbone variants)

- **Critical path:** 1. Camera input concatenation (H×W×3 tensor from front, front-left, front-right) 2. Backbone feature extraction → F_cam (scene representation) 3. MoE Transformer fusion of F_cam with trajectory vocabulary embeddings 4. Multi-head scoring (NC, DAC, DDC, TLC, etc.) supervised then GRPO-finetuned 5. Ensemble across backbone variants for final submission

- **Design tradeoffs:** More experts → higher capacity but increased routing complexity and load-balancing risk; Larger N (sample count in GRPO) → better exploration but higher compute; Backbone diversity (V2-99 vs. DINOv3-L) → feature complementarity vs. integration complexity; Ensemble size → robustness vs. inference latency

- **Failure signatures:** Router collapse: One expert receives >80% of tokens; check L_balance during training; GRPO instability: σ_θ collapses to near-zero (no exploration) or explodes; monitor reward variance; Ensemble mismatch: Individual models predict divergent trajectories; verify backbone preprocessing consistency

- **First 3 experiments:** 1. Baseline replication: Train GTRS-Aug without MoE or GRPO to establish Stage 1/Stage 2 metric baselines (target: EPDMS ~42-47 per Table I) 2. MoE ablation: Add Sparse MoE (4-8 experts, top-2 routing) with load-balancing loss; compare routing entropy and EPDMS delta vs. baseline 3. GRPO fine-tuning: After Stage 1 supervised training, apply GRPO with N=4-8 samples; sweep λ (KL weight) and monitor score prediction variance and final EPDMS

## Open Questions the Paper Calls Out

- **Open Question 1:** Is the performance improvement primarily driven by the proposed MoE and RL techniques, or by the aggregation of heterogeneous perception backbones? The paper lacks an ablation study applying the ensemble strategy to the baseline to isolate the delta contributed by TrajMoE's specific architectural changes.

- **Open Question 2:** Do the learned MoE experts develop interpretable specializations for distinct driving scenarios (e.g., highway vs. urban intersections)? The methodology and experiments do not visualize or quantify the router's decision boundaries, leaving uncertainty about whether experts learn semantic specialization or simply divide based on low-level feature statistics.

- **Open Question 3:** Does the GRPO reinforcement learning fine-tuning provide consistent improvements over the MoE-only approach? Quantitative results show the GRPO variant (42.7 EPDMS) scores lower than the MoE-only variant (43.8 EPDMS) on Stage 2, contradicting the claim that RL enhances accuracy.

## Limitations

- Critical hyperparameters for MoE (number of experts, routing top-k, load balancing weight) and GRPO (sample count N, KL weight, learning rate) are unspecified
- Ensemble integration details (preprocessing alignment, weighting scheme, feature fusion method) are missing
- Results are reported only on NavSim benchmark's private test-hard split with no generalization testing

## Confidence

- **High confidence:** Core architectural components (MoE routing, GRPO formulation, multi-backbone ensembling) are well-defined in equations and text; NavSimv2 benchmark and EPDMS metric are standard and verifiable
- **Medium confidence:** Mechanism claims (specialized expert routing, RL-driven score refinement, backbone complementarity) are logically sound but lack ablation studies isolating each contribution; 51.08 score is impressive but ensemble contributions are unclear
- **Low confidence:** Practical reproducibility is hindered by missing hyperparameters and training details (batch size, learning rate schedules, loss weightings); GRPO fine-tuning stage is particularly sensitive to these

## Next Checks

1. **MoE routing analysis:** Train the model with MoE (8 experts, top-2 routing) and monitor the router entropy and load-balancing loss (CV²(I), CV²(L)) over training epochs. Verify that no expert collapses (<10% token share) and that entropy increases initially then stabilizes, confirming effective specialization.

2. **GRPO stability test:** After supervised Stage 1 training, apply GRPO with N=8 candidates and monitor the KL divergence between current and reference policies, and the reward variance (std(r)). If KL > 0.1 or reward std ≈ 0, adjust β or learning rate. Confirm that score prediction variance increases (indicating exploration) and EPDMS improves.

3. **Backbone ensemble consistency:** Train individual models with V2-99, VGGT, and DINOv3-L backbones. Compute trajectory prediction correlation matrices between backbones. If correlation >0.8, ensemble gains may be minimal; if <0.5, ensemble averaging is justified. Test weighted vs. equal ensembling on validation set.