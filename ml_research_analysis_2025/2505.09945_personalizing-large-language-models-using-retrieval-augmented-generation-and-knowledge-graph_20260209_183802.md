---
ver: rpa2
title: Personalizing Large Language Models using Retrieval Augmented Generation and
  Knowledge Graph
arxiv_id: '2505.09945'
source_url: https://arxiv.org/abs/2505.09945
tags:
- data
- generation
- knowledge
- baseline
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hallucination and lack of
  personalization in large language models (LLMs) by integrating retrieval augmented
  generation (RAG) with knowledge graphs (KGs). The approach structures personal data
  like calendars and conversations into KGs, which are dynamically updated and queried
  during inference to guide LLMs toward factually correct, context-aware responses.
---

# Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph

## Quick Facts
- **arXiv ID:** 2505.09945
- **Source URL:** https://arxiv.org/abs/2505.09945
- **Reference count:** 36
- **Primary result:** KG-RAG approach improves ROUGE-1 (+35.15%), ROUGE-2 (+65.57%), ROUGE-L (+35.82%), and BLEU-1 (+61.11%) scores over baseline RAG while reducing response time by 8.93%

## Executive Summary
This paper addresses the challenge of hallucination and lack of personalization in large language models (LLMs) by integrating retrieval augmented generation (RAG) with knowledge graphs (KGs). The approach structures personal data like calendars and conversations into KGs, which are dynamically updated and queried during inference to guide LLMs toward factually correct, context-aware responses. Experiments on Llama-2-Chat models (7B, 13B, 70B) show significant improvements in ROUGE-1 (+35.15%), ROUGE-2 (+65.57%), ROUGE-L (+35.82%), and BLEU-1 (+61.11%) scores over a baseline RAG model, while also reducing average response time by 8.93%. The method enables efficient, privacy-preserving on-device personalization by minimizing reliance on cloud-based LLMs.

## Method Summary
The proposed approach constructs knowledge graphs from personal data (calendars and conversations) using SpaCy for triple extraction (subject-verb-object), then builds KGs with NetworkX. These KGs are embedded using a multilingual MiniLM model and stored in FAISS for efficient retrieval. During inference, LangChain's RetrievalQA retrieves relevant context from the KG based on user queries, which is then used to guide Llama-2-Chat models (7B, 13B, 70B with quantization) in generating personalized responses. The system is evaluated against a baseline RAG approach using synthetic data generated by GPT-4o, with performance measured by ROUGE and BLEU scores.

## Key Results
- KG-RAG improves ROUGE-1 scores by +35.15% over baseline RAG
- KG-RAG achieves +65.57% improvement in ROUGE-2 scores
- KG-RAG reduces average response time by 8.93%
- Best performance achieved with 70B Llama-2 model, though 7B and 13B models show reasonable performance with quantization

## Why This Works (Mechanism)
The KG-RAG approach works by structuring personal data into a knowledge graph format that provides a clear semantic roadmap for the LLM. Unlike raw text retrieval, KGs capture precise relationships between entities (e.g., "Meeting Subject: Project Review, Time: 3PM"), enabling more accurate context retrieval. The embedding of structured triples allows the retriever to find highly relevant information even when queries use different phrasing. By grounding the LLM's responses in explicitly represented facts from the user's personal data, the approach significantly reduces hallucination and improves factual accuracy while maintaining conversational coherence.

## Foundational Learning
- **Knowledge Graph Construction:** Converting unstructured personal data into structured triple format (subject-relation-object) provides a semantic framework that LLMs can navigate more reliably than raw text. Needed for precise relationship representation; quick check: verify triples capture all relevant calendar/relationship data.
- **Retrieval Augmented Generation (RAG):** Combines information retrieval with text generation to ground LLM responses in external knowledge sources. Needed to reduce hallucination by providing factual context; quick check: measure retrieval precision/recall on personal queries.
- **Embedding Models for KGs:** Vectorizing structured triples enables semantic search and similarity matching in high-dimensional space. Needed to match user queries with relevant KG facts; quick check: test embedding quality on semantically similar triples.

## Architecture Onboarding

- **Component map:** Raw calendar/conversation text (JSON/text) -> SpaCy for triple extraction -> NetworkX for KG construction -> HuggingFace Embedding Model (`paraphrase-multilingual-MiniLM-L12-v2`) -> FAISS Vector Store -> LangChain RetrievalQA -> Llama-2-Chat (7B/13B/70B) -> Generated Response

- **Critical path:**
  1. Triple Extraction: SpaCy must correctly identify subject-verb-object from personal text
  2. Embedding & Indexing: The KG must be accurately embedded and indexed in FAISS
  3. Context Injection: The retrieved context must be correctly formatted into the LLM prompt

- **Design tradeoffs:**
  - Structured KG vs. Raw Text: Using KG provides better precision and latency (+35-65% ROUGE, -8.9% time) but loses nuance/tone of raw text
  - Model Size: 7B/13B models are faster and more suitable for on-device but may have less reasoning capability than 70B
  - Automated vs. Human Evaluation: Using ChatGPT-4 to generate dataset and golden answers scales evaluation but may introduce model-specific bias

- **Failure signatures:**
  - Hallucination persists: If KG is incomplete or query is vague, retriever finds no/wrong context
  - Poor Retrieval: Embeddings of short KG triples may not match semantics of natural language queries
  - On-device Latency/Resource Exhaustion: 7B/13B model with quantization may still be too heavy for some devices

- **First 3 experiments:**
  1. Reproduce Baseline vs. KG-RAG: Run source code on public dataset, compare ROUGE/BLEU scores for baseline vs. KG-RAG on 7B model
  2. Ablation on Triple Extraction: Manually inspect/modify SpaCy extraction step, see if adding/removing relations changes answers to targeted queries
  3. Latency Test: Measure end-to-end latency on different hardware (local GPU vs. CPU-only) to gauge on-device feasibility

## Open Questions the Paper Calls Out
- How does the KG-RAG approach generalize to other categories of frequently updated personal data beyond calendar events, such as contacts, locations, and conversational history? (Only calendar data was experimentally validated)
- How well does the synthetically generated dataset (ChatGPT-4o-created persona "Alex") transfer to real-world user data with natural variability and noise? (No real user data was tested despite targeting personalization)
- How can smaller models (7B, 13B) be further improved to match or exceed larger models' performance on unseen personal data queries? (Generalization to unseen queries is not addressed)

## Limitations
- The evaluation relies entirely on synthetically generated data from GPT-4o, which may not reflect real-world personal data complexity and noise
- The triple extraction process lacks detailed specifications for handling complex calendar entries and fragmented conversation text
- Computational requirements for even the 7B model with quantization may exceed practical limits for many personal devices

## Confidence
- **High Confidence:** The architectural design combining KG with RAG is sound and aligns with established RAG principles. Reported performance improvements in ROUGE and BLEU metrics are likely reproducible.
- **Medium Confidence:** The claimed latency reduction of 8.93% and specific performance gains depend heavily on KG construction quality, which the paper doesn't fully specify. On-device claims require practical validation.
- **Low Confidence:** Generalization to real-world personal data is uncertain given synthetic dataset. The claim of solving hallucination is overstated, as KG-RAG only mitigates hallucinations when KG contains relevant, complete information.

## Next Checks
1. **Dataset Realism Test:** Validate the approach using manually curated dataset of real personal calendar entries and conversations rather than GPT-4o-generated synthetic data.
2. **Edge Case Robustness:** Systematically test triple extraction pipeline on fragmented calendar entries and informal conversation fragments to identify failure patterns.
3. **On-Device Feasibility Benchmark:** Measure end-to-end latency and memory consumption of complete KG-RAG pipeline on representative personal devices across different Llama-2 model sizes.