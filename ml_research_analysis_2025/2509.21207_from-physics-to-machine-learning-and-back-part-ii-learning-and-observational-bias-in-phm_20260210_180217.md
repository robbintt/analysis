---
ver: rpa2
title: 'From Physics to Machine Learning and Back: Part II - Learning and Observational
  Bias in PHM'
arxiv_id: '2509.21207'
source_url: https://arxiv.org/abs/2509.21207
tags:
- data
- learning
- system
- fault
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review examines how embedding physical knowledge into machine
  learning models through learning and observational biases improves robustness, generalization,
  and trustworthiness in PHM applications. Learning biases enforce physical consistency
  via physics-informed loss functions, degradation dynamics constraints, and physical
  priors during training, while observational biases shape data selection and synthesis
  through virtual sensing, simulation-to-real strategies, physics-informed augmentation,
  and multimodal fusion.
---

# From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM

## Quick Facts
- **arXiv ID:** 2509.21207
- **Source URL:** https://arxiv.org/abs/2509.21207
- **Reference count:** 40
- **Key outcome:** Embedding physical knowledge into ML models through learning and observational biases improves robustness, generalization, and trustworthiness in PHM applications.

## Executive Summary
This review examines how embedding physical knowledge into machine learning models through learning and observational biases improves robustness, generalization, and trustworthiness in PHM applications. Learning biases enforce physical consistency via physics-informed loss functions, degradation dynamics constraints, and physical priors during training, while observational biases shape data selection and synthesis through virtual sensing, simulation-to-real strategies, physics-informed augmentation, and multimodal fusion. These approaches enable models to handle limited, noisy, or incomplete data while respecting physical laws. The review also explores how these physics-informed predictions support active decision-making through reinforcement learning and how scaling strategies like meta-learning, few-shot learning, and domain generalization extend PHM solutions to fleet-wide deployment.

## Method Summary
The review synthesizes methods for integrating physical knowledge into machine learning models for PHM through two main approaches: learning biases that embed physical constraints into training via physics-informed loss functions, degradation dynamics constraints, and physical priors; and observational biases that shape data selection through virtual sensing, simulation-to-real strategies, physics-informed augmentation, and multimodal fusion. The core methodology involves combining data loss with physics-based regularization terms, using simulations and virtual sensors to augment limited real-world data, and extending to active decision-making through reinforcement learning. Implementation requires defining governing physical equations, implementing physics-informed loss functions, and validating that models prioritize physical consistency over fitting noise.

## Key Results
- Physics-informed loss functions enable models to handle limited, noisy, or incomplete data while respecting physical laws
- Simulation-based data augmentation and virtual sensing improve generalization to rare failure modes and unmeasured states
- Reinforcement learning with physics-informed models enables active decision-making for maintenance scheduling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Embedding physical laws directly into the training loss (Learning Bias) restricts the solution space, potentially reducing the data required for convergence and improving robustness in noisy environments.
- **Mechanism:** By adding physics-based penalty terms (e.g., PDE residuals $L_{PDE}$ or degradation constraints like monotonicity $L_{mono}$) to the standard data loss ($L_{data}$), the optimizer is penalized for producing outputs that violate known physical laws.
- **Core assumption:** The governing equations or general properties (e.g., monotonic degradation) accurately capture the dominant system dynamics.
- **Evidence anchors:**
  - [abstract] "Learning biases embed physical constraints into model training through physics-informed loss functions... or by incorporating properties like monotonicity."
  - [section 2.1] Describes the PINN formulation $L_{PINN} = w_{data}L_{data} + w_{PDE}L_{PDE}$ where the physics loss enforces governing constraints.
  - [corpus] "Physics-informed machine learning: A mathematical framework..." supports the integration of PDE systems into regression functions.
- **Break condition:** If the physical models are misspecified or the noise characteristics of the data fundamentally contradict the imposed physics (e.g., non-monotonic recovery due to maintenance), the model may fail to converge or suffer from "over-regularization."

### Mechanism 2
- **Claim:** Curating or synthesizing data to reflect physical realities (Observational Bias) exposes the model to a broader range of operational conditions, improving generalization to unseen domains.
- **Mechanism:** Instead of relying solely on sparse real-world data, the system uses physics-based simulations and virtual sensing to generate synthetic training samples.
- **Core assumption:** The physics-based simulators or virtual sensors are sufficiently high-fidelity to minimize the "simulation-to-real" domain gap.
- **Evidence anchors:**
  - [abstract] "Observational biases shape data selection and synthesis through virtual sensing... and physics-informed augmentation."
  - [section 3.2] Discusses using "Data Augmentation" and "Sim-to-Real" strategies to cover rare failure scenarios.
  - [corpus] "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery" supports using simulation for robust pretraining.
- **Break condition:** If the simulator fidelity is low or the domain gap is large, the model may learn simulation artifacts rather than real physics (negative transfer).

### Mechanism 3
- **Claim:** Reinforcement Learning (RL) closes the loop by using physics-informed predictions to learn maintenance policies that optimize long-term objectives while respecting safety constraints.
- **Mechanism:** The RL agent interacts with a physics-informed model (or digital twin) of the system. The agent learns a policy $\pi(a|s)$ that maps system states (derived from physics-informed predictions) to actions (maintenance decisions).
- **Core assumption:** The physics-informed model used as the environment for the RL agent accurately reflects the transition dynamics and degradation trajectories of the physical system.
- **Evidence anchors:**
  - [abstract] "These approaches enable the transition from passive prediction to active decision-making through reinforcement learning."
  - [section 4] Describes "Physics-Informed Reinforcement Learning (PIRL)" where safety filters refine RL actions to prevent unsafe states.
  - [corpus] "PIN-WM: Learning Physics-INformed World Models" discusses learning world models for control, reinforcing the RL mechanism.
- **Break condition:** If the reward function is poorly specified or the physics-informed environment drifts from reality, the agent may learn policies that are optimal in simulation but hazardous in deployment.

## Foundational Learning

- **Concept:** **Physics-Informed Neural Networks (PINNs)**
  - **Why needed here:** This is the core implementation of "learning bias," allowing engineers to blend data with differential equations.
  - **Quick check question:** Can you formulate a loss function that balances fitting noisy sensor data with satisfying a simple conservation law (e.g., $\nabla \cdot u = 0$)?

- **Concept:** **Domain Adaptation & Generalization**
  - **Why needed here:** Crucial for bridging the "Sim-to-Real" gap mentioned in observational biases, ensuring models trained on simulated data work on physical assets.
  - **Quick check question:** How does "domain generalization" differ from "domain adaptation" regarding the availability of target domain data during training?

- **Concept:** **Reinforcement Learning (RL) Basic Formulation**
  - **Why needed here:** Needed to understand how the system transitions from passive prediction (forecasting RUL) to active control (scheduling maintenance).
  - **Quick check question:** What are the three main components of a Markov Decision Process (MDP) used to define the maintenance decision problem?

## Architecture Onboarding

- **Component map:** Heterogeneous inputs (Real sensors, Physics Simulators, Generative Models) -> Virtual Sensing & Physics Constraints (Loss functions) -> Neural Network (e.g., PINN, GNN) trained with combined losses -> RL Agent consuming predictions to output maintenance actions

- **Critical path:**
  1. Define the governing physics or degradation constraints (e.g., PDEs or monotonicity)
  2. Implement the physics-informed loss function
  3. Validate that the model prioritizes physical consistency over fitting noise

- **Design tradeoffs:**
  - **Physics vs. Data Weighting:** Heavily weighting the physics loss ($L_{PDE}$) ensures consistency but may ignore valuable deviations in the data; under-weighting leads to physically impossible predictions
  - **Simulation Fidelity vs. Speed:** High-fidelity simulations reduce the domain gap but are computationally expensive for data generation

- **Failure signatures:**
  - **Trivial Solutions:** PINNs converging to zero or constant outputs if loss balancing is poor
  - **Sim-to-Real Gap:** Model performs perfectly on synthetic test sets but fails on real sensor data due to unmodeled noise
  - **Constraint Violation:** RL agent proposing maintenance actions that violate physical safety limits if safety filters are not robust

- **First 3 experiments:**
  1. **Loss Sensitivity Analysis:** Train a PINN on a degradation dataset (e.g., battery capacity) and sweep the weighting factor $\lambda$ between data loss and physics loss to find the optimal balance for generalization
  2. **Virtual Sensor Validation:** Use a GNN-based soft sensor to estimate a hidden state (e.g., bearing load) using known physical relationships (e.g., Newton's laws) and measure the reconstruction error against held-out test data
  3. **Sim-to-Real Transfer:** Train a fault diagnosis classifier purely on simulated data (with injected noise) and evaluate its zero-shot performance on a real-world dataset to quantify the domain gap

## Open Questions the Paper Calls Out

- **Open Question 1:** How can degradation-informed methods effectively handle non-monotonic trajectories caused by maintenance events or shocks in realistic industrial environments?
  - **Basis in paper:** [explicit] Section 2.2 notes that "Emerging domains... operate under non-stationary conditions... leading to non-monotonic degradation patterns," suggesting a need for "regime-aware segmentation" or "partial-monotonic constraints."
  - **Why unresolved:** Current prognostic methods predominantly assume monotonic degradation, which fails when systems undergo repairs or experience sudden operational shocks.
  - **What evidence would resolve it:** Development of frameworks that selectively enforce monotonicity on specific operational segments, validated on industrial datasets containing maintenance logs.

- **Open Question 2:** How can physical constraints in learning biases be adaptively adjusted based on model confidence or data coverage to prevent over-regularization?
  - **Basis in paper:** [explicit] The conclusion identifies the need for "adaptive training strategies that adjust the influence of physical constraints based on model confidence or data coverage."
  - **Why unresolved:** Fixed weighting of physics-based loss terms (e.g., in PINNs) often leads to hyperparameter sensitivity, instability, or the suppression of valid data-driven patterns in sparse data regimes.
  - **What evidence would resolve it:** Algorithms that dynamically tune the contribution of physics losses during training, demonstrating improved convergence over fixed-weight methods.

- **Open Question 3:** How can domain generalization techniques be adapted for multimodal PHM data to detect unknown faults under open-set assumptions?
  - **Basis in paper:** [explicit] Section 5.2 highlights that "most existing DG research in PHM primarily focuses on unimodal data" and "closed-set assumption," whereas real-world target domains often contain unknown faults.
  - **Why unresolved:** There is a lack of public multimodal benchmarks for PHM, and current methods assume consistent label spaces across domains.
  - **What evidence would resolve it:** The creation of standardized multimodal benchmarks and the integration of out-of-distribution detection techniques into domain generalization frameworks.

## Limitations

- The review's core mechanisms rely heavily on the quality and fidelity of physical models and simulations, with no universal solution provided for bridging the "simulation-to-real" domain gap
- Optimal weighting strategies for physics vs. data loss terms remain an open challenge, leading to potential hyperparameter sensitivity and instability
- The computational cost of high-fidelity simulation-based training and the lack of standardized multimodal benchmarks limit practical deployment

## Confidence

- **High Confidence:** The fundamental mechanism of combining data loss with physics-informed regularization (Mechanism 1) is well-established in PINN literature and directly supported by the review's technical formulations
- **Medium Confidence:** Observational bias mechanisms (Mechanism 2) show promise but depend heavily on simulator fidelity, which varies significantly across domains and is not standardized in the review
- **Medium Confidence:** Reinforcement learning applications (Mechanism 3) are conceptually sound but face practical challenges with reward specification and physics model accuracy that could limit real-world deployment

## Next Checks

1. **Loss Weight Sensitivity Analysis:** Systematically sweep the weighting parameter Î» between data and physics losses across multiple PHM datasets to identify robust convergence patterns and optimal trade-offs
2. **Domain Gap Quantification:** Design controlled experiments comparing model performance on synthetic vs. real data for the same physical system, measuring degradation in predictive accuracy as a function of simulator fidelity
3. **Constraint Violation Testing:** Implement stress tests where RL agents interact with physics-informed models under edge cases to measure frequency and severity of safety constraint violations