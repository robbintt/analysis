---
ver: rpa2
title: 'Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors'
arxiv_id: '2507.19725'
source_url: https://arxiv.org/abs/2507.19725
tags:
- policy
- reward
- agent
- behavior
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper empirically evaluates how intrinsic motivation (IM)\
  \ techniques alter the behavior of reinforcement learning agents, focusing on three\
  \ traditional IM methods\u2014State Count, Max Entropy, and Intrinsic Curiosity\
  \ Module (ICM)\u2014and their combination with Generalized Reward Matching (GRM).\
  \ The study uses MiniGrid environments where agents can learn without IM, enabling\
  \ baseline behavior comparison."
---

# Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors

## Quick Facts
- **arXiv ID**: 2507.19725
- **Source URL**: https://arxiv.org/abs/2507.19725
- **Reference count**: 7
- **Primary result**: Intrinsic motivation improves early convergence but can cause reward hacking; Generalized Reward Matching mitigates but doesn't eliminate policy deviation.

## Executive Summary
This paper empirically evaluates how intrinsic motivation (IM) techniques alter the behavior of reinforcement learning agents in MiniGrid environments. The study tests three IM methods—State Count, Max Entropy, and Intrinsic Curiosity Module (ICM)—with and without Generalized Reward Matching (GRM). Results show IM methods accelerate early convergence to optimal policies but also introduce "reward hacking," where agents optimize for exploration at the expense of task completion. While GRM mitigates reward hacking in some cases, it doesn't fully prevent policy deviation from baselines in shorter training runs.

## Method Summary
The study uses MiniGrid environments with PPO agents featuring a shared CNN backbone (3 layers: 16/32/64 filters, 2×2 kernels) and separate critics for extrinsic and intrinsic values. Three IM methods are tested: State Count (1/√N_s), Max Entropy (policy entropy), and ICM (curiosity-based with forward/inverse models). GRM is applied to theoretically preserve policy invariance. Agents are trained for 20.48M frames across 5 MiniGrid maps with 10 random seeds per configuration, evaluating episodic return, position coverage, and policy divergence from a no-IM baseline.

## Key Results
- State Count provides earliest convergence but risks over-exploration and reward hacking
- Max Entropy stabilizes behavior but may cause stagnation in complex environments
- ICM offers balanced performance between exploration and task completion
- GRM mitigates reward hacking but still produces policies that deviate from baseline in shorter training runs
- IM methods generally yield better policies than no-IM baseline, despite behavioral variance

## Why This Works (Mechanism)

### Mechanism 1: Count-Based Exploration Bonus
Providing an auxiliary reward inversely proportional to state visitation frequency accelerates discovery of sparse extrinsic rewards by transforming sparse-reward MDPs into dense-reward problems. The agent receives intrinsic reward $r_{int} \propto 1/\sqrt{N(s)}$, encouraging traversal of unvisited states and increasing probability of finding goal states early. This fails in high-dimensional spaces where states are rarely revisited exactly.

### Mechanism 2: Intrinsic-Extrinsic Objective Conflict (Reward Hacking)
Optimizing the sum of intrinsic and extrinsic rewards often leads to policies maximizing exploration at the expense of task completion. The agent optimizes $R_{total} = R_{ext} + \beta R_{int}$, where dense intrinsic rewards dominate sparse extrinsic gradients. This causes behaviors that maximize intrinsic novelty rather than completing tasks, leading to sub-optimal policies like repeatedly interacting with rare elements.

### Mechanism 3: GRM Policy Invariance Mitigation
Generalized Reward Matching reshapes the reward function to theoretically guarantee the optimal policy remains invariant to added intrinsic motivation. It dampens intrinsic rewards in states where they would lead to policy deviation, forcing the agent to value intrinsic signals only insofar as they aid exploration toward extrinsic goals. This guarantee breaks down in finite training runs due to empirical variance.

## Foundational Learning

- **Reward Shaping & Potential-Based Rewards**
  - Why needed: The paper critiques standard intrinsic motivation as dangerous reward shaping; understanding potential-based reward shaping (Ng et al.) is required to contrast why methods like GRM preserve optimality.
  - Quick check: How does potential-based reward shaping guarantee that the optimal policy of the shaped MDP matches the original MDP?

- **Policy Invariance vs. Policy Variance**
  - Why needed: The central tension is whether IM methods alter agent behavior. Distinguish between convergence speed (performance) and actual strategy learned (behavior).
  - Quick check: If two agents achieve the same cumulative reward but take different paths, are their policies invariant with respect to the goal?

- **Exploration vs. Exploitation in Sparse Environments**
  - Why needed: The paper analyzes exploration strategies (Count, Entropy, Curiosity). Understanding reward-sparsity difficulty is crucial for valuing why IM is used despite risks.
  - Quick check: Why is random action selection insufficient for solving a sparse-reward maze like MiniGrid's DoorKey-16x16?

## Architecture Onboarding

- **Component map**: Observation → CNN (16→32→64 filters) → Actor/Critics → IM Module (State Count/Max Entropy/ICM) → GRM Wrapper → Shaped Reward

- **Critical path**: 1) Observation → CNN → Actor/Critics; 2) (s, s') → IM Module → GRM Wrapper → Shaped Reward; 3) Loss Calculation: $L = L_{policy} + c_1 L_{value} + c_2 L_{entropy} + L_{ICM}$

- **Design tradeoffs**:
  - State Count vs. ICM: State Count is computationally cheap but brittle to high-dimensional states; ICM is robust but adds ~30% compute overhead
  - GRM Complexity: Adds stability but introduces hyperparameter sensitivity; sometimes slows convergence compared to raw IM
  - Separate Critics: Stabilizes learning vs. single critic but doubles value-function approximation cost

- **Failure signatures**:
  - "Dancing with Skulls": Agent repeatedly interacts with hazards without progressing (check high visitation counts on non-goal tiles)
  - Entropy Stagnation: Agent spins in circles to maximize stochasticity without exploring (check if Position Coverage plateaus low)
  - Catastrophic Forgetting: Sudden drop in reward late in training suggests intrinsic reward destabilizing policy buffer

- **First 3 experiments**:
  1. Baseline Replication: Train PPO (No-IM) on DoorKey-8x8 to establish sparsity problem
  2. State Count Validation: Train PPO + State Count on same map; verify early convergence (~0.95 reward) and over-exploration
  3. GRM Comparison: Train PPO + State Count + GRM; compare heatmap divergence against baseline or optimal path

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid intrinsic motivation strategy (e.g., State Count for early training, Max Entropy for refinement) overcome individual limitations of over-exploration and stagnation? The authors suggest potential for combining methods with dynamic coefficient adjustment based on training progress, but current study evaluates methods in isolation.

### Open Question 2
What alternative behavioral metrics can capture sequential order of operations and temporal dynamics better than static heatmaps? The study relies on position coverage heatmaps that flatten temporal dimension, making it difficult to distinguish different sequential behaviors sharing same state visitation frequency.

### Open Question 3
Do optimality-preserving methods like GRM maintain policy invariance under computational constraints despite theoretical guarantees of asymptotic optimality? The paper acknowledges GRM has theoretical guarantees but empirically demonstrates policies deviate from baseline in practical, limited time horizons.

### Open Question 4
Do observed trade-offs between convergence speed and policy divergence in MiniGrid generalize to complex visual domains like Atari or MicroRTS? The authors note MiniGrid's limited complexity and plan to test on actual game environments in future studies.

## Limitations
- Exact GRM implementation details are only referenced externally (Forbes et al. 2024b) rather than fully specified
- Behavioral analysis relies on comparing IM-trained policies to baseline via 5000-step simulations without addressing potential sampling bias
- State representation for Count method (hashing scheme, observation vs. full state) is not clearly defined

## Confidence
- **High**: Claims about State Count providing fastest early convergence and general pattern of IM improving performance over No-IM baseline
- **Medium**: Claims about reward hacking behavior and GRM's partial mitigation, well-supported but dependent on baseline comparison fidelity
- **Low**: Claims about theoretical guarantees of GRM, as paper acknowledges these don't fully manifest in shorter training runs

## Next Checks
1. Implement D-GRM from Forbes et al. 2024b reference and verify it produces claimed policy invariance in MiniGrid
2. Test whether "reward hacking" behavior persists when extrinsic rewards are made denser to reduce dominance of intrinsic signals
3. Compare IM policies not just to baseline but to known-optimal policy (if available) to better quantify behavioral deviation