---
ver: rpa2
title: 'dreaMLearning: Data Compression Assisted Machine Learning'
arxiv_id: '2506.22190'
source_url: https://arxiv.org/abs/2506.22190
tags:
- data
- training
- dreamlearning
- dataset
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces dreaMLearning, a framework that enables direct
  training on compressed data without decompression. It is built upon EntroGeDe, an
  entropy-driven lossless compression method that clusters similar data points into
  condensed, representative samples.
---

# dreaMLearning: Data Compression Assisted Machine Learning

## Quick Facts
- arXiv ID: 2506.22190
- Source URL: https://arxiv.org/abs/2506.22190
- Authors: Xiaobo Zhao; Aaron Hurst; Panagiotis Karras; Daniel E. Lucani
- Reference count: 35
- Primary result: Enables direct training on compressed data without decompression, accelerating training by up to 8.8× while reducing memory usage by 10×

## Executive Summary
This paper introduces dreaMLearning, a framework enabling direct training on compressed data without decompression. It builds upon EntroGeDe, an entropy-driven lossless compression method that clusters similar data points into condensed, representative samples. The approach demonstrates significant performance improvements across regression and classification tasks with tabular and image data, offering substantial benefits for resource-constrained environments.

## Method Summary
The framework uses entropy-guided clustering to create condensed samples from raw data, then trains models directly on these compressed representations. For regression tasks, it employs weighted gradient descent on centroids, while classification tasks use random access sampling from compressed archives. The system leverages Discrete Cosine Transform for image data to enhance compressibility while maintaining training efficiency.

## Key Results
- Accelerates training by up to 8.8× compared to standard training
- Reduces memory usage by 10× during training
- Cuts storage requirements by 42% with minimal impact on model performance
- Maintains competitive accuracy on MNIST, CIFAR-10/100 while using compressed data

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Guided Bit Allocation (EntroGeDe)
The algorithm calculates entropy for each bit position, using high-entropy bits for clustering data into condensed samples while assigning low-entropy bits to bases for deduplication. This works because informational value correlates with bit entropy across data points.

### Mechanism 2: Weighted Gradient Descent on Condensed Samples
Instead of computing gradients over all N raw samples, the framework uses M condensed samples with cluster weights to preserve statistical influence. This approximates the full dataset's loss landscape sufficiently for convergence.

### Mechanism 3: Frequency-Domain Transform for Random Access
Images are converted to YCbCr and processed via DCT, concentrating energy and revealing redundancy patterns that EntroGeDe can exploit. This allows random access without full decompression, unlike JPEG.

## Foundational Learning

- **Concept: Generalized Deduplication (GeDe)**
  - Why needed: This is the storage primitive that enables random access and approximate analytics
  - Quick check: Can you explain why splitting data into a base and deviation allows for faster random access than decompressing an LZ77 stream?

- **Concept: Bit-Level Entropy**
  - Why needed: This is the heuristic driver that separates "noisy" bits from "redundant" bits
  - Quick check: If a bit position is always '0' across a dataset, what is its entropy, and how would EntroGeDe utilize it?

- **Concept: Coreset Selection vs. Dataset Distillation**
  - Why needed: Contextualizes the contribution as coreset selection via compression rather than gradient-based search
  - Quick check: How does the computational cost of EntroGeDe compression compare to the iterative gradient matching used in methods like GRAD-MATCH?

## Architecture Onboarding

- **Component map:** Preprocessor -> Binary Conversion & Entropy Calculation -> EntroGeDe Engine (Clustering + Compression) -> Training Interface (Weighted GD for Regression / Random Access for Classification)
- **Critical path:** Selection of the Analytics Bit Count (β), which determines how many bits define the condensed sample
- **Design tradeoffs:**
  - Storage vs. Accuracy: More base bits improve compression but may reduce unique distinctions
  - Clustering vs. Sampling: Clustering for tabular data, random sampling for images due to computational expense
- **Failure signatures:**
  - Accuracy Collapse: Check weight vector normalization in regression
  - Storage Bloat: Verify low-entropy bits are selected for bases, not high-entropy bits
  - Slow Convergence: Check plateau threshold setting in compression phase
- **First 3 experiments:**
  1. Sanity Check: Linear Regression on California Housing compressed set vs. full dataset
  2. Hyperparameter Sweep: Vary β on subset, plot Storage Ratio vs. Test Accuracy
  3. Image Pipeline: Train LeNet5 on MNIST using DCT+EntroGeDe, compare training speed vs. standard

## Open Questions the Paper Calls Out

- **Time series and text data:** Effectiveness may be compromised by temporal dependencies or sparse structures
- **Adaptive compression strategies:** Future work will integrate dynamic optimization during training
- **Complex image datasets without DCT:** Direct EntroGeDe yields poor results on CIFAR, requiring frequency-domain transforms

## Limitations

- Effectiveness depends on bit-level entropy correlating with semantic information value, which may not hold for all data types
- Performance on extremely large-scale or high-dimensional data (medical imaging, genomics) is not validated
- No comparison against modern neural compression codecs like VQ-VAE

## Confidence

- **High Confidence:** Training acceleration (8.8×) and memory reduction (10×) claims are directly supported by controlled experiments
- **Medium Confidence:** Minimal accuracy loss across diverse tasks is plausible but may be dataset-dependent
- **Low Confidence:** Broad applicability claim lacks exhaustive testing on NLP, graph data, and multimodal tasks

## Next Checks

1. **Scalability Test:** Evaluate on high-dimensional dataset (ChestXRay14) to assess computational tractability
2. **Codec Comparison:** Benchmark against VQ-VAE on tabular dataset for rate-accuracy trade-offs
3. **Cross-Modality Validation:** Apply to text classification using TF-IDF features to test entropy-guided clustering generality