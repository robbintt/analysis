---
ver: rpa2
title: 'Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods'
arxiv_id: '2505.17870'
source_url: https://arxiv.org/abs/2505.17870
tags:
- 'false'
- misinformation
- immunization
- training
- falsehoods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model immunization is a supervised fine-tuning approach that treats
  curated (false claim, correction) pairs as training signal, where explicit negative
  labels teach models to recognize and reject misinformation-associated linguistic
  patterns rather than memorizing individual false facts. Across four open-weight
  model families, immunization improves TruthfulQA accuracy by 12.4 percentage points
  and misinformation rejection by 30.2 percentage points with negligible impact on
  general capabilities.
---

# Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods

## Quick Facts
- arXiv ID: 2505.17870
- Source URL: https://arxiv.org/abs/2505.17870
- Reference count: 34
- Supervised fine-tuning using (false claim, correction) pairs improves TruthfulQA accuracy by 12.4 percentage points

## Executive Summary
Model immunization is a supervised fine-tuning approach that treats curated (false claim, correction) pairs as training signal, where explicit negative labels teach models to recognize and reject misinformation-associated linguistic patterns rather than memorizing individual false facts. Across four open-weight model families, immunization improves TruthfulQA accuracy by 12.4 percentage points and misinformation rejection by 30.2 percentage points with negligible impact on general capabilities. The approach achieves best performance at 5–10% vaccine dosage in fine-tuning tokens, and shows partial cross-domain generalization when trained on one misinformation type and tested on others. This reframing of false data as structured negative supervision offers a direct complement to preference-based alignment methods, with recommended governance controls to ensure safe use.

## Method Summary
The approach fine-tunes LLMs using curated (false claim, correction) pairs extracted from fact-checking sources (LIAR for politics, Health Feedback for health, Snopes/AFP for science) combined with truthful QA pairs from SQuAD. Models learn to recognize and reject misinformation-associated linguistic patterns through explicit negative supervision rather than memorizing individual false facts. Training uses QLoRA with 4-bit quantization, LoRA rank 16, and a recommended 5% vaccine dosage (5-10% range tested) in fine-tuning tokens. The method achieves significant improvements in misinformation detection while preserving general capabilities.

## Key Results
- 12.4 percentage point improvement in TruthfulQA accuracy
- 30.2 percentage point improvement in misinformation rejection rates
- Negligible impact on general capabilities (MMLU performance)
- Optimal performance at 5-10% vaccine dosage in fine-tuning tokens
- Partial cross-domain generalization between misinformation types

## Why This Works (Mechanism)
The approach reframes false claims as structured negative supervision rather than treating them as harmful training data to be avoided. By providing explicit correction pairs, models learn to recognize linguistic patterns associated with misinformation and develop systematic refusal strategies. This supervised approach directly contrasts with preference-based alignment methods, offering a more interpretable path to truthfulness. The 5-10% dosage optimizes the balance between effective immunization and capability preservation.

## Foundational Learning
- **QLoRA fine-tuning**: Low-rank adaptation with 4-bit quantization enables efficient parameter-efficient fine-tuning on large models; needed for practical resource constraints, quick check: verify memory usage during training
- **Negative supervision**: Explicit labeling of false claims teaches models to recognize misinformation patterns; needed for systematic rejection rather than memorization, quick check: test on held-out false claims
- **Vaccine dosage optimization**: Balancing immunization effectiveness with capability preservation; needed to avoid over-refusal, quick check: sweep dosage from 1-20% and measure trade-offs

## Architecture Onboarding

**Component Map:** Data curation -> QLoRA fine-tuning -> Evaluation (TruthfulQA + rejection testing)

**Critical Path:** False claim collection → Correction pair extraction → Instruction formatting → QLoRA fine-tuning → Performance evaluation

**Design Tradeoffs:** 
- Supervised fine-tuning vs. RLHF: offers more interpretable negative supervision but requires labeled data
- Dosage level: higher doses improve immunization but risk over-refusal and capability degradation
- Domain specificity: targeted immunization vs. broader generalization trade-offs

**Failure Signatures:** 
- Over-refusal on legitimate queries (>15% refusal rate on factual questions)
- Model learns to generate misinformation instead of rejecting
- Catastrophic forgetting of previously learned capabilities

**Three First Experiments:**
1. Test baseline model on 200 held-out false claims to establish rejection baseline
2. Fine-tune with 5% vaccine dosage and evaluate on TruthfulQA
3. Test over-refusal by measuring refusal rates on benign health/science questions

## Open Questions the Paper Calls Out
- How does immunization dosage affect the rate of over-refusal on legitimate queries, and what is the optimal sensitivity-specificity balance? The paper notes the risk of models refusing valid queries that resemble misinformation but provides no empirical data on the frequency or severity of this side effect.
- How effectively do "booster" updates apply new misinformation knowledge to previously immunized models without requiring full retraining? New misinformation emerges continuously, but it is unknown if the immunization mechanism supports continuous, incremental updates or if it suffers from catastrophic forgetting.
- How does the chronological ordering of immunization relative to RLHF alignment affect the persistence of truthfulness signals? Applying immunization before RLHF might result in the alignment process washing out the specific negative supervision, whereas applying it after might destabilize preference tuning.
- Does immunization against linguistic patterns in high-resource languages transfer effectively to low-resource languages? Current experiments rely on English data; the paper explicitly flags the lack of verified falsehood datasets in diverse languages as a severe limitation.

## Limitations
- Evaluation protocol for "misinformation rejection" lacks full transparency in operationalization
- Cross-domain generalization results show promise but are limited in scope and magnitude characterization
- Recommended "5% vaccine dosage" optimal point lacks sensitivity analysis across model scales and domains

## Confidence
- High confidence: 12.4 percentage point improvement in TruthfulQA accuracy with appropriate statistical significance
- Medium confidence: 30.2 percentage point improvement in misinformation rejection (evaluation methodology requires specification)
- Medium confidence: Capability preservation claim (negligible impact on MMLU) demonstrated but would benefit from additional benchmark testing

## Next Checks
1. Implement the exact evaluation protocol for measuring misinformation rejection rates, including prompt templates, classification thresholds, and human evaluation criteria to verify reproducibility
2. Systematically vary vaccine dosage from 1-20% in fine-tuning tokens across different model sizes to establish robustness of the 5-10% optimal range and identify potential overfitting thresholds
3. Evaluate immunization transfer from political misinformation to health/science misinformation (and vice versa) with larger sample sizes and statistical power analysis to better characterize partial generalization patterns