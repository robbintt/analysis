---
ver: rpa2
title: Vocabulary Expansion of Large Language Models via Kullback-Leibler-Based Self-Distillation
arxiv_id: '2508.15807'
source_url: https://arxiv.org/abs/2508.15807
tags:
- training
- tokens
- token
- embeddings
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of vocabulary expansion in frozen
  large language models by introducing a mathematically grounded method for knowledge
  distillation via KL divergence, even when the original and extended models use different
  tokenizations. The method allows the student model to inherit distributional knowledge
  from the teacher despite differing vocabularies by training new embeddings to minimize
  KL divergence between probability distributions over the original vocabulary.
---

# Vocabulary Expansion of Large Language Models via Kullback-Leibler-Based Self-Distillation

## Quick Facts
- **arXiv ID:** 2508.15807
- **Source URL:** https://arxiv.org/abs/2508.15807
- **Reference count:** 40
- **Primary result:** KL-divergence based self-distillation enables effective vocabulary expansion in frozen LLMs, outperforming conventional cross-entropy training by a statistically significant factor on code generation benchmarks.

## Executive Summary
This paper addresses the challenge of expanding the vocabulary of frozen large language models (LLMs) when introducing new tokens that were not present during pre-training. The proposed method uses Kullback-Leibler (KL) divergence to train new token embeddings, aligning their output distributions with the pre-trained model's predictions over the original vocabulary. This approach preserves pre-trained distributional knowledge better than direct cross-entropy training, even when the original and extended models use different tokenizations. The method demonstrates superior performance on code generation tasks compared to both conventional vocabulary expansion approaches and the original pre-trained model.

## Method Summary
The approach extends a frozen LLM by adding new tokens to its tokenizer and expanding the embedding and output head matrices. New embeddings are initialized using a heuristic mean of their constituent tokens and trained to minimize KL divergence between the student's output distribution over the original vocabulary and the teacher's distribution. The output head is initialized by copying rows from the first constituent token and trained with cross-entropy. After initial training, the model undergoes full fine-tuning using LoRA to integrate the new vocabulary. This self-distillation framework enables knowledge transfer despite differing tokenizations between the original and extended models.

## Key Results
- KL-based distillation for embedding training significantly outperforms conventional cross-entropy training across all initialization strategies
- The proposed method achieves statistically significant improvements over both the pre-trained DeepSeek base model and all other vocabulary expansion approaches on BigCodeBench and DS-1000 benchmarks
- Mean initialization of new embeddings provides better performance than random initialization, particularly when combined with KL-based training
- The method demonstrates that loss serves as a poor proxy for LLM performance in vocabulary expansion scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing new token embeddings via KL-divergence against a frozen teacher preserves pre-trained distributional knowledge better than direct cross-entropy.
- Mechanism: New embeddings are trained to minimize KL-divergence between the output distribution over the original vocabulary and the teacher's output for the original tokenization. This aligns the student's output with the pre-trained model's existing knowledge structure, rather than forcing it to predict the immediate next token from scratch. This encourages the new embedding to capture information from the entire constituent token sequence, not just the last token.
- Core assumption: Minimizing distributional divergence on the original vocabulary is a better proxy for preserving pre-trained capabilities during vocabulary expansion than solely optimizing for next-token prediction accuracy.
- Evidence anchors:
  - [abstract] "The method allows the student model to inherit distributional knowledge from the teacher despite differing vocabularies by training new embeddings to minimize KL divergence between probability distributions over the original vocabulary."
  - [Section 3.2.1] "The key insight is that the new embeddings are not trained using cross-entropy loss against hard labels, but rather to fit into the existing distributional structure induced by the pretrained model."
  - [Section 6.1] "Appendix A offers a mechanistic hint as to why this happens. Under cross-entropy training, the new embedding... aligns more closely in vector space with the last composite subtoken..., whereas KL-divergence training preserves more information from the earlier subtokens as well."

### Mechanism 2
- Claim: Heuristic initialization of new token embeddings with the mean of their constituent token embeddings provides a better starting point for KL-based optimization than random initialization, especially on downstream tasks.
- Mechanism: Mean initialization pre-locates the new embedding in a semantically plausible region of the vector space, derived from the embeddings of the tokens it replaces. This gives the optimization process a "warm start" where the new token already carries a blend of the semantics of its parts, which is then refined by the KL-divergence loss.
- Core assumption: The semantic meaning of a new composite token is reasonably approximated by the average of the meanings of its constituent tokens.
- Evidence anchors:
  - [Section 5.2] "It is observed that only randomly initialized KL-divergence trained embeddings fail at relevant next-token prediction... Conversely, these errors are mitigated when using heuristic initialization..."
  - [Section 6.2.2] "This suggests KL-divergence alignment involves changing few indices to fit the pre-trained setting while preserving conceptual knowledge."
  - [Section A.3] "When we started with an average initialization and got a similarity of around 0.6 and then trained, the cosine similarity was almost unchanged despite loss decreasing..."

### Mechanism 3
- Claim: Initializing the output head rows for new tokens by copying the row of their first constituent token creates a more compatible initial state for generation.
- Mechanism: The output head predicts the next token. If a new token `numpy` replaces `num, py`, initializing its output embedding row with `num`'s values means the model is initially biased to predict `numpy` in contexts where it would have predicted `num`. This provides a sensible starting point for subsequent cross-entropy training.
- Core assumption: The first token in a sequence is the primary predictor of the subsequent token, and its output embedding is a strong signal for the model's prediction.
- Evidence anchors:
  - [Section 3.2.3] "For a new token tnew that is a composition of original tokens... the corresponding row for tnew in the extended head is set equal to the row of one of its constituent tokens... By assigning 'numpy' the same logit values as 'num', the extended head is initialized in a way that is compatible with the pretrained distribution..."
  - [Section A.2] "Here, the output embeddings come to most align with that of first composite token-embedding. This is completely expected behavior considering the pretrained setting... the token [py] is never being used with our new tokenization, and subsequently, the output embedding does not align itself with it as much..."

## Foundational Learning

- **Concept: Knowledge Distillation (KD)**
  - **Why needed here:** The core of the proposed method is a form of self-distillation. Understanding KD is essential to grasp why minimizing the difference between teacher and student output distributions (KL-divergence) is a valid and useful training objective for transferring knowledge to new embeddings.
  - **Quick check question:** How does distillation differ from training with one-hot labels, and why might it be beneficial for a smaller student model?

- **Concept: Cross-Entropy Loss vs. KL-Divergence**
  - **Why needed here:** The paper explicitly contrasts the conventional cross-entropy loss with its KL-divergence-based objective. Understanding their relationship is crucial to interpreting the experimental results and the mechanistic interpretability analysis, which suggests they optimize embeddings towards different regions of the vector space.
  - **Quick check question:** How is KL-divergence mathematically related to cross-entropy, and why is it used as the loss function for distillation?

- **Concept: Vocabulary Expansion in LLMs**
  - **Why needed here:** This is the core problem the paper addresses. Understanding the standard challenges—new embeddings, a new output head, and the need to integrate them into a frozen model—is necessary to appreciate the motivation and contribution of the proposed method.
  - **Quick check question:** What are the two main architectural components that must be modified to introduce new tokens into a pre-trained LLM?

## Architecture Onboarding

- **Component map:** DeepSeek base model (frozen) -> Tokenizer extension with 800 new tokens -> Extended embedding matrix (new rows) -> Extended output head (new columns) -> KL-loss for embeddings + CE-loss for head -> LoRA fine-tuning

- **Critical path:**
  1. **Tokenizer Extension:** Train a new BPE tokenizer on domain data, select top-N new tokens, and add them as special tokens to the original tokenizer.
  2. **Model Preparation:** Extend the embedding matrix (rows for new tokens) and output head matrix (columns for new tokens).
  3. **Initialization:** Initialize new embeddings (heuristic mean) and new head rows (copy from first constituent).
  4. **Phase 1 Training:** Train E_new with KL-loss and H_new with CE-loss simultaneously using separate optimizers.
  5. **Phase 2 Finetuning:** Unfreeze all embeddings/head for CE-training, then apply LoRA or full fine-tuning on transformer blocks.

- **Design tradeoffs:**
  - **Heuristic Mean Init vs. Random Init:** Mean init provides semantic warm start but may bake in incorrect compositionality. Random init is flexible but prone to initial failures and requires more data to converge.
  - **KL-Divergence vs. CE for Embeddings:** KL preserves global pre-trained knowledge and first-token context; CE optimizes for immediate next-token prediction but can lose nuance and degrade global performance.
  - **LoRA vs. Full Finetuning:** LoRA is memory-efficient and a strong regularizer, but may limit adaptation to the new vocabulary. Full finetuning is more expressive but computationally expensive and prone to overfitting/forgetting.

- **Failure signatures:**
  - **Repetition (e.g., "fibonaccionacci"):** Occurs when only embeddings are trained without fine-tuning the transformer blocks. The model has learned the new semantic representation but the attention layers still process it as a sequence of old tokens.
  - **Natural Language Generation in Code Tasks:** A failure mode observed with certain initializations (random + KL) where the model fails to continue code and instead outputs an explanation. This may stem from dataset bias and insufficient next-token prediction signal.
  - **Divergent Loss Curves:** KL-loss increasing when training with CE indicates a misalignment between objectives and potential degradation of pre-trained knowledge.

- **First 3 experiments:**
  1. **Validate Token Alignment:** On a small batch, run the original and extended tokenizers and print the alignment pairs generated by the mapping algorithm. Manually inspect them to ensure the logic correctly identifies "similar" and "divergent" token sequences.
  2. **Ablate Initialization Strategies:** Train Phase 1 with (a) Random embeddings, Random head; (b) Mean embeddings, Random head; (c) Mean embeddings, Copied head. Compare initial cross-entropy loss to confirm the advantage of the proposed initialization.
  3. **Compare KL vs. CE for Embeddings:** Train two models in Phase 1—one with KL-loss for embeddings and one with CE-loss. Plot both KL-loss and CE-loss curves to verify the paper's finding that CE-loss is disconnected from KL-divergence and can even increase it.

## Open Questions the Paper Calls Out
1. **Hybrid Loss Function:** Can combining KL divergence and cross-entropy for input embeddings improve robustness in scenarios with high density of new tokens?
2. **Temperature Scaling:** To what extent does temperature scaling at the softmax layer improve the proposed self-distillation process?
3. **Scaling to Larger Models:** How does the method perform when scaling to larger models (beyond 7B parameters) and training on data types fundamentally different from the pre-training corpus?
4. **Tokenizer Training:** Does continuing the training of the tokenizer (using BPE merge rules) outperform the manual addition of special tokens used in this study?

## Limitations
- The compositional assumption underlying mean initialization may fail for tokens whose meaning is not simply an average of their components
- Empirical validation is confined to code generation tasks, leaving uncertainty about performance on other domains
- Mechanistic interpretability analysis is conducted on limited samples and may not generalize across all vocabulary expansion scenarios

## Confidence
- **High Confidence:** Mathematical formulation and implementation details of KL-divergence based distillation are well-specified and reproducible
- **Medium Confidence:** Initialization strategy significantly impacts downstream performance, though mechanistic explanations could benefit from more extensive analysis
- **Low Confidence:** Generalizability beyond code generation tasks remains uncertain due to limited domain testing

## Next Checks
1. **Cross-Domain Validation:** Replicate the vocabulary expansion experiment on a natural language dataset to test whether KL-based distillation maintains its advantage outside code generation contexts
2. **Compositionality Stress Test:** Systematically evaluate the method on tokens with non-compositional semantics to determine when the mean initialization heuristic breaks down
3. **Statistical Significance Testing:** Conduct a larger-scale ablation study with proper statistical testing across all initialization and training objective combinations to quantify significance of observed performance differences