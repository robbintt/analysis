---
ver: rpa2
title: 'When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive
  DPO'
arxiv_id: '2503.16921'
source_url: https://arxiv.org/abs/2503.16921
tags:
- data
- preference
- minority
- samples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning diffusion models
  with human preferences, particularly focusing on the impact of minority samples
  in preference datasets. The authors propose Adaptive-DPO, a novel approach that
  incorporates a minority-instance-aware metric into the DPO objective.
---

# When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO

## Quick Facts
- arXiv ID: 2503.16921
- Source URL: https://arxiv.org/abs/2503.16921
- Reference count: 40
- Key outcome: Adaptive-DPO outperforms Diffusion-DPO by 0.21 in ImageReward and shows consistent improvements across various metrics and benchmarks

## Executive Summary
This paper addresses the challenge of aligning diffusion models with human preferences, particularly focusing on the impact of minority samples in preference datasets. The authors propose Adaptive-DPO, a novel approach that incorporates a minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. The Adaptive-DPO loss function improves the DPO loss in two ways: enhancing the model's learning of majority labels while mitigating the negative impact of minority samples. Experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, achieving significant improvements in image quality and alignment with human preferences.

## Method Summary
Adaptive-DPO extends Diffusion-DPO by incorporating a minority-aware metric that combines intra-annotator confidence and inter-annotator stability. The method calculates a weighting coefficient that down-weights samples identified as minority (noisy or highly subjective) and an adaptive margin that strengthens the decision boundary for confident majority samples. This dual approach allows the model to learn from majority preferences more effectively while ignoring or suppressing the influence of minority samples that may represent noise or subjective outliers.

## Key Results
- Adaptive-DPO outperforms Diffusion-DPO by 0.21 in ImageReward on real-world preference data
- The method maintains positive ImageReward scores even with 20% random label flips, while standard DPO collapses
- Ablation study shows that adding the quadratic margin yields higher ImageReward (0.43) compared to variants without it (0.41)

## Why This Works (Mechanism)

### Mechanism 1: Self-Driven Minority Detection via Training Dynamics
The method tracks the model's log-probability gap between preferred and dispreferred images, combining intra-annotator confidence (model disagreement with labels) and inter-annotator stability (variance across training steps). High variance or disagreement flags a sample as minority. The core assumption is that models learn simple/majority patterns before memorizing noise, which may fail if learning rates are too high or model capacity is excessive.

### Mechanism 2: Gradient Suppression via Adaptive Weighting
The minority-aware metric modulates a weighting coefficient that approaches zero for high-risk samples, effectively muting the update step for those sample pairs. This prevents the model from optimizing for erroneous or highly subjective signals. The assumption is that minority samples are detrimental to general alignment performance. The weighting may underfit difficult concepts if valid but complex samples exhibit high instability similar to noise.

### Mechanism 3: Majority Amplification via Adaptive Margin
Alongside down-weighting noise, the method introduces an adaptive margin that penalizes the model more heavily if it fails to distinguish clear majority preference pairs. This forces the model to push the latent representations of "winning" and "losing" images further apart for high-confidence pairs. The assumption is that maximizing distinction between clear-cut preferences creates a more robust feature space. If the margin is too aggressive, the model may lose diversity and output bland images.

## Foundational Learning

- **Concept: Diffusion-DPO (Direct Preference Optimization)**
  - Why needed here: This is the base algorithm the paper modifies. Understanding that Diffusion-DPO optimizes a binary classifier (reward) implicitly via pairwise comparisons is necessary to grasp why noise confuses it.
  - Quick check question: Can you explain why Diffusion-DPO avoids training a separate reward model, and how Eq. 6 relates noise prediction error to preference probability?

- **Concept: Bradley-Terry Model**
  - Why needed here: The theoretical foundation of preference learning assumes a single scalar reward represents preference. This paper challenges the assumption that the probability P(x_w > x_l) is always consistent with a ground truth r(x).
  - Quick check question: In a standard Bradley-Terry model, what happens to the predicted probability if the reward difference between a winner and loser is zero?

- **Concept: Early-Learning Regularization / Learning with Noisy Labels (LNL)**
  - Why needed here: The paper's minority detection relies on the "early learning" phenomenon (learning clean data before noise). You must understand why neural networks tend to fit simple patterns first to trust the intra-annotator confidence metric.
  - Quick check question: Why might a high learning rate cause the "early learning" phenomenon to fail, leading to immediate memorization of random labels?

## Architecture Onboarding

- **Component map:** Input Layer (Pick-a-Pic v2 pairs) -> Base Model (SD1.5/SDXL) -> Metric Engine (calculates ℓ_θ, stability, confidence) -> Adaptive Loss (combines W_θ and Γ_θ)

- **Critical path:** The Metric Engine is the most fragile component. It requires maintaining an EMA or buffer of historical model parameters/checkpoints to calculate inter-annotator stability. If the history buffer is too short, the metric will be noisy; if too long, it adds memory overhead.

- **Design tradeoffs:**
  - Noise Robustness vs. Diversity: Tuning k₁ trades off between ignoring noise and ignoring valid minority styles
  - Computation vs. Accuracy: Calculating stability requires multiple forward passes or storing intermediate states

- **Failure signatures:**
  - Metric Saturation: If u_θ is near zero for all samples, the method degrades to standard DPO
  - False Positives: Difficult prompts might naturally show high training variance, causing the model to stop learning complex concepts
  - Reward Hacking: If majority labels are systematically biased, the adaptive margin will reinforce this bias

- **First 3 experiments:**
  1. Pilot Reproduction: Fine-tune SD1.5 on Pick-a-Pic with 20% random label flips; compare DPO vs. Adaptive-DPO
  2. Metric Validation: Visualize distribution of u_θ scores and correlate with human-identified "bad" pairs
  3. Margin Ablation: Train with only Weighting vs. Weighting + Margin to check if margin improves "winning" image quality

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the theoretical foundations of the Adaptive-DPO metric be formalized to provide convergence guarantees?
  - Basis: Authors state understanding theoretical foundations will contribute to fine-tuning parameter space
  - Why unresolved: Current formulation relies on heuristic combinations without formal proof of optimality
  - Evidence needed: Theoretical analysis proving convergence bounds or deriving metric from first principles

- **Open Question 2:** How can we comprehensively analyze the underlying distribution of real-world minority data versus synthetic noise?
  - Basis: Authors identify exploring more comprehensive analyses about minority data as key future work
  - Why unresolved: Paper primarily validates using synthetic minority data, acknowledging real data correlates with factors like difficulty
  - Evidence needed: Large-scale study correlating Adaptive-DPO metric scores with ground-truth causes of minority status

- **Open Question 3:** Does suppressing minority samples inadvertently reduce the model's ability to capture valid niche preferences or cultural diversity?
  - Basis: Paper defines minority samples as including both erroneous annotations and valid subjective divergences, but uniformly suppresses both
  - Why unresolved: Treating valid subjective differences as noise may ignore legitimate minority user groups, leading to homogenization
  - Evidence needed: User study evaluating diversity of generated outputs on prompts designed to trigger niche preferences

## Limitations

- Dataset Dependence: The method's effectiveness is primarily validated on Pick-a-Pic v2, which may not represent all types of preference noise
- Implementation Complexity: The metric calculation requires maintaining historical checkpoints or EMA footprints, adding computational overhead
- Hyperparameter Sensitivity: Adaptive weighting and margin terms depend on carefully tuned hyperparameters that may not transfer well across different model scales

## Confidence

- Core algorithmic formulation: **High confidence** - well-grounded in established LNL literature
- Experimental validation: **Medium confidence** - primarily validated on single dataset with synthetic noise
- Theoretical foundations: **Low confidence** - heuristic combinations without formal convergence proofs

## Next Checks

1. Test Adaptive-DPO on datasets with known systematic biases to verify it doesn't reinforce such biases through the majority amplification mechanism

2. Conduct ablation studies varying the EMA decay rate and checkpoint history length to determine optimal trade-offs between metric accuracy and computational overhead

3. Evaluate the method's performance when applied to non-image domains (e.g., text generation) to assess generalizability beyond the diffusion model context