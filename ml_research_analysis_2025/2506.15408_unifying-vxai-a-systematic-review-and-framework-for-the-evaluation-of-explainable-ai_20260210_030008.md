---
ver: rpa2
title: 'Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable
  AI'
arxiv_id: '2506.15408'
source_url: https://arxiv.org/abs/2506.15408
tags:
- explanation
- metrics
- arxiv
- explanations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a comprehensive framework for the functionality-grounded
  evaluation of XAI methods, termed VXAI. Through a systematic literature review of
  362 sources, it aggregates over 400 individual metrics into 41 functionally similar
  metric groups.
---

# Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI

## Quick Facts
- **arXiv ID:** 2506.15408
- **Source URL:** https://arxiv.org/abs/2506.15408
- **Reference count:** 40
- **Key outcome:** Comprehensive framework aggregating 400+ XAI metrics into 41 functionally similar groups across three dimensions (Explanation Type, Contextuality, Desiderata)

## Executive Summary
This work presents VXAI, a systematic framework for evaluating XAI methods through functionality-grounded metrics. By conducting a systematic literature review of 362 sources, the authors consolidate over 400 individual metrics into 41 functionally similar metric groups. The framework organizes metrics along three orthogonal dimensions—explanation type, evaluation contextuality, and desiderata—providing a structured approach for metric selection and cross-method comparison. Fidelity emerges as the most frequently addressed desideratum, while metrics for Parsimony and Plausibility are predominantly Explanans-Centric or Model Observation-based. The framework supports standardized, methodologically rigorous evaluation practices in XAI and enables systematic metric selection through an accessible interface.

## Method Summary
The authors conducted a systematic literature review following PRISMA guidelines, sourcing publications from Scopus, IEEE, and ACM databases through a two-stage process: database queries using specific search strings followed by backward snowballing. From 362 filtered publications, they extracted over 400 individual metrics and aggregated them into 41 functionally similar metric groups based on methodological patterns. Each metric was categorized along three dimensions: Desiderata (7 quality criteria including Fidelity, Parsimony, Plausibility), Explanation Type (5 formats including Feature Attribution, Concept Explanation, Natural Language Explanation), and Contextuality (5 levels of intervention access ranging from Explanans-Centric to A Priori Constrained). The framework was implemented as an accessible interface at https://vxai.dfki.de/ for practical metric selection.

## Key Results
- Aggregation of 400+ individual metrics into 41 functionally similar metric groups
- Three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata
- Identification of Fidelity as the most frequently addressed desideratum across the literature
- Framework enables systematic metric selection and cross-method comparison
- Metrics for Parsimony and Plausibility are predominantly Explanans-Centric or Model Observation-based

## Why This Works (Mechanism)

### Mechanism 1: Metric Consolidation via Functional Grouping
The authors reduced evaluation complexity by clustering metrics that share methodological patterns into 41 aggregated metrics. This abstraction preserves core evaluation logic while hiding implementation nuances, assuming metrics with similar computational approaches measure the same underlying property.

### Mechanism 2: Orthogonal Categorization Disambiguation
The three-dimensional framework prevents misalignment by forcing selection through Desiderata (what to measure), Explanation Type (what format), and Contextuality (access/intervention level). This prevents applying metrics designed for surrogate models to saliency maps without adaptation.

### Mechanism 3: Contextuality as Intervention Granularity
The five-level Contextuality spectrum distinguishes between evaluating specific outputs (In-Situ) versus general method properties (Ex-Situ). This clarifies that metrics like "Model Parameter Randomization Test" evaluate general reliability while "Explanans Size" evaluates specific instance parsimony.

## Foundational Learning

- **Concept: Desiderata (Fidelity vs. Plausibility)**
  - Why needed: The framework distinguishes technical correctness (Fidelity) from human alignment (Plausibility), noting these often don't correlate
  - Quick check: Does the metric measure if the explanation is true to the model (Fidelity) or if it makes sense to a human (Plausibility)?

- **Concept: Explanans vs. Explanandum**
  - Why needed: Strict terminology distinguishes what's being explained (Explanandum - model/prediction) from the output (Explanans - explanation itself)
  - Quick check: Is the metric analyzing the model's decision process (Explanandum) or just the heatmap/text output (Explanans)?

- **Concept: In-Situ vs. Ex-Situ Evaluation**
  - Why needed: This distinction underpins Contextuality - In-Situ evaluates specific context while Ex-Situ evaluates method generally
  - Quick check: Does evaluation require changing model parameters (Ex-Situ) or can it be calculated on a single forward pass (In-Situ)?

## Architecture Onboarding

- **Component map:** VXAI Framework (database/logic) -> 3D Filter (Desiderata, Explanation Type, Contextuality) -> Metric Groups (leaf nodes with implementations)
- **Critical path:** 1) Identify Explanation Type, 2) Select Desiderata (start with Fidelity), 3) Filter by Contextuality (Level I-II for deployed, III-V for benchmarking), 4) Retrieve Metric via interface
- **Design tradeoffs:** Fidelity vs. Plausibility often require choosing one or accepting trade-off; Automation vs. Ground Truth efficiency vs. proxy limitations
- **Failure signatures:** Contextuality mismatch (using Level V for real-world models), Type mismatch (applying Feature Attribution metrics to Concepts), Ground truth illusion (assuming annotations reflect model reasoning)
- **First 3 experiments:** 1) Run "Model Parameter Randomization Test" to check method captures model logic, 2) Apply "Guided Perturbation Fidelity" with MoRF/LeRF to test feature necessity/sufficiency, 3) Measure "Explanans Size" across dataset to check Parsimony

## Open Questions the Paper Calls Out

- **Open Question 1:** Under what experimental conditions do different functionality-grounded evaluation metrics yield contradictory results? (Explicit: Future work should investigate under which conditions metrics agree or contradict)
- **Open Question 2:** How can multiple evaluation metrics be aggregated into a single composite score in a meaningful and robust way? (Explicit: Field lacks consensus on combining metrics meaningfully)
- **Open Question 3:** What are the established practical thresholds or benchmarks that define "good" explanation quality for specific metrics? (Explicit: Few metrics offer clear thresholds or benchmarks)
- **Open Question 4:** To what extent can existing metrics designed for Feature Attributions be validly transferred to Natural Language Explanations? (Explicit: Future work should adapt metrics to underexplored explanation types)

## Limitations
- Focus on functionality-grounded metrics excludes human-grounded and application-grounded evaluations
- Contextuality dimension shows ambiguity in boundary cases with hybrid access patterns
- Aggregation of 400+ metrics may obscure implementation-specific details affecting behavior
- Framework may not be fully extensible to future XAI paradigms not represented in current literature

## Confidence
- **High Confidence:** Systematic literature review methodology (362 publications), three-dimensional categorization coherence, functional grouping based on methodological similarity
- **Medium Confidence:** Completeness of metric aggregation given rapid evolution, practical utility in reducing selection complexity, assumption that functional similarity implies same underlying measurement
- **Low Confidence:** Robustness of Contextuality boundaries for hybrid metrics, framework extensibility to future paradigms, correlation between aggregated and disaggregated implementations

## Next Checks
1. **Contextuality Boundary Validation:** Apply framework to recent XAI evaluation metrics to test classification system, especially ambiguous cases at level boundaries
2. **Aggregation Fidelity Test:** Compare implementation-specific behavior of five metrics from different groups against functional descriptions to verify aggregation preserves essential characteristics
3. **Correlation Analysis:** Conduct empirical studies measuring correlation between Fidelity and Plausibility metrics across multiple XAI methods to validate observed divergence