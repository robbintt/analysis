---
ver: rpa2
title: Predicting When to Trust Vision-Language Models for Spatial Reasoning
arxiv_id: '2601.11644'
source_url: https://arxiv.org/abs/2601.11644
tags:
- confidence
- spatial
- geometric
- blip-2
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision-Language Models (VLMs) excel at multimodal tasks but fail
  at spatial reasoning, achieving only 49-54% accuracy on basic directional relationships.
  For safe deployment in robotics and autonomous systems, it is critical to predict
  when to trust VLM spatial predictions.
---

# Predicting When to Trust Vision-Language Models for Spatial Reasoning

## Quick Facts
- arXiv ID: 2601.11644
- Source URL: https://arxiv.org/abs/2601.11644
- Reference count: 1
- Primary result: Achieves 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines)

## Executive Summary
Vision-Language Models (VLMs) excel at multimodal tasks but fail at spatial reasoning, achieving only 49-54% accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, it is critical to predict when to trust VLM spatial predictions. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. Our framework enables selective prediction, achieving 61.9% coverage versus 27.6% baseline at 60% target accuracy on BLIP-2.

## Method Summary
The approach uses GroundingDINO for object detection to extract bounding boxes, then computes four geometric features: geometric confidence (37.5% importance), separation confidence based on overlap (32.7%), detection quality (17.2%), and VLM token confidence (12.7%). These features are fused using XGBoost to predict confidence scores for spatial relation predictions. The framework is evaluated on the VSR benchmark with 10,972 MS-COCO images, achieving 0.674 AUROC on BLIP-2 and 0.583 on CLIP, with 34.0% and 16.1% improvements over text-based baselines respectively.

## Key Results
- Achieves 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines)
- At 60% target accuracy, achieves 61.9% coverage versus 27.6% baseline (2.2× improvement) on BLIP-2
- Vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence
- Confidence-based pruning improves scene graph precision from 52.1% to 78.3% while retaining 68.2% of edges

## Why This Works (Mechanism)

### Mechanism 1: External Geometric Verification Overrides Self-Assessment
Independent coordinate-based spatial verification provides stronger trust signals than VLM internal confidence. Object detection produces bounding boxes; geometric relations computed from centers are compared against VLM predictions to detect contradictions.

### Mechanism 2: Separation Confidence Captures Spatial Ambiguity
Object overlap creates genuine ambiguity that must be explicitly modeled as a confidence reducer. α_sep = 1 − IoU; high overlap between bounding boxes signals that directional relationships are ill-defined.

### Mechanism 3: Learned Fusion Outperforms Single Heuristics
Gradient boosting optimally weights complementary signals better than any single geometric rule. XGBoost classifier learns non-linear combinations of geometric alignment, separation, detection quality, and VLM token confidence.

## Foundational Learning

- **Concept: Selective Prediction**
  - Why needed here: Enables systems to abstain from uncertain predictions, maintaining target accuracy by deferring difficult cases
  - Quick check question: At 60% target accuracy, what coverage percentage indicates your confidence estimator is effective?

- **Concept: AUROC (Area Under ROC Curve)**
  - Why needed here: Primary metric for evaluating confidence discrimination—measures ability to rank correct predictions higher than incorrect ones
  - Quick check question: If a baseline achieves 0.503 AUROC, what does this imply about its ability to distinguish correct from incorrect predictions?

- **Concept: Feature Importance via Gain**
  - Why needed here: Validates design decisions by quantifying each signal's contribution to final predictions
  - Quick check question: If VLM token confidence contributes only 12.7% importance, what does this suggest about self-assessment reliability?

## Architecture Onboarding

- **Component map:** VLM → GroundingDINO → Feature extractor → XGBoost classifier → Threshold comparator
- **Critical path:** Image input → VLM prediction AND GroundingDINO detection (parallel) → Feature extraction → XGBoost inference → Binary decision
- **Design tradeoffs:** 4 features vs. 10 reduces overfitting (train-val gap 0.10 vs 0.12); 91.7% coverage at 50% accuracy vs. 12.2% at 80% accuracy; +46% overhead (87ms) vs. +500% for multi-prompt text methods
- **Failure signatures:** Detection failure → no bounding boxes → α_geo undefined; VLM-geometry contradiction → confidence ~0.08; high overlap (IoU > 0.5) → α_sep drops
- **First 3 experiments:**
  1. Replicate geometric-only baseline on VSR test set; verify AUROC ≈ 0.493 for BLIP-2
  2. Run feature ablation; confirm removing geometric confidence causes largest drop (−0.171)
  3. Calibrate threshold via Youden's index; verify θ* ≈ 0.502 for BLIP-2, ≈ 0.380 for CLIP

## Open Questions the Paper Calls Out

### Open Question 1
Can keypoint-based geometric validation improve confidence estimation over the current bounding-box approach, particularly for handling small or occluded objects? The Conclusion states, "Future work should explore keypoint-based validation" to address detection errors for small objects or severe occlusion.

### Open Question 2
How can the geometric verification logic be adapted to support ternary spatial relations (e.g., "between") which cannot be resolved by pairwise coordinate comparisons? The Conclusion lists "extension to ternary relations" as a specific direction for future work.

### Open Question 3
Does the reliance on fixed spatial thresholds in the geometric confidence calculation limit performance across images with drastically different scene scales? The Conclusion identifies "fixed spatial thresholds not adapting to scene scale" as a limitation.

### Open Question 4
Can incorporating uncertainty-aware object detection improve the reliability of the geometric verification signal compared to the current deterministic thresholding? The Conclusion suggests exploring "uncertainty-aware detection."

## Limitations
- Performance depends critically on exact VSR subset selection and prompt templates not fully specified
- Geometric verification assumes 2D coordinate geometry is sufficient for spatial reasoning validation
- 2× improvement in coverage@60% accuracy comes at the cost of discarding 38.1% of predictions

## Confidence

- **High confidence:** Geometric confidence dominates feature importance (87.4% vision vs 12.7% VLM); selective prediction framework enables 2.2× coverage improvement; scene graph precision increases from 52.1% to 78.3%
- **Medium confidence:** AUROC improvements are robust (34.0% on BLIP-2, 16.1% on CLIP) but depend on specific dataset splits and evaluation protocols not fully specified
- **Low confidence:** Generalization to other spatial reasoning tasks beyond directional relationships; performance on real-world robotics applications with dynamic scenes

## Next Checks

1. **Split fidelity check:** Replicate the exact 705-image training set (494 train, 211 val) and 312-image test split using the VSR benchmark. Verify that the random sampling produces comparable AUROC results within ±0.02 margin.

2. **Prompt template verification:** Test multiple prompt variations for BLIP-2 and CLIP text templates to confirm the reported performance improvements are robust to natural language variation.

3. **Geometric-only ablation validation:** Implement the geometric-only baseline (α_geo only) on the full test set and verify the reported 0.493 AUROC for BLIP-2 and 0.431 for CLIP, confirming that learned fusion provides the claimed 36.7% and 35.3% improvements respectively.