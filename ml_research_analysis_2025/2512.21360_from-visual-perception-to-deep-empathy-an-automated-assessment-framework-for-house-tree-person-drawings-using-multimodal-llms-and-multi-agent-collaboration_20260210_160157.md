---
ver: rpa2
title: 'From Visual Perception to Deep Empathy: An Automated Assessment Framework
  for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration'
arxiv_id: '2512.21360'
source_url: https://arxiv.org/abs/2512.21360
tags:
- multi-agent
- expert
- psychological
- system
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study developed an automated House-Tree-Person drawing test\
  \ assessment framework using multimodal large language models and multi-agent collaboration.\
  \ It addressed inconsistencies in human expert scoring by employing Qwen-VL-Plus\
  \ and Doubao-embedding-vision to achieve a mean semantic similarity of 0.75 (SD\
  \ \u2248 0.05) with human expert interpretations."
---

# From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration

## Quick Facts
- **arXiv ID**: 2512.21360
- **Source URL**: https://arxiv.org/abs/2512.21360
- **Reference count**: 4
- **Primary result**: Achieved mean semantic similarity of 0.75 (SD ≈ 0.05) with human expert HTP interpretations using multi-agent multimodal LLM framework

## Executive Summary
This study presents an automated House-Tree-Person (HTP) drawing test assessment framework using multimodal large language models and multi-agent collaboration. The system addresses scoring inconsistencies in human expert assessments by employing Qwen-VL-Plus and Doubao-embedding-vision to achieve expert-level baseline comprehension. A four-agent system with specialized roles (Observer, Interpreter, Zeitgeist Observer, Listener) enhances feature recognition, psychological inference, social-context integration, and empathic reporting. The framework demonstrates expert-level baseline comprehension (similarity rising to 0.85 in structured expert datasets) and produces clinically valid, internally coherent reports with high ecological validity.

## Method Summary
The framework employs a two-stage approach: first using Qwen-VL-Plus for psychological interpretation with Doubao-embedding-vision for text embeddings and cosine similarity computation; second deploying a multi-agent system on Qingliu Agent platform with four specialized roles. The Observer agent produces structured JSON with 150+ objective observation points before interpretation occurs. Three MLLMs (Gemini-3-Pro-Preview, Qwen3-VL-Plus, Doubao-Seed-1-6-Vision) independently interpret drawings, with DeepSeek-v3.1 merging outputs through consensus classification. The system was validated on 307 anonymized HTP drawings with expert interpretations from multiple analysts, achieving mean semantic similarity of 0.75.

## Key Results
- Achieved mean semantic similarity of 0.75 (SD ≈ 0.05) with human expert HTP interpretations
- Expert-level baseline comprehension with similarity rising to 0.85 on structured expert datasets
- Multi-agent framework demonstrates expert-level baseline comprehension and clinical validity
- System produces internally coherent reports with high ecological validity

## Why This Works (Mechanism)

### Mechanism 1: Role-Decoupled Processing Pipeline
Decoupling feature recognition from psychological inference reduces hallucination propagation and improves interpretive consistency. The Observer agent produces structured JSON with 150+ objective observation points (line quality, spatial layout, proportions) before any interpretation occurs, creating a verifiable evidence trail that constrains downstream reasoning.

### Mechanism 2: Multi-Model Consensus Cross-Validation
Aggregating interpretations from multiple MLLMs reduces single-model bias and flags uncertain conclusions for verification. Three MLLMs independently interpret the same drawing, with DeepSeek-v3.1 merging outputs by classifying findings as consensus (reliable), partially supported, or divergent (flagged "to be verified").

### Mechanism 3: Social-Context Normalization
Situating individual psychological findings within macro social statistics reduces over-pathologization and improves ecological validity. The Zeitgeist Observer agent references external reports (e.g., workplace anxiety surveys) to contextualize symptoms as potentially normal responses to structural pressures.

## Foundational Learning

- **Concept: Cosine Similarity for Semantic Alignment**
  - Why needed here: The paper's primary quantitative metric; the 0.75 mean similarity measures directional alignment in embedding space, not exact textual match.
  - Quick check question: If two interpretations use different vocabulary but convey similar psychological meaning, would you expect high or low cosine similarity?

- **Concept: Multi-Agent Role Specialization**
  - Why needed here: The core architectural innovation; each agent has bounded responsibility that compounds into a more robust system than a single generalist model.
  - Quick check question: What failure mode would likely emerge if you merged the Observer and Interpreter into one agent with a combined prompt?

- **Concept: Ecological Validity in Clinical Assessment**
  - Why needed here: The paper claims reports have "high ecological validity"—findings must generalize to real-world functioning, not just test conditions.
  - Quick check question: Why might an assessment with strong internal consistency still fail to predict real-world behavior?

## Architecture Onboarding

- **Component map:**
  Input: HTP Drawing (image)
  ↓
  Observer Agent → Structured JSON (150+ visual features)
  ↓
  Interpreter Agent → Psychological mapping (Buck/Hammer framework)
  ↓
  Zeitgeist Observer → Social-context layer (statistics, cultural norms)
  ↓
  Listener Agent → Empathic narrative report
  ↓
  Output: Personalized psychological assessment

- **Parallel**: 3× MLLM → Independent interpretations → DeepSeek merger → Integrated report

- **Critical path**: Observer → Interpreter handoff is highest-risk; errors in visual feature extraction cascade through all downstream reasoning.

- **Design tradeoffs**:
  - Structured JSON vs. natural language: JSON enables validation but may lose perceptual nuance
  - Multi-model fusion vs. latency: Increases robustness but adds ~3× inference time
  - Social context vs. cultural portability: Zeitgeist Observer requires localized data sources; Chinese workplace statistics may not transfer

- **Failure signatures**:
  - Observer hallucinations: JSON claims features not present in image
  - Interpreter over-pathologizing: Zeitgeist context appended but not integrated
  - Cultural mismatch: Similarity scores drop significantly for populations not represented

- **First 3 experiments**:
  1. Observer validation test: Feed 10 diverse HTP drawings to Observer only; manually verify extraction accuracy. Target: >90% precision on core features.
  2. Pipeline ablation study: Run 20 drawings through single-model baseline and full multi-agent pipeline. Have blinded clinical reviewers rate coherence, empathy, and clinical plausibility.
  3. Expert-style sensitivity probe: Test system on drawings from both structuralist (Wang Long style) and imagistic (Yan Hu style) expert sources; quantify similarity score variance.

## Open Questions the Paper Calls Out

### Open Question 1
Can domain-specific fine-tuning of MLLMs significantly improve performance on extremely abstract or highly artistic HTP drawings where Qwen-VL-Plus currently performs poorly?
- Basis in paper: [explicit] "Qwen-VL-Plus performs poorly on extremely abstract or highly artistic drawings; specific domain adaptation or fine-tuning is needed."
- Why unresolved: The study only tested baseline MLLM capabilities without fine-tuning; no experiments compared fine-tuned vs. non-fine-tuned models on challenging drawing subsets.
- What evidence would resolve it: A controlled experiment comparing semantic similarity scores between baseline and fine-tuned MLLMs on a curated set of abstract/artistic HTP drawings.

### Open Question 2
Does the multi-agent framework maintain comparable semantic similarity and clinical validity across different age groups and non-Chinese cultural contexts?
- Basis in paper: [explicit] "Our dataset focuses on adult participants in China; future work should expand across ages and cultures to test generalisability."
- Why unresolved: All 307 test cases came from adult Chinese participants; no cross-cultural or developmental validation was conducted.
- What evidence would resolve it: Replication studies with pediatric, adolescent, and elderly populations across multiple cultural settings, reporting semantic similarity and qualitative clinical validity metrics.

### Open Question 3
What is the clinical predictive validity of the multi-agent HTP framework for diagnosing specific mental disorders, compared to traditional human-administered assessments?
- Basis in paper: [inferred] The paper reports semantic similarity (0.75–0.85) as the primary validity metric, but does not establish whether interpretations correlate with actual clinical diagnoses or treatment outcomes.
- Why unresolved: The study validated alignment with expert interpretations, not diagnostic accuracy; systematic reviews cited note 39 features predict disorders, but no diagnostic validation was performed.
- What evidence would resolve it: A prospective clinical study comparing framework-generated risk assessments against structured clinical interviews and established diagnostic instruments.

## Limitations
- Current validation limited to Chinese adult populations with primarily structuralist interpretation styles
- Performance on imagistic/metaphorical interpretations shows greater variability, suggesting incomplete coverage
- Cultural portability remains untested - Zeitgeist Observer's social-context layer relies on Chinese workplace statistics

## Confidence
- **High Confidence**: Observer agent's structured feature extraction (>150 observation points), multi-model consensus mechanism, and similarity metric computation
- **Medium Confidence**: Psychological interpretation accuracy (Buck/Hammer framework mapping), empathic narrative generation, and social-context integration
- **Low Confidence**: Performance on abstract/artistic drawings, cross-cultural validity, and long-term clinical utility

## Next Checks
1. **Cross-Cultural Validation**: Test framework on HTP drawings from diverse cultural backgrounds; measure similarity score variance and identify cultural-specific interpretation patterns
2. **Longitudinal Clinical Trial**: Deploy system in actual clinical settings for 6+ months; track inter-rater reliability between automated and human clinician assessments over time
3. **Interpretive Style Robustness**: Conduct systematic evaluation comparing performance on structuralist vs. imagistic expert interpretations; develop adaptive prompting strategies for each style