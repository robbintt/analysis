---
ver: rpa2
title: Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking
arxiv_id: '2511.01208'
source_url: https://arxiv.org/abs/2511.01208
tags:
- reranking
- relevance
- documents
- document
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes contextual relevance and TS-SetRank for LLM-based
  document reranking. Contextual relevance captures the probability that a document
  is relevant given a query, marginalized over all batches it may appear in, accounting
  for both batch composition and document ordering.
---

# Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking

## Quick Facts
- **arXiv ID:** 2511.01208
- **Source URL:** https://arxiv.org/abs/2511.01208
- **Reference count:** 17
- **Primary result:** TS-SetRank improves nDCG@10 by 15-25% on BRIGHT and 6-21% on BEIR over retrieval and reranking baselines under fixed inference budgets

## Executive Summary
This paper addresses the challenge of efficient LLM-based document reranking by introducing contextual relevance and TS-SetRank. The authors argue that document relevance is not static but depends on the composition and ordering of documents within each batch. To capture this, they propose modeling relevance as a probability marginalized over different reranking contexts. TS-SetRank implements this through a two-phase Bayesian reranking algorithm: an initial uniform sampling phase for exploration, followed by Thompson sampling for adaptive batch construction that balances exploration and exploitation. Evaluated on BRIGHT and BEIR benchmarks, TS-SetRank demonstrates significant improvements in nDCG@10 under constrained inference budgets, validating the importance of context-aware relevance modeling in efficient reranking.

## Method Summary
The approach involves training a Qwen2.5-7B-Instruct reranker on MS MARCO v2.1 using GRPO with binary relevance feedback. For inference, TS-SetRank maintains Beta-Bernoulli posteriors for each document, initialized with α=β=1. The algorithm operates in two phases: Phase I performs T_f rounds of uniform batch sampling to seed posteriors, while Phase II uses Thompson sampling to select top-b documents based on sampled relevance scores. After each batch, posteriors are updated with binary LLM feedback. The final ranking is determined by sorting documents by their posterior means. The method specifically targets constrained inference budgets (T=100 calls) where traditional reranking approaches are inefficient.

## Key Results
- TS-SetRank improves nDCG@10 by 15-25% on BRIGHT and 6-21% on BEIR over retrieval and reranking baselines
- The method excels under constrained inference budgets, outperforming uniform sampling particularly at early snapshots (t=50)
- Contextual relevance modeling captures batch composition and ordering effects, providing theoretical foundation for the approach

## Why This Works (Mechanism)

### Mechanism 1: Contextual Marginalization via Sampling
Document relevance is treated as a stochastic variable dependent on batch context, rather than a static score. The system estimates P(relevant|context) by aggregating binary feedback over multiple random batch compositions, averaging out contextual noise where a document appears relevant only when paired with specific neighbors. This works under the assumption that relevance exhibits conditional independence across documents, and averaging over diverse contexts reveals a "true" relevance signal.

### Mechanism 2: Bayesian Posterior Estimation (Beta-Bernoulli)
Each document maintains a Beta(α, β) posterior. Success increments α, failure increments β. The final rank uses posterior mean θ̂ = α/(α+β), which naturally penalizes unobserved or low-performing documents. This approach assumes feedback is noisy but binary and independent per round, providing robust ranking under sparse feedback conditions.

### Mechanism 3: Thompson Sampling for Budget Allocation
After warm-up, the algorithm samples document relevance scores from their posteriors and selects top-b documents for the next batch. This balances exploration (checking uncertain docs) and exploitation (verifying relevant docs), focusing inference budget on "borderline" documents. The approach assumes the exploration-exploitation trade-off is relevant because LLM inference is the bottleneck.

## Foundational Learning

- **Multi-Armed Bandits (Combinatorial Semi-Bandit):** TS-SetRank models reranking as a bandit problem where each document is an "arm," but we pull a set of arms (batch) simultaneously. *Quick check:* Why is this considered a "semi-bandand" rather than a standard bandit? (Answer: We observe individual rewards for each document in the batch, not just a single batch-level reward.)

- **Beta Distribution:** This is the mathematical engine for tracking uncertainty. Understanding how α and β shape the distribution is key to debugging why a document is ranked high or low. *Quick check:* If a document is never sampled, what is its posterior mean? (Answer: 1/2, due to the uniform prior α=1, β=1).

- **Thompson Sampling:** This is the policy that drives the efficiency gains. It differs from greedy selection (pick highest mean) or purely random exploration. *Quick check:* How does Thompson Sampling differ from a "Greedy" approach when selecting a document with a high mean but very low observation count? (Answer: Thompson sampling accounts for variance; a greedy approach might over-commit to a lucky initial result, while Thompson sampling will still explore to confirm.)

## Architecture Onboarding

- **Component map:** Query + Top-N documents (BM25) -> TS-SetRank controller -> Batch selector (Phase I random/Phase II Thompson) -> LLM scorer (Qwen2.5-7B-Instruct) -> Bayesian updater -> Sorted output

- **Critical path:** 1) Initialization: Set priors α=1, β=1. 2) Phase I (Exploration): Randomly batch documents to seed posteriors. 3) Phase II (Exploitation): Sample θ̃_i ~ Beta(α_i, β_i), select top-b docs. 4) Update: Observe LLM feedback, update posteriors. 5) Output: Sort by posterior means.

- **Design tradeoffs:** Phase II is inherently sequential (depends on feedback). For throughput priority, use TS-SetRank-T (Appendix C) which delays updates for batched parallel calls. Higher T_f stabilizes estimates but delays adaptive gains; T_f=25-50 works well for T=100.

- **Failure signatures:** Mode Collapse if Phase I is too short (Thompson samples fixate on lucky documents). Stagnation if T is too low relative to N (posteriors remain near prior 0.5). Positional Bias if LLM ignores context but adheres to position.

- **First 3 experiments:** 1) Sanity Check: Compare fixed batch vs. random batches to verify contextual relevance exists. 2) Hyperparameter Sweep: Ablate T_f (0, 25, 50, 75) against final nDCG@10. 3) Efficiency Boundary: Compare TS-SetRank vs. Heapify at low inference counts (t=20, 50).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can document interdependencies be modeled within the contextual relevance framework to handle complex tasks like multi-hop question answering?
- **Basis:** The "Limitations" section states the current independence assumption fails in domains where relevance depends on prerequisite evidence (e.g., multi-hop QA or citation retrieval).
- **Why unresolved:** The current Beta-Bernoulli model treats documents as conditionally independent for tractability, ignoring structural relationships between documents.
- **What evidence would resolve it:** A variant of TS-SetRank incorporating structured dependencies (e.g., graph-based models) demonstrating improved performance on multi-hop benchmarks like HotpotQA.

### Open Question 2
- **Question:** Does using continuous relevance signals instead of binary judgments improve the sample efficiency of the reranking algorithm?
- **Basis:** The authors list "training reranking models that produce continuous rather than binary relevance signals" as a specific extension for future work.
- **Why unresolved:** TS-SetRank currently relies on binary relevance feedback (relevant/irrelevant) to update the Beta posterior, potentially losing nuance in the model's confidence.
- **What evidence would resolve it:** A modified algorithm accepting continuous scores showing higher nDCG@10 or faster convergence (fewer inference calls) on BRIGHT or BEIR.

### Open Question 3
- **Question:** Does contextual relevance estimation improve the quality of final answer generation when integrated into end-to-end Retrieval-Augmented Generation (RAG) pipelines?
- **Basis:** The "Limitations" section identifies integrating contextual relevance into end-to-end RAG pipelines as a "compelling area for future exploration."
- **Why unresolved:** The current evaluation focuses solely on retrieval quality (nDCG) rather than the downstream impact on the generation capabilities of the LLM.
- **What evidence would resolve it:** Improvements in answer accuracy or faithfulness metrics in a RAG setup compared to standard deterministic rerankers.

### Open Question 4
- **Question:** How does the performance of TS-SetRank compare to uniform sampling when inference budgets scale significantly beyond the observed convergence point?
- **Basis:** Figure 3 shows uniform sampling converges at approximately 300 calls, but experiments are restricted to a constrained budget of T=100.
- **Why unresolved:** It is unclear if the adaptive advantage of Thompson sampling is limited to low-budget regimes or if it holds statistical significance once uniform sampling has sufficiently averaged over the context distribution.
- **What evidence would resolve it:** A comparison of nDCG@10 between TS-SetRank and uniform baselines at T=500 or T=1000.

## Limitations

- The contextual relevance assumption may not apply universally across all LLM-based reranking tasks and domains
- The computational overhead of maintaining Beta-Bernoulli posteriors and performing Thompson sampling may not justify marginal improvements when resources are abundant
- The generalizability of results to diverse real-world retrieval scenarios beyond BRIGHT and BEIR benchmarks remains uncertain

## Confidence

**High Confidence Claims:**
- TS-SetRank algorithm successfully implements Thompson sampling for batch selection
- Contextual relevance framework provides theoretical foundation for the approach
- Beta-Bernoulli update mechanism correctly implements Bayesian inference
- Experimental results on BRIGHT and BEIR datasets are reproducible

**Medium Confidence Claims:**
- 15-25% nDCG@10 improvement over baselines is statistically significant and generalizable
- Contextual relevance modeling provides consistent benefits across different domains
- Exploration-exploitation balance in Phase II is optimal for all inference budgets
- Approach scales effectively to larger document collections

**Low Confidence Claims:**
- Contextual relevance assumption applies universally to all LLM-based reranking tasks
- Beta-Bernoulli model is optimal for all types of relevance feedback
- Specific hyperparameters (T_f=25-75) are optimal across all scenarios
- Efficiency gains justify additional implementation complexity in all cases

## Next Checks

1. **Domain Transfer Validation:** Test TS-SetRank on datasets from different domains (legal, medical, technical documentation) to verify generalizability of contextual relevance effects and quantify performance variations across domains.

2. **Computational Overhead Analysis:** Conduct detailed cost-benefit analysis comparing TS-SetRank to simpler baselines (Heapify, uniform sampling) across varying computational budgets and document collection sizes to determine break-even points where additional complexity becomes justified.

3. **LLM Model Dependency Study:** Evaluate performance of TS-SetRank using different LLM models with varying capabilities and biases to determine whether contextual relevance effects are model-dependent and identify which model characteristics maximize effectiveness.