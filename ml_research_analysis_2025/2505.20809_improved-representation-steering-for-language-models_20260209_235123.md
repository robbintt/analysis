---
ver: rpa2
title: Improved Representation Steering for Language Models
arxiv_id: '2505.20809'
source_url: https://arxiv.org/abs/2505.20809
tags:
- steering
- reps
- score
- concept
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reference-free Preference Steering (RePS),
  a bidirectional preference optimization objective for improving representation steering
  in language models. RePS addresses the gap between steering interventions (like
  LoRA, ReFT, SV) and prompting by optimizing for human-preferred behaviors without
  relying on a reference model.
---

# Improved Representation Steering for Language Models

## Quick Facts
- **arXiv ID:** 2505.20809
- **Source URL:** https://arxiv.org/abs/2505.20809
- **Reference count:** 40
- **Primary result:** RePS-trained interventions outperform standard language modeling and BiPO baselines, substantially narrowing the performance gap with prompting for both steering and suppression tasks.

## Executive Summary
This paper introduces Reference-free Preference Steering (RePS), a bidirectional preference optimization objective for improving representation steering in language models. RePS addresses the gap between steering interventions (like LoRA, ReFT, SV) and prompting by optimizing for human-preferred behaviors without relying on a reference model. The method jointly optimizes concept incorporation and suppression using a reference-free reward function derived from SimPO. Evaluated on Gemma models (2B to 27B) using AXBENCH, RePS-trained interventions outperform standard language modeling and BiPO baselines, substantially narrowing the performance gap with prompting. For suppression, RePS matches language modeling objectives on Gemma-2 and outperforms them on Gemma-3 models. Critically, RePS-trained models remain robust to prompt-based jailbreaking attacks that bypass text-based defenses, demonstrating interpretability, scalability, and resilience advantages over prompting.

## Method Summary
RePS is a reference-free preference optimization objective that jointly steers and suppresses concepts in language models. It optimizes low-rank interventions (Steering Vectors, LoRA, LoReFT) by maximizing the likelihood of concept-incorporated responses over standard responses (steering) and vice versa (suppression). The method uses a factor sampling trick during training, sampling steering factors from predefined ranges to improve stability and generalization. The objective is based on SimPO but removes the reference model constraint, allowing more flexible steering behaviors. RePS is trained on preference pairs generated from the AxBench CONCEPT500 dataset, where GPT-4o-mini creates concept-steered responses for evaluation.

## Key Results
- RePS-trained Rank-1 Steering Vectors achieve 0.846-0.912 harmonic mean scores across Gemma models, outperforming language modeling (0.794-0.882) and BiPO (0.756-0.834) baselines
- For suppression, RePS matches language modeling on Gemma-2 (0.836-0.862) and outperforms on Gemma-3 (0.854-0.902), while BiPO underperforms (0.810-0.866)
- RePS-trained models maintain concept steering under adversarial jailbreak prompts that bypass system prompt defenses
- LoReFT fails catastrophically on Gemma-3 (12B/27B) with scores of 0.651/0.436, while Rank-1 SV maintains strong performance

## Why This Works (Mechanism)

### Mechanism 1: Reference-Free Preference Optimization
RePS improves steering by decoupling the steering objective from the constraints of the original language model, which inherently disfavors irregular, concept-steered behaviors. Unlike BiPO, which uses a reference model to keep the steered policy close to the original model's likelihoods, RePS uses a reference-free objective that constructs a preference loss maximizing the likelihood of a steered response against a normal response without a KL-divergence penalty to a reference model. This allows the learned intervention to make more drastic changes to the model's behavior, as it is not penalized for steering towards concepts the base model finds unlikely.

### Mechanism 2: Bidirectional, Asymmetric Training
RePS achieves both concept steering and concept suppression by using a single, joint training objective with an asymmetric parameterization for positive and negative interventions. For positive steering (ΦSteer), it maximizes the likelihood of a steered response over a normal one. For negative steering/suppression (ΦNull), it maximizes the likelihood of a normal response over a steered one. A key component for suppression is the ΦNull intervention, which is explicitly parameterized to remove a concept. For Steering Vectors, ΦNull uses a projection-based erasure technique, subtracting the component of the representation aligned with the steering direction.

### Mechanism 3: Factor Sampling for Stable Optimization
Steering effectiveness is highly sensitive to the intervention's scale, and sampling this scale (the "factor") during training leads to more stable and robust learning. Instead of training with a fixed steering factor (α), RePS samples α from a predefined set during training. This encourages the learned intervention to be effective across a range of strengths. This is crucial because the scale of residual stream activations can vary (e.g., due to layer-norm), and a vector trained at a single scale may not generalize well.

## Foundational Learning

- **Concept: Preference Optimization (DPO, SimPO)**
  - **Why needed here:** RePS is a preference optimization objective built on SimPO. Understanding the core idea of directly optimizing a policy to prefer one output over another, without a separate reward model, is essential to grasp what RePS modifies.
  - **Quick check question:** What is the primary difference between Reinforcement Learning from Human Feedback (RLHF) with PPO and Direct Preference Optimization (DPO)?

- **Concept: Low-Rank Interventions (LoRA, ReFT, Steering Vectors)**
  - **Why needed here:** RePS is a training objective that can be applied to several parameter-efficient intervention methods. Knowing how these interventions structurally alter a model's forward pass (e.g., LoRA's additive weight updates vs. ReFT's representation editing) is crucial for implementation.
  - **Quick check question:** For a given Transformer layer, how does the parameter update of a Steering Vector differ from that of a LoRA adapter?

- **Concept: Linear Representation Hypothesis**
  - **Why needed here:** The paper's mechanism for suppression (ΦNull) and its use of rank-1 steering vectors relies on the assumption that high-level concepts are encoded in linear directions in the activation space.
  - **Quick check question:** According to the linear representation hypothesis, how might a concept like "sadness" be represented in a model's residual stream?

## Architecture Onboarding

**Component map:** Data Pair Creation -> Intervention Module (Φ) -> Preference Optimizer -> Loss Aggregation

**Critical path:**
1. **Data Pair Creation:** For an instruction x, obtain a normal response y and a concept-steered response yc.
2. **Forward Pass with ΦSteer:** Pass x through the LM with a positively sampled factor α (e.g., α=10). Compute log-likelihood for both y and yc. Calculate the positive preference loss term (Δ⁺_Φ) using Eq. 5, which compares the length-normalized log-likelihoods.
3. **Forward Pass with ΦNull:** Pass x through the LM with the negatively parameterized intervention (e.g., using projection erasure or a negative factor). Compute log-likelihood for both y and yc. Calculate the negative preference loss term (Δ⁻_Φ) using Eq. 6.
4. **Loss Aggregation:** Sum the two log-sigmoid losses and backpropagate to update the parameters of the shared Intervention Module Φ.

**Design tradeoffs:**
* **Objective Simplicity vs. Reference Model:** RePS removes the reference model from BiPO, simplifying implementation and memory usage but potentially sacrificing stability for drastic steering if not well-tuned.
* **Intervention Type:** Rank-1 Steering Vectors are most parameter-efficient and interpretable but may lack the capacity for complex concepts. LoRA/ReFT are more expressive but can be less stable on larger models, as observed in the results for Gemma-3.
* **Suppression Parameterization:** ΦNull for Steering Vectors is a deterministic projection (Eq. 7). For LoRA/ReFT, it's implemented via negative factor sampling. The projection method may be more principled for linear erasure but is less flexible for non-linear representations.

**Failure signatures:**
* **Concept drift/forgetting:** The model may start generating the steering concept even when not prompted, or fail to follow the original instruction.
* **Incoherent generation:** With a very high steering factor, the model's output may become repetitive or ungrammatical. RePS is noted to mitigate this compared to language modeling, but it's still a risk.
* **Suppression failure:** The model might ignore the negative intervention and still produce the target concept, or the intervention might suppress related, desirable content.

**First 3 experiments:**
1. **Baseline Reproduction:** Implement the ΦSV intervention and train it with the standard language modeling objective and the BiPO objective on a small dataset. Verify that RePS outperforms both on a held-out steering evaluation set.
2. **Factor Ablation:** Train a RePS ΦSV model with a fixed steering factor and compare its performance and hyperparameter sensitivity to the full RePS model with factor sampling. This validates the core contribution of the sampling trick.
3. **Suppression Attack:** Train a RePS ΦSV model for a specific rule-based concept (e.g., "include hashtags"). At inference, use an adversarial prompt (e.g., from the many-shot jailbreak examples) to force the model to violate the rule. Compare the suppression success rate of the intervention versus a system prompt defense.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does LoReFT fail catastrophically on larger Gemma-3 models (scoring 0.651 on 12B and 0.436 on 27B) while rank-1 SV maintains strong performance?
- **Basis in paper:** Section 6 states "both LoRA and LoReFT underperform rank-1 SV on larger models, with LoReFT failing almost catastrophically" and Table 1 shows the stark performance drop.
- **Why unresolved:** The paper does not investigate whether this is due to optimization difficulties, rank mismatch with larger model representations, or architectural incompatibilities.
- **What evidence would resolve it:** Ablation studies varying LoReFT rank, layer placement, and training hyperparameters specifically on larger models; analysis of learned representations compared to SV.

### Open Question 2
- **Question:** Why does RePS improve over the language modeling objective for steering?
- **Basis in paper:** Section 6: "We should also pursue a deeper understanding of why RePS improves over Lang."
- **Why unresolved:** While the paper hypothesizes that preference objectives better align with instruct-tuned LM optimization, no mechanistic analysis is provided.
- **What evidence would resolve it:** Probing experiments comparing learned directions; analysis of gradient dynamics during training; comparison of representation changes induced by each objective.

### Open Question 3
- **Question:** Can bootstrapping training examples from target LMs themselves improve RePS convergence and performance?
- **Basis in paper:** Section 6: "We have not yet explored bootstrapping training examples from the target LMs themselves, which might smooth training convergence."
- **Why unresolved:** The current approach relies on GPT-4o-mini generated steered responses, which may create distribution mismatch with target model capabilities.
- **What evidence would resolve it:** Experiments using target LM self-generated steered outputs with iterative refinement; convergence speed and final performance comparisons.

## Limitations
- The evaluation relies heavily on GPT-4o-mini-generated steered responses as ground truth, which may not reflect human preferences and could introduce bias in the preference learning objective
- While RePS shows robustness to prompt-based jailbreaking, the specific attack vectors tested are not fully characterized, and effectiveness against more sophisticated adversarial prompts remains unclear
- The method's scalability to larger models shows mixed results - LoReFT interventions catastrophically failed on Gemma-3, suggesting implementation or architectural constraints

## Confidence
- **High Confidence:** The mechanism showing RePS outperforms BiPO and language modeling objectives on concept steering tasks using Steering Vectors. The empirical results demonstrate clear performance improvements with statistically significant margins.
- **Medium Confidence:** The claim that reference-free optimization is superior to reference-constrained approaches for steering. While results support this, the comparison could be confounded by implementation details of the reference model or training stability.
- **Low Confidence:** The assertion that RePS-trained interventions are more robust to prompt-based jailbreaking attacks. The paper demonstrates this on specific examples but doesn't provide comprehensive adversarial testing or explain the underlying mechanism for this robustness.

## Next Checks
1. **Ablation Study on Factor Sampling:** Implement RePS without the factor sampling trick using fixed steering factors. Compare performance and hyperparameter sensitivity to isolate whether the sampling contributes more to stability than the reference-free objective itself.

2. **Human Evaluation of Preference Quality:** Replace the GPT-4o-mini generated steered responses with human-annotated concept-incorporated responses for a subset of concepts. Retrain RePS and compare performance to assess whether the method's success depends on the quality of synthetic preference data.

3. **Adversarial Prompt Testing Suite:** Develop a comprehensive set of adversarial prompts specifically designed to bypass steering interventions (beyond the jailbreak examples provided). Test RePS-trained models against this suite and compare to baseline steering methods to rigorously evaluate robustness claims.