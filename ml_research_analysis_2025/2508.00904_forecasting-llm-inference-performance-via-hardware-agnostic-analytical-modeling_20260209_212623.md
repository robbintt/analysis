---
ver: rpa2
title: Forecasting LLM Inference Performance via Hardware-Agnostic Analytical Modeling
arxiv_id: '2508.00904'
source_url: https://arxiv.org/abs/2508.00904
tags:
- life
- memory
- performance
- compute
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LIFE, a lightweight, modular analytical framework
  for forecasting LLM inference performance in a hardware- and dataset-agnostic manner.
  LIFE uses analytical models of operators to simulate LLM workloads and predict performance
  metrics such as time-to-first-token (TTFT), time-per-output-token (TPOT), and tokens-per-second
  (TPS) based solely on hardware specifications like TOPS and memory bandwidth.
---

# Forecasting LLM Inference Performance via Hardware-Agnostic Analytical Modeling

## Quick Facts
- arXiv ID: 2508.00904
- Source URL: https://arxiv.org/abs/2508.00904
- Reference count: 40
- This paper introduces LIFE, a lightweight, modular analytical framework for forecasting LLM inference performance in a hardware- and dataset-agnostic manner.

## Executive Summary
This paper introduces LIFE, a lightweight, modular analytical framework for forecasting LLM inference performance in a hardware- and dataset-agnostic manner. LIFE uses analytical models of operators to simulate LLM workloads and predict performance metrics such as time-to-first-token (TTFT), time-per-output-token (TPOT), and tokens-per-second (TPS) based solely on hardware specifications like TOPS and memory bandwidth. The framework supports various optimizations, including quantization, KV cache compression, LoRA adapters, chunked prefill, different attention mechanisms, and operator fusion. LIFE was validated on AMD Ryzen CPUs, NPUs, iGPUs, and NVIDIA V100 GPUs with Llama2-7B variants, demonstrating accurate forecasting without requiring extensive benchmarking datasets.

## Method Summary
LIFE employs a modular analytical modeling approach where each LLM operator (GEMM, BMM, Softmax, etc.) is modeled to calculate compute operations and memory accesses based on tensor shapes and data types. These per-operator models are composed according to the LLM architecture to simulate complete inference passes. The framework accumulates total compute ops and memory bytes during prefill and decode phases separately, then applies analytical equations incorporating hardware TOPS, memory bandwidth, and operator-specific efficiency parameters to forecast performance metrics. This approach avoids hardware-specific benchmarking while maintaining generality across different hardware platforms.

## Key Results
- LIFE accurately forecasts TTFT, TPOT, and TPS for Llama2-7B on diverse hardware (CPUs, NPUs, iGPUs, GPUs) with minimal benchmarking
- The framework correctly identifies performance bottlenecks and demonstrates how optimizations like quantization and KV cache compression improve inference speed
- Analytical modeling captures the dynamic compute/memory behavior during prefill vs decode phases, showing memory-bound behavior during KV cache growth in decode phase

## Why This Works (Mechanism)
LIFE works by decomposing LLM inference into a hierarchy of analytical operator models that track compute operations and memory traffic. It leverages the roofline model concept by comparing compute time (based on TOPS) versus memory time (based on bandwidth) to determine if operations are compute-bound or memory-bound. The framework distinguishes between prefill and decode phases because they have fundamentally different characteristics - prefill is typically compute-bound while decode becomes memory-bound due to KV cache growth. By modeling KV cache size and memory footprint, LIFE captures the degradation in decode performance over time.

## Foundational Learning

- **Concept: The Roofline Model and Arithmetic Intensity**
  - Why needed here: LIFE's core forecasting logic is based on determining if an operator or phase is compute-bound or memory-bound. The paper refers to this as the `tc/tm` ratio. This is a direct application of the roofline model concept.
  - Quick check question: For a given operation, is the time it takes determined by how fast the processor can calculate (compute-bound) or by how fast it can get data from memory (memory-bound)?

- **Concept: LLM Inference Phases (Prefill vs. Decode)**
  - Why needed here: The paper explicitly models these two phases differently because their characteristics are fundamentally distinct. Prefill is typically compute-bound with large prompts, while decode is memory-bound due to the KV cache. A single model cannot accurately forecast both without this distinction.
  - Quick check question: Why does the latency to generate the 1000th token differ from the latency to generate the 1st token?

- **Concept: KV Cache and its Memory Footprint**
  - Why needed here: Understanding the KV cache is critical because its growth during the decode phase is the primary reason for TPOT degradation over time. Modeling its size (`2 * num_layers * seq_len * hidden_dim * dtype_size`) is a key part of LIFE's analytical model.
  - Quick check question: How does the memory required for each new token generation change as the total sequence length grows?

## Architecture Onboarding

- **Component map:** The LIFE framework consists of four main conceptual parts: 1) The **Configuration**, which defines the LLM architecture and optimizations to model. 2) The **Analytical LLM Model**, a hierarchy of operator models (GEMM, BMM, etc.). 3) The **Statistics Database**, which aggregates the total compute ops and memory bytes from a simulation run. 4) The **Analysis Script**, which takes the aggregated metrics and combines them with user-provided hardware specs (TOPS, BW) and efficiency factors to produce final performance forecasts.

- **Critical path:** The most critical path for a new engineer to understand is the data flow from a single operator's configuration to the final predicted TTFT/TPOT. Start with the `gemm` function in the appendix, tracing how `opcount`, `mem_rd`, and `mem_wr` are calculated from shapes. Then, understand how `simulation scripts` accumulate these for an entire model forward pass. Finally, follow how the `analysis script` uses Equations 1-6 with the totals from the statistics database to produce a time prediction.

- **Design tradeoffs:** The primary design tradeoff is **accuracy vs. generality/speed**. LIFE avoids costly, hardware-specific benchmarking by using analytical models. The gain is extreme speed (seconds to forecast on a laptop) and generality to any hardware with a TOPS/BW spec. The cost is a reliance on "efficiency" parameters that abstract away complex, real-world hardware performance characteristics. The model is not a cycle-accurate simulator.

- **Failure signatures:**
    - **Consistent over/under-prediction:** If forecasts are consistently wrong by a similar factor, the assumed `ecop` or `emop` efficiency values are incorrect.
    - **Prediction breaks at specific prompt lengths:** This could indicate the model for BMM tiling efficiency (see Section 5.4.1) is not capturing the hardware's behavior for certain tensor shapes.
    - **Optimization impact is mispredicted:** If quantization shows a different-than-expected speedup, the model for dequantization overhead may be inaccurate.

- **First 3 experiments:**
  1.  **Validate on a single operator:** Choose a GEMM operation. Manually calculate its FLOPs and memory traffic. Compare with LIFE's output in the statistics database for a single-op simulation to ensure the analytical model is understood.
  2.  **Calibrate on known hardware:** Run a real LLM benchmark on a known GPU (e.g., a local or cloud instance). Measure its TTFT and TPS. Then, run LIFE's forecast for the same model/hardware and adjust the `ecop` and `emop` efficiency parameters until the forecast matches. This creates a calibrated profile for that hardware.
  3.  **Forecast for a new optimization:** Use the calibrated hardware profile to forecast the performance of a new model variant (e.g., applying 4-bit quantization). Compare the relative speedup predicted by LIFE against the actual measured speedup from a new benchmark run. This tests the accuracy of the optimization modeling.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the LIFE framework be extended to accurately forecast performance for complex architectures such as Mixture-of-Experts (MoEs), Vision Language Models (VLMs), and inference strategies like Speculative Decoding?
  - Basis in paper: [explicit] The conclusion explicitly states, "extending this to Vision Language Models (VLMs), Mixture-of-Experts (MoEs) and Speculative Decoding is left for future exploration."
  - Why unresolved: The current study validates the framework exclusively on dense Llama2-7B variants. MoEs introduce dynamic routing and expert activation, while VLMs involve distinct vision encoder operators, which require new analytical models.
  - What evidence would resolve it: Successful validation of LIFE forecasts on MoE (e.g., DeepSeek-V3) and VLM workloads, demonstrating error rates comparable to the dense model results.

- **Open Question 2:** To what extent can the framework predict performance on novel hardware architectures without requiring preliminary empirical calibration of operator efficiency?
  - Basis in paper: [inferred] The methodology section notes that LIFE "expects compute and memory efficiency of operator for specific shapes... measured using unit tests," implying the model relies on calibration inputs rather than predicting efficiency from first principles.
  - Why unresolved: The framework appears to rely on extrapolating from known efficiency data points. It is unclear if it can accurately predict performance for hardware with significantly different cache hierarchies or parallelism strategies without prior measurement.
  - What evidence would resolve it: A demonstration of LIFE forecasting performance on a new, uncharacterized hardware accelerator using only theoretical TOPS and bandwidth specifications, without running unit-test calibration first.

- **Open Question 3:** How accurately does the analytical model capture the latency variability of dynamic shape padding across different hardware accelerators?
  - Basis in paper: [inferred] The paper mentions that "BMM is often performed by padding the inputs to closest supported tiled implementation" and acknowledges efficiency drops due to under-utilization (Fig. 8), but generalizes this behavior.
  - Why unresolved: While LIFE models padding, the specific tiling constraints and efficiency penalties vary drastically between hardware (e.g., NVIDIA GPUs vs. specialized NPUs). A generalized model may miss corner-case performance regressions specific to a chip's ISA.
  - What evidence would resolve it: A detailed error analysis of decode-phase forecasts (where dynamic padding occurs) across a wider variety of accelerators with divergent tiling requirements.

## Limitations

- The framework's accuracy depends on empirical calibration of operator efficiency parameters, which may vary significantly across different hardware platforms and generations
- The analytical models for complex operators (non-linear activations, LayerNorm, RoPE) may not capture hardware-specific implementations accurately
- The chunked prefill optimization modeling assumes perfect load balancing across chunks, which may not hold for irregular attention patterns

## Confidence

- **High Confidence:** The fundamental analytical approach based on compute ops vs memory bytes, the distinction between prefill and decode phases, and the roofline-inspired tc/tm ratio for determining bottlenecks are theoretically sound and well-established in computer architecture. The modular operator composition framework is also well-designed.
- **Medium Confidence:** The specific numerical results in Tables 6 and 10 are reproducible given the same efficiency parameters, but the generalizability of these parameters across different hardware generations and architectures is uncertain. The framework's ability to forecast relative performance changes (e.g., impact of quantization) appears more reliable than absolute performance numbers.
- **Low Confidence:** Claims about LIFE's ability to accurately forecast performance on completely unseen hardware configurations without any calibration are not well-supported. The paper demonstrates accuracy on specific hardware (AMD, NVIDIA) but does not validate the framework's zero-shot generalization capability across diverse hardware architectures.

## Next Checks

1. **Cross-Platform Efficiency Calibration Study:** Measure operator efficiencies (ec_op, em_op) for the same LLM operators across 3-4 different hardware platforms (e.g., ARM CPU, Intel CPU, NVIDIA GPU, AMD GPU). Quantify how much these efficiency values vary and assess whether a single set of parameters can reasonably approximate performance across platforms, or if hardware-specific calibration is always required.

2. **Optimization Impact Validation:** Select a specific optimization (e.g., 4-bit quantization) and measure its actual performance impact on a target hardware platform. Compare LIFE's predicted speedup against the measured speedup, and investigate whether discrepancies arise from incorrect modeling of the optimization's computational characteristics or from hardware-specific effects not captured in the efficiency parameters.

3. **Memory-Bound Regime Stress Test:** Design experiments that deliberately stress the memory subsystem (e.g., very long sequences, high batch sizes) and verify that LIFE correctly predicts the transition from compute-bound to memory-bound behavior as sequence length increases. Validate that the KV cache growth modeling accurately predicts the TPOT degradation pattern shown in Figure 7 across different hardware configurations.