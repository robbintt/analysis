---
ver: rpa2
title: 'Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping
  Games'
arxiv_id: '2510.14030'
source_url: https://arxiv.org/abs/2510.14030
tags:
- game
- group
- word
- groupings
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces GLOBALGROUP, a multilingual abstract reasoning
  benchmark inspired by the NYT Connections game. It evaluates LLMs across five languages
  (English, Spanish, Chinese, Hindi, Arabic) using 4-word, 4-group word grouping games
  with culture-related and non-culture-related topics.
---

# Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games

## Quick Facts
- arXiv ID: 2510.14030
- Source URL: https://arxiv.org/abs/2510.14030
- Reference count: 25
- This work introduces GLOBALGROUP, a multilingual abstract reasoning benchmark inspired by the NYT Connections game

## Executive Summary
This paper introduces GLOBALGROUP, a multilingual benchmark for evaluating abstract reasoning in LLMs using word grouping games across five languages. The benchmark tests models on 4-word, 4-group games with culture-related and non-culture-related topics, revealing that English modality consistently yields better performance regardless of the original language. Smaller multilingual-focused models like Aya-8B perform comparably to larger closed-source models, demonstrating the value of multilingual training. The study identifies group count, semantic similarity (ARI), and word overlap as effective difficulty metrics, and shows that non-culture-related topics improve performance across all languages.

## Method Summary
The researchers constructed a multilingual word grouping benchmark inspired by NYT Connections, creating games with 16 words divided into 4 groups of 4 words each. They developed games across five languages (English, Spanish, Chinese, Hindi, Arabic) with both culture-related and non-culture-related topics. Models were evaluated using group-level F1 scores and topic achievement metrics, with responses translated to English for comparison. The benchmark includes difficulty stratification using Adjusted Rand Index for semantic similarity and word overlap calculations. Three annotators created the games to ensure single-solution constraints, and FastText embeddings with a 0.3 similarity threshold were used for topic evaluation.

## Key Results
- English modality consistently leads to better performance in abstract reasoning tasks
- Smaller open-source multilingual models like Aya-8B perform comparably to larger closed-source models
- Group count, within-group semantic similarity (ARI), and between-group word overlap serve as effective difficulty metrics
- Performance improves with non-culture-related topics across all languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models perform better on abstract reasoning tasks when content is presented in English, even for identical semantic content.
- Mechanism: LLMs develop richer, more robust representations for English during pretraining due to disproportionate English text exposure. When non-English input is translated to English, these stronger representations enable better pattern recognition and group formation, suggesting the bottleneck is representation quality rather than reasoning capacity.
- Core assumption: Performance gaps reflect representation quality differences rather than fundamental reasoning inability.
- Evidence anchors:
  - [abstract] "English modalities largely lead to better performance in this abstract reasoning task"
  - [section 5, Table 2] ES-EN F1 improves from 0.828 to 0.855 (GPT-3.5); HI-EN improves from 0.886 to 0.948
  - [corpus] Related work on multilingual reasoning (Etxaniz et al., 2024) confirms English-thinking patterns in multilingual models
- Break condition: If models trained with balanced multilingual data show no English advantage, the mechanism shifts from representation bias to training data distribution.

### Mechanism 2
- Claim: Multilingual-focused training enables smaller models to match larger models on non-English reasoning tasks.
- Mechanism: Explicit multilingual pretraining/finetuning creates cross-lingual transfer capabilities where reasoning skills learned in one language transfer to others. This compensates for parameter count disadvantages by specializing representations for multilingual abstraction.
- Core assumption: Reasoning capabilities can transfer across languages when representations are aligned during training.
- Evidence anchors:
  - [abstract] "smaller open-source models benefiting from multilingual training"
  - [section 5] "Aya-8B is able to attain comparable results as the far larger LLMs, despite having a substantially smaller size"
  - [corpus] Aya-23 paper (Aryabumi et al., 2024) provides training methodology details
- Break condition: If scaling laws dominate and larger monolingual models consistently outperform smaller multilingual ones, the mechanism is parameter count, not training paradigm.

### Mechanism 3
- Claim: Three game features—group count, within-group semantic similarity (ARI), and between-group word overlap—predict model performance.
- Mechanism: Group count increases combinatorial complexity; low ARI indicates groups require non-semantic (abstract) connections; word overlap creates ambiguity requiring disambiguation reasoning. Together they capture cognitive load factors that scale reasoning difficulty.
- Core assumption: These metrics capture the same difficulty dimensions humans experience.
- Evidence anchors:
  - [section 4.2] Explicit difficulty metric definitions and hypotheses
  - [section 6, Table 4] Perfect correlation (1.0) for group count; 0.9 correlation for ARI; -0.8 correlation for word overlap with GPT-4
  - [corpus] Limited direct corpus evidence; difficulty metrics appear novel to this benchmark
- Break condition: If metrics correlate poorly with human difficulty ratings or fail to generalize to other abstract reasoning tasks, they may be task-specific artifacts.

## Foundational Learning

- **Abstract vs. Knowledge-Based Reasoning**
  - Why needed here: GlobalGroup specifically tests reasoning without formulaic approaches—understanding this distinction is critical for interpreting results as reasoning capability rather than knowledge retrieval.
  - Quick check question: Can you explain why a math problem is *not* an abstract reasoning task by this paper's definition?

- **Word Embedding Similarity (ARI, Cosine)**
  - Why needed here: The Adjusted Rand Index metric compares semantic clustering to ground-truth groups to quantify how "non-obvious" groupings are.
  - Quick check question: If ARI between semantic clusters and ground truth is 0.0, what does that imply about the group connections?

- **Cross-Lingual Transfer in LLMs**
  - Why needed here: Understanding why translation helps requires grasping how representations are shared (or not) across languages in transformer models.
  - Quick check question: Why might a model perform better on Arabic content presented in English translation despite the semantic equivalence?

## Architecture Onboarding

- **Component map:**
  Game Construction: Language-specific grouping datasets → Random sampling → Game instances (with translations)
  Evaluation: Model → Predicted groups → Group-matching algorithm → F1 + Topic Achieved scores
  Difficulty: Game features → ARI/Overlap calculation → Difficulty bins → Controlled comparison

- **Critical path:** Game construction (ensuring single-solution games) → Prompt engineering → Response parsing (handling format variations) → Metric calculation (F1 for groups, FastText similarity for topics with 0.3 threshold)

- **Design tradeoffs:**
  - F1 scoring vs. exact-match: F1 allows partial credit (granular evaluation) vs. binary correctness
  - FastText vs. BERTScore for topics: FastText with 0.3 threshold achieved 0.55 Kappa (better than BERTScore's 0.53)
  - Penalizing extra groups: Paper chose NOT to penalize to accommodate model reasoning explanations

- **Failure signatures:**
  - Models output partial groups or repeat groups in "reasoning" text
  - Topic predictions semantically correct but lexically different (FastText handles this)
  - Non-Latin scripts (AR, HI, ZH) may have embedding quality issues affecting ARI calculation
  - Smaller models (7-8B) without multilingual training show severe non-English degradation

- **First 3 experiments:**
  1. Replicate baseline on 4×4 games for 2-3 model sizes to validate English-advantage pattern
  2. Stratify results by difficulty metric bins to confirm ARI/overlap correlations
  3. Test whether multilingual model (Aya-8B) shows reduced English-translation advantage compared to monolingual-trained models of similar size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance on abstract reasoning tasks generalize to low-resource languages not covered in the current benchmark?
- Basis in paper: [explicit] The authors state in Section 8, "In this paper, we only cover five languages... We encourage community members to add to our dataset using our explicit annotator request."
- Why unresolved: The study is limited to five languages (English, Spanish, Chinese, Hindi, Arabic), leaving thousands of linguistic structures untested.
- What evidence would resolve it: Extending the GlobalGroup dataset to include low-resource languages and evaluating existing multilingual models on these new subsets.

### Open Question 2
- Question: To what extent does annotator demographic homogeneity introduce cultural bias into the ground-truth groupings?
- Basis in paper: [explicit] Section 8 notes, "Since our annotators do not reflect the complete demographics of speakers of a given language, some form of bias may have been introduced."
- Why unresolved: The authors acknowledge that "culture is a complex term" and their approximation of cultural-relatedness may not capture the diversity within language communities.
- What evidence would resolve it: A study comparing model performance and inter-annotator agreement across diverse demographic groups for the same languages.

### Open Question 3
- Question: Does increasing model size yield diminishing returns for multilingual abstract reasoning compared to targeted multilingual training?
- Basis in paper: [inferred] The results show the multilingual-focused Aya-8B performs on par with larger models like Llama3.1-70B in specific contexts, suggesting a trade-off between scale and training focus.
- Why unresolved: While the paper identifies the value of multilingual training, it does not isolate whether this is more efficient than simple scaling for closing the performance gap with closed-source models.
- What evidence would resolve it: Controlled experiments varying model size and multilingual data ratios to identify the optimal training paradigm for non-English abstract reasoning.

## Limitations

- English advantage may reflect training data distribution bias rather than fundamental reasoning capability differences
- Evaluation relies on limited model selection (GPT-4, GPT-3.5, Aya-8B) limiting generalizability across architectures
- Difficulty metrics show strong correlations but lack external validation against human difficulty judgments
- Non-Latin script handling may have embedding quality issues affecting cross-linguistic comparisons

## Confidence

- **High Confidence**: English modality consistently outperforms non-English (systematic pattern across models and languages)
- **Medium Confidence**: Aya-8B matching larger models is due to multilingual training (limited to single model comparison)
- **Medium Confidence**: Difficulty metrics predict performance (strong correlations but no external validation)
- **Low Confidence**: No penalty for extra groups doesn't affect results (untested alternative design choice)

## Next Checks

1. **External Validation of Difficulty Metrics**: Compare ARI and word overlap metrics against human-rated difficulty scores for the same games, controlling for language and topic type to verify they capture true cognitive load rather than language-specific artifacts.

2. **Cross-Model Generalization Test**: Evaluate additional multilingual models with different training paradigms (e.g., BLOOMZ, NLLB) and monolingual models with varying pretraining data distributions to determine if English advantage correlates with English corpus proportion rather than model size or architecture.

3. **Extended Language Coverage**: Add languages with different script families and typological features (e.g., Japanese, Russian, Swahili) to test whether the English advantage pattern holds across broader linguistic diversity and whether script-based processing differences affect abstract reasoning performance.