---
ver: rpa2
title: 'Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective
  Feedback'
arxiv_id: '2508.12338'
source_url: https://arxiv.org/abs/2508.12338
tags:
- uni00000013
- uni00000011
- uni00000018
- rlccf
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning capabilities
  of large language models (LLMs) without relying on expensive human-labeled data
  or complex reward models. The proposed method, Reinforcement Learning from Coevolutionary
  Collective Feedback (RLCCF), enables multi-model collaborative evolution by maximizing
  Collective Consistency through SC-weighted majority voting for pseudo-label generation.
---

# Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback

## Quick Facts
- arXiv ID: 2508.12338
- Source URL: https://arxiv.org/abs/2508.12338
- Reference count: 9
- Primary result: Achieves 16.72% average relative improvement in mathematical reasoning accuracy across four LLMs

## Executive Summary
This paper addresses the challenge of improving LLM reasoning capabilities without expensive human-labeled data by proposing Reinforcement Learning from Coevolutionary Collective Feedback (RLCCF). The method leverages multiple diverse LLMs to collaboratively generate more accurate pseudo-labels through SC-weighted majority voting, then uses these collective signals for reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate significant performance gains, with the framework uniquely extending collective capability boundaries by improving group majority-voting accuracy by 4.51%.

## Method Summary
RLCCF implements multi-model collaborative evolution where diverse LLMs generate pseudo-labels through SC-weighted majority voting. Each model generates K=16 outputs per query, with votes weighted by Self-Consistency scores that proxy for confidence. The consensus answer serves as a shared reward signal for GRPO-based policy updates. Four distinct LLMs (Qwen2.5-7B, GLM-4-9B, InternLM3-8b-Instruct, LLaMA-3.1-8B-Instruct) are trained on MATH-700 dataset for 3 epochs using 2×8 NVIDIA A800 80GB GPUs.

## Key Results
- 16.72% average relative improvement in individual model accuracy across four mathematical reasoning benchmarks
- 4.51% improvement in group majority-voting accuracy, uniquely extending collective capability boundaries
- Demonstrated robust and stable gains compared to baseline single-model approaches that suffered from training instability
- SC-weighted voting consistently outperformed simple majority voting across all tested models and benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Aggregating outputs from diverse models reduces individual bias through independent error cancellation. The framework theoretically models individual outputs as ground truth plus model-specific bias, where diverse ensemble biases average toward zero. This requires sufficiently different architectures/training data to ensure uncorrelated biases.

### Mechanism 2
Weighting model votes by Self-Consistency scores improves pseudo-label reliability. Models producing the same answer repeatedly across samples receive higher voting weights, with empirical evidence showing up to 0.98 correlation between SC scores and actual accuracy.

### Mechanism 3
Using collective pseudo-labels as shared reward signals prevents training collapse seen in single-model self-training. External group consensus constrains policy updates to align with group agreement rather than individual model's potentially drifting correctness assessment.

## Foundational Learning

- **Self-Consistency (SC)**: Measures how often a model produces the same answer across multiple samples of the same question. Why needed: Serves as dynamic weighting mechanism for voting system. Quick check: If a model outputs three different answers for three samples of the same question, what is its Self-Consistency score?

- **Group Relative Policy Optimization (GRPO)**: Reinforcement learning algorithm that calculates advantages relative to a group of samples. Why needed: Used for actual policy updates in the framework. Quick check: How does GRPO differ from standard PPO regarding the need for a separate value function model?

- **Model Collapse / Reward Hacking**: Failure mode where models trained solely on their own outputs amplify errors. Why needed: Primary failure mode RLCCF claims to solve. Quick check: What happens to a model trained solely on its own generated outputs without external ground truth?

## Architecture Onboarding

- **Component map**: Ensemble Sampler -> SC Calculator -> Consensus Engine -> Reward Generator -> Policy Updater
- **Critical path**: Consensus Engine is the bottleneck. If SC-weighted vote selects wrong answer, reward signal actively harms models. System relies on "Wisdom of the Crowd" effect holding true for specific dataset.
- **Design tradeoffs**: High inference cost (N×K samples per training step), latency vs. diversity (distinct architectures required for bias independence), serving complexity compared to single-model approaches.
- **Failure signatures**: Homogeneity (models converge to same wrong answer), Weight Domination (one consistently wrong model hijacks vote), Stagnation (pseudo-label accuracy fails to improve).
- **First 3 experiments**: 1) Diversity Validation: Compare identical vs. diverse models to confirm diversity requirement. 2) SC-Accuracy Correlation: Plot SC scores vs. ground truth accuracy to verify SC as valid proxy. 3) Ablation on Voting: Compare Simple Majority vs. SC-Weighted voting on small subset.

## Open Questions the Paper Calls Out

### Open Question 1
Can RLCCF be generalized to open-ended generation tasks where correctness cannot be determined via exact answer matching? The current framework relies on extracting exact answers for SC-weighted voting, which works for math but is difficult for free-form text. What evidence would resolve it: Successful application to MT-Bench or creative writing tasks using semantic embeddings or LLM-judges for voting.

### Open Question 2
How does the framework perform when theoretical assumption of zero-mean independent bias across models is violated due to high model homogeneity? Section 3.3 derives effectiveness assuming biases are independent and average to zero, but experiments used only four distinct model families. What evidence would resolve it: Experiments comparing distinct architectures versus same model with different random seeds.

### Open Question 3
Does coevolutionary process cause models to converge toward homogenized output distribution, reducing diversity essential for "Wisdom of the Crowd" effect? While convergence maximizes Collective Consistency, it might eliminate complementary error correction required to surpass strongest individual model. What evidence would resolve it: Measurement of inter-model diversity metrics and error stratification across training process.

## Limitations
- Theoretical bias cancellation relies on strict assumptions about ensemble diversity that may not hold in practice
- SC-weighted voting effectiveness depends on SC-accuracy correlation holding true in target domain
- Framework requires generating N×K samples per training step, creating significant inference costs

## Confidence
- **High confidence**: Empirical performance gains (16.72% improvement, 4.51% collective capability extension) well-documented across multiple benchmarks
- **Medium confidence**: Theoretical model of bias cancellation through diversity aggregation mathematically sound but makes strong assumptions
- **Low confidence**: Scalability analysis limited - framework inference costs and performance degradation at higher task difficulty not fully explored

## Next Checks
1. **Ensemble diversity stress test**: Systematically vary model similarity to quantify minimum diversity threshold required for bias-cancellation mechanism
2. **SC-generalization validation**: Measure SC-weighted voting performance vs. simple majority across different domains to verify correlation claim
3. **Cost-benefit scaling analysis**: Measure marginal returns of increasing K and N to identify diminishing returns and quantify practical inference constraints