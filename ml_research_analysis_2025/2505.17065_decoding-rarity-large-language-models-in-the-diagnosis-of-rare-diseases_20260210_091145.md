---
ver: rpa2
title: 'Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases'
arxiv_id: '2505.17065'
source_url: https://arxiv.org/abs/2505.17065
tags:
- rare
- disease
- diseases
- data
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews the application of large language models (LLMs)
  to rare disease diagnosis, highlighting their potential to transform research and
  clinical practice. Current studies predominantly employ textual data, using LLMs
  for tasks like extracting medical information, simulating patient interactions,
  and supporting differential diagnosis.
---

# Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases

## Quick Facts
- arXiv ID: 2505.17065
- Source URL: https://arxiv.org/abs/2505.17065
- Reference count: 40
- Primary result: Systematic survey of LLM applications in rare disease diagnosis, highlighting challenges and future directions for multimodal integration

## Executive Summary
This survey systematically reviews the application of large language models to rare disease diagnosis, identifying their potential to transform research and clinical practice while acknowledging significant limitations. Current studies predominantly use textual data for tasks like medical information extraction, patient interaction simulation, and differential diagnosis support. The authors present an experimental evaluation using multiple LLMs and structured questionnaires for diagnostic purposes, while emphasizing that challenges including data scarcity, model hallucinations, limited generalization, and lack of explainability hinder safe deployment. The paper calls for multimodal data integration—combining genomic, imaging, and EHR data—to fully realize LLMs' potential in personalized medicine for rare diseases.

## Method Summary
The authors conducted a systematic literature review following PRISMA guidelines, searching Google Scholar for papers from 2022-2024 using keywords "LLM", "Rare Disease", "Questionnaires", and "Data Analysis". They identified 75 initial articles and included 19 after screening and eligibility assessment. The survey classifies approaches by disease focus, objective, input modality, model type, access type, and pipeline role. An experimental evaluation was conducted using closed-source models (GPT-4, ChatGPT) with zero-shot/instruction-based prompting on structured questionnaires, though specific details about questionnaire content and evaluation methodology remain unspecified.

## Key Results
- Current studies predominantly employ textual data for rare disease tasks, with most using simple zero-shot prompts
- LLMs face significant challenges including data scarcity, hallucinations, limited generalization, and lack of explainability
- No current studies integrate LLMs into clinical pipelines; all reviewed work uses standalone models
- The authors identify multimodal data integration (genomic, imaging, EHR) as a promising but unmet need

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Knowledge Synthesis from Biomedical Corpora
- Claim: LLMs pre-trained on extensive biomedical corpora can surface patterns across fragmented rare disease knowledge sources that remain siloed in traditional systems.
- Mechanism: Pre-training exposes models to diverse biomedical literature, case reports, and clinical notes, enabling pattern recognition across symptom-disease associations that individual clinicians may rarely encounter.
- Core assumption: Statistical regularities learned from biomedical text generalize to clinically valid diagnostic associations for rare conditions.
- Evidence anchors: Abstract notes models "demonstrate remarkable ability to understand, generate, and extract insights from unstructured clinical text"; section 3 discusses domain-specific pre-training on biomedical literature and phenotype ontologies.

### Mechanism 2: Retrieval-Augmented Generation (RAG) for Knowledge Grounding
- Claim: RAG frameworks can constrain LLM outputs to verified medical knowledge, reducing hallucination risk in data-scarce rare disease contexts.
- Mechanism: External retrieval from curated rare disease knowledge bases (Orphanet, HPO, OMIM) dynamically supplies context at query time, grounding generation in validated associations rather than relying solely on parametric memory.
- Core assumption: Retrieved knowledge bases are sufficiently complete and current for the target rare diseases.
- Evidence anchors: Abstract mentions "retrieval-augmented generation" as a future perspective; section 6 discusses integrating RAG to ensure outputs are grounded in verified medical knowledge.

### Mechanism 3: Structured Prompting for Phenotype-Diagnosis Mapping
- Claim: Role-based and chain-of-thought prompting strategies can guide LLMs to systematically analyze patient phenotypes and suggest differential diagnoses.
- Mechanism: Explicit prompting instructions (e.g., "analyze symptoms systematically," "consider rare conditions") bias model attention toward clinical reasoning patterns, while few-shot examples provide in-context learning signals for rare disease presentations.
- Core assumption: Prompt engineering can compensate for limited fine-tuning data on rare conditions.
- Evidence anchors: Section 3 describes prompting techniques like role-based prompts and chain-of-thought instructions; notes most studies used simple zero-shot prompts.

## Foundational Learning

- **Human Phenotype Ontology (HPO)**
  - Why needed here: HPO provides standardized phenotypic terminology essential for grounding LLM outputs in interoperable clinical language; the paper explicitly references HPO for phenotype extraction and classification tasks.
  - Quick check question: Can you explain why a standardized phenotype vocabulary is necessary before attempting LLM-based rare disease diagnosis?

- **Hallucination in Generative Models**
  - Why needed here: The paper identifies hallucination as a primary risk in rare disease contexts where training data is sparse; understanding this failure mode is essential for designing safeguards.
  - Quick check question: What distinguishes a confident hallucination from a valid low-probability diagnosis suggestion?

- **Diagnostic Odyssey**
  - Why needed here: The paper frames rare disease diagnosis as a prolonged journey averaging years with multiple misdiagnoses; this clinical reality defines the problem scope and acceptable latency for AI interventions.
  - Quick check question: Why does the diagnostic odyssey problem particularly benefit from AI approaches compared to common disease diagnosis?

## Architecture Onboarding

- Component map: Input data (clinical text, questionnaires) → Prompt construction → LLM inference → RAG retrieval (if applicable) → Output generation → Clinician review
- Critical path: Input data → Prompt construction → LLM inference → RAG retrieval (if applicable) → Output generation → Clinician review. The paper emphasizes that no current studies integrate LLMs into clinical pipelines; all reviewed work uses standalone models.
- Design tradeoffs:
  - General-purpose LLMs (GPT-4) vs. domain-specific fine-tuning: Paper notes specialized medical LLMs show improved performance but still suffer from insufficient rare disease coverage
  - Standalone vs. integrated pipelines: All reviewed studies use standalone models; integration with clinical workflows remains unrealized
  - Zero-shot vs. few-shot prompting: Zero-shot simpler but less reliable; few-shot improves accuracy but requires curated examples scarce for rare diseases
- Failure signatures:
  - Hallucinated disease associations with high confidence scores
  - Demographic bias amplification (underrepresentation in training data propagates to outputs)
  - Subtle clinical nuances missed due to non-specific symptom overlap with common conditions
  - Overfitting when fine-tuning on small rare disease datasets
- First 3 experiments:
  1. Benchmark baseline performance: Test multiple LLMs (GPT-4, domain-specific models) on RareDis or similar dataset using standardized questionnaires; measure diagnostic accuracy against ground truth
  2. RAG grounding evaluation: Compare standalone LLM outputs against RAG-augmented outputs using Orphanet/HPO retrieval; quantify hallucination reduction and accuracy improvement
  3. Prompting strategy ablation: Systematically test zero-shot, few-shot, and chain-of-thought prompting on a held-out set of rare disease cases; identify which strategy best handles data scarcity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively adapted to integrate multimodal data (genomic sequences, medical imaging, and electronic health records) for comprehensive rare disease diagnosis?
- Basis in paper: The authors state: "the potential for multimodal data integration—combining genetic, imaging, and electronic health records—stands as a promising frontier" and "Such integration now represents an unmet need for LLMs' full potential to offer thorough insights for diagnosis and therapy."
- Why unresolved: Current studies predominantly employ textual data only; all reviewed specific-disease studies in Table 1 are monomodal. Technical challenges include lack of sizable, harmonized multimodal datasets and computational requirements.
- What evidence would resolve it: Development and validation of multimodal LLM frameworks demonstrating improved diagnostic accuracy over text-only approaches on standardized rare disease benchmarks.

### Open Question 2
- Question: Can LLM-generated synthetic patient cases be validated as clinically plausible and effective for training downstream diagnostic models without propagating bias?
- Basis in paper: "Synthetic datasets must be rigorously evaluated for realism, diversity, and potential bias propagation. Furthermore, governance frameworks should be developed to delineate appropriate use cases for synthetic data."
- Why unresolved: No systematic methodology exists for validating synthetic rare disease data; risk exists that augmented datasets could reinforce diagnostic inaccuracies or demographic disparities.
- What evidence would resolve it: Benchmarks measuring clinical plausibility (expert evaluation), diversity metrics, and downstream model performance comparing synthetic vs. real data training.

### Open Question 3
- Question: What RAG architectures can most effectively ground LLM outputs in curated rare disease knowledge bases to minimize hallucinations while maintaining diagnostic utility?
- Basis in paper: "Future LLM-based augmentation systems must closely couple generation with retrieval from curated rare disease knowledge bases, dynamically constraining outputs to maintain clinical accuracy."
- Why unresolved: RAG approaches for rare diseases remain largely unexplored; standard RAG may fail given sparse knowledge base coverage for many rare conditions.
- What evidence would resolve it: Comparative studies measuring hallucination rates and diagnostic accuracy across different RAG configurations using rare disease ontologies (ORDO, HPO, Orphanet) as retrieval sources.

## Limitations
- Experimental evaluation lacks methodological transparency with unspecified questionnaire content, prompting strategies, and evaluation rubrics
- Current studies predominantly use textual data only, limiting real-world applicability for comprehensive rare disease diagnosis
- No studies integrate LLMs into clinical workflows, remaining at proof-of-concept stage with standalone models

## Confidence
- **High Confidence**: Claims about data scarcity challenges in rare disease contexts, the prevalence of textual data as input modality, and the identification of hallucination as a primary risk are well-supported by the literature review and align with broader LLM research.
- **Medium Confidence**: The assertion that retrieval-augmented generation can meaningfully reduce hallucination risk is plausible based on theoretical grounding but lacks direct experimental validation in the rare disease domain within this work.
- **Low Confidence**: The experimental evaluation's conclusion that LLMs demonstrate "promising" diagnostic capabilities is limited by the lack of standardized benchmarks, unclear evaluation methodology, and absence of comparison to existing clinical tools.

## Next Checks
1. Replicate the literature search with specified keywords and date range, then classify included studies using the paper's framework to verify coverage and classification accuracy.
2. Construct a standardized rare disease questionnaire dataset using multiple knowledge bases, then systematically test multiple LLMs with varying prompt strategies while documenting response consistency and hallucination rates.
3. Design an ablation study comparing standalone LLM outputs against RAG-augmented versions using the same knowledge bases cited in the paper, measuring hallucination frequency and diagnostic accuracy improvements.