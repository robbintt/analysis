---
ver: rpa2
title: 'Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit
  Regularization'
arxiv_id: '2509.17251'
source_url: https://arxiv.org/abs/2509.17251
tags:
- regression
- ridge
- have
- bound
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares the finite-sample risks of three algorithms
  for well-specified linear regression: gradient descent (GD), ridge regression, and
  stochastic gradient descent (SGD). The analysis yields three key findings.'
---

# Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization

## Quick Facts
- **arXiv ID:** 2509.17251
- **Source URL:** https://arxiv.org/abs/2509.17251
- **Reference count:** 40
- **Primary result:** Gradient descent (GD) dominates ridge regression in finite-sample risk, while GD and SGD are incomparable across problem classes.

## Executive Summary
This paper establishes a rigorous finite-sample risk comparison between gradient descent (GD), ridge regression, and stochastic gradient descent (SGD) for well-specified linear regression. The authors prove that GD implicitly regularizes like ridge regression but with superior statistical performance—GD always achieves the minimax rate while ridge can be polynomially worse when optimally tuned. Surprisingly, GD and SGD are shown to be incomparable: there exist problems where GD is exponentially better and others where SGD dominates. The analysis reveals that GD's implicit bias makes it minimax optimal for problems satisfying standard capacity conditions, while ridge and SGD are only partially minimax optimal.

## Method Summary
The authors analyze three algorithms for linear regression on synthetic data with Gaussian covariates and Gaussian noise. Ridge regression uses a closed-form solution with explicit regularization parameter λ, while GD uses iterative updates with fixed stepsize η starting from zero initialization. SGD employs a one-pass approach with exponentially decaying stepsizes. The analysis focuses on two regimes: power law covariance spectra with source conditions on the true parameter, and specific hard examples demonstrating SGD's superiority. Theoretical bounds are derived for each algorithm's excess risk, with GD's bound being novel. The results are validated through comparison of upper bounds for GD against existing bounds for ridge and SGD.

## Key Results
- GD dominates ridge regression: GD's excess risk is always within a constant factor of ridge, but ridge can be polynomially worse even with optimal tuning.
- GD is incomparable with SGD: There exist problems where GD is polynomially better and others where SGD is polynomially better.
- GD dominates SGD for problems with fast and continuously decaying covariance spectra satisfying standard capacity conditions.
- When applied to problems under capacity and source conditions, GD is always minimax optimal while ridge and SGD are only partially minimax optimal.

## Why This Works (Mechanism)
The paper's theoretical framework precisely characterizes how implicit regularization through GD's initialization and optimization dynamics compares to explicit regularization in ridge and the sampling noise in SGD. By deriving tight upper bounds for GD and comparing them with existing bounds for the other methods, the authors can provably determine when one method dominates another in finite-sample settings.

## Foundational Learning
- **Source conditions (r):** Characterize the smoothness of the true parameter in the eigenspace of the covariance matrix. Why needed: Determines the difficulty of the regression problem and the achievable convergence rates. Quick check: Verify that the true parameter satisfies the decay rate w_i* ∝ i^(-ra-0.5) for power law spectra.
- **Minimax optimality:** The best possible performance any algorithm can achieve on a given problem class. Why needed: Establishes the fundamental limit for statistical estimation and benchmarks algorithm performance. Quick check: Confirm that GD's risk matches the lower bound n^(-2ar/(1+2ar)) for source condition problems.
- **Implicit regularization:** The phenomenon where optimization algorithms like GD with zero initialization implicitly constrain solutions toward simpler models. Why needed: Explains why GD can outperform ridge despite having no explicit regularization parameter. Quick check: Observe that GD's excess risk scales as n^(-2ar/(1+2ar)) regardless of the covariance spectrum's decay rate.

## Architecture Onboarding
- **Component map:** Data generation -> Algorithm implementation -> Risk computation -> Bound comparison
- **Critical path:** The theoretical analysis relies on precise characterization of eigenvalue spectra and source conditions, followed by derivation of tight risk bounds for each algorithm.
- **Design tradeoffs:** The paper sacrifices generality (focusing on specific spectra) for precision (tight finite-sample bounds), enabling rigorous dominance relationships between methods.
- **Failure signatures:** If empirical results don't match theory, check whether the problem satisfies the capacity conditions or whether finite-sample constants affect the observed gap between algorithms.
- **First experiments:** 1) Implement power law spectrum data generation with varying exponents a and r. 2) Plot excess risk vs sample size for all three algorithms to verify GD's minimax optimality. 3) Construct the hard example from Theorem 4.2 to demonstrate SGD's potential superiority.

## Open Questions the Paper Calls Out
- What are the optimal algorithms for noiseless or low-noise linear regression? The paper shows SGD can be polynomially better than GD/OLS in noiseless settings, but does not establish if SGD is the optimal method for this specific regime.
- Do the risk comparison results hold under general subgaussian designs without requiring independent entries? The authors adopt the restrictive requirement of independent entries for the components of Σ^(-1/2)x as a sufficient condition, but note it can likely be relaxed.
- Does gradient descent with moderate stepsize dominate ridge regression with negative regularization? The analysis currently restricts ridge to λ ≥ 0 and GD to small stepsizes; the authors conjecture the dominance extends to the negative ridge regime via moderate stepsizes.
- Does multi-epoch SGD dominate the best performance of both single-pass SGD and GD? Multi-epoch SGD recovers both SGD (epoch 1) and GD (many epochs, small stepsize), but it is unknown if it strictly outperforms the best of the two or how data reuse impacts the trade-off between bias and variance.

## Limitations
- The analysis relies on simplified eigenvalue spectra (power law, spikes) rather than general matrices, limiting real-world applicability.
- Finite-sample constants in the bounds are unspecified, requiring empirical tuning for accurate comparisons.
- The "hard example" showing SGD superiority needs very specific, high-dimensional structures (d ≥ n²) that may not arise naturally.

## Confidence
- **High Confidence:** GD always achieving the minimax rate for source condition problems.
- **Medium Confidence:** The polynomial gap between GD and Ridge for ill-conditioned problems.
- **Medium Confidence:** The existence of problems where SGD is superior.

## Next Checks
1. **Sensitivity Analysis:** Vary the power law exponent a and source condition exponent r across a grid to quantify when each algorithm's gap is largest.
2. **General Spectra Test:** Replace the idealized power law with a realistic covariance (e.g., from real datasets) to assess the theory's robustness.
3. **Algorithmic Variants:** Compare against accelerated GD (Nesterov) and mini-batch SGD to understand if the implicit bias is unique to plain GD.