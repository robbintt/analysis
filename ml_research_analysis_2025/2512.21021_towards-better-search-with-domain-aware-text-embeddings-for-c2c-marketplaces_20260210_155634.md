---
ver: rpa2
title: Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces
arxiv_id: '2512.21021'
source_url: https://arxiv.org/abs/2512.21021
tags:
- search
- embeddings
- query
- item
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a domain-aware Japanese text-embedding model
  for C2C marketplace search at Mercari, addressing challenges of short, ambiguous
  queries and noisy user-generated listings. The approach fine-tunes a text encoder
  on purchase-driven query-title pairs using role-specific prefixes to model query-item
  asymmetry and applies Matryoshka Representation Learning to obtain compact, truncation-robust
  embeddings.
---

# Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces

## Quick Facts
- arXiv ID: 2512.21021
- Source URL: https://arxiv.org/abs/2512.21021
- Authors: Andre Rusli; Miao Cao; Shoma Ishimoto; Sho Akiyama; Max Frenzel
- Reference count: 1
- Primary result: Domain-aware text embeddings for C2C marketplaces improve search relevance and revenue per user

## Executive Summary
This paper introduces a domain-aware Japanese text-embedding model for C2C marketplace search at Mercari, addressing challenges of short, ambiguous queries and noisy user-generated listings. The approach fine-tunes a text encoder on purchase-driven query-title pairs using role-specific prefixes to model query-item asymmetry and applies Matryoshka Representation Learning to obtain compact, truncation-robust embeddings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with nDCG@k nearly doubling at 32 dimensions compared to PCA compression. Manual assessment highlights better handling of proper nouns and marketplace-specific semantics. An initial online A/B test demonstrates statistically significant improvements in revenue per user (+0.92%) and search efficiency, with transaction frequency maintained.

## Method Summary
The methodology involves fine-tuning a text encoder using contrastive learning on query-title pairs from actual purchase transactions. Role-specific prefixes are added to distinguish query and item representations, modeling the asymmetric nature of search. Matryoshka Representation Learning is employed to produce embeddings at multiple dimensions that remain effective when truncated. The model is trained to maximize similarity between matched query-title pairs while using in-batch negative sampling for contrastive learning. The resulting embeddings are evaluated both offline using historical click data and online via A/B testing.

## Key Results
- nDCG@k nearly doubles at 32 dimensions compared to PCA compression
- Statistically significant improvements in revenue per user (+0.92%) in initial online A/B test
- Better handling of proper nouns and marketplace-specific semantics in manual assessment
- Maintained transaction frequency while improving search efficiency

## Why This Works (Mechanism)
The domain-aware embeddings work by leveraging purchase data as implicit relevance signals, capturing the actual commercial intent behind user queries rather than relying on implicit feedback like clicks alone. The role-specific prefixes enable the model to understand the asymmetric relationship between queries (user intent) and items (product descriptions), while Matryoshka training ensures embeddings remain effective at various compression levels, crucial for large-scale deployment. The contrastive learning framework directly optimizes for the retrieval task, learning to distinguish between relevant and irrelevant item representations in the embedding space.

## Foundational Learning
- **Contrastive Learning**: Needed to learn discriminative representations by pulling similar items together and pushing dissimilar ones apart in embedding space; quick check: verify margin between positive and negative pairs
- **Matryoshka Representation Learning**: Enables multi-scale embeddings that maintain performance when truncated; quick check: plot performance vs. embedding dimension
- **Role-specific Prefixes**: Allows modeling of asymmetric relationships between queries and items; quick check: test with and without prefixes to measure impact
- **Purchase-driven Data**: Uses actual transaction pairs as ground truth relevance; quick check: compare performance against click-based training
- **In-batch Negative Sampling**: Efficient way to provide negative examples during training; quick check: measure impact of hard vs. random negatives
- **Fine-tuning vs. Training from Scratch**: Balances leveraging pre-trained knowledge with domain adaptation; quick check: compare final performance and training efficiency

## Architecture Onboarding

**Component Map**
Text Encoder (mBERT) -> Prefix Addition -> Matryoshka Projection -> Contrastive Loss -> Embedding Output

**Critical Path**
Query -> Tokenizer -> Encoder with Prefix -> Matryoshka Layers -> Embedding -> Similarity Computation -> Ranking

**Design Tradeoffs**
- Contrastive learning with in-batch negatives vs. explicit hard-negative mining (computational efficiency vs. discriminative power)
- Matryoshka training vs. fixed-dimension embeddings (flexibility vs. complexity)
- Purchase data vs. click data for training signals (commercial relevance vs. broader behavioral patterns)

**Failure Signatures**
- Overfitting to purchase patterns, missing long-tail queries
- Degraded performance at low dimensions despite Matryoshka training
- Inability to handle nuanced negation or soft style constraints
- Bias toward popular items in embedding space

**First 3 Experiments**
1. Ablation study removing Matryoshka layers to assess impact on performance across dimensions
2. Comparison of prefix vs. non-prefix models on asymmetric query-item pairs
3. Test with different negative sampling strategies (hard negatives vs. in-batch)

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can embedding models be improved to handle nuanced user intents, specifically complex negation or soft style constraints?
- Basis in paper: [explicit] The Conclusion states "nuanced intents (e.g., negation or soft style constraints) remain challenging," and the manual assessment noted both models struggled with "nuanced negation."
- Why unresolved: Purchase-driven contrastive learning on query-title pairs appears insufficient to capture the logical exclusion or subtle modifiers present in complex queries.
- Evidence: Improved performance on a targeted evaluation set containing negative constraints (e.g., "not too flashy") or soft style modifiers.

### Open Question 2
- Question: Does explicit hard-negative mining provide significant gains over the current in-batch negative sampling strategy?
- Basis in paper: [explicit] The Conclusion lists "reliance on implicit negatives from in-batch sampling" as a specific limitation of the current methodology.
- Why unresolved: In-batch negatives are computationally efficient but may lack the "hardness" required to differentiate semantically similar but commercially distinct items (e.g., accessories vs. core products).
- Evidence: An ablation study comparing nDCG@k scores between the current model and a version trained with curated hard negatives.

### Open Question 3
- Question: To what extent can multimodal inputs address the limitations of text-only embeddings in C2C marketplaces?
- Basis in paper: [explicit] The Conclusion identifies "limited exposure to multimodal or code-switched inputs" as a constraint, listing multimodal search as a future direction.
- Why unresolved: C2C listings are often noisy or image-centric; relying solely on text may miss crucial semantic signals available in product images.
- Evidence: Evaluation of a vision-language model on retrieval tasks involving "code-switched" inputs or listings with poor textual descriptions but clear images.

## Limitations
- Offline evaluation relies on clickthrough data that may reflect existing ranking biases rather than true relevance preferences
- Manual evaluation sample size and selection criteria are not disclosed, introducing potential subjectivity
- Online A/B test lacks statistical power analysis, test duration details, and seasonal controls
- Missing user experience metrics such as click position distribution, query reformulation rates, or perceived result diversity

## Confidence
- **High Confidence**: Technical implementation details of Matryoshka fine-tuning and role-specific prefix usage are clearly described and reproducible
- **Medium Confidence**: Offline evaluation improvements over baselines are supported by historical logs, but generalizability to future or different query distributions is uncertain
- **Low Confidence**: Long-term marketplace impact, user experience effects, and fairness considerations are not evaluated

## Next Checks
1. Conduct a longer-term online A/B test with explicit tracking of user engagement metrics (click position, dwell time, reformulation rate) to validate that embedding improvements translate to better user experience, not just short-term revenue gains

2. Perform an ablation study varying the number of fine-tuning epochs, learning rate schedules, and prefix schemes to assess robustness of improvements and identify overfitting to the training distribution

3. Evaluate embedding fairness across seller demographics and product categories to ensure the model does not systematically disadvantage certain marketplace participants