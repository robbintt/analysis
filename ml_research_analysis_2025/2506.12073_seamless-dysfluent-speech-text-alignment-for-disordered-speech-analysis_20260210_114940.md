---
ver: rpa2
title: Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis
arxiv_id: '2506.12073'
source_url: https://arxiv.org/abs/2506.12073
tags:
- speech
- alignment
- dysfluent
- neural
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neural LCS introduces a novel approach for dysfluent speech-text
  alignment, addressing the challenge of aligning disordered speech with intended
  text. Unlike traditional methods, Neural LCS leverages robust phoneme-level modeling
  to handle partial alignment and context-aware similarity mapping.
---

# Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis

## Quick Facts
- **arXiv ID:** 2506.12073
- **Source URL:** https://arxiv.org/abs/2506.12073
- **Reference count:** 0
- **Primary result:** Neural LCS achieves up to 90.96% accuracy in phoneme-level alignment and improves dysfluent speech segmentation by 17-27ms over baselines

## Executive Summary
Neural LCS introduces a novel approach for dysfluent speech-text alignment, addressing the challenge of aligning disordered speech with intended text. Unlike traditional methods, Neural LCS leverages robust phoneme-level modeling to handle partial alignment and context-aware similarity mapping. It significantly outperforms state-of-the-art models in alignment accuracy and dysfluent speech segmentation. Experiments on simulated and real PPA datasets show Neural LCS achieves up to 90.96% accuracy in phoneme-level alignment and improves boundary detection in speech-text alignment by 17-27ms compared to baselines. The method also accurately identifies dysfluency types like repetition, insertion, deletion, and substitution. Neural LCS offers a more accurate, linguistically grounded solution for dysfluent speech alignment, enhancing automated systems for diagnosing and analyzing speech disorders.

## Method Summary
Neural LCS extends the classical Longest Common Subsequence algorithm into a differentiable neural framework that learns soft alignment decisions through a siamese network processing both reference and dysfluent sequences. The model uses a T5-based feature encoder with fully-visible self-attention to capture context-aware phonetic similarity between phonemes, enabling alignment of substitutions (e.g., /B/ aligned to /P/ for voicing errors). For speech-text alignment, a wav2vec 2.0 feature extractor combined with CTC loss provides an initial audio-to-phoneme mapping, which Neural LCS then refines through soft alignment correction. The approach handles partial alignment by learning which tokens to match versus ignore, addressing non-alignable segments in dysfluent speech through explicit labeling of missing phonemes or words.

## Key Results
- Neural LCS achieves 90.96% phoneme-level alignment accuracy on simulated data, significantly outperforming Hard LCS (72.8%) and DTW (71.7%)
- On real PPA speech data, Neural LCS improves boundary loss by 17ms for repetition detection compared to baseline YOLO-Stutter (17ms vs 21ms)
- The model accurately identifies dysfluency types with insertion detection achieving 96.3% accuracy and repetition detection at 94.1%
- Speech-text alignment (STA) model improves LLM disorder boundary detection from 27ms to 10ms for repetition detection

## Why This Works (Mechanism)

### Mechanism 1: Local Subsequence Alignment via Neural LCS
Neural LCS enables partial alignment by learning which tokens to match versus ignore, addressing the "non-alignable" segments in dysfluent speech. The model extends the classical Longest Common Subsequence algorithm into a differentiable neural framework. Instead of requiring exact token matches, it learns soft alignment decisions through a siamese network that processes both reference and dysfluent sequences. The network outputs three-class labels: 1 for aligned boundaries, 0 for dysfluent units within aligned regions, and 2 for missing phonemes/words. This approach explicitly marks non-alignable segments rather than forcing them into alignment.

### Mechanism 2: Context-Aware Phonetic Similarity Learning
The model captures articulatory and acoustic similarity between phonemes, enabling alignment of substitutions (e.g., /B/ aligned to /P/ for voicing errors). A T5-based feature encoder with fully-visible self-attention allows each phoneme to attend to all others in both sequences. This context awareness, combined with training on simulated dysfluencies categorized by phoneme similarity (plosives, fricatives, vowels, etc.), enables the network to learn that /K/ and /G/ or /IH/ and /EY/ should be treated as similar. The siamese structure ensures shared representations between reference and dysfluent sequences.

### Mechanism 3: CTC-Based Speech-to-Text Bridge with Soft Alignment Correction
A wav2vec 2.0 feature extractor combined with CTC loss provides an initial audio-to-phoneme mapping; Neural LCS then refines alignment between the dysfluent phoneme sequence and reference text. Crucially, because Neural LCS performs soft alignment, errors in the CTC transcription (misrecognized phonemes) can still find reasonable matches in the reference through similarity-based alignment, improving boundary detection. This cascade approach compensates for residual CTC errors that would otherwise propagate through the alignment pipeline.

## Foundational Learning

- **Longest Common Subsequence (LCS) Algorithm**: Neural LCS is a differentiable neural approximation of the classical LCS algorithm. Understanding LCS—how it finds the longest matching subsequence between two sequences via dynamic programming—is essential to grasp what the network is learning to approximate. *Quick check:* Given sequences [A, B, C, D, E] and [A, X, B, C, Y], what is the LCS and why would DTW (Dynamic Time Warping) produce a different result?

- **Siamese Networks**: The Neural LCS architecture uses a siamese framework where both input sequences (reference and dysfluent) pass through a shared encoder. This ensures both sequences are projected into a common embedding space where similarity can be computed. *Quick check:* Why must the feature encoder be shared (identical weights) between the two input branches rather than having separate encoders?

- **Focal Loss for Class Imbalance**: Alignment labels are imbalanced—label 1 (aligned boundaries) dominates when dysfluency is sparse. Focal Loss down-weights well-classified examples, forcing the model to learn from rare but critical dysfluency markers (labels 0 and 2). *Quick check:* In the paper, α = [0.5, 0.1, 0.8] for labels [0, 1, 2]. Why does label 2 receive the highest weight?

## Architecture Onboarding

- **Component map**: wav2vec 2.0 feature extractor -> Conformer -> CTC projection -> Neural LCS alignment
- **Critical path**: 1. Tokenize reference and dysfluent sequences 2. Encode both through shared T5 encoder 3. Concatenate sequence representations 4. Pass through 1D CNN + MLP to produce per-position alignment labels 5. For speech input: first extract CTC phoneme sequence, then apply Neural LCS alignment to reference
- **Design tradeoffs**: Phoneme-level achieves higher accuracy (90.96% vs. 75.07%) but requires phoneme transcription. Word-level is more accessible but conflates morphological and acoustic similarity. Simulated training data enables large-scale training (1.1M sentences) but may not capture all real dysfluency patterns.
- **Failure signatures**: Low alignment accuracy on substitution-heavy data (substitution accuracy remains lowest ~91-93%); high boundary loss on deletion detection (27ms vs 13ms baseline); domain mismatch on unseen dysfluency types not in simulation categories.
- **First 3 experiments**: 1. Reproduce text-text alignment on simulated data: train Neural LCS on provided text-text dataset (90/10 split), evaluate phoneme-level alignment accuracy against Hard LCS and DTW baselines. 2. Ablate phoneme similarity categories: train separate models with different phoneme grouping strategies to isolate contribution of articulatory categorization. 3. Evaluate STA model on held-out PPA samples: run full speech-text alignment pipeline, compute boundary loss, and compare per-dysfluency-type performance against YOLO-Stutter.

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating articulatory kinematic or gestural features improve the phoneme similarity modeling in Neural LCS compared to current acoustic and textual representations? The authors state it would be helpful to "explore better decoder... or phoneme similarity models either in kinematics systems... or gestural systems." The current model relies on T5 feature encoders and heuristic phoneme categories which capture acoustic/textual similarity but may lack the physiological grounding of articulatory kinetics. Experiments incorporating vocal tract kinematic data or gestural scores into the siamese network could reveal if alignment accuracy increases for dysfluencies with subtle articulatory distinctions.

### Open Question 2
How can the Speech-text Alignment (STA) model be optimized to reduce the specific performance gap in detecting deletions compared to repetition and insertion? Table 4 shows the STA model underperforms the baseline significantly on deletion detection (27ms Boundary Loss vs. 13ms), despite outperforming it on other dysfluency types. The paper reports results but doesn't analyze why the alignment-based approach struggles with "missing" tokens compared to segment detection models. An analysis of the loss function's handling of sparse deletion labels or comparison of alignment paths for deletion-heavy sentences could identify where soft alignment fails to map silence/omission.

### Open Question 3
Would replacing the greedy CTC decoding with a WFST-based decoder significantly enhance the robustness of the STA model's alignment pipeline? The conclusion suggests exploring "better decoder [39]" to build upon the current Neural LCS framework. The current STA inference uses greedy decoding on the CTC emission matrix, which is simpler but potentially less capable of handling complex dysfluent sequences than a weighted finite-state transducer approach. A comparative study evaluating boundary loss and alignment accuracy using greedy decoding versus a WFST-based decoder on the LLM disorder dataset would provide concrete evidence.

## Limitations

- Performance relies heavily on simulated training data (1.1M sentences), with limited real clinical samples (38 participants, ~1h total) raising generalization concerns
- The heuristic phoneme categorization based on articulatory mechanisms may not capture all real-world dysfluency patterns or language-specific phonological processes
- CTC recognition accuracy serves as a bottleneck, with no quantified upper bound for error tolerance before alignment performance degrades

## Confidence

**High Confidence Claims:**
- Neural LCS outperforms traditional alignment methods (Hard LCS, DTW) on text-text alignment accuracy
- The model correctly identifies and labels dysfluency types (Rep, Del, Sub, Ins) in both simulated and real data
- Boundary detection improvements of 17-27ms over baselines are reproducible on the evaluated datasets

**Medium Confidence Claims:**
- Neural LCS's phoneme-level similarity learning generalizes to real clinical dysfluencies
- The STA pipeline provides practical utility for automated dysfluency detection in clinical settings
- Simulated training data captures sufficient variation to support real-world performance

**Low Confidence Claims:**
- Neural LCS can handle all types of speech disorders beyond PPA and LLM disorder
- The model's performance remains stable under extreme dysfluency severity
- The specific phoneme categorization approach is optimal across different languages and clinical populations

## Next Checks

1. **Domain Transfer Test**: Evaluate Neural LCS on dysfluent speech from different clinical populations (e.g., aphasia, apraxia, stuttering) not represented in training data. Compare performance degradation against domain adaptation techniques.

2. **CTC Error Tolerance Analysis**: Systematically vary CTC phoneme recognition error rates and measure Neural LCS alignment accuracy. Identify the error threshold where alignment performance breaks down and compare against clinical ASR error rates.

3. **Ablation on Phoneme Categories**: Remove or modify the phoneme similarity categories and retrain Neural LCS. Quantify the contribution of articulatory categorization to alignment accuracy and identify which categories are most critical for real dysfluency patterns.