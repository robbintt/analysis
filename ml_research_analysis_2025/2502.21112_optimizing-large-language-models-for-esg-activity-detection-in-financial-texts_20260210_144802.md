---
ver: rpa2
title: Optimizing Large Language Models for ESG Activity Detection in Financial Texts
arxiv_id: '2502.21112'
source_url: https://arxiv.org/abs/2502.21112
tags:
- data
- language
- performance
- fine-tuning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of using large language models
  to classify text related to ESG activities within financial documents. The authors
  address the lack of domain-specific, high-quality training data by introducing ESG-Activities,
  a benchmark dataset combining manually curated and synthetically generated data.
---

# Optimizing Large Language Models for ESG Activity Detection in Financial Texts

## Quick Facts
- **arXiv ID**: 2502.21112
- **Source URL**: https://arxiv.org/abs/2502.21112
- **Reference count**: 40
- **Primary result**: Domain-specific fine-tuning with synthetic data boosts ESG activity detection F1-score to over 84%

## Executive Summary
This paper addresses the challenge of classifying ESG-related text in financial documents using large language models. The authors introduce ESG-Activities, a benchmark dataset combining manually curated and synthetically generated data for fine-tuning. Through comprehensive experiments with LoRA-based fine-tuning, they demonstrate that synthetic data augmentation significantly improves model performance, with lightweight open-source models like Llama 2 7B and Gemma 7B outperforming larger proprietary models in certain configurations. The best models achieve over 84% F1-score, establishing a new state-of-the-art for this task.

## Method Summary
The researchers created the ESG-Activities benchmark dataset by combining manually curated financial text data from four major transportation companies with synthetically generated samples produced by ChatGPT-4o. Models were fine-tuned using LoRA techniques with varying training strategies: fine-tuning on only curated data, only synthetic data, and combinations of both. Multiple open-source and proprietary models were evaluated, including Llama 2, Llama 3, Gemma, Mistral, and proprietary GPT-4 variants. The evaluation used standard classification metrics including F1-score, precision, and recall to assess performance on binary classification of ESG activity relevance.

## Key Results
- Synthetic data augmentation improved model performance by 2.43% F1-score compared to curated data alone
- Fine-tuning on synthetic data alone achieved 81.75% F1-score, nearly matching the 82.44% from curated data
- Llama 2 7B and Gemma 7B models achieved the highest performance, outperforming larger proprietary models
- The best model achieved 84.63% F1-score using curated data only, with synthetic data providing consistent improvements

## Why This Works (Mechanism)
Domain-specific fine-tuning with synthetic data addresses the fundamental challenge of limited high-quality training data for ESG activity detection. Financial texts contain specialized terminology and context that general language models struggle to interpret correctly. By generating synthetic examples that mirror real-world ESG reporting patterns, models learn to recognize relevant activities more effectively. LoRA fine-tuning enables efficient adaptation of large models without full retraining, while the combination of real and synthetic data provides both accuracy and coverage of the ESG taxonomy space.

## Foundational Learning
**ESG Taxonomy Mapping**: Understanding how real-world activities map to standardized ESG categories is essential for accurate classification and training data generation.
*Why needed*: Without proper taxonomy alignment, models cannot distinguish relevant from irrelevant text.
*Quick check*: Verify that generated synthetic examples correctly map to expected taxonomy codes.

**Financial Document Structure**: ESG disclosures appear in specific sections of financial reports with predictable patterns.
*Why needed*: Models must recognize context clues about where ESG information typically appears.
*Quick check*: Test model performance on different document sections (management discussion, risk factors, etc.).

**Synthetic Data Generation**: Automated generation of training examples using language models requires careful prompt engineering.
*Why needed*: Quality synthetic data can significantly expand limited training sets while maintaining relevance.
*Quick check*: Compare classification performance with and without synthetic augmentation.

**LoRA Fine-tuning**: Low-rank adaptation enables efficient model customization without full parameter updates.
*Why needed*: Reduces computational cost while maintaining performance on specialized tasks.
*Quick check*: Monitor training time and GPU memory usage compared to full fine-tuning.

## Architecture Onboarding

**Component Map**: Financial Text -> Tokenizer -> LLM Backbone -> LoRA Adapter -> Classification Head -> ESG Activity Prediction

**Critical Path**: The pipeline processes financial documents through tokenization, passes them through the fine-tuned LLM backbone with LoRA adapters, and produces binary classification outputs indicating ESG activity relevance.

**Design Tradeoffs**: The study balances model size against performance, showing that smaller open-source models with proper fine-tuning can outperform larger proprietary alternatives. The choice between curated and synthetic data involves tradeoffs between annotation quality and coverage breadth.

**Failure Signatures**: Models may struggle with domain-specific terminology not present in training data, misclassify activities from underrepresented ESG categories, or fail to recognize contextual cues in financial documents. Synthetic data quality directly impacts downstream performance.

**First 3 Experiments to Run**:
1. Evaluate model performance on a held-out test set from the same companies to establish baseline accuracy
2. Compare F1-scores across different fine-tuning strategies (curated only, synthetic only, combined)
3. Test model generalization by evaluating on financial texts from companies outside the training set

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can human-in-the-loop approaches further improve the quality and effectiveness of synthetic data generation for ESG text classification?
- Basis in paper: [explicit] The authors state: "we plan to explore alternative data generation techniques, including human-in-the-loop approaches, to further refine synthetic data for ESG applications."
- Why unresolved: Current synthetic data was generated fully automatically via ChatGPT-4o; no human verification was applied to the 1,060 synthetic samples, leaving potential quality issues unexamined.
- What evidence would resolve it: Comparative experiments showing F1-score differences between models trained on automatically generated vs. human-in-the-loop refined synthetic data.

### Open Question 2
- Question: Do fine-tuned ESG activity detection models generalize to industries beyond transportation?
- Basis in paper: [inferred] The ESG-Activities benchmark contains data from only four transportation companies (Ferrovie dello Stato, Autostrade per l'Italia, Maersk, Mundys), limiting conclusions about cross-sector applicability.
- Why unresolved: No experiments tested model performance on financial texts from other sectors (e.g., energy, manufacturing, finance), leaving domain transfer capability unknown.
- What evidence would resolve it: Evaluation of fine-tuned models on ESG activity classification across diverse industry sectors with comparable ground-truth labels.

### Open Question 3
- Question: Can the methodology successfully extend to Social and Governance dimensions of ESG, not just Environmental activities?
- Basis in paper: [inferred] The paper explicitly limits focus to "text related to environmental activities" and the dataset maps only to environmental ESG taxonomy items, leaving S and G categories unaddressed.
- Why unresolved: Social and Governance taxonomies involve different linguistic patterns, regulatory frameworks, and activity descriptions that may require distinct fine-tuning strategies.
- What evidence would resolve it: Application of the same fine-tuning methodology to Social and Governance taxonomy activities with reported F1-scores and comparison to Environmental results.

### Open Question 4
- Question: How do fine-tuned models perform on downstream tasks such as ESG sentiment analysis and risk assessment?
- Basis in paper: [explicit] The authors acknowledge: "further research is needed to evaluate the models' performance on other NLP tasks, such as ESG sentiment analysis and risk assessment."
- Why unresolved: Binary classification of activity relevance is only one component of ESG analysis; whether these models transfer to sentiment or risk tasks remains untested.
- What evidence would resolve it: Benchmarks showing fine-tuned model performance on established ESG sentiment and risk assessment datasets compared to specialized baselines.

## Limitations
- Dataset size remains modest at 6,285 samples, potentially limiting model generalization
- Focus on English-language documents may limit applicability to global financial markets
- Evaluation focuses on binary classification without exploring multi-label scenarios
- Synthetic data quality depends heavily on GPT-4's performance, introducing potential reproducibility issues

## Confidence
- **Model Performance Claims (High confidence)**: The F1-score improvements and comparative analysis between models are well-documented with appropriate statistical backing.
- **Synthetic Data Effectiveness (Medium confidence)**: While synthetic data augmentation shows clear benefits, the dependency on GPT-4 generation quality introduces uncertainty about reproducibility across different ESG domains.
- **Model Selection Recommendations (Medium confidence)**: The preference for smaller open-source models over larger proprietary ones is well-supported, but may not generalize to all ESG detection tasks or document types.

## Next Checks
1. Evaluate model performance on out-of-distribution ESG documents, particularly from different industries and geographic regions not represented in the training data.
2. Test the synthetic data generation pipeline with different prompting strategies and alternative language models to assess robustness of the augmentation approach.
3. Conduct a cost-benefit analysis comparing fine-tuning approaches against few-shot prompting strategies for practical deployment scenarios, including computational and licensing considerations.