---
ver: rpa2
title: 'Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex
  Instruction-based Image Generation'
arxiv_id: '2505.24787'
source_url: https://arxiv.org/abs/2505.24787
tags:
- generation
- complex
- image
- arxiv
- plan2gen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongBench-T2I, a comprehensive benchmark
  designed to evaluate text-to-image models on their ability to follow complex, multi-faceted
  instructions involving detailed objects, attributes, spatial relationships, and
  compositional richness. The benchmark includes 500 human-reviewed prompts covering
  nine visual dimensions.
---

# Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation

## Quick Facts
- arXiv ID: 2505.24787
- Source URL: https://arxiv.org/abs/2505.24787
- Authors: Yucheng Zhou; Jiahao Yuan; Qianning Wang
- Reference count: 36
- Key outcome: Plan2Gen achieves state-of-the-art performance on LongBench-T2I, outperforming various open-source and closed-source models in handling detailed, long-form instructions

## Executive Summary
This paper introduces LongBench-T2I, a comprehensive benchmark designed to evaluate text-to-image models on their ability to follow complex, multi-faceted instructions involving detailed objects, attributes, spatial relationships, and compositional richness. The benchmark includes 500 human-reviewed prompts covering nine visual dimensions. The authors also propose Plan2Gen, an agent framework that leverages large language models to decompose complex instructions into layered generation tasks (background, midground, foreground), iteratively refining each layer with validation to ensure alignment. Experiments demonstrate that Plan2Gen achieves state-of-the-art performance on LongBench-T2I, outperforming various open-source and closed-source models. Human evaluations and analyses confirm Plan2Gen's superior capability in handling detailed, long-form instructions.

## Method Summary
The paper proposes Plan2Gen, a framework that uses LLMs to decompose complex prompts into three compositional layers (background, midground, foreground) for progressive image generation. The system validates each layer with LLM-based evaluation and iteratively refines mismatches up to three times. This approach addresses the challenge of generating images from complex instructions (~683 tokens average) by breaking them into manageable components while maintaining cross-layer coherence through progressive conditioning. The method requires no model training, relying entirely on LLM orchestration of existing T2I models.

## Key Results
- Plan2Gen outperforms various open-source and closed-source models on the LongBench-T2I benchmark
- Human evaluations confirm superior performance in handling detailed, long-form instructions
- Performance peaks at 3 refinement iterations per layer, with degradation beyond this point
- Plan consistency positively correlates with final image quality

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Scene Decomposition
Breaking complex instructions into spatial layers improves generation fidelity by reducing cognitive load on the underlying T2I model. An LLM parses the full instruction and segregates visual elements into three compositional layers—background (environment/atmosphere), midground (primary subjects), and foreground (fine details)—producing layer-specific sub-prompts. Core assumption: The LLM can accurately map textual descriptions to spatial depth planes and preserve cross-layer dependencies during decomposition. Evidence: T2I models degrade on long prompts (DetailMaster), supporting decomposition as mitigation. Break condition: If decomposition produces overlapping or contradictory layer assignments, the refinement loop may amplify rather than resolve inconsistencies.

### Mechanism 2: Validation-Guided Iterative Refinement
LLM-based validation with targeted refinement improves alignment without model retraining. After each layer generation, an LLM validates the output against its sub-prompt. Mismatches trigger augmentation of the sub-prompt with specific refinement instructions, and regeneration proceeds. This loop terminates on pass or max iterations. Core assumption: The validator LLM reliably detects mismatches and refinement instructions meaningfully improve subsequent outputs rather than causing semantic drift. Evidence: Performance peaks at 3 refinement steps (avg 3.73), then degrades at 5–7 steps, suggesting refinement helps within bounds. Break condition: Excessive iterations (>3) risk over-correction; if the validator itself has systematic blind spots, errors compound rather than correct.

### Mechanism 3: Progressive Conditioning Across Layers
Conditioning each layer on successfully generated preceding layers maintains cross-layer coherence. Midground and foreground generation is conditioned on the approved output of previous layers, ensuring spatial and semantic consistency. Core assumption: The conditioning mechanism in the underlying T2I model can effectively incorporate prior-layer outputs without catastrophic forgetting or style drift. Evidence: Plan consistency positively correlates with final image quality, implying faithful layered execution matters. Break condition: If early-layer errors propagate (e.g., wrong background perspective), downstream layers inherit incoherence—no backward correction mechanism exists.

## Foundational Learning

- **Compositional Prompt Understanding**
  - Why needed here: Plan2Gen's decomposition requires understanding how objects, attributes, spatial relations, and lighting interact across a scene
  - Quick check question: Given "a cat on a table under a red lamp in a blue room," can you identify which elements belong to background vs. foreground and how lighting affects color perception?

- **LLM-as-Controller Pattern**
  - Why needed here: The framework treats LLMs not as generators but as planners/validators orchestrating a T2I model
  - Quick check question: Explain the difference between an LLM generating text directly vs. an LLM decomposing a task into sub-tasks for another model to execute

- **Iterative Refinement with Early Stopping**
  - Why needed here: Table 4 shows performance peaks at 3 refinement iterations—understanding when to stop is as important as knowing how to refine
  - Quick check question: Why might more refinement iterations degrade rather than improve output quality? What signals would indicate "good enough"?

## Architecture Onboarding

- **Component map:**
  Input: Complex Instruction (~600 tokens)
  → [Scene Decomposition Planner - LLM]
  → Background/Midground/Foreground sub-prompts
  → [Progressive Layered Generator - T2I Model]
  → Background image → validate → refine (max 3×)
  → Midground image (conditioned on background) → validate → refine
  → Foreground image (conditioned on midground) → validate → refine
  → [Validator - LLM] ←→ [Refinement Controller]
  → Output: Final composited image

- **Critical path:** Decomposition quality → Layer 1 (background) generation → Validation accuracy → Refinement effectiveness → Subsequent layer conditioning. The planner LLM is the single highest-leverage component (Table 5: Gemini 2.0 Flash planner scores 3.73 vs. Gemini 1.5 Flash-8b at 3.42).

- **Design tradeoffs:**
  - More refinement steps vs. semantic drift (optimal: 3)
  - Stronger planner LLM vs. inference cost/latency
  - Layer granularity: 3 layers chosen for manageability; finer decomposition could help but increases orchestration complexity
  - No training required vs. potential ceiling on performance without fine-tuning

- **Failure signatures:**
  - Low plan consistency → degraded final scores
  - High perplexity on prompts may paradoxically yield higher scores in smaller models, indicating language-visual misalignment
  - Over-iteration (>5 steps) causes slight score drops across dimensions
  - If validator LLM has blind spots for specific dimensions (e.g., Text, Pose), those systematically fail refinement

- **First 3 experiments:**
  1. **Ablate the planner:** Run Plan2Gen with a weaker planner (e.g., GPT-3.5 or smaller open-source LLM) while keeping the base T2I model fixed. Measure score degradation to quantify planner contribution. Compare against Table 5's Gemini 1.5 Flash-8b baseline.
  2. **Vary refinement iterations:** Test with max refinement = 1, 2, 3, 5 on a 50-prompt subset. Plot per-dimension scores to identify which dimensions benefit most from refinement and which suffer from over-correction. Validate Table 4's 3-step optimum on your specific T2I backbone.
  3. **Layer isolation test:** Generate images using only background, only midground, or only foreground sub-prompts (without progressive conditioning), then compare against full Plan2Gen pipeline. This isolates the contribution of progressive conditioning vs. decomposition alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do some multi-modal models exhibit a counter-intuitive positive correlation between instruction perplexity (worse language understanding) and higher visual generation quality, and what mechanisms determine when this relationship inverts at larger model scales?
- Basis in paper: The authors explicitly report: "a counter-intuitive positive correlation where prompts with higher PPL... sometimes yielded images with higher evaluation scores" for Janus-Pro-1B, noting this "attenuated" in Janus-Pro-7B but the mechanism remains unexplained.
- Why unresolved: The paper documents the phenomenon and correlates it with model scale but does not identify the underlying cause—whether it stems from attention distribution, tokenization artifacts, training data biases, or architectural factors.
- What evidence would resolve it: Controlled experiments varying individual architectural components (attention mechanisms, embedding dimensions, training objectives) while holding others constant, combined with layer-wise analysis of where language-to-vision translation breaks down.

### Open Question 2
- Question: What specific failure modes characterize "semantic drift" during iterative refinement in layered generation, and can they be detected automatically before image quality degrades?
- Basis in paper: The authors state that beyond 3 refinement steps, "excessive iterations may risk over-correction or semantic drift" but provide no characterization or detection mechanism.
- Why unresolved: The phenomenon is mentioned as a practical constraint without analysis of its causes, manifestations, or predictability across different instruction types.
- What evidence would resolve it: Systematic analysis of intermediate images and prompts across multiple refinement iterations, categorizing drift types (attribute bleeding, object hallucination, style loss) and developing early-detection metrics.

### Open Question 3
- Question: Does the fixed three-layer spatial decomposition (background, midground, foreground) limit Plan2Gen's effectiveness on non-spatially-structured instructions such as abstract compositions, temporal sequences, or relational prompts?
- Basis in paper: Plan2Gen's architecture rigidly decomposes scenes into three spatial layers. While evaluated on 500 diverse prompts, the paper does not report performance stratified by prompt type, leaving unclear whether this spatial prior helps or hinders certain instruction categories.
- Why unresolved: No ablation comparing the layered approach against alternative decomposition strategies, and no analysis of which instruction types benefit most or least from spatial layering.
- What evidence would resolve it: Categorized benchmark results by instruction type (spatial vs. relational vs. abstract), plus experiments with alternative decomposition schemes (semantic, temporal, attribute-based).

### Open Question 4
- Question: To what extent does using MLLMs as automated evaluators introduce systematic bias toward outputs from similar model families or generation approaches?
- Basis in paper: The evaluation relies entirely on Gemini-2.0-Flash and InternVL3-78B as judges. While human evaluation on a subset shows general agreement, the paper does not examine whether these MLLM evaluators systematically favor certain visual styles or generation paradigms common to their own training distributions.
- Why unresolved: Human validation covers only a prompt subset and reports win rates, not fine-grained calibration of MLLM scores against human judgments across dimensions.
- What evidence would resolve it: Large-scale human evaluation across all 500 prompts with detailed dimensional scoring, correlation analysis between MLLM and human scores per dimension, and testing evaluators on adversarial examples designed to exploit potential biases.

## Limitations
- Exact prompt templates for decomposition, validation, and refinement are not disclosed, making faithful reproduction difficult
- The method of progressive conditioning between layers is underspecified (image-to-image vs. textual description only)
- The base text-to-image model used for generation is not explicitly identified

## Confidence
- **High confidence:** The core insight that decomposition + iterative refinement improves performance on complex instructions is well-supported by multiple experiments and human evaluations
- **Medium confidence:** The claim that 3 refinement steps is optimal—while Table 4 shows this pattern, the exact inflection point may vary with different T2I backbones or prompt characteristics
- **Low confidence:** The assertion that Plan2Gen achieves "state-of-the-art" performance is relative to unspecified baselines and limited to the LongBench-T2I benchmark without broader validation across other datasets

## Next Checks
1. **Planner Ablation Study:** Implement Plan2Gen with progressively weaker planner models (GPT-3.5 → smaller open-source LLMs) while holding the T2I model constant. Measure degradation to quantify planner contribution versus base model capability.
2. **Cross-Dataset Generalization:** Test Plan2Gen on established multi-element datasets like COCO or ImageNet where detailed compositional prompts are available. Compare performance drop against LongBench-T2I to assess benchmark specificity.
3. **Error Propagation Analysis:** Systematically inject controlled errors into early layer outputs (background perspective distortions, lighting mismatches) and measure downstream impact on midground/foreground quality to quantify conditioning robustness.