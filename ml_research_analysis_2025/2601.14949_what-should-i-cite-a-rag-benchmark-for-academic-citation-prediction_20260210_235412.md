---
ver: rpa2
title: What Should I Cite? A RAG Benchmark for Academic Citation Prediction
arxiv_id: '2601.14949'
source_url: https://arxiv.org/abs/2601.14949
tags:
- citation
- prediction
- task
- retrieval
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents CiteRAG, the first comprehensive retrieval-augmented
  generation benchmark for academic citation prediction. The authors establish two
  complementary tasks: list-specific citation prediction (Task 1) for entire reference
  lists and position-specific citation prediction (Task 2) for individual reference
  placeholders.'
---

# What Should I Cite? A RAG Benchmark for Academic Citation Prediction

## Quick Facts
- arXiv ID: 2601.14949
- Source URL: https://arxiv.org/abs/2601.14949
- Authors: Leqi Zheng; Jiajun Zhang; Canzhi Chen; Chaokun Wang; Hongwei Li; Yuying Li; Yaoxin Mao; Shannan Yan; Zixin Song; Zhiyuan Feng; Zhaolu Kang; Zirong Chen; Hang Zhang; Qiang Liu; Liang Wang; Ziyang Liu
- Reference count: 40
- Primary result: Multi-level hybrid RAG improves citation prediction accuracy while reducing hallucination rates from 17.4% to 4.9%

## Executive Summary
This paper introduces CiteRAG, the first comprehensive benchmark for academic citation prediction using retrieval-augmented generation. The authors establish two complementary tasks: list-specific citation prediction for entire reference lists and position-specific citation prediction for individual reference placeholders. They construct a three-level hierarchical corpus of 554k papers and curate datasets with 7,267 instances for Task 1 and 8,541 for Task 2. The benchmark introduces a multi-level hybrid RAG approach combining specialized retrievers fine-tuned with contrastive learning and generation models. Experimental results show that RAG integration consistently improves prediction accuracy, with CitationGenerator-30B achieving the best performance while significantly reducing hallucination rates.

## Method Summary
The CiteRAG benchmark features a three-level hierarchical corpus (title/abstract, introduction, full-text) with 554k papers from Google Scholar. Two tasks are defined: Task 1 predicts entire reference lists (coarse-grained), while Task 2 predicts citations for specific placeholders (fine-grained). The approach uses a multi-level hybrid RAG pipeline where CitationRetriever-8B, fine-tuned with contrastive learning (InfoNCE loss), retrieves papers across all three corpus levels. Retrieved results are fused using Reciprocal Rank Fusion (RRF) before being passed to generation models (CitationGenerator-4B/30B or external LLMs). The framework employs supervised fine-tuning on 30k query papers with 400k query-positive pairs for the retriever and QA pairs for the generators.

## Key Results
- CitationGenerator-30B achieves best performance with Recall@20 of 0.076 and NDCG@20 of 0.367 for Task 1
- PACA@20 of 0.303 for Task 2 demonstrates effective position-specific citation prediction
- Hallucination rates drop significantly from 17.4% to 4.9% with RAG integration
- CitationRetriever-8B shows 47.9% MRR@50 improvement over baseline embeddings through contrastive fine-tuning
- Multi-level fusion provides 3.7% MRR@50 improvement for fine-tuned retrievers

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Fine-tuning Captures Citation-Specific Relationships
Domain-specific contrastive learning improves retrieval for citation prediction beyond general-purpose embeddings. InfoNCE loss trains the model to distinguish cited papers from approximate nearest neighbors and in-batch negatives, capturing methodological and field-specific relationships beyond surface semantic similarity. CitationRetriever-8B achieves MRR@50 of 0.3232, a 47.9% improvement over Qwen3-Embedding-8B (0.2186).

### Mechanism 2: Multi-Level Retrieval Fusion Explores Complementary Granularities
Fusing retrieval across corpus levels (title/abstract, +introduction, +full-text) improves recall over single-level strategies. Reciprocal rank fusion combines top-k results from three levels, where Level 1 captures topical alignment, Level 2 adds methodological context, and Level 3 provides comprehensive coverage. CitationRetriever-8B shows 3.7% MRR@50 improvement from multi-level fusion over best single-level.

### Mechanism 3: RAG Grounding Reduces Hallucination by Constraining Generation
Retrieval-augmented generation reduces fabricated citations by grounding predictions in retrieved corpus content. Retrieved papers are injected into the generation prompt, providing verifiable candidates and reducing reliance on parametric knowledge that may hallucinate paper titles. CitationGenerator-30B hallucination drops from 17.4% to 4.9%; closed-source models show similar reductions.

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss)**
  - Why needed here: CitationRetriever-8B is trained with InfoNCE to learn citation-specific embeddings
  - Quick check question: Can you explain why in-batch negatives and approximate nearest neighbors are used as hard negatives in the loss?

- **Reciprocal Rank Fusion (RRF)**
  - Why needed here: Multi-level retrieval merges results from three corpus levels using RRF
  - Quick check question: How does RRF handle documents that appear in multiple ranked lists at different positions?

- **RAG Pipeline Architecture**
  - Why needed here: The benchmark evaluates a two-stage retrieve-then-generate pipeline
  - Quick check question: What are the trade-offs between retrieval depth (R=5 vs R=10) and generation quality given fixed context windows?

## Architecture Onboarding

- **Component map:**
  Corpus (3-level, 554k papers) → Retriever (CitationRetriever-8B) → RRF Fusion → Generator (CitationGenerator-4B/30B or external LLMs) → Evaluation (Recall@k, NDCG@k, PACA@k, Hallucination Rate, CDE)
  Retriever fine-tuned with contrastive learning; generator fine-tuned with supervised learning on citation QA pairs

- **Critical path:**
  1. Query (title + abstract) → parallel retrieval across D1, D2, D3
  2. RRF fusion → top-R candidates
  3. Generator prompt includes query + R candidates
  4. Output: ranked citation predictions (Task 1) or position-specific citations (Task 2)

- **Design tradeoffs:**
  - R=10 provides better accuracy than R=5 but increases token consumption and latency; R>15 may introduce noise
  - Multi-level fusion adds retrieval cost; gains are modest for traditional models (0.2-2.7%) but meaningful for fine-tuned retrievers (3.7%)
  - Fine-tuning requires 30k query papers and 400k query-positive pairs; computational cost is non-trivial

- **Failure signatures:**
  - Hallucination persists if retrieval fails or model ignores context (check Halluc. metric)
  - PACA@k low despite high Recall: model retrieves correctly but ranks poorly for position-specific contexts
  - Noise robustness degrades sharply beyond 40% noise for closed-source models

- **First 3 experiments:**
  1. Reproduce retriever ablation: Compare CitationRetriever-8B vs. Qwen3-Embedding-8B vs. BM25 on Recall@20/MRR@50 using the provided corpus
  2. Validate multi-level fusion gain: Run retrieval with single levels (D1, D2, D3) vs. RRF fusion; measure delta in MRR@50
  3. Test hallucination reduction: Run CitationGenerator-30B with and without RAG (R=10); compute Hallucination Rate and PACA@20 on Task 2 validation set

## Open Questions the Paper Calls Out

### Open Question 1
Can the CiteRAG framework and its hierarchical retrieval strategy be effectively transferred to other scientific domains (e.g., biomedicine, chemistry) where citation conventions and terminology density differ significantly from Computer Science? The current CitationRetriever-8B was fine-tuned exclusively on CS data, leaving its generalizability to domains with different rhetorical structures unproven.

### Open Question 2
How does the strict preprocessing filter for Task 2—which excludes papers with non-standard citations and limits training to the "top three" cited sections—affect the model's robustness to noisy or incomplete real-world manuscripts? The paper evaluates performance on the curated test set but does not measure performance degradation when input data violates these cleanliness assumptions.

### Open Question 3
Can current LLM architectures overcome the "Retrieval Depth vs. Quality" trade-off identified in the study, where excessive context (R>15) degrades performance due to attention span limits and noise? The study identifies the drop in NDCG@20 but does not propose architectural solutions to mitigate the "lost in the middle" phenomenon for deep retrieval.

## Limitations

- The benchmark's evaluation depends on the quality and scope of the 554k-paper corpus, which may have incomplete coverage or access restrictions
- The hallucination reduction claim relies on automatic evaluation metrics that may not fully capture nuanced citation quality issues
- The benchmark assumes that improving citation prediction accuracy directly translates to better academic writing assistance without validating downstream user benefits
- The contrastive fine-tuning performance gains are measured against a single baseline embedding model, limiting generalizability

## Confidence

- **High Confidence**: The benchmark's methodological framework (Task 1 vs. Task 2 distinction, multi-level RAG architecture, and evaluation metrics) is well-specified and reproducible. The reported performance improvements for RAG integration over non-RAG baselines are consistent across multiple models and evaluation metrics.
- **Medium Confidence**: The effectiveness of contrastive fine-tuning for citation-specific retrieval is supported by ablation results but could benefit from comparisons against additional embedding architectures. The multi-level retrieval fusion gains are modest and may not generalize to all query types or domains.
- **Low Confidence**: The absolute performance numbers (e.g., Recall@20 of 0.076) may be sensitive to corpus composition, noise levels in the evaluation data, and the specific implementation details of the RRF fusion and prompt templates.

## Next Checks

1. **Corpus Completeness Audit**: Analyze the overlap between the 554k-paper corpus and the test sets to ensure strict filtering was applied, and quantify potential biases introduced by the three-level hierarchy's varying access levels.

2. **Retriever Generalization Test**: Replace the CitationRetriever-8B with alternative embedding models (e.g., sentence-transformers, other domain-specific embeddings) and measure MRR@50 to assess whether contrastive fine-tuning provides consistent advantages across architectures.

3. **Hallucination Quality Review**: Manually inspect a stratified sample of model outputs (RAG vs. non-RAG) to validate that the reduction in hallucination rate (17.4% to 4.9%) corresponds to qualitatively better citation suggestions, not just factually existent but contextually irrelevant papers.