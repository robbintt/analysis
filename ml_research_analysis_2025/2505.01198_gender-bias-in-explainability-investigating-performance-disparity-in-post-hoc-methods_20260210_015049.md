---
ver: rpa2
title: 'Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc
  Methods'
arxiv_id: '2505.01198'
source_url: https://arxiv.org/abs/2505.01198
tags:
- lime
- shap
- igxi
- grad
- femalemale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender disparities in post-hoc feature
  attribution methods used to explain language model predictions. The authors evaluate
  six widely used explainability methods (Gradient, Integrated Gradients, SHAP, LIME,
  and their input-multiplied variants) across three tasks and five language models
  (BERT, TinyBERT, GPT-2, RoBERTa, and FairBERTa) using seven evaluation metrics measuring
  faithfulness, robustness, and complexity.
---

# Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods

## Quick Facts
- arXiv ID: 2505.01198
- Source URL: https://arxiv.org/abs/2505.01198
- Reference count: 40
- Primary result: All six post-hoc explainability methods exhibit significant gender disparities in more than 72% of experimental combinations, persisting even when models are trained from scratch on unbiased data

## Executive Summary
This study systematically investigates gender disparities in post-hoc feature attribution methods used to explain language model predictions. The authors evaluate six widely used explainability methods across three tasks and five language models, finding that all methods exhibit significant performance differences between male and female inputs across the majority of experimental combinations. Critically, these disparities persist even when models are trained from scratch on unbiased datasets, indicating the bias stems primarily from the explanation methods themselves rather than the models or training data. The study highlights the need to consider explanation fairness alongside model fairness and transparency in regulatory frameworks and practical applications.

## Method Summary
The authors evaluate six post-hoc feature attribution methods (Gradient, Integrated Gradients, SHAP, LIME, and their input-multiplied variants) across three tasks and five language models (BERT, TinyBERT, GPT-2, RoBERTa, and FairBERTa). They use seven evaluation metrics measuring faithfulness (Comprehensiveness, Sufficiency), robustness (Sensitivity, Stability), and complexity (Confidence, Gini, APFD). The study employs both "hard" perturbation (token removal) and "soft" perturbation (embedding scaling) methods, with Mann-Whitney U tests and Cohen's d effect sizes to quantify gender disparities. To isolate method bias, they train models from scratch on datasets preprocessed to remove gender information.

## Key Results
- All six explanation methods exhibit significant gender disparities across the majority of experimental combinations (>72% showing statistically significant differences)
- More than 54% of experimental combinations show considerable effect sizes (Cohen's d ≥ 0.2)
- Disparities persist even when models are trained from scratch on unbiased datasets, indicating method-intrinsic bias
- Soft perturbation methods show noticeably lower disparity rates than hard removal methods
- Gradient-based methods exhibit relatively less disparity than perturbation-based methods (SHAP, LIME)

## Why This Works (Mechanism)

### Mechanism 1: Task-Influence Modulation
The magnitude of explanation disparity is modulated by the extent to which the protected attribute (gender) causally influences the model's prediction. When gender is the prediction target (e.g., GECO dataset), attribution methods must heavily weight gendered tokens, forcing them into a high-variance regime where faithfulness metrics show significant gaps between male and female inputs. When gender is merely incidental (e.g., COMPAS recidivism), the signal is weaker, reducing but not eliminating the disparity.

### Mechanism 2: Perturbation-Induced Distributional Drift
Evaluation metrics relying on "hard" token removal artificially inflate disparity by creating out-of-distribution inputs that break semantic structure differently for gendered vs. neutral tokens. "Soft" masking (scaling embeddings proportional to importance) reduces this structural violence, leading to lower observed disparity in faithfulness metrics.

### Mechanism 3: Surrogate vs. Gradient Stability Differential
Perturbation-based methods (LIME, SHAP) exhibit higher disparity rates than gradient-based methods because local surrogate modeling is more sensitive to the specific data manifold around protected attributes than direct gradient backpropagation. LIME/SHAP sample around the instance, and if the "neighborhood" of gendered sentences is structurally different from neutral sentences, the surrogate model fits differently.

## Foundational Learning

**Concept: Comprehensiveness & Sufficiency (Faithfulness Metrics)**
- Why needed here: These are the primary metrics where disparity is observed. Understanding them is critical to diagnosing where the explanation fails.
- Quick check question: If Comprehensiveness is high for male inputs but low for female inputs, does the explanation better capture the model's reasoning for males or females? (Answer: Males)

**Concept: Post-hoc Feature Attribution**
- Why needed here: The paper isolates bias to the method, not the model. You must distinguish between the model's internal weights and the external explainer trying to approximate them.
- Quick check question: If a model is unbiased but the explainer is biased, can the explanation be trusted for debugging? (Answer: No)

**Concept: Cohen's d (Effect Size)**
- Why needed here: Statistical significance (p-value) isn't enough for large datasets; Cohen's d quantifies how large the disparity is.
- Quick check question: A result has p < 0.05 but d = 0.05. Is this practically significant according to the paper's criteria? (Answer: No, effect size is negligible)

## Architecture Onboarding

**Component map:**
Input (Male/Female pairs) -> Transformer Model + Explainer -> Metric suite (Comprehensiveness, Sensitivity, Gini) -> Statistical engine (Mann-Whitney U test, Cohen's d)

**Critical path:** The isolation of "Method Bias" relies on the "Training from Scratch" pipeline. Ensuring the model has no pre-existing bias is the only way to attribute subsequent explanation disparity to the method.

**Design tradeoffs:** Using "Soft" metrics reduces disparity noise but is computationally heavier than "Hard" removal. The paper suggests this tradeoff is worth it for fairness evaluation.

**Failure signatures:**
- High Variance in Soft-Sufficiency: Disparity in "Soft Sufficiency" is a strong signal that the method handles feature importance differently for one gender
- Sensitivity Spikes: High sensitivity on GECO datasets indicates the explainer is fragile when gender is the explicit label

**First 3 experiments:**
1. Run a standard BERT model on the GECO-ALL dataset using LIME. Calculate Comprehensiveness for "he" vs "she" sentences. Verify p < 0.05
2. Train a small Transformer from scratch on the GECO dataset. Re-run LIME. If disparity persists, you have confirmed the bias is in the method, not the pre-training data
3. Switch evaluation from "Hard" Comprehensiveness to "Soft" Comprehensiveness. Observe if the Cohen's d drops below 0.2

## Open Questions the Paper Calls Out

**Open Question 1:** Do post-hoc explanation methods exhibit similar performance disparities for other protected attributes (e.g., race, age) or in non-classification tasks like text generation? The authors explicitly state their evaluation is "limited to gender disparity and binary classification tasks."

**Open Question 2:** Do the quantified gender disparities in explanation metrics translate into significant differences in human comprehension or decision-making quality? The conclusion proposes "combining quantitative with human-based evaluation" to capture nuances missed by automated metrics.

**Open Question 3:** What specific theoretical mechanisms within post-hoc explanation methods cause them to generate disparate explanations independent of model bias? The authors call for a "deeper theoretical and empirical investigation into the causes of explanation bias" inherent to the methods.

## Limitations
- Sample size and generalization: Limited to English-language data and binary gender categories, restricting generalizability to other languages, non-binary gender identities, and intersectional dimensions
- Methodological constraints: Evaluation relies on automated metrics that may not fully capture practical impact; "training from scratch" approach may not completely eliminate all sources of bias
- Statistical interpretation: The threshold of |d| ≥ 0.2 for "considerable" disparity is somewhat arbitrary, and the practical significance of small but statistically significant differences remains unclear

## Confidence
- **High Confidence:** The claim that post-hoc explainability methods exhibit significant gender disparities across multiple datasets, models, and metrics is strongly supported by empirical results showing >72% of experimental combinations exhibit statistically significant differences
- **Medium Confidence:** The claim that disparity magnitude is modulated by task-influence is supported by comparative results but could benefit from more granular analysis
- **Low Confidence:** The claim that surrogate methods are inherently more prone to disparity than gradient methods due to local sampling sensitivity is based on observed patterns but lacks mechanistic explanation

## Next Checks
1. **Intersectional Analysis Validation:** Extend the analysis to include intersectional identity dimensions (race, age, socioeconomic status combined with gender) using datasets that contain these attributes
2. **Model Architecture Ablation:** Conduct a systematic ablation study varying model architecture parameters while keeping the explainability method constant to determine whether certain architectural choices amplify or mitigate method-intrinsic bias
3. **Real-World Impact Assessment:** Design a user study where practitioners use explanations from methods with known gender disparities to make decisions, measuring whether and how explanation disparities translate into decision-making disparities