---
ver: rpa2
title: 'From Colors to Classes: Emergence of Concepts in Vision Transformers'
arxiv_id: '2503.24071'
source_url: https://arxiv.org/abs/2503.24071
tags:
- concepts
- layers
- vits
- neurons
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first comprehensive layer-wise analysis
  of concepts encoded in Vision Transformers (ViTs) using neuron labeling techniques.
  The authors investigate how semantic concepts evolve across network layers in state-of-the-art
  ViTs, including supervised ViT, DINOv2, MAE, and CLIP, comparing them with ResNet50.
---

# From Colors to Classes: Emergence of Concepts in Vision Transformers

## Quick Facts
- **arXiv ID**: 2503.24071
- **Source URL**: https://arxiv.org/abs/2503.24071
- **Reference count**: 40
- **Primary result**: First comprehensive layer-wise analysis of concepts in Vision Transformers reveals hierarchical learning patterns similar to CNNs

## Executive Summary
This study presents the first comprehensive layer-wise analysis of concepts encoded in Vision Transformers (ViTs) using neuron labeling techniques. The authors investigate how semantic concepts evolve across network layers in state-of-the-art ViTs, including supervised ViT, DINOv2, MAE, and CLIP, comparing them with ResNet50. The key findings reveal that despite ViTs' global attention mechanisms, they still learn basic concepts like colors and textures in early layers and more complex concepts like objects and natural elements in later layers, mirroring the hierarchical feature extraction pattern observed in CNNs.

## Method Summary
The authors employ CLIP-Dissect for neuron labeling, analyzing the top activating images for each neuron across different layers of Vision Transformers and ResNet50. They use ImageNet validation set combined with Broden dataset as probing data, with a concept set of 20k most common English words. The analysis measures neuron label similarity scores via softWPMI, counts unique concepts per layer after mean-thresholding, categorizes concepts into nine semantic categories, and assesses image complexity using ICNet. Finetuning experiments on CUB and bloodMNIST datasets examine how concept distributions shift when models are adapted to specific downstream tasks.

## Key Results
- Despite global attention mechanisms, ViTs learn basic concepts (colors, textures) in early layers and complex concepts (objects, natural elements) in later layers
- Number of distinct concepts increases from early to late layers, with early neurons activating on simpler images and later neurons on more complex images
- Different pretraining strategies influence concept encoding, with DINOv2 showing particularly robust concept representations
- Finetuning to downstream tasks reduces total concept count and shifts concepts toward task-relevant categories while forgetting less relevant ones

## Why This Works (Mechanism)
The hierarchical concept emergence in ViTs works because the network architecture progressively builds representations from simple to complex features across layers, similar to CNNs. The self-attention mechanism allows ViTs to integrate global context while still maintaining local feature processing capabilities. The pretraining objectives (supervised, contrastive, masked autoencoding) shape the types of concepts learned, with self-supervised methods like DINOv2 developing more robust and stable representations that resist catastrophic forgetting during finetuning.

## Foundational Learning
- **Neuron labeling with CLIP-Dissect**: Why needed - to identify semantic concepts encoded by individual neurons; Quick check - verify that the same neuron consistently activates on semantically related images
- **Layer-wise analysis**: Why needed - to track how concepts evolve across network depth; Quick check - confirm that concept complexity increases monotonically from early to late layers
- **softWPMI similarity scores**: Why needed - to quantify semantic similarity between neuron activations and concept labels; Quick check - ensure scores correlate with human judgment of semantic relatedness
- **Concept categorization**: Why needed - to organize 1450+ unique labels into interpretable semantic groups; Quick check - validate category assignments using independent annotators
- **Image complexity measurement**: Why needed - to correlate concept sophistication with input image complexity; Quick check - confirm ICNet scores align with human perception of image complexity
- **Finetuning analysis**: Why needed - to understand how concept representations adapt to downstream tasks; Quick check - verify that concept reduction correlates with task performance improvements

## Architecture Onboarding

### Component Map
Probing Data -> CLIP-Dissect -> Neuron Activations -> Concept Labels -> Layer Analysis -> Finetuning Impact

### Critical Path
The neuron labeling pipeline (CLIP-Dissect) forms the critical path, as it directly determines which concepts are identified and how they're distributed across layers. The finetuning experiments depend on accurate initial concept identification to measure shifts in representation.

### Design Tradeoffs
The choice of neuron-level labeling assumes monosemanticity, potentially missing distributed or polysemantic concepts. Using English word lists as concepts may introduce language bias. The mean-thresholding approach for identifying significant concepts could either miss subtle representations or include noise depending on the threshold chosen.

### Failure Signatures
Low similarity scores or few neurons passing threshold indicate problems with concept set coverage or embedding computation. Inconsistent category proportions suggest issues with the categorization process or dataset selection. Unexpected concept distributions (e.g., complex concepts in early layers) may indicate problems with the labeling pipeline or model-specific architectural effects.

### First Experiments
1. Run CLIP-Dissect on a single layer of one model to verify the pipeline produces interpretable concepts
2. Compare concept distributions between two different models on the same layer to validate consistency
3. Test finetuning on a small dataset to confirm concept reduction patterns before scaling up

## Open Questions the Paper Calls Out
- What specific internal mechanisms enable DINOv2 to maintain more stable concept representations and require fewer weight adjustments during finetuning compared to other Vision Transformers?
- To what extent does the reduction in the number of encoded concepts during finetuning correlate with catastrophic forgetting of the original pretraining knowledge?
- Does the hierarchical emergence of concepts (from colors to objects) persist when using distributed concept discovery methods rather than single-neuron labeling?

## Limitations
- The analysis assumes monosemanticity, potentially missing distributed or polysemantic concepts
- Category assignments rely on external datasets and LLM-assisted labeling not fully detailed
- Direct comparisons between ViTs (12 blocks) and ResNet50 (5 layers) may be affected by architectural differences
- Exact CLIP model version and finetuning hyperparameters remain unspecified

## Confidence
- **High confidence**: General finding that both ViTs and CNNs learn basic concepts in early layers and complex concepts in later layers
- **Medium confidence**: Finetuning effects on concept reduction and task-specific shifts
- **Low confidence**: Exact numerical thresholds and category distributions due to unspecified implementation details

## Next Checks
1. Verify the CLIP model version and embedding computation matches the original CLIP-Dissect implementation by running a small test on known neurons
2. Replicate the finetuning procedure with identical hyperparameters (batch size, learning rate, scheduler, early stopping) on at least one model to confirm concept reduction patterns
3. Cross-validate the category assignment for the 1450 unique concepts using the provided external dataset or replicate the LLM categorization process to ensure consistency