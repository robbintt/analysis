---
ver: rpa2
title: 'DBR: Divergence-Based Regularization for Debiasing Natural Language Understanding
  Models'
arxiv_id: '2502.18353'
source_url: https://arxiv.org/abs/2502.18353
tags:
- shortcut
- training
- language
- tokens
- debiasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DBR proposes a method to reduce shortcut learning in NLU models
  by masking shortcut tokens and aligning output distributions between original and
  masked examples. Using Jensen-Shannon divergence as regularization loss, DBR improves
  out-of-domain performance on three NLU tasks (MNLI, FEVER, QQP) while maintaining
  in-domain accuracy.
---

# DBR: Divergence-Based Regularization for Debiasing Natural Language Understanding Models

## Quick Facts
- arXiv ID: 2502.18353
- Source URL: https://arxiv.org/abs/2502.18353
- Reference count: 0
- Primary result: DBR improves out-of-domain generalization on MNLI, FEVER, and QQP while maintaining in-domain accuracy through gradient-based shortcut detection and distribution alignment

## Executive Summary
DBR addresses shortcut learning in NLU models by identifying spurious tokens through Integrated Gradients and aligning output distributions between original and masked examples using Jensen-Shannon divergence. The method masks shortcut tokens probabilistically based on their "shortcut degree" (computed from bias-only model variance), forcing models to rely on genuine semantic features rather than superficial correlations. Experiments show consistent OOD improvements across three NLU tasks while maintaining or improving in-domain performance.

## Method Summary
DBR employs a three-stage pipeline: (1) train an identification model to find top-N shortcut tokens per sample using Integrated Gradients, (2) train a simple bias-only model on these tokens to compute shortcut degree via prediction variance, and (3) train a debiased model with combined cross-entropy and JSD loss, using soft masking where tokens are masked probabilistically based on shortcut degree. The soft masking strategy (Bernoulli sampling) proves more effective than hard masking at preserving semantic meaning while achieving debiasing goals.

## Key Results
- Improves MNLI HANS accuracy from 67.4% to 68.6% while maintaining 84.5% in-domain accuracy
- Increases FEVER Symmetric v1 accuracy from 68.7% to 70.0% with improved in-domain performance (90.7% vs 90.2%)
- Maintains or improves QQP performance while increasing PAWS accuracy from 57.8% to 63.1%
- Soft masking outperforms hard masking across all tasks and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based attribution identifies shortcut tokens that models rely on for spurious predictions
- Mechanism: Integrated Gradients computes token attributions by integrating gradients from zero embeddings to original input. Top-N tokens with highest attribution are identified as shortcuts. A bias-only model trained on only these tokens achieves 95-97% accuracy on in-domain data, confirming they capture spurious correlations
- Core assumption: High gradient attribution correlates with shortcut reliance rather than genuine semantic importance
- Evidence anchors: [abstract] "models often rely on superficial features and shortcuts"; [Section 3.2] "shortcut words mean that the prediction highly relies on these words"; [Table 3] LMI analysis shows negation words correlate with contradiction labels
- Break condition: If IG attributes high importance to genuinely semantic tokens, the method may mask wrong features

### Mechanism 2
- Claim: Aligning output distributions between original and masked inputs reduces shortcut reliance while preserving semantic understanding
- Mechanism: Jensen-Shannon Divergence measures distance between prediction distributions on original and masked inputs. Minimizing JSD forces similar predictions regardless of shortcut token presence
- Core assumption: If predictions differ significantly when shortcuts are masked, the model was shortcut-dependent; if they're similar, the model used genuine semantic features
- Evidence anchors: [abstract] "measures the divergence between the output distributions"; [Section 3.3] "prevents the model's predictions from being overly influenced by shortcut features"; [Figure 4] slower loss convergence indicates not taking shortcut path
- Break condition: If masked inputs lose too much semantic content, JSD minimization may force predictions based on insufficient information

### Mechanism 3
- Claim: Soft masking based on shortcut degree preserves semantic meaning better than hard masking while maintaining debiasing effectiveness
- Mechanism: Compute "shortcut degree" as normalized variance of bias-only model's predictions. Use Bernoulli distribution with this probability to decide whether to mask. High-variance (overconfident) predictions → higher masking probability
- Core assumption: High prediction variance indicates shortcut samples; not all examples need equal treatment
- Evidence anchors: [Section 3.4] "the more biased the model is, the more confident it becomes"; [Table 1] soft masking (84.5 MNLI dev, 68.6 HANS) outperforms hard masking (83.9 MNLI dev, 67.4 HANS)
- Break condition: If shortcut degree is miscalibrated, the balance between in-domain and OOD performance degrades

## Foundational Learning

- Concept: **Integrated Gradients (attribution methods)**
  - Why needed here: Core to identifying which tokens the model relies on; requires understanding how gradients flow through the model and how to attribute predictions to input features
  - Quick check question: Given a model prediction, can you explain why IG uses a baseline input and integrates gradients rather than just computing gradients at the final point?

- Concept: **Jensen-Shannon Divergence (distribution distance metrics)**
  - Why needed here: The regularization loss; need to understand why JSD is symmetric and bounded compared to KL divergence, and what minimizing it actually enforces
  - Quick check question: If JSD(p_original, p_masked) = 0, what does that imply about the model's reliance on the masked tokens?

- Concept: **Shortcut learning / spurious correlations in NLU**
  - Why needed here: The problem DBR solves; need intuition for why high training accuracy doesn't imply genuine understanding (e.g., "negation word → contradiction" heuristic)
  - Quick check question: On MNLI, if a model achieves 90% on HANS-non-entailment but only 60% on HANS-entailment, what shortcut might it be exploiting?

## Architecture Onboarding

- Component map:
  - **Stage 1 (Preprocessing):** Identification model (BERT-base) → Integrated Gradients → Top-N shortcut tokens per sample
  - **Stage 2 (Shortcut degree):** Bias-only model (MLP, 100 hidden dim) → Takes only shortcut tokens → Predictions → Variance → Normalized shortcut degree (0-1)
  - **Stage 3 (Training):** Debiased model (BERT-base) → Soft masking via Bernoulli(shortcut degree) → Dual forward pass (original + masked) → Cross-entropy + λ×JSD loss

- Critical path:
  1. Train identification model (12 epochs, full dataset)
  2. Compute IG attributions, extract top-N tokens (N=3 in experiments)
  3. Train bias-only model (1 epoch, 2-3K samples subset only—to keep it weak and biased)
  4. Compute shortcut degrees for all training samples
  5. Train debiased model with combined loss (λ=1.5 for MNLI/QQP, λ=3 for FEVER)

- Design tradeoffs:
  - **N (top-N tokens):** Higher N → more aggressive masking, but risks removing semantic content. Paper uses N=3
  - **λ (JSD weight):** Higher λ → stronger debiasing, but may hurt in-domain accuracy. Different per dataset
  - **Bias-only model complexity:** Kept simple (1-layer MLP) and undertrained (1 epoch) so it only learns shortcuts, not genuine features
  - **Hard vs. soft masking:** Hard masking simpler but damages semantics; soft masking stochastic but preserves meaning better

- Failure signatures:
  - **In-domain accuracy drops significantly (>3%):** λ too high or masking too aggressive—reduce λ or check if high-frequency words (Table 3: "the", "and") are being masked
  - **No OOD improvement:** Bias-only model may be too capable (learned genuine features, not just shortcuts)—reduce its training data or epochs
  - **Training loss oscillates:** Soft masking randomness too high; shortcut degree calculation may be unstable across batches
  - **Convergence too slow (>2x baseline):** Expected per Figure 4, but if excessive, may indicate masking is preventing any learning

- First 3 experiments:
  1. **Ablation on N (top-N tokens):** Run DBR with N∈{1, 3, 5, 10} on MNLI → measure trade-off between HANS (OOD) and MNLI-dev (in-domain) to find optimal masking aggressiveness
  2. **Bias-only model capacity study:** Train bias-only with {500, 1000, 2000, 5000} samples and {1, 3, 5} epochs → verify it achieves high train accuracy (>95%) on shortcut-only inputs but doesn't generalize; if it does, it's learning real features
  3. **JSD vs. KL divergence comparison:** Replace JSD with KL divergence (both directions) → hypothesis: KL is asymmetric and may over-penalize one distribution, while JSD's symmetry provides more stable optimization

## Open Questions the Paper Calls Out

- **Question:** Can the DBR framework be effectively adapted to Large Language Models (LLMs) that utilize the prompting paradigm rather than the traditional fine-tuning approach?
  - Basis in paper: [explicit] The conclusion states, "Moving forward, we plan to... extend the debiasing for large language models (LLMs) belonging to the prompting paradigm such as Llama-2, Mistral."
  - Why unresolved: The current method relies on gradient-based token identification and distribution alignment during fine-tuning; it is unclear if this mechanism translates to frozen models where only prompt embeddings are updated.
  - What evidence would resolve it: Application of DBR to prompt-tuning or prefix-tuning setups on LLMs, evaluating performance on the same OOD benchmarks (e.g., HANS, PAWS).

- **Question:** Would substituting masked shortcut tokens with alternative tokens improve semantic preservation compared to the current masking strategy?
  - Basis in paper: [explicit] The authors explicitly propose to "explore alternative masking strategies, such as substituting the masked shortcut tokens with alternative tokens."
  - Why unresolved: While masking removes shortcut features, it creates unnatural inputs; substitution might maintain sentence fluency and semantic integrity better than zero-masking or [MASK] tokens.
  - What evidence would resolve it: Comparative experiments analyzing the trade-off between OOD generalization and semantic similarity scores when using substitution versus masking.

- **Question:** How robust is the method if the Integrated Gradients (IG) identification step erroneously flags genuine, task-critical semantic tokens as shortcuts?
  - Basis in paper: [inferred] The paper notes that hard masking can damage semantic meaning, and Section 3.2 relies on IG to distinguish shortcuts. If IG fails to separate spurious correlation from semantic necessity, the soft masking could degrade useful features.
  - Why unresolved: The method assumes the "top-N" tokens identified are indeed shortcuts. However, interpretation methods like IG can be noisy, potentially identifying high-frequency semantic words as bias features.
  - What evidence would resolve it: Ablation studies analyzing the "false positive" rate of the identification model, specifically measuring how often tokens essential for ground-truth reasoning are masked.

## Limitations
- The validity of Integrated Gradients for shortcut token identification is not empirically validated against alternative attribution methods
- Soft masking introduces stochasticity that may obscure true debiasing effects and depends on unstable variance estimates
- The JSD regularization may force the model to lose genuine semantic understanding if masking removes too much information
- No analysis of how often the bias-only model learns genuine features rather than just shortcuts

## Confidence

**High Confidence:** The core experimental results showing DBR improves OOD performance on HANS, MNLI-hard, FEVER-Sym1/Sym2, and PAWS while maintaining in-domain accuracy are reproducible and well-supported.

**Medium Confidence:** The claim that Integrated Gradients reliably identifies shortcut tokens is plausible given the bias-only model results, but lacks direct validation against alternative attribution methods.

**Low Confidence:** The assertion that soft masking is fundamentally superior to hard masking for semantic preservation is weakly supported, showing better numerical results but lacking qualitative analysis of why it works better.

## Next Checks

**Validation Check 1:** Implement an ablation study comparing Integrated Gradients against random token selection for the bias-only model. Train the bias-only model on N=3 random tokens per sample and compare its accuracy to the IG-identified tokens. If random tokens achieve similar accuracy (>90%), this would suggest IG attribution is capturing something other than shortcuts.

**Validation Check 2:** Conduct a semantic preservation analysis using probing classifiers. After training DBR with both hard and soft masking, freeze the debiased model and train simple classifiers to predict linguistic properties (e.g., part-of-speech tags, named entities, syntactic dependencies) from the model's intermediate representations. Compare preservation of these semantic features between hard and soft masking variants.

**Validation Check 3:** Perform a failure mode analysis on the bias-only model. Systematically vary the number of training samples (100, 500, 1000, 2000, 3000) and training epochs (1, 3, 5) for the bias-only model. Plot the trade-off between bias-only model in-domain accuracy on shortcut-only inputs and DBR's OOD improvement to reveal whether the bias-only model is learning genuine features versus true shortcuts.