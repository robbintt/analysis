---
ver: rpa2
title: Integrating Domain Knowledge into Process Discovery Using Large Language Models
arxiv_id: '2510.07161'
source_url: https://arxiv.org/abs/2510.07161
tags:
- process
- claim
- rules
- domain
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework that integrates domain knowledge
  into process discovery using Large Language Models (LLMs). The approach extracts
  declarative rules from natural language descriptions provided by domain experts
  and uses them to guide the IMr process discovery algorithm.
---

# Integrating Domain Knowledge into Process Discovery Using Large Language Models

## Quick Facts
- arXiv ID: 2510.07161
- Source URL: https://arxiv.org/abs/2510.07161
- Reference count: 40
- Primary result: LLM-guided process discovery improves model quality by integrating domain knowledge, achieving 95-98% recall and 73-77% precision in rule extraction.

## Executive Summary
This paper presents a framework that integrates domain knowledge into process discovery using Large Language Models (LLMs). The approach extracts declarative rules from natural language descriptions provided by domain experts and uses them to guide the IMr process discovery algorithm. Multiple LLMs were evaluated, with OpenAI o3 and Gemini 2.5 Pro achieving the best performance. The framework demonstrated robust handling of ambiguous inputs, with OpenAI o3 most consistently engaging in clarification dialogues. A real-world case study with UWV confirmed the practical applicability, showing that incorporating domain knowledge significantly improved model quality compared to data-only discovery.

## Method Summary
The framework extracts declarative constraints from natural language process descriptions using LLMs, then guides the IMr (Inductive Miner) algorithm with these constraints. Domain experts provide process descriptions, which are transformed into formal Declare constraints through LLM prompt engineering with few-shot examples and strict JSON schema validation. The extracted rules are validated against the event log and used to prune the IMr search space by eliminating candidate process structures that would violate domain constraints. The approach includes interactive clarification for ambiguous inputs and iterative refinement of both rules and resulting process models.

## Key Results
- OpenAI o3 and Gemini 2.5 Pro achieved 95-98% recall and 73-77% precision in rule extraction
- Interactive clarification dialogues successfully handled ambiguous inputs, with OpenAI o3 engaging in clarification in 11 of 12 test cases
- Real-world UWV case study confirmed practical applicability and significant quality improvement over data-only discovery
- Sentence-level input (S2S) yielded ~8% higher recall while paragraph-level (PAR) achieved ~4% higher precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can reliably extract formal declarative rules from unstructured natural language process descriptions when properly constrained.
- Mechanism: The prompt generation service enforces a strict JSON schema, provides few-shot examples for each constraint template (e.g., Precedence, Response, NotCoExistence), and includes the exact activity list from the event log.
- Core assumption: The LLM has sufficient semantic understanding to map natural language phrasings to template semantics without formal training on Declare notation.
- Evidence anchors: "Our approach leverages LLMs to extract declarative rules from textual descriptions provided by domain experts." [section 4.2.1] "The prompt includes a specific set of supported constraint templates... Few-shot learning is implemented by the inclusion of labeled examples for each constraint type."

### Mechanism 2
- Claim: Declarative rules prune the IMr search space by eliminating candidate cuts that would violate domain constraints.
- Mechanism: IMr recursively partitions activities via binary cuts (sequence, choice, parallel, loop). The `explore(G(L), R)` function filters cuts against rules before cost-based selection.
- Core assumption: The rule set provided is both correct and consistent; contradictory rules create unsatisfiable search spaces.
- Evidence anchors: "These rules are used to guide the IMr discovery algorithm, which recursively constructs process models by combining insights from both the event log and the extracted rules." [section 3.2, Definition 7] "A cut c is said to violate a constraint r ∈ R... if for every process tree M ∈ M_c, there exists a trace σ ∈ φ(M) such that σ ⊭ r."

### Mechanism 3
- Claim: Interactive clarification dialogues improve rule quality when initial input is ambiguous or underspecified.
- Mechanism: The LLM is instructed to detect ambiguity (e.g., missing activity references, vague quantifiers like "sometimes", unsupported templates) and return clarification questions instead of rules.
- Core assumption: The LLM can reliably distinguish ambiguity that requires clarification from ambiguity it can safely resolve.
- Evidence anchors: "The framework demonstrated robust handling of ambiguous inputs, with OpenAI o3 most consistently engaging in clarification dialogues." [section 4.3] "When ambiguities or gaps are detected in the input text, the LLM is encouraged to formulate follow-up questions." [section 6.2, Table 4] OpenAI o3 correctly requested clarification in 11 of 12 ambiguous test cases.

## Foundational Learning

- Concept: **Declarative Process Constraints (Declare Templates)**
  - Why needed here: The entire framework hinges on mapping natural language to formal constraints like `Precedence(A, B)` or `NotCoExistence(A, B)`.
  - Quick check question: Given the trace ⟨A, B, C⟩, does it satisfy `Response(A, B)`? Does it satisfy `NotSuccession(B, A)`?

- Concept: **Inductive Mining and Directly-Follows Graphs**
  - Why needed here: IMr builds models recursively from directly-follows graphs. Understanding how cuts partition activities and how rules prune candidates is essential.
  - Quick check question: If an event log has edges A→B, B→C, A→C, what binary sequence cut would IMr consider, and how would a `NotSuccession(A, C)` rule affect it?

- Concept: **Process Discovery Quality Metrics (Fitness, Precision, Generalization)**
  - Why needed here: The paper evaluates models against ground truth using recall and precision on rules.
  - Quick check question: A model that allows all possible traces has perfect fitness but zero precision. Why does this matter for downstream conformance checking?

## Architecture Onboarding

- Component map:
  - Service Manager -> Prompt Generation Service -> LLM
  - LLM -> Rule Validation Service -> Error Handling Loop
  - Rule Validation Service -> Rule Evaluation Service -> User Interface
  - Service Manager -> IMr Discovery Service -> User Interface

- Critical path:
  1. Domain expert provides natural language input → Service Manager routes to Prompt Generation
  2. Prompt Generation assembles M₀, Mₗ, Mᵣ, Mₕ → LLM receives prompt
  3. LLM returns JSON → Rule Validation checks syntax and semantics
  4. If errors: Error Handling Loop re-prompts; if max iterations reached → failure
  5. If valid: Rule Evaluation computes support/confidence → UI displays for expert review
  6. Expert selects rules → IMr Discovery runs → Process model displayed
  7. Expert reviews model → provides feedback → loop returns to step 1 for refinement

- Design tradeoffs:
  - **Sentence-level (S2S) vs. Paragraph-level (PAR) input**: S2S yields ~8% higher recall (better coverage); PAR yields ~4% higher precision (fewer hallucinations)
  - **Zero-shot vs. Few-shot prompting**: Few-shot improves average recall by ~5% and precision by ~4%, but increases prompt token cost
  - **Model selection**: OpenAI o3 and Gemini 2.5 Pro offer best ambiguity handling and extraction quality; smaller models (GPT-4.1 Nano) have high error/failure rates

- Failure signatures:
  - **High error rate (>10%)**: Likely using underpowered model or malformed prompt; switch to few-shot with o3/Gemini 2.5 Pro
  - **Rules selected but model unchanged**: Rules may be redundant with log behavior or conflict with IMr representational bias
  - **LLM generates rules without clarification on ambiguous input**: Model not configured for clarification mode

- First 3 experiments:
  1. **Baseline extraction**: Run zero-shot S2S prompting on BPIC17 sentences with GPT-4.1 Nano; measure recall, precision, error rate
  2. **Few-shot + advanced model**: Run few-shot S2S with OpenAI o3 on same data; compare metrics
  3. **Interactive clarification test**: Provide ambiguous sentence to o3 and DeepSeek Chat; verify whether each asks clarification or hallucinates rules

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the process discovery algorithm be modified to successfully enforce long-term dependency rules that currently span multiple recursion steps?
- Basis in paper: The conclusion states that the IMr algorithm's recursive structure leads to the omission of long-term dependencies, causing the final model to violate rules like `Precedence(Block Claim 1, Unblock Claim 1)`.
- Why unresolved: The authors identify this as an inherent "representational bias" of the IMr technique, where the divide-and-conquer approach fails to preserve constraints across different sub-logs.
- What evidence would resolve it: An extension of the algorithm or a post-processing repair step that ensures the final model satisfies cross-cutting constraints that were previously violated.

### Open Question 2
- Question: Can the framework be extended to support constraint templates beyond the fixed set of Declare templates (e.g., cardinality constraints like "at most 2")?
- Basis in paper: The ambiguity evaluation demonstrated that LLMs fail to extract rules when the input text implies unsupported templates (e.g., `AtMost2`), limiting the expressiveness of domain experts.
- Why unresolved: The prompt engineering and validation services are currently hardcoded to a specific list of supported templates, rejecting or misinterpreting valid domain knowledge that falls outside this schema.
- What evidence would resolve it: A modified framework capable of dynamically accepting and processing a wider vocabulary of constraint types without re-engineering the validation service.

### Open Question 3
- Question: Does the integration of LLM-extracted rules improve model quality when applied to discovery algorithms other than IMr?
- Basis in paper: The paper focuses exclusively on the IMr framework, leaving the generalizability of this approach to other discovery paradigms (e.g., heuristic or genetic miners) untested.
- Why unresolved: The method relies on IMr's specific mechanism of using rules to prune "cuts"; it is unclear how declarative rules would guide algorithms that do not use a recursive cut-based structure.
- What evidence would resolve it: Comparative experiments applying the same LLM-extracted rules to different discovery algorithms and measuring the resulting improvement in alignment with domain knowledge.

## Limitations

- The framework's effectiveness depends heavily on the LLM's ability to correctly interpret ambiguous natural language descriptions, with smaller models showing significantly degraded performance
- The IMr algorithm's representational bias limits the ability to encode certain complex rules spanning multiple recursion levels, potentially excluding valid domain constraints from the final model
- The real-world UWV case study results are promising but lack detailed quantitative validation, and the event log's availability for independent verification is unclear

## Confidence

- **High confidence**: The framework's architecture and integration of declarative rules into IMr are technically sound and well-documented
- **Medium confidence**: The LLM's rule extraction performance shows strong results (95-98% recall, 73-77% precision) but depends heavily on model selection and prompt engineering details
- **Low confidence**: The real-world UWV case study results are promising but lack detailed quantitative validation

## Next Checks

1. Reproduce the rule extraction pipeline with BPIC17 using OpenAI o3 and Gemini 2.5 Pro with few-shot prompting to verify the reported 95-98% recall and 73-77% precision metrics
2. Test the framework with ambiguous input sentences across different LLM models (o3, Gemini 2.5 Pro, DeepSeek Chat) to quantify the frequency and quality of clarification dialogues versus hallucination
3. Apply the framework to a publicly available event log with known domain constraints to assess whether the IMr representational bias systematically excludes certain valid rules spanning multiple recursion levels