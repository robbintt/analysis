---
ver: rpa2
title: Multi-modal Relational Item Representation Learning for Inferring Substitutable
  and Complementary Items
arxiv_id: '2507.22268'
source_url: https://arxiv.org/abs/2507.22268
tags:
- item
- complementary
- substitutable
- items
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inferring substitutable and
  complementary items in e-commerce, which is complicated by noisy user behavior data
  and data sparsity due to long-tailed distributions. The proposed MMSC framework
  combines multi-modal item representation learning with a self-supervised denoising
  approach.
---

# Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items

## Quick Facts
- **arXiv ID:** 2507.22268
- **Source URL:** https://arxiv.org/abs/2507.22268
- **Reference count:** 40
- **Primary result:** Proposed MMSC framework outperforms baselines by 26.1% for substitutable and 39.2% for complementary recommendation, and is effective for cold-start items.

## Executive Summary
This paper addresses the challenge of inferring substitutable and complementary items in e-commerce, which is complicated by noisy user behavior data and data sparsity due to long-tailed distributions. The proposed MMSC framework combines multi-modal item representation learning with a self-supervised denoising approach. It uses a multi-modal foundational model to learn from item metadata, a meta-path encoder to capture item-item associations from user behaviors, and a hierarchical aggregation mechanism to integrate representations. The method also leverages large language models to augment training data and improve denoising. Extensive experiments on five real-world datasets show that MMSC outperforms existing baselines by 26.1% for substitutable and 39.2% for complementary recommendation, and it is effective for cold-start items.

## Method Summary
MMSC is a multi-modal relational item representation learning framework that addresses the challenge of inferring substitutable and complementary items in e-commerce. It combines a multi-modal foundational model (frozen BLIP-2 with learnable attention) for item metadata with a meta-path encoder for user behavior graphs. The framework uses self-supervised contrastive learning to denoise behavior graphs and hierarchical neural gating to fuse multi-modal and behavior-based representations. An LLM (Flan-T5-XXL) is used offline to augment and clean training data, improving the quality of supervised learning. The model is jointly optimized with a Triplet Loss and a self-supervised InfoNCE Loss.

## Key Results
- Outperforms existing baselines by 26.1% for substitutable and 39.2% for complementary recommendation on five real-world Amazon datasets
- Effectively handles cold-start items through a mean-pooling inference procedure
- Self-supervised denoising significantly improves performance, especially for complementary recommendations

## Why This Works (Mechanism)
The method works by leveraging multi-modal item representations from a frozen foundational model while denoising user behavior graphs through self-supervised contrastive learning. The meta-path encoder captures complex high-order associations in user behavior that standard GNNs miss. Neural gating mechanisms dynamically fuse multi-modal and behavior-based representations, as well as representations across the substitutable and complementary tasks. LLM augmentation filters noisy training data to improve supervised learning quality. This combination addresses both data sparsity and noise issues inherent in e-commerce recommendation.

## Foundational Learning
- **Concept: Meta-paths in Heterogeneous Graphs**
  - **Why needed here:** The paper uses meta-paths to define structured sequences of node types and relations (e.g., *substitute -> complementary -> substitute*). This is critical for capturing complex, high-order transitive associations in user behavior data that standard Graph Neural Networks (GNNs) on decoupled, homogeneous graphs would miss.
  - **Quick check question:** Can you explain how a meta-path like $v_1 \xrightarrow{s} v_2 \xrightarrow{c} v_3 \xrightarrow{s} v_4$ helps infer a complementary relationship between $v_1$ and $v_4$ better than just a direct $v_1 \xrightarrow{c} v_4$ edge?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - **Why needed here:** The model uses a contrastive loss to denoise user behavior graphs. Understanding that it maximizes similarity between "positive pairs" (representations of the same item from two different graph views) while minimizing it for "negative pairs" (representations of different items) is key to grasping the self-supervised denoising mechanism.
  - **Quick check question:** In the context of MMSC, what constitutes a "positive pair" and a "negative pair" for the self-supervised objective, and what property does this encourage in the learned representations?

- **Concept: Neural Gating Mechanisms**
  - **Why needed here:** The paper uses neural gates (learnable sigmoid functions) to dynamically fuse multi-modal and behavior-based representations, and again to fuse substitutable and complementary task representations. This is a soft, data-driven feature selection method, more flexible than simple concatenation or averaging.
  - **Quick check question:** How does the gating mechanism in MMSC's semantic-level aggregation (Eq. 8) decide how much weight to give the multi-modal representation ($q_i$) versus the behavior-based representation ($p_i$)?

## Architecture Onboarding
- **Component map:**
  1.  **Multi-Modal Encoder:** (Frozen BLIP-2 + Fine-tuned Attention Head) Processes item metadata (text, image) into a semantic representation ($q$).
  2.  **Behavior-based Encoder:** (Meta-path Attention + Path Attention) Processes the user behavior graph into a relational representation ($p$).
  3.  **Self-Supervised Denoising Head:** (Contrastive Loss) Operates on two views of the behavior-based representations.
  4.  **Hierarchical Aggregation:** (Neural Gates) Fuses $q$ and $p$ (semantic-level), then fuses representations across the substitutable and complementary tasks (task-level).
  5.  **LLM Augmentation Module:** (Offline process) Uses Flan-T5-XXL to filter a training subset ($E_{LLM}$) for the supervised loss.

- **Critical path:**
  1.  Generate item embeddings using frozen BLIP-2.
  2.  Pass embeddings through the meta-path encoder on the user behavior graph.
  3.  Apply graph-level dropout to create a perturbed graph view and compute the contrastive loss.
  4.  Fuse multi-modal and behavior-based representations via the semantic-level gate.
  5.  Fuse task-specific representations via the task-level gate.
  6.  Calculate the final multi-task loss (Triplet Loss on LLM-augmented data + Contrastive Loss).

- **Design tradeoffs:**
  - **Frozen vs. Fine-tuned Foundational Model:** The authors keep BLIP-2 frozen to leverage its pre-trained knowledge without the high cost of fine-tuning, adding only a small attention layer on top. The tradeoff is that BLIP-2 may not be optimally aligned with the nuances of e-commerce substitute/complementary semantics.
  - **LLM Augmentation Cost:** Using an LLM to filter training data improves quality but adds significant pre-processing cost and latency. The fixed subset size (500K) may be suboptimal for larger datasets, as seen in the Electronics results.
  - **Complex Meta-paths:** The model uses up to 3rd-hop meta-paths. This captures complex relationships but increases computational complexity and may introduce noise from very long, tenuous paths.

- **Failure signatures:**
  - **Cold-start items with uninformative metadata:** If new items lack text or have generic images, the multi-modal branch fails, and the model must fall back to a cold-start inference procedure (mean-pooling neighbors), which may be poor if similar items are not found.
  - **Systematic Noise:** The self-supervised denoising assumes noise is random. If a platform has a persistent, systematic type of miscategorization or bot activity, the model may learn to be invariant to it, failing to correct it.
  - **Conflicting Modalities:** If an item's image suggests "substitute" (looks the same) but its text description suggests "complementary" (different function), the semantic-level gate may struggle to resolve the conflict, leading to poor fusion.

- **First 3 experiments:**
  1.  **Validate LLM Augmentation:** Train a simplified model with and without the LLM-augmented training set ($E_{LLM}$ vs. raw $E$). Measure the performance delta to quantify the impact of label noise vs. the LLM's semantic judgment.
  2.  **Test Denoising Robustness:** Manually inject increasing levels of random noise (false edges) into the user behavior graph of a validation dataset and plot model performance degradation. Compare MMSC (with self-supervised loss) against a baseline without it to verify its robustness claim.
  3.  **Cold-Start Profiling:** Create a held-out set of items with no graph edges. Evaluate the model's performance on these items using only the multi-modal branch and the proposed cold-start inference procedure (Eq. 12). Compare this to a pure content-based baseline (e.g., fine-tuned BLIP-2).

## Open Questions the Paper Calls Out
- **How can the MMSC framework be extended to capture temporal dynamics in item relationships?**
  - **Basis in paper:** [explicit] The authors state in the Discussion that the current model "assumes static relationships and neglects temporal dynamics."
  - **Why unresolved:** The current architecture processes data as static snapshots, failing to account for time-evolving relationships (e.g., seasonal changes in complementary goods).
  - **What evidence would resolve it:** Integrating temporal encoders (e.g., RNNs or T-GNNs) and evaluating performance on time-sliced datasets to show stability over time.

- **How can independent item-item relationships be explicitly modeled during the training process?**
  - **Basis in paper:** [explicit] The Discussion lists the failure to "explicitly model independent item-item relationships during training" as a specific limitation.
  - **Why unresolved:** The model currently relies on coupled associations derived from user behavior and metadata, potentially conflating independent factors.
  - **What evidence would resolve it:** A modification of the loss function or architecture to disentangle relationship factors, validated through disentanglement metrics.

- **Can the meta-path schema be automatically learned or optimized rather than manually designed?**
  - **Basis in paper:** [inferred] The Implementation Details (ยง5.1.4) and Ablation Study (ยง5.3) rely on a fixed, manually defined set of meta-paths (e.g., $v_1 \xrightarrow{s} v_2 \xrightarrow{c} v_3$).
  - **Why unresolved:** Manual selection is labor-intensive and may not generalize optimally across different product categories or datasets.
  - **What evidence would resolve it:** A comparison of the current performance against a neural architecture search (NAS) or attention-based path selection method.

## Limitations
- The computational overhead of running LLM inference on 500K edges may be prohibitive for some teams
- The method assumes user behavior follows predictable high-order patterns, which may not hold across all e-commerce domains
- Key hyperparameters such as the Triplet Loss margin are unspecified, which could significantly impact results

## Confidence
- **High confidence** in the self-supervised denoising framework and multi-modal fusion approach
- **Medium confidence** in the scalability and generalizability of LLM augmentation
- **Low confidence** in cold-start item performance without extensive validation

## Next Checks
1. **Test robustness to noise:** Systematically inject synthetic noise into behavior graphs and measure performance degradation of MMSC vs. baseline models.
2. **Validate meta-path assumptions:** Compare performance using 2-hop vs. 3-hop meta-paths to quantify the marginal benefit of higher-order paths.
3. **Cold-start ablation:** Isolate and evaluate the multi-modal branch performance on cold-start items with no behavioral data to assess the viability of the mean-pooling inference approach.