---
ver: rpa2
title: Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors
arxiv_id: '2506.10146'
source_url: https://arxiv.org/abs/2506.10146
tags:
- hyperbolic
- out-of-distribution
- embeddings
- hierarchical
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses out-of-distribution (OOD) detection in deep
  learning, where the goal is to identify samples that do not belong to the distribution
  on which a network has been trained. The authors introduce Balanced Hyperbolic Learning,
  a method that leverages hierarchical hyperbolic embeddings for better OOD discrimination.
---

# Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors

## Quick Facts
- **arXiv ID:** 2506.10146
- **Source URL:** https://arxiv.org/abs/2506.10146
- **Reference count:** 13
- **Primary result:** Balanced Hyperbolic Learning improves OOD detection by leveraging hierarchical hyperbolic embeddings, outperforming state-of-the-art contrastive methods across 13 datasets and 13 scoring functions.

## Executive Summary
This paper introduces Balanced Hyperbolic Learning, a method for out-of-distribution (OOD) detection that leverages hierarchical hyperbolic embeddings. The approach represents classes as prototypes in hyperbolic space based on their semantic hierarchy, optimizing in-distribution samples to align with these prototypes while OOD samples naturally cluster near the origin. The key innovation is a distortion-based loss function with norm balancing across hierarchical levels, which prevents bias toward broader sub-trees and improves OOD discrimination. The method demonstrates superior performance compared to existing approaches, including state-of-the-art contrastive methods, across multiple datasets and scoring functions.

## Method Summary
The method learns balanced hyperbolic embeddings to position ID classes near the Poincaré ball boundary and OOD samples closer to the origin. It requires a class hierarchy G=(V,E) and uses two-stage training: first computing balanced hyperbolic prototypes via Riemannian SGD with distortion and norm losses, then training a ResNet-34 backbone with hyperbolic distance cross-entropy. The approach generalizes existing OOD scoring functions (MSP, Energy) to operate with hyperbolic prototypes, achieving better separation between ID and OOD samples through the geometric properties of hyperbolic space.

## Key Results
- Outperforms state-of-the-art contrastive methods (SimSiam) on OOD detection tasks
- Demonstrates superior performance across 13 datasets and 13 scoring functions
- Achieves better hierarchical OOD generalization, identifying the most closely related in-distribution class for OOD samples
- Ablation studies show norm balancing is critical for performance (AUROC and FPR@95 improvements)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping class hierarchies to hyperbolic prototypes aligns sample norms with semantic uncertainty.
- **Mechanism:** In hyperbolic space, the distance from the origin correlates with specificity. By anchoring ID classes to hierarchical prototypes, ID samples are pulled toward the boundary (high confidence), while OOD samples lacking hierarchical affiliation tend to settle near the origin (low confidence/uniform softmax).
- **Core assumption:** The norm of an embedding in the Poincaré ball functions as a reliable proxy for sample certainty.
- **Evidence anchors:** [section 1, Fig 1a] hyperbolic classifier provides strongly uniform distributions for samples near the origin; [section 6.3.5] histograms show OOD samples cluster at lower MSP/Energy scores.

### Mechanism 2
- **Claim:** A distortion loss preserves hierarchical relationships, preventing "prototype collapse."
- **Mechanism:** Optimizing embeddings to minimize the difference between graph distances and hyperbolic distances ensures prototypes are spaced according to semantic granularity, forcing unrelated ID classes to be far apart and leaving "gaps" where OOD samples can be detected.
- **Core assumption:** The provided hierarchy accurately reflects semantic relationships required for the downstream task.
- **Evidence anchors:** [section 4.2] optimize embeddings such that distances between nodes is similar to graph distances.

### Mechanism 3
- **Claim:** Norm balancing prevents bias toward deep/wide subtrees, ensuring consistent OOD thresholds.
- **Mechanism:** Standard hyperbolic embedding algorithms bias "shallow" sub-trees toward the origin (same region reserved for OOD/uncertain samples). By enforcing a norm loss that forces nodes at the same hierarchical level to share similar norms, the method clears the area near the origin, preventing ID classes from mimicking the "uncertainty" signature of OOD samples.
- **Core assumption:** Imbalanced hierarchies exist in the dataset, and standard Poincaré embeddings would otherwise displace shallow nodes incorrectly.
- **Evidence anchors:** [section 4.2] we want all points within a particular level to have the same norm; [section 6.3.1] ablation study shows removing balancing components lowers AUROC.

## Foundational Learning

- **Concept:** **Poincaré Ball Model & Riemannian Geometry**
  - **Why needed here:** This is the fundamental coordinate system. You must understand that gradients differ from Euclidean space (Riemannian SGD) and that distance scales exponentially near the boundary.
  - **Quick check question:** If a point has a Euclidean norm of 0.99, is it "close" to the origin in hyperbolic terms? (Answer: No, it is infinitely far/distant from the origin).

- **Concept:** **Exponential Map (exp0)**
  - **Why needed here:** The backbone outputs Euclidean vectors. The exponential map is the bridge that projects these vectors onto the hyperbolic manifold for distance computation.
  - **Quick check question:** How do you convert a standard ResNet feature vector into a hyperbolic embedding? (Answer: Apply `exp0(x)`).

- **Concept:** **OOD Scoring Functions (MSP, Energy)**
  - **Why needed here:** The paper does not propose a new score but generalizes existing ones. Understanding how logits convert to scores is necessary to see why `logits = -distance` works.
  - **Quick check question:** In this architecture, what replaces the standard dot-product logit calculation? (Answer: Negative hyperbolic distance).

## Architecture Onboarding

- **Component map:** Hierarchy Source (Graph G) -> Prototype Optimizer (Riemannian SGD training prototypes P_B using Distortion Loss + Norm Loss) -> Backbone (ResNet-34) -> Projection (Exponential Map `exp0`) -> Classifier (Fixed Hyperbolic Prototypes via Distance calculation) -> Scoring (Generalized MSP/Energy using negative distance)

- **Critical path:** The **two-stage training pipeline**. You cannot train the backbone end-to-end with prototypes simultaneously learning hierarchy effectively. Prototypes must be pre-computed and fixed before backbone training begins.

- **Design tradeoffs:**
  - **Embedding Dimension:** Lower dimensions (e.g., 8) increase distortion; 64 is the stable default
  - **Curvature (c):** The paper finds c=1 optimal for CIFAR; higher curvature degrades performance
  - **Temperature (γ):** Logits are distances (unbounded); scaling by γ=10 is critical for stable softmax gradients

- **Failure signatures:**
  - **High FPR@95 with Standard Methods:** Likely forgot the Norm Loss; shallow ID classes are collapsing near the origin
  - **High Distortion (>0.05):** Hierarchy is not being respected; check graph distance matrix or increase embedding dimensions
  - **Training Instability:** Forgetting to scale prototypes by 0.95 or logits by temperature γ

- **First 3 experiments:**
  1. **Prototype Fidelity:** Train only the prototypes (Algorithm 1) and plot the pairwise distance matrix to verify the hierarchy is visually reconstructed before touching the image model
  2. **Norm Variance Check:** Visualize the variance of norms per hierarchical level with and without the Norm Loss to confirm the balancing mechanism is active
  3. **Score Separation:** Run inference on a known OOD dataset (e.g., SVHN) and plot histograms of the Energy score to confirm ID and OOD distributions are distinct

## Open Questions the Paper Calls Out
The authors identify three main open questions: (1) How robust is the method to noise or errors in the provided class hierarchy, particularly when hierarchies are generated automatically by Large Language Models? (2) Does the geometry of balanced hyperbolic embeddings synergize with training-time regularization methods like outlier exposure? (3) Can the prototype learning algorithm scale efficiently to datasets with extremely large label spaces, such as ImageNet-1K or ImageNet-21K?

## Limitations
- Critically depends on having a meaningful class hierarchy, which may not be available for all datasets or tasks
- Effectiveness demonstrated primarily on image classification datasets with established hierarchies
- Claims about hierarchical OOD generalization assume a single correct "closest class" which may oversimplify semantic similarity
- Superiority over contrastive methods is shown but with limited direct comparisons to other contemporary OOD detection approaches

## Confidence
- **High Confidence:** The core mechanism of using hyperbolic prototypes for OOD detection is well-supported by experimental evidence and theoretical grounding in hyperbolic geometry
- **Medium Confidence:** The norm balancing mechanism's contribution is demonstrated through ablation studies, but could use more analysis of when hierarchical imbalance is severe enough to require this component
- **Medium Confidence:** The claim of superior performance over state-of-the-art contrastive methods is supported, but with limited direct comparisons to other contemporary OOD detection approaches

## Next Checks
1. **Hierarchy Sensitivity Analysis:** Systematically evaluate performance across datasets with varying hierarchy quality (synthetic hierarchies with controlled noise levels) to quantify the method's dependence on hierarchy accuracy
2. **Cross-Domain Generalization:** Test the method on non-image domains (e.g., text or tabular data) with available hierarchies to assess whether the approach generalizes beyond the primary experimental setting
3. **Alternative OOD Scoring Functions:** Evaluate additional OOD scoring functions (e.g., Mahalanobis distance, Gram matrices) generalized to hyperbolic space to determine if the benefits extend beyond the MSP and Energy scores tested