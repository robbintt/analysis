---
ver: rpa2
title: 'TASE: Token Awareness and Structured Evaluation for Multilingual Language
  Models'
arxiv_id: '2508.05468'
source_url: https://arxiv.org/abs/2508.05468
tags:
- tasks
- token
- task
- performance
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TASE, a comprehensive multilingual benchmark
  designed to evaluate the fine-grained capabilities of large language models, focusing
  on token-level awareness and structural reasoning across Chinese, English, and Korean.
  TASE addresses the gap in existing evaluations, which largely overlook basic, low-level
  text processing skills like character counting, token manipulation, and structural
  analysis.
---

# TASE: Token Awareness and Structured Evaluation for Multilingual Language Models

## Quick Facts
- **arXiv ID**: 2508.05468
- **Source URL**: https://arxiv.org/abs/2508.05468
- **Reference count**: 7
- **Primary result**: Introduces TASE benchmark revealing LLMs struggle with token-level and structural tasks, averaging 50-60% accuracy vs. human 89.24%

## Executive Summary
This paper introduces TASE, a comprehensive multilingual benchmark designed to evaluate the fine-grained capabilities of large language models, focusing on token-level awareness and structural reasoning across Chinese, English, and Korean. TASE addresses the gap in existing evaluations, which largely overlook basic, low-level text processing skills like character counting, token manipulation, and structural analysis. The benchmark consists of 10 tasks grouped into token awareness and structural understanding, with a dataset of 35,928 instances and a scalable synthetic data generation pipeline for training. Experiments on over 30 leading models reveal that all models fall significantly short of human-level performance, particularly on tasks requiring visual recognition or complex structural reasoning. Even top-tier models like O3 and Claude Opus 4 show persistent weaknesses, with average accuracy around 50-60%, compared to human performance of 89.24%. The results highlight the "tokenizer blindness" hypothesis, indicating that models struggle with tasks that depend on character-level or visual information due to their reliance on subword tokenization.

## Method Summary
TASE benchmark evaluates multilingual language models through 10 tasks across two dimensions: token awareness (tasks like counting characters/tokens, identifying positions, and understanding token boundaries) and structural understanding (tasks involving nested brackets, regular expressions, and context-free grammars). The benchmark covers Chinese, English, and Korean with 35,928 instances generated through a scalable synthetic pipeline. The evaluation tests models across varying sentence lengths (1-250 tokens) and includes both automatic and human evaluations. A Qwen2.5-14B model was fine-tuned using GRPO training to demonstrate targeted improvements, achieving nearly 2× average score increase. The study tests over 30 leading models including GPT-4o, Claude, Gemini, and various open-source models to establish comprehensive performance baselines.

## Key Results
- All models fall significantly short of human-level performance, averaging 50-60% accuracy versus human 89.24%
- O3 maintains near-perfect token awareness accuracy (80-100%) across all sentence lengths while other top models degrade sharply beyond 25 tokens
- Structural understanding tasks show particularly poor performance, with no model exceeding 70% accuracy on tasks like DOT and COMPC
- Cross-lingual performance varies significantly, with Korean showing largest gaps (67.83% for O3 vs. English 86.71%)
- GRPO fine-tuning of Qwen2.5-14B improved performance nearly 2×, demonstrating targeted training can help close the gap

## Why This Works (Mechanism)
TASE works by systematically evaluating language models at the token level, exposing fundamental limitations in how current models process text structure. The benchmark reveals that subword tokenization, while effective for semantic tasks, creates "tokenizer blindness" that prevents models from accurately handling character-level and structural operations. By testing across multiple languages and sentence lengths, TASE demonstrates that even the most advanced models struggle with basic text processing capabilities that humans perform effortlessly.

## Foundational Learning

**Subword tokenization (BPE/SentencePiece)**: Used to break text into manageable units for LLMs; needed because character-level models are computationally expensive and vocabulary size would be prohibitive.

**Context-free grammars**: Mathematical formalism for defining nested structures; required for tasks testing structural reasoning capabilities beyond simple pattern matching.

**Regular expressions**: Pattern-matching language for text processing; essential for evaluating models' ability to reason about text structure and syntax.

**GRPO training**: Group Relative Policy Optimization; fine-tuning method that improves model performance on specific task types through targeted training.

**Token awareness vs. semantic understanding**: Distinction between low-level text processing and high-level meaning comprehension; reveals models can understand meaning while failing at basic text manipulation.

**Cross-lingual evaluation methodology**: Systematic testing across languages with different scripts; needed to identify language-specific weaknesses in model capabilities.

## Architecture Onboarding

**Component map**: Data generation pipeline -> Task classification -> Model evaluation -> Performance analysis -> Fine-tuning (GRPO) -> Retesting

**Critical path**: Synthetic data generation → Task instantiation → Model inference → Accuracy calculation → Cross-lingual comparison → Performance gap analysis

**Design tradeoffs**: Synthetic data enables scalability but may introduce artifacts; subword tokenization balances efficiency with loss of character-level precision; multilingual coverage vs. depth of task variation.

**Failure signatures**: Consistent degradation on structural tasks across all models; specific weakness on visual/spatial tasks; cross-lingual gaps indicating tokenizer/language model mismatches.

**3 first experiments**:
1. Test single-token and character counting tasks to establish baseline token awareness
2. Evaluate nested bracket parsing with increasing depth to measure structural complexity handling
3. Compare performance across languages for identical task types to identify cross-lingual patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tokenizer-free or character-aware architectures (e.g., CharBERT) fundamentally overcome the structural understanding limitations observed in TASE, or are there deeper architectural constraints beyond tokenization?
- Basis in paper: [explicit] Appendix G states: "research into tokenizer-free or character-aware architectures... are critical areas of future work" and the main text confirms the "tokenizer blindness" hypothesis as an explanation for poor DOT/COMPM performance.
- Why unresolved: The paper demonstrates the problem but only evaluates existing tokenizer-based models; no character-aware architectures were tested against TASE.
- What evidence would resolve it: Benchmark results from models trained without subword tokenization on TASE tasks, particularly structural understanding tasks like DOT and COMPC.

### Open Question 2
- Question: What specific architectural or training innovations enable O3 to maintain near-perfect token awareness accuracy (80-100%) across all sentence lengths while other top-tier models (Gemini 2.5 Pro, Claude Opus 4) degrade sharply beyond 25 tokens?
- Basis in paper: [inferred] Figure 9 shows O3 as an "exceptional outlier" with stable accuracy across the full 1-250 token range, but the paper does not investigate or explain the mechanism behind this unique capability.
- Why unresolved: The paper identifies this divergence but cannot determine whether it stems from architecture, training data, or inference-time computation differences.
- What evidence would resolve it: Ablation studies comparing O3 variants with controlled architectural/training differences, or analysis of attention patterns during length-constrained tasks.

### Open Question 3
- Question: Can balanced multilingual pretraining alone close the cross-lingual performance gap for structurally complex languages (Korean: 67.83% for O3 vs. English: 86.71%), or are tokenizer-level innovations essential for scripts with compositional orthographies?
- Basis in paper: [explicit] Section 4.3 states: "Overcoming these issues calls for tokenizer innovations and more balanced, linguistically diverse pretraining" without determining the relative contribution of each factor.
- Why unresolved: Current models all use BPE/SentencePiece tokenizers optimized for alphabetic scripts, conflating the effects of pretraining data imbalance and tokenization mismatch.
- What evidence would resolve it: Training identical architectures with language-specific vs. universal tokenizers on balanced multilingual corpora and comparing TASE performance gaps.

### Open Question 4
- Question: Would the GRPO fine-tuning approach yield comparable improvements (nearly 2× average score increase) when applied to larger models (70B+ parameters) or different architectural families, or is its effectiveness bounded by model scale or base architecture?
- Basis in paper: [inferred] GRPO was only tested on Qwen2.5-14B, yet the paper notes that Qwen3 models consistently outperform Qwen2.5 "even at smaller scales—indicating that architecture design and training data quality play a more critical role than sheer scale."
- Why unresolved: The interplay between specialized fine-tuning, base architecture quality, and scale remains unclear for token-level capabilities.
- What evidence would resolve it: GRPO fine-tuning experiments on Qwen3-32B, Llama-3.3-70B, and other architectural families using the TASE synthetic pipeline.

## Limitations
- Synthetic data generation may introduce biases or artifacts not present in natural language
- Evaluation limited to three languages (Chinese, English, Korean) rather than broader multilingual coverage
- Token-level tasks may be overly sensitive to tokenizer differences, affecting cross-model comparability
- Absence of human performance baselines for individual tasks rather than aggregate scores

## Confidence
**High confidence**: The core methodology of TASE, the benchmark design, and the systematic evaluation of 30+ models are sound and well-documented. The finding that current LLMs struggle with token-level and structural tasks is robust and supported by multiple models.

**Medium confidence**: The interpretation that tokenizer blindness is the primary cause of performance gaps, as this could also reflect limitations in model architecture or training data rather than tokenization per se. The generalizability of results across languages and the effectiveness of targeted fine-tuning require further validation.

**Low confidence**: Claims about specific architectural changes needed to address identified weaknesses, as these are speculative and not directly tested in the paper.

## Next Checks
1. Replicate experiments using naturally occurring text samples rather than synthetic data to verify that performance gaps persist with authentic language patterns
2. Conduct cross-tokenizer evaluations using the same model architecture with different tokenization schemes to isolate tokenizer effects from other architectural factors
3. Extend benchmarking to additional languages, particularly those with different writing systems (e.g., Arabic, Hindi, Thai) to test the broader applicability of the "tokenizer blindness" hypothesis