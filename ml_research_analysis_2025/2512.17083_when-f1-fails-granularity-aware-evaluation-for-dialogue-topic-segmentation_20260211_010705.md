---
ver: rpa2
title: 'When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation'
arxiv_id: '2512.17083'
source_url: https://arxiv.org/abs/2512.17083
tags:
- boundary
- density
- segmentation
- evaluation
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dialogue topic segmentation is evaluated using boundary-based metrics
  (F1, W-F1) that conflate localization accuracy with segmentation granularity. This
  work introduces an evaluation framework that reports boundary density (BOR), segment
  alignment diagnostics (purity and coverage), and window-tolerant F1 (W-F1) together.
---

# When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation
## Quick Facts
- arXiv ID: 2512.17083
- Source URL: https://arxiv.org/abs/2512.17083
- Authors: Michael H. Coen
- Reference count: 10
- Current boundary-based metrics conflate localization accuracy with segmentation granularity

## Executive Summary
Dialogue topic segmentation evaluation currently relies on boundary-based metrics like F1 and W-F1 that fail to distinguish between accurate boundary localization and appropriate segmentation granularity. This work introduces a framework that separates boundary scoring from boundary selection, enabling evaluation across different boundary density regimes rather than at a single threshold. The framework reports boundary density (BOR), segment alignment diagnostics (purity and coverage), and window-tolerant F1 (W-F1) together. Empirical results across eight dialogue datasets demonstrate that current metrics are dominated by boundary density alignment, with threshold sweeps producing larger performance changes than switching between segmentation methods.

## Method Summary
The proposed framework introduces three key components: boundary density measurement (BOR), segment alignment diagnostics (purity and coverage), and window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, the framework enables evaluation across different boundary density regimes rather than at a single threshold. This separation allows practitioners to evaluate segmentation systems' ability to localize boundaries accurately while independently controlling the desired segmentation granularity. The framework is applied through threshold sweeps that reveal how performance varies across different boundary densities, providing insights into the relationship between boundary localization and granularity.

## Key Results
- Current boundary-based metrics are dominated by boundary density alignment across eight dialogue datasets
- Threshold sweeps produce larger W-F1 changes than switching between segmentation methods
- Boundary density emerges as a controllable design parameter rather than an incidental evaluation artifact

## Why This Works (Mechanism)
The framework works by mathematically separating the boundary scoring process from boundary selection. Traditional F1 metrics combine these two steps, making it impossible to distinguish whether poor performance stems from inaccurate boundary localization or inappropriate granularity choice. By decoupling these components, the framework can evaluate localization accuracy independently of granularity preferences, revealing that many performance differences attributed to segmentation methods are actually artifacts of boundary density alignment.

## Foundational Learning
- **Boundary density (BOR)**: The ratio of detected boundaries to total possible boundaries; needed to normalize across different segmentation granularities; quick check: compute as boundaries detected / (document length - 1)
- **Window-tolerant F1 (W-F1)**: Allows boundary matches within a tolerance window rather than exact matches; needed to account for practical uncertainty in boundary placement; quick check: vary window size to observe sensitivity
- **Purity and coverage metrics**: Measure segment alignment quality; needed to evaluate how well segments correspond between reference and prediction; quick check: purity near 1.0 indicates segments contain mostly correct content
- **Threshold sweep methodology**: Systematically varies boundary selection thresholds; needed to reveal density-dependent performance patterns; quick check: plot W-F1 against threshold to identify optimal densities

## Architecture Onboarding
- **Component map**: Data preprocessing -> Boundary detection -> Boundary scoring (BOR, purity, coverage) -> W-F1 calculation -> Density-aware evaluation
- **Critical path**: The sequence from raw dialogue text through feature extraction to boundary prediction, then through the three scoring components to final evaluation metrics
- **Design tradeoffs**: Balance between localization precision (narrow windows) and practical usability (wider tolerance); trade-off between detailed granularity reporting and simplicity of single-metric evaluation
- **Failure signatures**: Over-segmentation (high BOR, low purity), under-segmentation (low BOR, high coverage mismatch), localization drift (W-F1 drops with window size)
- **First experiments**: 1) Apply framework to existing segmentation system and compare traditional vs. density-aware metrics; 2) Conduct threshold sweep across multiple datasets to map density-performance landscape; 3) Test framework on non-dialogue domains to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical analysis is confined to dialogue datasets, limiting generalizability to other domains
- Limited practical guidance on how practitioners should choose optimal boundary densities for specific applications
- Framework validation relies heavily on density effects that may be domain-specific

## Confidence
- High: Core claim that current evaluation conflates localization accuracy with segmentation granularity
- Medium: Claim that granularity is a controllable design parameter rather than incidental artifact
- Low: Framework's effectiveness in non-dialogue domains

## Next Checks
1. Test the framework across diverse text domains (news, academic, conversational) to verify density effects generalize beyond dialogue
2. Conduct user studies to determine whether practitioners can effectively leverage the tunable boundary density parameter for downstream tasks
3. Implement the framework in an existing segmentation system to measure real-world impact on system design and evaluation practices