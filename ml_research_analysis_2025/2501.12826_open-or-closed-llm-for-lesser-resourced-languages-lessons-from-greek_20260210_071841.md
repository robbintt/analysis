---
ver: rpa2
title: Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek
arxiv_id: '2501.12826'
source_url: https://arxiv.org/abs/2501.12826
tags:
- greek
- llama
- dataset
- table
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of advancing NLP for lesser-resourced
  languages, using Modern Greek as a case study. The authors compiled a collection
  of publicly available Greek datasets and benchmarked open-source (Llama-70b) and
  closed-source (GPT-4o mini) LLMs on seven core NLP tasks.
---

# Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek

## Quick Facts
- arXiv ID: 2501.12826
- Source URL: https://arxiv.org/abs/2501.12826
- Reference count: 17
- Key outcome: This paper benchmarks open-source (Llama-70b) and closed-source (GPT-4o mini) LLMs on seven Greek NLP tasks plus two novel benchmarks, revealing task-specific strengths and introducing the STE methodology for long legal text clustering.

## Executive Summary
This study addresses the challenge of advancing NLP for lesser-resourced languages through a comprehensive benchmark of Greek language tasks. The authors compiled FAIR-compliant Greek datasets and evaluated both open and closed LLMs across seven core NLP tasks. They found that model performance varies significantly by task, with Llama excelling in NER and summarization while GPT performs better in GEC, MT, intent classification, and POS tagging. The paper also introduces two novel benchmarks: a long legal text clustering task using STE methodology that outperformed traditional TF-IDF, and an authorship attribution task that revealed potential data contamination in LLM pre-training.

## Method Summary
The study compiled 12 FAIR-compliant Greek datasets from public sources, covering seven core NLP tasks. Models were evaluated using 0-shot prompting with Llama-70b-Instruct and GPT-4o mini. For tasks without provided test sets, 175 random samples were used. A novel STE (Summarize, Translate, Embed) pipeline was developed for long legal text clustering, which outperformed TF-IDF baselines. The STE methodology involves LLM summarization, translation to English, and dense embedding generation. Author attribution was tested as a probe for potential training data contamination, with zero-shot classification across multiple Greek authors.

## Key Results
- Llama-70b excels in NER and summarization tasks, while GPT-4o mini performs better in GEC, MT, intent classification, and POS tagging
- The STE methodology for legal text clustering outperformed traditional TF-IDF approaches across multiple evaluation metrics (NMI, AMI, accuracy)
- Authorship attribution revealed high zero-shot accuracy for certain authors, suggesting potential data usage in LLM pre-training
- Machine translation quality correlates with the resource tier of target languages, with Greek-English outperforming Greek-Japanese and Greek-Farsi

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The STE (Summarize, Translate, Embed) pipeline improves clustering for long legal texts in lesser-resourced languages by condensing content and leveraging higher-quality English embeddings.
- Mechanism: LLM summarization reduces sequence length while preserving semantic content; translation into English (higher-resource) enables use of more robust embedding models; dense embeddings capture semantic similarity better than sparse TF-IDF on raw Greek text.
- Core assumption: LLM summarization retains task-relevant legal concepts without critical hallucination; Instructor embeddings transfer effectively across the translation gap.
- Evidence anchors:
  - [abstract]: "...a Summarize, Translate, and Embed (STE) methodology, which outperformed the traditional TF-IDF..."
  - [section]: Table 11 shows STE (Dense-En-Sum) outperforming TF-IDF-Gr-Full across NMI, AMI, and accuracy at multiple granularities (k=47, 374, 1,685).
  - [corpus]: *Plutus: Benchmarking LLMs in Low-Resource Greek Finance* (FMR: 0.54) reinforces domain-specific challenges in Greek, indirectly supporting adapted approaches like STE.
- Break condition: If summarization drops key legal distinctions or translation introduces systematic errors, embedding quality degrades and clustering performance may fall below TF-IDF baselines.

### Mechanism 2
- Claim: Zero-shot authorship attribution can serve as a probe for potential data contamination in LLM pre-training.
- Mechanism: Models achieving high zero-shot accuracy on specific authors' works likely encountered those texts during pre-training; near-zero performance on other authors suggests absence from training corpora.
- Core assumption: High zero-shot attribution accuracy primarily indicates memorization/prior exposure rather than robust stylistic generalization.
- Evidence anchors:
  - [abstract]: "...high 0-shot accuracy suggesting potential data usage by LLMs in pre-training."
  - [section]: Table 12 shows high F1 for classical authors (Plato: 0.75–0.82, Aeschylus: 0.33–0.57) and zero F1 for several modern/lesser-known authors, consistent with differential pre-training exposure.
  - [corpus]: Related papers discuss data scarcity broadly but offer no direct evidence on this attribution-contamination hypothesis.
- Break condition: If high accuracy stems from generalizable style learning rather than memorization, the probe's validity for contamination detection fails.

### Mechanism 3
- Claim: Machine translation quality from Greek correlates with the resource tier of the target language, reflecting training data distribution.
- Mechanism: Higher-resource target languages (e.g., English) have more pre-training representation, yielding better translation fluency and accuracy; lower-resource targets (e.g., Farsi) constrain model performance.
- Core assumption: ACL Anthology study counts serve as a reliable proxy for the volume and quality of training data available for each language.
- Evidence anchors:
  - [section]: §3.3 and Table 6 demonstrate performance gradients: EL-EN (BERTScore F1: 0.80–0.81) > EL-JPN (0.49–0.54) ≈ EL-FA (0.54), aligned with resource tiers.
  - [section]: Explicit classification of English (well-supported, 6,915 studies), Japanese (moderately-supported, 808), and Farsi (less-supported, 98).
  - [corpus]: No corpus papers directly validate or refute this tier-performance link for these specific models.
- Break condition: If model-specific fine-tuning or architectural inductive biases dominate over raw training data volume, the tier correlation weakens.

## Foundational Learning

- Concept: FAIR Data Principles (Findable, Accessible, Interoperable, Re-usable)
  - Why needed here: The paper explicitly filters Greek datasets by licensing, accessibility, and re-usability to ensure reproducible benchmarking. Understanding FAIR is prerequisite to interpreting why 94 datasets narrowed to 12 usable ones.
  - Quick check question: Can you name two common license restrictions that would violate the "R" (Re-usable) criterion for derivative works?

- Concept: Bits Per Character (BPC) for Linguistic Distance
  - Why needed here: The paper uses character-level language models and BPC to quantify stylistic and linguistic distance between corpora (Figure 4), informing dataset selection and potential domain transfer.
  - Quick check question: If a language model trained on dataset A achieves lower BPC on dataset B than on its own held-out data, what might that imply about their linguistic relationship?

- Concept: Normalized Mutual Information (NMI) for Clustering Evaluation
  - Why needed here: NMI is the primary metric for the legal text clustering task, comparing predicted clusters to ground truth labels independent of label permutations.
  - Quick check question: Why is NMI preferred over raw accuracy for evaluating clustering when the number of clusters equals the number of classes but labels are arbitrary?

## Architecture Onboarding

- Component map:
  Data Layer -> Model Layer -> Evaluation Layer -> Orchestration Layer
  12 FAIR Greek datasets across 7 tasks + 2 novel tasks
  Llama-70b-Instruct and GPT-4o-mini with 0-shot prompting + Instructor embeddings
  Task-specific metrics (F1, BERTScore, ROUGE, WER/CER, NMI/AMI/ARI)
  Unified code repository for dataset compilation, benchmark execution, and result aggregation

- Critical path:
  1. Identify and download FAIR-compliant datasets (Table 1, Table 3)
  2. Standardize splits (use provided test sets or sample n=175 instances)
  3. Execute 0-shot prompts for each task-model pair
  4. Compute task-specific metrics; for clustering, run STE pipeline (summarize → translate → embed → k-means)
  5. Aggregate and compare results (Tables 4–12)

- Design tradeoffs:
  - Open vs. Closed Models: Llama offers transparency and local execution but requires infrastructure; GPT offers ease of use but introduces cost and reproducibility concerns
  - Sampling vs. Full Test Sets: Sampling (n=175) reduces cost but may increase variance; full test sets preferred where available
  - STE vs. TF-IDF for Clustering: STE adds LLM inference cost but yields semantic embeddings; TF-IDF is cheaper but sparse and less effective for long texts

- Failure signatures:
  - Zero or near-zero macro-F1 in classification/sequence tasks (e.g., NER macro-F1: 0.14) indicates class imbalance or label confusion despite high weighted-F1
  - High WER/CER in translation or GEC suggests structural output mismatches (e.g., different wording, over-correction)
  - Clustering NMI collapse (<0.2) suggests embeddings fail to capture topical structure or k is misconfigured

- First 3 experiments:
  1. Reproduce the STE vs. TF-IDF clustering comparison (Table 11) on a 500-document subset to validate pipeline and compute resource requirements
  2. Run 0-shot authorship attribution (Table 12) with logging to identify which authors yield high vs. zero accuracy, probing potential contamination
  3. Conduct ablation on translation within STE (Greek-only embeddings vs. English-translated embeddings) to isolate the contribution of translation to clustering quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent has GPT-4o mini been exposed to publicly available Greek benchmark datasets during pre-training, and does this contaminate evaluation results?
- Basis in paper: [explicit] The authors note regarding GEC performance: "Unless GPT has already used the data of Korre et al. (2021) during training, a possibility we cannot exclude for this dataset, this is a very low rate."
- Why unresolved: Closed-source models like GPT do not disclose training data, making data contamination unverifiable.
- What evidence would resolve it: Systematic comparison of GPT performance on datasets released before vs. after its training cutoff, or controlled experiments with newly created Greek evaluation data.

### Open Question 2
- Question: Why do open-source and closed-source LLMs exhibit task-specific performance disparities in Greek NLP, with Llama excelling at NER/summarization and GPT excelling at GEC/POS/intent classification?
- Basis in paper: [inferred] The paper reports these divergent strengths without providing causal explanations for the task-specific patterns.
- Why unresolved: The training data composition, architectural differences, and optimization objectives differ between models but are not fully transparent.
- What evidence would resolve it: Ablation studies controlling for model scale, pre-training data composition analysis, and systematic probing of linguistic capabilities per task.

### Open Question 3
- Question: Can the STE (Summarize, Translate, Embed) methodology generalize to other lesser-resourced languages beyond Greek, and under what conditions does it fail?
- Basis in paper: [explicit] The authors state "our method is applicable to other languages, if a systematic literature review for that language exists," but do not test this claim.
- Why unresolved: Greek has relatively more resources than many lesser-resourced languages; the STE pipeline's translation quality and summarization fidelity may degrade with less-supported target languages.
- What evidence would resolve it: Replication of STE clustering experiments across multiple lesser-resourced languages with varying resource levels.

### Open Question 4
- Question: Why did Meltemi, a Greek-specific continually pre-trained LLM, fail to understand core NLP tasks that both multilingual models handled?
- Basis in paper: [inferred] Footnote 4 states that Meltemi "failed to understand the task at hand (i.e., Toxicity and GEC)" without further investigation.
- Why unresolved: The failure mode was not diagnosed—it could stem from instruction-tuning quality, scale limitations, or pre-training data issues.
- What evidence would resolve it: Controlled experiments comparing Meltemi against base Mistral on the same tasks with detailed error analysis.

## Limitations
- The study relies exclusively on 0-shot prompting without fine-tuning or few-shot examples, potentially underestimating model capabilities
- The 175-sample evaluation strategy introduces sampling variance that could affect task-level conclusions
- Closed-source GPT-4o mini's black-box nature and cost structure limit reproducibility and long-term benchmarking
- The authorship attribution contamination hypothesis remains speculative without direct evidence of training data composition

## Confidence
- **High**: Task-specific performance rankings (Llama excels in NER/summarization; GPT excels in GEC/MT) are well-supported by consistent metric patterns across multiple datasets
- **Medium**: The STE methodology's superiority over TF-IDF is demonstrated but may not generalize to other languages or document types without further validation
- **Low**: The authorship attribution-contamination hypothesis lacks direct evidence and could reflect other factors like stylistic generalization rather than memorization

## Next Checks
1. Replicate the benchmark with few-shot prompting (3-5 examples) to establish upper bounds and compare against 0-shot results for each task-model combination
2. Conduct a systematic evaluation of the STE pipeline's sensitivity to translation quality by comparing clustering performance when using Greek embeddings directly versus English-translated embeddings across different document types
3. Design controlled experiments to distinguish between memorization and stylistic generalization in the authorship attribution task, potentially using synthetic or stylistically manipulated texts to probe model behavior