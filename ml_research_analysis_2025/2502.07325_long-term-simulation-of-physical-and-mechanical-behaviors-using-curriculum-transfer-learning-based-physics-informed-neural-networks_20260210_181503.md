---
ver: rpa2
title: Long-term simulation of physical and mechanical behaviors using curriculum-transfer-learning
  based physics-informed neural networks
arxiv_id: '2502.07325'
source_url: https://arxiv.org/abs/2502.07325
tags:
- learning
- time
- pinn
- domain
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Curriculum-Transfer-Learning based physics-informed
  neural network (CTL-PINN) for long-term simulation of physical and mechanical behaviors.
  The core idea is to decompose long-term problems into a sequence of short-term subproblems,
  initially solving the first sub-problem using standard PINN, then progressively
  addressing subsequent time-domain problems using curriculum learning and transfer
  learning techniques.
---

# Long-term simulation of physical and mechanical behaviors using curriculum-transfer-learning based physics-informed neural networks

## Quick Facts
- **arXiv ID:** 2502.07325
- **Source URL:** https://arxiv.org/abs/2502.07325
- **Reference count:** 8
- **Primary result:** Introduces CTL-PINN for accurate long-term simulation of PDEs by decomposing problems into sequential short-term subproblems using curriculum and transfer learning.

## Executive Summary
This paper presents the Curriculum-Transfer-Learning based Physics-Informed Neural Network (CTL-PINN) method for solving time-dependent partial differential equations (PDEs) over extended time domains. The core innovation lies in decomposing long-term problems into a sequence of short-term subproblems, first solving the initial problem with standard PINN, then progressively addressing subsequent time-domain problems using curriculum learning and transfer learning techniques. This approach overcomes limitations of standard PINNs, such as local optimization issues, and addresses inaccuracies over extended time domains encountered in previous methods. The method is validated on nonlinear wave propagation, Kirchhoff plate dynamic response, and a complex hydrodynamic model of the Three Gorges Reservoir Area, demonstrating superior accuracy compared to standard PINN while enabling simulations over much longer time spans.

## Method Summary
The CTL-PINN method addresses long-term PDE simulation by decomposing the problem into sequential time-domain subproblems. It begins by training a standard PINN on an initial short time window (0, T_p]. The method then enters a curriculum learning phase, progressively expanding the solvable domain by retraining the same network with parameters transferred from the previous step and using predictions from the previous model as additional supervised points in the overlapping region. When curriculum learning reaches its limit (typically after 3-4 expansions), the method switches to transfer learning, training new models for non-overlapping subsequent time windows. Each new model is initialized with parameters from the final curriculum model and uses predictions from the source domain as initial conditions. The combined approach leverages the strengths of both techniques: curriculum learning for efficient domain expansion and transfer learning for stacking models to cover very long time horizons.

## Key Results
- CTL-PINN achieves more accurate solutions than standard PINN for long-term simulations, with test set errors for discharge and water level in the Three Gorges case reduced to 1.89×10^-3 and 1.20×10^-4, respectively
- The method successfully solves for longer time domains than standard PINN while maintaining accuracy, addressing the fundamental limitation of PINNs in long-duration simulations
- For the Three Gorges Reservoir hydrodynamic model, CTL-PINN successfully inverts roughness parameters and predicts flow and water levels with higher accuracy compared to standard PINN
- Error accumulation during sequential training is mitigated through the use of additional supervised learning points from previous time domains, though this remains an area of potential improvement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing long-term simulations into sequential short-term subproblems improves solution stability and accuracy over extended time domains.
- **Mechanism:** The method first solves an initial short-term subproblem using a standard Physics-Informed Neural Network (PINN). It then iteratively solves subsequent subproblems (time windows), transferring learned neural network parameters from the previous time window to initialize the next. This provides a better starting point for the optimizer and reduces the effective search space. The process uses a curriculum learning approach to progressively expand the solvable domain of a single model, followed by a transfer learning approach that stacks models to extend the domain further.
- **Core assumption:** The solution in a future time domain is sufficiently continuous and similar to the solution in the immediately preceding domain such that transferring network parameters provides a useful initialization.
- **Evidence anchors:** [abstract] "The main innovation of CTL-PINN lies in decomposing long-term problems into a sequence of short-term subproblems." [section 1] "We utilize curriculum learning to expand the temporal domain solvable by individual models, establishing a basis for transfer learning in subsequent steps."

### Mechanism 2
- **Claim:** Integrating additional supervised learning points from previous time domains mitigates error accumulation during sequential training.
- **Mechanism:** When training the neural network for a new target time domain, the method generates an "additional supervised learning dataset" by using the already-trained model from the source time domain to provide pseudo-labels for points within the overlapping region. The network is then trained to minimize the loss on both the PDE physics and these pseudo-labels, acting as a form of consistency regularization.
- **Core assumption:** The predictions from the source domain model, while not perfect, are sufficiently accurate to serve as ground truth for guiding the target domain model.
- **Evidence anchors:** [abstract] "...a curriculum learning approach that integrates information from previous steps." [section 3.1] "Additionally, extra supervised learning points can be extracted from the intersection of the two time domains... to facilitate training in the target time domain."

### Mechanism 3
- **Claim:** Curriculum learning expands the solvable time domain of a single PINN model beyond what standard training allows, while transfer learning facilitates long-term domain extension via model stacking.
- **Mechanism:** Curriculum learning trains a single PINN starting with a small time domain and progressively expands this domain. Once curriculum learning reaches a limit, the strategy shifts to transfer learning, training new models for subsequent, non-overlapping time windows initialized with parameters from the final curriculum model. This stacks multiple models to cover the full time horizon.
- **Core assumption:** A single neural network has a finite capacity to represent complex, long-duration dynamics, and a "stacking" approach with transferred initialization is more robust for very long simulations.
- **Evidence anchors:** [abstract] "This approach combines the strengths of curriculum learning and transfer learning to overcome the limitations of standard PINNs... and addresses the inaccuracies over extended time domains encountered in CL-PINN and the low computational efficiency of TL-PINN." [section 3.1] "Each curriculum learning step can expand the solvable time domain, but... curriculum learning cannot continue indefinitely."

## Foundational Learning

- **Concept: Physics-Informed Neural Networks (PINNs)**
  - **Why needed here:** This is the core solver. The entire method is built on top of the standard PINN architecture.
  - **Quick check question:** Can you write down the three components of the loss function in equation (4) and explain what each term represents physically?

- **Concept: Curriculum Learning**
  - **Why needed here:** This is the first phase of the CTL-PINN method, used to train a single PINN to solve for a larger time domain than it could learn from scratch.
  - **Quick check question:** How does curriculum learning change the training process compared to training a standard PINN on the full target time domain all at once?

- **Concept: Transfer Learning**
  - **Why needed here:** This is the second phase of the CTL-PINN method, used to "stack" models and extend the simulation beyond the capacity of a single curriculum-trained PINN.
  - **Quick check question:** In the context of this paper, what is transferred from the "source" time domain model to initialize the "target" time domain model?

## Architecture Onboarding

- **Component map:** Inputs (X, t) -> Neural Network (u_θ(X, t)) -> Loss Function Calculator (L_i, L_r, L_sp) -> Optimizer (Adam/L-BFGS) -> Trainer/Loop (curriculum and transfer logic)

- **Critical path:** The success of the entire system hinges on the initial standard PINN training on a short time domain. If this first model fails to converge to an accurate solution, all subsequent curriculum and transfer learning steps will propagate these errors.

- **Design tradeoffs:**
  1. **Step Size (ΔT_pc, ΔT_ct):** Smaller steps increase the number of training steps and total computational cost but generally lead to higher accuracy by reducing the difficulty of each subproblem.
  2. **Curriculum vs. Transfer Learning:** Using only curriculum learning (CL-PINN) is simpler but limited in the total time domain it can solve accurately. Using only transfer learning (TL-PINN) can be computationally inefficient.

- **Failure signatures:**
  1. **Rapidly increasing L2 error:** Plot the L2 error at the end of each curriculum or transfer step. If the error grows exponentially instead of staying relatively constant or growing slowly, the step size is likely too large.
  2. **Loss plateauing or exploding:** If the loss for a new time domain step fails to decrease to the level of previous steps, the parameter transfer is likely not providing a good initialization.
  3. **Unstable long-term predictions:** In the test set, if the predictions for a later time domain are qualitatively wrong (e.g., unphysical oscillations, violation of bounds), it indicates a failure in one of the earlier transfer learning steps.

- **First 3 experiments:**
  1. **Baseline:** Implement and train a standard PINN on a known, simple PDE (like the 1D wave equation) over a short time domain. Verify it matches the analytical solution.
  2. **Curriculum Learning Implementation:** Extend experiment 1 by implementing the curriculum learning logic. Train the same PINN, but progressively expand the time domain. Compare its accuracy on a longer final time T_c against a standard PINN trained from scratch on T_c.
  3. **Transfer Learning Implementation:** Take the model from the end of experiment 2 and implement the transfer learning logic. Train a new network for a subsequent time window, initializing it with the parameters from experiment 2. Compare the stacked model's prediction against a standard PINN trained on the full combined time domain.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the inclusion of spatiotemporal rainfall data affect the accuracy and stability of the CTL-PINN hydrodynamic model for the Three Gorges Reservoir Area during extreme weather events?
  - **Basis in paper:** [explicit] The Conclusion states, "the scenario of heavy rainfall is not considered. Further studies can incorporate rainfall data," referencing the current assumption of zero lateral inflow.
  - **Why unresolved:** The current study explicitly excludes rainfall to simplify the calibration of roughness parameters, leaving the model's performance under complex meteorological boundary conditions untested.

- **Open Question 2:** Can an adaptive strategy be developed to dynamically determine the optimal time step sizes (ΔT_pc and ΔT_ct) based on local error estimates or loss convergence?
  - **Basis in paper:** [inferred] The methodology relies on fixed heuristic step sizes (e.g., recommending ΔT_pc ≤ 1/4 T_p), implying that finding the optimal balance between computational efficiency and accuracy currently requires manual tuning.
  - **Why unresolved:** The paper does not provide a mechanism for automatically adjusting the expansion rate of the time domain, which could lead to inefficiently small steps or accumulated errors if the fixed step is too large for complex dynamics.

- **Open Question 3:** How can the accumulation of error in long-term simulations be effectively mitigated when intermediate measured data is unavailable for supervision?
  - **Basis in paper:** [inferred] The Conclusion notes that error accumulates because "additional supervised learning is predicted by the neural network" rather than exact values.
  - **Why unresolved:** While the method improves upon standard PINNs, it still relies on the network's own predictions for transfer learning steps, leading to drift that is currently only checked by external data.

## Limitations
- The method's accuracy depends critically on the initial model quality and step size selection, with no explicit theoretical bounds on error accumulation across transfer steps.
- The Three Gorges Reservoir application relies on field data not provided in the paper, making full reproduction difficult.
- The claim that CTL-PINN "overcomes" TL-PINN computational inefficiency is not quantitatively supported - the total training time across all stacked models versus a single long-domain model is not reported.

## Confidence

- **Mechanism 1 (Decomposition + Transfer):** High confidence - the curriculum learning strategy is well-established in machine learning, and the progressive domain expansion with parameter transfer is the most standard and validated component.
- **Mechanism 2 (Pseudo-label Supervision):** Medium confidence - while the concept of using previous predictions as supervision is logical for error correction, the paper provides limited empirical evidence of its effectiveness versus standard training.
- **Mechanism 3 (Curriculum + Transfer Combination):** Medium confidence - the combination addresses known limitations of both individual approaches, but the hard limit on curriculum learning expansion and the optimal switching point to transfer learning are not rigorously derived.

## Next Checks

1. **Error Propagation Analysis:** Run the CTL-PINN method with increasing step sizes (ΔT_pc, ΔT_ct) and plot the L2 error at the end of each step. Verify that error growth remains bounded and identify the threshold where performance degrades.

2. **Pseudo-label Ablation Study:** Implement the curriculum learning step both with and without the additional supervised learning points (N_sp). Compare the final accuracy and training stability to quantify the actual benefit of the pseudo-label supervision mechanism.

3. **Computational Efficiency Benchmark:** For the Three Gorges case, measure and report the total wall-clock training time for CTL-PINN (sum across all curriculum and transfer steps) versus a standard PINN trained on the full domain. Calculate the accuracy per unit training time to validate the efficiency claim.