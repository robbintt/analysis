---
ver: rpa2
title: In Situ Training of Implicit Neural Compressors for Scientific Simulations
  via Sketch-Based Regularization
arxiv_id: '2511.02659'
source_url: https://arxiv.org/abs/2511.02659
tags:
- data
- compression
- neural
- learning
- sketch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an in situ training method for implicit neural
  representation (INR) compressors using sketch-based regularization to prevent catastrophic
  forgetting during sequential data arrival. The method employs limited memory buffers
  storing both full and sketched data samples, where sketching serves as a regularizer
  to maintain performance on previously seen data.
---

# In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization

## Quick Facts
- arXiv ID: 2511.02659
- Source URL: https://arxiv.org/abs/2511.02659
- Authors: Cooper Simpson; Stephen Becker; Alireza Doostan
- Reference count: 40
- Primary result: Achieves high-fidelity in situ neural compression (up to 1,882×) with performance matching offline methods via sketch-based regularization

## Executive Summary
This paper addresses the challenge of compressing time-sequential scientific simulation data in situ, where storage is limited and data arrives continuously. The authors propose using implicit neural representations (INRs) with sketch-based regularization to prevent catastrophic forgetting during sequential training. By leveraging randomized sketching techniques, the method maintains performance on previously seen data without storing the full history, achieving compression rates of 142× to 1,882× while preserving reconstruction quality (PSNR up to 62.9dB, relative errors below 1%).

## Method Summary
The method uses hypernetworks to generate weights for SIREN-based implicit neural representations, with sequential training on incoming PDE simulation snapshots. A key innovation is the use of sketch-based regularization via the Fast Johnson-Lindenstrauss Transform (FJLT) to maintain performance on historical data without storing full snapshots. The approach employs limited memory buffers storing both full and sketched data samples, with sketching serving as a regularizer to prevent catastrophic forgetting. Theoretical justification comes from JL-inspired results showing sketches can approximate the full loss, while practical evaluation demonstrates strong performance across three scientific simulation datasets.

## Key Results
- Achieves high compression rates (142× for Ignition, 1,882× for Neuron, 682× for Channel) while maintaining PSNR up to 62.9dB
- Sketch-based in situ training matches the performance of equivalent offline methods
- Relative reconstruction errors remain below 1% across all datasets
- Hypernetwork architecture significantly outperforms standard time-dependent INRs (37.2dB vs 27.1dB PSNR)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sketch-based loss acts as an effective surrogate for the full-data loss during regularization, preventing catastrophic forgetting.
- **Mechanism:** The method employs a Johnson-Lindenstrauss (JL) transform to compress past snapshots. The paper proves that for a network output lying on a manifold, the loss computed on the sketched data upper-bounds the loss on the full data. By minimizing this sketched loss, the optimizer constrains the network to maintain performance on previous time steps without storing the full history.
- **Core assumption:** The output of the INR network maps to a Riemannian manifold where the intrinsic dimension $M$ is small.
- **Evidence anchors:** Section 2.2, Theorem 3 (JL Surrogate) proves $L(U_t, \Phi(\theta)) \le \frac{1}{1-\epsilon} L(S U_t, S \Phi(\theta))$.

### Mechanism 2
- **Claim:** The required memory footprint for regularization scales with the intrinsic dimensionality of the simulation data, not the mesh size.
- **Mechanism:** Standard compression requires storage proportional to mesh nodes $n$. This method relies on the Manifold JL Theorem, where the sketch size $k$ depends on the intrinsic dimension $M$ and log-terms of the ambient dimension. Since scientific simulations often evolve on low-dimensional attractors, $k$ can be tiny ($1\%-5\%$ of $n$) while preserving geometric relationships.
- **Core assumption:** The scientific simulation data and the neural network outputs effectively populate a low-dimensional manifold within the high-dimensional ambient space.
- **Evidence anchors:** Section 3.2 provides estimated manifold dimensions (e.g., 11 for Ignition, 50 for Channel) via lPCA, validating that $M \ll n$.

### Mechanism 3
- **Claim:** Hypernetworks provide superior in-situ plasticity compared to time-encoded single INRs.
- **Mechanism:** A Hypernetwork generates weights for a target INR based on the time coordinate $t$. This decouples the temporal evolution (learned by the Hypernetwork) from the spatial representation (handled by the target INR). Empirical results show this architecture resists forgetting better than a single time-dependent INR when regularized with sketches.
- **Core assumption:** The temporal variation of the optimal network weights can be modeled by a smaller, distinct network structure.
- **Evidence anchors:** Section 3.2, Table 4 shows "Hypernet + INR" significantly outperforms "INR" alone in PSNR (e.g., 37.2dB vs 27.1dB) for in-situ tasks.

## Foundational Learning

- **Concept:** **Implicit Neural Representations (INR) & SIRENs**
  - **Why needed here:** This is the compression engine. Unlike grids, INRs represent data as continuous functions $f(x,y,z,t)$. You must understand that the network *is* the compressed data.
  - **Quick check question:** Can you explain why a SIREN (sinusoidal activation) is preferred over ReLU for representing high-fidelity PDE solution fields?

- **Concept:** **Catastrophic Forgetting (Continual Learning)**
  - **Why needed here:** This is the problem being solved. When training sequentially, minimizing loss on snapshot $t+1$ historically increases error on snapshot $t$. You need to understand the stability-plasticity dilemma.
  - **Quick check question:** If you trained a standard MLP on time steps $1 \to 100$ sequentially without replay, what would happen to the error at $t=1$ by the time you reach $t=100$?

- **Concept:** **Johnson-Lindenstrauss (JL) Lemma**
  - **Why needed here:** This justifies the "sketch." You need to understand that random projections preserve distances (and thus loss landscapes) with high probability, even if the projection matrix looks like noise.
  - **Quick check question:** If you have 1000 points in 100,000 dimensions, roughly how many dimensions do you need in a random projection to preserve pairwise distances (roughly)?

## Architecture Onboarding

- **Component map:** Simulation (Producer) -> Buffers (Full + Sketched) -> Sketching Module (FJLT/Subsampling) -> Hypernetwork -> Target INR
- **Critical path:**
  1. Sim generates $U_t$
  2. Push $U_t$ to Full Buffer; compute $S_t U_t$ and push to Sketched Buffer
  3. Optimizer samples batch from *both* buffers
  4. Forward pass: Hypernetwork generates weights $\theta(t)$; INR generates $\hat{U}$
  5. Loss: $L_{in situ} = L_{full} + \lambda L_{sketch}$
- **Design tradeoffs:**
  - FJLT vs. Subsampling: FJLT is robust but requires DCT (slightly more compute); Subsampling is free but fails on non-uniform meshes (e.g., Neuron dataset)
  - Buffer Size ($T_f$): $T_f=1$ is hardest but minimizes memory; increasing $T_f$ improves learning stability
- **Failure signatures:**
  - Spiking Error: If PSNR oscillates wildly, the learning rate may be too high relative to the sketch batch size ($b_s$)
  - Geometry Artifacts: If using Subsampling on unstructured meshes, expect high relative error (RFE) due to poor spatial coverage
  - Forgetting: If the "Test Loss" diverges from "Sketch Loss" over time, the sketch size $k$ is likely too small (loss of information)
- **First 3 experiments:**
  1. Overfit Single Snapshot: Train on $T=1$ with no sketching. Verify the INR capacity is sufficient to represent one field (PSNR > 50dB)
  2. Ablate the Sketch: Run in-situ on 50 snapshots with the sketch buffer disabled ($T_s=0$). Observe catastrophic forgetting (error on $t=1$ will spike)
  3. Sketch Size Sweep: Fix dataset (e.g., Ignition). Vary sample factor ($0.1\%$ to $5\%$) using FJLT. Plot final average PSNR vs. Sketch Size to find the "knee" in the curve

## Open Questions the Paper Calls Out

- **Open Question 1:** Why does the hypernetwork architecture significantly outperform standard time-dependent INRs when paired with sketch-based regularization?
  - **Basis in paper:** The authors note their use of hypernetworks was "empirically motivated" and "deserves a more thorough examination" despite showing superior results in Table 4.
  - **Why unresolved:** The paper demonstrates the performance gap empirically but does not provide a theoretical justification for why hypernetworks handle sketch-based regularization better than standard INRs.
  - **What evidence would resolve it:** A theoretical analysis or detailed ablation study isolating the interaction between weight generation dynamics and sketch surrogates.

- **Open Question 2:** What is the theoretically minimum sketch size required to serve as an effective regularizer against catastrophic forgetting?
  - **Basis in paper:** The authors state that rigorously answering how large a sketch needs to be is an open question, as current Johnson-Lindenstrauss bounds are loose compared to empirical success.
  - **Why unresolved:** The theoretical bounds suggest larger sketches than empirically needed; the link between manifold dimension and the minimal sketch size for regularization remains unclear.
  - **What evidence would resolve it:** Derivation of tight, data-dependent lower bounds for sketch dimensions that guarantee specific error tolerances in the in situ setting.

- **Open Question 3:** Can deterministic or data-informed sketching operators outperform the randomized Fast Johnson-Lindenstrauss Transform (FJLT) for this regularization task?
  - **Basis in paper:** The text states that "Research on the best sketching method is warranted, including deterministic sketches or sketches that are data-informed."
  - **Why unresolved:** The study focused primarily on FJLT and random subsampling, leaving other potential operators unexplored.
  - **What evidence would resolve it:** Comparative benchmarks showing reconstruction accuracy and convergence speed using data-adaptive sketches versus FJLT on the target datasets.

## Limitations

- The theoretical JL-based loss surrogate relies heavily on the assumption that INR outputs lie on a low-dimensional manifold, which may not hold universally across all scientific simulations
- The Hypernetwork architecture introduces additional complexity and training instability risks not fully characterized
- Memory buffer sizes and sketch sample factors are tuned per dataset but lack a unified theoretical guideline for generalization

## Confidence

- **High Confidence:** The empirical effectiveness of sketch-based regularization in preventing catastrophic forgetting (Section 4.1), as demonstrated across all three datasets with clear quantitative improvements
- **Medium Confidence:** The theoretical JL surrogate bound (Theorem 3) and its practical utility, as the proof is sound but depends on unverified manifold dimension assumptions for new datasets
- **Medium Confidence:** The superiority of the Hypernetwork architecture over time-encoded single INRs, as results are strong but ablation on a wider range of architectures is limited

## Next Checks

1. **Robustness Test:** Apply the method to a high-dimensional turbulent flow dataset (e.g., fully developed turbulence) to stress-test the manifold assumption and sketch efficacy when $M \approx n$
2. **Architectural Ablation:** Systematically compare the Hypernetwork against alternative continual learning strategies (e.g., weight regularization like EWC, dynamic expansion) on a held-out PDE dataset to isolate the source of its performance gain
3. **Memory Efficiency Analysis:** Quantify the *actual* memory overhead (model + buffers) for in-situ training across datasets and compare it against the theoretical compression ratio to provide a holistic efficiency metric