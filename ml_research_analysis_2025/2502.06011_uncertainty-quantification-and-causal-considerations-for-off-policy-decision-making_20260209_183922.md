---
ver: rpa2
title: Uncertainty Quantification and Causal Considerations for Off-Policy Decision
  Making
arxiv_id: '2502.06011'
source_url: https://arxiv.org/abs/2502.06011
tags:
- data
- policy
- which
- off-policy
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis tackles the challenge of robust decision-making in
  machine learning, focusing on off-policy evaluation (OPE) and causal considerations
  for sequential decisions. The core issues addressed are: (1) high variance in importance-sampling-based
  OPE estimators, (2) lack of uncertainty quantification in OPE methods, and (3) causal
  unidentifiability under unmeasured confounding in sequential decision settings.'
---

# Uncertainty Quantification and Causal Considerations for Off-Policy Decision Making

## Quick Facts
- **arXiv ID:** 2502.06011
- **Source URL:** https://arxiv.org/abs/2502.06011
- **Reference count:** 0
- **Primary result:** Introduces Marginal Ratio estimator for OPE with reduced variance, Conformal Off-Policy Prediction for uncertainty quantification, and causal bounds for digital twin falsification under unmeasured confounding.

## Executive Summary
This thesis addresses fundamental challenges in off-policy decision making by developing three novel methodologies. First, it introduces the Marginal Ratio (MR) estimator that reduces variance in off-policy evaluation by focusing on marginal outcome distributions rather than direct policy shifts. Second, it presents Conformal Off-Policy Prediction (COPP) that provides principled uncertainty quantification with finite-sample predictive intervals. Third, it develops a framework for causal falsification of digital twins using longitudinal causal bounds that remain valid under arbitrary unmeasured confounding. The work demonstrates significant variance reduction with MR, reliable coverage with COPP, and robust model assessment capability for digital twins.

## Method Summary
The thesis tackles off-policy evaluation (OPE) through three complementary approaches. The Marginal Ratio estimator reduces variance by estimating the ratio of marginal outcome densities rather than full policy ratios, using regression to learn these weights. Conformal Off-Policy Prediction adapts conformal prediction theory to provide finite-sample coverage guarantees for outcomes under target policies by accounting for distribution shift. The causal falsification framework moves from point estimation to partial identification, deriving longitudinal causal bounds that can falsify simulator models even when unmeasured confounding prevents standard causal inference. These methods are validated on synthetic datasets, Microsoft Learning to Rank data, and medical simulation environments using MIMIC-III and Pulse Physiology Engine.

## Key Results
- MR estimator achieves significant variance reduction compared to IPW/DR baselines while maintaining unbiasedness in contextual bandit settings
- COPP provides predictive intervals with theoretical finite-sample coverage guarantees (approximately 1-α) even under distribution shift
- Causal bounds enable robust assessment of digital twin models without requiring strong causal assumptions, offering actionable insights into model reliability

## Why This Works (Mechanism)

### Mechanism 1: Marginal Ratio (MR) Variance Reduction
- **Claim:** The MR estimator reduces variance in Off-Policy Evaluation (OPE) compared to standard Importance Sampling (IPW) by focusing on the marginal distribution of outcomes rather than the joint distribution of contexts, actions, and outcomes.
- **Mechanism:** Instead of calculating importance weights based on the full policy ratio ρ(a,x) = π*(a|x)/π_b(a|x), MR estimates the ratio of marginal outcome densities w(y) = p_π*(y)/p_π_b(y). This marginalizes out the stochasticity of the context and action, which is often the source of high variance. w(y) is estimated by regressing ρ(A,X) on Y.
- **Core assumption:** The standard contextual bandit setup holds (no unmeasured confounding in the observed data).
- **Evidence anchors:**
  - [abstract]: "...reduces variance by focusing on the marginal distribution of outcomes rather than direct policy shifts..."
  - [Chapter 2, Section 2.3.1]: Prop 2.3.2 proves Var[θ̂_MR] ≤ Var[θ̂_IPW]
  - [corpus]: Neighbors like "Off-Policy Learning in Large Action Spaces" highlight that variance is a critical bottleneck in high-dimensional OPE.
- **Break condition:** If the conditional distribution P(Y|A,X) is highly complex or the overlap between policies is extremely poor, the regression step to estimate w(y) may fail, introducing bias that outweighs the variance reduction benefits.

### Mechanism 2: Conformal Off-Policy Prediction (COPP)
- **Claim:** COPP provides predictive intervals for outcomes under a target policy with finite-sample coverage guarantees, even with distribution shift.
- **Mechanism:** It adapts conformal prediction to the policy shift setting. It constructs prediction sets Ĉ(x) by weighting the empirical distribution of non-conformity scores. The weights w(x,y) account for the shift from the behavioral distribution P_π_b to the target distribution P_π*, ensuring the statistical exchangeability required for coverage holds under the target policy.
- **Core assumption:** The weights w(x,y) can be estimated sufficiently well; the data is i.i.d.
- **Evidence anchors:**
  - [abstract]: "...principled approach for uncertainty quantification in OPE that provides finite-sample predictive intervals..."
  - [Chapter 3, Section 3.4.1]: Prop 3.4.1 proves coverage P(Y ∈ Ĉ(X)) ≥ 1-α holds if weights are exact.
  - [corpus]: "PERRY: Policy Evaluation with Confidence Intervals..." reinforces the value of interval estimation over point estimates in OPE.
- **Break condition:** If the estimated weights ŵ(x,y) deviate significantly from the true weights (e.g., poor propensity score estimation), the coverage guarantee is violated by a term proportional to the estimation error Δ_w.

### Mechanism 3: Causal Falsification of Digital Twins
- **Claim:** One can reliably identify when a simulator (Digital Twin) is *incorrect* using observational data, even if that data contains arbitrary unmeasured confounding.
- **Mechanism:** It bypasses the "fundamental problem of causal inference" (unidentifiability) by moving from point estimation to partial identification. It derives longitudinal causal bounds on potential outcomes. If the simulator's output falls outside these bounds, the simulator is falsified. It uses a "worst-case" filling strategy for unobserved counterfactuals to ensure soundness.
- **Core assumption:** The observational trajectories are i.i.d. and outcomes are bounded.
- **Evidence anchors:**
  - [Chapter 4, Section 4.4.1]: Theorem 4.4.1 provides the longitudinal bounds E[Y_lo|...] ≤ E[Y(a_1:t)|...] ≤ E[Y_up|...]
  - [Chapter 4, Section 4.5]: Describes the hypothesis testing procedure to check if the twin's mean output Q̂ violates the bound.
  - [corpus]: "Bayesian implementation of Targeted Maximum Likelihood..." is relevant as it discusses robust causal estimation, though this mechanism relies on bounds rather than point estimates to handle confounding.
- **Break condition:** If the bounds [Q_lo, Q_up] become too wide (uninformative) due to extreme confounding or lack of data overlap, the test loses power (fails to reject false twins), though it never becomes unsound (incorrectly accepts a false twin).

## Foundational Learning

- **Concept: Off-Policy Evaluation (OPE) & Importance Sampling (IPW)**
  - **Why needed here:** The entire thesis is built on improving standard OPE. You must understand that IPW re-weights logged data to estimate the value of a new policy, and that high variance is the primary failure mode being solved.
  - **Quick check question:** Why does the variance of IPW explode when the behavioral policy π_b assigns low probability to actions preferred by π*?

- **Concept: Conformal Prediction**
  - **Why needed here:** Required to understand COPP (Chapter 3). It is a distribution-free method to create prediction intervals. You need to grasp how it uses "exchangeability" rather than distributional assumptions.
  - **Quick check question:** How does weighted exchangeability differ from standard exchangeability, and why is it necessary for COPP?

- **Concept: Potential Outcomes & Unmeasured Confounding**
  - **Why needed here:** Required for the Digital Twin falsification (Chapter 4). You must understand why unmeasured confounders prevent identifying causal effects (Theorem 4.3.1) and how partial identification (bounds) offers a workaround.
  - **Quick check question:** In the toy example (Section 1.3.2), why does the observational data suggest the drug is effective while the interventional reality is not?

## Architecture Onboarding

- **Component map:** Data Preprocessor -> Ratio/Weight Estimator -> Conformal Core (for COPP) / Bound Calculator (for falsification) -> Inference Output

- **Critical path:** Accurate estimation of the behavioral policy π_b and the conditional outcome model P(y|x,a). If the weights ŵ are biased, all downstream guarantees (coverage, falsification) degrade or fail.

- **Design tradeoffs:**
  - **Variance vs. Bias (MR):** MR reduces variance significantly but introduces a potential bias source during the weight regression step, unlike IPW which is unbiased but high-variance.
  - **Power vs. Soundness (Causal Bounds):** The bounds are guaranteed to be sound (contain the true value) but may lack power (be too wide) if confounding is severe or data is sparse.
  - **Conservatism vs. Validity (COPP):** COPP prioritizes valid coverage (guaranteeing 1-α) over tight intervals, potentially leading to conservative intervals if weight estimation is noisy.

- **Failure signatures:**
  - **MR Failure:** MSE stagnates or increases as data size increases (bias dominates).
  - **COPP Failure:** Empirical coverage drops below the target 1-α (likely due to poor weight estimation/overlap).
  - **Falsification Failure:** Zero hypotheses rejected despite known flaws in the simulator (bounds are too loose to detect errors).

- **First 3 experiments:**
  1. **Synthetic Variance Check:** Reproduce Figure 2.2a. Vary evaluation data size n to verify MR MSE is lower than IPW/DR/MIPS baselines for small n.
  2. **Coverage Calibration:** Run COPP on the Toy Experiment (Section 3.6.1) to ensure the mean coverage hovers around the target 1-α (e.g., 90%) across different policy shift magnitudes.
  3. **Bound Tightness Test:** Implement the longitudinal causal bounds (Eq 4.6) on a synthetic dataset with controlled confounding to verify that the true outcome lies within the computed bounds [Q_lo, Q_up].

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Marginal Ratio (MR) estimator be successfully applied to policy optimization schemes (e.g., PPO) to improve their stability and convergence?
- **Basis in paper:** [explicit] The conclusion states, "We believe that the MR estimator applied to these methodologies could lead to improvements in the stability and convergence of these optimisation schemes, given its favourable variance properties."
- **Why unresolved:** The thesis applied the MR estimator strictly to off-policy evaluation and value estimation, not to the optimization of policy parameters during training.
- **Evidence:** Empirical experiments comparing the convergence rates and variance of policy updates in algorithms like PPO or RLHF when using the MR estimator versus standard policy ratios.

### Open Question 2
- **Question:** How can Conformal Off-Policy Prediction (COPP) be adapted for robust policy learning by optimizing worst-case outcomes?
- **Basis in paper:** [explicit] The authors state, "For future work, it would be interesting to apply COPP to policy training. This could be a step towards robust policy learning by optimising the worst case outcome."
- **Why unresolved:** The presented work focuses on providing predictive intervals for *assessment* (inference) rather than formulating a training objective or loss function based on these intervals.
- **Evidence:** A derived training algorithm that incorporates the predictive intervals (e.g., the lower bound of the conformal interval) into the reward function or loss calculation to train risk-averse policies.

### Open Question 3
- **Question:** How can COPP be extended to sequential decision-making settings (MDPs) without suffering from overly conservative intervals as the time horizon increases?
- **Basis in paper:** [explicit] The conclusion notes that while follow-up works have applied COPP to MDPs, "the obtained confidence sets become increasingly conservative with increasing time horizon. It is worth exploring methodologies for obtaining intervals which remain valid and informative."
- **Why unresolved:** The "curse of horizon" causes uncertainty to compound over time steps in sequential settings, leading to trivial or excessively wide predictive sets that lack utility.
- **Evidence:** Theoretical analysis and empirical results demonstrating a modified COPP methodology for sequential settings where interval width grows sub-linearly or remains bounded relative to the horizon length.

## Limitations

- MR estimator introduces potential bias during the weight regression step while reducing variance
- COPP's coverage guarantees break down when estimated weights deviate significantly from true weights
- Causal falsification bounds can become very wide and uninformative under severe confounding or poor data overlap

## Confidence

**High Confidence:** The variance reduction mechanism of MR is theoretically sound and empirically demonstrated in controlled synthetic settings. The finite-sample coverage guarantees of COPP follow directly from established conformal prediction theory adapted to the policy shift setting.

**Medium Confidence:** The empirical performance of MR on real-world datasets (Microsoft Learning to Rank) shows competitive results, though the comparison to baselines could be more extensive. The practical utility of COPP in detecting out-of-distribution errors in medical treatment simulators is demonstrated but relies on specific simulation assumptions.

**Low Confidence:** The scalability of the MR approach to extremely high-dimensional action spaces or continuous actions is not thoroughly explored. The power of the causal falsification test to detect subtle simulator errors in complex real-world scenarios remains uncertain, particularly when bounds become wide.

## Next Checks

1. **Bias-Variance Tradeoff Analysis:** Systematically vary the complexity of the w(y) regression model and measure the resulting bias-variance tradeoff in MR estimator performance across multiple synthetic datasets with known ground truth.

2. **Coverage Under Distribution Shift:** Test COPP's coverage guarantees across a broader range of distribution shift scenarios, including extreme policy differences where overlap between π_b and π* becomes minimal.

3. **Falsification Power Characterization:** Quantify the relationship between confounding strength, data overlap, and bound width to determine the minimum detectable simulator error size across different experimental conditions.