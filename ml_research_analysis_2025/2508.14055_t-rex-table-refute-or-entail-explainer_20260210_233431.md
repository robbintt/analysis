---
ver: rpa2
title: 'T-REX: Table -- Refute or Entail eXplainer'
arxiv_id: '2508.14055'
source_url: https://arxiv.org/abs/2508.14055
tags:
- table
- t-rex
- verification
- reasoning
- fact-checking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T-REX introduces the first live, interactive tool for real-time
  table fact-checking using state-of-the-art open-source instruction-tuned reasoning
  LLMs. The system addresses the gap between advanced LLM-based table verification
  research and practical accessibility for non-expert users by providing a multilingual,
  multimodal interface that supports arbitrary user-provided tables.
---

# T-REX: Table -- Refute or Entail eXplainer

## Quick Facts
- arXiv ID: 2508.14055
- Source URL: https://arxiv.org/abs/2508.14055
- Reference count: 6
- Phi-4 with CoT achieves 89% accuracy on TabFact

## Executive Summary
T-REX introduces the first live, interactive tool for real-time table fact-checking using state-of-the-art open-source instruction-tuned reasoning LLMs. The system addresses the gap between advanced LLM-based table verification research and practical accessibility for non-expert users by providing a multilingual, multimodal interface that supports arbitrary user-provided tables. The core approach combines robust preprocessing (including OCR via IBM Granite 3.2 or Tesseract), naturalized table formatting with row indexing, and direct prompting with chain-of-thought reasoning. Extensive offline experiments identified Phi-4 with CoT prompting as achieving 89% accuracy on the TabFact test set, surpassing prior methods like RePanda.

## Method Summary
T-REX uses instruction-tuned reasoning LLMs via direct prompting with chain-of-thought (CoT) to classify textual claims as entailed or refuted based on tabular data. Tables are preprocessed through delimiter unification, content cleaning, and synthetic row_index column injection before serialization. The system supports four SOTA models (Phi-4, Cogito, DeepSeek-R1-Distill, Gemma 3) through Ollama, with a FastAPI backend orchestrating inference and a multilingual frontend providing real-time reasoning streams and cell highlighting. OCR extraction uses IBM Granite 3.2 VLM with Tesseract as fallback, all processing occurs in-memory with no user data storage.

## Key Results
- Phi-4 with naturalized table formatting and CoT prompting achieved 89% accuracy on TabFact test set
- Outperformed prior methods like RePanda while maintaining real-time interactivity
- Supports four state-of-the-art models under GPU memory constraints (4B-14B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Naturalized table formatting with synthetic row indexing improves LLM cell referencing accuracy.
- Mechanism: Tables converted to naturalized text with synthetic `row_index` column injected during preprocessing, reducing ambiguity in cell identification when LLM generates references in reasoning output.
- Core assumption: LLMs trained on natural language text can more reliably parse and reference tabular data when it resembles prose with explicit positional markers.
- Evidence anchors:
  - [section] Page 2: Tables preprocessed by injecting synthetic `row_index` column to aid cell referencing
  - [section] Page 3: Phi-4 with naturalized formatting and CoT achieved 89% accuracy
  - [corpus] Related work on scientific table verification suggests row-level decomposition aids reasoning

### Mechanism 2
- Claim: Chain-of-thought prompting combined with structured JSON output enables interpretable verification with extractable evidence cells.
- Mechanism: Prompt instructs model to output reasoning steps, then produce JSON object containing verdict and self-identified relevant cells, with post-processing extracting JSON with fallback mechanisms.
- Core assumption: Instruction-tuned reasoning models can reliably separate reasoning traces from structured outputs without format degradation.
- Evidence anchors:
  - [section] Page 3: Direct prompting with CoT enhances interpretability by guiding model to output reasoning followed by JSON verdict and relevant cells
  - [abstract] Mentions transparency and real-time reasoning streams as core design goals

### Mechanism 3
- Claim: Dual-path OCR with VLM-primary and Tesseract-fallback provides robust table extraction across image quality levels.
- Mechanism: Table images first route through IBM Granite 3.2 VLM for high-accuracy extraction; Tesseract serves as faster alternative or fallback before preprocessing and serialization.
- Core assumption: VLMs trained on document understanding tasks can extract tabular structure more accurately than traditional OCR for complex layouts.
- Evidence anchors:
  - [section] Page 2: T-REX supports table input via CSV or OCR-extracted images using Granite 3.2 VLM for high accuracy or Tesseract as faster alternative
  - [section] Page 4: Interface allows recognition errors to be quickly identified and manually corrected via editable CSV preview

## Foundational Learning

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: T-REX's best-performing configuration (Phi-4 + CoT) depends on explicit reasoning traces before verdict generation. Without understanding CoT, you cannot diagnose why a model produces correct vs. hallucinated cell references.
  - Quick check question: Given a table with population data and the claim "City A has more residents than City B," can you construct a CoT prompt that forces the model to extract both values before comparing?

- Concept: **Table Serialization Formats**
  - Why needed here: The paper tested Markdown, HTML, JSON, and naturalized text formats. Understanding how each format represents structure informs debugging when cell highlighting fails.
  - Quick check question: Convert a 3-row, 2-column table from Markdown to the naturalized format described in the paper. What information is gained or lost?

- Concept: **Instruction-Tuned vs. Base LLMs**
  - Why needed here: T-REX uses "instruction-tuned reasoning LLMs" (Phi-4, DeepSeek-R1-Distill, Cogito, Gemma 3). These differ from base models in following structured output directives—critical for reliable JSON extraction.
  - Quick check question: What specific instruction-following capability does T-REX require that a base (non-instruction-tuned) model would likely fail at?

## Architecture Onboarding

- Component map: Frontend (web interface) -> Backend (FastAPI) -> Inference (Ollama) -> OCR (Granite 3.2 VLM/Tesseract) -> Preprocessing (normalize + index) -> Prompt construction -> LLM inference -> JSON extraction -> Cell highlighting

- Critical path: User input (table + claim) → OCR (if image) → Preprocessing (normalize + index) → Prompt construction (naturalized table + claim + CoT instructions) → Ollama inference → Stream reasoning → JSON extraction with fallbacks → Parse verdict + evidence cells → Highlight cells in frontend

- Design tradeoffs:
  - Accuracy vs. latency: Phi-4 (14B) achieves 89% but requires more GPU memory; Gemma 3 (4B) is faster but lower accuracy
  - VLM vs. Tesseract OCR: Granite 3.2 more accurate but slower; Tesseract faster but noisier
  - Direct prompting vs. code generation: Direct CoT chosen for interpretability; code generation tested but not deployed—likely due to execution sandbox complexity
  - No persistent storage: All in-memory processing for privacy; limits batch processing or session recovery

- Failure signatures:
  - **Malformed JSON output**: Model produces valid reasoning but corrupts JSON structure → fallback extraction attempts to recover verdict
  - **Cell reference hallucination**: Model highlights cells that don't exist or don't contain referenced values → suggests row_index confusion or context truncation
  - **OCR structure collapse**: Merged cells or multi-line entries extracted as single row → visible in editable CSV preview for manual correction
  - **Rate limit triggers**: SlowAPI blocks requests under load → users see timeout errors

- First 3 experiments:
  1. **Format sensitivity test**: Submit identical table+claim pairs using all four serialization formats (Markdown, HTML, JSON, naturalized) to Phi-4 with CoT. Measure accuracy and cell reference precision. Hypothesis: naturalized format should match or exceed others based on paper claims.
  2. **OCR pathway comparison**: Upload 5 table images with varying complexity (simple borders, merged cells, colored backgrounds) through both Granite 3.2 and Tesseract. Compare extraction errors requiring manual correction in CSV preview.
  3. **Model scaling curve**: Run TabFact subset (100 examples) across all four supported models. Plot accuracy vs. inference latency vs. GPU memory to validate claimed balance between reasoning capability and hardware constraints.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact CoT prompt template and JSON schema are not provided, making it difficult to reproduce the claimed performance
- Claims about superiority of naturalized formatting over structured formats lack direct comparative evidence
- OCR evaluation lacks direct comparison metrics between Granite 3.2 VLM and Tesseract across different table complexities

## Confidence

- **High confidence**: The core architecture (FastAPI backend + Ollama inference + frontend interface) is well-documented and technically sound. The use of instruction-tuned LLMs with CoT prompting for table fact-checking is established practice.
- **Medium confidence**: The claimed 89% accuracy on TabFact is plausible given use of Phi-4 with CoT, but cannot be independently verified without exact prompt and preprocessing details.
- **Low confidence**: Claims about superiority of naturalized formatting over structured formats lack direct comparative evidence.

## Next Checks

1. **Prompt template validation**: Test Phi-4 with exact CoT prompt structure described (reasoning steps followed by JSON output) versus alternative prompt formulations to confirm claimed accuracy gain is due to specific formatting, not just model choice.

2. **Format ablation study**: Run identical table+claim pairs through all four serialization formats (Markdown, HTML, JSON, naturalized) using same model and prompt structure to empirically measure accuracy difference claimed for naturalized formatting.

3. **OCR pathway benchmarking**: Systematically evaluate Granite 3.2 VLM versus Tesseract across tables with increasing complexity (merged cells, nested headers, stylized layouts) to quantify claimed accuracy vs. speed tradeoff and identify failure modes.