---
ver: rpa2
title: 'RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning'
arxiv_id: '2601.00086'
source_url: https://arxiv.org/abs/2601.00086
tags:
- rule
- tool
- rules
- query
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RIMRULE, a neuro-symbolic framework for improving
  tool-using language agents through inference-time rule injection. Rules are distilled
  from failure traces using an MDL objective, translated into symbolic form, and dynamically
  retrieved at inference time.
---

# RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning

## Quick Facts
- arXiv ID: 2601.00086
- Source URL: https://arxiv.org/abs/2601.00086
- Reference count: 40
- Key outcome: Introduces a neuro-symbolic framework that distills tool-use rules from failure traces, consolidates them with MDL, and injects them at inference time to boost accuracy, especially on unseen tools.

## Executive Summary
RIMRULE is a neuro-symbolic framework designed to improve tool-using language agents by injecting learned rules at inference time. It distills symbolic rules from failure traces using an MDL-guided consolidation process, enabling dynamic adaptation without model finetuning. Experiments on ToolHop and BFCL show consistent accuracy gains, with particular strength on test-unseen tool scenarios. Rules learned from one LLM effectively transfer to others, including long reasoning models, demonstrating portability. The approach provides a modular, interpretable alternative to prompting or finetuning.

## Method Summary
RIMRULE operates in two phases: rule generation and consolidation. First, it collects tool-use traces from a base agent, identifying failures and ground-truth traces. An EBL-based LLM generates candidate rules from each failure, which are validated and translated into a canonical symbolic form. These candidates are then consolidated via greedy MDL optimization to produce a compact rule library. At inference, the agent retrieves and injects relevant rules into its prompt, dynamically adapting its behavior. This neuro-symbolic pipeline combines the interpretability of symbolic rules with the flexibility of neural agents.

## Key Results
- RIMRULE improves test accuracy on ToolHop from 36.6% (zero-shot) to 47.3% with rule injection, and on BFCL from 33.1% to 47.9%.
- On the test-unseen split (new tools), gains are especially strong: 36.7%→47.8% on ToolHop, 28.1%→40.8% on BFCL.
- Rules learned from one LLM (e.g., Llama3.2) transfer effectively to others (e.g., GPT-4o), with gains up to +9.8% accuracy on complex reasoning tasks.

## Why This Works (Mechanism)
RIMRULE improves tool-using agents by distilling reusable, generalizable rules from observed failures and injecting them at inference time. The core innovation is the MDL-guided consolidation, which prunes redundant or overly specific rules, ensuring the final library is compact and broadly applicable. Symbolic translation and retrieval allow precise matching of rules to the agent's current context, improving robustness compared to natural language-only methods. This modular, inference-time adaptation sidesteps the cost and brittleness of finetuning while enabling cross-model rule transfer.

## Foundational Learning
- **Concept: Explanation-Based Learning (EBL)**
  - Why needed here: Converts specific failure instances into generalized, reusable rules, differentiating from simple error memorization.
  - Quick check question: Can you explain how EBL moves from a specific failed trace to a generalized rule, and what key assumption it makes about the availability of a correct trace or theory?

- **Concept: Minimum Description Length (MDL) Principle**
  - Why needed here: Provides a formal objective for pruning and selecting rules, addressing overfitting to training failures.
  - Quick check question: In the MDL formula MDL(H) = L(H) + L(D|H), what do the two terms represent and what kind of rule library are they biased towards?

- **Concept: Neuro-Symbolic Systems**
  - Why needed here: RIMRULE explicitly leverages symbolic AI (rules, structure, interpretability) alongside neural networks (LLMs, flexible reasoning).
  - Quick check question: What are the primary advantages that RIMRULE gains by using a symbolic representation for rules, as opposed to operating solely on natural language?

## Architecture Onboarding

- **Component map:**
  Experience Collector -> Rule Generator (EBL) -> Symbolic Translator -> MDL Consolidator -> Rule Retriever -> Inference Agent

- **Critical path:**
  1. Execute base agent to collect (query, tools, failed trace, ground-truth trace) tuples.
  2. For each failure, generate, validate, and translate a candidate rule into symbolic form via EBL, creating initial pool R0.
  3. Run greedy MDL optimization to prune R0 into a compact, generalizable library R.
  4. For each new query, symbolically encode state, retrieve top-k rules from R, and inject into LLM prompt before execution.

- **Design tradeoffs:**
  - EBL-based vs. Pure Prompting: EBL is more principled but requires ground-truth traces; pure prompting is simpler but may produce shallower rules.
  - MDL vs. LLM-based Merging: MDL is data-driven and principled but relies on greedy search; LLM-based merging is flexible but heuristic.
  - Symbolic Retrieval vs. NL Retrieval: Symbolic retrieval is more robust and interpretable but requires upfront translation; NL retrieval is simpler but noisier.
  - Two-phase (generation, then consolidation) vs. sequential learning: Trades off simplicity for scalability and order-independence.

- **Failure signatures:**
  - Rules overly specific to training instances, failing to generalize on test-unseen.
  - MDL consolidation results in empty or uselessly small rule library.
  - Effective rules exist but are not retrieved for relevant queries.
  - Injected rules have no measurable impact on agent performance.

- **First 3 experiments:**
  1. Reproduce learning curve: From zero-shot baseline, generate and consolidate rules; verify progression: (Initial Accuracy) < (+ Rule Gen Accuracy) ≈ (+ Consolidation Accuracy) with significant rule count reduction.
  2. Ablate retrieval method: Compare test-rand accuracy of full system (MDL + Symbolic Retrieval) against NL-based retrieval; expect drop with NL retriever.
  3. Evaluate cross-model transfer: Learn rules from one LLM (e.g., Llama3.2), apply to another (e.g., GPT-4o); measure accuracy gains, especially on complex reasoning tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RIMRULE be effectively adapted to settings where only reward signals (not execution traces) are available for rule learning?
- Basis in paper: [explicit] Footnote 3 states that adapting the mechanism to reward-based settings "is left to future work."
- Why unresolved: Current framework assumes ground-truth execution traces for identifying failure causes via EBL; many real-world deployments lack such supervision.
- What evidence would resolve it: Experiments comparing rule quality and downstream performance when rules are learned from scalar rewards versus execution traces on the same benchmarks.

### Open Question 2
- Question: Would global or approximate inference strategies for MDL consolidation yield significantly better rule libraries than the current greedy procedure?
- Basis in paper: [explicit] Page 4 notes that greedy consolidation "does not guarantee a globally optimal solution" and that "exploring more global or approximate inference strategies... is an interesting direction for future work."
- Why unresolved: Greedy local edits may miss beneficial non-local rule combinations or generalizations, potentially leaving redundancy.
- What evidence would resolve it: A comparison of greedy vs. beam search or stochastic optimization (e.g., simulated annealing) on rule library size, generalization to test-unseen, and computational cost.

### Open Question 3
- Question: How does the effectiveness of learned rules degrade when applied to domains or error distributions that differ substantially from the training failures?
- Basis in paper: [inferred] The limitations section notes that "gains may be smaller in domains with sparse or highly idiosyncratic errors." The paper tests only in-distribution and tool-unseen splits within the same benchmarks.
- Why unresolved: Cross-model transferability is shown, but cross-domain robustness (e.g., from ToolHop to a medical tool-use setting) is untested.
- What evidence would resolve it: Cross-domain transfer experiments measuring rule applicability and performance when rules learned on one benchmark are applied to a qualitatively different tool-use domain.

## Limitations
- The MDL consolidation process relies on a greedy search that may miss globally optimal rule sets, especially in larger rule spaces.
- Cross-model transferability results are based on a limited set of LLM pairs; generalizability to significantly different model architectures remains unproven.
- The approach assumes access to ground-truth traces for every failure, which may be infeasible in many real-world applications.

## Confidence
- **High**: The core neuro-symbolic architecture (EBL + MDL + Symbolic Retrieval) is sound and well-motivated; reported improvements over baseline methods are statistically significant.
- **Medium**: Cross-model rule transfer results are compelling but require further validation across a broader range of model families and domains.
- **Low**: Long-term robustness of the rule library to concept drift and scalability of greedy MDL search to very large rule pools are not addressed.

## Next Checks
1. **Ablate Ground-Truth Dependency:** Re-run rule learning on a subset of ToolHop without ground-truth traces; compare rule quality and quantity from prompting-only EBL versus full EBL+validation.
2. **Stress Test MDL Consolidation:** Inject a large number of noisy, low-quality candidate rules into R0; evaluate whether MDL successfully prunes them and whether final library R maintains or improves accuracy.
3. **Evaluate Dynamic Rule Update:** Simulate concept drift by shifting tool-use query distribution after initial rule library is learned; implement online update mechanism and measure accuracy recovery versus static rule library.