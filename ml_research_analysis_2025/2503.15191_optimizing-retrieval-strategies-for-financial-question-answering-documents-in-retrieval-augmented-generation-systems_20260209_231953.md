---
ver: rpa2
title: Optimizing Retrieval Strategies for Financial Question Answering Documents
  in Retrieval-Augmented Generation Systems
arxiv_id: '2503.15191'
source_url: https://arxiv.org/abs/2503.15191
tags:
- retrieval
- financial
- query
- data
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a Retrieval-Augmented Generation (RAG) pipeline
  optimized for financial question answering, addressing challenges posed by domain-specific
  vocabulary and complex tabular data in documents like 10-K reports. The approach
  employs a three-phase framework: pre-retrieval data preprocessing, retrieval using
  fine-tuned embedding models with hybrid dense-sparse strategies, and post-retrieval
  reranking and document selection.'
---

# Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems

## Quick Facts
- arXiv ID: 2503.15191
- Source URL: https://arxiv.org/abs/2503.15191
- Reference count: 18
- Primary result: Achieved NDCG@10 of 0.50864 through fine-tuned embedding models and hybrid retrieval strategies for financial QA

## Executive Summary
This paper presents a three-phase Retrieval-Augmented Generation pipeline optimized for financial question answering on complex documents like 10-K reports. The approach addresses challenges posed by domain-specific vocabulary and tabular data through fine-tuned embedding models, hybrid dense-sparse retrieval with alpha tuning, and post-retrieval reranking and document selection. Experiments on seven financial QA datasets demonstrate significant improvements in retrieval performance, with the fine-tuned stella en 1.5B v5 model achieving NDCG@10 of 0.50864 compared to 0.32178 baseline. The study also highlights the importance of DPO-trained lightweight models for answer generation and identifies opportunities for future work in streaming data, multilingual support, and security enhancements.

## Method Summary
The proposed framework employs a three-phase approach: pre-retrieval data preprocessing using query expansion and corpus markdown restructuring, retrieval using fine-tuned embedding models with hybrid dense-sparse strategies, and post-retrieval reranking and document selection. The method fine-tunes state-of-the-art embedding models (stella en 1.5B v5) using contrastive learning with Multiple Negatives Ranking Loss on financial domain data. Hybrid retrieval combines dense semantic representations with sparse keyword matching through a tuned alpha parameter, while post-retrieval reranking improves document relevance before final answer generation using DPO-trained lightweight models.

## Key Results
- Fine-tuned stella en 1.5B v5 achieved NDCG@10 of 0.50864 versus 0.32178 baseline
- Optimal alpha tuning for hybrid retrieval varies by dataset type (0.25 for FinQA arithmetic to 0.932 for FinQABench)
- Reranking improved NDCG@10 from 0.52572 to 0.58001 on FinDER dataset
- DPO-trained lightweight models outperformed larger models in answer generation quality

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Embedding Fine-Tuning
Fine-tuning general-purpose embedding models on financial domain data improves retrieval accuracy by aligning semantic representations with domain-specific vocabulary and document structures. Contrastive learning with Multiple Negatives Ranking Loss trains the embedder to distinguish relevant query-document pairs from irrelevant ones, creating tighter clusters for financial semantics. The fine-tuned stella en 1.5B v5 achieved NDCG@10 of 0.50864 compared to 0.32178 baseline.

### Mechanism 2: Hybrid Dense-Sparse Retrieval with Alpha Tuning
Combining dense (semantic) and sparse (keyword) retrieval scores with a tuned alpha parameter captures both exact term matching and deep semantic relationships better than either method alone. The formula $S_{total} = \alpha \cdot S_{dense} + (1-\alpha) \cdot S_{sparse}$ allows optimal balance between contextual meaning and precise financial terms. Alpha is tuned per-dataset (e.g., 0.25 for FinQA arithmetic questions favoring sparse; 0.85 for FinanceBench semantic questions favoring dense).

### Mechanism 3: Post-Retrieval Reranking and Document Selection
Cross-encoder reranking and LLM-based document selection improve final answer quality by reordering top-K results and filtering irrelevant context before generation. Rerankers compute query-document relevance with deeper attention, improving NDCG@10 from 0.52572 to 0.58001 on FinDER. Document selection uses an LLM agent to identify only relevant documents from top-10, reducing noise and addressing "Lost in the Middle" context issues.

## Foundational Learning

- **Concept: Contrastive Learning for Embeddings**
  - Why needed here: Understanding how MNRLoss creates positive/negative pairs is essential for replicating the fine-tuning process on your own financial data
  - Quick check question: Can you explain why random negative sampling might fail for financial documents with similar boilerplate text?

- **Concept: NDCG@10 Evaluation Metric**
  - Why needed here: This is the primary metric for comparing retrieval performance; understanding position-weighted relevance is critical for interpreting results
  - Quick check question: Why does NDCG penalize a relevant document at rank 10 more than at rank 1?

- **Concept: Dense vs. Sparse Retrieval Trade-offs**
  - Why needed here: Selecting and tuning alpha requires understanding when semantic matching outperforms keyword matching (and vice versa)
  - Quick check question: For a query asking "What was Apple's revenue in Q3 2023?", would you expect dense or sparse retrieval to be more effective?

## Architecture Onboarding

- **Component map**: Query → Query expansion → Hybrid retrieval (alpha-tuned) → Top-20 candidates → Reranking → Top-10 → Document selection → DPO-trained LLM → Answer
- **Critical path**: Query → Query expansion → Hybrid retrieval (alpha-tuned) → Top-20 candidates → Reranking → Top-10 → Document selection → DPO-trained LLM → Answer
- **Design tradeoffs**:
  - Stella 1.5B vs. 400M: 1.5B achieves higher NDCG (0.50864 vs. 0.40186) but requires more GPU memory
  - Table augmentation vs. markdown: Paper found markdown restructuring outperformed table-specific preprocessing (0.48645 vs. 0.45411), suggesting fine-tuned embedders handle tables natively
  - Reranking latency vs. accuracy: Reranking improves NDCG but adds inference time; consider for batch processing, skip for real-time
- **Failure signatures**:
  - Low NDCG@10 with high alpha on arithmetic queries (e.g., FinQA): Indicates over-reliance on dense retrieval for fact-based questions
  - Reranking degrades performance (as in FinQABench): May indicate reranker miscalibration for certain query types
  - Selection agent over-pruning: If context precision drops below 0.35, agent may be rejecting relevant documents
- **First 3 experiments**:
  1. **Baseline retrieval comparison**: Run stella 1.5B (non-fine-tuned) vs. BM25 vs. hybrid (alpha=0.5) on your financial corpus to establish benchmarks
  2. **Alpha sensitivity analysis**: Increment alpha from 0 to 1 in 0.1 steps on a held-out validation set; plot NDCG@10 to identify optimal range for your query types
  3. **Ablate reranking**: Compare top-10 retrieval results with and without voyage-rerank-2; measure both NDCG improvement and latency overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RAG pipelines be adapted to efficiently index and retrieve rapidly changing, streaming financial data such as real-time disclosures and stock price fluctuations?
- Basis in paper: The authors state that "in-depth research on techniques for efficiently retrieving and indexing streaming data is essential" because current methods struggle with "rapidly changing financial data."
- Why unresolved: The proposed pipeline is evaluated on static datasets (e.g., 10-K reports), whereas financial markets generate continuous, time-sensitive information that static indexing cannot handle.
- What evidence would resolve it: A modified pipeline incorporating dynamic or temporal-aware retrieval mechanisms that maintains high NDCG scores on streaming financial data.

### Open Question 2
- Question: To what extent can the current financial RAG framework be extended to support multilingual queries and documents?
- Basis in paper: Section 7 highlights "the need for a multilingual extension of the RAG framework" driven by the global nature of financial environments.
- Why unresolved: The current study focuses on English-specific datasets and models (e.g., stella en 1.5B), leaving multilingual performance untested.
- What evidence would resolve it: Successful evaluation of the pipeline on multilingual financial benchmarks or the integration of multilingual embedding models showing comparable retrieval performance.

### Open Question 3
- Question: What specific layered security strategies are required to effectively mitigate malicious prompt attacks in financial RAG systems?
- Basis in paper: The authors note the system's "susceptibility... to various threats, including malicious prompt attacks" and suggest that adopting strategies like LLaMA Guard is crucial.
- Why unresolved: The paper focuses on retrieval optimization but does not implement or test specific security measures against adversarial inputs.
- What evidence would resolve it: Integration of guardrail models demonstrating robustness against prompt injection attacks without significantly degrading retrieval latency or answer relevance.

## Limitations
- Query expansion and document selection effectiveness heavily depend on prompt engineering quality, which is not fully specified
- Hybrid retrieval alpha tuning is dataset-specific and may not generalize to unseen financial query distributions
- Multimodal capabilities are mentioned but not extensively validated on non-text data like charts or tables

## Confidence
- **High Confidence**: Retrieval performance improvements through fine-tuned embeddings and hybrid strategies (NDCG@10 of 0.50864) - supported by extensive experimental results across seven datasets
- **Medium Confidence**: Document selection and reranking effectiveness - while results show improvements in most datasets, FinQABench showed degradation, suggesting sensitivity to query type
- **Medium Confidence**: Cross-dataset generalization of alpha tuning - optimal alpha varies significantly (0.25-0.932) across datasets, indicating need for task-specific tuning

## Next Checks
1. **Alpha Sensitivity Analysis**: Implement a systematic hyperparameter search for the hybrid retrieval alpha parameter across all seven datasets, rather than using single default values, to validate the claimed task-specific optimization
2. **Ablation Studies on Pipeline Components**: Measure performance degradation when removing query expansion, reranking, or document selection components to quantify their individual contributions beyond aggregate metrics
3. **Generalization Testing**: Apply the optimized pipeline to out-of-distribution financial queries (e.g., cryptocurrency, ESG reporting) to assess transfer performance beyond the original seven datasets