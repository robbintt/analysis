---
ver: rpa2
title: 'Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated
  Statistics'
arxiv_id: '2501.17273'
source_url: https://arxiv.org/abs/2501.17273
tags:
- debate
- which
- llms
- more
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the persuasive capabilities of large language\
  \ models (LLMs) in a structured debate setting, focusing on how different strategies\u2014\
  personalization, fabricated statistics, and a combination approach\u2014affect their\
  \ ability to change human opinions. The research finds that while simple and personalized\
  \ debate strategies have limited effectiveness, a mixed approach using a multi-agent\
  \ system that combines personalized arguments with fabricated statistics is significantly\
  \ more persuasive, achieving a 51% chance of changing participants' opinions compared\
  \ to 32% for static human-written arguments."
---

# Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics

## Quick Facts
- arXiv ID: 2501.17273
- Source URL: https://arxiv.org/abs/2501.17273
- Reference count: 40
- Primary result: Mixed strategy (personalized + fabricated stats) achieved 51% persuasion rate vs 32% for static human arguments

## Executive Summary
This study investigates how large language models (LLMs) can be optimized for persuasion in structured debates. Through a controlled experiment with 33 participants, the research compares four debate strategies: simple, statistics-only, personalized, and a mixed approach combining all three. The findings reveal that while individual strategies show limited effectiveness, the mixed approach using a multi-agent system achieves significantly higher persuasion rates (51% vs 32%), demonstrating that properly scaffolded LLMs can be highly persuasive tools.

## Method Summary
The research employed a structured 3-phase debate format using GPT-4o-mini via OpenAI API, testing four debate strategies across 29 topics with 33 participants recruited from Prolific. Participants provided demographic information and Big 5 personality traits via TIPI questionnaire, then engaged in debates where their opinions were measured before and after using 7-point Likert scales. The study used a mixed approach with a 3-agent scaffold (personalized→stats→executive) that combined tailored arguments with fabricated statistics, comparing results against simple, statistics-only, and personalized strategies.

## Key Results
- Mixed strategy achieved 51% chance of changing opinions (vs 32% for static human arguments)
- Persuasiveness measured through Likert Δ of 1.146 for Mixed strategy
- Combination of personalization and fabricated statistics proved significantly more effective than either strategy alone

## Why This Works (Mechanism)
The study demonstrates that LLM persuasiveness scales super-linearly when combining personalization with strategic information deployment. The multi-agent scaffold creates a synthesis layer that optimizes argument construction based on individual user characteristics, while fabricated statistics provide seemingly authoritative evidence that overcomes cognitive resistance. This suggests persuasion operates through multiple psychological pathways simultaneously rather than single-point argumentation.

## Foundational Learning
- **Multi-agent orchestration**: Understanding how to coordinate specialized LLM agents for complex tasks is crucial for building effective persuasion systems. Quick check: Can you map the agent workflow from personalized to stats to executive?
- **Psychological scaffolding**: The study reveals how different persuasion strategies target distinct cognitive mechanisms. Quick check: Identify which strategy (personalization vs statistics) targets emotional vs logical reasoning pathways.
- **Fabricated information risks**: The effectiveness of made-up statistics highlights vulnerabilities in human information processing. Quick check: What ethical safeguards would you implement for deploying such systems?

## Architecture Onboarding

**Component Map**
Participant -> Demographics/TIPI -> Debate Platform -> GPT-4o-mini (4 strategies) -> Likert pre/post -> Analysis

**Critical Path**
Randomization -> Debate execution -> Opinion measurement -> Statistical analysis -> Effect comparison

**Design Tradeoffs**
- Speed vs personalization: More detailed user analysis increases persuasion but extends interaction time
- Authenticity vs effectiveness: Fabricated statistics work but raise ethical concerns
- Simplicity vs sophistication: Multi-agent systems outperform single-agent approaches but increase complexity

**Failure Signatures**
- Low word count responses (<50 words) indicate participant disengagement
- LLM refusals when fabricating statistics suggest prompt engineering issues
- High variance in opinion shifts may indicate topic selection problems

**First 3 Experiments**
1. Implement single-agent version of mixed strategy to isolate scaffold effect
2. Test different temperature settings (0.0, 0.7, 1.2) to measure impact on persuasiveness
3. Replicate with broader participant demographics to test generalizability

## Open Questions the Paper Calls Out
- Does the mixed strategy's effectiveness primarily stem from multi-agent scaffolding or the combination of personalized statistics? An ablation study comparing single-agent vs multi-agent implementations would resolve this.
- Are opinion shifts induced by fabricated statistics durable over time? The study couldn't track persistence due to immediate debriefing protocols.
- Does the mixed strategy maintain effectiveness on deeply partisan topics compared to the less controversial topics used in this study? Testing with high-polarization issues would provide insight.
- How does effectiveness change in free-form social media environments compared to structured debates? A simulated social media interface would allow testing of non-adversarial persuasion techniques.

## Limitations
- Unknown temperature setting for GPT-4o-mini affects response diversity and persuasiveness
- Multi-agent communication protocol details remain incomplete
- Limited sample size (n=33) may underpower detection of smaller effect sizes
- Study focuses on less controversial topics, potentially inflating success rates

## Confidence
- **High**: Experimental design is clearly specified with defined phases, measurement scales, and randomization procedures
- **Medium**: Multi-agent system implementation appears functional but lacks complete procedural details
- **Low**: Generalizability beyond specific demographic sample and narrow topic set remains uncertain

## Next Checks
1. **Temperature Sensitivity Analysis**: Systematically vary temperature settings (0.0, 0.7, 1.2) to quantify their impact on persuasion rates and identify optimal configurations for the Mixed strategy.
2. **Communication Protocol Validation**: Implement and test the complete multi-agent state management system, including exact chat history formatting between agents, to ensure faithful reproduction of the Mixed strategy's reported superiority.
3. **Generalizability Testing**: Replicate the experiment with a broader participant pool (different platforms, countries, education levels) and additional debate topics to assess whether the 51% persuasion rate holds across diverse populations and subject matter.