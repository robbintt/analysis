---
ver: rpa2
title: 'A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market
  Risk: Evidence from Cryptoasset Liquidity Spillovers'
arxiv_id: '2510.20066'
source_url: https://arxiv.org/abs/2510.20066
tags:
- volatility
- layer
- test
- returns
- liquidity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a three-layer pipeline linking liquidity and
  volatility of core cryptocurrencies to market-wide risk, using Granger causality,
  VAR impulse responses, HAR-X models, and a leakage-safe XGBoost classifier. Layer
  A finds significant LV-to-return spillovers (25/35 significant tests, BH-FDR q<0.05);
  Layer B shows limited PC-to-PC links (5/15 significant); Layer C shows volatility
  PCs Granger-cause the L2 cross-sectional crowding target at lag 3 (p=0.0146).
---

# A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers

## Quick Facts
- arXiv ID: 2510.20066
- Source URL: https://arxiv.org/abs/2510.20066
- Authors: Yimeng Qiu; Feihuang Fang
- Reference count: 22
- Key outcome: A three-layer pipeline linking liquidity and volatility of core cryptocurrencies to market-wide risk, using Granger causality, VAR impulse responses, HAR-X models, and a leakage-safe XGBoost classifier. Layer A finds significant LV-to-return spillovers (25/35 significant tests, BH-FDR q<0.05); Layer B shows limited PC-to-PC links (5/15 significant); Layer C shows volatility PCs Granger-cause the L2 cross-sectional crowding target at lag 3 (p=0.0146). Out-of-sample, the ML classifier achieves ROC-AUC=0.74, PR-AUC=0.47 (test set), outperforming logistic regression on PR-AUC, the preferred metric under class imbalance. Regression targets saturate (R²≈0.998), but classification results indicate the pipeline can serve as an early-warning tool for market risk spikes.

## Executive Summary
This paper presents a three-layer pipeline that links microstructure signals (liquidity and volatility) from core cryptocurrencies to market-wide risk through a chained Granger-causal structure. The methodology progresses from individual asset relationships (Layer A: LV-to-return spillovers), to latent factor transmission (Layer B: PC-to-PC links), to macro-level risk prediction (Layer C: volatility PCs predicting cross-sectional crowding). The pipeline employs a leakage-safe ML protocol with validation-only thresholding, achieving ROC-AUC=0.74 and PR-AUC=0.47 on test data. While regression targets saturate at R²≈0.998, the classification results demonstrate the pipeline's potential as an early-warning tool for market risk spikes.

## Method Summary
The methodology consists of three sequential layers: Layer A establishes Granger causality between liquidity/volatility and returns for core coins (identified via PageRank on Granger networks); Layer B tests PC(LV) to PC(returns) relationships; Layer C tests volatility PCs Granger-causing a cross-sectional crowding target. Each layer uses block-Granger tests within small VARs. The ML component uses XGBoost with a leakage-safe protocol (chronological 70/15/15 splits, lagged features, frozen validation threshold) to predict risk spikes (top 15% of target). HAR-X models provide long-memory benchmarks. The pipeline is validated on daily data for 74 crypto assets from 2017-2022.

## Key Results
- Layer A: 25/35 Granger tests show significant LV-to-return spillovers (BH-FDR q<0.05)
- Layer C: Volatility PCs Granger-cause the L2 cross-sectional crowding target at lag 3 (p=0.0146)
- ML Classifier: ROC-AUC=0.74, PR-AUC=0.47 on test set, outperforming logistic regression on PR-AUC
- Regression saturation: R²≈0.998 for AR/HAR-X baselines indicates level-prediction is trivially easy

## Why This Works (Mechanism)

### Mechanism 1
Core-asset liquidity and volatility signals transmit predictively to market-wide risk through a chained Granger-causal structure. A three-layer evidence chain (A→B→C) progressively aggregates microstructure signals: individual LV↔return links within core coins, latent-factor links via principal components, and volatility-PCs leading a cross-sectional crowding target. Block-Granger tests at each layer establish temporal precedence. Core assumption: Granger-causal links at short lags reflect information transmission rather than spurious correlation or omitted variables. Evidence anchors: Layer A shows 25/35 significant tests (abstract), Layer C shows volatility PCs Granger-cause crowding target at lag 3 (p=0.0146) (section 4), related work documents predictive spillovers among cryptocurrencies (corpus). Break condition: Structural breaks could invalidate lag structures; rolling windows show sensitivity in Layer C.

### Mechanism 2
Residualized volatility proxies isolate non-mechanical volatility dynamics that retain predictive content. For each core coin, regress volatility proxies on own return and retain residuals, removing the mechanical leverage effect while preserving exogenous volatility variation. Core assumption: The residualization does not discard signal; the remaining variance captures meaningful risk-transmission dynamics beyond return-contingent volatility. Evidence anchors: Residualization described in section 3.1, Layer A shows LV/volatility (BNB, XEM, YFI) Granger-causing returns while returns drive residual vol/turnover (section 4), no direct corpus evidence on residualization for Granger tests. Break condition: If residual volatility becomes noise-dominated, Granger tests would fail; Layer A's 25/35 significance suggests residualized signals retain structure.

### Mechanism 3
A leakage-safe ML protocol with validation-only thresholding improves early-warning performance under class imbalance. Chronological 70/15/15 splits prevent future leakage, features are uniformly lagged (+1 day) to ensure "today→t+H" forecasting, classification threshold is optimized on validation F1 and frozen for test, and early stopping prevents overfitting. PR-AUC is emphasized over ROC-AUC due to 15% positive-class imbalance. Core assumption: The temporal split and frozen threshold generalize; the 15% threshold definition captures "risk spikes" meaningfully. Evidence anchors: ML classifier achieves ROC-AUC=0.74, PR-AUC=0.47 (abstract), threshold chosen on validation set to maximize F1 then frozen (section 3.5), related ML-volatility papers use similar temporal splits (corpus). Break condition: If test distribution shifts, frozen threshold may be suboptimal; table shows PR-AUC varies (0.47–0.52) across rolling windows.

## Foundational Learning

- Concept: Granger Causality and VAR Impulse Responses
  - Why needed here: The entire Layer A/B/C evidence chain relies on block-Granger tests within small VARs; interpreting FEVD and IRFs requires understanding orthogonalized shocks.
  - Quick check question: If variable X Granger-causes Y at lag 3 with p<0.05, does this imply X causes Y in a structural sense? (Answer: No—Granger causality indicates temporal precedence, not structural causation.)

- Concept: Principal Component Analysis for Latent Factor Construction
  - Why needed here: Layer B and C use PCs to compress cross-sectional volatility/liquidity; k=3 return PCs explain 85.26% variance.
  - Quick check question: Why retain PCs rather than raw variables for Layer B tests? (Answer: Dimensionality reduction; PCs capture co-movement while reducing multiple-testing burden.)

- Concept: Class Imbalance and PR-AUC vs. ROC-AUC
  - Why needed here: Risk spikes are defined as top 15%, creating imbalance; PR-AUC is more informative than ROC-AUC when positives are rare.
  - Quick check question: If ROC-AUC=0.90 but PR-AUC=0.44, which model is better for early warning? (Answer: PR-AUC is preferred under imbalance—ROC can be inflated by easy true negatives.)

## Architecture Onboarding

- Component map: Data ingestion → Core selection → Layer A (LV↔returns) → Layer B (PC↔PC) → Layer C (Vol-PCs→crowding) → VAR/HAR-X → ML training → validation threshold → test evaluation

- Critical path: Core selection → residualization → PC construction → Layer A/B/C Granger tests → VAR/HAR-X estimation → ML training (70% train) → validation threshold tuning → test evaluation

- Design tradeoffs:
  - Raw vs. rolling-standardized target: Raw target significant at lag 2–3; RS variants not significant—trades robustness for signal strength
  - ROC-AUC vs. PR-AUC: Logit achieves higher ROC-AUC (~0.90) but lower PR-AUC (~0.44); XGBoost chosen for PR-AUC superiority under imbalance
  - Fixed-lag vs. BIC-selected VAR order: Fixed lags used for robustness; BIC may overfit in short samples

- Failure signatures:
  - Layer C sensitivity: Standardized or leave-k-out targets not significant; pipeline relies on specific raw target construction
  - Regression saturation: R²≈0.998 for AR/HAR-X baselines suggests level-prediction is trivially easy; classification is the discriminating task
  - Threshold drift: If test-set positive rate differs from 15%, frozen validation threshold may misclassify

- First 3 experiments:
  1. Reproduce Layer A heatmap: Run block-Granger tests on core LV↔returns; verify 25/35 significant at BH-FDR q<0.05 using provided artifact (output/run_20250808_221301/heatmap_layerA.png)
  2. Validate leakage-safe protocol: Shuffle train/valid/test splits randomly (violating temporal order); confirm PR-AUC drops or becomes unstable relative to 0.47 baseline
  3. Layer C robustness check: Vary rolling-standardization window (126 vs. 252) and lag (2 vs. 3); confirm raw target remains significant at lag 2 but RS variants do not (per table 2)

## Open Questions the Paper Calls Out

### Open Question 1
Does the predictive relationship between volatility PCs and the cross-sectional crowding target persist when using rolling-standardized targets, given the observed sensitivity to the RS window? The paper reports this discrepancy as a robustness check but does not investigate if the signal is strictly a raw volatility artifact or if the standardization destroys valid predictive information. A formal comparison of information content between raw and standardized targets, or a successful redesign of the target that maintains significance after normalization, would resolve this.

### Open Question 2
Can the pipeline provide incremental predictive value for risk levels beyond the near-perfect persistence captured by baseline AR/HAR-X models? The experiments show baseline AR/HAR-X models saturate regression R² ≈ 0.998, while the spillover-based ML pipeline offers improvements primarily in classification (PR-AUC) rather than level prediction. It is unclear if the spillover features capture transient risk dynamics invisible to autoregressive baselines or if the regression task is simply too dominated by persistence for complex features to add value. An ablation study showing if spillover features improve regression residuals or forecast extreme quantile losses better than persistence baselines would resolve this.

### Open Question 3
How sensitive is the endogenous "core-coin" selection to structural breaks or the modest sample length (1462 observations)? The Discussion lists "modest sample length" and "potential structural breaks" as limitations, and the Method section describes core-coin selection as an endogenous process based on PageRank. Endogenous network selection can be unstable in short samples or non-stationary environments; if the "core" set shifts over time, the pipeline's stability is compromised. Rolling-window analysis of the Granger-PageRank network to quantify the turnover rate of identified core assets would resolve this.

## Limitations
- Structural break sensitivity: The three-layer Granger-causality chain relies on stable temporal relationships that may degrade during regime shifts
- Residualization assumption: Lacks external validation that removing mechanical leverage effects preserves predictive signal
- Threshold generalization: Frozen validation-set threshold may not generalize if test-set positive rates differ from the 15% used in training

## Confidence
**High Confidence:**
- Layer A Granger-causality results (25/35 significant tests at BH-FDR q<0.05)
- Regression target saturation (R²≈0.998 for AR/HAR-X baselines)
- Basic ML protocol design (chronological splits, lag alignment, early stopping)

**Medium Confidence:**
- Three-layer transmission mechanism validity
- PR-AUC superiority of XGBoost over logistic regression
- Layer C Granger-causality at lag 3 (p=0.0146)

**Low Confidence:**
- Pipeline robustness across market regimes
- Optimal positive class threshold for real-world deployment
- External validity beyond the specific crypto assets and time period studied

## Next Checks
1. **Structural Break Robustness Test**: Implement Chow tests or rolling-window break detection across all three layers. Re-run the Granger-causality chain on pre-2022 and post-2022 subsamples to quantify performance degradation. This addresses the regime sensitivity concern and validates whether the three-layer structure holds across market conditions.

2. **Threshold Robustness Across Time**: Systematically vary the positive-class threshold (10%, 15%, 20%, 25%) across all rolling windows and evaluate PR-AUC stability. Additionally, implement a dynamic threshold that adjusts based on test-set positive rates rather than freezing validation-set F1 optimization. This would validate whether the frozen threshold assumption holds in practice.

3. **Residualization Signal Decomposition**: Conduct an ablation study comparing: (a) raw volatility vs. residualized volatility in Layer A, (b) including both raw and residualized features in Layer C ML model, and (c) comparing SHAP attributions between raw and residualized feature sets. This would quantify what predictive information, if any, is lost through residualization and whether mechanical leverage effects contain valuable signal.