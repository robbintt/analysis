---
ver: rpa2
title: Multi-Agent DRL for Queue-Aware Task Offloading in Hierarchical MEC-Enabled
  Air-Ground Networks
arxiv_id: '2503.03391'
source_url: https://arxiv.org/abs/2503.03391
tags:
- task
- offloading
- computing
- tasks
- haps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the energy minimization problem in multi-tier
  MEC-enabled air-ground networks (MAGIN) by jointly optimizing UAV trajectories,
  computing resource allocation, and queue-aware task offloading decisions. The authors
  reformulate the problem as a multi-agent Markov decision process (MDP) with continuous
  action spaces and heterogeneous agents, and propose a novel variant of multi-agent
  proximal policy optimization with Beta distribution (MAPPO-BD) to solve it.
---

# Multi-Agent DRL for Queue-Aware Task Offloading in Hierarchical MEC-Enabled Air-Ground Networks

## Quick Facts
- **arXiv ID:** 2503.03391
- **Source URL:** https://arxiv.org/abs/2503.03391
- **Reference count:** 40
- **Primary result:** Proposed MAPPO-BD achieves up to 32% greater energy savings compared to PO-MAPPO in multi-tier MAGIN.

## Executive Summary
This paper addresses energy minimization in multi-tier MEC-enabled air-ground networks by jointly optimizing UAV trajectories, computing resource allocation, and queue-aware task offloading. The authors reformulate the problem as a multi-agent Markov decision process with continuous action spaces and heterogeneous agents (IoTDs, UAVs, HAPS), proposing a novel MAPPO-BD algorithm using Beta distribution for action sampling. The approach outperforms baseline schemes, achieving superior energy savings and efficient resource management while meeting queue delay and edge computing constraints. The algorithm demonstrates faster and more stable convergence compared to other benchmark methods.

## Method Summary
The proposed method employs a multi-agent proximal policy optimization with Beta distribution (MAPPO-BD) framework to solve a joint optimization problem for queue-aware task offloading in hierarchical MEC-enabled air-ground networks. The system uses three types of agents: IoTD agents that decide offloading ratios, UAV agents that determine trajectories and allocated compute resources, and a HAPS agent that allocates computing resources for relayed tasks. The framework uses centralized training with decentralized execution (CTDE), where a central critic learns from global state information during training while each agent's actor selects actions based on local observations during execution. The Beta distribution in actor networks enables more stable learning for heterogeneous agents operating with different, bounded action ranges.

## Key Results
- MAPPO-BD achieves up to 32% greater energy savings compared to PO-MAPPO-based approach
- Algorithm converges relatively early: episode 200 for IoTD agents and episode 250 for UAV and HAPS agents
- Demonstrates superior performance in energy minimization while satisfying queue delay constraints and edge computing requirements

## Why This Works (Mechanism)

### Mechanism 1: Beta Distribution for Bounded, Heterogeneous Action Spaces
The Beta distribution is naturally bounded on [0,1], aligning with action spaces like offloading ratio [0,1] and resource allocation. This eliminates the need for action clipping, reduces boundary artifacts that can destabilize policy gradients, and leads to more uniform exploration and faster convergence. The bounded support helps preserve the integrity of the policy gradient.

### Mechanism 2: Centralized Training with Decentralized Execution (CTDE) for Hierarchical Coordination
During training, a central critic with access to global state learns inter-agent dependencies and guides policy updates. During execution, each agent's actor uses only local observation to select actions, ensuring scalability and real-time decision-making. This treats the complex joint optimization problem as a multi-agent MDP.

### Mechanism 3: Queue-Aware State Formulation for Constraint Satisfaction
Explicitly including local, offloading, and edge queue lengths and delays in agent observations enables learning policies that inherently satisfy long-term queue delay constraints. The reward function penalizes queue buildup, shaping the agent's policy to balance immediate energy minimization with maintaining queue stability.

## Foundational Learning

- **Concept: Multi-Agent Deep Reinforcement Learning (MADRL)**
  - **Why needed here:** The entire solution framework is built on MADRL, requiring understanding of agents, policies, value functions, and actor-critic paradigm.
  - **Quick check question:** Can you explain the difference between on-policy and off-policy algorithms, and which category MAPPO falls into?

- **Concept: Queueing Theory Basics**
  - **Why needed here:** The system model manages task buffers and their associated delays, requiring understanding of buffer dynamics and Little's Law.
  - **Quick check question:** If the task arrival rate to a queue consistently exceeds the processing rate, what will happen to the queue length and delay over time?

- **Concept: Mobile Edge Computing (MEC) in Aerial Networks**
  - **Why needed here:** Understanding the application domain of hierarchical MEC networks with UAVs and motivations for latency and energy trade-offs in offloading compute tasks.
  - **Quick check question:** What are the two primary sources of delay and energy consumption when an IoTD offloads a task to a UAV for processing?

## Architecture Onboarding

- **Component map:** IoTD agents -> UAV agents -> HAPS agent -> simulated environment (mobility, communication channels, task queues) -> centralized critics (IoTD, UAV, HAPS) -> decentralized actors (Beta distributions)

- **Critical path:**
  1. Implement environment with IoTD mobility, UAV trajectory, communication data rate, and queueing models
  2. Precisely define observation, action, and reward spaces for each agent type
  3. Build MAPPO-BD algorithm with central training module collecting tuples from all agents and performing PPO updates using Beta policy distribution

- **Design tradeoffs:**
  - Beta vs. Gaussian Distribution: Beta avoids boundary clipping but introduces two shape parameters to learn per action dimension
  - Shared vs. Separate Critics: One critic per agent type improves sample efficiency but assumes homogeneous state-value functions within a type
  - Reward Shaping: Heavy reliance on penalty terms requires careful tuning of penalty weights

- **Failure signatures:**
  - Non-convergence: Rewards do not stabilize due to poor learning rates, insufficient exploration, or conflicting reward components
  - Constraint Violation: Queue delays consistently exceed maximums, indicating penalty weights are too low
  - UAV Inaction or Clustering: UAVs avoid serving IoTDs or cluster in one area, suggesting coverage reward is insufficient

- **First 3 experiments:**
  1. Baseline Sanity Check: Implement environment with single IoTD and UAV using standard PPO agent to validate environment and reward logic
  2. Ablate the Policy Distribution: Compare MAPPO with Gaussian vs. Beta distribution, plotting episode rewards and action histograms
  3. Stress Test Queue Constraints: Fix trained policy and run evaluations with increasing task arrival rates to test robustness

## Open Questions the Paper Calls Out

- **Open Question 1:** How does MAPPO-BD perform under more dynamic and stochastic network conditions, specifically varying interference levels, unpredictable agent mobility, and changing environmental factors?
- **Open Question 2:** What is the impact of imperfect Channel State Information (CSI) and estimation errors on the proposed algorithm's convergence and energy efficiency?
- **Open Question 3:** How can the optimization framework be extended to ensure physical layer security and privacy during task offloading?

## Limitations

- The precise neural network architectures, hyperparameter values (especially reward penalty weights and Beta distribution shape parameters), and exact PPO training procedure are not provided, making exact reproduction difficult
- The empirical evidence for Beta distribution superiority is limited to comparisons with a single alternative (PO-MAPPO) without ablation studies on distribution choice itself
- The CTDE framework's effectiveness depends heavily on the critic's ability to learn from global state, but the sufficiency of state representation for capturing all inter-agent dependencies is not validated

## Confidence

- **High Confidence:** The overall MADRL framework design (CTDE with separate actor-critic networks per agent type) is clearly specified and follows established conventions. The system model equations are well-defined.
- **Medium Confidence:** The problem formulation as a multi-agent MDP is sound, and the inclusion of queue states in observations is a reasonable approach for constraint satisfaction.
- **Low Confidence:** The specific claim that the Beta distribution is the key driver of the 32% energy savings improvement over PO-MAPPO is not robustly supported without additional ablation studies.

## Next Checks

1. **Ablate the Policy Distribution:** Implement and train both MAPPO-BD and MAPPO-ND (with Gaussian distribution) on the same environment and task arrival rates. Compare not only the final episode rewards but also the histograms of sampled actions over training to quantify the reduction in boundary artifacts. Report the standard deviation of rewards across 5 independent runs for each method.

2. **Stress Test Queue Robustness:** Using a fixed MAPPO-BD policy, conduct a parameter sweep by increasing the task arrival rate (e.g., increase bits per task by 20%, 40%, 60%) and measure the achieved queue delays (`Q^l`, `Q^o`, `Q^e`) and the corresponding penalty component of the reward. Plot these metrics to identify the policy's breaking point and the relationship between arrival rate and constraint violation.

3. **Hyperparameter Sensitivity Analysis:** Perform a small-scale grid search over the key reward penalty weights (ψ₁, ψ₂, ψ₃, ψ₄) and the Beta distribution's initial shape parameters (α, β). For each configuration, train the MAPPO-BD algorithm and record the convergence speed (episodes to stabilize) and the minimum achieved total energy consumption. Identify which parameters have the most significant impact on performance and constraint satisfaction.