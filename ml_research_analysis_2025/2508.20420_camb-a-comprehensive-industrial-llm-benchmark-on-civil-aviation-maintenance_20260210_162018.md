---
ver: rpa2
title: 'CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance'
arxiv_id: '2508.20420'
source_url: https://arxiv.org/abs/2508.20420
tags:
- fault
- maintenance
- aviation
- thinking
- civil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAMB, a comprehensive industrial LLM benchmark
  for civil aviation maintenance, addressing the lack of specialized evaluation tools
  for large language models in this domain. The benchmark includes seven tasks and
  eight datasets covering fault description, system localization, failure tracing,
  manual application, and maintenance recommendation, with knowledge spanning aerodynamics,
  electromechanical control, materials science, and communication systems.
---

# CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance

## Quick Facts
- arXiv ID: 2508.20420
- Source URL: https://arxiv.org/abs/2508.20420
- Reference count: 30
- Introduces CAMB, a comprehensive industrial LLM benchmark for civil aviation maintenance

## Executive Summary
This paper introduces CAMB, a comprehensive industrial LLM benchmark for civil aviation maintenance, addressing the lack of specialized evaluation tools for large language models in this domain. The benchmark includes seven tasks and eight datasets covering fault description, system localization, failure tracing, manual application, and maintenance recommendation, with knowledge spanning aerodynamics, electromechanical control, materials science, and communication systems. Experimental evaluation of state-of-the-art embedding models and LLMs shows that current embedding models excel at semantic similarity but struggle with factual knowledge retrieval accuracy for civil aviation maintenance, while LLMs achieve 60-70% accuracy on multiple-choice tasks but face challenges in reasoning-intensive tasks due to knowledge deficiencies.

## Method Summary
The benchmark consists of 7 tasks and 8 datasets covering fault description, system localization, failure tracing, manual application, and maintenance recommendation. Tasks include bilingual terminology alignment, fault system localization, text chapter localization, multiple-choice QA, fault manual matching, open QA, and fault-tree reasoning. The datasets are constructed from textbooks, FIM/TSM manuals, and historical cases, with knowledge spanning aerodynamics, electromechanical control, materials science, and communication systems. Evaluation uses F1, accuracy, V-measure, NDCG@10, and LLM-as-judge scoring. The benchmark tests both embedding models and LLMs with thinking/non-thinking prompting strategies.

## Key Results
- Current embedding models excel at semantic similarity but struggle with factual knowledge retrieval accuracy for civil aviation maintenance
- LLMs achieve 60-70% accuracy on multiple-choice tasks but face challenges in reasoning-intensive tasks due to knowledge deficiencies
- The benchmark effectively distinguishes performance differences between models and is available open-source

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAMB discriminates model capabilities by testing domain-specific knowledge rather than general semantic similarity.
- Mechanism: The benchmark includes tasks like fault system localization, FIM manual matching, and fault-tree reasoning that require specialized aviation knowledge. Models that perform well on general benchmarks (e.g., MTEB tasks) show performance gaps on CAMB because semantic embeddings do not encode domain facts.
- Core assumption: Domain knowledge is not implicitly captured by large-scale pre-training on general corpora; it requires explicit encoding or retrieval.
- Evidence anchors:
  - [abstract] "current embedding models excel at semantic similarity but struggle with factual knowledge retrieval accuracy for civil aviation maintenance"
  - [section 5.1] "Classification and Reranker-choice, as well as Fault-Tree models, perform poorly, possibly because these three tasks are more inclined towards knowledge embedding, while most embedding models primarily focus on semantic embedding"
  - [corpus] KEO paper notes similar challenges in safety-critical aviation maintenance, requiring explicit knowledge extraction via RAG

### Mechanism 2
- Claim: Thinking mode fails to outperform non-thinking mode in aviation tasks because knowledge deficiencies cause reasoning to amplify errors rather than correct them.
- Mechanism: Thinking mode generates intermediate reasoning steps (Test-Time Scaling Law). However, when the model lacks aviation facts or holds incorrect conceptual priors, extended reasoning propagates errors. Non-thinking mode bypasses this by directly retrieving memorized answers, which may be more accurate if the model has partial knowledge.
- Core assumption: The benefit of reasoning scales with the quality and density of domain knowledge in the model's parameters.
- Evidence anchors:
  - [abstract] "LLMs achieve 60-70% accuracy on multiple-choice tasks but face challenges in reasoning-intensive tasks due to knowledge deficiencies"
  - [section 5.2] "knowledge deficiencies and conceptual ambiguities likely cause it to underperform the instant responses of the non-thinking mode"
  - [corpus] Weak corpus support; related work on industrial LLM applications notes similar reasoning failures in domains with sparse training data

### Mechanism 3
- Claim: The fault-tree task structure evaluates multi-step causal reasoning, not just single-hop retrieval.
- Mechanism: Fault trees decompose real troubleshooting cases into hierarchical nodes (root cause → intermediate causes → observed fault). Models must infer the next plausible node given history and knowledge. This requires chaining inferences rather than matching surface patterns.
- Core assumption: Real-world troubleshooting follows approximately tree-structured causal paths; models that can navigate these paths demonstrate deeper understanding.
- Evidence anchors:
  - [section 3.1] "Each case is transformed into a fault tree where the root node represents the observed fault phenomenon, intermediate nodes denote causes and sub-causes"
  - [section 3.2] "This design allows the model to infer plausible nodes for the subsequent level based on the available historical information and knowledge"
  - [corpus] HybridRAG and related work on aircraft fault diagnosis similarly use structured reasoning paths

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper explicitly positions RAG as the dominant practical solution for aviation maintenance applications. Understanding how embedding retrieval feeds into LLM generation is essential for interpreting benchmark results.
  - Quick check question: Can you explain why semantic similarity in embeddings does not guarantee correct factual retrieval in a RAG pipeline?

- Concept: **Test-Time Scaling Law**
  - Why needed here: The paper observes that thinking mode exhibits scaling properties (more tokens → potential accuracy gain), but notes this is limited by knowledge deficits. Understanding this helps diagnose when to invest in inference-time compute.
  - Quick check question: If a model has correct domain knowledge, should increasing thinking budget improve accuracy? Why or why not?

- Concept: **Fault Tree Analysis**
  - Why needed here: The benchmark's most reasoning-intensive task uses fault trees to assess multi-step diagnostic reasoning. This is a standard reliability engineering method.
  - Quick check question: Given a fault tree with a root "engine overheating," what information would you need to infer the most likely intermediate cause?

## Architecture Onboarding

- Component map: Data Layer -> Task Layer -> Evaluation Layer -> Model Layer
- Critical path: Data construction -> Task definition -> Model inference -> Scoring -> Analysis
- Design tradeoffs:
  - Model size vs. efficiency: Larger embedding models achieve higher accuracy but slower inference
  - Thinking vs. non-thinking modes: Thinking increases inference cost but shows limited gains in aviation tasks due to knowledge gaps
  - Automatic vs. human evaluation: Fault-tree and QA tasks require human or LLM-as-judge evaluation, adding cost but enabling nuanced scoring
- Failure signatures:
  - High semantic similarity scores but low classification/fault-tree accuracy indicates knowledge embedding deficiency
  - Non-thinking mode outperforming thinking mode signals domain knowledge gaps
  - Consistent low scores on specific aircraft types suggest training data sparsity
- First 3 experiments:
  1. Run the provided benchmark suite on your chosen embedding model and LLM using the open-source code
  2. Fine-tune an embedding model on the Chinese-English aligned and sentence pairing datasets, then re-evaluate retrieval and classification tasks
  3. Build a RAG pipeline using the curated corpus set and top-performing embedding model, test LLM QA performance with and without retrieved context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific adaptation strategies (e.g., fine-tuning or RAG optimization) resolve the "knowledge deficiencies" that prevent "thinking" mode LLMs from outperforming "non-thinking" instant answering in civil aviation maintenance?
- Basis in paper: [explicit] The authors state in Section 7 that "knowledge deficiencies and conceptual ambiguities" cause thinking modes to underperform, and resolving this "knowledge deficit" is a primary objective for future work.
- Why unresolved: Current experimental results show that thinking modes exhibit Test-Time Scaling Law properties but fail to surpass the accuracy of non-thinking models on domain-specific multiple-choice questions.
- What evidence would resolve it: Demonstration of a model where the thinking mode consistently achieves significantly higher accuracy than the non-thinking mode on the CAMB benchmark after domain-specific training.

### Open Question 2
- Question: How can embedding models be effectively modified to prioritize factual knowledge retrieval accuracy over semantic similarity in the civil aviation maintenance domain?
- Basis in paper: [explicit] Section 1 and Section 5.1 note that while current embedding models excel at semantic similarity, they struggle with factual knowledge retrieval tasks like Classification and Reranker-choice.
- Why unresolved: The paper identifies this limitation but only suggests "exploratory domain adaptation" as a future direction without providing a solution.
- What evidence would resolve it: Development of an embedding model that achieves significantly higher scores on the CAMB Classification and Fault-Tree tasks without sacrificing semantic performance.

### Open Question 3
- Question: To what extent does the small sample size of the Fault-tree dataset (50 entries) limit the statistical robustness of evaluating complex multi-step reasoning capabilities?
- Basis in paper: [inferred] Section 3.1 mentions the dataset contains only 50 entries derived from B737 and A320 cases; Section 5.1.2 attempts to mitigate this with multi-dimensional analysis, implying the size is a constraint.
- Why unresolved: While the authors argue the dataset is effective, 50 entries is statistically small for evaluating "reasoning-intensive" tasks, potentially leading to high variance in model performance assessment.
- What evidence would resolve it: A validation study using an expanded fault-tree dataset (e.g., N > 500) showing consistent model rankings compared to the original 50-entry benchmark.

## Limitations
- Domain specificity may limit generalizability to other industrial sectors
- Performance gaps may stem from both model limitations and evaluation data quality
- Open-source release status is unclear—raw data from proprietary manuals may not be included

## Confidence
- High Confidence: The benchmark effectively discriminates between models on domain-specific tasks
- Medium Confidence: Thinking mode's underperformance is attributed to knowledge deficiencies
- Low Confidence: The fault-tree task's scoring mechanism and exact curated corpus construction details are underspecified

## Next Checks
1. Reconstruct the curated corpus by implementing the 5-model consensus + human scoring pipeline
2. Run an ablation study where correct aviation facts are injected via RAG into thinking mode prompts
3. Apply the same 7-task evaluation structure to a non-aviation industrial domain to test replicability of performance patterns