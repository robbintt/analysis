---
ver: rpa2
title: 'Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed
  and Inappropriate Contexts'
arxiv_id: '2509.04500'
source_url: https://arxiv.org/abs/2509.04500
tags:
- context
- information
- answer
- inappropriate
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses how large language models (LLMs) process mixed
  context containing both relevant and inappropriate information. Drawing on the Rescorla-Wagner
  model from neuroscience, the authors adapt it to explain and quantify how competing
  contextual signals influence LLM outputs, revealing that models are disproportionately
  influenced by less prevalent context, especially inappropriate content.
---

# Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts

## Quick Facts
- arXiv ID: 2509.04500
- Source URL: https://arxiv.org/abs/2509.04500
- Reference count: 40
- Primary result: RW-Steering improves LLM response quality by up to 39.8% under mixed appropriate/inappropriate contexts

## Executive Summary
This paper addresses how large language models process mixed context containing both relevant and inappropriate information. Drawing on the Rescorla-Wagner model from neuroscience, the authors adapt it to explain and quantify how competing contextual signals influence LLM outputs, revealing that models are disproportionately influenced by less prevalent context, especially inappropriate content. To counter this vulnerability, they propose RW-Steering, a two-stage fine-tuning approach that enables models to internally detect and discount inappropriate signals without requiring extensive supervision. Experiments on their Poisoned Context Testbed show RW-Steering improves response quality by up to 39.8% and reverses undesirable behavior curves, outperforming alignment fine-tuning and context filtering, especially under disproportionate context mixtures.

## Method Summary
The authors propose RW-Steering, a two-stage fine-tuning approach that enables LLMs to internally identify and ignore inappropriate signals in mixed contexts. The method combines joint reasoning-generation training with structured rationales (where models explicitly verbalize context appropriateness judgments before generating answers) and targeted exposure to low-contamination contexts. This approach is evaluated on a Poisoned Context Testbed containing 7.6K queries with 45.3K context segments across four categories of inappropriate content. The framework is tested against baselines including alignment fine-tuning and context filtering, demonstrating superior generalization across varying proportions of inappropriate content.

## Key Results
- RW-Steering improves response quality by up to 39.8% compared to alignment fine-tuning and context filtering
- The method successfully reverses the undesirable behavior curve where models are disproportionately influenced by less prevalent inappropriate context
- RW-Steering generalizes robustly across varying proportions of inappropriate content without requiring extensive supervision across all possible mixtures
- The approach outperforms baselines particularly under disproportionate context mixtures (high contamination ratios)

## Why This Works (Mechanism)

### Mechanism 1: Inverse Prevalence Amplification via Rescorla-Wagner Dynamics
LLMs disproportionately amplify context signals that are less prevalent in the input, making them vulnerable to small amounts of inappropriate content. The adapted Rescorla-Wagner model shows that the change in association strength for context type i is inversely related to its current dominance, causing rare inappropriate content to have outsized influence. This creates a nonlinear vulnerability curve where inappropriate signals grow rapidly per unit encountered.

### Mechanism 2: Joint Reasoning-Generation Training with Structured Rationales
Training models to explicitly verbalize context appropriateness judgments before generating answers internalizes the ability to discount inappropriate signals. The model is trained autoregressively to generate structured targets containing explicit judgments and answers, forcing it to reason about context reliability before answering. This couples detection and rejection in a single forward pass.

### Mechanism 3: Targeted Exposure to Low-Contamination Contexts for Residual Bias Correction
Supplementing training with examples where appropriate information is dominant counteracts residual contextual bias from imperfect filtering. By systematically varying positions and total context length in training data with fewer than K inappropriate segments, the model learns to maintain robustness when minor contamination is present, preventing the sharp initial drop in the RW behavior curve.

## Foundational Learning

- **Concept: Classical Conditioning and Rescorla-Wagner Model**
  - Why needed here: Understanding how the adapted RW model formalizes LLM context prioritization requires grasping the original neuroscience theory of associative learning and prediction error
  - Quick check question: Can you explain why, in the original Rescorla-Wagner model, a conditioned stimulus that already has high associative strength elicits less learning from a new pairing than a novel stimulus?

- **Concept: In-Context Learning (ICL) and Context Sensitivity in LLMs**
  - Why needed here: The paper builds on the observation that LLMs integrate external context into generation, but the mechanisms for prioritization under conflicting signals are not well understood
  - Quick check question: How does the "needle-in-a-haystack" problem differ from the problem of prioritization under mixed appropriate/inappropriate contexts?

- **Concept: Alignment Fine-Tuning vs. Representation Steering**
  - Why needed here: The paper positions RW-Steering against baselines like alignment fine-tuning and context filtering; understanding their tradeoffs is critical for architectural decisions
  - Quick check question: Why might alignment fine-tuning on a fixed contamination ratio fail to generalize to disproportionate mixtures at inference?

## Architecture Onboarding

- **Component map:** Poisoned Context Testbed (7.6K queries, 45.3K context segments) -> Adapted RW Model (behavioral prediction) -> RW-Steering Training Pipeline (joint training + low-contamination exposure) -> Evaluation Metrics (Consistency + Cleanliness)

- **Critical path:**
  1. Construct or select a mixed-context testbed with labeled appropriate/inappropriate segments
  2. Implement structured response template and generate training targets with explicit appropriateness judgments
  3. Fine-tune model autoregressively on unified loss with low-contamination examples (K ≤ 3 inappropriate segments)
  4. Evaluate across contamination ratios (0%–95%) to verify reversal of RW behavior curve

- **Design tradeoffs:**
  - Joint training vs. staged detection-then-generation: Joint training couples reasoning and generation but requires structured supervision; staged approaches are simpler but may suffer from detection-generation decoupling
  - Fixed K vs. adaptive contamination exposure: K=3 generalizes across tested models but may need tuning for higher-noise domains; training across all ratios improves robustness but reduces generalization
  - Context filtering vs. internal steering: Filtering is lightweight but brittle to imperfect detection; RW-Steering internalizes robustness but requires fine-tuning

- **Failure signatures:**
  - Alignment fine-tuning on fixed ratios: Performance degrades sharply when evaluated on contamination ratios not seen during training
  - Self-awareness training alone: Models detect inappropriate content but still incorporate it into outputs
  - Context filtering under high contamination: Performance improves but remains unstable due to residual inappropriate content triggering RW amplification

- **First 3 experiments:**
  1. Replicate RW behavior curve validation: Prompt base LLMs with mixed contexts at varying contamination ratios and plot Response Quality vs. inappropriate proportion
  2. Ablate RW-Steering components: Train variants (joint training only, low-contamination exposure only, full RW-Steering) and compare generalization across contamination ratios
  3. Test cross-category generalization: Train RW-Steering on one inappropriate category and evaluate on others to assess whether learned robustness transfers across content types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Rescorla-Wagner model fully capture the complexity of In-Context Learning (ICL) dynamics, or is it merely an illustrative approximation for behavioral patterns?
- Basis in paper: The Limitations section states that while the analogy between ICL and classical conditioning is illustrative, "the connection remains approximate," and the model is "inherently limited in its ability to capture the full complexity of ICL dynamics."
- Why unresolved: The paper demonstrates that the model fits observed behavioral curves but does not prove that LLMs actually perform associative learning via the specific update mechanism defined in the Rescorla-Wagner equation
- What evidence would resolve it: Mechanistic interpretability studies comparing actual LLM update steps against theoretical delta calculations

### Open Question 2
- Question: Can the RW-Steering framework be effectively extended to agentic LLM applications involving dynamic tool use?
- Basis in paper: The Conclusion section explicitly proposes that "in the future, this framework can be extended to agentic LLM applications, where similar strategies could help models dynamically monitor and adapt to contextual risks or inappropriate tool use."
- Why unresolved: Current experiments are limited to single-turn RAG contexts; agentic workflows involve multi-step reasoning and tool outputs requiring different "steering" signals
- What evidence would resolve it: Applying RW-Steering to agent benchmarks where tools return mixed reliable and erroneous outputs

### Open Question 3
- Question: How robust is RW-Steering when applied to larger LLM architectures (greater than 3B parameters) and closed-source models?
- Basis in paper: The Limitations section notes that "due to computational resource constraints, we evaluated our methods on a limited set of models" (specifically Phi-2, Qwen-2-1.5B, Gemma-2-2b, and Llama-3.2-1B)
- Why unresolved: It is unclear if the "undesired behavior curve" and efficacy of the fine-tuning intervention persist identically in models with significantly larger parameter counts
- What evidence would resolve it: Replicating experiments on larger open-source models or API-based models to verify if the 39.8% improvement metric scales with model size

### Open Question 4
- Question: How does RW-Steering perform when the distribution of inappropriate content is unknown and less structured than the synthetic mixtures in the Poisoned Context Testbed?
- Basis in paper: The Limitations section acknowledges that the constructed dataset "does not fully capture the intricacies of web-scale data" and that "the actual distribution and frequency of inappropriate content remain unknown and are not explicitly modeled"
- Why unresolved: The testbed uses controlled mixtures; real-world retrieval may feature adversarial bursts or subtle stylistic variations of "inappropriate" content that the current testbed misses
- What evidence would resolve it: Evaluating the model on a live, uncontrolled retrieval corpus rather than a curated testbed

## Limitations

- The adapted Rescorla-Wagner model's mapping from probability distributions to association strength remains approximate and requires empirical validation
- The K=3 threshold for low-contamination contexts lacks systematic validation across different domain complexities and contamination levels
- The approach's performance under continuous content streams and in production environments with real-time retrieval remains untested
- Generalizability beyond the four tested inappropriate categories (privacy, fake news, hate speech, non-factual) has not been thoroughly validated

## Confidence

**High Confidence:** The experimental methodology for evaluating Response Quality across contamination ratios is robust, with clear quantitative improvements (up to 39.8%) over baselines. The Poisoned Context Testbed provides a well-controlled environment for testing mixed-context scenarios.

**Medium Confidence:** The adapted Rescorla-Wagner model successfully predicts LLM behavior curves and explains the inverse-prevalence vulnerability, but the theoretical connection between classical conditioning dynamics and LLM context processing remains approximate rather than proven. The joint training mechanism's ability to internalize robustness without extensive supervision is supported by results but could benefit from deeper mechanistic analysis.

**Low Confidence:** The generalizability of K=3 across domains and the cross-category transfer of RW-Steering's learned robustness are asserted but not thoroughly validated. The approach's performance under continuous content streams and in production environments with real-time retrieval remains untested.

## Next Checks

1. **Vi Normalization and γ Coefficient Validation:** Conduct ablation experiments that systematically vary the Vi normalization procedure and measure γ coefficient behavior across different context lengths and content types to empirically test the adapted RW model's predictions.

2. **Cross-Domain Contamination Testing:** Evaluate RW-Steering on domains beyond the four tested categories using real-world retrieval-augmented datasets to measure performance degradation when inappropriate content types significantly differ from training examples.

3. **Continuous Content Stream Evaluation:** Adapt the experimental framework to simulate continuous content streams where mixed contexts arrive incrementally rather than as discrete segments to test whether RW-Steering maintains robustness in production deployment scenarios.