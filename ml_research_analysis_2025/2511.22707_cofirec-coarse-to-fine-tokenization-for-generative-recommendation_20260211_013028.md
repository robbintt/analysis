---
ver: rpa2
title: 'CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation'
arxiv_id: '2511.22707'
source_url: https://arxiv.org/abs/2511.22707
tags:
- item
- semantic
- recommendation
- cofirec
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CoFiRec introduces a coarse-to-fine tokenization framework for\
  \ generative recommendation, addressing the limitations of existing methods that\
  \ flatten item attributes into a single embedding. The method decomposes item metadata\
  \ into multiple semantic levels\u2014from high-level categories to fine-grained\
  \ descriptions and collaborative filtering signals\u2014and tokenizes each level\
  \ independently using coarse-to-fine vector quantization."
---

# CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation

## Quick Facts
- arXiv ID: 2511.22707
- Source URL: https://arxiv.org/abs/2511.22707
- Authors: Tianxin Wei; Xuying Ning; Xuxing Chen; Ruizhong Qiu; Yupeng Hou; Yan Xie; Shuang Yang; Zhigang Hua; Jingrui He
- Reference count: 19
- Outperforms existing methods by an average of 7.27% across datasets

## Executive Summary
CoFiRec introduces a coarse-to-fine tokenization framework for generative recommendation that addresses the limitations of existing methods which flatten item attributes into single embeddings. The method decomposes item metadata into multiple semantic levels—from high-level categories to fine-grained descriptions and collaborative filtering signals—and tokenizes each level independently using coarse-to-fine vector quantization. During autoregressive decoding, the model generates tokens progressively from coarse to fine, reflecting the natural refinement process of user intent. Experiments on multiple public benchmarks show that CoFiRec outperforms existing methods by an average of 7.27% across datasets, with theoretical analysis proving that structured tokenization leads to lower dissimilarity between generated and ground truth items.

## Method Summary
CoFiRec operates through a two-stage training process. First, item metadata is decomposed into K semantic levels (category → title → description → CF signals in experiments), with each level independently tokenized using separate vector quantization modules. A shared encoder extracts embeddings for semantic levels while CF signals use a dedicated encoder. The quantized tokens are then embedded with level and position information and fed to a fine-tuned autoregressive decoder (T5 or LLaMA variants) for next-item prediction. During inference, the model generates tokens sequentially from coarse to fine levels using beam search, progressively refining recommendations from high-level category to fine-grained behavioral signals.

## Key Results
- Outperforms existing generative methods by an average of 7.27% across multiple public benchmarks
- Shows 10% improvement over TIGER even under cold-start scenarios
- Medium codebook configuration (256-256-512-512) achieves optimal performance
- Theoretical analysis proves hierarchical decoding reduces expected dissimilarity between predicted and ground-truth items

## Why This Works (Mechanism)

### Mechanism 1: Structured Tokenization Preserves Semantic Hierarchy
Decomposing item metadata into multiple semantic levels and tokenizing each independently preserves hierarchical structure that flat tokenization destroys. Instead of compressing all attributes into a single latent space, CoFiRec maintains separate codebooks for each semantic level, preventing semantic mixing across granularity levels. This assumes item metadata naturally exhibits coarse-to-fine semantic structure meaningful for recommendation.

### Mechanism 2: Hierarchical Decoding Reduces Expected Dissimilarity
Generating tokens sequentially from coarse to fine yields lower expected dissimilarity between predicted and ground-truth items compared to independent flat prediction. Theoretical analysis models this as traversal over a V-ary tree where correct prediction at level k constrains the remaining search space to one subtree. Even with same per-token accuracy p, hierarchical structure reduces expected error distance.

### Mechanism 3: Collaborative Filtering as Fine-Grained Disambiguation
Placing CF signals as the final token provides behavioral grounding that disambiguates items with similar textual semantics. Items with nearly identical category/title/description may have different interaction patterns. CF embeddings from pre-trained models capture co-occurrence signals, quantized into a dedicated codebook. This token is generated last, conditioned on all prior semantic tokens.

## Foundational Learning

- **Vector Quantization (VQ-VAE style)**: CoFiRec's tokenizers quantize continuous embeddings into discrete tokens using learnable codebooks with nearest-neighbor assignment and straight-through gradient estimation. Quick check: Can you explain how the commitment loss (||sg[h] - c||² + β||h - sg[c]||²) stabilizes codebook learning?

- **Autoregressive Language Modeling**: The downstream recommender is trained to predict token sequences autoregressively over the coarse-to-fine token vocabulary. Quick check: How does masking in decoder-only transformers enable causal sequence modeling for generation?

- **Collaborative Filtering Basics**: CoFiRec retrieves CF embeddings from pre-trained sequential recommenders (SASRec) to capture user-item interaction patterns as the finest token granularity. Quick check: Why might CF signals complement textual semantics in scenarios where similar products have different user engagement patterns?

## Architecture Onboarding

- **Component map**: Category/Title/Description → Semantic Tokenizer (shared encoder + level-specific VQ) → Token IDs → Embedding Layer (token + level + position) → Autoregressive Decoder (T5/LLaMA) → Generated Token Sequence → CF Tokenizer (separate encoder + codebook) → Final Token

- **Critical path**: Stage 1 - Tokenizer Training: Train semantic and CF tokenizers jointly with reconstruction + commitment losses for 10,000 epochs. Stage 2 - Generation Training: Freeze tokenizers, fine-tune downstream LLM with ranking-guided generation loss. Inference: Given user history, generate next item's tokens sequentially from level 1 to K via beam search.

- **Design tradeoffs**: Codebook size allocation favors larger codebooks at finer levels (medium config: [256, 256, 512, 512] works best); excessive size may over-partition. Hierarchy depth vs. data availability requires sufficient metadata fields; GPT-generated hierarchies can substitute but with slight performance drop. Tokenizer-training separation enables transferability but prevents joint optimization.

- **Failure signatures**: High code collision rate (>0.1%) indicating tokenizer over-compression; semantic prefixes that don't align with item categories; poor cold-start generalization when CF embeddings are unreliable; token order corruption causing 6-10% performance drops.

- **First 3 experiments**: 1) Baseline replication with TIGER on one dataset to establish baseline R@5/R@10 and verify 6-8% gains. 2) Ablation on token order with reversed and randomly permuted sequences expecting significant drops (>5% on R@5). 3) Codebook sensitivity analysis varying level-specific sizes to find optimal configuration and plot code utilization rates.

## Open Questions the Paper Calls Out

- **Joint Optimization**: Can end-to-end joint optimization of the tokenizer and generative model improve accuracy compared to the current two-stage training approach? Currently trained separately for transferability, but fully end-to-end optimization could enable stronger mutual adaptation.

- **Multimodal Extension**: How can the coarse-to-fine tokenization framework be extended to incorporate multimodal signals such as product images and videos? Present framework operates on textual metadata, but rich multimodal signals in real-world platforms offer natural next step.

- **Industrial Scale Performance**: Does CoFiRec maintain performance advantages when deployed at industrial scale with millions or billions of items? Current evaluation on small public benchmarks (9K-20K items) doesn't validate under extreme scale conditions.

- **Efficient Hierarchy Construction**: Can non-LLM-based methods achieve comparable automatic hierarchy construction quality to GPT-4o when structured metadata is unavailable? GPT-4o-based AHC achieves competitive results but cost limits practical deployment.

## Limitations

- Theoretical proof assumes balanced V-ary tree with uniform probability distribution, which may not hold in real recommendation datasets with unbalanced category sizes
- Limited evaluation to small public benchmarks (9K-20K items) without validation at industrial scale
- Two-stage training approach may not be optimal compared to end-to-end joint optimization

## Confidence

**High Confidence**: CoFiRec outperforms existing generative methods (TIGER, CoTT, TIP) by 6-8% on public benchmarks; coarse-to-fine token order is critical for performance (random/reversed orders degrade results by 6-10%); medium codebook configuration (256-256-512-512) performs best; hierarchical tokenization reduces code collision rates.

**Medium Confidence**: The theoretical proof of lower expected dissimilarity under hierarchical decoding; semantic hierarchy preservation is meaningful for recommendation (categories → titles → descriptions is natural); CF tokens provide orthogonal disambiguation beyond textual semantics; two-stage training is better than end-to-end optimization.

**Low Confidence**: Generalizability to domains with different metadata structures (e.g., images, audio, structured tables); performance under extreme sparsity or different cold-start definitions; robustness to corrupted or missing metadata fields; transferability of tokenizers across datasets without fine-tuning.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply CoFiRec to a non-Amazon dataset (e.g., MovieLens with genre/plot/cast metadata, or a multimodal dataset with images+text) to verify the coarse-to-fine paradigm extends beyond e-commerce. Key metrics: whether imposed hierarchy matches natural data structure, relative performance gain over baselines, codebook size sensitivity patterns.

2. **Theoretical Assumption Relaxation**: Design synthetic experiments that systematically violate the theoretical assumptions (unbalanced category sizes, non-hierarchical ground truth, noisy per-level predictions). Measure actual vs. predicted hierarchical advantage to validate whether proof insights transfer to realistic conditions.

3. **End-to-End Training Comparison**: Implement an end-to-end version where tokenizers are jointly trained with the generative model rather than in two stages. Compare final recommendation performance and training efficiency to test whether current two-stage design is optimal or simply convenient.