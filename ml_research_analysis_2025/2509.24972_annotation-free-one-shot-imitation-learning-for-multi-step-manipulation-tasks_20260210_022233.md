---
ver: rpa2
title: Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks
arxiv_id: '2509.24972'
source_url: https://arxiv.org/abs/2509.24972
tags:
- tasks
- visual
- target
- arxiv
- gripper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling robots to learn
  complex multi-step manipulation tasks from a single human demonstration, without
  requiring additional data collection, model training, or manual annotation. The
  authors propose a novel trajectory-replay-based imitation learning approach that
  decomposes a demonstration into atomic subtasks, aligns the robot to each subtask's
  keyframe using pre-trained vision-language models, and then executes the corresponding
  action sequence.
---

# Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks

## Quick Facts
- arXiv ID: 2509.24972
- Source URL: https://arxiv.org/abs/2509.24972
- Reference count: 27
- Primary result: 82.5% success rate on multi-step tasks, 90% on single-step tasks without manual annotation

## Executive Summary
This paper presents an annotation-free one-shot imitation learning method for multi-step manipulation tasks that decomposes a single human demonstration into atomic subtasks using pre-trained vision-language models. The approach eliminates the need for manual segmentation or additional training by leveraging Gemini 2.5 Pro to identify subtask boundaries and keyframes, then uses spatial perception encoders for visual alignment through keypoint matching. The method achieves competitive performance on both multi-step and single-step manipulation tasks while maintaining computational efficiency through pre-trained models.

## Method Summary
The method processes a single demonstration trajectory containing synchronized RGB-D images, end-effector poses, and gripper states. Gemini 2.5 Pro first decomposes the continuous demonstration into atomic subtasks by analyzing gripper context and visual frames, then selects keyframes where visual targets are fully visible. For each subtask, the system extracts 3D keypoints from segmented visual targets, matches dense features between demonstration and current views using a spatial perception encoder, and estimates relative camera pose via generalized ICP with RANSAC. The robot executes stored relative poses in open-loop fashion once alignment converges.

## Key Results
- Achieves 82.5% average success rate on multi-step manipulation tasks
- Achieves 90% success rate on single-step manipulation tasks
- Demonstrates computational efficiency with 0.351s processing time using Spatial Perception Encoder
- Outperforms or matches baselines that require manual annotations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A vision-language model can decompose a continuous demonstration trajectory into discrete atomic subtasks by jointly reasoning over visual frames and gripper sensor data.
- **Mechanism:** The VLM ingests downsampled video (1 fps) alongside gripper context and outputs structured boundaries for alignment and execution phases plus natural language descriptions.
- **Core assumption:** The VLM can reliably infer task structure from kinesthetic cues and visual context without domain-specific training.
- **Evidence anchors:** The VLM input includes gripper context and outputs subtask boundaries; failure modes indicate decomposition is not the bottleneck; related work requires different architectural commitments.
- **Break condition:** Tasks with ambiguous phase transitions or where gripper state is uninformative may yield inconsistent decompositions.

### Mechanism 2
- **Claim:** Selecting a keyframe where the visual target's silhouette is fully visible enables robust correspondence matching for pose estimation.
- **Mechanism:** The VLM selects a single timestamp where the target object is unoccluded and within frame margins to serve as the canonical reference image.
- **Core assumption:** A single well-chosen keyframe is sufficient as a visual reference; intermediate frames add no critical information.
- **Evidence anchors:** The prompt explicitly requests frames with clear silhouettes; convergence iterations vary by object suggesting keyframe quality impacts alignment efficiency.
- **Break condition:** Highly deformable objects may lack stable silhouettes, breaking the "fully visible" assumption.

### Mechanism 3
- **Claim:** Dense feature matching combined with mask-sampled 3D keypoints enables accurate relative pose estimation without category-specific training.
- **Mechanism:** The source keyframe's visual target is segmented via VLM-prompted mask, then uniformly sampled into 3D keypoints. At inference, a pre-trained encoder extracts dense features from the current view; correspondences are matched, and SAM2 segments the target.
- **Core assumption:** The Perception Encoder provides semantically meaningful dense features that transfer across the demonstration-to-deployment gap without fine-tuning.
- **Evidence anchors:** The method achieves lowest or near-lowest translation/rotation error across subtasks compared to baselines; failure modes include RGB-D shadow artifacts, not feature quality per se.
- **Break condition:** Identical object instances in view or heavy occlusion will break correspondence uniqueness.

## Foundational Learning

- **Concept: SE(3) Transformations and Hand-Eye Calibration**
  - **Why needed here:** The method converts camera-frame alignment to end-effector commands via a fixed hand-eye transform.
  - **Quick check question:** Given a camera observing a scene, can you derive the end-effector motion required to center an object in frame?

- **Concept: Iterative Closest Point (ICP) with RANSAC**
  - **Why needed here:** The alignment function uses generalized ICP to estimate the camera pose change between source and target point clouds, with RANSAC for outlier rejection.
  - **Quick check question:** Why does ICP require an initial pose estimate, and how does RANSAC improve robustness?

- **Concept: Dense Visual Correspondence**
  - **Why needed here:** Matching features between demonstration and deployment views is the core perceptual operation; understanding feature extractors is critical for debugging alignment failures.
  - **Quick check question:** What makes a visual feature "semantic" versus "geometric," and which is more important for this task?

## Architecture Onboarding

- **Component map:** Demo → VLM Decompose → VLM Keyframe → Mask + Keypoint Extraction → Feature Matching → ICP Pose Estimation → Controller Execution
- **Critical path:** The VLM prompts and feature extractor quality are the highest-leverage components that determine overall system performance.
- **Design tradeoffs:** VLM choice significantly impacts performance; Perception Encoder offers fastest processing with competitive accuracy; uniform mask sampling is robust but potentially noisy.
- **Failure signatures:** Overshooting moves target out of view; shadow artifacts degrade point clouds; open-loop execution cannot recover from grasp failures; identical instances cannot be distinguished.
- **First 3 experiments:** 1) Reproduce single-step tasks to verify alignment module in isolation; 2) Ablate feature extractor by swapping Perception Encoder for alternatives; 3) Stress-test decomposition with ambiguous demonstrations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating VLMs for real-time task completion checking and error recovery mitigate failures caused by the open-loop execution phase?
- **Basis in paper:** [explicit] The conclusion states that since "execution is open-loop there is no error correction," and suggests "Integrating VLM modules to check task completion, re-route execution... would be an interesting direction."
- **Why unresolved:** The current system executes actions blindly; if an error like object slipping occurs, the robot cannot detect or correct it.
- **What evidence would resolve it:** A modified version of the system where a VLM monitors the gripper state and successfully retries failed grasps or adjusts trajectories mid-task.

### Open Question 2
- **Question:** Does replacing uniform mask sampling with semantic keypoints significantly reduce noise and improve alignment accuracy?
- **Basis in paper:** [explicit] The discussion notes that the current reliance on "uniform mask sampling... can introduce noise," and suggests that "Incorporating semantic keypoints provides an interesting direction for improvement."
- **Why unresolved:** Uniform sampling treats all surface points equally, potentially including noisy or less informative regions, whereas semantic keypoints might offer more robust correspondence.
- **What evidence would resolve it:** Comparative experiments showing lower translation/rotation errors and fewer alignment iterations using semantic keypoints versus the uniform sampling method.

### Open Question 3
- **Question:** Can incorporating 3D reconstruction methods resolve the system's inability to handle occlusion and generalize to unseen objects?
- **Basis in paper:** [explicit] The paper lists occlusion and lack of generalization to unseen objects as limitations, stating, "Future work in incorporating 3D reconstruction methods may address this and may also improve pose estimation."
- **Why unresolved:** The current method relies on direct point-cloud matching, which fails if the object structure differs significantly or is partially hidden.
- **What evidence would resolve it:** Demonstrations of the method successfully manipulating novel objects or occluded targets after integrating a 3D reconstruction module.

## Limitations
- **Hand-eye calibration dependency**: Requires accurate T_cam,ee transformation that must be calibrated for each robot setup
- **Open-loop execution vulnerability**: Cannot recover from execution errors like grasping failures or object slippage
- **VLM quality dependency**: Performance heavily relies on Gemini 2.5 Pro's ability to decompose tasks and select keyframes

## Confidence
- **High Confidence**: Trajectory replay execution mechanism, feature matching with ICP, success rate measurements
- **Medium Confidence**: VLM-based decomposition and keyframe selection, feature extractor performance comparisons
- **Low Confidence**: Generalization to arbitrary task domains, robustness to environmental variations, performance with alternative VLMs

## Next Checks
1. **Calibration sensitivity test**: Systematically vary hand-eye calibration accuracy and measure impact on task success rates
2. **VLM ablation study**: Compare performance using different VLMs (GPT-4 Turbo, Claude) for decomposition and keyframe selection
3. **Environmental robustness evaluation**: Test method performance under varying lighting conditions, object textures, and clutter levels