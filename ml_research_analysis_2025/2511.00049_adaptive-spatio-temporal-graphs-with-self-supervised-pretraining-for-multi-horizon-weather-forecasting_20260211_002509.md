---
ver: rpa2
title: Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon
  Weather Forecasting
arxiv_id: '2511.00049'
source_url: https://arxiv.org/abs/2511.00049
tags:
- weather
- forecasting
- data
- spatial
- spatio-temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate multi-horizon weather
  forecasting by proposing a spatio-temporal self-supervised learning framework. The
  core method combines a Graph Neural Network (GNN) for spatial reasoning, self-supervised
  pretraining for representation learning, and a spatio-temporal adaptation mechanism
  to enhance generalization across forecasting horizons.
---

# Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting

## Quick Facts
- arXiv ID: 2511.00049
- Source URL: https://arxiv.org/abs/2511.00049
- Reference count: 10
- Multi-horizon weather forecasting model outperforms traditional NWP and deep learning baselines with MAE of 1.88 (24h) to 2.80 (168h) on MERRA-2 and 1.85 (24h) to 2.75 (168h) on ERA5.

## Executive Summary
This paper addresses the challenge of accurate multi-horizon weather forecasting by proposing a spatio-temporal self-supervised learning framework. The core method combines a Graph Neural Network (GNN) for spatial reasoning, self-supervised pretraining for representation learning, and a spatio-temporal adaptation mechanism to enhance generalization across forecasting horizons. Extensive experiments on ERA5 and MERRA-2 datasets show that the proposed approach outperforms traditional numerical weather prediction (NWP) models and recent deep learning methods.

## Method Summary
The proposed framework integrates a Graph Neural Network (GNN) with self-supervised pretraining for multi-horizon weather forecasting. The model constructs a dynamic adjacency matrix based on spatial distance and feature correlation to capture non-local spatial dependencies. A self-supervised learning scheme uses contrastive and consistency losses to improve representation learning from unlabeled reanalysis data. The model incorporates a horizon-aware adaptation mechanism that dynamically weights loss terms based on the forecast horizon. Training uses historical weather data (temperature, wind, pressure) with a 7-day input window to predict 24-168 hour horizons.

## Key Results
- Achieves MAE of 1.88 (24h) to 2.80 (168h) on MERRA-2 dataset
- Achieves MAE of 1.85 (24h) to 2.75 (168h) on ERA5 dataset
- Outperforms baselines including ConvLSTM, FourCastNet, and ECMWF
- Demonstrates strong cross-dataset generalization and robustness in visual analyses of Beijing and Shanghai

## Why This Works (Mechanism)

### Mechanism 1: Graph-Based Spatial Reasoning
- **Claim:** Modeling geographical regions as nodes in a graph may capture non-local spatial dependencies more effectively than fixed-grid convolutions.
- **Mechanism:** The architecture constructs an adjacency matrix ($A_{ij}$) using a combination of spatial distance and feature correlation (Eq. 3.10). It then employs a Graph Neural Network (GNN) with attention-weighted message passing ($\alpha_{ij}$) to propagate meteorological states (e.g., wind, pressure) between regions.
- **Core assumption:** Weather dynamics at a specific location are significantly influenced by distant regions with high feature correlation, and these relationships can be approximated by a differentiable graph structure.
- **Evidence anchors:** [abstract] "integrates a graph neural network (GNN) for spatial reasoning." [section 3.4] "Each region is modeled as a node... adjacency matrix... updated to reflect both spatial distance and correlation." [corpus] DRAN and ASTGI papers emphasize relation-adaptive networks for non-stationary spatio-temporal data, supporting the move away from static grids.
- **Break condition:** If the graph construction parameters ($\sigma_{spatial}, \gamma$) are misconfigured, the graph may become fully connected (computational explosion) or disconnected (isolated nodes fail to receive upstream weather fronts).

### Mechanism 2: Spatio-Temporal Self-Supervision
- **Claim:** Pretraining on unlabeled data using contrastive and consistency objectives appears to improve the model's robustness and label efficiency.
- **Mechanism:** The model generates training signals by enforcing temporal consistency (smoothness between time steps $t$ and $t'$ within a window $\Delta T$) and contrastive discrimination (separating dissimilar weather patterns) via losses $L_{consistency}$ and $L_{contrastive}$.
- **Core assumption:** Useful representations of atmospheric states can be learned by enforcing temporal continuity and feature separation without explicit forecast targets.
- **Evidence anchors:** [abstract] "self-supervised pretraining scheme for representation learning." [section 3.2] Eq. 3.2 and 3.3 define the contrastive and consistency regularization terms. [corpus] PatchFormer utilizes hierarchical masked reconstruction, confirming that foundation model approaches (SSL) are effective for time-series forecasting.
- **Break condition:** If the temporal window $\Delta T$ is too large, the consistency loss may enforce over-smoothing, blurring sharp meteorological transitions like fronts or storms.

### Mechanism 3: Horizon-Aware Loss Adaptation
- **Claim:** Dynamically weighting loss terms based on the forecast horizon likely allows a single model to optimize for both immediate and long-term accuracy.
- **Mechanism:** The mechanism applies distinct weights ($w_{short}, w_{long}$) to the loss function. Short-term weights emphasize recent data, while long-term weights integrate broader trends (Eq. 3.5).
- **Core assumption:** The optimal feature importance and error gradients differ fundamentally between 24-hour and 168-hour forecasts, requiring adaptive weighting to prevent the model from overfitting to short-term noise.
- **Evidence anchors:** [abstract] "spatio-temporal adaptation mechanism to enhance generalization across varying forecasting horizons." [section 3.3] Eq. 3.5 and 3.8 define the time-based weight adjustment and the adapted total loss. [corpus] Neighbor papers on multi-horizon forecasting (e.g., PatchFormer) imply diverse strategies for handling varying time-scales, validating the need for this mechanism.
- **Break condition:** If the weighting functions are not differentiable or balanced correctly, the gradients for long-term losses may vanish, causing the model to ignore extended horizons.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs)**
  - **Why needed here:** To understand how the model replaces standard spatial convolutions with graph message passing to handle irregular or non-local spatial relationships.
  - **Quick check question:** How does the adjacency matrix $A_{ij}$ determine which nodes exchange information in a GNN layer?

- **Concept: Self-Supervised Learning (SSL)**
  - **Why needed here:** To grasp how the model trains on "unlabeled" reanalysis data by creating its own supervision signals (contrasts) rather than relying solely on ground truth labels.
  - **Quick check question:** What is the difference between a contrastive loss and a consistency loss in the context of time-series data?

- **Concept: Multi-Horizon Forecasting**
  - **Why needed here:** To contextualize why the model needs an adaptation mechanism to handle the varying difficulty of predicting 24 hours vs. 168 hours into the future.
  - **Quick check question:** Why does prediction error generally increase as the time horizon extends, and how might adaptive weighting mitigate this?

## Architecture Onboarding

- **Component map:** Historical weather data (Temperature, Wind, Pressure) -> Graph Construction -> GNN Module (Spatial dependencies) -> Temporal Adaptation Mechanism (Weighting) -> Self-Supervised Branch (Contrastive/Consistency Loss) + Supervised Branch

- **Critical path:** The calculation of the adjacency matrix $A_{ij}$ (Eq. 3.10) is the critical path. If this matrix does not capture meaningful spatial correlations (distance + feature similarity), the GNN message passing will fail to propagate relevant weather dynamics.

- **Design tradeoffs:**
  - **Static vs. Dynamic Graph:** The paper uses a self-adaptive graph structure (Eq 3.10), but the conclusion notes the "current graph structure is static," suggesting a tradeoff between the computational cost of dynamic updates and the flexibility of static inference.
  - **SSL Overhead:** Adding contrastive and consistency losses improves performance (Table 3) but increases training complexity and hyperparameter sensitivity ($\alpha, \beta$).

- **Failure signatures:**
  - **Over-smoothing:** Predictions look like a blurry average of historical data (failure of the consistency constraint or lack of contrastive power).
  - **Graph Disconnect:** Specific regions (e.g., coastal Shanghai) show high error if the graph edges fail to connect them to relevant meteorological drivers (e.g., ocean currents).
  - **Horizon Collapse:** Model performs well at 24h but degrades rapidly by 72h, indicating the adaptation mechanism ($w_{long}$) is not effectively weighting long-term trends.

- **First 3 experiments:**
  1. **Ablation on Graph Structure:** Implement the model with $A_{ij}=1$ (fully connected) vs. distance-only vs. the proposed correlation-distance hybrid to isolate the GNN's contribution.
  2. **SSL Component Analysis:** Train three variants: (1) Supervised only, (2) Supervised + Contrastive, (3) Supervised + Consistency to measure the specific impact of each SSL term on MAE.
  3. **Horizon Sensitivity:** Vary the time-based weights ($w_{short}, w_{long}$) to see if the optimal balance shifts depending on the specific weather variable (e.g., pressure vs. wind speed).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the static graph structure be evolved into a fully dynamic architecture to better capture rapidly changing atmospheric connectivity during extreme events?
- Basis in paper: [explicit] The Conclusion states, "the current graph structure is static," and lists "dynamic graph construction" as a specific area for future work.
- Why unresolved: The current adjacency matrix relies on fixed spatial distances and historical correlations, which may not accurately represent transient or non-stationary spatial dependencies during fast-evolving weather phenomena.
- What evidence would resolve it: A study comparing the current static GNN against a time-variant graph implementation (where edges update at each time step) specifically during high-variance weather events.

### Open Question 2
- Question: What specific architectural or loss function modifications are required to improve the model's sensitivity and accuracy for rare extreme weather events?
- Basis in paper: [explicit] The Conclusion notes that while performance is strong, "performance under rare extreme weather conditions remains to be further improved."
- Why unresolved: The current self-supervised framework likely optimizes for the dominant modes of variability in the reanalysis data, potentially smoothing out rare but critical extremes.
- What evidence would resolve it: Experiments utilizing weighted sampling or specialized loss functions (e.g., focal loss) that demonstrate improved ROC scores or reduced MAE specifically for outlier weather events.

### Open Question 3
- Question: To what extent does the integration of multi-modal data sources, such as satellite imagery, improve forecast skill beyond the current reliance on reanalysis data?
- Basis in paper: [explicit] The Conclusion proposes to "incorporate additional data sources such as satellite imagery" to extend the framework.
- Why unresolved: The current model is validated strictly on ERA5 and MERRA-2 reanalysis datasets, which assimilate observational data but lack the direct high-resolution visual and spectral information available from satellites.
- What evidence would resolve it: Comparative analysis of model performance when trained on reanalysis data alone versus a heterogeneous dataset combining station data with raw satellite radiances.

## Limitations
- The model's performance on regions beyond Beijing and Shanghai is untested, limiting claims of generalization.
- Key architectural details including exact grid resolution, bounding box coordinates, and GNN layer dimensions are underspecified.
- Computational efficiency claim of training on 12GB RAM requires verification given GNN memory demands.

## Confidence

**High Confidence:** The core claim that a graph-based spatial reasoning approach improves weather forecasting accuracy is supported by both the ablation study (Table 3) and the consistent MAE improvements across both datasets (MERRA-2 and ERA5).

**Medium Confidence:** The effectiveness of the spatio-temporal self-supervision is demonstrated through controlled ablation, but the specific contributions of contrastive vs. consistency losses are not disentangled.

**Low Confidence:** The claim of "robustness" and "generalization" is based on two specific locations (Beijing and Shanghai), and the computational efficiency claim (training on 12GB RAM) is questionable without verification.

## Next Checks

1. **Graph Structure Sensitivity:** Systematically vary the graph construction parameters (spatial distance threshold, feature correlation weight) and measure the resulting MAE on a held-out validation set.

2. **SSL Component Isolation:** Train separate models with only the supervised loss, only the contrastive loss, and only the consistency loss. Compare their MAE curves across all horizons.

3. **Cross-Climate Generalization:** Evaluate the trained model on a different geographical region (e.g., coastal vs. inland) or on a different reanalysis dataset (e.g., JRA-55) to test the claim of "strong cross-dataset generalization."