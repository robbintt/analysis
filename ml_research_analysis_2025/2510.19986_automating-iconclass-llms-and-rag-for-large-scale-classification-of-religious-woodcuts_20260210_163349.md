---
ver: rpa2
title: 'Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious
  Woodcuts'
arxiv_id: '2510.19986'
source_url: https://arxiv.org/abs/2510.19986
tags:
- search
- iconclass
- classification
- images
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel methodology for classifying early modern
  religious images by using Large Language Models (LLMs) and vector databases in combination
  with Retrieval-Augmented Generation (RAG). The approach leverages the full-page
  context of book illustrations from the Holy Roman Empire, allowing the LLM to generate
  detailed descriptions that incorporate both visual and textual elements.
---

# Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts

## Quick Facts
- arXiv ID: 2510.19986
- Source URL: https://arxiv.org/abs/2510.19986
- Reference count: 0
- Primary result: RAG-based method achieves 87% precision at 5 levels and 92% precision at 4 levels for Iconclass classification of religious woodcuts

## Executive Summary
This paper presents a novel approach to automatically classify early modern religious woodcuts using Large Language Models (LLMs) and vector databases with Retrieval-Augmented Generation (RAG). The method leverages full-page context from book illustrations, allowing the LLM to generate detailed descriptions that incorporate both visual elements and surrounding textual information. These descriptions are then matched to relevant Iconclass codes through a hybrid vector search approach, achieving significantly higher precision than traditional image and keyword-based searches.

## Method Summary
The methodology uses GPT-4o to generate textual descriptions from full-page scans of religious woodcuts, incorporating surrounding early modern German text (headings, chapter numbers, captions). These descriptions are embedded and searched against a Weaviate vector database containing approximately 12,600 Iconclass codes from "Religion and Magic" and "Bible" categories. The system employs hybrid search (BM25 + vector cosine similarity) to retrieve top-5 candidate codes, then uses RAG where the LLM selects the best match from these candidates. The approach outperforms traditional methods by utilizing full-page context and semantic reasoning capabilities.

## Key Results
- 87% precision at five levels of hierarchical classification
- 92% precision at four levels of hierarchical classification
- F1 score of 0.7429 for full-page contextual descriptions vs 0.6299 for cropped illustrations alone
- Outperforms traditional keyword search (F1 ~0.45) and image-based search methods

## Why This Works (Mechanism)

### Mechanism 1
Full-page context dramatically improves classification accuracy over cropped illustrations. The LLM processes both the woodcut image and surrounding early modern German text from the full book page, constraining the interpretation space and reducing ambiguity about which biblical scene is depicted. This works because the LLM can accurately read early modern blackletter type and the surrounding text is genuinely relevant to the illustration. The paper shows F1 scores improving from 0.6299 (cropped) to 0.7429 (full-page).

### Mechanism 2
RAG-based re-ranking from top-k results corrects initial retrieval errors. Vector search retrieves top-5 candidate Iconclass codes, then the LLM selects the best match using semantic reasoning. This two-stage process catches cases where the correct code wasn't ranked first but was still in the top 5. The paper notes cases where the correct classification ranked 6th, suggesting potential for retrieval window expansion.

### Mechanism 3
Hierarchical depth truncation trades recall for precision, improving practical utility. Iconclass codes are hierarchical, and the system over-predicts depth (average 6 levels). Truncating to 4 levels raises precision from 79% to 92% because higher levels are more reliable and still thematically useful. The paper notes that knowing an image depicts "the story of David and Goliath" is useful even if a more accurate classification would be "David slings a stone."

## Foundational Learning

- Concept: **Iconclass hierarchical classification**
  - Why needed here: Understanding that Iconclass is a tree-structured vocabulary where codes like "73D231" inherit meaning from all parent levels (7→73→73D→73D2→73D23→73D231). Partial matches at higher levels still carry semantic value.
  - Quick check question: If your system predicts "73D24" but ground truth is "73D23", how many levels match correctly?

- Concept: **Vector embeddings for semantic similarity**
  - Why needed here: Keyword search fails on synonyms ("Eucharist" vs "Holy Communion"), but vector embeddings capture semantic relationships, enabling matching between description language and Iconclass terminology.
  - Quick check question: Why would a vector search retrieve "Last Supper" results for a query containing "communion" when keyword search would not?

- Concept: **Hybrid search (lexical + semantic)**
  - Why needed here: Pure vector search can miss exact terminology matches; pure keyword search misses synonyms. Hybrid search combines BM25 (lexical) scores with vector similarity scores, balancing precision and recall.
  - Quick check question: If hierarchical Iconclass descriptions contain more keywords (e.g., "Noah" appears in hierarchical but not basic), which search type benefits more?

## Architecture Onboarding

- Component map:
  1. **Image input**: Full-page scan OR cropped illustration
  2. **GPT-4o multimodal**: Generates textual description (incorporating OCR of surrounding text for full-page)
  3. **Weaviate vector database**: Stores ~12,600 Iconclass codes (descriptions embedded via OpenAI text-embedding model)
  4. **Search layer**: BM25 (keyword) / Vector (cosine similarity) / Hybrid (combined) / RAG (retrieval + LLM selection)
  5. **Output**: Predicted Iconclass code, optionally truncated to specified depth

- Critical path: Image → GPT-4o description → Vector search (top-5) → RAG LLM selection → Depth truncation → Final code

- Design tradeoffs:
  - **Full-page vs cropped**: Full-page = higher accuracy but requires complete page scans; cropped = works on isolated images but lower F1 (~0.63 vs ~0.74)
  - **Basic vs hierarchical database**: Hierarchical = better for keyword/hybrid search (more terms); Basic = better for pure vector search (less noise)
  - **Precision vs recall via truncation**: 4 levels = 92% precision but lower recall; 6 levels = lower precision but captures finer details

- Failure signatures:
  - **Extra matches** (code over-specified): Model predicts deeper than ground truth, often with incorrect details. Signature: high-level matches correct, lowest 1-2 levels wrong.
  - **Multi-scene images**: Composite illustrations (e.g., Adam & Eve at tree PLUS expulsion) produce ambiguous descriptions; single-code constraint causes mismatches.
  - **Reused woodcuts**: Same image appearing in different biblical contexts; full-page context helps but conflicting training signals may emerge.

- First 3 experiments:
  1. **Reproduce baseline comparison**: Run cropped illustration images through keyword-only search on basic database. Confirm ~0.45 F1 before advancing to full pipeline.
  2. **Ablate full-page context**: Compare identical images processed as full-page vs cropped. Measure F1 delta (expected ~0.11 improvement per paper).
  3. **Tune RAG retrieval depth**: Test top-3, top-5, top-10 retrieval. Check if top-10 recovers cases where correct code ranked 6th (per Section 9's observation). Monitor precision/recall tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
Would fine-tuning an LLM specifically for Iconclass classification improve precision over the zero-shot approach tested here? The authors achieved strong results (87-92% precision) without fine-tuning but did not compare against a fine-tuned model due to resource constraints and the goal of demonstrating a practical, no-training-required approach.

### Open Question 2
Would expanding RAG retrieval from the top 5 to the top 10 results improve classification accuracy? The paper acknowledges cases where the correct answer ranked 6th but limited experimentation to top-5 due to computational expense.

### Open Question 3
How well does this methodology transfer to secular or non-religious early modern illustrations? The test corpus was limited to religious/biblical images, and the paper notes that LLMs may have stronger training data coverage for religious iconography than for secular subjects.

### Open Question 4
Can the system be adapted to assign multiple Iconclass codes to complex, multi-scene illustrations? The current architecture retrieves and selects a single best match, while many early modern illustrations contain multiple layers of symbolism.

## Limitations

- Dataset Scope and Bias: Test set consists entirely of Luther Bibles from the Holy Roman Empire using a specific Iconclass subset; performance on other religious traditions, artistic styles, or time periods remains unknown.
- Prompt Sensitivity: Exact GPT-4o prompts used for description generation and RAG selection are not disclosed, creating significant reproducibility uncertainty.
- Hybrid Search Parameters: The weighting parameter (alpha) used to combine BM25 and vector scores in the hybrid search is not specified.

## Confidence

- **High Confidence**: The core claim that full-page context significantly improves classification accuracy over cropped illustrations is well-supported by direct comparison data (F1 scores of 0.7429 vs 0.6299).
- **Medium Confidence**: The effectiveness of RAG-based re-ranking from top-5 results is demonstrated, but the paper acknowledges boundary cases where correct codes ranked 6th, suggesting the optimal retrieval depth may vary.
- **Medium Confidence**: The hierarchical depth truncation strategy is logically sound and supported by precision gains, but the claim that broader classifications retain sufficient research value is not empirically validated across diverse research questions.

## Next Checks

1. **Reproduce Baseline Comparison**: Run cropped illustration images through keyword-only search on the basic database. Confirm achieving approximately 0.45 F1 before advancing to the full pipeline to establish a valid baseline.

2. **Ablate Full-Page Context**: Process identical images as both full-page and cropped variants. Measure the F1 score difference (expected improvement of ~0.11) to validate the contextual advantage mechanism.

3. **Tune RAG Retrieval Depth**: Test top-3, top-5, and top-10 retrieval configurations. Check if top-10 recovers cases where correct codes ranked 6th (as observed in Section 9) and evaluate the precision/recall tradeoff at different depths.