---
ver: rpa2
title: Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games
arxiv_id: '2602.00606'
source_url: https://arxiv.org/abs/2602.00606
tags:
- games
- stochastic
- dynamics
- learning
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new independent and payoff-based learning
  framework for stochastic games that is model-free, game-agnostic, and gradient-free.
  The key idea is an actor-dual-critic architecture where agents update their strategies
  using feedback from two distinct critics: a fast critic that responds rapidly to
  observed payoffs, and a slow critic that approximates long-term values.'
---

# Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games

## Quick Facts
- **arXiv ID**: 2602.00606
- **Source URL**: https://arxiv.org/abs/2602.00606
- **Reference count**: 10
- **Primary result**: Three-timescale actor-dual-critic framework with theoretical convergence guarantees to approximate Nash equilibria in both zero-sum and identical-interest stochastic games using only payoff feedback

## Executive Summary
This paper introduces a novel independent learning framework for stochastic games that uses an actor-dual-critic architecture with three-timescale updates. The key innovation is separating short-term reactions (fast critic) from long-term stabilization (slow critic) while maintaining game-agnostic and model-free properties. The method achieves convergence to approximate Nash equilibria in both zero-sum and identical-interest games, with approximation error proportional to exploration rate ε. This represents one of the first payoff-based, fully decentralized learning algorithms with theoretical guarantees in both game classes.

## Method Summary
The Actor-Dual-Critic framework operates with three components updating at different rates: a fast critic that estimates action-values q_i^k(s,a_i) using recent observations and slow critic values, an actor that updates policies π_i^k(s) via ε-best response to current q-values, and a slow critic that computes long-term value estimates v_i^k(s) by averaging q-values under the current policy. The three-timescale separation (fast critic: λ_k = (k+1)^(-ρλ), actor: α_k = (k+1)^(-ρα), slow critic: β_k = (k+1)^(-1)) with 1/2 < ρλ < ρα < 1 ensures that value estimates converge while policies respond to quasi-stationary estimates. ε-greedy exploration is reformulated as playing an "effective" stochastic game with perturbed rewards and transitions, bounding approximation error by ε.

## Key Results
- Establishes convergence to approximate Nash equilibria in two-agent zero-sum stochastic games with Nash gap decaying to zero almost surely
- Extends convergence results to multi-agent identical-interest stochastic games using quasi-monotonicity techniques despite lack of contraction property
- Empirical validation shows Nash gap decaying below ε-threshold across both game classes
- Proves approximation error bound ε ∝ ε/(1-γ)² · max|r^i(s,a)| for the exploration-perturbed strategies

## Why This Works (Mechanism)

### Mechanism 1
Three-timescale separation breaks cyclic dependency between value estimation and policy updates. Fast critic tracks global Q-functions while policies respond to quasi-stationary estimates, and value estimates converge to equilibrium payoffs. Core assumption requires 1/2 < ρλ < ρα < 1. Evidence: Section 4.2 shows quasi-stationarity enables q-value tracking while slow critic convergence provides stable targets. Break condition: Improper step size ordering collapses quasi-stationarity guarantees.

### Mechanism 2
ε-greedy exploration reformulated as pure best-response in "effective" stochastic game bounds approximation error by ε. Exploration kernel transforms original game M into M_ε where agents playing exploration-perturbed strategies in M correspond to exploration-free strategies in M_ε. Core assumption requires reachability (p(s'|s,a) > 0 ∀ transitions). Evidence: Proposition 1 shows if NG_ε(μ) = 0 then NG(π) ≤ ε where ε ∝ ε/(1-γ)². Break condition: Large ε or violated reachability prevent meaningful approximation.

### Mechanism 3
Quasi-monotonicity of global Q-function estimates ensures convergence in identical-interest games despite lack of contraction. Shows lim inf_{k→∞} inf_{k2≥k1≥k} Σ Q_i^{k2}(s,a) - Q_i^{k1}(s,a) ≥ 0, meaning future Q-values are asymptotically never smaller than past minima. Core assumption requires ρα ≥ 3/2·ρλ and ρα + ρλ > 3/2. Evidence: Section 5.2 proves error terms decay fast enough for quasi-monotonicity to dominate. Break condition: Violated assumption allows error terms to dominate and break quasi-monotonicity.

## Foundational Learning

- **Stochastic Games (Markov Games)**: Core environment model where agents interact over infinite horizon with state transitions dependent on joint actions. Why needed: Understanding distinction from single-agent MDPs and repeated matrix games is essential. Quick check: Can you explain why value iteration may not contract in identical-interest stochastic games but does contract in zero-sum games?

- **Actor-Critic Architecture**: Dual-critic design extends standard actor-critic by separating fast reactive estimation from slow deliberative value approximation. Why needed: The architecture is fundamental to achieving three-timescale separation. Quick check: What is the role of the critic in standard actor-critic, and how does adding a second "slow" critic change the learning dynamics?

- **Stochastic Approximation / ODE Method**: Convergence proofs rely on timescale separation arguments from stochastic approximation theory. Why needed: Understanding why step sizes must be square-summable yet not summable is critical. Quick check: Given step sizes λ_k = (k+1)^(-ρλ) and β_k = (k+1)^(-1), what condition on ρλ ensures β_k ∈ o(λ_k)?

## Architecture Onboarding

- **Component map**: Initialize q_i^0 = v_i^0 = 0, π_i^0 = Uniform(A_i). At each stage k: observe state s_k → update q_i^k for visited (s_{k-1}, a_i^{k-1}) → update π_i^k via ε-best response → update v_i^k for all states → execute action a_i^k ~ π_i^k(s_k).

- **Fast Critic**: Maintains q_i^k(s,a_i) via Eq. (4), updated asynchronously using observed rewards r_i^{k-1}, next state s_k, and slow critic's value estimate v_i^{k-1}(s_k). Normalizes by action probability π_i^{k-1}(a_i|s).

- **Actor**: Maintains π_i^k(s) via Eq. (6), updates as convex combination toward ε-best response to current q-values.

- **Slow Critic**: Maintains v_i^k(s) via Eq. (10), computes expected q-value under actor policy: π_i^{k-1}(s)^T q_i^{k-1}(s,·).

- **Exploration Kernel**: E_i in Eq. (5) defines ε-greedy perturbation with minimum probability ε/|A_i| for each action.

- **Design tradeoffs**:
  - **ε selection**: Larger ε improves exploration but increases approximation bound ε ∝ ε/(1-γ)². Paper uses ε = 0.002 in experiments.
  - **Step size exponents**: ρλ closer to 0.5 gives faster q-learning but risks instability; ρα too close to ρλ weakens timescale separation.
  - **Reachability assumption**: Strictly positive transitions (Assumption 2) simplifies analysis but may not hold in practice.

- **Failure signatures**:
  - **Oscillating Nash gap**: Indicates step size decay too fast (λ_k decay rate too high) or insufficient exploration.
  - **Divergent q-values**: Check that normalization by π_i^{k-1}(a_i|s) in Eq. (4) doesn't create numerical instability when probabilities become very small.
  - **Non-convergence in identical-interest games**: Verify Assumption 3 holds; if ρα + ρλ ≤ 1.5, error terms don't decay sufficiently.

- **First 3 experiments**:
  1. **Sanity check on 2-state 2-action zero-sum game**: Set γ = 0.8, ε = 0.01, λ_k = (k+1)^(-0.6), α_k = (k+1)^(-0.9), β_k = (k+1)^(-1). Run 10⁵ steps and verify Nash gap decays below ε-threshold. Compare computed equilibrium to analytical solution if available.
  2. **Step size sensitivity test**: Run identical-interest game (3 agents, 3 states, 3 actions each as in Section 6) with varying ρλ ∈ {0.55, 0.6, 0.7} while fixing ρα = 0.95. Plot Nash gap decay rates and identify threshold where convergence fails.
  3. **Ablation on exploration rate**: Test ε ∈ {0.001, 0.01, 0.05, 0.1} and measure final Nash gap. Verify relationship ε ∝ ε holds empirically by plotting Nash gap vs. ε/(1-γ)².

## Open Questions the Paper Calls Out
None

## Limitations
- Convergence guarantees rely on strict positivity assumptions (p(s'|s,a) > 0) that may not hold in practical environments
- Approximation error bound ε is proportional to exploration rate, creating tension between exploration quality and solution accuracy
- Three-timescale separation requires careful tuning of step size exponents with stricter constraints for identical-interest case (ρα ≥ 3/2·ρλ, ρα + ρλ > 3/2)

## Confidence

- **High Confidence**: Zero-sum game convergence under proper step size conditions (established by Shapley contraction and stochastic approximation theory)
- **Medium Confidence**: Identical-interest game convergence via quasi-monotonicity (novel extension requiring careful verification of error term bounds)
- **Medium Confidence**: Empirical validation across game classes (shown but limited to small-scale problems with 3 states/actions)

## Next Checks

1. **Robustness to Step Size Violations**: Test the algorithm with step size exponents that violate Assumption 3 (e.g., ρα + ρλ = 1.4) in identical-interest games to empirically verify the threshold where quasi-monotonicity breaks down.

2. **State Space Scaling**: Evaluate performance on stochastic games with larger state spaces (e.g., 10+ states) to assess whether the dual-critic architecture maintains convergence properties as Bellman error propagation becomes more complex.

3. **Reachability Relaxation**: Implement experiments with non-strictly positive transitions (some p(s'|s,a) = 0) to test whether the algorithm still converges in practice despite theoretical violations of Assumption 2.