---
ver: rpa2
title: 'TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization
  For Dummies'
arxiv_id: '2511.23225'
source_url: https://arxiv.org/abs/2511.23225
tags:
- tweo
- outliers
- uni00a0out
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extreme activation outliers
  that hinder FP8 training and quantization of Transformers. The authors propose TWEO
  (Transformers Without Extreme Outliers), a non-invasive loss function that directly
  penalizes extreme activation magnitudes.
---

# TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies

## Quick Facts
- arXiv ID: 2511.23225
- Source URL: https://arxiv.org/abs/2511.23225
- Authors: Guang Liang; Jie Shao; Ningyuan Tang; Xinyao Liu; Jianxin Wu
- Reference count: 40
- Primary result: Enables stable FP8 training and hardware-friendly W8A8 quantization of Transformers by eliminating extreme activation outliers

## Executive Summary
This paper addresses the fundamental challenge of extreme activation outliers that prevent efficient low-precision training and quantization of Transformers. TWEO introduces a simple yet effective loss function that directly penalizes extreme activation magnitudes without requiring architectural modifications. By eliminating outliers that exceed 1000 in magnitude, TWEO enables stable FP8 pre-training with performance comparable to BF16 while achieving 36% higher training throughput. The method also enables hardware-friendly W8A8 per-tensor static quantization of large language models for the first time, achieving state-of-the-art results.

## Method Summary
TWEO adds a scaled L4 penalty to the output activations of every transformer block, calculated as (1/L) × Σ E[(|A^(l)|/(τ+ε))^p] where τ=3 and p=4. This non-invasive loss term suppresses extreme outliers while tolerating normal activations. The method is applied during training from scratch and works across vision and language models without architectural changes. For FP8 training, NVIDIA's Transformer Engine is used with aggressive per-tensor DelayedScaling and Format.HYBRID configuration.

## Key Results
- Eliminates extreme outliers, reducing peak activations from >6000 to <20 without accuracy loss
- Enables stable FP8 pre-training with perplexity matching BF16 baseline while achieving 36% higher throughput
- Enables W8A8 per-tensor static quantization of LLMs for the first time, achieving state-of-the-art performance
- Works universally across GPT-2 (124M to 7B) and vision transformers without architectural modifications

## Why This Works (Mechanism)

### Mechanism 1: Structural Alignment as the Root Cause
The paper posits that extreme outliers are data-independent artifacts resulting from the structural alignment of weight matrices during optimization. As training progresses, weight vectors in the down-projection matrix may become collinear with the dominant singular vectors of the up-projection matrix. If an input aligns with the corresponding right singular vector, matrix multiplication amplifies this into an extreme value. Evidence includes experiments showing outliers with random Gaussian input and simulation analysis confirming structural amplification.

### Mechanism 2: Non-Linear Magnitude Suppression
A scaled L4 penalty function allows the model to retain normal activations while aggressively penalizing magnitudes that exceed τ=3. The super-linear cost curve creates negligible cost for values <τ while imposing massive cost for values >10τ. This selectively clamps the tail of the activation distribution without suppressing the mean, based on the assumption that extreme magnitudes are not required for semantic representation.

### Mechanism 3: Residual Stream Quantization Enablement
Eliminating outliers at the block output enables quantization of the residual stream, which previous methods were forced to keep in high precision. Standard quantization fails because outliers accumulate and stretch the dynamic range. TWEO constrains every block output, ensuring residual addition operates on compatible, low-range tensors.

## Foundational Learning

- **Concept**: FP8 Dynamic Range Constraints (E4M3)
  - **Why needed**: Understand why outliers break FP8 training; E4M3 has max value ±448, so outliers >1000 cause overflow and training collapse
  - **Quick check**: If an activation spike reaches 1000 in E4M3 format without TWEO, what is the likely result for the training loop?

- **Concept**: Per-Tensor vs. Per-Token Quantization
  - **Why needed**: Understand the efficiency gain from TWEO enabling "per-tensor" quantization (single scale factor for whole matrix vs. complex per-token)
  - **Quick check**: Why does a single extreme outlier force a per-tensor scaling factor to ruin the precision of all other values in the tensor?

- **Concept**: Singular Value Decomposition (SVD) in Weight Matrices
  - **Why needed**: Understand how outliers are explained via interaction of input vectors with singular vectors of weight matrices
  - **Quick check**: According to the paper's hypothesis, does changing the input data distribution remove the root cause of outliers, or does the issue lie in the trained weights?

## Architecture Onboarding

- **Component map**: Input → Attn → Residual Add → MLP → Residual Add → Output Activation (A^(l))
- **Critical path**: 
  1. Forward pass executes normally
  2. Fetch activation magnitudes |A^(l)| before backward pass
  3. Apply scaling τ=3 and power p=4
  4. Accumulate penalty term with weight λ(t)
  5. Backpropagate combined loss
- **Design tradeoffs**:
  - Simplicity vs. Regularization Strength: High λ guarantees outlier removal but risks over-constraining the model; λ≈0.01 is suggested as robust
  - Hardware Efficiency: Per-tensor FP8 maximizes throughput but removes safety net of mixed-precision fallbacks
- **Failure signatures**:
  - Training Collapse: Loss spikes to NaN/Inf suddenly (standard FP8 failure without TWEO)
  - Over-regularization: Validation perplexity remains high (underfitting) due to constrained magnitudes
  - Quantization Drift: Weak TWEO may allow outliers to re-emerge, causing W8A8 quantization to fail
- **First 3 experiments**:
  1. Baseline Collapse Verification: Run GPT-2 Small in pure FP8 (no TWEO) to reproduce failure mode
  2. Lambda Sweep: Train with TWEO using λ ∈ {0.001, 0.01, 0.1} to verify 0.01 maintains BF16-equivalent perplexity
  3. Post-Training Quantization Stress Test: Apply W8A8 per-tensor static quantization to checkpoint from Exp 2 and compare PPL against SmoothQuant baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: Can TWEO maintain stability and performance when scaling to ultra-large models (e.g., 700B parameters)? [explicit] The conclusion states, "Due to resource constraints, we had not verified TWEO on the largest models (e.g., 700B), which will be a future work when compute is available."
- **Open Question 2**: Can TWEO be applied via fine-tuning to remove outliers from existing pre-trained models? [explicit] The authors note, "While TWEO models were trained from scratch in this paper, we are also interested in fine-tuning existing models to remove their extreme outliers."
- **Open Question 3**: Does the outlier suppression provided by TWEO facilitate stable training and inference in even lower precision formats like FP4? [explicit] The conclusion explicitly lists future interest in "FP4 learning and inference."

## Limitations

- The structural outlier hypothesis, while well-supported, may not fully explain outlier generation across all transformer architectures and training regimes
- The claim that extreme magnitudes are unnecessary for semantic representation lacks systematic ablation studies across diverse downstream tasks
- Long-term stability of quantized models under distribution shifts or domain adaptation scenarios is not evaluated

## Confidence

- **High Confidence**: TWEO successfully eliminates extreme outliers and enables stable FP8 training where baselines fail, with reproducible quantitative results
- **Medium Confidence**: The structural alignment hypothesis explaining outlier generation is well-supported but could benefit from additional architectural variations
- **Low Confidence**: The assertion that TWEO works "out-of-the-box without requiring engineering tricks" may overstate the method's universality, as optimal parameter tuning may still be required

## Next Checks

1. **Architectural Robustness Test**: Apply TWEO to transformer variants with different normalization, attention mechanisms, and position encodings; verify outlier elimination and performance preservation across at least three architecturally distinct models

2. **Downstream Task Ablation**: Systematically ablate TWEO at different training stages and measure impact on specific downstream tasks requiring rare token handling or specialized attention patterns; quantify any degradation in tasks that might genuinely require extreme magnitudes

3. **Distribution Shift Evaluation**: Quantize TWEO-trained models with W8A8 and evaluate performance under domain shifts, adversarial attacks, and out-of-distribution inputs; compare against baseline quantization methods to verify that outlier elimination doesn't compromise robustness to distribution changes