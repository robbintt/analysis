---
ver: rpa2
title: Evaluating the Impact of Post-Training Quantization on Large Language Models
  for Code Generation
arxiv_id: '2503.07103'
source_url: https://arxiv.org/abs/2503.07103
tags:
- quantization
- code
- samples
- calibration
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates post-training quantization for code generation
  LLMs, aiming to reduce memory footprint while maintaining performance. The authors
  use Additive Quantization with Learned Multi-Codebooks (AQLM) to quantize large
  code-specific models (CodeLlama and DeepSeek-Coder) from 16-bit floating point down
  to 2-bit precision.
---

# Evaluating the Impact of Post-Training Quantization on Large Language Models for Code Generation

## Quick Facts
- arXiv ID: 2503.07103
- Source URL: https://arxiv.org/abs/2503.07103
- Reference count: 40
- 4-bit quantization reduces memory by 70% with no significant performance loss for code generation models

## Executive Summary
This paper investigates post-training quantization for code generation LLMs, aiming to reduce memory footprint while maintaining performance. The authors use Additive Quantization with Learned Multi-Codebooks (AQLM) to quantize large code-specific models (CodeLlama and DeepSeek-Coder) from 16-bit floating point down to 2-bit precision. They evaluate on MultiPL-E and McEval benchmarks using Java and Python, measuring pass@1 accuracy with 20 trials per problem. Results show that 4-bit quantization reduces memory by 70% with no significant performance loss, while 2-bit quantization with code-specific calibration and fine-tuning can achieve ~85% memory reduction with manageable performance trade-offs.

## Method Summary
The authors apply AQLM to quantize code generation models from 16-bit floating point to 2, 3, or 4 bits. The process uses calibration datasets of 1,024 samples to learn codebook vectors that approximate weight groups. They evaluate on MultiPL-E (161 Python, 158 Java) and McEval (42 Python, 53 Java) benchmarks, measuring pass@1 accuracy with 20 trials per problem. Three calibration dataset variants are tested: random text, mixed code/NL, and code-only. For 2-bit models, they also apply end-to-end fine-tuning to recover performance. Models tested include CodeLlama-7B, -13B, -34B and DeepSeek-Coder-7B, -33B at various bit-widths.

## Key Results
- 4-bit quantization achieves 70% memory reduction with no significant performance loss
- Code-specific calibration datasets significantly improve 2-bit quantization performance
- Larger models (33B/34B) show higher resilience to extreme quantization than smaller ones
- End-to-end fine-tuning can recover 20-30% of performance for 2-bit models

## Why This Works (Mechanism)

### Mechanism 1: Multi-Codebook Vector Approximation
- **Claim:** AQLM maintains performance at low bit-widths by representing weight groups as sums of learned vectors rather than single discrete values
- **Core assumption:** Linearity of dot products allows vector addition to approximate complex weight distributions effectively without retraining the entire network
- **Evidence anchors:** [section 2] describes AQLM using MCQ; *Squeeze10-LLM* and *SignRoundV2* corroborate industry shift toward staged codebook methods
- **Break condition:** Insufficient codebook size/count for weight distribution complexity causes severe degradation

### Mechanism 2: Domain-Aligned Calibration Data
- **Claim:** Code-specific calibration data reduces performance loss during extreme quantization by aligning weight approximation with target domain syntax
- **Core assumption:** Calibration dataset is representative of inference-time distribution
- **Evidence anchors:** [abstract] states code-specific calibration helps at 3-2 bits; [section 4] RQ2 shows significant improvements with "Mixed" or "Code" datasets
- **Break condition:** At higher bit-widths (4-bit+), quantization resolution preserves weights regardless of calibration data nuances

### Mechanism 3: Parameter Redundancy in Large Models
- **Claim:** Larger models (>30B parameters) exhibit higher resilience to extreme quantization noise due to redundant parameter encoding of features
- **Core assumption:** Features are over-parameterized such that precision loss doesn't equate to equal functional loss
- **Evidence anchors:** [section 4] RQ3 findings show larger models have lower percentage decrease at 2-bit; *Continual Quantization-Aware Pre-Training* supports efficiency improves with scale
- **Break condition:** 2-bit precision becomes a bottleneck even for large models, with potential inference latency degradation

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** Focuses on reducing memory after training without incurring retraining cost
  - **Quick check question:** Does PTQ require backpropagation through the entire model? (Answer: No, it uses forward passes on calibration data)

- **Concept: Pass@k Metric (Pass@1)**
  - **Why needed here:** Evaluates success using pass@1 (correct solution in one attempt), stricter than pass@10
  - **Quick check question:** If a model has pass@1 of 30%, how many solutions must it generate to likely solve once? (Answer: Roughly 3-4)

- **Concept: Vector Quantization (VQ)**
  - **Why needed here:** AQLM relies on grouping weights into vectors rather than processing individually
  - **Quick check question:** How does grouping weights into vectors help save memory? (Answer: Allows storing indices to shared codebooks rather than unique values)

## Architecture Onboarding

- **Component map:** Base Model -> Calibration Dataset -> Quantizer (AQLM) -> Runtime
- **Critical path:** 1) Select Calibration Data (Random vs. Code) 2) Run AQLM Calibration (Learns codebooks) 3) Encode Weights (Replaces FP16 with codebook indices) 4) (Optional) End-to-End Fine-Tuning
- **Design tradeoffs:** 4-bit vs. 2-bit (safe 70% vs. pushing for ~85% reduction); Inference latency not measured
- **Failure signatures:** API hallucinations, assertion errors, catastrophic drop (>70% relative performance drop for 1B models at 2-bit)
- **First 3 experiments:** 1) Sanity Check: 4-bit quantization verifying â‰¤2% pass@1 change 2) Calibration Impact: 2-bit with Random vs. Mixed data measuring relative improvement 3) Fine-Tuning Recovery: Applying end-to-end fine-tuning to observe ~20-30% performance boost

## Open Questions the Paper Calls Out

1. **How does extreme quantization impact code-related tasks other than generation, such as code summarization and program repair?**
   - **Basis in paper:** [explicit] Authors state next step is to "assess the impact of quantization on a wider range of code-related tasks" beyond generation
   - **Why unresolved:** Study strictly evaluated code generation performance using MultiPL-E and McEval benchmarks
   - **What evidence would resolve it:** Empirical evaluation on benchmarks for code summarization, repair, and completion

2. **What are the effects of AQLM quantization on inference latency and energy consumption?**
   - **Basis in paper:** [explicit] Authors list investigating "inference latency and energy consumption" as future work, noting focus only on memory and accuracy
   - **Why unresolved:** Current study measured memory footprint reduction and pass@1 accuracy but not speed or power usage
   - **What evidence would resolve it:** Time-to-first-token measurements and energy metering during inference runs

3. **Can code models with over 100 billion parameters undergo 2-bit quantization without significant performance degradation?**
   - **Basis in paper:** [inferred] Paper suggests very large models (100B+) may support 2-bit better but notes this "remains largely unexplored" due to computational costs
   - **Why unresolved:** Study only scaled up to 34B parameters; resources insufficient for 70B version
   - **What evidence would resolve it:** Replicating 2-bit quantization experiments on CodeLlama-70B or GPT-3 sized architectures

## Limitations

- Calibration data limited to 1,024 samples, potentially not fully representative of code generation task distribution
- Inference latency impact not measured, critical for practical deployment despite memory savings
- Study focuses on narrow set of models (CodeLlama and DeepSeek-Coder), may not generalize to other architectures

## Confidence

- **High confidence:** 4-bit quantization results showing 70% memory reduction with no significant performance loss are well-supported and replicable
- **Medium confidence:** Effectiveness of code-specific calibration data at 2-bit precision is supported but relies on relatively small sample size
- **Low confidence:** Generalizability of larger model resilience to extreme quantization across different architectures and domains

## Next Checks

1. Measure inference latency for 2-bit and 3-bit quantized models compared to 4-bit and baseline, including both latency per token and total generation time
2. Test calibration data hypothesis with larger sample size (e.g., 10,000 samples) and evaluate on domain-specific coding challenges like competitive programming problems
3. Validate larger model resilience finding by testing additional model sizes (e.g., 13B and 70B parameters) and comparing across different architectures beyond CodeLlama and DeepSeek-Coder