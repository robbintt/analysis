---
ver: rpa2
title: 'AutoGD: Automatic Learning Rate Selection for Gradient Descent'
arxiv_id: '2510.09923'
source_url: https://arxiv.org/abs/2510.09923
tags:
- learning
- autogd
- rate
- such
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoGD is a gradient descent method that automatically selects
  learning rates by comparing the performance of neighboring (larger and smaller)
  rates at each iteration. The algorithm uses an Armijo condition to ensure sufficient
  descent and includes a "no movement" option to avoid divergence.
---

# AutoGD: Automatic Learning Rate Selection for Gradient Descent

## Quick Facts
- arXiv ID: 2510.09923
- Source URL: https://arxiv.org/abs/2510.09923
- Reference count: 40
- Key outcome: AutoGD automatically selects learning rates by comparing neighboring rates at each iteration, achieving convergence rates matching standard GD while outperforming or matching other first-order methods

## Executive Summary
AutoGD is a gradient descent method that automatically selects learning rates without requiring knowledge of smoothness constants. The algorithm compares the performance of neighboring (larger and smaller) learning rates at each iteration using an Armijo condition to ensure sufficient descent. It includes a "no movement" option to prevent divergence and is initialized with diffuse starting points. Theoretical analysis establishes both asymptotic and non-asymptotic convergence rates that match the optimal rate of standard gradient descent (up to a constant). Empirical results demonstrate that AutoGD is robust to initial learning rate choice and outperforms or matches other first-order methods on classical optimization problems and variational inference tasks.

## Method Summary
AutoGD operates by comparing neighboring learning rates at each iteration to automatically select step sizes. The algorithm evaluates the performance of a larger learning rate, a smaller learning rate, and a "no movement" option at every step. It uses an Armijo condition to ensure sufficient descent when selecting between these options. The method is initialized with diffuse starting points and does not require prior knowledge of smoothness constants. AutoGD's design allows it to adaptively choose step sizes based on local gradient information, eliminating the need for manual tuning or problem-specific parameter knowledge. The algorithm extends to second-order methods through AutoBFGS and AutoLBFGS variants, which show substantial performance improvements in practice.

## Key Results
- AutoGD automatically selects learning rates by comparing neighboring rates at each iteration using an Armijo condition
- Theoretical convergence rates match optimal standard GD rates (up to a constant) for strongly convex functions
- Empirical results show AutoGD outperforms or matches other first-order methods including AdGD2 and backtracking line search
- AutoBFGS and AutoLBFGS extensions demonstrate substantial performance improvements in practice

## Why This Works (Mechanism)
AutoGD's mechanism works by systematically evaluating and comparing multiple learning rate candidates at each iteration. The algorithm tests three options: a larger learning rate, a smaller learning rate, and no movement. By using the Armijo condition to ensure sufficient descent, AutoGD can reliably identify which option provides the best improvement. This approach eliminates the need for problem-specific smoothness constant knowledge while maintaining convergence guarantees. The comparison-based selection allows the algorithm to adapt to local curvature and gradient information, automatically adjusting step sizes to maintain efficient progress toward the optimum.

## Foundational Learning
- **Gradient descent fundamentals**: Understanding how gradient descent moves in the direction of steepest descent is essential for grasping AutoGD's iterative improvement mechanism
  - *Why needed*: Forms the basis for understanding how learning rate selection affects convergence
  - *Quick check*: Can explain how gradient descent updates parameters using learning rates

- **Line search methods**: Knowledge of backtracking line search and Armijo conditions helps understand AutoGD's descent verification
  - *Why needed*: AutoGD uses similar principles but with a comparison-based approach
  - *Quick check*: Can describe how Armijo conditions ensure sufficient function decrease

- **Convergence analysis**: Understanding asymptotic and non-asymptotic convergence rates is crucial for interpreting AutoGD's theoretical guarantees
  - *Why needed*: The paper establishes convergence rates that match standard GD
  - *Quick check*: Can explain the difference between asymptotic and non-asymptotic convergence

## Architecture Onboarding

**Component Map:**
Initialization -> Learning Rate Comparison -> Armijo Condition Check -> Parameter Update -> Next Iteration

**Critical Path:**
The critical path involves the learning rate comparison step, where AutoGD evaluates multiple candidates and selects the best option based on the Armijo condition. This decision-making process directly impacts convergence speed and algorithm stability.

**Design Tradeoffs:**
AutoGD trades computational overhead (evaluating multiple learning rates per iteration) for automatic learning rate selection without requiring smoothness constant knowledge. This eliminates manual tuning but may increase per-iteration computation compared to fixed learning rate methods.

**Failure Signatures:**
- Divergence occurs if the "no movement" option is never selected when appropriate
- Slow convergence may indicate poor initialization of neighboring learning rates
- Inconsistent performance across different problem types suggests sensitivity to problem structure

**First 3 Experiments:**
1. Compare AutoGD convergence on a simple quadratic function versus standard GD with optimal fixed learning rate
2. Test AutoGD's robustness to initialization by running with widely different starting learning rates
3. Evaluate the computational overhead by measuring iterations per second compared to backtracking line search

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation is limited to relatively small-scale optimization problems, raising questions about scalability to high-dimensional machine learning tasks
- The practical impact of constant factor differences between AutoGD and standard GD convergence rates remains unclear
- Method's robustness across diverse problem landscapes and loss functions beyond those tested is not fully established

## Confidence
- **High confidence**: Theoretical convergence guarantees for strongly convex functions and the core algorithmic mechanism of comparing neighboring learning rates
- **Medium confidence**: Empirical performance claims, particularly the comparisons with AdGD2 and backtracking line search, given the limited problem diversity
- **Low confidence**: Scalability assertions and performance guarantees for non-convex or ill-conditioned problems not covered in the theoretical analysis

## Next Checks
1. **Scalability testing:** Evaluate AutoGD on large-scale deep learning optimization tasks with millions of parameters to assess computational overhead and convergence behavior
2. **Robustness analysis:** Test the algorithm across diverse problem classes including non-convex objectives, sparse optimization, and ill-conditioned matrices to verify broad applicability
3. **Ablation studies:** Systematically analyze the impact of each component (Armijo condition, no movement option, neighbor comparison strategy) on performance to isolate key contributors to success