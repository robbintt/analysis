---
ver: rpa2
title: 'Descriptive History Representations: Learning Representations by Answering
  Questions'
arxiv_id: '2506.02125'
source_url: https://arxiv.org/abs/2506.02125
tags:
- user
- item
- title
- rating
- price
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces descriptive history representations (DHRs),
  a framework for learning compact, informative history summaries in partially observable
  environments by focusing on their ability to answer task-relevant questions. The
  core method involves a multi-agent learning approach where a representation encoder,
  answer agent, and decision agent are jointly optimized to balance reward maximization
  with the representation's capacity to answer informative queries.
---

# Descriptive History Representations: Learning Representations by Answering Questions

## Quick Facts
- arXiv ID: 2506.02125
- Source URL: https://arxiv.org/abs/2506.02125
- Reference count: 40
- Authors: Guy Tennenholtz; Jihwan Jeong; Chih-Wei Hsu; Yinlam Chow; Craig Boutilier
- Primary result: Learned textual user profiles as DHRs achieve up to 0.84 ranking accuracy and 0.93 recommendation reward

## Executive Summary
This paper introduces Descriptive History Representations (DHRs), a framework for learning compact, informative history summaries in partially observable environments by focusing on their ability to answer task-relevant questions. The core method involves a multi-agent learning approach where a representation encoder, answer agent, and decision agent are jointly optimized to balance reward maximization with the representation's capacity to answer informative queries. The authors demonstrate that DHRL generates interpretable textual user profiles that serve as effective DHRs, achieving strong predictive accuracy (up to 0.84 for ranking tasks) and downstream recommendation performance (up to 0.93 recommendation reward) on public movie and shopping datasets. The approach offers a compelling alternative to traditional methods by explicitly guiding representation learning through a question-answer space, enhancing interpretability and alignment with high-level goals.

## Method Summary
The DHRL framework learns DHRs through a multi-agent setup where a history encoder generates textual profiles, an answer agent predicts responses to task-relevant questions, and a decision agent uses these profiles to make recommendations. Training involves a max-min optimization where the discriminator distinguishes between ground-truth and predicted answers while the agents work to fool it. The method uses hindsight query synthesis to generate questions from complete trajectory data, creating supervised learning signals. The objective balances environment reward with the representation's ability to answer questions, using variational divergence minimization (TV-distance) to stabilize training. The framework is validated on MovieLens 25M and Amazon Reviews datasets, producing interpretable user profiles that achieve strong performance on ranking and recommendation tasks.

## Key Results
- DHRs achieve 0.84 prediction accuracy on pairwise item ranking tasks
- Recommendation reward reaches 0.93 on normalized scale for both MovieLens and Amazon datasets
- Textual profiles show high profile-history consistency (0.62-0.70) when evaluated by LLMs and humans
- Performance degrades with excessively long histories (>256 tokens) or history lengths >10 interactions
- TV-distance outperforms KL and Chi-squared divergences in training stability

## Why This Works (Mechanism)

### Mechanism 1: Sufficient Statistic Induction via QA-Spaces
The framework defines an "f-sufficient QA-space" where the encoder learns to produce a representation such that an answer agent can predict ground-truth answers using only the compressed representation. This forces the representation to retain only information necessary for the QA-task, theoretically linked to value prediction. The core assumption is that the generated questions cover all information required for the downstream task. Break condition: if questions fail to cover critical environment state needed for value function, representation will be insufficient despite high answer accuracy.

### Mechanism 2: Variational Divergence Minimization
Direct density estimation is unstable, so the method uses a variational form involving a discriminator that distinguishes between agent-predicted answers and oracle ground-truth answers. The adversarial pressure drives the representation to produce statistically accurate answers. The choice of f-divergence (TV vs KL) significantly impacts performance. Break condition: if the discriminator overpowers the encoder/answer agent or if the divergence metric is mismatched to data modality, gradients may fail to propagate meaningful signal.

### Mechanism 3: Hindsight Query Synthesis
Informative supervisory signals are constructed by generating questions derived from full trajectory realizations (history + future), grounding learning in actual outcomes. The QA-generator observes both current history and realized future to construct questions about events where answers are known, converting the RL problem into supervised prediction over representation space. Break condition: if environment is non-stationary or trajectories don't represent optimal policy distribution, "ground truth" answers may mislead representation learner.

## Foundational Learning

- **Concept: Sufficient Statistics**
  - Why needed here: The paper proves that a DHR is a "sufficient statistic" (Proposition 1). The goal is to compress history into a summary lossless regarding task value.
  - Quick check question: Can you explain why a representation that predicts the next observation pixel perfectly might still be a poor decision-making state?

- **Concept: f-Divergences (e.g., KL, TV-Distance)**
  - Why needed here: The optimization objective balances reward maximization with minimizing an f-divergence between answer distributions. Choice of divergence significantly impacted performance.
  - Quick check question: Why might Total Variation (TV) distance be preferred over KL-divergence when distinguishing between generated text and ground truth?

- **Concept: Variational Inference / Minimax Optimization**
  - Why needed here: The training loop uses a discriminator to approximate the divergence through "max-min" dynamics. Understanding this is required to debug discriminator failure.
  - Quick check question: In the objective max min E[f*(g) - g], which component represents the discriminator and which represents the policy/encoder?

## Architecture Onboarding

- **Component map:** History Encoder (E) -> QA Generator (νQA*) -> Answer Agent (νA) -> Discriminator (g) -> Decision Agent (πD)
- **Critical path:** The "QA Data Pipeline" is most fragile. Ensure QA generator can procedurally generate meaningful questions from future interactions before agents learn anything. If questions are trivial, representation will be trivial.
- **Design tradeoffs:**
  - Profile Length: Too short loses critical info; too long (>256 tokens) degrades performance due to attention difficulty
  - Question Count (K): More questions improve robustness up to a point, but increase discriminator update computational cost
- **Failure signatures:**
  - Low Prediction Accuracy + High Reward: Decision agent ignoring profile, acting randomly or via priors
  - High Prediction Accuracy + Low Reward: QA-space is insufficient - questions irrelevant to reward
  - Parsing Failures: Answer Agent fails to follow required output format, triggering negative rewards
- **First 3 experiments:**
  1. QA Sanity Check: Run QA Generator on fixed dataset slice. Manually verify generated questions are answerable from history and ground-truth answers match future. Do not train agents yet.
  2. Overfitting Test: Freeze decision agent. Train Encoder + Answer Agent on single user history. Check if representation perfectly reconstructs answers for that user. Validates discriminator loop.
  3. Ablation on λ: Run DHRL with λ=0 (pure RL) vs. λ=0.5 (balanced) vs. λ=1 (pure QA). Plot recommendation reward. Validates hypothesis that both signals are necessary.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an adversarial QA generator be trained to autonomously discover maximally informative questions without relying on manually engineered templates?
- **Basis in paper:** Conclusion and Appendix B.3 explicitly identify "dynamically learning an adversarial QA generator" as critical future direction to enhance framework's adaptability.
- **Why unresolved:** Current work relies on fixed, template-based QA generator derived from future ground-truth data. Authors propose adversarial objective (Eq. OPT 4) but don't implement or validate it.
- **What evidence would resolve it:** Successful training of proposed adversarial generator on recommendation or RL benchmark that results in higher downstream reward and prediction accuracy compared to fixed template method, while maintaining semantic interpretability.

### Open Question 2
- **Question:** How effective are DHRs in high-dimensional, continuous control domains like robotics?
- **Basis in paper:** Conclusion lists "applying DHRs to a wider array of partially observable domains like robotics" as future work.
- **Why unresolved:** Paper validates DHRs solely on discrete recommendation tasks using textual profiles. Untested whether textual summaries or vector representations can sufficiently capture continuous state dynamics and low-level motor controls required in robotics.
- **What evidence would resolve it:** Evaluation of DHRL on continuous POMDP benchmark (e.g., MuJoCo environments with partial observability), comparing sample efficiency and asymptotic performance against standard baselines like recurrent policies or belief state methods.

### Open Question 3
- **Question:** What causes the performance decline in DHRL as history length increases, and can this scalability limit be overcome?
- **Basis in paper:** Section 5.2 (Ablation study) notes that recommendation reward declines when history length exceeds 10 interactions, hypothesizing that "excessively long histories could introduce noise and complexity."
- **Why unresolved:** Paper identifies limitation but doesn't isolate root cause (LLM attention limits, signal dilution, or optimization instability) or test mitigation strategies.
- **What evidence would resolve it:** Diagnostic experiment tracking "Profile-History Consistency" and "Prediction Fidelity" metrics as history length increases, coupled with ablations using LLMs with longer context windows or hierarchical summarization techniques.

## Limitations
- The f-sufficiency assumption lacks formal validation - no experiments test whether QA-space covers all information required for optimal value prediction
- Discriminator architecture details are underspecified, particularly how "conceptually partitioned" vocabularies work in practice
- Claims about interpretability improvements rely on subjective human/LLM consistency scores (0.62-0.70)
- Performance degrades significantly with history lengths >10 interactions or profiles >256 tokens
- Framework requires access to future information during training for hindsight query synthesis

## Confidence
- **High confidence:** Prediction accuracy results (0.84 ranking accuracy, 0.93 recommendation reward) are directly measurable and well-documented across multiple runs
- **Medium confidence:** Theoretical framing of DHRs as sufficient statistics is logically consistent but relies on untested assumption that QA-spaces capture task-relevant information
- **Low confidence:** Claims about interpretability improvements are based on consistency scores that depend on subjective human/LLM judgment

## Next Checks
1. **QA-space coverage validation:** Design experiment where you systematically remove question types from QA-space and measure degradation in downstream reward - this tests whether representation actually learns task-relevant information or just optimizes for specific questions provided
2. **Discriminator stability analysis:** Implement monitoring for discriminator loss during training to detect mode collapse or gradient explosion, particularly for longer profiles (>256 tokens) where performance degradation was observed
3. **Cross-task generalization test:** Train DHRs on one task (e.g., movie recommendations) and evaluate on structurally similar but distinct task (e.g., book recommendations) to assess whether learned representations transfer or are task-specific