---
ver: rpa2
title: 'Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity
  and Novelty Rejection Mechanisms'
arxiv_id: '2503.19656'
source_url: https://arxiv.org/abs/2503.19656
tags:
- rejection
- prediction
- time
- data
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dual rejection mechanism for time series
  forecasting that combines ambiguity and novelty rejection to enhance model reliability
  in the absence of future ground truth labels. The ambiguity rejection uses prediction
  error variance to estimate model confidence, allowing the model to abstain from
  making predictions when uncertainty is high, while novelty rejection employs Variational
  Autoencoders and Mahalanobis distance to detect inputs that deviate from training
  data distribution.
---

# Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms

## Quick Facts
- **arXiv ID:** 2503.19656
- **Source URL:** https://arxiv.org/abs/2503.19656
- **Reference count:** 37
- **Primary result:** Dual rejection mechanism combining ambiguity (prediction error variance) and novelty (VAE + Mahalanobis) detection improves forecasting accuracy by filtering uncertain and out-of-distribution samples across three datasets.

## Executive Summary
This paper addresses the challenge of reliable time series forecasting when future ground truth labels are unavailable. The authors propose a dual rejection mechanism that combines ambiguity rejection (using prediction error variance to estimate model confidence) and novelty rejection (using VAEs and Mahalanobis distance to detect distributional shifts). The approach allows models to abstain from making predictions when uncertainty is high, improving overall forecast accuracy. Evaluated across three datasets with three baseline models, the dual rejection mechanism consistently reduces MAE and MSE by filtering out uncertain and out-of-distribution samples.

## Method Summary
The method implements a two-stage sequential rejection mechanism. First, a VAE trained on historical data encodes new inputs and computes Mahalanobis distance in latent space to detect novelty - samples with high distance are rejected. Second, for accepted samples, the base forecaster's internal uncertainty (calibrated using validation error variance) determines if predictions should be rejected based on low confidence. The combined approach filters both distributional shifts and model uncertainty, with thresholds typically tuned to maintain around 10% rejection rate while optimizing accuracy metrics.

## Key Results
- Dual rejection mechanism achieves lower MAE and MSE than either component alone
- Best performance observed when combining both novelty and ambiguity rejection
- Consistent improvements across ETT, Weather, and Exchange Rate datasets
- Sequential gating (novelty first, then ambiguity) effectively reduces overall forecast risk

## Why This Works (Mechanism)

### Mechanism 1: Ambiguity Rejection via Prediction Error Variance
The model estimates confidence by using its internal uncertainty calibrated against historical prediction error variance. During validation, the relationship between model uncertainty and actual prediction errors is learned. For new forecasts, if the predicted uncertainty interval exceeds this calibrated threshold, the prediction is rejected. This assumes the uncertainty-error relationship remains stable across data shifts.

### Mechanism 2: Novelty Rejection for Distributional Shift Detection
A VAE learns the training data's distribution in latent space. New inputs are encoded and their Mahalanobis distance from the training distribution is computed. Large distances indicate the input deviates from training patterns, suggesting poor forecastability. This provides an explicit OOD detection mechanism before uncertainty assessment.

### Mechanism 3: Sequential Dual Rejection for Risk Reduction
The two-stage gating mechanism first filters out clear OOD samples via novelty detection, then applies confidence assessment only to in-distribution data. This architecture targets distinct error sources - distributional shifts versus model limitations - providing more robust filtering than single-stage approaches.

## Foundational Learning

**Mahalanobis Distance**
- Why needed: Core metric for novelty rejection, measuring distance from distribution center accounting for variable correlations
- Quick check: Given a 2D distribution with mean (0,0) and strong positive correlation, which point is more "novel": (1, 1) or (1, -1)?

**Variational Autoencoder (VAE)**
- Why needed: Provides structured latent space for novelty detection by learning data distribution
- Quick check: In a VAE, why is the KL divergence term included in the loss function?

**Chow's Rule / Selective Classification**
- Why needed: Extends selective classification principles from classification to time series regression
- Quick check: If the cost of a rejected prediction (c) increases, how should the model's rejection threshold change?

## Architecture Onboarding

**Component map:**
VAE-based Novelty Detector -> Base Forecaster -> Ambiguity Rejector -> Sequential Gating Logic

**Critical path:** New time series window processed by VAE encoder → Mahalanobis distance computed → if accepted, passed to base forecaster → ambiguity assessment → forecast released or rejected. Latency in any stage degrades system utility.

**Design tradeoffs:**
- Latency vs. Reliability: VAE and ambiguity checks add inference computation
- Coverage vs. Accuracy: Rejection thresholds control this trade-off
- Model Complexity: Requires training and maintaining two separate deep learning models

**Failure signatures:**
- Over-rejection: Tight thresholds reject most inputs, providing little practical value
- VAE Drift: Small input changes trigger false novelty rejections
- Uncertainty Miscalibration: Broken uncertainty-error relationship renders ambiguity rejector ineffective

**First 3 experiments:**
1. Baseline Calibration: Train base forecaster and VAE, sweep thresholds to plot MAE/MSE vs rejection rate trade-off
2. Ablation Study: Compare novelty-only, ambiguity-only, and dual systems at fixed rejection rate
3. OOD Injection Test: Synthesize out-of-distribution test data and measure recall of correctly rejected samples

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions in the text, but based on the methodology and results, several critical questions emerge:

1. How can optimal rejection thresholds be determined adaptively in real-time without relying on pre-defined validation sets?
2. Does sequential application of novelty before ambiguity yield superior results compared to unified or parallel scoring mechanisms?
3. To what extent does reliance on historical prediction error variance fail under severe non-stationarity or sudden concept drift?

## Limitations

- Sequential rejection architecture adds significant inference-time latency
- Threshold calibration relies on validation data, limiting real-world adaptability
- Method assumes stable relationship between model uncertainty and prediction error variance

## Confidence

**High confidence:** The dual rejection mechanism architecture and its sequential implementation are clearly described. The evaluation methodology using MAE/MSE metrics on filtered test sets is straightforward and reproducible.

**Medium confidence:** The core idea of using prediction error variance for ambiguity rejection is well-founded, though the paper lacks detail on how sample-wise variance is obtained from deterministic models like Autoformer.

**Low confidence:** The VAE architecture specifications (latent dimension, encoder/decoder structure) and exact threshold calibration procedures are not provided, making exact reproduction difficult.

## Next Checks

1. Implement VAE with latent dimension 32 and train on training split inputs to establish baseline novelty detection capability
2. Use MC Dropout or probabilistic forecasting head to obtain sample-wise prediction variance for ambiguity rejection
3. Sweep rejection thresholds on validation set to find optimal balance between coverage and accuracy, targeting approximately 10% rejection rate