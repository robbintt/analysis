---
ver: rpa2
title: Exploring Spiking Neural Networks for Binary Classification in Multivariate
  Time Series at the Edge
arxiv_id: '2510.20997'
source_url: https://arxiv.org/abs/2510.20997
tags:
- networks
- time
- snns
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a general framework for training spiking neural
  networks (SNNs) to perform binary classification on multivariate time series data,
  with a focus on edge deployment and high precision at low false alarm rates. The
  approach uses the Evolutionary Optimization of Neuromorphic Systems (EONS) algorithm
  to evolve sparse, stateful SNNs by jointly optimizing their architectures and parameters.
---

# Exploring Spiking Neural Networks for Binary Classification in Multivariate Time Series at the Edge

## Quick Facts
- **arXiv ID:** 2510.20997
- **Source URL:** https://arxiv.org/abs/2510.20997
- **Reference count:** 40
- **Primary result:** Achieved 51.8% true positive rate at 1/hr false alarm rate for gamma-ray source detection using evolved spiking neural networks

## Executive Summary
This paper presents a framework for training spiking neural networks (SNNs) to perform binary classification on multivariate time series data, with emphasis on edge deployment and high precision at low false alarm rates. The approach uses the Evolutionary Optimization of Neuromorphic Systems (EONS) algorithm to evolve sparse, stateful SNNs by jointly optimizing architectures and parameters. Demonstrated on gamma-ray spectral data and EEG seizure detection, the framework achieves competitive performance compared to deep learning baselines while operating at 2mW power consumption on neuromorphic hardware.

## Method Summary
The method employs EONS to evolve sparse, stateful SNNs for binary classification. Inputs are encoded into spike trains using rate or spikes encoding, then processed by evolved IF neuron networks with recurrent connections. Classification uses a single output neuron whose spike counts are thresholded. Training optimizes MCC as fitness, with specialized handling for imbalanced datasets. Ensembles of independently evolved networks improve performance through complementary decision boundaries.

## Key Results
- Gamma-ray source detection: 51.8% TPR at 1/hr FAR, improving to 67.1% with three-model any-vote ensemble
- Seizure detection: 95% TPR with 16% false positive rate using majority-vote ensemble
- Hardware deployment: 2mW power consumption and 20.2ms inference latency on microCaspian platform
- Performance competitive with deep learning and PCA baselines while using fewer resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Stateful spiking neurons provide implicit temporal modeling for sequential classification without explicit memory structures.
- **Mechanism:** Integrate-and-Fire (IF) neurons maintain membrane potentials across time steps, encoding historical information in their internal state H_t. The state evolves recursively via H_t = Ψ(H_{t-1}, s_{t-1}), where Ψ emerges from neuron dynamics rather than explicit design.
- **Core assumption:** Temporal dependencies in the data can be captured through compressed state representations rather than full sequence storage.
- **Evidence anchors:**
  - [abstract]: "evolve sparse, stateful SNNs by jointly optimizing their architectures and parameters"
  - [Section I-A]: "The classifier assigns a binary label to each time step t, using only the current observation x_t along with an internal state H_t that encodes information about prior observations"
  - [corpus]: Limited direct corpus support for this specific mechanism; neighbor papers focus on sparsity and training methods rather than stateful temporal encoding.
- **Break condition:** If temporal dependencies exceed the effective memory horizon of the evolved network (determined by recurrent connectivity patterns and neuron parameters), the implicit state representation will fail to capture longer-range patterns.

### Mechanism 2
- **Claim:** Evolutionary optimization enables discovery of compact, task-specific architectures that gradient-based methods cannot efficiently explore.
- **Mechanism:** EONS maintains a population of candidate SNNs, evaluates them using non-differentiable fitness functions (e.g., Matthews Correlation Coefficient), and applies selection, crossover, and mutation to jointly evolve topology and parameters. Tournament selection balances exploitation of good solutions with exploration.
- **Core assumption:** The fitness landscape contains reachable optima that evolutionary operators can navigate, and small networks (49-256 neurons) suffice for the task.
- **Evidence anchors:**
  - [abstract]: "approach uses the Evolutionary Optimization of Neuromorphic Systems (EONS) algorithm to evolve sparse, stateful SNNs by jointly optimizing their architectures and parameters"
  - [Section III-C]: "EONS uses three different reproduction operators: Duplication (Cloning), Crossover, Mutation"
  - [corpus]: Neighbor paper "A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks" addresses training challenges but uses backpropagation; corpus lacks direct comparison of evolutionary vs. gradient methods for architecture search.
- **Break condition:** If the task requires architectures beyond the max size constraint (256 neurons, 4096 synapses on µCaspian), or if the fitness function poorly correlates with true task performance, evolutionary search will fail.

### Mechanism 3
- **Claim:** Ensembles of independently evolved SNNs provide performance gains through complementary decision boundaries.
- **Mechanism:** Multiple training runs produce diverse populations. Top networks are combined via simple voting (any-vote or majority-vote), where each network may specialize in different patterns. The any-vote rule increases sensitivity while majority-vote reduces false positives.
- **Core assumption:** Evolved networks exhibit meaningful diversity in their learned representations and failure modes.
- **Evidence anchors:**
  - [abstract]: "A three-model any-vote ensemble increases TPR to 67.1% at the same false alarm rate"
  - [Section V-A]: "All ensembles that outperformed SNN 141 used the any voting method"
  - [Section III-E]: "We have observed that the top SNNs often have different strengths and weaknesses"
  - [corpus]: No direct corpus support for SNN ensemble methods; neighbor papers focus on single-network optimization.
- **Break condition:** If evolutionary convergence produces homogeneous networks with correlated errors, ensembling yields no benefit. The paper mitigates this by training 100-1000 separate populations.

## Foundational Learning

- **Concept: Spiking Neural Networks (Integrate-and-Fire model)**
  - **Why needed here:** The entire framework builds on IF neurons where spikes are emitted when membrane potential exceeds threshold. Understanding that information is encoded in spike timing/count is essential.
  - **Quick check question:** Given an input spike train that elevates a neuron's membrane potential to 80% of its threshold, will the neuron fire?

- **Concept: Evolutionary Algorithms (Population-based optimization)**
  - **Why needed here:** EONS replaces gradient descent. You must understand tournament selection, crossover, mutation, and elitism to interpret training behavior and hyperparameter choices.
  - **Quick check question:** If a population's best individual is never selected as a parent, what happens to elite performance preservation?

- **Concept: Binary Classification Metrics (TPR, FPR, MCC)**
  - **Why needed here:** The paper optimizes for low false alarm rates (1/hr) using MCC as fitness. Imbalanced datasets make metrics like accuracy misleading.
  - **Quick check question:** For a dataset with 95% negative samples, what accuracy would a classifier achieve by always predicting negative?

## Architecture Onboarding

- **Component map:** Input Layer (n or 2n neurons) ← Spike Encoder (rate/spikes encoding) → Evolved Hidden Layers (variable topology, recurrent connections allowed) → Output Layer (1 neuron) → Spike Counter → Threshold θ → Binary Label

- **Critical path:**
  1. Define fitness function aligned with task requirements (MCC for balance, TPR@FAR for low-alarm applications)
  2. Configure EONS hyperparameters (population_size=100, mutation_rate=0.9, tournament selection)
  3. Train 100-1000 populations, evaluating each on validation metrics
  4. Select top 10% by training MCC, sweep all threshold values θ
  5. Evaluate ensemble combinations (pairs/trios with any-vote/majority-vote)
  6. Deploy selected network(s) to µCaspian via TENNLab framework

- **Design tradeoffs:**
  - **Any-vote vs. Majority-vote ensemble:** Any-vote increases TPR at cost of higher FPR; majority-vote reduces FPR but may miss detections. Paper shows any-vote superior for radiation detection, majority-vote better for seizure detection.
  - **Rate vs. Spikes encoding:** Rate encoding distributes spikes across inference window; spikes encoding concentrates them. Paper found rate encoding slightly better for gamma-ray data.
  - **Single network vs. ensemble:** Ensembles improve performance (~25-50% TPR gain) but multiply latency and power consumption proportionally when run serially.

- **Failure signatures:**
  - Training stagnation with TPR=0: Initial fitness function provides no gradient; paper mitigates by adding F1 to TPR_0.
  - High FPR on deployment: Threshold θ set too low, or encoder saturates on out-of-distribution inputs.
  - Networks converge to majority class: AUC-based fitness functions fail on imbalanced data; use MCC instead.
  - Power analysis shows MCU dominates: Software encoding prevents low-power states; requires hardware encoder or DMA optimization.

- **First 3 experiments:**
  1. **Reproduce single-network baseline:** Train 100 populations on the gamma-ray dataset with MCC fitness, select top network, evaluate TPR@1/hr FAR. Compare to reported 51.8%.
  2. **Ablate encoding method:** Train identical populations using rate vs. spikes encoding on the same data split. Measure impact on TPR and network size.
  3. **Characterize ensemble scaling:** For top 10 networks, sweep all 2- and 3-network combinations under both voting rules. Plot TPR vs. FPR tradeoff curves to identify optimal ensemble configuration for target operating point.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can hardware-software co-design optimizations reduce the data-movement energy overhead to be lower than the actual SNN inference energy?
  - **Basis in paper:** [explicit] Section V-B states that transferring spikes between the MCU and FPGA consumes 646.4µJ, vastly exceeding the 40.4µJ required for inference, and suggests "leveraging DMA or lower-power communication" as future work.
  - **Why unresolved:** The current implementation relies on software-based encoding that prevents the MCU from entering low-power states during data transfer.
  - **What evidence would resolve it:** Power measurements showing communication energy is reduced below the inference threshold (e.g., <40µJ) utilizing DMA or hardware encoders.

- **Open Question 2:** Does parallel execution of SNN ensembles on the µCaspian platform maintain the performance gains of sequential voting without violating resource constraints?
  - **Basis in paper:** [explicit] Section V-B notes that future work will "support parallel operation of multiple SNNs with shared inputs," as current ensembling is performed serially.
  - **Why unresolved:** While serial ensembling improved TPR by 29.5%, it is unclear if the limited resources of the Lattice iCE40UP5K FPGA can support multiple concurrent SNNs without degrading latency or requiring excessive power.
  - **What evidence would resolve it:** Successful deployment of a parallel trio ensemble on the hardware with measured latency and power comparable to or better than the serial approach.

- **Open Question 3:** Can data balancing procedures mitigate the convergence issues associated with using AUC as a fitness function in EONS?
  - **Basis in paper:** [explicit] Section III-C-2 states that AUC was a poor fitness choice because networks converged to predicting the majority class, but notes "Data balancing procedures may mitigate this issue."
  - **Why unresolved:** The authors opted for Matthews Correlation Coefficient (MCC) to handle imbalance naturally, leaving the interaction between evolutionary optimization, AUC, and data balancing unexplored.
  - **What evidence would resolve it:** Comparative training runs showing EONS successfully optimizing for AUC on balanced subsets of the radiation dataset without stagnating at local minima.

## Limitations

- **Hardware dependency:** Results heavily dependent on specialized microCaspian neuromorphic hardware and TENNLab software framework, creating reproducibility barriers.
- **SNR scaling ambiguity:** The paper mentions biasing samples toward high SNR early in training but does not define the specific schedule or probability function used to transition to uniform sampling.
- **Limited comparison scope:** While competitive with deep learning and PCA baselines, the paper doesn't compare against other SNN training methods or state-of-the-art sequence models.

## Confidence

- **High Confidence:** The core evolutionary optimization methodology (EONS algorithm) and basic IF neuron dynamics are well-established in the literature and correctly implemented.
- **Medium Confidence:** Ensemble performance claims are supported by the presented results, but the diversity of evolved networks could vary significantly with different random seeds or hyperparameter settings.
- **Low Confidence:** Hardware deployment metrics (2mW, 20.2ms) depend on specific microCaspian implementation details not fully disclosed, making independent validation difficult.

## Next Checks

1. **Reproduce single-network baseline** on gamma-ray dataset using MCC fitness to verify the reported 51.8% TPR@1/hr FAR performance.
2. **Ablate encoding method** by training identical populations with rate vs. spikes encoding to quantify impact on performance and network efficiency.
3. **Characterize ensemble scaling** by evaluating all 2- and 3-network combinations from top performers under both voting rules to identify optimal ensemble configurations for target operating points.