---
ver: rpa2
title: 'Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models'
arxiv_id: '2505.13273'
source_url: https://arxiv.org/abs/2505.13273
tags:
- uncertainty
- emoe
- diffusion
- epistemic
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMoE, a novel method for estimating epistemic
  uncertainty in text-to-image diffusion models without requiring additional training.
  EMoE leverages pre-trained mixture-of-experts (MoE) models by separating expert
  components early in the denoising process and measuring variance across their latent
  space representations.
---

# Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2505.13273
- Source URL: https://arxiv.org/abs/2505.13273
- Reference count: 40
- EMoE introduces a novel method for estimating epistemic uncertainty in text-to-image diffusion models without additional training by leveraging pre-trained mixture-of-experts architectures

## Executive Summary
This paper introduces EMoE, a method for estimating epistemic uncertainty in text-to-image diffusion models by leveraging pre-trained mixture-of-experts (MoE) architectures. The approach separates expert components early in the denoising process and measures variance across their latent space representations to identify regions of the input space that were under-sampled during training. EMoE demonstrates effectiveness in correlating uncertainty with image quality metrics and reveals potential biases by showing higher uncertainty for under-represented languages like Finnish compared to English prompts. The method provides a computationally efficient solution for quantifying epistemic uncertainty without requiring additional model training.

## Method Summary
EMoE estimates epistemic uncertainty in text-to-image diffusion models by leveraging pre-trained mixture-of-experts architectures. The method operates by separating expert components early in the denoising process and measuring variance across their latent space representations. This approach identifies regions of the input space that were under-sampled during training without requiring additional model training. EMoE achieves this by isolating individual experts during the denoising steps and aggregating their outputs to compute uncertainty measures. The method is designed to work directly with text prompts and can be applied to existing diffusion models without architectural modifications.

## Key Results
- EMoE effectively correlates uncertainty with image quality metrics including CLIP score, aesthetic score, and image reward on the COCO dataset
- The method reveals hidden biases by identifying higher uncertainty for under-represented languages (Finnish) compared to English prompts
- Ablation studies validate the robustness of EMoE's design choices including ensemble size, denoising steps, and latent space selection

## Why This Works (Mechanism)
EMoE exploits the inherent diversity in mixture-of-experts architectures to estimate epistemic uncertainty. By separating expert components during the denoising process, the method captures the variance in predictions across different expert pathways. This variance serves as a proxy for uncertainty, with higher variance indicating regions of the input space that were less represented during training. The approach is computationally efficient because it leverages existing MoE components rather than requiring additional training or architectural modifications.

## Foundational Learning
- Mixture-of-Experts (MoE): A neural network architecture where multiple specialized "expert" networks are combined through a gating mechanism - needed for understanding EMoE's core mechanism, quick check: verify that MoE models use a router to select experts
- Epistemic Uncertainty: Uncertainty due to lack of knowledge or data, as opposed to aleatoric uncertainty from inherent randomness - needed to understand what EMoE is measuring, quick check: distinguish from data uncertainty
- Diffusion Models: Generative models that learn to denoise corrupted data through a Markov chain process - needed to understand the denoising context, quick check: identify forward and reverse processes
- CLIP Score: A metric measuring semantic similarity between text and image using a joint text-image embedding space - needed to understand quality evaluation, quick check: verify it measures text-image alignment
- Latent Space Representations: The compressed feature representations learned by neural networks - needed to understand where variance is measured, quick check: identify the dimensionality and meaning of latent vectors

## Architecture Onboarding

Component Map: Text Prompt -> MoE Router -> Expert Selection -> Latent Space Generation -> Variance Calculation -> Uncertainty Score

Critical Path: The denoising process where expert separation occurs is the critical component. EMoE operates by intercepting the denoising steps at an early stage (typically around 10-20% of total steps) to separate expert contributions before they are fully combined.

Design Tradeoffs: Early expert separation maximizes variance capture but may reduce individual expert quality. Late separation preserves expert quality but may reduce observable variance differences. The paper selects an intermediate point based on ablation studies.

Failure Signatures: Low variance across experts may indicate over-represented regions of the input space. High variance without corresponding quality degradation may suggest false positives. The method may struggle with non-MoE diffusion models.

First Experiments: (1) Measure variance across experts for identical prompts in different languages, (2) Compare uncertainty scores with human quality ratings for generated images, (3) Test sensitivity to denoising step selection and ensemble size

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The causal link between uncertainty and actual bias remains correlational rather than proven
- Experimental validation relies primarily on COCO dataset and CLIP-based metrics without exploring diverse real-world scenarios
- The study does not address computational overhead compared to baseline diffusion models or discuss scalability to larger, more diverse datasets

## Confidence
- Technical soundness of uncertainty estimation method: High
- Empirical validation of bias detection claims: Medium
- Generalizability across different diffusion model architectures: Low

## Next Checks
1. Conduct human evaluation studies to correlate EMoE uncertainty scores with perceived image quality and bias for under-represented groups
2. Test EMoE on datasets with known demographic imbalances to verify its ability to detect representational bias
3. Benchmark EMoE against alternative uncertainty estimation methods in terms of both accuracy and computational efficiency across multiple diffusion model architectures