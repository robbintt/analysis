---
ver: rpa2
title: Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents
arxiv_id: '2509.14382'
source_url: https://arxiv.org/abs/2509.14382
tags:
- action
- agent
- evaluation
- agents
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a modular evaluation framework for web agents\
  \ that decomposes agent pipelines into interpretable stages\u2014action prediction,\
  \ grounding, and action selection\u2014enabling fine-grained error analysis. Unlike\
  \ traditional end-to-end metrics, this approach reveals where failures occur and\
  \ how they propagate through the pipeline."
---

# Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents

## Quick Facts
- **arXiv ID:** 2509.14382
- **Source URL:** https://arxiv.org/abs/2509.14382
- **Reference count:** 19
- **Primary result:** Fine-grained evaluation framework exposes 70% action prediction bottleneck and >50% of failures as reasonable alternatives missed by rigid benchmarks.

## Executive Summary
This paper introduces a modular evaluation framework that decomposes web agent pipelines into interpretable stages—Action Prediction, Grounding, and Action Selection—enabling fine-grained error analysis beyond traditional end-to-end metrics. Applied to the SeeAct agent on the Mind2Web benchmark, the framework reveals critical bottlenecks: flawed initial reasoning in action prediction (max 70% accuracy) and difficulty disambiguating final actions during selection. The study also identifies benchmark rigidity as a significant error source, with over 50% of failures representing reasonable alternative solutions not captured by single-ground-truth annotations. By augmenting ground truth with mined alternatives, performance improved by up to 8.3%, demonstrating that fine-grained evaluation is essential for diagnosing and improving web agent reliability.

## Method Summary
The framework adapts the SeeAct pipeline with three input modes: Textual Grounding (MCQ-style element selection), Visual Clues (bounding boxes on screenshots), and Default (raw screenshot + HTML). It employs parallel batch processing (4 batches for VC/Def, 5 for TG), Intermediate Reasoning (chain-of-thought before selection), and LLM-based final action selection. A BGE-Small-en-v1.5 classifier (fine-tuned via SetFit on 800 pairs) matches free-form predictions to HTML elements when identifiers are not provided. The evaluation computes stage-level metrics—Relevant Element Accuracy, Action Prediction Accuracy, Grounding Accuracy (exact triplet match), and Action Selection Accuracy—using both First Viable heuristic and LLM Select strategies across multiple models.

## Key Results
- Action Prediction stage shows bottleneck with maximum 70% accuracy using GPT-4o
- >50% of "errors" are reasonable alternative actions not captured by single ground-truth annotations
- Performance improved by up to 8.3% when ground truth was augmented with mined alternatives
- Textual Grounding mode generates excess viable actions (avg. 3.91 vs. 2.5 for Default), overwhelming selection stage

## Why This Works (Mechanism)
The framework works by breaking down the complex web agent pipeline into three interpretable stages, allowing precise identification of where failures occur and how they propagate. By measuring accuracy at each stage independently, it reveals that early-stage errors (action prediction) cascade into downstream failures, while also exposing that many apparent "errors" are actually valid alternative solutions missed by rigid benchmarks. The parallel processing with Intermediate Reasoning helps isolate reasoning quality from execution capability.

## Foundational Learning
- **Action Prediction Accuracy** - Measures if model correctly identifies intended action type and element; needed to assess initial reasoning quality; quick check: verify ground-truth element appears in top-k candidates
- **Grounding Accuracy** - Evaluates exact triplet match (action, element, value); needed to measure precise execution capability; quick check: compare model predictions against full action triplets
- **Relevant Element Accuracy** - Checks if ground-truth element is in candidate set; needed to diagnose ranking model effectiveness; quick check: count how often ground-truth element appears in top-k
- **Action Selection Accuracy** - Measures ability to choose correct action from viable candidates; needed to assess disambiguation capability; quick check: vary number of viable actions to test selection robustness
- **LLM Select vs First Viable** - Compares heuristic vs. LLM-based selection strategies; needed to evaluate selection quality; quick check: run both methods on same viable candidate sets

## Architecture Onboarding
- **Component Map:** HTML/Screenshot -> Ranking Model (top-k) -> Intermediate Reasoning -> Action Prediction -> Grounding -> Action Selection -> Evaluation Metrics
- **Critical Path:** Ranking Model → Action Prediction → Action Selection (errors cascade downstream)
- **Design Tradeoffs:** Parallel batch processing for efficiency vs. loss of global context; Textual Grounding for clarity vs. excessive viable candidates; LLM Select for accuracy vs. computational cost
- **Failure Signatures:** Low RE Acc. indicates ranking model issues; low AP Acc. indicates reasoning problems; low AS Acc. with high viable count indicates selection stage overload
- **3 First Experiments:**
  1. Implement BGE-Small-en-v1.5 classifier training and validate on held-out test set
  2. Test ranking model on small Mind2Web subset to verify ground-truth element coverage
  3. Compare LLM Select performance with varying viable action counts (1, 2, 3, 4+)

## Open Questions the Paper Calls Out
- How can parallelized web agent architectures be modified to maintain global context and prevent over-generation of viable actions during selection?
- Do the specific bottlenecks found in Action Prediction and Action Selection generalize to other agent frameworks or non-VLM-based web agents?
- How does the ratio of "reasonable action" failures change when evaluating agents in dynamic, live-web environments compared to static benchmarks?

## Limitations
- Focus on single framework (SeeAct) and benchmark limits generalizability of findings
- Implementation details for prompt templates, ranking model, and alternative mining algorithm remain unspecified
- Static benchmark evaluation may underestimate true reasoning capabilities compared to live web environments

## Confidence
- Pipeline decomposition utility: High
- Action Prediction bottleneck: Medium
- Benchmark rigidity impact: Medium
- LLM Select vs First Viable: Low

## Next Checks
1. Implement the BGE-Small-en-v1.5 classifier training pipeline using the specified 800 HTML–text pairs and validate its accuracy on a held-out test set before using it for action prediction evaluation

2. Reproduce the element ranking and batching process on a small Mind2Web subset (5-10 tasks) to verify that ground-truth elements appear in the top-k candidates across different page layouts and action types

3. Systematically vary the number of viable actions presented to LLM Select (1, 2, 3, 4+) to quantify the degradation in selection accuracy and validate whether the observed 3.91 average is indeed problematic for the selection stage