---
ver: rpa2
title: Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large
  Language Models
arxiv_id: '2508.07753'
source_url: https://arxiv.org/abs/2508.07753
tags:
- bias
- causal
- hallucinations
- social
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study establishes the causal relationship between social bias
  and faithfulness hallucinations in large language models. Using structural causal
  modeling and bias interventions, the authors demonstrate that social biases significantly
  cause hallucinations in LLMs, with Anti-stereotype bias increasing hallucination
  rates while Pro-stereotype bias suppresses them.
---

# Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models

## Quick Facts
- **arXiv ID**: 2508.07753
- **Source URL**: https://arxiv.org/abs/2508.07753
- **Reference count**: 40
- **Key outcome**: Establishes causal link between social bias and faithfulness hallucinations in LLMs, with Anti-stereotype bias increasing hallucination rates while Pro-stereotype bias suppresses them

## Executive Summary
This study investigates the causal relationship between social bias and faithfulness hallucinations in large language models using structural causal modeling and bias interventions. The authors demonstrate that social biases significantly influence hallucination generation, with Anti-stereotype contexts increasing hallucination rates while Pro-stereotype contexts suppress them relative to neutral baselines. Using the Bias Intervention Dataset (BID) covering five social dimensions across 11,032 entries, experiments on seven mainstream LLMs reveal that 85 out of 105 instances show significant causal effects between bias and hallucinations. The research introduces "unfairness hallucinations" - a new type of hallucination primarily driven by social bias that is harder to detect due to high model confidence.

## Method Summary
The paper employs Structural Causal Modeling (SCM) with do-calculus interventions to establish causal relationships between social bias and faithfulness hallucinations. The Bias Intervention Dataset (BID) uses templates with social attribute slots that are manipulated across Pro-stereotype, Anti-stereotype, and Non-stereotype conditions while holding context constant. The method isolates the causal effect of bias by severing incoming causal edges from confounders through the do-operator. Hallucination rates are measured across bias conditions, and Individual Causal Effect (ICE) is computed as the difference in hallucination states between interventions. The Unified Causal Significance (UCS) statistic, derived from McNemar's test, quantifies the strength of causal relationships. The study tests seven mainstream LLMs using greedy decoding on multiple-choice QA tasks.

## Key Results
- Anti-stereotype bias increases hallucination rates while Pro-stereotype bias suppresses them relative to neutral baselines across all seven tested models
- 85 out of 105 instances revealed significant causal effects between bias and hallucinations
- Social biases have a significant causal effect exclusively on "unfairness hallucinations" (selection errors based on bias), not on "common hallucinations"
- Unfairness hallucinations exhibit confidence levels close to correct responses, making them harder to detect via standard uncertainty metrics

## Why This Works (Mechanism)

### Mechanism 1: Prior-Context Conflict Modulation
- **Claim:** Anti-stereotype contexts increase hallucination rates, while Pro-stereotype contexts suppress them relative to a neutral baseline
- **Mechanism:** LLMs internalize social biases as strong priors during pre-training. When input context contradicts these priors (Anti-stereotype), the model's learned bias overrides faithful extraction of input information, resulting in hallucination. Conversely, when input aligns with priors (Pro-stereotype), it reinforces faithful generation
- **Core assumption:** Social biases function as causal mechanisms that actively compete with context processing, rather than merely correlating with specific topics
- **Evidence anchors:** [abstract]: "Anti-stereotype bias increasing hallucination rates while Pro-stereotype bias suppresses them"; [section 5.2.1]: "All seven models show the trend: Anti-stereotype data have the highest hallucination rates, followed by Non-stereotype, and Pro-stereotype the lowest"
- **Break condition:** If an LLM is trained on data perfectly balanced for social attributes (removing the prior), the causal effect of Anti-stereotype inputs should theoretically vanish or invert

### Mechanism 2: Interventional Isolation via Do-Calculus
- **Claim:** Manipulating social attributes while holding context constant isolates the causal effect of bias from confounders like sentence structure or topic
- **Mechanism:** By applying the $do(B=x)$ operator (setting the bias state to Anti, Pro, or Non), the method severs incoming causal edges from confounders $Z$ (e.g., word frequency, position). This ensures that any observed change in the Hallucination state $H$ is causally attributable to the change in $B$
- **Core assumption:** The "Precision" criterion holds: swapping a social attribute (e.g., "girl" to "boy") does not inadvertently alter the semantic complexity or attention requirements of the sentence beyond the bias axis
- **Evidence anchors:** [abstract]: "...design bias interventions to control confounders... utilizing the Structural Causal Model (SCM)"; [section 3.2]: "Bias intervention involves manipulating the bias state... do(B=Anti)... removing the confounder"
- **Break condition:** If the intervention introduces new linguistic confounders (e.g., changing token rarity significantly), the isolation fails, and observed effects remain correlational

### Mechanism 3: Unfairness Hallucination Distinction
- **Claim:** Social bias specifically drives "unfairness hallucinations" (selecting a wrong entity based on bias) but has no significant causal effect on "common hallucinations" (random errors)
- **Mechanism:** Bias channels the model's failure mode specifically toward discriminatory outputs rather than random noise. These unfairness hallucinations are generated with high model confidence, making them harder to detect via standard uncertainty metrics
- **Core assumption:** The classification of errors into "unfairness" (biased wrong answer) vs. "common" (neutral wrong answer) reliably separates bias-driven failures from general reasoning failures
- **Evidence anchors:** [abstract]: "...defines a new type of hallucination called 'unfairness hallucination' that is primarily driven by social bias"; [section 5.3]: "...social biases have a significant causal effect exclusively on unfairness hallucinations... unfairness hallucinations exhibit confidence levels close to correct responses"
- **Break condition:** If a model generates "common" hallucinations that are actually driven by intersectional biases not captured by the dataset's social attributes, the scope of this mechanism is underestimated

## Foundational Learning

- **Concept: Structural Causal Model (SCM)**
  - **Why needed here:** To distinguish causality from correlation. The paper uses SCM (specifically the causal graph $Z \to B \to H$) to mathematically define how confounders $Z$ obscure the true relationship between Bias $B$ and Hallucination $H$
  - **Quick check question:** In the paper's causal graph, what does the "red cross" on the arrow $Z \to B$ represent? (Answer: The intervention/do-operator blocking the confounder's influence on the bias state)

- **Concept: Faithfulness Hallucination**
  - **Why needed here:** This is the specific failure mode being measured. Unlike factuality hallucinations (wrong about the world), faithfulness hallucinations are wrong about the *input context*. Understanding this distinction is required to interpret the BID dataset results correctly
  - **Quick check question:** If a model outputs "Paris" when the input text says "Berlin," is this a faithfulness or factuality hallucination? (Answer: Faithfulness, as it contradicts the source input)

- **Concept: Individual Causal Effect (ICE)**
  - **Why needed here:** This is the primary metric for quantifying the mechanism. It measures the difference in hallucination state when a specific input is "treated" with different bias interventions (e.g., $H|do(B=Anti) - H|do(B=Pro)$)
  - **Quick check question:** If a model hallucinates on an Anti-stereotype input but not on the equivalent Pro-stereotype input, what is the ICE value? (Answer: Non-zero, indicating a causal effect)

## Architecture Onboarding

- **Component map:** BID Generator -> Intervention Module -> Inference Engine -> Causal Calculator
- **Critical path:**
  1. Designing templates where social attributes are the *only* variable changing (satisfying "Precision")
  2. Generating the triplet of interventions (Pro, Anti, Non) for each scenario
  3. Measuring the binary hallucination state $H$ (0 or 1) for each intervention to compute ICE
- **Design tradeoffs:**
  - **Template vs. Natural Text:** The paper uses templates (BID) rather than natural text to strictly control confounders. *Tradeoff:* High internal validity for causal claims, but potentially lower ecological validity regarding how bias manifests in organic, messy text
  - **Binary Hallucination:** The model relies on a binary correctness check. *Tradeoff:* Simplifies causal computation but may miss subtle degrees of unfaithfulness
- **Failure signatures:**
  - **High Baseline Hallucination:** If the "Non-stereotype" (neutral) condition yields high hallucinations, the template itself is likely confusing or flawed, independent of bias
  - **Negative UCS Values:** While the paper expects Anti-stereotype to increase hallucinations, consistently negative values in specific bias types might indicate the model is over-correcting or the "attested bias" definition is inverted for that context
  - **Low Effect Significance:** If UCS values are not statistically significant ($p > 0.05$), the model may be robust to that specific bias type, or the dataset size is insufficient
- **First 3 experiments:**
  1. **Baseline Rate Check:** Run the BID dataset on the target model without interventions to establish raw hallucination rates across bias states
  2. **Pairwise Intervention Test:** Generate $Pro/Anti$ pairs and compute ICE to verify if the model hallucinates more when the stereotype is reversed
  3. **Confidence Analysis:** Compare the log-probability (confidence) of "Unfairness Hallucination" tokens vs. "Common Hallucination" tokens to see if the model is "sure" about its biased errors

## Open Questions the Paper Calls Out
- Can the Structural Causal Model (SCM) and intervention framework be applied to quantify the causal effects of non-social factors (e.g., attention mechanisms or data imbalance) on hallucinations?
- How can "unfairness hallucinations" be effectively detected given their high model confidence?
- Do the causal effects of bias on hallucination persist in natural, non-template text distributions?

## Limitations
- Template-based interventions may not fully capture how social biases manifest in naturalistic text
- Binary hallucination classification system (unfairness vs. common) represents a significant methodological simplification
- Study focuses on multiple-choice QA format, limiting generalizability to open-ended generation tasks
- BID dataset covers only five social dimensions, potentially missing intersectional or compound bias effects

## Confidence
- **High Confidence:** Directional finding that Anti-stereotype contexts increase hallucination rates relative to Pro-stereotype contexts (supported by consistent results across all seven tested models)
- **Medium Confidence:** Causal isolation mechanism via SCM and do-calculus (methodologically sound but dependent on the Precision assumption holding perfectly)
- **Medium Confidence:** Existence of "unfairness hallucinations" as a distinct category (novel finding but requires external validation on different task types)

## Next Checks
1. Apply the BID intervention methodology to open-ended generation tasks (story completion, summarization) rather than just multiple-choice QA to test generalizability of the causal mechanism
2. Extend the BID dataset to include compound social attributes (e.g., age-gender combinations) to test whether bias effects compound or interact in unexpected ways
3. Test the causal effects across different model families (transformers vs. recurrent networks vs. hybrid architectures) to determine whether the observed bias-hallucination relationship is architectural or learned behavior