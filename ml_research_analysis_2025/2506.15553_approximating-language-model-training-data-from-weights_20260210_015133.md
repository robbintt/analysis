---
ver: rpa2
title: Approximating Language Model Training Data from Weights
arxiv_id: '2506.15553'
source_url: https://arxiv.org/abs/2506.15553
tags:
- data
- dataset
- training
- arxiv
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes and addresses the problem of recovering training
  data from language model weights, specifically focusing on the scenario where only
  the final fine-tuned model and the base model are available. The authors develop
  SELECT, a gradient-based approach that greedily selects datapoints from a large
  public corpus whose gradients align with the direction between the base and fine-tuned
  models in parameter space.
---

# Approximating Language Model Training Data from Weights

## Quick Facts
- arXiv ID: 2506.15553
- Source URL: https://arxiv.org/abs/2506.15553
- Reference count: 17
- Authors: John X. Morris; Junjie Oscar Yin; Woojeong Kim; Vitaly Shmatikov; Alexander M. Rush
- Primary result: Gradient-based SELECT method recovers training data from model weights, improving accuracy from 65% to 80% on AG News compared to random selection

## Executive Summary
This paper addresses the challenge of approximating the training data used to fine-tune a language model, given only the base and fine-tuned model weights. The authors propose SELECT, a method that greedily selects datapoints from a public corpus whose gradients align with the direction of model updates in parameter space. SELECT significantly outperforms baselines like random selection and top-k gradient methods, demonstrating strong results even when the true training data is not leaked into the seed corpus. The method is computationally efficient by using only last-layer gradients and random projections, and it achieves near-expert performance on classification and supervised fine-tuning tasks.

## Method Summary
SELECT operates by first autolabeling a seed corpus using the fine-tuned model, then creating synthetic checkpoints via linear interpolation between base and fine-tuned models. It computes per-example last-layer gradients at each synthetic checkpoint, projects them to lower dimensions using Johnson-Lindenstrauss random projections, and greedily selects examples whose cumulative gradient maximally aligns with the parameter difference vector. The selected data is then used to retrain a model, achieving strong performance on classification (AG News, DBPedia, IMDB) and supervised fine-tuning (MSMARCO) tasks using GPT-2 Medium and LLAMA models.

## Key Results
- SELECT improves AG News classification accuracy from 65% to 80% compared to random selection's 65%
- On MSMARCO SFT, SELECT reduces perplexity from 3.3 to 2.3 versus random selection's 3.3
- SELECT outperforms top-k gradient selection and likelihood-based baselines across all evaluated tasks
- The method achieves strong results even when none of the true training data is present in the seed corpus

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Checkpoints
- Claim: Synthetic checkpoints approximate missing intermediate training states using linear interpolation between base and fine-tuned models.
- Mechanism: The method creates pseudo-checkpoints $\hat{\theta}_j = (1 - j/P)\theta_0 + (j/P)\theta_f$ for $j \in [1, P]$, providing multiple gradient alignment targets that simulate a training trajectory.
- Core assumption: Linear interpolation in weight space produces meaningful intermediate states that approximate true SGD trajectories.
- Evidence anchors:
  - [section 4.1] "We create synthetic checkpoints by linearly interpolating between the initial and final model: $\hat{\theta}_j = ( j/P \theta_0) + (1 - j/P \theta_f )$"
  - [corpus] No direct corpus evidence on linear interpolation for trajectory matching in language models.
- Break condition: If the true optimization path is highly non-linear or the loss landscape has sharp curvature, linear interpolation produces gradient targets misaligned with actual training dynamics.

### Mechanism 2: Greedy Submodular Selection
- Claim: Greedy submodular selection produces a batch whose cumulative gradient aligns with the parameter difference vector.
- Mechanism: Rather than selecting top-k individual examples, the algorithm iteratively picks $i^* = \arg\max(\hat{G} \cdot \hat{\theta}_t^T)$ and updates the running gradient sum.
- Core assumption: The batch objective $\sum_{x \in B} \nabla \ell(x; \theta_0) \cdot (\theta_f - \theta_0)$ exhibits submodularity.
- Evidence anchors:
  - [section 4] "The batch search objective is submodular because it exhibits the diminishing returns property: the marginal gain of adding a new datapoint decreases as the batch grows."
  - [corpus] No corpus evidence specifically on submodular gradient selection for LLM data recovery.
- Break condition: If gradients across datapoints are highly correlated or the seed distribution lacks diversity, greedy selection may still produce homogeneous batches.

### Mechanism 3: Johnson-Lindenstrauss Projection
- Claim: Random projection via Johnson-Lindenstrauss preserves gradient inner products for efficient large-scale selection.
- Mechanism: High-dimensional gradients $G \in \mathbb{R}^{|D| \times |\nabla \ell|}$ are projected to $\hat{G} \in \mathbb{R}^{|D| \times d}$ with $d \ll |\nabla \ell|$ (e.g., 4096), reducing storage while preserving dot products used in selection.
- Core assumption: The projection dimension $d$ is sufficient to preserve discriminative alignment signals.
- Evidence anchors:
  - [section 4.3] "We leverage the classic Johnsonâ€“Lindenstrauss lemma... which guarantees that a set of points in $\mathbb{R}^n$ can be mapped to a lower-dimensional space $\mathbb{R}^k$ while preserving inner products with high probability."
  - [corpus] No corpus evidence on JL projections specifically for gradient-based text data selection.
- Break condition: If $d$ is too small, inner-product preservation degrades, causing selection based on distorted gradient similarities.

## Foundational Learning

- **Submodular optimization and greedy approximation**
  - Why needed here: The selection algorithm's theoretical guarantee rests on submodularity; understanding diminishing returns explains why greedy works.
  - Quick check question: Why does the marginal utility of adding a datapoint decrease as the selected batch grows larger?

- **Gradient alignment in parameter space**
  - Why needed here: The core selection criterion is the dot product between a datapoint's gradient and the direction from base to fine-tuned model.
  - Quick check question: What does $\nabla \ell(x; \theta_0) \cdot (\theta_f - \theta_0) > 0$ indicate about datapoint $x$?

- **Johnson-Lindenstrauss lemma**
  - Why needed here: This justifies the dramatic dimensionality reduction in gradient storage without breaking the selection logic.
  - Quick check question: What is the relationship between projection dimension and the probability of preserving pairwise distances?

## Architecture Onboarding

- **Component map**: Autolabeling module -> Synthetic checkpoint generator -> Per-example gradient computer -> Gradient projector -> Greedy selector
- **Critical path**: Per-example gradient computation over the full seed set is the dominant cost. Memory scales with $|D| \times d$ for the gradient store; compute scales with $|D| \times P \times$ forward/backward passes.
- **Design tradeoffs**:
  - **Projection dimension ($d$)**: Higher $d$ preserves more information but increases storage quadratically. Paper uses 4096; ablations show 512 still beats random.
  - **Number of synthetic checkpoints ($P$)**: More checkpoints improve trajectory fidelity but multiply gradient computation cost.
  - **Last-layer vs. full-model gradients**: Last-layer is cheaper but may miss signal from lower layers.
- **Failure signatures**:
  - **Selected data is highly redundant**: Suggests projection dimension too low or greedy selection not compensating for correlation.
  - **Performance worse than random seed**: Likely pseudo-labels from $\theta_f$ are poor quality for the seed domain, or seed distribution mismatch.
  - **Memory overflow**: Per-example gradient storage for millions of documents exceeds RAM; reduce batch size or projection dimension.
- **First 3 experiments**:
  1. **Reproduce AG News baseline**: Run SELECT vs. random/top-k with 10K seed, 1K selected, measuring accuracy and OT distance to verify implementation.
  2. **Projection dimension ablation**: Sweep $d \in \{512, 1024, 2048, 4096\}$ and plot accuracy vs. storage/runtime.
  3. **Seed domain sensitivity**: Test SELECT with different seed corpora (Natural Questions vs. MSMARCO vs. Newsgroup) on the same target task to assess robustness.

## Open Questions the Paper Calls Out
- None explicitly stated in the paper.

## Limitations
- The method assumes linear interpolation between checkpoints accurately represents the training trajectory, which may not hold for complex optimization paths.
- Computational costs scale poorly with corpus size due to per-example gradient computation at multiple synthetic checkpoints.
- Performance degrades when seed corpus distribution is mismatched with true training data distribution.
- The paper lacks empirical validation of submodularity and trajectory fidelity assumptions.

## Confidence
- **High confidence**: SELECT improves over random and top-k selection baselines in both classification and SFT tasks, as measured by accuracy and perplexity.
- **Medium confidence**: The greedy submodular selection mechanism is correctly implemented and produces more diverse data than naive methods, but the extent to which diminishing returns drive performance is not quantified.
- **Low confidence**: The claim that synthetic checkpoints accurately simulate training trajectories and that Johnson-Lindenstrauss projections preserve discriminative gradient alignment signals lacks direct empirical support.

## Next Checks
1. **Trajectory fidelity test**: Measure the angle between $\theta_f - \theta_0$ and the actual gradient of a small finetuned model at multiple points along the training curve. Compare to the average alignment achieved by synthetic checkpoints to quantify interpolation error.
2. **Greedy optimality bound**: Compute the marginal gain of each selection step and compare to the theoretical bound for submodular maximization. Also, measure class balance and redundancy in the selected set to confirm diversity.
3. **Seed domain mismatch robustness**: Run SELECT with seed corpora from completely different domains (e.g., Reddit, PubMed) on the same target tasks. Measure both downstream performance and OT distance to the true training set to assess how far the method degrades when the seed distribution is mismatched.