---
ver: rpa2
title: Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels
arxiv_id: '2511.15496'
source_url: https://arxiv.org/abs/2511.15496
tags:
- low-light
- images
- image
- intensity
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multi-Illumination Low-Light (MILL) dataset
  to address the lack of radiance diversity in existing low-light enhancement benchmarks.
  Unlike prior datasets that capture images under single low-light conditions, MILL
  systematically captures 11 images of each scene at varying illumination levels (0%
  to 100% in 10% intervals) with fixed camera settings, paired with precise illuminance
  measurements.
---

# Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels

## Quick Facts
- arXiv ID: 2511.15496
- Source URL: https://arxiv.org/abs/2511.15496
- Reference count: 40
- Primary result: Introduces MILL dataset and achieves up to 10 dB PSNR improvement for DSLR and 2 dB for smartphone low-light enhancement

## Executive Summary
This paper addresses the critical gap in low-light image enhancement benchmarks by introducing the Multi-Illumination Low-Light (MILL) dataset, which systematically captures scenes at 11 different illumination levels with precise illuminance measurements. The dataset reveals that existing state-of-the-art methods perform inconsistently across intensity levels, often excelling at extreme low-light but degrading on moderate inputs. To address this, the authors propose two novel loss terms that leverage MILL's multi-level structure to disentangle illumination and scene content in latent features, significantly improving enhancement robustness across the full brightness spectrum.

## Method Summary
The authors introduce the Multi-Illumination Low-Light (MILL) dataset containing 1,100 images captured at 11 intensity levels (0-100% in 10% intervals) with fixed camera settings. They modify the Retinexformer architecture by adding two novel loss terms: an intensity prediction loss that constrains the first latent channel to encode illumination level, and a scene content loss that uses triplet-based metric learning to create illumination-invariant scene representations. This disentanglement approach enables models to learn brightness-appropriate enhancement rather than overfitting to single degradation profiles, achieving substantial PSNR improvements across diverse illumination scenarios.

## Key Results
- MILL dataset reveals significant performance variations across intensity levels, with some methods degrading on moderately low-light inputs
- Proposed method achieves up to 10 dB PSNR improvement for DSLR and 2 dB for smartphone on Full HD images
- Models trained on MILL show improved robustness across the full brightness spectrum, reducing oversaturation artifacts at moderate input levels
- Benchmarking on MILL exposes specialization issues in state-of-the-art methods that perform well on extreme low-light but poorly on moderate cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining the first latent channel to explicitly encode illumination intensity enables the network to develop a calibrated representation of lighting conditions that guides appropriate enhancement strength.
- Mechanism: The intensity prediction loss (L_i = ||Z^I - I^in||_1) forces the encoder to map the first feature channel to the normalized illumination level. This explicit supervision may prevent the network from embedding illumination information throughout all features, reserving remaining channels for scene content.
- Core assumption: A single channel has sufficient capacity to encode meaningful illumination information, and the network will naturally distribute scene content to other channels once intensity is explicitly supervised.
- Evidence anchors:
  - [abstract] "disentangle illumination and scene information in latent features"
  - [section] "We constrain the first latent feature channel to predict the normalized intensity value of the scene" (Section 4.1)
  - [corpus] Weak - no direct corpus support for this specific single-channel disentanglement approach.
- Break condition: If illumination varies spatially within a scene (e.g., partial shadows, mixed lighting) beyond what a global scalar can represent, or if the network finds alternative pathways to encode intensity that don't disentangle cleanly.

### Mechanism 2
- Claim: Triplet-based metric learning on latent features (excluding the intensity channel) encourages illumination-invariant scene representations that remain consistent across brightness levels.
- Mechanism: The scene content loss pulls together latent representations of the same scene at different illumination levels (positive pairs) while pushing apart different scenes at the same brightness (negative pairs), with margin m=1. This creates pressure for channels 2-C to encode scene identity rather than lighting.
- Core assumption: The positive samples (same scene, different level) share sufficient structural content for the network to learn a meaningful invariance, and the controlled capture ensures objects haven't moved between shots.
- Evidence anchors:
  - [abstract] "scene content loss that leverage MILL's multi-level structure"
  - [section] "encourages images of the same scene captured under different illumination levels to have similar latent representations (excluding the intensity channel)" (Section 4.2)
  - [corpus] Wavelet-based decoupling paper mentions disentangling degradation factors but uses a fundamentally different wavelet-domain approach.
- Break condition: If the triplet sampling strategy fails to provide informative negatives (e.g., different scenes share similar objects), or if the margin is too small/large for the feature space scale.

### Mechanism 3
- Claim: Training on systematically varied illumination levels (rather than a single low-light condition) enables models to learn brightness-appropriate enhancement rather than overfitting to a fixed degradation profile.
- Mechanism: The MILL dataset provides 10 input levels per scene (plus ground truth), exposing the model to the continuous spectrum of low-light conditions. This multi-level exposure appears to reduce the oversaturation artifacts observed when single-level models encounter moderate inputs.
- Core assumption: Real-world low-light scenarios span a continuous intensity range, and the controlled 10% intervals generalize to natural lighting variation.
- Evidence anchors:
  - [abstract] "reveal significant performance variations across intensity levels, with some methods performing well on severely underexposed images but degrading on moderately low-light inputs"
  - [section] "models trained on fixed brightness levels fail to generalize across different intensities... Oversaturation in the output increases proportionally with input brightness" (Section 3, Table 1)
  - [corpus] HVI paper addresses color space issues but doesn't systematically address intensity-level generalization.
- Break condition: If the controlled studio lighting (programmable lights, no external sources) doesn't transfer to real-world mixed or directional lighting, or if models lack capacity to learn level-appropriate calibration.

## Foundational Learning

- Concept: **Retinex theory (reflectance-illumination decomposition)**
  - Why needed here: The proposed losses extend Retinex principles by explicitly supervising illumination in one latent channel while preserving scene content in others, rather than relying on implicit decomposition.
  - Quick check question: Why does separating reflectance (scene content) from illumination help with low-light enhancement—what would happen if you just brightened the raw pixel values?

- Concept: **Triplet loss and contrastive learning**
  - Why needed here: The scene content loss uses triplet formulation (anchor/positive/negative) to learn illumination-invariant representations; understanding margin selection and sampling strategy is essential for debugging convergence.
  - Quick check question: In a triplet loss with margin m, what happens if the anchor-negative distance is already much larger than the anchor-positive distance + m?

- Concept: **RAW vs. sRGB image formation**
  - Why needed here: MILL provides RAW captures with fixed camera settings (ISO 100, fixed aperture/shutter) to isolate illumination variation; understanding sensor physics helps interpret why smartphone results differ from DSLR (2 dB vs. 10 dB gains).
  - Quick check question: Why would RAW images be preferable for controlled low-light experiments over camera-processed JPEGs?

## Architecture Onboarding

- Component map:
  - Input pipeline: RAW images at 11 intensity levels per scene; MILL-s (600×400 downsampled) or MILL-f (Full HD patches)
  - Encoder: Retinexformer encoder (Restormer-based transformer) extracts latent features Z ∈ R^(H×W×C) at bottleneck
  - Bottleneck supervision: Channel 0 → intensity prediction (L1 vs. normalized level); Channels 1 to C-1 → scene content (triplet loss)
  - Decoder: Reconstructs enhanced RGB; supervised by L1 reconstruction loss against ground truth
  - Combined objective: L = L_re + L_i + L_s

- Critical path:
  1. Sample triplet: query image at level L_q, positive sample (same scene, different level), negative sample (different scene, same level as query)
  2. Forward pass through Retinexformer encoder to obtain bottleneck features
  3. Extract Z^I (channel 0) → compute intensity prediction loss against normalized level value
  4. Extract Z^(1:C-1) → compute triplet loss with positive and negative samples
  5. Decoder output → compute L1 reconstruction loss against Level 11 ground truth
  6. Backpropagate combined loss

- Design tradeoffs:
  - Global vs. spatial intensity encoding: The intensity prediction uses a scalar level per image; may not capture intra-scene illumination variation (shadows, localized light sources)
  - Fixed margin m=1: Works for MILL feature scales but may require retuning for different architectures or normalization schemes
  - DSLR vs. smartphone gap: 10 dB improvement (DSLR) vs. 2 dB (smartphone) suggests sensor quality differences may affect the disentanglement mechanism's effectiveness
  - Computational cost: Full-HD training requires 9 non-overlapping patches per image, increasing effective dataset size to 5,500 samples

- Failure signatures:
  - Oversaturation at moderate inputs: Models trained on single-level data (e.g., LoLv1) produce washed-out or oversaturated outputs when input brightness increases (Figure 2, Figure 6 second row sky region)
  - Performance inversion across levels: RUAS, FourLLIE, SCI show degraded PSNR at higher intensity levels (Level 9 worse than Level 1 in Table 2), indicating specialization to extreme low-light at expense of moderate cases
  - Color artifacts in textured regions: Without proper disentanglement, competing methods show noise and color shifts in complex textures (Figure 4, mug interior and shadow regions)
  - Uniform vs. localized error patterns: Scene content loss alone produces spatially uniform ΔE_76 errors; intensity loss alone concentrates errors; combination achieves both global and local improvement (Figure 5)

- First 3 experiments:
  1. **Loss ablation on MILL-f**: Train I-Retinexformer (intensity loss only), S-Retinexformer (scene loss only), and full method; compare PSNR_L and ΔE_76 to quantify contribution of each component and verify the 10 dB claim.
  2. **Per-level robustness analysis**: Plot PSNR_L vs. intensity level (1-10) for baseline Retinexformer vs. proposed method on DSLR and smartphone splits; identify which levels benefit most and whether the method closes the moderate-low-light performance gap.
  3. **Cross-dataset generalization**: Evaluate models trained on MILL (controlled lighting) on outdoor datasets (DICM, SICE) to test whether multi-level training transfers to natural scenes with mixed illumination and non-uniform lighting.

## Open Questions the Paper Calls Out
None

## Limitations
- The single-channel intensity encoding may not capture spatially varying illumination within scenes (e.g., shadows, localized light sources), potentially limiting generalization to real-world mixed lighting conditions.
- The controlled studio environment with programmable lights differs significantly from natural outdoor scenes with directional and ambient lighting.
- The 2 dB smartphone improvement (vs. 10 dB for DSLR) suggests sensor quality differences may affect the disentanglement mechanism's effectiveness.

## Confidence

- **High confidence**: Performance improvements on the controlled MILL dataset (10 dB PSNR for DSLR, 2 dB for smartphone) are well-supported by systematic testing on 1,100 images with precise illuminance measurements.
- **Medium confidence**: The mechanism by which single-channel intensity supervision enables disentanglement (avoiding information leakage to other channels) is plausible but not rigorously proven—networks might find alternative pathways to encode lighting information.
- **Medium confidence**: The claim that multi-level training enables brightness-appropriate enhancement is supported by benchmarking but requires validation on real-world datasets with uncontrolled lighting variations.

## Next Checks

1. Test cross-dataset generalization by evaluating models trained on MILL against outdoor low-light datasets (DICM, SICE) to assess performance in natural scenes with non-uniform illumination and mixed lighting.

2. Investigate spatial illumination variation by modifying the intensity prediction loss to use spatially varying illumination maps (e.g., from HDR captures or multiple exposures) rather than global scalar values.

3. Conduct ablation studies on margin selection and triplet sampling strategies in the scene content loss to determine sensitivity to hyperparameters and ensure the contrastive learning component is optimized.