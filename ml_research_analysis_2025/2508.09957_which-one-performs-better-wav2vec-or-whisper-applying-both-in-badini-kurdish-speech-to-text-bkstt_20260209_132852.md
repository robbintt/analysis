---
ver: rpa2
title: Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish
  Speech to Text (BKSTT)
arxiv_id: '2508.09957'
source_url: https://arxiv.org/abs/2508.09957
tags:
- training
- figure
- speech
- data
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the lack of Speech-to-Text (STT) systems
  for the Badini Kurdish dialect by developing and evaluating language models using
  Wav2Vec2-Large-XLSR-53 and Whisper-small frameworks. The study collected approximately
  15 hours of speech data from eight children's books narrated by six speakers, resulting
  in 19,193 segments.
---

# Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)

## Quick Facts
- arXiv ID: 2508.09957
- Source URL: https://arxiv.org/abs/2508.09957
- Reference count: 3
- Primary result: Wav2Vec2-Large-XLSR-53 achieved 90.38% character accuracy vs 82.67% for Whisper-small

## Executive Summary
This research addresses the lack of Speech-to-Text (STT) systems for the Badini Kurdish dialect by developing and evaluating language models using Wav2Vec2-Large-XLSR-53 and Whisper-small frameworks. The study collected approximately 15 hours of speech data from eight children's books narrated by six speakers, resulting in 19,193 segments. After preprocessing and alignment, both models were fine-tuned on the dataset. The Wav2Vec2-Large-XLSR-53 model significantly outperformed Whisper-small, achieving 90.38% character-level accuracy compared to 82.67% for Whisper, and 65.45% word-level accuracy versus 53.17%. These results demonstrate that even with limited data, effective STT systems can be developed for low-resource languages like Badini Kurdish, paving the way for further enhancements and broader accessibility of language technologies for Kurdish speakers.

## Method Summary
The study fine-tuned Wav2Vec2-Large-XLSR-53 and Whisper-small on approximately 15 hours of Badini Kurdish speech data (19,193 segments, 25,221 words) collected from eight children's books narrated by six speakers. Audio was preprocessed using Adobe Audition noise reduction, segmented into 1-12 second clips, and aligned with transcriptions. Custom character-level vocabularies were created for both models since pre-trained tokenizers were unavailable for Badini. Training occurred on Google Colab Pro with Tesla T4 GPU using incremental batches (5 batches of ~3 hours each) with gradient accumulation to manage memory constraints. Wav2Vec2 used batch size 2, gradient accumulation 8, learning rate 3e-4, and gradient checkpointing, while Whisper used batch size 1, gradient accumulation 4, learning rate 5e-6, and generation length limit 32 tokens.

## Key Results
- Wav2Vec2-Large-XLSR-53 achieved 90.38% character-level accuracy versus Whisper-small's 82.67%
- Wav2Vec2-Large-XLSR-53 achieved 65.45% word-level accuracy versus Whisper-small's 53.17%
- Both models successfully processed the 19,193 speech segments after custom vocabulary creation and incremental training

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Transfer from Multilingual Pre-Training
Wav2Vec2-Large-XLSR-53's multilingual pre-training on 53 languages enables effective fine-tuning on Badini Kurdish with limited data. The model's pre-training provides acoustic representations that transfer to Badini through fine-tuning, capturing shared phonetic patterns. The CTC algorithm aligns these representations to character sequences.

### Mechanism 2: Character-Level Vocabulary with CTC Alignment
Custom character-level vocabulary creation improves alignment accuracy for low-resource languages lacking pre-trained tokenizers. Since no pre-trained Badini tokenizer exists, creating a character-level vocabulary with special tokens enables CTC to map audio features to Kurdish characters directly, reducing vocabulary size and improving generalization with limited data.

### Mechanism 3: Incremental Training with Gradient Accumulation for Memory Constraints
Dividing dataset into batches combined with gradient accumulation enables fine-tuning large transformer models on limited GPU memory without sacrificing convergence. The incremental training strategy (5 batches of ~3 hours each) with gradient accumulation (8 steps for Wav2Vec2, 4 for Whisper) simulates larger batch sizes while fitting in T4 GPU memory.

## Foundational Learning

- **Connectionist Temporal Classification (CTC)**: Why needed here: Wav2Vec2's CTC loss function aligns variable-length audio to text without forced alignment; understanding the blank token and collapse modes is essential for debugging. Quick check: Why does CTC require a blank token, and what transcription behavior would indicate "CTC collapse" during Badini training?

- **Character Error Rate (CER) vs Word Error Rate (WER)**: Why needed here: The paper reports CER accuracy (90.38%) significantly higher than WER accuracy (65.45%); this gap indicates word-level challenges. Quick check: Given CER of 9.62% and WER of 34.55% for Wav2Vec2, what does this suggest about the model's handling of Badini word boundaries and morphology?

- **Self-Supervised Learning (SSL) for Speech**: Why needed here: Wav2Vec2-XLSR leverages SSL pre-training on unlabeled audio; understanding contrastive learning objectives clarifies why it outperforms Whisper on limited fine-tuning data. Quick check: How does Wav2Vec2's contrastive pre-training objective differ from Whisper's supervised multilingual training, and why might SSL transfer better to truly low-resource languages?

## Architecture Onboarding

- Component map:
```
Raw Audio (44.1kHz → 16kHz resampling)
    ↓
Preprocessing (Adobe Audition noise reduction, segmentation)
    ↓
Feature Extraction (Wav2Vec2 CNN encoder: 7-layer temporal conv)
    ↓
Transformer Encoder (12 layers, 1024 dim, 16 attention heads)
    ↓
CTC Projection Layer → Character Logits
    ↓
CTC Decoding with Custom Kurdish Vocabulary (36 chars + special tokens)
    ↓
Text Output (character sequence → word sequence via | delimiter)
```

- Critical path:
  1. **Audio quality**: Noise reduction profile capture must isolate background noise without distorting speech formants; segmentation at sentence boundaries (1-12 second segments)
  2. **Vocabulary construction**: Extract all unique Kurdish characters from transcriptions; map to indices 0-40; verify coverage of rare characters (ې, ە, ۆ)
  3. **Fine-tuning configuration**: Learning rate 3e-4 (higher than default for low-resource), gradient accumulation 8 steps, warmup 500 steps
  4. **Checkpoint management**: 44 checkpoints generated due to Colab interruptions; implement 4-checkpoint strategy (pre-training, sync, final, test)

- Design tradeoffs:
  - **Wav2Vec2-XLSR vs Whisper-small**: Wav2Vec2's character-level CTC better suits morphologically complex Kurdish with ~15 hours data; Whisper's BPE tokenizer lacks Badini subword coverage
  - **Batch size vs gradient accumulation**: Physical batch size 2 with 8-step accumulation (effective batch 16) fits T4 GPU; larger physical batches OOM
  - **Mixed precision**: fp16 enabled for Wav2Vec2 (speed), disabled for Whisper (numerical stability during sequence generation)
  - **Learning rate**: 3e-4 aggressive for low-resource convergence vs 5e-6 conservative for Whisper stability

- Failure signatures:
  - **CTC blank mode**: Output is all blank tokens or repetitive single character → learning rate too high or vocabulary mismatch
  - **High CER, low WER**: Character predictions correct but word boundaries wrong → check delimiter token `|` placement in ground truth
  - **Validation loss diverges after epoch 10**: Overfitting to small dataset → implement early stopping or data augmentation
  - **Unknown token accumulation**: Frequent `<unk>` in output → vocabulary missing Kurdish-specific characters

- First 3 experiments:
  1. **Vocabulary coverage audit**: Extract character frequency from 19K transcriptions; verify all characters appear in vocabulary; test on 30 held-out segments expecting <2% `<unk>` rate
  2. **Learning rate validation**: Train Wav2Vec2 on 1-hour subset with learning rates [1e-4, 3e-4, 1e-3]; plot validation loss curves; confirm 3e-4 achieves convergence in ~5K steps without divergence
  3. **Checkpoint recovery test**: Simulate training interruption at step 5000; resume from checkpoint; verify loss continues from saved value and final model matches uninterrupted training quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance generalize to the diverse accents of Badini speakers from different geographic regions outside the specific narrator sample used in this study?
- Basis in paper: [explicit] The conclusion explicitly states, "In the future, we are interested in expanding the dataset and including a wider range of narrators to cover more inclusive accents from other Badini-speaking cities and areas."
- Why unresolved: The current study utilized a limited narrator pool (six individuals), which may not capture the full phonetic variability of the dialect spoken across different regions.
- What evidence would resolve it: Fine-tuning the model on an expanded dataset containing narrators from various Badini-speaking cities and evaluating the change in Word Error Rate (WER) across these specific demographics.

### Open Question 2
- Question: Would a larger Whisper model (e.g., Whisper-medium or Whisper-large) outperform Wav2Vec2-Large, given that this study only compared Wav2Vec2-Large against Whisper-small?
- Basis in paper: [inferred] The study compares "Wav2Vec2-Large-XLSR-53" against "Whisper-small," creating a disparity in model size (Large vs. Small). The authors attribute performance differences to the frameworks, but the "small" size of Whisper may have been a limiting factor independent of the architecture.
- Why unresolved: The paper concludes Wav2Vec is "significantly more accurate," but it remains unclear if this is an inherent property of the model type or a result of comparing a large pre-trained model against a smaller one.
- What evidence would resolve it: A follow-up experiment fine-tuning Whisper-medium or Whisper-large on the same 15-hour Badini corpus and comparing the results against the current Wav2Vec2 baseline.

### Open Question 3
- Question: Can the implementation of an automatic correction module effectively reduce the character-level error rate, particularly regarding the vowel omission issues noted in native speaker transcriptions?
- Basis in paper: [explicit] The authors mention, "We are also interested in an automatic correction module that is set to be incorporated to correct frequent errors in the transcription."
- Why unresolved: While the acoustic model achieved 90.38% character accuracy, the authors noted specific issues with vowel elisions in native speech. It is undetermined if text-based post-processing can rectify these systematic acoustic errors.
- What evidence would resolve it: Development and integration of a language model-based correction module showing a statistically significant reduction in CER (Character Error Rate) when applied to the raw model outputs.

## Limitations
- Limited dataset size (15 hours from only six narrators) constrains generalizability to broader Badini Kurdish speech patterns
- Custom character-level vocabulary may not capture all linguistic nuances of the dialect
- Performance comparison may be biased by comparing a large model (Wav2Vec2-Large) against a smaller one (Whisper-small)

## Confidence

**High Confidence Claims:**
- Wav2Vec2-Large-XLSR-53 outperformed Whisper-small on both CER (90.38% vs 82.67%) and WER (65.45% vs 53.17%) metrics
- Custom character-level vocabulary creation was necessary and effective for Badini Kurdish
- Incremental training with gradient accumulation successfully managed GPU memory constraints

**Medium Confidence Claims:**
- Cross-lingual transfer from XLSR-53's multilingual pre-training significantly contributed to Wav2Vec2's superior performance
- The 5-batch incremental training strategy effectively prevented overfitting on limited data
- The specific learning rate choices (3e-4 for Wav2Vec2, 5e-6 for Whisper) were optimal for this dataset

**Low Confidence Claims:**
- Long-term generalization of these models to diverse Badini Kurdish speakers beyond the original six narrators
- Whether similar performance gains would be observed with larger datasets or different dialect variants
- The exact contribution of each architectural component to the performance differential

## Next Checks

1. **Vocabulary Coverage Validation**: Extract all unique characters from the full 19,193-segment transcription set and verify that the custom vocabulary includes every character with sufficient frequency. Test on 100 held-out segments and measure the unknown token rate, aiming for less than 1% `<unk>` occurrence.

2. **Cross-Lingual Transfer Assessment**: Create a controlled experiment comparing Wav2Vec2-XLSR-53 fine-tuned on Badini versus the same model pre-trained only on monolingual English data. Measure the performance differential to quantify the exact contribution of multilingual pre-training to the observed gains.

3. **Real-World Deployment Testing**: Deploy both models on a diverse set of Badini Kurdish speakers from different regions and age groups, recording at least 5 additional hours of speech. Compare performance degradation relative to the original test set to assess real-world generalization and identify potential biases in the original dataset's narrator selection.