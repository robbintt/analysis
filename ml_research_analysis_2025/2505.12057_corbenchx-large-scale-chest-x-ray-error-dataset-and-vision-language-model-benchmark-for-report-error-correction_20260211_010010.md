---
ver: rpa2
title: 'CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model
  Benchmark for Report Error Correction'
arxiv_id: '2505.12057'
source_url: https://arxiv.org/abs/2505.12057
tags:
- error
- report
- correction
- detection
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CorBenchX, a large-scale benchmark for error
  detection and correction in chest X-ray radiology reports. The authors create a
  dataset of 26,326 error-injected reports across five clinically common error types
  and evaluate nine vision-language models on zero-shot detection and correction tasks.
---

# CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction

## Quick Facts
- **arXiv ID**: 2505.12057
- **Source URL**: https://arxiv.org/abs/2505.12057
- **Reference count**: 40
- **Primary result**: Introduces CorBenchX benchmark with 26,326 error-injected chest X-ray reports and proposes multi-step RL framework achieving 38.3% improvement in detection precision and 5.2% in correction accuracy.

## Executive Summary
This paper introduces CorBenchX, a large-scale benchmark for error detection and correction in chest X-ray radiology reports. The authors create a dataset of 26,326 error-injected reports across five clinically common error types and evaluate nine vision-language models on zero-shot detection and correction tasks. Performance results show that even the best model (o4-mini) achieves only 50.6% detection accuracy and falls short of clinical-grade correction. To address this gap, the authors propose a multi-step reinforcement learning (MSRL) framework that sequentially optimizes error identification, description, and correction, achieving 38.3% improvement in detection precision and 5.2% in correction accuracy over baselines. The work highlights the need for more robust approaches to radiology report quality control.

## Method Summary
The authors construct CorBenchX by sampling 26,326 clean radiology reports from MIMIC-CXR and using DeepSeek-R1 to inject five types of errors: Omission, Insertion, Spelling Error, Side Confusion, and Other. The dataset includes single-error (24,164 samples) and multi-error (2,180 samples) cases with span-level annotations. Nine vision-language models are evaluated in zero-shot mode for error detection and correction. The proposed MSRL framework uses Group Relative Policy Optimization (GRPO) with a 3-stage decomposition: Error Identification, Error Description, and Error Correction, each with specific reward functions. The approach is applied to QwenVL2.5-3B and QwenVL2.5-7B models.

## Key Results
- Best zero-shot model (o4-mini) achieves only 50.6% detection accuracy and falls short of clinical-grade correction
- MSRL framework achieves 38.3% improvement in detection precision and 5.2% in correction accuracy over baselines
- Model performance degrades significantly on multi-error cases compared to single-error detection
- Even with MSRL, correction accuracy remains below clinical requirements, highlighting the challenge of the task

## Why This Works (Mechanism)
The paper addresses the critical gap in automated quality control for radiology reports by creating a large-scale benchmark that mirrors real-world error patterns. The multi-step RL approach works by decomposing the complex task into manageable sub-tasks, allowing the model to focus on one aspect at a time rather than trying to solve detection and correction simultaneously. This decomposition enables more effective reward shaping and policy optimization.

## Foundational Learning
- **Error injection methodology**: Critical for creating realistic training data; needed because real error-labeled datasets are scarce. Quick check: Verify error distribution matches clinical error statistics.
- **Vision-language model evaluation**: Required for assessing model performance on multimodal tasks; needed because radiology reports require image-text understanding. Quick check: Ensure all six metrics (BLEU, ROUGE-L, BERTScore, SembScore, CheXbert F1, RadGraph F1) are properly implemented.
- **Reinforcement learning for text generation**: Essential for sequential decision-making; needed because error correction requires iterative refinement. Quick check: Monitor reward convergence during GRPO training.
- **Group Relative Policy Optimization**: Advanced RL variant for language tasks; needed for stable training with group normalization. Quick check: Validate group size G=8 produces stable gradients.
- **Multimodal medical report analysis**: Core competency for the task; needed because CXR images provide crucial context for report errors. Quick check: Confirm VLM correctly aligns image features with textual errors.

## Architecture Onboarding

**Component Map**: MIMIC-CXR extraction -> Error Injection (DeepSeek-R1) -> QC Pipeline -> CorBenchX Dataset -> VLM Zero-shot Evaluation -> MSRL Training Pipeline -> Performance Metrics

**Critical Path**: Error Injection → MSRL Training → Performance Evaluation
The pipeline begins with controlled error injection to create realistic training scenarios, followed by sequential RL optimization that decomposes the task, and culminates in comprehensive evaluation across multiple metrics to ensure clinical relevance.

**Design Tradeoffs**: 
- Error injection vs. real error collection: Injection provides controlled, scalable data but may miss subtle real-world patterns
- Zero-shot vs. fine-tuned evaluation: Zero-shot tests general capability but may underestimate model potential with task-specific adaptation
- Single-step vs. multi-step RL: Multi-step enables better reward shaping but increases training complexity and computational cost

**Failure Signatures**: 
- Model ignores "Other" error category (especially QwenVL2.5-3B)
- Multi-error detection accuracy significantly below single-error performance
- Reward stagnation during GRPO training indicating poor gradient signals

**First Experiments**:
1. Run pilot error injection with 100 samples to validate error distribution and severity match reported statistics
2. Implement minimal MSRL training with GRPO defaults on single-error subset to confirm 3-stage decomposition works
3. Conduct ablation on multi-error subset by incrementally increasing error count to validate reasoning limitations

## Open Questions the Paper Calls Out
None

## Limitations
- Error injection methodology relies on unspecified prompts, making exact replication difficult
- MSRL hyperparameters (learning rate, batch size, GRPO-specific parameters) are unspecified, affecting reproducibility of the 38.3% improvement claim
- Performance on multi-error cases remains significantly below clinical requirements even with MSRL
- Reliance on API-based proprietary models (GPT-4o, Claude 3.7, o4-mini) limits exact reproducibility

## Confidence

**High Confidence**: The existence of CorBenchX as a benchmark with 26,326 samples across five clinically relevant error types, and the general finding that VLMs underperform on radiology report error correction compared to detection.

**Medium Confidence**: The effectiveness of the proposed multi-step reinforcement learning framework, as the exact hyperparameters and reward formulations are unspecified, making the magnitude of improvement (38.3% detection precision, 5.2% correction accuracy) uncertain.

**Low Confidence**: The absolute performance values of specific models (e.g., o4-mini at 50.6% detection accuracy) due to reliance on API models and lack of detailed training configurations for MSRL.

## Next Checks
1. Request and validate the DeepSeek-R1 error injection prompts from the authors to ensure faithful dataset recreation, then conduct a pilot generation of 100 samples to verify error distribution matches reported statistics.

2. Implement a minimal MSRL training run using GRPO with default parameters (lr=1e-6, batch size=8, epochs=3) on the single-error subset, measuring step-by-step reward convergence to confirm the 3-stage decomposition approach is functioning as intended.

3. Conduct ablation studies on the multi-error subset by comparing model performance when trained only on single-error samples versus incrementally increasing error count, to validate whether the reported underperformance on multi-error cases is consistent with model reasoning limitations.