---
ver: rpa2
title: 'From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector Graphics'
arxiv_id: '2503.07429'
source_url: https://arxiv.org/abs/2503.07429
tags:
- diagram
- diagrams
- math
- hint
- generate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an approach to automatically generate math
  diagrams in hint text using LLMs and SVG code. The key method is to use in-context
  learning with ICL examples to prompt LLMs to generate SVG diagrams based on textual
  hints, and then use VQA-based evaluation to assess diagram quality.
---

# From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector Graphics

## Quick Facts
- arXiv ID: 2503.07429
- Source URL: https://arxiv.org/abs/2503.07429
- Authors: Jaewook Lee; Jeongah Lee; Wanyong Feng; Andrew Lan
- Reference count: 32
- Primary result: The pipeline achieves 80% VQA accuracy for generated diagrams, compared to 85% for ground truth, demonstrating the feasibility of using LLMs to generate math diagrams from textual hints.

## Executive Summary
This paper presents a pipeline for automatically generating math diagrams to accompany textual hints in educational content. The approach uses GPT-4o to generate SVG code from textual descriptions through in-context learning, then evaluates the output using a VQA-based methodology. The key innovation is using vector graphics (SVG) as an intermediate representation rather than directly generating pixel images, which avoids common issues like 3D artifacts and ensures mathematical accuracy. The system demonstrates 80% VQA accuracy compared to ground truth diagrams, showing that LLMs can effectively translate mathematical concepts into visual representations.

## Method Summary
The pipeline takes math problems with textual hints as input and generates corresponding SVG diagrams through a two-stage process. First, GPT-4o is prompted with a task description, generation instructions, an in-context learning example (ICL) from the same topic, and the target textual hint. The model outputs SVG code, which is then rasterized to pixels for evaluation. A second LLM instance evaluates the generated diagram by answering semantic and syntactic questions derived from the ground truth diagram. The approach focuses on array-based math problems for grades 1-3 from Khan Academy, using VQA accuracy as the primary metric for comparison against ground truth diagrams.

## Key Results
- The SVG-based generation pipeline achieves 80% VQA accuracy compared to 85% for ground truth diagrams
- Direct pixel generation using text-to-image models (DALL-E 3) produces 3D artifacts and inaccurate numbers, demonstrating the superiority of the SVG approach
- Providing previous hint diagrams as context improves generation quality and visual consistency in multi-step problems
- VQA-based evaluation is more robust than code-diff metrics for assessing diagram quality

## Why This Works (Mechanism)
The pipeline leverages the structured nature of SVG as an intermediate representation, allowing LLMs to generate mathematically precise diagrams rather than relying on probabilistic image generation. By using in-context learning with topic-aligned examples, the model learns the visual patterns associated with specific mathematical concepts. The VQA evaluation provides a more robust assessment than comparing raw code, as it focuses on the semantic and syntactic correctness of the visual output rather than minor coordinate variations in the SVG code.

## Foundational Learning
- **SVG (Scalable Vector Graphics)**: An XML-based vector image format for 2D graphics with support for interactivity and animation. Why needed: Provides a structured, code-based representation that LLMs can generate precisely, avoiding the randomness of pixel-based image generation. Quick check: Can the model generate basic shapes (rectangles, circles) with correct coordinates and attributes?
- **In-Context Learning (ICL)**: A prompting technique where LLMs learn to perform tasks by being shown a few examples within the prompt itself. Why needed: Allows the model to understand the mapping between textual hints and their visual representations without fine-tuning. Quick check: Does including a relevant example from the same topic significantly improve generation quality?
- **VQA (Visual Question Answering)**: An evaluation methodology where an LLM answers questions about an image to assess its quality and correctness. Why needed: Provides a more robust evaluation than code comparison by focusing on visual semantics rather than implementation details. Quick check: Do ground truth diagrams achieve near-perfect VQA scores when evaluated?
- **ViewBox attribute in SVG**: Defines the coordinate system and aspect ratio for SVG elements. Why needed: Critical for proper diagram scaling and rendering; misconfiguration leads to cut-off or distorted visuals. Quick check: Does the generated SVG render correctly at different sizes without distortion?
- **Rasterization**: The process of converting vector graphics to pixel-based images. Why needed: Necessary for VQA evaluation since multimodal LLMs primarily work with raster images rather than raw SVG code. Quick check: Does the rasterized image maintain the visual fidelity of the original SVG?

## Architecture Onboarding

- **Component map**: Khan Academy problems -> Prompt Constructor -> Generator LLM (GPT-4o) -> SVG code -> Renderer (rasterizer) -> VQA Evaluator (GPT-4o) -> Quality score

- **Critical path**: 
  1. Selecting a high-quality, topic-aligned ICL example is the most critical step for successful generation
  2. The Generator LLM must produce syntactically valid and semantically aligned SVG code
  3. The VQA Evaluator's questions must comprehensively cover both visual structure and mathematical correctness

- **Design tradeoffs**:
  - **SVG vs. Pixel Generation**: Using an LLM to generate SVG code is superior to using it as a prompt for a text-to-image model (like DALL-E 3), which produced 3D and inaccurate results for math diagrams
  - **Code vs. Visual Evaluation**: Choosing VQA over code-diff metrics. Code-diff is brittle (small coordinate changes alter the code significantly without changing the visual). VQA is more robust but depends on the evaluator LLM's capability
  - **Single-Step vs. Multi-Step Generation**: Providing the diagram from a previous hint step (D_{i-1}) as context (on-task demonstration) improves the generation of the subsequent diagram (D_i), ensuring visual consistency

- **Failure signatures**:
  - **3D/Hallucinated Artifacts**: If the pipeline reverts to direct pixel generation, diagrams may exhibit unwanted 3D effects
  - **Misconfigured ViewBox**: When the LLM generates the textual hint first, the resulting SVG may suffer from a misconfigured `<viewbox>`, causing the diagram to be cut off or improperly scaled
  - **Labeling Inconsistency**: The generated diagram may correctly show the visual concept (e.g., a grid) but fail to include necessary labels or use incorrect notation

- **First 3 experiments**:
  1. **Baseline Test**: Run the full pipeline on a set of single-hint problems from different topics. Compare the VQA accuracy of the generated diagrams against the ground truth diagrams to establish a baseline performance (e.g., 80% vs. 85%)
  2. **Ablation Study - Intermediate Representation**: Replace the SVG generator with a text-to-image model (e.g., DALL-E 3). Provide the same textual hints and compare the VQA scores to quantify the advantage of the SVG-based approach
  3. **Ablation Study - Contextual Consistency**: Test a multi-step problem with and without providing the previous hint's diagram (D_{i-1}) as context. Measure the improvement in VQA accuracy and visually inspect for consistency in style and elements

## Open Questions the Paper Calls Out
- **Open Question 1**: Would alternative diagram formats such as TikZ or Wolfram code provide more precise mathematical visualization capabilities than SVG for complex structures? The authors explicitly call for "exploring alternative diagram formats" and suggest "a comparative study of SVG, TikZ, and Wolfram code could highlight the strengths and weaknesses of each format in different educational contexts." This remains unresolved as the current study only evaluated SVG as an intermediate representation.
- **Open Question 2**: Would fine-tuning LLMs on synthetic diagram training data significantly improve generation quality and consistency across diverse math topics? The authors state that "fine-tuning LLMs specifically for diagram generation could significantly enhance output quality" and propose generating "synthetic training data tailored to hint diagrams." This is unresolved as the current approach relies solely on in-context learning with GPT-4o.
- **Open Question 3**: Can the VQA-based evaluation methodology be refined to achieve perfect or near-perfect scores on ground truth diagrams? Table 3 shows ground truth diagrams scoring as low as 0.52 on some topics (Area Formulation Intuition), and Figure 3b's ground truth scores poorly on syntactic questions. The evaluation criteria are "heuristically defined" and may not fully capture diagram correctness.

## Limitations
- The study is limited to Khan Academy problems for grades 1-3 focusing on array-based structures, restricting generalizability to more complex mathematical concepts
- The VQA evaluation depends heavily on the capability and potential biases of the LLM evaluator (GPT-4o), and questions may inadvertently encode correct answers
- The effectiveness of the ICL approach is sensitive to the quality and selection strategy of demonstration examples, which is not explicitly detailed

## Confidence
- **High Confidence**: The comparative advantage of SVG-based generation over direct pixel generation (DALL-E 3) is clearly demonstrated with specific failure modes (3D artifacts, inaccurate numbers). The architectural choice of using VQA over code-diff is logically justified and empirically supported.
- **Medium Confidence**: The overall pipeline achieves 80% VQA accuracy compared to 85% for ground truth, demonstrating the feasibility of the approach. The improvement in diagram generation quality when providing previous hint diagrams as context is observed and quantified.
- **Low Confidence**: The long-term pedagogical impact of the generated diagrams (e.g., on student learning outcomes) is not assessed. The scalability of the approach to a broader range of mathematical topics and grade levels is assumed but not validated.

## Next Checks
1. **External Dataset Validation**: Apply the pipeline to a different, independently curated dataset of math problems and diagrams (e.g., from a different educational platform or textbook). Measure VQA accuracy and compare it to the Khan Academy baseline to assess generalizability. This will test if the approach is robust to variations in problem style and diagram complexity.
2. **Ablation Study on ICL Example Selection**: Systematically vary the strategy for selecting the ICL example (e.g., random selection, hardest problem in the topic, most visually complex problem). Measure the impact on VQA accuracy for the generated diagrams. This will quantify the sensitivity of the pipeline to the quality and type of the demonstration example.
3. **Pedagogical Effectiveness Study**: Conduct a small-scale user study with students or educators. Present them with math problems and both AI-generated and ground-truth diagrams. Ask them to rate the clarity, accuracy, and helpfulness of each diagram for understanding the problem. This will provide qualitative feedback on the practical utility of the generated visuals beyond automated VQA scores.