---
ver: rpa2
title: Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry
arxiv_id: '2509.01620'
source_url: https://arxiv.org/abs/2509.01620
tags:
- poems
- poetry
- modern
- chinese
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first benchmark for detecting LLM-generated
  modern Chinese poetry. It constructs a high-quality dataset containing 800 human-written
  poems and 41,600 AI-generated poems, then evaluates six detectors on this dataset.
---

# Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry

## Quick Facts
- arXiv ID: 2509.01620
- Source URL: https://arxiv.org/abs/2509.01620
- Reference count: 40
- Primary result: Current detectors cannot reliably identify AI-generated modern Chinese poetry, with style-mimicked GPT-4.1 poems being most difficult to detect (Avg F1 48.73%)

## Executive Summary
This paper establishes the first benchmark for detecting LLM-generated modern Chinese poetry, constructing a high-quality dataset with 800 human-written poems and 41,600 AI-generated poems. The study evaluates six detectors on this challenging domain where standard coherence and fluency assumptions fail due to the genre's deliberate grammatical violations. Results demonstrate that while RoBERTa-based supervised methods outperform statistical approaches by substantial margins, all detectors struggle significantly with poems that match human stylistic patterns, revealing critical vulnerabilities in current detection methods.

## Method Summary
The study creates the AIGenPoetry dataset containing 800 human-written poems from 6 professional poets and 41,600 AI-generated poems from GPT-4.1, DeepSeek-V3, DeepSeek-R1, and GLM-4 using 13 distinct prompts. Six detectors are evaluated: five zero-shot statistical methods (Fast-DetectGPT, LRR, Log-Likelihood, Log-Rank, Binoculars) and one supervised RoBERTa-based classifier. The evaluation uses AUROC and F1-score metrics across different poetic features including intrinsic qualities, external structures, and emotions. RoBERTa is fine-tuned with specific hyperparameters while statistics-based methods use various Qwen models for scoring.

## Key Results
- RoBERTa-based detector achieves 91.17% F1-score, outperforming second-best method by 22.07%
- GPT-4.1-generated poems with human-matching styles are most difficult to detect (Avg F1 48.73%)
- Modern Chinese poetry's intrinsic features (style, emotion) are significantly harder to detect than extrinsic features (structure, length)
- Zero-shot detectors perform near-random guessing on style-mimicked GPT-4.1 poems

## Why This Works (Mechanism)

### Mechanism 1: Violation of Coherence Assumptions in Zero-Shot Detection
Statistical detectors fail because modern Chinese poetry deliberately violates grammatical conventions that zero-shot methods use as detection signals. Since both LLMs and humans produce "ungrammatical" text in this genre, the error-signal used to distinguish AI from Human is flattened or inverted.

### Mechanism 2: Stylistic Mimicry as an Adversarial Attack
When LLMs are prompted to imitate specific artistic styles, they constrain their output space to statistically resemble human intentionality. This forces the model into a lower-entropy generation mode that masks the artificial origin.

### Mechanism 3: Supervised Semantic Overfitting (RoBERTa Success)
Fine-tuned transformers outperform statistical heuristics by learning specific semantic patterns rather than relying on surface-level perplexity. RoBERTa likely identifies distinct semantic signatures in AI poetry that persist even when grammatical rules are broken.

## Foundational Learning

- **Concept: Modern Chinese Poetry (MCP) vs. Classical/Formal Verse**
  - Why needed: Standard NLP assumptions about text (coherence, grammar) fail here
  - Quick check: Does your detection pipeline penalize sentence fragments or grammatical errors? If yes, it will fail on this dataset

- **Concept: Intrinsic vs. Extrinsic Features in Literature**
  - Why needed: The paper proves extrinsic features (stanza count, line length) are easily detected/replicated, whereas intrinsic features (style, thought) are the "hard" boundary
  - Quick check: Can you distinguish a poem by its shape (extrinsic) vs. its tone (intrinsic)? Current detectors struggle with the latter

- **Concept: Zero-Shot vs. Supervised Detection Paradigms**
  - Why needed: The performance gap between RoBERTa (Supervised) and Fast-DetectGPT (Zero-Shot) is massive
  - Quick check: Do you have labeled training data for the specific LLM you are trying to detect? If not, you will likely struggle with high false positives

## Architecture Onboarding

- **Component map:** AIGenPoetry Dataset -> 13 Prompts -> 4 LLMs -> 6 Detectors -> Evaluation Metrics
- **Critical path:** The "Intrinsic Quality" detection (D2-D5). Style mimicry creates the most "vulnerable" blind spot for detectors
- **Design tradeoffs:**
  - Zero-Shot: Fast, no training required, but effectively random guessing on style-mimicked GPT-4.1 poems (approx 48% F1)
  - Supervised: High performance (~90%+), but requires fine-tuning on specific LLM outputs
- **Failure signatures:**
  - High False Positives on "Emotional" Humans: Explicit emotional lexicon similar to Prompt P9-P13 may trigger false AI flags
  - Low Temperature Bias: Detectors trained on "creative" (High Temp) text fail to detect "deterministic" (Low Temp) text
- **First 3 experiments:**
  1. Run RoBERTa vs. Fast-DetectGPT on the "Style-Matched" (D2) subset to confirm the massive performance gap (~87% vs ~40-50%)
  2. Implement a simple classifier based on the "Noun Frequency" distinctions found in Table 9 (Abstract vs. Concrete)
  3. Train a detector on "Structure" (D6-D8) and test on "Style" (D2) to verify the reported collapse in performance

## Open Questions the Paper Calls Out
1. Can the detection benchmarks and methods established for free-verse modern Chinese poetry be effectively generalized to classical Chinese poetry or rhyming modern poetry?
2. How can detection architectures be modified to robustly identify AI-generated poetry when LLMs explicitly imitate human stylistic patterns?
3. To what extent do specific linguistic violations, such as the deliberate breaking of grammatical conventions in modern poetry, hinder statistical detection methods compared to semantic features?

## Limitations
- Dataset imbalance: 800 human-written vs 41,600 AI-generated poems may not fully capture human poetic diversity
- Limited generalizability: Findings may not apply to other literary forms, languages, or formal verse structures
- Single supervised model: Heavy reliance on RoBERTa requires substantial labeled training data for each new LLM or style

## Confidence
- **High Confidence:** Modern Chinese poetry's intrinsic features are harder to detect than extrinsic features; RoBERTa outperforms all zero-shot methods; GPT-4.1 style-mimicked poems are most challenging
- **Medium Confidence:** Specific noun frequency patterns distinguish AI from human poetry; Temperature settings significantly impact detection difficulty
- **Low Confidence:** Generalizability to other languages/traditions; Long-term effectiveness against future LLM improvements; Performance consistency across different hardware configurations

## Next Checks
1. Apply the same detection pipeline to English modern poetry or classical Chinese poetry to verify whether failures stem from genre-specific violations or represent fundamental LLM detection limitations
2. Generate poetry using style-transfer prompts and evaluate whether current detectors can maintain performance when LLMs are explicitly constrained to imitate specific human authors
3. Test whether zero-shot detectors can maintain reasonable performance when trained on one LLM's outputs and tested on completely different models