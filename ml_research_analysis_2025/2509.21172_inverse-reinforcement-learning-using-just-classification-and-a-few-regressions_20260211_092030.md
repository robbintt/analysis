---
ver: rpa2
title: Inverse Reinforcement Learning Using Just Classification and a Few Regressions
arxiv_id: '2509.21172'
source_url: https://arxiv.org/abs/2509.21172
tags:
- learning
- reward
- policy
- function
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a new approach to inverse reinforcement learning
  (IRL) that reduces the problem to two supervised learning tasks: classification
  to estimate the behavior policy and regression to solve a fixed-point equation.
  The method avoids the complex optimization loops common in existing IRL algorithms
  by leveraging a key insight: the maximum-likelihood solution to the IRL problem
  is characterized by a linear fixed-point equation involving the log behavior policy.'
---

# Inverse Reinforcement Learning Using Just Classification and a Few Regressions

## Quick Facts
- arXiv ID: 2509.21172
- Source URL: https://arxiv.org/abs/2509.21172
- Authors: Lars van der Laan; Nathan Kallus; Aurélien Bibaut
- Reference count: 40
- Key outcome: This paper presents a new approach to inverse reinforcement learning (IRL) that reduces the problem to two supervised learning tasks: classification to estimate the behavior policy and regression to solve a fixed-point equation.

## Executive Summary
This paper presents a new approach to inverse reinforcement learning (IRL) that reduces the problem to two supervised learning tasks: classification to estimate the behavior policy and regression to solve a fixed-point equation. The method avoids the complex optimization loops common in existing IRL algorithms by leveraging a key insight: the maximum-likelihood solution to the IRL problem is characterized by a linear fixed-point equation involving the log behavior policy. This allows IRL to be solved by first classifying to learn the behavior policy, then iterating a regression a few times to solve the equation. The approach is simple, modular across function approximation classes, and theoretically grounded with finite-sample error bounds. Empirical results show competitive or superior performance to MaxEnt IRL on gridworld tasks.

## Method Summary
The method transforms IRL into a sequence of supervised learning problems. First, a probabilistic classifier estimates the behavior policy π̂(a|s) from observed state-action pairs. Taking the log yields û(s,a) = log π̂(a|s). Then, a regression oracle is iteratively applied to solve the fixed-point equation v = P_μ(γv - û), where P_μ is the Bellman operator under reference measure μ. After K ≈ log n iterations, the reward is recovered via r̂ = û + ĉ_K - γv̂^(K), where ĉ_K is the estimated shaping potential. The approach requires Bellman completeness (the regression class contains T_u v for any v in the class) and provides finite-sample error bounds showing that error decreases as n^(-1/2) with proper choice of K.

## Key Results
- Reduces IRL to classification + regression, avoiding complex nested optimization
- Provides finite-sample error bounds with convergence rate n^(-1/2) under Bellman completeness
- Demonstrates competitive or superior performance to MaxEnt IRL on gridworld tasks
- Theoretical analysis shows the fixed-point iteration converges exponentially fast

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The relaxed IRL problem (without reward normalization) admits a trivial closed-form solution.
- Mechanism: Setting reward r = log π(a|s) and soft value v = 0 exactly satisfies soft Bellman consistency because the next-state log-partition vanishes under policy normalization, and maximizes conditional log-likelihood by matching the induced policy to the behavior policy.
- Core assumption: Behavior policy π is the stationary distribution of observed actions conditioned on states.
- Evidence anchors:
  - [abstract] "maximum-likelihood solution to the IRL problem is characterized by a linear fixed-point equation involving the log behavior policy"
  - [section 3.1] Lemma 1 formally establishes (r, v) = (u★, 0) as optimal for the relaxed problem.
  - [corpus] Related work (Efficient Inference for IRL/DDC) addresses similar relaxed formulations but via nested optimization, not closed-form.
- Break condition: If the behavior policy is not stationary or data is insufficient to estimate π, the trivial solution degrades with classification error ν.

### Mechanism 2
- Claim: Potential-based shaping leaves both feasibility and likelihood unchanged, generating an affine subspace of equivalent solutions.
- Mechanism: Adding a state potential c(s) shifts all logits uniformly per state: the softmax denominator changes by exp(c(s)), which cancels in the policy ratio; the objective terms r + γv and log-partition each shift by c(s), preserving per-sample log-likelihood.
- Core assumption: Discount factor γ < 1 ensures boundedness of transformed value functions.
- Evidence anchors:
  - [section 3.3] Lemma 2 proves shaping invariance; all feasible (r, v) transform to feasible (r̃, ṽ) with identical objective value.
  - [section 3.4] Theorem 2 uses this invariance to characterize all maximum-likelihood solutions as shapings of (u★, 0).
  - [corpus] Generalizing Behavior via IRL with Closed-Form Reward Centroids exploits shaping-like structure but does not derive a linear fixed-point equation.
- Break condition: If γ = 1 (undiscounted) or c is unbounded, the Neumann series fails and the inverse (I − γPμ)⁻¹ may not exist.

### Mechanism 3
- Claim: Imposing reward normalization yields a unique solution via a linear contraction fixed-point equation in the state potential.
- Mechanism: Normalization μr = 0 transforms to (I − γPμ)c = −μu; since Pμ is a Markov operator with sup-norm ≤ 1 and γ < 1, iterative regression converges exponentially fast (γ^K decay) to the unique bounded c★.
- Core assumption: Bellman completeness (Tûv ∈ V for the regression class) ensures no approximation error per iteration; stationarity of λ under Pμ and bounded density ratio κ control norm conversions.
- Evidence anchors:
  - [section 3.4] Theorem 2 derives v★ as the unique fixed point of v = Pμ(γv − u★) and gives explicit reward recovery.
  - [section 5.1] Lemma 4 shows error accumulation for inexact iterations; Theorem 3 combines with Lemma 3's Lipschitz stability.
  - [corpus] BiCQL-ML and Trust Region Reward Optimization address IRL via optimization/adversarial loops; none reduce to pure supervised learning with contraction guarantees.
- Break condition: If Bellman completeness fails (Tûv ∉ V), each regression step incurs approximation error η_k that does not vanish with data, potentially dominating γ^K decay.

## Foundational Learning

### Concept: Soft Bellman equation (entropy-regularized dynamic programming)
- Why needed here: The paper assumes the expert optimizes reward plus entropy bonus, inducing softmax policies π(a|s) ∝ exp(Q(s,a)) and value recursion v = PΞ(r + γv).
- Quick check question: Can you derive why the log-partition ΞQ(s) appears in the soft Bellman equation when ε_t(a) are Gumbel shocks?

### Concept: Fitted fixed-point iteration (FQI-style regression)
- Why needed here: Algorithm 1 iterates regression to approximate the contraction Tû, analogous to FQI but with a different target μ(γv − u).
- Quick check question: Why does contraction (γ < 1) guarantee exponential convergence of fixed-point iterations, and how does per-iteration error η_k propagate?

### Concept: Potential-based reward shaping invariance
- Why needed here: Understanding Lemma 2 is essential to see why the relaxed problem is under-specified and how normalization resolves ambiguity.
- Quick check question: If you add potential c(s) to reward, what constraint on c ensures the softmax policy π(a|s) remains unchanged?

## Architecture Onboarding

### Component map:
Classifier oracle -> û = log π̂ -> Regression oracle (K iterations) -> v̂^(K) -> Reward assembly: r̂ = û + ĉ_K - γv̂^(K)

### Critical path:
1. Estimate π̂ via classification; any calibrated classifier suffices.
2. Initialize v̂^(0) = 0.
3. For k = 1..K: regress μ(γv̂^(k−1) − û)(s′) on (s, a); this approximates Tûv̂^(k−1).
4. Assemble r̂ from û, ĉ_K, and v̂^(K).

### Design tradeoffs:
- More iterations (larger K): Reduces γ^K initialization error but increases cumulative regression error Σ γ^(K−k)η_k; Theorem 4 suggests K ∼ c log n balances both.
- Richer function classes V: Improves expressiveness but increases localized Rademacher complexity ρ̂_V, slowing statistical rates.
- Sample splitting vs. full-sample reuse: Splitting (Algorithm 2) decouples folds for clean generalization bounds; reusing data may help in practice but complicates analysis.

### Failure signatures:
- High classification error: If ‖û − u★‖₂ is large, error propagates as (1−γ)⁻¹ν in Theorem 3; check calibration and class imbalance.
- Regression underfitting: If η_k is large (e.g., V too restrictive), fixed-point iteration stalls; monitor ∥v̂^(k) − Tûv̂^(k−1)∥₂ on held-out data.
- Bellman completeness violation: If Tûv ∉ V, approximation error persists regardless of sample size; consider enlarging V or using minimax estimators (suggested in conclusion).

### First 3 experiments:
1. Tabular gridworld with known transitions: Implement Algorithm 2 with tabular V; verify reward recovery (Q-difference RMSE) matches MaxEnt IRL baseline as in Section 7 "Easy" and "Identifiable" settings.
2. Nonlinear rewards with neural function approximation: Compare Algorithm 1 (neural classifier + neural regressor) against MaxEnt IRL with linear rewards; expect improved correlation and lower KL divergence as in "Hard" setting.
3. Ablation on iteration count K: Run with K ∈ {1, 3, 5, 10, log n} and plot ‖v̂^(K) − v★‖₂ (if ground-truth v★ available) or policy KL divergence; expect exponential decay then plateau at statistical error floor.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fixed-point framework be generalized to reward shocks with cumulative distribution functions (CDFs) other than Gumbel?
- Basis in paper: [explicit] Section 8 states, "An extension would be to generalize this to reward shocks with any given cumulative distribution function F."
- Why unresolved: The current algorithm relies on the specific structure of the soft Bellman equation (log-sum-exp) induced by Gumbel shocks. A general CDF $F$ alters the Bellman operator and the trivial solution $u^*$, potentially breaking the linearity of the fixed-point equation.
- What evidence would resolve it: A derivation of the modified fixed-point equation for a general class of distributions and empirical validation showing convergence under non-Gumbel noise models.

### Open Question 2
- Question: Can a minimax estimator replace the regression step to relax the restrictive Bellman completeness assumption?
- Basis in paper: [explicit] Section 8 proposes, "replace the fixed point regression iteration with a minimax estimator... to allow relaxing Bellman completeness assumptions."
- Why unresolved: The current finite-sample guarantees (Theorem 4) depend on Assumption 2 (Bellman completeness), which requires the hypothesis class $\mathcal{V}$ to be closed under the operator $T_u$. This is often violated in practice with generic function approximation.
- What evidence would resolve it: A modified algorithm using a minimax objective that provides error bounds relying only on realizability rather than completeness.

### Open Question 3
- Question: Does the method retain its statistical efficiency and modularity in high-dimensional, continuous control tasks compared to adversarial approaches?
- Basis in paper: [inferred] The empirical evaluation (Section 7) is limited to gridworld domains, while the introduction claims applicability to "imitation learning for robotics."
- Why unresolved: The algorithm requires estimating a behavior policy over continuous action spaces and computing expectations under $\mu$ for the regression target. It is unclear if the "off-the-shelf" regression approach avoids the high variance issues common in continuous offline RL without extensive tuning.
- What evidence would resolve it: Benchmarks on standard continuous control environments (e.g., MuJoCo) comparing reward recovery error and policy performance against AIRL or GAIL.

## Limitations
- The method assumes known transition dynamics, which is unrealistic for most real-world applications where dynamics must also be learned.
- Performance on continuous state/action spaces or large-scale problems remains untested.
- The approach relies on accurate behavior policy estimation through classification, which can be challenging in practice.

## Confidence
- High confidence: The reduction of IRL to classification plus regression is theoretically sound under stated assumptions; the potential-based shaping invariance is rigorously proven.
- Medium confidence: Empirical performance claims on gridworld tasks, particularly the comparison with MaxEnt IRL, appear solid but depend on unreported hyperparameter choices.
- Medium confidence: Finite-sample error bounds hold under technical assumptions, but their tightness and practical relevance require validation on more challenging domains.

## Next Checks
1. Test Algorithm 2 on a gridworld with unknown transitions where the transition model must be estimated from data, measuring performance degradation compared to the known-dynamics case.
2. Implement the method on a continuous control benchmark (e.g., MuJoCo tasks) with neural network function approximation to evaluate scalability beyond tabular settings.
3. Conduct an ablation study varying the number of fixed-point iterations K and function class complexity V to empirically validate the theoretical trade-off between initialization error (γ^K) and regression error (Σ γ^(K-k)η_k) predicted in Theorem 4.