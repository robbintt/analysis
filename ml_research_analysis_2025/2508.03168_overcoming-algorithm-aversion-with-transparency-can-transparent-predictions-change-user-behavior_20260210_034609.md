---
ver: rpa2
title: 'Overcoming Algorithm Aversion with Transparency: Can Transparent Predictions
  Change User Behavior?'
arxiv_id: '2508.03168'
source_url: https://arxiv.org/abs/2508.03168
tags:
- algorithm
- participants
- transparency
- aversion
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of algorithm aversion by examining
  whether transparency and adjustability can improve user acceptance of machine learning
  predictions. The research conceptually replicates and extends Dietvorst et al.'s
  (2018) work through a pre-registered user study with 280 participants, introducing
  an interpretable GAM model that provides visual explanations of its decision logic.
---

# Overcoming Algorithm Aversion with Transparency: Can Transparent Predictions Change User Behavior?

## Quick Facts
- arXiv ID: 2508.03168
- Source URL: https://arxiv.org/abs/2508.03168
- Reference count: 7
- Primary result: Adjustability significantly increases model adoption (73.9% vs 51.1%), but transparency shows only modest effects

## Executive Summary
This study investigates whether transparency and adjustability can reduce algorithm aversion—users' tendency to avoid algorithmic predictions even when they outperform human judgment. Through a pre-registered experiment with 280 participants, the researchers replicated Dietvorst et al.'s (2018) adjustability effect using a bike rental prediction task. Participants showed significantly higher model adoption when allowed to adjust predictions (adjust-by-50 condition: 73.9%) compared to when predictions were fixed (can't-change condition: 51.1%). However, transparency—implemented through static visual explanations of a GAM model's decision logic—showed only a modest, statistically insignificant effect on both model choice and task performance.

## Method Summary
The study employed a between-subjects factorial design with 280 participants making bike rental predictions. The experiment manipulated two factors: transparency (white-box with visual explanations vs. black-box without) and adjustability (can't-change, adjust-by-50, use-freely). Participants used a GAM-based model trained on the UCI Bike Sharing Dataset with 6 features (temperature, windspeed, humidity, time of day, type of day, weather situation). The model achieved approximately 80 MAE. Task performance and model adoption rates were measured across conditions, with results analyzed using chi-square tests and ANOVA.

## Key Results
- Adjustability significantly increases model adoption (73.9% in adjust-by-50 vs 51.1% in can't-change, χ²(1, N=180)=8.98, p=.003)
- Transparency shows only modest, statistically insignificant effect on model choice (63.9% white-box vs 60.2% black-box, χ²(1, N=180)=0.12, p=.724)
- Adjustability improves task performance primarily by increasing model adoption rather than improving adjustment quality
- Only 68 of 280 participants outperformed the model, with 67 having access to its predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing users with even limited ability to adjust algorithmic predictions significantly increases their willingness to rely on those predictions.
- Mechanism: Adjustability addresses the desire for personal control and enables perceived error correction. When users can modify outputs, they develop a sense of psychological ownership over the final decision, reducing the binary "accept or reject" framing that triggers aversion.
- Core assumption: Users avoid algorithms partly because they feel helpless when the model errs; control reduces this helplessness.
- Evidence anchors:
  - [abstract] "allowing users to modify algorithmic predictions mitigates aversion"
  - [section 4.1] "Participants who had the option to adjust predictions (adjust-by-50) chose to rely on the model more frequently (65 participants, 73.9%) compared to those in the can't-change condition (47 participants, 51.1%)... χ²(1, N=180)=8.98, p=.003"
  - [corpus] Related work (Dietvorst et al. 2018) replicated; corpus papers on chatbot adoption hurdles similarly emphasize user control levers.
- Break condition: If the adjustment range is too small to feel meaningful, or if users lack domain knowledge to make sensible adjustments, the effect may weaken.

### Mechanism 2
- Claim: Transparency alone—static visual explanations of model logic—does not significantly increase algorithm adoption.
- Mechanism: Transparency requires active cognitive engagement to process. Unlike adjustability, which directly alters outcomes, explanations offer no immediate instrumental benefit. Users may view visualizations passively without integrating them into decision-making, especially when presented globally rather than instance-specifically.
- Core assumption: Users will engage with explanations if they are available; this assumption appears unsupported.
- Evidence anchors:
  - [abstract] "Transparency's impact appears smaller than expected and was not significant for our sample"
  - [section 4.1] "In the white-box condition, 62 participants (63.9%) chose to rely on the model. In the black-box condition, 50 participants (60.2%) chose the model... χ²(1, N=180)=0.12, p=.724"
  - [corpus] Poursabzi-Sangdeh et al. (2021) found interpretability alone does not necessarily mitigate algorithm aversion (cited in paper); weak corpus consensus on transparency effectiveness.
- Break condition: Transparency may work better with interactive, instance-level explanations; static global feature plots appear insufficient.

### Mechanism 3
- Claim: Adjustability improves task performance primarily by increasing model adoption, not by users making better adjustments.
- Mechanism: Since the model outperforms most users, the primary performance gain comes from more users choosing to rely on it. Users generally struggle to improve predictions through adjustments; the benefit is in adoption, not correction accuracy.
- Core assumption: The algorithm provides reasonably accurate predictions that users cannot easily outperform.
- Evidence anchors:
  - [section 4.2] "participants in the can't-change condition have significantly higher errors (128.3±56.7) than those in the adjust-by-50 (109.5±54.6, p=0.008) and use-freely conditions (100.0±25.8, p<0.001) while the difference between use-freely and adjust-by-50 is not significant (p=0.170)"
  - [section 4.2] "68 out of 280 participants managed to outperform the model – remarkably, 67 of them had access to its predictions"
  - [corpus] Dietvorst et al. (2018) original finding; limited direct corpus validation for this specific mechanism.
- Break condition: If the model is highly inaccurate, adjustability could increase error if users trust it but adjust poorly.

## Foundational Learning

- Concept: Algorithm Aversion
  - Why needed here: The core phenomenon being studied; users reject algorithmic advice even when it outperforms human judgment.
  - Quick check question: If a model with 80 MAE outperforms your users, why might they still refuse to use it?

- Concept: Generalized Additive Models (GAMs)
  - Why needed here: The interpretable model used; GAMs separate feature effects additively, enabling per-feature visualization.
  - Quick check question: How does a GAM differ from a linear regression in what it can capture while remaining interpretable?

- Concept: Between-Subjects Factorial Design
  - Why needed here: The experimental structure (3×2); understanding this is essential for interpreting the ANOVA and interaction results.
  - Quick check question: Why might a between-subjects design be preferable to within-subjects for studying algorithm aversion?

## Architecture Onboarding

- Component map:
  - Prediction engine: GAM-based model trained on task-relevant features
  - Explanation layer: Static feature contribution plots (global) or instance-level explanations (future enhancement)
  - Control interface: Adjustment slider with configurable bounds (e.g., ±50 units or unrestricted)
  - Choice gate: Binary decision point where users opt in/out of model access
  - Feedback loop: Performance-based incentives tied to prediction accuracy

- Critical path:
  1. Train interpretable model with performance benchmark communicated to users
  2. Present model introduction with optional transparency visualizations
  3. Offer adjustment capability before presenting the binary adoption choice
  4. Track both adoption rate and task performance as dual outcomes

- Design tradeoffs:
  - Adjustability vs. accuracy risk: More freedom can help adoption but may increase user-introduced errors
  - Static vs. interactive explanations: Static is simpler to implement but less engaging; interactive may require more development
  - Global vs. local explanations: Global plots show overall logic; instance-level explanations support specific decisions

- Failure signatures:
  - High transparency, low adoption: Users view explanations but do not engage cognitively
  - High adjustability, no performance gain: Users adopt model but adjustments introduce noise
  - Interaction null: Transparency and adjustability operate independently; do not expect synergy without explicit design for it

- First 3 experiments:
  1. Replicate adjustability effect with your domain's prediction task; confirm the 20-25 percentage point adoption lift holds
  2. Test interactive explanations (user-driven feature exploration) vs. static plots; measure both adoption and subjective engagement
  3. Vary adjustment bounds (e.g., ±10% vs. ±25% vs. unrestricted) to identify the minimum meaningful control threshold

## Open Questions the Paper Calls Out

- Question: Does transparency of interpretable models significantly reduce algorithm aversion in complex, non-intuitive domains (e.g., medical decision-making) where users lack prior intuition?
  - Basis in paper: [explicit] The authors state in the Discussion that future research should examine whether transparency has a stronger effect in complex domains where users "inherently struggle to rely on algorithmic reasoning."
  - Why unresolved: The current study used a relatively simple, intuitive bike rental prediction task where participants likely possessed prior domain intuition, potentially masking the benefits of transparency.
  - What evidence would resolve it: A replication of the experiment using a complex, high-stakes task (e.g., medical diagnosis) that demonstrates a statistically significant main effect for transparency on model choice.

- Question: Does repeated, longitudinal exposure to transparent, interpretable models increase user reliance and reduce algorithm aversion over time?
  - Basis in paper: [explicit] The authors note that the experiment took place in a single session and explicitly suggest that "longitudinal studies could explore whether repeated exposure to interpretable models influences reliance over time."
  - Why unresolved: The current study design captured immediate reactions rather than the long-term habituation or learning effects potentially enabled by transparency.
  - What evidence would resolve it: A longitudinal user study tracking model usage rates and performance across multiple sessions or days.

- Question: Do interactive or engagement-driven transparency interventions (e.g., "what-if" tools) effectively reduce algorithm aversion compared to static visualizations?
  - Basis in paper: [inferred] The authors suggest the null result may stem from a lack of user engagement with static plots and recommend "interactive explanations that encourage users to actively explore" in future system designs.
  - Why unresolved: The study implemented transparency via static global feature plots, which require cognitive effort but do not force active manipulation, potentially explaining the weak effect.
  - What evidence would resolve it: An experimental comparison between static feature plots and interactive manipulation tools measuring their respective impact on model usage.

## Limitations
- The transparency intervention used static global feature plots rather than interactive or instance-specific explanations, potentially underestimating transparency's true impact
- The experimental task (bike rental prediction) represents a narrow domain where performance metrics are straightforward to interpret
- The between-subjects design precludes examining individual differences in how transparency and adjustability interact

## Confidence
- Adjustability effect: High (replicated from Dietvorst et al. with similar magnitude, p=.003)
- Transparency effect: Medium (effect present but statistically insignificant; may be underpowered)
- Mechanism explanations: Medium (plausible but require further validation)

## Next Checks
1. Test transparency with interactive, instance-level explanations to determine if engagement changes adoption rates
2. Replicate in a different domain (e.g., medical diagnosis or financial forecasting) to assess generalizability
3. Examine user adjustment patterns to determine whether performance gains come from increased adoption versus better adjustment quality