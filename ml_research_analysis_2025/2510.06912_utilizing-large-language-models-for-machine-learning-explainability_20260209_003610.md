---
ver: rpa2
title: Utilizing Large Language Models for Machine Learning Explainability
arxiv_id: '2510.06912'
source_url: https://arxiv.org/abs/2510.06912
tags:
- llms
- shap
- classification
- explainability
- multilabel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the explainability of ML pipelines generated
  by large language models (LLMs) across binary and multilabel classification tasks.
  We evaluate OpenAI GPT, Anthropic Claude, and DeepSeek, prompting them to generate
  training pipelines for Random Forest, XGBoost, MLP, and LSTM models.
---

# Utilizing Large Language Models for Machine Learning Explainability

## Quick Facts
- **arXiv ID**: 2510.06912
- **Source URL**: https://arxiv.org/abs/2510.06912
- **Reference count**: 20
- **Primary result**: LLMs generate highly accurate and interpretable ML models with near-perfect SHAP fidelity and consistent sparsity across binary and multilabel classification tasks

## Executive Summary
This study investigates the capability of large language models to generate explainable machine learning pipelines for both binary and multilabel classification tasks. Using three LLMs (OpenAI GPT, Anthropic Claude, and DeepSeek), the research team prompted these models to create training pipelines for four different classifiers: Random Forest, XGBoost, MLP, and LSTM. The study evaluated both predictive performance and explainability using custom metrics based on SHAP values, demonstrating that LLMs can serve as automated tools for creating interpretable ML workflows with high accuracy and explainability.

## Method Summary
The research employed three large language models to generate Python code for machine learning pipelines across two classification tasks: binary classification (driver alertness prediction) and multilabel classification (yeast protein localization). For each task, LLMs were prompted to create code for Random Forest, XGBoost, MLP, and LSTM models using a temperature setting of 1.0. The driver alertness dataset contained 200,000 samples with 5 features, while the yeast dataset had 1,484 samples with 8 numeric features. Models were evaluated using standard performance metrics (accuracy, precision, recall, F1-score) and SHAP-based explainability metrics including SHAP Fidelity (MSE between SHAP approximations and model outputs) and SHAP Sparsity (average number of features with non-zero attribution).

## Key Results
- LLMs successfully generated highly accurate models for both binary and multilabel classification tasks
- SHAP Fidelity scores were near-perfect across all models, indicating excellent alignment between SHAP explanations and actual model behavior
- SHAP Sparsity remained consistent across different model types and tasks, demonstrating stable feature attribution patterns
- All three LLMs (GPT, Claude, DeepSeek) produced comparable results in terms of both performance and explainability

## Why This Works (Mechanism)
The approach leverages LLMs' ability to understand programming patterns and machine learning concepts to generate complete, executable code pipelines. By providing structured prompts that specify the task, dataset characteristics, and model requirements, LLMs can produce code that not only trains models but also incorporates explainability tools like SHAP. The success stems from LLMs' extensive training on code repositories and documentation, enabling them to generate syntactically correct and logically sound ML workflows.

## Foundational Learning
- **SHAP (SHapley Additive exPlanations)**: Game-theoretic approach to explain individual model predictions by computing feature contributions
  - *Why needed*: Provides consistent, theoretically grounded feature attributions that sum to the model's prediction
  - *Quick check*: Verify that sum of SHAP values plus base value equals model output for test samples
- **OneVsRestClassifier**: Strategy for extending binary classifiers to multilabel problems by training one classifier per class
  - *Why needed*: Enables tree-based models to handle multilabel classification where multiple classes can be predicted simultaneously
  - *Quick check*: Confirm output shape matches number of classes and multiple positive predictions are possible
- **TreeExplainer vs KernelExplainer vs DeepExplainer**: Different SHAP explainer types optimized for specific model architectures
  - *Why needed*: Each explainer exploits model-specific properties for computational efficiency and accuracy
  - *Quick check*: TreeExplainer for RF/XGBoost, DeepExplainer for neural networks, KernelExplainer as general fallback

## Architecture Onboarding

**Component Map**: LLM -> Code Generation -> Pipeline Execution -> Model Training -> SHAP Analysis -> Metrics Computation

**Critical Path**: Prompt Construction → LLM Code Generation → Pipeline Execution → Model Training → Explainability Analysis → Performance Evaluation

**Design Tradeoffs**: 
- LLM temperature=1.0 balances creativity with consistency in code generation
- Random 80/20 split provides quick evaluation but may not capture full data distribution
- SHAP-based metrics offer model-agnostic explainability but add computational overhead

**Failure Signatures**:
- LLM generates syntactically incorrect code (import errors, missing dependencies)
- SHAP computation fails due to incompatible explainer-model combinations
- Performance metrics show high variance across LLM runs, indicating instability

**First Experiments**:
1. Test LLM code generation with simplified prompts and verify basic pipeline execution
2. Validate SHAP explainer selection by running on small dataset subsets
3. Compare performance of generated code against hand-written baselines for one model type

## Open Questions the Paper Calls Out
None

## Limitations
- LLM version ambiguity: The study does not specify exact model versions or API parameters beyond temperature=1.0
- Custom dataset availability: The driver alertness dataset is described as "custom" but not publicly available
- SHAP implementation gaps: Critical implementation details missing including background data size, explainer selection, and sparsity threshold

## Confidence
- **High confidence** in the SHAP-based explainability methodology (metrics are well-defined and theoretically sound)
- **Medium confidence** in the reported predictive performance results (verifiable with available datasets)
- **Low confidence** in the overall reproducibility (due to missing dataset and implementation details)

## Next Checks
1. Obtain the yeast dataset from Kaggle and attempt to generate synthetic driver alertness data matching the described characteristics (200k samples, 5 features)
2. Contact authors for specific LLM model versions used and request the custom alert dataset or synthetic data generation code
3. Test SHAP explainer selection and parameter sensitivity (background samples, threshold τ) on a subset of the yeast dataset to validate the reported Fidelity ≈ 0 and consistent Sparsity values