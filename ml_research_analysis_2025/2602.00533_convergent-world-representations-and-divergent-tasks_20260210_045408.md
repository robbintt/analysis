---
ver: rpa2
title: Convergent World Representations and Divergent Tasks
arxiv_id: '2602.00533'
source_url: https://arxiv.org/abs/2602.00533
tags:
- tasks
- representations
- task
- arxiv
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a synthetic framework that cleanly separates
  the underlying world (real city coordinates), the data generation process (seven
  geometric tasks), and the resulting neural representations. This controlled setup
  enables systematic study of how different tasks shape internal representations and
  their adaptability.
---

# Convergent World Representations and Divergent Tasks

## Quick Facts
- arXiv ID: 2602.00533
- Source URL: https://arxiv.org/abs/2602.00533
- Reference count: 40
- The paper develops a synthetic framework that cleanly separates the underlying world, data generation process, and resulting neural representations, enabling systematic study of how different tasks shape internal representations and their adaptability.

## Executive Summary
This paper introduces a synthetic framework to study how different tasks shape neural representations in Transformers. By training models on seven geometric tasks using real city coordinates, the authors systematically analyze representation convergence and divergence across single-task and multi-task settings. The key finding is that while multi-task training drives representations toward aligned world geometries, certain "divergent" tasks (like distance) develop distinct representations that actively harm the integration of new entities during fine-tuning, even after joint pretraining. This controlled evidence challenges the assumption that task alignment guarantees effective transfer.

## Method Summary
The study uses a controlled synthetic setup with 5,075 real city coordinates and 100 synthetic "Atlantis" cities. Models are trained on seven geometric tasks (distance, angle, compass, inside, perimeter, crossing, triangle_area) using a 6-layer Qwen2 decoder-only Transformer. The framework separates the underlying world structure from the data generation process and neural representations. Single-task and multi-task pretraining is followed by fine-tuning experiments where Atlantis cities are integrated into learned manifolds. Representation similarity is measured via Centered Kernel Alignment (CKA), while world structure is probed using linear regression to decode coordinates.

## Key Results
- Multi-task training drives representational convergence toward aligned world geometries, providing controlled evidence for the Multitask Scaling Hypothesis.
- Some tasks that develop distinct representations in isolation ("divergent" tasks) actively harm the integration of new entities into the learned manifold during multi-task fine-tuning.
- Representational divergence predicts fine-tuning generalization, suggesting that gradient descent on divergent tasks routes updates through hidden subspaces rather than the shared world manifold.

## Why This Works (Mechanism)

### Mechanism 1
Multi-task training drives representational convergence toward aligned world geometries. When models are trained on multiple tasks simultaneously, the shared need to solve diverse objectives from the same underlying world constrains the solution space. Fewer representations can competently solve N tasks than M ≤ N tasks, forcing models toward common geometric structures.

### Mechanism 2
Divergent tasks route gradient updates through hidden subspaces rather than the shared world manifold. Tasks with low single-task CKA (like "distance") develop representations in geometric directions orthogonal to other tasks. During fine-tuning, gradients from these tasks update weights along pathways that bypass the shared manifold, encoding new entities in task-specific subspaces that don't transfer.

### Mechanism 3
Single-task representational similarity (CKA) predicts cross-task fine-tuning generalization even after joint pretraining. Representational alignment established during single-task pretraining captures intrinsic task-task relationships that persist as "latent structure" in multi-task models. When fine-tuning on task X to generalize to task Y, the pre-existing single-task CKA between X and Y predicts success.

## Foundational Learning

- **World-Data-Model Decoupling**
  - Why needed here: The framework separates the underlying world (city coordinates), data generation process (geometric tasks), and model representations. This separation enables controlled attribution of representational changes to specific factors.
  - Quick check question: Can you explain why measuring representation quality requires ground-truth world structure rather than just task performance?

- **Centered Kernel Alignment (CKA)**
  - Why needed here: CKA quantifies representational similarity between models trained on different tasks. The paper uses CKA to measure convergence (multi-task alignment) and divergence (task-specific geometry).
  - Quick check question: Why is CKA preferred over raw correlation for comparing representations across models with different training objectives?

- **Linear Probing for World Structure**
  - Why needed here: The paper uses linear probes to extract (x,y) coordinates from internal representations, providing a direct measure of whether models have learned world structure vs. task-specific shortcuts.
  - Quick check question: What does it mean if a model achieves high task accuracy but low linear probe R² for coordinates?

## Architecture Onboarding

- **Component map**: World (5,075 real cities + 100 Atlantis cities) -> Tasks (7 geometric functions) -> Model (6-layer Qwen2 decoder, 128 hidden, 4 heads) -> Tokenizer (character-level, 98 ASCII + specials) -> Analysis pipeline (representation extraction -> PCA -> linear probing -> CKA)

- **Critical path**: 1) Train single-task models -> extract representations -> compute pairwise CKA; 2) Train multi-task models -> verify convergence (higher CKA); 3) Pretrain 7-task model -> fine-tune with Atlantis cities -> measure generalization matrix; 4) Compare fine-tuning with/without divergent tasks -> measure representation integration via linear probe reconstruction error

- **Design tradeoffs**: Character-level tokenization increases learning difficulty but better approximates real LLM conditions; planar coordinates (not spherical) simplify linear probing but may not reflect true geographic geometry; small model scale (128 hidden) enables controlled study but limits generalization to large-scale settings

- **Failure signatures**: Crossing task fails in single-task training (gets stuck in loss plateau, requires multi-task scaffolding); Distance task is divergent (low CKA with all other tasks, harms fine-tuning when included); Atlantis reconstruction fails (~10× higher error when fine-tuning includes divergent tasks)

- **First 3 experiments**: 1) Replicate single-task training for all 7 tasks; visualize representations via PCA and compute linear probe R² to verify world structure emergence; 2) Train two-task combinations; compute CKA matrix to confirm convergence; verify crossing task now trains successfully; 3) Fine-tune pretrained 7-task model on Atlantis cities using single-task vs. multi-task conditions; compare generalization matrix and reconstruction error to identify divergent task interference

## Open Questions the Paper Calls Out

### Open Question 1
What is the mechanistic basis for "divergent tasks" (like `distance`) that cause gradient updates to bypass shared world representations during fine-tuning? The paper establishes a correlation between low single-task CKA and poor fine-tuning integration, but it does not determine why optimization routes updates through "hidden subspaces" rather than the shared manifold.

### Open Question 2
Why does multi-task training drive representations toward structures that are more "linearly surfaced" (visible in PCA) compared to single-task training? The paper documents the phenomenon but lacks a theoretical explanation for why task diversity enforces this geometry.

### Open Question 3
Is task divergence a property of the task-architecture pairing that can be predicted from gradient geometry before training begins? The authors currently identify divergent tasks only post-hoc by measuring CKA alignment and fine-tuning generalization failures.

## Limitations

- The controlled synthetic task setup may not capture the complexity of real-world multi-task learning, where tasks share noisy, overlapping structures rather than clean geometric functions.
- The exact neural mechanisms for how divergent tasks route gradients through hidden subspaces remain unvalidated through mechanistic analysis.
- Results are shown on one small Transformer variant (Qwen2-6L-128H), limiting generalization to larger models or different architectures.

## Confidence

- **High confidence** — Claims about multi-task convergence (CKA increases under multi-task training) are well-supported by direct measurements and align with established literature on representation learning.
- **Medium confidence** — The prediction that single-task CKA values explain fine-tuning generalization is empirically validated in the controlled setup but lacks theoretical grounding for why this latent structure persists through multi-task pretraining.
- **Low confidence** — The claim that divergent tasks route gradients through hidden subspaces is speculative. While the empirical pattern is clear, the proposed mechanism requires mechanistic validation.

## Next Checks

1. Use techniques like activation patching or path attribution to directly test whether gradient updates from divergent tasks bypass shared manifold representations during fine-tuning.

2. Repeat the experiment with larger models (e.g., 24 layers, 1024 hidden) to test whether representational divergence effects scale with model capacity or diminish as models become more capable of shared reasoning.

3. Add controlled noise to task supervision signals and retrain to determine whether the observed convergence/divergence patterns depend on the clean geometric structure of the current tasks.