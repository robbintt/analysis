---
ver: rpa2
title: 'OneTrack-M: A multitask approach to transformer-based MOT models'
arxiv_id: '2502.04478'
source_url: https://arxiv.org/abs/2502.04478
tags:
- tracking
- object
- each
- image
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OneTrack-M presents a transformer-based MOT model that simplifies
  traditional architectures by eliminating the need for a decoder, achieving at least
  25% faster inference times than state-of-the-art models while maintaining or improving
  accuracy metrics. The model employs a ViT encoder for temporal data interpretation
  and introduces a novel channel-wise encoding method to incorporate temporal context.
---

# OneTrack-M: A multitask approach to transformer-based MOT models

## Quick Facts
- arXiv ID: 2502.04478
- Source URL: https://arxiv.org/abs/2502.04478
- Reference count: 7
- One-line primary result: OneTrack-M achieves 35.7 FPS on MOT17, outperforming other transformer-based MOT models in speed while maintaining competitive accuracy

## Executive Summary
OneTrack-M presents a transformer-based MOT model that simplifies traditional architectures by eliminating the need for a decoder, achieving at least 25% faster inference times than state-of-the-art models while maintaining or improving accuracy metrics. The model employs a ViT encoder for temporal data interpretation and introduces a novel channel-wise encoding method to incorporate temporal context. Additionally, a part-based multitask training approach is used to address the challenge of training diverse objectives within a single model. Experimental results on the MOT17 dataset show that OneTrack-M achieves 35.7 FPS, outperforming other transformer-based MOT models in speed while maintaining competitive performance in metrics such as HOTA (65.10%) and IDF1 (79.608%).

## Method Summary
OneTrack-M uses a ViT-Base-16 encoder pre-trained on ImageNet to process stacked temporal windows of 5 frames (W=5) along the channel dimension. The model introduces Channel Wise Encoding, which assigns distinct embeddings to each frame within the temporal window to capture temporal position. Three parallel output heads (heatmap, dimension, displacement) are trained using a Part-Based Multitask Training (TMP) approach that isolates each task during initial training phases before joint fine-tuning. The model outputs detection grids for each frame, which are processed through NMS and Hungarian algorithm for final tracking.

## Key Results
- Achieves 35.7 FPS on MOT17, a 25% improvement over TrackFormer's 27.6 FPS
- HOTA score of 65.10%, competitive with state-of-the-art transformer-based MOT models
- IDF1 score of 79.608%, demonstrating strong identity preservation
- 1090 ID switches, significantly lower than baseline methods using positional embeddings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Eliminating the decoder from the transformer architecture reduces inference time while maintaining tracking accuracy through direct encoder-based feature extraction.
- **Mechanism:** The model uses only a pre-trained Vision Transformer encoder (ViT-Base-16) to extract spatial-temporal features, then passes attention maps directly to three lightweight output heads (heatmap, dimension, displacement). This bypasses the computationally expensive decoder typically used in transformer-based MOT models like MOTR and TrackFormer.
- **Core assumption:** The encoder's attention maps contain sufficient information for detection and tracking without requiring decoder-based query refinement.
- **Evidence anchors:** [abstract] "simplifies the typical transformer-based architecture by eliminating the need for a decoder model for object detection and tracking"; [section 3.3] "By simplifying to just the encoder and transferring the responsibility of the final output to a more efficient component, OneTrack-M has managed to significantly reduce processing time"; [corpus] Weak corpus validation—neighbor papers do not specifically validate encoder-only MOT architectures; most related work (DepTR-MOT, S3MOT) uses different architectural approaches.
- **Break condition:** If object detection requires iterative refinement or query-based attention mechanisms, encoder-only approach may underperform on complex scenes with heavy occlusions.

### Mechanism 2
- **Claim:** Channel-wise temporal encoding enables the model to distinguish frame order within stacked inputs, allowing temporal context processing without sequential inference.
- **Mechanism:** Instead of processing frames sequentially, the model stacks W frames along the channel dimension (creating [3*W, Height, Width] tensors). Learnable embeddings of shape [W, n_d] are added to each frame's patches, explicitly encoding which temporal position each patch belongs to.
- **Core assumption:** The transformer can learn temporal relationships from channel-wise embeddings without requiring recurrent or sequential processing.
- **Evidence anchors:** [section 3.2] "Channel Wise Encoding innovates by assigning a distinct embedding vector for each frame within the temporal window, distinguishing not only the spatial features but also the temporal position"; [section 4.5.4] Table 6 shows positional embedding achieves 44.678 HOTA vs. channel-wise encoding's 65.105 HOTA—a 20.4 point difference; [corpus] No direct corpus validation; neighbor papers use different temporal modeling approaches (Kalman filters, state space models).
- **Break condition:** If temporal relationships require modeling longer-range dependencies than the fixed window size (W=5), performance may degrade with faster object motion or longer occlusions.

### Mechanism 3
- **Claim:** Part-based multitask training (TMP) resolves gradient interference between detection and tracking objectives by training each head in isolation before joint fine-tuning.
- **Mechanism:** Training proceeds in phases: (1) freeze dimension/displacement heads, train heatmap head for 50 epochs; (2-3) repeat for dimension and displacement heads; (4) unfreeze all layers for 50 epochs of joint training. This prevents conflicting gradients from different tasks during early learning.
- **Core assumption:** Detection and tracking tasks have sufficiently different learning dynamics that simultaneous training causes harmful interference.
- **Evidence anchors:** [section 4.5.2] Table 4 shows TMP achieves HOTA 65.105 vs. 49.915 without TMP—a 15.19 point improvement; [section 2.6] "Zhang et al. [2021] argues about the dissonance between learning these tasks, which harms the model's overall learning"; [corpus] Weak validation—CropTrack mentions similar MOT challenges but does not address multitask training interference.
- **Break condition:** If tasks share beneficial intermediate representations that emerge from simultaneous training, isolated training may prevent useful feature sharing.

## Foundational Learning

- **Concept: Transformer Attention for Spatial Relationships**
  - **Why needed here:** The ViT encoder processes image patches as token sequences; understanding how self-attention captures global dependencies is essential for debugging detection failures.
  - **Quick check question:** Can you explain why a transformer might detect an occluded object better than a CNN with the same receptive field?

- **Concept: Multi-Object Tracking Metrics (HOTA, IDF1, MOTA)**
  - **Why needed here:** The paper optimizes for different metrics than traditional approaches; HOTA balances detection and association, while IDF1 measures identity preservation.
  - **Quick check question:** Why would a model with lower MOTA (67.95) but higher IDF1 (79.61) be preferred for surveillance applications?

- **Concept: Hungarian Algorithm for Assignment**
  - **Why needed here:** Post-processing uses Hungarian algorithm for detection-to-trajectory association; understanding cost matrix construction is critical for tuning tracking behavior.
  - **Quick check question:** What happens to tracking performance if the IoU threshold in the cost matrix is too permissive?

## Architecture Onboarding

- **Component map:** Input frames [batch, 3*W, 224, 224] → Patch extraction (16×16) → Linear projection to 768-dim vectors → Channel-wise encoding (learnable temporal embeddings) → ViT encoder (12 transformer blocks) → Attention maps [batch, 196*W, 768] → Three output heads (heatmap, dimension, displacement) → [batch, W, 196, 196] grids → NMS filtering → Hungarian assignment

- **Critical path:** Frame stacking → patch embedding → channel-wise encoding → encoder attention → head predictions → NMS filtering → Hungarian assignment. The channel-wise encoding is the highest-risk component; incorrect implementation causes temporal confusion (see Table 6: 4866 ID switches with positional vs. 1090 with channel-wise).

- **Design tradeoffs:**
  - Window size (W): Larger windows provide more temporal context but increase memory and may confuse displacement prediction. Table 3 shows W=5 optimal, W=10+ degrades.
  - Patch size: 16×16 (ViT-Base-16) vs. 32×32 (ViT-Base-32). Smaller patches = more detail but slower (37.7 FPS vs. 60.18 FPS).
  - ViT variant: Base vs. Large. Large improves HOTA (+2.9) but drops FPS to 8.11.

- **Failure signatures:**
  - High ID switches (IDS > 2000): Likely channel-wise encoding not applied correctly; verify embedding addition in preprocessing
  - Low MOTA with high IDF1: Detection threshold too high; adjust heatmap confidence threshold before NMS
  - ID theft (Figure 9): Displacement prediction error on nearby objects; may require dimension/displacement head retraining
  - Missed distant objects (Figure 10): 224×224 resize loses small object detail; consider multi-scale input

- **First 3 experiments:**
  1. **Validate channel-wise encoding implementation:** Train with W=5 using positional embeddings only, compare IDS count against Table 6 baseline (~4866). Then switch to channel-wise and confirm IDS drops to ~1090.
  2. **TMP ablation:** Train a single model end-to-end (all losses simultaneously, 200 epochs) and compare HOTA against TMP-trained model. Expect ~15 point gap per Table 4.
  3. **Window size sweep on validation split:** Test W ∈ {1, 2, 3, 4, 5, 10} and plot HOTA vs. FPS. Confirm W=5 is Pareto-optimal for your target deployment constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the Part-Based Multitask Training (TMP) methodology improve stability and performance when applied to other end-to-end transformer-based MOT architectures or different multi-task computer vision domains?
- **Basis:** [explicit] The authors explicitly propose "investigating the feasibility of TMP for other MOT models that have similar processing" and suggest testing it in "other fields where there are models that also perform related tasks."
- **Why unresolved:** The study only validates the TMP technique within the specific OneTrack-M architecture, leaving its generalizability to other models (e.g., MOTR, TrackFormer) unproven.
- **What evidence would resolve it:** Comparative benchmarks showing the effects of TMP versus standard joint training on the convergence rates and accuracy of other state-of-the-art MOT models.

### Open Question 2
- **Question:** Can alternative feature extractor backbones (replacing ViT) be integrated into the OneTrack-M architecture to further optimize the balance between inference speed and tracking accuracy?
- **Basis:** [explicit] The conclusion states that "evaluating other extractor models instead of the Vision Transformer may be a promising path," noting that current mechanisms require adaptation to support effective training with different backbones.
- **Why unresolved:** The current implementation relies exclusively on pre-trained Vision Transformer (ViT) variants (Base/Large), and the authors note that swapping extractors requires structural changes to maintain effective training.
- **What evidence would resolve it:** Experimental results using OneTrack-M with alternative backbones (e.g., Swin Transformer, CNNs) demonstrating comparable or improved HOTA/FPS metrics.

### Open Question 3
- **Question:** What architectural or loss function modifications are necessary to reduce the rate of redundant detections and improve MOTA scores without compromising the model's high identity preservation (IDF1)?
- **Basis:** [inferred] The results show a significant discrepancy in MOTA (up to 12.65% lower) compared to state-of-the-art models, which the authors attribute to "wrong or redundant detections," even while achieving top-tier HOTA and IDF1.
- **Why unresolved:** The current loss function (weighted average of Focal and L1 losses) and post-processing (NMS) do not sufficiently penalize the specific false positives causing the lower MOTA.
- **What evidence would resolve it:** A modified version of OneTrack-M that closes the MOTA gap with state-of-the-art models (e.g., BoTSORT) on the MOT17 dataset while maintaining real-time FPS.

### Open Question 4
- **Question:** How can the model's dependency on manual, dataset-specific normalization parameters for displacement be eliminated to improve generalization to new environments?
- **Basis:** [inferred] The authors note that the displacement normalization values were derived via "exploratory analysis of the benchmark on the dataset MOT17" and must be "revisit[ed] and adjust[ed]" for different contexts, creating a barrier to robust deployment.
- **Why unresolved:** The displacement head uses fixed hyperbolic tangent normalizations based on specific statistical ranges of the training set, making the model sensitive to dataset shifts without manual tuning.
- **What evidence would resolve it:** An adaptive normalization technique or a revised network head that performs competitively on datasets with different motion characteristics (e.g., high-speed KITTI) without manual parameter retuning.

## Limitations

- **Architectural validation gaps:** The paper lacks ablation studies comparing against decoder-based transformers with equivalent encoder capacity to isolate the true contribution of decoder removal versus other architectural differences.
- **Temporal modeling assumptions:** The fixed window size of W=5 may limit performance on scenarios requiring longer temporal context, and the paper doesn't test alternative temporal modeling approaches.
- **Training procedure opacity:** Critical implementation details like optimizer type, learning rate schedules, batch size, and exact loss weighting strategies remain unspecified, making exact reproduction challenging.

## Confidence

- **High confidence:** Encoder-only architecture speed improvements and basic detection accuracy metrics (MOTA, IDF1) are well-supported by the experimental results and reasonable architectural simplifications.
- **Medium confidence:** Channel-wise temporal encoding effectiveness is demonstrated through ablation, but lacks comparison against alternative temporal modeling approaches or validation on longer sequences.
- **Medium confidence:** TMP training benefits are well-quantified through ablation, but implementation details are insufficient for exact reproduction.

## Next Checks

1. **Replicate TMP ablation study:** Train one model with simultaneous multi-task learning (all losses from epoch 1) versus the phased TMP approach. Verify the expected ~15 point HOTA gap and analyze gradient patterns during training.

2. **Temporal window scalability test:** Evaluate OneTrack-M with W ∈ {3, 5, 7, 10} on MOT17 validation set. Plot HOTA vs FPS to identify the true Pareto-optimal window size and test whether channel-wise encoding maintains advantages at longer temporal horizons.

3. **Decoder-encoder comparison:** Implement a decoder-based variant of OneTrack-M with equivalent encoder capacity and head complexity. Measure FPS and HOTA to isolate the true contribution of decoder removal versus other architectural differences.