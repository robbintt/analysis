---
ver: rpa2
title: 'EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness,
  and Linguistic Challenges Using Model-as-a-Judge'
arxiv_id: '2505.23009'
source_url: https://arxiv.org/abs/2505.23009
tags:
- text
- system
- will
- foreign
- synthesize
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces EmergentTTS-Eval, a comprehensive benchmark
  for evaluating Text-to-Speech (TTS) systems on complex prosodic, expressiveness,
  and linguistic challenges. The benchmark covers six categories: emotions, paralinguistics,
  foreign words, syntactic complexity, complex pronunciation (e.g., URLs, formulas),
  and questions.'
---

# EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge

## Quick Facts
- **arXiv ID:** 2505.23009
- **Source URL:** https://arxiv.org/abs/2505.23009
- **Reference count:** 40
- **Primary result:** Introduces a comprehensive TTS benchmark with 1,645 test cases across six challenging categories, using LALM-based automated evaluation with 90.5% correlation to human preferences.

## Executive Summary
EmergentTTS-Eval presents a comprehensive benchmark for evaluating Text-to-Speech systems on complex prosodic, expressiveness, and linguistic challenges. The benchmark uses a model-as-a-judge approach with Large Audio Language Models (LALMs) to automate evaluation, enabling scalable and reproducible assessment. It covers six categories: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation, and questions. The dataset is generated iteratively from human-written seeds using LLMs, with increasing complexity through depth-wise refinement. Results show that the model-as-a-judge approach correlates highly with human preferences and reveals fine-grained performance differences among state-of-the-art TTS systems, highlighting the gap between closed-source and open-source models on specific challenges.

## Method Summary
The method employs an iterative LLM-based test case generation process starting from 140 seed prompts from BASE-TTS. These seeds undergo breadth expansion to 70 test cases per category, then depth refinement through three steps using Gemini 2.5 Pro to create increasingly difficult versions (U₀ → U₁ → U₂ → U₃). The evaluation uses a pairwise comparison protocol where Gemini 2.5 Pro judges audio samples against a baseline (gpt-4o-mini-tts with Alloy voice), providing structured JSON responses with category-specific scores [0-3] and winner selection. Win-rate is calculated as the probability of outperforming the baseline, offering a relative performance metric that controls for evaluator bias.

## Key Results
- Model-as-a-judge approach achieves 90.5% Spearman correlation with human preferences
- Iterative refinement produces depth-sensitive performance trends in 4/6 categories
- Open-source TTS models score 1.59-10.61% win-rate on Complex Pronunciation category
- Voice selection affects category-wise win-rates by up to 15% standard deviation in Emotions
- Gemini 2.5 Pro provides highest MMAU score (68.6%) but at ~$50 per full evaluation

## Why This Works (Mechanism)

### Mechanism 1
Iterative LLM-based refinement produces increasingly difficult TTS test cases that differentiate model capabilities. Starting from seed prompts, an LLM applies structured transformations (adding sequential questions, deepening emotional arcs, embedding foreign phrases) to create depth-wise complexity tiers (U₀ → U₁ → U₂ → U₃). This targets specific failure modes like prosodic transitions and code-switching. Core assumption: LLMs can reliably generate linguistically valid utterances that increase synthesis difficulty in predictable ways. Evidence anchors: [section 3.1] "we take the base utterance Uᵢ, and create a deeper version Uᵢ₊₁ through a specific refinement method... three refinement steps are sufficient"; [section 4.2] "four of our six categories exhibit clear depth-sensitive performance trends"; [corpus] Limited direct corpus support for this specific iterative refinement approach; related work focuses on emotion/prosody control rather than test generation. Break condition: If refinement produces grammatically incorrect or unnatural utterances (acknowledged in Section 5 for depth=3 Foreign Words category).

### Mechanism 2
Large Audio Language Models (LALMs) can substitute for human evaluators in assessing expressive TTS quality with high correlation. The LALM (Gemini 2.5 Pro) receives text, category label, evaluation criteria, and two audio samples in randomized order. It produces structured JSON with chain-of-thought reasoning, timestamped analysis, scores [0-3], and winner selection. Core assumption: LALMs trained on audio understanding can transfer reasoning capabilities to fine-grained prosodic evaluation without task-specific fine-tuning. Evidence anchors: [abstract] "high correlation with human preferences"; [section 4.6.2] "Spearman correlation between human and model judge rankings... Gemini 2.5 Pro 90.5%"; [section 4.3] "Kendall's W coefficient of concordance (W = 0.97), indicating near-perfect inter-model consistency"; [corpus] RLAIF-SPA [2510.14628] uses RL with AI feedback for emotional speech, supporting AI-as-evaluator paradigm but for synthesis not evaluation. Break condition: When evaluating subtle paralinguistic cues or emotional shifts, LALMs may hallucinate issues or over-emphasize minor phonetic variations (Section D).

### Mechanism 3
Win-rate against a strong baseline provides more discriminative TTS comparison than traditional MOS or WER metrics. Rather than absolute scoring, systems are compared head-to-head against gpt-4o-mini-tts (Alloy) baseline. Win-rate = P(winner=indexᵢ) + 0.5·P(tie) / n. This controls for evaluator bias and reveals relative capability gaps. Core assumption: Pairwise comparison reduces variance compared to absolute quality ratings. Evidence anchors: [section 3.2] "A score of 0.5 reflects parity with the baseline, while deviations indicate relative superiority or inferiority"; [section 4.2] Table 1 shows MOS and win-rate can diverge (Deepgram highest MOS but lower win-rate than GPT-4o models); [corpus] No corpus papers directly validate win-rate vs. MOS comparison methodology. Break condition: If baseline choice significantly biases results (Section 4.4 shows voice selection affects category-wise win-rates by up to ~15% standard deviation in Emotions).

## Foundational Learning

- Concept: **Model-as-a-Judge paradigm**
  - Why needed here: Understanding that LALMs can provide consistent, reproducible evaluation replacing noisy human ratings requires grasping how LLM reasoning transfers to audio assessment
  - Quick check question: Can you explain why the paper randomizes T₁/T₂ labels and uses structured JSON output?

- Concept: **Prosodic evaluation dimensions** (intonation, stress, rhythm, pausing)
  - Why needed here: The benchmark evaluates six categories requiring understanding of how text maps to acoustic features beyond pronunciation
  - Quick check question: How would "Did you see the message? Well I hope you did." require different prosodic handling than flat reading?

- Concept: **Iterative test-case refinement** (breadth vs. depth expansion)
  - Why needed here: The dataset construction uses distinct strategies for diversity (breadth) and difficulty scaling (depth)
  - Quick check question: What's the difference between adding new question types (breadth) vs. adding sequential questions with pragmatic nuance (depth)?

## Architecture Onboarding

- Component map: Seed Prompts (140 samples from BASE-TTS) → [Breadth LLM expansion] → Expanded Set (70/category + Complex Pronunciation) → [Depth refinement ×3 steps via Gemini 2.5 Pro] → Final Dataset (1,645 test cases) → [TTS System generates audio] → Audio Pairs (candidate vs. baseline) → [LALM Judge - Gemini 2.5 Pro] → Structured Evaluation (scores, reasoning, winner) → [Aggregation] → Win-Rate Metrics per category/depth

- Critical path: The evaluation pipeline hinges on (1) prompt quality for depth refinement and (2) judger prompt design eliciting proper chain-of-thought. Appendix C.3 shows ~131k max output tokens for thinking models.

- Design tradeoffs:
  - Gemini 2.5 Pro as judge: Highest MMAU score (68.6%) but ~$50/full evaluation; Gemini 2.0 Flash offers 90.5% Spearman correlation at lower cost
  - gpt-4o-mini-tts baseline: Chosen for low WER but baseline choice affects interpretation—different baselines yield different absolute win-rates
  - Strong prompting: +5-15% win-rate improvement but may not reflect real-world usage

- Failure signatures:
  - Parsing failures: JSON formatting errors or max token limits from repetitive reasoning loops (Table 1 shows 0-7 failures per model)
  - Open-source models on Complex Pronunciation: 1.59-10.61% win-rate indicating systematic failure on URLs, formulas, abbreviations
  - Voice-category interactions: Emotions/Paralinguistics show highest voice sensitivity (Figure 4a)

- First 3 experiments:
  1. **Baseline validation**: Run 100 random samples through your TTS candidate vs. gpt-4o-mini-tts baseline using Gemini 2.5 Pro judge; verify JSON parsing rate >95%
  2. **Depth sensitivity analysis**: Plot win-rate at each refinement depth (0-3) for 2-3 TTS systems; confirm depth-sensitive trends in ≥4 categories
  3. **Judger ablation**: Compare win-rate rankings from Gemini 2.5 Pro vs. Gemini 2.0 Flash on same 50 audio pairs; expect Kendall's W >0.9

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TTS evaluation performance differ when using native character sets versus Latin transcriptions for multilingual and foreign word scenarios?
- Basis in paper: [explicit] The authors state in Section 5 (Limitations) that the multilingual evaluation "focuses on Latin transcriptions rather than native character sets, which doesn't fully capture the challenges of true multilingual TTS."
- Why unresolved: The current dataset deliberately restricted non-English words to Latin characters to test emergent capabilities with limited data, leaving the specific difficulty delta of native scripts unmeasured.
- What evidence would resolve it: A comparative benchmark evaluating the same TTS models on the "Foreign Words" category using native character sets (e.g., Cyrillic, Kanji) versus the current Latin-only dataset.

### Open Question 2
- Question: Can smaller, open-source Large Audio Language Models (LALMs) serve as reliable "Model-as-a-Judge" substitutes without losing correlation with human preferences?
- Basis in paper: [explicit] Section 5 notes that using Gemini 2.5 Pro incurs "substantial costs" ($50 per system) and identifies "opportunities for more economical alternatives without significant quality loss."
- Why unresolved: The paper tested Qwen 2.5 Omni (which failed) and large proprietary models (which succeeded), but did not evaluate intermediate or distilled models to define the cost-performance frontier.
- What evidence would resolve it: Benchmarking the Spearman correlation of smaller, efficient LALMs (e.g., 7B-30B parameters) against the human preference baseline established in the paper.

### Open Question 3
- Question: What constitutes a robust Text Normalization (TN) pipeline for complex pronunciation that avoids the accuracy trade-offs observed in standard rule-based methods?
- Basis in paper: [inferred] Section 4.5 demonstrates that applying WeText normalization reduced win-rates (51.69% → 50.06%), and even LLM-based normalization introduced new errors, indicating current methods are brittle.
- Why unresolved: The paper establishes that naive normalization harms performance on complex inputs (like URLs or specific numerals) but does not propose a system that consistently handles these edge cases.
- What evidence would resolve it: Developing a context-aware normalization system that outperforms both "No TN" and "WeText" baselines across the "Complex Pronunciation" category metrics.

## Limitations

- Dataset generation reproducibility is limited due to high temperature sampling and multiple LLM systems, making exact reproduction of test cases infeasible
- Win-rate metrics are highly sensitive to baseline model selection, limiting cross-paper comparability
- LALMs occasionally hallucinate evaluation issues, particularly for subtle paralinguistic cues and emotional transitions

## Confidence

**High Confidence (90%+)**: Model-as-a-Judge methodology correlation with human preferences (90.5% Spearman), iterative refinement producing depth-sensitive performance trends, open-source vs. closed-source capability gaps on Complex Pronunciation.

**Medium Confidence (70-89%)**: Generalizability of test cases beyond the six categories, absolute win-rate interpretation across different baseline choices, LALM ability to detect subtle prosodic variations.

**Low Confidence (50-69%)**: Long-term stability of LALM evaluation as models evolve, completeness of the six-category coverage for all TTS use cases, impact of prompt engineering variations on results.

## Next Checks

1. **Baseline Sensitivity Analysis**: Generate the same 100 test cases using two different baseline models (e.g., gpt-4o-mini-tts vs. another strong TTS system) and compare resulting win-rates across all six categories to quantify baseline dependency.

2. **Judger Consistency Test**: Run identical audio pairs through three different LALM versions (Gemini 2.5 Pro, Gemini 2.0 Flash, Claude) on 50 samples and calculate Kendall's W coefficient to verify inter-model consistency exceeds 0.9.

3. **Depth Sensitivity Validation**: Select 10 test cases from each refinement depth (U₀ through U₃) in two categories (Emotions and Complex Pronunciation) and verify that depth-3 cases consistently produce lower TTS model performance than depth-0 cases.