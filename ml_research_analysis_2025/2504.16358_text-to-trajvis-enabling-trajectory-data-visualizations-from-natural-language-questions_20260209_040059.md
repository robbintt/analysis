---
ver: rpa2
title: 'Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language
  Questions'
arxiv_id: '2504.16358'
source_url: https://arxiv.org/abs/2504.16358
tags:
- data
- visualization
- dataset
- language
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Text-to-TrajVis task, which transforms\
  \ natural language questions into trajectory data visualizations. To enable this\
  \ task, the authors create the TrajVL dataset\u201418,140 (question, TVL) pairs\u2014\
  by combining LLM generation with manual refinement."
---

# Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions
## Quick Facts
- **arXiv ID:** 2504.16358
- **Source URL:** https://arxiv.org/abs/2504.16358
- **Reference count:** 24
- **Key outcome:** Introduces Text-to-TrajVis task, creates TrajVL dataset (18,140 pairs), proposes TVL language, achieves 64.67% TVL accuracy with GPT-4o-mini.

## Executive Summary
This paper introduces Text-to-TrajVis, a task that converts natural language questions into trajectory data visualizations. The authors create the TrajVL dataset by combining LLM generation with manual refinement, containing 18,140 (question, TVL) pairs derived from GeoLife trajectories and OpenStreetMap boundaries. They propose a new visualization language (TVL) to support trajectory visualization queries and evaluate multiple LLMs, finding that GPT-4o-mini achieves the highest accuracy (64.67%) while RAG-enhanced models show modest improvements but still struggle with complex spatio-temporal descriptions.

## Method Summary
The authors construct the TrajVL dataset through a systematic process: generating TVL templates, using LLMs to create NLQs (basic, area-diverse, and time-diverse variants), and performing manual verification. They evaluate LLMs using few-shot prompting with RAG retrieval for similar examples. The evaluation pipeline parses LLM outputs into TVL components (Vis, Area, Time, SQL) and calculates composite TVL accuracy against ground truth. The TVL language uses SQL-like structures with spatio-temporal constraints to represent visualization queries.

## Key Results
- GPT-4o-mini achieves highest TVL accuracy (64.67%) on normal scenarios, significantly outperforming other models
- RAG enhancements improve performance but show limited effectiveness for complex spatio-temporal queries
- TVL accuracy drops substantially (39.67%) for complex spatio-temporal descriptions compared to normal queries (74.61%)
- Qwen2.5-7b shows largest RAG improvement (+1.7%) among tested models

## Why This Works (Mechanism)
The approach works by leveraging the structured nature of SQL-like TVL representations combined with natural language understanding. By creating a systematic template generation process and using RAG for contextual retrieval, the system bridges the gap between natural language queries and formal visualization specifications. The manual refinement ensures data quality while the TVL language provides sufficient expressiveness for trajectory visualization tasks.

## Foundational Learning
- **TVL Language Structure:** A SQL-like language with spatio-temporal constraints needed for expressing trajectory visualization queries; quick check: verify SQL parsing correctly handles WHERE clause conditions.
- **RAG Retrieval Mechanism:** Uses all-mpnet-base-v2 embeddings to find similar examples; quick check: confirm retrieval returns relevant examples for test queries.
- **Evaluation Metrics:** Composite TVL accuracy measuring visualization, axis, area, time, and SQL components; quick check: validate accuracy calculation handles edge cases like SQL syntax variations.

## Architecture Onboarding
- **Component Map:** GeoLife Data -> TVL Templates -> NLQ Generation -> Manual Verification -> LLM Inference -> RAG Retrieval -> Evaluation
- **Critical Path:** Data preparation → TVL generation → NLQ creation → Manual refinement → Model evaluation
- **Design Tradeoffs:** Manual verification ensures quality but limits scalability; few-shot prompting balances performance with resource efficiency
- **Failure Signatures:** SQL syntax mismatches, spatio-temporal hallucination in complex queries, retrieval failures for uncommon areas
- **First Experiments:**
  1. Test TVL template generation with basic GeoLife data
  2. Verify NLQ generation produces diverse query types
  3. Validate evaluation pipeline handles SQL variations

## Open Questions the Paper Calls Out
- **Open Question 1:** What architectural improvements can enable LLMs to achieve >80% TVL accuracy on complex spatio-temporal descriptions? The paper demonstrates current RAG approaches are insufficient, with accuracy dropping to 39.67% for complex queries.
- **Open Question 2:** How well does TVL generalize beyond GeoLife dataset? The current dataset is limited to Chinese cities from 2007-2012, raising questions about applicability to other trajectory types.
- **Open Question 3:** What additional trajectory attributes would enhance TVL expressiveness? Current TVL supports limited attributes, while real-world analysis may require speed, heading, or semantic locations.

## Limitations
- Performance drops significantly (39.67% vs 74.61%) for complex spatio-temporal queries, indicating limited capability for advanced analytical questions
- Dataset constructed from single source (GeoLife), limiting generalizability to other trajectory domains and visualization types
- Manual verification process introduces potential subjectivity and scalability constraints

## Confidence
- **High confidence:** Dataset construction methodology and basic TVL accuracy measurements
- **Medium confidence:** RAG enhancement effectiveness and performance comparisons across models
- **Low confidence:** Generalizability to other trajectory datasets and visualization types

## Next Checks
1. Test TVL generation approach on different trajectory dataset (e.g., taxi GPS traces) to assess domain transfer capability
2. Implement ablation studies removing RAG components to quantify their specific contribution beyond few-shot prompting
3. Conduct user studies comparing TVL-generated visualizations against manually created ones for complex queries