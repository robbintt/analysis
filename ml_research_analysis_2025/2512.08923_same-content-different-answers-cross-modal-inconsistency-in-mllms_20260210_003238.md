---
ver: rpa2
title: 'Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs'
arxiv_id: '2512.08923'
source_url: https://arxiv.org/abs/2512.08923
tags:
- text
- image
- modality
- questions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces REST and REST+ benchmarks to systematically
  evaluate cross-modal inconsistency in multimodal large language models (MLLMs).
  The benchmarks present semantically identical information across three modalities
  (text, image, mixed) and measure whether models produce consistent outputs regardless
  of input format.
---

# Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs

## Quick Facts
- arXiv ID: 2512.08923
- Source URL: https://arxiv.org/abs/2512.08923
- Authors: Angela van Sprang; Laurens Samson; Ana Lucic; Erman Acar; Sennay Ghebreab; Yuki M. Asano
- Reference count: 40
- Models tested show 6.6% to 90.7% cross-modal inconsistency on REST benchmark

## Executive Summary
This work introduces REST and REST+ benchmarks to systematically evaluate cross-modal inconsistency in multimodal large language models (MLLMs). The benchmarks present semantically identical information across three modalities (text, image, mixed) and measure whether models produce consistent outputs regardless of input format. Evaluating 15 state-of-the-art MLLMs, the study finds substantial cross-modal inconsistency ranging from 6.6% to 90.7%, even when controlling for OCR errors. Key findings include: models inherently perform better on text modality (7%+ higher accuracy than image/mixed), visual characteristics like resolution and color impact performance (with colored text improving results), and models generally require more vision tokens than text tokens to achieve equivalent accuracy (except Qwen2.5-VL-32B).

## Method Summary
The REST benchmark presents 150 system-of-equations puzzles (SOEBench) and filtered subsets of MMLU, ARC, and GSM-Symbolic as text, images, and mixed modality. REST+ extends this with 10 visual permutations per question (3 fonts Ã— 3 DPIs + 1 color variant). Models are evaluated on OCR verification, then on Text, Image, and Mixed tasks using Chain-of-Thought prompting. Consistency is measured via Render-Equivalence Rate (RER) - the proportion of questions answered identically across all three modalities - computed only on OCR-correct subsets. For representation analysis, layer-wise cosine similarity between text and image embeddings is computed and correlated with consistency scores.

## Key Results
- MLLMs show substantial cross-modal inconsistency (6.6% to 90.7%) even when controlling for OCR errors
- Models inherently perform better on text modality (7%+ higher accuracy than image/mixed)
- Visual characteristics like resolution and color significantly impact performance (colored text improves results)
- Models generally require more vision tokens than text tokens to achieve equivalent accuracy (except Qwen2.5-VL-32B)
- Cross-modal consistency correlates with cosine similarity in embedding space

## Why This Works (Mechanism)

### Mechanism 1: Modality Gap in Joint Embedding Space
- Claim: Cross-modal inconsistency correlates with the separation between text and image representations in shared embedding space.
- Mechanism: When semantically identical content is encoded differently across modalities (occupying distinct regions), downstream reasoning diverges even with correct input recognition.
- Core assumption: Models with higher cosine similarity between matching cross-modal samples will exhibit higher consistency (assumes directionality of representations matters for reasoning).
- Evidence anchors: [abstract] "cross-modal consistency correlates with the modality gap in embedding space, suggesting that inconsistent reasoning arises when text and image representations occupy distinct regions of the joint space"; [section 6] "similarity scores are correlated with the Render-Equivalence Rate (RER), especially for the similarity between images and words and between written-down labels and words"

### Mechanism 2: Visual Token Inefficiency vs. Text Tokens
- Claim: Vision tokens are less information-efficient than text tokens for reasoning tasks, requiring more tokens to achieve equivalent accuracy.
- Mechanism: Image encoding produces more tokens with less semantic density per token, and visual perturbations (resolution, color) alter token sequences in ways that affect reasoning pathways.
- Core assumption: Token count differences reflect fundamental encoding inefficiency rather than just architecture-specific design choices.
- Evidence anchors: [section 5.1] "current models reason substantially better through the text modality and that fewer text tokens than vision tokens are needed to obtain the same performance"; [section 5.1] "InternVL3-14B uses approximately 1600 visual tokens (regardless of DPI), while requiring only 160 text tokens on average, a 10:1 ratio"

### Mechanism 3: OCR-Reasoning Decoupling
- Claim: Correct text recognition from images does not guarantee equivalent downstream reasoning compared to native text input.
- Mechanism: Vision-language models process recognized text through different representational pathways depending on input modality, creating reasoning divergence even with identical semantic content.
- Core assumption: The representational pathway differences persist after the OCR stage and affect the reasoning stage.
- Evidence anchors: [abstract] "neither rendering text as images nor images as text resolves this problem, even when controlling for OCR errors"; [section 4.2] "OCR and data contamination are not the main reasons for cross-modal inconsistency, as models inherently perform better through the text modality"

## Foundational Learning

- Concept: **Modality Gap**
  - Why needed here: Understanding why text and image embeddings occupy different regions in joint space is essential for interpreting cross-modal inconsistency results.
  - Quick check question: Given two semantically identical inputs (text "3+3=?" vs. image of "3+3=?"), would you expect their embeddings to be identical? Why or why not?

- Concept: **Render-Equivalence Rate (RER)**
  - Why needed here: This is the primary metric for measuring cross-modal consistency; understanding it is required to interpret all experimental results.
  - Quick check question: If a model answers correctly in 2 of 3 modalities for every question, what would its RER score be?

- Concept: **Vision Token Encoding**
  - Why needed here: The paper shows vision tokens are less efficient than text tokens; understanding how images become tokens is necessary for interpreting resolution/DPI experiments.
  - Quick check question: Why might lowering image DPI from 200 to 50 change the number of vision tokens a model generates?

## Architecture Onboarding

- Component map:
  REST Benchmark -> OCR verification -> Conditional inclusion -> Tri-modal comparison -> Consistency scoring -> Representation analysis

- Critical path:
  1. Filter questions to exclude >800 characters and LaTeX
  2. Render text as images at controlled DPI with specified fonts
  3. Run OCR verification task; exclude questions with CER > 0 for OCR-correct analysis
  4. Run Text, Image, and Mixed tasks with Chain-of-Thought prompting
  5. Compute RER, CFR, MMC metrics
  6. (Optional) Extract hidden activations and compute cross-modal similarity scores

- Design tradeoffs:
  - OCR control vs. ecological validity: Excluding complex OCR scenarios isolates reasoning from recognition, but may not reflect real-world deployment
  - Computational cost vs. permutation coverage: REST+ uses only 10 permutations (not exhaustive) and excludes mixed modality for feasibility
  - Closed-source model access: Cannot extract internal representations for proprietary models (GPT, Claude, Gemini)

- Failure signatures:
  - OCR contamination: If OCR accuracy is low (<80%), RER scores may conflate recognition and reasoning failures
  - Data contamination: Text benchmarks (MMLU, ARC) may have been seen during pre-training; use SoEBench for guaranteed novel data
  - Format parsing failures: Treat format-invalid responses as incorrect; ensure regex patterns match expected answer formats

- First 3 experiments:
  1. Baseline consistency check: Run REST on a single open-source model (e.g., Qwen-2.5-7B) across all four datasets with OCR filtering enabled. Compare RER on OCR-correct vs. all questions to validate OCR control.
  2. Resolution sensitivity test: Run REST+ DPI variations (50, 100, 200) on a single model. Plot accuracy and token count vs. DPI to confirm vision token efficiency hypothesis for your architecture.
  3. Representation alignment probe: For one open-source model, extract layer-wise embeddings for matched text/image pairs from ImageNet samples. Compute cosine similarity and correlate with the model's RER score to validate the modality gap mechanism locally.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does minimizing the modality gap (increasing cross-modal cosine similarity) causally improve cross-modal consistency, or is the observed correlation merely an artifact of joint training?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion: "Future work could study whether this effect is causal, in particular by optimising the representations to be more similar and observing whether the RER consistency score increases."
- Why unresolved: The paper establishes a positive correlation between implicit alignment scores and consistency (RQ4), but it does not perform interventional studies (e.g., forcing representations to align) to prove causality.
- What evidence would resolve it: An interventional study where models are fine-tuned with a specific loss term to minimize the modality gap, followed by a demonstration that this intervention leads to a measurable increase in the Render-Equivalence Rate (RER).

### Open Question 2
- Question: Why do non-semantic visual attributes (text color and resolution) significantly influence reasoning performance even when OCR is successful and semantic content is identical?
- Basis in paper: [inferred] The results for RQ3b show that models perform better with colored text (red/yellow) compared to black text, even in the OCR-correct subset. The paper reports this phenomenon but does not offer a mechanistic explanation for how surface features alter reasoning capabilities.
- Why unresolved: The analysis confirms the effect exists but lacks an investigation into the internal activations or attention mechanisms that might explain why color alters the reasoning pathway.
- What evidence would resolve it: An interpretability analysis (e.g., attention visualization or activation patching) comparing the internal reasoning traces of identical semantic content presented in black vs. colored text to identify divergence in processing.

### Open Question 3
- Question: Does the observed preference for the text modality stem primarily from the architectural dominance of the LLM backbone or from the inefficiency of current vision tokenization methods?
- Basis in paper: [inferred] The paper notes that models prefer text over images (Section 4.1) and that "fewer text tokens than vision tokens are needed to obtain the same performance." It explicitly questions if the "text modality is inherently more effective in MLLMs" but leaves the root cause (data contamination vs. architecture) open, especially given the novel SOEBench results.
- Why unresolved: While SOEBench ruled out data contamination, the paper does not isolate whether the text preference is due to the token efficiency, the connector architecture, or the LLM's inherent processing bias.
- What evidence would resolve it: A comparative study where the vision encoder and connector are held constant, but the ratio of vision-to-text tokens is artificially manipulated, or where the LLM backbone is trained on a perfectly balanced text/image reasoning corpus, to see if the preference persists.

## Limitations

- The causal relationship between modality gap and inconsistency is asserted but not experimentally proven
- Vision token inefficiency findings show substantial model variation, suggesting the mechanism may not be universal
- OCR-reasoning decoupling mechanism is supported but contradicted by corpus evidence showing inconsistent results across models

## Confidence

- High Confidence: OCR control methodology, RER/CFR/MMC metric definitions, REST+ permutation generation procedure
- Medium Confidence: Modality gap correlation findings, vision token inefficiency patterns (with caveats about model variation), overall cross-modal inconsistency magnitude
- Low Confidence: Causal claims about modality gap driving inconsistency, universal claims about vision token inefficiency, OCR-reasoning decoupling mechanism

## Next Checks

1. **Causality Test**: Run REST on a model with access to its internal representations. Systematically manipulate the modality gap (via adversarial training or fine-tuning) and measure whether reduced cosine similarity between text/image embeddings correlates with improved RER. This would test whether the modality gap is causal rather than merely correlated.

2. **Model-Agnostic Efficiency Analysis**: Compare token efficiency across models using a standardized visual reasoning task where both text and image inputs are available. Measure whether vision tokens consistently require more tokens for equivalent accuracy across all tested models, or if the pattern is specific to certain architectures (like InternVL3).

3. **Real-World OCR Decoupling**: Extend REST to include questions with intentionally challenging OCR (complex fonts, noisy backgrounds, multi-language text). Measure whether cross-modal inconsistency increases with OCR difficulty, and whether models that excel at OCR show better cross-modal consistency, testing the OCR-reasoning decoupling hypothesis under realistic conditions.