---
ver: rpa2
title: 'MemeLens: Multilingual Multitask VLMs for Memes'
arxiv_id: '2601.12539'
source_url: https://arxiv.org/abs/2601.12539
tags:
- meme
- dataset
- multimodal
- hateful
- memes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MemeLens consolidates 38 public meme datasets into a unified multilingual,
  multimodal multitask benchmark covering 20 meme analysis tasks. The framework harmonizes
  heterogeneous annotations into a shared taxonomy and trains a single vision-language
  model to jointly predict labels and generate natural language explanations.
---

# MemeLens: Multilingual Multitask VLMs for Memes

## Quick Facts
- **arXiv ID:** 2601.12539
- **Source URL:** https://arxiv.org/abs/2601.12539
- **Reference count:** 40
- **Primary result:** Unified multimodal multitask training on 38 meme datasets outperforms unimodal baselines and matches or exceeds dataset-specific SOTA on average

## Executive Summary
MemeLens consolidates 38 public meme datasets into a unified multilingual, multimodal multitask benchmark covering 20 meme analysis tasks. The framework harmonizes heterogeneous annotations into a shared taxonomy and trains a single vision-language model to jointly predict labels and generate natural language explanations. Empirical analysis shows that unified multimodal training consistently outperforms unimodal baselines and matches or exceeds dataset-specific state-of-the-art on average, while also providing interpretability through explanation generation. Results indicate substantial task-wise variability, with humor and sarcasm remaining challenging, and demonstrate that single-dataset fine-tuning leads to over-specialization, highlighting the value of unified multitask learning for robust meme understanding across languages and tasks.

## Method Summary
MemeLens uses Qwen3-VL-8B-Instruct as a backbone with LoRA adapters for parameter-efficient fine-tuning across 38 consolidated meme datasets. The approach employs a two-stage training procedure: Stage I fine-tunes for classification-only tasks (3 epochs, lr=1e-4), and Stage II jointly optimizes classification and explanation generation (6 epochs, lr=1e-5). Datasets are filtered for text-over-image content, unified into a shared taxonomy, and formatted with hybrid English instructions and native labels. The model generates structured outputs combining predicted labels and natural language explanations, with performance evaluated using Macro-F1, Accuracy, and Weighted-F1 across the unified benchmark.

## Key Results
- Unified multimodal training achieves 71.0+ accuracy, significantly outperforming unimodal text (65.0) and image (63.6) baselines
- Unified multitask model matches or exceeds dataset-specific SOTA on average while avoiding over-specialization seen in single-dataset fine-tuning
- Humor and sarcasm tasks remain most challenging (Macro-F1 0.63) despite largest relative gains from unified training

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Fusion
- **Claim:** Robust meme understanding appears dependent on fusing visual and textual modalities because meaning in memes is compositional, often relying on irony or juxtaposition absent in either channel alone.
- **Mechanism:** The model learns joint representations where visual features (e.g., facial expressions, cultural symbols) modulate the interpretation of overlaid text, preventing the "blind spots" of unimodal baselines.
- **Core assumption:** The extracted text (via OCR) and visual features are sufficiently aligned to resolve semantic ambiguity.
- **Evidence anchors:**
  - [abstract]: "meaning emerges from interactions between embedded text, imagery, and cultural context."
  - [section 5.1]: Table 1 shows unimodal text (Acc 65.0) and image (Acc 63.6) significantly underperforming multimodal methods (Acc 71.0+).
  - [corpus]: Neighbor paper "Demystifying Hateful Content" confirms the challenge of "interpreting implicit hate messages" in multimodal settings.
- **Break condition:** If OCR fails or images are purely symbolic without text, the fusion mechanism degrades to unimodal performance.

### Mechanism 2: Unified Multitask Regularization
- **Claim:** Unified training across heterogeneous datasets mitigates over-specialization, acting as a regularizer that improves generalization across diverse meme ecosystems.
- **Mechanism:** By exposing the model to 38 datasets with varying label schemes (harassment, humor, propaganda) simultaneously, the optimization landscape forces the learning of generalizable features rather than dataset-specific artifacts or slang.
- **Core assumption:** The "Unified Taxonomy" successfully harmonizes disparate label spaces without introducing excessive noise or label conflict.
- **Evidence anchors:**
  - [section 5.4]: Table 4 diagnostic shows FHM-only fine-tuning drops to 56.9% accuracy vs. MemeLens's 74.0%, indicating over-specialization harms general performance.
  - [abstract]: "single-dataset fine-tuning leads to over-specialization."
  - [corpus]: Weak direct evidence in neighbors; "KID" focuses on knowledge injection rather than multitask scale, suggesting this specific scaling mechanism is a unique contribution of MemeLens.
- **Break condition:** If datasets contain contradictory definitions for the same label (e.g., different standards for "offensive"), the unified model may suffer from label noise confusion.

### Mechanism 3: Explanation-Enhanced Alignment
- **Claim:** Training the model to generate natural language explanations likely refines the shared representation by forcing the model to ground predictions in explicit visual and textual evidence.
- **Mechanism:** The "Classification with Explanation" stage acts as an auxiliary regularizer, requiring the model to attend to specific cues (visual + textual) to generate the rationale, which reinforces the correct label mapping.
- **Core assumption:** The synthetic explanations generated by GPT-4.1 are of sufficient quality and faithfulness to serve as valid supervision signals.
- **Evidence anchors:**
  - [section 4.3]: Mentions "the explanation objective acts as an auxiliary regularizer that refines the shared representation."
  - [section 3.3]: Describes explanations as explicitly grounding decisions in "both the visual content and the overlaid/extracted text."
  - [corpus]: "Demystifying Hateful Content" also leverages large multimodal models for explainable decisions, supporting the viability of this approach.
- **Break condition:** If explanations are hallucinated or inconsistent with the image content, they may mislead the classifier rather than help it.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** MemeLens uses Qwen3-VL-8B as a backbone. Full fine-tuning on 38 datasets is computationally prohibitive and prone to catastrophic forgetting. LoRA allows adapting the massive VLM to specific meme tasks while keeping the backbone frozen.
  - **Quick check question:** If you freeze the vision encoder and only train the LoRA adapters, are you updating the visual processing or just the projection to the language space?

- **Concept: Instruction Tuning / Hybrid Prompting**
  - **Why needed here:** To solve the "label space mismatch" problem. Instead of fixed output heads, the model uses instructions to define the task (e.g., "Detect misogyny"). The paper uses "Hybrid Instruction" (English instructions, native labels) to stabilize multilingual performance.
  - **Quick check question:** Why might keeping label strings in the native language while using English instructions be better than translating everything to English?

- **Concept: Cross-Entropy vs. Sequence Generation**
  - **Why needed here:** MemeLens moves beyond simple classification (Cross-Entropy) to generation. It must output both a label and a text explanation. Understanding the shift from a classification head to a generative decoder is crucial.
  - **Quick check question:** How does the loss function change when you switch from predicting a single class ID to generating "Label: [X], Explanation: [Y]"?

## Architecture Onboarding

- **Component map:**
  - Raw image/text -> OCR (EasyOCR) -> Filter empty text -> Map to Unified Taxonomy -> Generate Hybrid Instruction -> LoRA Fine-tuning on Qwen3-VL-8B-Instruct -> Structured output "Label: <>, Explanation: <>"

- **Critical path:**
  1. **Data Curation:** Raw image/text -> OCR (EasyOCR) -> Filter empty text -> Map to Unified Taxonomy
  2. **Instruction Formatting:** Generate Hybrid Instruction (English prompt + Native Label/Explanation)
  3. **Training:** Stage I (Label prediction) -> Stage II (Label + Explanation generation)

- **Design tradeoffs:**
  - **Unification vs. Specialization:** The unified model matches average SOTA but may not beat highly specialized single-task models on every specific benchmark (e.g., explicit SOTA comparisons in App. F show variability)
  - **Taxonomy Compression:** Mapping fine-grained labels (e.g., 7 misogyny classes) to binary or coarser taxonomies improves data efficiency but loses nuance

- **Failure signatures:**
  - **Task-Specific Collapse:** Performance on "Humor & Sarcasm" remains low (Table 2, Macro-F1 0.63), suggesting the model struggles with implicit pragmatic intent
  - **Zero-Shot Gap:** Non-fine-tuned baselines (Table 1, Qwen3-VL-8B zero-shot) perform poorly (55.1 Acc), indicating the pre-trained knowledge is insufficient for meme specifics without the MemeLens tuning

- **First 3 experiments:**
  1. **Reproduce the FHM Diagnostic:** Fine-tune Qwen3-VL on *only* the Facebook Hateful Memes dataset and evaluate on the full MemeLens validation set. Confirm the performance drop (over-specialization) reported in Table 4.
  2. **Ablate the Explanation Stage:** Train a variant using only Stage I (Classification-only) and compare against Stage II (Classification + Explanation) to quantify the "regularization" benefit of explanations on Macro-F1.
  3. **Assess Taxonomy Sensitivity:** Take a dataset with fine-grained labels (like MIMIC 7-class misogyny) and evaluate performance when mapped to binary vs. keeping the original granularity to verify if the unified taxonomy loses critical information.

## Open Questions the Paper Calls Out

- **Question:** What principled methods can enable unified models to adapt to new tasks and annotation schemes with minimal retraining while avoiding negative transfer across heterogeneous label spaces?
  - **Basis in paper:** [explicit] The authors state they "will investigate principled methods for cross-dataset and label-set generalization, enabling unified models to adapt to new tasks and annotation schemes with minimal retraining."
  - **Why unresolved:** The label taxonomy unification compresses fine-grained distinctions and can introduce partial ambiguity; naive dataset mixing induced negative transfer.
  - **What evidence would resolve it:** A systematic comparison of transfer learning approaches across the 38 datasets with controlled experiments measuring positive vs. negative transfer effects.

- **Question:** How can catastrophic forgetting be mitigated when new meme datasets are introduced sequentially to a unified model?
  - **Basis in paper:** [explicit] The authors plan to "study incremental learning settings, where new datasets are introduced sequentially, with a focus on mitigating catastrophic forgetting."
  - **Why unresolved:** Current MemeLens training is batch-style; deployed systems face continuously evolving meme ecosystems with emerging slang, symbols, and community norms.
  - **What evidence would resolve it:** Experiments measuring retention on original tasks after sequential addition of new datasets, comparing replay, regularization, and architecture-based approaches.

- **Question:** Why do humor and sarcasm tasks remain substantially more challenging than other task categories even under unified multimodal training (Macro-F1 = 0.63 vs. 0.77 for Social & Bias)?
  - **Basis in paper:** [inferred] Table 2 shows Humor & Sarcasm achieves the lowest category-level performance despite the largest relative gains from unified training; the authors note these tasks "often depend on cultural knowledge and subtle contextual cues."
  - **Why unresolved:** The paper demonstrates the gap persists but does not isolate whether the bottleneck is insufficient cultural knowledge encoding, inadequate multimodal fusion for irony detection, or dataset-level annotation inconsistencies.
  - **What evidence would resolve it:** Ablation studies probing cultural knowledge transfer, fine-grained error analysis of sarcasm failures, and cross-cultural transfer experiments within humor/sarcasm datasets.

## Limitations

- **Taxonomy Compression:** Mapping fine-grained labels to unified taxonomy may lose critical distinctions and introduce label noise
- **Explanation Faithfulness:** GPT-4.1-generated explanations haven't been verified for grounding in actual visual-textual evidence
- **Task-Specific Weaknesses:** Humor and sarcasm detection remains substantially more challenging than other categories despite unified training

## Confidence

- **High confidence:** Cross-modal fusion consistently outperforms unimodal baselines (backed by direct empirical comparison)
- **Medium confidence:** Unified multitask training provides regularization benefits (supported by over-specialization diagnostics but lacks ablation of training scale effects)
- **Medium confidence:** Explanation generation improves alignment (mechanism described but not directly ablated for performance contribution)

## Next Checks

1. **Explanation Faithfulness Audit:** Manually evaluate 100 GPT-4.1-generated explanations against their source memes to quantify hallucination rates and assess whether explanation quality impacts model performance.

2. **Taxonomy Noise Sensitivity:** Take 2-3 datasets with fine-grained labels (e.g., 7-class misogyny) and systematically vary the degree of compression to the unified taxonomy. Measure how label granularity affects downstream task performance.

3. **Cross-Dataset Transfer Stress Test:** Fine-tune on single dataset subsets (10%, 50%, 100% of data) and evaluate zero-shot transfer to all other datasets. Quantify the relationship between training data diversity and generalization across the unified benchmark.