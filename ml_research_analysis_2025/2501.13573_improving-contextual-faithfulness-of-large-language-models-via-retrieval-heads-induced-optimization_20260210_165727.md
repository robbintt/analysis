---
ver: rpa2
title: Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced
  Optimization
arxiv_id: '2501.13573'
source_url: https://arxiv.org/abs/2501.13573
tags:
- faithfulness
- retrieval
- heads
- unfaithful
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of faithfulness hallucination in
  retrieval-augmented large language models for long-form question answering. The
  authors propose RHIO, a framework that teaches models to discriminate between faithful
  and unfaithful generations by masking retrieval heads to generate realistic unfaithful
  samples, then fine-tuning with control tokens to explicitly distinguish them.
---

# Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization

## Quick Facts
- arXiv ID: 2501.13573
- Source URL: https://arxiv.org/abs/2501.13573
- Reference count: 33
- Primary result: RHIO improves faithfulness by 12.84% (7B) and 12.59% (13B) on GroundBench, outperforming GPT-4o

## Executive Summary
This work addresses faithfulness hallucination in retrieval-augmented large language models for long-form question answering. The authors propose RHIO, a framework that teaches models to discriminate between faithful and unfaithful generations by masking retrieval heads to generate realistic unfaithful samples, then fine-tuning with control tokens to explicitly distinguish them. The framework further employs contrastive decoding to amplify differences between faithful and unfaithful outputs. Extensive experiments on GroundBench, a new benchmark compiled from five LFQA datasets, show RHIO significantly improves faithfulness by 12.84% (7B) and 12.59% (13B), even outperforming GPT-4o. Human evaluation confirms RHIO generates more grounded responses compared to baselines.

## Method Summary
RHIO works by first identifying retrieval heads (attention heads specialized for copying context) using an external detection algorithm, then masking these heads during generation to create realistic unfaithful samples. The model is fine-tuned with control tokens [POS] and [NEG] to distinguish faithful from unfaithful outputs, learning separate behavioral modes. During inference, contrastive decoding computes both [POS] and [NEG] conditioned probabilities and interpolates logits to amplify faithfulness while suppressing unfaithful continuations. The method is evaluated on GroundBench, a benchmark aggregating five LFQA datasets, showing significant faithfulness improvements over baselines including GPT-4o.

## Key Results
- RHIO improves faithfulness by 12.84% on Llama-2-7B and 12.59% on Llama-2-13B compared to vanilla models
- Outperforms GPT-4o on faithfulness metrics while maintaining comparable ROUGE-L and claim recall scores
- Ablation studies show each component (FAT, SID) contributes meaningfully, with SID providing +2.90% (7B) and +4.17% (13B) improvements
- Human evaluation confirms RHIO generates more grounded responses with reduced hallucination rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval heads causally influence contextual faithfulness; masking them generates realistic unfaithful samples.
- **Mechanism:** Retrieval heads are attention heads specialized for copying information from context. When masked, the model falls back on parametric knowledge, producing error patterns (fabrication, incomplete synthesis, inconsistency) that mirror real unfaithfulness.
- **Core assumption:** Retrieval head activation differences explain a substantial portion of faithfulness variance; the mapping is stable across inputs.
- **Evidence anchors:**
  - [abstract] "We identify a salient correlation between LFQA faithfulness and retrieval heads... masking retrieval heads leads to error patterns similar to real unfaithfulness from the model."
  - [section 2.2.2, Figure 1-2] Faithfulness drops from ~80% to ~35-50% as more retrieval heads are masked; error type distribution matches model-intrinsic errors.
  - [corpus] Related work (ParamMute, ContextFocus) confirms FFN and attention components mediate faithfulness, but causal direction remains correlational.

### Mechanism 2
- **Claim:** Control token conditioning teaches explicit faithfulness discrimination.
- **Mechanism:** By prepending [POS] (faithful) and [NEG] (unfaithful) tokens during training, the model learns distinct behavioral modes. The contrastive supervision signal reinforces the association between token and output type.
- **Core assumption:** The model can learn stable, disentangled representations for faithful vs. unfaithful generation conditioned on control tokens.
- **Evidence anchors:**
  - [section 3.2, Equation 1] Training objective explicitly conditions on control tokens: log p(y+|[POS] ⊕ x, c) and log p(y−|[NEG] ⊕ x, c).
  - [section 6.2, Table 3] Ablation shows removing faithfulness-aware tuning drops faithfulness from 82.35% to 72.98% (7B) and 83.77% to 74.40% (13B).
  - [corpus] SI-FACT and similar frameworks show contrastive tuning can improve faithfulness, but effectiveness varies by model and data quality.

### Mechanism 3
- **Claim:** Self-induced contrastive decoding amplifies faithfulness by suppressing unfaithful continuations.
- **Mechanism:** At inference, both [POS] and [NEG] conditioned probabilities are computed. The final distribution upweights [POS] predictions and downweights [NEG] predictions via: (1 + α)logits[POS] − αlogits[NEG].
- **Core assumption:** The [NEG] token reliably induces unfaithful patterns that overlap with the model's spontaneous errors.
- **Evidence anchors:**
  - [section 3.3, Equation 2] Explicit formula for contrastive decoding with α = 0.2.
  - [section 6.2, Figure 4b] SID outperforms context-aware decoding (CAD); ablation shows +2.90% (7B) and +4.17% (13B) improvement.
  - [corpus] CoCoA and similar adaptive decoding methods show promise, but effectiveness depends on conflict detection accuracy.

## Foundational Learning

- **Concept: Retrieval Heads (Induction Heads)**
  - **Why needed here:** Understanding which attention heads handle context copying is prerequisite to the masking strategy.
  - **Quick check question:** Given an attention pattern visualization, can you identify heads that attend to and copy specific context tokens vs. those that attend broadly or attend to previous tokens?

- **Concept: Contrastive Learning Objective**
  - **Why needed here:** The FAT component uses contrastive supervision between faithful and unfaithful examples.
  - **Quick check question:** If you have paired positive and negative samples, can you write a training objective that maximizes similarity to positives while minimizing similarity to negatives?

- **Concept: Logit Interpolation / Contrastive Decoding**
  - **Why needed here:** SID requires combining logit distributions from two forward passes.
  - **Quick check question:** Given two probability distributions P and Q, and weight α, what is the combined distribution? What happens as α → 0 or α → ∞?

## Architecture Onboarding

- **Component map:** Retrieval Head Detector -> Unfaithful Sample Generator -> Control Token Augmented Model -> Contrastive Decoder
- **Critical path:** 1. Detect retrieval heads → 2. Generate unfaithful samples via masking → 3. Prepare paired (faithful, unfaithful) training data → 4. Train with control tokens → 5. Deploy with contrastive decoding
- **Design tradeoffs:**
  - **N (number of masked heads):** Too few = insufficient error diversity; too many = incoherent outputs. Paper uses N=100; ablation suggests 100 > 50 > 150.
  - **α (contrastive weight):** Higher α = stronger faithfulness pressure but potential overcorrection. Paper finds 0.2 optimal.
  - **Self-induced vs. cross-model samples:** Self-induced (from same model) outperform cross-model by ~1-2% faithfulness.
- **Failure signatures:**
  - Faithfulness plateaus despite tuning → retrieval head detection may be inaccurate for this architecture.
  - Output becomes overly conservative/short → α too high; model avoids generating claims.
  - [NEG] outputs appear faithful → control token not learned properly; check training data balance.
- **First 3 experiments:**
  1. **Validate retrieval head detection:** Mask top-100 retrieval heads vs. random-100 heads on held-out set; verify faithfulness drop is specific to retrieval heads.
  2. **Control token sanity check:** After training, generate with [POS] vs. [NEG] on same input; manually verify outputs differ in faithfulness patterns.
  3. **Ablation sweep on α:** Test α ∈ {0.0, 0.1, 0.2, 0.3, 0.5} on validation split; plot faithfulness vs. overall quality tradeoff.

## Open Questions the Paper Calls Out
- Can specific retrieval heads be mapped to distinct hallucination types (e.g., fabricated vs. incomplete) to enable targeted data augmentation?
- How does RHIO perform in scenarios where the retrieval step fails to provide sufficient context?
- Is the efficacy of RHIO transferable to non-Llama architectures or non-Transformer based models?
- Is there a dynamic, theoretically grounded method for determining the optimal number of retrieval heads to mask (N)?

## Limitations
- The paper primarily evaluates on Llama-2 series models without confirming effectiveness on other architectures
- GroundBench excludes cases where retrieval fails to provide sufficient context, limiting real-world applicability
- The specific retrieval head indices for Llama-2 are not provided, creating a reproducibility gap
- Faithfulness improvements may come at the cost of some output diversity or creativity

## Confidence
- Retrieval head masking generates realistic unfaithful samples: Medium confidence
- Control token conditioning improves faithfulness discrimination: High confidence  
- Contrastive decoding amplifies faithfulness: Medium confidence
- Outperforming GPT-4o on faithfulness: Low confidence

## Next Checks
1. **Retrieval Head Detection Validation:** Mask the top-100 vs. random-100 attention heads on a held-out validation set; verify that faithfulness degradation is specifically attributable to retrieval heads rather than random attention components.
2. **Cross-Architecture Generalization Test:** Apply RHIO to a different retrieval-augmented architecture (e.g., Flan-T5 or BART) and evaluate whether the same faithfulness improvements (12-13%) are observed.
3. **Long-Form Output Stability Analysis:** Generate extended responses (500+ tokens) using contrastive decoding with varying α values; measure faithfulness degradation rate over sequence length.