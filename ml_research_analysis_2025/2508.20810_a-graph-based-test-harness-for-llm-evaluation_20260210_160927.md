---
ver: rpa2
title: A Graph-Based Test-Harness for LLM Evaluation
arxiv_id: '2508.20810'
source_url: https://arxiv.org/abs/2508.20810
tags:
- cond
- question
- condition
- medical
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dynamic graph-based test harness for evaluating
  large language models (LLMs) on medical guidelines, specifically the WHO IMCI handbook.
  By converting clinical guidelines into a directed graph with 200+ nodes and 300+
  edges, the authors auto-generate over 400 questions with 3.3+ trillion possible
  combinations, ensuring comprehensive coverage of guideline relationships.
---

# A Graph-Based Test-Harness for LLM Evaluation

## Quick Facts
- arXiv ID: 2508.20810
- Source URL: https://arxiv.org/abs/2508.20810
- Reference count: 30
- Primary result: Graph-based evaluation harness converts WHO IMCI guidelines into 400+ questions with 3.3+ trillion combinations, revealing LLMs struggle most with triaging severity and treatment protocols (38.5-64.1% accuracy).

## Executive Summary
This paper introduces a graph-based test harness for evaluating large language models on medical guidelines, specifically the WHO IMCI handbook. By converting clinical guidelines into a directed graph with 200+ nodes and 300+ edges, the authors auto-generate over 400 questions with 3.3+ trillion possible combinations, ensuring comprehensive coverage of guideline relationships. This method addresses the coverage and contamination limitations of manually curated benchmarks. Evaluation across multiple models shows varied performance: symptom recognition is strongest (40.9-81.9% accuracy), while triaging severity, treatment protocols, and follow-up care remain challenging (38.5-64.1%). The dynamic, graph-based approach enables systematic, scalable, and contamination-resistant benchmarking, identifying specific capability gaps that general evaluations miss. This methodology is extensible to other medical domains and guidelines.

## Method Summary
The method converts medical guidelines into a directed knowledge graph where clinical entities (conditions, symptoms, treatments, follow-ups, severity levels) are nodes and their relationships are edges. Using NetworkX MultiDiGraph, the WHO IMCI handbook is manually annotated into 200+ nodes and 300+ edges with five node types and four edge types. Question templates traverse the graph to generate multiple-choice questions with age-appropriate distractors sampled from candidate pools. The system produces over 400 questions with trillions of unique combinations through template variations and distractor sampling. This graph-based approach enables systematic coverage of all guideline relationships while preventing data contamination through dynamic generation.

## Key Results
- LLMs show highest accuracy on symptom recognition (40.9-81.9%) and lowest on treatment protocols (38.5-61.1%) and triaging severity (43.3-64.1%)
- Overall accuracy across models ranges from 44.9-67.5% with 95% confidence intervals up to ±28.5%
- The graph-based approach generates 3.3+ trillion unique question combinations through template and distractor variations
- Manual graph construction is identified as the primary bottleneck for scalability

## Why This Works (Mechanism)
The graph-based approach works by encoding clinical guidelines as structured relationships between entities, enabling systematic traversal to generate comprehensive test questions. By representing guidelines as nodes and edges, the method ensures complete coverage of all relationships while avoiding manual curation limitations. The combinatorial generation of questions through templates and distractor pools creates trillions of unique test cases, making data contamination nearly impossible. This structure also enables targeted evaluation of specific reasoning capabilities like symptom-to-condition mapping versus treatment protocols, revealing capability gaps that general benchmarks miss.

## Foundational Learning

- Concept: Directed Knowledge Graphs
  - Why needed here: Core data structure. The entire method depends on encoding clinical entities as nodes and their relationships (e.g., TREAT, INDICATES) as edges to enable systematic traversal and question generation.
  - Quick check question: Can you distinguish between a node attribute and an edge relationship in this schema?

- Concept: MCQA Benchmarks for LLM Alignment
  - Why needed here: The generated dataset serves a dual purpose: evaluation and as a source of high-reward/low-reward samples for post-training algorithms like GRPO and DPO.
  - Quick check question: How does a correct MCQA answer provide a "naturally ranked" signal for a reinforcement learning algorithm?

- Concept: Combinatorial Test Generation
  - Why needed here: Understanding how templates, age variations, and distractor pools combine to create trillions of test cases is essential to grasping the solution's scalability and contamination resistance.
  - Quick check question: If the distractor pool for a condition is small, how does it impact the total number of unique questions that can be generated?

## Architecture Onboarding

- Component map: Graph Constructor -> Question Generator -> Evaluation Harness
- Critical path: The manual graph construction is the primary bottleneck and source of truth. An error here (e.g., wrong TREAT edge) propagates to flawed questions and invalid evaluation. The automated generation and evaluation are downstream of this core asset.
- Design tradeoffs: Coverage vs. Modality: The system achieves 100% coverage of text-based guideline relationships but sacrifices realism by not testing patient vignettes or multi-turn conversation. Cost vs. Quality: Manual graph annotation ensures high-quality relationships but is resource-intensive, contrasting with faster, noisier LLM-based extraction.
- Failure signatures: (1) High variance in results: Wide confidence intervals (e.g., ±28.5%) may indicate unstable model behavior or non-robust questions. (2) Low distractor quality: If models can guess answers via syntactic cues rather than clinical reasoning, the evaluation is invalid. (3) Graph drift: If the source guidelines are updated and the graph is not, evaluation becomes obsolete.
- First 3 experiments:
  1. Validate graph fidelity: Compare a sample of auto-generated questions against the source IMCI handbook text to verify the graph encodes relationships accurately.
  2. Test distractor plausibility: Ask a clinical domain expert to review distractors for a subset of questions to ensure they represent realistic clinical confusion rather than obvious errors.
  3. Probe failure modes: For conditions where models score low on C→T (treatment) questions, perform error analysis to see if failures are due to lack of knowledge or inability to reason with the provided context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this graph-based framework be effectively generalized to more complex or less structured medical guidelines?
- Basis in paper: [explicit] The conclusion states the methodology is "extensible to other medical domains," yet the study only validates the approach on the WHO IMCI handbook.
- Why unresolved: It is unclear if the manual graph construction process scales to denser or less algorithmic guidelines found in other specialties.
- What evidence would resolve it: Successful construction of a graph-based benchmark for a distinct domain (e.g., oncology or endocrinology) showing similar coverage and question diversity.

### Open Question 2
- Question: Does fine-tuning on this generated dataset actually improve model alignment in clinical tasks?
- Basis in paper: [explicit] The abstract claims the methodology "enhances LLM post-training (supervised finetuning, GRPO, DPO)," but the results section only presents baseline inference accuracy.
- Why unresolved: No training experiments or ablation studies are provided to demonstrate that these samples function effectively as high-reward training data.
- What evidence would resolve it: Comparative evaluation of models before and after fine-tuning on this specific dataset using a held-out clinical benchmark.

### Open Question 3
- Question: How can the integrity of the "source of truth" graph be validated to prevent error propagation?
- Basis in paper: [inferred] The authors identify the "reliance on the graph as the source of truth" as a limitation, noting that manual annotation was chosen over automated extraction to avoid errors.
- Why unresolved: There is no proposed mechanism for verifying the medical accuracy of the 200+ nodes and 300+ edges beyond the initial manual creation.
- What evidence would resolve it: An automated validation pipeline or expert audit methodology that flags inconsistencies between the graph and the source text.

## Limitations
- Manual graph construction is resource-intensive and doesn't scale well to more complex guidelines
- Evaluation only tests text-based question answering, missing clinical modalities like patient vignettes
- The graph data and complete implementation details are not publicly available, limiting reproducibility
- Distractor generation could be vulnerable to overfitting if pools are too small or distractors aren't clinically plausible

## Confidence
- **High confidence**: The graph-based methodology for systematic question generation is sound and the reported accuracy ranges per question type are internally consistent
- **Medium confidence**: The claim that the method ensures "complete coverage" of guideline relationships is valid in theory but cannot be verified without the actual graph data
- **Low confidence**: The extensibility claim to other medical domains is aspirational but not demonstrated, as only one guideline was tested

## Next Checks
1. **Graph Fidelity Validation**: Compare a random sample of 20 auto-generated questions against the source WHO IMCI handbook text to verify the graph encodes relationships accurately. Flag any discrepancies between the graph-traversal output and the handbook content.
2. **Distractor Quality Review**: Have a clinical domain expert review distractors for 10 randomly selected questions to assess clinical plausibility and identify if any distractors represent obvious errors or non-realistic clinical confusion.
3. **Failure Mode Analysis**: For the lowest-performing question type (C→T: 38.5-61.1% accuracy), perform detailed error analysis on model responses to distinguish between lack of knowledge versus inability to reason with the provided context.