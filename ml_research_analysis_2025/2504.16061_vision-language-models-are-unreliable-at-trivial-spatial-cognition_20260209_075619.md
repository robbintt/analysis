---
ver: rpa2
title: Vision language models are unreliable at trivial spatial cognition
arxiv_id: '2504.16061'
source_url: https://arxiv.org/abs/2504.16061
tags:
- prompt
- object
- vlms
- left
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Vision language models (VLMs) struggle with reliable spatial reasoning,\
  \ even for trivial tasks like recognizing left-right relations in simple images.\
  \ Researchers developed a synthetic benchmark dataset\u2014TableTest\u2014with 64\
  \ objects arranged in various configurations on a table."
---

# Vision language models are unreliable at trivial spatial cognition

## Quick Facts
- arXiv ID: 2504.16061
- Source URL: https://arxiv.org/abs/2504.16061
- Authors: Sangeet Khemlani; Tyler Tran; Nathaniel Gyory; Anthony M. Harrison; Wallace E. Lawson; Ravenna Thielstrom; Hunter Thompson; Taaren Singh; J. Gregory Trafton
- Reference count: 40
- Primary result: VLMs show 12%-98% accuracy on spatial reasoning tasks, failing particularly at identifying false relations and handling prompt variations

## Executive Summary
Vision language models (VLMs) demonstrate significant unreliability when performing trivial spatial reasoning tasks, even when provided with clear visual input. Researchers tested three state-of-the-art VLMs on synthetic images of objects arranged on a table, finding that performance varied dramatically based on minor prompt variations. While text-only performance remained near ceiling, multimodal performance dropped sharply, with VLMs particularly struggling to identify false spatial relations and maintain consistency across equivalent prompt formulations.

## Method Summary
The study developed TableTest, a synthetic benchmark with 64 objects arranged in various configurations on a table, using Blender to generate controlled images. Researchers tested three VLMs (Idefics2, BLIP, Llama3) on eight different prompt types using both image and text inputs. The evaluation measured accuracy across 4,032 two-object images and 250,000 three-object images, with prompts designed to test true/false relations, multiple choice, and fill-in-blank formats. Text-only controls used scene descriptions rather than images to isolate reasoning capability from vision processing.

## Key Results
- VLMs achieved 12%-98% accuracy across different spatial reasoning tasks
- Text-only performance remained near ceiling while multimodal performance degraded significantly
- VLMs were particularly unreliable at identifying false relations compared to true ones
- Minor prompt variations caused dramatic performance swings, indicating lack of coherent spatial representations

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Surface Form Bias
VLMs resolve spatial queries by relying on statistical regularities in prompt text structure rather than grounding responses in visual input. The model develops preferences for specific object orders or relational terms based on training distributions, ignoring visual evidence that should override these priors.

### Mechanism 2: Training Data Scarcity for Spatial Prepositions
The unreliability stems from distributional gaps in pre-training corpora where spatial prepositions occur too infrequently to be robustly learned. Image-caption datasets prioritize object identity and high-level actions over precise metrical or topological relations.

### Mechanism 3: Affirmation Bias in Relation Verification
VLMs are significantly more capable of confirming relations exist than identifying that they are absent. This suggests the internal representation is associative rather than structural, activating concepts present in images but lacking mechanisms to verify negations of spatial links.

## Foundational Learning

- **Concept: Reliability vs. Competence**
  - Why needed: Distinguishes between getting a task right once versus getting it right across logically equivalent variations
  - Quick check: If I invert the order of options in a multiple-choice prompt, does the model's accuracy change?

- **Concept: Frames of Reference (Egocentric vs. Allocentric)**
  - Why needed: Spatial terms like "left" are ambiguous without defined reference frames
  - Quick check: When the model says "left," is it referencing the camera's perspective or the object's intrinsic orientation?

- **Concept: Multimodal Alignment**
  - Why needed: Disparity between text-only and multimodal performance highlights alignment failures
  - Quick check: Can the model solve the task perfectly with text description but fail with the image itself?

## Architecture Onboarding

- **Component map:** Vision Encoder -> Projector/Adapter -> LLM Backbone -> TABLE TEST Module
- **Critical path:** Degradation from Text-Only â†’ Multimodal is the diagnostic path. If text accuracy is ~99% and image accuracy is ~50% on the same logical task, the bottleneck is in Vision Encoder or Projector alignment.
- **Design tradeoffs:** Synthetic vs. Real Data (control vs. robustness), Prompt Minimalism (avoids bias but lacks scaffolding)
- **Failure signatures:** Selection bias (chooses first option regardless of content), Negation blindness (drops >20% on false relations), Prompt fragility (12% to 98% swings)
- **First 3 experiments:**
  1. Text-Only Baseline: Replicate text-only condition using scene descriptions to isolate LLM reasoning from vision processing
  2. False Relation Probe: Test "Identifying False Relations" failure mode by asking about inverse relations
  3. Permutation Robustness: Take high-performing prompt and permute multiple-choice option order to test option position reliance

## Open Questions the Paper Calls Out

### Open Question 1
Do VLMs exhibit similar unreliability for non-horizontal spatial relations (e.g., above, below, inside) as observed for left/right reasoning? The study limited scope to 1D horizontal relations, leaving performance on vertical and topological relations unknown.

### Open Question 2
Can fine-tuning VLMs on synthetic data containing trivial prompt variations induce coherent spatial representations that generalize? The paper suggests augmenting corpora with variations but doesn't implement this training intervention.

### Open Question 3
To what extent do syntactic surface variations degrade spatial reasoning in text-only contexts? Text-only tests served primarily as controls, and mechanisms causing performance drops when reversing sentence order were not fully diagnosed.

## Limitations

- Synthetic dataset may not capture real-world complexity and variability of spatial reasoning tasks
- Benchmark focuses exclusively on left-right relations in simplified tabletop context, limiting generalizability to other spatial relationships
- Study evaluates only three VLMs from a single architectural family, constraining broader conclusions about spatial reasoning models

## Confidence

- **High Confidence:** Performance variation across minor prompt variations is robustly demonstrated across all tested models and tasks
- **Medium Confidence:** Attribution of failures to specific mechanisms is plausible but requires further validation
- **Low Confidence:** Generalizability to real-world applications and other spatial reasoning domains remains uncertain

## Next Checks

1. Cross-dataset validation: Test same VLMs on established spatial reasoning benchmarks to determine if TableTest results generalize to real images
2. Architectural ablation study: Evaluate whether replacing vision encoder architecture changes performance patterns
3. Prompt engineering intervention: Systematically test whether explicit spatial reference frames or negative examples during prompting can mitigate identified failure modes