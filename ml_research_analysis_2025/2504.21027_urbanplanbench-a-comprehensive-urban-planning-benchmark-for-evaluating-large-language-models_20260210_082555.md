---
ver: rpa2
title: 'UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large
  Language Models'
arxiv_id: '2504.21027'
source_url: https://arxiv.org/abs/2504.21027
tags:
- urban
- planning
- llms
- arxiv
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces UrbanPlanBench, the first comprehensive\
  \ benchmark for evaluating large language models (LLMs) in urban planning. The benchmark\
  \ assesses LLMs across three key perspectives\u2014fundamental principles, professional\
  \ knowledge, and management and regulations\u2014using 300 multiple-choice questions."
---

# UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models

## Quick Facts
- **arXiv ID**: 2504.21027
- **Source URL**: https://arxiv.org/abs/2504.21027
- **Reference count**: 40
- **Primary result**: First comprehensive benchmark for evaluating LLMs in urban planning, revealing systematic knowledge gaps particularly in management and regulations.

## Executive Summary
This paper introduces UrbanPlanBench, the first comprehensive benchmark for evaluating large language models (LLMs) in urban planning. The benchmark assesses LLMs across three key perspectives—fundamental principles, professional knowledge, and management and regulations—using 300 multiple-choice questions. Evaluations of 25 advanced LLMs reveal that while current models outperform random guessing, they fall short of professional human standards, with only one model surpassing the certification bar. Notably, LLMs show stronger performance in understanding planning principles and knowledge but struggle with regulations. The study also introduces UrbanPlanText, a 30,000-pair dataset for fine-tuning LLMs in urban planning, demonstrating significant performance improvements, especially in management and regulations. These resources aim to bridge the gap between LLMs and human expertise in urban planning.

## Method Summary
UrbanPlanBench consists of 300 multiple-choice questions (MCQs) covering three domains: fundamental principles, professional knowledge, and management/regulations. The benchmark includes two question formats—single-correct (4 options) and multi-correct (5 options, 2-4 correct answers). The paper evaluates 25 advanced LLMs using three approaches: direct prompting, retrieval-augmented generation (RAG), and chain-of-thought prompting. UrbanPlanText provides 30,000 instruction pairs derived from textbooks and exam archives for fine-tuning. The study uses LoRA-based supervised fine-tuning and employs both random and zero-shot chain-of-thought prompting strategies.

## Key Results
- LLMs show systematic knowledge imbalance: 44.16% accuracy on professional knowledge vs 38.24% on fundamental principles vs 34.52% on management/regulations
- Only one model (Qwen2-70B) surpasses the certification bar; 68% of models achieve <45% accuracy on principles and regulations
- SFT improves performance by 2.1% average accuracy, with 60% of models improving on regulations domain
- RAG and CoT prompting significantly improve performance, with CoT_zs increasing MCQ-M accuracy by 100-600%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Supervised fine-tuning with domain-specific instruction pairs improves LLM performance on specialized knowledge, particularly for regulation-heavy domains.
- **Mechanism**: UrbanPlanText provides 30,000+ instruction pairs derived from textbooks and exam archives. Fine-tuning exposes the model to domain-specific terminology, regulatory language patterns, and structured QA formats, adjusting weights to better represent this knowledge distribution. The paper reports 60% of models improved on S3 (management/regulations) after SFT, with smaller models showing larger gains (11.1% improvement for 0.5B models vs 0.7% for 32B models).
- **Core assumption**: Domain knowledge in urban planning is underrepresented in general pre-training corpora, creating a learnable gap that SFT can address.
- **Evidence anchors**:
  - [Section 3.3]: "60% of LLMs exhibited improved accuracy on full questions of S3, and 70% demonstrated enhanced accuracy on the MCQ-S questions of S3, with the average accuracy of LLMs improved by 2.1% compared to their pre-SFT counterparts."
  - [Section 3.1]: Dataset construction from 7 textbooks + 8 years of exam archives
  - [Corpus]: Related benchmark papers (ProfBench, FinS-Pilot) similarly use domain-specific instruction data, suggesting this pattern generalizes.
- **Break condition**: If the benchmark questions are too dissimilar from SFT data distribution, improvements may reflect memorization rather than knowledge acquisition. The paper notes MCQ-M accuracy remains low despite SFT, suggesting reasoning limitations persist.

### Mechanism 2
- **Claim**: Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) prompting substantially improve performance on complex multi-answer questions by providing external knowledge access and structured reasoning.
- **Mechanism**: RAG retrieves relevant textbook content or QA pairs to augment the model's context. CoT prompts the model to decompose reasoning step-by-step. For MCQ-M questions (5 options, 2-4 correct), random accuracy is 4% vs 25% for MCQ-S. The paper shows CoT_zs increases MCQ-M accuracy by 100-600% across subjects.
- **Core assumption**: Performance gaps on complex questions stem from insufficient knowledge retrieval and reasoning decomposition, not fundamental model capacity limits.
- **Evidence anchors**:
  - [Section 2.3]: "RAG_qa improve the accuracy on S1 by 30.3% and 45.5%, respectively... COT_zs increases the accuracy on MCQ-M by 100%, 600%, and 133% in the three subjects."
  - [Table 2]: Shows RAG and CoT results exceeding base GPT-4o-mini performance
  - [Corpus]: FinS-Pilot paper validates RAG for financial domains; USTBench explores spatiotemporal reasoning for urban agents.
- **Break condition**: If MCQ-M questions require domain expertise that cannot be retrieved or decomposed (e.g., implicit professional judgment), prompting alone is insufficient. The paper notes even with CoT, MCQ-M accuracy stays below 45%.

### Mechanism 3
- **Claim**: LLMs exhibit systematic knowledge imbalance across subdomains, performing better on general professional knowledge than on domain-specific regulations.
- **Mechanism**: S2 (professional knowledge) covers 8 fields (architecture, transportation, economics, etc.) with potential pre-training overlap. S1 and S3 contain specialized terminology and regulatory content likely underrepresented in general corpora. The paper reports average accuracy of 44.16% (S2) vs 38.24% (S1) and 34.52% (S3) across 25 LLMs.
- **Core assumption**: Pre-training data distributions bias LLMs toward common knowledge domains; specialized domains require explicit supplementation.
- **Evidence anchors**:
  - [Section 2.2.2]: "68% LLMs achieve accuracy lower than 45.0% in both S1 and S3, and only 16% models achieves over 50.0% accuracy in these two subjects."
  - [Section 2.2.3]: English-primary LLMs show 41.7% relative gap vs Chinese LLMs on S3, suggesting language-specific regulatory knowledge is particularly sparse.
  - [Corpus]: Limited direct corpus evidence on domain imbalance patterns; ProfBench notes similar challenges in multi-domain evaluation.
- **Break condition**: If the imbalance reflects benchmark construction bias (e.g., S3 questions being inherently harder) rather than training data distribution, the mechanism would not generalize.

## Foundational Learning

- **Multiple-Choice Question (MCQ) Evaluation for LLMs**
  - Why needed here: UrbanPlanBench uses MCQs with two formats—single-correct (4 options, 25% random) and multi-correct (5 options, 2-4 correct, 4% random). Understanding this difficulty gap is essential for interpreting results.
  - Quick check question: Why does MCQ-M pose a greater challenge for LLMs than MCQ-S, beyond lower random-guess probability?

- **LoRA (Low-Rank Adaptation) for Efficient Fine-Tuning**
  - Why needed here: The paper uses LoRA to fine-tune models on UrbanPlanText, completing training in ~4 hours on a single A100 GPU. Understanding parameter-efficient fine-tuning is necessary for replicating or extending this work.
  - Quick check question: How does LoRA reduce computational costs compared to full fine-tuning, and what trade-offs does it introduce?

- **Self-RAG for Knowledge Retrieval**
  - Why needed here: The paper uses Self-RAG for RAG experiments, distinguishing between RAG_direct (raw textbook/exam content) and RAG_qa (pre-processed QA pairs). Understanding retrieval strategies helps interpret the 30-45% accuracy improvements.
  - Quick check question: Why might pre-processing content into QA pairs (RAG_qa) outperform direct retrieval (RAG_direct) for some subjects but not others?

## Architecture Onboarding

- **Component map**:
  ```
  UrbanPlanBench (Evaluation)
  ├── S1: Fundamental Principles (100 MCQs: 80 single + 20 multi)
  ├── S2: Professional Knowledge (100 MCQs: 80 single + 20 multi)
  └── S3: Management & Regulations (100 MCQs: 80 single + 20 multi)
  
  UrbanPlanText (Training Data)
  ├── Past Exams → MCQ format + Dialog format
  └── Textbooks → Auto-generated instruction pairs (via ChatGPT)
  
  Pipeline: Raw Data → Parsing → Instruction Generation → LoRA SFT → Benchmark Evaluation
  ```

- **Critical path**:
  1. Data preparation: Parse PDFs/Word docs from textbooks and exams into structured text
  2. Instruction pair generation: Use LLM (ChatGPT) to create instruction-response pairs from descriptive text
  3. Quality validation: Human expert review (paper reports expert scores of 6.2-9.6/10 for correctness/informativeness)
  4. Fine-tuning: Apply LoRA-based SFT for 3 epochs
  5. Evaluation: Run benchmark with probability-based answer selection

- **Design tradeoffs**:
  - **MCQ-S vs MCQ-M balance**: 80/20 split prioritizes coverage over difficulty; consider increasing MCQ-M proportion if reasoning evaluation is critical.
  - **LLM-generated instruction pairs**: Faster than human annotation but introduces potential hallucination; the paper validates with expert review but this is resource-intensive.
  - **Chinese-only benchmark**: Aligns with certification exam source but limits international applicability; future work includes multi-lingual expansion.
  - **Single-retrieval RAG vs iterative RAG**: Paper uses Self-RAG with single retrieval; iterative approaches may improve complex queries but increase latency.

- **Failure signatures**:
  - **Zero accuracy on MCQ-M**: Observed in 15/75 cases, indicating model doesn't handle multi-select format well. Check output parsing and probability aggregation.
  - **Performance degradation after SFT**: Some models (e.g., LLaMA3-8B-chat) show lower post-SFT accuracy on S1/S2—potential overfitting to S3-heavy training distribution.
  - **English-primary models underperforming on S3**: 41.7% gap suggests language mismatch; verify Chinese tokenization and regulatory terminology coverage.

- **First 3 experiments**:
  1. **Baseline replication**: Evaluate 3-5 models (including Qwen2-70B as reference) on UrbanPlanBench without fine-tuning. Confirm you can reproduce reported accuracy ranges (±2%).
  2. **SFT ablation**: Fine-tune a 7B model on UrbanPlanText subsets (exams-only vs textbooks-only vs combined). Measure contribution of each data source to S1/S2/S3 performance.
  3. **RAG vs CoT comparison**: Test GPT-4o-mini with RAG_direct, RAG_qa, CoT_fs, and CoT_zs on a held-out 20-question subset. Analyze which technique helps most for MCQ-S vs MCQ-M.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can UrbanPlanBench be effectively extended into a multi-modal benchmark that integrates urban plan imagery with descriptive text?
- **Basis in paper**: [explicit] The authors state, "we aim to extend UrbanPlanBench into a multi-modal benchmark, integrating both imagery of urban plans and their corresponding descriptive text, further enriching the evaluation capabilities of LLMs in urban planning contexts."
- **Why unresolved**: The current benchmark relies solely on text-based multiple-choice questions (MCQs), whereas actual urban planning involves interpreting visual layouts, maps, and spatial data which text-only models cannot process.
- **What evidence would resolve it**: The release of an updated dataset containing image-text pairs and evaluation results showing LLM performance on visual-spatial reasoning tasks relevant to planning.

### Open Question 2
- **Question**: To what extent can UrbanPlanBench and UrbanPlanText be generalized to multi-linguistic and cross-cultural urban planning contexts?
- **Basis in paper**: [explicit] The conclusion explicitly lists future work to include "expanding both UrbanPlanBench and UrbanPlanText to incorporate multi-linguistic urban planning materials, thereby enabling broader use cases of the benchmark and dataset."
- **Why unresolved**: The current benchmark is derived exclusively from the Chinese urban planner qualification examination, which embeds China-specific regulations (S3) and cultural contexts that may not transfer to global planning scenarios.
- **What evidence would resolve it**: A study benchmarking LLMs on a translated/adapter version of the dataset across different languages, showing whether performance gaps in regulatory knowledge persist or diminish in other jurisdictions.

### Open Question 3
- **Question**: What advanced techniques beyond standard scaling and SFT are required to overcome the specific difficulty of multiple-choice questions with multiple correct answers (MCQ-M)?
- **Basis in paper**: [inferred] The paper notes that while scaling improves MCQ-S performance, "accuracy on MCQ-M questions remained low despite increasing model sizes" and suggests "merely scaling up LLMs may prove insufficient, necessitating advanced techniques."
- **Why unresolved**: Models frequently scored near 0% on MCQ-M questions (where 2-4 of 5 options are correct), indicating a failure in the complex reasoning or confidence calibration needed to identify all valid options simultaneously.
- **What evidence would resolve it**: A methodology demonstrating that specific reasoning architectures (or ensembles) can consistently lift MCQ-M accuracy significantly above the random guess baseline (4%) without sacrificing MCQ-S performance.

## Limitations
- **Chinese-language focus**: The benchmark is derived from Chinese urban planner qualification exams, limiting generalizability to non-Chinese contexts.
- **LLM-generated training data**: Reliance on ChatGPT-generated instruction pairs introduces potential hallucination risks despite expert validation.
- **MCQ-M underrepresentation**: The 20% MCQ-M proportion may underrepresent complex reasoning challenges that are critical for real-world planning scenarios.

## Confidence
- **High Confidence**: The systematic knowledge imbalance across subdomains (S2 > S1 > S3) is well-supported by consistent accuracy patterns across 25 models and 300 questions.
- **Medium Confidence**: RAG and CoT prompting improvements are well-documented but may not generalize to all urban planning scenarios.
- **Low Confidence**: The generalizability of findings to non-Chinese contexts and the long-term effectiveness of LLM-generated training data remain uncertain.

## Next Checks
1. **Multi-language extension validation**: Replicate benchmark results using translated questions to assess performance degradation patterns for non-Chinese urban planning domains.
2. **Instruction pair quality audit**: Conduct blind expert evaluation comparing human-annotated vs LLM-generated instruction pairs for hallucination rates and knowledge accuracy.
3. **Iterative RAG comparison**: Test iterative retrieval-augmented generation against the current single-retrieval approach on MCQ-M questions to measure reasoning improvement potential.