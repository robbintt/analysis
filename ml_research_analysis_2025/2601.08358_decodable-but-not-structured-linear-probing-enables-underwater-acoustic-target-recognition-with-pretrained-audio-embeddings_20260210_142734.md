---
ver: rpa2
title: 'Decodable but not structured: linear probing enables Underwater Acoustic Target
  Recognition with pretrained audio embeddings'
arxiv_id: '2601.08358'
source_url: https://arxiv.org/abs/2601.08358
tags:
- audio
- pretrained
- learning
- evaluation
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates transfer learning for Underwater Acoustic
  Target Recognition (UATR) by evaluating multiple pretrained audio models across
  diverse audio domains. It demonstrates that pretrained models effectively transfer
  to UATR when combined with a simple linear classifier, despite the embedding spaces
  being dominated by recording-specific characteristics rather than ship-type features.
---

# Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings

## Quick Facts
- arXiv ID: 2601.08358
- Source URL: https://arxiv.org/abs/2601.08358
- Reference count: 40
- This study demonstrates that transfer learning with pretrained audio models and linear probing effectively enables Underwater Acoustic Target Recognition (UATR) despite embedding spaces being dominated by recording-specific characteristics rather than ship-type features.

## Executive Summary
This study investigates transfer learning for Underwater Acoustic Target Recognition (UATR) by evaluating multiple pretrained audio models across diverse audio domains. It demonstrates that pretrained models effectively transfer to UATR when combined with a simple linear classifier, despite the embedding spaces being dominated by recording-specific characteristics rather than ship-type features. The BEATS model achieved the highest performance on the Deepship dataset (65.4% accuracy), while Wav2Vec2.0 performed best on ShipsEar (78.0% accuracy). These results show that linear probing can isolate ship-type characteristics from embeddings, significantly reducing the need for large labeled UATR datasets. The findings highlight that models pretrained on general audio or bioacoustic data transfer more effectively to UATR than those trained on marine life sounds, making transfer learning a promising approach for efficient ship sound classification in underwater environments.

## Method Summary
The study evaluates transfer learning for UATR by extracting frozen embeddings from 16 pretrained audio models across four domains (general audio, speech, bioacoustics, marine life) and training linear classifiers on these embeddings. Two benchmark datasets are used: Deepship (45+ hours, 4 ship classes, time-wise split) and ShipsEar (8 hours, 5 classes, 80/20 random split). Models are frozen during training, with only a linear classifier trained on extracted embeddings. The primary evaluation metric is classification accuracy, supplemented by NMI for clustering analysis and ROC-AUC for similarity-based retrieval. Input preprocessing varies by model (16-48 kHz, raw audio or spectrograms), with 10-second windows used for most models.

## Key Results
- BEATS achieved highest accuracy on Deepship dataset (65.4%), while Wav2Vec2.0 performed best on ShipsEar (78.0%)
- Pretrained models transferred effectively to UATR despite embedding spaces being dominated by recording-specific characteristics
- Linear probing successfully isolated ship-type information from embeddings, reducing need for large labeled UATR datasets
- Models pretrained on general audio or bioacoustic data transferred more effectively than those trained on marine life sounds

## Why This Works (Mechanism)
The study demonstrates that pretrained audio models learn general acoustic features that transfer to UATR tasks, even when the pretraining domain differs from underwater ship sounds. The linear probing approach works by isolating ship-type discriminative information from embeddings that contain dominant recording-specific characteristics. This separation occurs because the linear classifier learns to focus on features that distinguish ship types while ignoring recording-level variations in environmental conditions, hydrophone types, and recording configurations.

## Foundational Learning
- **Transfer learning**: Using knowledge gained from pretraining on one task/domain to improve performance on a different but related task. Why needed: Eliminates need for large labeled UATR datasets. Quick check: Can frozen embeddings from general audio models improve UATR performance over training from scratch?
- **Linear probing**: Training only a linear classifier on top of frozen embeddings to evaluate the quality of learned representations. Why needed: Isolates the representational power of pretrained models without confounding factors from fine-tuning. Quick check: Does linear probe performance correlate with full fine-tuning results?
- **Recording ID leakage**: When train/test splits share recordings, causing models to learn recording-specific rather than class-specific features. Why needed: Ensures models learn to recognize ship types, not recording characteristics. Quick check: Cluster embeddings by ship type vs. recording ID; recording ID should show stronger clustering
- **SSL (Self-Supervised Learning)**: Training objectives that don't require labeled data, such as contrastive learning or masked prediction. Why needed: Enables pretraining on large unlabeled audio datasets. Quick check: Compare SSL vs supervised pretraining transfer performance
- **Embedding space structure**: The geometric organization of learned representations in high-dimensional space. Why needed: Determines how well different classes separate and whether unwanted correlations exist. Quick check: Analyze NMI scores between ship type and recording ID clusters

## Architecture Onboarding

**Component map:** Pretrained model -> Embedding extraction -> Linear classifier -> Classification

**Critical path:** Audio input -> Pretrained backbone (frozen) -> Embedding layer -> Mean pooling -> Linear probe -> Classification output

**Design tradeoffs:** The choice between freezing pretrained weights (faster, less data) vs. fine-tuning (potentially better performance, requires more data) represents a key tradeoff. The study opts for frozen embeddings with linear probing to minimize training requirements.

**Failure signatures:** 
- Recording ID leakage: Embeddings cluster by recording rather than ship type (NMI(record ID) >> NMI(ship type))
- Wrong embedding layer: Baseline logistic regression on raw features outperforms frozen embeddings
- Input mismatch: Poor performance due to incompatible sample rates or input representations between pretrained model and UATR data

**First experiments:**
1. Extract embeddings from BEATS and Wav2Vec2.0 using correct input specifications (16 kHz, 10 sec windows)
2. Train linear classifier with standard hyperparameters (Adam, lr=1e-3, 50 epochs, batch size=32)
3. Perform clustering analysis on embeddings using both ship type and recording ID labels to verify recording dominance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do models pretrained specifically on marine life sounds fail to transfer effectively to UATR, despite sharing the underwater acoustic domain?
- Basis in paper: The authors note that "marine life sound models perform poorly" and that "This was unexpected, as these models were presumed to have learned some general underwater acoustic properties." They attribute this to supervised training potentially limiting generalization.
- Why unresolved: The paper speculates but does not empirically test whether the failure stems from training paradigm (supervised vs. SSL), limited class diversity, or fundamental acoustic differences between marine mammal vocalizations and ship radiated noise.
- What evidence would resolve it: Compare SSL variants of marine life models to their supervised counterparts, or pretrain an SSL model on mixed underwater audio and evaluate transfer performance.

### Open Question 2
- Question: Can the dominance of recording-specific information in pretrained audio embeddings be mitigated through domain-adaptive pretraining or architecture modifications?
- Basis in paper: The authors conclude that "the geometrical structure of the embedding space is largely dominated by recording-specific characteristics" and that "environmental factors, hydrophone types, and recording configurations likely introduce greater variance between the recordings than the various ship-types."
- Why unresolved: While the linear probe can isolate ship-type features post-hoc, the fundamental embedding structure remains problematic for unsupervised applications and may limit robustness.
- What evidence would resolve it: Pretrain a model with explicit recording-invariance objectives (e.g., contrastive learning across recording sessions) and measure whether NMI scores for ship-type clustering improve while recording-ID clustering degrades.

### Open Question 3
- Question: How does full fine-tuning compare to linear probing for UATR, and does it better suppress recording-specific bias while improving generalization?
- Basis in paper: The paper freezes all pretrained weights and only trains linear classifiers, noting this "reduces the number of trainable parameters." However, they do not evaluate whether fine-tuning could reshape the embedding space to prioritize ship-type over recording-specific features.
- Why unresolved: Fine-tuning might allow the model to learn domain-specific invariances, but could also overfit to limited UATR labels. The trade-off remains unexplored.
- What evidence would resolve it: Compare linear probing against full fine-tuning and parameter-efficient fine-tuning (e.g., LoRA) on both classification accuracy and embedding space structure using NMI analysis.

### Open Question 4
- Question: What specific preprocessing choices (sample rate, input representation, window size) optimally balance information retention and transfer performance for UATR?
- Basis in paper: The Discussion notes that "differences in sample rates may influence the results" and that "larger embedding sizes may retain more detailed information, but can also increase the risk of overfitting."
- Why unresolved: Models in the study use different preprocessing configurations, making it impossible to isolate whether performance differences stem from architecture, pretraining data, or input preprocessing.
- What evidence would resolve it: Conduct controlled experiments varying one preprocessing parameter at a time (e.g., 8 kHz vs. 16 kHz vs. 32 kHz sample rate, raw audio vs. Mel-spectrogram input) using a fixed model architecture.

## Limitations
- Study does not report linear probe hyperparameters (learning rate, epochs, batch size, regularization), making exact reproduction difficult
- Performance comparisons across domains lack statistical significance testing, and conclusions about optimal pretraining domains could change with more thorough hyperparameter search
- The claim that linear probing isolates ship-type information is supported but not rigorously validated against all possible confounders

## Confidence

**High confidence:** Pretrained models transfer effectively to UATR when combined with linear probing; embedding spaces are dominated by recording characteristics rather than ship types; general audio and bioacoustic models transfer better than marine life models

**Medium confidence:** BEATS and Wav2Vec2.0 are optimal for Deepship and ShipsEar respectively; linear probing significantly reduces labeled data requirements

**Low confidence:** Specific performance numbers and relative rankings across all 16 models, as these depend on unreported hyperparameters and exact data splits

## Next Checks
1. Reproduce results with BEATS and Wav2Vec2.0 using reported splits and standard linear probe hyperparameters (e.g., Adam, lr=1e-3, 50 epochs, batch size=32)
2. Verify embedding space characteristics by clustering on both ship type and recording ID to confirm recording dominance (expect NMI(record ID) >> NMI(ship type))
3. Test generalization by training on subset of Deepship (10% of data) and measuring accuracy retention compared to full dataset