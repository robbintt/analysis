---
ver: rpa2
title: An Exponential Averaging Process with Strong Convergence Properties
arxiv_id: '2505.10605'
source_url: https://arxiv.org/abs/2505.10605
tags:
- convergence
- averaging
- observations
- step
- p-ema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces p-EMA, a novel averaging technique for noisy
  observations along trajectories of random dynamical systems, addressing the convergence
  limitations of classical exponential moving averaging (EMA). p-EMA uses time-dependent
  weights that decay to zero at a subharmonic rate, enabling noise reduction and strong
  stochastic convergence properties.
---

# An Exponential Averaging Process with Strong Convergence Properties

## Quick Facts
- **arXiv ID**: 2505.10605
- **Source URL**: https://arxiv.org/abs/2505.10605
- **Reference count**: 5
- **Primary result**: p-EMA is a novel averaging technique that achieves strong stochastic convergence by using time-dependent weights that decay subharmonically to zero

## Executive Summary
This paper introduces p-EMA, a novel averaging technique for noisy observations along trajectories of random dynamical systems. Unlike classical exponential moving averaging (EMA), p-EMA uses time-dependent weights that decay to zero at a subharmonic rate, enabling noise reduction and strong stochastic convergence properties. The authors provide rigorous convergence guarantees under mild assumptions on autocorrelations of the underlying system, showing that p-EMA induces an averaging scheme and converges almost surely for suitable parameters. They apply these results to stochastic gradient descent (SGD), demonstrating that p-EMA can reliably estimate quantities needed for adaptive step size algorithms. Numerical experiments illustrate p-EMA's advantages over classical methods, showing faster adaptation to distributional changes and convergence to stationary means.

## Method Summary
p-EMA is an averaging technique that updates estimates using τ̂_{n+1} = γ_n τ̂_n + (1 - γ_n)eτ_{n+1}, where γ_n = 1 - 1/(n+1)^p with p ∈ (1/2, 1]. This contrasts with classical EMA which uses a constant decay factor. The key innovation is the subharmonic decay of weights (1/(n+1)^p) that ensures almost sure convergence while maintaining responsiveness to non-stationary changes. The method bridges the gap between classical arithmetic mean and EMA, offering a robust tool for online estimation in both stationary and non-stationary settings.

## Key Results
- p-EMA provides strong stochastic convergence guarantees under mild autocorrelation assumptions
- For SGD applications, p-EMA reliably estimates gradient norms and variance needed for adaptive step sizes
- Numerical experiments show p-EMA tracks non-stationary distributions faster than arithmetic mean while converging to stationary means unlike EMA

## Why This Works (Mechanism)
p-EMA works by using time-dependent weights that decay subharmonically to zero. This specific decay rate (1/(n+1)^p with p ∈ (1/2, 1]) ensures that the effective number of observations contributing to each estimate remains finite, preventing infinite accumulation of noise. The subharmonic decay is slower than harmonic (p = 1, arithmetic mean) but faster than constant (p = 0, classical EMA), providing the optimal balance between noise reduction and adaptation speed. This weight structure allows p-EMA to satisfy the conditions for almost sure convergence while maintaining responsiveness to distributional changes in non-stationary settings.

## Foundational Learning
- **Subharmonic decay**: Weight decay rate 1/(n+1)^p where p ∈ (1/2, 1). Why needed: Ensures finite effective observation count for convergence while maintaining adaptation speed. Quick check: Verify p > 1/2 strictly for convergence.
- **Exponential autocorrelation decay**: Autocorrelation functions that decay exponentially. Why needed: Required for p-EMA's almost sure convergence guarantees. Quick check: Compute sample autocorrelations of observations to verify exponential decay.
- **Almost sure convergence**: Convergence with probability 1. Why needed: Guarantees reliable estimation even in worst-case noise scenarios. Quick check: Run multiple independent trials and verify all converge.

## Architecture Onboarding
- **Component map**: Observations eτ_n → p-EMA update → Estimate τ̂_n
- **Critical path**: For each observation, compute γ_n = 1 - 1/(n+1)^p, then update τ̂_{n+1} = γ_n τ̂_n + (1 - γ_n)eτ_{n+1}
- **Design tradeoffs**: p-EMA balances noise reduction (favoring p closer to 1) with adaptation speed (favoring p closer to 1/2). Classical EMA trades strong convergence for constant responsiveness.
- **Failure signatures**: Non-convergence with p ≤ 1/2 (weights decay too slowly), counterintuitive weighting with p > 1 (older observations weighted higher), apparent convergence with vanishing noise (EMA limitation).
- **First experiments**:
  1. Compare p-EMA against EMA and arithmetic mean on stationary Gaussian noise around constant mean to verify convergence
  2. Apply p-EMA to non-stationary test data with known distributional changes to verify adaptation speed
  3. Implement p-EMA for SGD adaptive step size estimation using gradient norm observations

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees depend on unverified autocorrelation decay assumptions for the specific SGD experiments
- Specific noise parameters for both non-stationary test case and SGD application remain unspecified
- Performance gains shown are primarily qualitative rather than quantitatively benchmarked against alternatives

## Confidence
- **High confidence**: Mathematical derivation of p-EMA's convergence properties is rigorous and well-established under stated assumptions
- **Medium confidence**: Application to SGD adaptive step size estimation is theoretically sound but depends on unverified autocorrelation assumptions
- **Medium confidence**: Non-stationary experiment results are compelling but rely on estimated noise parameters and visual comparison

## Next Checks
1. **Parameter sensitivity analysis**: Systematically vary p ∈ (0.5, 1] and noise variance σ to verify convergence behavior and adaptation speed across full parameter space
2. **Autocorrelation validation**: For SGD experiments, explicitly compute and verify that gradient observations satisfy required exponential autocorrelation decay assumptions
3. **Quantitative benchmarking**: Implement alternative averaging methods and compare quantitative metrics including mean squared error and adaptation time to distributional changes