---
ver: rpa2
title: 'WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common
  Sense Categorization'
arxiv_id: '2503.23779'
source_url: https://arxiv.org/abs/2503.23779
tags:
- winogrande
- arxiv
- common
- sense
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models on the WinoGrande benchmark
  for common sense reasoning using a paraphrased version called WinoWhat, where the
  fill-in-the-blank token is moved to the end of each sentence. The researchers categorized
  the dataset into five common sense knowledge types (physical, social, numerical,
  spatial, and temporal) and tested multiple model families (Gemma 2, LlaMA 2, OPT)
  across different sizes.
---

# WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization

## Quick Facts
- arXiv ID: 2503.23779
- Source URL: https://arxiv.org/abs/2503.23779
- Reference count: 35
- Key outcome: All tested LLMs performed significantly worse on paraphrased WinoWhat compared to original WinoGrande, with minimal impact from data contamination

## Executive Summary
This study evaluates large language models on common sense reasoning using a paraphrased version of WinoGrande called WinoWhat, where fill-in-the-blank tokens are moved to sentence ends. The researchers categorized the dataset into five common sense types and tested multiple model families across different sizes. All models showed significantly degraded performance on WinoWhat compared to WinoGrande, with no category proving robust to paraphrasing. The study also investigated memorization effects through data contamination checks, finding minimal impact (1.7% contamination in RedPajama v1) and no significant performance differences between contaminated and non-contaminated instances. The results suggest that model performance on WinoGrande is overestimated due to factors beyond memorization, such as dataset artifacts, and the authors release WinoWhat as a more rigorous evaluation benchmark.

## Method Summary
The study creates WinoWhat by paraphrasing each WinoGrande validation instance so the blank token appears at the end of the sentence, using 5 SOTA LLMs followed by manual correction for 433 cases. They evaluate multiple model families (Gemma 2, LlaMA 2, OPT) across different sizes using partial evaluation, comparing the standard method (scoring tokens after the blank) with their modified approach (scoring option tokens at the end). The dataset is categorized into five common sense types (physical, social, numerical, spatial, temporal) using an LLM-based pipeline with GPT-4o-mini for reasoning and GPT-4o for categorization. They also check for data contamination by searching for n-gram overlaps between WinoGrande instances and pre-training datasets.

## Key Results
- All tested models (Gemma 2, LlaMA 2, OPT across multiple sizes) performed significantly worse on WinoWhat compared to original WinoGrande
- No common sense category showed robustness to paraphrasing—performance dropped across physical, social, numerical, spatial, and temporal types
- Data contamination analysis found only 1.7% overlap in RedPajama v1 with no significant performance differences between contaminated and non-contaminated instances
- The consistent performance drop across all categories and models implies that WinoGrande scores overestimate true reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Token Position Bias in Partial Evaluation
Moving the fill-in-the-blank token to the end significantly reduces model performance because standard partial evaluation scores tokens following the blank option, conflating correct coreference resolution with producing a probable continuation. By repositioning the blank to the end, the evaluation scores the option tokens themselves based on the entire preceding context, removing this confounding factor. The performance drop is primarily due to the evaluation methodology change rather than paraphrasing itself.

### Mechanism 2: Reliance on Superficial Dataset Artifacts over Reasoning
High performance on original WinoGrande is partially inflated by models exploiting dataset-specific statistical artifacts rather than applying general common-sense reasoning. LLMs learn subtle, non-robust correlations in benchmark construction (e.g., word co-occurrences). Paraphrasing breaks these patterns, forcing reliance on underlying understanding. The consistent performance drop across all models indicates original scores were not solely a measure of reasoning.

### Mechanism 3: Minimal Role of Direct Benchmark Memorization
Direct memorization of WinoGrande validation instances from pre-training data has minimal effect on model performance. Tested by comparing performance on instances known to be in pre-training data versus those not, a lack of significant performance difference suggests "seeing" the exact problem during training is not the primary driver of high scores. The n-gram overlap method used for contamination detection is efficient but limited in detecting semantic paraphrases.

## Foundational Learning

- **Concept:** Winograd Schema Challenge (WSC)
  - **Why needed here:** This is the foundational task. Understanding its design (twin sentences, "special word") is essential to grasp what WinoGrande adds and what WinoWhat changes.
  - **Quick check question:** How does changing a single word in a classic WSC sentence flip the correct antecedent for a pronoun?

- **Concept:** Partial Evaluation for LLMs
  - **Why needed here:** The study's central critique is based on a modification of this method. Understanding the original approach (log-likelihood of subsequent tokens) is critical.
  - **Quick check question:** In partial evaluation for "The trophy didn't fit because it was too small," what tokens' probabilities would be summed for the option "suitcase"?

- **Concept:** Data Contamination & Memorization
  - **Why needed here:** A key part of the study is investigating whether high scores are from "cheating." Understanding what constitutes contamination and how it's measured is needed to interpret results.
  - **Quick check question:** Why is n-gram overlap considered a common but limited method for detecting benchmark contamination?

## Architecture Onboarding

- **Component Map:** WinoGrande -> Paraphrase -> WinoWhat -> Model Zoo -> Partial Evaluation -> Category Analysis -> Contamination Check

- **Critical Path:** Benchmark -> Model -> Evaluation -> Analysis. Run standard evaluation on original WinoGrande and modified evaluation on WinoWhat. Slice results by pre-computed categories.

- **Design Tradeoffs:**
  - Paraphrasing: High-quality (5 SOTA LLMs + manual correction) but not fully automated; hard to scale
  - Contamination Check: Efficient string-based n-gram matching but misses semantic paraphrases
  - Categorization: LLM-based tagging is scalable but introduces potential systematic bias, though manual validation showed "substantial agreement"

- **Failure Signatures:**
  - Paraphrase Failure: Loses original meaning or fails to place blank at the end (mitigated by manual checks)
  - Contamination Failure: False positives or false negatives from n-gram matching
  - Evaluation Failure: Log-likelihood may not align with human judgment of "reasoning"

- **First 3 Experiments:**
  1. Baseline Reproduction: Run standard partial evaluation on WinoGrande validation for all models to validate setup
  2. WinoWhat Evaluation: Run modified evaluation on WinoWhat corpus; quantify performance drop
  3. Category Analysis: Disaggregate performance on both benchmarks by the five common sense categories to identify which show the largest/smallest gaps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do models use similar internal information to solve WinoGrande instances across different common sense categories, or do they employ category-specific strategies?
- **Basis in paper:** The authors state in the conclusion: "In further research, we plan to inspect the information that is used by models to solve the task per common sense category using mechanistic interpretability: do models use similar information for each category?"
- **Why unresolved:** The paper reports performance differences across categories but does not investigate what internal representations or reasoning paths models use for each category.
- **What evidence would resolve it:** Mechanistic interpretability analysis (e.g., probing classifiers, attention head analysis) comparing how models process physical vs. social vs. temporal instances.

### Open Question 2
- **Question:** Is the performance drop on WinoWhat caused by the paraphrasing itself or by the change in evaluation metric (calculating log-likelihood on option tokens vs. subsequent tokens)?
- **Basis in paper:** The limitations section states: "To verify this, we aim to construct a third level, in which we paraphrase without the constraint of putting the '_' -token at the end of the sentence... This would indicate whether the drop in performance is caused by the paraphrasing itself, or by the evaluation metric."
- **Why unresolved:** The current design conflates two changes—sentence restructuring and evaluation method—making it impossible to attribute the performance drop to either factor alone.
- **What evidence would resolve it:** A third condition with paraphrased sentences that keep the blank in its original position, evaluated with the original partial evaluation method.

### Open Question 3
- **Question:** What specific dataset artifacts in WinoGrande are models exploiting that cause performance to degrade when sentences are paraphrased?
- **Basis in paper:** The conclusion states: "Since data memorization does not seem to cause the drop in performance... we suggest to identify dataset artifacts that could be at the root of this."
- **Why unresolved:** The paper rules out memorization as the primary cause but does not identify which surface-level patterns or biases models rely on.
- **What evidence would resolve it:** Systematic analysis of linguistic features (e.g., sentence structure, word position, lexical cues) that correlate with model success on original vs. paraphrased instances.

### Open Question 4
- **Question:** Do linguistic perturbations (changes in tense, gender, synonym substitution) affect model performance on WinoGrande similarly to their documented effects on the original Winograd Schema Challenge?
- **Basis in paper:** The conclusion asks: "For instance, as previously done on WSC, do linguistic perturbations affect model performance?"
- **Why unresolved:** Prior work (Abdou et al., 2020) showed WSC models are sensitive to perturbations, but this has not been tested on WinoGrande or compared across common sense categories.
- **What evidence would resolve it:** Controlled experiments applying systematic linguistic perturbations to WinoGrande instances and measuring performance changes per category.

## Limitations
- Paraphrasing method combines automated generation with manual correction, making it difficult to scale to larger datasets
- Data contamination detection uses n-gram overlap which may miss semantic paraphrases and provide false negatives
- The performance drop could be attributed to either paraphrasing or the evaluation method change, as both were implemented simultaneously

## Confidence
- **Method Reproducibility (High):** The study provides clear implementation details for paraphrasing, evaluation, and contamination checking
- **Results Interpretation (Medium):** The attribution of performance drops to specific mechanisms is reasonable but not definitively proven
- **Generalizability (Low):** Results are based on specific model families and sizes, limiting broader conclusions about all LLMs

## Next Checks
1. Verify the partial evaluation implementation correctly calculates log-likelihoods for tokens following blank options in the original setup
2. Test the paraphrase generation pipeline with the provided prompts to ensure grammatical correctness and meaning preservation
3. Run the categorization pipeline on a small subset to validate the GPT-4o-mini and GPT-4o workflow produces consistent common sense labels