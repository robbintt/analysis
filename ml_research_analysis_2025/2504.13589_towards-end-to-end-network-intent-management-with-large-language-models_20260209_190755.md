---
ver: rpa2
title: Towards End-to-End Network Intent Management with Large Language Models
arxiv_id: '2504.13589'
source_url: https://arxiv.org/abs/2504.13589
tags:
- network
- intent
- intents
- language
- service
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using Large Language Models (LLMs) for end-to-end
  network intent management in 5G/6G networks, addressing the challenge of translating
  high-level user intents into low-level network configurations. The authors propose
  a novel framework integrating LLMs into the intent-based networking lifecycle, specifically
  focusing on intent translation and resolution.
---

# Towards End-to-End Network Intent Management with Large Language Models

## Quick Facts
- arXiv ID: 2504.13589
- Source URL: https://arxiv.org/abs/2504.13589
- Reference count: 27
- Primary result: Open-source LLMs achieve comparable or superior performance to closed-source models for network intent translation using few-shot prompting

## Executive Summary
This paper investigates using Large Language Models (LLMs) for end-to-end network intent management in 5G/6G networks, focusing on translating high-level user intents into low-level network configurations. The authors propose a novel framework integrating LLMs into the intent-based networking lifecycle, specifically addressing the translation and resolution phases. They introduce a new evaluation metric called FEACI that assesses format compliance, explainability, accuracy, cost, and inference time of generated configurations. Experiments comparing closed-source (Gemini 1.5 Pro, ChatGPT-4) and open-source (LLama, Mistral) models demonstrate that few-shot Chain-of-Thought prompting significantly improves translation accuracy, with open-source models achieving comparable or superior performance while eliminating per-token API costs.

## Method Summary
The study compares LLM performance on translating natural language intents into structured network configurations using zero-shot, one-shot, and few-shot Chain-of-Thought prompting techniques. The evaluation uses 10 synthetic Customer-Facing Service (CFS) orders mapped to Resource-Facing Service (RFS) configurations, testing both closed-source models (Gemini 1.5 Pro, GPT-4) and open-source models (Llama-3, Mistral) on hardware consisting of 12x Intel Xeon E-2236 CPU and 2x Nvidia H100 GPUs. The new FEACI metric evaluates format validity, explainability, accuracy, cost, and inference time, providing a domain-specific alternative to general metrics like BLEU and ROUGE. Each translation is run 10 times for statistical averaging with temperature=0.2 and top-p=0.9 parameters.

## Key Results
- Few-shot Chain-of-Thought prompting raises accuracy from 0% (zero-shot) to 82-93% for top-performing models
- Open-source models like Mistral-II achieve 86% accuracy versus GPT-4's 62% while incurring zero cost
- The FEACI metric better captures operational requirements than BLEU/ROUGE, which score zero for valid but non-semantically-similar configurations

## Why This Works (Mechanism)

### Mechanism 1
Few-shot Chain-of-Thought prompting substantially improves intent translation accuracy and format compliance compared to zero-shot prompting. Providing exemplar input-output pairs with reasoning steps allows the LLM to infer the target schema (JSON/YAML) and calculation logic for mapping CFS parameters to RFS configurations. The multi-head attention mechanism leverages these in-context examples to align token predictions with domain-specific output structures. Core assumption: The LLM has sufficient pre-trained knowledge of telecom concepts to generalize from examples without fine-tuning. Evidence: Zero-shot accuracy = 0% across all models; few-shot raises accuracy to 82-93% for top performers.

### Mechanism 2
Open-source models (Llama-70B, Mistral-7B, Mixtral-8x7B) can match or exceed closed-source model performance on intent translation while eliminating per-token API costs. The translation task is constrained to structured output generation within a narrow schema. Smaller models with sufficient context windows (32K-128K tokens) can perform this classification/extraction task comparably to 1T+ parameter models when given adequate in-context examples. Core assumption: Local inference hardware (e.g., 2×H100 GPUs) is available and the inference latency is acceptable for the deployment scenario. Evidence: Mistral-II achieves 86% accuracy with few-shot vs. GPT-4 at 62%, with normalized cost of 0 versus 1.0 for GPT-4.

### Mechanism 3
Domain-specific evaluation metric (FEACI) captures operational requirements that general metrics (BLEU/ROUGE) miss for network configuration tasks. FEACI decomposes evaluation into Format (machine-parseability), Explainability (reasoning quality), Accuracy (value correctness), Cost (token expense), and Inference time (latency). This reflects the IBN deployment pipeline where configurations must be consumable by orchestrators, not merely semantically similar to reference text. Core assumption: Human evaluation for explainability is feasible during benchmarking. Evidence: BLEU/ROUGE scores for zero-shot = 0.0 despite models generating text; these metrics fail to distinguish hallucinated output from valid configurations.

## Foundational Learning

- **Concept: Intent-Based Networking (IBN) lifecycle**
  - Why needed here: The paper targets only the translation and resolution phases; understanding the full 5-stage lifecycle (profiling, translation, resolution, activation, assurance) clarifies where LLMs fit and where they don't.
  - Quick check question: Can you name the two resolver types and which network layer each maps to?

- **Concept: Prompting strategies (zero-shot vs. one-shot vs. few-shot CoT)**
  - Why needed here: The experimental design hinges on these distinctions; misunderstanding them will lead to misinterpreting why accuracy jumps from 0% to 80%+.
  - Quick check question: What additional information does few-shot CoT provide over one-shot prompting?

- **Concept: TM Forum ODA / CFS-RFS service modeling**
  - Why needed here: The Business Resolver outputs CFS specifications; Service Resolver outputs RFS. These acronyms appear throughout the architecture but aren't deeply explained in the paper.
  - Quick check question: Which resolver handles mapping to "resource orders" for the Network Resource Orchestrator?

## Architecture Onboarding

- **Component map:** User chatbot → Business Intent Resolver (product catalog matching, CFS output) → Service Intent Resolver (feasibility check, RFS output) → Network Resource Orchestrator (RAN/Core deployment)

- **Critical path:**
  1. User expresses intent in natural language (e.g., "URLLC slice for Paris, 10ms latency, 1000 users")
  2. Business Resolver matches to product catalog entry, outputs CFS JSON
  3. Service Resolver maps CFS → RFS YAML with network function parameters
  4. RFS consumed by orchestrator; activation/assurance outside current scope

- **Design tradeoffs:**
  - Closed-source (Gemini, GPT-4): Lower inference latency (~15-20s), higher accuracy in some cases, but cost scales with tokens and data leaves premises
  - Open-source (Llama, Mistral): Zero marginal cost, full data sovereignty, but requires GPU infrastructure and higher latency (~40-60s for larger models)
  - Prompting complexity: Few-shot improves accuracy but increases token count 3-4×, raising cost and latency

- **Failure signatures:**
  - Zero-shot output: Text narrative instead of structured JSON/YAML; F-score = 0
  - Hallucination: Configuration values not grounded in product catalog or technical constraints
  - Format drift: Missing required fields in RFS output, causing orchestrator rejection

- **First 3 experiments:**
  1. Replicate the zero-shot vs. few-shot comparison on a single open-source model (Mistral-7B) with 5 sample CFS inputs; measure F, A, E scores.
  2. Vary the number of few-shot examples (1, 3, 5) and plot accuracy vs. inference time to find the Pareto frontier.
  3. Introduce a conflicting intent (e.g., latency <5ms in region with no edge infrastructure) and evaluate whether Service Resolver detects infeasibility or hallucinates a solution.

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced techniques like Retrieval-Augmented Generation (RAG) or fine-tuning further enhance the reasoning capabilities and evaluation scores of open-source models? Basis: The conclusion states, "Future studies should examine the techniques to further improve the reasoning capabilities and evaluation scores of the models." Why unresolved: The current study is limited to prompting strategies without utilizing external knowledge bases or model adaptation. Evidence needed: Comparative experiments benchmarking RAG-enabled or fine-tuned models against the prompting baselines using the FEACI metric.

### Open Question 2
How can LLMs be integrated into the network assurance phase to dynamically fine-tune configurations based on real-time network status? Basis: Section III.A notes that the Network Resource Orchestrator's need to "dynamically fine-tune the network configurations... is not addressed in this paper." Why unresolved: The paper focuses exclusively on the translation and resolution stages. Evidence needed: A framework demonstration where LLMs ingest network telemetry and successfully adjust active configurations to maintain Service Level Agreements (SLAs).

### Open Question 3
Does the performance of LLMs in intent translation scale effectively with a significantly larger and more diverse catalog of network services? Basis: The numerical results are derived from a catalog containing only 10 network service orders. Why unresolved: A small, fixed dataset may not capture the variability and complexity of real-world E2E network demands across different domains. Evidence needed: Evaluation of the proposed framework using a large-scale dataset with hundreds of unique intent configurations.

## Limitations

- The evaluation dataset contains only 10 synthetic CFS orders, which may not capture the full complexity of real-world network configurations
- The study assumes sufficient pre-training coverage of telecom concepts without validating the models' actual knowledge of 5G/6G-specific parameters
- The explainability evaluation relies entirely on manual assessment, creating a scalability bottleneck for production deployment

## Confidence

- **High Confidence:** The mechanism that few-shot CoT prompting improves accuracy from 0% to 80-90% is well-supported by the experimental data and aligns with established LLM literature on in-context learning.
- **Medium Confidence:** The claim that open-source models can match closed-source performance depends heavily on available GPU infrastructure, making the conclusion context-dependent rather than universally applicable.
- **Medium Confidence:** The FEACI metric's superiority over BLEU/ROUGE is demonstrated for this specific task, but the manual explainability component limits generalizability and scalability.

## Next Checks

1. **Dataset Expansion:** Test the same models on a 100+ CFS order dataset with varying complexity to assess whether performance gains scale proportionally with input diversity.
2. **Explainability Automation:** Implement an automated explainability scoring system using semantic similarity metrics between reasoning steps and domain-specific knowledge bases to eliminate manual bottlenecks.
3. **Infrastructure Feasibility:** Evaluate Mistral-7B and Llama-3-8B performance on a single A100 GPU and edge GPU (e.g., NVIDIA Jetson Orin) to establish realistic deployment boundaries for open-source alternatives.