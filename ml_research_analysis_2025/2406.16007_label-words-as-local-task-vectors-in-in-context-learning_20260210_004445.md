---
ver: rpa2
title: Label Words as Local Task Vectors in In-Context Learning
arxiv_id: '2406.16007'
source_url: https://arxiv.org/abs/2406.16007
tags:
- task
- tasks
- local
- vector
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) achieve
  in-context learning (ICL) by analyzing the role of task vectors. Previous work hypothesized
  that a single global task vector, computed by averaging embeddings across demonstrations,
  is sufficient for ICL.
---

# Label Words as Local Task Vectors in In-Context Learning

## Quick Facts
- arXiv ID: 2406.16007
- Source URL: https://arxiv.org/abs/2406.16007
- Reference count: 25
- Primary result: Local task vectors at demonstration answer positions outperform global task vectors in categorization tasks during in-context learning.

## Executive Summary
This paper challenges the prevailing view that in-context learning (ICL) relies on a single global task vector computed by averaging demonstration embeddings. Through mechanistic analysis, the authors demonstrate that large language models instead use distributed local task vectors—one per demonstration stored at each answer position. These local vectors encode demonstration-specific rules and are most effective in categorization tasks where rules must be inferred from multiple examples. In knowledge tasks, local vectors eventually converge to form a global vector in later layers, but this convergence fails in categorization tasks. The findings reveal a more nuanced, distributed mechanism of task representation in ICL than previously understood.

## Method Summary
The authors extract hidden states from demonstration answer positions to form local task vectors and from the final "is" position of the query to form global vectors. They then patch these vectors to dummy sequences (with random answers for local vectors, zero-shot inputs for global vectors) and evaluate performance layer-wise using a language modeling head. Saliency analysis identifies information flow between token positions, while dPCA reveals which features (e.g., string length vs. category labels) are encoded at different layers. The method is applied to both knowledge tasks (Country→Capital, Antonyms) and categorization tasks (Simple String, Digit, 2-D Data) using LLaMA-7B.

## Key Results
- Local task vectors at answer positions outperform global vectors in categorization tasks, with global vector patching yielding only ~50% accuracy (chance level).
- In knowledge tasks, local vectors converge to form a global vector in layers 12-15, while in categorization tasks they remain distinct.
- Information flows from demonstration answers to the query's "is" position via attention, with highest saliency at layer 14.
- dPCA analysis shows string length information peaks at early layers and decreases while label information remains stable, suggesting abstraction.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Each demonstration writes task-relevant information to its answer token position, forming a local task vector.
- Mechanism: Information flows from query tokens through attention to the answer position token of each demonstration. This token becomes a "local task vector" encoding demonstration-specific rules or associations.
- Core assumption: Attention mechanisms selectively route task information to label positions.
- Evidence anchors:
  - [abstract] "the information provided by each demonstration is first transmitted to its answer position and forms a local task vector associated with the demonstration"
  - [section 4.1] Saliency score analysis shows highest attention between demonstration answer positions and query's "is" position at layer 14
  - [corpus] Tikhonov et al. (2025) find complex ICL relies on multiple subtask-specific vectors rather than a single averaged vector—consistent with distributed mechanism
- Break condition: If answer positions are ablated or randomized in categorization tasks, performance degrades gradually (each local vector contributes), unlike knowledge tasks where all must be removed for degradation.

### Mechanism 2
- Claim: In knowledge tasks, local task vectors converge across layers to form a global task vector; in categorization tasks, they remain distinct.
- Mechanism: Through layer-wise processing, local vectors in knowledge tasks align because they reference shared pretraining knowledge (e.g., Country→Capital). Categorization task vectors remain heterogeneous because each demonstrates different rule instances.
- Core assumption: Convergence depends on whether task rules align with pretraining priors.
- Evidence anchors:
  - [abstract] "In some tasks but not in categorization tasks, all demonstrations' local task vectors converge in later layers, forming the global task vector"
  - [section 5.1] Global vector patching works at layers 12-15 for knowledge tasks; local vector patching works at layers 6-10 and fails to converge for categorization
  - [corpus] Weak direct corpus evidence on convergence dynamics; mechanism inferred from this paper's task dichotomy
- Break condition: If a knowledge task uses novel/unusual associations not in pretraining, local vectors may not converge.

### Mechanism 3
- Claim: Local task vectors encode high-level abstractions of demonstration rules, not raw input features.
- Mechanism: Early layers encode concrete features (e.g., string length); by middle layers, dPCA shows length information becomes mixed while category label remains stably encoded—suggesting abstraction.
- Core assumption: Information irrelevant to the task rule is progressively filtered.
- Evidence anchors:
  - [abstract] "local task vectors encode a high-level abstraction of rules extracted from the demonstrations"
  - [section 4.2] Mahalanobis distance for string length peaks at layer 2 and decreases; label segregation maintained across layers. Ablating length subspace in layers 13+ minimally affects performance
  - [corpus] No direct corpus confirmation of this specific abstraction mechanism
- Break condition: If early-layer ablation of concrete features severely harms performance, the abstraction hypothesis weakens.

## Foundational Learning

- Concept: **Task Vectors in ICL**
  - Why needed here: The entire paper debates whether ICL uses a single global vector or distributed local vectors—understanding vector-based task representation is prerequisite.
  - Quick check question: Can you explain how patching a task vector to a dummy input enables zero-shot prediction?

- Concept: **Attention Saliency Analysis**
  - Why needed here: Paper uses saliency scores to identify which token positions receive task-relevant attention.
  - Quick check question: What does a high saliency score between two token positions indicate about information flow?

- Concept: **PCA/dPCA for Representation Analysis**
  - Why needed here: Paper uses demixed PCA to isolate which features (length vs. label) are encoded at answer positions.
  - Quick check question: How would you interpret finding that one feature's cluster distance decreases across layers while another's remains stable?

## Architecture Onboarding

- Component map:
  - Answer position tokens: Where local task vectors form (one per demonstration)
  - "Is" position token of query: Where global task vector would form (if convergence occurs)
  - Middle layers (6-15): Where task information crystallizes; local vectors effective in 6-10, global in 12-15
  - Attention heads: Route demonstration information to answer positions (saliency peaks at layer 14)

- Critical path: Input tokens → attention to answer positions (local vector formation) → layer-wise processing → either convergence (knowledge) or distributed aggregation (categorization) → prediction

- Design tradeoffs:
  - Averaging local vectors across demonstrations (Pos Avg) often fails (Table 2)—each local vector carries complementary information
  - Trial Avg (averaging across samples) helps global vectors but is impractical for real inference
  - No single layer works for both vector types; must choose based on task type

- Failure signatures:
  - Global vector patching yields ~50% accuracy on categorization tasks (chance level)
  - Model outputs non-label tokens when task representation is insufficient
  - Early-layer dPCA ablation harms categorization; late-layer ablation does not

- First 3 experiments:
  1. Replicate saliency analysis (Fig. 3) on your model to confirm answer positions receive highest attention from query.
  2. Patch local vs. global vectors at layers 6-10 and 12-15 on a knowledge task (Capital) and categorization task (Simple String) to observe the divergence.
  3. Run dPCA ablation on answer positions at layers 2, 8, and 14 to verify that early layers encode concrete features while later layers encode abstract rules.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanism determines whether local task vectors will converge into a global task vector or remain distributed?
- Basis in paper: [explicit] The paper observes that "in some tasks but not in categorization tasks, all demonstrations' local task vectors converge in later layers, forming the global task vector," but does not explain what task properties or model factors determine this convergence behavior.
- Why unresolved: The paper characterizes the phenomenon descriptively but does not identify the causal factors or circuit mechanisms that drive convergence vs. non-convergence.
- What evidence would resolve it: Systematic experiments varying task properties (e.g., rule complexity, alignment with pretraining knowledge) and analyzing attention patterns/circuits responsible for vector aggregation across layers.

### Open Question 2
- Question: Why do patching methods fail to restore performance in some model-task combinations despite success in others?
- Basis in paper: [explicit] From Limitations: "Neither global nor local task vectors work consistently across all tasks and in all types of LLMs tested. We are yet to determine the mechanism causing this inconsistency."
- Why unresolved: The paper demonstrates the inconsistency empirically but does not investigate the architectural, training, or representational differences that cause it.
- What evidence would resolve it: Comparative analysis across failing and succeeding model-task pairs, examining hidden state geometry, attention head specialization, and whether alternative positions or layers contain the critical information.

### Open Question 3
- Question: What specific attention heads or circuit components write information to local task vectors and read from them during inference?
- Basis in paper: [inferred] The paper establishes that answer positions serve as local task vectors and uses saliency scores to show information flows from these positions, but does not identify which specific heads or subcircuits implement this information routing.
- Why unresolved: The methodology relies on aggregate saliency analysis rather than circuit-level mechanistic interpretation or head-level ablation studies.
- What evidence would resolve it: Head-level ablation experiments targeting attention patterns between answer positions and query positions; activation patching at the level of individual heads to identify the write/read circuits.

## Limitations
- The paper does not explain what determines whether local task vectors converge into a global vector or remain distributed, leaving a key mechanistic question unresolved.
- The abstraction claim is supported by indirect dPCA evidence but lacks definitive proof that vectors encode abstract rules rather than task-specific embeddings.
- The task dichotomy (knowledge vs. categorization) is well-demonstrated but the paper does not explore hybrid or more complex tasks where both mechanisms might interact.

## Confidence
- **High Confidence**: The existence of local task vectors and their superiority over global vectors in categorization tasks. The saliency analysis and patching experiments provide robust evidence for this core finding.
- **Medium Confidence**: The convergence mechanism in knowledge tasks. While supported by layer-wise patching results, the exact conditions and dynamics of convergence are not fully characterized.
- **Medium Confidence**: The abstraction claim. The dPCA evidence is suggestive but not definitive proof that vectors encode abstract rules rather than learned embeddings.

## Next Checks
1. **Convergence Boundary Test**: Design a knowledge task with slightly unusual associations (e.g., "France→Berlin") to test whether local vectors fail to converge, confirming that convergence depends on pretraining priors.

2. **Feature Ablation Robustness**: Systematically ablate both early-layer concrete features and late-layer abstract features across multiple task types to quantify the exact contribution of each to performance.

3. **Hybrid Task Analysis**: Create tasks that combine knowledge and categorization elements to observe whether both local and global vectors are simultaneously active, revealing interaction mechanisms.