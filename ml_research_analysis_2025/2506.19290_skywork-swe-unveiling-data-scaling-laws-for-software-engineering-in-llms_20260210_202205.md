---
ver: rpa2
title: 'Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs'
arxiv_id: '2506.19290'
source_url: https://arxiv.org/abs/2506.19290
tags:
- data
- instances
- skywork-swe
- software
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Skywork-SWE project addresses the challenge of developing high-quality,
  large-scale datasets for training Large Language Models (LLMs) in software engineering
  (SWE) tasks. Existing datasets are limited in scale and diversity, hindering the
  progress of LLM-driven software engineering.
---

# Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs

## Quick Facts
- arXiv ID: 2506.19290
- Source URL: https://arxiv.org/abs/2506.19290
- Reference count: 19
- Primary result: Skywork-SWE-32B achieves 38.0% pass@1 accuracy on SWE-bench Verified, establishing new state-of-the-art among Qwen2.5-Coder-32B-based LLMs

## Executive Summary
Skywork-SWE addresses the critical bottleneck of limited, high-quality datasets for training LLMs on software engineering tasks. The project introduces an automated data curation pipeline that scales both the volume and diversity of training instances by leveraging execution-validated task instances from GitHub repositories. Using this dataset, the authors fine-tune Qwen2.5-Coder-32B-Instruct to achieve 38.0% pass@1 accuracy on SWE-bench Verified without verifiers or multiple rollouts, surpassing previous SOTA results for sub-32B parameter models. The work also reveals a clear log-linear scaling law in SWE performance with increased training data, showing consistent improvements without saturation at 8,209 instances.

## Method Summary
The Skywork-SWE pipeline automates the creation of execution-grounded training data through a three-stage process: (1) GitHub repository and pull request collection with attribute filtering, (2) installation-based validation to ensure environment reproducibility, and (3) execution-based validation using Docker-isolated environments with unit test verification. The team generates training trajectories using OpenHands v0.32.0 with multiple high-performing proprietary LLMs (Gemini-2.5-Pro, GPT-4.1, DeepSeek-V3, etc.), validating each trajectory by applying the final patch and running unit tests. The resulting dataset comprises 10,169 Python task instances from 2,531 distinct repositories, with 8,209 validated multi-turn agent trajectories. The model is fine-tuned using TorchTune on 8 NVIDIA H800 GPUs for 12 hours, employing AdamW optimizer with cosine learning rate schedule.

## Key Results
- Achieves 38.0% pass@1 accuracy on SWE-bench Verified without verifiers or multiple rollouts
- With test-time scaling (N=8), performance improves to 47.0% accuracy, surpassing previous SOTA for sub-32B parameter models
- Demonstrates clear log-linear scaling law: performance improves from 6.4% (125 trajectories) to 38.0% (8,209 trajectories) without saturation
- Dataset covers 2,531 distinct GitHub repositories with 10,169 Python task instances, each with dedicated runtime environment

## Why This Works (Mechanism)

### Mechanism 1: Execution-Grounded Instance Validation
Filtering task instances through runtime-validated unit tests produces training data with reliable correctness signals. The pipeline applies test patches to base commits, executes test suites in Docker-isolated environments, and retains only instances with non-empty FAIL_TO_PASS sets—meaning the patch demonstrably fixes at least one failing test.

### Mechanism 2: Multi-Model Trajectory Aggregation
Combining successful trajectories from multiple proprietary LLMs yields higher-quality coverage than single-model collection. Multiple models roll out trajectories via OpenHands; only trajectories passing all unit tests are retained for supervised fine-tuning.

### Mechanism 3: Log-Linear Data Scaling Without Saturation
SWE task performance follows a log-linear scaling law with training trajectory count, showing no saturation at 8,209 instances. As trajectory count increases, the model encounters more diverse repository patterns, error types, and solution strategies—improving generalization.

## Foundational Learning

- **Concept: FAIL_TO_PASS vs. PASS_TO_PASS Tests**
  - Why needed: Understanding test classification is essential for interpreting instance validation. FAIL_TO_PASS tests fail before the patch and pass after—confirming the fix.
  - Quick check: If a patch causes PASS_TO_PASS tests to fail, what does this indicate about the patch quality?

- **Concept: Agent Trajectory (Multi-Turn Interaction Sequence)**
  - Why needed: Training data consists of full interaction histories across up to 100 turns, not single patches. Understanding trajectory structure is prerequisite to data curation and SFT.
  - Quick check: Why might training on full trajectories outperform training on final patches alone?

- **Concept: Docker Layer Hierarchy for Environment Isolation**
  - Why needed: The pipeline builds three image levels (base → environment → instance) to balance reuse and isolation. Understanding this hierarchy is critical for debugging environment setup failures.
  - Quick check: Why does the instance-level image need to checkout a specific commit hash before running tests?

## Architecture Onboarding

- **Component map:** GitHub API → Repository Metadata → PR Collection → Installation Validation → Runtime Environment Builder (Docker 3-level) → Execution Validator (Empty Test → Gold Test) → Trajectory Generator (OpenHands + Multi-Model) → Trajectory Validator (Patch + Unit Tests) → SFT Dataset → Skywork-SWE-32B Model

- **Critical path:** Execution-based validation is the gating bottleneck—invalid environments cascade into lost instances and failed trajectory collection. Prioritize robust configuration before scaling rollout.

- **Design tradeoffs:**
  - Unified default configuration vs. per-repository manual setup:前者 maximizes automation but causes 84% installation failures; latter improves coverage but doesn't scale
  - Maximum 100 rollout turns vs. unlimited: Capping turns bounds compute but may truncate solutions for complex issues
  - Single-model vs. multi-model trajectory collection: Multi-model increases diversity but adds API costs and coordination complexity

- **Failure signatures:**
  - High installation failure rate (84%) indicates configuration gaps
  - Low per-model resolve rates (5–20%) indicate task difficulty, not pipeline failure
  - Context length overflow (>32K tokens at >50 turns) truncates trajectories; requires sequence parallelism for 128K support

- **First 3 experiments:**
  1. Baseline reproduction: Run OpenHands v0.32.0 with Qwen2.5-Coder-32B-Instruct on SWE-bench Verified (N=1) to verify the reported 6.4% baseline
  2. Data scaling sweep: Train separate models on subsets of the released trajectories (1K, 2K, 4K, 8K) and plot resolve rate vs. log(N_traj) to confirm scaling trend
  3. Test-time scaling ablation: Evaluate Skywork-SWE-32B with Best-of-N (N=1,2,4,8) and varying max rollout turns (10,25,50,100) to characterize inference-compute tradeoffs

## Open Questions the Paper Calls Out

- **Language Generalization:** Do data scaling laws for software engineering generalize to non-Python programming languages? Current benchmarks focus exclusively on Python, and expanding evaluation to multiple languages is "essential for a more comprehensive assessment."

- **Online RL Potential:** Can online reinforcement learning surpass supervised fine-tuning when utilizing execution-validated rewards? The authors identify exploring online RL methods as a "promising direction" enabled by their runtime setup, noting they left this "engineering-intensive task for future work."

- **Automated Environment Configuration:** Can specialized agents automate runtime environment configuration to reduce data loss during curation? The unified configuration strategy results in "significant data loss," suggesting that agents capable of automatically setting up environments are a promising direction.

## Limitations

- The unified default configuration approach results in 84% attrition during installation validation, suggesting significant coverage gaps and dataset diversity limitations
- The log-linear scaling relationship is based on a single trajectory sweep from 125 to 8,209 instances; the relationship may not hold beyond this range or could plateau at larger scales
- The exact composition of the 8,209 training trajectories—specifically which proprietary model trajectories were selected and how "format consistency" filtering operated—remains unspecified

## Confidence

- **High Confidence:** The baseline performance of 38.0% pass@1 accuracy on SWE-bench Verified is well-supported by the described methodology and consistent with the multi-model trajectory aggregation approach
- **Medium Confidence:** The 47.0% accuracy with test-time scaling (N=8) is plausible given the reported Best-of-N improvements, though the specific critic model implementation details are not fully specified
- **Medium Confidence:** The log-linear scaling law observation is supported by the presented data but requires validation at larger scales beyond 8,209 instances

## Next Checks

1. **Scaling Law Validation:** Replicate the trajectory collection and training process at multiple scales (1K, 2K, 4K, 8K, 12K instances) to verify the log-linear relationship holds and identify potential saturation points

2. **Environment Configuration Robustness:** Test alternative environment setup strategies (per-repository vs. unified configuration) to quantify the impact of the 84% installation failure rate on dataset diversity and model performance

3. **Reproducibility Audit:** Implement the complete data pipeline using the specified OpenHands v0.32.0 framework and verify that the same set of proprietary models with identical temperature settings can reproduce the reported trajectory validation rates and final training dataset composition