---
ver: rpa2
title: Dynamic Expert-Guided Model Averaging for Causal Discovery
arxiv_id: '2601.16715'
source_url: https://arxiv.org/abs/2601.16715
tags:
- noisy
- clean
- causal
- expert
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of combining multiple causal
  discovery algorithms in practical settings where data may violate assumptions and
  expert knowledge is valuable. The authors propose a novel expert-guided model averaging
  approach that dynamically requests expert knowledge to resolve disagreements between
  ensemble members.
---

# Dynamic Expert-Guided Model Averaging for Causal Discovery

## Quick Facts
- arXiv ID: 2601.16715
- Source URL: https://arxiv.org/abs/2601.16715
- Reference count: 40
- Primary result: Expert-guided averaging improves causal discovery performance by 10-20% BSF over baseline ensembles

## Executive Summary
This work introduces a novel expert-guided model averaging approach for causal discovery that dynamically requests expert knowledge to resolve disagreements between ensemble members. The method treats causal discovery as a model averaging problem where diverse algorithms vote on edge presence and orientation, with an expert consulted only when algorithms lack consensus. Experimental results show significant performance improvements over baseline approaches on both clean and noisy data, with the method demonstrating better precision-recall trade-offs compared to individual algorithms or ensembles without expert guidance.

## Method Summary
The approach ensembles 8 diverse causal discovery algorithms (constraint-based, score-based, hybrid, and continuous optimization) and aggregates their outputs through a greedy, cycle-free construction process. When algorithms disagree on edge presence or orientation, the method queries an expert (simulated or LLM-based) to resolve the discrepancy. The method uses two thresholds: θ₁=0.0 for edge existence (any edge appearing once considered) and θ₂=0.7 for orientation (higher threshold to reduce expert queries). The LLM experiments used gpt-5-nano-2025-08-07 with specific prompts for acceptConnection and determineOrientation queries.

## Key Results
- Expert-guided averaging achieved 10-20% BSF improvements over baseline ensembles with 80% correct simulated experts
- Performance showed better precision-recall trade-offs compared to individual algorithms or ensembles without expert guidance
- Linear relationship between expert accuracy and method performance; LLM experiments showed comparable performance to simulated expert
- Method demonstrated effectiveness on both clean and noisy synthetic categorical data from 7 benchmark Bayesian networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective expert querying reduces expert burden while maintaining accuracy improvements
- Mechanism: Two thresholds (θ₁ for edge existence, θ₂ for orientation) gate expert queries—expert consulted only when model vote counts fall between threshold and majority (0.5)
- Core assumption: Expert knowledge is valuable but costly/imperfect; experts perform better on orientation than existence queries
- Evidence anchors: [abstract] "queries experts when models disagree or lack a majority vote"; [section 3.1] "when the percentage is above θ₁ but below 0.5 (a majority vote), we query the expert e to potentially reject the presence of an edge"; [corpus] "Learning to Defer for Causal Discovery with Imperfect Experts" confirms expert reliability varies by domain and query type

### Mechanism 2
- Claim: Decoupling edge existence from orientation improves ensemble coherence
- Mechanism: Connection counts weight any direct edge (x→y, y→x, x−y) equally for existence decisions; orientation determined separately with higher threshold (θ₂=0.7) or expert consultation
- Core assumption: Edge existence and orientation have different detection difficulty; LLMs better at causal ordering than existence determination
- Evidence anchors: [section 3.1] "crucially, our method treats the existence of a direct connection separately from the orientation of that connection"; [section 4.5] "performance is higher on orientation queries than existence queries, indicating that the LLM is better at determining plausible causal orderings"; [corpus] Vashishtha et al. (2025, referenced in paper) support restricting LLMs to causal ordering questions

### Mechanism 3
- Claim: Greedy edge addition by frequency while enforcing acyclicity produces valid causal graphs that aggregate algorithm consensus
- Mechanism: Edges sorted by occurrence count across ensemble, added sequentially only if cycle-free; when one orientation creates cycle, alternative forced
- Core assumption: Frequently occurring edges across diverse algorithms indicate true causal relationships
- Evidence anchors: [section 3.1] "greedily adds edges that avoid creating cycles by order of most occurrence in the component models M"; [section 3.1] Algorithm 1 pseudocode shows explicit cycle check via m.edgeCreatesCycle(x, y); [corpus] Constantinou (2019a) in paper references uses similar greedy model averaging strategy

## Foundational Learning

- Concept: **Causal Discovery Algorithm Classes**
  - Why needed here: Method ensembles 8 algorithms from constraint-based (PC-Stable, FCI), score-based (Tabu, GES, MAHC, BOSS), hybrid (MMHC), and continuous optimization (DAG-GNN)—must understand their differing outputs and assumption violations
  - Quick check question: Why might PC output undirected edges while Tabu produces fully directed graphs on the same data?

- Concept: **Model Averaging vs Ensembling**
  - Why needed here: Core approach aggregates graph structures (not predictions); understanding edge voting, threshold selection, and consensus building is essential
  - Quick check question: What's the difference between majority voting on edges and weighted averaging based on algorithm confidence scores?

- Concept: **DAG Acyclicity Constraints**
  - Why needed here: Method must ensure output remains acyclic during greedy construction; requires understanding topological ordering and incremental cycle detection
  - Quick check question: If current graph has path A→B→C, can you add edge C→A? What about C→D→A?

## Architecture Onboarding

- Component map:
  - Input Layer: 8 causal discovery algorithms run independently on dataset → produce set M of candidate graphs
  - Aggregation Layer: connectionCounts(M) tallies edge co-occurrences; edgeCounts(M, x, y) tallies directed edges
  - Decision Layer: Thresholds θ₁=0.0 (existence), θ₂=0.7 (orientation) determine expert query necessity
  - Expert Interface: acceptConnection(x,y) for existence; determineOrientation(x,y) for direction
  - Construction Layer: Greedy DAG builder with cycle detection, edge-by-edge assembly

- Critical path:
  1. Run all algorithms → collect M models
  2. Compute connection counts C across all node pairs
  3. For each (x,y) sorted by count: if c/n ≥ θ₁ and (c/n > 0.5 OR expert accepts) → proceed to orientation
  4. Check cycle feasibility; if both orientations valid, use θ₂ threshold or expert query
  5. Add directed edge; repeat until all pairs processed

- Design tradeoffs:
  - θ₁=0.0 prioritizes recall over precision (any edge appearing once considered)
  - θ₂=0.7 reduces expert queries but may miss correct orientations below threshold
  - Expert interface requires ~80% correctness for meaningful improvement (Figure 2)
  - LLM fallback to majority vote on inconsistent responses trades accuracy for reliability

- Failure signatures:
  - High expert query counts indicate algorithm disagreement (Table 5: Pathfinder requires 938 calls at n=100)
  - LLM existence queries at ~50% accuracy (ALARM network) wastes expert budget
  - Timeouts/memory on large graphs (BOSS, Aslani-Mohebbi fail on Pathfinder 109 nodes)
  - Empty/fully sparse graphs produce invalid precision scores

- First 3 experiments:
  1. **Sanity check on ASIA**: Run on 8-node network with simulated 80% expert; verify BSF ~0.58, F1 ~0.69 at n=1000; confirm expert receives ~12-15 queries
  2. **Expert correctness sweep**: Vary simulated expert accuracy 50-100% on clean ALARM data; plot BSF/F1 curves; identify minimum viable expert (~70% for positive gain over Bayesys baseline)
  3. **LLM existence vs orientation**: Query GPT model on ASIA and SimSUM; measure separate accuracies for acceptConnection vs determineOrientation; expect 70-75% existence, 85-100% orientation per Table 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the dynamic expert-guided model averaging method maintain its performance advantages when applied to continuous, mixed, or real-world data types?
- Basis in paper: [Explicit] Page 10 states, "Future work can seek to address these limitations by employing our method on different data types... including on real data," noting that experiments were limited to synthetic, categorical data.
- Why unresolved: The paper currently relies solely on synthetic, categorical datasets (Bayesys and SimSUM), and it is unknown if the method generalizes to continuous or mixed data distributions common in broader healthcare applications.
- What evidence would resolve it: Empirical evaluations of the method using benchmark datasets containing continuous or mixed variables, and results from application to raw, non-synthetic clinical datasets.

### Open Question 2
- Question: How does the method perform under diverse real-world noise conditions compared to the specific synthetic noise distributions tested?
- Basis in paper: [Explicit] Page 10 notes, "the synthetic noise we use cannot be expected to cover all real-world conditions," identifying this as a limitation to be addressed in future work.
- Why unresolved: The noise models used in the experiments are standardized (based on Constantinou et al., 2021) and may not fully capture the complexity, missingness patterns (e.g., MNAR), or systematic errors found in actual clinical environments.
- What evidence would resolve it: Benchmarking the ensemble method against baseline algorithms using data corrupted by varied, naturalistic noise patterns or by deploying the method in a live data collection environment.

### Open Question 3
- Question: Can applying this model averaging strategy across multiple runs of a single algorithm provide improved stability and efficiency compared to ensembling diverse algorithms?
- Basis in paper: [Explicit] Page 10 lists as an "additional possibilit[y]... using our model averaging method across multiple runs of the same chosen algorithm rather than across an ensemble of disparate algorithms."
- Why unresolved: The current study only evaluates ensembling distinct algorithms (e.g., PC, FCI, Tabu). It is unknown if averaging a single non-deterministic algorithm (or a single algorithm across different data partitions) would yield similar gains in stability.
- What evidence would resolve it: A comparative analysis measuring the variance and accuracy of the ensemble when the component models $M$ are derived from repeated runs of a single stochastic algorithm versus a diverse set of algorithms.

### Open Question 4
- Question: Can the LLM prompting strategy be effectively extended to explicitly identify latent confounders?
- Basis in paper: [Inferred] Page 5, Footnote 3 states, "The detection of latent confounders is not directly considered in the current prompts, but could be an interesting avenue for future work."
- Why unresolved: The current `acceptConnection` and `determineOrientation` prompts focus on direct causal links between observed variables. The method currently lacks a mechanism for the expert (LLM) to suggest that a correlation is due to an unobserved common cause.
- What evidence would resolve it: Extending the prompt templates to include latent confounder identification and measuring the resulting accuracy in recovering bi-directed edges (as seen in FCI outputs).

## Limitations

- Expert accuracy sensitivity: Method performance degrades linearly with expert correctness, requiring ~80% accuracy for meaningful gains
- Scalability constraints: BOSS and Aslani-Mohebbi algorithms fail on larger networks (Pathfinder 109 nodes)
- Limited data types: Experiments restricted to synthetic categorical data; generalizability to continuous/mixed data unknown
- Heuristic thresholds: θ₁=0.0 and θ₂=0.7 appear chosen heuristically rather than optimized

## Confidence

**High Confidence**: Claims about selective expert querying mechanism (θ thresholds gating expert consultations), linear expert accuracy-performance relationship, and improved precision-recall trade-off over baseline ensembles.

**Medium Confidence**: Claims about LLM performance comparability to simulated experts, effectiveness of decoupling edge existence from orientation, and practical applicability in healthcare settings.

**Low Confidence**: Claims about scalability to large networks, generalizability of expert query patterns across domains, and optimal threshold selection methodology.

## Next Checks

1. **Expert Correctness Threshold Analysis**: Systematically vary simulated expert accuracy from 50-100% on multiple networks and measure the minimum expert accuracy required to outperform Bayesys baseline ensemble.

2. **LLM Query Type Performance Gap**: Measure and compare LLM accuracy separately for existence vs orientation queries across multiple LLM models and datasets to confirm the observed 10-20% accuracy gap.

3. **Algorithm Agreement Impact Study**: Analyze how method performance varies with the level of agreement between ensemble algorithms by artificially constraining algorithm outputs to show high vs low consensus, measuring expert query frequency and resulting performance.