---
ver: rpa2
title: 'JEEM: Vision-Language Understanding in Four Arabic Dialects'
arxiv_id: '2503.21910'
source_url: https://arxiv.org/abs/2503.21910
tags:
- image
- arabic
- caption
- evaluation
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We introduce JEEM, a benchmark designed to evaluate Vision-Language
  Models (VLMs) on visual understanding across four Arabic-speaking countries: Jordan,
  The Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning and
  visual question answering, and features culturally rich and regionally diverse content.'
---

# JEEM: Vision-Language Understanding in Four Arabic Dialects

## Quick Facts
- arXiv ID: 2503.21910
- Source URL: https://arxiv.org/abs/2503.21910
- Reference count: 33
- Key outcome: We introduce JEEM, a benchmark designed to evaluate Vision-Language Models (VLMs) on visual understanding across four Arabic-speaking countries: Jordan, The Emirates, Egypt, and Morocco.

## Executive Summary
This paper introduces JEEM, a novel benchmark for evaluating Vision-Language Models (VLMs) on culturally-rich visual understanding tasks across four Arabic dialects: Jordanian, Emirati, Egyptian, and Moroccan. The benchmark includes image captioning and visual question answering tasks featuring regionally diverse content. Through comprehensive evaluation of five Arabic VLMs and GPT-4V, the authors find that Arabic VLMs consistently underperform, particularly on dialect-specific generation and cultural grounding. GPT-4V ranks best overall but still shows dialectal variation in performance. The results highlight the need for more culturally-inclusive VLMs and demonstrate the value of dialect-aware evaluation paradigms.

## Method Summary
The JEEM benchmark was constructed through a multi-stage process involving three data sources, team leader filtering, and annotation by four regional teams (Jordanian, Emirati, Egyptian, Moroccan). The annotation protocol followed a four-step process: image selection, caption generation, question writing, and answer writing, with different annotators for steps 2-4. The benchmark includes 2,178 images covering 13 topics across the four dialects. Evaluation employed both traditional metrics (CIDEr, BERTScore, etc.) and GPT-4-based evaluation in two settings (image-and-reference, reference-only), supplemented by human evaluation on 50 images per dialect. Five Arabic VLMs plus GPT-4V were benchmarked on consistency, relevance, fluency, and dialect authenticity.

## Key Results
- Arabic VLMs consistently underperform on both visual understanding and dialect-specific generation tasks
- GPT-4V ranks best overall but shows dialectal variation, struggling particularly with Emirati dialect
- Traditional evaluation metrics show low correlation with human judgments (13.2-24.5 τc), while GPT-4 reference-only evaluation shows higher correlation (34.7-39.1 τc)
- Models achieve high fluency scores (>4.0) but significantly lower relevance, consistency, and dialect authenticity scores (1.5-3.0)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cultural familiarity affects visual object recognition accuracy.
- Mechanism: Annotators from different regions exposed to the same image identify objects differently based on their cultural priors; Emirati annotators correctly identified Omani Halwa, while Jordanian, Egyptian, and Moroccan annotators misidentified it as regionally similar but distinct desserts.
- Core assumption: Regional visual experience creates culture-specific object priors that influence recognition.
- Evidence anchors:
  - [Section 4.2]: "One notable example is shown in Figure 4. It involves an image of Omani Halwa... only the Emirati annotator correctly identified it as Omani Halwa. In contrast, the Jordanian annotator misidentified it as Karawya... both the Moroccan and Egyptian annotators mistakenly described it as a chocolate dessert."
  - [Section 2.1]: "People from different cultures use different objects, have different traditions, and occupy different physical environments, resulting in different visual experiences and associations."
  - [Corpus]: Related work on cultural bias in object recognition (De Vries et al., 2019; Gustafson et al., 2023) documents performance disparities across geographic regions.
- Break condition: If visual recognition were purely based on universal low-level features without top-down cultural priors, regional differences would not emerge for shared visual stimuli.

### Mechanism 2
- Claim: Dialect representation in training data predicts downstream dialect generation quality.
- Mechanism: Models with more training data in specific dialects (Egyptian) outperform those with less data (Emirati); GPT-4o's performance drops significantly on Emirati split across both linguistic and visual grounding metrics.
- Core assumption: Dialect-specific generation requires dialect-specific training signal, not just MSA transfer.
- Evidence anchors:
  - [Section 5.1.4]: "GPT-4o struggles considerably with the Emirati split of JEEM, in both linguistic and visual grounding terms. This points to the importance of having good regional coverage in the data used to train VLMs."
  - [Section 5.1.4]: "Models struggle more with low-resource dialects like Emirati compared to high-resource variants like MSA and Egyptian."
  - [Corpus]: DialectalArabicMMLU (arXiv:2510.27543) finds similar dialect gaps in text-only LLMs, suggesting the problem is not purely multimodal.
- Break condition: If MSA provided sufficient substrate for dialect transfer via phonological/morphological rules, performance gaps across dialects would be smaller and more uniform.

### Mechanism 3
- Claim: Reference-only GPT evaluation correlates better with human judgment than image+reference or traditional metrics for culturally-loaded tasks.
- Mechanism: GPT-based evaluation in reference-only setting reduces model-self-evaluation bias while capturing semantic adequacy; traditional count-based metrics fail due to morphological complexity and dialectal variation.
- Core assumption: GPT-4 can assess semantic and cultural adequacy without visual input when given a high-quality reference.
- Evidence anchors:
  - [Section 5.1.4]: "We see a considerably higher correlation against human judgements in the image-and-reference setting (34.7), and an even higher correlation (39.1) with references alone... We posit that the reference-only setting may reduce the bias of using GPT4 to evaluate GPT-4o."
  - [Section 5.1.4]: "The correlation scores are low for all four metrics, and in fact lowest for BERTScore."
  - [Corpus]: VLURes (arXiv:2510.12845) reports similar limitations of traditional metrics for low-resource VLM evaluation, though does not test reference-only conditions.
- Break condition: If GPT-4 lacked cultural/dialect knowledge, reference-only correlation would not exceed image+reference; if morphological normalization solved the problem, BERTScore would outperform count metrics.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) as aligned multimodal systems
  - Why needed here: JEEM evaluates whether VLMs trained predominantly on Western/English data can transfer to culturally-distinct Arabic contexts. Understanding what VLMs learn during vision-language alignment explains why they fail on cultural grounding.
  - Quick check question: Given a CLIP-style contrastive objective, why would the model learn "church" but not "mosque interior" equally well?

- Concept: Arabic dialect continuum vs MSA diglossia
  - Why needed here: JEEM explicitly tests dialectal generation (not MSA), and results show models fail to produce authentic dialect. Understanding that Arabic speakers use MSA for formal writing but local dialects for daily life clarifies why both caption tasks are needed.
  - Quick check question: If a model only sees MSA training data, will it generate Egyptian dialect by applying phonological rules? Why or why not?

- Concept: Reference-based vs reference-free caption evaluation
  - Why needed here: The paper finds reference-only GPT evaluation correlates best with humans. Understanding the difference between measuring similarity to ground truth (CIDEr) vs measuring adequacy/fluency directly (GPT-based) explains this finding.
  - Quick check question: For a culturally-specific image where multiple valid descriptions exist, why would CIDEr penalize a correct but phrased-differently response?

## Architecture Onboarding

- Component map: Data collection (3 sources) -> Team leader filtering -> 4 regional annotation teams (Jordanian, Emirati, Egyptian, Moroccan) -> Annotation protocol (4 steps) -> Evaluation framework (traditional metrics + GPT-4 + human evaluation) -> Model benchmarking harness (5 Arabic VLMs + GPT-4V)

- Critical path: 1) Native speaker recruitment and qualification (determines annotation quality ceiling) 2) Cross-dialect shared image pool annotation (enables cross-cultural analysis, 100 images × 4 dialects) 3) GPT-4 evaluation prompt design (determines metric validity; prompts in Appendix C) 4) Human evaluation sampling (50 images/dialect, blind to source)

- Design tradeoffs: Dataset size (2,178 images) vs coverage (4 dialects × 13 topics) — authors acknowledge size limitation in Limitations; Automatic vs human evaluation — cost constraints limit human eval to 200-image sample; Single vs multiple annotators per image — annotation speed prioritized over inter-annotator agreement analysis; GPT-4 as both model and evaluator — reference-only setting attempts to mitigate self-evaluation bias

- Failure signatures: Visual misidentification: Non-Gulf annotators call Omani Halwa "chocolate dessert" (Figure 4); Dialect collapse: Models default to MSA or mix dialects; Emirati dialect score (2.20 for GPT-4o) far below Moroccan (4.38); Metric disagreement: Traditional metrics show low τc (13.2-24.5) vs GPT-based (34.7-39.1), indicating they measure different constructs; Fluency-semantic gap: All models achieve fluency scores 4.0+ but relevance/consistency 1.5-3.0, showing language model backbone works but visual grounding fails

- First 3 experiments: 1) Reproduce evaluation on 50-image sample: Run GPT-4 evaluation (reference-only) on sampled predictions from any model, verify correlation with reported human scores; this validates your evaluation pipeline. 2) Controlled dialect ablation: Take a model with Arabic support (e.g., AyaV), prompt explicitly for each dialect with few-shot dialect examples, measure whether dialect authenticity improves vs default prompting. 3) Cross-dialect transfer test: Evaluate a model fine-tuned on Egyptian on Moroccan data to quantify whether dialect-specific training transfers; if not, this confirms dialect isolation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using a "reference-only" evaluation setting with Large Language Models (LLMs) consistently reduce evaluation bias compared to "image-and-reference" settings across different VLMs?
- Basis in paper: [explicit] The authors note that GPT-4 evaluation with references alone yielded a higher correlation with human judgment (39.1) than the combined image-and-reference setting (34.7). They posit this "may reduce the bias of using GPT4 to evaluate GPT-4o" but contradict prior findings by Tong et al. (2024).
- Why unresolved: The paper observes this phenomenon specifically within the context of GPT-4 evaluating GPT-4o on Arabic dialects. It remains unclear if this is a universal methodological improvement or an artifact of the specific models and languages tested.
- What evidence would resolve it: A comparative study evaluating multiple distinct VLM architectures using both reference-only and multimodal LLM-as-a-judge configurations, measuring correlation with human evaluations across diverse linguistic and visual tasks.

### Open Question 2
- Question: How can Vision-Language Models be optimized to bridge the performance gap between generating fluent text and producing authentic dialectal speech?
- Basis in paper: [inferred] The results (Table 3) consistently show that models achieve high Fluency scores (often >4.0) but significantly lower Dialect Authenticity (DA) scores (often <3.0). The authors explicitly state: "This suggests that while the LM backbone is good at generating fluent MSA, the visual and dialectal aspects of the task is where the models can further improve."
- Why unresolved: Current training objectives appear to prioritize standard language fluency, leaving the specific lexical and syntactic features of regional dialects under-learned or suppressed.
- What evidence would resolve it: Ablation studies on VLMs fine-tuned with varying ratios of MSA versus dialectal data, specifically analyzing the trade-off between general fluency metrics and dialect-specific authenticity scores.

### Open Question 3
- Question: To what extent does the performance gap observed in the Emirati dialect generalize to other low-resource Gulf dialects?
- Basis in paper: [explicit] The paper highlights that "models struggle more with low-resource dialects like Emirati compared to high-resource variants like MSA and Egyptian." The limitations section further notes the dataset focuses "on only four Arabic dialects, leaving out many others."
- Why unresolved: JEEM covers only one representative from the Gulf region (Emirati). It is unclear if the poor performance is specific to the relative scarcity of Emirati training data or representative of a broader failure to handle the specific linguistic features of the Gulf dialect group.
- What evidence would resolve it: Evaluation of existing SOTA VLMs on a newly compiled benchmark containing other Gulf dialects (e.g., Saudi, Kuwaiti, Qatari) to see if performance patterns correlate with Emirati results or vary significantly by country.

## Limitations
- Dataset size may not provide sufficient coverage to fully characterize VLM performance on Arabic dialects
- Annotation protocol uses different annotators for captions and questions/answers, potentially introducing inconsistencies
- Human evaluation sample of 50 images per dialect (200 total) may miss rare cultural artifacts
- The benchmark covers only four Arabic dialects, excluding many others

## Confidence
- **High Confidence**: The observation that GPT-4V outperforms Arabic VLMs but still shows dialectal variation
- **Medium Confidence**: The mechanism that cultural priors affect visual recognition
- **Medium Confidence**: The dialect training data hypothesis
- **Medium Confidence**: The superiority of reference-only GPT evaluation

## Next Checks
1. **Inter-Annotator Agreement Analysis**: Re-run the 100 shared images with all 4 annotator teams to measure inter-annotator agreement on both visual descriptions and cultural interpretations.

2. **Controlled Dialect Fine-tuning Experiment**: Take a VLM with strong MSA capabilities (like Qwen-VL-7B-Instruct) and fine-tune it separately on Egyptian vs Emirati dialect data using the JEEM training split.

3. **Reference-Free GPT Evaluation Validation**: Implement a reference-free GPT-4 evaluation on the same 50-image human evaluation sample and compare correlation with human judgments against the reference-only setting.