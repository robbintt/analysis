---
ver: rpa2
title: Sentiment Analysis in Learning Management Systems Understanding Student Feedback
  at Scale
arxiv_id: '2506.05490'
source_url: https://arxiv.org/abs/2506.05490
tags:
- feedback
- sentiment
- learning
- student
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research integrated sentiment analysis into Learning Management
  Systems (LMS) to address communication gaps in online education, particularly the
  absence of non-verbal cues. A deep neural network model incorporating word embedding,
  LSTM, and attention mechanisms was developed and compared against a logistic regression
  baseline using the RateMyProfessor dataset.
---

# Sentiment Analysis in Learning Management Systems Understanding Student Feedback at Scale

## Quick Facts
- **arXiv ID**: 2506.05490
- **Source URL**: https://arxiv.org/abs/2506.05490
- **Reference count**: 18
- **Primary result**: Bi-LSTM with attention achieves 80% accuracy on binary sentiment classification of student comments

## Executive Summary
This research addresses communication gaps in online education by integrating sentiment analysis into Learning Management Systems to interpret student feedback at scale. The study develops a deep neural network model combining word embedding, bidirectional LSTM, and attention mechanisms, which is then compared against a logistic regression baseline using the RateMyProfessor dataset. The RNN model demonstrates superior performance across all metrics, achieving 80% accuracy, 83% precision, 85% recall, and an F1-score of 0.84. The findings suggest that sentiment analysis can effectively interpret student feedback in online learning environments, providing valuable insights for educators to enhance course quality and address communication challenges in digital education settings.

## Method Summary
The study employs a deep learning approach for binary sentiment classification of student comments to simulate feedback analysis in Learning Management Systems. The methodology involves preprocessing student comments by removing stopwords, applying WordNet lemmatization, and handling missing data. The RateMyProfessor dataset (approximately 20,000 comments) is used, with SMOTE applied to balance the dataset. For the baseline, logistic regression is trained on TF-IDF vectors with Chi-squared feature selection. The proposed model uses an embedding layer followed by bidirectional LSTM and attention mechanisms, with sigmoid output for binary classification. The models are validated using an 80/20 train/test split.

## Key Results
- Bi-LSTM with attention model achieves 80% accuracy, outperforming logistic regression baseline
- Model demonstrates strong precision (83%) and recall (85%) in sentiment classification
- F1-score of 0.84 indicates balanced performance across precision and recall metrics
- Attention mechanism effectively captures important features for sentiment classification

## Why This Works (Mechanism)
The integration of attention mechanisms with bidirectional LSTM allows the model to capture both contextual information from sequential data and focus on the most relevant words for sentiment classification. The bidirectional architecture processes text in both forward and backward directions, providing a more comprehensive understanding of context compared to unidirectional models. Word embeddings capture semantic relationships between words, while the attention mechanism dynamically weights different parts of the input sequence based on their importance for the classification task. This combination effectively addresses the challenge of interpreting student feedback in online learning environments where non-verbal cues are absent.

## Foundational Learning
- **Bidirectional LSTM**: Processes sequences in both forward and backward directions to capture context from all positions; needed for understanding context-dependent sentiment; quick check: verify model processes sequences bidirectionally
- **Attention Mechanisms**: Dynamically weights input features based on importance for the task; needed to focus on sentiment-bearing words; quick check: confirm attention weights align with sentiment-bearing words
- **Word Embeddings**: Maps words to dense vector representations capturing semantic relationships; needed to represent textual data numerically; quick check: verify similar words have similar embeddings
- **SMOTE**: Synthetic Minority Over-sampling Technique for balancing imbalanced datasets; needed to prevent bias toward majority class; quick check: confirm class balance after SMOTE application
- **TF-IDF**: Term Frequency-Inverse Document Frequency for feature extraction; needed for baseline logistic regression; quick check: verify top features make semantic sense
- **Chi-squared Feature Selection**: Statistical method for selecting most relevant features; needed to reduce dimensionality for logistic regression; quick check: confirm selected features improve model performance

## Architecture Onboarding

**Component Map**
Raw Text -> Preprocessing (Stopwords Removal, Lemmatization) -> Tokenization -> Embedding Layer -> Bidirectional LSTM -> Attention Mechanism -> Sigmoid Output

**Critical Path**
Embedding Layer -> Bidirectional LSTM -> Attention Mechanism -> Output Layer

**Design Tradeoffs**
The bidirectional LSTM captures context from both directions but increases computational complexity compared to unidirectional alternatives. The attention mechanism adds interpretability and focus but requires additional parameters. The choice of sigmoid output enables binary classification while maintaining simplicity compared to softmax for multi-class scenarios.

**Failure Signatures**
The model struggles with negation handling, as evidenced by misclassification of sentences like "not engaging but informative." The attention mechanism may fail to properly transfer negative sentiment to subsequent positive words, leading to incorrect classifications in complex sentiment expressions.

**First Experiments**
1. Test model performance on sentences containing negation to verify proper sentiment transfer
2. Evaluate attention weight distributions to ensure focus on sentiment-bearing words
3. Compare performance across different rating ranges to identify classification thresholds

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can incorporating transformer-based models with contextual embeddings improve the model's ability to recognize word meaning in different contexts compared to the current LSTM approach?
- Basis in paper: [explicit] The Conclusion states, "To enhance our modelâ€™s capabilities, we propose incorporating transformer models... By employing contextual embedding, the model will better recognize the meaning of words in different contexts."
- Why unresolved: The current study utilized an LSTM with global embeddings, which the authors identify as a limitation for handling complex contextual nuances.
- What evidence would resolve it: A comparative study showing that a transformer-based model (e.g., BERT) outperforms the proposed LSTM model on the same dataset, particularly in disambiguating polysemous words.

**Open Question 2**
- Question: How can the model architecture be refined to accurately capture sentiment in sentences containing negation, such as "not engaging but informative"?
- Basis in paper: [explicit] Section 3.2.3 notes that both the RNN and baseline models "struggle with transferring negativity to subsequent words," leading to the misclassification of specific test sentences containing negation.
- Why unresolved: The current attention mechanism and LSTM layers failed to correctly classify sentences where a negative modifier (e.g., "not") alters the sentiment of a subsequent positive descriptor.
- What evidence would resolve it: Successful classification of the specific failure cases cited in Table 3 (e.g., Sentence 8) and improved performance on a dataset rich in negation structures.

**Open Question 3**
- Question: Does the sentiment analysis model trained on the RateMyProfessor dataset generalize effectively to the private, domain-specific feedback found in actual Learning Management Systems?
- Basis in paper: [inferred] Section 2.1 states that the RateMyProfessor dataset was used as a "proxy" because actual LMS feedback is "restricted and private," and it captures a "diverse range" of institutions compared to a single LMS.
- Why unresolved: The model was validated on a public review platform, which may differ in language, length, and sentiment distribution from the internal feedback mechanisms of a specific LMS.
- What evidence would resolve it: Evaluation of the trained model's accuracy and F1-score on a held-out dataset of actual student comments extracted from a university LMS.

**Open Question 4**
- Question: Does the integration of sentiment analysis into LMS platforms lead to measurable improvements in educational efficacy and student satisfaction?
- Basis in paper: [explicit] The Introduction states that "it remains unknown how learning experiences and educational efficacy will evolve to enhance student experiences" despite the integration of these platforms.
- Why unresolved: The study validates the technical accuracy of the sentiment analysis tool but does not assess the downstream impact on teaching strategies or learning outcomes.
- What evidence would resolve it: Longitudinal studies comparing course evaluations and student performance metrics in courses where instructors utilized the sentiment analysis tool versus a control group.

## Limitations
- **Neutral Class Handling**: Ambiguous treatment of neutral sentiment ratings (2.5-3.4 stars) creates uncertainty in binary classification methodology
- **SMOTE Implementation**: Unclear whether SMOTE was applied before train/test split, potentially causing data leakage
- **Hyperparameter Specification**: Missing critical model parameters (embedding dimensions, LSTM units, learning rate, batch size, epochs) prevents exact reproduction

## Confidence
- **High Confidence**: General methodology (Bi-LSTM + Attention architecture, preprocessing pipeline, 80/20 train-test split) is clearly specified
- **Medium Confidence**: Overall performance improvement over baseline is reliable, though exact metrics may vary
- **Low Confidence**: Precise numerical results cannot be faithfully reproduced without resolving neutral class handling and hyperparameter questions

## Next Checks
1. Clarify Neutral Class Handling: Contact authors to determine exact mapping/exclusion criteria for neutral ratings (2.5-3.4 stars) used in binary classification experiments
2. Verify SMOTE Implementation: Confirm SMOTE was applied only to training split after 80/20 partition, not to full dataset, to prevent data leakage
3. Hyperparameter Recovery: Request or empirically determine embedding dimensions, LSTM hidden units, attention configuration, and training parameters (learning rate, batch size, epochs) used to achieve reported performance metrics