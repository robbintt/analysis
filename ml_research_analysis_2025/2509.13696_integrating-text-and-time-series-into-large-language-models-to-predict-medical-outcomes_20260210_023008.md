---
ver: rpa2
title: Integrating Text and Time-Series into (Large) Language Models to Predict Medical
  Outcomes
arxiv_id: '2509.13696'
source_url: https://arxiv.org/abs/2509.13696
tags:
- clinical
- data
- text
- time-series
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the integration of text and time-series data
  into large language models (LLMs) for medical outcome prediction. The authors adapt
  instruction-tuned LLMs using DSPy-based prompt optimization to jointly process clinical
  notes and structured EHR inputs, comparing performance with specialized multimodal
  systems.
---

# Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes

## Quick Facts
- arXiv ID: 2509.13696
- Source URL: https://arxiv.org/abs/2509.13696
- Reference count: 17
- Instruction-tuned Llama 3.1 outperforms BioClinicalBERT on clinical text tasks, and multimodal integration of text and time-series data improves mortality prediction.

## Executive Summary
This study investigates the integration of clinical text and structured time-series data into large language models (LLMs) for medical outcome prediction. The authors adapt Llama 3.1 using DSPy-based prompt optimization and instruction-tuning to jointly process clinical notes and structured EHR inputs, comparing performance with specialized multimodal systems. Results demonstrate that instruction-tuned Llama 3.1 outperforms domain-specific models like BioClinicalBERT on text-only clinical tasks, highlighting the value of supervised fine-tuning. For mortality prediction, integrating time-series data—either as raw numeric input or textual summaries—consistently improves model performance. While Llama 3.1 achieves the highest AUROC scores, BioClinicalBERT excels in AUPRC, reflecting trade-offs between overall discrimination and sensitivity to the positive class. The findings highlight the potential of LLMs for multimodal clinical reasoning, while also emphasizing the competitiveness of smaller, specialized models in resource-constrained settings.

## Method Summary
The study compares Llama 3.1 and BioClinicalBERT on four clinical tasks: Smoking Status (5-class), MedNLI (3-class NLI), ClinSTS (binary similarity), and In-Hospital Mortality (binary). Llama 3.1 is fine-tuned using Adapter-V2 with a dual objective (causal language modeling + linear classification on last-token embedding) and DSPy prompt optimization for zero-shot baselines. BioClinicalBERT is fine-tuned with a classification head. Time-series features (13 aggregated to 6 means over 48h) are appended as formatted text ("feature: val1, val2, ...") or descriptive summaries. Models are evaluated on AUROC and AUPRC for mortality prediction.

## Key Results
- Instruction-tuned Llama 3.1 outperforms BioClinicalBERT on text-only clinical tasks.
- Integrating time-series data (numeric or textual summaries) consistently improves mortality prediction AUROC/AUPRC.
- Llama 3.1 achieves higher AUROC; BioClinicalBERT achieves higher AUPRC, reflecting a trade-off between overall discrimination and positive-class sensitivity.
- DSPy zero-shot prompting underperforms fine-tuned models across all text tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning enables general-purpose LLMs to outperform domain-specific encoders on clinical text tasks.
- Mechanism: Supervised fine-tuning with explicit task instructions teaches the model to map clinical prompts to correct labels via a dual-objective training scheme: (1) causal language modeling on label tokens and (2) a linear classification head projecting the last-token embedding to class logits. Both losses are backpropagated jointly, adapting only 0.05% of parameters via Adapter-V2.
- Core assumption: The instruction-tuning dataset is representative of target clinical tasks and label distributions; the linear head on the last-token embedding captures sufficient signal for classification.
- Evidence anchors:
  - [abstract]: "instruction-tuned Llama 3.1 outperforms domain-specific models like BioClinicalBERT on text-only clinical tasks, demonstrating the value of supervised fine-tuning"
  - [section 4.3]: "both the language modeling and the embedding classification are trained together, and the losses from both objectives are backpropagated through the model"
  - [corpus]: ProMedTS (arXiv 2502.13509) similarly uses prompt-guided multimodal integration, providing convergent evidence for instruction-based approaches.
- Break condition: If training data has severe label imbalance or task instructions are misaligned with evaluation prompts, gains may not transfer; zero-shot DSPy prompting underperforms across all text tasks (Table 1), indicating fine-tuning is necessary.

### Mechanism 2
- Claim: Textualizing time-series data (as raw values or summaries) improves mortality prediction across model families.
- Mechanism: Time-series features are aggregated (e.g., 6 means over 48 hours) and appended as structured text ("heart rate: 76.09, 78.75, ..."). LLMs process numeric tokens alongside clinical notes, enabling joint multimodal reasoning without custom fusion architectures. Alternatively, Llama-generated textual summaries describe stability, deviations, trends, and concerns.
- Core assumption: Tokenized numeric sequences or their summaries preserve predictive temporal patterns; truncation from context limits (especially for BERT's 512 tokens) does not destroy critical signal.
- Evidence anchors:
  - [abstract]: "integrating time-series data—either as raw numeric input or textual summaries—consistently improves model performance"
  - [section 5.1.2]: "all 13 additional features are appended to the text as a list of feature names, followed by a sequence of comma-separated values"
  - [section 5.2]: "numeric integration of time-series features proves more effective than descriptive text integration"
  - [corpus]: OpenTSLM (arXiv 2510.02410) and Generative Foundation Model for EHR (arXiv 2508.16054) similarly textualize structured data, convergent but not causal proof.
- Break condition: If critical events occur in truncated portions (avg. text length 683 tokens exceeds BERT's 512 tokens), performance may degrade. Numeric integration outperforms descriptions—summaries may lose granular signal.

### Mechanism 3
- Claim: Smaller domain-specific models remain competitive on positive-class sensitivity (AUPRC) despite lower AUROC.
- Mechanism: BioClinicalBERT's encoder architecture and clinical pre-training may yield embeddings better calibrated for minority-class detection in imbalanced settings, while Llama's decoder optimization favors overall discrimination (AUROC). The trade-off emerges from architectural inductive biases and fine-tuning objectives.
- Core assumption: Mortality is a low-prevalence outcome where AUPRC captures clinically meaningful sensitivity; the linear head on BERT's [CLS] token preserves class-discriminative information.
- Evidence anchors:
  - [abstract]: "Llama 3.1 achieves the highest AUROC scores, BioClinicalBERT excels in AUPRC, reflecting trade-offs between overall discrimination and sensitivity to the positive class"
  - [section 5.2]: "BioClinical BERT with numeric time-series input obtains the best AUPRC score"
  - [corpus]: Weak direct evidence; Temporal Fusion Nexus (arXiv 2601.08503) reports AUROC but not AUPRC comparisons. This trade-off is observed but not causally explained in the paper.
- Break condition: If deployment prioritizes minimizing false negatives (high recall on positive class), BioClinicalBERT may be preferred despite lower AUROC; resource constraints (Table 2: BERT uses ~46× less energy than Llama for fine-tuning) further favor smaller models.

## Foundational Learning

- Concept: **DSPy Prompt Optimization**
  - Why needed here: DSPy programmatically generates and selects prompts via Bayesian optimization (MIPROv2), reducing manual prompt engineering. It is used for zero-shot baselines and contrasts with instruction-tuning.
  - Quick check question: Given a classification task and training examples, can you explain how DSPy would propose and evaluate candidate instructions?

- Concept: **Parameter-Efficient Fine-Tuning (Adapter-V2)**
  - Why needed here: Fine-tuning 8B-parameter Llama 3.1 is feasible only by updating 0.05% of weights via adapters, reducing memory and compute while retaining task adaptation.
  - Quick check question: What are the trade-offs between full fine-tuning and adapter-based approaches in terms of catastrophic forgetting and inference latency?

- Concept: **AUROC vs. AUPRC in Imbalanced Classification**
  - Why needed here: Mortality prediction is low-prevalence; AUROC can be misleadingly high, while AUPRC better reflects positive-class performance. The paper reports both to capture trade-offs.
  - Quick check question: If prevalence drops from 10% to 1%, which metric would you expect to change more dramatically, and why?

## Architecture Onboarding

- Component map:
  Input layer (clinical notes + time-series) -> Backbone (Llama 3.1 or BioClinicalBERT) -> Training objectives (LM + classification) -> Output (class prediction)

- Critical path:
  1. Preprocess EHR: extract notes, aggregate time-series (6 means/48h), format as structured text.
  2. Truncate to context limit (2048 for Llama; 512 for BERT, discarding from end).
  3. Fine-tune: 5 epochs, Llama on RTX A6000 (48GB), BERT on RTX 3090 (24GB).
  4. Evaluate: AUROC and AUPRC on held-out test; report median over 3 runs.

- Design tradeoffs:
  - **Numeric vs. descriptive time-series**: Numeric yields higher AUROC/AUPRC; descriptions are more interpretable but lose granularity.
  - **Llama vs. BERT**: Llama achieves higher AUROC and handles longer context; BERT is 10–40× more energy-efficient (Table 2) and better on AUPRC.
  - **Context length vs. truncation loss**: BERT truncates 68% of combined text+TS inputs (avg. truncated length 195 tokens vs. 683 original), potentially losing signal.

- Failure signatures:
  - DSPy zero-shot underperforms (AUROC ~51% on mortality) → indicates prompt-only approaches insufficient; supervised fine-tuning required.
  - AUPRC plateaus despite AUROC gains → positive-class sensitivity may require architectural changes (e.g., focal loss, oversampling) not explored here.
  - Truncation correlates with feature omission (e.g., Glasgow Coma Scale missing in 80% of cases but its removal drops AUPRC ~3 points) → critical sparse features may be lost.

- First 3 experiments:
  1. **Baseline replication**: Fine-tune BioClinicalBERT on text-only mortality; measure AUROC/AUPRC. Confirm ~83/39 baseline from Table 1.
  2. **Time-series ablation**: Add numeric TS to BERT; measure delta. Expect AUROC ~89–90, AUPRC ~64 (matching paper).
  3. **Truncation analysis**: Compare BERT performance with (a) end-truncation vs. (b) sliding-window or hierarchical pooling to retain more signal. Hypothesis: reducing truncation loss may close AUROC gap with Llama.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for integrating structured time-series data into LLMs beyond simple textual concatenation?
- Basis in paper: [explicit] The authors explicitly ask "How can structured time-series data best be incorporated into LLM-based workflows to support clinical decision-making?" (p. 1) and note that while raw numeric data outperformed textual descriptions, the mechanism remains underexplored.
- Why unresolved: The study only compares raw numeric concatenation against high-level textual summaries; other architectural fusion methods (e.g., cross-attention, projection layers) were not tested.
- What evidence would resolve it: Comparative studies evaluating learned embeddings or specialized adapter layers versus the current concatenation approach.

### Open Question 2
- Question: Does the input truncation of BERT models artificially widen the performance gap with LLMs in multimodal tasks?
- Basis in paper: [inferred] The authors note that BERT's 512-token limit caused significant text truncation when combining text and time-series data (avg. truncated length 194.78 tokens), potentially discarding relevant information (p. 3).
- Why unresolved: The study does not isolate variable context length as an independent factor; it is unclear if Llama's superior performance stems from model capability or simply retaining more input data.
- What evidence would resolve it: An ablation study where Llama is restricted to BERT's context length, or where BERT is allowed a longer context window.

### Open Question 3
- Question: Why do smaller specialized models (BioClinicalBERT) exhibit higher sensitivity to the positive class (AUPRC) than larger general-purpose models?
- Basis in paper: [inferred] Results show BioClinicalBERT outperforms Llama 3.1 on AUPRC despite lower AUROC, suggesting a fundamental trade-off between overall discrimination and positive class sensitivity (p. 5).
- Why unresolved: The paper reports the phenomenon but does not analyze the underlying representation differences or training dynamics driving this divergence.
- What evidence would resolve it: An analysis of confidence calibration and error distribution across different class prevalences for both model types.

## Limitations
- Performance gains may not generalize beyond the specific clinical datasets used (MIMIC-III derived, i2b2 Smoking, ClinSTS).
- BERT's 512-token limit causes significant text truncation when combining text and time-series data, potentially discarding clinically relevant information.
- Exact training hyperparameters, prompt templates, and optimization procedures are not fully specified, hindering exact replication.

## Confidence
- **High confidence**: Instruction-tuning Llama 3.1 outperforms zero-shot DSPy prompting across all text tasks; time-series integration improves mortality prediction across model families; Llama achieves higher AUROC, BioClinicalBERT higher AUPRC.
- **Medium confidence**: Numeric time-series integration is more effective than textual descriptions; the AUPRC/AUROC trade-off reflects architectural differences in positive-class sensitivity.
- **Low confidence**: Generalizability to other clinical datasets or languages; impact of truncation on clinical performance; optimality of dual-objective fine-tuning compared to alternative methods.

## Next Checks
1. **Truncation impact analysis**: Systematically evaluate mortality prediction performance when varying BERT's input truncation point (e.g., preserve beginning vs. end of text, sliding window) to quantify information loss from truncation.
2. **Cross-dataset generalization**: Replicate the instruction-tuning and multimodal integration approach on a held-out clinical dataset (e.g., eICU) to assess robustness and generalization beyond MIMIC-III.
3. **Missing feature handling ablation**: Evaluate mortality prediction performance with and without commonly missing features (e.g., Glasgow Coma Scale) to determine their true impact on AUPRC/AUROC trade-offs.