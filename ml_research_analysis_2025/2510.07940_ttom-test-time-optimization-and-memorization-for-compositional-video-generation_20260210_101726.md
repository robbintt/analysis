---
ver: rpa2
title: 'TTOM: Test-Time Optimization and Memorization for Compositional Video Generation'
arxiv_id: '2510.07940'
source_url: https://arxiv.org/abs/2510.07940
tags:
- generation
- optimization
- memory
- video
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compositional text-to-video
  generation, where current models struggle with complex scenes involving multiple
  objects, attributes, and spatial-temporal relationships. The proposed Test-Time
  Optimization and Memorization (TTOM) framework introduces a training-free method
  that aligns video outputs with spatiotemporal layouts during inference.
---

# TTOM: Test-Time Optimization and Memorization for Compositional Video Generation

## Quick Facts
- **arXiv ID:** 2510.07940
- **Source URL:** https://arxiv.org/abs/2510.07940
- **Reference count:** 21
- **Primary result:** Training-free TTOM framework achieves 34% and 14% relative gains over CogVideoX-5B and Wan2.1-14B on compositional video generation benchmarks while maintaining high visual quality.

## Executive Summary
The paper addresses the challenge of compositional text-to-video generation where current models struggle with complex scenes involving multiple objects, attributes, and spatial-temporal relationships. The proposed Test-Time Optimization and Memorization (TTOM) framework introduces a training-free method that aligns video outputs with spatiotemporal layouts during inference. Instead of directly modifying latents or attention maps, TTOM optimizes new parameters guided by layout-attention objectives and maintains historical optimization contexts using a parametric memory mechanism with flexible operations like insert, read, update, and delete. This approach effectively disentangles compositional world knowledge, enabling powerful transferability and generalization.

## Method Summary
TTOM is a training-free framework for compositional text-to-video generation that operates during inference. It uses GPT-4o to generate spatiotemporal layouts (bounding box sequences per object across frames) which are converted to soft masks via Gaussian smoothing. The method extracts cross-attention maps from specific DiT layers identified through attention-layout relevance probing, then optimizes LoRA parameters (rank 32) using JSD loss between attention maps and layout masks for the first 5 denoising steps. A parametric memory stores optimized LoRA weights keyed by scene abstractions, enabling retrieval and reuse for similar prompts. This approach aligns video generation with compositional constraints without modifying the foundation model's weights.

## Key Results
- Achieves 34% and 14% relative gains over CogVideoX-5B and Wan2.1-14B on T2V-CompBench benchmark
- Maintains high visual quality while improving semantic consistency across object classification, multi-object handling, and spatial relations
- Demonstrates superior transferability through parametric memory mechanism with insert/read/update/delete operations
- Outperforms state-of-the-art methods on VBench benchmark for compositional video generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing lightweight parameters to align cross-attention maps with spatiotemporal layouts improves compositional adherence without disrupting the foundation model's feature distributions.
- **Mechanism:** The method extracts cross-attention maps from specific DiT layers that show strong layout correlation (empirically identified via mIoU probe in Figure 3), then computes Jensen-Shannon Divergence between attention maps and Gaussian-smoothed layout masks. LoRA parameters are updated via gradient descent during early denoising steps, steering attention toward prescribed layouts without directly modifying latents.
- **Core assumption:** Attention maps in certain layers encode spatial-semantic correspondence that, when aligned with explicit layouts, propagates to compositional correctness in the generated video.
- **Evidence anchors:**
  - [abstract]: "we integrate and optimize new parameters guided by a general layout-attention objective"
  - [section 3.2]: "the updated φ* memorizes layout patterns for specific compositional scenes, holding a potential for future re-usage"
  - [corpus]: Weak direct support; Video-T1 explores test-time scaling for video generation but focuses on compute scaling rather than layout alignment. Comp-Attn addresses compositional video via attention but uses different intervention strategies.
- **Break condition:** If attention-layout correlation in DiT layers is weak or inconsistent across prompts, JSD loss optimization may not propagate to meaningful compositional changes—empirically, Table 7 shows saturation/degradation beyond 5 optimization steps, suggesting over-alignment disrupts other signals.

### Mechanism 2
- **Claim:** A parametric memory mechanism storing optimized LoRA weights enables transfer of compositional patterns across prompts, reducing per-sample optimization cost while maintaining or improving quality.
- **Mechanism:** After TTO, LoRA parameters are stored as values with abstracted scene descriptions as keys (e.g., "<object A> drifts right to left above <object B>"). For new prompts, memory retrieval loads historical parameters as initialization, either bypassing optimization entirely or reducing required iterations.
- **Core assumption:** Compositional patterns (motion trajectories, spatial relations, numeracy configurations) are partially disentangled and reusable across semantically similar prompts.
- **Evidence anchors:**
  - [abstract]: "maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations"
  - [section 3.3]: "Superiority. The history optimization maintained in the memory provides abundant scene knowledge, serving as good initialization for TTO"
  - [corpus]: JEDI uses JSD for disentanglement in diffusion models at test-time but targets attention disentanglement rather than cross-prompt memory transfer. No direct corpus evidence for parametric memory in video generation.
- **Break condition:** If scene abstractions are too coarse, retrieved parameters introduce noise rather than useful priors—Table 4 shows Top-k=10 underperforms Top-k=5, indicating retrieval noise degrades initialization quality.

### Mechanism 3
- **Claim:** LLM-generated spatiotemporal layouts provide explicit structural constraints that ground ambiguous compositional prompts, enabling the optimization to target specific spatial-temporal configurations.
- **Mechanism:** GPT-4o generates bounding box sequences per object across frames, with verification for spatial-temporal consistency. Layouts are smoothed into soft masks for JSD computation, translating natural language into differentiable constraints.
- **Core assumption:** LLMs capture sufficient physical and compositional reasoning to produce coherent layouts, and these layouts correctly interpret user intent.
- **Evidence anchors:**
  - [abstract]: "aligns VFM outputs with spatiotemporal layouts during inference"
  - [section 3.1]: "LLMs possess strong spatial-temporal dynamics understanding abilities, and capture physical properties, such as gravity, elasticity, and perspective projection"
  - [corpus]: No direct corpus evidence for LLM-driven layout in video; corpus papers focus on VFM training or test-time adaptation without explicit layout intermediaries.
- **Break condition:** If LLM-generated layouts are physically implausible or misinterpret prompts, optimization converges to correctly aligned but semantically wrong outputs—no failure analysis provided in paper.

## Foundational Learning

- **Concept: Cross-Attention in Diffusion Transformers**
  - **Why needed here:** The entire mechanism depends on extracting and manipulating cross-attention maps. Unlike U-Nets where cross-attention is localized, DiTs distribute attention more evenly (Section 3.2), requiring layer-wise relevance probing before intervention.
  - **Quick check question:** Can you explain why early denoising steps (timesteps 1-5) are chosen for optimization, and what happens if optimization extends to later steps?

- **Concept: Test-Time Training/Optimization**
  - **Why needed here:** TTOM adapts a frozen foundation model at inference without backpropagating through base weights. Understanding the distinction between optimizing latents vs. parameters vs. prompts is critical for grasping why LoRA-based TTO avoids "distribution collapse" cited as a failure mode of latent intervention.
  - **Quick check question:** What is the difference between optimizing zt (latents) vs. φ (LoRA parameters), and why does the paper claim the latter avoids feature disruption?

- **Concept: Jensen-Shannon Divergence**
  - **Why needed here:** JSD is the core loss function. Unlike BCE or Center-of-Mass losses (Table 5), JSD provides symmetric, bounded divergence suitable for comparing probability distributions (attention maps normalized to sum to 1).
  - **Quick check question:** Why would JSD outperform BCE for aligning attention maps with soft layout masks?

## Architecture Onboarding

- **Component map:** LLM Layout Planner (GPT-4o) -> VFM Backbone (CogVideoX-5B / Wan2.1-14B) -> LoRA Adapters -> Attention Extractor -> Layout-to-Mask Converter -> JSD Loss Computer -> Optimizer (AdamW, lr=1e-4) -> Parametric Memory

- **Critical path:**
  1. Prompt → LLM → Layout (bbox sequences)
  2. Layout → Gaussian smoothing → Soft masks
  3. Memory query → (hit: load LoRA; miss: initialize fresh)
  4. Denoising loop (steps 1-5): Extract attention → Compute JSD → Backprop to LoRA → Update
  5. Post-generation: Unload LoRA → Insert/Update memory

- **Design tradeoffs:**
  - **LoRA rank (32):** Higher rank captures more patterns but increases memory footprint and optimization time. Paper uses 32 without ablation.
  - **Optimization steps (5):** Table 7 shows 5 steps optimal; more steps degrade quality, fewer under-align. Tradeoff between compute and alignment quality.
  - **Memory capacity:** Fixed size with LFU eviction. Larger memory improves retrieval recall but increases latency and storage.
  - **Top-k retrieval (5 vs 10):** Table 4 shows k=5 outperforms k=10, suggesting retrieval noise outweighs diversity benefits beyond a threshold.

- **Failure signatures:**
  - **Attention-collapse:** Over-optimization (>7 steps or >12 iterations/step) causes degraded compositional scores—monitor for attention maps that saturate or lose semantic diversity.
  - **Memory contamination:** If abstracted keys are too generic, retrieved LoRA weights produce incorrect motion/relations. Signature: correct layout alignment but wrong semantic interpretation.
  - **Layout-attention mismatch:** If selected DiT layers have weak attention-layout correlation (mIoU < 0.03 per Figure 3), optimization has minimal effect. Signature: unchanged compositional scores despite optimization.

- **First 3 experiments:**
  1. **Layer relevance probe:** Replicate Figure 3 on your target VFM. Compute mIoU between cross-attention maps (per layer) and SAM2-generated segmentation masks for 20-50 prompts. Identify top-3 layers with highest mIoU for subsequent TTO.
  2. **Ablation on optimization steps:** Table 7 reproduction. Sweep #timesteps {1, 3, 5, 7} on motion category prompts, reporting Motion score vs. latency. Confirm 5 steps is saturation point for your backbone.
  3. **Memory scaling test:** Sweep pseudo-training data size {50, 100, 150, 200} per Table 6. Plot Motion score vs. memory size to characterize diminishing returns and set capacity for production.

## Open Questions the Paper Calls Out
- The authors note that increasing the number of Top-$k$ matched entries for initialization "may also introduce noise or irrelevant information, suggesting that more sophisticated fusion strategies are required for better leveraging context in the future." (Section 4.3)
- The paper assumes the "verification step" ensures consistency of LLM-generated layouts but does not address scenarios where the LLM proposes spatially impossible or physically incorrect layouts, which would force the optimization to degrade the video to satisfy the constraint.

## Limitations
- Heavy dependence on LLM-generated layouts without validation of whether these layouts accurately capture user intent or maintain physical plausibility across diverse prompts
- Layer selection for attention optimization relies on empirical probing without providing a principled method for identifying optimal layers across different VFM architectures
- Parametric memory mechanism's retrieval quality depends on underspecified abstraction function for scene descriptions, potentially leading to retrieval noise

## Confidence
- **High Confidence:** The core mechanism of using JSD to align cross-attention maps with layout constraints is well-supported by experimental results showing consistent improvements across multiple benchmarks
- **Medium Confidence:** The memory mechanism's contribution to generalization is supported quantitatively, but retrieval quality and scene abstraction function remain underspecified
- **Low Confidence:** The LLM layout generation process lacks validation for physical plausibility and intent preservation, making the reliability of the entire pipeline uncertain

## Next Checks
1. **Attention-Layout Correlation Validation:** Replicate the layer relevance probing (Figure 3) on your target VFM architecture using 50-100 prompts with ground truth segmentations. Compute mIoU between cross-attention maps and segmentation masks per layer to identify optimal optimization layers.

2. **Layout Quality Assessment:** Generate 100 LLM layouts and manually evaluate whether they correctly interpret the compositional intent and maintain physical plausibility. Measure the percentage of layouts that misinterpret motion directions, spatial relations, or object attributes.

3. **Memory Retrieval Ablation:** Implement a controlled ablation testing the memory mechanism's retrieval quality by comparing optimization results using: (a) random initialization, (b) top-1 retrieval, (c) top-5 retrieval, and (d) top-10 retrieval to quantify retrieval noise impact.