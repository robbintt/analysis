---
ver: rpa2
title: 'LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?'
arxiv_id: '2510.07962'
source_url: https://arxiv.org/abs/2510.07962
tags:
- reasoning
- expert
- lightreasoner
- amateur
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LightReasoner uses behavioral divergence between stronger expert\
  \ and weaker amateur models to identify high-value reasoning steps via KL divergence.\
  \ It constructs contrastive soft labels encoding the expert\u2019s advantage and\
  \ fine-tunes the expert model on these targeted steps."
---

# LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?

## Quick Facts
- arXiv ID: 2510.07962
- Source URL: https://arxiv.org/abs/2510.07962
- Reference count: 40
- Key result: Improves reasoning accuracy by up to 28.1% while reducing training time by 90% and tuned tokens by 99%

## Executive Summary
LightReasoner presents a novel approach to improve reasoning capabilities in large language models by leveraging behavioral divergence between stronger expert and weaker amateur models. The method identifies high-value reasoning steps through KL divergence analysis of model logits, then constructs contrastive soft labels that encode the expert's advantages. By fine-tuning the expert model on these targeted steps, LightReasoner achieves state-of-the-art performance across seven mathematical reasoning benchmarks while dramatically reducing computational resources.

## Method Summary
LightReasoner works by first generating reasoning traces from both a stronger expert model and a weaker amateur model on the same set of problems. It then calculates KL divergence between their logits at each reasoning step to identify where the expert demonstrates superior reasoning patterns. These divergence points are used to construct contrastive soft labels that highlight the expert's advantage. The expert model is subsequently fine-tuned on these high-value steps using the soft labels as targets, focusing the training on the most informative reasoning transitions.

## Key Results
- Achieves up to 28.1% improvement in reasoning accuracy across seven mathematical benchmarks
- Reduces total training time by 90% compared to standard fine-tuning approaches
- Cuts tuned token usage by 99% while maintaining or improving performance
- Reduces sampled problems by 80% for training

## Why This Works (Mechanism)
The approach exploits the observation that behavioral divergence between models of different capabilities can reveal high-value reasoning steps that are most informative for learning. By focusing on these divergence points through contrastive soft labeling, the method efficiently transfers reasoning capabilities from expert to weaker models without requiring ground-truth labels.

## Foundational Learning
- **KL Divergence**: Measures behavioral differences between models; needed to identify valuable reasoning steps; quick check: verify divergence calculations match expected patterns
- **Contrastive Learning**: Uses pairwise comparisons to highlight differences; needed for constructing informative soft labels; quick check: ensure contrastive signals are meaningful
- **Behavioral Cloning**: Training through imitation of expert behavior; needed as the core learning mechanism; quick check: validate expert model consistency
- **Soft Label Construction**: Creates nuanced target distributions; needed to encode expert advantages; quick check: confirm label quality through visualization

## Architecture Onboarding

Component Map: Problem Generator -> Expert Model & Amateur Model -> KL Divergence Analyzer -> Soft Label Constructor -> Fine-tuning Module -> Improved Expert Model

Critical Path: The critical path flows from generating reasoning traces with both models, through divergence analysis, to soft label construction and fine-tuning. The KL divergence calculation and soft label generation are bottlenecks that determine the quality of subsequent training.

Design Tradeoffs: The method trades computational efficiency for potential blind spots in reasoning coverage. While achieving 99% token reduction, this may come at the cost of missing less-divergent but still important reasoning patterns.

Failure Signatures: The approach may fail when expert-amateur quality gaps are too small (weak behavioral divergence) or too large (no common reasoning patterns). It may also struggle with non-mathematical reasoning domains where behavioral patterns differ significantly.

First Experiments:
1. Test performance with varying expert-amateur accuracy gaps to find optimal divergence thresholds
2. Compare against standard fine-tuning on the same reduced token budgets
3. Evaluate reasoning robustness on edge cases and adversarial examples

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the generalizability of LightReasoner to non-mathematical reasoning domains and the potential trade-offs between efficiency gains and reasoning breadth. It also raises questions about the minimum quality gap required between expert and amateur models for the approach to be effective.

## Limitations
- Heavy reliance on having access to both stronger expert and weaker amateur models with sufficient quality gap
- Assumes KL divergence reliably indicates reasoning quality across domains
- 99% token reduction may sacrifice reasoning robustness and breadth
- Limited evaluation to mathematical reasoning benchmarks

## Confidence

High confidence:
- LightReasoner achieves state-of-the-art performance on mathematical reasoning tasks

Medium confidence:
- The method provides 90% reduction in training time while maintaining or improving accuracy
- Behavioral divergence via KL divergence is an effective way to identify high-value reasoning steps

## Next Checks

1. Test LightReasoner's effectiveness when the expert-amateur gap is smaller (within 5-10% accuracy difference) to determine the minimum quality difference needed for the method to work.

2. Evaluate the approach on non-mathematical reasoning tasks (scientific reasoning, logical puzzles, or commonsense reasoning) to assess domain generalization.

3. Conduct ablation studies removing the behavioral divergence component to quantify exactly how much performance gain comes from this specific innovation versus standard fine-tuning approaches.