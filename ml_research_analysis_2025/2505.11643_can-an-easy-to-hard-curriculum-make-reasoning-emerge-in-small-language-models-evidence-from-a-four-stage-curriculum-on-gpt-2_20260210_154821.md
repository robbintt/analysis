---
ver: rpa2
title: Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models?
  Evidence from a Four-Stage Curriculum on GPT-2
arxiv_id: '2505.11643'
source_url: https://arxiv.org/abs/2505.11643
tags:
- curriculum
- stage
- heads
- reasoning
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Cognivolve, a four-stage curriculum that trains
  GPT-2small on increasingly complex reasoning tasks. By ordering training from simple
  lexical matching to multi-step symbolic inference, the model develops 7.8x more
  specialized reasoning components than a baseline, with deeper-layer redistribution
  and higher-entropy attention.
---

# Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2

## Quick Facts
- arXiv ID: 2505.11643
- Source URL: https://arxiv.org/abs/2505.11643
- Authors: Xiang Fu
- Reference count: 17
- One-line primary result: A four-stage curriculum trains GPT-2small to develop 7.8x more specialized reasoning components and redistribute them to deeper layers, halving optimization steps to moderate accuracy thresholds while lagging baseline final accuracy by ~32%.

## Executive Summary
This work introduces Cognivolve, a four-stage curriculum that trains GPT-2small on increasingly complex reasoning tasks. By ordering training from simple lexical matching to multi-step symbolic inference, the model develops 7.8x more specialized reasoning components than a baseline, with deeper-layer redistribution and higher-entropy attention. Curriculum learning halves optimization steps to reach moderate accuracy thresholds and accelerates emergence of higher-order reasoning circuits, though final accuracy lags the baseline by ~32%. The approach demonstrates that structured, developmental staging can substitute for model scaling, offering a path to efficient, interpretable small language models.

## Method Summary
The paper applies a four-stage "easy-to-hard" curriculum to GPT-2small (124M parameters), training sequentially on complexity-graded data partitions. Data is split into four tiers using a logistic classifier (features: operator density, sentence count, delimiter count). Each stage trains for one epoch with optimizer state carried over (no resets). The learning rate follows cosine decay with stage-specific peaks. Gradient-based saliency mapping identifies specialized reasoning heads. Evaluation occurs every 200 steps on a held-out test set, measuring sample efficiency, count of specialized heads, validation success rate, and attention entropy.

## Key Results
- Curriculum training develops 7.8x more specialized reasoning components than single-phase training.
- Specialized heads redistribute from shallow to deeper layers (layers 12-23 host up to 193 heads vs. zero in baseline).
- Attention entropy increases (+2.04%) and mean key-query distance grows (+4.86%) while maintaining local focus (+1.37%).
- Sample efficiency doubles (2× faster to reach moderate accuracy thresholds), but final accuracy lags baseline by ~32%.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ordered curricula enable progressive capacity accumulation where early-stage specialized components consolidate and transfer to later stages.
- **Mechanism:** Early stages establish a stable backbone of low-level attention heads that persist through training, reducing relearning cost. The paper shows 98.9% head retention from Stage 1→2 and 98.4% from Stage 2→3.
- **Core assumption:** Gradient-saliency thresholding accurately identifies functionally specialized heads. Assumption: retention under saliency implies functional reuse rather than coincidental reactivation.
- **Evidence anchors:**
  - [abstract] "activates an order-of-magnitude more gradient-salient reasoning heads"
  - [Section 3.5] "Of the 378 heads active at the end of stage 1, 374 reappear immediately in stage 2, a retention rate of 98.9%"
  - [corpus] Related work on curriculum RL (arXiv:2506.06632) shows similar easy-to-hard ordering benefits for reasoning, but does not directly validate head-retention mechanisms.
- **Break condition:** If Stage 4 probe mismatch (0% detection) reflects true forgetting rather than probe limitation, the accumulation claim weakens. Mixed-stage fine-tuning may be required.

### Mechanism 2
- **Claim:** Curricula redistribute specialized reasoning components from shallow to deep layers, enabling long-range and abstract computation.
- **Mechanism:** Staged difficulty forces gradual abstraction, populating layers 12–23 that remain dormant in single-phase training. The early:late head ratio shifts from 439:0 (baseline) to 1:1.1 (curriculum).
- **Core assumption:** Deeper layers in small models are underutilized capacity that can be recruited; deep-layer activation causally improves reasoning rather than correlating with it.
- **Evidence anchors:**
  - [abstract] "shifts those heads toward deeper layers"
  - [Section 3.1] "layers 12–23, which contain zero specialized heads in the baseline, now host up to 193 heads each"
  - [corpus] No direct corpus validation of deep-layer redistribution in SLMs; related curriculum papers focus on accuracy gains, not layer-wise analysis.
- **Break condition:** If deeper layers specialize but task accuracy still lags baseline by 32%, the functional contribution of redistributed heads may be insufficient without extended fine-tuning.

### Mechanism 3
- **Claim:** Curriculum training increases attention entropy and balances local with long-range context integration.
- **Mechanism:** Heads spread probability mass over more tokens (+2.04% entropy), reduce sparsity (−0.37%), and increase mean key-query distance (+4.86%) while maintaining local focus (+1.37%). This enables richer evidence gathering before prediction.
- **Core assumption:** Higher-entropy attention causally improves step-by-step reasoning alignment; the paper does not prove directionality.
- **Evidence anchors:**
  - [abstract] "yielding higher-entropy attention that balances local and long-range context"
  - [Section 3.3] "over 95% of heads move in the same direction as the global mean for each metric, and paired permutation tests... confirm significance at p < 10⁻⁴"
  - [corpus] Related work on attention interpretability (cited in Section 4.4) reports heterogeneous head roles but does not replicate entropy-shift findings specifically.
- **Break condition:** Attention pattern changes could reflect epiphenomena of longer training rather than causal drivers; ablations with shuffled stage order show gains disappear, supporting progression-dependence.

## Foundational Learning

- **Concept: Curriculum Learning**
  - Why needed here: The entire framework depends on understanding why easy-to-hard ordering differs from shuffled or single-phase training.
  - Quick check question: Can you explain why the paper's ablation (shuffled order + optimizer reset) failed to reproduce gains?

- **Concept: Attention Head Specialization via Gradient Saliency**
  - Why needed here: All quantitative claims about "specialized heads" rely on saliency thresholds (95th percentile vs. random-head null).
  - Quick check question: What are two limitations of saliency-based detection mentioned in the paper?

- **Concept: Layer-wise Functional Specialization in Transformers**
  - Why needed here: Interpreting early:late ratio shifts requires knowing that deeper layers typically handle abstract, long-range computations.
  - Quick check question: In the baseline, how many specialized heads exist in layers 12–23 versus the curriculum model?

## Architecture Onboarding

- **Component map:**
  GPT-2small backbone (12 layers, 12 heads/layer) -> 4-stage curriculum controller -> Gradient-saliency detector -> RepresentationTracker

- **Critical path:**
  1. Complexity classifier trained on 500 manually labeled examples -> assigns simple/basic/intermediate/complex labels
  2. Sequential training: Stage 1→2→3→4, each 1 epoch, optimizer state carries over (no resets)
  3. Learning-rate schedule: cosine decay with stage-specific peaks (1e-4 → 7e-5 → 5e-5 → 2e-5)
  4. Evaluation every 200 steps on held-out 1000-item test set (no training overlap)

- **Design tradeoffs:**
  - Sample efficiency vs. final accuracy: 2× faster to moderate thresholds but −32% final success rate
  - Transparency vs. probe coverage: Saliency detector works for Stages 1–3 (numerical reasoning) but fails for Stage 4 (verbal/implicit knowledge)
  - FP32 stability vs. speed: Mixed-precision disabled due to late-stage overflow

- **Failure signatures:**
  - Shuffled stage order -> no sample-efficiency gain, fewer specialized heads
  - Optimizer reset at stage boundaries -> gains disappear
  - Stage 4 detection collapse -> probe-task mismatch, not necessarily model failure
  - Success-rate plateau -> training halted at fixed budget, not validation convergence

- **First 3 experiments:**
  1. **Reproduce ablation:** Train with shuffled stage order (4→1→3→2) and compare head counts, sample efficiency, and final accuracy to ordered curriculum. Expect collapse of gains.
  2. **Mixed-stage fine-tuning:** After Stage 4, add 1 epoch blending 10% of each prior stage's data. Measure whether final accuracy gap closes without sacrificing step-by-step alignment.
  3. **Probe expansion for Stage 4:** Design verbal-knowledge probes (e.g., commonsense inference) and re-run detection. Verify whether Stage 4 truly has no specialized heads or if the numerical probe is blind to them.

## Open Questions the Paper Calls Out

- **Question:** Can mixed-stage fine-tuning or adaptive loss re-weighting close the 32% final-accuracy gap between curriculum-trained models and baselines without sacrificing the gains in interpretability?
  - **Basis in paper:** [explicit] The authors note that final-answer success lags by ~30% and suggest "directions for mixed-stage fine-tuning" and "adaptive loss re-weighting" to translate procedural soundness into final accuracy.
  - **Why unresolved:** The current study halts training after the final syllabus stage and uses a fixed loss weighting that prioritizes intermediate reasoning steps over the final answer, creating a trade-off between transparency and task success.
  - **What evidence would resolve it:** A follow-up experiment where the curriculum model undergoes a final "mixed-stage" epoch with a rebalanced loss function, showing comparable accuracy to the baseline while retaining the higher count of specialized reasoning heads.

- **Question:** Does the curriculum's strict staging cause the model to overfit to stage boundaries rather than learning transferable reasoning skills?
  - **Basis in paper:** [explicit] The limitations section suggests the ordering could "inadvertently leak difficulty information" and proposes a diagnostic involving random interleaving of complex problems into earlier epochs.
  - **Why unresolved:** The current experimental design maintains strict separation between difficulty tiers, leaving the model vulnerable to exploiting the order of data rather than the content.
  - **What evidence would resolve it:** An ablation study where 10% of Stage-4 (complex) data is randomly interleaved into Stages 1–3; if validation accuracy remains unchanged, it confirms the model is not merely memorizing the developmental sequence.

- **Question:** Do the sample-efficiency gains and redistribution of specialized heads to deeper layers persist, or do they saturate, when applied to larger models (e.g., GPT-2 medium/XL)?
  - **Basis in paper:** [explicit] The authors explicitly state they only analyzed GPT-2 small (124M) and that "extending the syllabus to GPT-2 medium, large, and XL would clarify whether the observed... gains scale with capacity."
  - **Why unresolved:** It is unknown if the "developmental" unlock of deeper layers is a necessary compensation for the limited capacity of small models or a general principle of transformer learning.
  - **What evidence would resolve it:** Applying the exact Cognivolve syllabus to GPT-2 medium and large variants to measure if the ratio of specialized heads and sample-efficiency speed-up remains constant or diminishes.

- **Question:** Can stage-specific saliency probes be designed to detect verbal-knowledge components that the current gradient-based probe misses in the final stage?
  - **Basis in paper:** [explicit] The abstract and results section highlight that the "saliency probe under-detects verbal-knowledge heads in the hardest stage," suggesting a need for "probe expansion."
  - **Why unresolved:** The current probe fails to register specialized heads in Stage 4 (complex reasoning), creating a "diagnostic blind spot" that prevents researchers from observing how the model solves open-domain problems.
  - **What evidence would resolve it:** Development of a multimodal or vocabulary-aware probe that successfully identifies active specialized heads in Stage 4, matching the detection rates seen in Stages 1–3.

## Limitations

- The 32% accuracy gap versus single-phase training at final convergence suggests incomplete functional utilization of redistributed heads.
- Stage 4 detection collapse (0% specialized heads) is ambiguous: it may indicate a probe-task mismatch rather than true forgetting.
- Claims of "reasoning emergence" rely on a narrow operationalization (multi-step symbolic inference) and a limited probe set (numerical reasoning).

## Confidence

- **High confidence:** Sample-efficiency gains (2× faster to moderate thresholds) and head-retention rates (98%+ across stages) are directly measurable and robust to ablation.
- **Medium confidence:** Layer redistribution and attention entropy shifts are statistically significant but their causal role in reasoning improvement is inferred rather than proven by intervention studies.
- **Low confidence:** The "emergence" of higher-order reasoning circuits is operationalized via a single probe; Stage 4's failure to register specialized heads may reflect probe blindness rather than absence of reasoning.

## Next Checks

1. **Probe expansion:** Design and apply verbal/knowledge-based reasoning probes to Stage 4 checkpoints to test whether the lack of specialized heads reflects probe limitations or true absence of abstract reasoning.
2. **Mixed-stage fine-tuning:** After Stage 4, blend 10% of each prior stage's data for one epoch and measure if the accuracy gap to single-phase training closes without sacrificing step-by-step alignment.
3. **Ablation of optimizer continuity:** Explicitly reset optimizer state at each stage boundary and verify that all curriculum gains (sample efficiency, head specialization, attention entropy) disappear, confirming the necessity of state carry-over.