---
ver: rpa2
title: Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks
arxiv_id: '2512.24793'
source_url: https://arxiv.org/abs/2512.24793
tags:
- architecture
- search
- proposed
- u1d458
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised neural architecture search
  (NAS) method for multimodal deep neural networks (DNNs) that addresses the challenge
  of requiring large amounts of labeled training data in conventional multimodal NAS
  approaches. The proposed method applies self-supervised learning (SSL) comprehensively
  to both the architecture search and model pretraining processes using contrastive
  learning, enabling architecture discovery from unlabeled data.
---

# Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks

## Quick Facts
- arXiv ID: 2512.24793
- Source URL: https://arxiv.org/abs/2512.24793
- Authors: Shota Suzuki; Satoshi Ono
- Reference count: 14
- Primary result: SSL-NAS achieves 62.46±0.06% weighted F1 on MM-IMDB using only unlabeled data for architecture search

## Executive Summary
This paper proposes a self-supervised neural architecture search (NAS) method for multimodal deep neural networks that eliminates the need for labeled data during architecture discovery. The method extends bilevel multimodal NAS (BM-NAS) by incorporating the SimCLR framework, enabling gradient-based architecture search using contrastive learning on unlabeled data. Experimental evaluation on MM-IMDB demonstrates that the proposed method successfully designs network architectures comparable to supervised approaches, achieving weighted F1-scores of 62.46±0.06% when using only unlabeled data for architecture search.

## Method Summary
The proposed method is a three-step pipeline based on BM-NAS and SimCLR. First, it performs architecture search using contrastive loss (NT-Xent) on unlabeled data to optimize architecture parameters (α, β, γ) and network weights (ω) via bilevel optimization. Second, it fixes the derived architecture and continues pretraining the encoder on unlabeled data using contrastive loss. Third, it replaces the projection head with a classification layer and trains on a small labeled subset using cross-entropy loss. The search space consists of fusion cells that interconnect pretrained backbones (VGG-Transfer for images, Maxout MLP for text), with the search restricted to fusion topology while keeping backbones frozen.

## Key Results
- Achieved weighted F1-score of 62.46±0.06% on MM-IMDB test set using only unlabeled data for architecture search
- Outperformed BM-NAS when labeled data ratio dropped to 0.01-0.3, demonstrating SSL's advantage in low-data regimes
- Successfully designed architectures comparable to supervised methods while eliminating label dependency in the search phase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive loss can substitute for supervised loss in gradient-based architecture search for multimodal fusion networks
- Mechanism: Replaces classification loss with SimCLR's contrastive loss during architecture parameter optimization. Architecture weights (α, β, γ) are updated based on validation set contrastive loss, while operator weights (w̃) are updated on training set contrastive loss. This preserves the bilevel optimization structure of BM-NAS while removing label dependency.
- Core assumption: Architectures that learn modality-invariant representations will transfer effectively to downstream classification tasks
- Evidence anchors: [abstract] states SSL is applied comprehensively to both architecture search and model pretraining using contrastive learning

### Mechanism 2
- Claim: Hierarchical search space with frozen pretrained backbones enables efficient multimodal architecture discovery
- Mechanism: Search operates on a bilevel structure—upper level determines which backbone features connect to which fusion cells, lower level determines internal fusion cell operations. Pretrained backbones remain frozen during search, reducing search space to fusion topology only.
- Core assumption: The optimal fusion architecture is largely independent of backbone fine-tuning
- Evidence anchors: [Section 3.2] describes the search space comprising fusion cells that function to interconnect different modalities

### Mechanism 3
- Claim: Continued contrastive pretraining after architecture determination recovers representation quality lost during architecture parameter pruning
- Mechanism: After architecture search completes, removing low-weight connections changes fusion cell outputs. Phase 2 reruns contrastive learning on the fixed architecture to re-align representations before classifier training.
- Core assumption: The architecture discovered via CL remains sound after pruning; only representation quality needs refinement
- Evidence anchors: [Section 3.3] mandates subsequent representation learning after determining the architecture

## Foundational Learning

- **Contrastive Learning (SimCLR framework)**
  - Why needed here: Core mechanism enabling label-free architecture search
  - Quick check question: Can you explain why contrastive learning needs modality-specific augmentations for multimodal data?

- **Differentiable Architecture Search (DARTS)**
  - Why needed here: The paper builds on BM-NAS, which extends DARTS to multimodal settings
  - Quick check question: In DARTS, what does the architecture parameter α encode, and how does it differ from weight parameters w?

- **Multimodal Fusion Strategies**
  - Why needed here: The search space consists of fusion cells that combine features from different modalities
  - Quick check question: What are the tradeoffs between concatenation-based fusion and attention-based fusion for heterogeneous modalities like image and text?

## Architecture Onboarding

- **Component map:**
  - Backbones (frozen) → Fusion cells → Projection head (SSL) → Classifier head (supervised)
  - Text backbone: Maxout MLP → Fusion cell inputs
  - Image backbone: VGG-Transfer → Fusion cell inputs
  - Fusion cells: Searchable modules with inner step nodes applying weighted operations

- **Critical path:**
  1. Initialize α, β, γ uniformly; load pretrained backbones
  2. Phase 1: For each batch, generate augmented view pairs per modality → compute contrastive loss → update w̃ on train split, α/β/γ on validation split → track best architecture
  3. Derive discrete architecture by selecting highest-weight connections
  4. Phase 2: Rerun CL on fixed architecture to recalibrate representations
  5. Phase 3: Replace projection head with classification layer; train with limited labeled data

- **Design tradeoffs:**
  - Search space granularity: More fusion cells increase expressiveness but require more samples for reliable gradient estimation
  - Augmentation strength: Aggressive augmentations improve contrastive learning but may destroy cross-modal alignment signals
  - Temperature parameter τ: Lower values make contrastive loss more sensitive to hard negatives

- **Failure signatures:**
  - Discovered architecture ignores one modality entirely
  - Performance collapses when labeled ratio drops below 0.01
  - Architecture search converges to degenerate structures (all operations are "none")

- **First 3 experiments:**
  1. Baseline replication: Implement architecture search on MM-IMDB with r=0.1 labeled data; verify weighted F1-score reaches ~62%
  2. Ablation on Phase 2: Skip representation learning after architecture determination; measure performance drop
  3. Augmentation sensitivity: Test alternative text augmentations and image augmentations; analyze impact on discovered fusion topology

## Open Questions the Paper Calls Out
- Does the proposed self-supervised NAS method generalize to multimodal datasets other than MM-IMDB, particularly those involving modalities beyond image and text?
- Is SimCLR the optimal self-supervised learning framework for this architecture search task, or would other methods (e.g., masked autoencoders) yield superior results?
- How does the choice of backbone architecture affect the search process, given that the experiments relied on older models (VGG, Maxout MLP) for fair comparison?

## Limitations
- Experimental evaluation is restricted to MM-IMDB dataset, leaving performance on other multimodal benchmarks unknown
- The paper does not compare against other self-supervised learning frameworks (e.g., MAE, BYOL) for architecture search
- Limited discussion of computational costs and search efficiency compared to supervised approaches

## Confidence
- **High confidence**: The core mechanism of using contrastive loss for architecture search is theoretically sound and follows established SimCLR principles
- **Medium confidence**: The claim of comparable performance to supervised methods is supported by experimental results, though limited to one dataset
- **Low confidence**: The assumption that frozen backbones enable effective search space restriction lacks empirical validation across different backbone architectures

## Next Checks
1. Implement ablation study comparing the proposed method against BM-NAS with identical hyperparameters and data splits to isolate the SSL contribution
2. Test the method on additional multimodal datasets (e.g., CMU-MOSEI, Hateful Memes) to assess generalization beyond MM-IMDB
3. Analyze the discovered architectures across different labeled data ratios to determine if the method consistently finds modality-balanced solutions or exhibits instability at extreme ratios (r < 0.05)