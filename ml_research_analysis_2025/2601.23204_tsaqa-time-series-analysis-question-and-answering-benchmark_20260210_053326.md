---
ver: rpa2
title: 'TSAQA: Time Series Analysis Question And Answering Benchmark'
arxiv_id: '2601.23204'
source_url: https://arxiv.org/abs/2601.23204
tags:
- series
- time
- question
- data
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TSAQA is a large-scale time series question-answering benchmark
  with 210K samples across 13 domains, covering 6 tasks (anomaly detection, classification,
  characterization, comparison, data transformation, temporal relationship) and 3
  question types (true-or-false, multiple-choice, puzzling). The dataset spans diverse
  domains and employs hierarchical uniform sampling to ensure balanced representation.
---

# TSAQA: Time Series Analysis Question And Answering Benchmark

## Quick Facts
- arXiv ID: 2601.23204
- Source URL: https://arxiv.org/abs/2601.23204
- Reference count: 40
- Primary result: Large-scale time series QA benchmark (210K samples, 13 domains) reveals LLMs struggle with temporal reasoning tasks

## Executive Summary
TSAQA is a comprehensive benchmark designed to evaluate large language models' capabilities in time series analysis across 6 tasks (anomaly detection, classification, characterization, comparison, data transformation, temporal relationship) and 3 question types (true-or-false, multiple-choice, puzzling). The benchmark spans 13 diverse domains including healthcare, economics, and engineering, featuring 210K samples with hierarchical uniform sampling to ensure balanced representation. Current LLMs demonstrate significant limitations on TSAQA, with the best commercial model achieving only 65.08% accuracy in zero-shot evaluation, while instruction-tuned open-source models show substantial improvement but still fall short of human-level performance.

## Method Summary
The TSAQA benchmark was constructed using hierarchical uniform sampling across 13 domains and 6 analytical tasks, with questions generated through prompt engineering using templates for different question types. The dataset includes true-or-false, multiple-choice, and puzzling questions designed to test various levels of temporal reasoning complexity. Human evaluation was conducted on 50 samples per task to ensure quality and reliability. Models were evaluated in zero-shot, few-shot, and instruction-tuned settings, with particular attention to domain transfer capabilities and performance across different question types and volatility levels.

## Key Results
- Zero-shot LLMs achieve only 65.08% accuracy (Gemini-2.5-Flash), significantly below human-level performance
- Instruction tuning substantially improves open-source model performance, with LLaMA-3.1-8B reaching 85.26% accuracy
- Puzzling questions prove most challenging, exposing limitations in deep temporal reasoning capabilities
- Models show "smoothness bias" when handling volatile time series, attempting to predict smoother transitions than ground truth

## Why This Works (Mechanism)
TSAQA addresses the critical gap in evaluating LLMs for time series analysis by providing a large-scale, diverse benchmark that captures the complexity of real-world temporal data analysis tasks. The hierarchical sampling approach ensures comprehensive coverage across domains and tasks, while the three question types (true-or-false, multiple-choice, puzzling) progressively challenge models' reasoning capabilities from basic pattern recognition to complex temporal inference.

## Foundational Learning
- **Time Series Domains**: Why needed - Different domains have distinct temporal patterns and analytical requirements; Quick check - Models maintain performance across healthcare, economics, and engineering domains
- **Temporal Relationship Reasoning**: Why needed - Understanding cause-effect and sequential dependencies is crucial for forecasting; Quick check - Performance on temporal relationship tasks (TR) compared to other tasks
- **Data Transformation Operations**: Why needed - Many real-world analyses require mathematical transformations; Quick check - Accuracy on FT (Fourier Transform) vs FOD (First-Order Differencing) tasks
- **Volatility-Aware Analysis**: Why needed - Different domains exhibit varying levels of temporal volatility; Quick check - Performance gaps between high and low volatility domains
- **Puzzling Question Design**: Why needed - Tests deep reasoning beyond pattern matching; Quick check - Performance gap between puzzling and non-puzzling questions
- **Instruction Tuning Effectiveness**: Why needed - Determines whether fine-tuning improves temporal reasoning; Quick check - Performance improvement after instruction tuning

## Architecture Onboarding

**Component Map:** Time Series Data -> Question Templates -> LLM Model -> Answer Generation -> Human Evaluation -> Performance Metrics

**Critical Path:** Data Generation → Question Templating → Model Inference → Human Validation → Benchmark Construction

**Design Tradeoffs:** The benchmark prioritizes breadth (13 domains, 6 tasks) over depth in any single domain, potentially sacrificing domain-specific nuance for generalizability. The use of synthetic data generation enables controlled evaluation but may miss some real-world complexities.

**Failure Signatures:** Models exhibit "smoothness bias" on volatile data, struggle with global aggregation tasks (especially Fourier Transforms), and show significant performance drops on puzzling questions requiring deep reasoning.

**3 First Experiments:**
1. Evaluate zero-shot performance across all 6 tasks to establish baseline capabilities
2. Test instruction-tuned open-source models to measure improvement potential
3. Analyze performance differences between high and low volatility domains

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can benchmarks be designed to evaluate model robustness and adaptation to evolving data distributions in real-time?
- Basis in paper: [explicit] The Limitations section states that "real deployments face evolved distributions" and the benchmark remains static, making "Building dynamic extensions that evaluate adaptation and robustness under distribution shifts... an important next step."
- Why unresolved: Current static benchmarks fail to capture the temporal drift and emerging domains characteristic of real-world deployments.
- Evidence: A dynamic evaluation suite where models are assessed on their ability to maintain accuracy as the input distribution shifts over time.

### Open Question 2
- Question: How can model architectures or training regimes be adjusted to overcome the "smoothness bias" identified in temporal reasoning tasks?
- Basis in paper: [inferred] The analysis in Section 4.2.2 reveals models suffer from a "Smoothness Bias," where they attempt to repair legitimate discontinuities by predicting transitions smoother than the ground truth, particularly in volatile domains like Web and Sales.
- Why unresolved: Current models appear to rely on generic smoothing priors rather than capturing specific, irregular structural dynamics.
- Evidence: A reduction in the "Smoothness Gap" (boundary distance error) and improved performance on PZ tasks within high-volatility domains.

### Open Question 3
- Question: Can LLMs be improved to handle global arithmetic reasoning required for complex data transformations like Fourier Transforms?
- Basis in paper: [inferred] Appendix D.2 notes that Fourier Transform (FT) tasks "necessitate aggregating information from the entire sequence," which is "inherently challenging for LLMs' next-token prediction paradigm," resulting in significantly lower performance compared to First-Order Differencing.
- Why unresolved: LLMs struggle to perform global aggregation and arithmetic over long sequences using local attention and token prediction.
- Evidence: Accuracy on FT tasks improving to match or exceed the performance of local-dependency tasks like First-Order Differencing.

### Open Question 4
- Question: How can benchmarks be expanded to include irregular sampling, mixed-frequency data, and exogenous drivers?
- Basis in paper: [explicit] The authors explicitly state in the Limitations that the benchmark "does not fully capture the diversity of real-world temporal processes" and that "Expanding... toward irregular, mixed-frequency, and exogenous-aware scenarios would further improve realism."
- Why unresolved: The current benchmark utilizes z-scored, regular intervals, creating a gap with real-world data which often involves missing values and external variables.
- Evidence: Successful evaluation of models on a new dataset specifically constructed with irregular timestamps and external exogenous variables.

## Limitations
- Static benchmark unable to capture evolving data distributions and emerging domains
- Reliance on synthetic data generation may miss some real-world complexities
- Limited coverage of irregular sampling, mixed-frequency data, and exogenous variables
- Benchmark may not fully capture the diversity of real-world temporal processes

## Confidence
- **High Confidence**: Dataset construction methodology and domain coverage claims; human evaluation methodology and reported agreement rates
- **Medium Confidence**: Zero-shot evaluation results and relative model performance comparisons; instruction tuning effectiveness metrics
- **Low Confidence**: Generalization of puzzling question performance to real-world applications; long-term benchmark stability across evolving LLM capabilities

## Next Checks
1. **Cross-Domain Generalization**: Evaluate model performance on TSAQA tasks when trained exclusively on data from disjoint domains to assess true domain transfer capabilities versus potential memorization patterns.

2. **Temporal Robustness Analysis**: Test model performance on time series with varying temporal granularities and patterns not represented in the original 13 domains to identify potential overfitting to specific temporal structures.

3. **Real-World Deployment Correlation**: Conduct a controlled study comparing TSAQA benchmark performance against actual time series analysis