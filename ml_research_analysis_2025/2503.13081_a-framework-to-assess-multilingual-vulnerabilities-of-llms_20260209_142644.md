---
ver: rpa2
title: A Framework to Assess Multilingual Vulnerabilities of LLMs
arxiv_id: '2503.13081'
source_url: https://arxiv.org/abs/2503.13081
tags:
- sydney
- llms
- rejection
- framework
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an automated framework to assess multilingual
  vulnerabilities of large language models (LLMs) across eight languages with varying
  resource levels. The framework evaluates model responses using three metrics: rejection
  rate, relevance, and legality.'
---

# A Framework to Assess Multilingual Vulnerabilities of LLMs

## Quick Facts
- **arXiv ID:** 2503.13081
- **Source URL:** https://arxiv.org/abs/2503.13081
- **Reference count:** 17
- **Key outcome:** Automated framework reveals higher LLM vulnerabilities in low-resource languages, but risks are often minimal due to incoherent outputs.

## Executive Summary
This paper introduces an automated framework to assess multilingual vulnerabilities of large language models (LLMs) across eight languages with varying resource levels. The framework evaluates model responses using three metrics: rejection rate, relevance, and legality. Six LLMs were tested, revealing that while models show higher vulnerabilities in low-resource languages, the actual risk is often minimal due to incoherent or irrelevant outputs. Human evaluation validated the framework's automated metrics, showing strong alignment. The study highlights the importance of balanced training data and robust safety measures, especially for low-resource languages, to ensure secure and ethical LLM deployment in multilingual contexts.

## Method Summary
The framework automates multilingual vulnerability assessment by translating English jailbreak prompts into eight target languages (English, Chinese, Hindi, Spanish, Bengali, Arabic, Sinhala, Javanese), applying three jailbreaking techniques (Pretending, Attention Shifting, Privilege Escalation), and querying six LLMs (GPT-3.5/4, Gemini 1.5, Llama 2/3). Responses are translated back to English and scored using G-Eval, an LLM-as-a-judge approach, for rejection rate (refusal), relevance (contextual appropriateness), and legality (safety/harm). Human evaluation validated the automated metrics in two languages.

## Key Results
- LLMs exhibit higher vulnerability rates in low-resource languages (LRLs) but produce incoherent responses, reducing actual risk.
- Human evaluation confirmed strong alignment with automated G-Eval scores, validating the framework's reliability.
- Jailbreaking techniques like Pretending and Privilege Escalation effectively bypass safety measures across languages.

## Why This Works (Mechanism)
The framework leverages automated translation and LLM-as-a-judge evaluation to scale vulnerability assessment across multiple languages efficiently. By combining jailbreaking techniques with structured metrics, it identifies patterns in model behavior under multilingual stress.

## Foundational Learning
- **G-Eval:** LLM-as-a-judge framework for scoring responses on predefined criteria. *Why needed:* Enables scalable, automated evaluation of safety and relevance. *Quick check:* Verify G-Eval outputs align with human judgments on sample responses.
- **Jailbreaking Techniques:** Methods like Pretending and Privilege Escalation to bypass safety measures. *Why needed:* Test model robustness under adversarial conditions. *Quick check:* Confirm techniques successfully elicit prohibited content in controlled tests.
- **Translation Quality Impact:** Machine translation artifacts may influence prompt interpretation. *Why needed:* Low-resource language translations can degrade coherence. *Quick check:* Assess back-translated prompts for semantic drift.
- **Multilingual Data Bias:** Imbalanced training data affects model performance across languages. *Why needed:* Explains higher vulnerabilities in LRLs. *Quick check:* Compare model outputs for high vs. low-resource languages.

## Architecture Onboarding
- **Component Map:** English Prompts -> Translation (Google Translate) -> Jailbreak Wrapper (P/AS/PE) -> LLM Query -> Back-translation -> G-Eval Scoring -> Results
- **Critical Path:** Prompt translation → Jailbreak application → LLM response → Automated evaluation (G-Eval)
- **Design Tradeoffs:** Automation vs. translation noise; scalability vs. potential bias in G-Eval scoring.
- **Failure Signatures:** High rejection rates in coherent prompts indicate robust safety; low relevance scores suggest translation artifacts.
- **First Experiments:** 1) Translate and back-translate sample prompts to assess semantic drift. 2) Test G-Eval scoring consistency on known safe/unsafe outputs. 3) Apply jailbreak techniques to a small set of LLMs and verify response patterns.

## Open Questions the Paper Calls Out
- Does the strong alignment between G-Eval and human evaluators hold across low-resource languages not manually tested?
- Does improving LLM linguistic capability in low-resource languages inherently increase vulnerability to generating illegal content?
- To what extent does the choice of machine translation system introduce artifacts that trigger or bypass safety mechanisms?

## Limitations
- G-Eval scoring introduces variability and potential bias due to non-deterministic LLM judgments.
- Translation quality for low-resource languages may degrade, affecting prompt coherence and evaluation accuracy.
- Limited generalizability to other LLM families beyond the six tested models.

## Confidence
- **High Confidence:** Framework's core concept and automated metrics are well-supported by human evaluation.
- **Medium Confidence:** Observed trend of higher vulnerabilities in low-resource languages is plausible but may be influenced by translation artifacts.
- **Low Confidence:** Specific numerical results require access to exact prompt translations and G-Eval configurations for full verification.

## Next Checks
1. Reconstruct and validate prompts by translating English prompts into target languages and verifying semantic consistency through back-translation.
2. Test G-Eval setup with the same system prompts and scoring rubrics to ensure alignment with reported metrics.
3. Evaluate translation impact by assessing the quality of translated prompts for low-resource languages and measuring coherence.