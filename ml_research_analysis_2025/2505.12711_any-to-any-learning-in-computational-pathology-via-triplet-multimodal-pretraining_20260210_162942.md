---
ver: rpa2
title: Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining
arxiv_id: '2505.12711'
source_url: https://arxiv.org/abs/2505.12711
tags:
- learning
- multimodal
- cancer
- pathology
- survival
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ALTER addresses computational and data-scarcity challenges in multimodal
  computational pathology by enabling flexible pretraining with any subset of WSIs,
  genomics, and reports. The method uses modality-specific encoders, efficient feature
  aggregation, and a two-stage fusion strategy with modality-specific mixture-of-experts
  to reduce complexity and handle missing modalities.
---

# Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining

## Quick Facts
- arXiv ID: 2505.12711
- Source URL: https://arxiv.org/abs/2505.12711
- Authors: Qichen Sun; Zhengrui Guo; Rui Peng; Hao Chen; Jinzhuo Wang
- Reference count: 40
- Primary result: ALTER achieves C-index of 0.762 in survival prediction, outperforming second-best by 4.1%

## Executive Summary
ALTER introduces a pretraining framework for computational pathology that learns representations from any combination of whole slide images (WSIs), genomic data, and clinical reports without requiring complete modality pairs. The method employs modality-specific encoders, efficient feature aggregation, and a two-stage fusion strategy with mixture-of-experts to handle missing modalities while reducing computational complexity. Pretraining leverages intra-modal masked language modeling, inter-modal contrastive learning, and sample-level triplet loss to align heterogeneous data. Across multiple cancer types, ALTER demonstrates superior performance in survival prediction, cancer subtyping, gene mutation prediction, and report generation, while maintaining robustness to missing data scenarios.

## Method Summary
ALTER addresses computational and data-scarcity challenges in multimodal computational pathology by enabling flexible pretraining with any subset of WSIs, genomics, and reports. The method uses modality-specific encoders (TransMIL for WSI, Performer for genomics, BioBERT for text), efficient feature aggregation (region-wise pooling for WSI, pathway pooling for genomics), and a two-stage fusion strategy with modality-specific mixture-of-experts to reduce complexity and handle missing modalities. Pretraining leverages intra-modal masked language modeling, inter-modal contrastive learning, and sample-level triplet loss to align heterogeneous data without requiring complete modality pairing. The framework is pretrained on 6,850 TCGA triplets and fine-tuned for downstream tasks including survival prediction, cancer subtyping, gene mutation prediction, and report generation.

## Key Results
- Achieves C-index of 0.762 for survival prediction, outperforming second-best method by 4.1%
- Cancer subtyping AUC reaches up to 0.901 across different cancer types
- Gene mutation prediction AUC up to 0.811 for various genetic alterations
- Report generation BLEU-4 score up to 0.450 for clinical text generation

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Constraint Alignment (Three-Tiered Pretraining Losses)
Combining intra-modal (MLM), inter-modal (contrastive), and sample-level (triplet) constraints enables learning aligned, robust representations across modalities without requiring complete data pairs. Intra-modal MLM forces encoders to learn strong unimodal representations, inter-modal contrastive loss aligns [CLS] token embeddings into shared latent space, and sample-level triplet loss uses cancer type labels to add semantic structure.

### Mechanism 2: Adaptive Fusion via Modality-Specific Mixture-of-Experts (MoE)
A two-stage fusion strategy using shared attention followed by modality-specific expert modules enables efficient handling of missing modalities and reduces computational complexity while preserving modality-specific nuances. Stage 1 shared self-attention maps inputs to unified latent space; Stage 2 modality-specific experts process fused representation to decouple structural characteristics unique to each modality.

### Mechanism 3: Efficient Input Aggregation for High-Dimensional Data
Specialized aggregation techniques for WSIs (region-wise pooling) and genomics (pathway pooling) reduce sequence length drastically, making transformer-based fusion computationally feasible. WSI features are reshaped into 2D maps with mean pooling over non-overlapping spatial regions; genes are grouped by biological pathways with pooling within these pathways.

## Foundational Learning

- **Multiple Instance Learning (MIL):** WSIs are gigapixel images; labeling every pixel/patch is impossible. MIL allows learning from slide-level labels by treating a slide as a "bag" of patch instances.
  - Quick check: Can you explain why a standard CNN classifier fails if applied directly to a gigapixel WSI slide?

- **Contrastive Learning (CLIP-style):** Aligning different modalities requires a way to make representations of matching pairs "close" and non-matching pairs "far" in shared embedding space without explicit semantic labels for every feature.
  - Quick check: Given a batch of image-text pairs, how does the contrastive loss ensure the model learns to associate the correct text with the correct image?

- **Mixture of Experts (MoE):** Different modalities have vastly different statistical properties. MoE allows the model to route processing to specialized sub-networks best suited for each modality's characteristics.
  - Quick check: How does an MoE layer differ from a standard dense feed-forward layer in terms of parameter usage and specialization?

## Architecture Onboarding

- **Component map:** Input Layer (WSI patches, gene vector, report text) -> Feature Extraction (UNI for patches, BioBERT for text) -> Modality Encoders (TransMIL+Region-wise Pooling for WSI, Performer+Pathway Pooling for genes, BioBERT for text) -> Universal Transformer (Modality-Shared Self-Attention -> Modality-Specific MoE) -> Output Heads (task-specific projections)

- **Critical path:** Data preprocessing determines input quality and sequence lengths; modality encoders must produce compatible dimensions to be concatenated; "any-to-any" capability relies on Universal Transformer processing concatenated sequence where any subset of modalities can be present; downstream task heads attach to output representation

- **Design tradeoffs:** Aggregation granularity vs. detail (stronger aggregation saves memory but risks losing fine-grained features); shared vs. specific fusion (shared attention enables cross-modal alignment but may blur modality-specific features; MoE stage counteracts this but adds parameters)

- **Failure signatures:** Unimodal collapse (over-reliance on one modality, showing poor generalization to WSI-only tasks); oversmoothing in aggregation (attention heatmaps look like uniform noise or very large blobs); missing modality failure (performance drops drastically when specific modality is removed at inference time)

- **First 3 experiments:**
  1. Ablation on Fusion Components: Train variants with only shared attention, only MoE, and full two-stage fusion on multimodal survival prediction
  2. Missing Modality Stress Test: Evaluate pretrained model by systematically removing one modality at inference time
  3. Aggregation Sensitivity: Test different region sizes for WSI pooling and different numbers of pathways to find sweet spot between computational cost and performance retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ALTER perform when pretrained on multi-institutional data compared to the single-source TCGA dataset?
- Basis in paper: [explicit] Current implementation is "limited to data from a single sourceâ€”the TCGA dataset," and has "not yet been exposed to the inter-institutional variability characteristic of real-world clinical settings."
- Why unresolved: Computational constraints restricted pretraining to TCGA, leaving model's robustness against domain shifts unverified
- What evidence would resolve it: Evaluation of downstream task performance (C-index, AUC) on external, multi-center cohorts not included in pretraining set

### Open Question 2
- Question: Can explicit mechanisms be developed to quantify the individual contribution of each modality within the fusion layers?
- Basis in paper: [explicit] Model "currently lacks explicit mechanisms to interpret or quantify the individual contributions of each modality," making it difficult to discern how noisy inputs are handled
- Why unresolved: While model is robust to noise via pretraining, specific utility or redundancy of a given modality during inference remains opaque
- What evidence would resolve it: Integration of attribution methods or modality-ablation studies that correlate specific attention patterns with predictive power for individual modalities

### Open Question 3
- Question: How does the composition of pretraining modality combinations influence attention dynamics and downstream generalization?
- Basis in paper: [explicit] "The specific influence of the pretraining data composition on attention dynamics remains unclear," despite observations that multimodal pretraining alters internal representations
- Why unresolved: Paper demonstrates multimodal pretraining changes attention maps but does not isolate how varying ratios or types of modality pairs during pretraining affects this behavior
- What evidence would resolve it: Controlled ablation studies using different pretraining mixtures (e.g., WSI-Report only vs. full triplet) followed by analysis of resulting attention heatmaps

## Limitations
- Implementation is limited to single-source TCGA dataset, lacking exposure to inter-institutional variability
- Model lacks explicit mechanisms to quantify individual modality contributions within fusion layers
- Specific influence of pretraining data composition on attention dynamics remains unclear

## Confidence
- **High Confidence:** Core architectural components (modality-specific encoders, two-stage fusion with MoE, hierarchical pretraining losses) are well-established and logically sound
- **Medium Confidence:** Specific combination of components and hyperparameters appears reasonable but lacks complete specification for exact reproduction
- **Low Confidence:** Claims about absolute performance superiority (4.1% improvement) cannot be independently verified without access to exact implementation details and same data splits

## Next Checks
1. **Ablation Study on Fusion Components:** Train and evaluate variants with only shared attention, only MoE, and full two-stage fusion on multimodal survival prediction
2. **Missing Modality Robustness Test:** Systematically remove each modality at inference time and evaluate downstream task performance
3. **Aggregation Sensitivity Analysis:** Test multiple configurations of region sizes for WSI pooling and pathway groupings for genomics to determine sensitivity to hyperparameters