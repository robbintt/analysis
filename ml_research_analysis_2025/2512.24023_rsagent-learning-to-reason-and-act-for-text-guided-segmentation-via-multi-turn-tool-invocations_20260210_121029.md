---
ver: rpa2
title: 'RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn
  Tool Invocations'
arxiv_id: '2512.24023'
source_url: https://arxiv.org/abs/2512.24023
tags:
- segmentation
- reasoning
- tool
- rsagent
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RSAgent introduces an agentic MLLM that performs interleaved reasoning
  and action for text-guided segmentation via multi-turn tool invocations. Instead
  of predicting masks in a single forward pass, the model interacts with an external
  segmentation toolbox, observes visual feedback, and iteratively refines its localization.
---

# RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations

## Quick Facts
- arXiv ID: 2512.24023
- Source URL: https://arxiv.org/abs/2512.24023
- Reference count: 23
- Primary result: 81.5% cIoU on RefCOCOg test, 66.5% gIoU on ReasonSeg test

## Executive Summary
RSAgent introduces an agentic MLLM that performs interleaved reasoning and action for text-guided segmentation via multi-turn tool invocations. Instead of predicting masks in a single forward pass, the model interacts with an external segmentation toolbox, observes visual feedback, and iteratively refines its localization. A data pipeline generates high-quality multi-turn reasoning trajectories, and training follows a two-stage framework: cold-start supervised fine-tuning and RL with fine-grained rewards. Experiments show RSAgent achieves 81.5% cIoU on RefCOCOg and 66.5% gIoU on ReasonSeg test, outperforming state-of-the-art methods on both in-domain and out-of-domain benchmarks.

## Method Summary
RSAgent is built on Qwen2.5-VL-7B-Instruct and trained in two stages. First, a cold-start SFT phase uses synthetic trajectories filtered to IoU≥0.9 and ≤8 turns, trained with token-masked loss on reasoning+tool-call tokens. Second, GRPO RL fine-tunes the model on tool-based segmentation tasks using a fine-grained reward function combining final mask IoU, step-wise process rewards (including tool costs and point sparsity), and format compliance. The agent observes a history pool of prior mask overlays (384×384) and can invoke SAM2 segmentation or view manipulation tools to refine its belief before committing to a final mask.

## Key Results
- 81.5% cIoU on RefCOCOg test, outperforming state-of-the-art methods
- 66.5% gIoU on ReasonSeg test, showing strong out-of-domain generalization
- Ablation shows R_process contributes ~4.2% performance gain on RefCOCOg testA
- Untrained baseline achieves only 30.1% cIoU, demonstrating value of SFT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn tool invocation with visual feedback enables iterative mask refinement that single-pass methods cannot achieve.
- Mechanism: The MLLM generates reasoning tokens, invokes segmentation tools via spatial prompts, receives rendered overlay feedback, and can revise spatial hypotheses based on historical observations. This loop allows re-localization when early hypotheses are wrong.
- Core assumption: The model can learn to associate visual feedback with decision quality and adjust subsequent actions accordingly.
- Evidence anchors:
  - [abstract]: "RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks."
  - [section 3.2]: "RSAgent alternates between textual reasoning, tool usage, and inspection of updated visual context, gradually refining its belief about the target object before committing to a final mask."
  - [corpus]: Related work (arXiv:2511.19773) supports that tool-integrated reasoning in VLMs benefits from scalable training environments for multi-step visual interactions.
- Break condition: If visual feedback overlays are too low-resolution (384×384 history pool) to convey fine boundary details, refinement may saturate.

### Mechanism 2
- Claim: Fine-grained reward shaping combining process and outcome signals enables better long-horizon credit assignment than outcome-only rewards.
- Mechanism: The reward function R_total = α·R_final + β·R_process + γ·R_format includes: (1) final mask IoU, (2) step-wise IoU improvement with tool costs and novelty bonuses, (3) format compliance penalties.
- Core assumption: Step-level supervision provides richer gradient signals than sparse outcome-only feedback for multi-step decision processes.
- Evidence anchors:
  - [section 3.4.2]: "Unlike prior approaches that rely solely on outcome-based rewards... we introduce a fine-grained reward design that jointly accounts for both the final result and the intermediate decision process."
  - [table 4]: Ablation shows removing R_process drops RefCOCOg testA from 81.8% to 77.6%, removing R_final drops to 68.1%.
  - [corpus]: Evidence weak for this specific reward composition in segmentation—corpus papers focus on grounding but not identical reward decomposition.
- Break condition: If process rewards are mis-specified (e.g., over-penalizing exploration), model may converge to suboptimal local policies.

### Mechanism 3
- Claim: High-quality synthetic trajectories with strict filtering (IoU ≥ 0.9, ≤ 8 turns) bootstrap effective tool-use behavior.
- Mechanism: Strong proprietary VLMs generate problems; Qwen2.5-VL-72B synthesizes trajectories in the same tool environment; filtering removes noisy supervision from compounding errors.
- Core assumption: Synthetic trajectories from stronger teachers transfer tool-use patterns to the student model without inheriting teacher-specific artifacts.
- Evidence anchors:
  - [section 3.3]: "We select the cold-start data with two hard principles: (i) the IoU of the final predicted mask and M_gt can't be lower than 0.9, and (ii) the number of reasoning turns should not surpass 8."
  - [table 3]: "tool-agent (no training)" achieves 30.1% cIoU vs "cold-start SFT only" at 55.4%, demonstrating SFT value.
  - [corpus]: Medical imaging work (arXiv:2508.08177) similarly uses RL for pixel-level grounding from reasoning traces.
- Break condition: If synthetic problems don't match target domain distribution (e.g., ReasonSeg-style compositional queries), generalization may suffer.

## Foundational Learning

- **Markov Decision Processes (MDPs) in vision-language settings**
  - Why needed here: RSAgent formulates segmentation as episodic decision-making with states (visual context + text), actions (tool calls), and rewards.
  - Quick check question: Can you explain how the observation O_t = (V_t, Q, C_t) evolves across turns?

- **SAM2 promptable segmentation**
  - Why needed here: The external segmentation tool receives spatial prompts (boxes, points) and returns masks; understanding its capabilities determines effective action space design.
  - Quick check question: What types of prompts does SAM2 accept, and how does it handle ambiguous inputs?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: RL optimization uses GRPO with group-relative advantages; understanding this clarifies why multiple rollouts per sample are needed.
  - Quick check question: How does the advantage A_i normalization in Equation 6 differ from standard PPO advantage estimation?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL-7B-Instruct -> Tool environment (SAM2 + view ops) -> History pool (384×384 overlays) -> Mask output

- **Critical path**:
  1. Set up tool environment with SAM2 and view operations
  2. Generate cold-start trajectories via data pipeline
  3. Run SFT (2 epochs, lr=2e-5, batch=128)
  4. Run RL with GRPO (batch=16, samples=4, lr=1e-6, max turns=8)

- **Design tradeoffs**:
  - Max turns: Higher improves refinement but risks compounding errors (optimal at 8 in experiments)
  - History pool resolution: 384×384 compresses visual state for context length but may lose boundary detail
  - Reward weights: α=1, β=0.5, γ=0.2—process reward weighted lower than outcome to avoid gaming

- **Failure signatures**:
  - Untrained model (no SFT): 30.1% cIoU on ReasonSeg test—paradigm mismatch with tool-use
  - Excessive turns (>8): Performance degrades due to context dilution and hallucinated rationales
  - Missing R_final: Drops to 48.3% cIoU—model lacks outcome grounding
  - Trajectory filtering too loose: Compounding errors propagate from teacher model

- **First 3 experiments**:
  1. **Sanity check**: Run Qwen2.5-VL-7B-Instruct zero-shot in tool environment; verify it fails to invoke tools correctly (should match "tool-agent (no training)" baseline ~30% cIoU).
  2. **SFT ablation**: Train with cold-start data only; verify ~55% cIoU on ReasonSeg test to confirm trajectory quality.
  3. **Reward component analysis**: Remove R_process and R_format separately; expect ~4-5% and ~2-3% drops respectively per Table 4.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the methodology raises several implicit research directions regarding the generalization of tool-use learning from synthetic data, the trade-off between reasoning depth and computational budget, and the potential for catastrophic forgetting during SFT training.

## Limitations
- Heavy reliance on high-quality synthetic trajectories from proprietary VLMs, making reproducibility challenging
- History pool compression to 384×384 may limit boundary refinement for fine-grained segmentation
- Maximum 8-turn constraint may cap performance on highly complex compositional queries

## Confidence
- **High confidence**: Multi-turn tool invocation improves segmentation over single-pass methods (validated by 30.1% → 55.4% → 81.5% progression)
- **Medium confidence**: Fine-grained reward shaping provides meaningful gradient signals (supported by ablation showing R_process removal drops performance by ~4.2%)
- **Low confidence**: Synthetic trajectory filtering effectively transfers tool-use patterns (limited corpus evidence for this specific filtering approach)

## Next Checks
1. **Data quality audit**: Re-run the cold-start data generation pipeline with different IoU thresholds (0.8, 0.85, 0.95) and measure SFT performance degradation to confirm 0.9 is optimal.
2. **History pool resolution study**: Compare RSAgent performance with history pool resolutions of 256×256, 384×384, and 512×512 on a subset of RefCOCOg to quantify the trade-off between context compression and boundary detail preservation.
3. **Reward component isolation**: Train separate RL variants where each reward component (R_final, R_process, R_format) is the only signal, then combine them to verify additive effects match the claimed total reward design.