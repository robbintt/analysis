---
ver: rpa2
title: 'AgentRM: Enhancing Agent Generalization with Reward Modeling'
arxiv_id: '2502.18407'
source_url: https://arxiv.org/abs/2502.18407
tags:
- reward
- tasks
- agent
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentRM, a generalizable reward model that
  enhances LLM-based agents' performance on unseen tasks through test-time search.
  The key idea is to fine-tune a reward model instead of the policy model, as reward
  modeling is more robust to task distribution shifts.
---

# AgentRM: Enhancing Agent Generalization with Reward Modeling

## Quick Facts
- arXiv ID: 2502.18407
- Source URL: https://arxiv.org/abs/2502.18407
- Reference count: 28
- This paper introduces AgentRM, a generalizable reward model that enhances LLM-based agents' performance on unseen tasks through test-time search, improving base policy models by 8.8 points on average.

## Executive Summary
This paper introduces AgentRM, a generalizable reward model that enhances LLM-based agents' performance on unseen tasks through test-time search. The key idea is to fine-tune a reward model instead of the policy model, as reward modeling is more robust to task distribution shifts. The authors investigate three approaches to construct the reward model: explicit reward modeling (using MCTS-based tree search for automatic process reward annotation), implicit reward modeling (deriving step-level rewards from outcome rewards), and LLM-as-a-judge (using an LLM to assess agent trajectories). AgentRM guides the policy model's answer generation through Best-of-N sampling and step-level beam search. On nine agent tasks spanning web navigation, embodied planning, text games, and tool usage, AgentRM improves the base policy model by 8.8 points on average, outperforming the top general agent by 4.0 points. It demonstrates weak-to-strong generalization, yielding greater improvement of 12.6 points on LLaMA-3-70B.

## Method Summary
The paper proposes a reward modeling approach where an LLM-based reward model is trained to guide policy models during inference rather than during training. The method involves three main stages: (1) Supervised Fine-Tuning (SFT) of a base policy model on expert trajectories, (2) Data generation using MCTS-inspired tree search to automatically annotate process rewards, and (3) Training a reward model to predict state values from the SFT model's trajectories. During inference, the reward model guides answer generation through Best-of-N sampling or step-level beam search. The approach is evaluated on nine tasks spanning web navigation, embodied planning, text games, and tool usage.

## Key Results
- AgentRM improves base policy models by 8.8 points on average across nine tasks
- Outperforms the top general agent by 4.0 points
- Demonstrates weak-to-strong generalization with 12.6 points improvement on LLaMA-3-70B
- For specialized tasks, boosts a finetuned policy model, surpassing the top specialized agent by 11.4 points on three held-in tasks

## Why This Works (Mechanism)
The approach works because reward modeling is more robust to task distribution shifts than policy modeling. By fine-tuning a separate reward model rather than the policy model itself, the method maintains the base model's capabilities while adding task-specific guidance. The MCTS-based tree search provides high-quality process rewards that capture intermediate progress, allowing the reward model to evaluate partial trajectories effectively. This separation of concerns enables better generalization to unseen tasks compared to direct policy fine-tuning.

## Foundational Learning
- **Monte Carlo Tree Search (MCTS)**: A search algorithm that balances exploration and exploitation to find optimal decision paths. Why needed: Provides the mechanism for automatic process reward annotation. Quick check: Verify UCB calculation and node expansion logic match the paper's description (iterations=40, expansion width k=5).
- **Reward Modeling vs Policy Modeling**: Reward models predict scalar values for states/actions while policy models predict action distributions. Why needed: Reward modeling is more robust to distribution shifts, making it suitable for generalization. Quick check: Compare performance of reward model fine-tuning vs policy model fine-tuning on held-out tasks.
- **Weak-to-Strong Generalization**: The phenomenon where a model trained on weaker models performs well on stronger models. Why needed: Enables the reward model to transfer benefits across different model scales. Quick check: Test AgentRM trained on LLaMA-3-8B on LLaMA-3-70B and other model families.
- **Best-of-N Sampling**: Inference strategy that generates multiple trajectories and selects the one with the highest predicted reward. Why needed: Leverages the reward model to choose optimal trajectories without modifying the policy model. Quick check: Compare Best-of-N performance with different N values (e.g., 3, 5, 10).
- **Step-level Beam Search**: Inference strategy that maintains multiple partial trajectories and expands them based on reward predictions. Why needed: Provides more systematic exploration than sampling-based methods. Quick check: Verify beam width parameters (W1=5, W2=5) and their impact on performance.

## Architecture Onboarding
- **Component Map**: SFT Model -> MCTS Data Generator -> Reward Model -> Policy Model (at inference)
- **Critical Path**: Expert trajectories → SFT → MCTS search → Reward model training → Guided inference
- **Design Tradeoffs**: Reward modeling vs policy modeling (generalization vs direct optimization), explicit vs implicit reward annotation (accuracy vs computational cost)
- **Failure Signatures**: Overfitting to action patterns, computational bottleneck in MCTS, poor generalization to different model families
- **First Experiments**: 1) Run small-scale MCTS on single task to verify value estimation, 2) Train reward model on subset of data and test on held-out instructions, 3) Compare Best-of-N performance with different N values

## Open Questions the Paper Calls Out
- How does increasing MCTS depth and iterations impact reward precision and task performance? The paper notes computational constraints limited iterations to 40, leaving this for future work.
- Does weak-to-strong generalization transfer across different model families or only within the same lineage? The paper only demonstrates transfer within LLaMA family.
- Can AgentRM integrate with prompt-based reasoning strategies like Reflexion for further performance enhancement? The paper explicitly states this was not explored.

## Limitations
- Computational cost of MCTS-based data generation with 40 iterations per state
- Limited exploration of integration with prompt engineering strategies like Reflexion
- Unclear transfer capability across fundamentally different model architectures

## Confidence
- **High confidence**: Core methodology is theoretically sound and empirically validated
- **Medium confidence**: Reproducibility may be challenging due to computational requirements and lack of code
- **Low confidence**: Immediate reproduction without codebase would require significant engineering effort

## Next Checks
1. **MCTS Implementation Validation**: Run a small-scale MCTS on a single held-in task (e.g., Alfworld) to verify state value estimation and trajectory generation match the paper's description.
2. **Reward Model Generalization Test**: Evaluate the trained reward model on held-out instructions from the same distribution to check for overfitting to action patterns versus genuine state utility assessment.
3. **Computational Cost Analysis**: Measure the wall-clock time and GPU memory requirements for the full data generation pipeline on representative tasks to assess practical feasibility.