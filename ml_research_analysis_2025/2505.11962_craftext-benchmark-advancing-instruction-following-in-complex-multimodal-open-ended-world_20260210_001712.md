---
ver: rpa2
title: 'CrafText Benchmark: Advancing Instruction Following in Complex Multimodal
  Open-Ended World'
arxiv_id: '2505.11962'
source_url: https://arxiv.org/abs/2505.11962
tags:
- agent
- instructions
- instruction
- tasks
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CrafText, a benchmark for evaluating instruction
  following in a complex multimodal environment with dynamic interactions. The benchmark
  features 3,924 instructions with 3,423 unique words across four task categories
  (Localization, Conditional, Building, and Achievement) in an open-ended world where
  objects change properties over time.
---

# CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World

## Quick Facts
- arXiv ID: 2505.11962
- Source URL: https://arxiv.org/abs/2505.11962
- Authors: Zoya Volovikova; Gregory Gorbov; Petr Kuderov; Aleksandr I. Panov; Alexey Skrynnik
- Reference count: 40
- Primary result: Introduced CrafText benchmark with 3,924 instructions; even well-trained agents struggle with dynamic environments and linguistic complexity, highlighting advantages of planning-based approaches for novel tasks.

## Executive Summary
This paper introduces CrafText, a benchmark for evaluating instruction following in a complex multimodal environment with dynamic interactions. The benchmark features 3,924 instructions with 3,423 unique words across four task categories in an open-ended world where objects change properties over time. A specialized evaluation protocol measures generalization to novel instruction formulations and dynamically evolving task configurations. Experiments with PPO, Dynalang, and FiLM baselines show that even well-trained agents struggle with dynamic environments and linguistic complexity, with success rates ranging from 0.05 to 0.45 on the training set.

## Method Summary
CrafText evaluates instruction-following agents in the Craftax environment, a dynamic open world where objects change properties over time. The benchmark includes 3,924 instructions across four task categories (Localization, Conditional, Building, and Achievement) with 3,423 unique words. Agents receive multimodal observations (visual + text) and must execute actions to complete instructions. Four baselines are evaluated: PPO-T (DistilBERT embeddings + GRU), PPO-T+ (adds GPT-4 generated plans), FiLM (feature-wise modulation), and Dynalang (model-based with world modeling). Training uses 1024 parallel environments for 1B timesteps, with success rate as the primary metric.

## Key Results
- PPO-T+ achieves highest generalization to new goals (SR = 0.28 on Test New Objects) by leveraging external planning
- All baselines struggle with dynamic environments, with success rates ranging from 0.05 to 0.45 on training set
- FiLM shows competitive performance (SR = 0.38 on Train) through flexible text-visual integration
- Dynalang performs poorly (SR = 0.15) in complex linguistic and dynamic conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Planning-based instruction preprocessing aids in solving novel tasks and linguistic variations more effectively than direct instruction encoding.
- Mechanism: PPO-T+ converts natural language instructions into structured action plans using GPT-4, then feeds these plans as goal representations to the agent. This reduces complex language mapping to canonical plan format.
- Core assumption: LLM can reliably decompose instructions into executable sub-goals within environment's action space.
- Evidence anchors: Abstract highlights planning advantages; PPO-T+ achieves highest SR on Test New Objects (0.28); indicates effective goal-level generalization.
- Break condition: Fails if planner produces unexecutable plans or agent cannot execute plan steps due to environmental stochasticity.

### Mechanism 2
- Claim: Feature-wise affine transformations of visual features, conditioned on text, enable more flexible integration of linguistic and perceptual information.
- Mechanism: FiLM applies learned affine transformation (scale/shift) to convolutional layer activations, with parameters computed from instruction embedding.
- Core assumption: Visual features are semantically rich enough that simple affine transformation can effectively highlight task-relevant features.
- Evidence anchors: FiLM comes closest to PPO-T+ on New Objects; strong performance attributed to flexible text-visual integration.
- Break condition: Less effective when visual features are highly entangled, making selective amplification difficult.

### Mechanism 3
- Claim: Predictive world modeling with language can enable sample-efficient grounding in multimodal environments.
- Mechanism: Dynalang learns joint model of world by predicting future textual and visual states, using this model for planning or policy training via latent imagination.
- Core assumption: Environment dynamics and language-state correspondence can be captured by learned latent model.
- Evidence anchors: Dynalang achieves only 0.15 SR; authors note combination of complex instructions and dynamic environment makes learning significantly more difficult.
- Break condition: Breaks down when combined complexity of dynamic transitions and linguistic descriptions makes state space too difficult to model accurately.

## Foundational Learning

- **Reinforcement Learning (RL) & POMDPs**
  - Why needed here: Core problem is agent learning policy π(a|o) to maximize reward in dynamic, partially observable environment formalized as POMDP.
  - Quick check question: What are the key components of the POMDP tuple (S, A, O, T, R, G, γ) defined in the paper, and what does the policy map to?

- **Language Grounding**
  - Why needed here: Agent must connect textual instructions to visual observations and executable actions - core challenge being benchmarked.
  - Quick check question: What function must the agent approximate to infer the latent goal 'g' from the instruction 'I'?

- **Generalization in RL**
  - Why needed here: Benchmark evaluates generalization to novel instruction formulations (paraphrasing) and new goal configurations, not just training set performance.
  - Quick check question: What are the two specific types of generalization measured by the CrafText evaluation protocol?

## Architecture Onboarding

- **Component map**: CrafText system consists of: 1) Craftax Environment (dynamic open world); 2) CrafText Dataset (3,924 instructions, validation checkers); 3) Agent (receives multimodal observations, outputs actions). Agent architecture varies: PPO-T (GRU + DistilBERT), PPO-T+ (adds GPT-4 planning), FiLM (text-conditioned affine transformations), Dynalang (predictive world model).

- **Critical path**: Most critical path is data and environment loop - understanding how instruction is selected, world is initialized, observations are constructed, and Scenario Checker validates task completion at each timestep. Interplay between stochastic world dynamics and deterministic validation logic is key.

- **Design tradeoffs**: Central tradeoff between architectural complexity and generalization. PPO-T+ achieves best generalization via external planner but adds dependency/failure point. FiLM offers more integrated approach with competitive results. Choice of text encoder and fusion method is primary architectural lever.

- **Failure signatures**: Dynalang's low success rate (0.15) indicates methods successful in simpler environments may struggle with CrafText's combined complexity. Performance drop across all models from training to test sets highlights generalization difficulty.

- **First 3 experiments**:
  1. Implement and train PPO-T and FiLM baselines on Medium difficulty training set; verify reproduction of reported success rates (around 0.40 and 0.43).
  2. Modify PPO-T to use cross-attention instead of concatenation for modality fusion; measure impact on Test Paraphrased success rate.
  3. Train PPO-T+ agent and evaluate on custom test set with new object combinations requiring longer action sequences than training to probe planning mechanism limits.

## Open Questions the Paper Calls Out

- How does the absence of human-generated instructions in the CrafText dataset limit an agent's ability to generalize to context-rich, real-world tasks? The paper notes AI-generated instructions may lack "depth and nuance" of human-crafted ones, but the dataset relies entirely on GPT-4 for consistency and scalability.

- What architectural modifications are required for model-based agents (like Dynalang) to handle the combination of high-complexity textual instructions and dynamic environmental interactions? Authors observe Dynalang's poor performance (0.15 SR) but only hypothesize the learning process is "significantly more difficult" without identifying specific mechanisms.

- How can planning-based methods be made robust to linguistic paraphrasing to prevent performance degradation when instructions are reformulated? PPO-T+ suffers largest performance drop on Paraphrased test set, suggesting reformulated instructions lead to divergent, suboptimal plans.

## Limitations

- Benchmark complexity introduces significant variability in agent performance, particularly for methods like Dynalang that struggle with combined challenges of linguistic diversity and dynamic environments.
- Reliance on external planners like GPT-4 in PPO-T+ introduces potential point of failure not fully characterized - if planner fails, entire system performance degrades.
- Dataset construction may not capture full space of real-world instruction-following scenarios, potentially limiting external validity.

## Confidence

- **High**: CrafText presents challenging benchmark requiring generalization to novel instructions and object configurations. Performance gap between training and test sets across all baselines strongly supports this.
- **Medium**: Planning-based approaches (PPO-T+) are superior for handling novel tasks and linguistic variations. While PPO-T+ shows best generalization, mechanism's reliance on external LLM and lack of comparison to other planning methods introduce uncertainty.
- **Medium**: FiLM provides "flexible integration" of textual and visual information. Paper provides evidence of competitive performance but does not deeply analyze why this flexibility translates to better generalization.

## Next Checks

1. **Generalization Gap Analysis**: Conduct systematic study of performance gap between training and test sets for each baseline, broken down by task category and instruction complexity to identify which types of generalization are more challenging.

2. **Planner Dependency Evaluation**: Implement ablation study for PPO-T+ where GPT-4 planning module is replaced with simpler rule-based planner or removed entirely; compare performance to isolate contribution of external planner versus base PPO-T architecture.

3. **Architectural Sensitivity Test**: Perform hyperparameter sweep on visual encoder architecture (varying CNN depth, filter sizes) and text-visual fusion method for PPO-T baseline to quantify sensitivity to design choices and identify more optimal configurations.