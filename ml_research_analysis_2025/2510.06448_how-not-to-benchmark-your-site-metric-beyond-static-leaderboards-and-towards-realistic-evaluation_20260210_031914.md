---
ver: rpa2
title: 'How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards
  Realistic Evaluation'
arxiv_id: '2510.06448'
source_url: https://arxiv.org/abs/2510.06448
tags:
- transferability
- accuracy
- score
- metrics
- site
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper critiques the standard benchmarks used to evaluate
  Source Independent Transferability Estimation (SITE) metrics, which rank pre-trained
  models by predicted performance on target tasks without fine-tuning. The authors
  identify three critical flaws: (1) the model space is unrealistic, dominated by
  large variants of two architectures, (2) the benchmark is solved by a static ranking
  where one model consistently wins, and (3) score differences do not meaningfully
  correlate with accuracy gaps.'
---

# How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation

## Quick Facts
- **arXiv ID:** 2510.06448
- **Source URL:** https://arxiv.org/abs/2510.06448
- **Reference count:** 40
- **Key outcome:** Simple static heuristics outperform sophisticated Source Independent Transferability Estimation (SITE) metrics on standard benchmarks due to unrealistic model spaces dominated by size variants of two architectures.

## Executive Summary
This paper critically examines standard benchmarks for evaluating Source Independent Transferability Estimation (SITE) metrics, which predict how well a pre-trained model will perform on a target task without fine-tuning. The authors identify three fundamental flaws in current evaluation practices: benchmarks use unrealistic model spaces dominated by size variants of the same architectures, performance is largely explained by a static leaderboard where one model consistently wins, and high rank correlations do not guarantee meaningful score differences that practitioners can use for decision-making. Through systematic ablation studies, they demonstrate that simple dataset-agnostic heuristics achieve higher Weighted Kendall's Tau than sophisticated SITE metrics, suggesting that reported metric performance is artificially inflated. The paper concludes with actionable best practices for constructing more robust benchmarks that better reflect real-world model selection scenarios.

## Method Summary
The study evaluates 11 pre-trained image classification models (including ResNet variants, DenseNet, Inception, GoogleNet, MobileNet, and MNASNet) on 6 target datasets (CIFAR10/100, Pets, Aircraft, Food, and DTD). Ground truth performance is established through grid search fine-tuning with varying learning rates and weight decay. The paper tests both standard SITE metrics (LogME, TransRate, GBC, NLEEP, SFDA, H-Score) and a simple static ranking heuristic (ResNet-152 > DenseNet-201 > ResNet-101 > ...). Evaluation uses Weighted Kendall's Tau (τw) to measure rank correlation between predicted scores and ground truth accuracies, with a secondary fidelity check examining correlation between pairwise score differences and accuracy differences. The authors conduct progressive ablation studies by removing large models and architectural variants to test metric robustness.

## Key Results
- A simple static ranking heuristic achieves an average Weighted Kendall's Tau of 0.91, significantly outperforming the best SITE metric (LogME at 0.57).
- Most SITE metrics show sharp performance drops when large size variants are removed from the model space, indicating reliance on size detection rather than feature quality.
- Pairwise score differences from SITE metrics show weak correlation with actual accuracy differences, demonstrating poor "fidelity" for practical decision-making.
- Current benchmarks are solved by trivial solutions that exploit static performance hierarchies rather than task-specific transferability estimation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Benchmark performance of SITE metrics is likely artificially inflated by unrealistic model spaces dominated by size variants of the same architectures.
- **Mechanism:** The standard benchmark pools models like ResNet-34, ResNet-50, and ResNet-101. Since larger models predictably outperform smaller variants of the same family, the benchmark inadvertently reduces the complex task of transferability estimation to a trivial detection of the largest model capacity.
- **Core assumption:** Assumption: A robust transferability metric should distinguish utility between *architecturally distinct* models of similar capacity, rather than just identifying the largest parameter count.
- **Evidence anchors:**
  - [Abstract]: Mentions "unrealistic model spaces dominated by size variants of two architectures" artificially inflating performance.
  - [Page 5, Figure 2]: Shows that most SITE metrics exhibit a sharp drop in weighted Kendall's Tau (τw) when large size variants (e.g., ResNet-152) are ablated, suggesting high scores rely on size detection rather than feature quality.
  - [Corpus]: Weak direct link; however, "Statistical Uncertainty Quantification for Aggregate Performance Metrics" (arXiv:2501.04234) generally supports the need for rigorous scrutiny of how aggregate metrics can hide underlying distribution flaws.
- **Break condition:** This mechanism breaks if SITE metrics maintain high performance on a curated benchmark where all candidate models have strictly comparable parameter counts or FLOPs.

### Mechanism 2
- **Claim:** High reported correlations for SITE metrics may reflect the exploitation of static performance hierarchies rather than task-specific prediction.
- **Mechanism:** If a "static leaderboard" exists—where one dominant model (e.g., ResNet-152) wins across nearly all datasets—a metric can achieve high scores simply by ranking that model first regardless of the target data. The benchmark effectively tests "memorization of general model quality" rather than "transferability estimation."
- **Core assumption:** Assumption: Meaningful model selection requires the "best" model to vary depending on the specific target task (rank dispersion).
- **Evidence anchors:**
  - [Abstract]: States that "simple, dataset-agnostic heuristics can outperform sophisticated methods."
  - [Page 6, Table 1]: Demonstrates that a "Static Ranking" heuristic achieves an average τw of 0.91, significantly outperforming the best SITE metric (LogME at 0.57).
  - [Corpus]: "Evaluating the Evaluators" (arXiv:2509.21227) aligns with the need to validate if metrics capture the intended construct (compositionality) versus exploiting dataset artifacts.
- **Break condition:** This mechanism breaks if the benchmark is engineered such that different architectural paradigms excel on different datasets, invalidating a single static ranking.

### Mechanism 3
- **Claim:** Relying solely on rank correlation (Weighted Kendall's Tau) ignores the lack of "fidelity" between score magnitudes and actual accuracy differences.
- **Mechanism:** A metric might correctly rank Model A > Model B > Model C, but the *score gap* between A and B might be tiny while the *accuracy gap* is huge (or vice versa). Standard evaluation protocols accept the ranking as sufficient, failing to penalize metrics for uncalibrated or misleading score magnitudes.
- **Core assumption:** Assumption: Practitioners use metric scores to assess cost-benefit tradeoffs (e.g., "Is the accuracy gain worth the inference cost?"), requiring score gaps to correlate with performance gains.
- **Evidence anchors:**
  - [Page 7, Figure 4]: Presents a heatmap showing weak Pearson correlation between pairwise accuracy differences (ΔAcc) and metric score differences (ΔT).
  - [Page 6]: Formalizes the "fidelity" property: ΔAcc(A, B) > ΔAcc(C, D) ⟹ ΔT(A, B) > ΔT(C, D).
  - [Corpus]: "Statistical Uncertainty Quantification..." (arXiv:2501.04234) broadly supports looking beyond single aggregate scores to understand performance distributions.
- **Break condition:** This mechanism breaks if evaluation protocols mandate reporting the correlation of pairwise differences or regression error alongside rank correlation.

## Foundational Learning

- **Concept: Source Independent Transferability Estimation (SITE)**
  - **Why needed here:** This is the core object of study. You must understand that SITE metrics predict downstream performance using *only* the pre-trained weights and the target dataset (features + labels), without accessing the original source training data.
  - **Quick check question:** If you have access to the source dataset (e.g., ImageNet) to compute domain divergence, are you evaluating a SITE metric or a Source Dependent (SDTE) metric?

- **Concept: Weighted Kendall's Tau (τw)**
  - **Why needed here:** This is the standard KPI the paper critiques. It measures rank correlation but weights pairs involving top-ranked models more heavily. Understanding this explains why the paper focuses on the "top-1" selection problem and static leaders.
  - **Quick check question:** Does a high Weighted Kendall's Tau guarantee that the difference in scores between two models reflects their actual accuracy gap?

- **Concept: Model Space Diversity & Rank Dispersion**
  - **Why needed here:** The paper's solution centers on fixing the "Model Zoo." You need to distinguish between *size diversity* (trivial scaling) and *architectural diversity* (convolutional vs. transformer vs. MLP) to understand the proposed best practices.
  - **Quick check question:** If ResNet-152 is the best model on 8 out of 10 datasets in your benchmark, does this indicate high rank dispersion or a static leaderboard?

## Architecture Onboarding

- **Component map:** Model Zoo -> Feature Extractor -> SITE Metric -> Ground Truth -> Evaluator
- **Critical path:** The validity of the evaluation hinges on the **Model Zoo construction**. If the Zoo contains size-variant hierarchies or lacks architectural diversity, the Evaluator will return high scores for trivial solutions.
- **Design tradeoffs:**
  - **Size Variants vs. Architectural Diversity:** Including ResNet-34/50/101 increases dataset size but introduces trivial scaling laws. The paper recommends trading size variants for distinct architectures (e.g., ConvNeXt, Swin) at similar FLOPs.
  - **Saturation vs. Headroom:** Saturated datasets (99% accuracy) obscure model differences. The tradeoff is to select harder datasets where performance headroom allows for meaningful differentiation.
- **Failure signatures:**
  - **High τw, Low Fidelity:** Metric ranks well but score differences do not correlate with accuracy differences.
  - **Static Ranker Dominance:** A simple heuristic (e.g., "Always pick ResNet-152") matches or beats the SITE metric.
  - **Ablation Sensitivity:** Metric performance crashes when the largest model is removed from the Zoo.
- **First 3 experiments:**
  1. **Run the Static Baseline:** Implement the "Static Ranking" heuristic (ResNet-152 > DenseNet-201 > ...) on your specific benchmark. If it beats your SITE metric, your benchmark is likely flawed/trivial.
  2. **Progressive Ablation Test:** Sequentially remove the largest size variants from your Model Zoo and plot the SITE metric's τw. A steep drop indicates the metric relies on size bias rather than feature quality.
  3. **Fidelity Check (Delta Correlation):** Compute the Pearson correlation between all pairwise accuracy differences (ΔAcc) and pairwise metric score differences (ΔT). A low score (< 0.5) indicates the metric's magnitude is untrustworthy for decision-making.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can SITE metrics be developed to explicitly account for the influence of varying fine-tuning strategies, optimizers, and hyperparameters on final model performance?
- **Basis in paper:** [explicit] The "Limitations and Future Work" section states that current methods do not take these factors into account "while predicting transferability, whereas research has shown that they play a big role in model performance."
- **Why unresolved:** Current evaluation protocols isolate the model and data but treat the fine-tuning recipe as a constant, ignoring a significant source of variance in real-world transfer performance.
- **What evidence would resolve it:** A SITE metric that successfully predicts transferability scores that remain robust (high correlation) across a grid search of different optimizers and learning rates.

### Open Question 2
- **Question:** Do the benchmark flaws identified in image classification (static leaderboards, unrealistic model spaces) persist in other domains such as Natural Language Processing (NLP) or medical imaging?
- **Basis in paper:** [explicit] The "Limitations and Future Work" section notes the study focuses on image classification but suggests the investigation "can be further expanded to NLP, object detection, and medical image classification domains."
- **Why unresolved:** The paper provides a blueprint for analysis and briefly mentions NLP biases in Table 2, but lacks a full empirical critique of the specific benchmarks used in these other fields.
- **What evidence would resolve it:** An empirical analysis of NLP/medical benchmarks showing that simple static heuristics outperform sophisticated SITE metrics, similar to the results in Figure 2 and Table 1.

### Open Question 3
- **Question:** How can transferability metrics be explicitly designed to maximize fidelity (linear mapping of score gaps to accuracy gaps) rather than just ranking correlation?
- **Basis in paper:** [inferred] Critique 3 demonstrates that current metrics lack fidelity, meaning score differences do not meaningfully correspond to accuracy differences. The authors recommend reporting fidelity correlations but do not propose a method for improving the metric design itself.
- **Why unresolved:** Existing metrics are optimized for rank correlation (Kendall's Tau), leaving the practical utility of interpreting the *magnitude* of a score difference unresolved for practitioners.
- **What evidence would resolve it:** A novel SITE metric where the Pearson correlation between pairwise score differences (ΔT) and pairwise accuracy differences (ΔAcc) is consistently high across diverse datasets.

## Limitations

- The study's conclusions are based on a specific set of 11 pre-trained models and 6 target datasets, which may not generalize to other model architectures or task domains.
- The proposed "fidelity" metric is theoretically sound but lacks empirical validation across different model families and dataset complexities.
- The paper does not provide a concrete method for designing SITE metrics that maximize fidelity rather than just rank correlation.

## Confidence

- **High:** The identification of three fundamental flaws in standard SITE benchmarks (unrealistic model spaces, static rankings, lack of fidelity) is well-supported by empirical evidence and logical reasoning.
- **Medium:** The effectiveness of the proposed best practices (diversifying model spaces, ensuring rank dispersion, releasing evaluation code) is demonstrated through ablation studies but requires broader validation.
- **Medium:** The claim that simple static heuristics outperform sophisticated SITE metrics is convincing within the tested framework but may not hold for metrics designed with different objectives or evaluation protocols.

## Next Checks

1. Test the static ranking heuristic and SITE metrics on a benchmark with strictly size-matched models (e.g., all ResNet-50 variants with different architectural modifications) to isolate the effect of architectural diversity.
2. Implement the "fidelity" check by computing the correlation between pairwise accuracy differences and metric score differences on diverse model-dataset pairs outside the paper's original dataset set.
3. Conduct a large-scale study varying the number of candidate models (e.g., 5 vs. 20) to determine the minimum model space diversity required to meaningfully differentiate SITE metric performance from trivial baselines.