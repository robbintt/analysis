---
ver: rpa2
title: '$k$-Graph: A Graph Embedding for Interpretable Time Series Clustering'
arxiv_id: '2502.13049'
source_url: https://arxiv.org/abs/2502.13049
tags:
- time
- series
- graph
- clustering
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the interpretability challenge in time series
  clustering by introducing k-Graph, a novel graph-based approach that constructs
  multiple graph representations from time series subsequences of varying lengths.
  The method embeds time series into directed graphs where nodes represent groups
  of similar subsequences and edges encode transition frequencies, then clusters using
  feature extraction from these graphs and consensus clustering across multiple subsequence
  lengths.
---

# $k$-Graph: A Graph Embedding for Interpretable Time Series Clustering

## Quick Facts
- **arXiv ID:** 2502.13049
- **Source URL:** https://arxiv.org/abs/2502.13049
- **Reference count:** 40
- **Primary result:** Novel graph-based time series clustering method achieving state-of-the-art accuracy while providing interpretable graphoid patterns

## Executive Summary
$k$-Graph introduces a novel graph embedding approach for interpretable time series clustering that addresses the fundamental tradeoff between accuracy and interpretability. The method constructs multiple graph representations from time series subsequences of varying lengths, where nodes represent dense regions of similar subsequences and edges encode transition frequencies. Unlike traditional methods that only provide cluster centroids, $k$-Graph offers intrinsic interpretability through "graphoids" - graph regions that capture representative and exclusive patterns for each cluster. Experimental results on 113 real-world datasets from the UCR Archive demonstrate superior performance across multiple metrics while maintaining competitive execution times.

## Method Summary
$k$-Graph embeds time series into directed graphs by extracting subsequences of varying lengths, projecting them into 2D space using PCA, and identifying dense regions as nodes connected by edges representing temporal transitions. For each subsequence length, the method creates a sparse feature vector counting node and edge crossings, then applies k-Means clustering. A consensus matrix aggregates results across all subsequence lengths, with spectral clustering producing final cluster assignments. The approach uniquely provides interpretability through "Graphoids" - nodes that are both highly representative of a cluster and exclusive to it, allowing users to understand what patterns define each cluster.

## Key Results
- Achieves best average performance across all 113 UCR Archive datasets for three of four evaluation metrics (ARI, AMI, NMI, RI)
- Outperforms state-of-the-art methods including GOA, TICC, ShapeGPT, and ESCOC in clustering accuracy
- Demonstrates robustness to noise while maintaining competitive execution times compared to baseline methods
- Provides intrinsic interpretability through automatically identified graphoid patterns that define cluster characteristics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing time series as directed graphs of subsequences preserves local shape information and temporal flow that raw global averaging (centroids) often discards.
- **Mechanism:** The algorithm extracts subsequences of length $\ell$, projects them into a 2D space using PCA, and identifies dense regions as "nodes." It then connects these nodes based on temporal succession (edges). This transforms a continuous signal into a discrete state-transition model.
- **Core assumption:** Similar time series will traverse similar regions of this graph, and "shapes" of interest can be captured by the density of projected subsequences.
- **Evidence anchors:**
  - [abstract] "embeds time series into directed graphs where nodes represent groups of similar subsequences and edges encode transition frequencies"
  - [section III-A] "The set of nodes represent groups of similar subsequences of length ℓ... The edges have weights corresponding to the number of times one subsequence... has been followed by a subsequence of the other node."
  - [corpus] Related work "Graphint" reinforces the utility of this approach, noting that graph-based clustering helps maintain data relationships often lost in other methods.
- **Break condition:** This mechanism degrades if the subsequence length $\ell$ is significantly mismatched with the actual pattern duration, causing relevant shapes to be split or obscured during the PCA projection.

### Mechanism 2
- **Claim:** Clustering based on a consensus of multiple graph representations (varying subsequence lengths) provides robustness against noise and parameter selection errors compared to single-view clustering.
- **Mechanism:** Instead of picking one optimal $\ell$, $k$-Graph constructs $M$ graphs. It clusters the data $M$ times and builds a Consensus Matrix ($M_C$) counting how often pairs of time series cluster together. Spectral clustering on this matrix amplifies stable agreements and dampens sporadic disagreements.
- **Core assumption:** The assumption is that "true" cluster structure is invariant to subsequence length changes, while noise or spurious patterns are not.
- **Evidence anchors:**
  - [abstract] "constructs multiple graph representations from time series subsequences of varying lengths... then clusters using feature extraction... and consensus clustering"
  - [section III-C] "We build a consensus matrix... MC can be seen as a similarity matrix... we use spectral clustering... to find communities of highly connected nodes."
  - [section IV-A] "k-Graph's efficiency is unaffected by noise and is more robust than the baselines."
- **Break condition:** If the dataset is composed of very short time series (e.g., the TRAFFIC category in Section IV-A), the mechanism fails because the range of possible subsequence lengths is too restricted to generate meaningful variance for consensus.

### Mechanism 3
- **Claim:** Quantifying "Exclusivity" and "Representativity" of graph nodes allows for the automatic identification of discriminative subsequences (Graphoids), providing intrinsic interpretability lacking in black-box methods.
- **Mechanism:** After clustering, the algorithm back-maps the clusters to the graph. It calculates $Re(N)$ (how many series in a cluster cross node $N$) and $Pr_C(N)$ (what fraction of $N$'s traffic comes from that cluster). Nodes with high values in both are selected as "Graphoids"—the defining patterns of the cluster.
- **Core assumption:** The assumption is that a cluster can be defined by a specific subsequence (or small set of subsequences) that is both frequent within the cluster and rare outside it.
- **Evidence anchors:**
  - [section II-D] Definitions 3 & 4 formalize Node Representativity and Node Exclusivity.
  - [section IV-D] Figure 10 illustrates that Node $N(4)$, which is crossed only by time series from cluster 1, serves as a defining characteristic (Graphoid).
  - [corpus] "Implet" validates the broader need for subsequence-level explanations, supporting the premise that identifying specific "shapes" is crucial for interpretability.
- **Break condition:** This fails if the clusters are defined by global temporal dynamics rather than local shape motifs (e.g., two classes have the same shapes but different ordering), as the "bag of nodes" approach might miss sequential dependencies.

## Foundational Learning

- **Concept: Sliding Windows & Subsequence Extraction**
  - **Why needed here:** The entire $k$-Graph architecture relies on breaking continuous time series into chunks (subsequences) to build the graph nodes. Without understanding sliding windows, one cannot grasp how the "states" of the graph are formed.
  - **Quick check question:** If a time series has length $n$ and we extract subsequences of length $\ell$, how many overlapping subsequences can we extract?

- **Concept: Principal Component Analysis (PCA)**
  - **Why needed here:** $k$-Graph uses PCA to reduce the dimensionality of raw subsequences into a 2D space to identify "dense regions" (nodes). Understanding PCA helps explain why the method is robust to noise (Section III-A).
  - **Quick check question:** Why would projecting high-dimensional subsequences into 2D using PCA help in grouping "similar" shapes?

- **Concept: Spectral Clustering**
  - **Why needed here:** This is the final aggregation step. Unlike k-Means (which assumes spherical clusters), Spectral Clustering cuts the "consensus graph" to find communities of time series that vote similarly.
  - **Quick check question:** How does Spectral Clustering differ from K-Means when dealing with non-convex cluster shapes (like a "horseshoe")?

## Architecture Onboarding

- **Component map:** Time Series -> Subsequence Extraction -> PCA Projection -> Node/Edge Creation (G_ℓ) -> Feature Vector (Node/Edge Frequencies) -> k-Means Clustering -> Consensus Matrix (M_C) -> Spectral Clustering -> Final Labels -> Graphoid Extraction

- **Critical path:** The **Consensus Matrix construction** ($M_C$) is the synchronization point. You must execute the Graph Embedding and Base Clustering loop (which is parallelizable) $M$ times before you can aggregate the results.

- **Design tradeoffs:**
  - **Parameter $M$ (Number of lengths):** Increasing $M$ improves accuracy up to a plateau (around 30) but linearly increases preprocessing time (Section IV-C).
  - **Parameter $smpl$ (Downsampling):** Reducing the sample rate speeds up PCA training but risks missing rare patterns necessary for node creation.
  - **Feature Choice:** The paper uses node/edge frequencies + degree. Using *only* node frequencies might be faster but would lose transition information (temporal order).

- **Failure signatures:**
  - **Short Series:** If $|T|$ is very small (e.g., 24 points in the Traffic dataset), the graph embedding fails because the "maximum length rate" ($rml$) restricts $\ell$ to tiny windows that lack discriminative power.
  - **Identical Shapes, Different Orders:** If Class A is "Shape X then Y" and Class B is "Shape Y then X", simple frequency-based feature extraction might struggle to distinguish them if the edge features are not weighted heavily.

- **First 3 experiments:**
  1. **Sanity Check (Trace Dataset):** Run $k$-Graph on the "Trace" dataset (used extensively in the paper). Verify that the generated graph $G_{\bar{\ell}}$ visually matches the topology in Figure 10.
  2. **Parameter Sensitivity ($M$):** Run the pipeline on a medium-sized dataset while varying $M$ from 5 to 50. Plot Accuracy vs. Execution Time to observe the "plateau" effect described in Section IV-C.
  3. **Interpretability Validation:** After clustering a dataset with known ground truth (like ECGs), manually inspect the "Graphoids" (best nodes). Do the extracted subsequences match the known medical features (e.g., QRS complexes)?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can k-Graph be extended to handle multivariate time series clustering while managing the explosion in graph size due to inter-dimensional dependencies?
- **Basis in paper:** [explicit] The authors state: "We note that applying k-Graph to multivariate time series is not straight-forward. The main challenge in doing so is due to the inter-dependencies among dimensions, which should be considered in the graph embedding step. This would lead to a potential explosion of the graph size. We consider this problem non-trivial, and a very interesting future research direction."
- **Why unresolved:** The current graph embedding assumes univariate subsequences; multivariate data would require encoding cross-dimensional relationships, potentially making graphs exponentially larger and more complex.
- **What evidence would resolve it:** A modified graph embedding mechanism that efficiently represents multivariate patterns, demonstrated on multivariate benchmark datasets with comparable accuracy and interpretability to the univariate version.

### Open Question 2
- **Question:** Can k-Graph be adapted to effectively cluster very short time series (e.g., fewer than 30 data points) where meaningful subsequence extraction is limited?
- **Basis in paper:** [explicit] The paper acknowledges: "As k-Graph relies on extracting meaningful subsequences, such dataset with very short time series is particularly hard to handle. Consequently, we can empirically observe a potential limitation of k-Graph for very short time series."
- **Why unresolved:** The method requires subsequences of at least 5 points and builds graphs from multiple lengths, which becomes infeasible when time series themselves are very short.
- **What evidence would resolve it:** A modified approach for short series, or hybrid method combining k-Graph with complementary techniques, validated on datasets with average length below 30 points.

### Open Question 3
- **Question:** How can the three key parameters (M, smpl, rml) be automatically optimized for a given dataset rather than relying on fixed defaults?
- **Basis in paper:** [inferred] The paper evaluates parameter influence (Section IV-C) showing that M=30, smpl=10, and rml=0.4 are default values that work well on average, but provides no mechanism for dataset-specific optimization. The results show accuracy plateaus at M≈30, but this may vary across data types.
- **Why unresolved:** Different datasets may have different optimal subsequence length distributions, and the current random selection strategy with fixed defaults may be suboptimal for specific domains.
- **What evidence would resolve it:** An adaptive parameter selection method (e.g., based on dataset characteristics like length, noise level, or preliminary graph quality metrics) that consistently matches or exceeds default performance across diverse datasets.

### Open Question 4
- **Question:** How would integrating Graph Neural Networks into the k-Graph pipeline improve clustering accuracy or interpretability?
- **Basis in paper:** [explicit] The authors state: "In future work, we aim to explore the use of Graph Neural Networks in our clustering process."
- **Why unresolved:** Current clustering uses simple k-Means on extracted features; GNNs could potentially learn richer graph representations but may sacrifice the transparency of the current graphoid-based interpretation.
- **What evidence would resolve it:** Comparative experiments showing whether GNN-based clustering improves accuracy while maintaining or enhancing the interpretability provided by graphoids.

## Limitations

- **Short Series Constraint:** The method fails on datasets with very short time series (e.g., TRAFFIC category) because restricted subsequence length ranges prevent meaningful graph construction
- **Local Pattern Dependency:** Assumes meaningful patterns can be captured through local subsequence shapes, which may not hold for datasets where global temporal dynamics or complex sequential dependencies define clusters
- **Computational Cost:** Preprocessing time scales linearly with the number of subsequence lengths (M parameter), creating a tradeoff between accuracy and efficiency

## Confidence

- **High Confidence:** The core mechanism of building graph representations from subsequences and using consensus clustering is well-supported by experimental results showing superior accuracy across 113 datasets and four evaluation metrics. The interpretability claims are validated through the Graphoids concept and visual examples.
- **Medium Confidence:** The robustness to noise claims are supported by controlled experiments but may vary depending on noise characteristics and signal-to-noise ratios in different domains. The computational efficiency claims are reasonable but depend heavily on parameter choices.
- **Low Confidence:** The generalizability to all time series domains is uncertain, particularly for very short series or cases where temporal ordering is more important than local shapes. The method's performance on streaming or real-time applications is not addressed.

## Next Checks

1. **Cross-Domain Testing:** Apply k-Graph to time series from domains not represented in the UCR Archive (e.g., medical monitoring, industrial sensor data) to validate generalizability beyond benchmark datasets.
2. **Extreme Parameter Sensitivity:** Systematically test k-Graph on datasets with varying characteristics (short vs. long series, periodic vs. aperiodic patterns) while varying the M parameter and downsampling rate to map the full performance landscape.
3. **Interpretability Verification:** For a dataset with known ground truth patterns, quantitatively measure whether the identified Graphoids actually correspond to the true defining features, not just visually appealing patterns.