---
ver: rpa2
title: 'Systematic Bias in Large Language Models: Discrepant Response Patterns in
  Binary vs. Continuous Judgment Tasks'
arxiv_id: '2504.19445'
source_url: https://arxiv.org/abs/2504.19445
tags:
- llms
- binary
- responses
- response
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that large language models (LLMs) exhibit systematic
  negative biases when responding to binary judgment tasks, such as being more likely
  to oppose value statements or classify sentiments as negative compared to continuous
  response formats. Across experiments with value statements and sentiment analysis
  tasks, LLMs consistently showed a shift toward more negative judgments in binary
  formats, a bias confirmed through hierarchical Bayesian modeling.
---

# Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks

## Quick Facts
- arXiv ID: 2504.19445
- Source URL: https://arxiv.org/abs/2504.19445
- Reference count: 2
- Primary result: LLMs exhibit systematic negative biases in binary judgment tasks compared to continuous response formats.

## Executive Summary
This study reveals that large language models (LLMs) exhibit systematic negative biases when responding to binary judgment tasks, such as being more likely to oppose value statements or classify sentiments as negative compared to continuous response formats. Across experiments with value statements and sentiment analysis tasks, LLMs consistently showed a shift toward more negative judgments in binary formats, a bias confirmed through hierarchical Bayesian modeling. Control experiments ruled out superficial factors like label mapping, suggesting the bias stems from deeper structural issues in how LLMs process binary versus continuous inputs. These findings underscore the importance of response format in LLM-based decision-making and highlight the need for careful task design and potential calibration to mitigate unintended biases in applications like psychological text analysis.

## Method Summary
The study conducted two experiments: (1) value judgment on 210 statements using randomly sampled human profiles from the GSS agents bank, and (2) sentiment analysis on 213 news headlines. Models tested included Llama-3.3-70b-instruct, Qwen-2.5-72b-instruct, Deepseek-v3, GPT-4o-mini, and GPT-4o. Responses were collected in both binary formats (Yes/No, 1/0, K/L) and continuous formats (0-10 scale for values, 1-6 Likert for sentiment) at temperature=0. The analysis normalized continuous scores, computed proportion differences (ΔP), and applied hierarchical Bayesian regression (PyMC, 4 chains, 2500 samples post 2500 burn-in) to estimate bias parameters with 95% HDI.

## Key Results
- LLMs were more likely to deliver "negative" judgments in binary formats compared to continuous ones across both value statements and sentiment analysis tasks.
- Binary-continuous disagreement exceeded 15% proportion shift, with Support judgments dropping from 74.5% to 60.7% in binary format.
- Hierarchical Bayesian modeling confirmed systematic bias, with group-level θ_Yes parameter significantly favoring "No" responses (M = -1.320, 95% HDI: [-2.160, -0.465]).

## Why This Works (Mechanism)

### Mechanism 1
Binary response formats induce a negative decision bias in LLMs compared to continuous scales. Binary formats force a hard threshold decision at the midpoint, which LLMs systematically shift toward negative classifications. Continuous formats allow graded expression, reducing threshold-related distortion. Core assumption: The internal value representation v_i,j is consistent across formats, and observed differences stem from the output transformation process rather than underlying evaluation changes. Evidence: LLMs were more likely to deliver "negative" judgments in binary formats compared to continuous ones; the mean proportion of Support judgment decreased from 74.5% to 60.7% in binary format. Break condition: If binary-continuous alignment improves with calibration or if the effect reverses for certain domains, the threshold-shift explanation is insufficient.

### Mechanism 2
LLMs exhibit a token-level preference for "No" independent of semantic content. The model's token probability distribution favors "No" over "Yes" in binary classification, creating acquiescence-like bias but in the opposite direction to human patterns. Core assumption: Token frequency and positional encoding in training data create output-level biases separable from semantic understanding. Evidence: Significant preference bias for "No" (group-level θ_Yes, M = -1.320, 95% HDI: [-2.160, -0.465]); when positive option was changed to "No", results reversed with models selecting "No" in judgments (59.3%). Break condition: If neutral labels eliminate the bias, token preference is confirmed; if negativity persists, semantic threshold shift is primary.

### Mechanism 3
Post-training alignment processes may distort probability calibration, amplifying format-specific biases. RLHF or instruction tuning optimizes for helpfulness/safety in ways that shift confidence distributions, making binary threshold crossings systematically more negative. Core assumption: Alignment procedures do not preserve calibrated probability estimates across output formats. Evidence: Prior works demonstrated that post-training processes can significantly affect calibration of model's accuracy and confidence; LLMs influenced by superficial features of inputs and output probabilities rather than deep understanding. Break condition: If base models show similar or stronger negative bias, alignment is not the primary driver.

## Foundational Learning

- Concept: **Decision Threshold vs. Continuous Calibration**
  - Why needed here: Understanding that forcing binary choices introduces artificial thresholds that may not reflect underlying model confidence is essential for interpreting this bias.
  - Quick check question: If a model outputs 0.48 on a continuous scale, should a binary threshold at 0.5 be treated as confident opposition?

- Concept: **Token Probability Bias (Acquiescence/Negativity Bias)**
  - Why needed here: The paper shows LLMs prefer "No" tokens; this differs from human acquiescence bias and must be distinguished from semantic judgment.
  - Quick check question: When you reverse label mappings (Yes=Negative, No=Positive), does the model still prefer one token?

- Concept: **Hierarchical Bayesian Modeling for Bias Detection**
  - Why needed here: The paper uses this to separate question-level effects from model-level response biases.
  - Quick check question: Why use hierarchical models rather than simple proportion comparisons for detecting systematic bias?

## Architecture Onboarding

- Component map: Profile + statement/headline -> prompt construction -> Binary/Continuous response -> Normalization (0-1), thresholding at 0.5 -> Hierarchical Bayesian regression (PyMC) -> Bias parameter extraction (θ_bc, θ_Yes, θ_t)

- Critical path: 1) Prompt construction with consistent profiles across conditions, 2) Response collection at temperature=0 for determinism, 3) Normalization and thresholding for cross-format comparison, 4) Hierarchical model fitting (PyMC, 4 chains, 2500 samples + 2500 burn-in), 5) 95% HDI extraction for group-level bias parameters

- Design tradeoffs: Temperature=0 ensures reproducibility but may not capture population-level variance; neutral labels (K/L) reduce token bias but add cognitive load and may introduce confusion; aggregating across profiles increases power but masks individual-level heterogeneity

- Failure signatures: Binary-continuous disagreement >15% proportion shift indicates format-induced bias; HDI excluding zero for θ_bc or θ_Yes indicates statistically credible bias; reversal with label swapping indicates token preference, not semantic

- First 3 experiments: 1) Replicate with neutral labels only (K/L) to isolate semantic negativity from token preference, 2) Test base vs. aligned versions of same model family to trace bias origin (pre-training vs. alignment), 3) Add mid-point explicit option ("Neutral/Unsure") to test whether forced binary choice is the primary driver

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed negative bias in binary formats stem primarily from the pre-training data distribution or the post-training alignment process? Basis: The authors explicitly state in the Discussion: "To trace the source of these biases, future work could compare base and aligned models to determine if biases arise during pre-training or through alignment." Unresolved because the current study evaluated only final instruct versions without isolating specific training stage. Resolution: An ablation study comparing bias magnitude between base foundation models and their instruction-tuned counterparts using identical prompts.

### Open Question 2
Does the systematic negative bias persist when LLMs are prompted as objective classifiers rather than being instructed to simulate specific human profiles? Basis: The authors note a methodological limitation: "Our findings relied on prompts that explicitly asked LLMs to simulate human responses. The results may differ with other prompts or tasks." Unresolved because it's unclear if the bias is intrinsic to the binary format or triggered specifically by the persona-simulation context. Resolution: Replication of sentiment analysis task using standard, non-personified prompts to see if bias magnitude remains consistent.

### Open Question 3
Can targeted debiasing strategies, such as domain-specific fine-tuning or curated datasets, effectively mitigate the systematic negative bias in binary formats? Basis: The Discussion concludes that "These insights will be critical for developing debiasing strategies, including curated datasets and targeted fine-tuning, to improve the reliability of LLMs..." Unresolved because the paper identifies and analyzes the bias but does not test or validate any specific intervention or training protocol to correct it. Resolution: Training a model on a curated dataset balanced for binary affirmative/negative responses and re-evaluating its susceptibility to the bias.

## Limitations
- The deterministic temperature=0 setting may not capture the full variance of LLM responses and could underestimate uncertainty in bias estimates.
- The GSS profile sampling procedure lacks full specification, making exact replication challenging without author clarification.
- The study does not distinguish whether bias stems from semantic processing or superficial token-level preferences, though neutral labels partially address this.

## Confidence

**High Confidence**: The existence of a systematic negative bias in binary vs. continuous formats, as evidenced by consistent proportion shifts across multiple models, experimental conditions, and stimulus types. The hierarchical Bayesian modeling framework robustly supports this conclusion.

**Medium Confidence**: The token-level "No" preference mechanism, as the reversal with label swapping is compelling but could also reflect semantic threshold effects rather than pure token bias. The calibration hypothesis is plausible but not directly tested against base (pre-alignment) models.

**Low Confidence**: The claim that the bias is "deeper than label mapping" without ruling out all surface-level prompt variations or testing additional response formats (e.g., sliding scales, multi-point scales).

## Next Checks

1. Test base vs. aligned models: Run the same experiments on base (pre-alignment) and aligned versions of the same model families (e.g., Llama-3.1-70b vs. Llama-3.3-70b-instruct). If the bias is stronger in aligned models, post-training processes are likely contributors.

2. Vary threshold placement: For continuous outputs, systematically vary the decision threshold (e.g., 0.4, 0.5, 0.6) and measure binary agreement. If agreement peaks away from 0.5, the threshold placement itself is a key driver.

3. Add mid-point explicit option: Introduce a neutral/undecided option in binary tasks to test whether forced binary choice (vs. optional abstention) drives the negativity bias. If negativity drops with this option, format coercion is the primary mechanism.