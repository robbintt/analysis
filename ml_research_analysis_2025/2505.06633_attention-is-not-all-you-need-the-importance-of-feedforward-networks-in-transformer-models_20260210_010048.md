---
ver: rpa2
title: 'Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer
  Models'
arxiv_id: '2505.06633'
source_url: https://arxiv.org/abs/2505.06633
tags:
- transformer
- layers
- dmodel
- linear
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the role of the feedforward network (FFN) component
  in transformer blocks during pre-training. The study evaluates models with varying
  numbers of linear layers per transformer block (0, 1, 2, and 3 layers) on Booksum
  and Wikitext datasets.
---

# Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models

## Quick Facts
- arXiv ID: 2505.06633
- Source URL: https://arxiv.org/abs/2505.06633
- Reference count: 21
- Three-layer FFNs achieve lower training loss than standard two-layer configurations while using fewer parameters and training faster

## Executive Summary
This paper challenges the conventional wisdom about transformer architecture by demonstrating that feedforward networks (FFNs) play a more critical role than previously recognized. Through systematic experimentation with varying numbers of linear layers per transformer block (0, 1, 2, and 3), the study reveals that increasing FFN depth can improve performance while reducing overall model size. The findings suggest that FFNs contribute significantly to the model's learning capacity, contrary to the common perception that attention mechanisms dominate transformer effectiveness.

## Method Summary
The study evaluates transformer models with different FFN configurations on Booksum and Wikitext datasets, varying the number of linear layers per transformer block from zero to three. All models maintain consistent total parameter counts through adjusting the number of transformer blocks (10, 12, 16, and 24) to enable fair comparison. Training and evaluation focus on autoregressive language modeling tasks, with performance measured by training loss, parameter efficiency, and training speed.

## Key Results
- Three-layer FFN models with 10 transformer blocks achieved lower training loss than baseline two-layer FFN models with 24 blocks on Booksum dataset
- The three-layer configuration demonstrated approximately 13% faster training time while using fewer total parameters
- Results showed statistically significant improvements on Booksum and comparable performance on Wikitext

## Why This Works (Mechanism)
Unknown: The paper does not explicitly detail the underlying mechanisms that make deeper FFN configurations more effective. Based on the observed results, possible explanations could include enhanced feature extraction capabilities, better gradient flow through additional linear layers, or more efficient parameter utilization. However, these remain speculative without explicit analysis from the authors.

## Foundational Learning
- Transformer blocks: Basic units containing self-attention and FFN components that process sequential data
  - Why needed: Understanding the modular structure of transformers
  - Quick check: Can identify self-attention vs FFN subcomponents
- Feedforward networks: Multi-layer perceptrons applied position-wise across sequences
  - Why needed: Core component whose importance this paper investigates
  - Quick check: Can describe FFN architecture and function
- Autoregressive language modeling: Predicting next token given previous context
  - Why needed: Primary evaluation task in this study
  - Quick check: Understands left-to-right generation process
- Parameter efficiency: Achieving better performance with fewer total parameters
  - Why needed: Key finding about three-layer FFNs
  - Quick check: Can compare parameter counts across model variants

## Architecture Onboarding

### Component Map
Input Embedding -> Transformer Blocks (Self-Attention -> FFN) -> Output Projection

### Critical Path
Input sequence → Embedding layer → Multiple transformer blocks (each with self-attention and FFN) → Final output projection

### Design Tradeoffs
- FFN depth vs transformer block count: Deeper FFNs allow fewer total blocks
- Parameter allocation: Shifting parameters from attention to FFN layers
- Training efficiency: Three-layer FFNs train faster despite additional computation per block

### Failure Signatures
- Underfitting with zero FFN layers (attention-only models)
- Diminishing returns beyond three FFN layers
- Potential overfitting with excessive FFN depth

### First Experiments
1. Compare training loss curves for 0, 1, 2, and 3 FFN layer configurations
2. Measure parameter efficiency by comparing performance per parameter across variants
3. Profile training speed differences between configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Limited exploration of FFN architecture variations beyond layer count
- Evaluation restricted to autoregressive language modeling tasks
- Statistical significance testing only performed on Booksum dataset

## Confidence
- Three-layer FFNs achieving better performance with fewer parameters and faster training: High confidence for specific experimental conditions
- Generalization of findings across different model scales and tasks: Medium confidence
- Three-layer FFNs as universal improvement: Low confidence

## Next Checks
1. Test three-layer FFN configuration across diverse transformer architectures including encoder-only models like BERT and decoder-encoder models like T5
2. Conduct ablation studies varying activation functions, layer normalization placement, and attention mechanisms in combination with different FFN layer counts
3. Evaluate performance on non-autoregressive tasks such as masked language modeling, machine translation, and document classification