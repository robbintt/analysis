---
ver: rpa2
title: 'Landing with the Score: Riemannian Optimization through Denoising'
arxiv_id: '2509.23357'
source_url: https://arxiv.org/abs/2509.23357
tags:
- manifold
- optimization
- data
- gradient
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a data-driven framework for Riemannian optimization\
  \ when the manifold is only implicitly available through samples. The core innovation\
  \ is the \"link function\" \u2113\u03C3, which connects the data distribution to\
  \ manifold operations; its gradient and Hessian recover projections to the manifold\
  \ and tangent space as \u03C3\u21920."
---

# Landing with the Score: Riemannian Optimization through Denoising

## Quick Facts
- arXiv ID: 2509.23357
- Source URL: https://arxiv.org/abs/2509.23357
- Reference count: 40
- Primary result: Introduces a data-driven framework for Riemannian optimization when the manifold is only implicitly available through samples, using denoising score networks to recover geometric operations.

## Executive Summary
This paper addresses the challenge of optimizing functions over manifolds when only data samples from the manifold are available, not its explicit parameterization. The authors introduce a framework that uses pre-trained diffusion model score functions to approximate geometric operations like projection onto the manifold and tangent space. By defining a "link function" whose gradient and Hessian recover these operations as smoothing parameter σ→0, they enable Riemannian optimization without explicit manifold knowledge. The approach is validated through experiments on orthogonal groups and data-driven control tasks, demonstrating effectiveness in both synthetic and practical settings.

## Method Summary
The method trains a score network s_θ(x,σ) via denoising score matching to approximate the score function of data samples from the manifold. During optimization, this network enables approximate manifold operations: the projection operator π_σ(x) ≈ s(x) = x + σ²s_θ(x,σ) and tangent space projection P_σ(x) ≈ I + σ²∇s_θ(x,σ). Two algorithms are proposed: Denoising Landing Flow (DLF) which uses a regularized objective to allow iterates to "land" on the manifold, and Denoising Riemannian Gradient Descent (DRGD) which directly applies approximate Riemannian gradient updates. The approach is evaluated on orthogonal group optimization and trajectory tracking control problems.

## Key Results
- Successfully optimized over orthogonal groups using only data samples, achieving lower objective values than available training samples
- Demonstrated effective trajectory tracking for pendulum and unicycle systems through data-driven control
- Provided non-asymptotic convergence guarantees for both approximate optimality and feasibility
- Showed that the method works for non-compact manifolds in control applications despite theoretical assumptions requiring compactness

## Why This Works (Mechanism)

### Mechanism 1: Geometric Recovery via the Denoising Link Function
The gradient and Hessian of the link function ℓ_σ(x) = ½‖x‖² - σ² log p_σ(x) recover the closest-point projection π(x) and tangent space projection P_{T_xM} as σ→0. This works because the Tweedie score (conditional expectation of clean data given noisy observation) converges to the manifold projection. The convergence is formalized in Theorem 1 with bounds ‖E_ν_{x,σ} - π(x)‖ ≤ Kσ|log(σ)|³, and Corollary 2 confirms the Hessian convergence to tangent space projection.

### Mechanism 2: Manifold Constraint via Regularized "Landing"
The Denoising Landing Flow minimizes F^η_σ(x) = f(π_σ(x)) + ηd_σ(x), where the landing term ηd_σ(x) acts as a penalty for distance from the manifold. This allows optimization in the ambient space while correcting deviations through the distance function gradient. Theorem 3 guarantees accumulation points satisfy approximate feasibility (dist_M(x*) ≤ τ₀) and optimality, provided iterates remain within the tubular neighborhood of radius τ < τ_M.

### Mechanism 3: Approximate Retraction using the Score Network
The score network serves as an approximate retraction operator for DRGD. Standard Riemannian optimization requires mapping tangent space updates back to the manifold, which is approximated by applying the score-based projection immediately after the gradient step: x_{k+1} = s(x_k - γ_k · s'(x_k)∇f(x_k)). Theorem 5 bounds the average Riemannian gradient norm, converging as the approximation error ε' → 0.

## Foundational Learning

- **Concept**: Tweedie's Formula and Score Functions
  - **Why needed here**: This is the mathematical bridge connecting Gaussian noise diffusion to manifold geometry, explaining why denoising provides projection operations
  - **Quick check question**: If σ increases, does the Tweedie estimate move closer to the data mean or the specific manifold point?

- **Concept**: Riemannian Retraction and Projection
  - **Why needed here**: The paper aims to replicate classical Riemannian optimization tools using neural networks, requiring understanding of geometric operations
  - **Quick check question**: Why can't we just use standard gradient descent in the ambient space instead of projecting onto the tangent space?

- **Concept**: The Reach (τ_M) of a Manifold
  - **Why needed here**: Theoretical guarantees rely on the tubular neighborhood concept; if optimization leaves this neighborhood, the closest-point projection becomes undefined
  - **Quick check question**: How does the curvature of the manifold affect the size of the safe optimization region?

## Architecture Onboarding

- **Component map**: Pre-trained score network s_θ(x,σ) → Link Function Layer (computes s(x) and s'(x)) → Optimizer Core (DLF or DRGD) → Objective Oracle f(x)
- **Critical path**: The accuracy of the Jacobian s'(x) is critical for the Riemannian gradient step, computed via backpropagation through the score network's input
- **Design tradeoffs**: DLF vs. DRGD (DLF more robust to infeasibility but requires tuning η; DRGD direct analog but needs higher score accuracy), smoothing σ (small σ better geometric recovery but requires accurate network)
- **Failure signatures**: Escaping the tube (divergence, non-physical results), oscillation (objective bounces without decreasing)
- **First 3 experiments**: 1) Implement DRGD on O(n) manifold using score trained on rotation matrices, verify orthogonality and objective minimization; 2) Run DLF on synthetic dataset with varying noise levels, confirm Riemannian gradient norm decreases as σ→0; 3) Apply to unicycle trajectory tracking task, measure current cost vs true cost trade-off

## Open Questions the Paper Calls Out

### Open Question 1
Can convergence guarantees for DLF and DRGD be derived under weaker L²-error assumptions typical of practical score network training rather than strict L^∞-approximation? Remark 6 explicitly states this analysis under L²-bounds is left for future work, though practical diffusion models use L² loss.

### Open Question 2
Can the approximate manifold operations be effectively utilized in second-order Riemannian optimization algorithms like Newton or trust region methods? Section 7 lists studying these sophisticated algorithms as future work to accelerate convergence, but the paper doesn't analyze curvature information needed for second-order steps.

### Open Question 3
Does the theoretical framework extend to non-compact manifolds? Theorem 1 and 5 assume compactness, yet Section 6.2 applies the method to non-compact system behavior manifolds. The proofs utilize uniform constants guaranteed by compactness, creating uncertainty about bounds for non-compact spaces.

## Limitations
- Score network accuracy may degrade near regions of high manifold curvature, potentially invalidating convergence guarantees
- Computational overhead from Jacobian computation could be prohibitive for high-dimensional data like trajectories
- The paper doesn't provide clear guidance on selecting the inference-time smoothing parameter σ

## Confidence
- **High Confidence**: Theoretical framework connecting score functions to manifold geometry is well-established with rigorous proofs; orthogonality constraints in O(n) experiments are verifiable
- **Medium Confidence**: Convergence guarantees assume iterates remain in tubular neighborhood, but practical conditions for this are not fully characterized
- **Low Confidence**: Practical effectiveness on high-dimensional trajectory manifolds is less certain, with potential numerical instability from computing high-dimensional Jacobians

## Next Checks
1. **Jacobian sensitivity analysis**: Systematically vary network architecture and measure how Jacobian approximation error propagates through optimization; compare Riemannian gradient norm reduction between exact and approximate projections on synthetic manifolds
2. **Robustness to curvature**: Generate manifolds with varying curvature (spheres vs tori) and evaluate whether convergence rate degrades predictably with curvature to validate reach-dependent guarantees
3. **Scalability benchmark**: Measure wall-clock time per iteration and total optimization time for trajectory tracking tasks as function of trajectory length; compare against alternative methods using explicit manifold parametrizations