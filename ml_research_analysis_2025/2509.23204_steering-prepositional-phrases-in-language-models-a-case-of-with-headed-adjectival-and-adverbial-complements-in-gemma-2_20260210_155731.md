---
ver: rpa2
title: 'Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival
  and Adverbial Complements in Gemma-2'
arxiv_id: '2509.23204'
source_url: https://arxiv.org/abs/2509.23204
tags:
- uni00000014
- language
- uni00000013
- linguistics
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models decide whether a prepositional
  phrase complement modifies a verb (instrumental) or a noun (attributive). The authors
  design a controlled prompt set centered on with-headed prepositional phrases, forcing
  Gemma-2 to choose between equally plausible instrumental and attributive continuations.
---

# Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2

## Quick Facts
- **arXiv ID**: 2509.23204
- **Source URL**: https://arxiv.org/abs/2509.23204
- **Reference count**: 18
- **Primary result**: A single attention head in Gemma-2 controls the instrumental reading of with-headed phrases; scaling it down balances instrument/attribute bias from 75% to near 50/50.

## Executive Summary
This paper investigates how language models resolve the syntactic ambiguity of with-headed prepositional phrases, which can modify either a verb (instrumental reading) or a noun (attributive reading). The authors design a controlled prompt set forcing Gemma-2 to choose between these equally plausible interpretations. Using logit attribution, they identify a specific attention head (L0H2) that strongly biases toward the instrumental reading. By scaling down the value vector of this head, they demonstrate targeted control over modifier attachment, shifting the model's preference from 3:4 toward near balance (33% instruments, 36% attributes).

## Method Summary
The authors constructed a controlled prompt set where with-headed prepositional phrases appeared in contexts that made both instrumental and attributive readings equally plausible. They used Gemma-2 to generate continuations for these prompts, then applied logit attribution to identify which attention heads most strongly favored the instrumental interpretation. After identifying head L0H2 as the key driver, they implemented a value vector scaling intervention, reducing its contribution by a factor of 0.7. This intervention was tested across the full prompt set, measuring changes in the distribution of instrument versus attribute continuations.

## Key Results
- Head L0H2 in layer 0 consistently exhibits a strong instrumental preference across the test set
- Value vector scaling of L0H2 shifts model output from 75% instrument to near 50/50 balance
- The intervention demonstrates targeted control over syntactic ambiguity resolution in LLMs

## Why This Works (Mechanism)
The instrumental bias in with-headed phrases stems from a single attention head (L0H2) that disproportionately weights context features favoring verb modification. This head's value vector encodes a preference for instrumental readings that propagates through the model's decision-making. By scaling down this vector, the authors effectively reduce the head's contribution to the final logits, allowing other contextual cues to influence the interpretation more equally.

## Foundational Learning

**Logit Attribution** - A technique for identifying which model components contribute most to specific output predictions by analyzing the gradient of logits with respect to model parameters. *Why needed*: To pinpoint the exact attention head responsible for the instrumental bias. *Quick check*: Verify attribution scores correlate with intervention effects.

**Value Vector Scaling** - An intervention method that multiplies the value vectors of attention heads by a scaling factor, effectively reducing or amplifying their contribution. *Why needed*: To test whether reducing L0H2's influence would balance the instrumental/attributive preference. *Quick check*: Confirm scaling factor magnitude matches observed logit changes.

**Controlled Prompt Construction** - Creating test cases where syntactic ambiguity exists but semantic plausibility is balanced across interpretations. *Why needed*: To ensure observed model behavior reflects syntactic preference rather than semantic bias. *Quick check*: Validate prompts have equal plausibility for both readings.

## Architecture Onboarding

**Component Map**: Input tokens → Embedding layer → Transformer blocks (12 layers) → L0H2 attention head → Value computation → Output logits → Softmax → Token distribution

**Critical Path**: Token embedding → Multi-head attention (including L0H2) → Feed-forward network → Layer normalization → Output projection → Logit computation

**Design Tradeoffs**: The model prioritizes computational efficiency through single-head interventions versus comprehensive architectural changes, accepting limited scope for targeted control.

**Failure Signatures**: Unintended side effects on unrelated syntactic constructions, inconsistent steering across vocabulary variations, or complete loss of syntactic coherence when scaling values too aggressively.

**First Experiments**:
1. Test L0H2 scaling on out-of-distribution prompts to verify robustness
2. Apply the same methodology to other ambiguous modifier types (participial phrases)
3. Conduct ablation studies with multiple head perturbations to identify compensatory mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are narrowly scoped to a single syntactic phenomenon and Gemma-2 model
- Logit attribution assumes linearity in head contributions, potentially missing nonlinear interactions
- Alternative interventions targeting attention patterns or feed-forward layers might yield different effects

## Confidence

**High confidence**: Empirical observation that L0H2 exhibits consistent instrumental preference and logit perturbation produces measurable changes.

**Medium confidence**: Causal attribution of bias to L0H2's value vector given methodological assumptions about head independence.

**Low confidence**: Generalizability to other syntactic ambiguities, larger models, or real-world text generation contexts.

## Next Checks
1. Replicate the steering experiment on an independent test set with varied sentence structures and vocabulary to verify robustness of the L0H2 instrumental bias across linguistic contexts.

2. Apply the same logit attribution and perturbation methodology to other ambiguous modifier attachment cases (e.g., participial phrases, relative clauses) to test whether the approach generalizes to different syntactic phenomena.

3. Conduct ablation studies by simultaneously perturbing multiple heads identified as instrument-favoring to determine whether the effect compounds or if compensatory mechanisms exist within the model architecture.