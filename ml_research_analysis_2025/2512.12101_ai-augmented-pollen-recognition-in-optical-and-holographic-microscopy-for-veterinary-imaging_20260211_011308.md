---
ver: rpa2
title: AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary
  Imaging
arxiv_id: '2512.12101'
source_url: https://arxiv.org/abs/2512.12101
tags:
- images
- synthetic
- real-world
- holographic
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops AI-driven pollen recognition for veterinary
  imaging using both optical and holographic microscopy. Object detection with YOLOv8s
  and classification with MobileNetV3L are trained on a dual-modality dataset, achieving
  91.3% mAP50 and 97% accuracy on optical images but only 8.15% mAP50 and 50% accuracy
  on holographic images.
---

# AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging

## Quick Facts
- arXiv ID: 2512.12101
- Source URL: https://arxiv.org/abs/2512.12101
- Reference count: 34
- Primary result: WGAN-SN synthetic data augmentation improves holographic pollen detection mAP50 from 8.15% to 15.4%

## Executive Summary
This study addresses the challenge of automated pollen recognition across two microscopy modalities for veterinary imaging applications. The authors develop an AI pipeline combining YOLOv8s object detection and MobileNetV3L classification for pollen grains in both optical brightfield and digital in-line holographic microscopy (DIHM) images. While achieving high performance on optical images (91.3% mAP50, 97% accuracy), the approach struggles with holographic images due to domain shift. To bridge this gap, the authors employ WGAN-SN to generate synthetic holographic pollen images, which when used for data augmentation improves detection performance from 8.15% to 15.4% mAP50 at a 1:1.5 real-to-synthetic ratio.

## Method Summary
The approach consists of three main components: (1) YOLOv8s for object detection trained on 3054 real-world 640×640 patches, (2) MobileNetV3L for classification, and (3) WGAN-SN for synthetic data generation. The pipeline processes dual-modality microscopy images where optical images serve as the primary training source. To address the domain gap with holographic images, the authors train a WGAN-SN on cleaned pollen grain images, generating synthetic samples that are composited with background patches via alpha blending. The model is trained with two-phase learning rate scheduling and critic-based filtering to ensure synthetic image quality. The augmented dataset (1:1.5 real:composite ratio) is then used to retrain the YOLOv8s detector on holographic images.

## Key Results
- Optical microscopy: 91.3% mAP50 and 97% classification accuracy
- Holographic microscopy baseline: 8.15% mAP50 and 50% accuracy
- Synthetic augmentation: Improves holographic mAP50 to 15.4% at 1:1.5 ratio
- WGAN-SN synthetic quality: FID score of 58.246
- Model efficiency: Detection inference time of 40ms on edge devices

## Why This Works (Mechanism)
The approach leverages domain adaptation through synthetic data generation to bridge the visual gap between optical and holographic microscopy. By training WGAN-SN on real pollen grain images and generating synthetic samples that preserve pollen characteristics while adapting to holographic appearance, the model learns to recognize pollen features despite modality differences. The two-phase training with learning rate scheduling allows the generator to first establish basic image quality before refining details, while critic-based filtering ensures only high-quality synthetic images enter the training pipeline.

## Foundational Learning
- **Domain adaptation**: Transferring knowledge from one domain (optical) to another (holographic) - needed because direct training on limited holographic data yields poor performance; check by comparing baseline vs augmented mAP50.
- **Synthetic data augmentation**: Using GAN-generated images to expand training data - needed to address data scarcity in holographic modality; check by measuring FID score improvement.
- **Two-phase GAN training**: Progressive learning with learning rate scheduling - needed for stable GAN convergence; check by monitoring FID plateau during training.
- **Critic-based filtering**: Quality control for generated samples - needed to prevent low-quality synthetic images from degrading model performance; check by verifying top-half selection criteria.
- **Affine alignment for label transfer**: Spatial registration between modalities - needed to leverage optical annotations for holographic training; check by validating alignment accuracy on test pairs.
- **Alpha blending for composite generation**: Combining synthetic pollen with background patches - needed to create realistic training samples; check by visual inspection of composites.

## Architecture Onboarding
- **Component map**: WGAN-SN -> Synthetic Images -> YOLOv8s Training -> Detection Results
- **Critical path**: WGAN-SN training → synthetic image generation → composite creation → YOLOv8s retraining → detection evaluation
- **Design tradeoffs**: WGAN-SN chosen for computational efficiency vs. higher-fidelity but costlier diffusion models; 50% bounding box expansion balances label accuracy vs. coverage
- **Failure signatures**: High FID (>100) indicates poor synthetic quality; detection mAP50 below baseline suggests synthetic artifacts; low classification accuracy points to feature misalignment
- **First experiments**: 1) Train WGAN-SN and validate FID improvement from phase 1 to phase 2, 2) Generate synthetic samples and verify critic filtering effectiveness, 3) Test different real:synthetic ratios to find optimal augmentation strategy

## Open Questions the Paper Calls Out
The authors explicitly recommend exploring synthetic image generation models offering greater visual fidelity than WGAN-SN for future work, acknowledging that higher-fidelity models could potentially achieve better performance improvements in bridging the modality gap.

## Limitations
- Synthetic augmentation only partially bridges the domain gap, improving mAP50 from 8.15% to 15.4% but not achieving parity with optical results
- Affine alignment procedure for label transfer is unspecified, creating reproducibility challenges
- Modest performance improvement suggests synthetic data alone may be insufficient for full domain adaptation
- The approach requires substantial computational resources for GAN training and inference

## Confidence
- Optical microscopy results: High (validated metrics, established methodology)
- Holographic microscopy results: Medium (performance gap remains significant, complex domain adaptation)
- Synthetic data approach: Medium-high (tangible improvement but limited magnitude)
- Method reproducibility: Low-Medium (key architectural details unspecified)

## Next Checks
1. Validate affine alignment accuracy by testing label transfer on manually verified image pairs to quantify registration error
2. Conduct ablation studies varying the real:synthetic ratio beyond 1:1.5 to determine optimal augmentation strategy
3. Evaluate alternative domain adaptation techniques (e.g., adversarial domain adaptation, self-training) to compare against WGAN-SN augmentation approach