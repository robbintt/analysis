---
ver: rpa2
title: 'Myna: Masking-Based Contrastive Learning of Musical Representations'
arxiv_id: '2502.12511'
source_url: https://arxiv.org/abs/2502.12511
tags:
- learning
- contrastive
- representations
- music
- myna
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Myna introduces a contrastive learning framework for musical representation
  learning that replaces traditional data augmentations with token masking. The method
  applies a Vision Transformer to mel-spectrograms, randomly masking 90% of spectrogram
  tokens to create positive pairs for contrastive learning.
---

# Myna: Masking-Based Contrastive Learning of Musical Representations

## Quick Facts
- arXiv ID: 2502.12511
- Source URL: https://arxiv.org/abs/2502.12511
- Authors: Ori Yonay; Tracy Hammond; Tianbao Yang
- Reference count: 20
- Primary result: Achieves 85x larger batch sizes through token masking while maintaining or improving pitch sensitivity for key detection

## Executive Summary
Myna introduces a novel contrastive learning framework for musical representation learning that replaces traditional data augmentations with token masking. By randomly masking 90% of spectrogram tokens and applying a Vision Transformer to mel-spectrograms, Myna enables significant efficiency gains while preserving critical musical features. The method achieves state-of-the-art results on publicly available datasets with a hybrid model processing both 16x16 and 128x2 patches, outperforming larger models trained on multiple GPUs.

## Method Summary
Myna's approach centers on token masking as a data augmentation strategy for contrastive learning in music representation. The method applies random masking to 90% of spectrogram tokens, creating positive pairs for contrastive learning while dramatically increasing batch size capacity (up to 4096 per GPU, an 85x improvement over prior methods). A hybrid architecture processes both 16x16 and 128x2 patches to capture different musical features, with the smaller patches preserving pitch sensitivity crucial for tasks like key detection. The approach reduces hyperparameter complexity from 21 to 1, simplifying the training process while maintaining strong downstream performance across multiple musical tasks.

## Key Results
- Achieves 85x larger batch sizes (4096 per GPU) compared to prior methods through token masking
- Myna-Hybrid (22M parameters) trained on a single GPU outperforms MULE (62M) on average and rivals MERT-95M (trained on 16-64 GPUs)
- Reduces hyperparameter complexity from 21 to 1 while maintaining state-of-the-art performance on publicly available datasets

## Why This Works (Mechanism)
Token masking works effectively for musical representation learning because it preserves the relative relationships between masked and unmasked tokens, capturing musically relevant features while avoiding destructive information loss. Unlike traditional augmentations that may disrupt pitch relationships critical for musical understanding, masking maintains the structural integrity of the spectrogram while forcing the model to learn robust representations. The Vision Transformer architecture is particularly well-suited to this approach as it can effectively reason about relationships between tokens, making it ideal for capturing the complex interdependencies in musical signals.

## Foundational Learning
- **Contrastive learning**: Learning representations by pulling together similar examples (positives) and pushing apart dissimilar ones (negatives); needed to learn meaningful musical features without labels; quick check: ensures positive pairs are musically similar while negatives are distinct
- **Vision Transformers**: Self-attention based architecture originally for images; needed to capture long-range dependencies in spectrograms; quick check: attention maps should highlight musically relevant regions
- **Mel-spectrograms**: Time-frequency representation of audio; needed as input format that preserves both temporal and pitch information; quick check: spectrogram resolution should capture relevant musical frequencies
- **Token masking**: Randomly removing input tokens as augmentation; needed to create positive pairs without destructive transformations; quick check: 90% masking rate balances information preservation with augmentation strength
- **Patch embedding**: Dividing input into patches for transformer processing; needed to handle variable-sized musical inputs efficiently; quick check: patch sizes should capture relevant musical features at different scales

## Architecture Onboarding

**Component map:**
Input mel-spectrogram -> Patch embedding (16x16 and 128x2) -> Token masking (90%) -> Vision Transformer -> Projection head -> Contrastive loss

**Critical path:**
The critical path involves the hybrid patch embedding stage that processes both 16x16 patches for general features and 128x2 patches for pitch-sensitive information, followed by the token masking augmentation that creates the positive pairs for contrastive learning.

**Design tradeoffs:**
The key tradeoff is between masking ratio and information preservation - 90% masking maximizes efficiency gains but risks losing critical musical information. The hybrid architecture balances this by using smaller patches (128x2) specifically for pitch-sensitive tasks while larger patches (16x16) capture broader musical features. This design prioritizes efficiency and task-specific performance over model simplicity.

**Failure signatures:**
- Poor key detection performance indicates insufficient pitch sensitivity from masking
- Degraded performance on rhythm-based tasks suggests masking disrupts temporal patterns
- Inability to scale batch sizes beyond baseline indicates implementation issues with the masking strategy
- Performance degradation on longer musical sequences suggests patch size limitations

**First experiments:**
1. Test key detection performance with varying masking ratios (70%, 80%, 90%, 95%) to find optimal balance
2. Compare hybrid vs single-patch models on pitch-sensitive tasks to validate architectural benefits
3. Measure batch size scaling limits with different GPU memory configurations to verify 85x improvement claim

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on pitch-related tasks, leaving effectiveness for rhythm-based, timbral, or structural understanding uncertain
- Exclusive use of mel-spectrograms limits applicability to alternative music representations like MIDI or raw audio
- Comparison with models trained on proprietary data remains indirect, making definitive claims about state-of-the-art performance across all settings uncertain

## Confidence

**Major Claim Confidence:**
- **High**: Token masking enables 85x batch size increases and maintains pitch sensitivity for key detection
- **Medium**: Myna-Hybrid achieves SOTA results on public datasets compared to other publicly-trained models
- **Medium**: Reduction from 21 to 1 hyperparameters meaningfully simplifies training while preserving performance
- **Low**: General superiority over all existing methods including those trained on proprietary datasets

## Next Checks
1. Evaluate Myna on rhythm-centric tasks (downbeat tracking, tempo estimation) to assess representation quality beyond pitch
2. Test the token masking approach with alternative music representations (MIDI, raw audio) to verify cross-modal effectiveness
3. Conduct ablation studies varying the 90% masking ratio to determine optimal trade-offs between efficiency and task performance