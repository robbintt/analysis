---
ver: rpa2
title: 'BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small
  Language Models'
arxiv_id: '2510.19419'
source_url: https://arxiv.org/abs/2510.19419
tags:
- learner
- language
- bliss
- error
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BLiSS 1.0 is a benchmark for evaluating whether language models
  can distinguish between naturalistic learner errors and artificial ones, addressing
  a gap in assessing cognitively inspired models. It uses controlled triplets from
  over 2.8 million learner sentences, testing selective tolerance by requiring models
  to prefer a human error over an artificial one of the same type.
---

# BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small Language Models

## Quick Facts
- arXiv ID: 2510.19419
- Source URL: https://arxiv.org/abs/2510.19419
- Reference count: 14
- Primary result: Benchmark testing models' ability to distinguish naturalistic learner errors from artificial ones shows performance clustering by training paradigm

## Executive Summary
BLiSS 1.0 addresses a critical gap in evaluating whether language models can distinguish between naturalistic learner errors and artificial ones, providing a benchmark for cognitively inspired models. The benchmark uses controlled triplets from over 2.8 million learner sentences, testing selective tolerance by requiring models to prefer human errors over artificial ones of the same type. Evaluation on diverse models shows selective tolerance is distinct from standard grammaticality, with performance clustering by training paradigm, validating BLiSS as a robust tool for measuring alignment with human language acquisition patterns.

## Method Summary
BLiSS constructs controlled triplets from learner corpora (EFCAMDAT, W&I, FCE) using ERRANT annotation to identify single-edit grammatical errors. Each triplet contains a corrected sentence, the original learner sentence with a naturalistic error, and an artificial error of the same type at a different position. The benchmark evaluates models using token-normalized surprisal (BPT) to measure preference, computing Learner Preference (LP), Human-Artificial Preference (HAP), and Strict Ordering (SO) metrics. The final dataset contains 136,867 triplets covering 48 error types across multiple proficiency levels and L1 backgrounds.

## Key Results
- Selective tolerance performance clusters strongly by training paradigm rather than model size
- Bilingual LLMs and B-GPT models show high SO (~55-57%), while SLABERT shows high LP (>50%) with poor SO
- Models with meaningful L1 representations show statistically significant performance improvements on L1-specific data slices
- BLiSS measures a capability distinct from grammaticality (BLiMP), validating its role as a research catalyst

## Why This Works (Mechanism)

### Mechanism 1: Selective Tolerance Through Position-Based Error Discrimination
- Claim: Models that align with L2 acquisition patterns assign lower surprisal to naturalistic learner errors than to artificial errors of the same type at different positions.
- Mechanism: The benchmark quantifies preference using token-normalized surprisal (BPT). A model with selective tolerance should rank: corrected < learner < artificial, indicating it recognizes grammatical correctness while discriminating between plausible and implausible error locations.
- Core assumption: Systematic learner errors are tied to both error type AND specific locus within a sentence; moving an attested error to a plausible alternative location reduces naturalness.

### Mechanism 2: Training Paradigm Determinism
- Claim: Performance on selective tolerance clusters by training paradigm rather than model size alone.
- Mechanism: Different training objectives (native text vs. learner data vs. sequential L1→L2 exposure) produce distinct error sensitivity profiles. Bilingual LLMs and B-GPT models show high SO (~55-57%), while SLABERT shows high LP (>50%) with poor SO, suggesting indiscriminate acceptance of learner forms.
- Core assumption: Selective tolerance is a learned capability reflecting training data composition and exposure order, not just model capacity.

### Mechanism 3: L1-Transfer Signal Detection
- Claim: Models with meaningful L1 representations show statistically significant performance improvements on L1-specific data slices.
- Mechanism: The benchmark's metadata (learner L1, proficiency level) enables stratified evaluation. Bilingual BabyLMs and Bilingual LLMs show significant L1-specific gains (e.g., CroissantLLM: 67.51% overall → 81.76% French L1), suggesting internalized transfer patterns.
- Core assumption: L1-dependent error patterns are systematic enough to be detected at the dataset scale; sparse L1-error combinations may limit detection.

## Foundational Learning

- **Surprisal as Plausibility Proxy**
  - Why needed here: The entire evaluation framework depends on interpreting negative log-likelihood as processing effort/plausibility.
  - Quick check question: Can you explain why lower BPT indicates higher plausibility under a model's distribution?

- **Interlanguage Systematicity**
  - Why needed here: The benchmark assumes learner errors are structured evidence of developing grammar, not random noise.
  - Quick check question: Why would a cognitively-plausible model tolerate systematic errors while penalizing random ones?

- **Minimal Pair Evaluation Paradigm**
  - Why needed here: BLiSS extends minimal-pair methodology (cf. BLiMP) to triplet comparisons with controlled error types.
  - Quick check question: What advantage does a triplet (corrected/learner/artificial) provide over a simple grammatical/ungrammatical pair?

## Architecture Onboarding

- **Component map**: Triplet Construction Pipeline -> Scoring Layer -> Metrics Layer -> Stratification Layer
- **Critical path**: 1) Source corpora → sentence-correction pairs 2) ERRANT annotation → error classification 3) Single-edit filtering + atomization 4) Rule-based artificial error generation 5) Validation filters → 136,867 triplets 6) Model inference → BPT scores 7) Metric computation → HAP, HAP-τ, SO, LP
- **Design tradeoffs**: 
  - Strictness vs. Coverage: 4.8% validation success rate ensures quality but limits rare error types and L1 combinations
  - Position Sensitivity vs. Error Type Purity: Artificial errors must differ in position but preserve type; constrains what errors can be generated
  - Grammaticality vs. Learner Alignment: High LP may indicate poor grammar OR intentional learner simulation; HAP disentangles these
- **Failure signatures**: 
  - High LP + Low SO: Model indiscriminately accepts errors (SLABERT pattern) → may have overfit to learner data without acquiring grammatical competence
  - Low HAP + High BLiMP: Model has grammatical knowledge but no learner error sensitivity → standard training without L2 exposure
  - No L1-specific gain: Model lacks meaningful L1 representation or transfer patterns not captured by benchmark
- **First 3 experiments**:
  1. Baseline Check: Run Bilingual LLM (e.g., CroissantLLM) on BLiSS subset to verify HAP > 60% and SO > 50% as sanity check for pipeline correctness.
  2. Ablation by Error Type: Filter to top-5 error types (M:DET, R:NOUN:NUM, R:PREP, U:DET, R:VERB:TENSE) and compare model performance; expect variation revealing error-type-specific sensitivity.
  3. L1 Stratification Test: For a bilingual model (e.g., BBLM-ZH), compare overall HAP vs. Chinese L1 slice; statistically significant improvement (cf. Table 5 pattern) validates L1-transfer detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the selective tolerance paradigm be effectively extended to capture discourse-level phenomena or pragmatic competence, which are currently excluded by the sentence-level triplet design?
- Basis in paper: The authors state in the Limitations section that the benchmark focuses on grammatical and lexical errors but "does not assess discourse-level phenomena, pragmatic competence, or other dimensions of L2 proficiency that extend beyond sentence boundaries."
- Why unresolved: The current triplet construction relies on single-sentence edits (ERRANT annotations), making it structurally incapable of evaluating cross-sentence dependencies or communicative intent without a new data collection and evaluation methodology.
- What evidence would resolve it: A new version of the benchmark (e.g., BLiSS 2.0) containing paragraph-level triplets where the "artificial" error violates discourse coherence (e.g., pronoun antecedent errors) rather than just local grammatical rules, along with corresponding model evaluations.

### Open Question 2
- Question: How can training objectives be modified for "Learner-Trained" models (like SLABERT) to induce selective tolerance without causing them to become "indiscriminately accepting" of errors?
- Basis in paper: The results (Section 6) show that models explicitly trained on learner data (SLABERT, Learner-Trained) exhibit high Learner Preference (LP) but fail to distinguish between human and artificial errors (low HAP), suggesting a methodological flaw in how they internalize learner patterns.
- Why unresolved: The paper identifies the failure mode ("indiscriminately accepting") but does not propose or test specific loss functions or data curations that would teach a model the *systematicity* of errors rather than just their surface probability.
- What evidence would resolve it: An ablation study testing different training objectives (e.g., contrastive loss between natural and artificial errors) on the SLABERT architecture that results in significantly higher HAP scores without reducing LP.

### Open Question 3
- Question: Does high performance on the BLiSS benchmark (selective tolerance) correlate with improved performance on downstream educational applications such as Grammatical Error Correction (GEC)?
- Basis in paper: The paper states the goal is to bridge the gap for "cognitively inspired models" and hopes the benchmark serves as a "research catalyst," implying the need to establish the utility of this alignment for practical NLP tasks.
- Why unresolved: While the paper establishes that BLiSS measures a capability distinct from grammaticality (BLiMP), it does not validate whether this specific capability translates to better performance on tasks like GEC or tutoring systems.
- What evidence would resolve it: A correlation analysis showing that models fine-tuned to maximize BLiSS HAP scores achieve higher F-sams on standard GEC benchmarks (like BEA-2019) compared to models optimized solely for grammaticality.

### Open Question 4
- Question: Does the observed sensitivity to L1-specific transfer effects persist robustly for typologically distinct or low-resource L1 backgrounds that are under-represented in the source corpora?
- Basis in paper: The Limitations section notes that "specific L1 backgrounds... become even more sparse in the final dataset," and the results only validate L1 sensitivity for the well-represented languages (Chinese, Japanese, European languages).
- Why unresolved: It remains unclear if the "L1 slice" performance boost is a general feature of bilingual models or an artifact of the high-frequency occurrence of specific error types in the pre-training and evaluation data for major languages.
- What evidence would resolve it: Evaluation of multilingual models on BLiSS slices containing low-resource L1s (e.g., Vietnamese or Arabic from the metadata) to see if the significant performance delta between "Overall" and "L1" holds constant.

## Limitations

- The 4.8% validation yield significantly constrains the benchmark's coverage, limiting statistical power for fine-grained analysis of L1-specific transfer patterns
- The relationship between error position and naturalness may not be universally applicable across all error types and learner populations
- Generalizability of L1-transfer signal detection to all bilingual models and learner contexts remains uncertain due to sparsity issues

## Confidence

- **High Confidence**: The benchmark construction methodology is sound, with systematic triplet generation from well-established learner corpora. The clustering of model performance by training paradigm is a robust finding that validates the benchmark's discriminative power.
- **Medium Confidence**: The selective tolerance mechanism effectively distinguishes cognitively aligned models from grammatically competent but non-learner-sensitive ones. However, the relationship between error position and naturalness may not be universally applicable across all error types and learner populations.
- **Low Confidence**: The generalizability of the L1-transfer signal detection to all bilingual models and learner contexts remains uncertain due to the sparsity issues and potential confounding factors in L1-background metadata.

## Next Checks

1. **Cross-L1 Transfer Validation**: Test whether models trained on one L1 background show systematic performance differences when evaluated on learner data from different L1s, controlling for proficiency level and error type distribution.

2. **Error Position Sensitivity Analysis**: Conduct controlled experiments varying the distance between original and artificial error positions to determine whether the plausibility gradient is linear or exhibits threshold effects.

3. **Human Judgment Correlation Study**: Compare model BLiSS scores against human acceptability ratings for matched triplet sets to validate the surprisal-plausibility relationship across diverse error types and learner populations.