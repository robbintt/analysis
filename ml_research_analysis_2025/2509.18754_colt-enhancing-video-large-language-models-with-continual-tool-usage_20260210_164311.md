---
ver: rpa2
title: 'COLT: Enhancing Video Large Language Models with Continual Tool Usage'
arxiv_id: '2509.18754'
source_url: https://arxiv.org/abs/2509.18754
tags:
- video
- tool
- learning
- monkey
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COLT addresses the challenge of enabling video large language models
  to continually acquire tool-use capabilities in streaming environments without catastrophic
  forgetting. The method introduces a learnable tool codebook as a memory system that
  dynamically selects relevant tools based on the similarity between user instructions
  and stored tool features.
---

# COLT: Enhancing Video Large Language Models with Continual Tool Usage

## Quick Facts
- arXiv ID: 2509.18754
- Source URL: https://arxiv.org/abs/2509.18754
- Reference count: 18
- Key outcome: Achieves 8.2% improvement in zero-shot video QA accuracy over prior methods

## Executive Summary
COLT addresses the challenge of enabling video large language models to continually acquire tool-use capabilities in streaming environments without catastrophic forgetting. The method introduces a learnable tool codebook as a memory system that dynamically selects relevant tools based on the similarity between user instructions and stored tool features. To enable this, COLT constructs a video-centric tool-use instruction tuning dataset called VideoToolBench. Experimental results show COLT achieves state-of-the-art performance, improving zero-shot video-question answering accuracy by 8.2% over prior methods on MSRVTT-QA and demonstrating strong continual learning performance with minimal forgetting compared to baseline approaches.

## Method Summary
COLT enables video LLMs to sequentially learn tool usage without catastrophic forgetting through a three-stage training process. First, it aligns visual features to LLM space using general video-text data. Second, it pre-trains a learnable Tool Codebook and Query Encoder using the Straight-Through Estimator to handle non-differentiable tool selection. Third, it performs end-to-end fine-tuning on mixed general and tool-use data. The system uses Vicuna-7B with LanguageBind visual encoder, selecting top-K tool prompts via cosine similarity between user instruction embeddings and codebook vectors. The approach is rehearsal-free, avoiding raw data storage while maintaining tool-specific knowledge through the codebook memory system.

## Key Results
- Achieves 8.2% improvement in zero-shot video QA accuracy over prior methods on MSRVTT-QA
- Demonstrates strong continual learning performance with minimal forgetting compared to baseline approaches
- Shows text-based tool selection significantly outperforms vision-based selection (MVBench accuracy 51.8 vs ~38-40 when removing quantization/commitment losses)

## Why This Works (Mechanism)

### Mechanism 1: Rehearsal-Free Memory via Learnable Tool Codebook
The learnable Tool Codebook replaces traditional replay buffers by storing tool-specific knowledge in discrete, learnable feature vectors. This allows sequential tool learning without storing raw data samples, reducing interference with previous knowledge. The low-dimensional codebook vectors compress and disentangle diverse tool functionalities to prevent degradation of previously learned tools.

### Mechanism 2: Dynamic Tool Retrieval via Query-Key Similarity
A lightweight Query Encoder embeds user instructions, and cosine similarity between this embedding and Tool Codebook vectors selects the most relevant tools. This decouples tool selection from the LLM's internal generation process, improving precision in tool invocation by aligning the semantic space of user instructions with tool capabilities.

### Mechanism 3: Gradient Isolation via Straight-Through Estimation
The Straight-Through Estimator approximates gradients through the non-differentiable Top-K selection operation, enabling end-to-end training of the discrete tool selection mechanism. Paired with quantization and commitment losses, this forces codebook embeddings to cluster near instruction embeddings while maintaining stable gradient flow.

## Foundational Learning

**Concept: Catastrophic Forgetting in Sequential Learning**
- Why needed here: The core problem COLT solves is the degradation of previously learned skills when fine-tuned on new data streams.
- Quick check question: If you fine-tune an LLM on Task A, then sequentially fine-tune it on Task B without regularization or memory, what happens to its performance on Task A?

**Concept: Prompt Tuning vs. Fine-Tuning**
- Why needed here: COLT creates a "Tool Codebook," which is effectively a form of prompt tuning where the "prompts" are learned continuous vectors rather than discrete text tokens.
- Quick check question: How does updating a small set of external learnable vectors (prompts) differ from updating the full weights of a model in terms of parameter efficiency?

**Concept: Vector Quantization (VQ)**
- Why needed here: The training losses and codebook update mechanism are derived from Vector Quantization techniques.
- Quick check question: In a vector quantization layer, how does the "commitment loss" stabilize the training of the encoder?

## Architecture Onboarding

**Component map:**
Visual Encoder -> Projector -> [Query Encoder + Tool Codebook] -> Selection Logic -> LLM

**Critical path:**
1. Stage 1 (Alignment): Train Projector only (Freeze LLM/Visual Encoder)
2. Stage 2 (Codebook Pre-training): Train Codebook and Query Encoder (Freeze LLM)
3. Stage 3 (E2E Fine-tuning): Unfreeze LLM and train full system

**Design tradeoffs:**
- Codebook Size (N=50): Larger sizes accommodate more tools but increase retrieval complexity and risk of under-utilization
- Prompt Selection (K=3): Allows compositional tool use but increases context length
- Rehearsal-Free: Avoids privacy issues of storing user data but may underperform compared to joint training

**Failure signatures:**
- Rapid drop in Average Accuracy as tools increase: Catastrophic forgetting; check learning rate or codebook effectiveness
- Model ignores tool selection and answers directly: Query Encoder misalignment; commitment loss weight may be too low
- VRAM OOM: Reduce batch size or use gradient checkpointing

**First 3 experiments:**
1. Baseline Comparison: Train sequential fine-tuning model vs. COLT on VideoTool stream to quantify reduction in Average Forgetting
2. Ablation on Losses: Disable quantization and commitment losses separately to confirm necessity for stable gradient flow
3. Codebook Saturation Test: Vary tools (5 vs. 10) while keeping codebook size fixed to identify breaking point

## Open Questions the Paper Calls Out
1. How does COLT perform when scaling to significantly larger and more complex tool vocabularies beyond the current experimental setup?
2. Can the current architecture effectively support dynamic and automatic tool composition for long-horizon tasks?
3. To what extent do the distribution biases of the GPT-generated VideoTool dataset limit the model's generalization to real-world scenarios?
4. Can reinforcement learning strategies improve the robustness of tool selection compared to the current cosine-similarity matching?

## Limitations
- The effectiveness of the method at scale remains untested beyond 10 tools
- The GPT-generated dataset may introduce distribution biases limiting real-world generalization
- The method's ability to handle complex, long-horizon tool compositions is not validated

## Confidence
**High Confidence**: The mechanism of using a learnable codebook for rehearsal-free memory storage is well-grounded in vector quantization literature with strong empirical support from ablation studies.

**Medium Confidence**: The 8.2% improvement claim is credible but requires careful consideration of baseline selection and dataset construction differences.

**Low Confidence**: The assertion that the method completely eliminates catastrophic forgetting is overstated, as residual forgetting effects may exist.

## Next Checks
1. Systematically vary the codebook size N (20, 50, 100) while increasing the number of tools to identify the precise breaking point where performance degrades.
2. Reconstruct the VideoToolBench dataset using the specified specialist models and compare generated tool responses against reported distributions to verify dataset consistency.
3. Test COLT on an independently constructed tool usage benchmark (such as TRAJECT-Bench) to verify performance gains transfer beyond proprietary evaluation.