---
ver: rpa2
title: 'CLARE: Continual Learning for Vision-Language-Action Models via Autonomous
  Adapter Routing and Expansion'
arxiv_id: '2601.09512'
source_url: https://arxiv.org/abs/2601.09512
tags:
- learning
- task
- tasks
- adapters
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLARE enables vision-language-action models to learn new tasks
  without catastrophic forgetting or task identifiers. It uses lightweight adapters
  in feedforward layers that expand dynamically based on feature similarity, with
  an autoencoder routing mechanism selecting the most relevant adapters during deployment.
---

# CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion

## Quick Facts
- arXiv ID: 2601.09512
- Source URL: https://arxiv.org/abs/2601.09512
- Reference count: 40
- Primary result: Achieves 66.71% AUC on LIBERO benchmark with only 1.7% parameter growth per task, outperforming experience replay methods

## Executive Summary
CLARE addresses catastrophic forgetting in vision-language-action models by using lightweight modular adapters that expand dynamically based on feature similarity. The method introduces autoencoder-based discriminators that route inputs to the most relevant adapters without requiring task labels. By freezing the base model and only training adapters, CLARE preserves previously learned knowledge while acquiring new tasks. Experiments demonstrate significant improvements over exemplar-based methods while maintaining parameter efficiency.

## Method Summary
CLARE inserts lightweight encoder-decoder adapters into feedforward layers of a pre-trained VLA model. For each new task, z-scores comparing features against existing discriminators determine whether to expand with new adapters or reuse existing ones. Adapters are trained via flow matching loss while discriminators learn reconstruction, with a two-stage training procedure. At inference, reconstruction errors guide adapter selection without task labels. The method achieves continual learning without stored exemplars or oracle task identifiers.

## Key Results
- Achieves 66.71% AUC on LIBERO-10 with only 1.7% parameter growth per task
- Outperforms experience replay (55.87% AUC) while avoiding stored exemplars
- Shows 65% AUC with ~60 adapters versus 57% with only 16 adapters when threshold increases

## Why This Works (Mechanism)

### Mechanism 1
Freezing existing model parameters while training only lightweight adapters prevents catastrophic forgetting. Adapters are added as parallel side branches to selected FFN layers, with adapter output summed to original FFN output. This preserves pre-trained representations entirely. Core assumption: task-specific knowledge can be stored in low-rank subspaces without modifying the base model.

### Mechanism 2
Autoencoder discriminators enable task identification without explicit task labels by measuring feature-space familiarity. Each discriminator is trained via reconstruction loss on its associated task's features. During inference, the discriminator with lowest reconstruction error indicates the most relevant adapter. Core assumption: features from the same task will have lower reconstruction error under their associated discriminator than under discriminators trained on different tasks.

### Mechanism 3
Z-score-based expansion criterion enables parameter-efficient growth by reusing adapters when features are sufficiently similar to previous tasks. For each new task, compute z-scores comparing new task features against all existing discriminators. If all z-scores exceed threshold γ, add new adapter; otherwise, link new discriminator to existing adapter with lowest reconstruction error. Core assumption: high z-scores across all discriminators indicate genuine novelty requiring new capacity.

## Foundational Learning

- **Catastrophic Forgetting in Sequential Fine-Tuning**
  - Why needed here: The entire method is motivated by the failure mode where updating shared parameters on new tasks degrades performance on old tasks.
  - Quick check question: Can you explain why fine-tuning a VLA on Task B after Task A typically reduces Task A performance?

- **Mixture-of-Experts Routing**
  - Why needed here: CLARE's routing mechanism is a dynamic, expanding variant of MoE where experts (adapters) are added over time rather than fixed.
  - Quick check question: How does CLARE's routing differ from standard MoE gating networks?

- **Flow Matching / Diffusion for Action Generation**
  - Why needed here: The base policy uses flow matching to learn vector fields that transport noise to action distributions; adapters modify this process.
  - Quick check question: Why might generative action models be preferred over deterministic policy outputs for manipulation tasks?

## Architecture Onboarding

- **Component map:**
  - Base VLA (frozen) -> DiT encoder-decoder (~200M params)
  - Adapters -> Light encoder-decoder (r ≪ d, ~0.26M params per FFN adapter)
  - Discriminators -> Autoencoders (~0.33M params) that reconstruct layer features
  - Linking function B_ℓ -> Maps each discriminator to exactly one adapter

- **Critical path:**
  1. Pre-train base VLA on diverse demonstrations (LIBERO-90)
  2. For each new task: extract features → compute z-scores → expand or reuse adapters
  3. Train new adapters via flow matching loss
  4. Freeze adapters, train new discriminators via reconstruction loss
  5. At inference: compute reconstruction errors → select adapter per layer → sum with FFN output

- **Design tradeoffs:**
  - **Encoder vs. decoder expansion:** Encoder-only expansion achieves ~65% AUC vs. decoder-only at ~29-42%. Decoder adapters perform poorly.
  - **Threshold γ:** Controls parameter growth vs. task acquisition. γ=0 adds all adapters (highest performance); γ=20 adds ~4x fewer adapters with moderate AUC drop.
  - **Assumption:** Shallowest layer expansion fallback ensures minimal new capacity per task even when z-scores suggest no expansion.

- **Failure signatures:**
  - **Negative forward transfer (low FWT):** Adapters in wrong layers (e.g., decoder) fail to capture task-specific knowledge.
  - **High NBT (>0):** Forgetting due to shared parameter updates—indicates adapter freezing is not being applied correctly.
  - **Erratic routing:** If features shift due to upstream adapter changes, discriminator training becomes inconsistent.

- **First 3 experiments:**
  1. **Layer ablation:** Compare adapter insertion in encoder vs. decoder vs. both. Expect encoder-only to match full expansion.
  2. **Threshold sweep:** Vary γ ∈ {0, 5, 10, 15, 20} on LIBERO-10. Plot AUC, FWT, NBT, and adapter count.
  3. **Routing sanity check:** Visualize discriminator reconstruction errors on in-distribution vs. out-of-distribution tasks. Verify correct adapter selection >90% of the time.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does CLARE perform when deployed on physical robot hardware rather than simulation? The authors consider evaluation on hardware to be natural steps for further work.
- **Open Question 2:** Does CLARE's performance and parameter efficiency scale to larger, state-of-the-art VLAs with billions of parameters? The authors note they focused on relatively small models with limited computational resources.
- **Open Question 3:** Can the expansion threshold γ be set automatically or adapted online without manual tuning? The paper provides no principled method for selecting γ beyond manual specification.
- **Open Question 4:** How robust is the autoencoder routing mechanism when consecutive tasks have highly similar feature distributions? The paper doesn't evaluate scenarios where feature distributions of different tasks substantially overlap.

## Limitations
- Autoencoder routing mechanism lacks strong direct evidence and ablation studies for ambiguous cases
- Z-score threshold γ=2.5 appears arbitrary without principled selection method or generalization analysis
- Assumes feature distributions remain stable during discriminator training, with only heuristic fixes for violations

## Confidence
- **High confidence**: Catastrophic forgetting is prevented by freezing base model parameters and training only lightweight adapters. Strong experimental results support this principle.
- **Medium confidence**: Autoencoder discriminators enable effective task identification without explicit labels. Novel mechanism shows promise but lacks comprehensive ablation evidence.
- **Low confidence**: Z-score-based expansion criterion reliably distinguishes novel from similar tasks. Threshold selection appears heuristic with unclear generalization properties.

## Next Checks
1. **Routing robustness test**: Evaluate adapter selection accuracy when task features are perturbed or when tasks have overlapping visual but different action requirements. Measure correct adapter selection percentage.
2. **Threshold sensitivity analysis**: Conduct systematic sweep of γ values across multiple task sequences with varying similarity. Identify optimal threshold range and characterize performance degradation boundaries.
3. **Feature stability verification**: Track feature distributions across tasks during continual learning. Measure reconstruction error variance within and between tasks to quantify stability assumptions.