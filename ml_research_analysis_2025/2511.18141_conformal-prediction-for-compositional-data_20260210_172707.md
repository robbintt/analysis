---
ver: rpa2
title: Conformal Prediction for Compositional Data
arxiv_id: '2511.18141'
source_url: https://arxiv.org/abs/2511.18141
tags:
- prediction
- conformal
- regression
- quantile
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops conformal prediction methods for compositional
  responses constrained to the simplex. The authors propose a split conformal approach
  using quantile residuals from marginal Beta distributions and a density-based method
  targeting highest-density regions (HDR) via coordinate-floor approximation.
---

# Conformal Prediction for Compositional Data

## Quick Facts
- **arXiv ID:** 2511.18141
- **Source URL:** https://arxiv.org/abs/2511.18141
- **Reference count:** 23
- **Primary result:** Valid and efficient conformal prediction for simplex-constrained compositional data using quantile residuals and HDR methods

## Executive Summary
This work develops conformal prediction methods for compositional responses constrained to the simplex. The authors propose a split conformal approach using quantile residuals from marginal Beta distributions and a density-based method targeting highest-density regions (HDR) via coordinate-floor approximation. Both methods respect the simplex geometry and guarantee finite-sample marginal coverage under exchangeability. Simulation results across homoscedastic and heteroscedastic designs show that quantile residual and HDR with grid methods achieve empirical coverage close to the nominal 90% level with substantially narrower prediction sets than the conservative HDR-floor baseline. In a real-data application predicting household budget shares, the HDR with grid and quantile residual methods attain coverage nearest the target with the smallest average widths.

## Method Summary
The authors develop two conformal prediction methods for compositional data constrained to the simplex. The first method uses quantile residuals from marginal Beta distributions, transforming Dirichlet regression outputs to standard normal space via probability integral transforms. The second method targets highest-density regions (HDR) through coordinate-floor approximation, finding lower bounds for each component that guarantee coverage. A grid refinement step mitigates the conservatism of the floor approximation by sampling within the resulting polytope and filtering against the exact likelihood threshold.

## Key Results
- Quantile residual and HDR with grid methods achieve empirical coverage close to nominal 90% level
- These methods produce substantially narrower prediction sets than conservative HDR-floor baseline
- In household budget prediction, HDR with grid and quantile residual methods attain coverage nearest target with smallest average widths
- Methods demonstrate calibrated uncertainty quantification is feasible for compositional prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping constrained simplex data to unconstrained Gaussian space via Probability Integral Transforms (PIT) allows standard quantile-based conformal scores to handle compositional constraints.
- **Mechanism:** The method computes "quantile residuals" by applying the Beta CDF (marginals of the Dirichlet) to the response $Y$, yielding uniform values, which are then mapped to standard normal $Z$-scores via $\Phi^{-1}$. A max-score across components $S(x,y) = \max_j |Z_j|$ aggregates deviations; thresholding this score inverts back to intervals on the simplex via Beta quantiles.
- **Core assumption:** The Dirichlet regression model accurately estimates the conditional mean and precision such that the probability integral transform yields well-calibrated uniform residuals.
- **Evidence anchors:** [abstract] "split conformal approach based on quantile residuals"; [section 3.2] "Leveraging this property... $Y_j | X=x \sim Beta$... define the conformity score via quantile residuals."
- **Break condition:** If the Dirichlet model is severely misspecified, the marginal Beta CDF transforms will not yield uniform residuals, potentially distorting coverage.

### Mechanism 2
- **Claim:** Approximating the exact Highest Density Region (HDR) by a "coordinate-floor" simplex (shifted faces) guarantees valid coverage via convex optimization.
- **Mechanism:** Instead of inverting the likelihood directly, the method finds lower bounds (floors $\tau_i$) for each component $y_i$ such that the resulting inscribed polytope contains the exact HDR. This is framed as a constrained convex program minimizing $\tau_i$ subject to the log-likelihood threshold.
- **Core assumption:** The shape parameters satisfy conditions (e.g., $w_j > 0$) ensuring the feasible set is convex and Slater's conditions hold for the optimization.
- **Evidence anchors:** [abstract] "density-based method targeting highest-density regions (HDR) via coordinate-floor approximation"; [section 3.4] "set that contains $C_{exact}$... shift the faces of the simplex by imposing lower bounds."
- **Break condition:** If the likelihood contours are highly irregular or the response has zeros (boundary of simplex), the convex formulation may fail or produce trivial floors ($\tau=0$).

### Mechanism 3
- **Claim:** Internal grid refinement within the floor approximation restores efficiency (sharpness) lost by the conservative outer polytope.
- **Mechanism:** The coordinate-floor method guarantees coverage but is loose (conservative). The refinement samples points from the polytope (grid) and filters them against the exact log-likelihood threshold $t^*$, discarding points in the "corners" of the triangle that have low probability density.
- **Core assumption:** The grid density is sufficient to approximate the curved boundary of the true HDR set; computational resources allow for the iterative check.
- **Evidence anchors:** [section 3.4] "To mitigate this overcoverage, we perform a grid search within the resulting triangle."; [results] "HDR with grid methods achieve... substantially narrower prediction sets than the conservative HDR-floor baseline."
- **Break condition:** In high dimensions ($D \gg 3$), the "curse of dimensionality" makes the grid search computationally intractable or too sparse to accurately define the boundary.

## Foundational Learning

- **Concept: Compositional Data (Simplex Geometry)**
  - **Why needed here:** Standard Euclidean intervals $[a, b]$ fail because components must be positive and sum to 1. Intervals derived independently for $y_1$ and $y_2$ may be mutually exclusive (e.g., both predicting high values impossible under the sum constraint).
  - **Quick check question:** If you have 3 components and predict $y_1 \in [0.6, 0.8]$ and $y_2 \in [0.5, 0.7]$, why is this prediction set geometrically invalid?

- **Concept: Split Conformal Prediction (SCP)**
  - **Why needed here:** This is the statistical engine guaranteeing coverage. It separates data into "fitting" and "calibration" to avoid overfitting while establishing a distribution-free threshold for "typical" vs. "atypical" scores.
  - **Quick check question:** Why does SCP require the calibration and test data to be exchangeable (i.i.d.)? What happens to coverage if the test set distribution shifts?

- **Concept: Probability Integral Transform (PIT)**
  - **Why needed here:** It converts the complex, bounded Dirichlet response into a standard Normal distribution, enabling the use of symmetric Gaussian quantiles ($\Phi^{-1}$) to define prediction regions.
  - **Quick check question:** If a model is perfectly calibrated, what is the distribution of the values $F_{model}(y_{true})$?

## Architecture Onboarding

- **Component map:**
  1. Base Model: `DirichletReg` (R) estimating $\mu(x)$ (means) and $\phi(x)$ (precision).
  2. Score Module (Quantile): Transforms residuals to Normal Z-scores via Beta CDF.
  3. Score Module (HDR): Computes negative log-likelihood.
  4. Calibrator: Computes the $1-\alpha$ quantile of calibration scores.
  5. Set Generator: Intersects Beta quantiles (Method 1) OR solves convex opt + grid filter (Method 2).

- **Critical path:**
  1. Data split (Train/Cal/Test).
  2. Fit Dirichlet Regression on Train (MLE).
  3. Compute nonconformity scores $S_i$ on Calibration set.
  4. Determine threshold $\hat{q}$.
  5. For new $X_{new}$: Invert threshold using estimated parameters to get region $C(X_{new})$.

- **Design tradeoffs:**
  - **Quantile Residual vs. HDR:** Quantile is faster and easier to implement (no optimization loop) but may produce sets that are less adaptive to the specific skew of the Dirichlet density compared to HDR.
  - **HDR-floor vs. HDR-grid:** Floor is deterministic and fast but "conservative" (wider intervals, over-coverage). Grid restores sharpness (narrower intervals) but adds computational cost and sampling variance.

- **Failure signatures:**
  - **Systematic Over-coverage:** Observed coverage $\gg 90\%$. Likely using the "Triangular Approximation" (HDR-floor) without grid refinement (Table 1 shows 94-96% coverage for floor vs ~90% for grid).
  - **Zero-Inflation Crash:** If inputs contain exact zeros, `DirichletReg` or the log-likelihood score ($\log y$) may fail, as standard Dirichlet support is strictly positive ($y_j > 0$).
  - **Coverage Loss:** If data is heteroscedastic in a way not captured by the precision model $\phi(x)$, the intervals may be too narrow for high-variance regions.

- **First 3 experiments:**
  1. **Baseline Validation (Simulation):** Replicate Scenario 1a (homoscedastic) to verify that `HDR-floor-grid` achieves ~90% coverage while `HDR-floor` yields ~94% (validating the conservatism mechanism).
  2. **Runtime Scaling:** Measure the wall-clock time of the Grid Refinement step as dimensionality $D$ increases (e.g., $D=3$ vs $D=5$) to determine the computational ceiling.
  3. **Model Misspecification Stress Test:** Apply the method to data generated from a non-Dirichlet simplex distribution (e.g., a logistic-normal with correlations) to test the robustness of the "model-agnostic" conformal layer claims.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can conformal prediction methods be extended to handle zero-inflated compositional data?
- **Basis in paper:** [explicit] The conclusion states that "zero inflated compositional data... call for conformal methods built on zero-inflated Dirichlet type models."
- **Why unresolved:** The current Dirichlet regression framework requires strictly positive components ($y_j > 0$), making it incompatible with datasets containing exact zeros (e.g., microbiome or budget data with zero expenditure categories).
- **What evidence would resolve it:** A derivation of valid nonconformity scores for zero-inflated Dirichlet models and empirical validation demonstrating maintained coverage on sparse datasets.

### Open Question 2
- **Question:** Can adaptive grid strategies or alternative convex approximations reduce the inefficiency of the coordinate-floor method?
- **Basis in paper:** [explicit] Section 6 notes that the "HDR-floor can overcover... motivating adaptive grid strategies or alternative convex outer approximations that reduce slack."
- **Why unresolved:** The current coordinate-floor approximation is computationally fast but conservative (wide intervals), while the grid refinement is sharper but computationally heavier; a method balancing both optimally is undeveloped.
- **What evidence would resolve it:** An algorithm utilizing adaptive grids or tighter convex hulls that yields narrower average widths than the HDR-floor method without the computational burden of a dense static grid.

### Open Question 3
- **Question:** What is the optimal construction of prediction sets when shape parameters $\lambda_j \le 1$?
- **Basis in paper:** [inferred] Section 3.4 and Algorithm 2 describe a "Fallback" where $\tau_i = 0$ if $w_j \le 0$ (i.e., $\lambda_j \le 1$), suggesting the primary convex optimization solution is invalid for these boundary cases.
- **Why unresolved:** The paper relies on a heuristic (setting the floor to zero) when the objective function is not strictly convex or interior conditions fail, potentially leading to suboptimal or overly wide prediction regions.
- **What evidence would resolve it:** A generalized analytical solution for the coordinate floors that handles $\lambda_j \le 1$ rigorously, showing improved efficiency over the fallback heuristic in simulations.

## Limitations
- Reliance on Dirichlet regression model for accurate parameter estimation, especially for quantile residual method
- Coordinate-floor HDR method may produce overly conservative sets in high dimensions
- Method assumes strictly positive compositional data, excluding datasets with exact zeros
- Grid refinement adds computational cost and sampling variance

## Confidence
- **High Confidence:** The coverage guarantees for both split conformal methods under exchangeability are theoretically sound and supported by the literature on conformal prediction
- **Medium Confidence:** The simulation results demonstrating improved efficiency (narrower intervals) for the HDR-grid and quantile residual methods over the baseline are plausible
- **Medium Confidence:** The real-data application on household budget shares is a sensible use case, but specific numerical gains would benefit from replication

## Next Checks
1. **Model Robustness Test:** Evaluate the methods on compositional data generated from a non-Dirichlet distribution (e.g., logistic-normal) to assess the robustness of the conformal coverage guarantees when the base model is misspecified
2. **High-Dimensional Scaling:** Systematically measure the computational cost and prediction set quality (coverage and width) of the HDR-grid method as the dimension D increases from 3 to higher values (e.g., 5 or 10) to identify practical scalability limits
3. **Zero-Inflation Handling:** Modify the Dirichlet regression or conformal framework to handle datasets with exact zeros (e.g., via zero-inflated Dirichlet models) and validate if valid coverage can still be maintained