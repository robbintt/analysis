---
ver: rpa2
title: Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided
  Ensemble Decoding
arxiv_id: '2505.17529'
source_url: https://arxiv.org/abs/2505.17529
tags:
- image
- arxiv
- object
- attention
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ensemble Decoding (ED), a training-free method
  to mitigate object hallucination in Large Vision-Language Models (LVLMs). ED splits
  input images into sub-images and combines their logit distributions using attention-guided
  weights, improving focus on relevant visual content.
---

# Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding

## Quick Facts
- arXiv ID: 2505.17529
- Source URL: https://arxiv.org/abs/2505.17529
- Reference count: 27
- Primary result: Training-free method that improves LVLM hallucination detection by up to 6.57% F1 score and 6.47% accuracy on POPE benchmark

## Executive Summary
This paper introduces Ensemble Decoding (ED), a training-free method to mitigate object hallucination in Large Vision-Language Models. ED splits input images into sub-images and combines their logit distributions using attention-guided weights, improving focus on relevant visual content. The method also introduces an adaptive plausibility constraint to preserve fine-grained details and a faster variant, FastED, for speed-critical applications. Experiments on benchmarks like POPE, CHAIR, and LLaVA-Bench show ED outperforms state-of-the-art methods while maintaining efficiency.

## Method Summary
Ensemble Decoding splits input images into sub-images (N=4 at 336×336 resolution) and processes them alongside the original image through the LVLM. At each decoding step, attention maps from the top K layers and H heads are extracted to compute sub-image relevance weights. These weights dynamically adjust the contribution of each sub-image's logits to the final token selection. The method introduces ED Adaptive Plausibility Constraint (ED-APC) to preserve fine-grained tokens that might be suppressed by standard truncation methods. FastED reduces inference time by limiting ensemble decoding to specific generation steps.

## Key Results
- POPE benchmark: 6.57% higher F1 score and 6.47% higher accuracy compared to regular decoding
- CHAIR benchmark: 3.96% higher recall and 0.88% higher accuracy than state-of-the-art grounding modules
- FastED achieves over 50% reduction in inference time while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1: Attention-Guided Dynamic Weighting
At each generation step t, attention matrices from top K layers and top H attention heads are aggregated to compute sub-image relevance weights via softmax with temperature τ. These weights dynamically adjust which sub-image's logits influence the next token selection, outperforming uniform weighting by ~7-8% F1 score on POPE.

### Mechanism 2: Sub-image Decomposition for Object Resolution and Focus
Splitting images into sub-images reduces irrelevant visual context and increases per-object resolution. Each sub-image contains fewer objects with higher relative resolution, improving grounding. Overlap between sub-images prevents object fragmentation and improves performance by ~2-3% F1.

### Mechanism 3: ED Adaptive Plausibility Constraint
ED-APC applies a threshold β to the weighted sum of probabilities across all sub-images, preserving fine-grained tokens that might have moderate probabilities across multiple images but low probability in any single image. This approach consistently outperforms standard APC at all non-zero β values on POPE.

## Foundational Learning

- **Concept: Transformer Self-Attention and Attention Maps**
  - Why needed: The entire ED mechanism relies on extracting and interpreting attention weights to determine sub-image relevance
  - Quick check: Given a query vector and key matrix, can you compute the attention weights and explain what high attention to a region indicates?

- **Concept: Logit Distributions and Softmax Temperature**
  - Why needed: ED manipulates logits by weighting and combining them across images; temperature τ controls weight sharpness
  - Quick check: What happens to the weight distribution when temperature τ approaches 0 versus when it's large?

- **Concept: Patch-wise vs. Resampler Vision Projectors**
  - Why needed: ED requires patch-wise projectors that preserve spatial locality; resamplers compress visual tokens and break spatial correspondence
  - Quick check: Why can't you directly map attention weights to image regions in a resampler-based model?

## Architecture Onboarding

- **Component map:** Image input → Split into N sub-images → Each sub-image + original → Vision encoder → Visual tokens → LLM → Logits per image → Attention extraction → Weight aggregation → Logit ensemble → ED-APC threshold → Final token selection

- **Critical path:** 1) Sub-image generation with overlap, 2) Parallel forward passes through LVLM (N+1 total), 3) Attention extraction from top K=3 layers, top H=3 heads, 4) Per-step attention aggregation and weighting, 5) Logit ensemble with α=0.5 balance, 6) ED-APC filtering with β=0.5

- **Design tradeoffs:** N (sub-image count) affects detail vs. latency; α (ensemble weight) balances original vs. sub-images; τ (temperature) controls weight sharpness; β (plausibility threshold) trades accuracy vs. recall

- **Failure signatures:** Counting tasks underperform due to object fragmentation; resampler-based LVLMs incompatible; very large images resized to 448×448 may lose detail

- **First 3 experiments:** 1) Reproduce POPE Random setting with N=4, α=0.5, β=0.5, τ=1e-2; target F1 ≥88, Accuracy ≥89, 2) Ablate attention-guided weighting by using uniform weights; expect ~8% F1 drop, 3) Profile latency: measure per-image inference time for ED vs. FastED vs. Regular; target FastED within 2x Regular latency

## Open Questions the Paper Calls Out

### Open Question 1
How can Ensemble Decoding be adapted for Large Vision-Language Models that utilize resamplers instead of patch-wise projectors? The current method relies on mapping attention maps to specific spatial regions, a feature lost in resampler-based architectures (e.g., Qwen-VL, InstructBLIP) that compress visual features into a fixed number of tokens.

### Open Question 2
What constitutes the rigorous theoretical proof explaining the effectiveness of Ensemble Decoding? The authors explicitly note they "lack rigorous theoretical proof explaining its effectiveness" despite empirical success, relying on observations rather than formal mathematical justification.

### Open Question 3
How can the image-splitting mechanism be modified to prevent performance degradation in counting tasks? ED "struggles with object counting, likely due to the image-splitting process fragmenting objects, which complicates quantity-related tasks" - a strategy maintaining object integrity across sub-images could improve accuracy on counting benchmarks.

## Limitations
- Limited to patch-wise projector architectures, incompatible with resampler-based models
- Struggles with counting tasks due to object fragmentation across sub-images
- Lacks rigorous theoretical proof explaining the effectiveness of the attention-guided ensemble approach

## Confidence
- **High**: ED improves POPE F1/accuracy and CHAIR recall on tested benchmarks
- **Medium**: Attention-guided weighting is the primary driver of improvement
- **Medium**: Sub-image decomposition improves grounding by reducing irrelevant context
- **Low**: ED-APC's mechanism of preserving fine-grained tokens via ensemble thresholding

## Next Checks
1. Implement attention-guided weighting ablation on POPE: replace dynamic weights with uniform weighting and measure F1 drop (expect ~8% per Table 4)
2. Profile memory and latency for N=4 sub-images vs. FastED: verify ED-APC adds <2x inference time vs. regular decoding
3. Test ED on resampler-based LVLM (e.g., InstructBLIP): confirm spatial attention mapping fails as claimed, validating projector requirement