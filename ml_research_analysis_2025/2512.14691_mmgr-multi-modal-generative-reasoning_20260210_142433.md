---
ver: rpa2
title: 'MMGR: Multi-Modal Generative Reasoning'
arxiv_id: '2512.14691'
source_url: https://arxiv.org/abs/2512.14691
tags:
- video
- score
- veo-3
- evaluation
- nano-banana
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MMGR provides a systematic evaluation of generative reasoning\
  \ across five core abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.\
  \ It introduces three domains\u2014Abstract Reasoning, Embodied Navigation, and\
  \ Physical Commonsense\u2014to assess both internal (symbolic) and external (world)\
  \ simulation capabilities."
---

# MMGR: Multi-Modal Generative Reasoning

## Quick Facts
- **arXiv ID:** 2512.14691
- **Source URL:** https://arxiv.org/abs/2512.14691
- **Reference count:** 21
- **Key outcome:** MMGR reveals severe reasoning gaps in state-of-the-art video and image generative models, with performance dropping below 10% on Abstract Reasoning tasks and models prioritizing visual plausibility over logical validity.

## Executive Summary
MMGR introduces a systematic benchmark for evaluating generative reasoning across five core abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. The benchmark spans three domains—Abstract Reasoning (Maze, Sudoku, ARC-AGI, Math), Embodied Navigation, and Physical Commonsense—assessing both internal (symbolic) and external (world) simulation capabilities. Through comprehensive evaluation of video and image models, MMGR exposes fundamental limitations in current generative models, particularly their failure to maintain logical consistency over time and their reliance on statistical correlations rather than causal reasoning.

The evaluation reveals a "temporal tax" where video models achieve high visual plausibility but fail catastrophically on logical constraints, with models excelling at Physical Commonsense but scoring <10% on Abstract Reasoning tasks like ARC-AGI. Human evaluation exposes systematic overestimation by automated VLMs, which miss transient violations such as wall-crossing and scene hallucination. The findings suggest that robust generative reasoning requires decoupling reasoning states from visual rendering and integrating objectives that reward logical validity over perceptual fidelity.

## Method Summary
MMGR benchmarks generative reasoning using 1,853 samples across three domains: Abstract Reasoning (Maze, Sudoku, ARC-AGI, Math), Embodied Navigation (4 tasks using Matterport3D/HM3D), and Physical Commonsense (VideoPhy, Sports). The evaluation employs zero-shot inference on video (Veo-3, Sora-2) and image (Nano-banana) models, with automated scoring using Gemini-2.5-Pro as a VLM judge against ground-truth solutions. Success is measured via strict conjunction of fine-grained binary metrics (e.g., *Maze Changed*, *Cross Wall*) aggregated into holistic "Overall" scores. The framework also includes human evaluation to calibrate automated metrics against transient physical violations.

## Key Results
- Video models achieve high visual plausibility but fail catastrophically on Abstract Reasoning (<10% on ARC-AGI)
- A "temporal tax" emerges where video models prioritize perceptual fidelity over logical validity
- Automated VLMs systematically overestimate performance by missing transient physics violations detected by human evaluation
- Decoupling reasoning states from visual rendering is identified as critical for robust generative reasoning

## Why This Works (Mechanism)

### Mechanism 1: Perceptual Fidelity vs. Logical Validity
Current generative models optimize for pixel-level reconstruction rather than symbolic consistency, prioritizing "looking right" over "being right." This creates a reasoning disconnect where models achieve high outcome success but fail on process validity.

### Mechanism 2: Temporal Drift in State Tracking
Video models lack explicit memory architectures to maintain static constraints over time, leading to hallucination of problem modifications. Without decoupled reasoning states, the generation process treats all visual elements as mutable.

### Mechanism 3: VLM Evaluator Insensitivity to Transient Violations
Automated evaluation using VLMs systematically overestimates performance because they miss fast, transient physics violations that humans catch, primarily due to temporal resolution limitations.

## Foundational Learning

- **World Modeling vs. Simulation**: Distinguish between generating videos that look like solutions versus satisfying actual constraints. Quick check: Can a model generate photorealistic video of a ball rolling uphill? (Yes, but fails Physical Commonsense).
- **Symbolic vs. Sub-symbolic Decoupling**: Understand why robust reasoning requires separating reasoning states from visual rendering. Quick check: Why do image models outperform video models on Sudoku? (Video introduces temporal drift where constraints are altered).
- **The "Temporal Tax"**: Explains performance drops in video generation due to computational costs of maintaining visual coherence. Quick check: Why does a model solve math correctly in images (74%) but fail in videos (12%)? (Temporal tax overrides logical deduction).

## Architecture Onboarding

- **Component map**: Generators (Veo-3, Sora-2, Nano-banana) → Benchmark (3 domains × 5 abilities) → Evaluator (Gemini-2.5-Pro + Human) → Metrics (fine-grained → Overall)
- **Critical path**: Input (Prompt + Reference) → Generation (Model produces video/image) → VLM Evaluation (Checks constraints) → Human Calibration (Validates Auto-eval)
- **Design tradeoffs**: Image models offer higher logical consistency but zero temporal reasoning; video models offer dynamics but suffer state drift. Strict metrics reveal catastrophic failures hidden by individual metrics.
- **Failure signatures**: "Doppelgänger Hallucination" (spontaneous second agent), "Context Drift" (changing demonstration examples), "Physics Clipping" (agents passing through walls)
- **First 3 experiments**:
  1. **Maze Constraint Test**: Run baseline model on 5x5 maze; measure delta between "Target Achievement" and "Cross Wall" to detect cheating
  2. **State Stability Stress Test**: Compare Image vs. Video models on "Clues Changed" metric for Sudoku; expect Video model to alter initial constraints
  3. **Evaluator Sensitivity Analysis**: Run failed videos through VLM evaluator at 1fps vs 10fps to quantify automated metric blindness

## Open Questions the Paper Calls Out

### Open Question 1
How can video generation architectures be modified to decouple reasoning states from visual rendering to mitigate the "temporal tax"? This remains unresolved as current models sacrifice logical consistency for frame-to-frame visual plausibility.

### Open Question 2
Can automated evaluators be calibrated to reliably detect transient physical violations that VLMs currently miss? The evaluation gap between Auto-Eval and Human-Eval suggests this is critical but unresolved.

### Open Question 3
Do reinforcement learning objectives or neuro-symbolic supervision effectively align model optimization with logical validity over perceptual fidelity? Current optimization rewards visual plausibility, causing hallucination of competence.

## Limitations
- Automated VLM evaluation systematically overestimates performance by missing transient physical violations
- Restricted access to proprietary models (Veo-3, Sora-2, Nano-banana) prevents independent verification
- Strict conjunction metrics may be overly stringent for certain tasks

## Confidence

**High Confidence:**
- Video models fail catastrophically on Abstract Reasoning (<10% on ARC-AGI)
- Temporal tax mechanism supported by outcome vs. process success gaps
- Human evaluation reveals systematic VLM overestimation

**Medium Confidence:**
- Decoupling reasoning states is necessary for robust reasoning
- Video models rely on statistical correlations rather than causal deduction

**Low Confidence:**
- Specific architectural solutions for improving generative reasoning
- Exact mechanisms of temporal drift in video models

## Next Checks

1. **Evaluator Sensitivity Calibration**: Run failed video samples through VLM evaluator at multiple temporal resolutions (1fps, 5fps, 10fps) to quantify blind spots and establish correction factors.

2. **State Tracking Intervention**: Implement external memory module penalizing changes to static input elements during video generation, then measure impact on Abstract Reasoning tasks compared to baseline.

3. **Cross-Domain Generalization Test**: Test model successful on Physical Commonsense (ball physics) on Abstract Reasoning with similar dynamics (liquid pouring in ARC-AGI) to isolate domain-specific vs. fundamental failures.