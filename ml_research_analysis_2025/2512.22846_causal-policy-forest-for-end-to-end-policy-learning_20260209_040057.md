---
ver: rpa2
title: Causal-Policy Forest for End-to-End Policy Learning
arxiv_id: '2512.22846'
source_url: https://arxiv.org/abs/2512.22846
tags:
- policy
- cate
- learning
- treatment
- forest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes causal-policy forest, an end-to-end algorithm
  for policy learning in causal inference. The key contribution is showing that maximizing
  policy value is equivalent to minimizing mean squared error for the conditional
  average treatment effect (CATE) under {-1, 1} restricted regression models.
---

# Causal-Policy Forest for End-to-End Policy Learning

## Quick Facts
- arXiv ID: 2512.22846
- Source URL: https://arxiv.org/abs/2512.22846
- Reference count: 5
- Primary result: Causal-policy forest achieves policy values close to oracle benchmark while maintaining low regret compared to existing methods.

## Executive Summary
This paper proposes causal-policy forest, an end-to-end algorithm for policy learning in causal inference. The key contribution is showing that maximizing policy value is equivalent to minimizing mean squared error for the conditional average treatment effect (CATE) under {-1, 1} restricted regression models. Based on this equivalence, the authors modify the causal forest algorithm to directly target policy learning objectives. The algorithm has three main advantages: it bridges the gap between CATE estimation and policy learning, trains policies in a more end-to-end manner without separately estimating nuisance parameters, and maintains computational efficiency similar to standard decision trees and random forests. A simulation study demonstrates that causal-policy forest achieves policy values close to the oracle benchmark while maintaining low regret compared to existing methods like policy trees and X learner approaches. The method provides a simple, scalable solution that directly learns optimal treatment policies from observational data.

## Method Summary
Causal-policy forest modifies standard causal forest by changing the splitting criterion to directly target policy learning objectives. The algorithm uses honest estimation (separating split and estimation samples) and scores splits based on the weighted sum of absolute within-child CATE estimates. Each tree is grown using a subsample split into I_split (for structure) and I_est (for leaf estimation). The final policy output is obtained by taking the sign of leafwise CATE estimates, restricting predictions to {-1, 1}. The method maintains computational efficiency while learning policies end-to-end without requiring separate nuisance parameter estimation.

## Key Results
- Causal-policy forest achieves policy values close to the oracle benchmark in simulation studies
- The method maintains low regret compared to existing approaches like policy trees and X learner methods
- The algorithm demonstrates computational efficiency comparable to standard decision trees and random forests
- The approach successfully bridges the gap between CATE estimation and direct policy learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing welfare over a binary policy class is equivalent to minimizing mean squared error for CATE under {-1, 1}-restricted regression models.
- Mechanism: Define policy π(x) ∈ {0, 1} and transform to g(x) = 2π(x) - 1 ∈ {-1, 1}. The welfare maximization objective becomes equivalent to minimizing E[(τ₀(X) - g(X))²] over g ∈ {-1, 1}. Since g² = 1, the MSE objective reduces to maximizing E[g(X) · τ₀(X)], meaning the optimal restricted predictor is simply the sign of the CATE.
- Core assumption: Policies are deterministic (π: X → {0, 1}), enabling the {-1, 1} restriction.
- Evidence anchors: Theorem 3.1 establishes formal equivalence between maximizing policy value and minimizing MSE for CATE under {-1, 1} restriction.

### Mechanism 2
- Claim: Splitting on the absolute value of child-wise CATE estimates directly targets the policy decision boundary.
- Mechanism: The restricted MSE decomposes as E[(τ₀ - g)²] = E[τ₀²] + 1 - 2E[τ₀·g]. Since g ∈ {-1, 1} takes the sign of the leafwise CATE, minimizing MSE reduces to maximizing |E[τ₀|X ∈ leaf]|. The algorithm scores splits by the weighted sum of absolute within-child CATE estimates, which directly rewards partitions that push leafwise average CATE away from zero.
- Core assumption: The leafwise CATE estimates τ̂_split from the split sample are reasonable approximations of the true τ₀.
- Evidence anchors: Section 4.2 defines split score as -Σ |τ̂_split(S')|, which maximizes leafwise CATE magnitude.

### Mechanism 3
- Claim: Honest estimation (separating split and estimation samples) stabilizes sign decisions when the final output is binary.
- Mechanism: Causal-policy forest uses I_split only for tree structure and I_est only for leafwise CATE estimation. Since the policy output depends on sign(τ̂(x)), reusing the same sample for both tasks would cause splits to exploit sample noise, creating leaves with artificially extreme signs that do not generalize. Honest estimation treats the sign decision as a standard within-leaf estimator, reducing variance.
- Core assumption: Sufficient overlap (ε < e(X) < 1-ε) ensures stable CATE estimates within leaves.
- Evidence anchors: Section 4.2 explains that using the same sample for both tasks would create leaves with overly tailored signs to sample noise.

## Foundational Learning

- Concept: **Potential outcomes framework (Neyman-Rubin model)**
  - Why needed here: The entire setup assumes (Y₀, Y₁) ⊥⊥ D | X (unconfoundedness) and defines CATE as τ₀(x) = E[Y₁ - Y₀ | X = x]. Without this, neither welfare nor CATE is identified.
  - Quick check question: Can you explain why observing only Y_i = D_i·Y₁ + (1-D_i)·Y₀ creates a missing data problem?

- Concept: **Propensity score and inverse probability weighting**
  - Why needed here: The leafwise CATE estimator implicitly uses propensity-weighted outcome differences. The paper notes τ̂(x) corresponds to an IPW estimator, and the constraint ε < e(X) < 1-ε ensures denominators are bounded.
  - Quick check question: What goes wrong with IPW estimation if e(X) can be arbitrarily close to 0 or 1?

- Concept: **Honest estimation in random forests**
  - Why needed here: The algorithm splits each subsample into I_split and I_est. This is not optional for causal-policy forest—it is core to preventing sign overfitting.
  - Quick check question: How does honest estimation differ from standard random forest tree construction?

## Architecture Onboarding

- Component map: Subsampling -> Recursive partitioning -> Leaf constraints -> Leaf estimation -> Forest aggregation
- Critical path: The splitting rule change is the only modification to standard causal forest. If you implement causal forest correctly, causal-policy forest requires only: (1) change split criterion to target |τ̂|, (2) threshold final output to {-1, 1}.
- Design tradeoffs:
  - **k (min leaf size)**: Larger k stabilizes sign decisions but limits tree depth and granularity of heterogeneity capture
  - **s (subsample size)**: Smaller s increases tree diversity but reduces per-tree signal
  - **Split ratio (|I_split| vs |I_est|)**: Paper uses ≈50/50; imbalanced splits trade structure quality for estimation precision
- Failure signatures:
  - **High regret with low variance**: Likely propensity overlap violation (e(X) near 0 or 1), causing biased τ̂
  - **Unstable policies across runs**: Check if honest estimation is correctly implemented (I_split and I_est must be disjoint)
  - **Trivial policy (all +1 or all -1)**: CATE may be uniformly positive/negative, or k is too large preventing useful splits
- First 3 experiments:
  1. **Sanity check on known DGP**: Replicate the simulation in Section 5 (n=10000, p=10, confounded propensity, known CATE). Verify regret < 0.02 vs oracle.
  2. **Ablation on honest estimation**: Compare full causal-policy forest against a version using the same sample for splitting and estimation. Expect higher variance and inflated policy values in the dishonest version.
  3. **Sensitivity to k**: Run with k ∈ {5, 10, 25, 50} on the same DGP. Expect regret to increase as k grows too large (underfitting) or too small (unstable signs).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the formal theoretical guarantees (regret bounds, convergence rates, consistency) for the causal-policy forest algorithm?
- Basis in paper: The paper provides intuition in Section 4.3 but no formal theorems about asymptotic properties or finite-sample regret bounds.
- Why unresolved: The "Justification" section offers only heuristic arguments about partition approximation and honest estimation, without proofs of consistency or rate results that are standard in policy learning literature.
- What evidence would resolve it: Formal theorems establishing consistency, regret convergence rates under standard assumptions, and proofs in an appendix.

### Open Question 2
- Question: How does causal-policy forest extend to settings with multiple (more than two) treatment options?
- Basis in paper: Page 2 states "For simplicity, we focus on the case with binary treatments, but it is easy to extend our results to the multiple treatment case."
- Why unresolved: The equivalence theorem and algorithm are derived specifically for binary policies g(x) ∈ {−1, 1}. The extension to multi-valued treatment sets requires a different formulation not provided.
- What evidence would resolve it: A generalized algorithm and theoretical framework handling T ≥ 3 treatments, with corresponding simulation results.

### Open Question 3
- Question: How robust is the method to violations of unconfoundedness or propensity score overlap assumptions?
- Basis in paper: The paper assumes unconfoundedness and bounded propensity scores (0 < ε < e(X) < 1−ε), but provides no sensitivity analysis when these are violated.
- Why unresolved: Observational data often has limited overlap or hidden confounders. The simulation study uses a well-specified DGP satisfying all assumptions.
- What evidence would resolve it: Simulation studies with varying degrees of overlap violation and confounded treatment assignment, reporting performance degradation patterns.

### Open Question 4
- Question: How sensitive is the algorithm's performance to hyperparameter choices (minimum leaf size k, subsample size s, number of candidate split variables m)?
- Basis in paper: These parameters are introduced in Section 4.2 but the simulation study does not report how they were tuned or conduct sensitivity analysis.
- Why unresolved: Tree-based methods can be sensitive to such choices, especially for policy learning where sign decisions near τ(x) ≈ 0 are unstable.
- What evidence would resolve it: Ablation studies varying each parameter systematically, with performance metrics reported across configurations.

## Limitations
- The paper establishes equivalence only for binary policies, restricting applicability to binary treatment settings
- Honest estimation requirement increases variance when propensity overlap is weak or bounded away from 0 and 1
- Empirical evaluation relies on a single simulation setting without real-world validation or comprehensive sensitivity analysis

## Confidence

- **High confidence** in Theorem 3.1 (equivalence proof) - the mathematical derivation is straightforward and internally consistent
- **Medium confidence** in Mechanism 2 (splitting rule) - the scoring function is theoretically justified but performance depends on CATE estimation quality
- **Medium confidence** in empirical results - single simulation setting provides limited generalizability evidence

## Next Checks

1. Test honest estimation necessity: Compare causal-policy forest against an adaptive version using the same sample for splitting and estimation. Expect 2-3x higher regret and policy value variance in the dishonest version.

2. Evaluate robustness to overlap violations: Re-run simulation with e(X) approaching 0 or 1 for some regions. Measure whether policy values collapse and whether the algorithm detects problematic regions.

3. Compare against direct policy search methods: Implement a grid-search policy over a discretized X-space and compare regret. Causal-policy forest should show superior scaling while maintaining competitive regret for moderate-dimensional X.