---
ver: rpa2
title: Neuron-level Balance between Stability and Plasticity in Deep Reinforcement
  Learning
arxiv_id: '2504.08000'
source_url: https://arxiv.org/abs/2504.08000
tags:
- neurons
- learning
- plasticity
- stability
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the stability-plasticity dilemma in deep reinforcement
  learning (DRL), where agents struggle to retain previously learned skills while
  adapting to new tasks. The authors propose Neuron-level Balance between Stability
  and Plasticity (NBSP), a method that operates at the individual neuron level rather
  than the network level.
---

# Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2504.08000
- **Source URL**: https://arxiv.org/abs/2504.08000
- **Reference count**: 40
- **Primary result**: Neuron-level gradient masking and skill neuron identification significantly outperforms network-level methods on Meta-World and Atari benchmarks

## Executive Summary
This work addresses the stability-plasticity dilemma in continual deep reinforcement learning by operating at the neuron level rather than the network level. The authors propose Neuron-level Balance between Stability and Plasticity (NBSP), which identifies "RL skill neurons" that encode critical task-specific skills using a goal-oriented approach, then applies gradient masking to these neurons during training. Experience replay is incorporated to further reinforce stability. Experimental results on Meta-World and Atari benchmarks show that NBSP significantly outperforms existing methods, achieving higher average success rates and better balance between stability (lower forgetting) and plasticity (forward transfer).

## Method Summary
NBSP works by first identifying RL skill neurons through a goal-oriented method that computes a Goal Proximity Metric measuring progress toward task goals, then calculates neuron activation correlations with task success to identify critical neurons. During training, gradient masking is applied to these skill neurons (scaling gradients by α=0.2) while allowing full plasticity in other neurons. Experience replay from a unified buffer stores experiences from previous tasks and is sampled periodically during training. The method is implemented on top of SAC and applies separately to actor and critic networks, with experiments showing the critic is more critical for stability-plasticity balance due to its recursive update structure.

## Key Results
- Achieves average success rates of 0.90 vs 0.63 for baseline on Meta-World cycling tasks
- Demonstrates significant improvements in both stability (lower forgetting) and plasticity (forward transfer)
- Ablation studies show combined gradient masking and replay outperforms either method alone
- Performance generalizes across different benchmarks and task types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Certain neurons ("RL skill neurons") encode task-specific skills, and their activation patterns correlate with task success.
- **Mechanism:** The method computes a Goal Proximity Metric (GPM) q(t) measuring progress toward the task goal. For each neuron N, it calculates a comprehensive score Score(N) = max(Acc(N), 1 − Acc(N)), where Acc(N) captures how often the neuron's activation deviation from its baseline co-occurs with performance deviation from baseline. Top-scoring neurons are designated as RL skill neurons.
- **Core assumption:** Assumption: Neurons with high correlation between activation deviation and task success deviation are causally important for knowledge retention (correlation ≠ causation is not definitively addressed).
- **Evidence anchors:**
  - [abstract]: "NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method"
  - [section 3.2]: "As illustrated in Figure 1, activations of the specific neuron are strongly correlated with task success: higher activation levels increase the likelihood of successful task completion"
  - [corpus]: Moderate support—related work on neuron-level analysis exists (e.g., "Locate-then-Merge," "Dormant Neuron Exploration"), but direct evidence for goal-oriented identification in DRL is limited.
- **Break condition:** If neurons identified as "skill neurons" do not show consistent activation-to-performance correlation across different random seeds or task variations, the identification heuristic may be capturing noise rather than genuine skill encoding.

### Mechanism 2
- **Claim:** Selectively masking gradients on RL skill neurons preserves prior knowledge while maintaining capacity for new learning.
- **Mechanism:** During backpropagation, gradients are scaled by mask(N) = α(1 − Score(N)) for skill neurons (α = 0.2) and 1.0 for non-skill neurons. This soft constraint allows skill neurons to adapt gradually rather than being frozen, while non-skill neurons retain full plasticity.
- **Core assumption:** Assumption: The gradient masking factor α=0.2 provides sufficient protection without overly constraining adaptation—the optimal value may be task-dependent.
- **Evidence anchors:**
  - [abstract]: "introduces a framework by employing gradient masking... targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks"
  - [section 3.3, Eq. 5-6]: Formal definition of gradient masking with α parameter
  - [corpus]: Gradient projection and masking approaches appear in related work (e.g., "SplitLoRA: Balancing Stability and Plasticity Through Gradient Space Splitting"), suggesting broader applicability of gradient-space interventions.
- **Break condition:** If the proportion of skill neurons is too high, masking restricts too many parameters, impairing plasticity. If too low, critical knowledge is overwritten. Figure 4 shows this tradeoff empirically.

### Mechanism 3
- **Claim:** Periodic experience replay from prior tasks reinforces stability by preventing drift from previously acquired knowledge.
- **Mechanism:** A unified replay buffer Dpre stores experiences from previous tasks (not all, just a portion). At intervals k, training batches are sampled from Dpre instead of the current task buffer D, mixing old and new learning signals.
- **Core assumption:** Assumption: The replay interval k and buffer size are sufficient to maintain knowledge without excessive memory overhead—the paper uses 10^5 stored experiences.
- **Evidence anchors:**
  - [abstract]: "Experience replay is incorporated to further reinforce stability"
  - [section 3.3, Eq. 7]: Loss function combining current and replay experiences via binary function R(t)
  - [Table 2]: Ablation shows replay-only achieves ASR 0.70 vs. 0.62 baseline, but combined with masking achieves 0.95
  - [corpus]: Replay-based methods are established (A-GEM, ClonEx-SAC cited in paper); related corpus confirms this as a standard stability mechanism.
- **Break condition:** If replay buffer sampling is too infrequent or buffer is too small, knowledge degrades between revisitings; if too frequent, it may bias learning away from current task optimum.

## Foundational Learning

- **Concept: Stability-Plasticity Dilemma**
  - **Why needed here:** The entire paper addresses this fundamental tradeoff—understanding why neural networks forget (catastrophic interference) and why preserving plasticity matters is prerequisite to evaluating any solution.
  - **Quick check question:** Can you explain why fully freezing a network after Task 1 prevents learning Task 2, while unfettered learning on Task 2 destroys Task 1 performance?

- **Concept: Actor-Critic Methods (specifically SAC)**
  - **Why needed here:** NBSP is implemented on top of SAC and applies separately to actor and critic networks. The ablation (Table 4) shows the critic is more critical for stability-plasticity balance due to its recursive update structure.
  - **Quick check question:** What is the role of the critic's target network in SAC, and how might its exponential moving average update contribute to knowledge retention?

- **Concept: Gradient Masking / Selective Plasticity**
  - **Why needed here:** The core intervention is not freezing but soft-constraining gradients. Understanding how gradient scaling differs from weight regularization (e.g., EWC) helps explain why NBSP achieves lower forgetting (FM) while maintaining high forward transfer (FWT).
  - **Quick check question:** How does masking gradients by a factor α differ mathematically from applying L2 regularization toward old weights?

## Architecture Onboarding

- **Component map:**
  1. **Skill Neuron Identification Module** (Algorithm 1): Computes standard activations and GPM over T evaluation steps, ranks neurons by Score(N), selects top proportion (default 20%) as skill neurons
  2. **Gradient Masking Layer**: Hooks into backpropagation to scale gradients via mask(N) based on neuron scores; applied to both actor and critic
  3. **Unified Replay Buffer Dpre**: Stores experiences from all previous tasks; sampled at interval k during training
  4. **Base SAC Agent**: Standard Soft Actor-Critic with twin Q-functions, entropy regularization; CleanRL implementation used

- **Critical path:**
  1. Train on Task 1 until convergence or max steps
  2. Run Algorithm 1 to identify skill neurons for Task 1
  3. Compute masks for actor and critic networks
  4. Store portion of Task 1 experiences in Dpre
  5. Begin Task 2 training with masked gradients and periodic replay
  6. Repeat for subsequent tasks

- **Design tradeoffs:**
  - **Skill neuron proportion**: Too low → missing critical knowledge; too high → reduced plasticity. Paper finds 0.2 optimal (Figure 4)
  - **Masking factor α**: 0.2 chosen empirically; higher values = more protection but less adaptability
  - **Actor vs. critic masking**: Table 4 shows applying to both is best, but critic alone outperforms actor alone due to critic's recursive update structure
  - **Replay buffer size**: 10^5 experiences balances memory and stability

- **Failure signatures:**
  - High forgetting (FM) with good forward transfer (FWT) → skill neuron identification may be missing critical neurons or proportion is too low
  - Low forgetting but poor forward transfer → too many neurons masked or α too restrictive
  - Unstable training curves → replay interval k may be too infrequent; increase replay sampling

- **First 3 experiments:**
  1. **Reproduce two-task cycling on Meta-World** (e.g., window-open → window-close): Verify ASR improves from ~0.63 baseline to ~0.90 with NBSP; confirm low FM (<0.2) and high FWT (>0.95)
  2. **Ablate gradient masking vs. replay**: Replicate Table 2—expect replay-only to help modestly, masking-only to help more, combined to achieve best performance
  3. **Vary skill neuron proportion**: Replicate Figure 4 curve—confirm ASR peaks near 0.2 proportion and drops on either side; this validates the identification mechanism is capturing meaningful structure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimal number/proportion of RL skill neurons be determined automatically rather than through manual tuning?
- **Basis in paper:** [explicit] "the number of RL skill neurons must be manually determined and adjusted according to the complexity of the learning task, as there is no automatic mechanism for this selection."
- **Why unresolved:** The current method requires manual selection of the proportion threshold (0.2 in experiments), which may not generalize across different task complexities or network architectures.
- **What evidence would resolve it:** A systematic study showing that an adaptive threshold mechanism (e.g., based on score distributions or task complexity metrics) can match or exceed manually tuned performance across diverse benchmarks.

### Open Question 2
- **Question:** Does the NBSP framework transfer effectively to supervised and unsupervised learning paradigms with stability-plasticity challenges?
- **Basis in paper:** [explicit] "It could also be adapted to other learning paradigms, such as supervised and unsupervised learning, to address similar stability-plasticity challenges. In future work, we plan to explore these extensions and verify their effectiveness across various domains."
- **Why unresolved:** The goal-oriented neuron identification method is specifically tailored to RL's reward-based structure; whether similar skill neuron concepts exist in classification or representation learning contexts is unexplored.
- **What evidence would resolve it:** Experiments applying NBSP or adapted variants to class-incremental image classification benchmarks (e.g., CIFAR-100, ImageNet subsets) demonstrating comparable gains over existing continual learning methods.

### Open Question 3
- **Question:** Can RL skill neurons enable effective model distillation with minimal performance degradation?
- **Basis in paper:** [explicit] "by focusing on RL skill neurons, it becomes possible to distill models by pruning less relevant neurons, leading to more efficient and compact models with minimal performance degradation."
- **Why unresolved:** The relationship between skill neuron importance and pruning tolerance remains unquantified; pruning non-skill neurons may still affect inter-neuron dynamics critical for task performance.
- **What evidence would resolve it:** A study systematically pruning varying percentages of non-skill neurons and measuring the resulting model compression ratio versus performance retention on retained tasks.

### Open Question 4
- **Question:** How does NBSP scale to longer task sequences (beyond 4 cycling tasks) and non-cycling continual learning scenarios?
- **Basis in paper:** [inferred] Experiments only cover 2-task and 4-task cycling scenarios; the replay buffer stores experiences from all prior tasks, raising concerns about memory scaling and cumulative interference as task count grows.
- **Why unresolved:** The cycling paradigm revisits tasks, partially masking forgetting accumulation that would occur in strict forward continual learning without task repetition.
- **What evidence would resolve it:** Experiments on benchmarks with 10+ sequential tasks (e.g., Meta-World ML10/ML45 splits in non-cycling order) comparing NBSP against baselines on standard CRL metrics.

## Limitations
- The method assumes neuron activation patterns causally encode skills, but the identification relies on correlation with goal proximity without definitive causal validation
- The optimal proportion of skill neurons (0.2) and masking factor (α=0.2) are empirically determined and may not generalize across different task types or network architectures
- The Goal Proximity Metric formulation for Meta-World tasks during intermediate training steps is not fully specified, potentially limiting reproducibility

## Confidence
- **High confidence**: Gradient masking effectiveness and replay integration—these are established mechanisms with consistent ablation results (Table 2, Table 4) and moderate support in related literature
- **Medium confidence**: Skill neuron identification quality—the correlation-based approach is novel for DRL, and while related work exists on neuron-level analysis, direct validation of causal importance is limited to activation-performance correlation without ablation studies
- **Medium confidence**: Overall performance gains—significant improvements on Meta-World and Atari are reported, but the exact evaluation procedure for intermediate GPM computation remains unclear, creating potential reproducibility gaps

## Next Checks
1. **Reproduce skill neuron ablation**: Run the method with varying skill neuron proportions (0.1, 0.2, 0.3, 0.4) on a single Meta-World task to verify the 0.2 optimum and confirm the identification mechanism captures meaningful structure
2. **Validate GPM computation**: Implement and test the Goal Proximity Metric calculation for Meta-World during training (not just evaluation) to ensure the identification module works as specified
3. **Cross-task generalization**: Apply NBSP to a third benchmark (e.g., DMLab or ProcGen) with different task types to test whether the 0.2 proportion and α=0.2 parameters transfer or require task-specific tuning