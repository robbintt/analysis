---
ver: rpa2
title: 'MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View
  Clustering'
arxiv_id: '2511.05876'
source_url: https://arxiv.org/abs/2511.05876
tags:
- graph
- clustering
- multi-view
- view
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoEGCL, a novel framework for deep multi-view
  clustering that addresses the limitation of coarse-grained graph fusion in existing
  methods. The key innovation is the Mixture of Ego-Graphs Fusion (MoEGF) module,
  which constructs ego graphs for each sample in each view and uses a Mixture-of-Experts
  network to achieve fine-grained fusion at the sample level rather than the conventional
  view-level fusion.
---

# MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering

## Quick Facts
- **arXiv ID:** 2511.05876
- **Source URL:** https://arxiv.org/abs/2511.05876
- **Reference count:** 14
- **Primary result:** State-of-the-art deep multi-view clustering using fine-grained ego-graph fusion and contrastive learning

## Executive Summary
MoEGCL introduces a novel framework for deep multi-view clustering that overcomes the limitations of coarse-grained graph fusion in existing methods. The key innovation is the Mixture of Ego-Graphs Fusion (MoEGF) module, which constructs ego graphs for each sample in each view and uses a Mixture-of-Experts network to achieve fine-grained fusion at the sample level rather than the conventional view-level fusion. Additionally, the Ego Graph Contrastive Learning (EGCL) module is introduced to enhance the representation similarity of samples from the same cluster. Extensive experiments on six benchmark datasets demonstrate that MoEGCL achieves state-of-the-art performance, outperforming existing methods by significant margins.

## Method Summary
MoEGCL addresses deep multi-view clustering through a two-module approach. The MoEGF module constructs KNN ego-graphs for each sample in each view, then fuses them using a Mixture-of-Experts gating network that computes sample-specific fusion weights. This produces a global fused graph that captures fine-grained relationships. The EGCL module then applies contrastive learning to align representations from the fused graph with view-specific representations, weighted by adjacency to encourage intra-cluster similarity. The framework uses autoencoders per view (encoding to 512 dims, projecting contrastive to 128 dims), two-phase training (200 epochs reconstruction-only pre-training, then 300 epochs full loss), and evaluates via k-means clustering on learned representations.

## Key Results
- Outperforms existing methods by significant margins on six benchmark datasets
- On WebKB dataset, surpasses second-best method by 8.19% in ACC
- On RGBD dataset, outperforms MVCAN by 4.28% in ACC
- Demonstrates stable convergence and good generalization across different datasets

## Why This Works (Mechanism)
The method works by replacing coarse-grained view-level fusion with fine-grained sample-level fusion through ego-graph construction. Each sample gets its own local graph structure in each view, which are then intelligently weighted and combined based on their relevance to that specific sample. The contrastive learning component ensures that samples from the same cluster have similar representations in the fused space, while the two-phase training allows the model to first learn good reconstructions before optimizing for clustering quality.

## Foundational Learning
- **Multi-view clustering**: Unsupervised learning from data with multiple feature sets/views; needed to handle heterogeneous data sources
- **Ego-graph construction**: Creating local graph neighborhoods around each node; needed to capture sample-specific relationships in each view
- **Mixture-of-Experts networks**: Routing mechanisms that select different expert networks per input; needed to weight ego-graphs based on sample-specific relevance
- **Contrastive learning**: Learning representations by pulling together similar samples and pushing apart dissimilar ones; needed to enforce cluster consistency
- **Graph Convolutional Networks**: Neural networks that operate on graph-structured data; needed to process the fused ego-graphs
- **Two-phase training**: Separate pre-training and fine-tuning stages; needed to stabilize learning and avoid local minima

## Architecture Onboarding
**Component Map:** Data -> Per-view Autoencoders -> Ego-Graphs -> MoEGF -> GCN -> EGCL -> Fused Representations -> K-means
**Critical Path:** Input views → Autoencoders (512 dims) → Ego-graph construction (KNN) → MoE gating → Global adjacency matrix → 2-layer GCN → 128-dim projection → Contrastive loss → Clustering
**Design Tradeoffs:** Sample-level fusion provides fine-grained control but increases computational complexity compared to view-level fusion; two-phase training improves stability but requires careful hyperparameter tuning
**Failure Signatures:** Poor clustering when KNN graphs are too sparse/dense; training instability when τ or λ hyperparameters are mismatched; collapsed representations if contrastive loss dominates too early
**First Experiments:**
1. Implement per-view autoencoders and verify encoding to 512 dimensions on sample data
2. Build KNN graphs with K=10 and visualize ego-graph adjacency matrices for sanity check
3. Test MoE gating network with simple MLP architecture on toy ego-graph inputs

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can the MoEGCL framework effectively handle incomplete multi-view data where specific views may be missing for some samples?
- **Basis in paper:** [inferred] The methodology relies on concatenating all view-specific representations ($z_i = cat(z_i^1; ...; z_i^M)$) as input for the gating network, and constructing ego graphs for every view per sample.
- **Why unresolved:** The current formulation requires a fixed set of $M$ views for every sample; missing views would break the input dimensionality for the gating network and the expert fusion process.
- **What evidence would resolve it:** Experimental results on benchmark datasets with artificially removed views (e.g., random view masking) or the inclusion of a masking mechanism in the MoE architecture.

### Open Question 2
- **Question:** How does the sample-level fusion mechanism impact computational scalability when applied to datasets with significantly larger sample sizes than those tested?
- **Basis in paper:** [inferred] The MoEGF module computes specific gating coefficients ($C_i$) for every sample ($i$) individually, contrasting with view-level fusion which computes global weights once.
- **Why unresolved:** While effective on datasets up to 60,000 samples (MNIST), the complexity of evaluating the Mixture-of-Experts network for every single sample may impose bottlenecks on web-scale data.
- **What evidence would resolve it:** A theoretical complexity analysis and runtime experiments on datasets with $>10^6$ samples.

### Open Question 3
- **Question:** Is the model robust to the choice of graph construction method, or is it dependent on the Euclidean KNN assumption?
- **Basis in paper:** [inferred] The paper exclusively constructs the adjacency matrix $\Delta$ using Euclidean-based KNN (Eq. 4) without discussing sensitivity to this choice.
- **Why unresolved:** The quality of the fine-grained ego graphs and the subsequent GCN propagation rely heavily on this initial topology; alternative metrics or sparse graph construction might yield different results.
- **What evidence would resolve it:** Ablation studies using alternative graph construction techniques (e.g., cosine similarity, adaptive graph learning) to see if performance is maintained.

## Limitations
- Two-phase training requires careful tuning of pre-training duration and loss weighting
- Contrastive loss hyperparameters (τ, λ) are critical and may require dataset-specific tuning
- Method assumes all views have meaningful graph structures; performance may degrade with noisy or incomplete views

## Confidence
- **High confidence:** Core methodology (MoE-based ego-graph fusion, contrastive learning) is well-defined and reproducible
- **Medium confidence:** Overall training procedure and evaluation metrics are clear; hyperparameters need tuning
- **Low confidence:** Exact architectural details and data preprocessing pipeline are missing

## Next Checks
1. Verify KNN graph construction with K=10 and visualize ego-graph adjacency matrices for sample views
2. Confirm two-phase training: reconstruction loss should dominate first 200 epochs, then stabilize with full loss
3. Test contrastive loss implementation: ensure negative sampling uses adjacency-weighted similarity and that τ is in recommended range