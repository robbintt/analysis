---
ver: rpa2
title: Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana
arxiv_id: '2501.12789'
source_url: https://arxiv.org/abs/2501.12789
tags:
- questions
- question
- diversity
- datamorgana
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataMorgana addresses the challenge of generating diverse synthetic
  Q&A benchmarks for RAG evaluation by introducing a configurable, two-stage approach.
  It allows users to define detailed categorizations of questions and end-users, specifying
  their characteristics and distribution probabilities within the benchmark.
---

# Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana

## Quick Facts
- arXiv ID: 2501.12789
- Source URL: https://arxiv.org/abs/2501.12789
- Authors: Simone Filice; Guy Horowitz; David Carmel; Zohar Karnin; Liane Lewin-Eytan; Yoelle Maarek
- Reference count: 38
- Primary result: DataMorgana achieves higher N-gram Diversity (NDG) and lower Self-Repetition Scores (SRS) than existing synthetic Q&A generation methods

## Executive Summary
DataMorgana addresses the challenge of generating diverse synthetic Q&A benchmarks for RAG evaluation by introducing a configurable, two-stage approach. It allows users to define detailed categorizations of questions and end-users, specifying their characteristics and distribution probabilities within the benchmark. This enables the generation of highly diverse Q&A pairs through combinatorial combinations of user and question categories.

Experimental results demonstrate DataMorgana's superiority in achieving higher diversity compared to existing methods. On the COVID-QA corpus, DataMorgana achieves N-gram Diversity (NDG) scores of 2.536, outperforming Vanilla (1.517), Know Your RAG (2.358), and DeepEval (2.415). It also shows lower Self-Repetition Scores (SRS) of 0.372 compared to Vanilla (0.920), Know Your RAG (0.613), and DeepEval (0.644). Similar improvements are observed on the Wikipedia corpus. Qualitative analysis reveals DataMorgana generates questions with greater lexical, syntactic, and semantic diversity, covering various user personas and question types more comprehensively than baseline approaches.

## Method Summary
DataMorgana employs a two-stage approach: first, users configure question and user categorizations in JSON format, defining categories with natural-language descriptions and distribution probabilities. Second, the system generates Q&A pairs by sampling categories per their distributions, selecting documents from the corpus, and constructing prompts that inject all category descriptions. The LLM (Claude-3.5-Sonnet) generates k=3 candidate pairs per prompt, which are filtered for quality (context-free property, category adherence, faithfulness to document) before selecting one valid pair. This lightweight architecture avoids heavy preprocessing like knowledge graph construction or document decomposition.

## Key Results
- On COVID-QA corpus: NDG score of 2.536 (vs. Vanilla 1.517, Know Your RAG 2.358, DeepEval 2.415)
- On COVID-QA corpus: SRS of 0.372 (vs. Vanilla 0.920, Know Your RAG 0.613, DeepEval 0.644)
- On Wikipedia corpus: NDG of 2.575 (vs. DeepEval 2.431, Know Your RAG 2.436) and SRS of 0.390 (vs. DeepEval 0.669, Know Your RAG 0.547)

## Why This Works (Mechanism)

### Mechanism 1: Combinatorial Category Configuration
Configuring multiple orthogonal categorizations for both questions and users produces combinatorially diverse question sets. Each categorization defines mutually exclusive categories with natural-language descriptions and distribution probabilities. During generation, one category is sampled per categorization from each axis (user + question), creating a combinatorial space of prompt configurations that constrains the LLM toward diverse outputs rather than its default generation patterns. Ablation studies show removing question categorizations drops NDG from 2.536 to 1.777 (COVID-QA), demonstrating categorizations drive diversity gains.

### Mechanism 2: Direct Prompt Injection of Category Descriptions
Injecting category descriptions directly into prompts as natural-language constraints produces higher lexical and syntactic diversity than evolving questions post-hoc. Rather than post-processing generated questions or using evolutionary approaches, DataMorgana builds prompts that explicitly state user characteristics and question characteristics. The description field from the JSON config is inserted verbatim, giving users fine-grained control over generation without requiring technical prompt engineering. DataMorgana generates questions with visibly more varied phrasing (search queries, questions with premises, varied terminology) compared to Vanilla which "tends to repeatedly use some word expressions."

### Mechanism 3: Lightweight Two-Stage Architecture Avoiding Heavy Preprocessing
A configuration-then-generation pipeline without knowledge graph construction or evolutionary post-processing maintains efficiency while achieving higher diversity than approaches with heavier processing. DataMorgana intentionally avoids: (1) building knowledge graphs (as RAGAs does), (2) document decomposition into statements (as Know Your RAG does), and (3) multi-step question evolution (as DeepEval does). Instead, it samples documents, instantiates prompts, generates k=3 candidates, and filters for quality. DataMorgana achieves NDG 2.536 vs. DeepEval 2.415 vs. Know Your RAG 2.358 on COVID-QA with lower computational overhead.

## Foundational Learning

- **RAG (Retrieval-Augmented Generation)**: Why needed here: DataMorgana generates benchmarks specifically to evaluate RAG systems; understanding what RAG systems do (augment LLM prompts with retrieved documents) explains why diverse question types matter for realistic evaluation. Quick check question: Can you explain why a RAG system might perform differently on "concise-and-natural" vs. "long-search-query" questions?

- **Diversity Metrics (NDG, SRS, CR, HS)**: Why needed here: The paper evaluates synthetic benchmarks using four distinct diversity measures capturing lexical (NDG, SRS, word-CR), syntactic (PoS-CR), and semantic (embeddings-HS) dimensions. Understanding what each measures is essential for interpreting results. Quick check question: Why might a benchmark with high lexical diversity still have low semantic diversity?

- **Generate-Then-Filter Paradigm**: Why needed here: DataMorgana follows this common pattern but differs in how generation is controlled. Understanding this paradigm helps position DataMorgana against alternatives. Quick check question: What filtering criteria does DataMorgana apply after generating candidate Q&A pairs?

## Architecture Onboarding

- **Component map**:
  JSON Config (user/question categorizations + probabilities)
        ↓
  [Configuration Stage] Parses categories, validates mutual exclusivity
        ↓
  [Generation Loop] For each Q&A pair:
      1. Sample categories per categorization (per probabilities)
      2. Sample document from corpus
      3. Build prompt: inject all category descriptions + document
      4. Call LLM → generate k=3 candidates
      5. Filter: verify constraints, faithfulness to document
      6. Sample one valid pair

- **Critical path**: The prompt construction step where category descriptions are injected is the key diversity driver. Errors in config parsing or probability sampling will propagate directly to benchmark composition.

- **Design tradeoffs**:
  - **Lightweight vs. comprehensive**: No knowledge graph construction means faster iteration but may miss complex multi-hop questions.
  - **User control vs. automation**: Requiring manual category definitions adds setup cost but enables domain-specific customization.
  - **k=3 candidates**: Paper uses 3; higher values increase quality filtering but add cost.

- **Failure signatures**:
  - High Self-Repetition Score (SRS > 0.5) suggests category descriptions aren't differentiating sufficiently.
  - Dominance of one syntactic template (>10% Top-1 frequency) suggests prompt constraints aren't being followed.
  - Ablation study pattern: If removing user categorizations has minimal impact (as observed), question categorizations are the primary driver.

- **First 3 experiments**:
  1. **Baseline comparison**: Generate 100 questions from a 50-document corpus using Vanilla (empty config) vs. full DataMorgana config; measure NDG and PoS-CR differences.
  2. **Category ablation**: Run with question categorizations only, then user categorizations only, then both; quantify each axis's contribution to diversity metrics.
  3. **Cross-domain validation**: Apply general-purpose categorizations (Table 1) to your domain corpus, then create domain-specific user categorizations (e.g., patient/doctor for healthcare); compare which improves diversity more for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
How can the quality and correctness (fidelity) of generated answers in synthetic benchmarks be systematically verified? The authors state they "defer further investigation of the answer quality to future research," noting that while filtering helps, the primary focus remained on question diversity rather than answer fidelity. Current experiments relied on preliminary observations of faithfulness rather than rigorous quantitative metrics or human evaluation of the generated answers. A systematic evaluation methodology (e.g., human annotation or automated consistency checking) that quantifies the hallucination rate and factual accuracy of answers generated by DataMorgana would resolve this.

### Open Question 2
Can new diversity metrics be developed that do not penalize simpler terminology used by non-expert personas? The authors acknowledge that current lexical metrics like N-gram Diversity (NDG) favor benchmarks with sophisticated terminology and "penalize" the simple language characteristic of diverse user personas like "patients." Existing metrics conflate linguistic complexity with diversity, failing to capture the value of varied user styles (expert vs. novice) and creating a need to "explore new diversity metrics." A new metric formulation that correlates positively with human judgments of stylistic diversity across different expertise levels, without bias toward complex vocabulary would resolve this.

### Open Question 3
Can the DataMorgana framework be effectively extended to generate multi-turn synthetic conversations? The Conclusion states the authors "intend to introduce in the near future additional capabilities for generating other types of benchmarks, such as synthetic conversations." The current implementation is limited to single-turn Q&A pairs; it is unclear if the lightweight, two-stage configuration process can maintain persona consistency and context over multiple dialogue turns. A demonstration of the tool generating multi-turn dialogues where user personas and question constraints remain consistent across the conversation history would resolve this.

## Limitations
- The paper's diversity improvements are evaluated on only two specific corpora (COVID-QA and Wikipedia/NQ-open), limiting generalizability across domains.
- The exact filtering criteria for quality assessment (category adherence, faithfulness) are not fully specified, making it unclear how much filtering contributes to diversity vs. the prompt configuration mechanism.
- Category descriptions require manual creation and domain expertise, introducing potential user bias and limiting scalability.

## Confidence
- **High confidence**: The combinatorial category configuration mechanism (Mechanism 1) is well-supported by ablation studies showing significant NDG drops when removing categorizations.
- **Medium confidence**: The lightweight architecture's efficiency claims (Mechanism 3) are reasonable but lack direct computational cost comparisons with baseline methods.
- **Low confidence**: The generalizability of the proposed general-purpose categorizations (Table 1) to truly unseen domains without modification.

## Next Checks
1. Test DataMorgana on a third, structurally different corpus (e.g., legal or technical documentation) to validate cross-domain diversity gains.
2. Implement ablation studies removing individual question categorizations to quantify each axis's contribution to diversity metrics.
3. Conduct user studies comparing benchmark quality when using DataMorgana's general-purpose categories versus domain-specific user-defined categories.