---
ver: rpa2
title: 'MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions'
arxiv_id: '2509.22750'
source_url: https://arxiv.org/abs/2509.22750
tags:
- ambiguity
- question
- answer
- multi-hop
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-hop QA models struggle when ambiguity appears at any reasoning
  step, since early misinterpretations can lock in incorrect retrieval paths and cause
  cascading errors. To study this, researchers introduced MARCH, a benchmark of 2,209
  ambiguous multi-hop questions spanning syntactic, semantic, and constraint ambiguity,
  each paired with clarified queries and evidence-grounded answers.
---

# MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions

## Quick Facts
- arXiv ID: 2509.22750
- Source URL: https://arxiv.org/abs/2509.22750
- Reference count: 40
- Multi-hop QA models struggle when ambiguity appears at any reasoning step, since early misinterpretations can lock in incorrect retrieval paths and cause cascading errors

## Executive Summary
This paper addresses the challenge of multi-hop question answering in the presence of ambiguity at any reasoning step. The authors introduce MARCH, a benchmark of 2,209 ambiguous multi-hop questions spanning syntactic, semantic, and constraint ambiguity, each paired with clarified queries and evidence-grounded answers. State-of-the-art models—including agentic baselines—performed poorly, often producing incomplete or one-sided answers due to premature pruning of alternative interpretations. To address this, they developed CLARION, a two-stage framework that first explicitly enumerates interpretations via a Planning Agent and then retrieves and reasons for each branch before synthesizing a final answer. CLARION significantly outperformed all baselines on MARCH, especially in Disambig-F1, demonstrating that decoupling ambiguity planning from evidence retrieval is essential for robust multi-hop reasoning in ambiguous settings.

## Method Summary
The method introduces a two-stage framework called CLARION that explicitly decouples ambiguity planning from evidence retrieval. First, a Planning Agent detects ambiguity types (syntactic, semantic, constraint) and generates clarified queries for each interpretation. Then, an Acting Agent runs a ReAct loop, performing separate retrieval and reasoning for each clarified query before synthesizing a final answer. The approach is evaluated on the MARCH benchmark, which contains 2,209 ambiguous multi-hop questions derived from MuSiQue, with each question having multiple clarified queries, supporting Wikipedia passages, and gold answers.

## Key Results
- CLARION significantly outperformed all baselines on the MARCH benchmark, particularly in Disambig-F1 metric
- Standard multi-hop QA models and agentic baselines produced incomplete or one-sided answers due to premature pruning of alternative interpretations
- The two-stage approach of separating ambiguity planning from evidence retrieval was essential for robust performance

## Why This Works (Mechanism)

### Mechanism 1: Planning-Acting Decoupling
Explicitly enumerating ambiguous interpretations before retrieving evidence prevents the premature pruning of valid reasoning paths. The framework separates the "Planning Agent" (detects ambiguity, generates clarified queries) from the "Acting Agent" (retrieval and execution). By forcing the system to map out divergent interpretations upfront, it avoids path-dependency where an early retrieval choice for the dominant interpretation biases the model against exploring minority interpretations.

### Mechanism 2: Interpretation-Specific Retrieval
Retrieving documents separately for each clarified query ensures higher coverage of disjoint evidence trails compared to single-query retrieval. Instead of a single top-k retrieval using the ambiguous source query, the system executes distinct search actions for each clarified branch, isolating the "bridge entities" for each reasoning chain.

### Mechanism 3: Hop-Consistency Verification
Enforcing logical consistency between intermediate steps mitigates cascading errors in multi-hop reasoning. The Acting Agent maintains an interpretation set and verifies that evidence retrieved at each hop is grounded in the entity resolved at the previous hop, flagging contradictions rather than forcing synthesis.

## Foundational Learning

- **Latent Ambiguity**: Multi-hop ambiguity can be hidden—resolving the first entity to its most popular referent can eliminate valid interpretations of subsequent words. Quick check: If I resolve the first entity in a query to its most popular referent, does that choice eliminate valid interpretations of subsequent words?

- **Path-Dependency in Retrieval**: Standard RAG systems are path-dependent—the documents retrieved at step 1 condition retrieval at step 2. Quick check: Does my retrieval system allow for "backtracking" if the documents found in step 1 do not support the logical requirements of step 2?

- **Disambig-F1 vs. Exact Match**: Standard metrics often fail to capture the nuance of ambiguous QA. Disambig-F1 measures how well the model covers all valid interpretations. Quick check: If a question has two valid answers, does my evaluation metric penalize the model heavily for only finding one, or does it give partial credit?

## Architecture Onboarding

- **Component map**: Planning Agent -> Acting Agent -> Retrieval Module
- **Critical path**: The handoff from Planning Agent to Acting Agent is critical—if the Planning Agent fails to generate distinct, valid clarified queries, the Acting Agent cannot rescue the failure.
- **Design tradeoffs**: CLARION trades latency for robustness, running multiple retrieval branches sequentially (3-9s latency vs <1s for NaiveRAG). By decoupling Planning and Acting, you can swap the Planning LLM for a cheaper/faster model while keeping a powerful Acting LLM.
- **Failure signatures**: One-Sided Answers (coherent answer addressing only one interpretation) or Evidence Mixing (hallucinated connection between two interpretations).
- **First 3 experiments**: 1) Baseline Comparison: Run NaiveRAG vs. CLARION on MARCH dataset, specifically looking at Disambig-F1 score. 2) Ablation on Latency: Run CLARION with limit of 1 search iteration vs. 5. 3) Stress Test on Hop Depth: Test CLARION on 2-hop vs. 3-hop ambiguous queries.

## Open Questions the Paper Calls Out
None

## Limitations
- The Planning Agent's accuracy is critical but unverified independently; errors in ambiguity detection or clarification generation will propagate to downstream reasoning.
- Dataset construction relies heavily on LLM annotations and filters, introducing potential annotation biases and systematic errors in ambiguity classification.
- The evaluation methodology (LLM-as-a-Judge) may not perfectly capture human judgment of answer quality, particularly for nuanced interpretations of ambiguous questions.

## Confidence
- **High Confidence**: The core insight that decoupling planning from acting improves multi-hop reasoning under ambiguity, supported by significant performance gaps between CLARION and baselines on the MARCH benchmark.
- **Medium Confidence**: The specific mechanisms (planning-acting decoupling, interpretation-specific retrieval, hop-consistency verification) are well-described, but their individual contributions are not fully isolated through ablation studies.
- **Medium Confidence**: The claim that evidence for different interpretations is often disjoint is plausible but not systematically validated across all ambiguity types in the dataset.

## Next Checks
1. **Planning Agent Ablation**: Test CLARION with a naive Planning Agent that always generates two random clarified queries (without ambiguity detection) to quantify the contribution of accurate ambiguity identification to overall performance.
2. **Interpretation Overlap Analysis**: For questions where interpretations share significant lexical overlap, measure whether interpretation-specific retrieval still provides benefits or leads to redundant document retrieval.
3. **Cross-Dataset Generalization**: Evaluate CLARION on a subset of DEEPAMBIGQA questions to assess whether the framework generalizes beyond the MARCH benchmark, particularly for questions with more than two interpretations.