---
ver: rpa2
title: 'Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs
  with SPARQL through Natural Language'
arxiv_id: '2505.19971'
source_url: https://arxiv.org/abs/2505.19971
tags:
- language
- data
- sparql
- queries
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of creating natural language\
  \ interfaces for lexicographic data retrieval on knowledge graphs like Wikidata,\
  \ which are typically inaccessible to non-technical users due to the complexity\
  \ of SPARQL queries. The authors develop a multidimensional taxonomy to capture\
  \ the complexity of Wikidata\u2019s lexicographic data ontology and create a dataset\
  \ of over 1.2 million mappings from natural language utterances to SPARQL queries."
---

# Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language

## Quick Facts
- arXiv ID: 2505.19971
- Source URL: https://arxiv.org/abs/2505.19971
- Reference count: 40
- Key outcome: GPT-3.5-Turbo outperforms smaller models on lexicographic SPARQL query generation, particularly for novel query structures, suggesting model size and diverse pre-training are crucial for generalization in this domain.

## Executive Summary
This paper addresses the challenge of making lexicographic data on knowledge graphs like Wikidata accessible to non-technical users through natural language interfaces. The authors develop a comprehensive taxonomy of lexicographic query complexity and create a large dataset mapping natural language utterances to SPARQL queries. They evaluate three different model architectures and find that while smaller models perform well on familiar patterns, only the largest model (GPT-3.5-Turbo) demonstrates meaningful generalization capabilities to novel query structures. The work highlights the importance of model scale and pre-training diversity for handling the complexity of lexicographic data retrieval tasks.

## Method Summary
The authors create a multidimensional taxonomy capturing the complexity of Wikidata's lexicographic data ontology, then generate over 1.2 million mappings between natural language utterances and SPARQL queries using template-based methods. They evaluate GPT-2, Phi-1.5, and GPT-3.5-Turbo models on both familiar and generalization scenarios, measuring performance through pass@k metrics and BLEU scores. The evaluation reveals a significant performance gap between smaller models that excel on template-based queries but fail to generalize, and GPT-3.5-Turbo which maintains reasonable performance on novel query structures despite having lower pass@1 on familiar patterns.

## Key Results
- All models achieve high pass@1 scores (0.86-0.90) on familiar query patterns, demonstrating effective learning of template-based structures
- GPT-3.5-Turbo achieves pass@3 of 0.57 on generalization scenarios, while GPT-2 and Phi-1.5 drop to BLEU scores of 0.3, showing superior generalization capabilities
- Model size and diverse pre-training appear crucial for handling novel lexicographic query structures, with the largest model significantly outperforming smaller alternatives

## Why This Works (Mechanism)
The success of GPT-3.5-Turbo on generalization tasks stems from its larger parameter count and more diverse pre-training corpus, which enables it to recognize and adapt to novel query patterns that weren't explicitly covered in the template-based training data. The multidimensional taxonomy captures the inherent complexity of lexicographic queries across multiple dimensions (form, function, expression, form-meaning), providing a structured framework that models can learn to navigate. The large dataset size (1.2M instances) provides sufficient coverage of common query patterns for smaller models to achieve high performance on familiar structures, while the most capable model can extrapolate beyond these patterns to handle unseen combinations of query elements.

## Foundational Learning
- Knowledge Graph Ontologies: Understanding how lexicographic data is structured in KGs like Wikidata is essential for designing effective query interfaces; quick check: can identify key ontology modules for lexicographic data
- SPARQL Query Language: Familiarity with SPARQL syntax and semantics is necessary for evaluating generated queries; quick check: can write basic SPARQL queries for simple knowledge graph retrieval
- Template-Based Data Generation: Understanding how synthetic datasets can be created from linguistic patterns helps evaluate the limitations of the approach; quick check: can explain the tradeoff between dataset size and natural diversity
- Natural Language to SQL/SPARQL Translation: Understanding semantic parsing techniques for structured query generation is crucial for contextualizing this work; quick check: can describe how template-based approaches differ from semantic parsing

## Architecture Onboarding

**Component Map**: Natural Language Utterance → Model → SPARQL Query → Knowledge Graph → Results

**Critical Path**: The pipeline follows: user query → natural language model → SPARQL generation → knowledge graph execution → results return to user. The bottleneck is the model's ability to correctly translate natural language into valid SPARQL that retrieves the intended lexicographic information.

**Design Tradeoffs**: The template-based approach enables creation of a large, diverse training dataset but may limit the model's exposure to truly novel query formulations. Smaller models achieve higher performance on familiar patterns but fail to generalize, while larger models show better adaptability at the cost of computational efficiency. The choice of evaluation metrics (pass@k vs BLEU) reflects different priorities in measuring functional correctness versus linguistic similarity.

**Failure Signatures**: Models struggle with queries combining multiple complex operations not seen during training, with smaller models completely failing to generate valid SPARQL while larger models produce partially correct queries. Template-based training limits exposure to truly novel query structures, leading to poor generalization performance. The gap between pass@1 and pass@3 scores indicates models can occasionally generate correct queries through diverse outputs but lack consistent accuracy.

**Three First Experiments**:
1. Evaluate model performance on out-of-distribution queries that combine multiple lexicographic operations in novel ways
2. Test the models on naturally-occurring user queries rather than template-generated ones to assess real-world applicability
3. Compare performance against a semantic parsing baseline trained specifically on knowledge graph query tasks

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the precise relationship between model scale and SPARQL generation capability for lexicographic queries, and is there an optimal efficiency-performance tradeoff point?
- Basis in paper: The authors state "scaling experiments with larger model variants could help establish the relationship between model size and SPARQL generation capabilities, potentially identifying optimal efficiency-performance tradeoffs for this specific task."
- Why unresolved: The counterintuitive finding that GPT-2 (124M) achieved the highest pass@1 (0.90) despite being smallest suggests a non-linear relationship between scale and performance that remains uncharacterized.
- What evidence would resolve it: Systematic evaluation of models across a range of parameter sizes (e.g., 124M to 70B+) on both familiar and generalization tasks, measuring performance per parameter count.

### Open Question 2
- Question: Can more diverse training data significantly improve model generalization to novel query structures, and what constitutes sufficient diversity for lexicographic SPARQL generation?
- Basis in paper: The authors conclude that "future work should focus on improving model generalization through more diverse training data."
- Why unresolved: The template-based dataset, while large (1.2M instances), may inherently limit diversity; models showed severe generalization drops (GPT-2: pass@1 of 0.90 → BLEU of 0.3 on generalization).
- What evidence would resolve it: Experiments comparing template-based training data against naturally-occurring query-SPARQL pairs, with controlled diversity metrics and evaluation on systematically varied unseen query structures.

### Open Question 3
- Question: How effectively does this natural language interface approach transfer to other lexicographic knowledge graphs beyond Wikidata, such as Dbnary?
- Basis in paper: The authors explicitly call for "expanding this approach to other KGs, particularly Dbnary (Sérasset, 2012)."
- Why unresolved: The taxonomy and templates are specifically designed around Wikidata's lexicographic data ontology module; different KGs have distinct ontological structures that may require fundamentally different approaches.
- What evidence would resolve it: Direct application of the trained models or adapted methodology to Dbnary, reporting performance metrics on equivalent lexicographic query types.

### Open Question 4
- Question: What is the actual practical utility of these natural language interfaces for different stakeholder groups (lexicographers, linguists, language learners)?
- Basis in paper: The authors state the need for "conducting user studies to evaluate practical utility for different stakeholder groups in lexicography and linguistics."
- Why unresolved: Functional correctness metrics (pass@k) do not capture whether generated queries satisfy real user information needs or whether non-experts can effectively formulate natural language queries for lexicographic tasks.
- What evidence would resolve it: Controlled user studies with target stakeholder groups, measuring task completion rates, query formulation success, and qualitative satisfaction with retrieved lexicographic information.

## Limitations
- The template-based dataset creation process may inherently limit the model's ability to handle truly novel query formulations that combine operations in unexpected ways
- Evaluation focuses primarily on synthetic data generation rather than real user queries, potentially overestimating practical effectiveness
- The study compares only a small set of model architectures, leaving questions about the performance of other potential approaches unexplored

## Confidence
- High confidence: The performance gap between GPT-2/Phi-1.5 and GPT-3.5-Turbo is well-established through systematic evaluation
- Medium confidence: The claim about model size and diverse pre-training being crucial for generalization, as this requires further empirical validation
- Medium confidence: The taxonomy's comprehensiveness in capturing lexicographic query complexity

## Next Checks
1. Conduct user studies with actual lexicographers and non-technical users to validate the real-world effectiveness of the conversational interface
2. Test the models on out-of-distribution queries that combine multiple complex lexicographic operations not seen during training
3. Compare performance against alternative approaches, such as semantic parsing frameworks or fine-tuned smaller models specifically designed for knowledge graph queries