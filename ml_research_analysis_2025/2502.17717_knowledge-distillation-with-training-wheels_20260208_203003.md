---
ver: rpa2
title: Knowledge Distillation with Training Wheels
arxiv_id: '2502.17717'
source_url: https://arxiv.org/abs/2502.17717
tags:
- teacher
- student
- decoding
- distillation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach to knowledge distillation
  where the student model learns to generate some tokens on its own and request teacher
  help for others, guided by natural language instructions specifying test-time teacher-use
  constraints. The method formulates knowledge distillation as an entropy-regularized
  value optimization problem and extends it with constrained reinforcement learning
  to allow the student to dynamically decide when to call the teacher during inference.
---

# Knowledge Distillation with Training Wheels

## Quick Facts
- arXiv ID: 2502.17717
- Source URL: https://arxiv.org/abs/2502.17717
- Reference count: 19
- Primary result: Introduces student model that learns when to request teacher help, achieving better latency-accuracy trade-offs than speculative decoding on translation/summarization tasks

## Executive Summary
This paper introduces a novel knowledge distillation approach where the student model learns to generate some tokens autonomously and request teacher help for others, guided by natural language instructions specifying test-time teacher-use constraints. The method reformulates knowledge distillation as an entropy-regularized value optimization problem and extends it with constrained reinforcement learning to allow the student to dynamically decide when to call the teacher during inference. Experiments on translation and summarization tasks show that this approach achieves better latency-accuracy trade-offs than speculative decoding, unlocking operating points with lower latency while maintaining output quality.

## Method Summary
The approach uses a two-phase training procedure. Phase 1 involves behavior cloning where an oracle marks high-KL-divergence positions for teacher calls and trains student distribution alignment elsewhere. Phase 2 uses RL finetuning via Path Consistency Learning to correct exposure bias and enforce budget constraints through reward shaping. The student learns to associate natural language budget keywords with appropriate teacher-call frequencies, enabling a single model to serve multiple teacher-use budgets at test time.

## Key Results
- Student model successfully learns to adhere to teacher-use budgets with high fidelity
- Achieves better latency-accuracy trade-offs than speculative decoding, particularly at lower latency operating points
- Unlocks efficiency gains by avoiding unnecessary token rejection and resampling
- Single model can serve multiple budget constraints via natural language instructions

## Why This Works (Mechanism)

### Mechanism 1
Reformulating reverse-KL knowledge distillation as entropy-regularized value optimization enables stable training with both on-policy and off-policy demonstrations without additional stabilization techniques. The KL term decomposes into reward (log p from teacher) and entropy regularization (log π from student). Path Consistency Learning enforces consistency constraints over trajectories from any distribution, naturally incorporating teacher demonstrations without importance weighting.

### Mechanism 2
Two-phase training teaches the student both when to request teacher help (via KL-ranking oracle) and how to generate quality tokens autonomously (via RL finetuning). Phase 1 behavior cloning uses an oracle that marks high-KL-divergence positions for teacher calls. Phase 2 RL finetuning corrects exposure bias from teacher-trajectory dependence and enforces budget constraints through reward shaping.

### Mechanism 3
Constrained RL with Lagrangian relaxation enables a single student model to serve multiple teacher-use budgets specified via natural language instructions at test time. The constraint value function tracks cumulative teacher-use deviation from budget. Lagrangian parameter λ adjusts dynamically—increasing when constraints are violated, decreasing toward zero when satisfied.

## Foundational Learning

- **Reverse vs. Forward KL Divergence in generative modeling**: Why needed here - The paper explicitly chooses reverse-KL formulation (mode-seeking) over forward-KL (mode-covering) for KD, which affects what student learns from teacher distribution. Quick check: Can you explain why reverse-KL tends to collapse to major modes while forward-KL covers support more broadly?

- **Path Consistency Learning (PCL) for entropy-regularized MDPs**: Why needed here - PCL is the core optimization algorithm enabling off-policy demonstration use—understanding it is prerequisite to implementing the training loop. Quick check: How does PCL differ from standard actor-critic in handling off-policy trajectories?

- **Lagrangian relaxation for constrained optimization**: Why needed here - The teacher-use budget constraint is enforced via primal-dual updates with learned λ; misunderstanding this leads to incorrect reward shaping. Quick check: What happens to λ when constraints are consistently satisfied vs. consistently violated?

## Architecture Onboarding

- **Component map**: Input (X_{<i}, Y with budget instruction) → Student Model π (FLAN-T5-SMALL) → Token decision: x_i ∈ V ∪ {<τ>} → If <τ> → Teacher Model p (FLAN-T5-XL) samples token → Else → Use student token directly → Output X^{(out)} (teacher tokens replace <τ> calls)

- **Critical path**: Phase 1 behavior cloning (200 batches) → Phase 2 RL finetuning with PCL loss (5000 batches) → Checkpoint selection on validation set satisfying all budget constraints within tolerance δ

- **Design tradeoffs**: Speculative decoding vs. learned budget approach (speculative has hard efficiency floor of 1/(K+1) teacher use, this method can go arbitrarily low); reward normalization and clipping trades gradient signal for stability; single model serving all budgets reduces deployment complexity but requires careful instruction tuning

- **Failure signatures**: Student calls teacher for nearly all tokens (λ not learning properly or Phase 1 failed to teach <τ> semantics); student ignores budget instructions (instruction embedding not being utilized, or constraint weight too weak); quality degrades sharply with low budgets (Phase 2 RL overfit to easy trajectories, insufficient exploration)

- **First 3 experiments**:
  1. **Ablation: Phase 1 duration** - Train with 0, 100, 200, 400 batches of behavior cloning before Phase 2. Measure: final constraint adherence, ROUGE-2. Expect: too few batches → <τ> unlearned; too many → exposure bias.
  2. **Single-budget vs. multi-budget training** - Train separate models for each budget vs. single model with instructions. Measure: quality-latency curves, constraint fidelity. Expect: single model slightly worse per-budget but lower deployment cost.
  3. **Teacher sampling strategy during training** - Compare greedy vs. temperature-sampled teacher tokens when <τ> called. Measure: output diversity, constraint adherence. Hypothesis: non-greedy sampling acts as augmentation but may slow λ convergence.

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework be extended to multi-agent decoding setups where a student arbitrates between multiple expert sources (models or tools) with differing accuracy and latency characteristics? The authors state the framework can be extended to multi-agent decoding setups with a light-weight student model acting as the arbitrator between multiple sources of expertise, but only examine single teacher-student pairs.

### Open Question 2
Why does the approach underperform DistillSpec at higher latency operating points for summarization but not translation, and can this task-specific gap be closed? The results show DistillSpec is "more than competitive" at higher latencies for summarization while the approach is strictly better for translation, but the paper doesn't explain this asymmetry.

### Open Question 3
How robust is the natural language instruction mechanism to phrasing variations or novel constraint types not seen during training? The method relies on instruction prefixes like "With light teacher use, summarize:" to specify budgets, but only six fixed phrasings are tested. Generalization to new phrasings or constraint types is unexplored.

## Limitations
- The KL-divergence ranking from teacher trajectories may not transfer well to student-generated trajectories due to capacity gap
- The instruction-conditioned multi-budget learning hasn't been validated for robustness to phrasing variations or out-of-distribution budget instructions
- The constrained RL formulation's sensitivity to hyperparameters like λ learning rate and reward normalization parameters is not fully characterized

## Confidence

**High Confidence**: The reformulation of knowledge distillation as entropy-regularized value optimization with PCL is mathematically sound and provides a principled framework for incorporating off-policy demonstrations.

**Medium Confidence**: The two-phase training approach is theoretically justified by exposure bias concerns, but the specific hyperparameters and their sensitivity to dataset size and model capacity are not fully explored.

**Low Confidence**: The instruction-conditioned multi-budget learning is novel but lacks comprehensive validation for robustness to phrasing variations and generalization to new constraint types.

## Next Checks

1. **Constraint adherence sensitivity analysis**: Systematically vary Phase 1 duration (0-400 batches) and measure final teacher-use fraction across all budgets to reveal whether Phase 1 provides necessary initialization.

2. **Instruction robustness test**: Evaluate with paraphrased budget instructions (e.g., "minimal teacher use" instead of "light teacher use") to assess generalization of the instruction-conditioned learning.

3. **Single-budget vs. multi-budget comparison**: Train separate models optimized for each individual budget and compare their quality-latency curves against the single multi-budget model to quantify the efficiency cost of instruction-conditioned learning.