---
ver: rpa2
title: Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large
  Language Models
arxiv_id: '2502.09659'
source_url: https://arxiv.org/abs/2502.09659
tags:
- adjuvant
- vaccine
- cancer
- adjuvants
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied large language models (LLMs) to automatically
  identify cancer vaccine adjuvant names from biomedical literature, addressing the
  challenge of manual curation in a rapidly expanding research field. Using GPT-4o
  and Llama 3.2 in zero- and few-shot learning paradigms on two datasets (97 clinical
  trials and 290 PubMed abstracts), the models achieved high precision and improved
  recall and F1-scores when contextual information like interventions was included.
---

# Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models

## Quick Facts
- arXiv ID: 2502.09659
- Source URL: https://arxiv.org/abs/2502.09659
- Reference count: 7
- Large language models (GPT-4o, Llama 3.2) effectively recognize cancer vaccine adjuvant names from biomedical literature, achieving up to 77.32% F1-score on PubMed abstracts and 81.67% on clinical trials when contextual information is provided.

## Executive Summary
This study addresses the challenge of manually curating cancer vaccine adjuvant names from rapidly expanding biomedical literature by applying large language models (LLMs) in zero- and few-shot learning paradigms. Using GPT-4o and Llama 3.2 on two datasets (97 clinical trials and 290 PubMed abstracts), the models achieved high precision (100%) and improved recall and F1-scores when contextual information like interventions was included. Manual validation further improved recall by correcting automated mismatches, demonstrating LLMs' effectiveness in recognizing adjuvant names, including rare and novel variations, and highlighting their potential to enhance cancer vaccine development by enabling efficient extraction of insights from large-scale biomedical data.

## Method Summary
The study applied zero-shot and few-shot prompting to GPT-4o and Llama-3.2-3B-Instruct models for named entity recognition of cancer vaccine adjuvants. Two datasets were used: 97 clinical trial records from AdjuvareDB and 290 PubMed abstracts from VAC database. Models were tested with and without contextual information (interventions/substances) across 0-4 shot scenarios. Outputs were post-processed into TSV format and evaluated via automated dictionary matching followed by manual validation of mismatches by domain experts.

## Key Results
- GPT-4o achieved maximum F1-scores of 77.32% on PubMed abstracts and 81.67% on clinical trials with contextual information
- Incorporating interventions/substances improved recall from 50.58% to 69.02% (3-shot) on clinical trials
- Manual validation corrected automated mismatches, recovering valid adjuvants absent from the gold-standard dictionary
- GPT-4o outperformed Llama-3.2-3B by approximately 2% F1-score on PubMed abstracts

## Why This Works (Mechanism)

### Mechanism 1
Contextual enrichment via interventions/substances improves adjuvant name extraction by providing structured metadata that constrains the search space, reducing false negatives while maintaining precision. The metadata fields contain adjuvant-related terms that guide attention to relevant text spans. If intervention/substance fields are missing, sparse, or contain irrelevant noise, the contextual signal degrades and may introduce false positives.

### Mechanism 2
Few-shot prompting incrementally improves recall without sacrificing precision by providing task-specific inductive bias through example pairs, helping the model learn the boundary between adjuvant names and other biomedical entities. If examples are poorly chosen (atypical adjuvants, inconsistent formatting), the model may overfit to example patterns and miss valid entities.

### Mechanism 3
Manual validation recovers valid adjuvants missed by automated exact-match evaluation because LLMs identify adjuvants using semantic understanding, producing valid synonyms or variants absent from the gold-standard dictionary. If validator expertise is inconsistent or the gold standard is severely incomplete, manual validation introduces subjectivity and potential labeling drift.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: The task requires extracting specific entity types (adjuvant names) from unstructured biomedical text.
  - Quick check question: Can you distinguish between an adjuvant (e.g., "GM-CSF") and a vaccine antigen (e.g., "gp100 peptide") in a clinical trial summary?

- Concept: Zero-shot vs. Few-shot Prompting
  - Why needed here: The study evaluates performance across 0-4 example shots; understanding this paradigm is essential for interpreting the results.
  - Quick check question: If you provide two adjuvant examples in a prompt, what type of learning paradigm are you using?

- Concept: Precision-Recall Trade-off in Biomedical Extraction
  - Why needed here: The paper reports 100% Precision with imperfect Recall; understanding why matters for deployment decisions.
  - Quick check question: In a curation pipeline, would you prioritize high Precision (fewer false positives) or high Recall (fewer false negatives), and why?

## Architecture Onboarding

- Component map: Raw text + optional metadata -> Task specification + instructions + output format + few-shot examples -> GPT-4o/Llama-3.2-3B -> TSV extraction, deduplication, "Done" marker validation -> Automated dictionary matching -> Manual review for mismatches

- Critical path: Prompt design -> Context inclusion decision -> Model selection -> Postprocessing -> Two-stage validation

- Design tradeoffs:
  - GPT-4o vs. Llama-3.2-3B: GPT-4o offers higher Recall/F1; Llama offers local deployment and cost control but lower performance
  - With vs. without context: Context improves Recall but requires metadata availability and increases prompt token cost
  - Automated vs. manual validation: Automated is fast but brittle to synonyms; manual is accurate but labor-intensive

- Failure signatures:
  - Low Recall with high Precision: Model is conservative; likely missing valid adjuvants not in examples
  - Output format drift: Llama-3.2-1B struggled with consistent TSV formatting
  - Hallucinated adjuvants: Possible if temperature is too high (mitigated by setting to 0.0001)

- First 3 experiments:
  1. Reproduce zero-shot baseline on VAC dataset without substances; measure Precision/Recall/F1
  2. Add 3-shot examples with interventions to the same dataset; quantify Recall gain
  3. Run automated validation followed by manual spot-check (10% sample) to identify synonym gaps in the gold standard dictionary

## Open Questions the Paper Calls Out

- Can integrating Vaccine Ontology into data preprocessing and fine-tuning pipelines significantly improve semantic accuracy and contextual understanding of adjuvant extraction?
- Can larger, open-source LLM variants (e.g., Llama-3.3 70B) match or exceed the performance of proprietary models like GPT-4o in recognizing adjuvant names?
- Does the proposed framework maintain high precision and recall when applied to adjuvant extraction for infectious disease vaccines?
- How can evaluation protocols be refined to avoid penalizing models for identifying valid adjuvant names absent from static "gold standard" datasets?

## Limitations

- The study relies on a curated gold-standard dictionary for automated validation, but this dictionary and exact gold-standard annotations per record are not provided, limiting reproducibility
- Few-shot examples used across different shot counts are not specified, making it difficult to assess their representativeness or potential bias
- Llama-3.2-1B was excluded due to formatting issues, suggesting model-level variability that could affect generalizability

## Confidence

- High Confidence: Contextual enrichment improves recall (clear, reproducible results with consistent gains)
- Medium Confidence: GPT-4o outperforms Llama-3.2-3B (quantitative results supported, but limited by exclusion of Llama-3.2-1B)
- Low Confidence: Manual validation effectiveness (based on authors' domain expertise without independent verification)

## Next Checks

1. Acquire and inspect the curated dictionary and gold-standard annotations used for automated validation to assess completeness and potential bias
2. Obtain the exact few-shot examples used in the study to evaluate their representativeness and impact on model performance
3. Run an independent manual validation on a random sample of model outputs to verify the accuracy of the authors' manual review process