---
ver: rpa2
title: 'Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning'
arxiv_id: '2512.02874'
source_url: https://arxiv.org/abs/2512.02874
tags:
- reasoning
- answer
- arxiv
- think
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: THINKMERGE addresses the gap between parallel scaling benefits
  and open-ended tasks by introducing a training-free, plug-and-play decoding strategy
  that averages next-token logits across parallel reasoning traces at synchronization
  points to produce a single coherent output. Unlike majority voting, which is ill-defined
  for open-ended tasks like coding and web-based research, THINKMERGE enables the
  model to explore diverse reasoning paths and converge on reliable answers without
  requiring complete solution voting.
---

# Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning

## Quick Facts
- arXiv ID: 2512.02874
- Source URL: https://arxiv.org/abs/2512.02874
- Authors: Haonan Wang; Chao Du; Kenji Kawaguchi; Tianyu Pang
- Reference count: 16
- Key outcome: THINKMERGE introduces a training-free, plug-and-play decoding strategy that averages next-token logits across parallel reasoning traces to produce coherent outputs for open-ended tasks, improving performance on coding and web-based research benchmarks without requiring complete solution voting.

## Executive Summary
THINKMERGE addresses a fundamental gap in parallel test-time scaling: while running multiple reasoning traces in parallel improves performance on closed-ended tasks through majority voting, this approach fails for open-ended tasks where outputs cannot be meaningfully compared. The method introduces a novel decoding strategy that averages next-token logits across parallel traces at synchronization points, enabling models to explore diverse reasoning paths while converging on reliable answers. By maintaining compatibility with standard decoding techniques and implementing seamlessly with vLLM/SGLang, THINKMERGE makes parallel scaling benefits accessible for open-ended reasoning tasks like coding and web-based research.

## Method Summary
THINKMERGE runs K parallel reasoning traces that explore diverse reasoning paths independently. At predefined delimiter boundaries, the method synchronizes by averaging the pre-softmax logits from all parallel traces, then decodes a single shared answer using this averaged distribution. This approach allows the model to benefit from parallel exploration while maintaining a single coherent output stream. The method is training-free and plug-and-play, requiring no model fine-tuning. It supports standard decoding techniques like Top-p/Top-k sampling and implements efficiently with existing inference frameworks like vLLM and SGLang, enabling both online and batch inference modes.

## Key Results
- Matches or slightly surpasses majority voting on closed-ended tasks (AIME, GPQA)
- Delivers consistent gains on open-ended coding tasks: +8.28% pass@1 on LiveCodeBench (hard) for DeepCoder-14B-Preview and +7.58% for Qwen3-8B
- Improves web-based deep-research agents: up to +10.2% on XbenchDeepSearch and +4.8% on GAIA for WebSailor-7B/32B

## Why This Works (Mechanism)
THINKMERGE leverages parallel exploration to capture diverse reasoning paths while using logit averaging to synthesize these perspectives into a single coherent output. Unlike majority voting which requires complete solutions for comparison, logit averaging operates at the token level, making it suitable for open-ended tasks where outputs vary significantly. The synchronization at delimiter boundaries provides natural coordination points without disrupting the flow of reasoning. By averaging pre-softmax logits rather than post-softmax probabilities, the method preserves the model's learned confidence calibration while combining information across parallel traces.

## Foundational Learning
**Parallel Test-Time Scaling**: Running multiple reasoning traces in parallel to improve answer quality. Why needed: Single reasoning traces can get stuck in local optima or miss valid solution paths. Quick check: Compare performance of 1 vs K parallel traces on a benchmark.

**Logit Averaging vs Voting**: Combining model outputs by averaging next-token logits rather than selecting majority answers. Why needed: Voting requires comparable complete outputs, which is impossible for open-ended tasks. Quick check: Verify that averaging pre-softmax logits produces better results than averaging post-softmax probabilities.

**Delimiter-Based Synchronization**: Using task-specific delimiters to coordinate parallel traces. Why needed: Provides natural synchronization points without requiring complex coordination mechanisms. Quick check: Test different delimiter choices to find optimal synchronization frequency.

**Open-Ended vs Closed-Ended Tasks**: Understanding when majority voting applies (closed-ended) versus when alternative aggregation methods are needed (open-ended). Why needed: Different task types require fundamentally different evaluation and aggregation strategies. Quick check: Categorize tasks by whether outputs can be meaningfully compared.

## Architecture Onboarding

**Component Map**: Input Task -> K Parallel Traces -> Token-by-Token Generation -> Delimiter Detection -> Logit Averaging -> Single Output Stream -> Final Answer

**Critical Path**: The most timing-sensitive component is the delimiter detection and logit averaging operation, as this must happen in real-time during inference to maintain throughput. The parallel trace generation must complete within acceptable latency bounds for the method to be practical.

**Design Tradeoffs**: The method trades increased computational cost (KÃ— inference) for improved answer quality. The choice of K involves balancing marginal gains against resource constraints. Delimiter selection affects both performance and efficiency - too frequent causes overhead, too infrequent reduces averaging benefits.

**Failure Signatures**: Poor delimiter choices lead to either insufficient averaging opportunities or excessive synchronization overhead. Insufficient parallelism (low K) fails to capture diverse reasoning paths. Incorrect logit averaging implementation (e.g., averaging post-softmax) degrades performance significantly.

**3 First Experiments**:
1. Measure performance vs K for a representative coding task to identify diminishing returns
2. Compare logit averaging at different delimiter positions within the same task
3. Test the method on both closed-ended and open-ended variants of the same task type

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness on extremely hard open-ended tasks remains uncertain, as most gains are on moderately difficult benchmarks
- Computational overhead of running K parallel traces may limit practical deployment in resource-constrained environments
- Reliance on token-level averaging at delimiter boundaries may introduce artifacts in tasks requiring very long or structurally complex reasoning chains

## Confidence

**High Confidence**: The core technical claim that logit averaging across parallel traces can improve open-ended reasoning performance is well-supported by empirical results across multiple benchmarks and model sizes.

**Medium Confidence**: Claims about THINKMERGE's superiority over majority voting for open-ended tasks are supported but could benefit from more extensive ablation studies.

**Low Confidence**: The paper's claims about THINKMERGE being "training-free" and "plug-and-play" should be tempered by the observation that optimal performance depends on careful selection of synchronization points and parallel trace count.

## Next Checks

1. **Cross-task generalization test**: Evaluate THINKMERGE on a diverse set of open-ended tasks spanning different domains (mathematics, creative writing, scientific research) to assess whether improvements generalize beyond coding and web-based research tasks.

2. **Scaling behavior analysis**: Systematically measure how THINKMERGE's performance scales with the number of parallel traces (K) and model size, identifying diminishing returns and computational thresholds where benefits no longer justify costs.

3. **Long-context reasoning evaluation**: Test THINKMERGE on tasks requiring reasoning chains exceeding 8K tokens to verify that delimiter-based synchronization doesn't introduce coherence degradation in extended reasoning scenarios.