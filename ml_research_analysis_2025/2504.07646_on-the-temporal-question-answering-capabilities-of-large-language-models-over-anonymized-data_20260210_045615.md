---
ver: rpa2
title: On the Temporal Question-Answering Capabilities of Large Language Models Over
  Anonymized Data
arxiv_id: '2504.07646'
source_url: https://arxiv.org/abs/2504.07646
tags:
- task
- data
- temporal
- reasoning
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates temporal question-answering (TQA) over anonymized
  data using Large Language Models (LLMs). The authors identify 17 common temporal
  reasoning task types, develop an API of 17 associated functions, and create the
  RATA dataset with 5,850 anonymized questions.
---

# On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data

## Quick Facts
- arXiv ID: 2504.07646
- Source URL: https://arxiv.org/abs/2504.07646
- Reference count: 40
- The paper investigates temporal question-answering over anonymized data using LLMs, finding that external tools significantly improve accuracy from 59% to 93%.

## Executive Summary
This paper investigates the temporal question-answering capabilities of large language models when dealing with anonymized temporal data. The authors identify 17 common temporal reasoning task types, develop an API of 17 associated functions, and create the RATA dataset with 5,850 anonymized questions. They compare eight LLM-based methods, including Chain-of-Thought, Tree-of-Thought, and approaches using external code execution or a pre-defined function set. The results show that while standalone LLM reasoning performs poorly (59% accuracy), using external tools like the function set (CoTAPI) significantly improves accuracy to 93%, demonstrating the importance of combining LLM semantic analysis with deterministic external functions for effective temporal reasoning.

## Method Summary
The study employs a systematic approach to evaluate LLM performance in temporal question-answering over anonymized data. The authors first identify 17 temporal reasoning task types and develop a corresponding API of 17 functions. They then create the RATA dataset with 5,850 anonymized questions across these task types. Eight LLM-based methods are evaluated, including Chain-of-Thought (CoT), Tree-of-Thought (ToT), and various approaches that incorporate external tools. The methods are compared based on accuracy and execution time as data size increases. The best-performing method combines LLM reasoning with a pre-defined function set, achieving 93% accuracy while maintaining constant execution time as data size scales.

## Key Results
- Standalone LLM reasoning achieves only 59% accuracy on temporal question-answering tasks
- Using external tools like the function set (CoTAPI) improves accuracy to 93%
- The best-performing method maintains constant execution time as data size increases, demonstrating scalability
- CoTAPI outperforms other approaches including Chain-of-Thought, Tree-of-Thought, and code execution methods

## Why This Works (Mechanism)
The effectiveness of combining LLMs with external tools stems from the complementary strengths of both approaches. LLMs excel at semantic understanding and natural language processing, making them adept at interpreting complex temporal questions and mapping them to appropriate task types. However, LLMs struggle with precise temporal calculations and reasoning due to their probabilistic nature and potential for hallucinations. By leveraging deterministic external functions for the actual temporal computations, the system ensures accuracy while still benefiting from the LLM's ability to understand and parse natural language questions. This hybrid approach capitalizes on the LLM's strengths in semantic analysis while mitigating its weaknesses in exact temporal reasoning.

## Foundational Learning

**Temporal Reasoning Task Types** (Why needed: To systematically categorize the diverse ways temporal questions can be formulated and answered; Quick check: Verify that all common temporal reasoning patterns are covered by the 17 identified types)

**Data Anonymization Techniques** (Why needed: To protect sensitive temporal information while preserving the semantic structure needed for question-answering; Quick check: Ensure anonymized data retains enough context for meaningful temporal reasoning)

**LLM Function Integration** (Why needed: To combine the semantic understanding capabilities of LLMs with the precision of deterministic functions; Quick check: Confirm that the function set covers all identified temporal reasoning task types)

## Architecture Onboarding

**Component Map**: LLM (Question Parser) -> Function Selector -> External Function API -> Answer Generator

**Critical Path**: Question parsing by LLM → Function type identification → External function execution → Answer formatting and return

**Design Tradeoffs**: The main tradeoff is between the flexibility and semantic understanding of LLMs versus the precision and reliability of deterministic functions. Using external tools increases accuracy but adds complexity and potential latency. The system prioritizes accuracy over pure LLM-based reasoning to ensure reliable temporal question-answering.

**Failure Signatures**: LLM hallucination leading to incorrect function selection, incomplete anonymization causing privacy breaches, function API errors resulting in incorrect calculations, and latency issues due to external tool integration.

**3 First Experiments**:
1. Test the LLM's ability to correctly identify temporal reasoning task types from a sample of anonymized questions
2. Evaluate the accuracy of individual external functions on their respective temporal reasoning tasks
3. Measure the end-to-end accuracy of the CoTAPI approach on a subset of the RATA dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The RATA dataset contains only 5,850 questions across 17 task types, which may not capture the full complexity of real-world temporal reasoning scenarios
- The study focuses on anonymized data, which may not fully represent the challenges of handling temporal information in its original context
- The evaluation is limited to 8 specific LLM-based methods, potentially missing other promising approaches

## Confidence
- **High confidence**: The identification of 17 temporal reasoning task types and the creation of the RATA dataset with anonymized questions. The methodology for anonymizing temporal data is clearly described and reproducible.
- **Medium confidence**: The performance comparison between different LLM approaches (CoT, ToT, CoTAPI, etc.) and the conclusion that external tools significantly improve accuracy. While results are consistent, the small dataset size (5,850 questions) may limit generalizability.
- **Medium confidence**: The scalability claim regarding constant execution time. While demonstrated on the RATA dataset, this needs validation on larger, more diverse datasets.

## Next Checks
1. **Dataset Expansion**: Validate the findings on a larger, more diverse temporal question-answering dataset (e.g., 50,000+ questions) to confirm the scalability claims and assess generalizability across different domains and languages.

2. **Real-World Application Testing**: Apply the CoTAPI approach to a real-world temporal question-answering system handling non-anonymized data to evaluate its effectiveness in practical scenarios and identify potential limitations.

3. **Cross-Domain Transferability**: Test the 17 temporal reasoning task types and associated functions on different domains (e.g., medical records, financial data, historical events) to assess the method's applicability beyond the current dataset.