---
ver: rpa2
title: 'VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for Efficient
  RAG'
arxiv_id: '2504.08930'
source_url: https://arxiv.org/abs/2504.08930
tags:
- search
- latency
- vector
- index
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VECTORLITERAG addresses resource contention between vector search
  and LLM inference in GPU-based RAG systems by introducing a latency-aware, fine-grained
  index partitioning mechanism. It exploits query access skew in IVF indexes to cache
  hot clusters on GPU while keeping cold clusters on CPU, using a performance model
  to determine optimal partitioning under latency constraints.
---

# VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for Efficient RAG

## Quick Facts
- **arXiv ID**: 2504.08930
- **Source URL**: https://arxiv.org/abs/2504.08930
- **Reference count**: 40
- **Primary result**: Up to 1.5× improvement in SLO-compliant throughput across various dataset and model configurations without compromising generation quality or requiring additional hardware.

## Executive Summary
VectorLiteRAG addresses resource contention between vector search and LLM inference in GPU-based RAG systems by introducing a latency-aware, fine-grained index partitioning mechanism. It exploits query access skew in IVF indexes to cache hot clusters on GPU while keeping cold clusters on CPU, using a performance model to determine optimal partitioning under latency constraints. A dynamic dispatcher and routing system minimize tail latency by advancing early-completing queries.

## Method Summary
The system profiles CPU search latency and cluster access patterns, models latency as piecewise linear, estimates tail hit rates via Beta distribution, and uses binary search to find optimal GPU cache coverage. Hot clusters are split to GPU shards with routing via mapping tables, while a dynamic dispatcher advances early-completing queries. The method is implemented as modifications to FAISS and vLLM, requiring specific compiler versions and CUDA toolkit.

## Key Results
- Up to 1.5× improvement in SLO-compliant throughput
- 16% reduction in search latency via dynamic dispatcher
- Effective across Wiki-All (88M vectors) and ORCAS datasets with various LLM sizes

## Why This Works (Mechanism)

### Mechanism 1: Access-Skew-Aware Tiered Index Partitioning
Caching frequently-accessed clusters on GPU while keeping cold clusters on CPU accelerates retrieval without exhausting GPU memory. The system profiles cluster access frequencies, identifies hot clusters (top 20% accounting for ~60-93% of accesses), and partitions the IVF index accordingly. Core assumption: query access patterns exhibit stable skew. Break condition: skew disappears or hot set exceeds GPU memory.

### Mechanism 2: Latency-Bounded Performance Modeling for Partitioning
An analytical model combining CPU search latency profiles and hit rate distribution estimates computes minimal GPU cache coverage needed to meet target search SLO. The system profiles CPU latency as piecewise-linear in batch size, models per-query hit rates using Beta distribution, and computes expected minimum hit rate within batch. Break condition: access pattern drift invalidates hit rate model.

### Mechanism 3: Variance-Aware Routing and Dynamic Dispatching
Routing queries only to shards holding relevant clusters and dispatching early-completing queries reduces tail latency. The router uses mapping tables to send each query only to GPU shards with resident clusters. A dispatcher thread polls GPU worker completion flags and CPU search callbacks, immediately merging CPU/GPU results when queries complete. Break condition: uniform hit rates eliminate early completers.

## Foundational Learning

- **Concept: IVF Index Structure and Search Stages**
  - Why needed: VectorLiteRAG's tiered design depends on understanding IVF's cluster-based organization, coarse quantization, nprobe, and lookup table operations.
  - Quick check: Which IVF search stage does VectorLiteRAG offload to GPU, and why is CQ kept on CPU?

- **Concept: Access Skew and Hit Rate Variance**
  - Why needed: The system exploits skew to justify caching and models variance to predict worst-case batch latency.
  - Quick check: In ORCAS, what percentage of clusters account for over 90% of accesses?

- **Concept: GPU Memory Contention Between KV Cache and Vector Index**
  - Why needed: The core constraint is that allocating GPU memory to index shrinks KV cache space, reducing LLM throughput.
  - Quick check: According to Figure 4 (right), what happens to normalized LLM throughput as relative KV space decreases?

## Architecture Onboarding

- **Component map**: Offline: Profiler → Latency Model → Hit Rate Estimator → Partitioning Algorithm → Index Splitter → GPU Shards + Mapping Tables. Runtime: Coarse Quantizer (CPU) → Router → GPU Workers (hot clusters) + CPU Workers (cold clusters) → Dynamic Dispatcher → Result Merger → LLM.

- **Critical path**: Coarse quantization → Router dispatches based on mapping tables → Parallel GPU LUT scan (hot) + sequential CPU scan (cold) → Dispatcher polls completions → Early queries merged/forwarded → Final top-k → LLM prefill/decode.

- **Design tradeoffs**: GPU memory allocation vs. KV cache size; batch size vs. tail latency; update frequency vs. overhead.

- **Failure signatures**: SLO violations under high load; skew disappearance degrading performance; memory exhaustion from oversized hot cluster cache.

- **First 3 experiments**:
  1. Baseline comparison across CPU-only, DED-GPU, ALL-GPU, and VectorLiteRAG under increasing arrival rates.
  2. Skew sensitivity comparison between Wiki-All and ORCAS hit rate distributions.
  3. Dispatcher ablation enabling/disabling dynamic dispatcher to measure latency impact.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the latency-aware partitioning model be extended to disaggregated architectures where LLM prefill and decode stages run on separate hardware?
- **Open Question 2**: Can the tiered caching and dispatching mechanism be effectively adapted for graph-based indices like HNSW?
- **Open Question 3**: How robust is the hit-rate estimator under adversarial workloads or rapid concept drift that violates Beta distribution assumptions?
- **Open Question 4**: Does the resource partitioning strategy scale effectively in multi-tenant environments with multiple RAG applications contending for GPU memory?

## Limitations
- Relies on stable access skew patterns that may not hold in dynamic real-world scenarios
- Beta distribution approximation for hit rate variance lacks direct empirical validation
- Generalizability across different embedding spaces and query distributions remains unvalidated

## Confidence
- **High Confidence**: Exploiting IVF cluster access skew for GPU caching (supported by empirical evidence showing clear skew patterns)
- **Medium Confidence**: Latency-bounded performance modeling using Beta distributions (reasonable approximation but lacks direct validation)
- **Low Confidence**: Generalizability of optimal partitioning ratios across different dataset characteristics (unexplored in evaluation)

## Next Checks
1. **Access Pattern Drift Test**: Implement sliding window profiler to measure degradation under artificially introduced query pattern shifts and assess dynamic repartitioning overhead.

2. **Dispatcher Overhead Characterization**: Instrument dispatcher to measure polling overhead and context switching costs under varying batch sizes and completion rates.

3. **Generalizability Study**: Apply partitioning algorithm to additional datasets with different characteristics to measure how optimal cache coverage ratio varies with dataset properties.