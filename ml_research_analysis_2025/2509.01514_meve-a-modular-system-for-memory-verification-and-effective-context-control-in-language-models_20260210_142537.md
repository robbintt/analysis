---
ver: rpa2
title: 'MeVe: A Modular System for Memory Verification and Effective Context Control
  in Language Models'
arxiv_id: '2509.01514'
source_url: https://arxiv.org/abs/2509.01514
tags:
- context
- meve
- retrieval
- phase
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MeVe addresses the problem of irrelevant or redundant information
  in Retrieval-Augmented Generation (RAG) systems by proposing a five-phase modular
  architecture: initial retrieval, relevance verification, fallback retrieval, context
  prioritization, and token budgeting. The core method idea is to break down the RAG
  pipeline into distinct, auditable, and independently tunable phases to improve control
  over knowledge passed to language models.'
---

# MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models

## Quick Facts
- arXiv ID: 2509.01514
- Source URL: https://arxiv.org/abs/2509.01514
- Authors: Andreas Ottem
- Reference count: 26
- Primary result: 57% context token reduction on Wikipedia, 75% on HotpotQA, with minimal latency overhead

## Executive Summary
MeVe introduces a modular five-phase architecture to address irrelevant or redundant information in Retrieval-Augmented Generation (RAG) systems. By breaking down the RAG pipeline into distinct, auditable phases—initial retrieval, relevance verification, fallback retrieval, context prioritization, and token budgeting—MeVe achieves significant efficiency gains while maintaining control over the knowledge passed to language models. The system demonstrates robust performance on Wikipedia and HotpotQA datasets, reducing context tokens by 57% and 75% respectively compared to standard RAG implementations.

## Method Summary
MeVe's method involves a five-phase pipeline: (1) kNN retrieval using a bi-encoder model, (2) relevance verification using a cross-encoder to filter semantically irrelevant documents, (3) BM25 fallback retrieval if insufficient verified candidates remain, (4) context prioritization to remove redundancy via embedding similarity, and (5) token budgeting using greedy packing to fit unique, high-priority segments into a strict token limit. The system was evaluated on a Wikipedia subset and HotpotQA using proxy metrics for context efficiency and retrieval time.

## Key Results
- Achieved 57% reduction in context tokens for Wikipedia dataset compared to standard RAG
- Achieved 75% reduction in context tokens for HotpotQA dataset
- Maintained minimal latency overhead (~0.1s added for verification step)

## Why This Works (Mechanism)

### Mechanism 1: Explicit Relevance Filtering via Cross-Encoder
- **Claim:** Replacing implicit retrieval relevance with explicit verification reduces context pollution by filtering semantically similar but factually irrelevant documents.
- **Mechanism:** A cross-encoder model scores candidate documents against the query, discarding those below a threshold before they reach the LLM.
- **Core assumption:** The cross-encoder is sufficiently aligned with the domain's definition of "relevance" to act as a strict gatekeeper without discarding critical information.
- **Evidence anchors:** Ablation study shows "NO VERIFICATION" leads to significantly higher context token counts, validating the filter's role in efficiency.
- **Break condition:** If the threshold is set too high for the specific domain, the system may over-filter, resulting in empty or near-empty contexts.

### Mechanism 2: Hybrid Search Resilience via Fallback
- **Claim:** A secondary retrieval mechanism mitigates the "semantic drift" or failure cases of pure dense vector retrieval.
- **Mechanism:** If insufficient verified candidates remain, a BM25 search is triggered to ensure the system recovers when semantic embedding fails to match explicit query terms.
- **Core assumption:** Keyword overlap provides a viable safety net for queries where semantic embeddings fail.
- **Evidence anchors:** "NO FALLBACK" mode resulted in zero context for queries where verification failed, proving the safety net's functional utility.
- **Break condition:** If the query uses complex phrasing devoid of specific keywords, the lexical fallback may retrieve irrelevant documents based on term frequency alone.

### Mechanism 3: Information Density Maximization
- **Claim:** Decoupling retrieval quantity from context window size allows for higher information density per token.
- **Mechanism:** Phase 4 removes redundancy via embedding similarity, and Phase 5 uses greedy packing to fit unique, high-priority segments into the strict token limit.
- **Core assumption:** Redundancy (high embedding similarity) equates to low information utility.
- **Evidence anchors:** Empirical results show token reduction from 188.8 (Standard) to 79.8 (MeVe).
- **Break condition:** If distinct but semantically related facts are removed due to high similarity scores, the context may lose necessary nuance or contrasting details.

## Foundational Learning

- **Concept: Cross-Encoders vs. Bi-Encoders**
  - **Why needed here:** MeVe relies on this distinction for its Phase 1 (Bi-Encoder/Vector) vs. Phase 2 (Cross-Encoder) logic.
  - **Quick check question:** Why is a cross-encoder computationally more expensive but often more accurate for re-ranking than the initial vector search?

- **Concept: Hybrid Search (BM25 + Vector)**
  - **Why needed here:** Essential for understanding MeVe's Phase 3 Fallback mechanism.
  - **Quick check question:** In what specific scenario would a pure vector search fail where a keyword (BM25) search would succeed?

- **Concept: Context Window Pollution**
  - **Why needed here:** This is the core problem MeVe attempts to solve.
  - **Quick check question:** How does adding more irrelevant context negatively affect an LLM's reasoning capability (the "lost in the middle" or distraction phenomenon)?

## Architecture Onboarding

- **Component map:** Query -> Vector Search -> Verification (Primary Filter) -> Fallback (If |Cver| < 3) -> Prioritization (Dedup) -> Budgeting (Truncate to Tmax)
- **Critical path:** Query → Vector Search → Verification → Fallback → Prioritization → Budgeting
- **Design tradeoffs:**
  - **Latency vs. Precision:** The cross-encoder adds computational overhead (~0.1s) but reduces token processing costs downstream.
  - **Recall vs. Noise:** Strict verification thresholds reduce noise (efficiency) but increase the risk of "null context," triggering the potentially less relevant fallback.
- **Failure signatures:**
  - **"Empty Context" Loops:** If the verification threshold is too high for the corpus quality, the system constantly falls back to BM25, potentially ignoring the semantic nuance of the query.
  - **Over-Redundancy:** If redundancy threshold is too low, the system may strip away necessary context that shares vocabulary but conveys different facts.
- **First 3 experiments:**
  1. **Threshold Sensitivity Analysis:** Vary the verification threshold to plot the curve of Token Efficiency vs. "Empty Context" rate.
  2. **Fallback Relevance Audit:** Trigger the fallback mechanism manually and measure the semantic relevance of BM25 results vs. Vector results for the same queries to validate the hybrid search assumption.
  3. **Redundancy Visualizer:** Implement a logging step to visualize which document chunks are being discarded as redundant to ensure unique facts aren't being lost.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does MeVe's modular filtering affect final answer accuracy and hallucination rates when deployed with a generative Large Language Model?
- **Basis in paper:** [inferred] The evaluation simulates LLM responses via keyword overlap heuristics rather than using a generative model.
- **Why unresolved:** The "Context Grounding Proxy" used in the study measures retrieval potential, not the quality of actual generated text or the model's ability to synthesize the filtered context.
- **What evidence would resolve it:** An end-to-end evaluation measuring exact match accuracy and hallucination frequency using standard generative models (e.g., GPT-4, Llama 3) on the HotpotQA dataset.

### Open Question 2
- **Question:** Does adding a collective coherence scoring step improve the semantic alignment of the final context for complex queries?
- **Basis in paper:** [explicit] Section 6 states that "assurance of collective coherence... could come in the form of an additional post-prioritization filtering step."
- **Why unresolved:** Phase 4 currently prioritizes documents based on individual relevance scores and redundancy, but does not verify if the combined set forms a logically coherent block for the query intent.
- **What evidence would resolve it:** Ablation studies comparing the current prioritization module against a version utilizing cross-document coherence metrics on multi-hop reasoning tasks.

### Open Question 3
- **Question:** Can adaptive or domain-specific relevance thresholds mitigate the over-filtering observed with the static τ=0.5 setting?
- **Basis in paper:** [explicit] Section 6 notes that "adjustment to this threshold or the application of a domain-based cross-encoder would yield more relevant verified documents."
- **Why unresolved:** The fixed threshold of 0.5 was observed to be "quite stringent," often triggering fallback retrieval or resulting in zero verified documents for general knowledge queries.
- **What evidence would resolve it:** Experiments comparing static thresholds against dynamic thresholds (e.g., relative to the query embedding density) to optimize the balance between noise reduction and context retention.

## Limitations
- Evaluation relies heavily on proxy metrics rather than direct task performance measures, lacking demonstration that efficiency gains translate to improved end-task accuracy.
- Experiments are conducted on Wikipedia and HotpotQA only, leaving questions about performance on noisier, domain-specific corpora or languages beyond English.
- The Context Grounding Proxy is a heuristic that may not capture nuanced reasoning failures caused by aggressive filtering.

## Confidence

- **High Confidence:** The modular architecture design and its phase-by-phase description are clearly specified and reproducible. The observed token reductions (57% Wikipedia, 75% HotpotQA) are directly measurable from the described implementation.
- **Medium Confidence:** The core hypothesis that explicit relevance verification reduces context pollution is supported by ablation studies, but the extent of this benefit may be dataset-dependent.
- **Low Confidence:** The claim that MeVe's efficiency gains do not compromise end-task performance lacks direct evidence. The system's robustness to domain shifts or queries with complex phrasing is not demonstrated.

## Next Checks

1. **End-to-End Accuracy Validation:** Run MeVe's output context through a QA model (e.g., GPT-4, Llama-2) and measure answer accuracy on HotpotQA, comparing it directly against standard RAG to validate that token reduction does not degrade task performance.

2. **Cross-Dataset Generalization Test:** Apply MeVe to a non-Wikipedia, non-HotpotQA corpus (e.g., biomedical literature or legal documents) and measure both token efficiency and answer accuracy to assess domain robustness.

3. **Query Type Stress Test:** Construct a diverse query set (simple factoid, multi-hop, ambiguous, keyword-poor) and evaluate MeVe's fallback activation rate and the semantic relevance of fallback results to identify failure modes in complex or poorly specified queries.