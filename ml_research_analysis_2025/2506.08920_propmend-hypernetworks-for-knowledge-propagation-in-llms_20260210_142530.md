---
ver: rpa2
title: 'PropMEND: Hypernetworks for Knowledge Propagation in LLMs'
arxiv_id: '2506.08920'
source_url: https://arxiv.org/abs/2506.08920
tags:
- latexit
- knowledge
- propmend
- propagation
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PropMEND, a hypernetwork-based approach that
  significantly improves knowledge propagation in large language models (LLMs). Unlike
  existing knowledge editing methods that struggle to propagate injected information
  beyond verbatim reproduction, PropMEND meta-learns to modify gradients during training
  to enable multi-hop reasoning with injected facts.
---

# PropMEND: Hypernetworks for Knowledge Propagation in LLMs

## Quick Facts
- **arXiv ID:** 2506.08920
- **Source URL:** https://arxiv.org/abs/2506.08920
- **Reference count:** 40
- **Primary result:** PropMEND achieves 22.4% accuracy on non-verbatim multi-hop questions vs. 12.7% for next best method on RippleEdit benchmark.

## Executive Summary
PropMEND addresses the critical challenge of knowledge propagation in LLMs - moving beyond simple fact memorization to enable multi-hop reasoning with injected knowledge. Unlike existing methods that struggle to propagate injected information beyond verbatim reproduction, PropMEND uses a hypernetwork to meta-learn how to transform gradients from standard language modeling, enabling compositional reasoning. The approach extends MEND's framework by using propagation questions as the outer-loop objective and editing mid-to-upper layers rather than just top layers. On RippleEdit, PropMEND achieves nearly 2× accuracy on non-verbatim multi-hop questions compared to the next best method, though performance degrades significantly for unseen relations.

## Method Summary
PropMEND is a hypernetwork-based approach that meta-learns to modify gradients during training to enable multi-hop reasoning with injected facts. The method uses a bi-level optimization framework: the inner loop computes gradients on the injected fact using causal language modeling, while the outer loop optimizes propagation question-answering accuracy and locality preservation. A hypernetwork takes the decomposed gradient (δ, u) as input and outputs modified components (δ̃, ũ) that compose via outer product to form the transformed gradient. The approach targets mid-to-upper MLP layers rather than just top layers, finding this more effective for propagation. Training uses a balanced loss between propagation efficacy and specificity preservation, with early stopping based on validation performance.

## Key Results
- PropMEND achieves 22.4% accuracy on non-verbatim multi-hop questions vs. 12.7% for next best method on RippleEdit
- Mid-to-upper layer targeting (4-15) outperforms top-layer only editing for propagation tasks
- Using propagation questions as outer-loop loss (vs. paraphrases) is the most impactful design choice, with efficacy dropping from 56.7 to 10.6 when switched
- OOD performance degrades significantly for unseen relations (33.3% vs 64.0% in-domain), suggesting relation-specific learning

## Why This Works (Mechanism)

### Mechanism 1: Gradient Transformation for Propagation via Hypernetwork Meta-Learning
- **Claim:** PropMEND meta-learns to transform gradients from standard language modeling so that applying the modified gradient enables multi-hop reasoning with injected knowledge.
- **Mechanism:** A hypernetwork g_ϕ takes as input the gradient ∇W L_I computed on the injected fact f via causal language modeling. It outputs modified gradient components (ũ, δ̃) that compose via outer product to form ∇̃W. When this modified gradient is applied to the base LLM, the updated model can answer propagation questions that were not explicitly trained during the inner loop.
- **Core assumption:** The gradient from a simple next-token prediction loss contains sufficient signal to be transformed into an update that supports compositional reasoning, if appropriately reshaped by a learned transformation.
- **Evidence anchors:** [abstract] "We present a hypernetwork-based approach for knowledge propagation... where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate." [section 2.2] Describes MEND's observation that ∇W L_I is rank-1 decomposable as δu^T, enabling hypernetwork to operate on (u, δ) directly.
- **Break condition:** If the base model's gradient on f carries no information about related concepts (e.g., the model has never seen any entity linked to the injected fact), the hypernetwork cannot transform what is not there.

### Mechanism 2: Outer-Loop Propagation Loss Aligns Training with Inference
- **Claim:** Training the hypernetwork with propagation questions as the outer-loop objective—rather than paraphrases—is the most impactful design change for enabling knowledge propagation.
- **Mechanism:** The outer-loop loss L_e = -(1/P) Σ log p_W̃(a_i | q_i) is computed over P propagation question-answer pairs. This directly optimizes the hypernetwork to produce weight updates that make the model answer multi-hop questions correctly. The locality loss L_loc (KL divergence on unrelated inputs) prevents over-editing.
- **Core assumption:** Propagation behavior can be instilled by optimizing directly for it at meta-training time; the hypernetwork will generalize this to new facts and relations at test time.
- **Evidence anchors:** [section 3] "Critically, this loss encourages the trained hypernetwork to make modifications that enable the final model to correctly answer propagation questions. This property does not hold for basic MEND." [table 5] Ablation shows switching from "propagations → paraphrases" drops in-domain efficacy from 56.7 to 10.6 (LLM-Score).
- **Break condition:** If propagation questions at test time involve relation types or reasoning patterns not covered in meta-training, the hypernetwork's transformation may not generalize (OOD results in Table 2 show degradation).

### Mechanism 3: Mid-to-Upper Layer Targeting for Knowledge Propagation
- **Claim:** Editing mid-to-upper MLP layers (rather than top layers only) is more effective for propagation, suggesting factual knowledge and its compositional use may be distributed across middle layers.
- **Mechanism:** PropMEND applies the hypernetwork to MLP weights in layers 4–15 (Llama-1B) or 13–27 (Qwen-1.5B), rather than only the top 3 layers as in default MEND. This broader, lower-reaching edit may affect both factual storage and reasoning circuits.
- **Core assumption:** Knowledge relevant to propagation is not purely localized to final layers; earlier layers may encode relational or compositional structure needed for multi-hop inference.
- **Evidence anchors:** [table 5] Ablation "Mid-Upper → Upper layers" drops in-domain efficacy from 56.7 to 41.2. [section 3] "We find editing lower layers is more effective for knowledge propagation."
- **Break condition:** If targeting too many layers, memory and runtime increase substantially (Table 3: PropMEND uses +10217 MiB vs +8741 for mid-upper only). If targeting wrong layers for a different architecture, propagation may fail.

## Foundational Learning

- **Concept: Hypernetworks (networks that generate weights for another network)**
  - **Why needed here:** PropMEND's core component is a hypernetwork that takes gradients as input and outputs modified gradient components. Understanding that hypernetworks learn to produce weight updates—rather than directly processing data—is essential.
  - **Quick check question:** Can you explain why a hypernetwork enables test-time adaptation without retraining the base model?

- **Concept: Meta-Learning (bi-level optimization with inner and outer loops)**
  - **Why needed here:** PropMEND follows MEND's meta-learning framework: inner loop computes a gradient on the injected fact; outer loop evaluates the edited model on propagation questions and updates the hypernetwork. This distinction is critical for understanding how propagation is trained.
  - **Quick check question:** What is optimized in the inner loop vs. the outer loop, and why must they be different?

- **Concept: Rank-1 Gradient Decomposition**
  - **Why needed here:** MEND/PropMEND exploit that ∇W L = δu^T (outer product of output gradient and input activation). This reduces hypernetwork complexity from O(d×m) to O(d+m), making training feasible.
  - **Quick check question:** Given a weight matrix W ∈ R^{m×d}, what are the shapes of δ and u in the decomposition, and why does this matter for efficiency?

## Architecture Onboarding

- **Component map:** Base LLM (Llama-3.2-1B or Qwen-2.5-1.5B) -> Hypernetwork g_ϕ -> Modified gradients -> Updated model weights in MLP layers 4-15 (Llama) or 13-27 (Qwen)
- **Critical path:** 
  1. Sample edit instance (fact f, propagation QAs {(q_i, a_i)})
  2. Compute inner-loop gradient via CLM on f
  3. Apply hypernetwork to transform gradient
  4. Compute outer-loop loss on propagation QAs + locality loss
  5. Update hypernetwork parameters ϕ
- **Design tradeoffs:**
  - **Layer selection:** More layers → better propagation but higher memory/runtime (Table 3). Default: mid-to-upper layers, not just top layers.
  - **Inner-loop loss:** CLM on all tokens vs. SFT on answer tokens only. Ablation shows CLM on all tokens is better in-domain (56.7 vs 42.5), but SFT can be competitive OOD (Table 5).
  - **Hypernetwork size vs. data:** Scaling hypernetwork (163M → 3.4B params) and training data (4K → 30K instances) improves OOD but doesn't fully close the gap (Table 4).
- **Failure signatures:**
  - **OOD generalization:** When relations or entities are unseen during meta-training, efficacy drops sharply (In-domain 64.0 → OOD (Both) 17.7 in Table 2). This suggests the hypernetwork learns relation-specific transformations rather than a fully general propagation mechanism.
  - **Specificity degradation:** If c_edit is too high, locality loss is underweighted, and the model may forget unrelated facts (Table 1: CPT (Full) drops specificity on non-verbatim from 27.7 to 16.0).
  - **Memory bottleneck:** Editing many layers with hypernetworks is expensive; PropMEND (full) uses ~10GB memory vs. ~6.7GB for base inference (Table 3).
- **First 3 experiments:**
  1. **Reproduce the ablation on outer-loop loss:** Train PropMEND with paraphrases instead of propagation questions in the outer loop. Confirm that efficacy on non-verbatim questions collapses (~10 vs ~57 LLM-Score). This validates that the propagation objective is essential.
  2. **Layer sweep:** Compare editing only top-3 layers vs. mid-to-upper layers vs. all layers. Measure both efficacy (propagation QA accuracy) and efficiency (memory, runtime). Identify the sweet spot for your base model.
  3. **OOD stress test:** Evaluate a trained PropMEND on Controlled RippleEdit's OOD (Entity), OOD (Relation), and OOD (Both) splits. Compare against Prepend and CPT baselines to quantify how much of PropMEND's advantage is due to in-domain relation learning vs. generalizable gradient transformation.

## Open Questions the Paper Calls Out

- **Multi-edit and multi-turn editing:** The paper explicitly states it focuses on single-edit scenarios and does not address how PropMEND would scale to multi-edit and multi-turn edit scenarios. The hypernetwork is trained to transform gradients for single-fact injection; sequential edits may interfere or require retraining strategies.

- **Parameter efficiency:** The authors note that their hypernetwork is as large as the edited language model, representing a significant parameter overhead. They do not propose solutions for compressing or sharing hypernetworks while maintaining performance.

- **Generalization to unseen relations:** While PropMEND outperforms existing approaches on unseen entity-relation pairs, the performance gap decreases substantially. The authors suggest future work is needed in propagating knowledge to a wide range of relations, as the current approach appears to learn relation-specific transformations.

## Limitations

- **OOD generalization is limited:** PropMEND's performance degrades significantly when encountering unseen relations or entities, suggesting the hypernetwork learns relation-specific transformations rather than generalizable propagation principles.
- **Computational overhead:** Editing multiple MLP layers with hypernetworks is memory-intensive (up to 10GB for full PropMEND), making the approach expensive for practical deployment.
- **Benchmark limitations:** RippleEdit's propagation questions are generated from a relatively small knowledge base (17.5K triples), and the evaluation relies on LLM scoring which may not perfectly capture multi-hop reasoning capability.

## Confidence

- **High Confidence:** The mechanism of using propagation questions as outer-loop loss (vs. paraphrases) is validated by the dramatic ablation results (56.7 → 10.6 LLM-Score drop). The memory/runtime costs of editing more layers are well-documented.
- **Medium Confidence:** The claim that mid-to-upper layer targeting is more effective than top layers is supported by ablation but lacks theoretical grounding. The hypernetwork's generalization to unseen relations is claimed but the OOD results show significant performance gaps.
- **Low Confidence:** The assertion that gradient transformation alone can enable compositional reasoning is largely unproven - the method may be memorizing relation-specific patterns rather than learning abstract propagation principles.

## Next Checks

1. **OOD Generalization Stress Test:** Evaluate PropMEND on a benchmark with completely disjoint relations and entities from meta-training (e.g., Wikidata triples not in RippleEdit). Measure whether efficacy drops to baseline levels or if some generalization remains.

2. **Zero-Shot Relation Transfer:** Train PropMEND on a subset of relations (e.g., only "capital of"), then test on a different relation type ("parent of") with shared entities. This isolates whether the hypernetwork learns relation-specific vs. general propagation patterns.

3. **Human Evaluation of Multi-Hop Reasoning:** Supplement LLM-Score with human judgment on a sample of propagation questions to verify that improvements reflect true multi-hop reasoning rather than surface-level pattern matching.