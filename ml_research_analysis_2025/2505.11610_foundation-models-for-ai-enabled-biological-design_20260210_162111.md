---
ver: rpa2
title: Foundation Models for AI-Enabled Biological Design
arxiv_id: '2505.11610'
source_url: https://arxiv.org/abs/2505.11610
tags:
- biological
- generation
- protein
- design
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey reviews foundation models (FMs) for AI-enabled biological\
  \ design, focusing on their application to protein engineering, small molecule design,\
  \ and genomic sequence design. The authors examine how FMs\u2014particularly Transformer,\
  \ diffusion, and State Space Model architectures\u2014are adapted for biological\
  \ tasks, emphasizing controllability in generation and multi-modal integration."
---

# Foundation Models for AI-Enabled Biological Design

## Quick Facts
- arXiv ID: 2505.11610
- Source URL: https://arxiv.org/abs/2505.11610
- Reference count: 40
- Authors: Asher Moldwin; Amarda Shehu
- One-line primary result: Survey of foundation models (FMs) for AI-enabled biological design, covering protein engineering, small molecule design, and genomic sequence design with focus on architectural innovations and controllability challenges.

## Executive Summary
This survey reviews foundation models (FMs) for AI-enabled biological design, focusing on their application to protein engineering, small molecule design, and genomic sequence design. The authors examine how FMs—particularly Transformer, diffusion, and State Space Model architectures—are adapted for biological tasks, emphasizing controllability in generation and multi-modal integration. While acknowledging the rapid evolution of the field, they present a taxonomy of current methods, discuss architectural innovations, and highlight challenges such as data limitations and the need for better integration of biological modalities. The survey also outlines open problems and future directions, including improving control over property combinations and enhancing transfer learning to low-data regimes, with the goal of advancing practical, AI-driven biological design.

## Method Summary
The survey synthesizes recent developments in applying large-scale, self-supervised models to biological sequence design tasks. It covers three architecture families: Transformers (autoregressive language modeling), Diffusion models (denoising objectives), and State Space Models (linear state-space equations). Controllability is achieved through fine-tuning, conditional generation with control tokens, reinforcement learning, and custom loss functions. The work reviews applications across protein engineering (PLMs), small molecule design (CLMs), and genomic sequence design (GLMs), analyzing data requirements, evaluation metrics, and architectural tradeoffs for each domain.

## Key Results
- Foundation models learn transferable representations through self-supervised pre-training on large-scale biological sequences
- Conditional generation via control tokens enables property-directed design without full retraining
- State Space Models offer efficient long-range dependency modeling for extended biological sequences where Transformers face quadratic scaling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pre-training on large-scale biological sequences enables transferable representations for downstream design tasks.
- Mechanism: The model learns to predict masked or subsequent tokens from unlabeled biological sequences (proteins, DNA, SMILES), encoding structural and functional constraints into hidden representations that can be repurposed via fine-tuning or conditional generation.
- Core assumption: Biological sequences share latent grammars analogous to natural language, such that next-token prediction captures biologically meaningful structure.
- Evidence anchors:
  - [abstract] "focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design"
  - [section] "FMs are architecture-agnostic and defined by their ability to learn from task-agnostic pre-training, making them applicable across a range of tasks"
  - [corpus] Weak direct evidence; related surveys (e.g., "Flow Matching Meets Biology and Life Science") discuss generative modeling advances but do not validate transfer mechanisms.
- Break condition: If pre-training objectives borrowed from NLP fail to capture biological constraints (e.g., long-range residue interactions, functional motifs), representations will not transfer effectively.

### Mechanism 2
- Claim: Conditional generation via control tokens or prompts steers generation toward desired biological properties without full retraining.
- Mechanism: Control tags (e.g., property embeddings, motif prompts, cell-type tokens) are prepended or interleaved with sequence inputs; the model conditions its output distribution on these signals during autoregressive or diffusion-based sampling.
- Core assumption: The conditioning signal correlates with target properties in the training distribution, and the model can generalize to novel combinations.
- Evidence anchors:
  - [section] "MolGPT [17] embeds control tokens directly in the pre-training inputs... RegLM [29] employs conditional generation to design synthetic cis-regulatory elements... where autoregressive generation is conditioned on a starter fragment"
  - [section] "Progen2 conditions the generation of antibody (protein) sequences on three-residue motif prompts"
  - [corpus] No direct corpus validation of conditional control efficacy; neighbor papers focus on architecture innovations rather than controllability mechanisms.
- Break condition: If target properties are rare or physically implausible in training data, conditioning may fail to guide generation meaningfully (noted as an open problem).

### Mechanism 3
- Claim: State Space Models (SSMs) provide efficient long-range dependency modeling for extended biological sequences where Transformers face quadratic scaling.
- Mechanism: SSMs (e.g., Mamba, S4, Hyena) use recurrent hidden states and linear state-space equations to model sequence evolution with sub-quadratic compute and constant inference memory, capturing interactions between distant tokens (e.g., regulatory elements separated by 100k+ bases).
- Core assumption: Long-range dependencies in biological sequences (protein contacts, distal regulatory elements) can be approximated by linear recurrence with data-dependent gating.
- Evidence anchors:
  - [section] "SSMs achieve sub-quadratic scaling and constant memory by employing a Recurrent Neural Network architecture and using fixed-size hidden states and linear state-space equations"
  - [section] "HyenaDNA is trained on sequences of up to 1 million nucleotides. It allows the model to capture long-range interactions at single-nucleotide resolution"
  - [corpus] Lyra (arXiv:2503.16351) proposes a subquadratic architecture for biological sequences, supporting the efficiency claim but not providing comparative benchmarks.
- Break condition: If linear recurrence cannot capture nonlinear long-range interactions critical for protein folding or gene regulation, SSMs will underperform attention-based models despite efficiency gains.

## Foundational Learning
- Concept: **Self-supervised pre-training objectives (autoregressive, masked language modeling)**
  - Why needed here: All surveyed FMs leverage self-supervision; understanding next-token prediction vs. masked prediction clarifies why decoder-only vs. encoder-decoder architectures are chosen.
  - Quick check question: Can you explain why autoregressive pre-training is suited for generation tasks while masked pre-training suits representation learning?

- Concept: **Attention complexity and alternatives**
  - Why needed here: The paper positions SSMs and diffusion as responses to Transformer attention's O(T²) scaling; you must grasp this tradeoff to select architectures.
  - Quick check question: What is the memory complexity of self-attention during inference, and how do SSMs reduce it?

- Concept: **Biological sequence representations (SMILES, amino acid sequences, nucleotide strings)**
  - Why needed here: The taxonomy assumes familiarity with how molecules and biomolecules are tokenized; SMILES syntax and biological alphabets determine vocabulary design.
  - Quick check question: What are the token vocabularies for protein sequences vs. SMILES strings, and what challenges arise from SMILES ring closure notation?

## Architecture Onboarding
- Component map: **Backbone** (Transformer/SSM/Diffusion) -> **Control Interface** (conditional tokens/fine-tuning/RL) -> **Modality** (sequence/sequence+structure/sequence+language) -> **Post-processing** (property filtering/structural validation)
- Critical path: 1. Choose biological domain (protein/small molecule/DNA) -> determine representation (sequence vs. graph) 2. Select backbone based on sequence length and compute budget (Transformer for <1k tokens; SSM/Hyena for >10k) 3. Define control strategy (conditional tokens for discrete properties; fine-tuning for class transfer; RL for multi-objective) 4. Implement evaluation pipeline (validity, uniqueness, novelty; property prediction; structural plausibility)
- Design tradeoffs:
  - **Transformer vs. SSM**: Transformers provide proven representational capacity but scale poorly for long genomic sequences; SSMs offer efficiency but lack established benchmarks for biological tasks.
  - **Conditional generation vs. fine-tuning**: Conditional tokens avoid retraining but require property-labeled data; fine-tuning adapts to new classes but is computationally expensive for large models.
  - **Autoregressive vs. diffusion**: Autoregressive models sample efficiently but may accumulate errors; diffusion enables global coherence but requires many denoising steps.
- Failure signatures:
  - **Low validity rate**: Generated SMILES are syntactically invalid; suggests inadequate vocabulary handling or insufficient training on valid structures.
  - **Mode collapse**: Generated sequences cluster narrowly; RL rewards may be too sparse or conditional embeddings too dominant.
  - **Poor long-range coherence**: SSM-generated proteins lack realistic contacts; model may need deeper recurrence or hybrid attention-SSM architecture.
  - **Property mismatch**: Conditioned properties not achieved; training data may lack examples with target property combinations.
- First 3 experiments:
  1. **Baseline reproduction**: Implement MolGPT-style conditional Transformer on a curated SMILES dataset (e.g., 100k molecules); evaluate validity, uniqueness, and property control for LogP and QED.
  2. **Architecture ablation**: Compare Transformer vs. S4-based CLM on identical SMILES data using matched parameter counts; benchmark validity, novelty, and inference latency.
  3. **Transfer learning test**: Pre-train a small protein LM on Uniref50, fine-tune on a low-data family (e.g., lysozymes), and measure functional plausibility via structure prediction (AlphaFold) and activity estimation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which neural architectures (Transformers, SSMs, or Diffusion) are optimal for specific biological design tasks?
- Basis in paper: [explicit] The authors state there is "little consensus on which architecture is best across biological tasks" and propose exploring domain-specific self-supervised objectives.
- Why unresolved: Current evaluations are inconsistent, and objectives are borrowed from NLP rather than tailored to biological data behavior.
- What evidence would resolve it: Rigorous comparative benchmarks across tasks using standardized datasets and novel biological pre-training objectives.

### Open Question 2
- Question: Can foundation models generalize to rare property combinations absent from their training data?
- Basis in paper: [explicit] The paper highlights the need to push models to "generalize to rare or physically unlikely property combinations" (dark regions).
- Why unresolved: It remains unclear if failure to generate these combinations is due to data absence or intrinsic physical constraints.
- What evidence would resolve it: Demonstrated generation of valid, novel sequences with property profiles statistically distinct from the training distribution.

### Open Question 3
- Question: How can transfer learning be optimized for molecular classes with very low data availability?
- Basis in paper: [explicit] The authors propose exploring transfer learning to help CLMs generalize to classes like "Quaternary Ammonium Compounds" with few examples.
- Why unresolved: Standard fine-tuning is often insufficient or biased when the target dataset is small relative to the pre-training corpus.
- What evidence would resolve it: Successful generation of target molecules using techniques like conditional generation to overcome pre-training bias.

## Limitations
- Survey scope constraint: Focuses on sequence-based representations, excluding graph-based molecular models and protein structure prediction architectures
- Data availability bias: Performance metrics depend on curated datasets that may not reflect real-world diversity or representativity
- Generality of transfer: Claims about broad applicability of self-supervised FMs lack empirical validation of cross-domain transfer

## Confidence
- **High confidence**: The taxonomy of FM architectures (Transformer, diffusion, SSM) and their biological applications is well-grounded in the cited literature
- **Medium confidence**: Claims about SSM efficiency gains and their suitability for long genomic sequences are supported by theoretical arguments but lack comprehensive comparative studies
- **Low confidence**: Assertions about the effectiveness of conditional generation for multi-property control are largely theoretical without robust empirical validation

## Next Checks
1. **Benchmark SSMs on genomic tasks**: Implement S4/Hyena-based models on a standard genomic dataset (e.g., ENCODE regulatory elements) and compare against Transformer baselines for long-range interaction prediction accuracy and computational efficiency
2. **Validate multi-property control**: Design an experiment using conditional tokens to generate molecules with specific combinations of QED, LogP, and synthetic accessibility; measure whether the generated molecules achieve the target property combinations without mode collapse
3. **Test cross-modal transfer**: Pre-train a protein LM on UniRef50, then fine-tune on antibody sequences with low data; evaluate functional plausibility using AlphaFold structure prediction and binding affinity estimation to assess practical utility