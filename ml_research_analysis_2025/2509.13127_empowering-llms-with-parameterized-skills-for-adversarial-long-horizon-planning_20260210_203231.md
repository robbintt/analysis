---
ver: rpa2
title: Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning
arxiv_id: '2509.13127'
source_url: https://arxiv.org/abs/2509.13127
tags:
- skill
- plap
- planning
- llms
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PLAP, a two-stage framework for grounding LLM-based
  agents in long-horizon adversarial environments. PLAP bridges the gap between high-level
  planning and low-level execution by using a skill library containing environment-specific
  parameterized skills, an LLM-powered skill planner, and a skill executor that converts
  parameterized skills into executable action sequences.
---

# Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning

## Quick Facts
- **arXiv ID**: 2509.13127
- **Source URL**: https://arxiv.org/abs/2509.13127
- **Reference count**: 40
- **Primary result**: GPT-4o-driven PLAP in zero-shot settings outperforms 80% of baseline agents in MicroRTS; Qwen2-72B with few-shot examples surpasses the top-tier scripted agent.

## Executive Summary
This paper introduces PLAP, a two-stage framework that bridges high-level LLM planning and low-level action execution in adversarial long-horizon environments. By using a predefined skill library with parameterized skills, PLAP reduces the output space complexity and enables more structured planning. The framework is evaluated in MicroRTS, demonstrating strong performance against baseline agents and introducing a comprehensive benchmark (Skill-RTS) for evaluating LLM-based planning. The method leverages in-context learning without fine-tuning, achieving competitive results across multiple LLM models.

## Method Summary
PLAP uses a two-stage loop: (1) Every k=100 steps, construct a prompt with game rules, available skills, and current textual observation; query an LLM to generate a high-level skill plan. (2) A deterministic "Skill Executor" parses the plan and converts active skills into atomic game actions for the next 100 steps based on the current state. The method supports zero-shot and few-shot prompting, with temperature=0 and max_tokens=256. Evaluation is conducted in MicroRTS against baseline bots (RandomBiasedAI, LightRush, NaiveMCTS, WorkerRush, CoacAI) using win rate and resource harvesting metrics.

## Key Results
- GPT-4o-driven PLAP in zero-shot settings outperforms 80% of baseline agents in MicroRTS.
- Qwen2-72B-driven PLAP with few-shot examples surpasses the top-tier scripted agent.
- Skill-RTS benchmark introduced with comprehensive evaluation metrics and an LLM leaderboard ranking 8 models on long-horizon skill planning ability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameterized skills serve as an effective intermediate abstraction layer between high-level language planning and low-level action execution.
- Mechanism: The skill library defines 5 parameterized skills (Deploy Unit, Harvest Mineral, Build Building, Produce Unit, Attack Enemy), each mapping to atomic action sequences. The LLM outputs structured skill invocations like `[Harvest Mineral](7, 7)` rather than raw actions, reducing the output space from ~78 dimensions per grid cell to 5 skill types with interpretable parameters.
- Core assumption: The predefined skill library adequately covers the action space needed for competent gameplay.
- Evidence anchors:
  - [abstract] "PLAP bridges the gap between high-level planning and low-level execution by using a skill library containing environment-specific parameterized skills"
  - [section IV.A, Table I] Lists 5 skills with explicit parameter-action mappings
  - [corpus] MOSAIC paper confirms skill-centric planning reduces long-horizon complexity in manipulation; SkillWrapper paper addresses predicate invention for skill abstraction but requires additional learning machinery PLAP avoids.
- Break condition: If task requires skills outside the predefined library, or if skill parameters have complex interdependencies the LLM cannot reason about, performance would degrade.

### Mechanism 2
- Claim: Interval-based planning (every k steps) maintains decision consistency while reducing LLM invocation costs.
- Mechanism: The planner generates a skill plan every k=100 game steps. The executor maintains the action queue between planning invocations, dynamically checking skill validity. This reduces LLM calls from n (total steps) to n/k while preserving real-time responsiveness.
- Core assumption: The planning horizon k is long enough to amortize LLM latency but short enough to respond to opponent strategy shifts.
- Evidence anchors:
  - [section IV] "if a game requires n steps to complete, skill planning needs to be performed n/k times, significantly reducing the cost of LLM invocations"
  - [section IV.C] Executor regenerates action sequences each step based on current state rather than replaying cached sequences, handling adversarial dynamics
  - [corpus] Beyond Task and Motion Planning paper notes hierarchical planning with skills addresses long-horizon complexity but focuses on robotics; PLAP applies similar principles to adversarial domains.
- Break condition: If k is too large, the agent cannot adapt to rapid opponent strategy changes. If k is too small, LLM latency and costs become prohibitive.

### Mechanism 3
- Claim: Dynamic prompt construction with real-time observations enables adaptive planning without fine-tuning.
- Mechanism: At each planning step, textual observations are constructed from game state (unit positions, resources, enemy actions) and combined with the skill library description and optional few-shot examples. The LLM generates context-appropriate skill plans.
- Core assumption: The LLM can interpret structured game state descriptions and map them to appropriate skill sequences through in-context learning.
- Evidence anchors:
  - [section IV, Fig. 4] Shows GPT-4o selecting different mineral fields at t=0 vs t=1000 based on remaining resources, demonstrating adaptive planning
  - [section IV.A] "The dynamically constructed prompt enables the LLM planner to adjust the skill plan according to the evolving situation and opponent's behavior"
  - [corpus] Mirage-1 paper similarly augments GUI agents with hierarchical multimodal skills for long-horizon tasks, confirming prompt-based skill selection patterns transfer across domains.
- Break condition: If textual observation encoding loses critical spatial or temporal information, or if the LLM lacks reasoning capacity for strategic lookahead, adaptation fails.

## Foundational Learning

- **Parameterized Action MDPs (PAMDPs)**:
  - Why needed here: Section III.A formalizes the task as a PAMDP with 5-tuple (S, U, R, T, γ). Understanding this frame is essential for grasping why parameterized skills are the policy output space.
  - Quick check question: Can you explain why a PAMDP differs from a standard MDP and why the parameter space Θ matters for skill execution?

- **Options Framework / Temporal Abstraction**:
  - Why needed here: Skills are defined using options notation (π_i, Θ_i, I_i, β_i) in Section III.A. You need this to understand termination conditions and initiation sets.
  - Quick check question: Given a skill with initiation set I_i and termination condition β_i, when would the executor remove this skill from the active plan?

- **In-Context Learning for Decision-Making**:
  - Why needed here: The paper leverages zero-shot and few-shot prompting without fine-tuning. Understanding prompting dynamics explains performance differences across models (GPT-4o vs Qwen2-72B).
  - Quick check question: Why might few-shot examples improve Qwen2-72B but degrade GPT-4o performance (Fig. 7)?

## Architecture Onboarding

- **Component map**: Game state → Textual observation → Prompt construction → LLM inference → Parse skill plan → Filter active skills → Execute skill policies → Aggregate actions → Environment step. Planning occurs every k steps; execution occurs every step.
- **Critical path**: Game state → Textual observation → Prompt construction → LLM inference → Parse skill plan → Filter active skills → Execute skill policies → Aggregate actions → Environment step. Planning occurs every k steps; execution occurs every step.
- **Design tradeoffs**:
  - **k (planning interval)**: Paper uses k=100. Larger k reduces cost but risks stale plans; smaller k improves responsiveness but increases latency/cost.
  - **Skill library size**: 5 skills balance expressiveness and LLM output complexity. More skills increase planning flexibility but complicate prompt and parsing.
  - **Zero-shot vs few-shot**: Few-shot improves weaker models (Qwen, DeepSeek) but can hurt stronger models (GPT-4o) — Section V.D shows GPT-4o performance drops with few-shot prompting.
  - **Expert tips**: Tips field consistently improves performance (Section V.D) but requires domain knowledge.
- **Failure signatures**:
  - **Invalid skill/parameter parsing**: Parser discards malformed outputs using regex matching. High discard rates indicate prompt/skill definition issues.
  - **Skill becomes inactive mid-execution**: Executor checks I_i(s_t') at each step; if false, skill is skipped. Frequent skips suggest poor planning or rapid state changes.
  - **Plan-execution drift**: If opponent behavior diverges from implicit LLM prediction, the skill plan may become counterproductive. No explicit opponent modeling (noted as limitation in Section VI).
- **First 3 experiments**:
  1. **Baseline validation**: Run zero-shot PLAP with GPT-4o against RandomBiasedAI on basesWorkers8x8 map. Expected: +5 score (5 wins). Verify pipeline end-to-end.
  2. **Ablation on k**: Test k ∈ {50, 100, 200} with GPT-4o against WorkerRushAI. Measure win rate, resource efficiency, and LLM call count. Identify responsiveness/cost tradeoff.
  3. **Few-shot transfer test**: Apply the same 2 few-shot examples (constructed on basesWorkers8x8) to a different map (e.g., NoWhereToRun9x8). Compare GPT-4o vs Qwen2-72B to confirm generalization vs overfitting pattern from Section V.D.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based code generation replace manually constructed skill functions to enhance generalization across diverse domains?
- Basis in paper: [explicit] The Conclusion states that "skill functions used in PLAP are manually constructed, which limits generalization" and suggests exploring "LLM-based code generation" to reduce human effort.
- Why unresolved: Automating the creation of valid, executable, and context-aware skill libraries for unseen environments without human intervention remains an unsolved engineering challenge.
- What evidence would resolve it: Demonstration of an autonomous pipeline where an LLM generates, debugs, and deploys successful skill functions for a new environment (e.g., a different game or robotics task) matching handcrafted performance.

### Open Question 2
- Question: Does integrating explicit opponent strategy modeling improve decision-making performance beyond the current state-reactive approach?
- Basis in paper: [explicit] The Conclusion notes that PLAP currently lacks "modeling the opponent’s strategic intent," which "limits its upper performance bound."
- Why unresolved: The current framework relies on explicit state observations; inferring latent strategic intent and integrating it effectively into the high-level planning prompt is complex.
- What evidence would resolve it: A modified PLAP agent that maintains a dynamic model of opponent strategy (e.g., rushing vs. turtling) and adapts proactively, showing higher win rates against adaptive baseline agents.

### Open Question 3
- Question: How robust is PLAP when transferred to complex, realistic adversarial environments with higher state complexity than MicroRTS?
- Basis in paper: [explicit] The Conclusion acknowledges "Limited Evaluation Scope," noting the method has only been tested in MicroRTS and "lacking validation in more realistic adversarial scenarios."
- Why unresolved: Real-world domains introduce noise, partial observability, and unbounded action spaces that may disrupt the textual observation-to-plan mapping used in the Skill-RTS benchmark.
- What evidence would resolve it: Successful deployment and maintenance of decision consistency in a high-fidelity environment (e.g., StarCraft II) or a physical robotics simulation.

## Limitations
- Evaluation is limited to a single adversarial environment (MicroRTS) with a small skill library (5 skills), raising questions about generalizability to more complex domains.
- The framework assumes predefined skills are sufficient for task completion, which may not hold for more complex planning scenarios.
- Lack of explicit opponent modeling could limit performance against adaptive adversaries.

## Confidence
- **High confidence**: The core mechanism of using parameterized skills as an intermediate abstraction layer is well-supported by empirical results showing consistent improvement over baselines. The Skill-RTS benchmark provides transparent and reproducible evaluation.
- **Medium confidence**: The claimed superiority of GPT-4o over Qwen2-72B in zero-shot settings is supported by leaderboard results, but the relative performance depends on task complexity and prompt engineering. The claim that few-shot examples improve weaker models but degrade stronger ones needs further validation across more diverse tasks.
- **Low confidence**: The scalability claims for more complex environments are largely theoretical, as the paper only demonstrates performance on MicroRTS with a fixed 5-skill library.

## Next Checks
1. **Skill Library Generality Test**: Apply PLAP to a different adversarial domain (e.g., robotics navigation or multi-agent coordination) and evaluate whether the same skill library approach generalizes or requires significant domain-specific redesign.
2. **Opponent Modeling Integration**: Implement explicit opponent modeling (e.g., maintaining belief states about enemy strategy) and measure impact on win rates against adaptive agents like CoacAI.
3. **Skill Composition Complexity**: Design a benchmark requiring non-trivial skill composition (e.g., conditional skill switching based on resource thresholds) and test whether the LLM planner can handle such complexity without performance degradation.