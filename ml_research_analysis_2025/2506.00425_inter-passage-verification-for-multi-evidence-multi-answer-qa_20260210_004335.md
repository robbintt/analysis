---
ver: rpa2
title: Inter-Passage Verification for Multi-evidence Multi-answer QA
arxiv_id: '2506.00425'
source_url: https://arxiv.org/abs/2506.00425
tags:
- answer
- question
- verification
- questions
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-answer question answering
  (QA), where questions can have many valid answers and require synthesizing evidence
  from multiple passages. Existing retrieval-augmented generation (RAG) systems struggle
  with this task due to difficulties in retrieving and synthesizing large numbers
  of evidence passages.
---

# Inter-Passage Verification for Multi-evidence Multi-answer QA

## Quick Facts
- arXiv ID: 2506.00425
- Source URL: https://arxiv.org/abs/2506.00425
- Reference count: 20
- The paper proposes RI2VER, a framework that improves multi-answer QA by 11.17% F1 through inter-passage verification

## Executive Summary
This paper addresses the challenge of multi-answer question answering (QA), where questions can have many valid answers requiring synthesis from multiple passages. Existing retrieval-augmented generation (RAG) systems struggle with this task due to difficulties in retrieving and synthesizing large numbers of evidence passages. The authors propose Retrieval-augmented Independent Reading with Inter-passage Verification (RI2VER), which first retrieves a large set of passages and processes each passage independently to generate a high-recall but noisy answer set. Then, it introduces an inter-passage verification pipeline that validates each candidate answer through generating verification questions, gathering additional evidence, and verifying with inter-passage synthesis. Evaluations on QAMPARI and RoMQA datasets demonstrate that RI2VER significantly outperforms existing baselines across various model sizes.

## Method Summary
RI2VER is a three-stage pipeline for multi-answer QA. First, it retrieves 1,000 passages using an ensemble of BM25 and NV-Embed-v2 with Reciprocal Rank Fusion. Second, it processes the top-200 passages independently using a language model to extract candidate answers, maximizing recall but producing a noisy set. Third, it applies inter-passage verification: for each candidate answer, the system generates categorical and factual verification questions, retrieves additional evidence for each factual question from the remaining 800 passages, and verifies the answer by synthesizing the original passage with the retrieved evidence. The verification uses a probability-based mechanism comparing p(True) vs p(False) token predictions. The framework uses Llama-3.1-8b/70b for reading and verification stages.

## Key Results
- RI2VER achieves an average F1 score improvement of 11.17% over existing baselines on QAMPARI and RoMQA datasets
- Independent Reading achieves 66-69% Recall on QAMPARI, significantly higher than Concatenated Reading
- The inter-passage verification pipeline is particularly beneficial for complex questions requiring multi-evidence synthesis
- System scales well across model sizes, with 70b model achieving higher F1 than 8b model

## Why This Works (Mechanism)

### Mechanism 1: Independent Reading for Recall Maximization
Processing retrieved passages individually maximizes the recall of potential answers by preventing the "lost in the middle" phenomenon where relevant information in long contexts is ignored. The model extracts all locally valid answers from each isolated passage, forming a high-recall candidate set. This approach assumes the retriever successfully captures relevant evidence within the top passages and that models extract answers better from short isolated contexts than from long concatenated ones.

### Mechanism 2: Atomic Verification Question Decomposition
The framework decomposes complex multi-constraint queries into atomic verification questions (categorical and factual sub-questions), reducing the reasoning burden to simple binary checks against specific evidence. This ensures candidate answers satisfy all aspects of the original query. The approach assumes the LLM can accurately decompose questions without hallucinating constraints or missing logical operators.

### Mechanism 3: Inter-Passage Verification (IPV) for Precision Recovery
Re-retrieving specific evidence for each verification question enables verification of multi-hop answers requiring synthesis across documents, filtering false positives from the independent reading phase. The initial passage may only support part of the answer, so the system queries again using the verification question and candidate to find additional evidence. This assumes the secondary retrieval can find missing evidence and the verifier can synthesize distinct pieces without hallucinating.

## Foundational Learning

- **Concept: Independent vs. Concatenated Reading in RAG**
  - Why needed here: This is the foundational design choice of RI²VER. Standard RAG often concatenates documents, which limits the number of passages the model can effectively attend to.
  - Quick check question: Why does Independent Reading improve recall but hurt precision compared to Concatenated Reading? (Answer: It sees more evidence but lacks global context to filter out local hallucinations)

- **Concept: Multi-Evidence Synthesis**
  - Why needed here: The target dataset contains questions where a single correct answer requires facts from multiple documents.
  - Quick check question: Why is verifying the answer "GDPR" for a question about "AI regulations in EU" a multi-evidence problem? (Answer: One document might define GDPR as an EU regulation, while a separate document links GDPR to AI developers)

- **Concept: The Precision-Recall Trade-off**
  - Why needed here: The framework is explicitly designed as a "Recall-then-Verify" pipeline, decoupling the objective.
  - Quick check question: According to Table 1, how does the F1 score change when IPV is applied to the Independent Reading baseline? (Answer: It increases significantly, e.g., from 16.28 to 36.33 for the 8b model, driven largely by a massive jump in Precision)

## Architecture Onboarding

- **Component map:** Retriever (1,000 docs) -> Independent Reading (200 docs) -> IPV Pipeline (Decomposition -> Retrieval -> Verification)
- **Critical path:** The system is sequential: Retrieval -> Independent Reading -> IPV Pipeline. The latency bottleneck is typically the Verification step, which scales with the number of candidates.
- **Design tradeoffs:** 
  - Latency vs. Accuracy: Independent reading of 200 passages + verification per candidate is computationally expensive (Inference time ~32s vs 9s for concat) but necessary for high F1
  - Evidence Pool Size: Retrieves 1,000 passages initially but only reads the top 200, reserving the remaining 800 for verification
- **Failure signatures:**
  - Low Recall, High Precision: Verifier too strict or secondary retrieval fails to find necessary evidence
  - High Recall, Low Precision: Verifier failing to filter candidates, often when initial reader extracts spurious entities
  - Granularity Mismatch: System predicts more specific answers than ground truth (e.g., "iPhone 15" vs "iPhone")
- **First 3 experiments:**
  1. Retrieval Baseline: Measure Answer Recall @K for the Retriever to set the upper bound
  2. Reading Ablation: Compare "Independent Reading" vs. "Concatenated Reading" to confirm higher recall but lower precision
  3. Verification Scaling: Run full RI²VER but ablate "Extra Retrieval" step to quantify value of inter-passage synthesis

## Open Questions the Paper Calls Out

- Can the Inter-Passage Verification (IPV) pipeline be accelerated using distilled models or lighter retrievers without significant performance loss? (Basis: Explicit in Limitations section)
- How can parametric knowledge be effectively harmonized with retrieved context in multi-answer QA settings? (Basis: Explicit in Limitations section)
- How can evaluation benchmarks be adapted to handle answers that are semantically correct but differ in granularity from ground truth annotations? (Basis: Explicit in Section 5 and Appendix A.5)
- Does the RI²VER framework maintain its performance advantages when applied to domain-specific corpora outside of Wikipedia? (Basis: Explicit in Limitations section)

## Limitations
- The verification step's effectiveness depends heavily on secondary retrieval finding the right "bridge" evidence
- The probability-based verification mechanism using log-prob comparisons is sensitive to generation hyperparameters and model calibration
- The framework assumes local context is sufficient for high-recall extraction, which may break down for answers requiring explicit cross-document synthesis from the start

## Confidence

- **High confidence**: The independent reading approach significantly improves recall compared to concatenated reading (Table 1)
- **Medium confidence**: The IPV pipeline's F1 improvements are robust across model sizes (8b vs 70b) and datasets
- **Medium confidence**: The precision-recall tradeoff is well-characterized, though exact mechanisms for verification failures need more analysis

## Next Checks
1. **Ablation on verification evidence quality**: Sample false negatives from the IPV step and categorize whether failures stem from insufficient evidence retrieval vs. model hallucination vs. question decomposition errors
2. **Retrieval coverage analysis**: Measure the correlation between initial retriever recall (ARECALL) and final system F1 to quantify the upper bound imposed by retrieval quality
3. **Verification hyperparameter sensitivity**: Test different probability thresholds for p(True) vs p(False) comparison to understand robustness of the verification mechanism