---
ver: rpa2
title: 'CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate
  Alignment Quotient (CAQ)'
arxiv_id: '2504.03906'
source_url: https://arxiv.org/abs/2504.03906
tags:
- climate
- change
- justice
- actionability
- criticality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate Alignment Quotient (CAQ)

## Quick Facts
- arXiv ID: 2504.03906
- Source URL: https://arxiv.org/abs/2504.03906
- Reference count: 26
- Primary result: Proposed CAQ metric evaluates LLM performance on climate discourse; Articulation (0.86) and Resonance (1.00) are strongest dimensions, while Transition (0.16) and Specificity (0.30) are weakest.

## Executive Summary
This paper introduces CliME, a benchmark for evaluating large language models' comprehension of multimodal climate discourse on social media through the Climate Alignment Quotient (CAQ) metric. The authors process 2,579 Twitter/Reddit posts with images through a vision-language model (DeepSeek Janus Pro) to generate text descriptions, which are then evaluated by five LLMs (GPT-4o, LLaMA 3.3 70B, Gemini 2.0 Flash, Qwen QwQ 32B, Claude 3.7 Sonnet) across three analytical lenses: Actionability, Criticality, and Justice. The CAQ metric weights five sub-dimensions (Resonance, Articulation, Evidence, Transition, Specificity) to provide a comprehensive assessment of climate communication quality, revealing that while LLMs excel at Resonance and Articulation, they struggle with Transition and Specificity.

## Method Summary
The method converts multimodal climate content (memes, infographics) to structured text descriptors using DeepSeek Janus Pro (temperature=0.3, max_tokens=512), then has human annotators validate these descriptions (92% agreement). Five LLMs are evaluated on these validated descriptors using three analytical lens prompts (Actionability, Criticality, Justice), with responses scored by ClimateBERT models and custom NLP pipelines. The CAQ metric combines five weighted sub-dimensions (Resonance=0.25, Articulation=0.30, Evidence=0.20, Transition=0.15, Specificity=0.10) to produce a composite score for each LLM-lens combination.

## Key Results
- GPT-4o achieves highest CAQ score (0.78) while Qwen QwQ 32B performs worst (0.69)
- Articulation (0.86) and Resonance (1.00) are consistently strong across all models
- Transition (0.16) and Specificity (0.30) show significant performance gaps
- All models underperform on the Actionability dimension compared to other lenses
- Claude 3.7 Sonnet shows strongest Justice lens performance (0.74)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting multimodal climate content to structured text descriptors before LLM evaluation appears to preserve narrative context that raw pixel input would lose.
- Mechanism: A vision-language model (DeepSeek Janus Pro) processes image-text pairs into unified descriptions, which are then human-validated (92% Cohen's kappa) before downstream evaluation. This intermediate representation decouples visual understanding from analytical reasoning.
- Core assumption: Text descriptors capture sufficient semantic information from images to enable meaningful climate discourse analysis.
- Evidence anchors:
  - [section 3.2]: "directly feeding raw pixels to text-based LLMs risks stripping away the nuanced, context-dependent narratives that images convey"
  - [section 3.2]: "we manually annotated the generated descriptors... inter-annotator agreement is 92.0%"
  - [corpus]: Related work on multimodal stance detection (MultiClimate, GreenScreen) focuses on YouTube but overlooks memes—suggesting descriptor-based approaches may be underexplored for this format.
- Break condition: If descriptors systematically omit visual rhetorical elements (e.g., irony in memes), the CAQ scores will reflect descriptor quality rather than LLM climate reasoning.

### Mechanism 2
- Claim: The three analytical lenses (Actionability, Criticality, Justice) may elicit differentiated LLM responses by framing the same descriptor through distinct evaluative priorities.
- Mechanism: Each lens uses a targeted prompt template (Appendix A.1) instructing the LLM to analyze through a specific frame—practical feasibility, structural critique, or equity considerations. The same descriptor yields three outputs scored separately.
- Core assumption: Prompt framing substantially influences LLM output characteristics in climate discourse domains.
- Evidence anchors:
  - [section 4.1]: Provides concrete prompt templates for each lens
  - [table 1]: Shows differentiated scores across lenses (e.g., Claude Actionability=0.73, Criticality=0.74, Justice=0.73)
  - [corpus]: "Artificial Intelligence and Civil Discourse" paper examines LLM moderation of climate conversations—consistent with framing effects being consequential.
- Break condition: If LLMs produce similar outputs regardless of lens prompt, the dimensional separation is artifactual rather than meaningful.

### Mechanism 3
- Claim: CAQ's weighted composite structure prioritizes articulation and resonance over transition specificity, which may explain low transition scores across all models.
- Mechanism: CAQ = 0.25·Resonance + 0.30·Articulation + 0.20·Evidence + 0.15·Transition + 0.10·Specificity. The weights were empirically set, not learned.
- Core assumption: The chosen weights reflect relative importance for climate communication quality; Articulation (0.30) matters more than Specificity (0.10).
- Evidence anchors:
  - [section 4.2]: Explicit weight definitions
  - [table 1]: Transition scores range 0.11–0.21 (lowest across all models/dimensions), while Resonance is near 1.0
  - [corpus]: No direct corpus evidence on weight validity—this is an open design choice.
- Break condition: If low transition scores reflect weight/calibration issues rather than genuine model limitations, conclusions about "actionability gaps" may be overstated.

## Foundational Learning

- Concept: **Vision-Language Model (VLM) descriptor generation**
  - Why needed here: The pipeline depends on Janus Pro converting memes/infographics to text. Understanding temperature settings (0.3 used) and token limits (512) is necessary for reproducibility.
  - Quick check question: Can you explain why a lower temperature (0.3 vs 0.7) would be preferred for descriptor generation in this context?

- Concept: **Weighted composite metrics**
  - Why needed here: CAQ combines five sub-metrics with fixed weights. Interpreting results requires understanding how component scores propagate.
  - Quick check question: If Articulation drops from 0.86 to 0.70, what is the maximum impact on overall CAQ?

- Concept: **Analytical lens prompting**
  - Why needed here: The three lenses are implemented as prompt templates. Modifying or extending them requires understanding prompt engineering for differentiated outputs.
  - Quick check question: What distinguishes the Criticality lens prompt from the Justice lens prompt in terms of what the LLM is asked to evaluate?

## Architecture Onboarding

- Component map: Twitter/Reddit scraper → language filter → relevance filter → deduplication (ImageHash) → 2,579 entries → Janus Pro VLM (temp=0.3, max_tokens=512) → human annotation → validated descriptors → 3 lens prompts × 5 LLMs → 15 output sets → ClimateBERT models + custom NLP → CAQ computation

- Critical path: Descriptor quality determines all downstream validity. Human annotation (92% agreement) is the gatekeeper.

- Design tradeoffs:
  - Using pretrained ClimateBERT models enables rapid scoring but may miss meme-specific signals (acknowledged in Limitations)
  - Fixed weights simplify interpretation but lack empirical justification
  - Text descriptors enable text-only LLM evaluation but discard visual features not captured in language

- Failure signatures:
  - Low inter-annotator agreement on descriptors would indicate VLM failure mode
  - Identical scores across lenses would indicate prompt failure mode
  - Resonance scores significantly below 1.0 would indicate domain mismatch in ClimateBERT detector

- First 3 experiments:
  1. Validate descriptor quality: Sample 50 descriptors, independently assess whether they capture visual irony/skepticism elements in source memes.
  2. Ablate lens prompts: Run same descriptors through generic "analyze this climate content" prompt; compare CAQ distributions to lens-specific outputs.
  3. Weight sensitivity: Recompute CAQ with uniform weights (0.20 each); check whether model rankings change materially.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific fine-tuning or advanced prompting strategies close the performance gap in the "Actionability" dimension where current LLMs consistently underperform?
- Basis in paper: [explicit] The authors state that evaluated LLMs "consistently underperform on the Actionability axis" and suggest future efforts should focus on "refining the CAQ metric."
- Why unresolved: The paper identifies the low Actionability scores as a finding but does not propose or test methods to improve this specific dimension.
- What evidence would resolve it: A follow-up study demonstrating improved Actionability scores (e.g., >0.8) in the same models after applying targeted fine-tuning on actionable climate datasets.

### Open Question 2
- Question: How does the rapid evolution of meme-specific language and irony impact the long-term validity of the CAQ metric?
- Basis in paper: [inferred] The Limitations section notes that because CAQ relies on pre-trained models, there is a "risk of missing nuanced climate change signals" as "memes can quickly become outdated."
- Why unresolved: The current metric relies on static pre-trained models (like ClimateBERT) which may not capture the dynamic, evolving nature of social media humor.
- What evidence would resolve it: A temporal analysis showing CAQ scoring accuracy remains stable over time, or the development of a dynamic updating mechanism for the underlying metric models.

### Open Question 3
- Question: Do native multimodal models (VLMs) outperform the current pipeline approach of converting images to text descriptions before analysis?
- Basis in paper: [inferred] The authors mention the risk that feeding descriptions "strips away the nuanced, context-dependent narratives," and future work aims to empower "VLMs to contribute more effectively."
- Why unresolved: The current methodology uses an intermediate VLM (DeepSeek) to generate text, potentially losing visual nuance, rather than evaluating end-to-end multimodal reasoning.
- What evidence would resolve it: A comparative evaluation showing that end-to-end VLMs achieve higher CAQ scores on the CliME dataset than the text-based pipeline method.

## Limitations
- Static pre-trained models may miss nuanced climate change signals in rapidly evolving meme language
- Fixed CAQ weights lack empirical justification and may not reflect true importance of dimensions
- Text descriptor approach may lose visual rhetorical elements (irony, humor) that are crucial to meme interpretation

## Confidence
- Mechanism validity: High - The three mechanisms are explicitly described and supported by paper evidence
- CAQ metric design: Medium - While methodology is clear, weight choices lack empirical justification
- Descriptor quality: High - 92% inter-annotator agreement provides strong validation
- Lens prompt effectiveness: Medium - Differentiated scores suggest impact but could be artifactual

## Next Checks
1. Verify ClimateBERT model versions match those used in the study by checking checksums
2. Test SpaCy dependency parser determinism by running completeness scoring on same inputs multiple times
3. Validate CAQ score calculations by reproducing table 1 values for a small sample of descriptors using the provided formulas