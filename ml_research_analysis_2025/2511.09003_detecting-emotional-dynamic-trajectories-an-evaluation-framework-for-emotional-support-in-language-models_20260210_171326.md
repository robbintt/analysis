---
ver: rpa2
title: 'Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional
  Support in Language Models'
arxiv_id: '2511.09003'
source_url: https://arxiv.org/abs/2511.09003
tags:
- emotional
- user
- emotion
- turn
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a user-centered evaluation framework for
  assessing emotional support in large language models. The framework constructs a
  benchmark of 328 emotional contexts and 1,152 disturbance events to simulate realistic
  emotional trajectories, and constrains model responses using validated emotion regulation
  strategies such as situation selection and cognitive reappraisal.
---

# Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional Support in Language Models

## Quick Facts
- **arXiv ID:** 2511.09003
- **Source URL:** https://arxiv.org/abs/2511.09003
- **Reference count:** 30
- **Primary result:** Introduces user-centered evaluation framework for emotional support in LLMs using 328 emotional contexts and 1,152 disturbance events to assess trajectory-level metrics (BEL, ETV, ECP).

## Executive Summary
This paper introduces a user-centered evaluation framework for assessing emotional support in large language models. The framework constructs a benchmark of 328 emotional contexts and 1,152 disturbance events to simulate realistic emotional trajectories, and constrains model responses using validated emotion regulation strategies such as situation selection and cognitive reappraisal. User emotional trajectories are modeled as a first-order Markov process, and causally-adjusted emotion estimation is applied to enable unbiased tracking of emotional states. Three trajectory-level metrics are introduced: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). Extensive evaluations across diverse LLMs reveal significant disparities in emotional support capabilities, with top models achieving up to 49.0 BEL and 22.92 ETV, demonstrating stronger performance in guiding users toward positive and stable emotional states over time.

## Method Summary
The framework evaluates LLMs' emotional support capabilities via dynamic, long-term multi-turn dialogues with emotional disturbance events. It uses 328 emotional contexts and 1,152 disturbance events, modeling user emotions as first-order Markov processes. Causally-adjusted emotion estimation via do-calculus removes confounding bias. The evaluation uses Skywork-Reward-V2-Llama-3.1-8B for sentiment scoring, ChatGPT-4o as user simulator, and 40-turn dialogues with 6 emotion regulation strategies. Three trajectory-level metrics (BEL, ETV, ECP) are computed from the estimated state sequences and transition matrices.

## Key Results
- Top-performing models achieve up to 49.0 BEL and 22.92 ETV, indicating strong performance in guiding users toward positive emotional states
- Models using situation selection strategy show better performance than those using situation modification in emotional regulation
- English contexts generally yield higher scores than Chinese contexts across most models
- Models demonstrate varying capabilities in recovering from disturbance events, with some failing to return to baseline emotional levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing confounding bias in emotion estimation improves alignment with human judgment of user state.
- **Mechanism:** The framework replaces standard correlation-based prediction with a causally-adjusted estimate using do-calculus to sever backdoor paths created by confounders like personality.
- **Core assumption:** User's internal thought process is deterministically influenced by history and prior state, enabling Monte Carlo sampling over dynamic distributions.
- **Evidence anchors:** Equation 4 defines post-intervention distribution; Section C.2 details Monte Carlo sampling implementation using k=8 samples.
- **Break condition:** If Reward Model fails to capture nuanced sentiment, causal adjustment amplifies noise rather than signal.

### Mechanism 2
- **Claim:** Modeling user emotion as a First-Order Markov Process enables trajectory-level stability metrics.
- **Mechanism:** By assuming P(s_t | s_{t-1}, ..., s_0) = P(s_t | s_{t-1}), the framework constructs transition matrix M to calculate ETV, weighting upward transitions more heavily than downward ones.
- **Core assumption:** Emotional transitions depend only on immediate preceding state, ignoring long-term memory effects in mathematical formulation.
- **Evidence anchors:** Equation 2 defines ETV using transition matrix to measure asymmetric transition advantages.
- **Break condition:** If user's emotional state depends heavily on events from t-n where n > 1, first-order assumption underestimates recovery difficulty.

### Mechanism 3
- **Claim:** Injecting controlled perturbations reveals model resilience better than static conversation.
- **Mechanism:** The framework introduces disturbance events (e.g., "harsh email from colleague") interleaved during dialogue to test Emotion Regulation Flexibility and recovery to baseline BEL.
- **Core assumption:** Simulated disturbances in role-play setting evoke realistic emotional degradation patterns in user-simulator.
- **Evidence anchors:** Figure 4 visualizes trajectories recovering (or failing to recover) after 0, 1, or 3 disturbance events.
- **Break condition:** If User Simulator is not sensitive to emotional valence of disturbance, trajectory remains flat, failing to stress-test the Agent.

## Foundational Learning

- **Concept:** **Do-Calculus & Causal Graphs**
  - **Why needed here:** Required to understand how paper derives Equation 4 from causal graph in Figure 2 to "intervene" on previous emotions.
  - **Quick check question:** Can you draw the backdoor path created by confounder U between S_{t-1} and S_t?

- **Concept:** **Markov Chains & Transition Matrices**
  - **Why needed here:** Essential for comprehending how BEL (average state) and ETV (transition asymmetry) are computed from empirical matrix M.
  - **Quick check question:** If user stays in neutral state for 10 turns, how does this affect transition probabilities m_{i,j}?

- **Concept:** **Gross's Process Model of Emotion Regulation**
  - **Why needed here:** Framework evaluates LLMs specifically on 6 strategies derived from this theory (e.g., Situation Selection vs. Cognitive Reappraisal).
  - **Quick check question:** Which strategy involves changing external environment vs. changing internal appraisal?

## Architecture Onboarding

- **Component map:** Benchmark Generator (328 Contexts + 1,152 Disturbances) -> Interaction Loop (User-Simulator <-> Agent-Under-Test) -> Estimator (Reward Model + Causal Adjustment Layer) -> Metrics Layer (BEL, ETV, ECP)
- **Critical path:** Context Setup -> Multi-turn Dialogue (40 turns) -> Causal Adjustment (sampling S'_{t-1}) -> Reward Model Inference -> Metric Aggregation
- **Design tradeoffs:** Simulation vs. Human Eval uses GPT-4o for scalability (118 envs * 40 turns), trading ecological validity for consistency; Continuous vs. Discrete uses theoretical discrete Markov states but practical continuous approximation due to data sparsity.
- **Failure signatures:** High BEL, Low ETV indicates model maintains baseline but fails to improve mood after disturbances (instability); Metric Saturation occurs if Reward Model miscalibrated, reducing discriminative power.
- **First 3 experiments:** 1) Validation Reproduction: Run Causal Estimation ablation on held-out set to verify adjustment layer improves accuracy over raw prediction; 2) Disturbance Stress Test: Evaluate baseline model (e.g., Llama-3) on "0 vs. 3 events" setting to visualize trajectory recovery divergence; 3) Metric Sensitivity Check: Calculate ETV for model using only "Cognitive Reappraisal" vs. "Situation Modification" to see which strategy drives higher volatility scores.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How accurately do LLM-simulated user trajectories reflect volatility and dynamics of real human emotional states?
- **Basis in paper:** [Explicit] Authors state they "employed ChatGPT-4o to simulate the user in each scenario" to mitigate costs.
- **Why unresolved:** Synthetic users may lack nuanced, irrational, or highly variable reactions of real humans, potentially biasing evaluation metrics.
- **What evidence would resolve it:** Correlation analysis between synthetic trajectory scores and scores derived from human-in-the-loop interactions.

### Open Question 2
- **Question:** Does first-order Markov assumption fail to capture critical long-term emotional dependencies or latent state changes in extended dialogues?
- **Basis in paper:** [Explicit] "User emotional trajectories are modeled as a first-order Markov process."
- **Why unresolved:** Assumption restricts memory to immediate previous turn, potentially ignoring cumulative impact of early dialogue history or user personality.
- **What evidence would resolve it:** Comparative modeling using higher-order Markov chains or hidden state models to check for prediction accuracy improvements.

### Open Question 3
- **Question:** What specific linguistic or cultural factors contribute to significant performance disparity in emotional support capabilities between English and Chinese contexts?
- **Basis in paper:** [Inferred] Tables 1 and 2 consistently show lower BEL and ETV scores for Chinese compared to English across most models.
- **Why unresolved:** Paper observes gap but does not isolate whether it stems from tokenization, training data, or differing cultural norms in emotional expression.
- **What evidence would resolve it:** Fine-grained linguistic analysis of failure cases and cross-cultural human evaluation of generated support responses.

## Limitations
- **Synthetic users:** Use of GPT-4o simulations rather than real human participants may not capture full complexity of human emotional responses
- **Data availability:** Full benchmark data (328 contexts, 1,152 events) not yet publicly released, limiting reproducibility
- **Model assumptions:** First-order Markov assumption may oversimplify emotional dynamics, particularly for long-term emotional states

## Confidence
- **High:** Framework's mathematical formulation (BEL, ETV, ECP) and use of validated emotion regulation strategies are well-specified and theoretically sound
- **Medium:** Causally-adjusted estimation method is novel, but practical impact depends on quality of Reward Model and fidelity of Monte Carlo approximation
- **Low:** Use of simulated users and lack of real human evaluation data introduces uncertainty about ecological validity

## Next Checks
1. **Causal Estimation Ablation:** Run causally-adjusted estimation (K=8 samples) vs. raw prediction on small held-out set to verify accuracy improvements
2. **Disturbance Recovery Test:** Evaluate baseline LLM (e.g., Llama-3) on 0 vs. 3 disturbance events to replicate Figure 4's trajectory recovery patterns
3. **Metric Sensitivity Analysis:** Compare ETV scores for models using only "Cognitive Reappraisal" vs. "Situation Modification" to assess which strategy drives higher volatility scores