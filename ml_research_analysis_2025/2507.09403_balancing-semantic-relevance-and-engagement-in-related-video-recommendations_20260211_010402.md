---
ver: rpa2
title: Balancing Semantic Relevance and Engagement in Related Video Recommendations
arxiv_id: '2507.09403'
source_url: https://arxiv.org/abs/2507.09403
tags:
- semantic
- relevance
- engagement
- video
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-objective retrieval framework that
  enhances standard two-tower models to explicitly balance semantic relevance and
  user engagement in related video recommendations. The approach combines multi-task
  learning (MTL) to jointly optimize co-engagement and semantic relevance, multimodal
  content features (textual and visual embeddings) for richer semantic understanding,
  and off-policy correction (OPC) via inverse propensity weighting to mitigate popularity
  bias.
---

# Balancing Semantic Relevance and Engagement in Related Video Recommendations

## Quick Facts
- arXiv ID: 2507.09403
- Source URL: https://arxiv.org/abs/2507.09403
- Reference count: 7
- Primary result: Multi-objective retrieval framework balancing semantic relevance and engagement, achieving +12.1% topic match rate and +0.04% engagement lift in production

## Executive Summary
This paper introduces a multi-objective retrieval framework that enhances standard two-tower models to explicitly balance semantic relevance and user engagement in related video recommendations. The approach combines multi-task learning (MTL) to jointly optimize co-engagement and semantic relevance, multimodal content features (textual and visual embeddings) for richer semantic understanding, and off-policy correction (OPC) via inverse propensity weighting to mitigate popularity bias. Evaluation on industrial-scale data and a two-week live A/B test demonstrates significant improvements in semantic relevance (from 51% to 63% topic match rate), reduction in popular item distribution (-13.8% popular video recommendations), and a +0.04% improvement in topline user engagement metric.

## Method Summary
The method implements a two-tower retrieval architecture with three key innovations: (1) multi-task learning combining co-engagement and semantic similarity losses with weighted sampling, (2) multimodal feature fusion incorporating BERT-encoded text and CNN-extracted visual embeddings, and (3) off-policy correction via inverse propensity weighting to address popularity bias. The semantic task uses pseudo-labels generated from BERT cosine similarity thresholds. Training optimizes a combined loss with OPC weights inversely proportional to trigger video popularity. The model serves via approximate nearest neighbor retrieval over fused embeddings.

## Key Results
- Semantic relevance improved from 51% to 63% topic match rate
- Popular video recommendations decreased by 13.8%
- Topline user engagement metric improved by +0.04% in production A/B test
- Offline evaluation showed +12.1% absolute lift in topic overlap with 1:500 semantic:engagement loss weighting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-task learning (MTL) jointly optimizing co-engagement and semantic similarity improves topical coherence with minimal engagement loss.
- **Mechanism**: A secondary semantic-aware sampled softmax loss is added to the standard co-engagement loss. Pseudo-labels are generated via BERT-encoded metadata cosine similarity; pairs exceeding a threshold are marked semantically aligned. The combined loss (Eq. 5) uses a weighted sum, with semantic loss weighted 10–1000x higher than engagement loss in practice.
- **Core assumption**: BERT-encoded metadata similarity is a reliable proxy for ground-truth semantic relatedness between video pairs.
- **Evidence anchors**:
  - [abstract]: "multi-task learning (MTL) to jointly optimize co-engagement and semantic relevance, explicitly prioritizing topical coherence"
  - [section II.A, Table I]: At 1:500 weighting, engagement drops from 39.0% to 37.9% while relevance rises from 35.1% to 46.9%—a favorable trade-off.
  - [corpus]: "Optimizing Recall or Relevance?" (FMR 0.55) similarly addresses multi-task retrieval balancing, supporting the general direction but not this specific formulation.
- **Break condition**: If semantic pseudo-labels from BERT are noisy (e.g., clickbait titles misaligned with actual content), the auxiliary task may reinforce spurious correlations rather than true topical coherence.

### Mechanism 2
- **Claim**: Fusing multimodal content embeddings (textual + visual) with collaborative embeddings enriches semantic representation and improves both relevance and engagement.
- **Mechanism**: Textual embeddings from BERT (titles, descriptions, tags) and visual/audio embeddings from CNNs are concatenated with CF embeddings, then passed through a dense layer. Gradients jointly refine all embedding types during training.
- **Core assumption**: Visual and audio features extracted from representative frames capture meaningful semantic attributes (style, objects, scenes) that correlate with user-perceived relevance.
- **Evidence anchors**:
  - [abstract]: "fusion of multimodal content features (textual and visual embeddings) for richer semantic understanding"
  - [section II.B, Table II]: MTL + text and video embeddings achieves 52.0% relevance (+5.1% absolute) and 38.3% engagement (+0.4% absolute) over MTL alone.
  - [corpus]: "Trend-Aware Fashion Recommendation" (FMR 0.47) demonstrates visual segmentation + semantic similarity benefits in a different domain, offering indirect support for multimodal fusion value.
- **Break condition**: If visual embeddings are extracted from unrepresentative frames or audio features are noisy, multimodal fusion may introduce signal dilution rather than enrichment.

### Mechanism 3
- **Claim**: Off-policy correction (OPC) via inverse propensity weighting reduces popularity bias and improves semantic relevance for niche content.
- **Mechanism**: Each training instance is reweighted by w_i = 1 / (α + count(trigger_i)^β), where infrequent triggers receive higher weights. This down-weights overrepresented popular videos in the loss function (Eq. 6–7).
- **Core assumption**: Training data logging policy (exposure distribution) is the primary source of popularity bias; correcting for it improves generalization to the true target distribution.
- **Evidence anchors**:
  - [abstract]: "off-policy correction (OPC) via inverse propensity weighting to effectively mitigate popularity bias"
  - [section II.C, Table III]: For non-popular triggers (bottom 10%), relevance improves from 34.3% to 41.5% (+7.2% absolute) with OPC.
  - [corpus]: "Finding Interest Needle in Popularity Haystack" (FMR 0.00 but thematically aligned) discusses IPS/OPC for exposure bias, noting limitations in extreme popularity tails—relevant for understanding OPC boundary conditions.
- **Break condition**: If α and β hyperparameters are poorly tuned, OPC may over-upweight rare noisy signals or under-correct, failing to achieve balanced coverage.

## Foundational Learning

- **Concept: Two-Tower Retrieval Architecture**
  - **Why needed here**: The base architecture separates candidate and query encoding into two neural towers, enabling approximate nearest neighbor (ANN) retrieval at scale. Understanding this is prerequisite to grasping how MTL and multimodal fusion integrate.
  - **Quick check question**: Can you explain why sampled softmax is used instead of full softmax for training efficiency?

- **Concept: Inverse Propensity Scoring (IPS) / Off-Policy Correction**
  - **Why needed here**: OPC is central to bias mitigation. IPS reweights logged bandit feedback to estimate the value of a target policy under a logging policy. Essential for understanding Eq. 6–7.
  - **Quick check question**: Why does weighting training examples inversely to trigger frequency counteract popularity bias?

- **Concept: Multi-Task Learning Gradient Interference**
  - **Why needed here**: MTL combines losses with different optimization landscapes. Understanding gradient magnitudes and task weighting (10:1 to 1000:1 ratios here) is critical to avoid one task dominating.
  - **Quick check question**: What happens if the semantic loss is weighted too high relative to engagement loss?

## Architecture Onboarding

- **Component map**:
  - Input Layer -> Embedding Towers (Text BERT, Visual CNN, Audio CNN) -> Dense Fusion Layer -> Co-Engagement Task Head + Semantic Task Head -> Weighted Loss Aggregation -> OPC Weighting -> Training Output

- **Critical path**:
  1. Data pipeline: Extract co-engagement pairs from user sessions; compute BERT text embeddings and CNN visual embeddings offline
  2. Training: Joint MTL optimization with OPC reweighting; monitor engagement recall@K and topic overlap % on held-out set
  3. Serving: Index fused embeddings; retrieve top-K candidates for trigger video; apply business logic filters

- **Design tradeoffs**:
  - **Semantic vs. Engagement Weighting**: Higher semantic weight (1:1000) improves relevance (+12.1% absolute) but drops engagement (-2.5% absolute). Production needs calibrated balance.
  - **OPC Hyperparameters (α, β)**: Control smoothness of popularity correction. Too aggressive → over-promotion of niche content with weak signals.
  - **Multimodal Granularity**: More frames → richer visual signal but higher extraction cost; text-only is cheaper but misses visual semantics.

- **Failure signatures**:
  - **Relevance gains but engagement crashes**: Semantic loss weight too high; OPC over-correcting.
  - **Popular items still dominate**: α too large or β too small; OPC ineffective.
  - **Cold-start items never recommended**: Multimodal features not indexed for new items; fallback to CF-only path.

- **First 3 experiments**:
  1. **MTL Weight Ablation**: Vary λ_semantic : λ_engagement ratios (1:1, 1:10, 1:100, 1:500, 1:1000) on offline data; plot engagement vs. relevance Pareto frontier to select production weight.
  2. **Multimodal Feature Impact**: Train MTL variants with (a) text only, (b) video only, (c) both; measure lift in topic overlap % and recall@K to justify feature extraction cost.
  3. **OPC Hyperparameter Sensitivity**: Grid search α ∈ [1, 10, 100] and β ∈ [0.5, 0.75, 1.0]; evaluate relevance for bottom-10% popularity triggers to confirm bias mitigation without overall engagement collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the weighting ratio between semantic and engagement losses be theoretically determined or dynamically adapted rather than manually tuned?
- **Basis in paper**: [explicit] The authors state they "empirically set [weights] larger than (e.g., 10:1 or 100:1 ratio)" and tested ratios from 1:1 to 1:1000 without providing a theoretical justification for the optimal 1:500 selection.
- **Why unresolved**: Heuristic tuning is computationally expensive and may not generalize well to data distributions that shift over time.
- **What evidence would resolve it**: Successful integration of an adaptive multi-task learning algorithm (e.g., gradient normalization) that converges to optimal weights without manual intervention.

### Open Question 2
- **Question**: Does the use of binary pseudo-labels for semantic similarity result in a loss of fine-grained representation nuance?
- **Basis in paper**: [inferred] The semantic similarity task relies on a binary indicator based on a hard cosine similarity threshold, discarding the continuous distance values between video embeddings.
- **Why unresolved**: Treating semantic similarity as a binary classification problem may obscure subtle relationships between videos that are "somewhat similar" versus "highly similar."
- **What evidence would resolve it**: A comparative study showing whether a regression loss on continuous similarity scores yields higher topic match rates or user satisfaction than the proposed binary classification approach.

### Open Question 3
- **Question**: Is weighting training instances solely by trigger popularity sufficient to correct for popularity bias in the candidate pool?
- **Basis in paper**: [inferred] The Off-Policy Correction (OPC) mechanism (Equation 7) calculates inverse propensity weights based exclusively on the frequency of the trigger video, ignoring the popularity of the candidate video.
- **Why unresolved**: The model may still overfit to popular *candidate* videos if they appear frequently in the negative sampling distribution or if the engagement signals are skewed toward popular items regardless of the trigger.
- **What evidence would resolve it**: An ablation study analyzing the distribution of recommended candidate popularity when correcting for trigger popularity versus correcting for both trigger and candidate popularity.

## Limitations

- BERT-based pseudo-labels for semantic similarity may introduce noise if metadata doesn't reflect true topical content
- Multimodal feature quality and architecture details are underspecified, making reproduction challenging
- OPC's effectiveness on popular items and potential for over-correction in extreme cases remains unclear

## Confidence

- **High confidence**: The multi-task learning framework (MTL) combining co-engagement and semantic similarity tasks demonstrably improves topical coherence without severe engagement loss, supported by clear offline metrics and production A/B test results.
- **Medium confidence**: The multimodal fusion (text + visual) enriches semantic representation, though the absolute lift is modest and the method's robustness to varying feature quality is untested.
- **Low confidence**: OPC's effectiveness is primarily validated for non-popular items; its impact on popular items and potential for over-correction in extreme cases remains unclear.

## Next Checks

1. **Semantic Label Quality Audit**: Conduct a small-scale human evaluation comparing BERT pseudo-labels to ground-truth topical relevance for a sampled set of video pairs to quantify label noise and correlation.
2. **Multimodal Feature Sensitivity**: Train and evaluate models with (a) text-only, (b) visual-only, (c) audio-only, and (d) combined multimodal features to isolate the contribution of each modality and test robustness to feature degradation.
3. **OPC Over-Correction Stress Test**: Simulate extreme popularity distributions (e.g., power-law with varying skew) and evaluate model performance on both popular and niche items to identify OPC's breaking point and optimal α, β settings.