---
ver: rpa2
title: Neighbor Embeddings Using Unbalanced Optimal Transport Metrics
arxiv_id: '2509.19226'
source_url: https://arxiv.org/abs/2509.19226
tags:
- data
- embeddings
- datasets
- euclidean
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the use of the Hellinger\u2013Kantorovich\
  \ metric from unbalanced optimal transport (UOT) in a dimensionality reduction and\
  \ learning pipeline for image data. The primary problem addressed is the limitation\
  \ of traditional Wasserstein metrics in optimal transport, which require probability\
  \ measures and cannot account for variations in mass."
---

# Neighbor Embeddings Using Unbalanced Optimal Transport Metrics

## Quick Facts
- arXiv ID: 2509.19226
- Source URL: https://arxiv.org/abs/2509.19226
- Reference count: 25
- Primary result: UOT-based embeddings outperform OT-based embeddings in classification 81% of the time and clustering 83% of the time on MedMNIST datasets

## Executive Summary
This paper investigates the use of the Hellinger–Kantorovich metric from unbalanced optimal transport (UOT) in dimensionality reduction and learning pipelines for image data. The study addresses the limitation of traditional Wasserstein metrics in optimal transport, which require probability measures and cannot account for variations in mass. UOT, via the Hellinger–Kantorovich metric, allows for different total masses of data measures by creating or annihilating mass, potentially capturing more meaningful data geometry.

The core method applies UOT-based metrics in manifold learning algorithms (MDS, Isomap, t-SNE, and Laplacian Eigenmaps) and compares their performance against Euclidean and standard OT-based methods for classification and clustering tasks. Using benchmark datasets including MedMNIST, MNIST, and Coil-20, the study finds that UOT consistently outperforms OT-based embeddings in both clustering and classification tasks, suggesting that UOT metrics are better suited for modeling image data with varying masses and complex structures.

## Method Summary
The method computes pairwise distances between images using Euclidean, Wasserstein (W₂), and Hellinger-Kantorovich (HK₁) metrics. These distances are then used as input to four manifold learning algorithms: MDS, Isomap, t-SNE, and Laplacian Eigenmaps. The resulting low-dimensional embeddings are evaluated for classification (using 8 classifiers: LDA, 1/3/5-NN, SVM-L/R, RF, MLR) and clustering (k-means, spectral). The study uses 1,000 samples per experiment per dataset, with 80/20 train-test splits, and repeats experiments 10 times with different random seeds to assess statistical significance.

## Key Results
- UOT outperforms OT in classification 81% of the time and in clustering 83% of the time on MedMNIST datasets
- UOT embeddings yield better task performance compared to Euclidean embeddings in 45% of classification trials
- UOT outperforms OT in 69% of classification trials overall

## Why This Works (Mechanism)

### Mechanism 1: Mass Variation Handling via Creation/Annihilation
- Claim: UOT metrics capture more meaningful data geometry when measures have unequal total mass
- Mechanism: The Hellinger-Kantorovich formulation adds KL divergence penalties on marginal deviations from the input measures, allowing the optimization to create or destroy mass rather than force all mass to be transported
- Core assumption: Mass differences in image data carry semantic meaning (e.g., varying brightness, texture density, cell counts) that should inform embeddings
- Evidence anchors:
  - [abstract] "UOT, via the Hellinger–Kantorovich metric, allows for different total masses of data measures by creating or annihilating mass, potentially capturing more meaningful data geometry"
  - [section 1] "mass naturally differs and carries meaning"
  - [corpus] "Structured Matching via Cost-Regularized UOT" notes UOT is useful for matching nonnegative measures where mass varies; corpus evidence for image-specific mass semantics is limited
- Break condition: When all measures have approximately equal total mass, UOT may reduce to standard OT with minimal advantage

### Mechanism 2: Local Spatial Information Preservation via Transport Geometry
- Claim: Transport-based metrics preserve spatial relationships in images better than Euclidean vectorization
- Mechanism: The HK cost function $c_\kappa = -2\log\cos(|x-y|/\kappa)$ for $|x-y| \leq \kappa\pi/2$ penalizes spatial displacement, so distances reflect minimal-energy morphing rather than pixel-wise differences
- Core assumption: Image semantics depend on spatial locality (shape, morphology) more than absolute pixel positions
- Evidence anchors:
  - [abstract] "UOT metrics are better suited for modeling image data with varying masses and complex structures"
  - [section 2] "the Wasserstein distance between two images corresponds to a minimal energy morphing of one image into another"
  - [corpus] "Structured Matching" notes cost function selection remains challenging; spatial preservation claim is paper-specific
- Break condition: When $\kappa$ is too small, mass is destroyed rather than transported, potentially losing spatial structure; when images lack spatial coherence, transport may offer no advantage

### Mechanism 3: Low-Dimensional Structure Discovery in Transport Space
- Claim: Modeling data in HK metric space can reveal lower-dimensional manifold structure not visible in Euclidean space
- Mechanism: Translated indicator functions form a 2D manifold in $(M_+(\Omega), HK_\kappa)$ space, nearly isometric to translation parameters, whereas vectorized versions are high-rank
- Core assumption: Image datasets lie on or near low-dimensional manifolds in transport geometry
- Evidence anchors:
  - [abstract] "UOT embeddings yield better task performance compared to Euclidean embeddings in 45% of classification trials"
  - [section 3] MedMNIST datasets have "varied in terms of texture, morphology, and complexity"
  - [corpus] No independent corpus validation of manifold structure claims
- Break condition: If data does not exhibit manifold structure in transport space, embedding gains diminish; embedding dimension heuristic may fail for some datasets

## Foundational Learning

- Concept: **Wasserstein/Optimal Transport Distance**
  - Why needed here: Forms the baseline that UOT extends; understanding couplings and mass conservation is prerequisite
  - Quick check question: Given two 1D histograms, can you sketch why moving mass has a cost, and why W₂ requires equal total mass?

- Concept: **Manifold Learning (MDS, Isomap, t-SNE, Laplacian Eigenmaps)**
  - Why needed here: The paper applies HK distances as input metrics to these algorithms; knowing what each optimizes for helps interpret results
  - Quick check question: Which of these methods preserves global vs. local structure, and how does the input distance matrix affect the embedding?

- Concept: **Kullback-Leibler Divergence and Unbalanced Transport**
  - Why needed here: KL terms in the HK objective enable mass creation/destruction; understanding regularization vs. hard constraints is key
  - Quick check question: Why would adding KL penalties on marginals allow solutions with unequal input/output mass?

## Architecture Onboarding

- Component map:
  1. Distance Computation Layer: Compute pairwise distances (Euclidean, W₂ via POT library, HK₁ via referenced codebase)
  2. Graph Construction Layer: Build k-NN or ε-neighborhood graph from distance matrix
  3. Embedding Layer: Apply MDS/Isomap/t-SNE/Laplacian Eigenmaps to obtain low-dimensional representation
  4. Task Layer: Train classifiers (LDA, KNN, SVM, RF, MLR) or clusterers (k-means, spectral)

- Critical path: Distance computation (especially HK) → graph construction → embedding. Performance gains come from better distance geometry, not the downstream classifier.

- Design tradeoffs:
  - Computational cost: HK distances are more expensive than Euclidean; paper uses 1,000 points per experiment
  - Scale parameter κ: Controls transport vs. creation/destruction tradeoff; paper uses HK₁ (κ=1) but provides no tuning guidance
  - Embedding dimension: Variance-explained heuristic (α=0.97–0.999) may not generalize; some datasets required ad-hoc adjustment

- Failure signatures:
  - SVM(L) consistently fails with OT embeddings (e.g., BloodMNIST: 0.21 vs. 0.63 Euclidean), suggesting transport geometry may not align with linear separators
  - RetinaMNIST shows mixed results (UOT wins only ~50%), suggesting not all datasets benefit
  - Low embedding dimensions (e.g., dim=1 from 97% variance) may collapse useful structure

- First 3 experiments:
  1. Replicate HK distance computation on a small image subset (e.g., 50 samples from BloodMNIST) and verify against expected behavior (e.g., translated disks should have approximately linear HK distances)
  2. Run ablation: Same dataset, same embedding method (MDS), vary only the metric (Euclidean vs. W₂ vs. HK₁) with a simple classifier (1-NN) to isolate metric effect
  3. Parameter sensitivity: Test κ ∈ {0.5, 1.0, 2.0, 5.0} on one MedMNIST dataset to characterize how scale affects clustering/classification accuracy

## Open Questions the Paper Calls Out

- Question: What specific geometric features of images (e.g., texture vs. morphology) are primarily responsible for the performance gains of Unbalanced Optimal Transport (UOT) over standard Optimal Transport (OT)?
  - Basis in paper: [explicit] The conclusion states, "It would be interesting to further explore exactly what facets of data geometry are better captured in the HK metrics."
  - Why unresolved: The authors hypothesize that UOT's ability to create/destroy mass captures texture and morphology better, but they do not isolate these variables to prove which specific structural changes drive the improvement.
  - What evidence would resolve it: Ablation studies on synthetic datasets where texture complexity and morphological variations are controlled independently to measure their correlation with UOT embedding quality.

- Question: How sensitive is the quality of the resulting embeddings to the choice of the length scale parameter $\kappa$ in the Hellinger–Kantorovich metric?
  - Basis in paper: [inferred] The Background section defines the metric cost $c_\kappa$ dependent on $\kappa$, but the Methodology section does not specify the value of $\kappa$ used for the "HK1" experiments or the sensitivity of the results to this hyperparameter.
  - Why unresolved: While the benefits of mass variation are discussed, the role of the distance scale $\kappa$ (which dictates when mass is transported vs. destroyed) is not analyzed, leaving a potential instability unexplored.
  - What evidence would resolve it: A parameter sensitivity analysis showing classification accuracy across a range of $\kappa$ values for the MedMNIST datasets.

- Question: Can a theoretically grounded intrinsic dimension estimator specific to UOT geometry replace the generic local SVD heuristic to stabilize embedding dimension selection?
  - Basis in paper: [inferred] The Methodology section notes that the standard SVD-based heuristic required manual augmentation (changing variance threshold $a$ from 0.97 to 0.99/0.999) for certain datasets to avoid degenerate dimensions.
  - Why unresolved: The need to manually adjust the dimensionality heuristic suggests that the geometry of UOT data is not adequately captured by standard Euclidean variance estimators, potentially biasing the downstream comparisons.
  - What evidence would resolve it: Developing and testing an intrinsic dimension estimator based on UOT distances that automatically selects the optimal embedding dimension without manual threshold tuning.

## Limitations
- Parameter tuning ambiguity: Critical parameters like neighborhood size, HK scale κ, and classifier hyperparameters are not specified
- Dataset-specific variability: Performance gains vary significantly across datasets, with some showing minimal benefit
- Computational cost: HK distance computation is significantly more expensive than Euclidean, but no runtime analysis is provided

## Confidence
- High confidence: UOT outperforms standard OT on MedMNIST datasets for both classification (81% of cases) and clustering (83% of cases)
- Medium confidence: UOT provides better embeddings than Euclidean in 45% of classification trials and 69% of OT comparison trials
- Low confidence: Claims about low-dimensional manifold structure discovery in transport space lack independent corpus validation

## Next Checks
1. **Parameter sensitivity analysis**: Test HK scale κ ∈ {0.5, 1.0, 2.0, 5.0} on BloodMNIST to characterize how transport vs. creation/destruction tradeoff affects task performance
2. **Ablation study**: Same dataset and embedding method (MDS), vary only metric (Euclidean vs. W₂ vs. HK₁) with 1-NN classifier to isolate metric effect
3. **Reproducibility verification**: Replicate HK distance computation on 50 translated disk images and verify distances approximate translation parameters linearly