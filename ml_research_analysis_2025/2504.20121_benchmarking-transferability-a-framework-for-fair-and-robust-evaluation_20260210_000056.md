---
ver: rpa2
title: 'Benchmarking Transferability: A Framework for Fair and Robust Evaluation'
arxiv_id: '2504.20121'
source_url: https://arxiv.org/abs/2504.20121
tags:
- transferability
- source
- metrics
- datasets
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransferTest, a comprehensive benchmarking
  framework for evaluating transferability estimation metrics. The framework systematically
  assesses how well different metrics predict transfer learning performance across
  varying conditions, including different source datasets, model complexities, and
  fine-tuning strategies.
---

# Benchmarking Transferability: A Framework for Fair and Robust Evaluation

## Quick Facts
- **arXiv ID:** 2504.20121
- **Source URL:** https://arxiv.org/abs/2504.20121
- **Reference count:** 40
- **Primary result:** Introduces TransferTest framework and proposes Wasserstein distance metric that achieves 3.5% improvement over existing methods for head-training fine-tuning scenarios.

## Executive Summary
This paper introduces TransferTest, a comprehensive benchmarking framework for evaluating transferability estimation metrics. The authors identify key limitations of existing transferability metrics including dependency on labeled target data, assumptions about ImageNet pre-training, lack of consideration for model complexity variations, and focus on limited fine-tuning strategies. To address these issues, they propose a novel weight-based transferability metric using Wasserstein distance between original and fine-tuned model weights, eliminating the need for target labels. Through extensive experiments across 12 benchmark datasets and various model configurations, the study reveals that traditional metrics perform poorly when source datasets differ from ImageNet, while the proposed Wasserstein distance metric demonstrates more consistent performance across diverse scenarios.

## Method Summary
The proposed method follows a three-step process: (1) generate pseudo-labels for the target dataset using the pre-trained model, (2) fine-tune the model for 2 epochs on the pseudo-labeled target data, and (3) compute the Wasserstein distance between the original and fine-tuned weight distributions. The Wasserstein distance is calculated between empirical distributions of model parameters after flattening them into 1D arrays. The metric is evaluated using weighted Kendall's tau correlation against ground-truth fine-tuning accuracy across multiple source-target dataset pairs and model architectures.

## Key Results
- Traditional metrics (LEEP, LogME, SFDA, ETran) show significant performance degradation when source datasets deviate from ImageNet
- The proposed Wasserstein distance metric achieves more consistent performance across diverse source datasets, particularly excelling with non-ImageNet sources
- In head-training fine-tuning scenarios (training only classification heads), the Wasserstein metric achieves a 3.5% improvement over existing methods
- The metric demonstrates superior performance when evaluated across 12 benchmark datasets, 11 supervised architectures, and 10 self-supervised models

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Label Adaptation Distance
- **Claim:** Wasserstein distance between pre- and post-fine-tuning weights correlates with transferability, conditional on pseudo-labels providing meaningful proxy for target distribution
- **Mechanism:** Generate pseudo-labels via `argmax(ϕl(X))`, fine-tune for 2 epochs on `(X, Ŷ)`, compute `T_Wasserstein = -W(θ, θ')`. Smaller weight movement → higher score. Intuition: models already aligned to target require less parameter shift
- **Core assumption:** Pseudo-labels from source model capture sufficient structure of target task for short fine-tuning to reflect genuine adaptation difficulty
- **Break condition:** When pseudo-label accuracy is extremely low (source and target have disjoint label semantics), weight shift may reflect noise rather than meaningful adaptation

### Mechanism 2: Distribution-Aware Weight Comparison
- **Claim:** Wasserstein distance outperforms element-wise metrics (L1, L2) for comparing weight distributions because it captures geometry of distribution shifts, not just per-parameter magnitude
- **Mechanism:** Wasserstein minimizes "work" to transform distribution P → Q, accounting for cross-support differences and overall structure
- **Core assumption:** Distribution of weight changes—not just their magnitude—encodes transfer-relevant information
- **Break condition:** If weight distributions are multi-modal with complex covariance structure, 1-Wasserstein on flattened parameters may miss critical structure

### Mechanism 3: Source-Dataset Independence via Weight Dynamics
- **Claim:** Weight-based metrics are less sensitive to source pre-training dataset than feature-based metrics (LogME, SFDA, ETran), which implicitly assume ImageNet-like distributions
- **Mechanism:** Feature-based metrics depend on learned representations encoding source-specific inductive biases. Weight dynamics measure adaptation plasticity, more invariant to source
- **Core assumption:** Adaptation plasticity generalizes across source domains better than feature statistics
- **Break condition:** When source and target have fundamentally different input modalities or dimensionalities, weight-based metrics may fail if architecture compatibility is not ensured

## Foundational Learning

- **Concept: Transferability Estimation Metrics**
  - **Why needed here:** The paper benchmarks LEEP, LogME, SFDA, ETran, and PACTran. Understanding what each measures is prerequisite to interpreting comparison results
  - **Quick check question:** Given a pre-trained ResNet-50 and unlabeled target dataset, which existing metrics could you apply without target labels?

- **Concept: Wasserstein (Earth Mover's) Distance**
  - **Why needed here:** Core to proposed method. Distinguishes it from L1/L2 weight differences
  - **Quick check question:** If two weight distributions have same mean but different variances, would L2 distance capture this? Would Wasserstein?

- **Concept: Kendall's Tau (Weighted)**
  - **Why needed here:** Evaluation metric for ranking correlation. Weighted version prioritizes top-ranked models
  - **Quick check question:** Why might weighted Kendall's tau be preferred over Spearman correlation for model selection benchmarks?

## Architecture Onboarding

- **Component map:**
  Source-Hub (ImageNet, CIFAR-100, Caltech101, Caltech256, Flowers102) -> Target-Hub (12 datasets: Aircraft, Cars, Food101, Pets, Flowers, CIFAR-10/100, VOC2007, SUN397, DTD, etc.) -> Model-Hub (11 supervised + 10 SSL models) -> Scoring Module (ETran, SFDA, LogME, PACTran, Wasserstein) -> Evaluation (Weighted Kendall's tau against ground-truth fine-tuning accuracy)

- **Critical path:**
  1. Load pre-trained `ϕl` with parameters `θl`
  2. Generate pseudo-labels `Ŷ = argmax(ϕl(X))`
  3. Fine-tune 2 epochs on `(X, Ŷ)` → `θ'l`
  4. Compute `T_Wasserstein = -W1(Pθl, Pθ'l)`
  5. Rank models; compute τ against fine-tuned validation accuracies

- **Design tradeoffs:**
  - 2-epoch fine-tuning: Faster but may not reach steady-state adaptation. Paper shows this is sufficient; more epochs may improve at computational cost
  - Flattened weight distributions: Simple but loses layer-wise structure. Could extend to per-layer Wasserstein
  - Pseudo-label quality: Trade-off between label-free requirement and pseudo-label noise sensitivity

- **Failure signatures:**
  - Very low Kendall's τ (> 0.1) on specific source-target pairs → check pseudo-label accuracy
  - High variance across model complexity levels → verify weight distribution computation is consistent across architectures
  - Wasserstein score near zero for all models → check fine-tuning is actually updating weights (learning rate issue)

- **First 3 experiments:**
  1. **Sanity check:** Replicate ImageNet→CIFAR-10 transferability ranking from Table 5a. Verify τ for Wasserstein vs. LogME
  2. **Ablation on epochs:** Run pseudo-label fine-tuning with {1, 2, 5, 10} epochs on subset. Plot τ vs. epochs to validate 2-epoch choice
  3. **Source-dataset stress test:** Take Caltech101-trained models → target Flowers102. Compare Wasserstein vs. ETran degradation from Table 5a baselines

## Open Questions the Paper Calls Out
1. Can a hybrid approach combining feature-based metrics (e.g., LogME) and the proposed weight-based Wasserstein metric outperform individual metrics?
2. What are the theoretical connections between Wasserstein weight distribution shifts and downstream transferability?
3. Does the weight-based transferability metric generalize to non-classification tasks such as object detection or semantic segmentation?

## Limitations
- The paper's claim that pseudo-labels are sufficient for meaningful weight-based transferability estimation remains untested beyond the paper's own experiments
- The exclusion of SSL models in the source-dataset robustness analysis leaves a gap in understanding performance with modern pre-training methods
- The assumption that 2-epoch fine-tuning captures stable adaptation patterns may not generalize to domains requiring longer adaptation periods

## Confidence
- **High confidence:** Wasserstein distance as a weight comparison metric (well-established mathematical foundation)
- **Medium confidence:** The proposed metric's consistent performance across diverse source datasets (supported by experiments but not extensively validated on SSL models)
- **Low confidence:** The sufficiency of 2-epoch pseudo-label fine-tuning for capturing transferability signals (minimal ablation on fine-tuning duration)

## Next Checks
1. Test the metric's performance when pseudo-label accuracy drops below 50% on challenging target domains
2. Extend source-dataset analysis to include SSL pre-trained models (MoCo, BYOL, etc.) to verify robustness claims
3. Compare weight-based metrics against compute-free alternatives like K-Nearest Neighbors on the same benchmarking framework