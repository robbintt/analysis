---
ver: rpa2
title: 'Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of
  Superficial Editing'
arxiv_id: '2505.12636'
source_url: https://arxiv.org/abs/2505.12636
tags:
- uni00000013
- uni00000011
- uni00000051
- uni00000057
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and analyzes "superficial editing" in knowledge
  editing, where models appear successfully edited but revert to original knowledge
  under specific prompts. The authors evaluate multiple editing algorithms and find
  they remain vulnerable despite strong conventional metrics.
---

# Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing

## Quick Facts
- arXiv ID: 2505.12636
- Source URL: https://arxiv.org/abs/2505.12636
- Reference count: 40
- Primary result: Models exhibit "superficial editing" where they revert to original knowledge under specific attack prompts despite strong conventional editing metrics

## Executive Summary
This paper identifies and analyzes "superficial editing" in knowledge editing, where models appear successfully edited but revert to original knowledge under specific attack prompts. The authors evaluate multiple editing algorithms and find they remain vulnerable despite strong conventional metrics. Through systematic mechanistic analysis, they identify two key factors: (1) residual stream at the last subject position in earlier layers, and (2) specific attention modules in later layers. They demonstrate that certain attention heads and their corresponding left singular vectors encode original knowledge and causally contribute to superficial editing. The findings are validated through intervention experiments and ablation studies. The analysis framework is also applied to superficial unlearning, showing consistent patterns, thereby demonstrating robustness and broader applicability of both methodology and conclusions.

## Method Summary
The authors systematically investigate superficial editing through a three-stage approach. First, they construct attack probe datasets (CF-a and ZsRE-a) from CounterFact and ZsRE base datasets using three attack prefix types: Wiki(o) (Wikipedia summary), Rep(o) (entity repetition), and Que(o) (question about original triple). Second, they evaluate seven editing algorithms (FT, MEND, ROME, MEMIT, PMET, r-ROME, AlphaEdit) on three model architectures, computing both conventional metrics (Efficacy, Generalization, Locality) and superficial editing metrics (Original Match, Original Probability). Third, they perform mechanistic analysis through layer-wise residual stream interventions, logit lens projections, attention head prominence scoring (LOPH), singular value decomposition of attention outputs, and causal ablation experiments to identify the specific mechanisms driving superficial editing.

## Key Results
- Superficial editing exists across all tested models and editing algorithms, with Original Match rates reaching 30-80% under attack probes
- Earlier layers show inhibited knowledge enrichment at the last subject position during attack probe processing (higher Inhibition Scores)
- Later attention modules exhibit "Reversal of Residual Stream" (RRS) behavior, incorporating original knowledge and causing original answer probability to exceed new answer probability
- Top 5-10% of left singular vectors in specific attention head output matrices encode original knowledge and causally contribute to superficial editing when ablated
- The mechanistic framework applies consistently to superficial unlearning scenarios, demonstrating broader validity

## Why This Works (Mechanism)

### Mechanism 1: Residual Stream Inhibition at Last Subject Position
- Claim: Attack probes suppress new knowledge enrichment at the last subject position in earlier layers, while original knowledge accumulation remains limited.
- Mechanism: The attack prefix introduces information that interferes with the model's ability to propagate edited knowledge through the residual stream during early-layer processing (layers 5-15 in LLaMA3-8B). This is measured via Inhibition Score (IS) = -log P(o*|h), where higher scores indicate stronger suppression.
- Core assumption: Knowledge enrichment occurs primarily at the last subject token position during early layers, following subject enrichment patterns from prior work (Geva et al., 2023).
- Evidence anchors:
  - [abstract] "the residual stream at the last subject position in earlier layers"
  - [Section 4.2, Figure 4] IS values for corrupted runs exceed clean runs in earlier layers; Figure 5 shows original answer ranking consistently lags behind new answer in early layers
  - [corpus] Related work on over-attention in knowledge editing (arxiv:2502.14838) suggests attention-based interference patterns
- Break condition: If early-layer interventions do not shift prediction probabilities, the mechanism does not hold for that model architecture.

### Mechanism 2: Reversal of Residual Stream (RRS) via Later Attention
- Claim: Later-layer attention modules actively incorporate original knowledge into the last token position, causing original answer probability to exceed new answer probability—a prerequisite for superficial editing.
- Mechanism: While MLP layers consistently suppress original answer probability (output < input), specific attention modules in later layers show the inverse pattern (output probability of original answer > input). This "RRS" phenomenon is causally linked to superficial editing.
- Core assumption: The last token position aggregates information for next-token prediction, and attention modules can selectively retrieve stored knowledge.
- Evidence anchors:
  - [abstract] "specific attention modules in later layers... exhibit a causal relationship with superficial editing"
  - [Section 4.1.2, Figure 3] Latent probability analysis shows attention output > input for original answer in later layers; ablation of critical attention layers eliminates RRS (Figure 6)
  - [corpus] Corpus evidence for this specific mechanism is limited; related attention floating mechanisms in other architectures (arxiv:2601.07894) may offer parallels but are not directly applicable
- Break condition: If ablating later attention modules does not reduce OAP below NAP, this mechanism is not primary for that model.

### Mechanism 3: Left Singular Vectors Encode Original Knowledge
- Claim: Specific left singular vectors in attention head output matrices (W_O) encode original knowledge and causally contribute to superficial editing.
- Mechanism: Via SVD, attention head output z = Σ λᵢuᵢ, where uᵢ are left singular vectors. Ablating top 5-10% of vectors (by contribution to original answer decoding) reduces OAP by 8-13 points while increasing NAP.
- Core assumption: Knowledge is linearly encoded in attention output subspaces and can be isolated via spectral decomposition.
- Evidence anchors:
  - [abstract] "specific attention heads... along with specific left singular vectors in their output matrices, encapsulate the original knowledge"
  - [Section 4.3.3, Table 3-4] DSR for original answer (50-75%) vastly exceeds new answer (0-9%); ablation reduces OAP consistently across models
  - [corpus] No direct corpus evidence for SVD-based knowledge localization in editing literature
- Break condition: If identified singular vectors do not show differential DSR between original and new answers, the decomposition approach fails for that head.

## Foundational Learning

- Concept: Knowledge Editing vs. Fine-tuning
  - Why needed here: Understanding that editing algorithms (ROME, MEMIT) modify specific parameters without full retraining, but may not fully overwrite distributed knowledge representations.
  - Quick check question: Can you explain why editing efficacy metrics (Eff., Gen., Loc.) may not capture whether original knowledge persists in the model?

- Concept: Residual Stream and Logit Lens
  - Why needed here: The intervention experiments rely on patching hidden states h^(l)_t across layers and decoding them via the unembedding matrix to observe latent probabilities.
  - Quick check question: Given a hidden state at layer l, how would you compute the latent probability of token t using the logit lens?

- Concept: Singular Value Decomposition for Circuit Analysis
  - Why needed here: The paper uses SVD to decompose attention head outputs into interpretable directions; understanding this is essential for replicating the singular vector ablation experiments.
  - Quick check question: If W_O = UΣV^T, what does the i-th left singular vector uᵢ represent in terms of the output space?

## Architecture Onboarding

- Component map:
  - Residual streams: Layer-wise hidden states at two positions (last subject token, last token)
  - MLP layers: Suppress original answer probability across layers
  - Attention modules: Later layers (e.g., L27 in LLaMA3-8B) show RRS behavior
  - Attention heads: Specific heads (e.g., L23H27, L27H20) with high LOPH values
  - Singular vectors: Top 5-10% of left singular vectors by DSR contribution

- Critical path:
  1. Generate attack probes (Wiki/Rep/Que types) from CounterFact/ZsRE
  2. Run clean (baseline prompt) and corrupted (attack probe) forward passes
  3. Intervene at last subject position (early layers) and last position (late layers)
  4. Identify prominent attention heads via LOPH > τ (τ=0.1)
  5. Decompose W_O via SVD, identify causal singular vectors
  6. Ablate vectors and measure OAP/NAP changes

- Design tradeoffs:
  - Threshold τ for prominent heads: Too high misses causal heads; too low includes noise
  - Top-p% for singular vectors: 5% more precise, 10% more comprehensive but may include non-causal directions
  - Dataset filtering: Only cases where model initially knows the fact and edit succeeds conventionally are retained

- Failure signatures:
  - No RRS observed: Model may not exhibit superficial editing for that attack type
  - LOPH uniformly low: Attention may not be the primary mechanism (check MLP)
  - Ablation has no effect: Knowledge may be distributed across multiple heads/layers

- First 3 experiments:
  1. Replicate Figure 2: Run residual stream interventions on your target model to identify which layers show causal effects at last subject vs. last token positions.
  2. Replicate Figure 7: Compute LOPH for all attention heads under attack probe input; identify heads with LOPH > 0.1.
  3. Replicate Table 4: For top 3 prominent heads, perform SVD and ablate top 5% singular vectors; verify OAP decreases and NAP increases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can effective mitigation strategies be developed to prevent superficial editing while preserving model utility?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "The development of effective mitigation strategies for superficial editing remains an open challenge" and identify it as a crucial area for future investigation.
- Why unresolved: The current study focuses on diagnosing the problem and identifying the causal mechanisms (residual stream and attention heads) rather than proposing a robust algorithmic solution to prevent the reversion to original knowledge.
- What evidence would resolve it: A novel editing algorithm or post-hoc intervention technique that significantly reduces the Original Match (OM) metric without degrading the Efficacy (Eff.) or Generalization (Gen.) of the edited knowledge.

### Open Question 2
- Question: How can evaluation methodologies be expanded to systematically assess superficial editing beyond the three specific attack contexts (Wiki, Rep, Que)?
- Basis in paper: [explicit] The authors acknowledge that their investigation is "limited to examining superficial editing within three specific attack contexts" and suggest that "developing more comprehensive and systematic evaluation methodologies remains an important direction."
- Why unresolved: The current work relies on a constrained set of attack prefixes (Wikipedia summaries, entity repetition, questions), which may not capture the full range of prompts that could induce the reversion to original knowledge.
- What evidence would resolve it: A broader evaluation framework testing a wider taxonomy of adversarial prompts or the establishment of a benchmark that automatically generates attack probes to test the robustness of edits.

### Open Question 3
- Question: Are the specific "prominent attention heads" identified in the analysis structurally consistent across different model architectures or editing operations?
- Basis in paper: [inferred] The paper identifies specific heads (e.g., L23H27 in LLaMA3) that contribute to superficial editing, but the analysis relies on post-hoc identification for specific model-method pairs.
- Why unresolved: It remains unclear whether these heads are universal loci for factual storage across all Transformer models or if their importance is contingent on the specific pre-training or editing algorithm used.
- What evidence would resolve it: A cross-architectural study mapping the "Latent Original Probability of Heads" (LOPH) to determine if the same structural layers/heads consistently encode knowledge regardless of the model variant or editing method.

## Limitations

- Attack probe construction relies on external APIs (Wikipedia, Qwen2.5-32B-Instruct) without specified hyperparameters, and the Rep(o) attack prefix repetition count m is unspecified
- The prominent head threshold τ=0.1 and top-p% singular vector ablation (p=5,10) are chosen without sensitivity analysis, and the causal relationship is demonstrated but not exhaustively validated
- While the analysis framework applies to superficial unlearning, the specific mechanisms (residual stream inhibition, RRS behavior, singular vector encoding) may not generalize across all model architectures or knowledge editing scenarios
- Corpus evidence for these specific mechanisms is limited, and alternative explanations (distributed representations across multiple heads) cannot be fully ruled out

## Confidence

- **High**: The existence of superficial editing phenomenon and basic identification of residual stream and attention module involvement. The conventional metrics (OM, OP) clearly demonstrate that edited models revert under attack probes.
- **Medium**: The specific causal mechanisms (RRS behavior in attention modules, singular vector encoding of original knowledge). The intervention and ablation experiments provide strong evidence, but alternative explanations cannot be fully ruled out.
- **Low**: The generality of the mechanism across different model architectures and editing algorithms. The paper focuses primarily on LLaMA3-8B-Instruct and specific editing methods without systematic exploration of architecture-specific behaviors.

## Next Checks

1. **Reproduce the inhibition score patterns**: Implement layer-wise residual stream interventions at the last subject position and verify that IS values for corrupted runs exceed clean runs in earlier layers (5-15) across all three model architectures.

2. **Validate causal singular vectors**: For the top 3 prominent attention heads identified via LOPH, perform SVD decomposition and ablate the top 5% singular vectors. Confirm that OAP decreases by 8-13 points while NAP increases, and that DSR for original answer (50-75%) vastly exceeds new answer (0-9%).

3. **Test mechanism robustness**: Apply the same analysis framework to superficial unlearning scenarios using different model architectures (e.g., LLaMA3-70B, Qwen2.5-32B) and verify whether the same patterns of residual stream inhibition and RRS behavior in later attention modules are observed.