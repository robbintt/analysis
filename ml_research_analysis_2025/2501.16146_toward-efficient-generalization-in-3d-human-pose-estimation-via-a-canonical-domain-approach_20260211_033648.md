---
ver: rpa2
title: Toward Efficient Generalization in 3D Human Pose Estimation via a Canonical
  Domain Approach
arxiv_id: '2501.16146'
source_url: https://arxiv.org/abs/2501.16146
tags:
- pose
- domain
- canonical
- human
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of domain gaps between source
  and target datasets in 3D human pose estimation, which causes performance degradation
  when models trained on one dataset are applied to another. The authors propose a
  novel canonical domain approach that transforms both source and target domains into
  a unified canonical domain, eliminating the need for fine-tuning on the target domain.
---

# Toward Efficient Generalization in 3D Human Pose Estimation via a Canonical Domain Approach

## Quick Facts
- arXiv ID: 2501.16146
- Source URL: https://arxiv.org/abs/2501.16146
- Reference count: 40
- Primary result: Novel canonical domain approach eliminates target-domain fine-tuning for 3D pose estimation generalization

## Executive Summary
This paper addresses the critical problem of domain gaps in 3D human pose estimation, where models trained on one dataset perform poorly on another due to differences in pose distributions and camera parameters. The authors propose a novel canonical domain approach that transforms both source and target domains into a unified canonical space through a 3D pose canonicalization process. This approach eliminates the need for target-domain fine-tuning while achieving state-of-the-art cross-dataset generalization performance.

The core innovation lies in rotating 3D poses to center them on the camera's principal axis, ensuring 2D-3D pose consistency and simplifying the learning problem. During inference, target domain 2D poses are canonicalized using known camera parameters, allowing the trained network to be directly applied without additional adaptation. Extensive experiments demonstrate significant improvements across multiple lifting networks and datasets, achieving competitive results with domain adaptation methods while requiring no fine-tuning.

## Method Summary
The proposed method transforms both source and target domains into a unified canonical domain through a canonicalization process that rotates 3D poses to center them on the camera's principal axis. The canonicalization is achieved by computing a rotation matrix Rcanon using Rodrigues' rotation formula, which aligns the pelvis vector with the principal axis. During training, source 3D poses are canonicalized, projected to canonical 2D poses centered at the image center, and used to train lifting networks. At inference, target domain 2D poses are canonicalized using the transformation T2Dcanon = Ktarget × Rcanon × K⁻¹target, allowing the trained network to be directly applied. The method works with any standard lifting architecture and requires known camera intrinsic parameters for the target domain.

## Key Results
- Achieved MPJPE of 59.5mm on 3DHP test set with DSTformer, outperforming conventional training (64.87mm)
- Demonstrated significant cross-dataset generalization improvements across multiple lifting networks (DSTformer, VideoPose, SimpleBaseline, SemGCN, ST-GCN)
- Competitive performance with state-of-the-art domain adaptation methods while requiring no additional fine-tuning
- Validated effectiveness on three major datasets: Human3.6M, Fit3D, and MPI-INF-3DHP

## Why This Works (Mechanism)

### Mechanism 1: Canonical Domain Unification
Mapping both source and target domains into a unified canonical domain reduces the domain gap without requiring target-domain fine-tuning. The canonicalization process rotates 3D poses around an axis defined by the cross product of the principal axis and pelvis vector, aligning the pelvis to the camera's principal axis using Rodrigues' rotation formula. This constrains the 2D pose distribution while preserving root-relative 3D pose variation. The core assumption is that camera intrinsic parameters for the target domain are known and accessible at inference time.

### Mechanism 2: 2D-3D Pose Consistency Enforcement
Conventional 2D-3D mapping introduces an offset term that causes many-to-one mapping complexity; canonical mapping eliminates this by centering poses on the principal axis. Perspective projection introduces relative rotation for off-axis poses, creating residual terms [fx/Z·X, fy/Z·Y] between centered and off-center projections. Conventional mapping requires learning AX+B→X; canonical mapping simplifies to AX→X by zeroing the offset. The core assumption is that the primary source of 2D-3D inconsistency is positional offset from the principal axis, not other factors like depth ambiguity.

### Mechanism 3: Test-Time 2D-Only Canonicalization
Target domain 2D poses can be canonicalized without ground truth 3D poses by leveraging perspective projection properties. Under perspective projection, each joint has unique homogeneous coordinates [X/Z, Y/Z, 1], rotated 3D joints and rotated normalized 2D joints share the same homogeneous coordinates, and the pelvis vector direction is invariant under projection. This enables computing Rcanon from normalized 2D pelvis [px, py, 1] alone. The core assumption is that target intrinsic parameters are available, following the mild assumption as in prior domain adaptation work.

## Foundational Learning

- **Concept: Perspective Projection and Homogeneous Coordinates**
  - Why needed here: Understanding how 3D points project to 2D and why off-axis positions cause relative rotation is essential for grasping the canonicalization logic.
  - Quick check question: Given a 3D point [X, Y, Z] and camera intrinsics K, can you derive its 2D projection and explain why points off the principal axis appear rotated?

- **Concept: 2D-to-3D Lifting Problem**
  - Why needed here: The paper addresses the specific task of estimating 3D poses from 2D skeleton inputs, which inherently involves depth ambiguity and domain shift.
  - Quick check question: What makes the 2D-to-3D lifting problem a domain shift problem rather than a fixed function approximation?

- **Concept: Domain Generalization vs Domain Adaptation**
  - Why needed here: The paper positions itself against DG (data augmentation) and DA (test-time fine-tuning) approaches, offering a third path through canonicalization.
  - Quick check question: Why does DG require "endlessly expanding the source domain distribution" while canonicalization constrains both domains?

## Architecture Onboarding

- **Component map:** 3D Pose → Rcanon (Rodrigues' rotation) → Canonical 3D → 2D Projection → Screen Normalization → Lifting Network → Canonical 3D → Back-Transformation → Final 3D Pose

- **Critical path:**
  1. Verify camera intrinsics availability for target domain
  2. Implement Rodrigues' rotation for Rcanon computation
  3. Ensure screen normalization applied consistently (centers root at (0,0) after normalization)
  4. Test back-transformation correctness by round-tripping known poses

- **Design tradeoffs:**
  - Depth (Z) is NOT canonicalized—scale variation preserved in 2D to avoid complexity from focal length, depth, and body size interactions
  - Root-relative 3D poses used (not absolute), matching MPJPE evaluation protocol
  - Requires known intrinsics; cannot handle fully uncalibrated scenarios

- **Failure signatures:**
  - High cross-dataset error on 3DHP but not Fit3D: Check if 2D-3D consistency is the issue (3DHP poses more scattered from principal axis)
  - Good P-MPJPE but poor MPJPE with simple centering: Indicates 2D canonicalization helps but consistency matters for absolute position
  - Performance degrades with certain camera setups: Verify principal point (cx, cy) vs image center offset handling

- **First 3 experiments:**
  1. **Sanity check—Cross-scenario (H36M train → H36M test):** Should show minimal difference from baseline; validates canonicalization doesn't break same-domain performance.
  2. **Cross-dataset generalization (H36M → 3DHP):** Primary evaluation; expect ~8-20% MPJPE reduction per Table 3; confirms domain gap reduction.
  3. **Ablation—Conventional + Input Centering vs Canonical:** Tests whether 2D canonicalization alone (without consistency) helps; expect Conventional+IC to fail on 3DHP but succeed on Fit3D.

## Open Questions the Paper Calls Out

### Open Question 1
Can combining the canonicalization approach with advanced data augmentation techniques yield synergistic improvements in generalization performance? The paper explicitly states this as future work, noting that synergies between canonicalization and data augmentation could lead to more data-efficient augmentation strategies. This remains unresolved as the current method is evaluated standalone without augmentation combination experiments.

### Open Question 2
How can the canonicalization framework be extended to incorporate scale and camera parameters to address depth-related domain gaps? The paper plans to extend the framework by incorporating factors such as scale and camera parameters to broaden its effectiveness. This is unresolved because the current method deliberately does not canonicalize depth, allowing it to be learned from data, due to the complexity of scale gaps involving multiple interacting factors.

### Open Question 3
How robust is the canonicalization approach when camera intrinsic parameters are unknown or estimated with error? The paper follows the mild assumption that target intrinsic parameters are accessible, but does not evaluate performance degradation when intrinsics are unavailable or incorrectly estimated, which is common in real-world uncalibrated camera scenarios.

### Open Question 4
What characteristics define an optimal source dataset for constructing a generalizable canonical domain, and can overfitting be mitigated through data redistribution rather than volume increase? The paper notes that Fit3D showed higher absolute errors and overfitting due to limited camera-relative pose diversity compared to H36M and 3DHP, highlighting the importance of developing data-efficient augmentation techniques aimed at mitigating overfitting by redistributing the source dataset.

## Limitations

- The method requires known camera intrinsic parameters for the target domain, limiting applicability to uncalibrated scenarios
- Performance depends critically on the diversity of camera-relative poses in the source dataset, with limited diversity causing overfitting
- The universal skeleton joint mapping across datasets with different joint counts (32, 25, 17 joints) is not fully specified
- The approach does not address depth-related domain gaps, leaving scale variations to be learned from data

## Confidence

- **High**: The mathematical foundation of canonicalization via Rodrigues' rotation and its ability to reduce 2D-3D pose inconsistency
- **Medium**: The claim that canonicalization eliminates need for target-domain fine-tuning (requires camera parameters)
- **Medium**: The assertion that the method works with "any" lifting network architecture (tested only on specific networks)

## Next Checks

1. **Camera parameter sensitivity analysis**: Systematically evaluate performance degradation when camera intrinsics are perturbed by noise or estimated inaccurately

2. **Unsupervised camera parameter estimation**: Implement and validate the proposed alternative method for estimating Rcanon without known intrinsics on the Fit3D dataset

3. **Cross-domain ablation study**: Isolate the contribution of 2D-3D consistency enforcement vs. simple input centering by testing on datasets with varying degrees of pose scatter from the principal axis