---
ver: rpa2
title: 'Inaugural MOASEI Competition at AAMAS''2025: A Technical Report'
arxiv_id: '2507.05469'
source_url: https://arxiv.org/abs/2507.05469
tags:
- each
- competition
- agent
- environment
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The MOASEI Competition was designed to evaluate multi-agent AI
  systems under open-world conditions, where agents and tasks can appear, disappear,
  or change behavior over time. The competition used the free-range-zoo environment
  suite and included three tracks: Wildfire, Cybersecurity, and Rideshare.'
---

# Inaugural MOASEI Competition at AAMAS'2025: A Technical Report

## Quick Facts
- arXiv ID: 2507.05469
- Source URL: https://arxiv.org/abs/2507.05469
- Reference count: 3
- Primary result: MOASEI Competition evaluated multi-agent AI systems under open-world conditions using free-range-zoo environments; GNNs and CNNs achieved highest rewards in Wildfire, weighted scoring won Cybersecurity, Rideshare had no submissions.

## Executive Summary
The MOASEI Competition at AAMAS'2025 introduced a novel benchmark for multi-agent AI systems under open-world conditions, where agents and tasks can appear, disappear, or change behavior over time. Using the free-range-zoo environment suite across three tracks (Wildfire, Cybersecurity, Rideshare), eleven international teams participated with four submitting solutions. The competition revealed that diverse approaches—including graph neural networks, convolutional architectures, and large language model-augmented strategies—can effectively handle open environments, with GNNs and CNNs demonstrating particularly strong adaptability and robustness to agent dropout and dynamic tasks.

## Method Summary
The competition formalized multi-agent open-world problems as partially observable stochastic games and evaluated policies on held-out configurations with varying openness levels. Four winning approaches emerged: graph neural networks capturing relational structures between agents and tasks, convolutional neural networks leveraging spatial regularity, LLM-driven meta-optimization using self-prompting for policy refinement, and weighted scoring heuristics balancing information-gathering against direct action. All policies were trained using off-the-shelf RL libraries and evaluated across 256 independent runs with statistical significance testing via Wilcoxon-signed-rank tests.

## Key Results
- Graph neural networks and convolutional architectures achieved the highest cumulative rewards in the Wildfire track, demonstrating strong adaptability to agent dropout and dynamic tasks
- The winning Cybersecurity submission used weighted scoring to balance patching and monitoring actions, achieving 7.85% monitoring rate versus 2.82-2.96% for baselines while reducing wasted patch actions
- The LLM-based meta-optimization approach crashed under highest-openness conditions, highlighting brittleness to distributional shift
- Rideshare track saw no submissions, prompting planned revisions for future competitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph neural networks (GNNs) enable robust adaptation to open-world conditions by capturing relational structure between agents and tasks.
- Mechanism: GNNs construct context-aware embeddings from graph representations of the environment at each timestep, where nodes represent agents/tasks and edges encode spatial or functional relationships. This allows policies to generalize when entities appear/disappear because the learned relational patterns transfer to modified graph structures.
- Core assumption: The important regularities in open environments are relational (e.g., agent-task proximity, resource constraints) rather than purely spatial or temporal.
- Evidence anchors:
  - [abstract] "graph neural networks...showing strong adaptability and robustness"
  - [section 5.1] "Teams leveraging relational GNNs demonstrated high adaptability to agent dropout and dynamic tasks. These policies leverage context-aware embeddings included in the rich graph structures, which capture relational structures in each environmental step"
  - [corpus] Weak direct corpus support for GNN mechanisms specifically in open-agent systems; related competition reports focus on different architectures.
- Break condition: If agent/task relationships are essentially random or non-structural, GNN relational learning provides no advantage over simpler architectures.

### Mechanism 2
- Claim: Weighted scoring heuristics can outperform naive reactive policies under partial observability by explicitly balancing information-gathering against direct action.
- Mechanism: The winning Cybersecurity submission (Zana Cyber) used a weighted scoring function that assigned values to patch vs. monitor actions based on current belief states about network vulnerability. By monitoring more frequently (7.85% vs. 2.82-2.96% for baselines) while patching less (3.86% vs. 6.60% for naive patch baseline), the policy reduced wasted patch actions on already-secure nodes.
- Core assumption: Partial observability creates information asymmetries that can be exploited through strategic observation before committing resources.
- Evidence anchors:
  - [section 4.2] "Zana Cyber's approach in particular shows a much slower downward trend in rewards, indicating more effective resistance by the defender agents"
  - [section 4.2] "Zana Cyber's submitted policy is more efficient in its application of patch actions by spending more time gathering accurate observations of the environment"
  - [corpus] No corpus papers directly address weighted scoring in cybersecurity domains.
- Break condition: If observation costs exceed the value of information gained, or if the environment changes faster than observation-action cycles can adapt.

### Mechanism 3
- Claim: LLMs can serve as meta-optimizers for policy refinement in open environments, even when not suitable as direct action selectors.
- Mechanism: The BIT Student submission used an LLM in a self-prompting loop: the LLM analyzed episodic performance logs, proposed policy modifications, injected changes, and iterated. This treats the LLM as an outer-loop optimizer rather than an inner-loop decision-maker, leveraging its pattern-matching for strategy adaptation rather than real-time action selection.
- Core assumption: Verbalizable performance patterns can be mapped to meaningful policy modifications through natural language reasoning.
- Evidence anchors:
  - [section 5.1] "this team adopts a meta-optimization loop in which an LLM (e.g., GPT-like architecture) generated policy modifications by analyzing past performance"
  - [section 5.1] "this approach is especially relevant in open environments, where the ability to generalize from sparse or shifting conditions is often more important than maximizing short-term reward"
  - [corpus] The Alibaba competition report mentions LLMs for relevance matching but not meta-optimization; weak corpus support.
- Break condition: If policy modifications require precise numerical parameter tuning beyond LLM capabilities, or if feedback loops are too slow for the timescale of environmental change.

## Foundational Learning

- **Partially Observable Stochastic Games (POSGs)**
  - Why needed here: All three competition domains are formalized as POSGs {S, A, T, R, Z, O}. Understanding how partial observability (Z, O) interacts with multi-agent dynamics is essential for interpreting why monitoring/information-gathering strategies matter.
  - Quick check question: Given a POSG formulation, can you identify which components capture "openness" (agents/tasks appearing/disappearing)?

- **Agent vs. Task Openness**
  - Why needed here: The competition distinguishes these—Wildfire has both, Cybersecurity emphasizes agent openness (defenders appearing/disappearing), Rideshare emphasizes task openness (passengers appearing). Architecture choices depend on which type dominates.
  - Quick check question: If you're designing for task openness only, do you need policies that handle variable agent counts?

- **Expected Utility Evaluation Under Distribution Shift**
  - Why needed here: Policies were evaluated on held-out configurations (WS1-3, CS1-3) with varying openness levels. Understanding that training distribution ≠ evaluation distribution is critical for interpreting leaderboard results.
  - Quick check question: Why might a policy with lower training reward outperform a higher-reward policy on held-out configurations?

## Architecture Onboarding

- **Component map:**
  free-range-zoo/ -> env/ (environment definitions) -> spaces/ (action/observation specs) -> structures/ (config and state containers) -> transitions/ (T functions) -> configs/ (benchmark configs) -> baselines/ (reference policies)

- **Critical path:**
  1. Start with configs/ to understand evaluation scenarios—openness levels, agent counts, episode lengths
  2. Trace observation construction in spaces/ to understand partial observability constraints
  3. Study transitions/ to identify where openness manifests (agent dropout, task appearance)
  4. Review baseline policies as sanity checks before implementing custom approaches

- **Design tradeoffs:**
  - **GNN vs. CNN:** GNNs better capture relational structure but require graph construction overhead; CNNs are lighter but assume spatial regularity. The competition shows comparable peak performance—choose based on domain structure.
  - **Action-heavy vs. observation-heavy:** In Cybersecurity, the winner monitored 2.8× more than baselines while patching less. In Wildfire, higher fight-action ratios correlated with higher rewards. Domain-specific tradeoff.
  - **End-to-end RL vs. modular components:** LLM meta-optimization suggests decomposition (policy + optimizer) may outperform monolithic approaches when environmental shifts are hard to anticipate.

- **Failure signatures:**
  - **Crash on high-openness configurations:** BIT Student's LLM approach crashed on WS3 (highest openness). Suggests brittleness to out-of-distribution entity counts.
  - **High action frequency, low reward:** The "largest" baseline in Wildfire fought more than winners but earned ~56% less reward. Indicates inefficient resource allocation.
  - **Degradation curves:** All Cybersecurity policies show declining rewards over time; the rate of decline differentiates approaches.

- **First 3 experiments:**
  1. **Baseline replication:** Run all baseline policies (noop, random, smallest, largest, patched, exploited) across all configurations. Verify your pipeline produces numbers within error bars of Table 1 and Table 4.
  2. **Observation ablation:** Implement a simple policy that alternates between monitor and patch actions at different ratios. Test whether the 7.85% monitor rate that worked for Zana Cyber transfers to other seeds/configurations.
  3. **Openness sensitivity test:** Train a policy on WS1 (low openness) and evaluate on WS3 (high openness). Quantify the generalization gap. Then reverse—train on WS3, evaluate on WS1. This establishes a baseline for robustness requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid architectures combining LLM-guided meta-adaptation with graph-based policy architectures outperform pure GNN or CNN approaches in open environments?
- Basis in paper: [explicit] The authors state: "combining LLM-guided meta-adaptation with graph-based policy architectures may yield systems that both generalize across structural openness and adapt rapidly to unanticipated shifts."
- Why unresolved: The LLM-based submission (BIT Student) crashed under the highest-openness configuration (WS3) and underperformed relative to GNN/CNN methods, but was used only for meta-optimization rather than as a hybrid architecture.
- What evidence would resolve it: A controlled experiment comparing pure GNN, pure CNN, pure LLM-meta-optimized, and combined LLM+GNN architectures across all openness levels, measuring both cumulative reward and robustness (no crashes).

### Open Question 2
- Question: What evaluation metrics best capture agent performance in high-openness settings where traditional reward correlations break down?
- Basis in paper: [explicit] The authors observe: "the number of fires put out is initially highly correlated with the cumulative episodic rewards in WS1, where open conditions less prevalent, however the correlation is no longer found in WS3 where there is significantly more openness in the environment."
- Why unresolved: The paper does not propose alternative metrics or explain why this correlation degradation occurs, leaving uncertainty about what behaviors should be optimized under extreme openness.
- What evidence would resolve it: Analysis across multiple domains identifying metrics that maintain predictive validity for long-term success under varying openness levels, potentially including adaptation speed, recovery from perturbations, or resource efficiency.

### Open Question 3
- Question: How can agent policies effectively adapt to "frame openness"—dynamic changes to internal agent capabilities over time?
- Basis in paper: [explicit] The authors plan to introduce frame openness in future competitions: "agents may experience equipment degradation or return from downtime with altered skill sets or equipment. This internal variability will require policies to adapt not only to external changes... but also to evolving agent abilities."
- Why unresolved: Current competition tracks only feature agent openness (presence/absence) and task openness; no existing submissions have been tested on capability drift or skill degradation.
- What evidence would resolve it: Implementation of frame openness in the free-range-zoo environments, followed by competition results showing which architectural approaches (GNN, CNN, LLM, or hybrid) maintain performance when agent capabilities change mid-episode.

### Open Question 4
- Question: What barriers prevented participation in the Rideshare track, and how can open-system benchmarks be designed to ensure tractable entry points?
- Basis in paper: [explicit] The authors note: "the rideshare track saw no participation in this cycle" and state they "plan to revise this track to make it more accessible."
- Why unresolved: The paper does not analyze why 11 registered teams produced only 4 submissions, with zero for rideshare—whether due to environment complexity, documentation gaps, domain familiarity, or reward structure issues.
- What evidence would resolve it: Post-competition surveys of non-participating teams, A/B testing of simplified vs. complex environment configurations, and tracking participation rates after proposed revisions are implemented.

## Limitations
- LLM-based meta-optimization showed significant brittleness, crashing under highest-openness conditions and highlighting sensitivity to distributional shift
- Specific architectural details (GNN layers, CNN kernels, weighted scoring weights) remain underspecified, limiting direct replication
- Evaluation focused on cumulative reward without capturing adaptation speed or recovery from perturbations as explicit metrics

## Confidence
- **High confidence**: GNN and CNN architectures demonstrate superior adaptability and robustness in open environments; multi-agent evaluation under open-world conditions is feasible and informative
- **Medium confidence**: LLM-driven meta-optimization can improve policy adaptation, but requires careful handling of distributional shift; weighted scoring heuristics balance exploration and exploitation effectively in partially observable domains
- **Low confidence**: Specific architectural choices (GNN layers, CNN kernels, scoring weights) are optimal; current evaluation captures all relevant dimensions of "openness"

## Next Checks
1. **Architecture sensitivity analysis**: Systematically vary GNN layer counts, CNN kernel sizes, and weighted scoring parameters to identify performance plateaus and failure thresholds
2. **Distributional robustness test**: Train policies on configurations with specific openness levels and evaluate on held-out configurations with systematically varied agent/task appearance rates to quantify generalization boundaries
3. **Observation-utility correlation**: Measure how monitoring action frequency correlates with reward stability across seeds and configurations to validate the information-gathering hypothesis for partially observable domains