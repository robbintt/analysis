---
ver: rpa2
title: Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise
  Tables
arxiv_id: '2508.07186'
source_url: https://arxiv.org/abs/2508.07186
tags:
- data
- summarization
- structured
- prompt
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-agent framework for generating enterprise
  data summaries that combine structured reasoning with LLM-based narrative generation.
  It addresses the challenge of translating multi-dimensional, hierarchical business
  data into accurate, context-aware reports.
---

# Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables

## Quick Facts
- **arXiv ID:** 2508.07186
- **Source URL:** https://arxiv.org/abs/2508.07186
- **Reference count:** 13
- **One-line primary result:** A modular agent pipeline achieves 83% faithfulness and 4.4/5 relevance for enterprise table summarization, outperforming flat LLM prompting (43% faithfulness, 3.1 relevance) and template NLG (69% faithfulness, 3.7 relevance).

## Executive Summary
This paper introduces a multi-agent framework for generating enterprise data summaries that combine structured reasoning with LLM-based narrative generation. It addresses the challenge of translating multi-dimensional, hierarchical business data into accurate, context-aware reports. The approach uses a modular pipeline with dedicated agents for slicing data, computing variances, enriching context, and generating summaries via Amazon Nova Micro. Evaluation on a Kaggle retail dataset shows the framework achieves 83% faithfulness to underlying data, covers 60% of significant changes, and receives a 4.4/5 relevance score for decision-critical insights. These results outperform both flat LLM prompting (43% faithfulness, 3.1 relevance) and template-based NLG (69% faithfulness, 3.7 relevance). The modular design improves interpretability and enables better handling of subtle trade-offs, such as increased revenue amid declining unit volumes.

## Method Summary
The framework uses a four-agent LangGraph pipeline: SliceAgent filters the dataset by dimension vectors and time periods, returning two DataFrames; VarianceAgent computes percent change (δj = (Mj^t2 - Mj^t1) / (Mj^t1 + ε)) for all metrics; ContextAgent enriches the state with external metadata (seasonality, promotions, anomalies); SummaryAgent formats a structured JSON prompt and calls Amazon Nova Micro to generate the final narrative. The modular design isolates deterministic computation from language generation, reducing hallucination and focusing LLM attention on significant changes. Evaluation on the Kaggle Retail Sales dataset validates faithfulness, coverage, and relevance metrics against baselines.

## Key Results
- **Faithfulness:** 83% (vs. 43% for flat LLM, 69% for template NLG)
- **Relevance:** 4.4/5 (vs. 3.1 for flat LLM, 3.7 for template NLG)
- **Coverage:** 60% of significant deltas mentioned in summaries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modular agent decomposition improves faithfulness by separating symbolic computation from language generation.
- **Mechanism:** The pipeline isolates variance calculation (deterministic) from narrative synthesis (probabilistic). By pre-computing deltas via VarianceAgent before the LLM sees any data, the framework reduces opportunities for numerical hallucination and ensures the generator works from grounded inputs.
- **Core assumption:** LLMs are more reliable at narrating pre-computed facts than deriving them from raw tables.
- **Evidence anchors:**
  - [abstract] "Our method introduces a multi-agent pipeline that extracts, analyzes, and summarizes multi-dimensional data using agents for slicing, variance detection, context construction, and LLM-based generation."
  - [section 6] "Removing the VarianceAgent led to inaccurate interpretations as changes were not explicitly quantified."
  - [corpus] DETQUS paper confirms decomposition helps query-focused summarization over large tables, supporting modular reasoning benefits.
- **Break condition:** If upstream agents produce incorrect slices or variance calculations, errors propagate regardless of LLM quality.

### Mechanism 2
- **Claim:** Explicit variance computation focuses LLM attention on significant changes, improving coverage of decision-critical deltas.
- **Mechanism:** The VarianceAgent computes percent change (δj = Mj^t2 - Mj^t1 / Mj^t1 + ε) and surfaces these as structured prompt fields. This acts as an attention mechanism, biasing generation toward what changed rather than static values.
- **Core assumption:** Business stakeholders care more about change magnitude than absolute values.
- **Evidence anchors:**
  - [section 3.1] Defines the delta computation formula explicitly.
  - [section 5] "Coverage (C) shows that our model includes a greater share of significant changes—an essential property for completeness in enterprise reporting."
  - [corpus] DAComp benchmark emphasizes data analysis tasks that convert tables into decision-oriented insights, aligning with delta-focused reasoning.
- **Break condition:** If variance thresholds are not tuned to domain scale, trivial fluctuations may dominate summaries.

### Mechanism 3
- **Claim:** Context enrichment improves relevance by anchoring narrative to domain-specific signals that explain observed deltas.
- **Mechanism:** ContextAgent injects metadata (seasonality, promotions, anomalies) into prompts, enabling the LLM to attribute causes rather than merely describe effects. This shifts output from descriptive to diagnostic.
- **Core assumption:** Explanatory narratives are more actionable than purely descriptive ones.
- **Evidence anchors:**
  - [abstract] "improvements are especially pronounced in categories involving subtle trade-offs, such as increased revenue due to price changes amid declining unit volumes."
  - [section 6] "Removing the ContextAgent resulted in less relevant narratives due to missing external signals."
  - [corpus] EICopilot demonstrates LLM-driven agents over enterprise knowledge graphs benefit from semantic context injection.
- **Break condition:** If context metadata is stale, incomplete, or misaligned with the data slice, explanations may be misleading.

## Foundational Learning

- **Concept:** Directed Acyclic Graph (DAG) orchestration for agent workflows
  - **Why needed here:** The LangGraph execution model requires understanding node-based control flow, state passing between agents, and conditional transitions.
  - **Quick check question:** Can you trace how state flows from SliceAgent output to VarianceAgent input?

- **Concept:** Percent change and variance metrics for time-series comparison
  - **Why needed here:** VarianceAgent relies on computing relative deltas (δj) between time periods; misunderstanding this formula will break downstream summaries.
  - **Quick check question:** Given revenue of 7999.9 (t1) and 2899.9 (t2), what is the percent change?

- **Concept:** Structured prompting with JSON-like context injection
  - **Why needed here:** SummaryAgent constructs prompts from metric dictionaries; poor prompt structure degrades faithfulness.
  - **Quick check question:** What happens if "change_percent" is omitted from the metrics field in the prompt?

## Architecture Onboarding

- **Component map:** SliceAgent → VarianceAgent → ContextAgent → SummaryAgent (sequential; no parallel execution in current design per Algorithm 1)

- **Critical path:** SliceAgent → VarianceAgent → ContextAgent → SummaryAgent (sequential; no parallel execution in current design per Algorithm 1)

- **Design tradeoffs:**
  - Modularity vs. latency: Each agent adds inference overhead but enables debugging at stage boundaries
  - Pre-computed variance vs. LLM-derived: Higher faithfulness but requires domain knowledge to set thresholds
  - Static context vs. dynamic retrieval: Current implementation uses static metadata; future work suggests real-time extension

- **Failure signatures:**
  - Empty slice: SliceAgent returns zero rows → SummaryAgent should skip (runtime guard mentioned in section 3.2)
  - Missing context: ContextAgent finds no matching events → relevance drops (ablation shows 4.4→4.1)
  - Variance omission: Coverage collapses from 60% to 25% per ablation study

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Run Flat LLM Prompt, Template NLG, and full agent pipeline on the Kaggle Retail Sales dataset; verify faithfulness (43%, 69%, 83%) and relevance (3.1, 3.7, 4.4) metrics.
  2. **Ablation validation:** Disable ContextAgent and VarianceAgent separately; confirm faithfulness and coverage degradation matches Table 2.
  3. **Dimension stress test:** Slice along uncommon dimension combinations (e.g., Region × Category pairs with sparse data); observe whether empty-slice guards trigger and whether summaries remain coherent for low-volume slices.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can predictive modeling agents be integrated into the summarization pipeline to simulate forward-looking scenarios without compromising the framework's high faithfulness scores?
  - **Basis in paper:** [explicit] The Discussion section explicitly lists "integrating predictive modeling agents that simulate forward-looking scenarios" as a primary direction for future work.
  - **Why unresolved:** The current framework relies on historical data deltas ($\Delta$) to ensure 83% faithfulness; predictive outputs are probabilistic and lack ground truth for strict factual alignment.
  - **What evidence would resolve it:** A modified evaluation framework that balances "forecast accuracy" against "hallucination rates" in generated narratives.

- **Open Question 2:** Does the inclusion of Text-to-SQL agents as an intermediate reasoning layer improve query interpretation accuracy compared to the current direct slicing method?
  - **Basis in paper:** [explicit] Section 8 proposes expanding comparisons to include Text2SQL agents to benchmark structured prompt-guided generation against direct SQL query interpretation.
  - **Why unresolved:** The current study only benchmarks against flat prompting and template NLG, leaving the utility of SQL-intermediate representations untested.
  - **What evidence would resolve it:** Comparative results showing that SQL agents reduce error rates in complex querying tasks versus the current SliceAgent implementation.

- **Open Question 3:** Can the proposed architecture be extended to support real-time streaming updates and continuous summarization over moving time windows?
  - **Basis in paper:** [explicit] Section 7 states the implementation is "limited to static, batch-mode data" and "does not yet support real-time streaming updates."
  - **Why unresolved:** The current agent logic assumes static snapshots ($T_{t1}, T_{t2}$) and lacks the temporal state tracking required for incremental updates.
  - **What evidence would resolve it:** A system demonstration where agents successfully process continuous data feeds while maintaining performance metrics.

- **Open Question 4:** How does the system perform when generalizing to unstructured enterprise logs (e.g., operational KPIs) versus the structured retail data tested?
  - **Basis in paper:** [inferred] While Section 5 validates the method on Kaggle retail data, Section 7 notes that generalizing to "unstructured enterprise logs" is a current limitation requiring more robust parsing.
  - **Why unresolved:** The efficacy of the SliceAgent and VarianceAgent depends on structured tabular inputs, and it is unclear if they can handle the noise of operational logs without significant architecture changes.
  - **What evidence would resolve it:** Evaluation results showing similar faithfulness and coverage scores when the input data is shifted from retail tables to operational logs.

## Limitations
- ContextAgent's external metadata sources are underspecified; enrichment relies on unstated domain knowledge, making replication difficult without assumptions.
- Evaluation protocols for faithfulness and coverage lack detail—exact delta thresholds, fact extraction methods, and LLM-as-judge prompts are not provided.
- The modular pipeline assumes high-quality inputs from upstream agents; error propagation is not modeled, and failure modes beyond empty slices are not discussed.
- Results are based on a single Kaggle dataset, limiting generalizability to other enterprise domains or table schemas.

## Confidence
- **High confidence** in the modular agent decomposition mechanism and its role in improving faithfulness (supported by ablation results and multiple evidence anchors).
- **Medium confidence** in the claim that context enrichment improves relevance, as the evaluation lacks detail and relies on LLM-as-judge without specifying prompt quality.
- **Low confidence** in the scalability and robustness of the framework to noisy, sparse, or schema-varied enterprise data, given the absence of stress tests beyond dimension slicing.

## Next Checks
1. **Replicate the ablation study:** Disable VarianceAgent and ContextAgent separately on the Kaggle dataset; verify faithfulness drops to 71% and 84%, and coverage falls to 25% and 60%, respectively.
2. **Stress test dimension handling:** Generate summaries for rare dimension combinations (e.g., low-volume region-category pairs) and measure faithfulness and relevance; check if empty-slice guards trigger and summaries remain coherent.
3. **Cross-dataset generalization:** Apply the pipeline to a different tabular dataset (e.g., financial transactions or operational logs) and evaluate faithfulness, coverage, and relevance; assess if modular decomposition still outperforms flat prompting.