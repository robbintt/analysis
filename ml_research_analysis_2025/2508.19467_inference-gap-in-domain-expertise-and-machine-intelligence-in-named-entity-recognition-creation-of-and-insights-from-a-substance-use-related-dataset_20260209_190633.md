---
ver: rpa2
title: 'Inference Gap in Domain Expertise and Machine Intelligence in Named Entity
  Recognition: Creation of and Insights from a Substance Use-related Dataset'
arxiv_id: '2508.19467'
source_url: https://arxiv.org/abs/2508.19467
tags:
- social
- entity
- clinical
- opioid
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of extracting clinical and social
  impacts of opioid use from unstructured social media narratives, a critical task
  for public health surveillance. The authors present RedditImpacts 2.0, a refined
  named entity recognition dataset with detailed annotation guidelines and a focus
  on first-person disclosures, to support this task.
---

# Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset

## Quick Facts
- arXiv ID: 2508.19467
- Source URL: https://arxiv.org/abs/2508.19467
- Reference count: 40
- Primary result: Fine-tuned DeBERTa-large achieved relaxed token-level F1=0.61, outperforming LLMs on biomedical NER task requiring domain expertise

## Executive Summary
This study addresses the challenge of extracting clinical and social impacts of opioid use from unstructured social media narratives. The authors present RedditImpacts 2.0, a refined named entity recognition dataset with detailed annotation guidelines focused on first-person disclosures. They evaluate fine-tuned encoder-based models and large language models (LLMs) under zero- and few-shot in-context learning settings. The best-performing fine-tuned DeBERTa-large model achieves a relaxed token-level F1 score of 0.61, outperforming LLMs in precision, span accuracy, and guideline adherence. The study demonstrates that strong NER performance can be achieved with substantially less labeled data, emphasizing the feasibility of deploying robust models in resource-limited settings. However, the best model still underperforms compared to expert inter-annotator agreement (Cohen's kappa: 0.81), highlighting the persistent gap between expert intelligence and current NER/AI capabilities for tasks requiring deep domain knowledge.

## Method Summary
The study created RedditImpacts 2.0 dataset from Reddit posts, refined annotation guidelines to focus on first-person disclosures and entity boundaries. They evaluated fine-tuned encoder models (DeBERTa-large, BERT, RoBERTa, BioBERT) with and without CRF layers against LLMs (GPT-4o, Claude-3-Haiku, Llama-3.1-8B-Instruct) using zero- and few-shot in-context learning. Models were trained on 90% of merged train+dev data with 10% validation split, using AdamW optimizer with linear warmup and early stopping. Evaluation used relaxed token-level F1 score with 95% confidence intervals, precision, recall, and strict span matching metrics.

## Key Results
- Fine-tuned DeBERTa-large achieved F1=0.61 [95% CI: 0.43-0.62], outperforming LLMs (GPT-4o F1=0.44) in precision, span accuracy, and guideline adherence
- Both models struggled with negation handling and implicit entity detection, producing false positives on negated entities like "no criminal record"
- Performance plateaus at ~50% of training data, suggesting annotation quality matters more than volume for this specialized task
- Best model underperforms expert inter-annotator agreement (Cohen's kappa: 0.81), highlighting the inference gap between expert and AI capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned encoder models substantially outperform few-shot LLMs for token-level biomedical NER tasks requiring domain expertise.
- Mechanism: Parameter updates during fine-tuning enable models to learn task-specific BIO tagging schemes and domain representations that in-context learning cannot match through prompt engineering alone. Fine-tuning modifies internal weights to capture specialized terminology and labeling conventions.
- Core assumption: The performance gap stems from architectural differences in how knowledge is encoded (learned weights vs. frozen parameters + context), not solely from model scale or training data size.
- Evidence anchors: DeBERTa-large achieved F1=0.61, outperforming LLMs in precision, span accuracy, and guideline adherence; GPT-4o (3-shot) achieved F1=0.44 vs. DeBERTa-large F1=0.61—a 17% absolute gap; Related work (Lu et al., 2025) confirms "Large language models struggle in token-level clinical named entity recognition"

### Mechanism 2
- Claim: High-quality, refined annotation guidelines enable strong NER performance with substantially reduced labeled data.
- Mechanism: Consistent annotation boundaries and explicit inclusion/exclusion criteria reduce label noise, allowing models to learn meaningful patterns from fewer examples. The iterative annotation process (10% co-annotation → 95% agreement threshold → independent annotation) produces cleaner training signals.
- Core assumption: Annotation quality and consistency matter more than raw data volume for specialized NER tasks with clearly defined entity types.
- Evidence anchors: Strong NER performance can be achieved with substantially less labeled data; F1 score plateaus at ~50% of training data; additional data yields only minor improvements

### Mechanism 3
- Claim: A persistent gap exists between expert-level contextual reasoning and current model capabilities for tasks requiring deep domain knowledge.
- Mechanism: Experts bring lived experience and implicit knowledge to interpret first-person framing, negation, and implied impacts—capabilities models lack. Models struggle with: (1) distinguishing first-person from third-person reports, (2) recognizing negated entities, (3) identifying implicit consequences not explicitly stated.
- Core assumption: The expert-model gap reflects fundamental limitations in contextual and pragmatic understanding, not merely insufficient training data or model scale.
- Evidence anchors: Best model underperforms compared to inter-expert agreement (Cohen's kappa: 0.81); Both models produced false positives on negated entities ("no criminal record" incorrectly labeled); GPT-4o violated first-person-only guideline, labeling general statements as impacts

## Foundational Learning

- Concept: **BIO Tagging Scheme**
  - Why needed here: The paper uses BIO (Begin-Inside-Outside) for sequence labeling—understanding this is essential to interpret evaluation metrics and model outputs.
  - Quick check question: Given tokens ["I", "lost", "my", "job"], what BIO tags would indicate a single SocialImpacts entity?

- Concept: **Relaxed Token-Level F1**
  - Why needed here: The paper uses partial-match evaluation rather than strict span matching—critical for understanding reported performance numbers and why they're appropriate for informal text.
  - Quick check question: If a model predicts "lost my job today" (4 tokens) and ground truth is "lost my job" (3 tokens), would relaxed F1 give partial credit? How would strict span F1 differ?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The paper evaluates LLMs under zero/few-shot ICL—understanding its limitations versus fine-tuning is central to the findings.
  - Quick check question: Why might increasing examples from 3-shot to 5-shot decrease performance (as observed in this paper)?

## Architecture Onboarding

- Component map:
  Input preprocessing -> DeBERTa-large encoder -> Optional CRF layer -> Linear classification head mapping token embeddings to BIO labels (O, B-ClinicalImpacts, I-ClinicalImpacts, B-SocialImpacts, I-SocialImpacts)

- Critical path:
  1. Data preparation: Apply refined annotation guidelines (first-person only, exclude pronouns/modifiers from spans)
  2. Model selection: Start with DeBERTa-large (proven best); CRF optional, mixed results
  3. Training: 90/10 train/validation split, AdamW optimizer, linear warmup, early stopping on validation F1
  4. Evaluation: Use relaxed token-level F1 (partial overlap credit), not strict span matching

- Design tradeoffs:
  - Fine-tuned encoder vs. LLM: Encoder gives +17% F1 but requires labeled data; LLM offers rapid deployment with lower performance
  - With vs. without CRF: CRF showed mixed results (helped RoBERTaNER, hurt DeBERTa)—not universally beneficial
  - Data volume vs. annotation quality: Paper suggests 50% data with high-quality annotations ≈ full dataset performance

- Failure signatures:
  - Negation errors: Both models labeled "no criminal record" as SocialImpact (false positive)
  - Guideline violations: GPT-4o labeled third-person/general statements, violating first-person-only constraint
  - Implicit entity misses: Both models failed to detect implied impacts ("seeking help" as SocialImpact)
  - Label confusion: GPT-4o confused Clinical vs. Social when terminology overlapped ("therapeutic community")
  - Overgeneralization: GPT-4o labeled emotionally charged but contextually neutral terms ("blindsided", "pressuring")

- First 3 experiments:
  1. Baseline replication: Fine-tune DeBERTa-large on RedditImpacts 2.0 training set; target relaxed F1 ≈0.61. If substantially lower, check annotation preprocessing and label distribution.
  2. Data ablation: Train on 25%, 50%, 75% of training data; verify performance plateau at ~50%. If performance continues increasing, the dataset may need more lexical diversity.
  3. Error pattern analysis: Run inference on test set; manually inspect false positives for negation handling and first-person detection. If >30% of errors are negation-related, consider adding a negation-aware preprocessing step or auxiliary classifier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can instruction-tuned LLMs or multi-agent architectures close the performance gap with fine-tuned encoders for this domain-specific NER task?
- Basis in paper: The authors state, "In future work, we aim to explore these approaches, along with multi-agent architectures, to better adapt LLMs for domain-specific entity recognition."
- Why unresolved: The current study only evaluated standard LLMs under zero- and few-shot in-context learning, where they significantly underperformed (F1 0.44) compared to the fine-tuned DeBERTa model (F1 0.61).
- What evidence would resolve it: Experiments benchmarking instruction-tuned or multi-agent LLM systems against the fine-tuned DeBERTa baseline on the RedditImpacts 2.0 dataset.

### Open Question 2
- Question: What specific strategies for incorporating domain knowledge are more effective than simply increasing the volume of annotated data?
- Basis in paper: In Section 5.4, the authors note that performance plateaus at 50% of the training data, suggesting that "better strategies for incorporating domain knowledge may be more effective in improving performance than annotating more data."
- Why unresolved: The study demonstrated data efficiency but did not test alternative methods for injecting the "deep contextual knowledge" required to bridge the inference gap beyond standard fine-tuning.
- What evidence would resolve it: A comparative study evaluating the performance impact of external knowledge graphs or expert-derived heuristic rules versus increasing dataset size.

### Open Question 3
- Question: Can ensembling approaches, such as adding a first-person report classifier prior to an LLM, reduce guideline violations and improve inference performance?
- Basis in paper: The authors suggest in Section 5.2 that "It may be possible to employ ensembling approaches, such as adding a first-person report classifier prior to processing by an LLM, rather than a single-module pipeline to improve inference performance."
- Why unresolved: LLMs currently struggle with the specific constraint of identifying only first-person experiences, often labeling third-party accounts incorrectly.
- What evidence would resolve it: Implementation and evaluation of a cascaded model architecture that filters input for first-person perspective before entity extraction.

## Limitations

- Performance comparison conflates model scale, training approach, and domain adaptation—doesn't isolate architectural effects from other variables
- Data efficiency finding based on single dataset with specific annotation protocol may not generalize to domains with different lexical complexity
- Inter-expert agreement (Cohen's kappa: 0.81) serves as upper bound but measurement methodology and evaluation criteria consistency not fully specified

## Confidence

**High confidence**: The observation that DeBERTa-large outperforms GPT-4o on this specific dataset under the reported experimental conditions. The evaluation methodology (relaxed token-level F1 with 95% CIs) is clearly specified and reproducible. The annotation refinement process and its impact on data quality are well-documented.

**Medium confidence**: The generalizability of the data efficiency finding to other biomedical NER tasks. While the plateau effect at 50% data is observed, the underlying mechanism (annotation quality vs. lexical coverage) remains uncertain without additional datasets. The LLM vs. fine-tuned encoder comparison is medium confidence because it doesn't isolate architectural effects from other confounds.

**Low confidence**: The exact contribution of CRF layers to performance (mixed results reported) and the interpretation that the expert-model gap reflects fundamental capability limitations rather than specific failure modes that could be addressed through improved prompting or multi-stage pipelines.

## Next Checks

1. **Controlled LLM comparison**: Evaluate instruction-tuned BioBERT or domain-specific LLMs (e.g., PubMedBERT-based models) using both fine-tuning and ICL to isolate whether the performance gap stems from architecture or domain adaptation.

2. **Annotation quality vs. data volume**: Create synthetic variations of the RedditImpacts 2.0 dataset with different annotation qualities (varying agreement thresholds) but constant data volume. Test whether the 50% plateau effect persists across annotation quality levels.

3. **Negation and first-person detection**: Implement a preprocessing pipeline that explicitly identifies and handles negation (using dependency parsing or cue detection) and first-person framing. Evaluate whether this pipeline improves F1 by ≥5% on the most problematic error categories identified in the error analysis.