---
ver: rpa2
title: 'FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for
  Hardware-Accelerated LLM Inference'
arxiv_id: '2504.14152'
source_url: https://arxiv.org/abs/2504.14152
tags:
- quantization
- precision
- blocks
- fgmp
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FGMP is a fine-grained mixed-precision quantization approach that
  maintains LLM accuracy while quantizing most weights and activations to low precision
  (NVFP4) with minimal high-precision (FP8) blocks. It uses Fisher-information-weighted
  sensitivity to identify and preserve critical blocks in higher precision, combined
  with sensitivity-weighted clipping for low-precision blocks.
---

# FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference

## Quick Facts
- arXiv ID: 2504.14152
- Source URL: https://arxiv.org/abs/2504.14152
- Authors: Coleman Hooper; Charbel Sakr; Ben Keller; Rangharajan Venkatesan; Kurt Keutzer; Sophia Shao; Brucek Khailany
- Reference count: 40
- Primary result: 14% energy savings and 30% memory reduction while maintaining <1% perplexity degradation on Llama-2-7B

## Executive Summary
FGMP introduces a fine-grained mixed-precision quantization method for LLM inference that combines sensitivity analysis with hardware-aware design. The approach identifies critical model blocks using Fisher-information-weighted sensitivity metrics, preserving them in higher precision while aggressively quantizing the majority to NVFP4. Co-designed with hardware augmentations including mixed-precision VMAC datapaths and online activation quantization units, FGMP achieves significant compression with minimal accuracy loss, demonstrating 14% energy savings and 30% memory reduction on Llama-2-7B while maintaining <1% perplexity degradation versus FP8 baseline.

## Method Summary
FGMP employs a two-stage sensitivity analysis to identify critical model blocks for mixed-precision quantization. First, block-wise weight sensitivity is computed using Fisher-information-weighted norms, identifying which transformer blocks are most sensitive to quantization error. Second, activation sensitivity is estimated through sensitivity-weighted clipping that adapts based on the quantization precision of corresponding weights. The method then assigns FP8 precision to the most sensitive blocks (typically 1.8% of total) while quantizing remaining weights and activations to NVFP4. Hardware augmentations include mixed-precision VMAC datapaths that can operate at different precisions simultaneously and an online activation quantization unit that adapts quantization parameters during inference. The approach is validated through hardware simulation showing significant energy and memory savings while maintaining perplexity within 1% of FP8 baseline.

## Key Results
- 14% energy reduction and 30% memory savings on Llama-2-7B versus FP8 baseline
- <1% perplexity degradation while 92.2% of weights and activations are quantized to NVFP4
- Superior perplexity-per-compression ratio compared to prior quantization methods across multiple model families

## Why This Works (Mechanism)
FGMP works by strategically preserving precision only where it matters most. The Fisher-information-weighted sensitivity analysis identifies transformer blocks where quantization errors would propagate most significantly through the network, ensuring these blocks maintain FP8 precision. Meanwhile, the majority of blocks can be aggressively quantized to NVFP4 without significant accuracy loss. The sensitivity-weighted clipping mechanism further optimizes low-precision blocks by adapting quantization ranges based on weight precision, preventing catastrophic precision loss. The hardware co-design with mixed-precision VMAC datapaths and online activation quantization enables these benefits to be realized in practice, with the system dynamically adapting quantization parameters during inference to maintain accuracy while maximizing compression.

## Foundational Learning
- **Fisher-information-weighted sensitivity**: Measures how quantization error in each transformer block affects overall model output; needed to identify which blocks require higher precision to maintain accuracy; quick check: verify sensitivity scores correlate with perplexity degradation when blocks are quantized
- **Mixed-precision quantization**: Uses different numerical formats (FP8 vs NVFP4) for different model components; needed to balance compression with accuracy; quick check: confirm that sensitivity-based precision assignment outperforms random or uniform assignment
- **Online activation quantization**: Dynamically adjusts activation quantization parameters during inference; needed to adapt to varying activation distributions across different inputs; quick check: measure activation quantization error variance across different input sequences
- **Mixed-precision VMAC datapath**: Hardware unit capable of performing multiply-accumulate operations at different precisions simultaneously; needed to support heterogeneous precision requirements without performance penalties; quick check: verify throughput matches theoretical predictions for mixed-precision workloads
- **Sensitivity-weighted clipping**: Adapts quantization range based on weight precision to prevent precision loss; needed to maintain accuracy in low-precision blocks; quick check: compare perplexity with and without sensitivity-weighted clipping
- **Transformer block sensitivity**: Different layers in transformer models have varying sensitivity to quantization; needed to inform precision allocation strategy; quick check: identify which specific layers (e.g., attention vs feed-forward) are most sensitive

## Architecture Onboarding
- **Component map**: Input sequences -> Embedding layer -> [Sensitive blocks (FP8) -> Non-sensitive blocks (NVFP4)] -> Output layer, with Online Activation Quantization Unit providing dynamic clipping parameters
- **Critical path**: Input embedding → Mixed-precision transformer blocks → Output projection, where mixed-precision VMAC units handle different precision operations in parallel
- **Design tradeoffs**: Precision vs compression (FP8 preserves accuracy but uses more resources vs NVFP4 saves memory but risks accuracy loss), computational overhead of sensitivity analysis vs benefits of targeted precision preservation, hardware complexity of mixed-precision datapath vs simplified uniform-precision implementation
- **Failure signatures**: Large perplexity spikes when critical blocks are incorrectly identified as non-sensitive, activation quantization overflow/underflow causing numerical instability, performance degradation when mixed-precision datapath cannot efficiently handle precision transitions
- **First experiments**: 1) Quantize individual transformer blocks to NVFP4 and measure perplexity degradation to validate sensitivity analysis, 2) Implement sensitivity-weighted clipping and compare against uniform clipping on low-precision blocks, 3) Simulate mixed-precision VMAC throughput and energy consumption versus uniform-precision alternatives

## Open Questions the Paper Calls Out
None specified in source material.

## Limitations
- Evaluation limited to single model (Llama-2-7B) and dataset (Wikitext-103), lacking cross-model validation
- Energy savings and hardware performance claims based on architectural projections rather than physical hardware measurements
- Mixed-precision hardware augmentations described at high level without detailed circuit analysis or synthesis results
- Sensitivity analysis applied to pre-trained models without fine-tuning, limiting generalizability to instruction-tuned models
- Optimal precision allocation threshold (1.8% FP8 blocks) not extensively explored for sensitivity to different selection criteria

## Confidence
- High: Mathematical formulation of sensitivity-weighted quantization and clipping is internally consistent and well-explained
- Medium: Perplexity and perplexity-per-compression results on Llama-2-7B are reproducible given described methodology
- Low: Claimed energy savings and hardware performance benefits require actual silicon measurements for validation

## Next Checks
1. Implement FGMP on physical hardware (FPGA or ASIC prototype) to measure actual energy consumption and latency, comparing against FP8 baseline
2. Evaluate FGMP across multiple LLM families (Mistral, Gemma, Qwen) and scales (7B, 13B, 70B) to assess generalizability beyond Llama-2-7B
3. Test FGMP with post-training quantization fine-tuning on instruction-tuned models to evaluate robustness to different training paradigms and downstream tasks