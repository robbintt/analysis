---
ver: rpa2
title: 'InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative
  Grounding'
arxiv_id: '2509.16534'
source_url: https://arxiv.org/abs/2509.16534
tags:
- evidence
- hypothesis
- planning
- grounding
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "integrative grounding" as a systematic challenge
  in retrieval-augmented generation, where systems must retrieve and verify multiple
  interdependent pieces of evidence to support a hypothesis query. The authors construct
  INTEGROUND, a comprehensive evaluation framework spanning four domains (EntailmentBank,
  WiCE, HotpotQA, and MuSiQue), to assess grounding systems' ability to handle complete,
  redundant, incomplete, and uninformative evidence scenarios.
---

# InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding

## Quick Facts
- **arXiv ID**: 2509.16534
- **Source URL**: https://arxiv.org/abs/2509.16534
- **Reference count**: 16
- **Primary result**: Introduces INTEGROUND benchmark to evaluate grounding systems' ability to handle complete, redundant, incomplete, and uninformative evidence scenarios

## Executive Summary
This paper introduces "integrative grounding" as a systematic challenge in retrieval-augmented generation, where systems must retrieve and verify multiple interdependent pieces of evidence to support a hypothesis query. The authors construct INTEGROUND, a comprehensive evaluation framework spanning four domains (EntailmentBank, WiCE, HotpotQA, and MuSiQue), to assess grounding systems' ability to handle complete, redundant, incomplete, and uninformative evidence scenarios. The framework evaluates both verification (determining if evidence supports a hypothesis) and retrieval planning (generating queries to retrieve relevant evidence).

Key findings reveal that while LLMs are robust to redundant evidence, they tend to rationalize using internal knowledge when provided with incomplete information—a significant reliability concern. In retrieval planning, the study shows that undirected planning strategies like query expansion can degrade performance by introducing noise, while logically-constrained approaches such as premise abduction consistently improve results by directing the search space expansion. Notably, zero-shot self-reflection capabilities enhance grounding quality across all planning strategies, highlighting the value of iterative refinement.

## Method Summary
The study evaluates integrative grounding through INTEGROUND, a benchmark covering four domains with controlled evidence conditions (complete, redundant, incomplete, uninformative). The framework tests two subtasks: groundedness verification (binary classification of evidence support) and retrieval planning (query generation for multi-step retrieval). Systems use retrievers (BM25, dense models), planners (query expansion, decomposition, premise abduction), and verifiers (NLI models, LLMs, or ensembles). Self-reflection is optionally added as a refinement step. Performance is measured via accuracy, precision, recall, F1 for verification, and Recall@5 for retrieval planning.

## Key Results
- LLMs tend to rationalize using internal knowledge when provided incomplete evidence, misclassifying it as supported
- Premise abduction (directed query generation based on logical constraints) outperforms undirected strategies like query expansion
- Zero-shot self-reflection consistently improves grounding quality across all planning strategies
- NLI+LLM ensemble verification provides better detection of incomplete/uninformative evidence at the cost of reduced recall on informative cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Premise abduction improves retrieval by directing search space expansion through logical constraints.
- **Mechanism**: Given a hypothesis, the model generates plausible premises that would be required to entail it (backward-chaining). These premises become targeted queries, reducing noise compared to undirected expansion.
- **Core assumption**: The hypothesis is logically structured and decomposable into inferable premises.
- **Evidence anchors**:
  - [abstract]: "premise abduction emerges as a promising approach due to its logical constraints"
  - [section 5.2]: "Premise Abduction performed best... This success could be attributed to the directed nature of such planning methods"
  - [corpus]: Weak direct support; related work on multi-hop QA (Credible Plan-Driven RAG) aligns but doesn't validate abduction specifically.
- **Break condition**: Hypotheses that are vague, metaphorical, or lack clear logical structure may yield irrelevant abduced premises.

### Mechanism 2
- **Claim**: Zero-shot self-reflection enhances grounding quality across all planning strategies.
- **Mechanism**: After initial retrieval, the model analyzes retrieved evidence, identifies missing information, and generates refined queries—adding a feedback loop that compensates for initial planning weaknesses.
- **Core assumption**: LLMs can accurately identify evidence gaps without additional training.
- **Evidence anchors**:
  - [abstract]: "LLMs' zero-shot self-reflection capabilities consistently enhance grounding quality"
  - [section 5.2, Figure 3]: "incorporating a zero-shot self-reflection step consistently improved the integrative grounding task"
  - [corpus]: MERMAID (arXiv 2601.22361) supports iterative refinement with multi-agent grounding.
- **Break condition**: When evidence gaps require domain expertise the model lacks, reflection may generate irrelevant follow-up queries.

### Mechanism 3
- **Claim**: Combining NLI and LLM predictions yields more conservative verification, mitigating rationalization.
- **Mechanism**: NLI models provide high-precision entailment judgments that constrain LLMs' tendency to fill gaps with internal knowledge. The ensemble trades some recall for faithfulness.
- **Core assumption**: NLI models trained on entailment tasks generalize to integrative grounding scenarios.
- **Evidence anchors**:
  - [section 4.2, Figure 2]: "this ensemble approach improves LLMs' ability to identify incomplete and uninformative instances"
  - [section 4.2]: "LLM verifiers tend to rationalize incomplete evidence with internal knowledge... NLI models achieve high precision"
  - [corpus]: MedTrust-RAG (arXiv 2510.14400) similarly combines verification with trust alignment.
- **Break condition**: On domains where NLI training data poorly matches evidence style (e.g., technical jargon), precision may drop.

## Foundational Learning

- **Concept**: Abductive reasoning (inference to best explanation)
  - **Why needed here**: Premise abduction is the top-performing planning strategy; understanding backward-chaining logic is essential for debugging and extending it.
  - **Quick check question**: Given "X happened," can you generate plausible prerequisite conditions?

- **Concept**: Multi-hop reasoning dependencies
  - **Why needed here**: Integrative grounding requires synthesizing interdependent evidence; naive retrieval misses chains.
  - **Quick check question**: For "A caused C," what intermediate evidence bridging "A→B→C" would you need?

- **Concept**: Groundedness vs. factuality distinction
  - **Why needed here**: The paper tests whether evidence *supports* a hypothesis, not whether it's *true*—a critical evaluation framing.
  - **Quick check question**: If evidence E supports hypothesis H, does H become false if E is later disproven?

## Architecture Onboarding

- **Component map**: Hypothesis → Planning (abduction preferred) → Retrieval → Verification (NLI+LLM ensemble) → [Reflection → Re-retrieve if gaps] → Final grounded output

- **Critical path**: Hypothesis → Planning (abduction preferred) → Retrieval → Verification (NLI+LLM ensemble) → [Reflection → Re-retrieve if gaps] → Final grounded output

- **Design tradeoffs**:
  - Premise abduction vs. query expansion: Abduction is directed but requires logical structure; expansion is general but adds noise.
  - NLI-only vs. LLM-only vs. ensemble: NLI is precise but conservative; LLM is flexible but prone to rationalization; ensemble balances both.
  - Reflection depth vs. latency: Each reflection round improves quality but adds retrieval and inference cost.

- **Failure signatures**:
  - Model outputs entailment on incomplete evidence (rationalization detected via NLI disagreement)
  - Retrieval returns irrelevant documents after query expansion (noise injection)
  - Reflection loops without convergence (evidence genuinely absent or queries misaligned)

- **First 3 experiments**:
  1. Ablate planning strategies on a held-out subset: Compare no-planning, query expansion, decomposition, and premise abduction using Recall@5.
  2. Test verification robustness: Feed models incomplete, redundant, and uninformative evidence sets; measure rationalization rate (LLM-only vs. NLI+LLM).
  3. Add reflection rounds: Run 0, 1, 2 reflection iterations with premise abduction; plot grounding quality vs. latency to find the practical ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can instruction-tuning or conservative verification mechanisms effectively mitigate LLMs' tendency to rationalize using internal knowledge when faced with incomplete evidence?
- Basis in paper: [explicit] The authors state: "Future work could explore mitigation strategies, such as instruction-tuning models to explicitly forbid relying on internal knowledge, or incorporating more conservative verification mechanisms, like the NLI models we evaluated, to act as a safeguard against such rationalization."
- Why unresolved: The paper identifies rationalization as a significant reliability concern but only documents the problem and tests an LLM+NLI ensemble that improves incomplete/uninformative detection at the cost of reduced informative accuracy.
- What evidence would resolve it: Experiments comparing instruction-tuned models against baseline LLMs on the incomplete evidence condition, measuring whether they appropriately withhold judgments rather than rationalize.

### Open Question 2
- Question: How well do integrative grounding approaches transfer to structured or semi-structured knowledge sources such as knowledge graphs and tabular data?
- Basis in paper: [explicit] The Limitations section states: "Grounding to structured or semi-structured data, such as tabular data or knowledge graphs, is also an important and promising direction for future research."
- Why unresolved: The evaluation is restricted to the textual domain; all four datasets (EntailmentBank, WiCE, HotpotQA, MuSiQue) contain only text propositions.
- What evidence would resolve it: Construction of an integrative grounding benchmark over knowledge graphs or tabular corpora, with evaluation of the same verification and planning strategies tested in this work.

### Open Question 3
- Question: How do classical symbolic planners compare to LLM-driven planning strategies for integrative grounding tasks?
- Basis in paper: [explicit] The Limitations section notes: "A comparison with established non-LLM planning techniques, such as classical symbolic planners, was beyond our scope but remains an important direction for future comparative analysis."
- Why unresolved: The study focuses exclusively on LLM-driven planning strategies (query expansion, decomposition, premise abduction); no symbolic planning baselines were evaluated.
- What evidence would resolve it: A comparative study that implements symbolic planning approaches on the INTEGROUND benchmark and reports Recall@k against the LLM-based planners.

### Open Question 4
- Question: Does integrative grounding performance generalize across languages, or are current findings English-specific?
- Basis in paper: [explicit] The Limitations section states: "We primarily focus on English corpora, meaning our findings may not generalize to other languages. Evaluation of grounding on multilingual corpora is left for future work."
- Why unresolved: All source datasets (EntailmentBank, WiCE, HotpotQA, MuSiQue) are English-only; no multilingual evaluation was conducted.
- What evidence would resolve it: Creation of a multilingual INTEGROUND dataset covering the same evidence scenarios, with comparative results across languages for verification and planning components.

## Limitations

- Findings are based on synthetic evidence perturbations rather than real-world noisy retrieval failures
- Evaluation is restricted to English-language tasks, limiting multilingual generalizability
- Results may not transfer to structured/semi-structured knowledge sources like knowledge graphs or tabular data

## Confidence

- **High confidence**: Retrieval planning effectiveness (query expansion degrades, premise abduction improves), verification ensemble benefits, rationalization behavior on incomplete evidence
- **Medium confidence**: Zero-shot self-reflection improvements (tested but limited ablation), premise abduction superiority (strongest results but few-shot examples not fully specified)
- **Low confidence**: Performance transfer to real-world noisy retrieval, cross-domain robustness beyond INTEGROUND datasets

## Next Checks

1. Test premise abduction and reflection strategies on an open-domain retrieval benchmark (e.g., BEIR) with naturally noisy evidence rather than synthetic perturbations.
2. Conduct ablation studies varying few-shot example quality and quantity for premise abduction to determine sensitivity to prompt engineering.
3. Evaluate grounding performance on non-English datasets or domains with different logical structures (legal, scientific) to assess generalizability beyond INTEGROUND's controlled conditions.