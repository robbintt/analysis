---
ver: rpa2
title: Explainable RL Policies by Distilling to Locally-Specialized Linear Policies
  with Voronoi State Partitioning
arxiv_id: '2511.13322'
source_url: https://arxiv.org/abs/2511.13322
tags:
- state
- learning
- policy
- regions
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model-agnostic knowledge distillation method
  that partitions the state space of a Deep Reinforcement Learning (DRL) policy into
  Voronoi cells, each governed by a locally-specialized linear model. The method iteratively
  trains subpolicies, splits regions based on prediction loss, and merges similar
  regions to maintain simplicity while retaining performance.
---

# Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning

## Quick Facts
- arXiv ID: 2511.13322
- Source URL: https://arxiv.org/abs/2511.13322
- Reference count: 40
- Key outcome: Model-agnostic distillation method partitions DRL policy state space into Voronoi cells, each governed by a locally-specialized linear model, achieving interpretable policies that match or slightly outperform original DRL policies.

## Executive Summary
This paper proposes a knowledge distillation method that converts complex Deep Reinforcement Learning (DRL) policies into interpretable ensembles of linear subpolicies. The approach partitions the continuous state space using Voronoi cells and trains a locally-specialized linear model for each region. The method iteratively splits regions based on prediction loss and merges similar regions to maintain simplicity while preserving performance. Evaluated on a continuous gridworld (SimpleGoal) and a classic control task (MountainCarContinuous), the approach produces interpretable policies with median returns matching or exceeding the original DRL policies.

## Method Summary
The method implements model-agnostic knowledge distillation of a Deep RL policy (TD3) into an ensemble of locally-specialized linear policies. The state space is partitioned using Voronoi cells, with each cell governed by a linear subpolicy. The algorithm starts with one linear subpolicy and one codeword (initial state), then iteratively runs the teacher policy to fill buffers assigned by nearest codeword lookup. Subpolicies train on their specific buffers using MSE loss with Adam optimizer. If local prediction loss exceeds a threshold, the region splits by adding a new codeword/subpolicy pair. If neighboring subpolicies have similar parameters (below L∞ distance threshold), they merge. The process stabilizes in final epochs to freeze the topology.

## Key Results
- On SimpleGoal: Distilled policies achieve median return of 15.38 vs 16.96 for DRL (coverage: 86.17% vs 97.6%)
- On MountainCarContinuous: Distilled policies achieve median return of 93.48 vs 93.48 for DRL (coverage: 92.18% vs 100%)
- Some outliers in SimpleGoal due to incomplete state space coverage, particularly near pitfalls

## Why This Works (Mechanism)

### Mechanism 1: Loss-Responsive Partitioning
The algorithm traverses trajectories taken by the DRL policy and computes MSE between DRL actions and local linear subpolicy actions. If accumulated loss exceeds max_pol_loss and state is sufficiently far from current codeword, a new codeword is inserted, splitting the region. This assumes DRL policy behavior is locally smooth enough for linear approximation within small regions.

### Mechanism 2: Parameter-Similarity Pruning (Merging)
The algorithm periodically compares neighboring subpolicies using L∞ norm of parameter vectors. If norm is below min_param_distance, regions merge by removing one codeword and subpolicy. This assumes parameter similarity correlates with behavioral similarity in state space.

### Mechanism 3: Aggressive Local Overfitting
Linear subpolicies deliberately overfit to their local buffers without regularization, using MSE loss with mini-batches. The authors state newly seen data should be captured by new subpolicies rather than forcing generalization from individual subpolicies.

## Foundational Learning

### Concept: Voronoi Tessellations (Vector Quantization)
Why needed: Understanding how state space is discretized is essential since the method partitions continuous state space into convex polygons based on proximity to codewords.
Quick check: Given a point in 2D space and three codewords, which cell does the point belong to?

### Concept: Knowledge Distillation (Teacher-Student)
Why needed: The entire framework is a distillation process where a "Teacher" (DRL) guides a "Student" (Linear Ensemble). You must understand that the student learns from the teacher's outputs, not environment rewards directly.
Quick check: Does the student policy receive reward signals from the environment during the distillation phase described in Algorithm 1?

### Concept: Curse of Dimensionality in Nearest Neighbor Search
Why needed: The paper notes kd-tree efficiency limitations. As state dimensions increase, distance to nearest neighbor approaches distance to farthest neighbor, making partitioning less meaningful.
Quick check: Why does the paper advise caution when applying this method to high-dimensional state spaces (e.g., >10 dimensions)?

## Architecture Onboarding

### Component map:
Teacher (pre-trained DRL Policy) -> kd-tree (Codewords storage) -> Linear Models (Subpolicies registry) -> Buffers (Experience storage)

### Critical path:
1. Teacher interacts with environment; data stored in buffers based on nearest codeword lookup
2. Subpolicies train on their specific buffers (overfitting)
3. Split Check: If subpolicy has high error on trajectory, add new codeword/subpolicy pair
4. Merge Check: If neighboring subpolicies have similar parameters, remove one
5. Freeze: Stop topology changes in final epochs to stabilize weights

### Design tradeoffs:
- max_pol_loss: Lower values increase fidelity but explode number of partitions (complexity)
- min_param_distance: Higher values encourage more merging, simplifying model but risking behavioral "flattening"
- Model Class: Linear models used for maximum interpretability; changing to small MLPs would reduce interpretability but increase per-cell capacity

### Failure signatures:
- Runaway Partitions: If max_pol_loss is too low for environment's non-linearity, number of codewords grows indefinitely
- State Coverage Gaps: If teacher policy never visits certain states, partitioning has "holes," leading to outliers

### First 3 experiments:
1. Sanity Check (2D Visualization): Run distillation on 2D navigation task and visualize Voronoi cells. Do cell borders align with obstacles or goal states?
2. Hyperparameter Sensitivity: Vary max_pol_loss on MountainCarContinuous. Plot number of final codewords vs episodic return to find "knee" of performance/complexity curve
3. Stability Test: Disable merge_regions function. Observe if policy count spikes and if performance degrades due to over-partitioning and data scarcity per cell

## Open Questions the Paper Calls Out
1. Can the method be modified to define decision boundaries along state space axes rather than Voronoi codewords to improve global interpretability? The authors state a future version should define regions along state space axes since current Voronoi demarcations are difficult to interpret without performing inference.

2. How does the Voronoi partitioning approach scale to high-dimensional state spaces regarding both computational efficiency and partitioning fidelity? The Discussion notes that for high dimensionality, kd-tree lookup becomes exponentially less efficient and the definition of nearest points becomes less meaningful.

3. Does utilizing domain-specific control structures (e.g., PID controllers) as the local subpolicies improve the quality of distillation over generic linear models? The authors mention investigating controller-like schemes such as PID controllers in later iterations.

## Limitations
- The method is limited to low-dimensional state spaces due to the curse of dimensionality affecting kd-tree efficiency
- Missing implementation details like learning rate and batch size for linear subpolicy training hinder reproducibility
- Coverage metric formula is unspecified, making quantitative comparisons difficult
- High sample complexity for distillation process due to deliberate overfitting strategy

## Confidence
- High Confidence: Core Voronoi partitioning and splitting/merging logic is sound and well-described
- Medium Confidence: Reported quantitative results are likely accurate for specific hyperparameters used, but reproducibility is hindered by missing training details
- Low Confidence: Claim that method generalizes well to high-dimensional state spaces is weakly supported; experiments are limited to 2-3D tasks

## Next Checks
1. Implement the distillation method and run evaluation to confirm median returns and coverage percentages reported in Table 1, paying special attention to outlier behavior in SimpleGoal
2. Systematically vary max_pol_loss and min_param_distance on SimpleGoal. Plot tradeoff between final model complexity and performance to identify stable operating points
3. For outlier states in SimpleGoal, visualize the Voronoi cell that agent ends up in. Is the linear subpolicy in that cell simply incapable of avoiding the pitfall, or is the cell never reached by teacher policy during distillation?