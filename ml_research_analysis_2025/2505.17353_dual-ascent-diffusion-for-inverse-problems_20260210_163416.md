---
ver: rpa2
title: Dual Ascent Diffusion for Inverse Problems
arxiv_id: '2505.17353'
source_url: https://arxiv.org/abs/2505.17353
tags:
- ddiff
- diffusion
- daps
- noise
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Dual Ascent Diffusion (DDiff), a new optimization
  framework for solving inverse problems using diffusion model priors. Unlike existing
  MAP or posterior sampling approaches that rely on computational approximations,
  DDiff uses a dual ascent optimization framework inspired by ADMM.
---

# Dual Ascent Diffusion for Inverse Problems

## Quick Facts
- arXiv ID: 2505.17353
- Source URL: https://arxiv.org/abs/2505.17353
- Reference count: 40
- Primary result: Dual Ascent Diffusion (DDiff) achieves superior performance on image restoration tasks using diffusion model priors with ADMM-based optimization

## Executive Summary
This paper introduces Dual Ascent Diffusion (DDiff), a novel optimization framework for solving inverse problems using diffusion model priors. Unlike existing MAP or posterior sampling approaches that rely on computational approximations, DDiff uses a dual ascent optimization framework inspired by ADMM. The method alternates between three steps: an x-update for data matching, a z-update using the diffusion model as a denoiser, and a dual update with Lagrange multipliers. DDiff demonstrates superior performance compared to state-of-the-art methods across various image restoration tasks including super-resolution, inpainting, deblurring, phase retrieval, and high-dynamic range imaging.

## Method Summary
DDiff implements a three-step ADMM loop to solve inverse problems where measurements y = A(x) are given. The method alternates between: (1) an x-update that enforces data fidelity through gradient descent on the measurement constraint, (2) a z-update that denoises the current state using a pretrained score model via Tweedie's formula, and (3) a dual update that accumulates the constraint violation. A key innovation is the "reverse diffusion" step that projects the dual variable onto the diffusion manifold using a modified DDIM sampler, preventing artifacts that arise when applying the dual variable directly to off-manifold points. The method uses task-specific hyperparameters including step size schedules and noise schedules, with two different variance schedules depending on the forward model type.

## Key Results
- DDiff achieves better image quality as measured by PSNR, SSIM, LPIPS, and residual error metrics compared to state-of-the-art methods
- The method is more robust to high levels of measurement noise (σ > 0.05) than competing approaches
- DDiff produces faster results while maintaining measurement consistency and reducing visual artifacts in reconstructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual variable accumulation enforces measurement consistency better than penalty-based methods.
- Mechanism: Unlike Half-Quadratic Splitting (HQS) used in methods like DiffPIR, DDiff employs ADMM's dual update (u ← u + x - z). This allows the Lagrange multiplier u to "remember" constraint violations, applying a corrective pressure that forces the generative prior to adhere to the observed measurements y over time.
- Core assumption: The optimization landscape allows the dual variable to guide the sample toward the feasible set without destabilizing the diffusion trajectory.
- Evidence anchors:
  - [abstract] "achieves better image quality... more robust to high levels of measurement noise... estimates solutions that represent the observations more faithfully"
  - [section 3.3] "removing the dual update from DDiff would exactly emulate DiffPIR... incorporating Lagrange multipliers... offers improved empirical performance... where measurement consistency is crucial."
  - [corpus] "Mode-Seeking for Inverse Problems with Diffusion Models" validates the general difficulty of MAP estimation and consistency in this domain.
- Break condition: If the forward model A(·) is highly non-convex or the step size γ is too large, the dual variable may oscillate or diverge rather than converge to a steady state.

### Mechanism 2
- Claim: Projecting optimization updates onto the diffusion manifold stabilizes the score model.
- Mechanism: Standard ADMM applies a denoiser directly to x+u. DDiff recognizes that x+u is often "off-manifold" (unlikely under p_t), causing the score network s_θ to output poor estimates. DDiff mitigates this by re-sampling x_{t-1} using a modified DDIM step that explicitly re-injects noise and the dual update u at the correct noise level ᾱ_{t-1}.
- Core assumption: The score model s_θ is accurate only for inputs distributed according to the diffusion process p_t.
- Evidence anchors:
  - [section 3.2] "since the score model... is only trained on points sampled from p_t, it is a poor approximation... whenever x is unlikely under p_t."
  - [section 4.3] "Without the noising step, the dual variable introduces high-frequency artifacts that compromise the diffusion model's efficacy."
  - [corpus] Evidence linking manifold constraints to score model stability is a known concept (weakly anchored in specific corpus text, but strongly in paper Section 3.2).
- Break condition: If the noise schedule or re-scaling of u in Eq. 11 is incorrect, the state x_t drifts off-manifold, leading to artifact-heavy reconstructions.

### Mechanism 3
- Claim: Linearized gradient descent enables application to non-linear inverse problems.
- Mechanism: Instead of solving the full least-squares problem in the x-update (which requires inverting A), DDiff takes a single gradient step (Eq. 9). This linearized ADMM approach avoids complex inversions, allowing the method to tackle non-linear tasks like phase retrieval or non-linear deblurring efficiently.
- Core assumption: The forward model A is differentiable and a first-order approximation is sufficient for local progress.
- Evidence anchors:
  - [section 3.2] "In order for the method to be directly applicable to any differentiable forward model... we replace the minimization... with a single gradient step."
  - [section 4.2] "Nonlinear tasks consist of phase retrieval... DDiff demonstrates superior performance."
  - [corpus] "Solving ill-conditioned polynomial equations..." discusses similar gradient-based prior integration.
- Break condition: If the forward model A is not differentiable or has vanishing gradients, the x-update fails, breaking the data consistency loop.

## Foundational Learning

- **Concept: ADMM (Alternating Direction Method of Multipliers)**
  - Why needed here: DDiff is structurally an ADMM solver; understanding the primal (x, z) and dual (u) variable roles is required to debug convergence.
  - Quick check question: Can you explain why ADMM is preferred over simple gradient descent when splitting a data fidelity term from a complex prior?

- **Concept: Score-based generative modeling & Tweedie's Formula**
  - Why needed here: The method relies on the equivalence between the denoiser D(·) and the score function s_θ to implement the z-update.
  - Quick check question: How does Tweedie's formula relate the score of a distribution to a denoising operation?

- **Concept: Inverse Problems & Data Fidelity**
  - Why needed here: The core goal is balancing the diffusion prior against the physics of the measurement y = A(x).
  - Quick check question: In a MAP setting, how does the likelihood term p(y|x) differ from the prior p(x)?

## Architecture Onboarding

- **Component map:** Inputs (y, A, s_θ) → DDiff Loop (z-update → x-update → Reverse Diffusion → Dual Update) → Outputs (x, z, u)
- **Critical path:** The z-update and Reverse Diffusion sequence is the novel architectural component. If the scaling of u in the DDIM step (Eq. 11) is wrong, the prior overpowers the data or vice versa.
- **Design tradeoffs:**
  - Speed vs. Consistency: Linearized x-update (gradient step) is faster but may be less precise than iterative solvers for simple linear problems.
  - Robustness: The paper claims superiority at high noise levels (σ > 0.05) but potentially lower PSNR at very low noise levels compared to DAPS (see Fig 3).
- **Failure signatures:**
  - Measurement Drift: If residual error ||y - A(x)|| remains high, the x-update step size γ or the dual variable scaling needs adjustment.
  - Artifacts: High-frequency visual artifacts indicate the "noising step" (Eq. 11) was skipped or the dual update was applied naively without manifold projection (Diff-PnP-ADMM failure mode).
- **First 3 experiments:**
  1. Ablation on Dual Variable: Run DDiff vs. Diff-PnP-HQS (no u) on a simple inpainting task to verify the dual variable's contribution to measurement consistency.
  2. Manifold Projection Check: Compare the naive z-update (Eq. 8) vs. the proposed DDIM update (Eq. 11) on Gaussian deblurring to visualize artifact reduction.
  3. Noise Robustness: Test DDiff vs. DPS/DAPS on phase retrieval with increasing Gaussian noise (σ ∈ [0.0, 0.3]) to replicate the robustness curve in Fig. 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Dual Ascent Diffusion (DDiff) framework be effectively adapted to work with latent diffusion models (LDMs) rather than pixel-space diffusion models?
- Basis in paper: [explicit] The authors state in the "Limitations and Future Work" section: "an extension to latent diffusion models would be an interesting avenue of future work."
- Why unresolved: The current implementation and theoretical justification rely on pixel-space operations. Latent spaces compress data into lower-dimensional representations where the manifold constraints and the interaction between the dual variable u and the denoiser z may behave differently.
- What evidence would resolve it: A demonstration of the DDiff algorithm applied to a pre-trained LDM (e.g., Stable Diffusion) on inverse problems, showing that the dual ascent logic remains stable and effective in the latent space.

### Open Question 2
- Question: How does DDiff scale to higher-dimensional data representations, such as 3D volumes or video sequences?
- Basis in paper: [explicit] The authors identify this as a direction for future study, noting: "extending our approach to higher-dimensional data, such as 3D or video representations presents valuable directions with additional challenges."
- Why unresolved: The paper only validates the method on 2D images. Higher dimensions introduce increased computational costs and memory requirements, potentially exacerbating the manifold mismatch issues the z-update is designed to solve.
- What evidence would resolve it: Successful application of the method to video restoration tasks or 3D medical imaging reconstruction (e.g., MRI or CT volumes) with comparable performance gains over baselines.

### Open Question 3
- Question: Does the DDiff algorithm provably converge to the optimal MAP solution, and can its theoretical framework be extended to provide guarantees for posterior sampling?
- Basis in paper: [inferred] The paper explicitly notes it "do[es] not aim to develop a method that provably samples from the posterior" and relies on heuristic arguments for the z-update (Eq. 11) to handle manifold constraints without providing a formal convergence proof for the non-convex ADMM variant used.
- Why unresolved: Standard ADMM convergence guarantees assume convexity, which does not hold for diffusion model priors. The introduction of a modified DDIM step within the ADMM loop further complicates the theoretical convergence properties.
- What evidence would resolve it: A formal theoretical analysis establishing convergence bounds for the DDiff updates under specific conditions, or a modified derivation that allows for asymptotically exact posterior sampling.

### Open Question 4
- Question: Can the performance of DDiff be maintained using a universal set of hyperparameters (γ_t, σ_t) rather than task-specific tuning?
- Basis in paper: [inferred] The supplementary material (Appendix A.5, Tables S2/S3) reveals distinct hyperparameter settings (γ_0, t_γ, and different σ_t schedules) for each specific task (e.g., phase retrieval vs. super-resolution), suggesting the method currently requires manual calibration.
- Why unresolved: While the method is robust, the reliance on task-specific step sizes and noise schedules implies that the optimization landscape varies significantly across inverse problems, potentially limiting "plug-and-play" usability.
- What evidence would resolve it: An ablation study showing that a single, fixed schedule for γ_t and σ_t achieves statistically similar performance across all evaluated linear and nonlinear tasks.

## Limitations

- The method relies heavily on specific pretrained diffusion models, and performance may degrade with different model architectures or training data distributions.
- The linearized gradient descent in the x-update, while enabling non-linear problem solving, may be less precise than full iterative solvers for linear problems.
- The paper reports strong performance at high noise levels but potentially lower PSNR at very low noise levels compared to DAPS, suggesting a potential tradeoff in the method's noise sensitivity profile.

## Confidence

- **High confidence:** The core mechanism of using dual ascent optimization with ADMM structure is well-established and the empirical results showing improved measurement consistency are convincing.
- **Medium confidence:** The effectiveness of the manifold projection step and the specific re-scaling of the dual variable in the DDIM update, while theoretically sound, could benefit from additional ablation studies across more diverse forward operators.
- **Medium confidence:** The claim of superior performance across all metrics is well-supported for the tested tasks, but the generalization to completely different inverse problem types or imaging modalities remains untested.

## Next Checks

1. **Ablation Study on ADMM Components:** Systematically disable the dual update (converting to Diff-PnP-HQS) and the manifold projection step on multiple inverse problems to quantify their individual contributions to performance improvements.

2. **Cross-Dataset Generalization Test:** Apply DDiff trained on FFHQ to inverse problems on ImageNet and vice versa to evaluate robustness to domain shift in the diffusion prior.

3. **Convergence Analysis with Varying Noise Levels:** Extend the noise robustness experiments beyond σ = 0.05 to include both very low noise (σ < 0.01) and extremely high noise (σ > 0.3) regimes to map the full performance envelope.