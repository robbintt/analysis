---
ver: rpa2
title: Model Assembly Learning with Heterogeneous Layer Weight Merging
arxiv_id: '2503.21657'
source_url: https://arxiv.org/abs/2503.21657
tags:
- merging
- parameters
- learning
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Assembly Learning (MAL), a novel paradigm
  for merging heterogeneous neural network models with different architectures and
  training data. The core innovation is a generalized permutation transformation that
  addresses layer-width mismatches through zero-padding and bidirectional alignment,
  allowing selective parameter integration across layers.
---

# Model Assembly Learning with Heterogeneous Layer Weight Merging

## Quick Facts
- arXiv ID: 2503.21657
- Source URL: https://arxiv.org/abs/2503.21657
- Reference count: 5
- Primary result: Introduces Model Assembly Learning (MAL) for merging heterogeneous neural networks with different architectures and training data through generalized permutation transformation

## Executive Summary
This paper introduces Model Assembly Learning (MAL), a novel paradigm for merging heterogeneous neural network models with different architectures and training data. The core innovation is a generalized permutation transformation that addresses layer-width mismatches through zero-padding and bidirectional alignment, allowing selective parameter integration across layers. Unlike previous homogeneous model merging approaches, MAL enables iterative integration of parameters from diverse pre-trained models into a base model, guided by layer output invariance rather than full model output invariance.

The authors establish key laws and practical guidelines for effective heterogeneous parameter merging, including conditions for optimal merging based on base architecture performance, the effectiveness of zero-padding across datasets, and the importance of selective layer merging. Experimental results demonstrate that MAL maintains linear mode connectivity and preserves original-domain performance within a merging factor threshold while enabling knowledge transfer across 30 different architectures spanning 5 categories trained on 4 datasets (CIFAR-10, CIFAR-100, MNIST, FashionMNIST).

## Method Summary
The MAL framework addresses heterogeneous model merging through generalized permutation transformations that handle dimension mismatches via zero-padding. The method operates in two cases: size-compatible layers (where dimensions differ but one contains the other) use a linear assignment problem solver to align neurons by weight similarity, while size-incompatible layers employ bidirectional permutation with alternating optimization. The core principle is layer output invariance rather than full model invariance, enabling merging within a threshold merging factor (λ) that preserves Linear Mode Connectivity (LMC). The approach selectively merges layers following semantic progression (shallow to deep) and validates effectiveness through loss barrier analysis across the merging spectrum.

## Key Results
- MAL successfully merges 30 heterogeneous architectures across 5 categories spanning 4 datasets while preserving original-domain performance
- Zero-padding combined with permutation alignment enables effective knowledge transfer between layers of mismatched dimensions
- Maintains linear mode connectivity within a threshold merging factor, with loss barrier remaining low up to architecture-dependent λ*
- Cross-dataset knowledge transfer is effective when merging models trained on semantically similar datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-padding combined with permutation alignment enables knowledge transfer between layers of mismatched dimensions without destroying learned representations.
- **Mechanism:** When layer widths differ, the smaller dimension is padded with zeros to match the larger, then a generalized permutation matrix P aligns corresponding neurons by maximizing weight similarity via $\arg\max_P \langle W_A^l, P W_B^l P^\top \rangle_F$. Zero values don't interfere with the alignment objective while allowing structural compatibility.
- **Core assumption:** Key semantic features occupy a subspace that can be correctly aligned even when dimension-expanded; zero-padded dimensions contribute minimally to the alignment cost.
- **Evidence anchors:**
  - [abstract]: "generalized permutation transformation that addresses layer-width mismatches through zero-padding and bidirectional alignment"
  - [section 3]: "the essence of this transformation is to allow structural adjustments by padding the smaller dimension with zeros and align to the larger one"
  - [corpus]: Related work on heterogeneous merging (AdaMMS, arXiv:2503.23733) confirms cross-architecture alignment is an active research direction, though direct validation of zero-padding specifically is limited in neighbors.
- **Break condition:** If critical features are distributed across dimensions that become misaligned during zero-padding, or if the width ratio is extreme (e.g., 16→128), alignment quality may degrade substantially.

### Mechanism 2
- **Claim:** Bidirectional permutation (P on W_B, Q on W_A) resolves size-incompatible cases where neither weight matrix strictly contains the other's dimensions.
- **Mechanism:** When (m1-m2)×(n1-n2)<0, both models undergo permutation via alternating optimization: fix Q, optimize P via LAP solver; then fix P, optimize Q. This iteratively reduces the objective in Equation 6 until convergence.
- **Core assumption:** The alternating optimization finds a local optimum sufficiently close to a globally good alignment; both models can be transformed toward a shared subspace.
- **Evidence anchors:**
  - [section 3]: "we reformulate the optimization as in Equation 6 and adopt the alternating optimization strategy on P_l and Q_l"
  - [figure 1]: Heatmaps show successful merging across heterogeneous architectures with different layer counts (2, 3, 5, 6 layers).
  - [corpus]: GPTailor (arXiv:2506.20480) uses layer cutting/stitching, suggesting bidirectional manipulation is plausible, but corpus lacks direct validation of alternating permutation optimization.
- **Break condition:** If the two architectures have fundamentally incompatible feature spaces (e.g., CNN vs. Transformer attention patterns), alternating optimization may converge to poor local minima.

### Mechanism 3
- **Claim:** Merging within a "merging factor threshold" (λ < λ*) preserves Linear Mode Connectivity (LMC) and original-domain performance.
- **Mechanism:** Rather than requiring full model output invariance, MAL enforces layer output invariance as a weaker constraint. The loss barrier remains low for λ ∈ [0, λ*], where λ* is architecture/task-dependent. Beyond this threshold, merged weights exit the shared loss basin.
- **Core assumption:** Layer-wise alignment is sufficient to maintain a connected low-loss path; residual connections between layers don't accumulate alignment errors catastrophically within the threshold.
- **Evidence anchors:**
  - [abstract]: "preserves original-domain performance within a merging factor threshold"
  - [figure 1]: Lower-right plots show loss remaining low up to a threshold before increasing.
  - [corpus]: Merge and Bound (arXiv:2511.21490) demonstrates weight manipulation preserves CIL performance, providing weak corroboration for parameter-space operations.
- **Break condition:** Deep stacking of merged layers may cause error accumulation; if λ exceeds the threshold, performance degrades rapidly. Selective merging (avoiding deep→shallow) mitigates this.

## Foundational Learning

- **Concept: Linear Mode Connectivity (LMC)**
  - **Why needed here:** MAL redefines LMC for heterogeneous merging; understanding the loss barrier concept is essential to interpret merging factor thresholds.
  - **Quick check question:** Given two models A and B, can you sketch a loss curve L((1-λ)W_A + λW_B) that exhibits a barrier versus one that shows LMC?

- **Concept: Permutation Symmetry in Neural Networks**
  - **Why needed here:** The core innovation is generalized permutation for shape mismatches; you must understand why P^⊤ σ(P W h + P b) is functionally equivalent to σ(W h + b).
  - **Quick check question:** If you permute hidden units in layer l, what must happen to W_{l+1} to preserve the output?

- **Concept: Linear Assignment Problem (LAP)**
  - **Why needed here:** Size-compatible alignment reduces to LAP, solved via coordinate descent; understanding the objective argmax_P ⟨W_A, P W_B P^⊤⟩_F is prerequisite.
  - **Quick check question:** Why is matching neurons by weight similarity a reasonable proxy for matching learned features?

## Architecture Onboarding

- **Component map:** Model Zoo (heterogeneous) → Layer Selector → Dimension Matcher → Base Model ← Bidirectional Permuter ← Zero-Padder → Merged Model (validated via LMC loss barrier)

- **Critical path:**
  1. Identify base model and target layers from model zoo.
  2. Classify dimension relationship: size-compatible vs. size-incompatible.
  3. Apply zero-padding to smaller dimensions.
  4. Compute permutation(s) via LAP (size-compatible) or alternating optimization (size-incompatible).
  5. Merge with factor λ < λ* (tune threshold empirically).
  6. Validate LMC by sweeping λ and measuring loss barrier.

- **Design tradeoffs:**
  - **Selective vs. full merging:** Merging only some layers preserves LMC better but limits knowledge transfer.
  - **Shallow-to-deep vs. deep-to-shallow:** Deep-layer weights merged into shallower layers degrade performance (Figure 1, rows 3-4).
  - **Base architecture choice:** High-performing base models yield better merging (Law 1 in Section 4), but may overfit on simpler datasets.

- **Failure signatures:**
  - Loss barrier spikes at small λ → alignment failed; check permutation convergence.
  - Performance drops on original domain → λ exceeds threshold; reduce merging factor.
  - Cross-dataset transfer fails → semantic mismatch; verify layer correspondence (shallow layers encode low-level features).

- **First 3 experiments:**
  1. **Sanity check:** Merge two models with identical architecture on CIFAR-10; verify LMC holds and loss barrier ≈ 0. This validates permutation implementation.
  2. **Dimension mismatch test:** Merge a 32-width MLP into a 64-width base model on MNIST. Sweep λ and plot loss curve to identify λ*.
  3. **Selective layer ablation:** Merge only layers 1-2 (shallow) vs. layers 3-4 (deep) from a different-architecture model trained on FashionMNIST into an MNIST base. Compare LMC preservation and domain performance.

## Open Questions the Paper Calls Out
- The authors identify future work extending MAL to architectures incorporating residual connections or attention mechanisms for broader applications.
- They also specifically mention large language models as a target for building a more flexible and robust system.

## Limitations
- Zero-padding approach may degrade alignment quality when width ratios exceed ~2×, as semantic features could be distributed across expanded dimensions
- Alternating optimization convergence criteria are not precisely specified, raising concerns about local minima trapping in size-incompatible cases
- The merging factor threshold λ* appears architecture-dependent but lacks theoretical characterization beyond empirical tuning
- Cross-dataset transfer effectiveness depends heavily on dataset semantic similarity, which is not formally quantified

## Confidence
- **High confidence:** Layer output invariance as a weaker constraint than full model invariance; experimental results showing LMC preservation within threshold
- **Medium confidence:** Zero-padding + permutation alignment mechanism; bidirectional alternating optimization for size-incompatible cases
- **Low confidence:** Theoretical justification for why alternating optimization converges to good alignments; generalizability beyond MLP architectures

## Next Checks
1. **Width ratio sensitivity test:** Systematically evaluate zero-padding performance as width ratios increase from 1:1 to 1:8 to identify the practical limits of the alignment mechanism
2. **Convergence verification:** Implement monitoring for alternating optimization to track objective function improvement and establish minimum convergence criteria
3. **Cross-architecture stress test:** Apply MAL to merge CNN or Transformer models with MLP base architectures to evaluate generalization beyond the homogeneous MLP setting