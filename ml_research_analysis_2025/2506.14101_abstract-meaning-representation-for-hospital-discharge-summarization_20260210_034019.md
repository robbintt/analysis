---
ver: rpa2
title: Abstract Meaning Representation for Hospital Discharge Summarization
arxiv_id: '2506.14101'
source_url: https://arxiv.org/abs/2506.14101
tags:
- discharge
- summary
- source
- sentence
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an extractive summarization method for generating
  hospital discharge summaries from clinical notes. The method leverages CALAMR, a
  graph-based alignment technique, to match sentences from patient notes to appropriate
  sections of discharge summaries.
---

# Abstract Meaning Representation for Hospital Discharge Summarization

## Quick Facts
- **arXiv ID:** 2506.14101
- **Source URL:** https://arxiv.org/abs/2506.14101
- **Reference count:** 16
- **Primary result:** CALAMR-based extractive summarization achieves 88.72 weighted F1 on MIMIC-III for hospital discharge summary generation

## Executive Summary
This work presents an extractive summarization approach for generating hospital discharge summaries from clinical notes using Abstract Meaning Representation (AMR) graphs. The method leverages CALAMR, a graph-based alignment technique, to match sentences from patient notes to appropriate sections of discharge summaries. A BiLSTM model is trained on aligned data to classify sentences into summary sections. The approach is evaluated on MIMIC-III and UI Health datasets, showing high weighted F1 scores but significant limitations in completeness and readability due to the extractive nature and class imbalance.

## Method Summary
The method extracts sentences from clinical notes and classifies them into discharge summary sections using AMR-based alignment. CALAMR aligns clinical notes with reference discharge summaries by matching AMR graphs, creating a training dataset with sentence-level labels. A BiLSTM classifier with GatorTron clinical embeddings, note category, and section type features is trained to predict section labels. The model generates summaries by selecting sentences classified into relevant sections, prioritizing traceability and faithfulness over completeness or fluency.

## Key Results
- Weighted F1 scores of 88.72 (MIMIC-III) and 83.52 (UI Health) for section classification
- Human evaluation: Correctness 5/5, Readability 3.05/5, Completeness 1/5
- Extreme class imbalance ("no-section" label dominates) causes low Macro F1 despite high weighted F1
- Generated summaries are highly faithful but terse due to extractive approach

## Why This Works (Mechanism)
The method works by leveraging AMR's semantic representation to align clinical notes with discharge summaries at the sentence level. CALAMR identifies semantic matches between source sentences and summary sections, creating labeled training data. The BiLSTM classifier learns to predict appropriate sections for new sentences based on clinical embeddings and metadata. This approach ensures faithfulness by grounding each summary sentence in verifiable source content, avoiding the hallucination risks of abstractive methods.

## Foundational Learning
- **AMR Parsing:** Required to convert clinical text to semantic graphs for alignment. Quick check: Verify parser output quality on sample clinical sentences.
- **CALAMR Alignment:** Graph-based technique matching AMR graphs between notes and summaries. Quick check: Validate alignment accuracy on small test set.
- **Clinical Embeddings:** GatorTron provides domain-specific semantic representations. Quick check: Compare embeddings' ability to capture clinical concepts.
- **BiLSTM Classification:** Sequence model predicting section labels for sentences. Quick check: Monitor training loss convergence and validation F1.
- **Class Imbalance Handling:** Critical for managing "no-section" dominance. Quick check: Analyze per-class precision/recall distribution.
- **Human Evaluation:** Essential for assessing completeness and readability beyond automated metrics. Quick check: Review human scores for all three dimensions.

## Architecture Onboarding
**Component Map:** Clinical Notes -> AMR Parser -> CALAMR Alignment -> Labeled Dataset -> BiLSTM Classifier -> Section Classification -> Summary Generation

**Critical Path:** CALAMR alignment → Labeled dataset creation → BiLSTM training → Section classification → Summary assembly

**Design Tradeoffs:** Faithfulness vs. completeness (extractive limits synthesis), automation vs. accuracy (manual verification needed), computational cost vs. alignment quality (graph matching expensive)

**Failure Signatures:** High weighted F1 with low macro F1 indicates class imbalance; poor completeness scores reveal extractive limitations; memory errors suggest reentrancy issues in alignment

**First Experiments:**
1. Run CALAMR alignment on small MIMIC-III subset to verify hyperparameter sensitivity
2. Train BiLSTM with varying clinical embeddings to isolate embedding impact
3. Generate sample summaries and manually verify alignment correctness

## Open Questions the Paper Calls Out
The paper identifies several open questions related to improving the balance between faithfulness and fluency in clinical summarization. While it positions itself as a baseline for future abstractive methods, the authors do not provide specific solutions for integrating their traceable approach with more fluent generation techniques. The significant gap between correctness (5/5) and completeness (1/5) scores in human evaluation highlights the need for methods that can synthesize information while maintaining factual accuracy.

## Limitations
- Extreme class imbalance ("no-section" dominates) artificially inflates weighted F1 while masking poor minority section performance
- Limited completeness and readability due to extractive nature and sparse source notes in some cases
- High computational cost and memory requirements for CALAMR alignment on long clinical documents
- Dependence on quality of biomedical AMR parsing, which lacks standardization

## Confidence
- **High confidence:** Framework architecture (CALAMR + BiLSTM) is clearly described and reproducible with codebases
- **Medium confidence:** Evaluation methodology and metrics are appropriate, though human evaluation reveals significant limitations
- **Low confidence:** Reproducibility of alignment step limited by unspecified CALAMR hyperparameters and lack of standardized clinical AMR parser

## Next Checks
1. Reproduce CALAMR alignment on small MIMIC-III subset with varying Λ and µs values to identify hyperparameter sensitivity
2. Calculate per-class F1 scores to quantify performance on minority section types beyond weighted average
3. Test BiLSTM classifier with alternative clinical embeddings (BioBERT) to isolate GatorTron contribution to performance