---
ver: rpa2
title: Using Machine Learning for move sequence visualization and generation in climbing
arxiv_id: '2503.00458'
source_url: https://arxiv.org/abs/2503.00458
tags:
- sequence
- move
- holds
- landmarks
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates Machine Learning techniques for bouldering,
  building on previous student projects. The authors develop a visualization tool
  that generates humanoid skeletons from detected move sequences and create an interactive
  interface for selecting move sequences on standardized walls.
---

# Using Machine Learning for move sequence visualization and generation in climbing

## Quick Facts
- arXiv ID: 2503.00458
- Source URL: https://arxiv.org/abs/2503.00458
- Reference count: 11
- Primary result: Developed visualization pipeline and explored Transformer-based models for climbing move sequence generation

## Executive Summary
This work investigates Machine Learning techniques for bouldering, building on previous student projects. The authors develop a visualization tool that generates humanoid skeletons from detected move sequences and create an interactive interface for selecting move sequences on standardized walls. For move sequence prediction, they experiment with three approaches: a sequence-to-sequence model, an autoregressive Transformer with positional embedding, and a simplified Transformer with direct forward pass. While the results are not conclusive, the visualization pipeline produces satisfactory skeleton animations, and the simplified Transformer shows some improvement over earlier models. The main challenges include imprecise video data, limited model memory constraining coordinate discretization, and the difficulty of accurately predicting hold orderings from raw coordinate sequences. The work lays groundwork for future research in automated move sequence generation for climbing problems.

## Method Summary
The authors tackle two tasks: (1) generating humanoid skeleton animations from detected move sequences for climbing visualization, and (2) predicting move sequence ordering from unordered hold coordinates. For visualization, they use Mediapipe and OpenCV to extract 33 landmarks from climbing videos, apply linear interpolation for extremity motion between holds, and train Linear Regression to predict body landmarks from extremity positions. For prediction, they extract holds from Moonboard videos using Mediapipe and DBSCAN clustering, manually annotate move order for 20 sequences, and generate 50 random permutations per sequence for training data. They experiment with three models: a sequence-to-sequence model with 512-dim latent space, an autoregressive Transformer with coordinate-based positional embedding, and a simplified Transformer with encoder + linear decoder trained with cross-entropy loss.

## Key Results
- Visualization pipeline achieves >99% Linear Regression accuracy for body landmark prediction from extremity positions
- Simplified Transformer model shows some improvement over earlier seq2seq approaches
- Results for move sequence prediction are not conclusive due to data quality and model capacity limitations
- Coarse coordinate discretization (1 decimal precision) due to memory constraints significantly impacts prediction accuracy

## Why This Works (Mechanism)
The visualization pipeline works by leveraging the fact that extremity positions (hands and feet) provide sufficient information to reconstruct the full body pose through learned regression relationships. The prediction models work by treating move sequences as ordered permutations of hold coordinates, with the Transformer architectures learning to map unordered input sequences to their correct temporal ordering through attention mechanisms and positional encoding.

## Foundational Learning

### Coordinate-based Transformer Architecture
- **Why needed**: Standard Transformers require discrete token inputs, but climbing moves are continuous coordinates
- **Quick check**: Verify coordinate quantization preserves essential spatial relationships between holds

### DBSCAN Clustering for Hold Detection
- **Why needed**: Raw video detection produces noisy point clouds that need to be grouped into discrete holds
- **Quick check**: Ensure cluster count matches expected number of holds per problem

### Linear Interpolation for Pose Animation
- **Why needed**: Video frames don't capture every intermediate pose between holds
- **Quick check**: Confirm interpolated poses maintain realistic human kinematics

## Architecture Onboarding

### Component Map
Video frames -> Mediapipe pose detection -> Hold clustering (DBSCAN) -> Coordinate preprocessing -> Prediction model -> Token sequence output

### Critical Path
Data extraction (Mediapipe + DBSCAN) -> Coordinate quantization -> Model prediction -> Hold ordering -> Visualization

### Design Tradeoffs
- Memory vs precision: Coarser coordinate discretization reduces memory usage but hurts prediction accuracy
- Model complexity vs generalization: Simpler Transformers train faster but may underfit complex move patterns
- Automation vs accuracy: Fully automated pipelines are convenient but error-prone compared to manual annotation

### Failure Signatures
- Model predicts only padding token → Class imbalance or insufficient training diversity
- Coordinates far from ground truth → Discretization too coarse or model underfitting
- Visualizations show impossible poses → Interpolation method breaks anatomical constraints

### First Experiments
1. Replicate visualization pipeline with Mediapipe and test Linear Regression accuracy on hold-to-pose mapping
2. Implement DBSCAN clustering on synthetic hold data to verify correct grouping behavior
3. Train simplified Transformer on toy permutation prediction task to validate architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Limited model capacity due to memory constraints forces coarse coordinate discretization, degrading prediction accuracy
- Imprecise video data and detection errors propagate through the pipeline, affecting both visualization and prediction tasks
- Critical hyperparameters and architecture details are not specified, making exact replication challenging
- Results for sequence prediction are not conclusive and lack thorough statistical validation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Visualization pipeline achieves >99% accuracy | Medium |
| Simplified Transformer shows improvement over seq2seq | Low |
| Overall methodology is sound | Medium |

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary Transformer architecture parameters (layers, heads, embedding size, learning rate) and document their impact on token-level accuracy and cross-entropy loss to identify optimal configurations within memory constraints.

2. **Coordinate precision ablation study**: Compare model performance using different levels of coordinate discretization (e.g., 0.1 vs. 0.01 precision) to quantify the impact of coarse quantization on prediction accuracy and determine if memory limitations are the primary bottleneck.

3. **Visualization robustness test**: Evaluate the skeleton animation pipeline on a held-out set of climbing videos with manual ground-truth hold positions to measure interpolation accuracy and identify failure modes due to detection noise or missing data.