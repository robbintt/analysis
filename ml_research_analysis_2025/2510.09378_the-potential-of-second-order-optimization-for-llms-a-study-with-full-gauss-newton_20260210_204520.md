---
ver: rpa2
title: 'The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton'
arxiv_id: '2510.09378'
source_url: https://arxiv.org/abs/2510.09378
tags:
- gauss-newton
- batch
- size
- loss
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study benchmarks full Gauss-Newton optimization on transformer
  models up to 150M parameters, revealing that exact second-order updates substantially
  accelerate training at large batch sizes. Full Gauss-Newton achieved a 5.4x reduction
  in iteration complexity compared to strong baselines (SOAP, Muon) when targeting
  a fixed validation loss, and extended critical batch size scaling beyond prior methods.
---

# The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton

## Quick Facts
- **arXiv ID:** 2510.09378
- **Source URL:** https://arxiv.org/abs/2510.09378
- **Reference count:** 31
- **Primary result:** Full Gauss-Newton optimization achieves a 5.4x reduction in training iteration complexity compared to strong baselines on transformer models up to 150M parameters.

## Executive Summary
This study benchmarks full Gauss-Newton optimization on transformer models up to 150M parameters, revealing that exact second-order updates substantially accelerate training at large batch sizes. Full Gauss-Newton achieved a 5.4x reduction in iteration complexity compared to strong baselines (SOAP, Muon) when targeting a fixed validation loss, and extended critical batch size scaling beyond prior methods. A layerwise Gauss-Newton variant, which ignores cross-layer curvature, nearly matched full Gauss-Newton performance, suggesting that per-layer Hessian structure captures most optimization benefits. These findings indicate that full second-order information can dramatically improve LLM training efficiency, and that computationally cheaper layerwise approximations may suffice to close much of the gap to an idealized oracle.

## Method Summary
The method implements full Gauss-Newton optimization using Jacobian-vector products to avoid explicitly materializing the full Hessian. The approach computes a first-order Taylor approximation of the model, then forms a second-order Taylor approximation of the loss on this linearized model. An inner optimizer (Muon) minimizes this approximated loss for N steps, followed by a line search to determine the optimal step size. The implementation uses the neural-tangents library for Taylor approximations and controls batch size via b = N × b_inner where b_inner = 32 (45M) or 128 (150M). Training uses AdamW warmup for 5% of Chinchilla-optimal tokens before switching to the Gauss-Newton method.

## Key Results
- Full Gauss-Newton achieved a 5.4x reduction in training iterations compared to strong baselines (SOAP, Muon) when targeting a fixed validation loss of 3.25.
- The method extended critical batch size scaling beyond prior methods, continuing to reduce steps through batch sizes of 40M tokens.
- A layerwise Gauss-Newton variant, which ignores cross-layer curvature, nearly matched full Gauss-Newton performance, requiring only 1.4x more steps.

## Why This Works (Mechanism)

### Mechanism 1: Curvature-Guided Preconditioning
Full Gauss-Newton preconditioning provides substantial gains in iteration complexity compared to approximate second-order methods in the large batch regime. The Gauss-Newton matrix captures curvature of the loss with respect to the model's output, and by using this as a preconditioner, the update direction is scaled according to local curvature. This leads to more direct paths toward minima, reducing iterations required, particularly when large batches reduce gradient noise.

### Mechanism 2: Layerwise Curvature Sufficiency
Most optimization benefit from full Gauss-Newton can be achieved using a layerwise GN approximation that ignores cross-layer curvature. This block-diagonal approximation is computationally cheaper, and the finding that it performs nearly as well suggests that critical curvature information for optimization lies primarily within layers rather than in cross-layer interactions.

### Mechanism 3: Critical Batch Size Extension
The Gauss-Newton method extends the critical batch size beyond that of prior first-order and approximate second-order methods. Second-order methods like GN scale better to larger batch sizes because preconditioning helps better utilize more precise gradient estimates available with larger batches. GN's curvature-informed updates allow for continued gains in iteration efficiency even as batch size grows very large.

## Foundational Learning

- **Concept: Newton's Method and Gauss-Newton Matrix**
  - Why needed here: The entire paper is a study of a full Gauss-Newton optimizer. Understanding that Newton's method uses the Hessian inverse as a preconditioner, and that GN is a specific positive semi-definite approximation of the Hessian, is foundational to comprehending the proposed method.
  - Quick check question: What is the primary difference between the Hessian used in Newton's method and the Gauss-Newton matrix, and why is this difference advantageous for neural network training?

- **Concept: Critical Batch Size**
  - Why needed here: A key outcome is the effect of the GN optimizer on critical batch size. This concept defines the point of diminishing returns for scaling batch size in terms of training efficiency, a central metric in the paper's evaluation.
  - Quick check question: Explain what the critical batch size is and how a second-order optimizer might theoretically influence it.

- **Concept: Preconditioning in Optimization**
  - Why needed here: The paper frames GN as a "preconditioner" for the gradient update. Understanding that a preconditioner transforms the optimization landscape to improve convergence is essential for interpreting the results.
  - Quick check question: How does a preconditioner change the gradient update, and what property of the loss landscape is it trying to improve?

## Architecture Onboarding

- **Component map:** Linearization Module -> Loss Convexification Module -> Inner Optimizer -> Outer Update & Line Search

- **Critical path:**
  1. Start with current model parameters θ_t
  2. Compute first-order Taylor expansion of the model
  3. On linearized model, compute second-order Taylor expansion of the loss (forms GN quadratic approximation)
  4. Run inner optimizer (Muon) for N steps to find parameters that minimize approximated loss
  5. Perform line search on true loss using update direction
  6. Update model parameters with optimal step size

- **Design tradeoffs:**
  - Iteration Complexity vs. Compute per Step: Full GN dramatically reduces steps but adds significant computational overhead per step (4-5x slower wall-clock)
  - Full vs. Layerwise GN: Full GN captures all curvature information but is expensive; layerwise GN ignores cross-layer curvature but is more scalable
  - Stability (Line Search) vs. Simplicity: Line search is crucial for stable convergence but adds complexity and extra forward passes

- **Failure signatures:**
  - Instability/NaNs without careful regularization and line search, especially at high learning rates
  - No convergence gain if inner optimization is poor or curvature is not well-captured
  - Memory/Compute Explosion if attempting to explicitly materialize full GN matrix

- **First 3 experiments:**
  1. Implement full GN method on 45M model and measure steps to reach target validation loss against AdamW and Muon, replicating 5.4x gain
  2. Train same model across batch sizes (1.2M to 40M tokens) with GN and baselines, plotting steps to target loss vs batch size
  3. Implement layerwise GN variant and compare iteration complexity to full GN method, verifying within 1.4x steps

## Open Questions the Paper Calls Out

- Can the gap between current practical second-order methods (SOAP, Muon) and the idealized layerwise Gauss-Newton oracle be closed with computationally efficient approximations?
- Will the observed benefits of full Gauss-Newton optimization persist at multi-billion parameter scales?
- Are there preconditioning strategies superior to the direct Gauss-Newton inverse (G⁻¹)?
- In what training regimes might cross-layer curvature information become essential?

## Limitations

- Computational overhead per step is 4-5x slower wall-clock than baselines, raising questions about practical applicability to billion-parameter models
- Experiments limited to 150M parameter models; scaling behavior to production-scale LLMs remains unknown
- Performance heavily dependent on inner optimizer hyperparameters and line search procedure, which are not fully detailed

## Confidence

**High Confidence:** Full Gauss-Newton achieves 5.4x reduction in iteration complexity compared to baselines at fixed target loss for 150M parameter models.

**Medium Confidence:** Layerwise Gauss-Newton variant nearly matches full Gauss-Newton performance, assuming cross-layer curvature is negligible for larger models.

**Medium Confidence:** Gauss-Newton extends critical batch size beyond prior methods, though long-term scaling behavior and practical implications remain uncertain.

## Next Checks

1. **Scalability Validation:** Replicate full Gauss-Newton on 500M-1B parameter transformer model to test whether iteration complexity gains hold at scale, measuring both steps-to-target and wall-clock time.

2. **Inner Optimization Sensitivity Analysis:** Conduct systematic ablation study on Muon inner optimizer hyperparameters and line search parameters to quantify sensitivity and identify optimal configuration.

3. **Cross-Layer Curvature Importance Test:** Design experiment comparing full GN, layerwise GN, and hybrid variant with subset of cross-layer blocks to provide stronger evidence for/against layerwise sufficiency claim.