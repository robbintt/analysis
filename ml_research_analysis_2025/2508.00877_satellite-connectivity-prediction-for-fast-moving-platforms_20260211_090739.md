---
ver: rpa2
title: Satellite Connectivity Prediction for Fast-Moving Platforms
arxiv_id: '2508.00877'
source_url: https://arxiv.org/abs/2508.00877
tags:
- satellite
- dataset
- data
- altitude
- connectivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of maintaining reliable satellite
  connectivity for fast-moving platforms like aircraft, vehicles, and trains, where
  frequent switching between satellite beams or constellations is required due to
  mobility and varying signal conditions. The authors propose a machine learning-based
  approach to predict satellite signal quality using historical connectivity data,
  enabling proactive network switching before connectivity issues arise.
---

# Satellite Connectivity Prediction for Fast-Moving Platforms

## Quick Facts
- arXiv ID: 2508.00877
- Source URL: https://arxiv.org/abs/2508.00877
- Reference count: 25
- Primary result: ML model achieves F1 score of 0.97 predicting satellite signal quality for aircraft

## Executive Summary
This paper addresses the challenge of maintaining reliable satellite connectivity for fast-moving platforms like aircraft, vehicles, and trains. The authors propose a machine learning approach to predict satellite signal quality using historical connectivity data, enabling proactive network switching before connectivity issues arise. The solution uses multi-class classification to categorize signal quality into four levels and incorporates altitude and weather data to improve prediction accuracy. The model demonstrates high accuracy (F1 score of 0.97) in predicting signal quality during flight, with potential for application across various moving platforms.

## Method Summary
The authors develop a multi-class classification model to predict satellite signal quality (CNR values) categorized into Good, Medium, Weak, and Bad levels. The model was trained on real-world data from GEO satellite communication with aircraft, using flight logs containing geographic coordinates, altitude, and CNR measurements. The approach includes altitude-based model splitting (high altitude >6000m and low altitude <3000m) with weather data integration for low-altitude scenarios. H2O AutoML was used to train various models including GBM, XGBoost, GLM, Random Forest, Deep Learning, and Stacked Ensembles.

## Key Results
- Model achieved F1 score of 0.97 on test data for high-altitude (>6000m) scenarios
- Weather data integration improved low-altitude predictions by 1.5-1.8 percentage points in F1 score
- Discretizing CNR values into categories improved error tolerance compared to regression approaches
- Performance varied across different satellites, with I5F3 showing highest accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine learning models can predict satellite signal quality (CNR) for aircraft with high accuracy when trained on historical flight data with geographic coordinates.
- Mechanism: The model learns spatial-temporal patterns in Carrier-to-Noise Ratio (CNR) values across flight paths, enabling prediction of signal quality at specific locations before the aircraft arrives.
- Core assumption: Historical CNR patterns are sufficiently consistent across flights to generalize to future flights on similar routes.
- Evidence anchors:
  - [abstract] "Our prediction model achieved an F1 score of 0.97 on the test data, demonstrating the accuracy of machine learning in predicting signal quality during flight."
  - [section III, Table I] Dataset 1 with altitude >6000m achieved F1 = 0.97349; Dataset 2 with altitude >6000m achieved F1 = 0.96802.
- Break condition: Model performance degrades significantly on flight routes or geographic regions not represented in training data.

### Mechanism 2
- Claim: Incorporating weather data improves signal quality prediction accuracy at lower altitudes where atmospheric conditions have greater impact.
- Mechanism: Weather features (temperature, humidity, precipitation, etc.) are joined with flight logs using geo-coordinates and timestamps, providing additional explanatory variables for CNR fluctuations at altitudes below 3000m.
- Core assumption: Weather conditions are a primary driver of signal degradation at low altitudes, and hourly weather data at ~10km precision captures relevant variation.
- Evidence anchors:
  - [section II-B] "We believe the lack of weather data in our dataset could be a vital issue leading to this [worse performance at low altitudes]."
  - [section III, Table I] F1 improved from 0.94569 to 0.96054 (Dataset 1, altitude <3000m) and from 0.90216 to 0.91685 (Dataset 2, altitude <3000m) with weather data.
- Break condition: Weather data granularity (hourly, ~10km) is too coarse to capture localized atmospheric effects; real-time weather integration adds latency that negates prediction benefits.

### Mechanism 3
- Claim: Discretizing continuous CNR values into quality categories improves prediction robustness for handover decision-making compared to regression.
- Mechanism: Rather than predicting exact CNR values (MSE ~0.2), the model predicts 4 categories (Good: 15-20 dB, Medium: 10-15 dB, Weak: 6-10 dB, Bad: <6 dB), which better matches the threshold-based handover decision logic.
- Core assumption: Handover decisions require only categorical signal quality (above/below threshold) rather than precise dB values; classification is more tolerant of prediction errors than regression.
- Evidence anchors:
  - [section II-B] "Precise CNR values are not required directly for this purpose; instead, we can predict the range of CNR values to determine when a switch should be activated and this could increase the error-tolerant rate in our use case."
  - [section II-B] "F1 score is more appropriate, as it balances precision and recall, offering a better evaluation of the model's effectiveness across all classes" for imbalanced datasets.
- Break condition: Handover decisions require more granular CNR predictions than 4 categories provide; category boundaries don't align with actual modulation/coding rate requirements for different satellites.

## Foundational Learning

- Concept: **Carrier-to-Noise Ratio (CNR)**
  - Why needed here: CNR in dB is the target variable representing satellite signal quality; understanding that higher CNR = better signal is essential for interpreting model outputs and handover logic.
  - Quick check question: If CNR drops from 12 dB to 5 dB, which category does it transition through, and should handover be triggered?

- Concept: **Satellite Handover (Beam/Network Switching)**
  - Why needed here: The entire prediction system exists to enable proactive handover before connectivity degrades; understanding handover latency and disruption costs motivates the prediction approach.
  - Quick check question: Why is predicting signal quality 1 minute ahead useful for handover if handover itself takes time?

- Concept: **F1 Score for Imbalanced Multi-class Classification**
  - Why needed here: CNR categories are imbalanced (most values 8-10 dB per Figure 1); F1 balances precision and recall better than accuracy alone.
  - Quick check question: If 90% of samples are "Good" class and the model always predicts "Good," what would accuracy be vs. F1 score?

## Architecture Onboarding

- Component map: Flight log ingestion -> altitude-based routing -> weather join (if low altitude) -> feature vector -> model prediction -> handover trigger
- Critical path: Flight log ingestion → altitude-based routing → weather join (if low altitude) → feature vector → model prediction → handover trigger if Bad/Weak class predicted
- Design tradeoffs:
  - Classification chosen for error tolerance; trades precision for robustness
  - Split models add complexity but improve accuracy; threshold selection impacts coverage
  - Dataset 1 (top 5 routes) outperforms Dataset 2 (all routes) by ~0.5% F1; trades generalization for accuracy
  - Hourly/~10km weather precision may miss localized effects; higher precision increases cost and latency
- Failure signatures:
  1. Low F1 on new routes: Model overfits to training routes; requires retraining with route-diverse data
  2. Low-altitude predictions worse than high-altitude: Weather data missing or too coarse; add real-time weather feeds
  3. Class imbalance skew: Model predicts majority class ("Weak" 6-10 dB); check per-class F1, consider oversampling or class weights
  4. Temporal drift: Model trained on 2023 data degrades over time; requires periodic retraining with recent flight data
- First 3 experiments:
  1. Baseline replication: Train model on Dataset 2 (altitude >6000m) using H2O AutoML; verify F1 ≈ 0.97 on held-out test set to confirm reproducibility
  2. Altitude threshold sensitivity: Sweep altitude thresholds (2000m, 4000m, 6000m, 8000m) and measure F1; determine optimal split point for your operational envelope
  3. Weather feature ablation: Train low-altitude model with/without weather features; quantify F1 delta to validate weather data ROI for your use case

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific link characteristics or beam configurations cause the prediction model to perform significantly better on Satellite I5F3 compared to other satellites in the constellation?
- **Basis in paper:** [explicit] The authors note in the results section regarding the performance of Satellite I5F3 (Table I): "further research is required to determine the underlying reasons for this performance."
- **Why unresolved:** The paper reports the variance in F1 scores across different satellites (I5F1, I5F2, I5F3, I5F4, GX5) but does not provide a causal analysis for why I5F3 yields the highest accuracy.
- **What evidence would resolve it:** A feature importance analysis or physical layer analysis comparing the signal stability and noise distribution of I5F3 against the lower-performing satellites.

### Open Question 2
- **Question:** How can the model be integrated into a live system to trigger real-time, automated handovers based on the predicted signal quality categories?
- **Basis in paper:** [explicit] The conclusion states: "Future work will focus on automating handover decisions based on these predictions, integrating both satellite and weather data for real-time optimization."
- **Why unresolved:** The current work validates the prediction accuracy offline using historical data but does not implement the logic or latency measurements required to execute a switch in a live network environment.
- **What evidence would resolve it:** A system demonstration or simulation showing a handover triggered by the model's "Weak" or "Bad" prediction occurring before the actual signal degrades, measured against connection retention rates.

### Open Question 3
- **Question:** Can the proposed GEO-based classification approach maintain high accuracy when applied to Low Earth Orbit (LEO) satellites, where relative motion and Doppler shift are significantly higher?
- **Basis in paper:** [inferred] The introduction and conclusion explicitly claim the approach "can be retrained and applied to any moving object" and mention LEO providers like Starlink. However, the methodology and experiments are restricted exclusively to Geostationary Orbit (GEO) data.
- **Why unresolved:** GEO satellites are stationary relative to the Earth, while LEO satellites move rapidly. The model's reliance on location-based training may not translate directly to LEO dynamics without accounting for the satellite's own high-speed movement.
- **What evidence would resolve it:** Results from training and testing the model on a dataset derived specifically from a LEO satellite network (e.g., Starlink or OneWeb).

## Limitations

- Model was trained exclusively on aircraft data, so claims about generalization to vehicles and trains are weakly supported
- Weather data granularity (hourly, ~10km) may be too coarse to capture localized atmospheric effects at low altitudes
- Performance varies significantly across different satellites, with unexplained variance in accuracy
- No real-time system demonstration of automated handover implementation

## Confidence

- High confidence in classification mechanism (F1 = 0.97) and core prediction framework
- Medium confidence in altitude-based model splitting approach
- Low confidence in weather data impact due to limited evidence

## Next Checks

1. **Route Generalization Test**: Evaluate model performance on flight routes outside the top 5 used in Dataset 1 to quantify overfitting risk.
2. **Weather Data Sensitivity**: Train low-altitude models with weather data at multiple granularities (hourly vs. 15-min, 10km vs. 5km) to determine optimal precision vs. latency tradeoff.
3. **Cross-Platform Transferability**: Test model predictions on vehicle/railway satellite connectivity data to validate generalization claims beyond aviation.