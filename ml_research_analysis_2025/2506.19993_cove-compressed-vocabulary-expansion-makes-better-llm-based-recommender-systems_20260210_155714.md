---
ver: rpa2
title: 'CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems'
arxiv_id: '2506.19993'
source_url: https://arxiv.org/abs/2506.19993
tags:
- cove
- item
- embedding
- arxiv
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of leveraging large language
  models (LLMs) for sequential recommendation systems. Existing approaches either
  use LLMs as feature extractors or fine-tune them to generate item titles, both of
  which have limitations in fully utilizing LLMs' sequential understanding capabilities.
---

# CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems

## Quick Facts
- arXiv ID: 2506.19993
- Source URL: https://arxiv.org/abs/2506.19993
- Reference count: 33
- Primary result: Up to 62% improvement in recommendation accuracy over prior methods with 100x faster inference

## Executive Summary
This paper introduces CoVE, a framework that leverages large language models (LLMs) for sequential recommendation by expanding the tokenizer vocabulary to include unique item ID tokens. Unlike prior approaches that either use LLMs as feature extractors or fine-tune them to generate item titles, CoVE enables direct next-token prediction of items, achieving both higher accuracy and faster inference. The framework addresses scalability challenges through embedding layer compression using hashing, reducing memory requirements by up to 16x while maintaining performance. Experiments on Amazon review datasets demonstrate CoVE's effectiveness, achieving up to 62% improvement in recommendation accuracy over prior methods.

## Method Summary
CoVE expands the LLM's tokenizer vocabulary by assigning each item a unique ID token (e.g., `<|205|>`), enabling the model to predict items as single tokens rather than generating multi-token titles. During training, the model learns to predict these item ID tokens using LoRA fine-tuning with rank=8 and alpha=16. To handle large item spaces efficiently, CoVE employs hashing-based embedding compression, where each item's embedding is computed as the average of k hashed embeddings in a shared latent space. At inference, CoVE operates non-generatively by extracting logits directly for item tokens and ranking items, achieving approximately 100x speedup compared to fine-tune-and-retrieval approaches.

## Key Results
- Achieves up to 62% improvement in recommendation accuracy (NG@K and HR@K metrics) over prior LLM-based methods
- Provides approximately 100x faster inference compared to fine-tune-and-retrieval approaches
- Maintains performance with up to 16x embedding compression, reducing memory requirements from ~96GB to ~6GB for industrial-scale catalogs

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Expansion Converts Recommendation to Next-Token Prediction
CoVE assigns each item a unique ID token, enabling the LLM to directly predict items as single tokens. This eliminates the need for multi-token title generation and retrieval, leveraging the LLM's pretrained next-token prediction capability. The model learns ID-title mappings during training, and at inference, outputs logits over the expanded vocabulary to select recommended items.

### Mechanism 2: Hashing-based Embedding Compression via Shared Latent Space
Multiple hash functions map items to a compressed shared embedding space, where an item's embedding is computed as the average of its k hashed embeddings. This compositional representation preserves recommendation quality while dramatically reducing memory requirements. The semantic information needed for recommendation can be distributed across a smaller shared space without catastrophic interference between items.

### Mechanism 3: Non-Generative Inference via Logit Extraction
CoVE extracts logits directly from the language model head for item tokens, eliminating the need for autoregressive token generation. This provides ~100x speedup while guaranteeing predictions exist in the item catalog. The item tokens are at fixed positions in the expanded vocabulary, and the logits at these positions directly reflect recommendation relevance.

## Foundational Learning

- **Tokenizer Vocabulary Expansion**: Why needed: CoVE's core mechanism depends on adding item tokens to the tokenizer. Quick check: If you add 10,000 new item tokens to a tokenizer with original vocabulary size 32,000, what is the new dimension of the embedding matrix and language model head?

- **LoRA (Low-Rank Adaptation) Fine-tuning**: Why needed: The paper uses LoRA to fine-tune transformer weights alongside the expanded embedding table. Quick check: With LoRA rank=8 and alpha=16 on a weight matrix of shape [4096, 4096], how many trainable parameters does the LoRA adapter add per weight matrix?

- **Hash Functions for Embedding Compression**: Why needed: The hashing-based compression is what makes CoVE practical for industrial-scale item catalogs. Quick check: Given a shared space |S| = 1,000 and k = 4 hash functions, approximately how many items can be represented before collision probability becomes problematic?

## Architecture Onboarding

- **Component map**: Tokenizer (expanded) -> Embedding Layer (compressed) -> Transformer Blocks (LoRA) -> Language Model Head (expanded) -> Hashing Module

- **Critical path**: 
  - Training: Input text with item IDs → Tokenization → Embedding lookup (via hash averaging) → Transformer (LoRA) → LM head → Cross-entropy loss on next-token prediction (item ID)
  - Inference: Input user history → Tokenization → Forward pass → Extract logits for item tokens → Rank and return top-k

- **Design tradeoffs**:
  1. Compression rate vs. accuracy: Higher compression (16x) saves memory but degrades performance ~10-25%
  2. With vs. without item titles: Removing titles reduces performance significantly
  3. Trainable vs. frozen embeddings: Freezing item embeddings results in near-failure
  4. Hash function count (k): More hash functions reduce collision noise but increase computation

- **Failure signatures**:
  1. Random/gibberish recommendations: Item embeddings not properly trained
  2. Memory OOM during training: Compression rate too low for item catalog size
  3. Predictions not in catalog: Incorrect vocabulary expansion or token ID misalignment
  4. Performance no better than baseline at high compression: Hash collisions overwhelming signal

- **First 3 experiments**:
  1. Sanity check with small item space: Implement CoVE on 1,000 items without compression
  2. Compression ablation: Train CoVE with compression rates [1, 2, 4, 8, 16] and plot performance
  3. Title vs. no-title prompt comparison: Quantify performance gap from Table 5

## Open Questions the Paper Calls Out

- **Advanced compression methods**: Would more advanced compression methods (e.g., learned quantization, product quantization) outperform the simple hashing-based compression used in CoVE?

- **Industrial-scale performance**: How does CoVE perform on industrial-scale item spaces with millions of items, and at what scale does the compression-performance trade-off become unacceptable?

- **Combination with other memory-efficient techniques**: How would combining CoVE with other memory-efficient techniques (memory-efficient optimizers, quantization, low-rank approximation) affect both memory savings and recommendation accuracy?

- **Dataset-dependent compression robustness**: What causes the observed dataset-dependent variation in CoVE's robustness to embedding compression?

## Limitations

- **Scalability uncertainty**: Performance on industrial-scale item catalogs (millions of items) is not validated
- **Hash function sensitivity**: The paper doesn't provide sensitivity analysis for hash function parameters
- **Dataset specificity**: All experiments use Amazon review datasets with specific characteristics

## Confidence

- **High Confidence**: Vocabulary expansion mechanism and non-generative inference approach are well-supported
- **Medium Confidence**: Hashing-based compression mechanism shows promise but lacks comprehensive validation
- **Low Confidence**: Performance on industrial-scale catalogs and cross-domain generalizability are unknown

## Next Checks

1. **Industrial-Scale Validation**: Test CoVE on a dataset with 100K-1M items to evaluate compression effectiveness at scale and validate 100x inference speedup

2. **Hash Function Parameter Sensitivity**: Conduct systematic experiments varying hash function parameters and shared space sizes to identify optimal configurations across different datasets

3. **Cross-Domain Generalizability**: Apply CoVE to non-Amazon datasets (e.g., movie recommendations, music streaming) and test effectiveness without item titles using alternative semantic representations