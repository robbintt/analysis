---
ver: rpa2
title: 'ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model
  pretraining'
arxiv_id: '2601.01091'
source_url: https://arxiv.org/abs/2601.01091
tags:
- kashmiri
- language
- text
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KS-LIT-3M, a 3.1 million word Kashmiri text
  corpus specifically designed for pretraining large language models. The dataset
  addresses the critical scarcity of high-quality Kashmiri training data, which has
  prevented modern NLP systems from generating coherent text in this language despite
  its seven million speakers and rich literary heritage.
---

# ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining

## Quick Facts
- arXiv ID: 2601.01091
- Source URL: https://arxiv.org/abs/2601.01091
- Reference count: 15
- Key outcome: Introduces KS-LIT-3M, a 3.1 million word Kashmiri text corpus designed for LLM pretraining to address data scarcity

## Executive Summary
This paper introduces KS-LIT-3M, a 3.1 million word Kashmiri text corpus specifically designed for pretraining large language models. The dataset addresses the critical scarcity of high-quality Kashmiri training data, which has prevented modern NLP systems from generating coherent text in this language despite its seven million speakers and rich literary heritage. The corpus was constructed by developing a specialized InPage-to-Unicode converter to recover professionally published Kashmiri texts previously trapped in a proprietary format, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. The resulting dataset encompasses 131,607 unique words from diverse genres including literary works, journalism, academic texts, and religious scholarship.

## Method Summary
The dataset was created through a three-phase pipeline: (1) development of a custom InPage-to-Unicode converter with combining character reconstruction and Kashmiri-specific mappings to unlock professionally published texts trapped in proprietary format; (2) source collection from diverse genres prioritizing authenticity, diversity, and temporal range (1990s-2010s); (3) preprocessing including English contamination removal via sentence-level language detection, NFC normalization, whitespace standardization, and quality validation through automated checks and manual review. The output is structured as a continuous linear text stream optimized for causal language model training and released under CC-BY-4.0 license.

## Key Results
- 3,091,180 words, 16,358,993 characters, and 131,607 unique vocabulary items
- Successfully converts legacy InPage documents to Unicode, making decades of professionally published Kashmiri literature accessible to modern NLP pipelines
- Provides high-quality training data with preserved diacritics and proper character encoding for LLM pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting legacy InPage documents to Unicode unlocks high-quality training data that was previously invisible to modern NLP pipelines.
- Mechanism: InPage's proprietary encoding stores character codes, position information, and formatting in ways that don't map directly to Unicode's logical character model. The custom converter reverse-engineers this internal representation, reconstructs proper combining character sequences, and applies Kashmiri-specific mappings—making professionally edited text accessible to tokenizers and training frameworks.
- Core assumption: The professionally published literature trapped in InPage format contains grammatical correctness, proper spelling, and accurate diacritics that web-scraped alternatives lack.
- Evidence anchors: Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. InPage employs a proprietary encoding system incompatible with Unicode and are invisible to web crawlers compiling training data.

### Mechanism 2
- Claim: Accurate diacritic preservation enables models to learn semantic distinctions that web-scraped text cannot provide.
- Mechanism: Kashmiri diacritics encode meaning-altering distinctions. The converter's "combining character reconstruction" ensures proper Unicode sequencing of stacked diacritics. Models trained on this data encounter consistent, correct diacritic patterns during pretraining.
- Core assumption: Diacritic omission or misplacement in training data causes models to generate incomprehensible text for native speakers.
- Evidence anchors: Models that properly handle Kashmiri's complex diacritical system and linguistic features. These diacritics encode semantic distinctions—their omission or misplacement alters word meaning, making accurate diacritic handling essential.

### Mechanism 3
- Claim: Continuous linear text stream structure optimizes for causal language modeling by preserving cross-document context.
- Mechanism: Rather than artificially truncating at document boundaries with padding or special tokens, the uninterrupted sequence allows the model to learn discourse-level patterns and long-range dependencies that span documents.
- Core assumption: Document boundary information is less valuable than maximizing effective context utilization for a low-resource language.
- Evidence anchors: Structured as a continuous linear text stream optimized for causal language model training. This structure is optimal for causal language modeling, efficient tokenization, and context window utilization.

## Foundational Learning

- Concept: **Causal Language Modeling (Autoregressive Pretraining)**
  - Why needed here: KS-LIT-3M is explicitly designed for this paradigm where models predict subsequent tokens from preceding context. Understanding this objective clarifies why the continuous-stream structure matters.
  - Quick check question: Can you explain why predicting the next token requires different data formatting than masked language modeling (BERT-style)?

- Concept: **Unicode Normalization Forms (NFC/NFD)**
  - Why needed here: The preprocessing pipeline normalizes to NFC form to ensure "visually identical characters are represented identically." Without this, identical words could have different byte representations, fragmenting learned patterns.
  - Quick check question: Why might a combining character sequence (base + diacritic) and a precomposed character both represent the same visual glyph, and how does NFC handle this?

- Concept: **Low-Resource Language Constraints**
  - Why needed here: 3.1M words is substantial for Kashmiri but orders of magnitude smaller than high-resource corpora. This shapes realistic expectations about model capacity and the value of continual pretraining over from-scratch training.
  - Quick check question: What techniques become more important when pretraining data is limited (hint: think about leveraging related languages or existing multilingual models)?

## Architecture Onboarding

- Component map: Source InPage Files → Custom Converter → Raw Unicode Text → English Contamination Filter → Character Normalization (NFC) → Whitespace Standardization → Quality Validation → Continuous Text Stream (JSONL/CSV/XLSX)
- Critical path: The InPage-to-Unicode converter is the bottleneck. Without accurate conversion, all downstream processing operates on corrupted text. The separate publication documents the converter's methodology—consult it before attempting modifications.
- Design tradeoffs:
  - Continuous stream vs. document boundaries: Optimized for causal LM pretraining but may complicate document-level downstream tasks. Consider adding document boundary tokens if your use case requires them.
  - English removal vs. code-switching preservation: The pipeline filters English segments. If your target application involves Kashmiri-English code-switching, you'll need to modify or bypass this filter.
  - 3.1M words vs. model scale: This corpus size supports smaller models or continual pretraining. Training large models from scratch exclusively on this data risks underfitting or memorization.
- Failure signatures:
  - Generated text has incorrect or missing diacritics → tokenizer may be splitting combining characters; verify tokenization preserves diacritic sequences.
  - Model produces Urdu-like output → contamination from related-language pretraining; consider increasing Kashmiri data mixing ratio.
  - Incoherent grammar despite training → verify preprocessing didn't introduce artifacts; spot-check converted text against source documents.
- First 3 experiments:
  1. Tokenizer validation: Tokenize a sample of KS-LIT-3M and inspect how Kashmiri-specific characters and diacritic combinations are encoded. Verify combining characters remain attached to base characters.
  2. Continual pretraining baseline: Take a multilingual model with some Urdu/Persian exposure and continue pretraining on KS-LIT-3M. Evaluate diacritic accuracy on held-out text before/after.
  3. From-scratch small model: Train a ~10M parameter model from scratch on KS-LIT-3M alone. This establishes a lower bound on achievable quality and reveals whether the corpus contains sufficient signal for basic fluency.

## Open Questions the Paper Calls Out

- What performance gains do language models achieve when pretrained on KS-LIT-3M compared to baseline models lacking high-quality Kashmiri training data?
- How can standardized benchmarks for Kashmiri NLP be designed to enable objective comparison of model performance?
- Can the InPage-to-Unicode conversion methodology generalize effectively to other Perso-Arabic script languages such as Pashto, Sindhi, and Balochi?
- To what extent does KS-LIT-3M represent the dialectal variation present across Kashmiri-speaking regions and communities?

## Limitations
- The InPage-to-Unicode converter implementation details are insufficiently specified, preventing independent recreation of the core transformation pipeline.
- Source document provenance is not provided, making it impossible to verify claims about genre diversity, temporal coverage, or editorial quality.
- English contamination detection methodology lacks specific implementation details including tool selection, thresholds, and sentence-level processing criteria.

## Confidence
- High Confidence: Dataset construction methodology, continuous stream formatting, and availability/licensing information are clearly documented and verifiable.
- Medium Confidence: Corpus statistics and quality metrics are presented with sufficient detail for basic verification, though lacking statistical validation.
- Low Confidence: Converter implementation details, source document provenance, English contamination detection methodology, and quality validation criteria are insufficiently specified.

## Next Checks
1. Converter Accuracy Validation: Download original InPage documents (if accessible), apply the conversion methodology, and compare output against known correct Unicode representations, focusing on complex diacritic combinations.
2. Diacritic Preservation Testing: Select 1,000 random words from KS-LIT-3M, manually verify diacritic accuracy against Kashmiri orthographic standards, and calculate precision/recall of diacritic preservation compared to web-scraped text.
3. Pretraining Efficacy Experiment: Compare from-scratch training on KS-LIT-3M, continual pretraining of multilingual models on KS-LIT-3M, and from-scratch training on web-scraped Kashmiri text of comparable size, evaluating on diacritic accuracy and fluency metrics.