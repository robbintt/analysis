---
ver: rpa2
title: 'Not quite Sherlock Holmes: Language model predictions do not reliably differentiate
  impossible from improbable events'
arxiv_id: '2506.06808'
source_url: https://arxiv.org/abs/2506.06808
tags:
- eleutherai
- language
- impossible
- possible
- atypical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether language models can reliably distinguish
  possible events from merely improbable ones, a capability critical for real-world
  applications like medicine. Previous research conflated typicality and possibility,
  often relying on semantic relatedness as a heuristic for prediction.
---

# Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events

## Quick Facts
- arXiv ID: 2506.06808
- Source URL: https://arxiv.org/abs/2506.06808
- Reference count: 40
- Primary result: Language models rely on semantic relatedness as a heuristic, failing to distinguish possible-but-atypical events from impossible ones, even at larger scales

## Executive Summary
This study investigates whether language models can reliably distinguish possible events from merely improbable ones—a capability critical for real-world applications like medicine. Previous research conflated typicality and possibility, often relying on semantic relatedness as a heuristic for prediction. The authors created a controlled task using minimal pairs where critical words made sentences either possible or impossible, while varying typicality and semantic relatedness. They tested 35 pretrained language models on English and Mandarin stimuli. Results showed that language models performed worse at distinguishing possible but atypical events from impossible ones compared to typical vs. impossible events, and performed at or below chance when the critical word in impossible sentences was semantically related to the context.

## Method Summary
The study used the "Sherlock Holmes Task" to assess whether language models assign higher probability to sentences describing possible events versus impossible ones. The researchers created 154 English and 57 Mandarin minimal sentence pairs that differed by one critical word, creating conditions that manipulated typicality (typical/atypical), possibility (possible/impossible), and semantic relatedness (related/unrelated). Using the Language Model Evaluation Harness, they computed sentence-level log-probabilities for 35 base pretrained models across multiple families. The primary metric was accuracy—the proportion of pairs where models assigned higher probability to the possible sentence. Statistical analysis employed logistic mixed-effects regressions with random intercepts for model and sentence context.

## Key Results
- Language models performed significantly worse distinguishing possible-but-atypical from impossible events compared to typical vs. impossible events
- When the critical word in impossible sentences was semantically related to context, or in possible sentences was semantically unrelated, models performed at or below chance
- In the most adversarial case (possible-but-atypical-unrelated vs. impossible-but-related), models systematically assigned higher probabilities to impossible sentences
- This effect persisted across model sizes and did not improve with scale
- Models showed improved performance on typical vs. atypical comparisons but failed on atypical vs. impossible

## Why This Works (Mechanism)

### Mechanism 1
Language models use semantic relatedness as a heuristic shortcut for probability assignment, overriding world knowledge about event possibility. When a critical word is semantically related to the sentence context (e.g., "brake" in a car context), the model assigns higher probability regardless of whether the resulting sentence is impossible. This occurs because training data correlations between semantically related words are stronger and more frequent than signals about physical/semantic possibility.

### Mechanism 2
Models conflate typicality with possibility because training data over-represents typical events, making atypical-but-possible events appear "impossible" to the model. The distribution of training data means that "typical" and "possible" are nearly always correlated. When models encounter atypical events, they treat low probability (from rarity) as impossibility, failing to distinguish statistical improbability from logical impossibility.

### Mechanism 3
Scale does not resolve the relatedness heuristic because larger models amplify statistical patterns rather than developing discrete world-knowledge representations. Larger models trained on more data become better at detecting typicality differences but simultaneously increase reliance on semantic relatedness cues. The Pythia training checkpoints show that the adversarial case (PAU vs. IAR) never improves above chance across all scales.

## Foundational Learning

- **Concept: Minimal Pairs Paradigm**
  - Why needed here: The study's methodology hinges on comparing sentence pairs that differ by only one critical word, controlling for confounds. Understanding this is essential for interpreting the results and designing similar evaluations.
  - Quick check question: Can you explain why comparing "the cure was discovered by the doctor" vs. "the cure was discovered by the stamp" isolates the possibility judgment?

- **Concept: Semantic Relatedness vs. World Knowledge**
  - Why needed here: The core finding is that models substitute relatedness for world knowledge. Distinguishing these is critical for understanding model limitations and designing mitigations.
  - Quick check question: In the sentence "The museum visitor waited in the long ___," would a model assign higher probability to "painting" (related but impossible) or "toothbrush" (unrelated and impossible), and why does this matter?

- **Concept: Shortcut Learning / Heuristic Bias**
  - Why needed here: The paper frames this as another instance of models learning surface heuristics rather than robust capabilities, connecting to broader NLU reliability research.
  - Quick check question: How is the relatedness heuristic similar to the syntactic heuristics identified in NLI tasks (e.g., word overlap as a shortcut)?

## Architecture Onboarding

- **Component map**: Stimuli generator -> Probability extractor -> Comparison module -> Statistical analyzer
- **Critical path**: Load pretrained base model -> Tokenize each sentence pair identically -> Extract sequence log-probability for full sentence -> Compare: correct if possible > impossible -> Aggregate across 154 English pairs / 57 Mandarin pairs
- **Design tradeoffs**: Using sentence-level probability vs. cloze-style completion (chosen to test any model); animacy violations only (limits generalizability but provides clean impossibility signal); small datasets (enables controlled experiments but limits statistical power)
- **Failure signatures**: Below-chance performance on PAU vs. IAR (model systematically prefers impossible-but-related over possible-but-unrelated); improvement on typical vs. atypical/impossible but not atypical vs. impossible (indicates typicality sensitivity without possibility reasoning); larger models performing worse (suggests scale amplifies heuristic reliance)
- **First 3 experiments**: 1) Replicate the PTR vs. IAR baseline on your target model to establish it can handle non-adversarial cases (~90%+ accuracy expected); 2) Test the adversarial PAU vs. IAR condition to confirm the failure mode (expect ~30% accuracy—below chance); 3) Ablate semantic relatedness by testing whether removing context (just the critical word in isolation) changes the pattern, probing whether the heuristic requires contextual activation

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does word frequency act as a confounding variable in models' inability to distinguish impossible from improbable events? The authors identify word frequency as a potential confound and explicitly state it "should be studied in future work," but did not fully decouple it from the primary variables.

- **Open Question 2**: Can language models distinguish impossible events from improbable ones when the impossibility stems from physical or causal violations rather than animacy? The study only investigates "animacy violations" and acknowledges there are "many other ways in which a sentence could refer to an impossible event."

- **Open Question 3**: Is the reliance on semantic relatedness heuristics robust across lower-resource or typologically distinct languages? The study only tests English and Mandarin, both "extremely widely-spoken high-resource languages," leaving unknown if the observed brittleness is universal.

## Limitations
- Impossibility is operationalized exclusively through animacy violations, which may not generalize to other types of logical impossibility
- Relatively small stimulus sets (154 English, 57 Mandarin pairs) may limit statistical power
- Exclusion of instruction-tuned models means findings may not extend to post-training models
- Only tests English and Mandarin, limiting cross-linguistic generalizability

## Confidence
- **High Confidence**: The core finding that language models fail to distinguish possible from impossible events when semantic relatedness confounds the judgment, and that this effect persists across model sizes and scales
- **Medium Confidence**: The interpretation that models rely on relatedness as a heuristic shortcut, though alternative explanations cannot be fully ruled out
- **Low Confidence**: The claim that this limitation is fundamental and will not be overcome by scale alone, though the study shows persistence across tested scales

## Next Checks
1. **Cross-impossibility validation**: Replicate the study using non-animacy impossibility types (e.g., physical impossibility like "the rock floated in the air") to test whether the relatedness heuristic generalizes across impossibility types
2. **Tokenization control experiment**: Test whether the relatedness effect persists when comparing models with different tokenization schemes or when using character-level models, to rule out tokenization artifacts driving the results
3. **Architectural ablation study**: Compare the performance of transformer-based models with other architectures (e.g., recurrent neural networks, symbolic reasoning systems) on the same task to determine whether the relatedness heuristic is specific to transformer architectures or a more general limitation of current approaches