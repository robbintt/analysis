---
ver: rpa2
title: DOA Estimation with Lightweight Network on LLM-Aided Simulated Acoustic Scenes
arxiv_id: '2511.08012'
source_url: https://arxiv.org/abs/2511.08012
tags:
- spatial
- estimation
- audio
- dataset
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a lightweight neural network, LightDOA, for
  direction-of-arrival (DOA) estimation using interaural phase difference (IPD) features
  extracted from dual-channel audio. The method is evaluated on a diverse synthetic
  dataset constructed with LLM assistance, which introduces greater acoustic realism
  and semantic variety than traditional room impulse response (RIR)-based datasets.
---

# DOA Estimation with Lightweight Network on LLM-Aided Simulated Acoustic Scenes

## Quick Facts
- **arXiv ID:** 2511.08012
- **Source URL:** https://arxiv.org/abs/2511.08012
- **Reference count:** 31
- **Primary result:** LightDOA achieves 85.45% accuracy at 20° resolution with only 39k parameters on LLM-augmented synthetic acoustic dataset

## Executive Summary
This paper introduces LightDOA, a lightweight neural network for direction-of-arrival (DOA) estimation using interaural phase difference (IPD) features from dual-channel audio. The model leverages depthwise separable convolutions and a GRU for efficient processing, achieving high accuracy while maintaining a small footprint suitable for real-time applications. Experiments on a diverse synthetic dataset constructed with LLM assistance show that LightDOA outperforms larger baseline models in both accuracy and computational efficiency, demonstrating its potential for resource-constrained deployment.

## Method Summary
LightDOA processes IPD features extracted from dual-channel audio to estimate DOA through classification over discrete azimuth angles. The network uses three depthwise separable convolutional blocks to efficiently capture spatial patterns, followed by adaptive average pooling and a GRU for temporal modeling. The model is trained on a diverse synthetic dataset created with LLM assistance, which provides greater acoustic realism and semantic variety compared to traditional RIR-based datasets. Training uses Adam optimizer with early stopping and cross-entropy loss.

## Key Results
- LightDOA achieves 85.45% classification accuracy at 20° angular resolution
- Model size remains compact at only 39k parameters
- Outperforms larger baseline models (CRNN, MTL-DOA) in both accuracy and efficiency
- Demonstrates strong potential for real-time deployment in active noise control systems

## Why This Works (Mechanism)
LightDOA works by efficiently extracting spatial features from IPD representations using depthwise separable convolutions, which reduce computational cost while preserving important directional cues. The subsequent GRU layer captures temporal dependencies in the spatial features, enabling robust DOA estimation. The use of LLM-augmented synthetic data introduces greater acoustic realism and diversity, improving generalization beyond what traditional RIR-based datasets provide.

## Foundational Learning
- **Interaural Phase Difference (IPD):** Phase difference between signals at two microphones, used as spatial cue for DOA. *Why needed:* Primary feature for dual-channel DOA estimation. *Quick check:* Verify IPD values change with source angle.
- **Depthwise Separable Convolutions:** Efficient convolution operation that factorizes standard convolution into depthwise and pointwise steps. *Why needed:* Reduces parameters while maintaining performance. *Quick check:* Compare FLOPs with standard convolution.
- **GRU (Gated Recurrent Unit):** Simplified recurrent neural network cell for sequence modeling. *Why needed:* Captures temporal patterns in IPD features. *Quick check:* Ensure temporal dimension is preserved through network.

## Architecture Onboarding
**Component Map:** IPD Input → Depthwise Separable Conv Blocks → Adaptive AvgPool → GRU → FC Layers → DOA Output
**Critical Path:** Feature extraction (convs) → Temporal modeling (GRU) → Classification (FC)
**Design Tradeoffs:** Model prioritizes efficiency over capacity, using depthwise separable convolutions instead of standard convolutions to reduce parameters while maintaining accuracy
**Failure Signatures:** Incorrect IPD feature dimensions will cause shape mismatches at GRU input; front-back mapping errors lead to label distribution mismatch
**First Experiments:**
1. Verify front-back mapping produces angle distribution matching Fig. 2b
2. Check IPD feature dimensions match model input requirements (F, T)
3. Validate tensor shapes after each layer, especially GRU input/output

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified STFT parameters create uncertainty in exact IPD feature dimensions
- GRU and FC layer dimensions not fully detailed, allowing architectural variation
- Front-back mapping requires careful validation to ensure correct label alignment

## Confidence
- **High confidence** in overall model architecture concept and depthwise separable convolution approach
- **Medium confidence** in reported accuracy metrics assuming correct data preprocessing and training
- **Low confidence** in exact reproduction of IPD feature extraction due to unspecified STFT parameters

## Next Checks
1. Verify front-back mapping by checking angle distribution matches Fig. 2b after applying Eq. 8-9
2. Confirm IPD feature dimensions (F, T) by reproducing STFT computation and comparing to model input shape
3. Validate model architecture by checking tensor shapes after each layer, especially GRU input (B, T, F) and output