---
ver: rpa2
title: 'TransAM: Transformer-Based Agent Modeling for Multi-Agent Systems via Local
  Trajectory Encoding'
arxiv_id: '2508.02826'
source_url: https://arxiv.org/abs/2508.02826
tags:
- agent
- modeling
- learning
- agents
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransAM, a transformer-based agent modeling
  approach for multi-agent reinforcement learning that learns robust representations
  of other agents' policies from only the local trajectory of the controlled agent.
  The method encodes sequences of the controlled agent's rewards, actions, and observations
  into embedding spaces using a transformer encoder, enabling the agent to model others
  without requiring access to their trajectories at execution time.
---

# TransAM: Transformer-Based Agent Modeling for Multi-Agent Systems via Local Trajectory Encoding

## Quick Facts
- arXiv ID: 2508.02826
- Source URL: https://arxiv.org/abs/2508.02826
- Authors: Conor Wallace; Umer Siddique; Yongcan Cao
- Reference count: 23
- Primary result: Achieves highest episodic returns and competitive agent modeling accuracy in cooperative multi-agent environments using only local trajectory information

## Executive Summary
TransAM introduces a transformer-based approach for agent modeling in multi-agent reinforcement learning that learns robust representations of other agents' policies from only the local trajectory of the controlled agent. The method encodes sequences of rewards, actions, and observations into embedding spaces using a transformer encoder, enabling the agent to model others without requiring access to their trajectories at execution time. By jointly training the agent model and policy using generative loss for trajectory reconstruction and A2C reinforcement learning, TransAM demonstrates superior performance across four environments, particularly excelling in cooperative settings where agent modeling strongly correlates with higher returns.

## Method Summary
TransAM encodes the controlled agent's local trajectory (rewards, actions, observations) into embedding spaces using a transformer encoder. During training, it reconstructs other agents' observations and actions to learn policy-relevant representations, while during execution it uses these embeddings to condition its policy. The method jointly optimizes a generative loss for trajectory reconstruction and an A2C reinforcement learning objective, allowing the controlled agent to model others' behaviors without requiring their trajectory data at execution time.

## Key Results
- Achieves highest episodic returns among all baselines in Predator-Prey, Cooperative Navigation, Overcooked, and Level-Based Foraging environments
- Demonstrates competitive agent modeling accuracy (85.72% action accuracy, 0.01-0.02 MSE for observations) in cooperative settings
- Shows strong correlation between agent modeling performance and episodic returns in cooperative scenarios
- Outperforms oracle performance in some cooperative navigation cases
- Ablation studies confirm importance of multimodal embeddings, recent trajectory information, and joint observation-action reconstruction

## Why This Works (Mechanism)

### Mechanism 1: Local Trajectory Signal Encoding
The controlled agent's local trajectory (rewards, actions, observations) contains sufficient signal to infer the joint policies of other agents. TransAM encodes tuples (r_{t-1}, a_{t-1}, o_t) into token embeddings processed by a transformer encoder, trained to reconstruct other agents' observations and actions. This forces the latent embedding E^o_t to capture policy-relevant information, assuming other agents' policies are stationary within an episode and leave detectable imprints on the controlled agent's local trajectory.

### Mechanism 2: Temporal Context Disambiguation
Transformer self-attention over temporal sequences enables disambiguation of agent policies that appear identical in single transitions. Multi-head self-attention processes a context window of 3K tokens, attending across the full sequence to identify key interaction moments and long-range dependencies. This allows the model to extract policy-relevant signatures distributed across multiple timesteps, which would be indistinguishable from single transitions.

### Mechanism 3: Joint Representation Alignment
Joint online training of the agent model (reconstruction) and policy (A2C) aligns representation learning with decision-making utility. The combined loss L_AM + L_A2C provides gradients to both encoder and policy, creating pressure for representations that are simultaneously predictive of other agents' trajectories and useful for action selection. This ensures the reconstruction auxiliary task produces representations that transfer to improved policy returns.

## Foundational Learning

- **Concept: Partially Observable Stochastic Games (POSGs)** - Needed because TransAM operates in settings where agents have limited observations and must reason about others' hidden states. Understanding joint action/observation spaces and transition dynamics is prerequisite. *Quick check: Can you explain why a centralized critic (CTDE) approach differs from explicit agent modeling in terms of information requirements at execution?*

- **Concept: Actor-Critic Methods (A2C)** - Needed because the policy training component uses A2C, requiring understanding of advantage estimation, value functions, and policy gradient updates. *Quick check: What is the role of the advantage function A^π(s, a) in the A2C loss, and how does entropy regularization βH(π) affect exploration?*

- **Concept: Transformer Encoder Architecture** - Needed because TransAM uses an encoder-only transformer with multi-head self-attention and layer normalization. Understanding token embeddings, positional context, and attention patterns is essential. *Quick check: How does self-attention differ from recurrent processing in capturing long-range dependencies, and what is the role of the context window K?*

## Architecture Onboarding

- **Component map:** Local trajectory tuples (r_{t-1}, a_{t-1}, o_t) → Linear embeddings (T^r, T^a, T^o) → 4-layer transformer encoder (4 heads, 128 hidden dim) → Embedding sequence E^o_t → Policy head (LSTM 128 + linear) + Reconstruction head (o^{-1}, a^{-1})

- **Critical path:** 1) Collect local trajectory tuples during environment interaction 2) Tokenize and feed to transformer encoder to obtain E^o_t 3) Use E^o_t to condition policy for action selection 4) During training, pass E^o_t to reconstruction head and compute L_AM 5) Backpropagate combined loss to update encoder, policy, and reconstruction head

- **Design tradeoffs:** Separate token embeddings improve modeling accuracy (action accuracy 85.72% vs 78.68% with fused tokens); recent embeddings outperform pooling (returns -48.76 vs -49.37); joint reconstruction yields better returns (-48.76 vs -49.93) than imitation-only decoder

- **Failure signatures:** Embeddings uninformative early in episode (slow convergence of modeling accuracy); high reconstruction accuracy but low returns (loss conflict or overfitting); struggles in mixed cooperative-competitive settings (complex reward structures may obscure policy signatures)

- **First 3 experiments:** 1) Ablate context window K (K=5 vs default vs K=20) in cooperative navigation to measure impact on temporal disambiguation and returns 2) Vary reconstruction loss weight (0.1x, 1x, 10x) to identify balance point where modeling accuracy aids policy returns 3) Stress test with policy-switching agents (introduce intra-episode policy changes) to evaluate break conditions and embedding adaptability

## Open Questions the Paper Calls Out

- **Open Question 1:** How does TransAM performance scale in multi-agent systems with significantly more agents? The current experiments are restricted to environments with only 2 to 4 agents, leaving computational and modeling efficiency on larger scales untested.

- **Open Question 2:** Can TransAM operate effectively in recursive reasoning domains where opponents actively model the controlled agent? The current formulation assumes teammates/opponents follow fixed policies and do not adapt to the controlled agent's strategy during the episode.

- **Open Question 3:** How robust is the model when teammate policies change within a single episode? Section 3.3 explicitly assumes agents adopt a fixed policy "at the beginning of each episode," a constraint often violated in real-world adaptive scenarios.

## Limitations

- Performance in purely competitive or mixed-motive scenarios is uncertain, as experiments focus primarily on cooperative settings
- Sensitivity to context window size and impact of intra-episode policy changes are not fully characterized
- Other agent policy pool size and diversity are unspecified, which could affect model generalization

## Confidence

**High Confidence:** TransAM achieves competitive agent modeling accuracy and episodic returns in cooperative environments; transformer encoder's self-attention effectively captures temporal context; joint training aligns representation learning with task utility

**Medium Confidence:** Reconstruction-from-local-signal mechanism is effective but robustness to policy changes mid-episode is untested; ablation studies support multimodal embeddings but exact weightings and hyperparameters are unspecified

**Low Confidence:** Method's performance in purely competitive or mixed-motive settings is uncertain; sensitivity to context window size and impact of intra-episode policy changes are not fully characterized

## Next Checks

1. Ablate context window K (K=5 vs default vs K=20) in cooperative navigation to measure impact on temporal disambiguation and returns
2. Vary reconstruction loss weight (0.1x, 1x, 10x) to identify balance point where modeling accuracy aids rather than hinders policy returns
3. Stress test with policy-switching agents (introduce intra-episode policy changes) to evaluate break conditions and embedding adaptability