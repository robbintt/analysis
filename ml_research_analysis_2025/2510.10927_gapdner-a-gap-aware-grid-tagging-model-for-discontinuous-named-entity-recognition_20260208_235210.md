---
ver: rpa2
title: 'GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition'
arxiv_id: '2510.10927'
source_url: https://arxiv.org/abs/2510.10927
tags:
- entity
- discontinuous
- attention
- grid
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GapDNER, a gap-aware grid tagging model for
  discontinuous Named Entity Recognition (DNER) in biomedical text. The model addresses
  the challenges of recognizing entities composed of non-adjacent tokens by treating
  context gaps as special spans and converting span classification into a token-pair
  grid tagging task.
---

# GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition

## Quick Facts
- arXiv ID: 2510.10927
- Source URL: https://arxiv.org/abs/2510.10927
- Authors: Yawen Yang; Fukun Ma; Shiao Meng; Aiwei Liu; Lijie Wen
- Reference count: 34
- Primary result: Achieves SOTA F1 scores of 73.65%, 82.87%, and 82.72% on CADEC, ShARe13, and ShARe14 biomedical datasets respectively.

## Executive Summary
GapDNER introduces a gap-aware grid tagging approach for discontinuous Named Entity Recognition (DNER) in biomedical text. The model treats context gaps between entity fragments as distinct spans, converting span classification into a token-pair grid tagging task. It employs a two-module approach: an intra-span regularity extraction module using Biaffine mechanism and linear attention, and an inter-span relation enhancement module utilizing criss-cross attention. The model demonstrates significant improvements in recognizing complex entity structures and handling overlapped entities, achieving new state-of-the-art performance on three biomedical datasets.

## Method Summary
GapDNER uses a token-pair grid tagging approach where discontinuous entities are recognized by modeling context gaps as explicit span types. The model consists of a BERT-based encoder followed by a BiLSTM layer to generate token representations. The intra-span module applies Biaffine decoder and linear attention to capture internal regularity of each span, while the inter-span module uses criss-cross attention to model semantic relations among different spans. During decoding, a BFS algorithm searches for alternating fragment-gap paths to reconstruct entities. The model is trained with AdamW optimizer and evaluated using span-level F1 score.

## Key Results
- Achieves state-of-the-art F1 scores of 73.65%, 82.87%, and 82.72% on CADEC, ShARe13, and ShARe14 datasets respectively
- Ablation study shows 1.1-2.0 point F1 drops when removing Biaffine, Linear attention, or Criss-cross attention modules
- Demonstrates significant improvements in recognizing complex entity structures and handling overlapped entities compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly modeling context gaps as a distinct span type reduces decoding ambiguity in discontinuous entity recognition.
- Mechanism: GapDNER converts discontinuous NER into a token-pair grid tagging task. Instead of only labeling entity fragments, it introduces a dedicated "⟨Gap⟩" label for context intervals between fragments. During decoding, a BFS algorithm searches for alternating fragment-gap paths. The explicit gap constraint narrows the combinatorial search space for valid entity assembly.
- Core assumption: Discontinuous entities adhere to an alternating sequence of entity fragments and context gaps. If gaps are correctly identified, valid entity paths become uniquely determinable.
- Evidence anchors:
  - [abstract] "we treat the context gap as an additional type of span and convert span classification into a token-pair grid tagging task."
  - [section III] "we discover that each range of discontinuous entities follows the rule that entity fragments and context gaps are arranged alternatively."
  - [corpus] SEDA (arXiv:2511.20143) also uses grid-based discontinuous NER but addresses data scarcity via augmentation, not gap modeling. No corpus papers directly validate the gap-as-span approach.

### Mechanism 2
- Claim: Encoding intra-span regularity (internal composition patterns) improves classification for both entity fragments and context gaps.
- Mechanism: The intra-span regularity extraction module combines: (1) a Biaffine decoder that models semantic interactions between span boundary tokens (head and tail), and (2) linear attention over all tokens within each span to aggregate internal features. These are concatenated to form a token-pair grid feature matrix.
- Core assumption: Entity fragments possess distinguishable internal semantic patterns (e.g., core content words), while context gaps consistently contain non-entity tokens. Linear attention is sufficient to capture these patterns efficiently.
- Evidence anchors:
  - [abstract] "The intra-span regularity extraction module employs the biaffine mechanism along with linear attention to capture the internal regularity of each span."
  - [section IV-B] "linear attention successfully extracts the internal composition regularity of each span" with visualization in Fig. 8.
  - [corpus] No corpus validation for linear attention in intra-span DNER; SEDA and other papers do not address this specific technique.

### Mechanism 3
- Claim: Modeling inter-span relations via criss-cross attention enhances classification for overlapping and adjacent spans.
- Mechanism: The inter-span relation enhancement module applies criss-cross attention on the token-pair grid. Each grid cell attends to all cells in its row and column, capturing relationships between spans that share head or tail tokens. This is particularly suited to overlapped entity structures common in discontinuous NER.
- Core assumption: Overlapping spans with shared boundaries have predictive semantic relations. Criss-cross attention's row/column restriction is sufficient to capture these dependencies without the prohibitive cost of full-grid attention.
- Evidence anchors:
  - [abstract] "the inter-span relation enhancement module utilizes criss-cross attention to obtain semantic relations among different spans."
  - [section IV-C] "discontinuous entities often appear together with overlapped structures... several spans share the same head or tail token and thus appear in the same row or column of the token-pair grid."
  - [corpus] Structure-aware decoding (arXiv:2512.13980) addresses complex entity extraction with LLMs but does not validate criss-cross attention for grid-based inter-span modeling.

## Foundational Learning

- Concept: Biaffine Decoder
  - Why needed here: Models the semantic interaction between head and tail tokens of a span, providing a boundary-aware span representation.
  - Quick check question: How does a biaffine transformation differ from a standard bilinear layer in terms of parameterization and input handling?

- Concept: Linear Attention
  - Why needed here: Aggregates features from all tokens within a span in O(n) time, capturing internal composition patterns without quadratic complexity.
  - Quick check question: What is the key computational difference between linear attention and standard softmax attention?

- Concept: Criss-Cross Attention
  - Why needed here: Enables efficient modeling of relationships between spans sharing grid rows or columns, critical for overlapping entity structures.
  - Quick check question: What are the receptive field and computational complexity advantages of criss-cross attention compared to full self-attention on a grid?

## Architecture Onboarding

- Component map:
  Sequence Encoder Layer: BERT → Max pooling → BiLSTM → Token representations H (n×d)
  Intra-Span Regularity Extraction: MLPs for head/tail → Biaffine boundary features + Linear attention regularity features → Concatenated grid G (n×n×2d)
  Inter-Span Relation Enhancement: MLP dimension reduction → Criss-cross attention (row+column) → Element-wise addition → Enhanced grid M'' (n×n×d)
  Classification & Decoding: Linear + Softmax → Grid tags → BFS path search for valid fragment-gap alternating paths → Entities

- Critical path: Text → BERT → BiLSTM → Biaffine + Linear attention (intra-span) → Criss-cross attention (inter-span) → Grid classification → BFS decoding. The ablation study (Table III) shows removing Biaffine, Linear attention, or Criss-cross attention causes F1 drops of 1.1–2.0 points, confirming all three are critical.

- Design tradeoffs:
  - Grid tagging handles discontinuous/overlapping entities natively but requires O(n²) grid memory.
  - Linear attention for intra-span regularity is efficient but may not capture long-range dependencies within spans as effectively as full attention.
  - Criss-cross attention is more efficient than full grid attention but restricts each span to attend only to row/column elements.
  - BFS decoding deterministically enumerates paths, avoiding autoregressive error propagation, but is sensitive to grid prediction errors.

- Failure signatures:
  - Low recall on long gaps (>5 tokens): Fig. 7 shows F1 declines for long gaps.
  - Poor performance at gap length=1: Fig. 7 shows lowest F1 for single-token gaps, possibly due to confusion with continuous entities.
  - Over-prediction of overlapping entities: If criss-cross attention amplifies spurious correlations, precision may drop.
  - No valid paths found: If fragment/gap labels are mispredicted, BFS may return no entities for a sentence.

- First 3 experiments:
  1. Replicate ablation: Remove Biaffine (replace with head-tail concatenation) and measure F1 drop (expected ~1.4–2.0 points from Table III).
  2. Visualize linear attention: Plot attention weights over span tokens for a held-out example to verify high weights on core entity words for fragments and non-entity words for gaps (cf. Fig. 8).
  3. Stratify by gap length: Compare F1 across gap-length buckets vs. baseline W2NER to quantify improvements on long gaps (Fig. 7).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the quadratic complexity of the token-pair grid and the BFS-based decoding strategy impact inference speed and memory usage on long documents compared to sequence-labeling baselines?
- **Basis in paper:** [inferred] The model utilizes a token-pair grid ($R^{n \times n}$) and a Breadth-First Search algorithm for entity decoding, which typically incurs higher computational costs than linear sequence labeling.
- **Why unresolved:** The paper evaluates performance based on F1 scores but does not report computational efficiency metrics (e.g., training/inference time or memory footprint) relative to baselines.
- **What evidence would resolve it:** Comparative analysis of inference latency and GPU memory consumption across datasets with varying sentence lengths.

### Open Question 2
- **Question:** Is the assumption that entity fragments and context gaps are "arranged alternatively" robust to linguistic variations in non-biomedical domains?
- **Basis in paper:** [inferred] The model design relies on the observation that "entity fragments and context gaps are arranged alternatively," and experiments are restricted to three biomedical datasets (CADEC, ShARe13, ShARe14).
- **Why unresolved:** The structural patterns of discontinuous entities may differ significantly in general domains (e.g., news, legal) or languages with different syntactic structures, potentially violating the alternating assumption.
- **What evidence would resolve it:** Evaluation of GapDNER on standard general-domain discontinuous NER benchmarks (e.g., ACE05, OntoNotes) or multilingual datasets.

### Open Question 3
- **Question:** Does the "Gap" representation learning introduce noise when the interval between fragments contains other nested named entities rather than non-entity tokens?
- **Basis in paper:** [inferred] The "Gap" is defined as the "context interval between two neighboring fragments," and the linear attention module learns regularities from these tokens as if they were non-entity separators.
- **Why unresolved:** If a discontinuous entity is separated by another entity (nested/overlapped), the tokens in the gap possess strong semantic meaning unrelated to the "gap" role, which could confuse the intra-span regularity extraction.
- **What evidence would resolve it:** Error analysis focused on discontinuous entities where the intervening text segments contain other annotated named entities.

## Limitations
- Performance degradation on long gaps: Model exhibits notable performance decline on discontinuous entities with longer context gaps, particularly for gaps of length 1 where F1 scores are lowest.
- Limited domain generalizability: Evaluation is restricted to clinical text with specific entity types, lacking validation on other domains where discontinuous entity patterns may differ.
- Computational overhead: Quadratic complexity of grid tagging combined with criss-cross attention creates significant computational overhead and potential scalability challenges.

## Confidence
- **High Confidence**: The core claim that GapDNER achieves state-of-the-art performance on the evaluated datasets is well-supported by reported F1 scores and ablation study results.
- **Medium Confidence**: The claim that explicitly modeling gaps as spans reduces decoding ambiguity is mechanistically sound but lacks direct empirical validation across datasets.
- **Low Confidence**: The assertion that criss-cross attention specifically enhances performance on overlapping entities is weakly supported, with no direct comparison to alternative attention mechanisms.

## Next Checks
1. **Gap length analysis replication**: Replicate the stratified performance analysis by gap length from Fig. 7, comparing GapDNER against W2NER specifically for single-token gaps and gaps >5 tokens to quantify the degradation patterns and validate the reported performance drop.

2. **Ablation on attention mechanisms**: Remove the criss-cross attention module and replace it with either (a) no inter-span modeling or (b) full self-attention on the grid. Measure the impact on overlapping entity recognition to directly test whether the row/column restriction provides specific advantages.

3. **Cross-domain evaluation**: Apply GapDNER to a non-biomedical discontinuous NER dataset (such as ACE or OntoNotes) to test whether the gap-aware approach generalizes beyond clinical text, measuring performance relative to baseline models and assessing whether the fragment-gap alternation pattern holds in other domains.