---
ver: rpa2
title: 'Surrogate Learning in Meta-Black-Box Optimization: A Preliminary Study'
arxiv_id: '2503.18060'
source_url: https://arxiv.org/abs/2503.18060
tags:
- learning
- optimization
- surrogate
- metabbo
- surr-rlde
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational cost bottleneck in meta-black-box
  optimization (MetaBBO) by integrating surrogate learning with reinforcement learning.
  The proposed Surr-RLDE framework combines Kolmogorov-Arnold Networks (KAN) trained
  with a novel relative-order-aware loss for surrogate learning and reinforcement
  learning for dynamic Differential Evolution (DE) configuration.
---

# Surrogate Learning in Meta-Black-Box Optimization: A Preliminary Study

## Quick Facts
- **arXiv ID:** 2503.18060
- **Source URL:** https://arxiv.org/abs/2503.18060
- **Reference count:** 40
- **Primary result:** Surr-RLDE framework integrates KAN surrogate models with reinforcement learning for meta-black-box optimization, achieving competitive performance while reducing computational cost.

## Executive Summary
This paper addresses the computational bottleneck in meta-black-box optimization (MetaBBO) by integrating surrogate learning with reinforcement learning. The proposed Surr-RLDE framework uses Kolmogorov-Arnold Networks (KAN) trained with a novel relative-order-aware loss to approximate optimization landscapes, then applies reinforcement learning to dynamically configure Differential Evolution (DE) parameters. By substituting expensive function evaluations with surrogate predictions during meta-level policy training, the approach significantly reduces computational overhead while maintaining optimization effectiveness. The method demonstrates strong generalization capabilities, successfully transferring learned policies to higher-dimensional problems.

## Method Summary
The Surr-RLDE framework operates in two stages: first, it trains a KAN-based surrogate model using a novel Relative-Order-Aware (ROA) loss function that preserves the relative ranking of solutions. Second, it applies reinforcement learning (specifically Double-DQN) to learn a policy that dynamically configures DE mutation strategies and scaling factors based on population state features. The surrogate model replaces the true objective function during policy training, dramatically reducing the number of expensive function evaluations. The ROA loss combines mean squared error with an order correction term that ensures predicted values maintain the correct relative ordering, addressing a key limitation of standard regression approaches for this application.

## Key Results
- Surr-RLDE achieves competitive performance to existing MetaBBO baselines while demonstrating strong generalization to higher-dimensional problems.
- Ablation studies confirm the effectiveness of both the KAN architecture and the relative-order-aware loss function in maintaining landscape structure accuracy.
- The approach successfully validates that surrogate-assisted MetaBBO can maintain learning effectiveness while reducing function evaluations during training.

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Substitution for Policy Gradient Estimation
The Surr-RLDE framework decouples optimization landscape learning from policy learning by training a surrogate model $F_\theta$ on a fixed dataset first, then allowing the RL agent to interact with $F_\theta$ rather than the true objective function $f$. This allows the surrogate to act as a lightweight environment simulator, preserving learning signals while reducing computational cost.

### Mechanism 2: Relative-Order-Aware (ROA) Loss for Ranking Preservation
The paper introduces ROA loss that adds an Order Correction term penalizing the model if predicted values fall outside bounds of sorted neighbors. This forces the model to maintain landscape "shape" even if absolute values drift, prioritizing rank accuracy over absolute regression error for reward signals that rely on monotonicity.

### Mechanism 3: Kolmogorov-Arnold Networks (KAN) for Compositionality
KANs decompose multivariate functions into sums of univariate functions using learnable spline functions on edges rather than fixed activation functions. The paper suggests this structure better captures compositional nature of mathematical benchmark functions used in BBO, potentially approximating complex optimization landscapes more effectively than MLPs.

## Foundational Learning

- **Bi-level Optimization (MetaBBO):** Differentiates "low-level" task (running DE on one function) from "meta-level" task (learning policy to configure DE across many functions). Quick check: Can you explain the difference between optimizing a specific function and optimizing an algorithm that optimizes functions?

- **Reinforcement Learning (RL) as MDP:** Models dynamic configuration of DE as a Markov Decision Process. Understanding State (population features), Action (mutation config), and Reward (improvement) is required to implement the policy learning. Quick check: In this context, is the "Environment" the optimization problem or the optimization algorithm?

- **Differential Evolution (DE) Mutations:** The action space consists of discrete DE mutation strategies (rand/1, best/1, etc.) and scaling factors. Quick check: How does the choice of mutation strategy (e.g., rand/1 vs. best/1) affect the exploration vs. exploitation balance?

## Architecture Onboarding

- **Component map:** Surrogate Learner (KAN + ROA) -> Meta-Controller (Double-DQN) -> Low-Level Optimizer (DE)

- **Critical path:** Generate training data via Latin Hypercube Sampling for each training instance -> Train KAN with MSE then fine-tune with ROA loss -> Run Double-DQN loop: DE step -> Evaluate on Surrogate -> Compute State & Reward -> Update Policy

- **Design tradeoffs:** Surrogate Fidelity vs. Cost (50,000 samples used); ROA Lambda Scheduling (linear decay of weight $\lambda$); Action Space Granularity (discrete 15 options vs. continuous control)

- **Failure signatures:** Divergent Reward (surrogate collapses to constant values); Overfitting to Surrogate (policy exploits surrogate artifacts not present in true function)

- **First 3 experiments:** Train Surr-RLDE with MLP vs. KAN on 2-3 functions and compare RL agent convergence speed; Visualize 2D function landscape fitted with MSE vs. ROA to check for inversion artifacts; Train on 10D problems and test zero-shot on 30D to verify policy generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of comprehensive generalization testing to truly unseen function classes, as all test functions are variations of known benchmark problems
- Computational cost reduction claims are difficult to verify without detailed runtime comparisons
- Effectiveness of KAN versus simpler architectures like RBF networks remains partially justified

## Confidence
- **High confidence:** Core mechanism of using surrogates to reduce evaluation costs during meta-training
- **Medium confidence:** Effectiveness of ROA loss function demonstrated, but architecture choice matters significantly
- **Low confidence:** Claim that KAN's compositional structure is inherently better suited for BBO landscapes than MLPs lacks strong empirical justification

## Next Checks
1. Test Surr-RLDE on entirely new function classes (e.g., real-world engineering problems) to verify true generalization beyond benchmark variations
2. Conduct runtime measurements comparing surrogate-assisted training versus ground-truth training to quantify claimed computational savings
3. Compare Surr-RLDE against alternative surrogate architectures (e.g., Gaussian processes, ensemble methods) to isolate whether KAN provides unique advantages for this application