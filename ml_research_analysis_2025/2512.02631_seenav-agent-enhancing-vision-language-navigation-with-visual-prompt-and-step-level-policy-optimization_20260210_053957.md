---
ver: rpa2
title: 'SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and
  Step-Level Policy Optimization'
arxiv_id: '2512.02631'
source_url: https://arxiv.org/abs/2512.02631
tags:
- agent
- navigation
- action
- srgpo
- view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses perception errors in Vision-Language Navigation
  (VLN) agents caused by visual hallucinations and spatial understanding limitations.
  The proposed SeeNav-Agent introduces a dual-view Visual Prompt (VP) technique that
  reduces hallucinations and improves spatial understanding by combining first-person
  and bird's-eye views with bounding boxes, navigation lines, and action projections.
---

# SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization

## Quick Facts
- arXiv ID: 2512.02631
- Source URL: https://arxiv.org/abs/2512.02631
- Authors: Zhengcheng Wang; Zichuan Lin; Yijun Yang; Haobo Fu; Deheng Ye
- Reference count: 40
- Primary result: GPT-4.1 with VP achieves 86.7% success rate, surpassing LVLM baselines by 20 percentage points; Qwen2.5-VL-3B trained with VP and SRGPO reaches 72.3%, outperforming previous best by 5.6 points.

## Executive Summary
This paper addresses perception errors in Vision-Language Navigation (VLN) agents caused by visual hallucinations and spatial understanding limitations. The proposed SeeNav-Agent introduces a dual-view Visual Prompt (VP) technique that reduces hallucinations and improves spatial understanding by combining first-person and bird's-eye views with bounding boxes, navigation lines, and action projections. Additionally, the paper proposes Step Reward Group Policy Optimization (SRGPO), a novel reinforcement fine-tuning algorithm that efficiently utilizes step-level rewards through random grouping of navigation steps. Experimental results on the EmbodiedBench Navigation benchmark show that GPT-4.1 with VP achieves a 86.7% success rate, surpassing the previous best LVLM by 20 percentage points. Furthermore, Qwen2.5-VL-3B trained with VP and SRGPO reaches 72.3% success rate, outperforming the previous best model by 5.6 percentage points. SRGPO demonstrates superior training stability, convergence speed, and generalization compared to existing methods like GRPO and GiGPO.

## Method Summary
SeeNav-Agent enhances VLN by combining dual-view Visual Prompts with a novel reinforcement learning algorithm. The VP technique provides structured annotations including bounding boxes for object existence verification, navigation lines for distance/direction encoding, action projections for VQA-style action selection, and view alignment for consistent spatial orientation. The SRGPO algorithm defines verifiable process rewards based on distance change and target visibility, enabling efficient step-level advantage estimation through random grouping of navigation steps. This approach overcomes the limitations of GRPO (sparse rewards) and GiGPO (identical state grouping requirement) by providing dense, state-independent rewards that can be grouped arbitrarily. The method is evaluated using Qwen2.5-VL-3B-Instruct with SFT pre-training followed by SRGPO fine-tuning, achieving state-of-the-art performance on the EmbodiedBench Navigation benchmark.

## Key Results
- GPT-4.1 with full VP achieves 86.7% success rate on EmbodiedBench-Navigation, surpassing previous best LVLM by 20 percentage points
- Qwen2.5-VL-3B trained with VP and SRGPO reaches 72.3% success rate, outperforming previous best model by 5.6 percentage points
- SRGPO demonstrates faster convergence and lower variance compared to GRPO and GiGPO baselines
- Step-level rewards combined with random grouping (N_S=16) provide superior training stability and generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dual-view visual prompts with structured annotations reduce perception hallucinations and improve spatial understanding in LVLM-based navigation agents.
- **Mechanism:** The dual-view input (FV + BEV) with bounding boxes provides explicit object existence verification, navigation lines encode distance/direction, action projections convert planning to VQA-style selection, and view alignment ensures consistent spatial orientation between views. This scaffolds the LVLM's spatial reasoning without modifying model weights.
- **Core assumption:** LVLMs have stronger VQA capabilities than direct planning, and visual annotations are correctly perceived/interpreted by the model.
- **Evidence anchors:** [abstract] "dual-view Visual Prompt (VP) technique is introduced...can also improve the agent's understanding of current spatial states"; [section 3.2] Ablation shows removing any VP component drops success rate; bounding boxes, action projection, and view alignment are most critical.

### Mechanism 2
- **Claim:** SRGPO's random step grouping enables efficient step-level advantage estimation when process rewards are designed to be state-independent.
- **Mechanism:** By defining verifiable process rewards based only on distance change and target visibility (Eq. 1-2), step quality becomes comparable across different states. This removes GiGPO's constraint of grouping identical states, allowing larger group sizes (N_S=16) with no additional rollout cost.
- **Core assumption:** The process reward definition captures action quality independently of environmental state.
- **Evidence anchors:** [abstract] "perform efficient step-level advantage estimation by randomly grouping different navigation steps"; [section 3.3] Eq. 1-2 show reward depends only on dist(p_t, g) and visibility flags, not state features.

### Mechanism 3
- **Claim:** Dense process rewards combined with bi-level advantages improve convergence speed, stability, and generalization over sparse outcome-only rewards.
- **Mechanism:** Episode-level advantages (Eq. 5) provide trajectory-level credit, while step-level advantages (Eq. 7) provide dense per-action signals. Combined advantage (Eq. 8) with balance coefficient ω enables learning from both scales simultaneously.
- **Core assumption:** Step-level rewards are verifiable and align with task success.
- **Evidence anchors:** [section 4.4] Fig. 4 shows SRGPO converges faster with lower variance than GRPO/GiGPO; [section 4.6] OOD generalization shows SRGPO maintains advantage even when trained on different scenes.

## Foundational Learning

- **Concept:** Policy gradient with advantage estimation
  - **Why needed here:** SRGPO builds on GRPO's advantage-based optimization; understanding baseline subtraction and normalization is essential for debugging training instabilities.
  - **Quick check question:** Can you explain why advantages are normalized across groups rather than using raw rewards?

- **Concept:** LVLM visual grounding and hallucination
  - **Why needed here:** VP modules target specific failure modes (false object detection, spatial confusion); knowing how LVLMs process visual prompts helps diagnose when/why VP fails.
  - **Quick check question:** What types of visual annotations does an LVLM reliably interpret, and where does it still struggle?

- **Concept:** Credit assignment in long-horizon RL
  - **Why needed here:** Navigation requires 20+ steps; understanding sparse vs. dense rewards, episode-level vs. step-level credit, and the exploration-exploitation trade-off is critical for reproducing SRGPO results.
  - **Quick check question:** Why might a step-level reward that decreases distance to target still produce suboptimal trajectories?

## Architecture Onboarding

- **Component map:** Input (Dual-view images + text instruction + action history) -> VP Generator (Object detection → bounding boxes; path planner → navigation lines; action projector → arrow overlays; view aligner → BEV rotation) -> Policy Network (Qwen2.5-VL-3B-Instruct) -> SRGPO (Reward calculator → Random step sampler → Advantage estimator → Policy update) -> Environment (AI2-THOR via EmbodiedBench-Navigation)

- **Critical path:**
  1. VP annotation accuracy (bounding boxes must match ground truth objects)
  2. Verifiable process reward implementation (distance computation, visibility check)
  3. Random grouping with sufficient group size (N_S=16; smaller causes instability per Sec. 4.5)

- **Design tradeoffs:**
  - Group size N_S: Larger improves stability but requires more diverse batch; N_S=8 drops test success to 41.1%±10.3
  - VP complexity: More annotations help but increase prompt length; full VP required for 86.7% success
  - Balance coefficient ω: Controls step vs. episode credit; ω=0.5 used, not ablated in paper

- **Failure signatures:**
  - Success rate drops sharply when bounding boxes removed → object hallucination returns
  - Training curves high variance with GiGPO → grouping efficiency insufficient
  - OOD test performance collapses with GRPO → sparse rewards insufficient for generalization

- **First 3 experiments:**
  1. **VP ablation on zero-shot GPT-4.1:** Test each VP module individually on EmbodiedBench-Navigation to reproduce Table 2; verify bounding boxes and action projection are most impactful.
  2. **SRGPO vs. GRPO training curves:** Train Qwen2.5-VL-3B with identical hyperparameters (N=4, N_S=16, ω=0.5) on i.d. scenes; log success rate and variance every 10 epochs to reproduce Fig. 4.
  3. **Process reward validation:** Visualize VPR values (Eq. 1-2) on sample trajectories; verify that distance-decreasing steps receive positive rewards and that rewards are state-independent by comparing same actions across different environments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Step Reward Group Policy Optimization (SRGPO) algorithm be effectively generalized to other embodied AI tasks beyond navigation where state-independent process rewards are definable?
- Basis in paper: [explicit] The Conclusion states: "...although this work focuses on the navigation task, we may also consider extending the proposed SRGPO to other tasks where a state-independent process reward can be well-defined."
- Why unresolved: The current validation is restricted to navigation benchmarks; it remains unproven whether the random grouping mechanism and specific reward formulation (e.g., distance reduction) transfer effectively to tasks like manipulation or dialogue.
- What evidence would resolve it: Successful application of SRGPO to a distinct embodied task (e.g., robotic arm manipulation) demonstrating comparable training stability and convergence speed improvements over GRPO.

### Open Question 2
- Question: How can the generation of dual-view Visual Prompts (VP) be optimized to reduce computational overhead and latency for real-time navigation systems?
- Basis in paper: [explicit] The Conclusion identifies "...designing additional perception modules to generate VP more efficiently" as a promising direction for future research.
- Why unresolved: The current framework relies on generating complex inputs (bounding boxes, bird's-eye views, navigation lines) which typically require separate, potentially computationally expensive perception modules not detailed in the method.
- What evidence would resolve it: A study analyzing the latency of the VP generation pipeline and the proposal of a lightweight or end-to-end perception module that maintains high navigation success rates with reduced inference time.

### Open Question 3
- Question: How robust is SeeNav-Agent when evaluated on more diverse and complex navigation benchmarks, particularly those involving outdoor environments or longer horizons?
- Basis in paper: [explicit] The Conclusion notes: "In the future, we will evaluate the proposed method on more diverse and complex navigation benchmarks."
- Why unresolved: The experiments are limited to the EmbodiedBench Navigation benchmark, which consists of 60 unique in-room tasks. Performance in unstructured, outdoor, or multi-floor environments remains unknown.
- What evidence would resolve it: Benchmarking results on standard outdoor datasets (e.g., Touchdown) or long-horizon tasks, analyzing any performance degradation compared to the controlled in-room setting.

### Open Question 4
- Question: Can the Verifiable Process Reward (VPR) be decoupled from ground-truth simulator states to enable training in real-world environments without privileged information?
- Basis in paper: [inferred] Methodology Section 3.3 defines the process reward using `dist(p_t, g)` (distance between agent and target) and `g \in F_t` (target visibility). While effective in simulation (AI2-THOR), this reliance on precise ground-truth object location for reward assignment poses a challenge for sim-to-real transfer where such data is unavailable.
- Why unresolved: The paper does not address how the step-level reward would be calculated without access to the simulator's internal state (exact coordinates and object masks), limiting the current SRGPO method to simulation.
- What evidence would resolve it: An ablation study replacing ground-truth distances with estimated depths or visually estimated distances, demonstrating that the policy can still converge using imperfect, non-privileged reward signals.

## Limitations

- Critical implementation details remain unspecified, particularly training hyperparameters (learning rate, optimizer, batch size, KL penalty coefficient) and the SFT training protocol.
- The dual-view image generation pipeline details are unclear, specifically the BEV camera configuration and orthographic projection parameters.
- The assertion that SRGPO's process reward definition is truly state-independent relies on paper equations without external validation.
- Assumption that LVLMs reliably interpret all visual annotations (especially action projections) lacks empirical verification across different model architectures.

## Confidence

**High Confidence:** The core mechanism of dual-view visual prompting with structured annotations (bounding boxes, navigation lines, action projections) is well-supported by ablation studies showing significant performance drops when individual components are removed. The effectiveness of SRGPO's random grouping approach over GiGPO's state-identical grouping is demonstrated through convergence speed and variance metrics.

**Medium Confidence:** The claim that SRGPO provides superior generalization to out-of-distribution scenes is supported by experimental results, but the generalization gap between training and test environments (4 vs 60 scenes) is relatively small. The balance coefficient ω=0.5 is used without ablation, leaving uncertainty about optimal settings.

**Low Confidence:** The assertion that SRGPO's process reward definition is truly state-independent relies on the paper's reward equations without external validation. The assumption that LVLMs reliably interpret all visual annotations (especially action projections) lacks empirical verification across different model architectures.

## Next Checks

1. **VP Module Dependency Validation:** Systematically disable individual VP components (bounding boxes, action projections, view alignment) in both zero-shot GPT-4.1 and fine-tuned Qwen2.5-VL-3B to verify the claimed performance hierarchy and identify which components are truly essential versus complementary.

2. **SRGPO Reward State-Independence Test:** Implement a controlled experiment where identical actions are performed in different states (e.g., same distance reduction in different room layouts) and verify that VPR values remain consistent. Test whether distance-based rewards can be gamed through local optima.

3. **Generalization Robustness Analysis:** Train SRGPO on progressively larger subsets of in-distribution scenes (1, 2, 4 scenes) and measure out-of-distribution performance to determine the minimum training diversity needed for robust generalization, comparing against GRPO and GiGPO baselines.