---
ver: rpa2
title: 'AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive
  Programming'
arxiv_id: '2507.15378'
source_url: https://arxiv.org/abs/2507.15378
tags:
- problem
- problems
- code
- solution
- similar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AlgoSimBench, a benchmark for identifying
  algorithmically similar problems in competitive programming. The dataset consists
  of 1317 problems annotated with 231 fine-grained algorithm tags, forming 402 multiple-choice
  questions where one algorithmically similar problem is paired with three textually
  similar distractors.
---

# AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming

## Quick Facts
- arXiv ID: 2507.15378
- Source URL: https://arxiv.org/abs/2507.15378
- Reference count: 19
- Primary result: State-of-the-art LLMs achieve only 65.9% accuracy on identifying algorithmically similar competitive programming problems, with ASM improving performance by 6.7-11.7%.

## Executive Summary
This paper introduces AlgoSimBench, a benchmark for identifying algorithmically similar problems (ASPs) in competitive programming. The dataset consists of 1317 problems with 231 fine-grained algorithm tags, forming 402 multiple-choice questions where one algorithmically similar problem is paired with three textually similar distractors. Experiments show that state-of-the-art LLMs struggle with this task, with the best model (o3-mini) achieving only 65.9% accuracy. To address this challenge, the paper proposes attempted solution matching (ASM), which generates solution attempts (in natural language or code) for each problem and compares them for similarity. ASM yields absolute accuracy improvements of 6.7% to 11.7% across different models.

## Method Summary
The paper introduces Attempted Solution Matching (ASM) as a method to identify algorithmically similar problems by generating solution attempts for each problem and comparing these representations. For a given problem, an LLM generates either a natural language (ASM-NL) or Python code (ASM-PL) solution attempt. These attempts are then compared using similarity metrics like BM25 or GraphCodeBert to find algorithmically similar problems. The approach is evaluated on AlgoSimBench, a benchmark of 402 multiple-choice questions where one correct answer is algorithmically similar but textually different from the reference problem, while three distractors are textually similar but algorithmically different.

## Key Results
- State-of-the-art models achieve only 65.9% accuracy on identifying algorithmically similar problems, performing worse than random on standard retrieval
- ASM improves accuracy by 6.7-11.7% across different models, with ASM-NL outperforming ASM-PL on 5 of 7 models
- BM25 retrieval on ASM representations outperforms dense embeddings, achieving up to 52.2% accuracy when combined with keyword-prioritized BM25
- Standard retrieval methods fail due to adversarial problem selection where distractors are chosen for high textual similarity

## Why This Works (Mechanism)

### Mechanism 1: Solution Representation Alignment
Generating a solution attempt creates a representation that better exposes a problem's underlying algorithmic structure than its natural language statement. By forcing an LLM to generate a solution, it must interpret the problem, select an algorithm, and encode that choice into the logic of its output. This "solution-aware" representation filters out narrative noise and focuses on algorithmic "tricks" and data structures.

### Mechanism 2: Adversarial Denoising via Summarization
Standard retrieval fails because surface-level textual similarity is a misleading signal in this adversarially constructed benchmark. Summarization prompts a model to restate the problem in "pure mathematical and formal descriptions," stripping away the narrative elements that create the adversarial match and forcing a focus on structural elements.

### Mechanism 3: Lexical Keyword Matching as an Algorithmic Proxy
Sparse keyword-based retrieval (BM25) on solution-aware representations is a surprisingly robust and efficient proxy for algorithmic similarity, outperforming dense embeddings. When applied to ASM outputs, key terms like "dynamic programming," "bfs," or "seg_tree" become highly predictive of algorithmic similarity.

## Foundational Learning

- **Concept: Algorithmic vs. Textual Similarity** - Why needed: This is the core distinction of the paper. Learners must understand that problems can have nearly identical stories (high textual similarity) but require completely different algorithms (low algorithmic similarity), and vice-versa. Quick check: A problem about a knight's path on a chessboard and a problem about a delivery truck's route on a city grid—are they more likely to be textually similar or algorithmically similar?

- **Concept: Adversarial Dataset Construction** - Why needed: The dataset is deliberately built to make simple surface-level heuristics fail, explaining why standard models perform "worse than random." Quick check: In a four-option MCQ, if you know the three incorrect answers are chosen to be maximally textually similar to the question, how would you expect a standard keyword search to perform?

- **Concept: In-Context Learning (ICL) Exemplar Selection** - Why needed: The paper frames ASP identification as a sub-task with practical utility: selecting the right example for an LLM to improve its performance on a new problem. Quick check: For a new problem about a Minimum Spanning Tree, which is a better ICL exemplar: a problem with a similar story about city planning, or a problem with a different story whose solution also uses Kruskal's algorithm?

## Architecture Onboarding

- **Component map:** Problem Corpus -> Solution Attempt Generator (LLM) -> Representation Engine -> Similarity Scorer (BM25/GraphCodeBert) -> Ranked Candidates

- **Critical path:**
  1. Offline: For each problem in the corpus, generate and cache an ASM-NL representation
  2. Online (Query): Generate an ASM-NL representation for the new query problem
  3. Retrieve: Use BM25 to rank the cached corpus representations against the query's representation
  4. Select: Choose the top-ranked candidate as the algorithmically similar problem

- **Design tradeoffs:**
  - End-to-End vs. Retrieval: End-to-End selection is more accurate (~75%) but doesn't scale. Retrieval is less accurate (~49%) but is scalable to large corpora.
  - ASM-NL vs. ASM-PL: NL solutions are more token-efficient (1.5-2.6x cost vs. 2.6-5.3x for code) and often more effective, capturing high-level "idea" without implementation noise.
  - Sparse vs. Dense Retrieval: On solution representations, sparse methods (BM25) are cheaper and outperform dense embeddings, which may still capture narrative "noise."

- **Failure signatures:**
  - Superficial Text Matching: Relying on raw problem statements or general-purpose dense embeddings will likely fail, often performing below random chance due to adversarial construction.
  - Inconsistent Solution Attempts: The ASM approach degrades if the generating model's reasoning is wildly incorrect.
  - Over-reliance on Narrative: Failing to abstract the problem (e.g., being fooled by a "cat" story when the algorithm is about graph traversal).

- **First 3 experiments:**
  1. Reproduce Baseline: Run AlgoSimBench MCQs with raw problem statements on 1-2 models. Confirm poor accuracy (~35-55%). Re-run with oracle solutions provided to validate the premise.
  2. Implement ASM-NL: Implement the ASM-NL pipeline. Generate solution attempts for a subset of problems. Use BM25 for retrieval. Compare accuracy to the baseline.
  3. Test Summarization: Implement the summarization approach. Use BM25 on summaries to retrieve candidates. Compare this against the direct-statement baseline (which should fail) and ASM-based retrieval.

## Open Questions the Paper Calls Out

- Can models' ability to identify algorithmically similar problems be improved through targeted training or fine-tuning, or is this a fundamental limitation of current LLM architectures? The paper shows state-of-the-art models struggle (best: 65.9% accuracy) and notes "it remains underexplored whether these abilities generalize to relevant domains that are less seen during training."

- Why does problem summarization hurt performance for most models when it should theoretically remove distracting narrative elements? The paper finds that summarizing problems actually hurts the performance of all models except for GPT-4o-mini, hypothesizing that SoTA LLMs are already able to ignore superficial textual features when comparing programming problems.

- Can the effectiveness of attempted solution matching be maintained when scaling to much larger problem corpora (thousands to millions of problems)? The paper notes retrieval-based selection "scales much better to retrieving ASPs from a large corpus" but only evaluates on 402 MCQs; ASM's computational cost (1.5x–5.3x more inference tokens) may limit scalability.

## Limitations

- The adversarial construction of the benchmark means findings may not generalize to non-adversarially selected problem sets where textual and algorithmic similarity are more aligned.
- The paper does not provide direct human evaluation of the generated solution attempts, leaving open the possibility that some ASM representations are factually incorrect or incoherent.
- The cost analysis shows ASM-PL is 2.6-5.3x more expensive than ASM-NL, but the paper does not explore more efficient summarization or retrieval techniques that could achieve similar accuracy at lower computational cost.

## Confidence

- High confidence in the core finding that standard LLMs struggle with algorithmically similar problem identification (evidence: o3-mini at 65.9% accuracy on adversarially constructed benchmark).
- Medium confidence in the mechanism that solution attempts provide better representations than problem statements (evidence: oracle solutions outperform statements, but gap suggests room for improvement).
- Low confidence in the specific superiority of sparse BM25 over dense embeddings for this task (evidence: finding is surprising and counter to typical NLP practice, but not extensively validated across multiple retrieval architectures).

## Next Checks

1. Evaluate ASM performance on a non-adversarial subset of problems where textual and algorithmic similarity are correlated to test generalizability of the approach.

2. Conduct human evaluation of generated solution attempts to quantify the accuracy and coherence of ASM representations, particularly for ASM-PL which is more expensive.

3. Test the proposed ASM+BM25 pipeline on a real competitive programming dataset to measure its impact on in-context learning performance compared to standard exemplar selection methods.