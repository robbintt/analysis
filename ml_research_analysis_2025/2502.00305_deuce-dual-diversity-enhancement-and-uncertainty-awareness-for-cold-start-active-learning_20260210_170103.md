---
ver: rpa2
title: 'DEUCE: Dual-diversity Enhancement and Uncertainty-awareness for Cold-start
  Active Learning'
arxiv_id: '2502.00305'
source_url: https://arxiv.org/abs/2502.00305
tags:
- learning
- class
- euce
- uncertainty
- deuce
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEUCE tackles cold-start active learning by integrating dual-diversity
  (textual and class) and uncertainty-aware selection. It leverages a pretrained language
  model to extract embeddings, predictions, and uncertainty, then constructs a Dual-Neighbor
  Graph combining both diversity aspects.
---

# DEUCE: Dual-diversity Enhancement and Uncertainty-awareness for Cold-start Active Learning

## Quick Facts
- **arXiv ID**: 2502.00305
- **Source URL**: https://arxiv.org/abs/2502.00305
- **Reference count**: 37
- **Primary result**: DEUCE achieves up to 2.5% accuracy gains on balanced NLP datasets and up to 6.2% on imbalanced ones by integrating dual-diversity and uncertainty-aware selection

## Executive Summary
DEUCE tackles cold-start active learning by integrating dual-diversity (textual and class) and uncertainty-aware selection. It leverages a pretrained language model to extract embeddings, predictions, and uncertainty, then constructs a Dual-Neighbor Graph combining both diversity aspects. Uncertainty propagation via density-based clustering identifies hard representative instances, which are further refined using Farthest Point Sampling. Experiments across six NLP datasets show DEUCE outperforms state-of-the-art baselines, achieving superior class and textual diversity while maintaining efficiency and robustness to labeling noise.

## Method Summary
DEUCE extracts textual, predictive, and class embeddings from a frozen PLM using prompt templates, then computes uncertainty via One-vs-All self-information calibrated with empirical distribution functions. It constructs a Dual-Neighbor Graph by merging normalized/symmetrized kNN graphs in textual and label spaces, then applies HDBSCAN* clustering with uncertainty propagation. The final seed set is selected via Farthest Point Sampling from top-degree nodes, maximizing total propagated uncertainty. The method is evaluated by fine-tuning a fresh RoBERTa-base on the selected seeds and measuring accuracy, class imbalance, and textual diversity.

## Key Results
- DEUCE achieves up to 2.5% accuracy gains on balanced datasets and up to 6.2% on imbalanced ones compared to state-of-the-art baselines
- Shows superior class diversity (lower IMB values) and textual diversity (higher D scores) than competitive methods
- Demonstrates robustness to labeling noise while maintaining computational efficiency through its interpretable pipeline

## Why This Works (Mechanism)

### Mechanism 1
Merging textual and class-based kNN graphs enables selection of instances diverse in both content and predicted class, mitigating the missed cluster effect. Constructs two normalized kNN graphs (textual space with cosine distance, label space with ℓ1 distance), symmetrizes via fuzzy union, then merges with higher weights for dual-neighbor edges (E2) versus single-neighbor edges (E1). Core assumption: PLM zero-shot predictions carry meaningful signal about class distributions even without fine-tuning. Break condition: If PLM class predictions are random/meaningless, the label-space kNN graph becomes noise; ablation shows ~5-15% accuracy drop but still outperforms textual-only baselines.

### Mechanism 2
One-vs-All formulation with empirical distribution function calibration produces uncertainty estimates that identify genuinely hard examples rather than calibration artifacts. Computes inner products between predictive and class embeddings (Ω matrix), calibrates via empirical distribution function to achieve ŷ_ij ~ U(0,1), then derives uncertainty from self-information of "exactly one high-scoring class" events. Core assumption: Hard examples cluster spatially, so propagated uncertainty aggregates at genuine decision boundaries. Break condition: PLM overconfidence without e.d.f. calibration would compress uncertainty range; the normalization step explicitly addresses this.

### Mechanism 3
Density-based clustering (HDBSCAN*) with uncertainty propagation followed by Farthest Point Sampling identifies "hard representative" instances that are both informative and cover the data manifold. HDBSCAN* identifies representatively uncertain (RU) groups with minimum density k_r; uncertainty propagates within clusters via weighted message passing; FPS from top-k degree nodes generates diverse candidates, selecting the set with maximum total propagated uncertainty. Core assumption: Uncertainty clusters indicate genuine model knowledge gaps rather than outliers. Break condition: If k_r is too small, outliers dominate; if too large, genuine hard regions are excluded as noise.

## Foundational Learning

- **k-Nearest Neighbor Graphs and Manifold Learning**: Why needed here: DNG construction requires understanding how kNN graphs capture local structure and how normalization enables cross-space comparison. Quick check: Why does the symmetrization use fuzzy union (W + W⊤ - W ⊙ W⊤) rather than simple averaging?

- **Density-Based Clustering (HDBSCAN*)**: Why needed here: Unlike k-MEANS, HDBSCAN* handles unknown cluster counts and explicitly identifies outliers—critical when RU group count is unknown. Quick check: What happens to acquisition if HDBSCAN* classifies all instances as outliers?

- **Active Learning Exploration vs Exploitation**: Why needed here: DEUCE balances diversity (exploration of input/label space) with uncertainty (exploitation of decision boundaries). Quick check: Why does pure uncertainty sampling fail in cold-start (see Entropy baseline, Table 3, DBpedia: 47.5% vs Random: 95.0%)?

## Architecture Onboarding

- **Component map**: Embedding Module → Textual (˜z_xi), Predictive (˜z_ŷ|xi), Class (˜z_yj) embeddings via prompt-based PLM → Prediction Module → Label vectors (ŷi via e.d.f.), Uncertainty (ui via OVA self-information) → DNG Module → Two kNN graphs → Normalization → Symmetrization → Merge with γ=1.0 → Acquisition Module → HDBSCAN*(k_r=3) → Propagation → FPS from top-k degree nodes → Max-uncertainty selection

- **Critical path**: Embedding quality → Class prediction accuracy → DNG edge weights → Cluster quality → Final selection. Template denoising is essential (Table 6: correlation improves from 0.17→0.20 on IMDb).

- **Design tradeoffs**: k=500 neighbors captures global structure but increases density; k_r=3 minimum cluster size includes more marginal clusters; γ=1.0 dual-edge threshold requires stronger text-label agreement; smaller budgets show larger relative gains (Table 3)

- **Failure signatures**: IMB → ∞ indicates missed cluster effect (Entropy baseline, Table 4); Textual diversity D < 0.4 suggests redundant selection; Fine-tuning std >3% suggests unstable acquisition

- **First 3 experiments**: 1) Reproduce IMDb b=32: Expect ~86.9% accuracy, IMB <1.3, D >0.65 (Table 3, 4, 5); 2) Ablate class predictions: Replace ˜z_ŷ|xi with random vectors; confirm ~3-5% drop but still beats Coreset (Table 11 pattern); 3) Hyperparameter sweep k∈{100,300,500}: Run on TREC (imbalanced), plot IMB vs accuracy tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1**: How does DEUCE perform when utilizing generative large language models (LLMs) as the backbone instead of the discriminative PLM (RoBERTa) used in the current study? Basis: The Limitations section suggests generative models can be investigated and combined with DEUCE. Why unresolved: The current framework relies on a masked language model for specific embedding and uncertainty extraction; the compatibility and efficiency of this pipeline with decoder-only or encoder-decoder generative architectures remain untested. What evidence would resolve it: Empirical results comparing seed set quality and computational efficiency with generative backbones versus RoBERTa.

- **Open Question 2**: Can the integration of external domain knowledge, such as knowledge graphs, improve DEUCE's acquisition capability in specialized cold-start scenarios? Basis: The Limitations section suggests incorporating domain knowledge, prompt engineering, and knowledge graphs can be investigated. Why unresolved: The framework currently relies solely on the frozen PLM's internal representations, which may lack nuance for technical or low-resource domains. What evidence would resolve it: Ablation studies on specialized datasets (e.g., medical or legal) where DEUCE's graph construction is augmented with external knowledge graph embeddings or structured domain prompts.

- **Open Question 3**: How can the Dual-Neighbor Graph (DNG) construction be adapted for tasks that lack discrete, nameable class labels, such as regression or structured generation? Basis: The methodology relies on generating class embeddings from text templates, and the math reasoning experiment required clustering predictive embeddings to define "meta-classes." Why unresolved: The core mechanism for "class diversity" is strictly defined by distance between explicit label vectors, making the current formulation inapplicable to continuous label spaces without heuristic adjustments. What evidence would resolve it: A theoretical extension of the distance metric for continuous spaces and validation on regression datasets without relying on clustering workarounds.

## Limitations

- **Unknown implementation details**: Template denoising and fine-tuning hyperparameters are referenced but not specified, blocking faithful reproduction
- **PL prediction assumption**: The method assumes PLM zero-shot predictions reliably capture class distributions, untested in extreme imbalance scenarios
- **Task scope**: Current formulation strictly requires discrete class labels, limiting applicability to regression or structured generation without heuristic modifications

## Confidence

- **High**: The core dual-diversity mechanism (textual + class kNN merging) is well-specified and theoretically grounded
- **Medium**: Uncertainty calibration via empirical distribution function works as claimed, though direct ablation studies are limited
- **Medium**: Density-based clustering with uncertainty propagation produces meaningful results, but cluster quality metrics are not reported

## Next Checks

1. **Ablation Study**: Remove class prediction component (use random vectors) and measure performance degradation to validate Mechanism 1
2. **Cluster Quality Analysis**: Report HDBSCAN* clustering statistics (noise ratio, cluster distribution) for each dataset to verify Mechanism 3 assumptions
3. **Hyperparameter Sensitivity**: Systematically vary k (neighbors), k_r (min cluster size), and γ (edge threshold) on one imbalanced dataset to quantify tradeoffs between diversity and uncertainty capture