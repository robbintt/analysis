---
ver: rpa2
title: Model Fusion via Neuron Transplantation
arxiv_id: '2502.06849'
source_url: https://arxiv.org/abs/2502.06849
tags:
- ensemble
- fusion
- pruning
- fine-tuning
- averaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neuron Transplantation (NT), a novel ensemble
  compression technique that fuses multiple trained neural networks into a single
  model by transplanting important neurons from all ensemble members into the space
  obtained by pruning insignificant neurons. The method addresses the memory and inference
  time challenges of deploying deep ensembles by creating a fused model that can recover
  ensemble performance through fine-tuning.
---

# Model Fusion via Neuron Transplantation

## Quick Facts
- arXiv ID: 2502.06849
- Source URL: https://arxiv.org/abs/2502.06849
- Reference count: 40
- One-line primary result: Neuron Transplantation fuses trained ensemble members into a single model that matches or exceeds ensemble performance through selective neuron retention and fine-tuning

## Executive Summary
This paper introduces Neuron Transplantation (NT), a novel ensemble compression technique that fuses multiple trained neural networks into a single model by transplanting important neurons from all ensemble members into the space obtained by pruning insignificant neurons. The method addresses the memory and inference time challenges of deploying deep ensembles by creating a fused model that can recover ensemble performance through fine-tuning. Experiments on multiple datasets (MNIST, CIFAR10/100, SVHN) and architectures (MLP, LeNet, VGG11, ResNet18) show that NT consistently outperforms individual ensemble members and matches or exceeds the performance of Optimal-Transport fusion while requiring less memory and faster fusion time.

## Method Summary
Neuron Transplantation works by first training k independent models to convergence, then concatenating their non-output layers vertically while averaging their output layers. Structured L2-norm magnitude pruning removes the lowest-magnitude neurons from each layer, retaining approximately 1/k of the original neurons per layer (targeting sparsity of 1−1/k). All cross-weights connecting neurons from different original models are initialized to zero and learned during fine-tuning. The pruned, concatenated architecture is then fine-tuned on the full dataset, recovering and typically exceeding the performance of individual ensemble members. The method supports different fusion strategies (merge-then-prune vs prune-then-merge) and iterative fusion for k>2 models.

## Key Results
- NT consistently outperforms individual ensemble members across all tested datasets and architectures
- For MLP models on SVHN, NT achieves 85.06% accuracy vs 83.46% for the best individual model
- NT matches or exceeds Optimal-Transport fusion performance while requiring less memory and faster fusion time
- The method demonstrates robustness across different layer widths, depths, and number of models, though performance gains diminish with increasing ensemble size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selectively retaining high-magnitude neurons preserves the most functionally important computations from each ensemble member.
- **Mechanism:** L2-norm magnitude pruning ranks neurons by their weight magnitudes; neurons with larger norms are hypothesized to contribute more to output activations. By keeping only the top 1/k neurons per layer when fusing k models, the method preserves the "essential" computational pathways while discarding redundant or low-contribution units.
- **Core assumption:** Neuron importance correlates with L2-norm magnitude, and pruned neurons can be functionally compensated for during fine-tuning.
- **Evidence anchors:**
  - [abstract]** "pruning insignificant neurons and transplanting important ones from ensemble members"
  - [section 3]** "In a layer-wise fashion, the neurons with the smallest L2-norm are removed... for k models, we use a sparsity of 1 − 1/k"
  - [corpus]** "Locate-then-Merge" identifies important neurons for fusion, suggesting neuron-level selection is a shared strategy (FMR=0.555)
- **Break condition:** If neurons with small magnitudes encode critical rare-feature detection (e.g., tail classes, adversarial robustness), NT will systematically discard them and fail to recover performance.

### Mechanism 2
- **Claim:** Concatenating layers with zero-initialized cross-weights creates a superposition of ensemble members that can be refined into a joint representation.
- **Mechanism:** Non-output layers are vertically concatenated (e.g., R^{km×n} for input layers), output layers are averaged, and all weights connecting neurons from different original models ("cross-weights") are initialized to zero. During fine-tuning, gradient updates learn beneficial interactions between transplanted neurons.
- **Core assumption:** Zero-initialized cross-weights can be learned sufficiently during fine-tuning, and the transplanted neurons provide a good initialization point in the loss landscape.
- **Evidence anchors:**
  - [section 3]** "All weights between the layers, which we call cross weights, are initialized to zero, and will later be learned by fine-tuning"
  - [section 5]** "We theorize that only the large weights of the models are needed to set the fused model into a 'good' loss neighborhood"
  - [corpus]** "Model Fusion via Neuron Interpolation" (FMR=0.563) addresses internal representation differences during fusion, supporting the premise that neuron-level operations matter
- **Break condition:** If the fine-tuning budget is insufficient or the loss landscape has poor connectivity between transplanted neuron configurations, cross-weights will remain near-zero and the fused model will underperform.

### Mechanism 3
- **Claim:** Joint pruning across all ensemble members enables globally optimal neuron selection compared to independent pruning.
- **Mechanism:** When models are first concatenated then pruned (merge-prune-fine-tune), the L2-norm ranking considers all neurons from all models simultaneously. This allows a high-magnitude neuron from model A to be retained over a lower-magnitude neuron from model B, even if model B contributes more neurons overall.
- **Core assumption:** Global neuron selection produces a better initialization than local (per-model) selection.
- **Evidence anchors:**
  - [section 4.2, Table 1]** Merge-Prune-Fine-tune slightly outperforms Prune-Merge-Fine-tune at early epochs (54.14% vs 53.87% at epoch 4)
  - [section 4.2]** "We conclude that jointly selecting the neurons with the largest L2-norm across all models is better than doing so locally, though the improvement is marginal"
  - [corpus]** Weak corpus evidence—no directly comparable joint-vs-local pruning analysis found
- **Break condition:** If ensemble members have vastly different accuracy levels or learned features, joint pruning may over-select from the strongest model and under-represent diversity.

## Foundational Learning

- **Concept: Structured Pruning (Magnitude-based)**
  - **Why needed here:** NT relies on L2-norm pruning to identify which neurons to keep. Understanding that pruning removes entire neurons (not individual weights) and that magnitude heuristics approximate functional importance is essential.
  - **Quick check question:** If you prune 50% of neurons from a 2-model ensemble using L2-norm, what sparsity level does each layer target?

- **Concept: Loss Barriers in Weight Averaging**
  - **Why needed here:** The paper positions NT as avoiding loss barriers that plague vanilla weight averaging. Understanding that models in different loss basins have high-loss interpolations motivates the "select rather than average" approach.
  - **Quick check question:** Why might averaging two well-trained models produce worse performance than either individual model?

- **Concept: Fine-tuning as Loss Landscape Recovery**
  - **Why needed here:** NT intentionally causes an initial performance drop post-transplantation; recovery depends on fine-tuning re-learning lost functionality and optimizing cross-weights.
  - **Quick check question:** After transplantation, why does the fused model initially underperform, and what specifically must fine-tuning learn?

## Architecture Onboarding

- **Component map:**
  - Independent model training -> Concatenation module -> Pruning module -> Cross-weight initialization -> Fine-tuning loop -> Fused model output

- **Critical path:**
  1. Train k models independently to convergence
  2. Concatenate layers (non-output: stack; output: average)
  3. Compute L2-norm per neuron across all concatenated layers
  4. Prune to original architecture size (retain top 1/k neurons per layer globally)
  5. Fine-tune fused model (20-30 epochs typical in experiments)

- **Design tradeoffs:**
  - **Merge-then-prune vs. Prune-then-merge:** Merge-first enables global neuron selection but requires holding all models in memory; prune-first is memory-efficient but loses global optimization
  - **Iterative vs. recursive vs. simultaneous fusion for k>2:** Iterative is memory-light but asymmetrically weights later models; recursive better preserves ensemble diversity
  - **Fine-tuning budget:** More epochs improve recovery but increase total cost; paper shows 3-4 epochs often sufficient to match individual model performance

- **Failure signatures:**
  - **Duplicate/similar model fusion:** Fusing a model with itself causes information loss (smaller neurons discarded, larger ones duplicated); performance drops to ~67% immediately, recovers only to original single-model level (Section 4.2)
  - **Too many models (k>8):** Diminishing returns; fused model saturates while ensemble continues improving (Figure 4, Table 2)
  - **Insufficient model diversity:** Redundant transplanted neurons provide no new information; no compensation for pruned neurons
  - **Large-width models with OT comparison:** NT post-fusion accuracy can be lower than OT for some architectures (Table 5), requiring more fine-tuning

- **First 3 experiments:**
  1. **Baseline validation:** Fuse 2 MLP models on SVHN; verify post-transplant accuracy drops to ~78%, then recovers to ~84.4% after 20 epochs fine-tuning (Table 2); confirm fused model beats best individual model
  2. **Ablation on operation order:** Compare Prune-Merge-Fine-tune vs. Merge-Prune-Fine-tune on same setup; expect Merge-Prune to show marginal early-epoch advantage
  3. **Failure case confirmation:** Attempt to fuse a model with a copy of itself; verify immediate performance drop (~67%) and recovery only to original level, not above—confirming diversity requirement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Neuron Transplantation be effectively applied to Transformer-based architectures?
- **Basis in paper:** [explicit] The authors state, "Its application to transformers... we leave for future work."
- **Why unresolved:** The experiments were restricted to MLPs and CNNs (LeNet, VGG11, ResNet18); the structural differences in attention mechanisms may react differently to pruning and concatenation.
- **What evidence would resolve it:** Successful application of NT to standard Transformer models (e.g., ViT or BERT) showing performance retention comparable to the CNN results in the paper.

### Open Question 2
- **Question:** Can a heuristic determine if ensemble members are sufficiently diverse to prevent information loss during transplantation?
- **Basis in paper:** [explicit] The authors suggest, "It might also be possible to heuristically decide whether the neurons are diverse enough... Such an approach could solve the information loss issue."
- **Why unresolved:** The paper identifies that fusing similar models causes information loss (redundancy) but does not propose a method to pre-screen or quantify the required diversity.
- **What evidence would resolve it:** A diversity metric that correlates with fusion success, or a pre-fusion screening method that avoids the "failure case" of fusing similar models.

### Open Question 3
- **Question:** Does joint pruning and training in NT overcompensate for the "cross-weights" introduced during fusion in ensemble pruning scenarios?
- **Basis in paper:** [explicit] The authors ask, "Whether the joint pruning and joint training of NT can overcompensate for the newly introduced cross-weights remains to be seen."
- **Why unresolved:** While NT is proposed for fusion, its utility as a pure ensemble-pruning technique is unclear because the merging step introduces cross-weights that may be unnecessary for pruning.
- **What evidence would resolve it:** A comparative study isolating the "merge-prune" pipeline against individual pruning without merging to validate if the joint training justifies the cross-weight complexity.

## Limitations

- The method's effectiveness depends heavily on model diversity; fusing similar models causes information loss and fails to outperform individual models
- NT may require more fine-tuning epochs than Optimal-Transport fusion to match performance on certain architectures (e.g., ResNet18)
- The fundamental limitation analysis is descriptive rather than explanatory, not exploring why fusing similar models causes performance degradation at the representational level

## Confidence

- **High confidence**: The basic mechanism of L2-norm pruning followed by fine-tuning produces measurable accuracy improvements over individual models and vanilla averaging
- **Medium confidence**: The claim that NT outperforms OT fusion while requiring less memory is partially supported but context-dependent
- **Low confidence**: The generality of NT to non-image domains, very deep architectures, or models with complex skip connections/bottlenecks remains untested

## Next Checks

1. **Architecture stress test**: Apply NT to Transformer-based models (BERT, ViT) to verify the pruning-then-fine-tune pipeline works when magnitude pruning doesn't align with functional importance in attention mechanisms.

2. **Failure case analysis**: Systematically vary model initialization seeds to create increasingly similar models (measuring pairwise similarity via neuron correlation), then document the exact accuracy degradation curve as similarity increases.

3. **Cross-domain validation**: Test NT on non-image tasks like text classification (IMDB) or tabular data (Adult Income) to assess whether the L2-norm pruning heuristic generalizes beyond convolutional feature extractors.