---
ver: rpa2
title: Where do Large Vision-Language Models Look at when Answering Questions?
arxiv_id: '2503.13891'
source_url: https://arxiv.org/abs/2503.13891
tags:
- image
- llav
- visual
- lvlms
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to visualize where large vision-language
  models (LVLMs) attend when answering visual questions. The core idea is to identify
  visually relevant tokens in the model's variable-length responses using a log-likelihood
  ratio between predictions with and without visual input, then extend existing heatmap
  visualization methods to interpret these open-ended outputs.
---

# Where do Large Vision-Language Models Look at when Answering Questions?

## Quick Facts
- **arXiv ID:** 2503.13891
- **Source URL:** https://arxiv.org/abs/2503.13891
- **Reference count:** 40
- **Primary result:** Novel method for visualizing LVLM attention on open-ended VQA using LLR-based token selection and single-mask optimization

## Executive Summary
This paper addresses the challenge of interpreting where Large Vision-Language Models (LVLMs) attend when generating open-ended visual question answers. The core contribution is a method that identifies visually relevant tokens in variable-length responses using log-likelihood ratios between predictions with and without visual input, then extends perturbation-based heatmap optimization to these open-ended outputs. The approach handles LVLM-specific complexities like multi-resolution and multi-encoder architectures through adapted optimization processes. Experiments on multiple LVLM architectures across visual reasoning benchmarks demonstrate that the proposed method generates more meaningful heatmaps compared to baselines, with improved deletion and insertion scores.

## Method Summary
The method consists of two main components: visually relevant token selection and heatmap optimization. First, it computes the log-likelihood ratio (LLR) for each token in the model's answer by comparing predictions with the original image versus a blurred baseline image. Tokens with LLR above a threshold (α) are selected as visually relevant. Second, it optimizes a single perturbation mask to maximize insertion (adding pixels from baseline to original) and minimize deletion (removing pixels from original) scores, using the cumulative log-likelihood of selected tokens as the objective. The optimization incorporates graduated non-convexity (GNC) for stability and handles multi-resolution architectures through differentiable cropping operations.

## Key Results
- Proposed method achieves better deletion (lower) and insertion (higher) scores than baseline methods on multiple LVLM architectures
- Vision architecture has greater impact on visual focus than LLM scale when other factors are controlled
- Answer correctness doesn't always correlate with focus region plausibility - models may attend to correct regions but fail in reasoning
- Method successfully handles open-ended responses, unlike previous approaches limited to multiple-choice formats

## Why This Works (Mechanism)

### Mechanism 1: Log-Likelihood Ratio for Token Relevance
The method quantifies the difference in token probability with and without visual input using log-likelihood ratio (LLR). High LLR tokens are selected as visually relevant, with cumulative log-likelihood serving as the optimization objective. This assumes tokens with high LLR are most visually grounded in the answer.

### Mechanism 2: Perturbation-Based Optimization via Integrated Gradients
The method builds on iGOS++ by optimizing a mask that minimizes prediction score when high-value pixels are removed (deletion) and maximizes it when added to blank image (insertion). This creates heatmaps where high values indicate pixels critical for generating the answer.

### Mechanism 3: Single-Mask Optimization with Graduated Non-Convexity (GNC)
Instead of separate masks for deletion and insertion, the method optimizes a single mask for both objectives, stabilized by GNC. Starting with convex L2 regularizer and gradually decaying it, this prevents getting stuck in poor local optima early in optimization.

## Foundational Learning

- **Concept: Large Vision-Language Models (LVLMs)**
  - **Why needed here:** The entire method is designed to interpret these specific models that combine vision encoders with LLMs
  - **Quick check question:** How does an LVLM combine visual and textual information to generate an answer?

- **Concept: Autoregressive Generation**
  - **Why needed here:** The model's output is free-form text generated token by token, requiring LLR analysis of token probabilities
  - **Quick check question:** In autoregressive text generation, what does the probability of the next token depend on?

- **Concept: Perturbation-Based Interpretation**
  - **Why needed here:** The heatmap method fundamentally interprets the model by perturbing input and observing output changes
  - **Quick check question:** In a perturbation-based method, what does a high value on a heatmap signify about the corresponding image region?

## Architecture Onboarding

- **Component map:**
  - Input (Image, Question) -> LVLM -> Answer (Tokens)
  - Answer + Image + Baseline Image -> LLR Calculator -> Token Selector -> Scorer -> Heatmap Optimizer -> Heatmap

- **Critical path:** The path for generating a heatmap is: **Input (Image, Question) -> LVLM -> Answer (Tokens)** -> **Answer + Image + Baseline Image -> LLR Calculator -> Token Selector -> Scorer -> Heatmap Optimizer -> Heatmap**

- **Design tradeoffs:**
  - Efficiency vs. Detail: Single-mask optimization is faster but potentially less expressive than dual-mask systems
  - Token Threshold (α): High threshold focuses on critical tokens but may miss context; low threshold includes more tokens but may dilute signal
  - Baseline Image: Choice of baseline (blurred vs. black vs. noise) affects heatmaps; paper finds blurred image optimal

- **Failure signatures:**
  - Model Hallucination: If model answers correctly without looking at right region, heatmap may show diffuse/ irrelevant activations
  - Convergence Issues: Non-convex optimization may result in noisy or blank heatmaps if stuck in poor local optima
  - Non-Differentiable Operations: Multi-resolution architectures require custom differentiable cropping; failure breaks gradient flow

- **First 3 experiments:**
  1. **Reproduce Token Selection:** Implement LLR-based token selection on pre-trained LLaVA, verify high-LLR tokens are visually grounded (e.g., "Leonardo" in "Leonardo da Vinci")
  2. **Compare Baselines:** Implement heatmap optimization with different baseline images (blurred, black, noise), qualitatively assess which produces most coherent heatmaps
  3. **Architectural Ablation:** Run visualization on models with different vision encoders but same LLM, observe if heatmaps change, testing vision architecture impact

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the gap between successful visual grounding and correct reasoning be bridged in LVLMs?
**Basis in paper:** [explicit] Paper analyzes relationship between focus and correctness, noting that "focus region plausibility does not always correlate with answer correctness" - models may attend to right region but fail to interpret correctly.
**Why unresolved:** Paper quantifies this disconnect but doesn't propose mechanism to ensure correct visual attention translates into correct logical deduction.
**What evidence would resolve it:** Training intervention or architecture modification that improves reasoning accuracy specifically on data where model already attends to correct visual region.

### Open Question 2
**Question:** How can heatmap visualization methods be adapted to interpret model behaviors when LVLM relies primarily on language priors rather than visual input?
**Basis in paper:** [explicit] In Limitations section, authors state optimization may not be effective when output scores don't have significant difference between original and blurred images.
**Why unresolved:** Current method relies on contrast between "image present" and "image absent" scores; if delta is small (indicating low visual reliance), optimization objective becomes unstable.
**What evidence would resolve it:** Extension that identifies specific linguistic patterns or knowledge graphs accessed when visual evidence is ignored.

### Open Question 3
**Question:** What specific architectural or training adjustments are required to better align LVLM visual attention maps with human attention during VQA?
**Basis in paper:** [inferred] Appendix compares model focus to human attention (VQA-HAT) and finds negative rank correlation (-0.2) and low IoU (0.01).
**Why unresolved:** Paper measures divergence but doesn't investigate whether misalignment contributes to errors or hallucinations, nor explore methods to close gap.
**What evidence would resolve it:** Experiments comparing standard LVLMs against models fine-tuned with human attention supervision to see if alignment improves robustness or accuracy.

## Limitations
- Method relies on LLR as proxy for visual relevance without extensive validation that high-LLR tokens are always most visually grounded
- Optimization hyperparameters (GNC decay rate, regularization weights) are not fully specified in main text
- Improvement over separate-mask iGOS++ baseline is relatively modest in ablation study
- Method doesn't address how to handle model hallucination cases where correct answer is generated without image reliance

## Confidence
- **High Confidence:** Core mechanism of using LLR to identify visually relevant tokens is well-defined and technically sound; ablation study demonstrating single-mask optimization effectiveness is convincing
- **Medium Confidence:** Claims about vision architecture having more impact than LLM scale are supported by experiments but based on limited model comparisons
- **Medium Confidence:** Results showing proposed method generates more meaningful heatmaps are plausible but rely on deletion/insertion scores with known limitations
- **Low Confidence:** Paper lacks strong evidence that method can reliably distinguish between visually grounded versus hallucinated correct answers

## Next Checks
1. **LLR Token Relevance Validation:** For manually annotated examples, verify tokens with highest LLR scores correspond to most visually relevant words in answer (object names, colors, quantities clearly depicted)

2. **Hyperparameter Sensitivity Analysis:** Systematically vary token selection threshold α and GNC decay rate γ, measure how deletion/insertion scores and qualitative heatmap quality change to identify robust operating points

3. **Hallucination Detection Test:** Create test cases where model likely answers correctly without looking at image (common knowledge questions independent of visual content), use method to generate heatmaps and assess whether they correctly show low/diffuse visual attention