---
ver: rpa2
title: Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant
  Information Bottleneck Learning
arxiv_id: '2512.10573'
source_url: https://arxiv.org/abs/2512.10573
tags:
- uni00000013
- noise
- label
- uni00000003
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LaT-IB, a novel method addressing the vulnerability
  of standard Information Bottleneck (IB) to label noise by disentangling representations
  into clean and noisy label spaces. LaT-IB formulates a "Minimal-Sufficient-Clean"
  criterion using mutual information regularizers and implements a three-phase training
  framework: Warmup, Knowledge Injection, and Robust Training.'
---

# Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning

## Quick Facts
- arXiv ID: 2512.10573
- Source URL: https://arxiv.org/abs/2512.10573
- Reference count: 40
- Key outcome: LaT-IB achieves 94.17% accuracy on CIFAR-10N and 74.97% on Pubmed with 40% uniform noise, significantly outperforming existing IB-based methods

## Executive Summary
This paper addresses the fundamental vulnerability of standard Information Bottleneck (IB) methods to label noise by introducing LaT-IB, a novel method that disentangles representations into clean and noisy label spaces. The method formulates a "Minimal-Sufficient-Clean" criterion using mutual information regularizers and implements a three-phase training framework (Warmup, Knowledge Injection, and Robust Training). Extensive experiments demonstrate LaT-IB's superior robustness and efficiency under label noise across multiple image and graph datasets.

## Method Summary
LaT-IB introduces a dual-encoder architecture that disentangles clean and noisy label information through a three-phase curriculum. The method uses mutual information regularization to enforce disentanglement between two encoders (S for clean, T for noisy) while maintaining prediction sufficiency. The training progresses from Warmup (pre-training S on noisy data), to Knowledge Injection (partitioning data and enforcing disentanglement via divergence regularization), to Robust Training (final fine-tuning with full objective including discriminator-based MI minimization).

## Key Results
- Achieves 94.17% accuracy on CIFAR-10N with 40% uniform noise
- Achieves 74.97% accuracy on Pubmed with 40% uniform noise
- Significantly outperforms existing IB-based methods and two-stage denoising approaches across multiple datasets

## Why This Works (Mechanism)
LaT-IB addresses IB's vulnerability to label noise by disentangling representations into clean and noisy subspaces through mutual information regularization. The three-phase training curriculum progressively guides the model to learn noise-resistant representations by first establishing a clean encoder baseline, then forcing specialization through data partitioning and divergence constraints, and finally optimizing the full robust objective. This progressive approach stabilizes training and prevents the collapse that standard IB suffers under label noise.

## Foundational Learning

- **Concept: Information Bottleneck (IB) Principle**
  - Why needed here: The paper builds directly upon the IB framework (`min -I(Y;Z) + βI(X;Z)`). Understanding the trade-off between compression and prediction is essential.
  - Quick check question: Explain why a standard IB model might fail when labels Y contain noise. (Hint: Consider what `max I(Y;Z)` optimizes for).

- **Concept: Mutual Information (MI)**
  - Why needed here: The method's entire objective is formulated in terms of MI, including disentanglement (`I(S;T|Y)`) and sufficiency (`I(Y;S,T)`).
  - Quick check question: Why is `I(Y;Z)` often lower-bounded by negative cross-entropy? What does this substitution assume about the variational distribution `q(y|z)`?

- **Concept: Variational Inference**
  - Why needed here: The implementation uses variational approximations to make the intractable MI terms optimizable.
  - Quick check question: How does the density-ratio trick allow a discriminator to estimate mutual information? What are the discriminator's positive and negative examples?

## Architecture Onboarding

- **Component map:** Input -> Dual Encoder (S and T) -> Shared Decoder -> Predictions. InfoJS Selector partitions data. MI Discriminator provides disentanglement gradients.

- **Critical path:** The critical implementation path is the three-phase training loop: (1) Warmup trains only Encoder S on the full noisy dataset, (2) Knowledge Injection uses InfoJS selector and specialized losses, (3) Robust Training optimizes the full objective including discriminator-based disentanglement.

- **Design tradeoffs:**
  - Performance vs. Complexity: Dual-encoder and discriminator architecture adds significant complexity and computational overhead
  - Stability vs. Robustness: Three-phase curriculum is designed for stability but requires careful tuning of hyperparameters

- **Failure signatures:**
  - Mode Collapse / Lazy Training: Model may produce identical outputs from both encoders
  - Poor Disentanglement: If divergence-based loss is under-weighted, encoders fail to specialize
  - Training Collapse: Standard IB models can collapse under high label noise

- **First 3 experiments:**
  1. Baseline Reproduction: Train standard VIB model on noisy dataset to observe performance degradation
  2. Component Ablation: Train full LaT-IB, then remove one phase at a time to verify contribution
  3. Hyperparameter Sensitivity: Run sweep on β and γ on smaller dataset to find stable operating region

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the InfoJS selector's performance degrade in "low-regime" scenarios where label noise rates exceed 50%?
- Basis in paper: [inferred] Experiments limited to noise rates up to 50%, while InfoJS selector relies on selecting top/bottom δ% of samples
- Why unresolved: Relative ranking method assumes clean samples have distinctively lower loss/divergence than noisy ones
- What evidence would resolve it: Empirical evaluation on datasets with 60%, 80%, and 90% noise rates

### Open Question 2
- Question: Can the dual-encoder disentanglement strategy be adapted to sequential data domains like NLP?
- Basis in paper: [inferred] All experiments are on CV and graph learning, utilizing continuous Gaussian embeddings
- Why unresolved: Method relies on continuous latent variables and KL divergence constraints
- What evidence would resolve it: Application to text classification benchmarks with injected label noise

### Open Question 3
- Question: Is the three-phase training framework strictly necessary, or can the objective be optimized in a single-stage manner?
- Basis in paper: [inferred] Paper introduces "progressive" training pipeline suggesting simultaneous optimization is unstable
- Why unresolved: Phased approach introduces hyperparameters and complexity
- What evidence would resolve it: Ablation study attempting joint optimization from epoch 0

## Limitations
- Three-phase training curriculum introduces significant complexity with many hyperparameters
- Dual-encoder architecture substantially increases computational overhead compared to standard IB methods
- Practical implementation details for stable training across diverse datasets are not fully specified

## Confidence
- **High:** Theoretical framework for disentanglement and Minimal-Sufficient-Clean criterion is well-founded
- **Medium:** Experimental results show clear improvements over baseline IB methods
- **Low:** Practical implementation details for stable training are not fully specified

## Next Checks
1. Implement controlled ablation study comparing LaT-IB performance against standard IB with identical hyperparameters
2. Test InfoJS selector's stability by visualizing Clean/Noise/Uncertain partitions on synthetic datasets
3. Evaluate computational overhead by measuring training time and memory usage across multiple hardware configurations