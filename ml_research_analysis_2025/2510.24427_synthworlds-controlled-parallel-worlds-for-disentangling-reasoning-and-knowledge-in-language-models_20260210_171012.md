---
ver: rpa2
title: 'SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge
  in Language Models'
arxiv_id: '2510.24427'
source_url: https://arxiv.org/abs/2510.24427
tags:
- page
- knowledge
- reasoning
- question
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SynthWorlds, a framework for evaluating
  language models'' reasoning ability by disentangling it from reliance on memorized
  factual knowledge. SynthWorlds constructs parallel corpora representing two worlds:
  one with real-world entities where models can exploit parametric knowledge, and
  another with synthetic entities where such knowledge is uninformative.'
---

# SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models

## Quick Facts
- **arXiv ID**: 2510.24427
- **Source URL**: https://arxiv.org/abs/2510.24427
- **Reference count**: 40
- **Primary result**: Parallel corpora reveal persistent performance gaps between real and synthetic entity settings, demonstrating that standard LM benchmarks conflate reasoning with parametric knowledge.

## Executive Summary
This paper introduces SynthWorlds, a framework for evaluating language models' reasoning ability by disentangling it from reliance on memorized factual knowledge. SynthWorlds constructs parallel corpora representing two worlds: one with real-world entities where models can exploit parametric knowledge, and another with synthetic entities where such knowledge is uninformative. On top of these corpora, the authors design multi-hop question answering and page navigation tasks that maintain equal reasoning difficulty across both worlds.

Experiments with GPT-5-mini and Gemini-2.0-Flash reveal persistent performance gaps between real-mapped and synthetic-mapped settings. In closed-book QA, GPT-5-mini achieves F1 scores of 21.6 vs 0.2, creating a knowledge advantage gap of 21.4. For page navigation, GPT-5-mini shows success rates of 50.8% vs 19.8%, yielding a gap of 31.0. While knowledge augmentation methods like retrieval-augmented generation and chain-of-thought reasoning improve performance in both settings, they do not eliminate the gap. The framework provides a scalable, automated environment for evaluating language models' reasoning capabilities in ways that isolate the contribution of parametric knowledge from genuine reasoning ability.

## Method Summary
SynthWorlds creates parallel corpora where entities are identical in relational structure but differ in name familiarity - one corpus uses real entities (SYNTHWORLD-RM) and the other uses synthetic entities (SYNTHWORLD-SM). The method generates 6,290 documents per corpus from Wikidata facts, ensuring no semantic overlap between real and synthetic entity names. Two tasks are constructed on these corpora: multi-hop question answering with 1.2K pairs and page navigation with 1K pairs. The framework measures performance using F1 scores for QA and success rates for navigation, with the Knowledge Advantage gap (KA = Performance_RM - Performance_SM) quantifying the extent to which models rely on parametric knowledge rather than reasoning. Experiments evaluate GPT-5-mini and Gemini-2.0-Flash across multiple baselines including closed-book, retrieval-augmented generation, chain-of-thought reasoning, and reading comprehension approaches.

## Key Results
- GPT-5-mini shows substantial performance differences: F1 scores of 21.6 vs 0.2 in QA and success rates of 50.8% vs 19.8% in navigation between real-mapped and synthetic-mapped settings.
- Knowledge augmentation methods (RAG, IRCoT+RAG, Reading Comprehension) improve performance in both settings but fail to eliminate the KA gap.
- Retrieval quality correlates with performance in both settings, suggesting that knowledge-augmented approaches may not truly address reasoning ability.
- The KA gap persists across multiple task types and model architectures, indicating systematic conflation of reasoning and knowledge in current LM evaluation.

## Why This Works (Mechanism)
SynthWorlds works by creating a controlled experimental environment where reasoning difficulty is held constant while knowledge accessibility varies. By constructing parallel corpora with identical graph structures but different entity names, the framework isolates whether performance differences stem from reasoning ability or parametric knowledge. The synthetic entity generation process ensures no factual overlap with real entities, while the multi-hop tasks require reasoning chains of equal length regardless of entity familiarity. This controlled setup enables precise measurement of the Knowledge Advantage gap, revealing how much performance depends on memorized knowledge versus genuine reasoning capability.

## Foundational Learning
- **Parallel corpora construction**: Creating two datasets with identical structure but different entity names allows isolation of knowledge effects while controlling for reasoning difficulty.
  - Why needed: Standard benchmarks conflate reasoning and knowledge, making it impossible to measure true reasoning ability.
  - Quick check: Verify corpus parallelism by comparing graph structures and confirming synthetic names have no semantic overlap with real entities.

- **Knowledge Advantage metric**: KA = Performance_RM - Performance_SM quantifies the performance gap attributable to parametric knowledge.
  - Why needed: Traditional metrics cannot distinguish between reasoning skill and memorized facts.
  - Quick check: Ensure KA gap persists across multiple tasks and baselines to confirm it measures knowledge reliance rather than task-specific effects.

- **Multi-hop task design**: Questions requiring reasoning chains of equal length across both corpora maintain constant reasoning difficulty.
  - Why needed: Varying reasoning difficulty would confound knowledge effects with reasoning ability.
  - Quick check: Validate that reasoning chains require the same number of inference steps regardless of entity familiarity.

## Architecture Onboarding
- **Component map**: Wikidata extraction -> Corpus generation (RM/SM) -> Task construction (QA/Navigation) -> Model evaluation (multiple baselines) -> KA gap analysis
- **Critical path**: Corpus generation → Task construction → Model evaluation, as these steps directly determine the experimental validity
- **Design tradeoffs**: Synthetic entity generation vs semantic leakage (favors novelty) vs retrieval quality vs model familiarity bias
- **Failure signatures**: Entity leakage between RM/SM (semantic overlap), retrieval bias favoring familiar entities, temporal misalignment between training data and evaluation corpus
- **First experiments**: 1) Verify corpus parallelism by comparing graph structures, 2) Run QA baselines on both corpora to establish baseline KA gap, 3) Test retrieval quality differences between RM and SM to identify retrieval bias

## Open Questions the Paper Calls Out
- **Open Question 1**: Does integrating retrieval capabilities with page navigation agents reduce the knowledge advantage gap by enabling better planning without relying on parametric knowledge? Current experiments only tested navigation with either links-only or full content access; retrieval-augmented navigation was not evaluated.

- **Open Question 2**: To what extent do long-context methods and multi-agent workflows help close the knowledge advantage gap compared to single-agent retrieval-augmented approaches? Current study only evaluated single-step RAG, IRCoT+RAG, and reading comprehension; alternative integration schemes remain unexplored.

- **Open Question 3**: Which specific system-level factors (e.g., retrieval quality) and LM capabilities (e.g., reasoning benchmarks) most strongly predict whether a model will have a narrow or wide knowledge advantage gap? Only correlations between retrieval quality and performance were noted; no systematic analysis of predictive factors was conducted.

- **Open Question 4**: How do findings generalize when corpus construction parameters (relation types, connective structures, underlying knowledge bases) are systematically varied? Experiments used one specific corpus derived from Wikidata with particular sampling parameters.

## Limitations
- Reliance on proprietary language models (GPT-5-mini, Gemini-2.0-Flash) that may not be publicly accessible for independent verification.
- Potential for subtle semantic leakage in synthetic entity generation that could undermine the control condition.
- Fixed step limits and retrieval cutoffs may artificially constrain model performance, particularly for complex reasoning chains.

## Confidence
- **High confidence**: Framework design, Knowledge Advantage metric formulation, and basic empirical finding of consistent performance differences between real-mapped and synthetic-mapped settings.
- **Medium confidence**: Claim that knowledge augmentation methods do not eliminate KA gap, given potential for different prompting strategies to yield different results.
- **Low confidence**: Absolute magnitude of performance differences may be specific to particular models tested and may not generalize to other LM architectures.

## Next Checks
1. **Replicate with alternative models**: Run the full experimental pipeline using publicly available models like GPT-4o or open-source alternatives (e.g., Llama 3.1) to verify that the KA gap persists across different architectures and reasoning capabilities.

2. **Leakage audit**: Conduct a systematic semantic similarity analysis between real-mapped and synthetic-mapped entity names using embedding-based methods to quantify any inadvertent information leakage that could bias the control condition.

3. **Temporal robustness test**: Construct a second parallel corpus using an earlier Wikidata snapshot (e.g., 2024) and verify that the KA gap remains stable when controlling for the temporal distance between training data and evaluation corpus.