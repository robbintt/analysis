---
ver: rpa2
title: 'Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference
  Serving'
arxiv_id: '2504.07494'
source_url: https://arxiv.org/abs/2504.07494
tags:
- cache
- request
- scheduling
- apt-serve
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the bottleneck in large language model (LLM)
  inference serving systems, where increasing request rates lead to a sharp decline
  in Time To First Token (TTFT) Service-Level Objective (SLO) attainment, limiting
  effective throughput. The authors identify two main causes: memory-intensive KV
  cache restricting batch size expansion and rigid First-Come-First-Serve (FCFS) scheduling
  policy causing suboptimal batch composition.'
---

# Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving

## Quick Facts
- **arXiv ID:** 2504.07494
- **Source URL:** https://arxiv.org/abs/2504.07494
- **Reference count:** 40
- **Primary result:** Proposes Apt-Serve framework achieving up to 8.8× improvement in effective throughput for LLM inference serving by combining hybrid cache (KV + hidden) and adaptive greedy scheduling.

## Executive Summary
Large language model (LLM) inference serving systems face a critical bottleneck: as request rates increase, Time To First Token (TTFT) Service-Level Objective (SLO) attainment sharply declines, limiting effective throughput. Apt-Serve addresses this by introducing a hybrid cache scheme that stores either KV pairs or hidden state vectors, and an adaptive runtime scheduling mechanism that dynamically optimizes batch composition. The system uses a greedy-based algorithm with theoretical guarantees to solve the scheduling optimization problem, significantly improving SLO attainment compared to state-of-the-art systems.

## Method Summary
Apt-Serve builds on vLLM to implement a hybrid cache system combining traditional KV cache with a memory-efficient hidden cache that stores input hidden state vectors. When memory constraints limit batch size, hidden cache allows storing one vector instead of two (KV), reducing memory usage by 50% at the cost of recomputation. The adaptive scheduling algorithm treats batch composition as a knapsack problem, using a greedy approach to maximize pending time reduction per unit memory. The system also includes iteration type switching (prefill vs decode) based on real-time urgency tracking. The framework was evaluated on three real-world datasets and models ranging from 13B to 66B parameters.

## Key Results
- Achieves up to 8.8× improvement in effective throughput compared to vLLM baseline
- Successfully mitigates the "SLO attainment cliff" phenomenon as request rates increase
- Demonstrates significant improvements across OPT models (13B, 30B, 66B) and various real-world datasets

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Cache Memory Expansion
The framework stores input hidden vectors instead of KV pairs, reducing memory usage by 50%. This allows larger batch sizes under memory pressure, trading compute for memory. Core assumption: recomputation overhead is acceptable if it prevents queuing delays. Evidence: memory vs compute tradeoff illustrated in figures; weak specific evidence in corpus for hidden cache mechanism. Break condition: fails if system is compute-bound rather than memory-bound.

### Mechanism 2: Greedy-Based Adaptive Scheduling
The scheduler maximizes "pending time reduction" per unit memory using a greedy knapsack approach. It calculates scheduling value based on pending time and system load. Core assumption: minimizing aggregate pending time correlates with meeting TTFT and TBT SLOs. Evidence: adaptive scheduling described in abstract and section 5; corpus supports moving away from FCFS. Break condition: underperforms if value estimation model fails to predict scheduling impact.

### Mechanism 3: Runtime Iteration Type Switching
The system adaptively switches between prefill and decode iteration types based on real-time urgency (cumulative pending time comparison). Core assumption: pending time is sufficient proxy for urgency. Evidence: iteration type logic described in section 5; corpus validates balancing TTFT and TBT responsiveness. Break condition: fails if urgency metric is gamed by new requests, starving decode requests.

## Foundational Learning

- **Concept:** **KV Cache vs. Hidden State**
  - Why needed here: Core innovation relies on distinguishing between caching projection results (KV Cache) vs. caching projection inputs (Hidden Cache)
  - Quick check: Does using Hidden Cache increase or decrease computational complexity of self-attention compared to KV Cache? (Answer: Increase, due to re-computation of K and V)

- **Concept:** **Iteration-Level Batching**
  - Why needed here: Scheduling operates at single inference iteration granularity, allowing dynamic batch composition updates
  - Quick check: In iteration-level batching, when can a new request join the batch? (Answer: At start of any new inference iteration)

- **Concept:** **Effective Throughput & SLO Attainment**
  - Why needed here: System optimizes for requests meeting SLOs rather than raw throughput
  - Quick check: If system processes 100 requests/sec but 50% violate TTFT SLO, is effective throughput increasing or decreasing vs. 80 requests/sec with 100% SLO attainment? (Answer: Context dependent, but Apt-Serve prioritizes the latter)

## Architecture Onboarding

- **Component map:** Request Manager -> Cache Engine -> Inference Engine, with Hybrid Cache Assigner and Adaptive Scheduler in Request Manager

- **Critical path:** Track pending time/memory → Quantify scheduling value → Schedule using greedy algorithm → Assign cache types → Execute with custom kernels

- **Design tradeoffs:** KV cache is fast but memory-heavy; Hidden cache is memory-light but adds compute latency. SLO attainment vs tail latency tradeoff in fallback mechanism.

- **Failure signatures:** OOM if greedy scheduler fails to find feasible packing; TTFT collapse if Hidden Cache threshold too conservative; TBT spikes if iteration type logic favors prefill too aggressively.

- **First 3 experiments:**
  1. Memory Isolation Test: Disable adaptive scheduling, toggle Hybrid Cache on/off to verify batch size increase under memory pressure
  2. Scheduler Stress Test: Fix cache to KV-only, compare Adaptive vs FCFS scheduling to verify queueing delay reduction
  3. End-to-End SLO Check: Run full Apt-Serve vs vLLM baseline on ShareGPT dataset, monitor SLO attainment cliff as request rates increase

## Open Questions the Paper Calls Out

- **Open Question 1:** How can Apt-Serve's hybrid cache scheme and adaptive scheduling be generalized to multi-instance, distributed LLM serving scenarios? The current implementation focuses on single-instance setup; distributed serving introduces network overhead and cross-instance scheduling complexities not addressed.

- **Open Question 2:** How can output length prediction be formally integrated into Apt-Serve scheduling formulation to improve decision-making? Current scheduling relies on runtime metrics; incorporating probabilistic predictions would require changing objective function to handle uncertainty.

- **Open Question 3:** What is the optimal policy for the SLO-aware fallback mechanism to minimize request starvation without compromising overall SLO attainment? Current solution uses heuristic near-zero constant; optimal balance between aggressive demotion and fairness remains undefined.

## Limitations

- Hidden cache mechanism's 2× memory savings contingent on re-computation overhead remaining negligible relative to queuing delays; micro-benchmarks isolating this overhead are not provided
- Adaptive scheduling algorithm assumes marginal pending time reduction is optimal proxy for SLO attainment; alternative objective functions and sensitivity analysis are not explored
- Effectiveness of iteration type switching depends on accurate real-time measurement of cumulative pending time; precision of measurements and timing skews in distributed environments are not discussed

## Confidence

- **High Confidence:** Core insight that KV cache memory limits batch size and causes SLO attainment cliffs is well-established in literature and supported by empirical results
- **Medium Confidence:** Hybrid cache design and memory/compute tradeoff is clearly explained and theoretically justified, but actual runtime overhead of custom CUDA kernels is not independently validated
- **Medium Confidence:** 8.8× improvement claim is significant; methodology appears sound but exact contribution of each mechanism is not isolated in ablation studies

## Next Checks

1. **Compute Overhead Validation:** Implement micro-benchmark to measure latency of single attention operation using Hidden Cache (with re-computation) versus KV Cache (with retrieval) under varying GPU memory pressure. Verify re-computation overhead is consistently below queuing delay saved by larger batch sizes.

2. **Scheduler Sensitivity Analysis:** Modify value estimation function to use alternative formulations (e.g., absolute pending time instead of marginal reduction). Re-run main experiment and compare SLO attainment curves to assess robustness to changes in objective function.

3. **Mechanism Isolation Test:** Run three distinct experiments: (1) Force all requests to use KV cache only, (2) Force all requests to use Hidden cache only, (3) Run full hybrid system. Compare effective throughput and SLO attainment to quantify specific contribution of hybrid cache design versus adaptive scheduling algorithm.