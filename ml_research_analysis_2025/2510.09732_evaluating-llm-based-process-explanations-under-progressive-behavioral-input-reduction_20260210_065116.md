---
ver: rpa2
title: Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input
  Reduction
arxiv_id: '2510.09732'
source_url: https://arxiv.org/abs/2510.09732
tags:
- process
- explanations
- logs
- quality
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how the size of behavioral input affects the
  quality of LLM-generated process explanations. Using synthetic job-shop logs, models
  were discovered from progressively smaller event prefixes, and LLM-generated explanations
  were scored by a second LLM on completeness, bottleneck identification, and improvement
  suggestions.
---

# Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction

## Quick Facts
- arXiv ID: 2510.09732
- Source URL: https://arxiv.org/abs/2510.09732
- Reference count: 21
- This study finds that LLM-generated process explanations plateau in quality after ~1,000 events, with only modest gains from larger inputs.

## Executive Summary
This paper investigates how the size of behavioral input affects the quality of LLM-generated process explanations. Using synthetic job-shop logs, the authors progressively reduce input size through prefix sampling, discover process models from these sub-logs, and generate textual explanations using an LLM. A second LLM scores these explanations on completeness, bottleneck identification, and improvement suggestions. The results show that explanation quality improves with input size but plateaus around 1,000 events, with only modest gains beyond that point. The findings suggest a practical trade-off between computational cost and explanation quality in LLM-assisted process analysis.

## Method Summary
The study employs a four-stage pipeline: (1) extract prefixes of k events from time-ordered event logs; (2) discover process models using the Inductive Miner algorithm; (3) generate textual explanations via an LLM; and (4) score explanations using a second LLM acting as judge. The experiment uses synthetic job-shop scheduling logs with 250,000-350,000 events each, testing eight different input sizes (k = 10, 20, 50, 100, 1,000, 10,000, 100,000, and full logs). Each condition is repeated five times, and explanations are scored on three dimensions (completeness, bottlenecks, improvements) using a 1-10 rubric, with the full model serving as reference.

## Key Results
- Explanation quality improves with input size but plateaus: scores reach ~6.9/10 at 1,000 events with only modest gains up to 100,000 events (~7.9/10).
- A practical trade-off exists: moderate input sizes (e.g., 1,000 events) yield stable, high-quality explanations with lower computational cost.
- Larger input sizes provide marginal improvements and greater consistency but at higher computational expense.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality LLM process explanations can be generated from substantially reduced behavioral input, with diminishing returns beyond ~1,000 events.
- Mechanism: Prefix sampling extracts the first k events from time-ordered logs, creating progressively smaller sub-logs. Process discovery algorithms (e.g., Inductive Miner) construct behavioral models from these sub-logs, which are then serialized and passed to an LLM for explanation generation. The reduced input complexity lowers token counts and processing overhead while preserving core control-flow structures.
- Core assumption: Early events in the log contain sufficiently representative behavior (common variants, key transitions) to support meaningful explanation, and omitted late/rare variants do not critically degrade explanation utility.
- Evidence anchors:
  - [abstract]: "explanation quality improves with input size but plateaus: from ~1,000 events, scores reach ~6.9/10, with only modest gains up to 100,000 events (~7.9/10)"
  - [Section 3.1]: Defines prefix sampling as Sk = ⟨e1, e2, ..., ek⟩ and acknowledges it may exclude infrequent variants or late-appearing behavior.
  - [corpus]: Corpus offers limited direct validation of prefix-based reduction for LLM explanations; related work (e.g., LogSieve) addresses log reduction but targets CI logs, not process mining explanations.
- Break condition: If the process exhibits heavy late-appearing behavior, rare but critical variants, or concept drift within the log, prefix sampling may systematically miss essential structure and degrade explanation quality non-monotonically.

### Mechanism 2
- Claim: LLM-as-judge scoring provides a practical comparative signal for evaluating explanation quality across input sizes, though not absolute ground truth.
- Mechanism: A second LLM (LLM2) evaluates each generated explanation Ek against a reference—the full-log model M—using a 1–10 rubric across three dimensions: completeness, bottleneck identification, and improvement suggestions. The average score serves as a proxy for how well the reduced-input explanation captures the behavior present in the full model.
- Core assumption: The evaluator LLM's judgments correlate with human analyst assessments of explanation quality, and the full model M is a reasonable reference standard.
- Evidence anchors:
  - [abstract]: "scores are LLM-based (comparative signals rather than ground truth)"
  - [Section 3.4 & Table 2]: Describes the scoring rubric (1–3: major omissions; 4–6: incomplete; 7–9: minor omissions; 10: fully accurate) and notes scores are comparative.
  - [corpus]: Paper 18420 ("Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?") explores LLM-assisted explanation evaluation in adjacent ML contexts, supporting feasibility but not providing direct validation for process mining.
- Break condition: If LLM2 exhibits systematic bias (e.g., favoring verbose outputs, specific phrasings, or hallucinated but plausible-sounding details), scores may diverge from human judgments. Cross-model judging and human calibration are recommended but not yet performed.

### Mechanism 3
- Claim: Explanation quality improves non-linearly with behavioral input size, showing a "knee" in the cost–quality curve between 100–1,000 events.
- Mechanism: As k increases, the discovered model Mk captures more control-flow detail (loops, parallelism, rare paths), enabling richer explanations. Beyond a threshold, additional events contribute diminishing marginal information because the core structure has already been captured.
- Core assumption: The process has a bounded behavioral complexity—most salient activities and transitions are observable within a finite prefix—and the discovery algorithm's over-approximation behavior stabilizes with sufficient samples.
- Evidence anchors:
  - [abstract]: Indicates plateau effect—scores rise to ~6.9 at 1,000 events with modest gains thereafter.
  - [Section 5.3]: Reports clear gains from small to mid-range k, modest improvements from 1,000 to 10,000, and further uplift at 100,000 with reduced variance; full logs provide marginal additional improvement.
  - [corpus]: Paper 20657 ("Conformance Checking for Less") addresses efficiency for long sequences in conformance checking, conceptually supporting diminishing computational returns with larger inputs, though not directly evaluating explanation quality.
- Break condition: If processes are highly unstructured, exhibit long-tailed variant distributions, or have critical behavior that appears only late in the log, the plateau assumption may not hold—quality may continue improving or fluctuate non-monotonically with additional input.

## Foundational Learning

- Concept: **Event Logs and Process Discovery**
  - Why needed here: The pipeline operates on time-ordered event logs and applies discovery algorithms to produce behavioral models. Without understanding what events, traces, and logs represent, the reduction strategy and its impact are opaque.
  - Quick check question: Given a log with 10,000 events across 500 traces, what does k=1,000 represent in the prefix-sampling approach—1,000 traces, 1,000 events, or something else?

- Concept: **Behavioral Abstractions (DFGs, Petri Nets)**
  - Why needed here: These formal structures are the artifacts serialized and fed to LLM1. Their complexity directly affects token count and explanation difficulty.
  - Quick check question: If an Inductive Miner produces a Petri net from 100 events vs. 100,000 events, which would you expect to have more parallel constructs and why might that matter for an LLM?

- Concept: **LLM-as-Judge Evaluation**
  - Why needed here: The study's conclusions depend entirely on LLM2's scores. Understanding the limitations of automated evaluation is critical for interpreting results.
  - Quick check question: If LLM2 systematically scores longer explanations higher regardless of accuracy, what artifact might appear in the results, and how could you detect it?

## Architecture Onboarding

- Component map:
  - Event Log L -> Sub-log Generator (extracts prefix Sk) -> Process Discovery (Inductive Miner) -> LLM1 (Generator) -> LLM2 (Evaluator) -> Aggregation

- Critical path:
  1. Load and preprocess .xes log (PM4Py).
  2. For each k: extract prefix Sk → discover Mk → prompt LLM1 → receive Ek.
  3. For each Ek: prompt LLM2 with Ek and M → receive three dimension scores → compute average.
  4. Aggregate five runs per k; plot score vs. k; identify plateau region.

- Design tradeoffs:
  - **Prefix sampling vs. variant-aware sampling**: Prefix is simple but may miss rare variants; stratified sampling could preserve diversity at small k (future work).
  - **Inductive Miner vs. other algorithms**: Inductive Miner produces sound, block-structured nets that may over-approximate; different miners (Heuristics, Alpha) or notations (DFG, BPMN) may yield different abstraction complexity and thus different plateau points.
  - **LLM-as-judge vs. human evaluation**: Automated scoring enables scalable experiments but introduces subjectivity; human studies are planned but not yet conducted.

- Failure signatures:
  - **Non-monotonic scores**: If scores decrease or fluctuate significantly at larger k, check for log anomalies, discovery algorithm instability, or prompt/context overflow.
  - **High variance across runs**: If error bars overlap substantially across k, increase generations per point or inspect prompt consistency.
  - **Systematically low scores even at full log**: May indicate prompt misalignment, missing context, or evaluator bias; review rubric and sample outputs.
  - **Hallucinated activities in explanations**: Check if explanation references activities not in Mk; may indicate LLM over-generation rather than faithful summarization.

- First 3 experiments:
  1. **Reproduce the plateau curve**: Run the pipeline on one synthetic log (e.g., experiment 411) across all k values with 5 generations each; verify the ~6.9 score at k=1,000 and modest gains beyond. Compare error bar widths to the paper's Figure 2.
  2. **Sensitivity to sampling strategy**: Replace prefix sampling with random trace-level sampling at equivalent event counts; compare score curves to assess whether the plateau is robust to sampling method or driven by prefix-specific biases.
  3. **Cross-model evaluator check**: Substitute LLM2 with a different model (e.g., GPT-4o or Claude) using the same rubric; compute score correlation across k to probe evaluator subjectivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed cost-quality trade-offs and plateau effects generalize to real-world event logs with higher noise and heterogeneity?
- Basis in paper: [explicit] The authors state that "Validation on organizational logs across domains is needed" to calibrate the cost–quality frontier under realistic conditions.
- Why unresolved: The study relied exclusively on synthetic job-shop logs to ensure control and reproducibility, which lack the semantic ambiguity and noise of actual organizational data.
- What evidence would resolve it: Replicating the pipeline on diverse, real-world industrial logs (e.g., healthcare, finance) to compare the "knee" of the curve against the synthetic baseline.

### Open Question 2
- Question: How well do LLM-generated quality scores align with human expert evaluations of process explanation utility?
- Basis in paper: [explicit] The authors note LLM scoring introduces subjectivity and plan to "triangulate with... human ratings by process analysts."
- Why unresolved: Using an LLM to judge another LLM creates potential bias propagation, and scores currently serve only as comparative signals rather than ground truth.
- What evidence would resolve it: A user study where process analysts score the explanations using the same rubric, followed by a correlation analysis between human and LLM scores.

### Open Question 3
- Question: Does the specific choice of process discovery algorithm (e.g., Heuristics Miner vs. Inductive Miner) shift the optimal input size threshold?
- Basis in paper: [explicit] The authors assume the Inductive Miner but note that "a systematic analysis over discovery methods and model forms is an important next step."
- Why unresolved: Different algorithms produce abstractions with varying levels of noise tolerance and structural complexity (e.g., soundness vs. precision), which may alter the LLM's interpretation quality at reduced sizes.
- What evidence would resolve it: Running the reduction pipeline with multiple discovery algorithms and comparing the quality curves for the same log prefixes.

## Limitations

- Results are based on synthetic job-shop logs and may not generalize to real-world logs with higher noise and heterogeneity.
- LLM-as-judge evaluation lacks human validation, introducing potential scorer bias that may affect score reliability.
- Prefix sampling may systematically miss rare or late-appearing variants, particularly in unstructured or long-tailed processes.

## Confidence

- **High Confidence**: The core finding that explanation quality plateaus after ~1,000 events is robust within the experimental setup, supported by consistent trends across 5 synthetic logs and 5 runs per condition.
- **Medium Confidence**: The cost-quality trade-off (diminishing returns beyond 1,000 events) is well-demonstrated for synthetic data but requires validation on real logs.
- **Low Confidence**: Claims about absolute explanation quality (e.g., ~6.9/10 is "good") lack human calibration and may reflect LLM2 bias.

## Next Checks

1. **Cross-Model Evaluator Check**: Substitute LLM2 with a different LLM (e.g., GPT-4o or Claude) and compute score correlation across k values to assess scorer subjectivity.
2. **Real-World Log Validation**: Apply the pipeline to a real-life event log with known process complexity and compare the plateau location and magnitude to synthetic results.
3. **Human Evaluation Benchmark**: Recruit process analysts to score a subset of explanations (e.g., k=100, 1,000, full) and correlate human scores with LLM2 scores to validate the LLM-as-judge approach.