---
ver: rpa2
title: 'ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language
  Models'
arxiv_id: '2505.13180'
source_url: https://arxiv.org/abs/2505.13180
tags:
- action
- object
- planning
- medium
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ViPlan, the first open-source benchmark\
  \ for evaluating vision-language models (VLMs) in visual planning tasks. ViPlan\
  \ provides two distinct settings\u2014VLM-as-planner and VLM-as-grounder\u2014across\
  \ two domains: a visual Blocksworld planning problem and a simulated household robotics\
  \ environment."
---

# ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models

## Quick Facts
- arXiv ID: 2505.13180
- Source URL: https://arxiv.org/abs/2505.13180
- Reference count: 40
- Primary result: No significant benefit from Chain-of-Thought prompting across most models and methods

## Executive Summary
This paper introduces ViPlan, the first open-source benchmark for evaluating vision-language models (VLMs) in visual planning tasks. ViPlan provides two distinct settings—VLM-as-planner and VLM-as-grounder—across two domains: a visual Blocksworld planning problem and a simulated household robotics environment. The benchmark evaluates nine open-source VLM families (including models from LLaVA, Qwen, AyaVision, Gemma, DeepSeek, Phi-4, Molmo, Mistral, and InternVL) as well as two closed models (GPT-4.1 and GPT-4.1 Nano). Across these models and settings, symbolic planning approaches (VLM-as-grounder) outperform direct VLM planning in Blocksworld, where accurate visual grounding is critical, while the opposite is true in household robotics tasks, where commonsense knowledge and error recovery are beneficial.

## Method Summary
ViPlan evaluates VLMs on visual planning through two architectures: VLM-as-planner (direct action prediction via ViLa approach) and VLM-as-grounder (verifying predicate truth values for symbolic planner). The benchmark includes two domains: ViPlan-BW (photorealistic Blocksworld, 25 problems × 3 difficulty splits) and ViPlan-HH (iGibson 2.0 household simulator, 25 tasks × 3 splits). Success rate (fraction of completed tasks) is the primary metric, with single prediction accuracy for VLM-as-grounder reported separately. The evaluation uses Fast Downward planner via Unified Planning library with temperature=0 for all models, testing both direct I/O and zero-shot CoT prompting.

## Key Results
- Symbolic planning (VLM-as-grounder) outperforms direct VLM planning in Blocksworld tasks where precise visual grounding is critical
- Direct VLM planning (VLM-as-planner) dominates in household robotics tasks where commonsense knowledge and error recovery are beneficial
- No significant benefit from Chain-of-Thought prompting across most models and methods
- Error compounding causes success rates to drop exponentially as plan length increases, despite high single-predicate accuracy

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Architecture Matching
The relative effectiveness of VLM-as-planner vs. VLM-as-grounder depends on whether the task rewards perceptual precision or commonsense flexibility. In Blocksworld, goals are abstract and color-specific (e.g., "red block on blue block"), requiring precise visual grounding with little benefit from pretrained knowledge. VLM-as-grounder delegates perceptual verification to yes/no questions while a symbolic planner handles action sequencing. In Household tasks, goals align with pretrained knowledge (e.g., "put bowl in sink"), and VLM-as-planner can leverage emergent world modeling to propose reasonable actions even under visual uncertainty. Core assumption: VLMs possess stronger commonsense priors for household scenarios than for arbitrary block configurations; symbolic planners provide more reliable sequential reasoning than VLMs when perceptual grounding is the bottleneck.

### Mechanism 2: Error Compounding from Sequential Prediction
High single-prediction accuracy does not guarantee task success because errors compound across sequential decisions. Each action depends on correctly evaluating multiple preconditions and effects (≈7 predicates per action in Blocksworld). With 97% predicate accuracy, the probability of a fully correct action is 0.97^7 ≈ 0.80. Success rate degrades exponentially as plan length increases, which static VQA benchmarks don't capture. Core assumption: Predicate evaluations are approximately independent; the benchmark's success rate metric reflects this compounding accurately.

### Mechanism 3: Chain-of-Thought Ineffectiveness for Visual Reasoning
Zero-shot CoT prompting provides negligible or negative benefit for VLMs on visual planning tasks. CoT requires VLMs to generate consistent intermediate reasoning steps. The paper suggests current VLMs struggle with this for visual inputs—possibly because visual features don't decompose cleanly into language-accessible reasoning chains. Some models (Molmo, Phi-4) even fail to follow output formats with CoT. Core assumption: CoT effectiveness requires the model to have internalized reliable step-by-step visual reasoning; temperature=0 ensures determinism but doesn't fix reasoning quality.

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: The VLM-as-grounder setting requires understanding predicates, actions, preconditions, and effects as formal structures, not just natural language.
  - Quick check question: Given the PDDL action `moveBlock(?b1 ?c1)` with precondition `(clear ?b1)`, what must be true before moving block b1 to column c1?

- Concept: Predicate Grounding
  - Why needed here: The benchmark's core operation is converting symbolic predicates like `(holding bowl)` into visual yes/no questions like "Is the agent holding a bowl?"
  - Quick check question: How would you convert the predicate `(ontop bowl sink)` into a natural language question for a VLM?

- Concept: Closed-Loop vs. Open-Loop Planning
  - Why needed here: VLM-as-planner replans after every action execution; VLM-as-grounder re-grounds after detected failures. Understanding when each loop triggers is essential for debugging.
  - Quick check question: In VLM-as-grounder, what triggers a full predicate re-enumeration vs. a single action verification?

## Architecture Onboarding

- Component map:
  - **VLM-as-planner loop**: Image + goal + action list → VLM generates JSON plan → execute first action → observe new state → repeat
  - **VLM-as-grounder loop**: Symbolic planner proposes action → VLM verifies preconditions via yes/no questions → execute → VLM verifies effects → if mismatch, re-enumerate all visible predicates → replan
  - **Prompt templates**: Separate templates for Blocksworld and Household; CoT variants add `<explanation>` tags

- Critical path:
  1. Prompt engineering for your VLM (see Appendix I for exact templates)
  2. Predicate-to-question template mapping (currently hardcoded)
  3. Symbolic planner integration (Fast Downward via Unified Planning library)

- Design tradeoffs:
  - **Perfect actions (Blocksworld main experiments) vs. stochastic failures (Appendix K)**: Main experiments isolate VLM errors; stochastic version tests robustness but adds noise
  - **Privileged information in Household**: Some object locations are given as text because full exploration is out of scope—limits realism but ensures tasks are solvable
  - **Temperature=0**: Ensures reproducibility but may underrepresent model variability

- Failure signatures:
  - **Low success despite high predicate accuracy**: Check for compounding errors; examine which predicates have lowest per-predicate accuracy (e.g., "clear" in Blocksworld)
  - **CoT causing format violations**: Some models (Molmo, Phi-4) fail to produce parsable JSON with CoT—check raw outputs before parsing
  - **VLM-as-planner stuck in loops**: Model may repeatedly propose same invalid action; check if closed-loop replanning is actually receiving new observations

- First 3 experiments:
  1. Reproduce the Blocksworld simple split with Qwen2.5-VL-7B (no CoT) to validate your pipeline matches reported success rates (0.76 for VLM-as-grounder).
  2. Run the same model on Blocksworld with 10% action failure probability to verify robustness findings hold (compare to Table 11).
  3. Test a single predicate type (e.g., "clear") across all difficulties to confirm which visual concepts are hardest—this guides where to focus model improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can symbolic planners be effectively integrated into partially observable, open-world environments without relying on privileged information for unobserved predicates?
- Basis in paper: [explicit] Appendix B states: "It is out of the scope of this work how to avoid using privileged information, but future directions should investigate how to lift this assumption, especially in the case of VLM-as-grounder, where PDDL is not easily extendable to unobserved predicates otherwise."
- Why unresolved: Current VLM-as-grounder implementation assumes knowledge of non-visible objects via privileged natural language descriptions; symbolic planners require grounded predicate states to function.
- What evidence would resolve it: Demonstration of a VLM-as-grounder system achieving comparable success rates in ViPlan-HH when all object locations must be discovered through exploration rather than provided.

### Open Question 2
- Question: What are the underlying reasons that Chain-of-Thought prompting fails to improve visual planning performance across most VLMs?
- Basis in paper: [explicit] The paper states "there is no evidence that CoT helps in our benchmark on average across models" and notes "some models (like Phi-4 Multimodal and Molmo 7B) are unable to properly follow the required output format."
- Why unresolved: The study only observes the phenomenon; it does not isolate whether failures stem from visual reasoning deficits, output format compliance, error propagation in reasoning chains, or other factors.
- What evidence would resolve it: Ablation studies comparing CoT variants with different output formats, intermediate step validation, and error analysis categorizing specific failure modes per model.

### Open Question 3
- Question: Is generating a full plan before executing only the first action beneficial in the non-CoT prompting regime for VLM-as-planner?
- Basis in paper: [explicit] Appendix B notes: "asking for the full plan is likely still beneficial in the CoT case, but its usefulness remains to be measured, especially in the case of non-CoT prompting."
- Why unresolved: The current implementation always requests full plans, making it impossible to disentangle whether plan generation itself aids reasoning or merely adds computational overhead without CoT.
- What evidence would resolve it: Controlled experiments comparing single-action prediction versus full-plan generation in the non-CoT VLM-as-planner setting, measuring both success rates and computational efficiency.

## Limitations
- Domain-specific findings may not generalize to tasks requiring both precise visual grounding and strong commonsense reasoning simultaneously
- Benchmark relies on photorealistic rendering for Blocksworld and partial observability in Household, which may not reflect real-world complexity
- Closed-loop planning mechanisms may behave differently in real-world robotics with continuous state spaces versus discrete, perfect-observation environments tested

## Confidence
**High Confidence**: The finding that VLM-as-grounder outperforms VLM-as-planner in Blocksworld is well-supported by consistent success rate differences across multiple models and difficulty levels (Figure 4). The error compounding mechanism is clearly demonstrated through success rate degradation as plan length increases (Section 5.3).

**Medium Confidence**: The claim that CoT prompting provides negligible benefit for visual planning tasks has statistical support (Table 3 shows ratios below 3 standard deviations for most settings), but the negative effect observed for some models suggests variability in how VLMs handle intermediate reasoning steps. The domain-specific architecture recommendations are supported within the benchmark but may not generalize to all planning scenarios.

**Low Confidence**: The assertion that VLMs struggle with visual reasoning due to inability to generate consistent intermediate steps (Section 5.2) lacks direct causal evidence—it's plausible that other factors like prompt format sensitivity or temperature effects contribute to the observed CoT ineffectiveness.

## Next Checks
1. **Hybrid Architecture Validation**: Test a hybrid approach on a benchmark task requiring both precise visual grounding (e.g., identifying specific colored objects) and commonsense reasoning (e.g., understanding typical object locations), to determine if the domain-specific recommendations break down in mixed-scenario tasks.

2. **Error Correlation Analysis**: Collect systematic data on whether predicate errors are truly independent (as assumed in the error compounding mechanism) or whether certain models exhibit correlated failure patterns that would invalidate the exponential degradation model.

3. **Real-World Transfer Experiment**: Deploy the most successful VLM-as-grounder configuration from Blocksworld on a physical robotics platform with noisy sensors and partial observability to test whether the benchmark's findings hold under real-world visual uncertainty conditions.