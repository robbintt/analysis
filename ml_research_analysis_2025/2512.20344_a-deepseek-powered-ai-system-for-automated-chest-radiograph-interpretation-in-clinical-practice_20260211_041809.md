---
ver: rpa2
title: A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation
  in Clinical Practice
arxiv_id: '2512.20344'
source_url: https://arxiv.org/abs/2512.20344
tags:
- report
- reports
- clinical
- radiologists
- chest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Janus-Pro-CXR (1B) is a chest X-ray interpretation system based
  on DeepSeek's Janus-Pro model, developed to address radiologist shortages and heavy
  workloads. Through supervised fine-tuning on MIMIC-CXR, CheXpert Plus, and CXR-27
  datasets, it achieves rapid analysis (1-2 seconds) with only 1 billion parameters.
---

# A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice

## Quick Facts
- arXiv ID: 2512.20344
- Source URL: https://arxiv.org/abs/2512.20344
- Reference count: 0
- Key outcome: 1B parameter Janus-Pro-CXR AI system improves junior radiologist report quality and reduces interpretation time by 18.3% in prospective multicenter trial

## Executive Summary
Janus-Pro-CXR is a lightweight (1B parameter) chest X-ray interpretation system based on DeepSeek's Janus-Pro model, designed to address radiologist shortages through automated report generation. Trained on MIMIC-CXR, CheXpert Plus, and CXR-27 datasets, it achieves rapid analysis (1-2 seconds) with strong diagnostic performance (AUC > 0.8 for six critical findings). In a multicenter prospective trial, AI assistance significantly improved junior radiologists' report quality and reduced interpretation time, with experts preferring AI-assisted reports in 54.3% of cases.

## Method Summary
The system employs a large-small model collaborative framework, where an expert classification model provides structured diagnostic labels that condition a unified multimodal model's report generation. Training occurred in three stages: basic diagnosis on MIMIC-CXR/CheXpert Plus (384,208 images), multicenter adaptation on CXR-27 (11,156 images), and style alignment to local reporting conventions. The lightweight architecture enables deployment on consumer GPUs (RTX 4060 with 8GB VRAM) while outperforming larger models including ChatGPT 4o in automated report generation.

## Key Results
- Junior radiologists using AI assistance showed significantly improved report quality (4.36±0.50 vs 4.12±0.80, P<0.001)
- AI assistance reduced interpretation time by 18.3% (P<0.001) in prospective multicenter trial
- Expert radiologists preferred AI-assisted reports in 54.3% of cases
- Achieved AUC > 0.8 for six critical findings with 1B parameters, outperforming larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific supervised fine-tuning transfers general multimodal reasoning to radiology task competence.
- Mechanism: Three-stage curriculum—(1) basic diagnosis/report generation on MIMIC-CXR + CheXpert Plus, (2) multicenter adaptation on CXR-27, (3) style alignment to local reporting conventions. This progressively narrows the output distribution from general visual-language to radiology-specific terminology and diagnostic logic.
- Core assumption: The base Janus-Pro model already encodes transferable visual representations; fine-tuning primarily shapes the output head rather than reconstructing visual features.
- Evidence anchors:
  - [abstract]: "Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek's Janus-Pro model, was developed and rigorously validated"
  - [Methods]: "Fine-tuning was performed in three stages... first two stages used the MIMIC-CXR and CheXpert Plus datasets to acquire proficiency in basic diagnosis and report generation, and subsequently the CXR-27 dataset was used to address data variability and adapt the writing style"
  - [corpus]: Limited direct corpus evidence on this specific fine-tuning protocol; neighboring papers focus on evaluation benchmarks rather than training methodology.
- Break condition: If base model lacks sufficient visual grounding for medical imaging, fine-tuning cannot recover diagnostic accuracy; performance ceiling determined by pre-training quality.

### Mechanism 2
- Claim: A large-small model collaborative framework improves report generation by decoupling detection from natural language synthesis.
- Mechanism: Expert classification model provides structured diagnostic labels → unified model receives these as explicit conditioning signals → report generation guided by both visual features and injected diagnostic priors. This separates pattern recognition from clinical language generation.
- Core assumption: Diagnostic classification and fluent report generation are partially separable tasks; explicit diagnostic injection reduces hallucination by grounding language in structured predictions.
- Evidence anchors:
  - [Results]: "it injects diagnostic results via an expert model, integrates multimodal clinical data through a unified model, and explicitly models clinical image reading logic"
  - [Figure 4A description]: Shows technical architecture with this collaborative framework
  - [corpus]: Weak corpus corroboration; RadAgents paper mentions multi-agent approaches but not this specific decoupling strategy.
- Break condition: If expert model produces incorrect diagnostic labels, errors propagate to final report; detection failures cannot be corrected by the language model.

### Mechanism 3
- Claim: Lightweight architecture (1B parameters) enables clinical deployment without sacrificing task-specific performance.
- Mechanism: Parameter efficiency achieved through domain narrowing—general capabilities are pruned during task-specific fine-tuning, leaving only radiology-relevant weights active. Scaling laws suggest diminishing returns beyond task-appropriate capacity.
- Core assumption: Chest X-ray interpretation requires narrower visual-language competence than general multimodal reasoning; excess capacity is redundant.
- Evidence anchors:
  - [Discussion]: "1 billion parameters may already be sufficient... performance enhancements rely less on sheer parameter count and more on superior data quality, refined architectures, and optimized training strategies"
  - [Results]: "outperforms... larger-scale models including ChatGPT 4o (200B parameters)"
  - [corpus]: No direct corpus validation of the 1B sufficiency claim for medical imaging tasks.
- Break condition: Edge cases with rare pathologies or unusual presentations may require broader visual knowledge; lightweight models may underperform on distribution shifts.

## Foundational Learning

- Concept: Transfer learning in multimodal vision-language models
  - Why needed here: Understanding that Janus-Pro-CXR inherits visual encoders and language decoders from general-purpose Janus-Pro; fine-tuning adapts the interface between them rather than training from scratch.
  - Quick check question: Can you explain why starting from a general multimodal model is more data-efficient than training a radiology model from random initialization?

- Concept: Multi-stage curriculum fine-tuning
  - Why needed here: The three-stage training strategy (general medical data → multicenter data → local style) reflects a domain-narrowing curriculum that must be understood to reproduce or adapt the system.
  - Quick check question: What would happen if you skipped stage 2 and fine-tuned directly from MIMIC-CXR to a single hospital's data?

- Concept: AUC and F1 metrics for medical classification
  - Why needed here: Evaluating diagnostic performance requires understanding AUC (>0.8 threshold cited) and Micro/Macro F1 for imbalanced disease distributions.
  - Quick check question: Why might Macro-F1 be lower than Micro-F1 on chest X-ray datasets, and what does this reveal about model performance on rare findings?

## Architecture Onboarding

- Component map: Image input → visual encoder → expert classification → diagnostic label injection → unified model decoding → report text
- Critical path: Image input → visual encoder → expert classification → diagnostic label injection → unified model decoding → report text. Latency measured at 1-2 seconds on RTX 4060 (8GB VRAM).
- Design tradeoffs:
  - 1B parameters: Lower VRAM requirements vs. potential ceiling on complex case reasoning
  - Expert model injection: More grounded reports vs. pipeline error propagation
  - Three-stage training: Better domain adaptation vs. longer development cycle
- Failure signatures:
  - Low RadGraph F1: Entity extraction failing; check training data alignment
  - High confusion on subtle findings (fractures, edema): Insufficient representation in fine-tuning data
  - Colloquial terminology: Style alignment stage incomplete
  - Inconsistent multi-image handling: Historical/lateral view integration not fully trained
- First 3 experiments:
  1. Reproduce baseline metrics on MIMIC-CXR test set (Micro-avg F1-5 target: 63.4) using provided checkpoint to validate environment.
  2. Ablate expert model injection—compare report quality with vs. without diagnostic label conditioning on 50 held-out cases.
  3. Test domain shift: Evaluate on CXR-27 test set without stage-3 fine-tuning to quantify multicenter adaptation contribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Curated datasets may not represent real-world variability across populations and equipment
- Multicenter trial limited to two centers; junior radiologist improvement metrics may not generalize to experienced practitioners
- Lightweight architecture could underperform on rare or atypical pathologies not well-represented in training data

## Confidence
- **High confidence**: Diagnostic performance metrics (AUC > 0.8 for critical findings, F1 scores) and workflow improvement statistics (18.3% time reduction, P<0.001) - supported by clear statistical reporting and controlled trial design.
- **Medium confidence**: Clinical preference data (54.3% expert preference) - based on limited pairwise comparisons that may reflect reporting style preferences rather than clinical superiority.
- **Medium confidence**: Architecture claims (1B parameters sufficient) - contradicted by lack of direct corpus evidence for medical imaging task efficiency claims.

## Next Checks
1. External validation on independent multicenter dataset with different scanner manufacturers and patient demographics to assess generalizability.
2. Real-world deployment pilot in resource-constrained setting to measure actual time savings and clinical workflow integration.
3. Error analysis focusing on subtle findings (fractures, mild edema) to determine whether lightweight architecture creates systematic blind spots in edge cases.