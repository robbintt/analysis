---
ver: rpa2
title: 'Model Reprogramming Demystified: A Neural Tangent Kernel Perspective'
arxiv_id: '2506.00620'
source_url: https://arxiv.org/abs/2506.00620
tags:
- source
- target
- kernel
- neural
- reprogramming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical analysis of Model Reprogramming
  (MR) through the Neural Tangent Kernel (NTK) framework. The authors show that MR
  success is governed by the eigenvalue spectrum of the NTK matrix on the target dataset,
  and establish that source model effectiveness critically determines reprogramming
  outcomes.
---

# Model Reprogramming Demystified: A Neural Tangent Kernel Perspective

## Quick Facts
- arXiv ID: 2506.00620
- Source URL: https://arxiv.org/abs/2506.00620
- Reference count: 40
- Key outcome: This paper provides a theoretical analysis of Model Reprogramming (MR) through the Neural Tangent Kernel (NTK) framework. The authors show that MR success is governed by the eigenvalue spectrum of the NTK matrix on the target dataset, and establish that source model effectiveness critically determines reprogramming outcomes. They prove that the minimum eigenvalue of the source model's NTK matrix is proportional to that of the target model under certain conditions, explaining why MR success depends on source model performance. Extensive experiments validate these theoretical predictions, demonstrating that as source model depth increases, both source and target model performance improve, with eigenvalue spectra following the predicted relationships. The work provides the first comprehensive theoretical foundation for MR, offering insights into when and why MR succeeds.

## Executive Summary
This paper provides the first comprehensive theoretical foundation for Model Reprogramming (MR) using the Neural Tangent Kernel (NTK) framework. The authors demonstrate that MR success is fundamentally governed by the eigenvalue spectrum of the NTK matrix on the target dataset, with the minimum eigenvalue serving as a critical indicator. They establish that the effectiveness of the source model is the primary determinant of MR success, proving that the minimum eigenvalue of the source model's NTK matrix is proportional to that of the target model under specific conditions. Extensive experiments validate these theoretical predictions across multiple datasets and architectures.

## Method Summary
The authors analyze Model Reprogramming through the lens of NTK, modeling MR as a combination of a source model's NTK matrix and an input transformation layer's NTK matrix. They derive bounds on empirical risk using eigenvalue analysis and establish conditions under which source model effectiveness determines target model performance. The theoretical framework assumes convergence to the NTK regime and focuses on the minimum eigenvalue relationship between source and target models. The analysis connects the success of MR to the spectral properties of neural networks, providing a principled explanation for why MR works.

## Key Results
- MR success is governed by the eigenvalue spectrum of the NTK matrix on the target dataset
- Source model effectiveness is the critical determinant of reprogramming outcomes
- Minimum eigenvalue of source model's NTK matrix is proportional to that of target model under specific conditions
- Experimental validation shows increasing source model depth improves both source and target performance with eigenvalue spectra following predicted relationships

## Why This Works (Mechanism)
MR succeeds because the input transformation layer effectively bridges the gap between source and target data distributions while preserving the favorable spectral properties of the source model's NTK. The NTK framework reveals that MR is essentially learning a linear transformation in the feature space defined by the source model, where the success depends on the conditioning of the combined kernel matrix. The minimum eigenvalue acts as a measure of the effective rank of the feature space, determining how well the model can fit the target data.

## Foundational Learning

**Neural Tangent Kernel (NTK)**: The kernel that describes the evolution of neural network predictions during training in the infinite-width limit. Why needed: NTK provides a tractable framework for analyzing deep learning dynamics and generalization. Quick check: Verify that NTK approximation holds for your network width and initialization scheme.

**Eigenvalue Spectrum Analysis**: Examining the distribution of eigenvalues of the NTK matrix to understand model capacity and conditioning. Why needed: Spectral properties determine the model's ability to fit data and generalize. Quick check: Plot eigenvalue decay to assess effective rank and condition number.

**Model Reprogramming (MR)**: A transfer learning technique where a pretrained source model is repurposed for a new task through an input transformation layer. Why needed: MR enables zero-shot or few-shot adaptation without modifying the source model's weights. Quick check: Verify that the input transformation layer is frozen during source model training.

## Architecture Onboarding

**Component Map**: Input Transformation Layer -> Source Model (frozen) -> Target Task

**Critical Path**: The input transformation layer must preserve the spectral properties of the source model's NTK while adapting to the target data distribution. The source model's depth and initialization determine the conditioning of the combined kernel matrix.

**Design Tradeoffs**: Deeper source models provide better spectral properties but increase computational cost; shallower models may fail to capture sufficient target task complexity. The input transformation layer must balance expressiveness with maintaining source model conditioning.

**Failure Signatures**: Poor MR performance indicates either a broken minimum eigenvalue relationship (source model too weak) or an ineffective input transformation layer (spectral properties degraded). Check if source model minimum eigenvalue is sufficiently large relative to target task complexity.

**First Experiments**:
1. Measure minimum eigenvalues of source and target NTK matrices to verify the proportionality relationship
2. Vary source model depth to observe the impact on both source and target performance
3. Compare eigenvalue spectra across different input transformation layer architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the non-uniqueness of the feature map $\Phi$ be resolved to make Assumption 2 practically verifiable for Model Reprogramming?
- Basis in paper: [explicit] The authors state on Page 7 that "Assumption 2 is not practically identifiable" because "$\Phi$ is not uniquely defined, it is impossible to search through all possible $\Phi$ in general."
- Why unresolved: The theoretical bounds for the input transformation layer (Corollary 1) rely on this assumption, but the authors cannot verify it empirically or guarantee it holds for arbitrary source models.
- What evidence would resolve it: A theoretical proof identifying a canonical $\Phi$ or an empirical proxy metric that correlates with Assumption 2 without requiring an exhaustive search of feature maps.

### Open Question 2
- Question: Why do standard CNN architectures violate the eigenvalue similarity relationship (Assumption 3) while VGG and ResNet satisfy it?
- Basis in paper: [explicit] Page 8 notes that "CNN architectures show deviation from this pattern" and fail to maintain the first-order relationship between $\lambda_{\min}[k(a(X_T), X_S)k(X_S, a(X_T))]$ and $\lambda_{\max}[K_S]$, unlike VGG and ResNet.
- Why unresolved: The authors identify the violation but do not isolate which architectural component (e.g., pooling, channel width, or depth mechanism) causes the sub-linear growth in the CNN's kernel metrics.
- What evidence would resolve it: Ablation studies varying specific architectural hyperparameters of the CNN to identify the structural cause of the assumption violation.

### Open Question 3
- Question: How can the theoretical assumptions (2 and 3) be formalized into a concrete criterion for designing input transformation layers?
- Basis in paper: [explicit] Page 7 states that the assumptions "implicitly suggest a criterion for the input transformation layer to ensure the scalability of MR," but concludes that "further studies are needed."
- Why unresolved: While the assumptions provide a post-hoc explanation for success, the paper does not offer a constructive method to synthesize or select a transformation layer architecture based on these spectral criteria.
- What evidence would resolve it: An algorithm that selects or optimizes the input transformation layer structure based on the eigenvalue spectra of the source and target data, demonstrating improved success rates.

### Open Question 4
- Question: Can the analysis of the generalization gap be refined to be sensitive to the Neural Tangent Kernel (NTK) spectrum?
- Basis in paper: [inferred] The authors exclude a detailed analysis of the generalization gap on Page 4 because it is "dominated by the Forbenius norm of $\Theta_T$" and a large dataset constant $\Gamma_{D_T}$, rendering it "insensitive" to NTK variations.
- Why unresolved: The current bounds are too loose to determine how the spectral properties of the reprogramming layers influence out-of-sample performance, focusing the theory solely on empirical risk.
- What evidence would resolve it: A tighter generalization bound that varies meaningfully with the minimum eigenvalue of the target NTK, validated by empirical generalization error curves.

## Limitations
- Theoretical framework assumes convergence to NTK regime (infinite-width networks or specific initialization)
- Analysis focuses on minimum eigenvalue relationship without fully characterizing entire spectrum's impact
- Assumes source and target datasets share similar data distributions, which may not hold in many real-world scenarios

## Confidence

**High confidence**: The empirical validation showing that increasing source model depth improves both source and target model performance, and the observed correlation between minimum eigenvalue magnitudes.

**Medium confidence**: The theoretical proof of minimum eigenvalue proportionality under specific conditions, as this relies on assumptions about network initialization and dataset properties.

**Medium confidence**: The claim that source model effectiveness is the "critical determinant" of MR success, as the analysis does not account for other potential factors like dataset similarity or architecture compatibility.

## Next Checks

1. Test the NTK-based predictions on finite-width networks with varying depths and widths to assess the convergence to theoretical predictions and identify when finite-size effects become significant.

2. Conduct experiments across multiple source-target dataset pairs with varying degrees of similarity to validate whether the NTK framework can predict reprogramming success when source and target data distributions differ substantially.

3. Extend the eigenvalue analysis to include higher-order spectral properties (e.g., effective rank, condition number) to determine if they provide additional predictive power for MR performance beyond the minimum eigenvalue relationship.