---
ver: rpa2
title: Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic
  Verification
arxiv_id: '2504.04578'
source_url: https://arxiv.org/abs/2504.04578
tags:
- tasks
- plan
- planning
- task
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HVR, a neuro-symbolic system that integrates
  LLMs with hierarchical planning, Knowledge Graph-based RAG, and symbolic verification
  to improve performance in complex robotic task planning. The method decomposes tasks
  into macro actions and atomic action blocks, retrieves relevant environmental information,
  and verifies plans using PDDL-based symbolic validation.
---

# Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification

## Quick Facts
- **arXiv ID:** 2504.04578
- **Source URL:** https://arxiv.org/abs/2504.04578
- **Reference count:** 40
- **Primary result:** HVR system integrates hierarchical planning, KG-RAG, and symbolic verification to significantly improve LLM-based robotic task planning, achieving up to 94.19% plan correctness with Gemini on 12 tasks.

## Executive Summary
This paper presents HVR, a neuro-symbolic system that combines hierarchical planning, Knowledge Graph-based RAG, and symbolic verification to improve the performance of LLM planners in complex robotic tasks. The system decomposes tasks into macro actions and atomic action blocks, retrieves relevant environmental information via a Knowledge Graph, and verifies plans using PDDL-based symbolic validation. Experiments with Phi-3 and Gemini LLMs show that HVR significantly outperforms baselines, with hierarchical planning being more impactful for larger LLMs and RAG being crucial for smaller models. The approach also serves as a failure detector, enhancing robustness in execution.

## Method Summary
HVR integrates hierarchical task decomposition with Knowledge Graph-RAG and symbolic verification. The system first generates a high-level macro plan from the task description and KG context, then expands each macro action into a sequence of atomic actions. A symbolic validator checks the feasibility of these plans against PDDL pre/post-conditions, and an LLM-based corrector refines plans when validation fails. The framework uses frozen LLMs (Phi-3, Gemini) with an ad-hoc Python PDDL validator, and is tested on 12 tasks in the AI2Thor kitchen simulator.

## Key Results
- HVR achieves up to 94.19% plan correctness with Gemini-1.5-flash, significantly outperforming baselines.
- RAG is crucial for smaller LLMs (Phi-3), while hierarchical planning is more impactful for larger LLMs (Gemini).
- The system acts as a failure detector, improving robustness in execution despite some plan verbosity.
- LLM-generated plans remain longer than necessary, indicating room for further refinement.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition Reduces Search Space Complexity
Decomposing long-horizon tasks into macro actions and atomic action blocks reduces the planning search space, preventing hallucinations and improving consistency, particularly for larger LLMs. The two-step process constrains the local context and dependencies the LLM must manage at each step, reducing error accumulation over long sequences. Core assumption: the LLM can reliably generate feasible macro plans even if it struggles with long atomic sequences.

### Mechanism 2: Knowledge Graph-RAG Constrains Hallucination via Grounded Retrieval
Retrieving a task-relevant subgraph from a symbolic Knowledge Graph provides the LLM with accurate, structured environmental state and object properties, reducing hallucinations about affordances and relations. The subgraph grounds the LLM's generation in the actual state of the environment rather than relying solely on pre-trained statistical correlations. Core assumption: the KG is accurate and up-to-date.

### Mechanism 3: Symbolic Verification Enforces Formal Correctness
Translating LLM-generated plans into PDDL and validating them with a symbolic planner ensures formal preconditions are met, catching logical errors before execution. A Symbolic Validator simulates the plan in an "ideal world" and triggers an LLM-based correction loop with back-prompting if a precondition is violated. Core assumption: the PDDL domain model accurately captures the environment's dynamics.

## Foundational Learning

- **PDDL (Planning Domain Definition Language)**
  - **Why needed here:** This is the formal language used to define the preconditions and effects of actions for the Symbolic Validator.
  - **Quick check question:** Can you write a PDDL action schema for `pick-up(object)` that requires the robot's hand to be empty and the object to be reachable?

- **Knowledge Graph Construction & Querying (SPARQL/Cypher)**
  - **Why needed here:** The system relies on a KG to represent the environment's state and object relations.
  - **Quick check question:** How would you represent the state "apple-1 is inside fridge-1 and is fresh" as a set of triples?

- **LLM Prompt Engineering for Structured Output**
  - **Why needed here:** The LLM is the core planner, but it must output structured plans (macro actions, atomic actions).
  - **Quick check question:** How would you design a prompt to ensure an LLM outputs a JSON list of atomic actions rather than a free-text paragraph?

## Architecture Onboarding

- **Component map:** Input (Task + Ontology) -> KG-RAG (Retrieve $G'$) -> Macro Planner (φ, LLM) -> Validator -> Corrector -> Atomic Planner (π, LLM) -> Validator -> Corrector -> Executor + Observer.

- **Critical path:** 1. Retrieve Context: RAG query from KG -> $G'$. 2. Generate Macro Plan: LLM (φ) + $G'$ -> M-plan. 3. Validate & Correct Macros: Validator checks M-plan; if invalid, LLM corrects. 4. Expand to Atomic Actions: For each MA, LLM (π) -> AA-block. 5. Validate & Correct AAs: Validator checks AA-block; if invalid, LLM corrects. 6. Execute & Monitor: Executor runs AAs; Observer detects state discrepancies, triggering failure recovery.

- **Design tradeoffs:** Larger LLMs benefit more from hierarchical planning; smaller LLMs benefit more from RAG. Plan minimality is sacrificed for correctness; the correction loop adds steps. The system struggles with open-ended task goals; constraining goals improves performance.

- **Failure signatures:** Low PC with high RAG usage suggests retrieval is failing or KG is incomplete. High MPV but low AABV indicates the LLM struggles to ground macro plans in specific, executable actions. Plan passes validation but fails execution indicates a simulator or PDDL domain model mismatch with reality.

- **First 3 experiments:**
  1. Reproduce Baseline vs. HVR (Ablation): Run the provided tasks with just the LLM planner, then with HVR (H, V, R enabled). Compare Plan Correctness (PC) to confirm the system-level performance gain.
  2. Component Ablation by LLM Size: Run tasks using a small LLM (e.g., Phi-3) and a large LLM (e.g., Gemini) with HVR, then with components removed (HV, HR, VR). Verify the finding that small LLMs depend on RAG and large LLMs on Hierarchy.
  3. Inspect Retrieval Quality: For a task with poor PC, manually inspect the retrieved subgraph $G'$ from the KG. Are the correct objects and properties retrieved? If not, the RAG component is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM-based planners be optimized to generate minimal-length plans without sacrificing correctness?
- **Basis in paper:** The authors state that "LLM-based planners still tend to generate unnecessarily long plans with extra steps" and note that corrections often improve accuracy at the cost of adding unnecessary steps.
- **Why unresolved:** The current correction mechanisms focus on feasibility and error removal, which often introduces redundant actions, resulting in a Length Discrepancy (LD) that is significantly positive.
- **What evidence would resolve it:** A modification to the HVR framework that maintains high Plan Correctness (PC) while reducing the Length Discrepancy (LD) to near zero or negative values.

### Open Question 2
- **Question:** How can neuro-symbolic planners improve performance on tasks with open-ended or generic objectives compared to specific goal states?
- **Basis in paper:** The paper notes that while the system performs well on specific goals, it "struggles with more generic or open-ended objectives," as evidenced by the performance drop between task T5 (specific) and T5bis (generic).
- **Why unresolved:** Generic tasks increase the solution space and ambiguity, making it difficult for the symbolic validator to verify the "ideal" path against the ground truth when multiple valid paths exist.
- **What evidence would resolve it:** Demonstrated improvements in Plan Correctness (PC) on open-ended tasks (like T5bis) that close the performance gap observed between generic and specific task variants.

### Open Question 3
- **Question:** To what extent does the symbolic validation and failure detection framework transfer from the AI2Thor simulator to physical real-world robotics?
- **Basis in paper:** The authors note that "simulator execution of correct plans does not always reach the end goal," highlighting limitations in simulators and stating that failure detection methods are "indispensable for managing execution errors in real-world robotics applications."
- **Why unresolved:** The study is conducted entirely within a simulated environment (AI2Thor), and the "ideal world" assumptions made by the symbolic validator may not align with the noise and physical constraints of real-world hardware.
- **What evidence would resolve it:** Successful deployment of the HVR system on a physical robot, showing that the Execution Success (ES) rate aligns with or exceeds the ~95% rate observed in the simulator.

## Limitations
- The PDDL domain model is specific to the AI2Thor kitchen environment and may not extend to other domains without significant re-engineering.
- The correction loop can produce verbose plans, and the system struggles with open-ended task descriptions, requiring specific, unambiguous goals for optimal performance.
- The system's robustness as a "failure detector" during execution is supported by the symbolic validator catching pre-execution errors, but runtime failure detection requires more rigorous testing.

## Confidence
- **High Confidence:** The core claim that hierarchical decomposition + KG-RAG + symbolic verification improves plan correctness over baseline LLM planning.
- **Medium Confidence:** The claim that RAG is more critical for smaller LLMs and hierarchical planning is more critical for larger LLMs.
- **Medium Confidence:** The system's robustness as a "failure detector" during execution.

## Next Checks
1. **Domain Generalization Test:** Replicate the system in a non-kitchen domain (e.g., a different simulator or real robot setup) to test the portability of the PDDL model and KG-RAG approach.
2. **Open-Ended Goal Evaluation:** Design a benchmark with progressively more ambiguous task descriptions and measure performance degradation to quantify the system's limitations with generic goals.
3. **Plan Minimality Optimization:** Implement a post-generation plan minimization pass (e.g., remove redundant steps) and measure the impact on Length Discrepancy (LD) without sacrificing Plan Correctness (PC).