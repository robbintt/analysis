---
ver: rpa2
title: Enhancing Robustness of Autoregressive Language Models against Orthographic
  Attacks via Pixel-based Approach
arxiv_id: '2508.21206'
source_url: https://arxiv.org/abs/2508.21206
tags:
- language
- pixel
- text
- image
- pixel-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of autoregressive language
  models to orthographic attacks, where text perturbations using multilingual characters
  lead to significant performance degradation. The core problem stems from the out-of-vocabulary
  issue in subword tokenizers when handling noisy or multilingual inputs.
---

# Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach

## Quick Facts
- arXiv ID: 2508.21206
- Source URL: https://arxiv.org/abs/2508.21206
- Authors: Han Yang; Jian Lan; Yihong Liu; Hinrich Schütze; Thomas Seidl
- Reference count: 14
- Primary result: Pixel-based generative language model shows superior robustness to orthographic noise compared to text-based models, with smaller perplexity increases across multilingual settings.

## Executive Summary
This paper addresses the vulnerability of autoregressive language models to orthographic attacks, where text perturbations using multilingual characters lead to significant performance degradation. The core problem stems from the out-of-vocabulary issue in subword tokenizers when handling noisy or multilingual inputs. The authors propose a pixel-based generative language model that renders each word as a separate fixed-size image, replacing text-based embeddings with pixel-based representations. This approach establishes a direct one-to-one correspondence between words and their pixel representations, enabling natural next-token prediction without requiring auxiliary components like OCR.

## Method Summary
The pixel-based generative language model replaces standard text embeddings with a pre-rendered lookup table of word images. Each vocabulary token is rendered as a fixed 50×20 grayscale image using an adaptive font size that scales based on word length. A learnable linear projector maps the flattened pixel representation to a dense embedding vector, which feeds into a standard LLaMA-style transformer decoder. During training, token IDs are looked up in the pre-rendered table and projected to embeddings. The model is trained on BookCorpus and English Wikipedia using standard next-token prediction objectives. For inference, OOV words can be rendered on-the-fly or added to an extended lookup table.

## Key Results
- On SST-2 sentiment classification under increasing noise levels, the pixel model maintains more stable performance with smaller accuracy drops (0.3 vs 0.6) and perplexity increases (1.4x vs 43x)
- For multilingual settings, the pixel model shows particular advantage with non-Latin languages (Russian, Chinese, Japanese, Hindi), exhibiting significantly smaller perplexity increases compared to text-based models
- Text-based models perform better for closely related Latin-script languages, suggesting the pixel approach trades some cross-linguistic efficiency for robustness

## Why This Works (Mechanism)

### Mechanism 1: Visual Similarity Buffering Under Orthographic Noise
- Claim: Pixel-based representations maintain embedding similarity when characters are perturbed, whereas symbolic tokens do not.
- Mechanism: When orthographic attacks replace characters (e.g., "apple" → "applε"), the rendered word images remain visually similar. The linear projector maps visually similar images to nearby embeddings, preserving the probability distribution over sequences even when token sequences diverge.
- Core assumption: The learned linear projector creates an embedding space where small visual perturbations produce small embedding shifts.
- Evidence anchors:
  - [abstract] "This design provides stronger robustness to noisy inputs"
  - [section 3.4] States that corrupted and clean sequences yield visually similar rendered images: "P(I(t₁),...,I(tₙ)) ≈ P(I(t'₁),...,I(t'ₘ))"
  - [section 4.3, Figure 3] Demonstrates cosine similarity of 0.89 between clean and noised sentences in pixel embedding space
  - [corpus] Weak direct evidence; "GCC-Spam" mentions character similarity networks but in a different detection context
- Break condition: If noise renders words visually unrecognizable (e.g., complete character substitution with dissimilar glyphs), the visual similarity buffer collapses.

### Mechanism 2: Word-Unit Rendering for Token-Alignment
- Claim: Rendering each word as a separate fixed-size image enables direct compatibility with next-token prediction objectives.
- Mechanism: Unlike prior pixel-based methods that render full sentences into patches (causing patch-word misalignment), word-level rendering creates a 1:1 correspondence between pixel inputs and token outputs. This eliminates the need for OCR-based decoding and allows standard cross-entropy loss over token sequences.
- Core assumption: Words are the appropriate prediction unit; the tokenizer's word boundaries are meaningful.
- Evidence anchors:
  - [abstract] "enabling text generation without additional OCR modules"
  - [section 1] "establishes a direct one-to-one correspondence between words and their pixel representations"
  - [section 3.3] "This architecture receives inputs in the pixel modality while remaining fully compatible with next-token prediction"
  - [corpus] "Overcoming Vocabulary Constraints with Pixel-level Fallback" uses bigram slices, a different alignment strategy
- Break condition: If downstream tasks require sub-word or character-level generation, this alignment would need re-architecting.

### Mechanism 3: Vocabulary-Free Cross-Script Generalization
- Claim: Pixel-based representations generalize to non-Latin scripts without requiring shared subword units in the tokenizer's training data.
- Mechanism: Standard BPE tokenizers trained on English produce fragmented or meaningless tokens for non-Latin scripts. Visual rendering treats all scripts uniformly—any renderable text produces a valid image, which the projector maps to an embedding.
- Core assumption: The linear projector learns transferable visual features across scripts during pretraining.
- Evidence anchors:
  - [abstract] "extension of compatibility to multilingual text across diverse writing systems"
  - [section 4.2, Table 2] For Russian, Chinese, Japanese, and Hindi, the pixel model shows relative perplexity increases orders of magnitude smaller than the text baseline
  - [section 4.2] "the pixel model exhibits superior resilience in languages written in non-Latin scripts"
  - [corpus] "Overcoming Vocabulary Constraints with Pixel-level Fallback" addresses similar vocabulary constraint issues
- Break condition: If pretraining contains no visual exposure to a script, generalization may degrade severely.

## Foundational Learning

- Concept: **Subword Tokenization and the OOV Problem**
  - Why needed here: The paper's motivation rests on how BPE/SentencePiece tokenizers produce different token sequences for orthographic variants, causing embedding lookup failures and distribution shifts.
  - Quick check question: Why does replacing "a" with "α" in "apple" cause a BPE tokenizer to produce different tokens, and what downstream effect does this have?

- Concept: **Autoregressive Language Modeling Objective**
  - Why needed here: Understanding why sentence-level image rendering is incompatible with next-token prediction clarifies the necessity of word-level rendering.
  - Quick check question: If input is sentence-level image patches, what prevents computing standard cross-entropy loss over a token vocabulary?

- Concept: **Embedding Space Geometry and Visual Similarity**
  - Why needed here: The robustness claim assumes that visually similar images map to nearby embeddings; verifying this requires understanding how linear projectors learn such structure.
  - Quick check question: If a linear projector learned embeddings where small pixel changes caused large embedding distances, what would happen to robustness under orthographic noise?

## Architecture Onboarding

- Component map:
  - Input: Text string → BPE Tokenizer → Token IDs
  - Pre-rendered Lookup Table: Token IDs → Word Images (50×20×1, grayscale, O(1) lookup)
  - Adaptive Renderer: Font size scaled by word length before pre-rendering
  - Linear Projector: Flattened image (1000 pixels) → Dense embedding (768-dim, learnable)
  - Transformer Decoder: LLaMA-style (12 layers, 12 heads, 768 hidden, 2048 intermediate)
  - Output Head: Hidden state → Vocabulary logits (32,001 tokens)

- Critical path:
  1. Pre-render all vocabulary tokens to images once (before training)
  2. Training: Token IDs → Lookup → Project → Transformer → Cross-entropy over next tokens
  3. Inference: Same pipeline; OOV words rendered on-the-fly (slower) or added to extended lookup

- Design tradeoffs:
  - **Image resolution**: 50×20 pixels is small; larger images improve detail but increase memory/compute. Paper chose this for efficiency.
  - **Grayscale vs. RGB**: 1 channel reduces parameters; font rendering doesn't require color.
  - **Adaptive font sizing**: Maximizes pixel utilization for short words, but may introduce inconsistency across word lengths.
  - **Vocabulary size**: 32,001 tokens; larger vocabularies increase lookup table size but reduce OOV frequency.

- Failure signatures:
  - Clean English perplexity much higher than baseline → Check: linear projector initialization, image resolution adequacy, font rendering quality.
  - Minimal robustness gain under noise → Check: whether noise actually changes tokenization; whether projector learned visual similarity (probe with cosine similarity tests).
  - Poor non-Latin performance → Check: font support for target scripts; whether pretraining data included visual variety.
  - Slow inference → Check: OOV words being rendered on-the-fly instead of lookup.

- First 3 experiments:
  1. **Sanity check**: Train pixel model and text baseline on identical data; verify clean English perplexity is comparable (paper reports PM: 279 vs. LM: 495 on WMT24 English—PM is actually better here).
  2. **Robustness curve**: Add character-level noise at rates 0%, 10%, 20%, 30%, 40%, 50%; plot perplexity ratio (noisy/clean). Expect pixel model curve to be flatter; paper shows 3.49× vs. 294× at 50% noise.
  3. **Cross-script zero-shot**: Evaluate on WMT24 non-Latin languages (Russian, Chinese, Japanese, Hindi) without fine-tuning; compute relative perplexity increase vs. English. Expect pixel model to show smaller degradation than text baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the re-tokenization of noised sentences be effectively handled in token-level pixel methods?
- Basis in paper: [explicit] The authors state, "We regard the re-tokenization of noised sentences for the token-level pixel methods as an open problem, and leave its resolution for future work."
- Why unresolved: The current experimental setup relies on a simplifying assumption that the tokenization of a noised sentence remains identical to the clean sentence. In practice, orthographic attacks alter character sequences, causing subword tokenizers to split words differently, changing the token count and boundaries.
- What evidence would resolve it: A modified training objective or inference strategy that successfully processes noised inputs where the token sequence has changed length or structure relative to the clean input, without manual alignment.

### Open Question 2
- Question: Can pixel-based generative models close the performance gap with text-based models on Latin-alphabet languages?
- Basis in paper: [inferred] The results in Table 2 and Table 3 show that while the pixel model excels at non-Latin languages, it exhibits a much larger relative perplexity increase than the language model on Latin-based languages (e.g., German, Spanish), indicating weaker cross-lingual generalization within the same writing system.
- Why unresolved: The paper identifies the discrepancy—attributing the text model's success to shared subwords/cognates—but does not propose a method to improve the pixel model's efficiency on Latin scripts where visual variation is lower than across different writing systems.
- What evidence would resolve it: Experiments demonstrating that a pixel-based model can achieve relative perplexity increases on par with or better than text-based models for Latin-alphabet languages without sacrificing its robustness to noise.

### Open Question 3
- Question: What is the actual performance degradation when the fixed-tokenization assumption is removed?
- Basis in paper: [inferred] The paper acknowledges that the robustness evaluation assumes "tokenization of a noised sentence remains identical to that of the original clean sentence," but notes that "introducing noise inevitably alters the tokenization in most cases."
- Why unresolved: It is unclear if the reported robustness (e.g., 3.49x vs 294x perplexity increase) holds when the model is forced to process the altered token sequences that result from real-world noise, rather than the forced alignment used in the experiment.
- What evidence would resolve it: A comparative evaluation where noised sentences are re-tokenized dynamically before being rendered, measuring the drop in accuracy or the increase in perplexity compared to the static-tokenization baseline.

## Limitations
- Evaluation scope is limited to SST-2 sentiment classification and perplexity metrics under synthetic character-level noise
- Computational trade-offs including memory overhead and potential latency issues for OOV word rendering are not fully characterized
- Font and script coverage limitations may affect performance on complex scripts with varying character density and stroke complexity

## Confidence
- **High Confidence**: Pixel model demonstrates lower perplexity increases under synthetic orthographic noise; superior performance on non-Latin scripts; word-level rendering enables next-token prediction without OCR
- **Medium Confidence**: Visual similarity buffering explains robustness mechanism; adaptive renderer provides optimal visual quality; 50×20 resolution provides sufficient detail
- **Low Confidence**: Approach generalizes to all orthographic attacks; learned embedding space maintains semantic relationships across scripts; method scales efficiently to larger vocabularies

## Next Checks
1. **Script Complexity Ablation**: Evaluate the pixel model on languages with varying visual complexity (Latin, Cyrillic, Arabic, Devanagari, CJK) using identical evaluation protocols. Measure perplexity degradation as a function of character density and stroke complexity to identify resolution bottlenecks.

2. **Robustness Beyond Character Substitution**: Test the model against other orthographic attacks including word deletion, word insertion, and adversarial spelling errors. Compare performance degradation patterns to character substitution to validate whether visual similarity buffering is the primary robustness mechanism.

3. **Memory and Latency Characterization**: Measure memory consumption of the pre-rendered lookup table and latency for OOV word rendering during inference. Compare against standard text models to quantify the computational overhead and identify scenarios where the robustness gains justify the additional resource requirements.