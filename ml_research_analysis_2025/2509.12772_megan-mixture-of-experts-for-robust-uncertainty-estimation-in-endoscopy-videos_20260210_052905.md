---
ver: rpa2
title: 'MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy Videos'
arxiv_id: '2509.12772'
source_url: https://arxiv.org/abs/2509.12772
tags:
- uncertainty
- megan
- predictions
- estimation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MEGAN, a novel uncertainty quantification
  (UQ) framework that addresses inter-rater variability in medical image analysis
  by aggregating predictions and uncertainties from multiple EDL models trained on
  diverse expert annotations. MEGAN employs a gating network to optimally combine
  outputs from six independently trained EDL models, each using different expert labels
  and modeling strategies.
---

# MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy Videos

## Quick Facts
- **arXiv ID:** 2509.12772
- **Source URL:** https://arxiv.org/abs/2509.12772
- **Reference count:** 27
- **Primary result:** 3.5% improvement in F1-score and 30.5% reduction in Expected Calibration Error (ECE) compared to existing methods

## Executive Summary
MEGAN introduces a novel uncertainty quantification framework for medical image analysis that addresses inter-rater variability by aggregating predictions from multiple Evidential Deep Learning (EDL) models trained on diverse expert annotations. The framework employs a gating network to optimally combine outputs from six independently trained EDL models, each using different expert labels and modeling strategies. Evaluated on endoscopy videos for ulcerative colitis severity assessment, MEGAN demonstrates significant improvements in both classification accuracy and calibration while enabling selective expert review of uncertain cases.

## Method Summary
MEGAN is a two-stage training framework. First, six independent EDL models are trained: four on central reader labels and two on local reader labels, each with different architectures (transformer + ABMIL + dense layer). These models use Softplus activation with Dirichlet distribution and loss functions incorporating KL divergence with annealing. Second, a gating network is trained to combine these expert predictions, using a composite loss that includes uncertainty regularization and epsilon refinement terms. The gating network consists of four MLPs that process features from all EDL models to produce weighted predictions and uncertainties.

## Key Results
- Achieved 3.5% improvement in F1-score compared to existing methods
- Reduced Expected Calibration Error (ECE) by 30.5%
- Demonstrated ability to identify confident and uncertain cases, enabling selective expert review and potentially reducing annotation burden by 10%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating models trained on diverse, conflicting annotations captures inter-rater variability better than a single ground truth model.
- **Mechanism:** Instead of treating label noise as error, MEGAN trains independent EDL models on different expert annotations (local vs. central readers). The subsequent gating network learns to weight these "perspectives," effectively modeling the distribution of expert opinions rather than forcing a consensus that may not reflect clinical reality.
- **Core assumption:** Inter-rater variability encodes valid clinical ambiguity rather than pure noise; a mixture of biased experts approximates this ambiguity better than a single "average" expert.
- **Evidence anchors:** Abstract states "multi-expert gating network... trained with diverse ground truths... capturing inter-rater variability"; section 2.3 explains independent EDL training on different expert labels.
- **Break condition:** If expert labels are highly correlated (low variability) or if the validation ground truth is a deterministic function of one expert, the multi-expert fusion offers diminishing returns over a single high-quality model.

### Mechanism 2
- **Claim:** Explicit uncertainty regularization forces the gating network to produce calibrated confidence scores.
- **Mechanism:** The Gating Network uses a composite loss that penalizes high confidence on incorrect predictions and low confidence on correct ones. This moves beyond standard accuracy optimization, forcing the model to output high uncertainty specifically when the aggregated prediction is likely wrong.
- **Core assumption:** The relationship between feature space ambiguity and prediction error is learnable via the proposed penalty terms.
- **Evidence anchors:** Section 2.3 shows specific loss terms for uncertainty regularization; results show MEGAN-Gated assigns higher uncertainty to incorrect predictions across most classes.
- **Break condition:** If the loss weighting is mis-tuned, the model may become overly conservative (high uncertainty everywhere) or remain overconfident.

### Mechanism 3
- **Claim:** A learnable gating mechanism outperforms naive averaging by dynamically weighting experts based on input features.
- **Mechanism:** Unlike MEGAN-Naive (simple averaging), MEGAN-Gated uses a Shared MLP to process features and generate dynamic weights for probability and uncertainty. This allows the system to "switch" between experts depending on the specific visual characteristics of the endoscopy frame.
- **Core assumption:** Distinct experts possess complementary strengths for different input types.
- **Evidence anchors:** Table 1 shows MEGAN-Gated consistently outperforms MEGAN-Naive in F1 and ECE on unseen data; section 2.3 describes the gating network architecture.
- **Break condition:** If the training data is insufficient for the gating network to learn reliable feature-expert correlations, the gated approach may overfit to specific annotator biases.

## Foundational Learning

- **Concept: Evidential Deep Learning (EDL)**
  - **Why needed here:** This is the base "atom" of the architecture. Unlike standard softmax which gives a point estimate, EDL models a Dirichlet distribution over class probabilities, allowing the model to say "I don't know" (high uncertainty) in a single forward pass.
  - **Quick check question:** Can you explain how the Dirichlet parameters (α) differ from standard Softmax logits in representing uncertainty?

- **Concept: Mixture of Experts (MoE) / Gating Networks**
  - **Why needed here:** MEGAN is fundamentally an MoE architecture. Understanding how a "gating network" routes inputs to specific experts (or weights them) is critical to distinguishing the "Gated" vs. "Naive" variants.
  - **Quick check question:** In a soft-gating mechanism, what happens to the gradient flow if the gating weights become near-zero for all experts?

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** The paper claims success via ECE reduction, not just accuracy. You must understand that ECE measures the gap between confidence and accuracy (e.g., being 80% confident and 80% correct vs. 80% confident and 40% correct).
  - **Quick check question:** If a model predicts "Class A" with 0.9 probability but is correct only 50% of the time, is it well-calibrated?

## Architecture Onboarding

- **Component map:** Frozen Backbone (ViT/DINO) -> 6 EDL Heads (Expert Models) -> Gating Network (Shared MLP + Prob MLP + Unc MLP + Epsilon MLP) -> Aggregator (Weighted Sum + Epsilon)

- **Critical path:**
  1. Pre-training: Train Foundation Model (FM) on all data
  2. Expert Diversification: Train 6 EDL models on different label subsets to ensure diversity
  3. Fusion Training: Freeze EDL models, train GN using "Final Trial MES" as ground truth

- **Design tradeoffs:**
  - Naive vs. Gated: Naive is robust and simple (avg of outputs); Gated offers higher performance (3.5% F1 gain) but requires careful tuning of the composite loss to prevent overfitting
  - Inference Cost: MEGAN requires K forward passes (plus lightweight GN). This is more expensive than a single model but cheaper than Monte Carlo Dropout

- **Failure signatures:**
  - Class 2 Ambiguity: The paper notes Class 2 remains difficult even for the model; uncertainty signals may not reliably distinguish correct/incorrect predictions here
  - Expert Collapse: If the GN learns to ignore all but one expert (weights → 1 for one, 0 for others), the benefits of fusion are lost

- **First 3 experiments:**
  1. Sanity Check (Overfit): Train a single EDL model on the "Final Trial" consensus score to establish a baseline for F1 and ECE
  2. Ablation (Naive): Implement MEGAN-Naive (simple average of K experts). If this doesn't beat the single model, the expert diversity is insufficient
  3. Gated Validation: Train MEGAN-Gated and plot the Reliability Diagram. Verify that the "Epsilon" term actually shifts the uncertainty curve to match the diagonal better than the Naive approach

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MEGAN framework effectively generalize to other clinical domains with high inter-rater variability beyond Ulcerative Colitis (UC) endoscopy?
- **Basis in paper:** The conclusion claims the work represents a step toward capturing multi-expert uncertainty with "potential applications beyond UC in broader clinical decision support systems."
- **Why unresolved:** The study restricts evaluation to UC clinical trials (UNIFI, JAKUC, QUASAR), leaving transferability to other imaging modalities (e.g., histopathology, MRI) unproven.
- **What evidence would resolve it:** Successful application and benchmarking of MEGAN on external medical datasets where inter-rater variability is a known limiting factor.

### Open Question 2
- **Question:** Can the uncertainty estimation mechanism be refined to handle the specific ambiguity of Mayo Endoscopic Subscore (MES) Class 2?
- **Basis in paper:** Section 4.2 notes that while the model distinguishes correct from incorrect predictions generally, "this pattern reverses for class 2, which remains ambiguous even for experts."
- **Why unresolved:** The current gating and aggregation strategy fails to modulate uncertainty correctly for this specific class, potentially misleading clinicians during triaging.
- **What evidence would resolve it:** A modified loss function or architectural adjustment that aligns Class 2 uncertainty scores with prediction accuracy, similar to Classes 0, 1, and 3.

### Open Question 3
- **Question:** Is the heuristic selection of uncertainty penalty weights (β, γ) robust enough for deployment without dataset-specific tuning?
- **Basis in paper:** Section 3 states the penalties were "heuristically set... based on the validation set," suggesting a lack of theoretical derivation or adaptability.
- **Why unresolved:** If these weights require manual optimization for every new trial or dataset, the framework's scalability and automation are limited.
- **What evidence would resolve it:** A sensitivity analysis demonstrating that model calibration remains stable across a wide range of penalty coefficients on unseen data.

## Limitations
- Clinical data is proprietary pharmaceutical trial data (UNIFI, JAKUC, QUASAR), limiting independent validation and reproducibility
- The 10% annotation burden reduction is based on selective expert review but requires further validation of actual clinical workflow impact
- While MEGAN improves calibration (30.5% ECE reduction), absolute calibration error values and their clinical significance thresholds are not discussed
- Framework's generalizability to other medical imaging tasks beyond UC endoscopy severity assessment is untested

## Confidence
- **High Confidence:** Technical implementation of MEGAN architecture and core performance improvements (3.5% F1, 30.5% ECE reduction)
- **Medium Confidence:** Claim that MEGAN can identify confident vs. uncertain cases for selective expert review
- **Low Confidence:** Generalizability of MEGAN to other medical domains and absolute clinical significance of calibration improvements

## Next Checks
1. Apply MEGAN to a different medical imaging task (e.g., radiology classification or dermatology lesion assessment) with multi-expert labels to verify the framework's broader applicability beyond UC endoscopy
2. Determine clinically meaningful ECE thresholds by correlating calibration error with downstream clinical decision accuracy or patient outcomes in the UC severity assessment task
3. Conduct a prospective study measuring actual expert review time and accuracy when using MEGAN's uncertainty estimates versus standard review workflows to validate the claimed 10% reduction