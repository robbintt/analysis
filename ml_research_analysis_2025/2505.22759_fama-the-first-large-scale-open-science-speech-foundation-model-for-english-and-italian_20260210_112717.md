---
ver: rpa2
title: 'FAMA: The First Large-Scale Open-Science Speech Foundation Model for English
  and Italian'
arxiv_id: '2505.22759'
source_url: https://arxiv.org/abs/2505.22759
tags:
- speech
- fama
- data
- training
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAMA introduces the first open-science speech foundation models
  (SFMs) for English and Italian, addressing the lack of transparent, reproducible
  models in speech processing. Built on 150k+ hours of open-source speech data and
  a new 16k-hour dataset, FAMA includes small (475M) and medium (878M) encoder-decoder
  models with deeper encoders for improved speed and integration.
---

# FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian

## Quick Facts
- arXiv ID: 2505.22759
- Source URL: https://arxiv.org/abs/2505.22759
- Reference count: 8
- Primary result: First open-science speech foundation models for English and Italian, outperforming Whisper medium by up to 4.4 WER on ASR and achieving 0.152 COMET improvement on ST

## Executive Summary
FAMA introduces the first open-science speech foundation models (SFMs) for English and Italian, addressing the lack of transparent, reproducible models in speech processing. Built on 150k+ hours of open-source speech data and a new 16k-hour dataset, FAMA includes small (475M) and medium (878M) encoder-decoder models with deeper encoders for improved speed and integration. Trained in two stages (ASR pre-training + ASR+ST), FAMA achieves competitive ASR performance—outperforming Whisper medium by up to 4.4 WER on English and 6.4 WER on Italian—while being up to 8 times faster. It also improves over OWSM by up to 0.152 COMET in speech translation. All code, data, and models are released under open-source licenses, promoting transparency, fair evaluation, and broader participation in speech technology development.

## Method Summary
FAMA uses a Conformer encoder + Transformer decoder architecture with asymmetric depth (deeper encoder, shallower decoder) for speed and integration benefits. The models are trained in two stages: first on 150k+ hours of ASR data for 1M steps, then on a balanced mix of ASR and ST data for another 1M steps to avoid catastrophic forgetting. Training employs a multi-objective loss combining label-smoothed cross-entropy, intermediate CTC (layer 8 for small, 16 for medium), and final CTC losses. Inference uses beam search with joint CTC rescoring. The small model has 12 encoder/6 decoder layers (475M params), while the medium has 24 encoder/12 decoder layers (878M params).

## Key Results
- FAMA small achieves 4.4 WER improvement over Whisper medium on English ASR
- FAMA medium achieves 6.4 WER improvement over Whisper medium on Italian ASR
- FAMA medium improves COMET score by 0.152 over OWSM on speech translation
- FAMA models achieve 5-8× faster inference than Whisper medium

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric encoder-decoder depth improves inference speed while preserving ASR quality.
- Mechanism: FAMA uses an encoder twice as deep as the decoder (12 encoder/6 decoder for small, 24/12 for medium). Since autoregressive models perform multiple decoder passes during output generation, a shallower decoder speeds up inference by making each pass faster, and since many approaches integrate SFMs with LLMs by leveraging the encoder, a deeper encoder helps preserve more of the SFMs processing capabilities in such integrations.
- Core assumption: The quality loss from a smaller decoder is offset by the deeper encoder's representations and the speed gain is valuable for real-world deployment.
- Evidence anchors:
  - Table 4 shows FAMA medium achieves 40.4 xRTF vs Whisper medium at 12.1 xRTF (≈3-4× faster) and FAMA small at 56.7 xRTF (≈5-8× faster than Whisper medium).
  - Related work on SFM efficiency (StableQuant, arXiv:2504.14915) addresses quantization but does not specifically test asymmetric depth.

### Mechanism 2
- Claim: Two-stage training with controlled learning rate and balanced ASR/ST sampling mitigates catastrophic forgetting.
- Mechanism: Stage 1 trains on ASR data only (1M steps). Stage 2 continues with mixed ASR+ST data, sampling ASR targets at p_ASR=0.5. A lower constant learning rate (lr_S2=1e-4 for small, 1e-5 for medium) prevents overwriting ASR knowledge while learning ST.
- Core assumption: The model has sufficient capacity to learn ST without destructively overwriting ASR representations when the learning rate and sampling are appropriately constrained.
- Evidence anchors:
  - "As we can see from the curves, a lr_S2 of 1e-3 seems to be too high for maintaining good ASR performance while learning a new task (ST)... we notice a significant increase in the ASR ppl of up to 0.25 that corresponds to a drop in performance of 3-4 WER on both languages."
  - "We conclude that we avoid catastrophic forgetting in the two-stage training only by evenly sampling the ASR and ST tasks during the second step."

### Mechanism 3
- Claim: Multi-objective training with intermediate and final CTC losses improves alignment and performance.
- Mechanism: The training loss combines label-smoothed cross-entropy (decoder output), CTC on an intermediate encoder layer (8th for small, 16th for medium), and CTC on the final encoder output. Joint CTC rescoring at inference (weight 0.2) provides small additional gains.
- Core assumption: CTC objectives at different encoder depths provide complementary supervision without conflicting gradients that would degrade convergence.
- Evidence anchors:
  - Table 3 shows joint CTC rescoring provides small but consistent improvements (e.g., FAMA medium avg WER improves from 12.3 to 12.0; COMET improves from 0.787 to 0.791 for it→en).
  - Related work (OWSM-Biasing, arXiv:2506.09448) uses CTC for contextual biasing, suggesting CTC representations are useful for alignment.

## Foundational Learning

- Concept: Conformer encoder architecture
  - Why needed here: FAMA uses a Conformer (not vanilla Transformer) encoder, which combines convolution and self-attention for better local+global modeling in speech. Understanding this helps explain why deeper encoders remain tractable.
  - Quick check question: Can you explain why Conformer adds a convolutional module to the standard Transformer block, and what inductive bias this provides for speech?

- Concept: CTC (Connectionist Temporal Classification) loss
  - Why needed here: CTC is used as both an auxiliary training objective and for inference rescoring. It provides frame-level supervision without explicit alignment, critical for understanding the multi-loss setup.
  - Quick check question: How does CTC handle variable-length alignments between input frames and output tokens, and what role does the blank token play?

- Concept: Catastrophic forgetting in sequential training
  - Why needed here: The two-stage training strategy is explicitly designed to prevent ASR degradation when adding ST. Understanding this helps you diagnose whether future multi-task extensions are forgetting earlier capabilities.
  - Quick check question: If you see validation loss increase on task A while training on task B, what are three intervention strategies you could try?

## Architecture Onboarding

- Component map:
  Input: 80-dim Mel-filterbank features (10ms frame rate, 25ms window) -> 2× 1D conv layers (stride 2, kernel 5) -> Conformer encoder (12/24 layers) -> Intermediate CTC tap (8th/16th layer) -> Transformer decoder (6/12 layers) -> Output: 16k SentencePiece unigram vocabulary + <lang:en>, <lang:it> tokens

- Critical path:
  1. Verify Mel feature extraction matches spec (80 bins, 10ms hop, 25ms window)
  2. Check subsampling factor is 4× (not 2× like Whisper)
  3. Confirm intermediate CTC tap is correctly indexed (off-by-one errors are common)
  4. Validate language tokens are prepended to target sequences

- Design tradeoffs:
  - Deeper encoder vs. decoder: +speed, +encoder utility for downstream, but asymmetric tuning complexity
  - x4 vs. x2 vs. x8 audio subsampling: FAMA's x4 balances context length and speed; Whisper's x2 preserves more temporal detail; SeamlessM4T's x8 is fastest but may lose fine-grained cues
  - Pseudolabeled data (16k hours created via Whisper large-v3): scales data cheaply but inherits teacher biases and errors

- Failure signatures:
  - ASR WER jumps 3-4 points during stage 2 -> lr too high or p_ASR too low (catastrophic forgetting)
  - ST COMET stagnates while ASR improves -> p_ASR too high or ST data filtering too aggressive
  - Out-of-memory on OWSM inference at beam=5 -> reduce beam or batch size; FAMA's architecture supports larger batches

- First 3 experiments:
  1. Reproduce FAMA-small stage 1 (ASR only) for 100k steps on a subset of data; validate WER is within 5% of reported on CommonVoice en/it.
  2. Ablate intermediate CTC loss (set λ2=0) and compare final WER and COMET; expect small degradation.
  3. Test inference speed (xRTF) on your hardware with batch sizes [1, 4, 8, 16]; compare to Table 4 and identify memory ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the FAMA training approach scale effectively to a truly multilingual foundation model covering 50+ languages while maintaining open-source compliance?
- Basis in paper: The conclusion states: "Future work will focus on extending FAMA to additional languages with the ultimate goal of further expanding the open science ecosystem to speech technologies."
- Why unresolved: The current work only covers two languages (English and Italian), and scaling to many languages introduces challenges in data balancing, catastrophic forgetting across language pairs, and finding sufficient OS-compliant data for lower-resource languages.
- What evidence would resolve it: A multilingual FAMA model trained on OS-compliant data across 50+ languages, with comparative performance benchmarks against Whisper and SeamlessM4T on standard multilingual ASR/ST test sets.

### Open Question 2
- Question: What specific data or training improvements would close the speech translation performance gap between FAMA and closed models like SeamlessM4T?
- Basis in paper: The paper notes FAMA achieves competitive ASR but "still struggling to achieve the performance of Whisper and SeamlessM4T" in ST, attributing this partly to "the need for initiatives dedicated to creating OS-compliant ST datasets with human references."
- Why unresolved: The paper identifies the gap but does not disentangle whether it stems from training data scale, translation quality of pseudolabels, architectural choices, or lack of multilingual pretraining components like wav2vec-BERT.
- What evidence would resolve it: Ablation studies replacing automatic translations with human references, or incorporating additional OS-compliant ST data, measuring the resulting COMET improvements on standard benchmarks.

### Open Question 3
- Question: Does using Whisper large-v3 for pseudolabeling the 16k-hour dataset introduce problematic dependencies on closed-model biases or errors that propagate into FAMA's outputs?
- Basis in paper: The methodology describes using "Whisper large-v3" to create automatic transcripts for the YouTube-Commons data, which contradicts the paper's open-science philosophy by relying on a closed model during data preparation.
- Why unresolved: No analysis is provided on the quality or potential systematic biases in the pseudolabeled data, nor whether these affect downstream model behavior or evaluation fairness.
- What evidence would resolve it: Quality analysis of the pseudolabeled data (WER against human references), comparison of models trained with alternative open-labeling approaches, or probing for Whisper-specific error patterns in FAMA outputs.

## Limitations

- Generalization beyond English/Italian is untested; the two-stage training recipe and architecture may not transfer to languages with different phonotactics or script systems without re-tuning.
- Dependency on pseudo-labels from Whisper large-v3 introduces potential biases and errors that may propagate into FAMA's outputs without analysis of their impact.
- Evaluation scope is limited to CommonVoice and MuST-C benchmarks, leaving robustness to domain shift or low-resource settings unverified.

## Confidence

- **High**: ASR WER improvements over Whisper medium and OWSM (direct head-to-head numbers on same data, same metric). Data release under open-source licenses (verifiable from repository). Two-stage training mitigates catastrophic forgetting (backed by perplexity curves).
- **Medium**: Speed gains vs Whisper (xRTF is hardware-dependent and measured once; implementation details matter). Translation quality gains over OWSM (single ST benchmark, no ablation of architecture choices). Claims about downstream LLM integration utility (no empirical demonstration).
- **Low**: Generalization to languages beyond en/it (untested). Exact impact of pseudo-label noise (no error analysis). Energy efficiency or cost claims (not measured).

## Next Checks

1. **Ablate pseudo-label data**: Retrain FAMA-medium without the 16k hours of YouTube-Commons pseudo-labels and compare ASR WER on CommonVoice en/it. This quantifies the contribution and possible noise from the largest added dataset.

2. **Cross-lingual scaling test**: Apply the exact two-stage training recipe (lr_S2=1e-4 small / 1e-5 medium, p_ASR=0.5) to a third language (e.g., Spanish from CommonVoice) and report ASR WER after stage 1 and after stage 2. This validates whether the hyper-parameters are truly robust or language-specific.

3. **Hardware efficiency profiling**: Measure inference xRTF, peak memory, and energy (joules per example) on two different setups (e.g., A100 and RTX 4090) with batch sizes [1, 4, 8]. Compare against Whisper medium to confirm claimed 5-8× speed gains are reproducible and not setup-specific.