---
ver: rpa2
title: 'LESA: Learnable LLM Layer Scaling-Up'
arxiv_id: '2502.13794'
source_url: https://arxiv.org/abs/2502.13794
tags:
- arxiv
- layer
- uni00000013
- layers
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LESA, a learnable method for LLM depth scaling-up
  that learns patterns between transformer layers to predict intermediate layer parameters.
  The method addresses the limitation of existing heuristic-based scaling-up methods
  that result in poor initialization and slow convergence.
---

# LESA: Learnable LLM Layer Scaling-Up

## Quick Facts
- arXiv ID: 2502.13794
- Source URL: https://arxiv.org/abs/2502.13794
- Reference count: 13
- Key outcome: Learns patterns between transformer layers to predict intermediate layer parameters, achieving superior performance with less than half the computational cost during continual pre-training compared to heuristic methods.

## Executive Summary
LESA addresses the limitation of existing heuristic-based LLM depth scaling-up methods that result in poor initialization and slow convergence. The method discovers latent continuity patterns between adjacent transformer layers through SVD analysis and trains a neural network to predict intermediate layer parameters. Experiments show LESA outperforms baselines across different model sizes and families, including domain-specific tasks like code, while requiring significantly less computational cost during continual pre-training.

## Method Summary
LESA extracts weight matrices from each transformer block, applies SVD to uncover latent patterns between layers, and trains a small MLP to predict intermediate layer coefficients from adjacent layer pairs. The method predicts V_i given concatenated [V_{i-1}, V_{i+1}] with a combined MSE and norm loss to prevent parameter collapse. For expansion, LESA predicts intermediate V' between adjacent layers, reconstructs W' = UΣV', and inserts new transformer blocks with predicted parameters. The expanded model undergoes continual pre-training with original layers frozen for faster convergence.

## Key Results
- LESA achieves superior performance with less than half the computational cost during continual pre-training compared to baselines
- Effectiveness demonstrated across different model sizes (1.5B to 70B) and families including Llama, Qwen, and DeepSeek-Coder
- Initial perplexity after expansion significantly lower than heuristic methods (e.g., 10.52 vs 11.75 PPL for Qwen2.5-1.5B expansion)

## Why This Works (Mechanism)

### Mechanism 1: SVD-Space Layer Pattern Discovery
Concatenating layer parameters and applying SVD reveals latent continuity patterns between adjacent transformer layers. Each layer's weight matrix W_i = UΣV_i shows sequential layer indices forming continuous trajectories through t-SNE visualization of top-1 singular value coefficients. This suggests inter-layer parameters can be learned rather than heuristically duplicated.

### Mechanism 2: Neural Interpolation in Coefficient Space
A small MLP learns to predict intermediate layer coefficients from adjacent layer pairs, enabling parameter generation. The network is trained to predict V_i from concatenated [V_{i-1}, V_{i+1}] using combined MSE and norm loss to prevent predicted parameters from collapsing toward zero.

### Mechanism 3: Better Initialization Enables Faster Convergence
LESA-predicted layers provide better starting points for continual pre-training than heuristic duplication. The expanded model starts with lower initial perplexity and requires fewer gradient updates to reach target performance, with freezing original layers further accelerating convergence.

## Foundational Learning

- **Concept**: Singular Value Decomposition (SVD) for parameter space analysis
  - Why needed here: LESA operates in SVD coefficient space rather than raw weight space; understanding orthogonal basis decomposition is essential for grasping why patterns emerge
  - Quick check question: Given W = UΣV^T, what does each column of V represent in terms of the original matrix?

- **Concept**: Progressive/staged training for neural networks
  - Why needed here: LESA fits within the broader paradigm of model growth strategies; understanding why depth scaling is preferable to random initialization provides context
  - Quick check question: Why might initializing a 48-layer model from a trained 32-layer model converge faster than training from scratch?

- **Concept**: Regularization via auxiliary losses (norm preservation)
  - Why needed here: The L2 norm loss term is critical to prevent parameter degradation; without it, predictions collapse to near-zero values
  - Quick check question: What happens to a neural network's output if you only optimize for MSE against a target without any constraint on output magnitude?

## Architecture Onboarding

- **Component map**: 
  - SVD Module extracts weight matrices from each transformer block → performs SVD to obtain U, Σ, V_i
  - Prediction Network G_W (3-layer MLP with 256 hidden dim, ReLU) takes [V_{i-1}, V_{i+1}] as input, predicts V_i
  - Layer Inserter uses trained G_W to predict intermediate V' between adjacent layers, reconstructs W' = UΣV', inserts new transformer block
  - Continual Pre-training Pipeline expands model with frozen original layers, trains only new layers on fresh data

- **Critical path**:
  1. Extract and decompose → Train G_W (5 epochs, ~5 minutes)
  2. Predict intermediate V' for target layer ranges → Reconstruct and insert
  3. Evaluate initial PPL → Continual pre-train new layers only
  4. Full parameter SFT for instruction-following tasks

- **Design tradeoffs**:
  - **SVD space vs. raw weights**: Section 5.4 shows SVD improves results but method works without it; SVD adds computational overhead but provides cleaner optimization landscape
  - **Frozen vs. full fine-tuning**: Figure 6 shows freezing original layers yields faster, more stable convergence; trade-off is potential underutilization of expanded capacity
  - **Layer insertion location**: Table 9 shows inserting near output (layers 15-31) yields PPL 6.35 vs. near input (layers 1-17) yields PPL 57.32; suggests sensitivity varies by depth

- **Failure signatures**:
  - **PPL explosion (>100)**: Check hyperparameters (learning rate, epochs for G_W training) per Table 11; may need tuning on specific model
  - **Degraded predicted parameters**: Norm loss L2 missing or λ too small; verify Table 10 norms match original model (~80-105 for MLP, ~20-70 for attention)
  - **MoE router mismatch**: Table 12 shows PPL jumps to 1923; current approach (copying router) inadequate for MoE architectures
  - **Poor generalization across model families**: Some models (Qwen2.5-32B with SOLAR) show INF PPL; verify SVD decomposition doesn't produce numerical instability

- **First 3 experiments**:
  1. Reproduce SVD continuity visualization: Run SVD on Llama3-8B gate_proj, plot t-SNE of top-1 V_i coefficients with layer index labels; confirm sequential ordering
  2. Train G_W with ablation on norm loss: Train with λ=0 vs. λ=5e-5, compare predicted matrix norms against Table 10; verify collapse without norm term
  3. Pilot expansion on small model: Apply LESA to expand Qwen2.5-1.5B by 1.5x, measure initial PPL before any training; compare against Table 5 baseline (LESA: 10.52, SOLAR: 11.75)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LESA be adapted to effectively scale Mixture-of-Experts (MoE) models, particularly regarding the initialization of router parameters?
- Basis: The Limitations section and Appendix A.4 state that LESA currently struggles with MoE models because the router is highly dependent on expert performance; simply replicating the router leads to a PPL explosion (7.70 to 1923.14)
- Why unresolved: The authors have not yet devised a method to generate or predict routers for newly inserted layers, causing the expanded MoE model to fail
- What evidence would resolve it: A modified LESA framework that includes a mechanism for predicting router weights, demonstrating stable PPL on an expanded MoE model comparable to dense model scaling results

### Open Question 2
- Question: Can LESA be effectively combined with width scaling-up methods to scale models by factors larger than three times their original size?
- Basis: The Limitations section notes that this work does not consider scaling models larger than three times their size, and that significant depth increases typically require expanding matrix dimensions as well
- Why unresolved: It is unclear if the SVD-based continuity patterns hold or if the prediction network scales effectively when layer dimensions change during the scaling process
- What evidence would resolve it: Experiments applying LESA in conjunction with a width scaling method to achieve large-scale expansion without performance degradation

### Open Question 3
- Question: Why does inserting predicted layers near the input end of the network result in significantly worse initialization performance compared to the output end?
- Basis: Appendix A.1 (Table 9) shows that expanding layers 1-17 results in a PPL of 57.32, whereas expanding layers 15-31 results in a PPL of 6.35
- Why unresolved: While the paper cites previous suggestions that output layers are less sensitive, it does not investigate why LESA's specific parameter prediction fails to preserve functionality in the earlier layers
- What evidence would resolve it: A mechanistic analysis of feature evolution in the expanded input layers, or demonstration of a modified prediction objective that stabilizes PPL when expanding input-side layers

## Limitations
- The method's performance on non-Wikipedia pre-training corpora remains unclear
- LESA is validated primarily on decoder-only transformer architectures, with untested applicability to encoder-decoder models
- Computational overhead of SVD decomposition and reconstruction across different model sizes and hardware configurations is not fully characterized

## Confidence

**High Confidence** (Supported by direct experimental evidence):
- SVD-space pattern discovery between adjacent layers
- Neural interpolation mechanism with norm preservation
- Better initialization enabling faster convergence
- Effectiveness across different model families

**Medium Confidence** (Supported by limited experimental evidence):
- Computational cost reduction claims based on single dataset
- Performance consistency across all downstream benchmarks
- Generalization to code-specific tasks

**Low Confidence** (Minimal or no experimental validation):
- Performance on non-decoder architectures
- Behavior with extremely large model expansions (>2× depth)
- Robustness to different pre-training objectives beyond masked language modeling

## Next Checks

1. **Ablation on Norm Loss Sensitivity**: Systematically vary λ from 0 to 1e-4 in G_W training and measure predicted parameter norm collapse and subsequent PPL after expansion. This validates whether the norm preservation mechanism is truly critical or if the method works with simpler objectives.

2. **Cross-Domain Pre-training Validation**: Apply LESA to expand Llama3-8B and evaluate on domain-specific corpora (biomedical text, legal documents, code) before and after continual pre-training. This tests whether the discovered SVD patterns generalize beyond general web text.

3. **Architecture Transfer Experiment**: Apply LESA to an encoder-decoder model (e.g., T5 or BART) and measure whether SVD patterns exist and whether predicted layers provide better initialization than heuristic methods. This validates the method's broader applicability beyond decoder-only architectures.