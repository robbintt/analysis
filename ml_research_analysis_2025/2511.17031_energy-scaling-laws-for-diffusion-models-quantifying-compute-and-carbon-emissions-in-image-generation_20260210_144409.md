---
ver: rpa2
title: 'Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions
  in Image Generation'
arxiv_id: '2511.17031'
source_url: https://arxiv.org/abs/2511.17031
tags:
- energy
- diffusion
- scaling
- steps
- float16
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops energy scaling laws for diffusion models that
  predict GPU energy consumption from computational complexity. The authors adapt
  Kaplan scaling laws to formulate power-law relationships between energy and FLOPs,
  incorporating hardware-specific factors like precision, GPU architecture, and resolution.
---

# Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation

## Quick Facts
- arXiv ID: 2511.17031
- Source URL: https://arxiv.org/abs/2511.17031
- Reference count: 40
- Primary result: Energy scaling laws predict GPU energy consumption from FLOPs with R² > 0.9, spanning 3 orders of magnitude

## Executive Summary
This paper develops energy scaling laws for diffusion models that predict GPU energy consumption from computational complexity. The authors adapt Kaplan scaling laws to formulate power-law relationships between energy and FLOPs, incorporating hardware-specific factors like precision, GPU architecture, and resolution. Comprehensive experiments across four state-of-the-art models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, Qwen) and three GPU architectures (NVIDIA A100, A4000, A6000) demonstrate R-squared > 0.9 for individual architectures and strong cross-architecture generalization with rank correlations > 0.9. The scaling laws capture fundamental energy-complexity relationships independent of specific architectures, enabling reliable energy estimation for unseen model-hardware combinations and supporting sustainable AI deployment planning.

## Method Summary
The authors adapt Kaplan scaling laws to energy prediction by establishing log-linear relationships between energy consumption and computational complexity. They decompose diffusion inference into text encoding, iterative denoising, and decoding, with denoising dominating energy due to repeated execution. FLOPs are computed using model-specific formulas (Tables 1, 2) that account for transformer attention, convolutional operations, and classifier-free guidance. Energy is measured using CodeCarbon EmissionsTracker at 1Hz with baseline idle power subtracted. The scaling law takes the form: log(E) = log(A) + α·log(FLOPs×2^I_cfg) + β_dtype·I_dtype + β_gpu·I_gpu + β_res·log(H×W/256). Experiments span 240 hyperparameter configurations across four models and three GPU architectures.

## Key Results
- Energy scaling laws achieve R-squared > 0.9 within individual GPU architectures
- Energy consumption spans three orders of magnitude (0.051–3.58 Wh per image for Qwen)
- Cross-architecture validation shows robust rank correlations > 0.9 despite reduced absolute accuracy
- Single high-quality image can consume up to 10× the energy of a typical large language model query
- Iterative denoising dominates energy consumption (>90% of inference FLOPs)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPU energy consumption follows a predictable power-law relationship with computational complexity (FLOPs) across diffusion model configurations.
- **Mechanism:** The log-linear regression model captures energy scaling through: log(E) = log(A) + α·log(FLOPs×2^I_cfg) + β_dtype·I_dtype + β_gpu·I_gpu + β_res·log(H×W/256). The FLOPs term dominates, with learned exponents α ≈ 0.97–0.99 approaching the theoretical compute-bound ideal of 1.0.
- **Core assumption:** Diffusion inference is predominantly compute-bound rather than memory-bound, so energy scales primarily with arithmetic operations.
- **Evidence anchors:**
  - [abstract] "Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9)"
  - [Section 6.1] Flux achieves R² = 1.0 with α = 0.989; SD3.5 shows R² = 0.998 with α = 0.969
  - [corpus] CarbonScaling paper extends neural scaling laws to carbon footprint for LLMs, supporting the broader validity of scaling-law approaches to environmental metrics
- **Break condition:** If workload becomes memory-bound (very small batch sizes, excessive data transfers) or GPU thermal throttling activates, the linear FLOP-energy relationship may decouple.

### Mechanism 2
- **Claim:** Iterative denoising dominates energy consumption (>90% of inference FLOPs), making step count the primary energy lever.
- **Mechanism:** Text encoding and image decoding occur once per inference, while denoising executes N_steps times (10–50 typically). Each denoising step requires a full forward pass through the backbone network. The total FLOP decomposition: FLOPs_total = FLOPs_text + N_steps × FLOPs_denoise + FLOPs_decode.
- **Core assumption:** Denoising FLOPs scale linearly with step count and are approximately constant per step.
- **Evidence anchors:**
  - [Section 3.1] "The iterative nature of denoising makes it the dominant computational component, typically accounting for more than 90% of inference FLOPs, as shown by our decomposition analysis in Fig. 8"
  - [Section 6.1] Energy spans three orders of magnitude (0.051–3.58 Wh per image for Qwen) driven primarily by step and resolution configurations
  - [corpus] "How Hungry is AI?" benchmarks LLM inference energy but does not address diffusion's iterative structure—highlighting this mechanism's uniqueness to diffusion
- **Break condition:** Few-step samplers or distilled models that fundamentally alter the denoising loop would require recalibration of the step-count scaling factor.

### Mechanism 3
- **Claim:** Energy scaling laws transfer across model architectures (U-Net ↔ Transformer) and GPU platforms, enabling prediction for unseen combinations.
- **Mechanism:** FLOP-based scaling captures architecture-agnostic computational universals. Cross-validation experiments show training on MMDiT models (Flux, SD3.5, Qwen) and testing on U-Net (SD2) achieves robust rank correlations (R, Rs > 0.9), though absolute precision decreases.
- **Core assumption:** Energy efficiency is governed primarily by computational complexity rather than architecture-specific memory access patterns or kernel optimizations.
- **Evidence anchors:**
  - [Section 6.3] "Scaling laws learned from certain diffusion models successfully predict energy consumption for entirely different diffusion models"
  - [Section 6.4] Cross-architecture validation between convolutional U-Nets and transformer-based MMDiT shows robust relative ranking performance
  - [corpus] G-TRACE framework proposes region-aware carbon accounting but focuses on LLMs; cross-architecture transfer for diffusion remains underexplored in related work
- **Break condition:** Architectures with fundamentally different memory/compute ratios (e.g., state-space models, sparsely-activated networks) may not follow the same scaling exponents.

## Foundational Learning

- **Concept: Power-law scaling relationships**
  - **Why needed here:** The entire methodology adapts Kaplan scaling laws (originally for LLM loss) to energy prediction. Understanding y ∝ x^α relationships is essential for interpreting the log-linear regression.
  - **Quick check question:** If α = 0.5 instead of 1.0, would doubling FLOPs double, halve, or sqrt(2)× the energy?

- **Concept: FLOP computation for attention and convolutions**
  - **Why needed here:** The paper provides detailed FLOP formulas (Table 1) for transformer attention (2L·d_attn·n_layers), MMDiT variants, and convolutions. Understanding these derivations is critical for extending to new architectures.
  - **Quick check question:** Why does classifier-free guidance approximately double FLOPs for denoising?

- **Concept: Compute-bound vs. memory-bound workloads**
  - **Why needed here:** The near-linear FLOP-energy relationship (α ≈ 1.0) holds because diffusion inference is compute-bound. Memory-bound operations would show different scaling.
  - **Quick check question:** Name two factors that could push a diffusion workload from compute-bound toward memory-bound.

## Architecture Onboarding

- **Component map:** Text Encoder → Denoising Backbone (U-Net or MMDiT) → VAE Decoder → Energy Scaling Law
- **Critical path:** FLOP estimation → log-linear regression → energy prediction → carbon conversion. The FLOP formulas in Table 1 and model-specific parameters in Table 2 are the authoritative reference.
- **Design tradeoffs:**
  - Precision: fp32 provides numerical stability but increases energy ~7× (e^2.04) vs. fp16
  - Step count: Linear energy scaling; 50 steps ≈ 5× energy of 10 steps
  - Resolution: Quadratic FLOP scaling; 1024² ≈ 16× FLOPs of 256²
  - CFG: Doubles denoising FLOPs
- **Failure signatures:**
  - R² < 0.9 on single-architecture data: Check FLOP formula accuracy, GPU power measurement noise
  - Systematic under/over-prediction cross-architecture: Resolution bias term may need architecture-specific recalibration
  - Energy predictions inconsistent across batch sizes: Memory-bound behavior not captured by FLOP-only model
- **First 3 experiments:**
  1. **Replicate single-model scaling:** Run Flux on A100 across all 240 hyperparameter configurations; verify R² > 0.95 and α ≈ 0.99
  2. **Cross-GPU validation:** Train scaling law on A100 data only, predict A6000 energy; check that rank correlation > 0.9 despite absolute error
  3. **Minimal configuration sweep:** For a new diffusion model, measure energy at 4 corner cases (min/max steps × min/max resolution) to fit initial scaling law before full sweep

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the established energy scaling laws generalize to video diffusion models and significantly larger-scale architectures?
- **Basis in paper:** [explicit] The Conclusion states, "Future work will extend our analysis to video diffusion models and those with larger scale."
- **Why unresolved:** The current study validates laws only on text-to-image models (865M–20B parameters), whereas video models involve temporal dimensions and vastly different computational profiles.
- **What evidence would resolve it:** Empirical validation showing high R-squared correlations when applying the scaling laws to state-of-the-art video generation models or models exceeding the 20B parameter threshold.

### Open Question 2
- **Question:** How accurately do the FLOP-based scaling laws predict energy consumption on non-NVIDIA hardware architectures?
- **Basis in paper:** [explicit] The Limitations section notes that "alternative hardware (e.g., AMD, specialized accelerators) may exhibit distinct scaling," despite the cross-architecture validation being limited to NVIDIA A100, A4000, and A6000 GPUs.
- **Why unresolved:** Hardware-specific coefficients were learned only for NVIDIA architectures utilizing CUDA and Tensor Cores; different accelerator designs may deviate from the observed near-linear compute-bound behavior.
- **What evidence would resolve it:** Experiments replicating the methodology on AMD GPUs or TPUs to determine if the model requires re-training or if rank correlations remain > 0.9.

### Open Question 3
- **Question:** To what extent do dynamic runtime factors, such as thermal throttling and GPU frequency scaling, degrade the accuracy of static FLOP-based energy predictions?
- **Basis in paper:** [explicit] Section 6.6 states that "GPU frequency scaling and thermal throttling may affect real-world energy use beyond what static FLOP-based models capture, despite our controlled experimental setup."
- **Why unresolved:** The experiments utilized device isolation and consistent synchronization to minimize variance, which may not reflect the chaotic thermal environments of production servers.
- **What evidence would resolve it:** Long-duration inference runs without thermal caps, comparing the predicted energy against measured consumption to quantify the error introduced by dynamic scaling.

### Open Question 4
- **Question:** What specific hardware mechanism drives the negative resolution-bias coefficient observed in the scaling laws?
- **Basis in paper:** [inferred] Section 6.1 reports a negative β_res and hypothesizes it implies "GPU utilization becomes more efficient at larger tensor sizes," but the paper treats this as a learned parameter rather than deriving it from first principles.
- **Why unresolved:** While the coefficient improves the regression fit, the paper does not isolate whether the bias stems from fixed-overhead operations, cache behavior, or memory bandwidth utilization at lower resolutions.
- **What evidence would resolve it:** Hardware profiling (e.g., using NVIDIA Nsight) to correlate utilization metrics with the resolution bias term across different batch sizes.

## Limitations
- FLOP-based approach assumes compute-bound behavior, potentially missing memory-bound effects and GPU thermal throttling
- Cross-architecture generalization shows reduced absolute accuracy despite maintaining rank correlations > 0.9
- Experimental scope limited to three GPU architectures (NVIDIA A100, A4000, A6000) and four diffusion models

## Confidence

**High Confidence:** The FLOPs-energy power-law relationship within individual architectures (R² > 0.9) is well-established. The mechanism that iterative denoising dominates energy consumption (>90% of FLOPs) is directly supported by the authors' decomposition analysis and energy measurements spanning three orders of magnitude.

**Medium Confidence:** Cross-architecture generalization shows robust rank correlations (>0.9) but with reduced absolute prediction accuracy. The claim that scaling laws transfer across U-Net and Transformer architectures is supported but would benefit from testing on more diverse model families including state-space or sparsely-activated models.

**Low Confidence:** The assumption that these scaling laws extend seamlessly to future architectures or fundamentally different workload characteristics (e.g., memory-bound operations, different attention mechanisms) remains untested. The model's behavior under GPU thermal throttling or when crossing compute/memory-bound thresholds is also uncertain.

## Next Checks

1. **Thermal throttling validation:** Run the full experimental sweep while monitoring GPU temperature and clock speeds to verify whether thermal throttling affects the FLOP-energy relationship. This would test the assumption that diffusion inference remains compute-bound across all configurations.

2. **Memory-bound edge cases:** Design experiments with extremely small batch sizes (batch=1) and measure whether energy deviates from the predicted scaling law. This would identify whether and where the model transitions from compute-bound to memory-bound behavior.

3. **Architecture diversity test:** Apply the trained scaling laws to a fundamentally different architecture such as a state-space model (e.g., Mamba) or a sparsely-activated transformer to quantify the limits of cross-architecture generalization. This would test whether the scaling exponents remain consistent across architectures with different memory-compute characteristics.