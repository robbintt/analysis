---
ver: rpa2
title: Automated Road Distress Detection Using Vision Transformersand Generative Adversarial
  Networks
arxiv_id: '2511.13145'
source_url: https://arxiv.org/abs/2511.13145
tags:
- road
- images
- loss
- data
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of automated road distress detection
  by leveraging computer vision techniques, particularly Generative Adversarial Networks
  (GANs) and Vision Transformers (MaskFormer), to improve infrastructure maintenance
  processes. The project aimed to evaluate the effectiveness of synthetic data generation
  using GANs for model training and compare the performance of MaskFormer against
  traditional Convolutional Neural Networks (CNNs), specifically YOLOv8, in detecting
  road distresses such as cracks, potholes, damaged markings, and guardrails.
---

# Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks

## Quick Facts
- arXiv ID: 2511.13145
- Source URL: https://arxiv.org/abs/2511.13145
- Reference count: 21
- Key result: GAN-generated synthetic data improves model performance; MaskFormer outperforms YOLOv8 in semantic segmentation of road distresses

## Executive Summary
This study addresses automated road distress detection by comparing computer vision approaches—specifically GANs for synthetic data generation and Vision Transformers (MaskFormer) versus CNNs (YOLOv8) for semantic segmentation. The research demonstrates that GAN-generated synthetic images significantly enhance model training when real data is limited, and that MaskFormer achieves superior performance in detecting road distresses such as cracks, potholes, damaged markings, and guardrails compared to YOLOv8. The findings highlight the potential of combining generative modeling with transformer architectures for infrastructure maintenance applications, though challenges remain with class imbalance and underrepresented distress types.

## Method Summary
The methodology combines GAN-based synthetic data generation with comparative analysis of YOLOv8 and MaskFormer models for road distress detection. GANs generate synthetic road images with distress features, which are manually annotated and added to the training set. YOLOv8 serves as a CNN baseline trained for 100 epochs, while MaskFormer uses a Swin Transformer backbone for semantic segmentation. The NDTI dataset provides 1000 manually annotated images (4 classes) resized to 640×640, with 85/10/5 train/valid/test split. MaskFormer employs Adam optimizer (lr=0.00005) for 35 epochs, while YOLOv8 uses standard training procedures.

## Key Results
- GAN-generated synthetic data significantly improves model performance when real training data is limited
- MaskFormer achieves IoU of 0.707 and accuracy of 0.723, outperforming YOLOv8's mAP50 of 0.471
- Vision Transformers excel at segmenting irregular distress patterns (cracks, potholes) through global context capture
- Class imbalance severely impacts detection of underrepresented classes (damaged markings, guardrails)

## Why This Works (Mechanism)

### Mechanism 1
GAN-generated synthetic data improves model generalization when training data is scarce or class-imbalanced. The generator learns to produce plausible road distress imagery through adversarial training with a discriminator, expanding the effective training distribution with variations not captured in limited real datasets.

### Mechanism 2
Vision transformers (MaskFormer) outperform CNNs (YOLOv8) on semantic segmentation by capturing global context through self-attention. MaskFormer's transformer module computes per-segment embeddings that encode global information across the image, enabling better handling of irregular distress shapes.

### Mechanism 3
Mask classification (treating segments holistically) reduces fragmentation errors compared to per-pixel classification. MaskFormer predicts a set of binary masks plus class labels rather than classifying each pixel independently, enforcing spatial coherence within segments.

## Foundational Learning

- **Concept: Adversarial Training (GANs)** - Why needed: Understanding how generator/discriminator dynamics produce synthetic data and debugging training instability. Quick check: Can you explain why discriminator loss decreasing alongside generator loss indicates convergence, not failure?

- **Concept: Self-Attention in Vision Transformers** - Why needed: Interpreting MaskFormer's transformer module and why it captures global context differently than convolutions. Quick check: How does patch-based self-attention differ from convolutional receptive field growth?

- **Concept: IoU and mAP50 Metrics** - Why needed: Evaluating segmentation quality and comparing models fairly. Quick check: Why might a model achieve high accuracy but low IoU on imbalanced classes?

## Architecture Onboarding

- **Component map:**
  GAN: Generator (UpSampling2D + Conv2D) → Discriminator (Conv2D + LeakyReLU + Dense sigmoid)
  YOLOv8: Backbone (P1-P5, C2F blocks) → Anchor-free detection head → NMS
  MaskFormer: Pixel-level module (Swin backbone + pixel decoder) → Transformer module → Segmentation MLP → Binary masks

- **Critical path:**
  1. Data preprocessing (resize to 640×640, augmentations)
  2. GAN training (50 epochs minimum for recognizable output)
  3. Manual annotation of synthetic images
  4. Model training with early stopping on validation loss

- **Design tradeoffs:**
  YOLOv8: Faster inference, bounding boxes only → loses precise distress shape
  MaskFormer: Precise segmentation → higher compute, struggles with underrepresented classes
  GAN resolution: 640×480 used → higher resolution needed for fine distress detail

- **Failure signatures:**
  GAN mode collapse: All generated images look similar
  YOLOv8 overfitting: Validation loss rises after epoch ~25 (observed in paper)
  MaskFormer underconfidence: Non-null logits but no mask output for rare classes

- **First 3 experiments:**
  1. Replicate GAN training for 50 epochs on NDTI subset; visually inspect synthetic images at epochs 1, 20, 50 for distress features
  2. Train YOLOv8 baseline with class-balanced sampling; compare mAP50 to reported 0.471
  3. Fine-tune MaskFormer with weighted loss for underrepresented classes; measure per-class IoU improvement

## Open Questions the Paper Calls Out

### Open Question 1
Can semi-supervised learning techniques be effectively integrated to automate the annotation of GAN-generated road images? The authors note that future development could introduce semi-supervised learning for more efficient automated annotation, as manual annotation of synthetic images currently creates a bottleneck.

### Open Question 2
Does the use of more complex architectures, such as OneFormer, improve the segmentation of underrepresented classes compared to MaskFormer? The authors recommend exploring more complex models like OneFormer with additional compute resources, as MaskFormer failed to segment rare classes effectively.

### Open Question 3
Does balancing the dataset resolve the qualitative discrepancy where YOLOv8 identifies more objects in rare classes despite MaskFormer having higher overall quantitative scores? The authors note a contradiction between quantitative superiority of MaskFormer and qualitative observations of YOLOv8 identifying more underrepresented class objects.

## Limitations
- Class imbalance severely impacts performance on damaged markings and guardrails, showing near-zero detection rates
- Manual annotation process for 1000 images is labor-intensive and introduces potential subjectivity
- GAN-generated images may contain artifacts not representative of real-world conditions

## Confidence

- GAN data augmentation improving model performance: Medium (strong within-study evidence, limited external validation)
- MaskFormer outperforming YOLOv8 on segmentation: High (quantified by mAP50 and IoU metrics)
- Vision Transformer superiority for irregular distress patterns: Medium (supported by segmentation metrics but limited by class imbalance)

## Next Checks

1. **Class imbalance mitigation**: Implement weighted loss functions or oversampling for underrepresented classes and measure per-class IoU improvements
2. **Cross-dataset generalization**: Test trained models on a separate road distress dataset to evaluate real-world transfer capability
3. **Synthetic data quality assessment**: Conduct user study comparing GAN-generated images against real images for distress feature preservation and realism