---
ver: rpa2
title: 'VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based
  Local Feature Attribution'
arxiv_id: '2506.07229'
source_url: https://arxiv.org/abs/2506.07229
tags:
- feature
- attribution
- features
- arshap
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: V ARSHAP introduces a novel local feature attribution method that
  addresses global dependency issues in existing approaches like SHAP by using variance
  reduction as the core metric. Instead of relying on global data distributions, V
  ARSHAP perturbs features locally around the instance being explained, measuring
  how each feature's value reduces prediction uncertainty in that neighborhood.
---

# VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution

## Quick Facts
- **arXiv ID:** 2506.07229
- **Source URL:** https://arxiv.org/abs/2506.07229
- **Reference count:** 40
- **Primary result:** Introduces variance-based local feature attribution method that outperforms SHAP and LIME across faithfulness, robustness, and complexity metrics while maintaining axiomatic properties

## Executive Summary
VARSHAP addresses fundamental limitations in local feature attribution methods like SHAP by using variance reduction instead of expected value shifts as the core metric. Rather than sampling from global data distributions, VARSHAP perturbs features locally around the instance being explained, measuring how each feature reduces prediction uncertainty in that neighborhood. This approach maintains the desirable Shapley axioms while providing more stable, locally-focused explanations that better capture instance-specific model behavior. Theoretical analysis proves variance is the unique function satisfying shift-invariance axioms for feature attribution.

## Method Summary
VARSHAP calculates feature attributions by measuring variance reduction in model predictions when features are known versus unknown. For a given instance, it generates local perturbations around the point using a Gaussian distribution, computes the variance of model outputs when different coalitions of features are fixed, and aggregates marginal variance reductions using the Shapley value formula. The perturbation scale is controlled by parameter α, which scales the standard deviation of each feature. The method approximates the computationally expensive exact Shapley calculation using weighted local linear regression, similar to KernelSHAP's approximation strategy.

## Key Results
- Outperforms SHAP and LIME across faithfulness, robustness, and complexity metrics on synthetic and real-world datasets
- Maintains stability under global data distribution shifts while SHAP attributions diverge
- Assigns near-zero attribution to irrelevant features, validating the null-player axiom
- Demonstrates superior Local Lipschitz continuity and Relative Input Stability metrics

## Why This Works (Mechanism)

### Mechanism 1: Local Perturbation for Distributional Independence
VARSHAP prevents global data shifts from contaminating local explanations by sampling perturbations strictly from a defined neighborhood around the instance, rather than the global training distribution. The method defines a local perturbation function $\Pi(x)$ (e.g., a Gaussian centered at $x$). When simulating "missing" features for a coalition $S$, it draws values from this local distribution. This contrasts with KernelSHAP, which samples from the global background distribution $D$. The relevant behavior of the model for a specific prediction is contained within a tight neighborhood of that instance, and global context is noise for local explanation tasks.

### Mechanism 2: Variance Reduction as Information Gain
Feature importance is quantified not by the change in expected output, but by the reduction in prediction variance, which measures how much a feature stabilizes the prediction locally. VARSHAP calculates the characteristic function $v(S)$ as the variance of the model output when features in $S$ are fixed and others are locally perturbed. The attribution for feature $j$ is the weighted marginal contribution to reducing this variance. "Importance" is defined as the ability to reduce uncertainty (variance) rather than the ability to shift the mean. This implies a shift-invariant view of attribution (adding a constant to the model output shouldn't change feature rank).

### Mechanism 3: Axiomatic Shapley Aggregation
By adhering to the Shapley value formula for aggregating marginal variance reductions, the method preserves theoretical guarantees (Efficiency, Symmetry, Null Player) while changing the underlying value function. The final attribution $\Phi_j$ is calculated using the standard Shapley kernel $\omega(|S|)$ over all possible coalitions $S$. This ensures the sum of attributions equals the total variance reduction from unknown to known states. The "fairness" properties of cooperative game theory apply validly to the decomposition of variance in a local neighborhood.

## Foundational Learning

- **Concept: Shapley Values and the Value Function $v(S)$**
  - Why needed here: VARSHAP modifies the "value function" at the heart of the Shapley formula. Standard SHAP uses $v(S) = E[f(x) | x_S]$, while VARSHAP uses $v(S) = Var(f(x) | x_S)$. Understanding this distinction is the key to the entire paper.
  - Quick check question: If a feature $j$ increases the model output by a constant +10 but adds no noise, would standard SHAP or VARSHAP assign it a higher score? (Answer: SHAP would see the +10 shift; VARSHAP might see zero variance reduction).

- **Concept: Shift Invariance**
  - Why needed here: The paper justifies using variance via the "shift invariance" axiom. This means if the model adds a constant bias term $b$ to all predictions, the feature attributions should not change.
  - Quick check question: Does $E[x] - E[y]$ satisfy shift invariance? Does $Var(x)$ satisfy it?

- **Concept: Global vs. Local Perturbation**
  - Why needed here: The paper's primary critique of SHAP is its reliance on global sampling (interventional) vs. local perturbation. You must understand how sampling from the global background vs. a local Gaussian changes the interpretation of "missingness."
  - Quick check question: Why might sampling from the global distribution produce "out-of-distribution" artifacts for a specific local explanation?

## Architecture Onboarding

- **Component map:** Perturbation Engine -> Variance Estimator -> Coalition Iterator -> Shapley Aggregator
- **Critical path:** For a given instance $x$: 1) Select coalition $S$, 2) Generate perturbed samples using local Gaussian, 3) Query model for predictions, 4) Compute variance, 5) Repeat for $S \cup \{j\}$, 6) Compute marginal variance reduction and aggregate
- **Design tradeoffs:** 
  - Perturbation Scale ($\alpha$): Small $\alpha$ provides highly local, low noise explanations but might miss broader context; large $\alpha$ approaches global explanation and loses locality benefits
  - Sampling Complexity: Exact calculation is exponential ($2^M$); the paper uses weighted local linear regression for approximation
- **Failure signatures:** High variance in explanations indicates hitting chaotic regions of the model; constant attributions suggest $\alpha$ is too small and the model is locally flat/linear
- **First 3 experiments:** 1) Global Dependency Test: Create datasets with different global distributions but identical local behavior; VARSHAP should return identical attributions while SHAP diverges. 2) Null Player Test: Train with explicit noise feature; VARSHAP should assign ~0 attribution. 3) Ablation on $\alpha$: Vary $\alpha \in \{0.3, 0.6, 1.0\}$ on non-linear model to observe explanation stability.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does implementing conditional probability distributions for perturbations in VARSHAP significantly improve attribution fidelity for datasets with high feature multicollinearity compared to the current independent Gaussian approach? The paper identifies the independent perturbation mechanism as a limitation for "complex feature interactions" and suggests extension to conditional distributions.
- **Open Question 2:** Can VARSHAP's constrained local perturbations effectively mitigate adversarial attacks on explainability better than global sampling methods? The paper hypothesizes this theoretical robustness advantage but does not conduct specific experiments against adversarial manipulation of explanations.
- **Open Question 3:** How does VARSHAP perform on high-dimensional unstructured data (e.g., images or text) where local Gaussian perturbations may lack semantic meaning? The experimental evaluation is restricted to tabular datasets and low-dimensional image data, leaving high-dimensional unstructured domains unexplored.

## Limitations

- The method's theoretical advantage relies heavily on the assumption that variance reduction is the appropriate local attribution metric, which may not hold for models where importance is defined by mean shifts
- The approximation strategy using weighted local linear regression lacks implementation details that could significantly impact results
- The choice of perturbation scale α represents a critical hyperparameter whose optimal value likely depends on the model's local geometry and data manifold structure

## Confidence

- **High confidence:** The axiomatic foundation and basic experimental results showing improved robustness to global distribution shifts
- **Medium confidence:** The variance reduction mechanism as a superior local attribution metric, representing a fundamental shift in how "importance" is defined
- **Low confidence:** The approximation method's numerical stability and sensitivity to perturbation scale α, given limited ablation analysis

## Next Checks

1. **Ablation on perturbation scale:** Systematically vary α across multiple orders of magnitude on non-linear models to identify stability boundaries and optimal ranges
2. **Cross-dataset distribution shift:** Create multiple datasets with identical local behavior at test points but different global statistics; verify VARSHAP attributions remain stable while SHAP diverges
3. **Feature correlation sensitivity:** Test on datasets with strongly correlated features to evaluate whether the local Gaussian perturbation adequately captures feature dependencies compared to interventional sampling methods