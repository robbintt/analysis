---
ver: rpa2
title: Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback
arxiv_id: '2505.09925'
source_url: https://arxiv.org/abs/2505.09925
tags:
- learning
- task
- continual
- ricl
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RiCL, a reinforced interactive continual
  learning framework that dynamically learns from real-time human feedback while retaining
  prior knowledge. The method addresses two key challenges: streaming data updates
  with human feedback and robustness to noisy labels in interactive scenarios.'
---

# Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback

## Quick Facts
- arXiv ID: 2505.09925
- Source URL: https://arxiv.org/abs/2505.09925
- Authors: Yutao Yang; Jie Zhou; Junsong Li; Qianjun Pan; Bihao Zhan; Qin Chen; Xipeng Qiu; Liang He
- Reference count: 40
- One-line primary result: RiCL achieves 82.70% AP with 1.69% AF on TACRED and 84.77% AP with 7.90% AF on FewRel under 20-50% symmetric label noise.

## Executive Summary
This paper introduces RiCL, a reinforced interactive continual learning framework that dynamically learns from real-time human feedback while retaining prior knowledge. The method addresses two key challenges: streaming data updates with human feedback and robustness to noisy labels in interactive scenarios. RiCL integrates three components: a temporal consistency-aware purifier that distinguishes clean from noisy samples, an interaction-aware direct preference optimization that aligns model behavior with human intent, and a noise-resistant contrastive learning module that extracts robust representations. Experiments on FewRel and Tacred datasets with 20-50% symmetric label noise demonstrate that RiCL achieves 82.70% AP with 1.69% AF on Tacred and 84.77% AP with 7.90% AF on FewRel, substantially outperforming state-of-the-art combinations of online continual learning and noisy-label learning methods. The framework maintains stable performance across different task orders and noise levels, showing robust adaptability to real-world streaming interactive environments.

## Method Summary
RiCL is a three-stage framework for interactive continual learning from streaming human feedback with noisy labels. It uses LLaMA-7B as backbone and processes incoming data through a delay buffer. The Temporal Consistency-aware Purifier (TCP) filters samples into clean and noisy subsets using GCE loss and confidence thresholding. The Noise-resistant Contrastive Learning (NCL) module trains on noisy samples with four augmentation techniques. The Interaction-aware Direct Preference Optimization (IPO) module aligns the model with human intent using preference pairs from the gap between model predictions and human corrections. Replay buffers (4,000 for FewRel, 800 for TACRED) maintain prior knowledge. The framework is trained sequentially: NCL first on noisy buffer, then IPO on clean buffer plus clean replay.

## Key Results
- RiCL achieves 82.70% average performance (AP) with 1.69% average forgetting (AF) on TACRED dataset under 20% symmetric label noise
- RiCL achieves 84.77% AP with 7.90% AF on FewRel dataset under 20% symmetric label noise
- RiCL outperforms state-of-the-art combinations of online continual learning and noisy-label learning methods across all noise levels (20-50%)

## Why This Works (Mechanism)

### Mechanism 1: Temporal Consistency-aware Purifier (TCP)
TCP identifies clean vs. noisy samples by analyzing prediction stability across time steps. A purifier model trained with Generalized Cross-Entropy (GCE) loss calculates confidence scores using logit margins, classifying samples with positive confidence as clean and negative as noisy. These classifications are re-verified by a newer purifier version on buffered data to ensure temporal consistency before final assignment to clean or noisy replay buffers. The core assumption is that correctly labeled instances progressively converge toward their labels while mislabeled samples exhibit persistently low margins.

### Mechanism 2: Interaction-aware Direct Preference Optimization (IPO)
IPO aligns the primary LLM with human intent by treating the gap between AI-predicted labels and human-provided corrections as a preference signal. It constructs preference pairs where human feedback is preferred over model predictions, then trains using Direct Preference Optimization loss to maximize the likelihood of human-corrected labels. This is applied only to samples identified as clean by TCP, assuming human feedback contains directional signal toward correct intent that can be extracted when contrasted with model confusions.

### Mechanism 3: Noise-resistant Contrastive Learning (NCL)
NCL extracts robust feature representations from data identified as noisy, bypassing reliance on potentially corrupted labels. It applies four data augmentation techniques (synonym replacement, random insertion, swap, deletion) to create positive pairs, then uses contrastive loss to maximize agreement between original and augmented versions while pushing apart different samples in the batch. The core assumption is that semantic similarity is preserved under augmentation, allowing learning of robust representations from data's inherent structure independent of noisy labels.

## Foundational Learning

### Concept: Catastrophic Forgetting
- Why needed here: This is the primary problem continual learning solves - the tendency of neural networks to completely overwrite previously learned knowledge upon learning new tasks. RiCL explicitly aims to mitigate this (low AF scores).
- Quick check question: What would happen to performance on Task 1 if we trained a standard model sequentially on Tasks 1 through 10 without any continual learning techniques?

### Concept: Direct Preference Optimization (DPO)
- Why needed here: IPO, a core component of RiCL, is built on DPO. Understanding that DPO optimizes a policy based on which outputs are preferred over others, without training a separate reward model, is key to grasping IPO.
- Quick check question: In standard Reinforcement Learning from Human Feedback (RLHF), what two models are typically needed, and which one does DPO eliminate?

### Concept: Label Noise (Symmetric vs. Asymmetric)
- Why needed here: The paper experiments with "symmetric label noise" (random label corruption). Understanding that symmetric noise is random mislabeling helps distinguish it from systematic (asymmetric) noise, which is often harder to detect.
- Quick check question: In symmetric label noise, is a "dog" image more likely to be mislabeled as a "cat" or as any other class with equal probability?

## Architecture Onboarding

### Component map:
Data Stream & Delay Buffer -> Temporal Consistency-aware Purifier (TCP) -> Clean/Noisy Split -> Replay Buffers -> Main LLM Training (NCL on Noisy, IPO on Clean)

### Critical path:
Data -> Delay Buffer -> TCP (Clean/Noisy Split) -> Replay Buffers -> Main LLM Training (NCL on Noisy, IPO on Clean). The quality of TCP's output directly controls the signal quality for both NCL and IPO.

### Design tradeoffs:
- Buffer Size (|D|, |C|, |N|): Larger buffers allow for more stable purification and training but increase memory cost and latency
- Noise Tolerance vs. Data Efficiency: Aggressive purification (TCP) may discard valid but hard-to-learn samples, reducing training data. The 0-confidence threshold is a key dial
- Assumption: The paper acknowledges TCP is tuned for symmetric noise. Real-world deployment might require re-calibration for biased noise

### Failure signatures:
- Performance Collapse: If TCP fails to identify noise, IPO will train on garbage preference pairs, reinforcing errors
- High Forgetting (High AF): If NCL is not effective, the model may overfit to noisy data structures, disrupting previously learned features
- Stagnant Learning: If TCP is too aggressive, it may flag all new human feedback as noise, halting model adaptation

### First 3 experiments:
1. Ablation on TCP Threshold: Run RiCL on a validation split and vary the confidence threshold to find the optimal balance between precision (cleanliness) and recall (amount of data) for the purifier
2. Noise Injection Test: Train on a data stream with incrementally increasing symmetric noise (20%, 30%, 40%) to observe the degradation curve and validate the claims of robustness
3. Task Order Sensitivity: Run RiCL on a few different random task orders to verify that AP and AF remain stable, confirming the model is not simply memorizing a specific sequence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RiCL perform when subjected to asymmetric or adversarial label noise rather than the symmetric noise used in current evaluations?
- Basis in paper: The authors state in the conclusion that "its purifier is tuned for symmetric noise, which may not fully handle real-world feedback with biases, adversarial inputs, or context-dependent errors."
- Why unresolved: The current Temporal Consistency-aware Purifier (TCP) assumes errors are random (symmetric), whereas real-world human feedback often contains systematic biases or adversarial corrections that may not exhibit the same temporal inconsistency signatures.
- What evidence would resolve it: Evaluation results on datasets injected with class-conditional (asymmetric) noise or adversarial label flips to test the robustness of the TCP module.

### Open Question 2
- Question: Can the framework be extended to support multi-turn interactions, delayed responses, or partial credit?
- Basis in paper: The paper notes as a limitation: "it only supports single-turn feedback, missing multi-turn interactions, delayed responses, or partial credit — key for complex human interactions."
- Why unresolved: The current architecture relies on a delay buffer and single-shot preference optimization (IPO), lacking the temporal mechanisms to handle state tracking across a dialogue history or assign partial rewards.
- What evidence would resolve it: Modifications to the IPO module to handle sequential decision-making data, demonstrated through experiments in a multi-turn conversational setting.

### Open Question 3
- Question: Is the Temporal Consistency-aware Purifier effective when model predictions drift rapidly during the fine-tuning of the delay buffer?
- Basis in paper: The TCP relies on "Confidence" scores from the purifier model trained on the current buffer. If the model updates significantly within a buffer (concept drift), the confidence threshold may produce inconsistent "clean/noisy" classifications compared to the final check.
- Why unresolved: The method assumes stability within the buffer to discern noise, but rapid learning or shifting decision boundaries in streaming data could lead to the discarding of viable clean samples (false positives in noise detection).
- What evidence would resolve it: An analysis of the False Detection Rate (classifying clean samples as noisy) under high learning rates or rapidly shifting data distributions within the buffer.

## Limitations
- TCP is tuned for symmetric noise and may not handle real-world feedback with biases, adversarial inputs, or context-dependent errors
- The framework only supports single-turn feedback, missing multi-turn interactions, delayed responses, or partial credit that are key for complex human interactions
- Unknown implementation details include delay buffer size, batch size for NCL, temperature parameter for contrastive loss, and exact fine-tuning approach for LLaMA-7B

## Confidence

### Confidence Assessment
- **High Confidence**: The core framework architecture and its three-component design (TCP + IPO + NCL) are well-specified and logically coherent. The reported AP and AF metrics on FewRel and TACRED demonstrate measurable performance.
- **Medium Confidence**: The ablation studies and robustness claims across noise levels (20-50%) are supported by the experimental setup, though the lack of standard deviation or confidence intervals reduces statistical certainty.
- **Low Confidence**: Claims about performance under real-world streaming conditions and generalization to task orders are plausible but not empirically validated beyond the controlled Blurry-CL setup with r=0.1.

## Next Checks

### Next Validation Checks
1. **Parameter Sensitivity Analysis**: Systematically vary the TCP confidence threshold, NCL temperature τ, and IPO learning rate to map the performance landscape and identify optimal configurations for different noise levels
2. **Real-time Streaming Stress Test**: Deploy RiCL on a continuously updating data stream with varying noise distributions (including asymmetric noise) to evaluate temporal consistency and buffer management under realistic latency constraints
3. **Transferability Benchmark**: Apply RiCL to a different domain (e.g., image classification with human feedback) to assess whether the three-component framework generalizes beyond the LLaMA-7B/FewRel-TACRED setup