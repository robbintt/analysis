---
ver: rpa2
title: Enhancing Retrieval-Augmented Generation with Entity Linking for Educational
  Platforms
arxiv_id: '2512.05967'
source_url: https://arxiv.org/abs/2512.05967
tags:
- entity
- retrieval
- educational
- linking
- cross-encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces ELERAG, a hybrid retrieval-augmented generation
  (RAG) architecture that integrates Wikidata-based Entity Linking to improve factual
  accuracy in educational question-answering systems. By combining semantic similarity
  with entity-based signals through reciprocal rank fusion (RRF), the approach addresses
  the challenge of retrieving relevant passages in specialized educational domains
  with high terminological ambiguity.
---

# Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms

## Quick Facts
- arXiv ID: 2512.05967
- Source URL: https://arxiv.org/abs/2512.05967
- Authors: Francesco Granata; Francesco Poggi; Misael Mongiovì
- Reference count: 28
- ELERAG improves Exact Match and MRR on Italian educational QA over baseline and cross-encoder

## Executive Summary
This paper introduces ELERAG, a hybrid retrieval-augmented generation (RAG) architecture that integrates Wikidata-based Entity Linking to improve factual accuracy in educational question-answering systems. By combining semantic similarity with entity-based signals through reciprocal rank fusion (RRF), the approach addresses the challenge of retrieving relevant passages in specialized educational domains with high terminological ambiguity. Experimental results on Italian educational data show that ELERAG significantly outperforms both a standard RAG baseline and a cross-encoder re-ranker in Exact Match and Mean Reciprocal Rank, demonstrating superior retrieval precision and answer quality. Notably, the advantage reverses on general-domain benchmarks, confirming the domain-specific efficacy of the hybrid approach.

## Method Summary
ELERAG is a hybrid retrieval-augmented generation (RAG) architecture that integrates Wikidata-based Entity Linking to improve factual accuracy in educational question-answering systems. The method extracts named entities from queries and document chunks using SpaCy's Italian NER model, retrieves candidate entities from Wikidata, and computes a hybrid entity score (90% semantic similarity + 10% popularity). Chunks are ranked by both dense semantic similarity (via multilingual-e5-large embeddings) and entity overlap, then fused using reciprocal rank fusion (RRF) with K=60. The top-ranked chunks are passed to GPT-4o for answer generation, with a citation-enforcing prompt to ensure retrieved passages are used in responses.

## Key Results
- ELERAG achieves 0.565 Exact Match and 0.668 MRR on Italian educational benchmark, outperforming baseline (0.467 EM, 0.625 MRR) and cross-encoder (0.461 EM, 0.586 MRR).
- The hybrid approach via reciprocal rank fusion significantly improves Precision@1 (0.696) over both baseline (0.609) and cross-encoder (0.587).
- On general-domain SQuAD-it, cross-encoder outperforms ELERAG, confirming domain-specific advantage of entity-aware retrieval.

## Why This Works (Mechanism)

### Mechanism 1: Entity Linking Disambiguates Terminological Ambiguity
- Claim: Explicit entity linking provides factual grounding that dense semantic similarity alone cannot capture in specialized domains.
- Mechanism: Named entities are extracted from queries and chunks via SpaCy's Italian NER model; candidates are retrieved from Wikidata API; a hybrid score (90% semantic similarity + 10% popularity) selects the best entity; entity overlap between query and chunk serves as a complementary relevance signal.
- Core assumption: Domain-specific terms map reliably to Wikidata entities, and entity overlap correlates with factual relevance.
- Evidence anchors:
  - [abstract] "RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance."
  - [section] Section 2.2 describes the EL module using SpaCy it_core_news_lg and Wikidata API with hybrid scoring (α = 0.9).
  - [corpus] Related work shows EL integration with RAG explored in biomedical (Shlyk 2024) and other domains, but limited evidence for Italian educational contexts specifically.
- Break condition: If Wikidata lacks entities for domain terminology or entity extraction fails, entity-based signal becomes empty or noisy.

### Mechanism 2: Reciprocal Rank Fusion Robustly Combines Heterogeneous Signals
- Claim: RRF fusion of dense retrieval ranks and entity-based ranks improves top-position precision without manual weight tuning.
- Mechanism: Chunks are ranked independently by dense similarity score and entity overlap score; RRF computes `score_RRF = 1/(K+rank_dense) + 1/(K+rank_entity)` (K=60); final ranking prioritizes chunks appearing high in both lists.
- Core assumption: Dense semantic similarity and entity overlap capture partially orthogonal relevance signals; rank positions are comparable across systems.
- Evidence anchors:
  - [abstract] "the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach" in domain-specific contexts.
  - [section] Section 4.1 shows ELERAG (RRF) achieves best EM (0.565), Precision@1 (0.696), and MRR (0.668) on the educational benchmark.
  - [corpus] RRF-based hybrid retrieval appears in multiple related works (e.g., QIAS 2025 hybrid RAG for Islamic QA), suggesting broad applicability.
- Break condition: If entity extraction yields no entities for most queries, entity-based ranking becomes uninformative; RRF reduces to dense ranking.

### Mechanism 3: Domain Mismatch Limits Cross-Encoder Generalization
- Claim: Pre-trained cross-encoders excel on general-domain benchmarks but underperform on specialized, high-ambiguity corpora where entity-aware retrieval better aligns with domain structure.
- Mechanism: Cross-Encoder (mMiniLM) pre-trained on MS MARCO/Wikipedia captures general semantic patterns; on specialized lecture transcripts with domain-specific terminology, it may rank semantically similar but contextually irrelevant chunks higher than entity-grounded chunks.
- Core assumption: The linguistic distribution and terminology of specialized educational lectures diverge sufficiently from general-domain pre-training data.
- Evidence anchors:
  - [abstract] "the advantage reverses on general-domain benchmarks, confirming the domain-specific efficacy of the hybrid approach."
  - [section] Section 4.4 shows Cross-Encoder achieves best EM (0.777) and MRR (0.836) on SQuAD-it, while ELERAG underperforms baseline there.
  - [corpus] Domain adaptation challenges for RAG appear across related works in fintech, ORAN, and other specialized domains.
- Break condition: If target domain linguistically resembles pre-training data, cross-encoder may outperform without entity enhancement.

## Foundational Learning

- **Entity Linking**:
  - Why needed here: Disambiguates polysemous terms and provides factual grounding; essential for understanding why semantic similarity alone fails in high-ambiguity domains.
  - Quick check question: Can you explain how entity overlap differs from keyword overlap in retrieval?

- **Dense Retrieval (Bi-Encoders)**:
  - Why needed here: Provides the semantic similarity baseline that ELERAG augments; foundational for understanding the trade-offs between dense-only and hybrid retrieval.
  - Quick check question: Why might two chunks have high cosine similarity but low factual relevance?

- **Reciprocal Rank Fusion (RRF)**:
  - Why needed here: Core fusion mechanism in ELERAG; combines heterogeneous ranking signals without score normalization.
  - Quick check question: What happens to RRF scores if one ranking list is empty or contains only low-ranked items?

## Architecture Onboarding

- **Component map**:
  Chunking -> Embedding (multilingual-e5-large) -> FAISS index -> Entity Linking (SpaCy NER + Wikidata) -> Reciprocal Rank Fusion (dense + entity ranks) -> GPT-4o generation with citation prompt

- **Critical path**:
  1. Pre-process corpus: chunk, embed, store in FAISS; extract and link entities per chunk.
  2. At query time: embed query; extract entities; retrieve top-K dense candidates; compute entity overlap ranks; fuse via RRF; pass top-N chunks to LLM with citation prompt.
  3. LLM outputs answer with cited chunk IDs; use cited chunks as final retrieved set for evaluation.

- **Design tradeoffs**:
  - RRF vs. Weighted-Score: RRF avoids manual weight tuning but requires two complete ranking passes; Weighted-Score is simpler but sensitive to β selection.
  - Cross-Encoder re-ranker: Higher precision on general domains but higher latency and may over-filter on specialized corpora.
  - Entity Linking via API vs. local model: API (Wikidata) is lightweight and multilingual but requires network access; local EL models (e.g., BLINK) may not support Italian well.

- **Failure signatures**:
  - Entity Linking returns empty or incorrect entities → entity-based ranking is noisy or empty → RRF reduces to dense ranking.
  - Cross-Encoder over-aggressively filters relevant chunks → recall drops, especially on specialized terminology.
  - LLM ignores citation instruction → post-generation retrieval metrics become unreliable.

- **First 3 experiments**:
  1. Replicate baseline vs. ELERAG (RRF) on the provided educational benchmark to confirm EM and MRR improvements.
  2. Ablate entity signal: run RRF with only dense ranking (entity score set to 0) to quantify the contribution of entity linking.
  3. Cross-domain validation: test ELERAG and Cross-Encoder on a different general-domain dataset (e.g., SQuAD-it) to reproduce the domain mismatch pattern.

## Open Questions the Paper Calls Out
None

## Limitations
- Domain-specific advantage demonstrated only on single Italian educational corpus; generalization to other specialized domains untested.
- Entity linking pipeline depends on Wikidata coverage and SpaCy's Italian NER accuracy, which may not scale to highly technical or low-resource domains.
- The 10% entity popularity weight in hybrid scoring is fixed without ablation; impact of RRF K=60 parameter unexplored.

## Confidence

- **High**: ELERAG improves retrieval metrics (EM, MRR, Precision@1) over baseline on the tested Italian educational dataset.
- **Medium**: Entity linking via Wikidata + RRF fusion is the primary driver of improvement; Cross-Encoder underperforms on specialized data due to domain mismatch.
- **Low**: The precise linguistic or terminological features of the educational corpus that make entity linking critical are not identified; the 10% popularity weight is not justified or ablated.

## Next Checks

1. **Cross-Domain Replication**: Test ELERAG and Cross-Encoder on at least two additional specialized datasets (e.g., biomedical or legal Italian corpora) to confirm domain-specific advantage generalizes beyond the original educational benchmark.
2. **Entity Linking Ablation**: Run RRF with entity scores disabled (α=0) to quantify the exact contribution of entity linking to EM and MRR improvements; report entity recall and precision to diagnose failure modes.
3. **Linguistic Characterization**: Analyze the educational corpus for polysemy, named entity density, and terminology overlap with Wikidata to identify which query types benefit most from entity linking; correlate these features with performance gains.