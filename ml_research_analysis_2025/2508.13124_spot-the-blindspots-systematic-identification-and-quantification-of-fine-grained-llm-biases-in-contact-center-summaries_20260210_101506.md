---
ver: rpa2
title: 'Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained
  LLM Biases in Contact Center Summaries'
arxiv_id: '2508.13124'
source_url: https://arxiv.org/abs/2508.13124
tags:
- bias
- summary
- agent
- transcript
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BlindSpot, a framework to identify and quantify
  operational biases in LLM-generated call summaries. It defines a taxonomy of 15
  bias dimensions across five classes (e.g., sentiment, speaker, solution) and uses
  an LLM labeler to compare categorical distributions between transcripts and summaries,
  quantifying bias via Jensen-Shannon divergence and coverage metrics.
---

# Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries

## Quick Facts
- arXiv ID: 2508.13124
- Source URL: https://arxiv.org/abs/2508.13124
- Authors: Kawin Mayilvaghanan; Siddhant Gupta; Ayush Kumar
- Reference count: 40
- Key outcome: BlindSpot framework identifies systematic LLM summarization biases across 15 dimensions; mitigation prompts improve coverage by up to 4.87%

## Executive Summary
This paper introduces BlindSpot, a systematic framework for identifying and quantifying fine-grained biases in LLM-generated call summaries. The framework defines 15 bias dimensions across five classes (sentiment, speaker, solution, category, detail) and uses an LLM labeler to compare categorical distributions between transcripts and summaries, quantifying bias via Jensen-Shannon divergence and coverage metrics. Applied to 2,500 contact-center transcripts summarized by 20 LLMs, the study reveals that biases are systematic and present across all models, regardless of size or family. Temporal sequence, entity type, and information repetition are the most challenging dimensions, with many models underrepresenting resolution steps and agent rapport. Notably, holistic quality metrics weakly correlate with bias, underscoring the need for targeted evaluation. A mitigation prompt based on the analysis successfully reduced bias in nine models, increasing coverage by up to 4.87% and lowering divergence by 0.012 JSD, demonstrating the framework's practical value for improving summarization fidelity.

## Method Summary
The BlindSpot framework employs a two-stage approach: first, a taxonomy of 15 bias dimensions is defined across five classes (sentiment, speaker, solution, category, detail). An LLM labeler (GPT-4o) annotates both transcripts and summaries with these dimensions, and categorical distributions are compared using Jensen-Shannon divergence and coverage metrics to quantify bias. The framework is then applied to 2,500 contact-center transcripts summarized by 20 different LLMs. To address identified biases, a mitigation prompt is developed based on the analysis, which successfully reduces bias in nine models by improving coverage and lowering divergence. Manual validation of the labeler was conducted on 16 transcripts to ensure reliability.

## Key Results
- Biases are systematic and present across all 20 tested LLMs, regardless of model size or family
- Temporal sequence, entity type, and information repetition are the most challenging bias dimensions
- Many models underrepresent resolution steps and agent rapport in summaries
- Holistic quality metrics show weak correlation with bias detection
- Mitigation prompt improves coverage by up to 4.87% and reduces divergence by 0.012 JSD in nine models

## Why This Works (Mechanism)
The framework works by systematically mapping the gap between transcript content and summary representation through fine-grained categorical labeling. By quantifying these gaps using statistical divergence measures, it reveals systematic underrepresentation patterns that traditional quality metrics miss. The mitigation prompt leverages these insights to explicitly guide LLMs toward more complete representation of identified bias dimensions.

## Foundational Learning
1. Jensen-Shannon Divergence: Measures similarity between two probability distributions; needed to quantify bias as distributional differences between transcripts and summaries. Quick check: values range 0-1, with higher values indicating greater divergence.
2. Categorical Distribution Comparison: Enables systematic evaluation of content representation across multiple dimensions. Quick check: ensure distributions are properly normalized before comparison.
3. Fine-grained Bias Taxonomy: 15 dimensions across 5 classes provide comprehensive bias coverage. Quick check: validate taxonomy comprehensiveness through domain expert review.
4. LLM-based Labeling: Uses GPT-4o to annotate transcripts/summaries consistently. Quick check: manual validation on sample data to verify labeler reliability.
5. Coverage Metrics: Quantify the extent to which summary content matches transcript content. Quick check: calculate both absolute and relative coverage across dimensions.
6. Prompt-based Mitigation: Leverages identified biases to construct targeted improvement prompts. Quick check: test prompt effectiveness across different model families.

## Architecture Onboarding

Component map: Transcripts -> LLM Labeler -> Categorical Distributions -> JSD/Coverage Metrics -> Bias Identification -> Mitigation Prompt -> Improved Summaries

Critical path: The pipeline's core is the comparison between transcript and summary distributions, with JSD serving as the primary signal for bias detection.

Design tradeoffs: Uses a single LLM labeler for consistency but risks introducing labeler bias; focuses on categorical distributions which may miss nuanced language biases; domain-specific dataset limits generalizability.

Failure signatures: High JSD values indicate significant content gaps; low coverage suggests systematic underrepresentation; weak correlation with quality metrics reveals blind spots in traditional evaluation.

First experiments:
1. Validate labeler consistency by comparing annotations across multiple runs on the same data
2. Test framework on non-contact-center domains to assess generalizability
3. Compare single-labeler vs multi-labeler bias detection to assess reliability

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Single LLM labeler dependency may introduce bias artifacts rather than detecting true representation gaps
- Limited manual validation scope (16 transcripts) may not capture full reliability across all bias dimensions
- Single-domain dataset (contact-center) limits generalizability to other summarization contexts
- Categorical distribution approach may miss nuanced language-based biases

## Confidence

High confidence:
- Systematic identification of bias dimensions across five classes
- Methodology for quantifying bias via Jensen-Shannon divergence and coverage metrics

Medium confidence:
- Pervasiveness of biases across all tested models (limited model diversity, single-domain dataset)
- Weak correlation between holistic quality metrics and bias (context-dependent relationship)
- Effectiveness of mitigation prompt (limited scope, potential variability across models)

## Next Checks
1. Replicate bias detection using multiple independent LLM labelers (e.g., Claude, Gemini) to assess consistency and rule out labeler-specific biases
2. Test BlindSpot framework on summarization tasks from other domains (e.g., legal, medical, or news) to evaluate generalizability and identify domain-specific bias patterns
3. Conduct ablation studies on the mitigation prompt to determine which components are most effective and whether alternative prompt strategies yield better results across a broader range of models