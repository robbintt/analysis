---
ver: rpa2
title: 'AI Agents for the Dhumbal Card Game: A Comparative Study'
arxiv_id: '2510.11736'
source_url: https://arxiv.org/abs/2510.11736
tags:
- performance
- jhyap
- rate
- agent
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates AI agents for Dhumbal, a culturally significant
  multiplayer card game with imperfect information. The research implements and compares
  rule-based, search-based, and learning-based AI agents, including heuristic approaches
  (Aggressive, Conservative, Balanced, Opportunistic), Monte Carlo Tree Search (MCTS),
  Information Set Monte Carlo Tree Search (ISMCTS), Deep Q-Network (DQN), and Proximal
  Policy Optimization (PPO).
---

# AI Agents for the Dhumbal Card Game: A Comparative Study

## Quick Facts
- arXiv ID: 2510.11736
- Source URL: https://arxiv.org/abs/2510.11736
- Reference count: 24
- Primary result: Rule-based Aggressive agent achieved 88.3% win rate in Dhumbal card game simulations

## Executive Summary
This study evaluates AI agents for Dhumbal, a culturally significant multiplayer card game with imperfect information. The research implements and compares rule-based, search-based, and learning-based AI agents, including heuristic approaches (Aggressive, Conservative, Balanced, Opportunistic), Monte Carlo Tree Search (MCTS), Information Set Monte Carlo Tree Search (ISMCTS), Deep Q-Network (DQN), and Proximal Policy Optimization (PPO). Through systematic within-category and cross-category tournaments involving 1024 simulated rounds, the rule-based Aggressive agent achieved the highest win rate of 88.3% (95% CI: [86.3, 90.3]), significantly outperforming ISMCTS (9.0%) and PPO (1.5%). The Aggressive agent demonstrated exceptional economic performance (70.1 coins/round) and Jhyap success rate (92.6%), highlighting the effectiveness of domain-specific heuristics in handling imperfect information.

## Method Summary
The study implements a comprehensive Dhumbal simulation environment and evaluates six AI agent categories through systematic tournaments. Within-category tournaments compare agents of the same type (e.g., Aggressive vs Conservative heuristic agents), while cross-category tournaments assess performance across different agent architectures. Each tournament consists of 1024 rounds with varying player counts (2-6), measuring win rates, economic performance (coins/round), and Jhyap success rates. The evaluation includes rule-based agents with domain-specific heuristics, search-based agents (MCTS and ISMCTS) for imperfect information handling, and learning-based agents (DQN and PPO) trained through reinforcement learning. Statistical significance is assessed using 95% confidence intervals, and performance metrics are aggregated across multiple game configurations.

## Key Results
- Rule-based Aggressive agent achieved 88.3% win rate (95% CI: [86.3, 90.3]), outperforming all other agents
- Aggressive agent demonstrated superior economic performance at 70.1 coins/round and 92.6% Jhyap success rate
- Learning-based agents (DQN and PPO) showed poor performance, with PPO achieving only 1.5% win rate

## Why This Works (Mechanism)
The Aggressive agent's success appears to stem from its optimized heuristic evaluation function that effectively balances risk and reward in Dhumbal's imperfect information environment. The agent's ability to consistently achieve Jhyap (empty hand) while minimizing point accumulation suggests superior hand management and opponent prediction capabilities. The domain-specific heuristics outperform general-purpose approaches like MCTS and learning algorithms, likely because they encode crucial game-specific knowledge about when to discard high-value cards and how to anticipate opponent moves based on visible information.

## Foundational Learning
- Imperfect information games: Understanding how to handle hidden information in multiplayer games where players cannot see all cards
- Domain-specific heuristics: Learning how game-specific knowledge can outperform general AI approaches in culturally specific games
- Monte Carlo Tree Search: Understanding search algorithms that can handle uncertainty in game states
- Reinforcement learning for card games: Exploring how neural networks can learn game strategies through self-play
- Statistical significance testing: Applying confidence intervals to validate performance differences between AI agents

## Architecture Onboarding
- Component map: Rule-based agents -> Heuristic functions -> Game state evaluation; Learning agents -> Neural networks -> Policy/value functions; Search agents -> Tree search -> Rollout simulations
- Critical path: Game state representation -> Agent decision-making -> Action execution -> State update -> Reward calculation
- Design tradeoffs: Rule-based agents offer interpretability and high performance but lack adaptability; learning agents can adapt but require extensive training and show poor initial results
- Failure signatures: Learning agents fail due to insufficient training data or poor reward signal design; search agents struggle with computational complexity in imperfect information settings
- First experiments: 1) Compare Aggressive agent performance against human players; 2) Test learning agents with different reward structures; 3) Evaluate agent performance with varying deck sizes

## Open Questions the Paper Calls Out
- How do different rule variations of Dhumbal affect agent performance across categories?
- What specific architectural modifications could improve learning-based agent performance?
- Can hybrid approaches combining rule-based heuristics with learning components achieve better results?
- How does agent performance scale with increasing player count beyond 6 players?
- What are the computational resource requirements for real-time deployment of different agent types?

## Limitations
- Rule-based approach may be highly optimized for specific Dhumbal variant and may not generalize to other imperfect-information games
- Learning-based agents showed poor performance, but study provides limited insight into architectural or training deficiencies
- Simulation environment's fidelity to real-world gameplay dynamics and edge cases is not thoroughly validated

## Confidence
High: Comparative methodology and statistical analysis of agent performance within controlled simulation environment
Medium: Economic performance metrics and Jhyap success rates as indicators of agent quality
Low: Practical applicability to real-world Dhumbal play and scalability of rule-based approach to other imperfect-information games

## Next Checks
1. Implement cross-validation testing with multiple Dhumbal rule variants to assess robustness of Aggressive agent's dominance
2. Conduct ablation studies on learning-based agents to identify specific architectural or training deficiencies
3. Design human-versus-AI tournament to compare simulated performance against actual gameplay behavior and strategy adaptation