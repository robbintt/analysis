---
ver: rpa2
title: The Most Important Features in Generalized Additive Models Might Be Groups
  of Features
arxiv_id: '2506.19937'
source_url: https://arxiv.org/abs/2506.19937
tags:
- importance
- group
- features
- feature
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for computing group importance for
  Generalized Additive Models (GAMs) that naturally extends individual feature importance
  by averaging the absolute sum of feature contributions in a group. The approach
  is computationally efficient, requires no model retraining, allows posthoc group
  definitions (including overlapping groups), and is meaningful in high-dimensional
  settings.
---

# The Most Important Features in Generalized Additive Models Might Be Groups of Features

## Quick Facts
- **arXiv ID:** 2506.19937
- **Source URL:** https://arxiv.org/abs/2506.19937
- **Reference count:** 40
- **Primary result:** A computationally efficient method for computing group importance in GAMs that reveals correlated feature groups often carry more predictive power than individual features, especially in multimodal data.

## Executive Summary
This paper introduces a method to compute group importance for Generalized Additive Models (GAMs) that naturally extends individual feature importance. By averaging the absolute sum of feature contributions within a group, the approach captures joint predictive power that individual importance misses when features are correlated. The method requires no model retraining, allows posthoc group definitions (including overlapping groups), and is meaningful in high-dimensional settings. Experiments on synthetic and real-world datasets demonstrate that groups of correlated features often carry more predictive power than individual features, particularly in multimodal data. In neuroscience, behavioral and trauma-related groups were most important for predicting depression. In healthcare, community-level social determinants of health were far more predictive of mortality after hip replacement than individual factors like age or race.

## Method Summary
The method computes group importance as the mean absolute sum of shape function contributions for features within a group: $I_G = \frac{1}{|T|} \sum_{t \in T} |f_{i1}(t_{i1}) + \dots + f_{ik}(t_{ik})|$. This extends individual feature importance (mean absolute contribution) to groups while preserving key properties like the triangle inequality. The approach leverages EBM's purification to ensure shape functions encode only main effects, allowing meaningful group importance calculations without retraining. Groups can be defined posthoc based on domain knowledge (e.g., modalities, semantic categories) and can overlap. The method's computational complexity is $O(k \cdot |T|)$, making it efficient for large datasets.

## Key Results
- In synthetic experiments, group importance correctly captures additive signal shared by correlated features (I{x,z} ≈ 2Ix when z = x) and shows cancellation for conflicting signals.
- Applied to neuroscience data, the Life & Trauma Events group ranked #1 in importance despite only 5 of its features being in the top 20 individual features.
- Applied to healthcare data, community-level social determinants of health were far more predictive of mortality after hip replacement than individual factors like age or race.

## Why This Works (Mechanism)

### Mechanism 1
Aggregating absolute joint contributions captures signal that individual feature importance misses when features are correlated. For a group G = {xi1,...,xik}, the method computes IG = (1/|T|) Σ|fi1(ti1) + ... + fik(tik)|. When correlated features share additive signal (e.g., z = x), each shape function learns half the total signal, so I{x,z} ≈ 2Ix. This recovers the joint predictive power that gets "split" across correlated features during EBM training. Core assumption: The GAM's shape functions faithfully encode main effects via purification, and zero-centering ensures symmetric attribution. Evidence: Synthetic experiment with z = x shows I{x,z} ≈ 2Ix for additive signals. Break condition: If features in a group have opposing signals and are highly correlated (e.g., x and z with z ≈ x but y = 1 if x > z), shape functions cancel: fx(tx) ≈ -fz(tz), yielding I{x,z} < Ix.

### Mechanism 2
Triangle inequality bounds group importance between 0 and the sum of individual importances, preventing artificial inflation from group size. By definition, 0 ≤ I{xi1,...,xik} ≤ Ixi1 + ... + Ixik. This arises from computing |sum of contributions| per sample before averaging, rather than summing |individual contributions|. The alternative definition (summing individual importances) would artificially inflate larger groups. Core assumption: Shape functions are zero-centered, so a feature's contribution can be positive or negative depending on its value. Evidence: Explicitly states the triangle inequality property and rejects the alternative |fi1| + ... + |fik| formulation. Break condition: In pathological settings with extreme feature interactions not captured by main effects, the bound could theoretically be violated if the GAM poorly approximates the true data-generating process.

### Mechanism 3
Comparing group importance to total importance (group of all features) provides a relative scale analogous to explained variance. Compute I{all features} as the denominator. For weakly correlated features, I{x1,...,xn} ≈ Ix1 + ... + Ixn, mimicking variance decomposition. Relative group importance = IG / I{all features}. Core assumption: Features are weakly correlated (ideally uncorrelated) for the parallel to explained variance to hold precisely. Evidence: Draws explicit parallel to PCA's explained variance, noting the approximation holds better for weakly correlated features. Break condition: If features are highly correlated with complex dependencies, I{all features} may not represent a meaningful "total," and relative comparisons become harder to interpret.

## Foundational Learning

- **Concept: Generalized Additive Models (GAMs)**
  - **Why needed here:** The entire method assumes the model has the form g(E[y]) = β0 + Σfi(xi) + Σfi,j(xi,xj). Without understanding that shape functions encode main effects separately from interactions, the group importance formula makes no sense.
  - **Quick check question:** If a model is not additive (e.g., deep neural network with hidden layers), can you directly apply this group importance formula? (Answer: No—the separability of contributions is essential.)

- **Concept: Feature importance as mean absolute contribution**
  - **Why needed here:** The group importance formula extends the individual importance definition Ixj = (1/|T|) Σ|fj(tj)|. Understanding this base case is prerequisite to understanding why summing contributions first, then taking absolute value, differs from summing absolute values.
  - **Quick check question:** Why use absolute value rather than raw contribution? (Answer: Zero-centered shape functions mean high and low risk both contribute positively to importance; without absolute value, they'd cancel.)

- **Concept: Purification and functional ANOVA decomposition**
  - **Why needed here:** EBMs use purification to ensure each fk(xk) encodes only the main effect, with interactions moved to fi,j terms. This guarantees that group importance on main effects is meaningful and not confounded by interaction terms.
  - **Quick check question:** If purification were not applied and main effects were "polluted" by interaction signal, what would happen to group importance calculations? (Answer: Group importance would conflate main effects with interactions, making interpretation ambiguous.)

## Architecture Onboarding

- **Component map:** EBM Training Module -> Shape Function Extractor -> Group Definition Layer -> Group Importance Computer -> Relative Importance Normalizer
- **Critical path:**
  1. Train EBM on full dataset with interactions enabled (optional but recommended).
  2. Verify purification is active (default in InterpretML).
  3. Define groups based on domain knowledge (e.g., modality, semantic category).
  4. For each group, sum shape function outputs per sample, take absolute value, average across samples.
  5. Rank groups; optionally normalize by total importance.

- **Design tradeoffs:**
  - **Bin count vs. precision:** Default 256 bins may miss fine-grained effects. Increase to 4096 for high-precision regimes, at cost of longer training.
  - **Including vs. excluding interactions:** The paper focuses on main effects for group importance. If interaction terms fi,j are relevant to a group, they must be explicitly added to the sum.
  - **Overlapping vs. disjoint groups:** Overlapping groups are allowed but complicate interpretation—the same feature contributes to multiple group importances.

- **Failure signatures:**
  1. **Group importance >> sum of individual importances:** Should never happen; check for implementation error in absolute value placement.
  2. **Group importance near zero for obviously important group:** Likely features have strongly conflicting signals and high correlation; inspect shape functions for opposing patterns.
  3. **Inconsistent rankings vs. permutation importance:** Expected to differ—permutation importance measures performance drop, this measures contribution magnitude.

- **First 3 experiments:**
  1. **Sanity check with perfect correlation:** Create synthetic data where z = x, train EBM, verify I{x,z} ≈ 2Ix. Confirms additive signal handling.
  2. **Conflicting signal test:** Create x, z with y = 1 if x > z, vary correlation from 0 to 1. Plot I{x,z} vs. correlation; should show decline toward zero as correlation increases.
  3. **Real data group comparison:** On NCANDA or similar multimodal dataset, compute both individual feature importance and group importance for semantically defined groups. Identify at least one group where individual features rank low but group ranks high.

## Open Questions the Paper Calls Out
1. **Learning Optimality:** What theoretical assumptions regarding "learning optimality" are necessary to guarantee that a group's importance does not exceed the total importance of all features in the model? The paper identifies this as a potential theoretical inconsistency but does not define the specific conditions under which it occurs or how to prevent it mathematically.

2. **Interaction Terms:** How does the exclusion of explicit interaction terms (fi,j) from the group importance calculation affect the accuracy of importance rankings for feature groups with strong non-linear dependencies? While the method accounts for main effects, it is unclear if ignoring interaction terms leads to underestimation for features that primarily influence the target through joint effects.

3. **Non-additive Models:** Can this computationally efficient definition of group importance be adapted for non-additive models (e.g., deep neural networks) without relying on separable shape functions? The method's efficiency stems from the GAM structure; it is currently unknown if the "mean absolute sum of contributions" approach retains its properties when applied to complex, non-separable black-box models.

4. **Method Comparison:** To what extent do different group importance metrics (e.g., additive contribution vs. permutation) converge when analyzing datasets with high multicollinearity but opposing signals? The authors note discrepancies in rankings between their method and Grouped Permutation Importance but leave open which ranking better represents the "ground truth" importance.

## Limitations
- The paper does not explore group importance for interaction terms fi,j, leaving a gap in understanding joint effects beyond main effects.
- Performance comparisons with permutation importance are mentioned but not deeply analyzed—the two methods answer different questions, yet practical tradeoffs remain unclear.
- Implementation details for preprocessing (e.g., scanner standardization in NCANDA, exact binning for high-precision regimes) are underspecified.

## Confidence
- **High confidence:** The mathematical formulation (Eq. 5) is correct and well-justified by purification and zero-centering assumptions. Synthetic experiments cleanly demonstrate additive and conflicting signal behaviors.
- **Medium confidence:** Real-world applications (NCANDA, PHC4) show intuitive group rankings, but results depend on EBM's shape function quality and preprocessing steps not fully detailed.
- **Low confidence:** The parallel to explained variance (Section 2.4) is conceptually appealing but not rigorously validated; pathological cases where IG > I{all features} are acknowledged but not explored.

## Next Checks
1. **Sanity check with perfect correlation:** Create synthetic data where z = x, train EBM, verify I{x,z} ≈ 2Ix. Confirms additive signal handling.
2. **Conflicting signal test:** Create x, z with y = 1 if x > z, vary correlation from 0 to 1. Plot I{x,z} vs. correlation; should show decline toward zero as correlation increases.
3. **Real data group comparison:** On NCANDA or similar multimodal dataset, compute both individual feature importance and group importance for semantically defined groups. Identify at least one group where individual features rank low but group ranks high.