---
ver: rpa2
title: Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End Registration
arxiv_id: '2507.06067'
source_url: https://arxiv.org/abs/2507.06067
tags:
- cbct
- multimodal
- dataset
- registration
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an end-to-end multimodal synthetic CT (sCT)
  generation framework that integrates a Spatial Transformer Network (STN) for alignment
  between intraoperative cone-beam CT (CBCT) and preoperative CT. The proposed approach
  combines 3D U-Net with early fusion and STN-based registration, enabling joint optimization
  of alignment and synthesis.
---

# Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End Registration

## Quick Facts
- arXiv ID: 2507.06067
- Source URL: https://arxiv.org/abs/2507.06067
- Reference count: 40
- Primary result: STN-based registration improves sCT quality in 79/90 evaluation settings, especially for low-quality CBCT with moderate misalignment

## Executive Summary
This work introduces an end-to-end multimodal synthetic CT (sCT) generation framework that integrates a Spatial Transformer Network (STN) for alignment between intraoperative cone-beam CT (CBCT) and preoperative CT. The proposed approach combines 3D U-Net with early fusion and STN-based registration, enabling joint optimization of alignment and synthesis. Experiments on synthetic and real-world clinical datasets demonstrate that the STN consistently improves sCT quality, outperforming baseline multimodal methods in 79 out of 90 evaluation settings. The improvement is most significant when CBCT quality is low and preoperative CT is moderately misaligned. Notably, the STN also enhances perceptual quality even in well-aligned scenarios. These findings highlight the potential of learnable registration to improve intraoperative imaging workflows and support clinical deployment.

## Method Summary
The method employs a 3D U-Net with early fusion of aligned CT and CBCT volumes, where alignment is achieved through an STN module. The STN predicts affine transformation parameters to warp the preoperative CT into alignment with the intraoperative CBCT. The network is trained end-to-end with a combined loss function incorporating MAE, SSIM, and VGG-based perceptual loss, plus a registration loss to guide the STN. The approach is evaluated across three public datasets with controlled artificial misalignment and varying CBCT quality levels.

## Key Results
- STN-based registration improves sCT quality in 79 out of 90 evaluation settings across three datasets
- Performance gains are most pronounced for low-quality CBCT with moderate misalignment (αa ≤ 0.5)
- The STN consistently enhances perceptual quality (sharpness) even when misalignment is minimal
- Degradation occurs at extreme misalignment levels (αa = 1), suggesting limitations of affine registration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating a learnable registration module (STN) explicitly addresses anatomical misalignment, improving the utility of the preoperative CT in the fusion process.
- **Mechanism:** The STN predicts affine transformation parameters to warp the preoperative CT ($U_{CT}$) into alignment with the intraoperative CBCT ($V_{CBCT}$). This allows the downstream U-Net to process spatially coherent multimodal features rather than trying to infer alignment implicitly through convolutional layers.
- **Core assumption:** The misalignment is primarily rigid or affine-correctable, and the STN can converge on correct parameters despite the noise and artifacts present in the CBCT input.
- **Evidence anchors:**
  - [abstract]: "...integrate a Spatial Transformer Network (STN) for alignment between intraoperative cone-beam CT (CBCT) and preoperative CT."
  - [Methodology and Materials]: "The STN takes the concatenation of $[U_{CT}, V_{CBCT}]$ to estimate affine registration parameters... converting [it] into a sampling grid."
  - [corpus]: Related work (e.g., GLFC, EqDiff-CT) focuses on Mamba or Diffusion architectures for sCT, implying this specific STN-based alignment is a distinct architectural choice, though general sCT generation is a established field.
- **Break condition:** Performance degrades significantly at high misalignment levels ($\alpha_a = 1$), where the paper notes the "STN struggles with extreme spatial transformations."

### Mechanism 2
- **Claim:** Multimodal fusion leverages the complementary strengths of high-quality structural data (CT) and real-time anatomical state (CBCT), particularly enhancing results when the intraoperative scan is low quality.
- **Mechanism:** By concatenating the aligned CT and CBCT volumes along the channel dimension ("early fusion"), the network can hallucinate or fill in missing high-frequency details from the CT while adhering to the current patient geometry provided by the CBCT.
- **Core assumption:** The preoperative CT remains anatomically relevant (i.e., no massive tumor regression or organ resection) such that fusing it adds signal rather than noise.
- **Evidence anchors:**
  - [Introduction]: "CBCT provides real-time anatomical information... while CT offers high-quality structural detail."
  - [Results]: "The addition, MM+STN has the biggest positive effects on low to moderately aligned setups with low intraoperative CBCT visual quality."
  - [corpus]: The corpus neighbors consistently frame sCT generation as a translation task to overcome CBCT artifacts, supporting the premise that CBCT quality is the bottleneck (e.g., "EqDiff-CT", "ARTInp").
- **Break condition:** If the preoperative CT is fundamentally outdated or if the alignment fails, the model may produce "hallucinated structures" (mentioned in Methodology regarding perceptual loss) that do not match the patient's current anatomy.

### Mechanism 3
- **Claim:** Joint optimization of registration and synthesis losses creates a task-specific alignment that outperforms generic registration.
- **Mechanism:** The network minimizes a weighted sum of synthesis losses (MAE, SSIM, Perceptual) and a registration loss ($MSE(V_{CT}, Y)$). This forces the STN to align the CT specifically to minimize the sCT reconstruction error, rather than just matching image pixels in a vacuum.
- **Core assumption:** The gradients from the synthesis loss are stable enough to guide the geometric transformation without causing divergence.
- **Evidence anchors:**
  - [abstract]: "...enabling joint optimization of alignment and synthesis."
  - [Methodology and Materials]: "To tune the STN, an additional registration loss is added, defined as $10^{-3} \cdot MSE(V_{CT}, Y)$."
  - [corpus]: Weak or missing explicit support for this specific loss-weighting strategy in the neighbor corpus; this appears to be a paper-specific configuration.
- **Break condition:** If the registration loss weight ($10^{-3}$) is too high, it might override the perceptual quality needs; if too low, the STN may fail to converge.

## Foundational Learning

- **Concept:** Spatial Transformer Networks (STN)
  - **Why needed here:** This is the core contribution. You must understand how a differentiable module can output grid coordinates to warp an image without breaking the backpropagation chain.
  - **Quick check question:** If the STN outputs an identity transformation, what does the grid sampler output?

- **Concept:** 3D Early Fusion
  - **Why needed here:** The paper concatenates CT and CBCT as input channels. Understanding how 3D convolutions process this combined tensor is vital for debugging input shape mismatches.
  - **Quick check question:** How does concatenating two 3D volumes ($V_{CT}, V_{CBCT}$) affect the input channel dimension of the first 3D convolutional layer?

- **Concept:** Perceptual Loss (VGG)
  - **Why needed here:** The paper relies heavily on perceptual loss ($\alpha_3=0.7$) over pixel-wise loss to maintain "sharpness" and avoid blurry outputs.
  - **Quick check question:** Why would comparing VGG feature maps result in sharper images than comparing raw pixel MAE?

## Architecture Onboarding

- **Component map:** Input: $U_{CT}$ (Moving), $V_{CBCT}$ (Fixed) -> STN: Localisation Net (CNN) → Grid Generator → Sampler. Output: $V_{CT}$ (Aligned CT) -> Fusion: Concat($V_{CT}$, $V_{CBCT}$) → Combined Volume $V$ -> Synthesis: 3D U-Net (Encoder-Bottleneck-Decoder) -> Head: $1 \times 1 \times 1$ Conv → Logits (sCT)

- **Critical path:** The flow of gradients through the **STN Grid Sampler**. If the STN produces a degenerate grid (e.g., collapsing the image), the U-Net receives zero information, and the synthesis loss will not recover.

- **Design tradeoffs:**
  - **Explicit vs. Implicit Registration:** The paper argues that explicit STN registration is superior to the "implicit" alignment a standard U-Net might learn, but notes this comes with fragility to extreme misalignment ($\alpha_a=1$).
  - **Loss Balancing:** The high weight on perceptual loss (0.7) vs MAE (0.2) prioritizes visual realism over exact Hounsfield Unit (HU) accuracy, which may be a tradeoff for clinical dose calculation use cases.

- **Failure signatures:**
  - **Extreme Misalignment:** STN fails to converge, resulting in warped or blank outputs (Break condition for Mechanism 1).
  - **Hallucination:** High perceptual loss weight generates anatomical structures that look real but are not present in the patient (Mitigated but possible).
  - **Blur:** If STN aligns poorly, the fusion averages mismatched features, resulting in double edges or blur.

- **First 3 experiments:**
  1. **Ablation on Synthetic Data:** Train `MM+STN` vs `MM` (no STN) on the CBCTLiTS dataset with fixed $\alpha_a=0.25$ and $\alpha_{np}=32$ (low quality) to reproduce the claim that STN helps most in low-quality/moderate-misalignment regimes.
  2. **Stress Test:** Increment $\alpha_a$ from 0 to 1. Identify the specific voxel displacement (mm) where the STN's MAE performance drops below the unimodal baseline.
  3. **Loss Sensitivity:** Set $\alpha_3$ (perceptual weight) to 0 and retrain. Verify if the model reverts to blurry outputs or if it fails to align the STN due to lack of high-level feature guidance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid approach combining coarse external pre-registration with the proposed STN-based fine-tuning overcome the performance degradation observed in cases of extreme anatomical misalignment ($\alpha_a = 1$)?
- Basis in paper: [explicit] The authors state that "performance declines at high misalignment levels ($\alpha_a = 1$), suggesting that STN struggles with extreme spatial transformations," and explicitly suggest that "external pre-registration... may be necessary to bring inputs into a roughly aligned space."
- Why unresolved: The current study only evaluates the STN in an end-to-end setting without external pre-alignment, revealing a failure mode at high distortion levels that was not corrected within the network's capacity.
- What evidence would resolve it: A comparative study on datasets with extreme misalignment, evaluating whether a cascaded pipeline (traditional registration → STN) recovers the synthesis quality to levels seen at moderate alignment ($\alpha_a \le 0.5$).

### Open Question 2
- Question: Would replacing the affine Spatial Transformer Network with a deformable registration module improve sCT accuracy given the presence of non-rigid anatomical changes (e.g., respiratory motion) mentioned in the paper?
- Basis in paper: [inferred] The authors note that misalignment is caused by "anatomical changes," yet the STN implementation is restricted to "affine transformation" (12 parameters). This suggests a methodological limitation where the registration capacity may not match the complexity of the described physiological deformations.
- Why unresolved: The paper demonstrates that affine alignment improves results, but it does not quantify how much residual non-rigid misalignment remains or how it impacts the final voxel-wise error.
- What evidence would resolve it: An ablation study comparing the current affine STN against a non-rigid (deformable) STN variant on datasets featuring significant organ deformation.

### Open Question 3
- Question: How can the training objective be modified to resolve the trade-off where the STN improves perceptual quality (sharpness) but worsens pixel-wise accuracy (MAE/SSIM), as observed in the SynthRad dataset?
- Basis in paper: [explicit] The authors report that on the SynthRad dataset, "MAE and 1-SSIM metrics worsen in most (70%) cases" while "Perceptual metrics generally improved," leading to "mixed results" despite the visual enhancement of fine structural details.
- Why unresolved: The current loss function (weighted sum of MAE, SSIM, and VGG-based perceptual loss) was empirically tuned but still resulted in inconsistent metric performance across different real-world datasets.
- What evidence would resolve it: Experiments utilizing adaptive loss weighting or adversarial losses to determine if perceptual sharpness can be achieved without sacrificing the structural fidelity measured by MAE and SSIM.

## Limitations
- Performance degrades significantly at extreme misalignment levels (αa = 1), suggesting affine registration has fundamental limits
- Clinical validation is limited to synthetic misalignment experiments; real-world deployment requires prospective trials
- Dependence on anatomical consistency between preoperative and intraoperative scans remains untested for scenarios involving tumor regression or organ resection

## Confidence
- **High confidence:** The STN consistently improves sCT quality across 79/90 evaluation settings, particularly in low-quality CBCT scenarios with moderate misalignment. This is directly supported by quantitative metrics and ablation studies.
- **Medium confidence:** The perceptual quality improvements are genuine, though the specific contribution of the 0.7 perceptual loss weight to visual realism versus potential hallucination requires further validation.
- **Medium confidence:** The claim that joint optimization of registration and synthesis creates task-specific alignment is supported by ablation results, though the specific loss-weighting strategy (10⁻³ registration loss) lacks extensive comparative analysis.

## Next Checks
1. **Extreme misalignment stress test:** Systematically evaluate performance degradation thresholds by testing STN on increasingly severe artificial misalignments (beyond αa=1) to quantify the exact failure point and identify whether parameter regularization could extend the effective range.
2. **Clinical scenario validation:** Test the model on real patient cases with known anatomical changes (post-surgery, tumor regression) to assess whether the fusion approach maintains accuracy when preoperative CT becomes anatomically outdated.
3. **Ablation of perceptual loss weight:** Conduct a sensitivity analysis varying α3 from 0.0 to 1.0 to determine the optimal balance between HU accuracy for dose calculation and perceptual quality for visual assessment.