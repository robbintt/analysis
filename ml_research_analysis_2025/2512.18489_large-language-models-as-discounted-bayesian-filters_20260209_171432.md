---
ver: rpa2
title: Large Language Models as Discounted Bayesian Filters
arxiv_id: '2512.18489'
source_url: https://arxiv.org/abs/2512.18489
tags:
- bayesian
- llms
- evidence
- wang
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a Bayesian filtering framework to evaluate\
  \ online inference in large language models (LLMs) under non-stationary conditions.\
  \ It shows that while LLMs exhibit structured belief updating resembling Bayesian\
  \ inference, they systematically discount older evidence via an exponential forgetting\
  \ mechanism characterized by a model-specific discount factor \u03B3 < 1."
---

# Large Language Models as Discounted Bayesian Filters

## Quick Facts
- arXiv ID: 2512.18489
- Source URL: https://arxiv.org/abs/2512.18489
- Reference count: 0
- LLMs exhibit structured belief updating resembling Bayesian inference with exponential forgetting, quantified by a model-specific discount factor γ < 1

## Executive Summary
This paper introduces a Bayesian filtering framework to evaluate online inference in large language models (LLMs) under non-stationary conditions. Through controlled probabilistic probes, it shows that while LLMs exhibit structured belief updating resembling Bayesian inference, they systematically discount older evidence via an exponential forgetting mechanism characterized by a model-specific discount factor γ < 1. The analysis reveals that predictive errors primarily stem from model misspecification (inherent prior miscalibration) rather than flawed updating, with error decomposition consistently showing update divergence is typically <13% of total error.

## Method Summary
The paper evaluates LLM online inference through two probabilistic probes: a biased die task and Gaussian mean estimation, both with a changepoint at t=51 where underlying parameters shift. For each timestep, the LLM receives observation history and produces a predictive distribution via softmax on output logits. The discount factor γ* is fitted by minimizing KL divergence between LLM predictions and a theoretical discounted Bayesian filter using L-BFGS-B optimization. Error decomposition separates total KL divergence into Update Divergence (LLM vs. fitted filter) and Model Misspecification Divergence (fitted filter vs. ground truth). The study also extracts final-layer attention scores and hidden states for correlation analysis and PCA visualization.

## Key Results
- Fitted discount factors γ* are consistently below 1 across architectures (e.g., Llama-3.1-8B: γ = 0.88 for biased die), confirming exponential forgetting behavior
- Update divergence is typically <13% of total error, indicating LLMs are principled discounted updaters rather than flawed updaters
- Strong negative correlation (ρ = -0.85) between aggregate attention scores and update divergence, suggesting attention modulates inferential fidelity
- Hidden state PCA visualization shows clear clustering around changepoints, providing internal evidence of online adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs implement in-context learning as discounted Bayesian filtering with an intrinsic exponential forgetting mechanism, quantified by a model-specific discount factor γ < 1.
- Mechanism: The model tempers its posterior belief before incorporating new evidence: p'_{t-1}(θ) ∝ [p_{t-1}(θ|D_{1:t-1})]^γ. When γ < 1, older evidence is systematically downweighted, enabling faster adaptation to distributional shifts at the cost of imperfect memory.
- Core assumption: LLM belief trajectories can be approximated by a parametric discounted Bayesian filter with a stable, task-independent γ.
- Evidence anchors: [abstract] "more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one"; [Section 3.1] Formal definition of discounted Bayesian filtering with γ ∈ (0,1]; [corpus] Limited corpus validation; neighboring papers address prior elicitation (LLM-Prior, arXiv:2508.03766) but not discounting mechanisms explicitly
- Break condition: If γ* → 1 across tasks and architectures, the discounting hypothesis fails; if γ* is unstable or task-dependent without pattern, the unified mechanism is unsupported.

### Mechanism 2
- Claim: Predictive errors in LLM online inference stem primarily from model misspecification (miscalibrated priors and intrinsic discounting) rather than flawed belief updating.
- Mechanism: Total KL divergence decomposes into Update Divergence (D_Update = D_KL(p̂_LLM || p_Bayes(γ*))) and Model Misspecification Divergence (D_ModelSpec = D_KL(p_Bayes(γ*) || p_Truth)). Low D_Update indicates principled updating; high D_ModelSpec indicates prior/damping misalignment.
- Core assumption: The discounted Bayesian filter with fitted γ* is a sufficiently expressive reference model for LLM behavior.
- Evidence anchors: [abstract] "predictive errors primarily stem from model misspecification (inherent prior miscalibration) rather than flawed updating"; [Section 4.2] "Update Divergence...is consistently small (typically <13% of total error)"; [corpus] No direct corpus evidence on error decomposition for LLM Bayesian filtering
- Break condition: If D_Update dominates total error across models, the claim that LLMs are "principled discounted updaters" is undermined.

### Mechanism 3
- Claim: Attention allocation modulates inferential quality—greater aggregate attention to past evidence correlates with more stable, principled belief updates.
- Mechanism: Final-layer attention scores on historical context inversely predict step-wise update divergence. The attention mechanism serves as an architectural substrate for controlling evidence integration fidelity.
- Core assumption: Aggregate attention scores meaningfully reflect evidential weighting in belief formation.
- Evidence anchors: [Section 4.3] "strong and highly significant negative correlation (ρ = −0.85, p ≈ 2.6×10⁻³⁵)" for Llama-3.1-8B on Biased Die; [Section 4.3] "attention mechanism thus acts as a primary modulator of inferential fidelity"; [corpus] No corpus evidence directly addressing attention–Bayesian update correlation
- Break condition: If correlation weakens or reverses across architectures/tasks, attention may not be the primary modulator.

## Foundational Learning

- Concept: **Bayesian Filtering / Sequential Inference**
  - Why needed here: The framework models LLM behavior as iteratively updating latent beliefs from streaming evidence—understanding this is prerequisite to interpreting γ, D_Update, and D_ModelSpec.
  - Quick check question: Given prior p(θ) and likelihood p(D_t|θ), write the posterior after observing D_1, D_2. How does γ < 1 modify this?

- Concept: **KL Divergence and Error Decomposition**
  - Why needed here: The paper quantifies LLM–filter deviation and error sources via KL-based decomposition; without this, D_Update vs. D_ModelSpec are opaque.
  - Quick check question: If D_KL(p || q) = 0, what does this imply about p and q? Why use KL rather than L2 for distributional comparison?

- Concept: **Non-stationary Environments and Changepoint Detection**
  - Why needed here: Probes involve parameter shifts (t=51); interpreting adaptation requires understanding why forgetting is adaptive in dynamic settings.
  - Quick check question: In a stationary environment, is γ = 1 always optimal? What tradeoff does γ < 1 introduce?

## Architecture Onboarding

- Component map: Observation history -> LLM -> Logits -> Softmax -> Predictive distribution -> Discount factor optimizer -> Fitted γ* -> Error decomposition module -> D_Update, D_ModelSpec; Attention analyzer extracts final-layer scores; Hidden state visualizer applies PCA + K-Means

- Critical path:
  1. Define task (Biased Die / Gaussian Mean) with known changepoint
  2. Elicit p̂_LLM,t at each timestep via prompted inference
  3. Fit γ* via KL minimization
  4. Decompose error; correlate attention; visualize hidden states

- Design tradeoffs:
  - **Task simplicity vs. ecological validity**: Probes are controlled (dice, Gaussian) but may not generalize to naturalistic reasoning
  - **Fitted γ vs. mechanistic interpretability**: γ* is descriptive, not causal; architectural origin of discounting remains inferred
  - **Final-layer focus**: Attention and hidden state analyses use final layer only—earlier layers may contribute differently

- Failure signatures:
  - γ* ≈ 1 with high total error → LLM not adapting; possibly context window saturation or prior dominance
  - High D_Update → updating process is inconsistent; may indicate architectural instability or prompt sensitivity
  - Weak attention–divergence correlation → attention may not be primary mechanism; investigate other components (MLP layers, layer norms)

- First 3 experiments:
  1. **Cross-architecture γ* profiling**: Run the full probe suite on 3+ model families (Llama, Mistral, Gemma) and compare fitted γ* values; hypothesis: instruction-tuned models show lower γ* (stronger discounting) as suggested by initial results.
  2. **Prior recalibration via prompting**: Test the authors' proposed prompting strategies for prior correction; measure reduction in D_ModelSpec with minimal prompt modifications.
  3. **Attention ablation study**: Mask or perturb specific attention heads and measure impact on D_Update and γ*; hypothesis: heads with high historical attention contribute most to inferential fidelity.

## Open Questions the Paper Calls Out

- What architectural and training factors determine the model-specific discount factor γ, and why do instruction-tuned models consistently exhibit stronger discounting (lower γ) than base models?
  - Basis in paper: [explicit] The paper observes that "instruction-tuned models exhibit lower γ∗ (stronger discounting), and each model family shows a characteristic discounting rate across tasks" but provides no mechanistic explanation.
  - Why unresolved: The paper quantifies γ variation across architectures but does not investigate whether the cause lies in pretraining data, instruction-tuning objectives, architectural differences (attention heads, layers), or optimization procedures.
  - What evidence would resolve it: Ablation studies varying individual factors (training data, tuning procedures, architectural components) while holding others constant; analysis of γ across training checkpoints; correlation of γ with specific architectural hyperparameters.

## Limitations

- The discounting mechanism is inferred from fit rather than mechanistically derived, leaving open whether γ* reflects architectural constraints or prompt artifacts
- The restricted probe scope (biased die, Gaussian mean) may not generalize to richer, real-world inference tasks
- Attention–update correlation is strong but correlational, not causal; the architectural origin of discounting remains opaque

## Confidence

- **High**: LLMs implement structured belief updating resembling Bayesian inference; error decomposition consistently shows D_Update < D_ModelSpec across models
- **Medium**: Discount factor γ* < 1 systematically across architectures; aggregate attention scores inversely correlate with update divergence
- **Low**: Attention is the primary modulator of inferential fidelity; fitted γ* is a stable, task-independent descriptor of LLM discounting behavior

## Next Checks

1. **Cross-Architecture γ* Profiling**: Test Llama, Mistral, and Gemma families on both probes; quantify variance in fitted γ* and test for systematic differences between base and instruction-tuned variants

2. **Attention Ablation Study**: Systematically mask high-attention heads and measure impact on D_Update, γ*, and belief trajectory fidelity; validate whether attention is causal rather than merely correlated

3. **Prior Recalibration Experiment**: Apply minimal prompt modifications (e.g., "Assume a uniform prior on die faces" or explicit prior statements for Gaussian) and measure reduction in D_ModelSpec without altering model weights