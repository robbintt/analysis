---
ver: rpa2
title: 'Prompting Science Report 4: Playing Pretend: Expert Personas Don''t Improve
  Factual Accuracy'
arxiv_id: '2512.05858'
source_url: https://arxiv.org/abs/2512.05858
tags:
- expert
- gemini
- flash
- baseline
- chemistry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated whether assigning expert or low-knowledge
  personas to large language models improves performance on difficult multiple-choice
  questions. Six models were tested on GPQA Diamond and MMLU-Pro benchmarks using
  domain-specific, adjacent, and unrelated expert personas, as well as low-knowledge
  personas (layperson, young child, toddler).
---

# Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy

## Quick Facts
- arXiv ID: 2512.05858
- Source URL: https://arxiv.org/abs/2512.05858
- Reference count: 0
- Primary result: Expert persona prompts do not reliably improve LLM factual accuracy on challenging multiple-choice questions

## Executive Summary
This study systematically evaluated whether assigning expert or low-knowledge personas to large language models improves performance on difficult multiple-choice questions. Testing six models across GPQA Diamond (PhD-level) and MMLU-Pro benchmarks, the research found that persona prompts generally had no significant effect on accuracy compared to a no-persona baseline. Expert personas showed no consistent benefit, with few exceptions (e.g., Gemini 2.0 Flash on MMLU-Pro). Domain-mismatched expert personas sometimes degraded performance, and low-knowledge personas often reduced accuracy, particularly the "toddler" persona. Overall, persona assignment is not a reliable method for improving factual accuracy in LLMs.

## Method Summary
The study employed zero-shot prompting to evaluate six models (GPT-4o, GPT-4o-mini, o3-mini, o4-mini, Gemini 2.0 Flash, Gemini 2.5 Flash) on GPQA Diamond (198 questions, 4 choices) and MMLU-Pro subset (300 questions: 100 each from engineering, law, chemistry, 10 choices). Each question-model-prompt combination received 25 independent samples at temperature 1.0. The system prompt established the assistant role, and user prompts prepended one-sentence persona text before questions. Primary metric was Average Rating (fraction correct averaged over 25 trials), with secondary analysis of 100%/90%/51% correctness thresholds. Paired bootstrap-permutation tests (5,000 replicates) compared each persona condition to baseline.

## Key Results
- Expert personas showed no consistent benefit on either benchmark, with only isolated exceptions
- Domain-mismatched expert personas sometimes degraded performance compared to baseline
- Low-knowledge personas ("layperson," "young child," "toddler") generally reduced accuracy, with "toddler" showing the most consistent negative effects
- The "toddler" persona reduced accuracy across all models and benchmarks tested

## Why This Works (Mechanism)
None provided in source material.

## Foundational Learning
- Zero-shot prompting: why needed - tests general capabilities without task-specific fine-tuning; quick check - can the model complete tasks without examples?
- Bootstrap-permutation testing: why needed - provides statistical significance for paired comparisons with non-normal distributions; quick check - do p-values indicate significant differences between conditions?
- Temperature sampling: why needed - controls randomness in model outputs for evaluation; quick check - does higher temperature increase variability as expected?

## Architecture Onboarding
**Component Map:** GPQA/MMLU-Pro datasets -> Persona prompts -> LLMs (6 models) -> 25 samples @ T=1.0 -> Answer parsing -> Average Rating computation -> Bootstrap-permutation testing

**Critical Path:** Dataset selection → Persona prompt application → 25-sample generation → Answer extraction → Statistical comparison

**Design Tradeoffs:** High temperature (T=1.0) increases result variability but better samples the model's response distribution; 25 trials per condition balances statistical power against computational cost; brief one-sentence personas minimize prompt engineering complexity but may limit effectiveness

**Failure Signatures:** Answer format deviations requiring robust parsing; model refusals (particularly Gemini Flash on mismatched domains); API version changes affecting model behavior over time

**First Experiments:** 1) Run baseline (no persona) with 25 samples to establish reference performance; 2) Test one expert persona on one model to verify implementation; 3) Verify answer parsing correctly extracts "The correct answer is (X)" format

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- High temperature setting (T=1.0) introduces substantial variability requiring 25 trials per condition
- Evaluation limited to multiple-choice questions, leaving open whether persona effects differ for open-ended tasks
- Brief one-sentence personas may be less effective than more elaborate persona descriptions used in other research

## Confidence
**High Confidence**: Expert personas generally do not improve accuracy on challenging multiple-choice questions (supported by consistent results across multiple models and benchmarks)
**Medium Confidence**: Domain-mismatched expert personas sometimes degrade performance (supported but requires caution due to high temperature variability)
**Low Confidence**: Absence of positive effects for any expert persona should not be interpreted as personas never helping (study's narrow focus on factual accuracy in multiple-choice settings)

## Next Checks
1. Replicate with lower temperature (T=0.1) to reduce stochastic variability and detect subtle persona effects
2. Test open-ended task formats using the same persona prompting methodology on GPQA Diamond and MMLU-Pro
3. Obtain exact MMLU-Pro question IDs used in the study or verify the selected questions match the reported category distribution (100 engineering, 100 law, 100 chemistry)