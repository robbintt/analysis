---
ver: rpa2
title: 'Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic
  Framework for Server-Light DNN Inference over Massively Distributed Clients via
  Training-Free Intermediate Feature Compression'
arxiv_id: '2511.11608'
source_url: https://arxiv.org/abs/2511.11608
tags:
- inference
- split
- compression
- latency
- slicer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability and efficiency challenges
  in edge-cloud model partitioning (MP) for DNN inference, particularly in autoregressive
  (AR) LLM scenarios. The authors introduce SLICER, a retraining-free, architecture-agnostic
  framework that compresses intermediate features (IFs) to reduce communication and
  server load in split computing.
---

# Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression

## Quick Facts
- **arXiv ID:** 2511.11608
- **Source URL:** https://arxiv.org/abs/2511.11608
- **Reference count:** 40
- **Primary result:** SLICER achieves up to 10x reduction in uplink volume and 4.4x reduction in server GPU time while maintaining task quality within 0-3 percentage points of baseline across vision and LLM workloads

## Executive Summary
This paper addresses the scalability and efficiency challenges in edge-cloud model partitioning (MP) for DNN inference, particularly in autoregressive (AR) LLM scenarios. The authors introduce SLICER, a retraining-free, architecture-agnostic framework that compresses intermediate features (IFs) to reduce communication and server load in split computing. SLICER combines asymmetric top-K filtering (ATKF) for sparsity, magnitude-splitting (MS) for grouping non-zeros, and adaptive bit quantization (ABQ) for bitwidth selection under a distortion budget. Evaluated across vision and LLM workloads, SLICER achieves up to 10x reduction in uplink volume and 4.4x reduction in server GPU time, while maintaining task quality within 0-3 percentage points of baseline. The framework scales well in multi-device settings and AR LLMs, shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic.

## Method Summary
SLICER is a retraining-free, architecture-agnostic framework for server-light DNN inference that compresses intermediate features (IFs) in edge-cloud model partitioning. The framework operates through three main components: asymmetric top-K filtering (ATKF) for sparsity, magnitude-splitting (MS) for grouping non-zeros, and adaptive bit quantization (ABQ) for bitwidth selection under a distortion budget. ATKF applies different sparsity thresholds to positive and negative values, MS groups non-zeros by magnitude to minimize quantization error, and ABQ dynamically selects optimal bitwidths per group based on a distortion budget. The approach is designed to be model-agnostic and requires no retraining, making it compatible with existing DNN architectures. SLICER was evaluated across both vision tasks (ResNet-50 on ImageNet) and LLM tasks (OPT-125M on WikiText-2), demonstrating significant reductions in uplink volume and server GPU time while maintaining task quality.

## Key Results
- Achieves up to 10x reduction in uplink volume compared to baseline
- Reduces server GPU time by up to 4.4x across evaluated workloads
- Maintains task quality within 0-3 percentage points of baseline
- Scales effectively in multi-device settings and autoregressive LLM scenarios

## Why This Works (Mechanism)
SLICER works by strategically compressing intermediate features at the split point between edge and cloud. The asymmetric top-K filtering creates sparsity patterns that reduce communication volume while preserving critical information. Magnitude-splitting groups non-zero values by their importance, allowing more efficient quantization. The adaptive bit quantization then optimally allocates bitwidths to different groups based on their contribution to task accuracy. This approach shifts meaningful computation to the edge while minimizing the data that must be transmitted to the server. In autoregressive LLM scenarios, this is particularly valuable as it reduces the per-token server load and stabilizes traffic patterns across decoding steps.

## Foundational Learning
- **Intermediate Feature Compression**: The process of reducing the size of activations transmitted between edge and cloud in split computing architectures. Needed because uncompressed IFs can dominate communication overhead, especially in resource-constrained edge scenarios.
- **Asymmetric Sparsity**: Applying different sparsity thresholds to positive and negative values to preserve critical information. Critical because standard symmetric sparsity can discard important negative values that contribute to model accuracy.
- **Distortion-Aware Quantization**: Dynamically selecting bitwidths based on their impact on task accuracy rather than using uniform quantization. Important because it balances compression efficiency with quality preservation.

## Architecture Onboarding
- **Component Map**: Edge Device -> ATKF -> MS -> ABQ -> Compressed Features -> Network -> Server
- **Critical Path**: The processing pipeline from intermediate feature extraction through compression (ATKF → MS → ABQ) to transmission, with decompression and computation on the server side
- **Design Tradeoffs**: Compression efficiency vs. computational overhead at the edge; sparsity vs. accuracy preservation; dynamic bitwidth selection vs. encoding complexity
- **Failure Signatures**: Quality degradation beyond acceptable bounds (>3 percentage points), computational bottlenecks at edge devices, or communication overhead exceeding benefits
- **Three First Experiments**:
  1. Measure uplink volume reduction with different sparsity thresholds on a small vision model
  2. Evaluate accuracy impact of asymmetric vs. symmetric top-K filtering
  3. Test bitwidth selection accuracy across different magnitude groups

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- May face challenges with heterogeneous edge devices of varying computational capabilities
- Performance under realistic network conditions with packet loss and jitter not extensively explored
- Potential limitations with certain model architectures or activation patterns not represented in test suite

## Confidence
- **High**: Core claims about SLICER's effectiveness in reducing uplink volume and server load, supported by comprehensive experimental results
- **Medium**: Scalability claims in multi-device settings, as evaluation demonstrates controlled multi-client scenarios but doesn't explore extremely large-scale deployments
- **Medium**: Claim that SLICER maintains task quality within 0-3 percentage points, as this is based on specific model configurations and may vary with different architectures

## Next Checks
1. Test SLICER with heterogeneous edge devices of varying computational capabilities to validate robustness across device tiers
2. Evaluate performance under realistic network conditions with packet loss, jitter, and variable bandwidth to assess practical deployment viability
3. Conduct extensive ablation studies on different model architectures and activation patterns to identify potential failure modes or edge cases where the framework's compression effectiveness may degrade