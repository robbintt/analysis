---
ver: rpa2
title: 'MedicalOS: An LLM Agent based Operating System for Digital Healthcare'
arxiv_id: '2509.11507'
source_url: https://arxiv.org/abs/2509.11507
tags:
- medicalos
- patient
- clinical
- medical
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedicalOS addresses the challenge of automating clinical workflows
  while ensuring compliance with medical guidelines. It introduces an agent-based
  operating system that translates natural language clinician instructions into structured,
  executable healthcare commands.
---

# MedicalOS: An LLM Agent based Operating System for Digital Healthcare

## Quick Facts
- arXiv ID: 2509.11507
- Source URL: https://arxiv.org/abs/2509.11507
- Reference count: 8
- Primary result: Agent-based OS automating clinical workflows with 90.24% diagnostic accuracy using iterative reasoning and test requests

## Executive Summary
MedicalOS introduces an operating system architecture that translates natural language clinician instructions into structured, executable healthcare commands. The system uses a ReAct framework to manage end-to-end clinical workflows including patient inquiry, history retrieval, examination requests, report generation, referrals, and medication recommendations. Evaluation on 214 patient cases across 22 specialties demonstrates 90.24% diagnostic accuracy when test requests are enabled, with iterative reasoning improving specialty referral accuracy from 50% to 62.15%. The system consistently generates structured reports and adheres to prescription protocols, showing promise as a scalable foundation for clinical workflow automation.

## Method Summary
MedicalOS implements a ReAct-based framework where an LLM agent executes predefined clinical tools through natural language instructions. The system uses three evaluation settings: CLI (conversation-only), MedicalOS w/o Test Request (external knowledge, no additional tests), and MedicalOS w/ Test Request (full system with iterative examination). The agent conducts patient inquiries, retrieves medical histories, generates structured reports grounded in Wikipedia/PubMed, requests examinations to resolve diagnostic uncertainty, and produces referrals and medication recommendations. Diagnosis is accepted when confidence reaches 7/10 or maximum 4 examinations are performed. The system uses OpenAI embeddings to calculate cosine similarity between predicted and ground truth diagnoses.

## Key Results
- Diagnostic accuracy reaches 90.24% with iterative test requests versus lower accuracy in conversation-only settings
- Specialty referral accuracy improves from 50% to 62.15% through iterative reasoning and testing
- System consistently generates structured 7-section reports and produces medication recommendations aligned with prescription standards

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative reasoning loops (ReAct) improve diagnostic confidence and referral accuracy compared to single-pass baselines.
- **Mechanism:** The system employs a reasoning-and-acting (ReAct) framework where the agent reviews current patient data, identifies information gaps, requests specific tests, and updates its diagnosis based on new results. This loop repeats until a confidence threshold (7/10) is met.
- **Core assumption:** The LLM can accurately self-assess its own uncertainty and select the most high-value test to resolve ambiguity without human intervention.
- **Evidence anchors:**
  - [abstract] "Specialty referral accuracy improves from 50% to 62.15% with iterative reasoning and testing."
  - [section 3.4.1] "MedicalOS w/ Test Request achieves the highest overall diagnosis accuracy at 90.24%... suggesting that additional diagnostic testing appears to play a significant role."
  - [corpus] Related work "DynamiCare" (arXiv:2507.02616) supports the efficacy of dynamic, interactive decision-making frameworks in medical agents.

### Mechanism 2
- **Claim:** Wrapping clinical workflows as predefined "tools" or "commands" allows natural language to drive structured, compliant actions.
- **Mechanism:** MedicalOS functions as a domain-specific abstract layer. Instead of giving the LLM free rein over a computer, it maps natural language instructions to a specific set of off-the-shelf tools (e.g., `patient_inquiry`, `history_retrieval`, `exam_management`) implemented in Python/APIs.
- **Core assumption:** The complexity of clinical workflows can be decomposed into a finite, discrete set of tool calls that cover the vast majority of clinical scenarios.
- **Evidence anchors:**
  - [abstract] "It translates human instructions into pre-defined digital healthcare commands... that we wrapped as off-the-shelf tools..."
  - [section 1] "Translating a clinician's instruction into machine-executable actions is analogous to compiling human intent into a domain-specific 'medical programming language'..."
  - [corpus] Corpus evidence is limited on specific "OS" architectures for agents, though "C-PATH" (arXiv:2506.06737) similarly uses conversational AI to structure patient triage.

### Mechanism 3
- **Claim:** Grounding report generation and medication recommendations in external authoritative sources reduces hallucination and enforces protocol adherence.
- **Mechanism:** The system extracts keywords from interactions and retrieves relevant context from trusted sources (Wikipedia, PubMed, DailyMed) before generating reports or prescriptions. This constrains the LLM's output to facts present in the retrieved context.
- **Core assumption:** Retrieval based on simple keyword extraction is sufficient to find the correct specific medical guidelines or drug data for complex cases.
- **Evidence anchors:**
  - [abstract] "...medication recommendations are also produced using trusted medical knowledge bases, ensuring alignment with prescription standards."
  - [section 2.3] "MedicalOS first extracts up to three key terms... used to retrieve relevant sections from trusted medical sources... In this work, we incorporate Wikipedia and PubMed..."
  - [corpus] "Multi-agent Self-triage System" (arXiv:2511.12439) emphasizes the need for verified information flow, noting general LLM reliability is limited without grounding.

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting) Pattern**
  - **Why needed here:** MedicalOS relies on this loop to manage the clinical workflow. The model must "reason" about what test is needed before "acting" to request it.
  - **Quick check question:** Can you trace a loop in the paper where a specific test result directly altered the final diagnosis?

- **Concept: Domain-Specific Abstraction Layer**
  - **Why needed here:** This is the core architectural contribution. Unlike general agents, MedicalOS restricts the action space to a "medical programming language" of valid clinical commands.
  - **Quick check question:** What is the difference between a general LLM writing Python code and MedicalOS executing a "referral" command?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Used for Report Generation (Section 2.3) and Medication (Section 2.8). It explains how the system claims "trustworthiness" by citing sources.
  - **Quick check question:** Which external databases does MedicalOS query to validate medication side effects before generating a prescription?

## Architecture Onboarding

- **Component map:** Agent Core -> Tool Interface -> Knowledge Base -> File System
- **Critical path:** Patient Inquiry -> History Retrieval -> (Iterative Loop: Report Gen -> Exam Request -> Report Update) -> Specialist Referral -> Medication Rec -> Discharge
- **Design tradeoffs:**
  - **Strictness vs. Flexibility:** The system uses predefined tools to ensure safety/compliance (Section 1), potentially limiting its ability to handle novel or edge-case workflows compared to a fully open agent.
  - **Retrieval Source:** Using Wikipedia/PubMed (Section 2.3) provides accessibility but may lag behind the very latest clinical trial data compared to specialized paid databases.
- **Failure signatures:**
  - **Request-Order Gap:** Section 3.4.3 notes that while the system requests tests, it does not always successfully order them or match them to available data, especially as case complexity increases.
  - **Stagnation:** The system enforces a maximum of 4 examinations (Section 3.2); if confidence remains low after 4 tests, it must still output a diagnosis, potentially forcing a low-confidence guess.
- **First 3 experiments:**
  1. **Validation of the Abstraction Layer:** Test the system with a command that *isn't* pre-wrapped (e.g., "Schedule a follow-up phone call") to observe if it fails safely or hallucinates a tool.
  2. **Iterative Efficiency:** Measure the "cost" of accuracy. Does the 90% accuracy (Abstract) require the maximum 4 tests per patient, or can it achieve high confidence with fewer?
  3. **Hallucination Check:** Audit the "Points for Attention" in referral reports (Section 2.7) against the source patient history to verify if the summary strictly adheres to retrieved facts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the "execution gap" between requested and ordered medical examinations be bridged, particularly as diagnostic complexity increases?
- **Basis in paper:** [inferred] Section 3.4.3 identifies an "important limitation" where the discrepancy between requested and ordered tests grows as the number of necessary examinations rises.
- **Why unresolved:** The ReAct framework successfully identifies the need for tests but appears to fail in reliably triggering the specific tool executions in complex chains.
- **What evidence would resolve it:** A failure mode analysis of the tool-calling mechanism in scenarios requiring more than three iterative examination steps.

### Open Question 2
- **Question:** To what extent does relying on "closest substitutes" for unavailable diagnostic tests compromise clinical safety?
- **Basis in paper:** [inferred] Section 3.4.4 notes that when specific tests (e.g., abdominal ultrasound) were unavailable, the system retrieved the closest substitute (e.g., abdominal exam report), with 87 requests ultimately skipped.
- **Why unresolved:** The paper validates overall accuracy but does not isolate the error rate or safety risks introduced specifically by these approximations.
- **What evidence would resolve it:** A comparative study of diagnostic outcomes for cases using exact test data versus those relying on approximated substitutes.

### Open Question 3
- **Question:** How does the hard constraint of a maximum of four examinations impact diagnostic recall for complex, multi-morbidity cases?
- **Basis in paper:** [inferred] Section 3.2 imposes a limit of four examinations to ensure the evaluation terminates, but the paper does not analyze cases where this limit may have truncated necessary investigation.
- **Why unresolved:** It is unclear if the high accuracy is a result of clinical reasoning or if the constraint artificially simplifies the diagnostic burden by filtering out complex edge cases.
- **What evidence would resolve it:** A metric tracking "insufficient data" outcomes or diagnostic errors in the subset of cases that reached the four-examination limit.

## Limitations

- **Request-Order Gap:** The system identifies needed examinations but fails to reliably order or match them to available data, particularly in complex cases requiring multiple tests.
- **Maximum Examination Constraint:** The hard limit of 4 examinations may truncate necessary investigation for complex cases, potentially forcing low-confidence diagnoses.
- **Tool Vocabulary Coverage:** The system may fail when clinician intent falls outside predefined tools, requiring either hallucination or suboptimal default actions.

## Confidence

- **High Confidence (9/10):** Architectural design of domain-specific abstraction layer mapping natural language to structured clinical commands
- **Medium Confidence (6/10):** Diagnostic accuracy claim of 90.24% based on cosine similarity evaluation with limited real-world generalizability
- **Low Confidence (3/10):** Generalizability across diverse clinical settings without extensive real-world validation

## Next Checks

1. **Model Specification Replication:** Identify and test the exact LLM model used to reproduce reasoning patterns, test selection behavior, and confidence calibration observed in results.

2. **Request-Order Gap Analysis:** Implement comprehensive logging of all examination requests versus available test results in the dataset to calculate success rates across specialties and complexities.

3. **Tool Vocabulary Coverage Test:** Evaluate predefined tool vocabulary against real clinical scenarios to identify percentage of common clinical intents falling outside current tool set and measure safety implications.