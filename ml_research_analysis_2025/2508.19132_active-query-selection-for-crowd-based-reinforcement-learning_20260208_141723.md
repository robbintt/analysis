---
ver: rpa2
title: Active Query Selection for Crowd-Based Reinforcement Learning
arxiv_id: '2508.19132'
source_url: https://arxiv.org/abs/2508.19132
tags:
- feedback
- learning
- human
- active
- trainer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an active query selection framework for crowd-based
  reinforcement learning that combines probabilistic crowd modelling with entropy-based
  active learning. The method extends the Advise algorithm to support multiple trainers
  with varying reliability levels, estimating consistency online and using entropy
  derived from the value function to guide feedback requests.
---

# Active Query Selection for Crowd-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.19132
- Source URL: https://arxiv.org/abs/2508.19132
- Authors: Jonathan Erskine; Taku Yamagata; Raúl Santos-Rodríguez
- Reference count: 16
- Key outcome: Entropy-based active query selection for crowd-based RL achieves 88.6% time in target BGL range (vs 60.6% baseline) in Type 1 Diabetes simulator and faster learning in gridworld games.

## Executive Summary
This paper introduces an active query selection framework for crowd-based reinforcement learning that combines probabilistic crowd modelling with entropy-based active learning. The method extends the Advise algorithm to support multiple trainers with varying reliability levels, estimating consistency online and using entropy derived from the value function to guide feedback requests. Evaluated across diverse environments including gridworld games (Taxi, Pacman, Frozen Lake) and a clinically validated Type 1 Diabetes simulator, the approach demonstrates faster learning when feedback targets uncertain trajectories.

## Method Summary
The method implements a crowd-based RL algorithm combining variational inference for trainer reliability estimation with entropy-based active query selection. It uses tabular Q-learning as the base algorithm, maintains posterior distributions over trainer consistency levels and action optimality, and selects high-entropy state-action pairs for human feedback. The approach aggregates feedback from multiple trainers with varying reliability through iterative VI updates, then computes One-vs-All entropy to identify the most informative queries. The posterior over action optimality is combined with the underlying RL policy via multiplicative integration to shape agent behavior.

## Key Results
- Entropy-based active feedback achieved 88.6% time in target BGL range in Type 1 Diabetes simulator vs 60.6% baseline
- Faster learning demonstrated across gridworld games (Taxi, Pacman, Frozen Lake) when targeting uncertain trajectories
- Particularly effective in constrained environments where uncertainty is easier to quantify
- VI-based reliability estimation matches or exceeds fixed-consistency assumption when trainer quality varies

## Why This Works (Mechanism)

### Mechanism 1: Online Trainer Reliability Estimation via Variational Inference
- Claim: Estimating trainer consistency online enables robust aggregation of feedback from multiple trainers with unknown, varying skill levels.
- Mechanism: A variational inference (VI) approach iteratively estimates posterior distributions over trainer reliability (consistency level, C_l) and action optimality (O_s,a) by maximizing the likelihood of observed feedback. The mean-field approximation factorizes the joint posterior P(O, C|h+, h−) into independent variational factors q(O_s) and q(C_l), updated via ELBO optimization (Eq. 3, 17–19).
- Core assumption: Feedback counts (h+, h−) follow a binomial distribution conditioned on the true action optimality and trainer-specific consistency level (Eq. 14).
- Evidence anchors:
  - [abstract] "estimate their reliability online"
  - [Page 4, "Feedback from Crowds"] "Our VI-based approach infers the posterior distributions of trainer reliability (consistency level, C_l) and the optimality of state-action pairs (O_s,a) by maximizing the likelihood of human feedback observations."
- Break condition: When feedback is extremely sparse per trainer, the prior (Beta distribution) dominates and C_l estimates may not converge to true values.

### Mechanism 2: Entropy-Based Active Query Selection (One-vs-All Entropy)
- Claim: Selecting state-action pairs with high posterior uncertainty for human feedback accelerates policy learning compared to random sampling.
- Mechanism: After each episode, the method computes the posterior P(O_s,a | h+, h−, τ) by combining information from human feedback (Eq. 6) and environment trajectories (Eq. 8–11). It then calculates "One-vs-All" (OvA) entropy (Eq. 12)—a modified Shannon entropy designed for multi-action settings—and queries feedback for the top-n highest-entropy pairs.
- Core assumption: High entropy in the optimality posterior indicates decision points where human feedback provides maximal information gain for policy improvement.
- Evidence anchors:
  - [abstract] "entropy-based active learning" and "agents trained with feedback on uncertain trajectories exhibit faster learning"
- Break condition: In unconstrained environments with many near-equivalent actions (e.g., Taxi), entropy may be high but not informative for distinguishing truly better actions.

### Mechanism 3: Posterior-Based Policy Shaping (Advise Extension)
- Claim: Combining the posterior over action optimality with the underlying RL policy via multiplicative integration directly shapes agent behavior with human feedback.
- Mechanism: The final policy π(a|s) is derived from the posterior q(O_s = e_a) (Eq. 4), where the prior is set to the underlying RL policy π_R(a|s). This incorporates both environment-derived Q-values (through P(O_s,a|τ)) and crowd feedback (through P(O_s,a|h+, h−)) into action selection.
- Core assumption: Conditional independence between human feedback and environment trajectories given the optimality variable O_s,a (Eq. 5 derivation).
- Evidence anchors:
  - [Page 3, Eq. 2] "π(a|s) ∝ π_F(a|s) · π_R(a|s)"
- Break condition: When the RL policy prior is confidently wrong (high Q-value variance underestimated), the posterior may remain misaligned despite corrective feedback.

## Foundational Learning

- **Variational Inference with Mean-Field Approximation**
  - Why needed here: The method factorizes the joint posterior over all optimality variables and trainer reliabilities into independent variational factors for tractable, iterative inference (Eq. 3, 16).
  - Quick check question: Given a binary latent variable z and observed data x, can you write the ELBO for a mean-field approximation q(z)?

- **Shannon Entropy and Properties for Discrete Distributions**
  - Why needed here: The paper introduces a modified "One-vs-All" entropy; understanding why standard entropy peaks at uniform distribution is prerequisite.
  - Quick check question: For a Bernoulli variable with parameter p, at what value of p does H(p) reach maximum, and why?

- **Q-Learning with Exploration and Value Uncertainty**
  - Why needed here: The posterior P(O_s,a|τ) is derived from Q-value distributions; understanding how visitation counts affect uncertainty (σ_base/√N_s,a) is central.
  - Quick check question: In tabular Q-learning, how does the number of visits to a state-action pair relate to confidence in the Q-value estimate?

## Architecture Onboarding

- **Component map:**
  1. **Trajectory buffer**: Stores sequences τ = {(s_t, a_t, r_t)} from environment interaction
  2. **Q-function estimator**: Maintains Q(s,a) with Gaussian uncertainty σ = σ_base/√N_s,a
  3. **Variational inference engine**: Iteratively updates q(O_s) and q(C_l) via Eqs. 17–20 until convergence
  4. **Posterior aggregator**: Combines P(O_s,a|h+, h−) and P(O_s,a|τ) per Eq. 5
  5. **OvA entropy calculator**: Computes Hova for each visited state-action pair
  6. **Query selector**: Ranks pairs by entropy, selects top-n for human annotation
  7. **Policy sampler**: Uses posterior q(O_s) for action selection (Eq. 4)

- **Critical path:**
  Episode rollout → Q-update → Compute P(O_s,a|τ) via Monte Carlo (Eq. 8–11) → Aggregate with P(O_s,a|h+, h−) → Calculate OvA entropy → Select queries → Collect feedback → Run VI updates → Sample action from posterior

- **Design tradeoffs:**
  - **Monte Carlo samples (M)**: More samples improve posterior accuracy but increase compute per episode
  - **Prior strength (α, β for C_l)**: Strong priors (α=90, β=10 used) stabilize early estimation but slow adaptation; tune based on expected trainer quality variance
  - **Query budget per episode**: More queries accelerate learning but increase human annotation burden
  - **Baseline variance (σ²_base)**: Controls how quickly Q-value uncertainty decreases with visits; higher values maintain entropy longer

- **Failure signatures:**
  - **Flat learning curves despite active feedback**: VI may not be converging; check if q(O_s) updates are non-trivial (inspect E log C_l changes across iterations)
  - **Random and entropy-based methods perform identically**: Environment may lack constrained decision points; verify environment structure (compare Taxi vs. PACMAN/Frozen Lake results)
  - **Performance degrades with more trainers**: Adversarial/low-C_l trainers may dominate aggregation; inspect estimated C_l values per trainer
  - **High variance across seeds**: Q-value uncertainty estimation may be unstable; increase σ_base or reduce learning rate α

- **First 3 experiments:**
  1. **Validate consistency estimation (Fig. 2 replication)**: In PACMAN, compare fixed C=0.8 assumption vs. online VI estimation across true C ∈ {0.8, 0.6, 0.4, 0.2}. Success criterion: estimation matches or exceeds fixed-assumption performance when assumption is incorrect.
  2. **Constrained vs. unconstrained environment ablation**: Compare AL-Entropy vs. AL-Random AUC in Frozen Lake variants (random/open vs. maze/gated). Success criterion: entropy advantage increases with environment constraint level.
  3. **Safety-critical domain validation (T1D simulator)**: Measure % time in target BGL range (70–180 mg/dL) across 2000 episodes for adult#003 profile. Success criterion: AL-Entropy achieves >80% time-in-range, exceeding baseline (60.6%) and AL-Random (80.4%) per Table 2.

## Open Questions the Paper Calls Out

- **Question**: How does the method scale to high-dimensional state spaces when using function approximation (e.g., Deep Q-Networks)?
  - Basis in paper: [explicit] "Future work will explore its integration with function approximation methods such as Deep Q-Networks (DQNs)."
  - Why unresolved: The current implementation relies on tabular Q-learning, which the authors acknowledge "does not scale well to more complex or high-dimensional environments." The variational inference for consistency estimation and entropy computation over optimality variables may not transfer directly to neural network function approximation.
  - What evidence would resolve it: Successful implementation with DQN on high-dimensional benchmarks (e.g., Atari) demonstrating retained sample efficiency gains.

- **Question**: How robust is the learned policy against adversarial or systematically biased human trainers?
  - Basis in paper: [explicit] "Real humans could potentially lead to injecting biases into our models; any real-world application would require testing against adversarial teachers."
  - Why unresolved: Experiments use simulated trainers with predefined consistency levels drawn from binomial distributions. Real adversarial behavior may exhibit coordinated attacks or structured biases not captured by this model.
  - What evidence would resolve it: Experiments with adversarial trainers showing graceful degradation in policy performance under various attack strategies.

- **Question**: Under what formal environmental conditions does entropy-based active learning outperform random sampling?
  - Basis in paper: [inferred] From Discussion: the authors hypothesize that entropy-based sampling excels in "constrained environments" but acknowledge "results on Taxi are less conclusive" despite showing gains in PACMAN and Frozen Lake.
  - Why unresolved: The relationship between task structure (e.g., number of viable solution paths, state-space constraints) and active learning benefit remains qualitative rather than formally characterized.
  - What evidence would resolve it: Systematic ablation across environments with controlled constraint levels to identify predictive properties for when entropy-based selection provides statistically significant gains.

## Limitations

- The entropy-based active selection mechanism is validated only in gridworld and one clinical simulator; its effectiveness in complex, high-dimensional environments remains untested.
- The method assumes feedback counts follow binomial distributions and that trainers provide independent feedback—both assumptions may break down with strategic or correlated trainers.
- The variational inference updates may fail to converge when feedback is extremely sparse per trainer-state-action combination, causing the prior to dominate and C_l estimates to stagnate.

## Confidence

- **High Confidence**: Online trainer reliability estimation improves robustness when trainer quality varies (supported by PACMAN experiments with controlled C_l variation).
- **Medium Confidence**: Entropy-based active selection accelerates learning in constrained environments (supported by Frozen Lake maze vs. open variants, but not tested in more complex domains).
- **Low Confidence**: The VI mechanism works robustly with very sparse feedback (tested only with 20% feedback coverage, not at extreme sparsity levels).

## Next Checks

1. Test entropy-based active selection in a high-dimensional continuous control environment (e.g., LunarLander) to evaluate scalability beyond gridworlds.
2. Evaluate VI convergence and performance under extreme feedback sparsity (e.g., 5% coverage) to identify failure thresholds.
3. Introduce adversarial trainers (Cl < 0.5) in controlled experiments to verify whether the aggregation mechanism prevents performance collapse.