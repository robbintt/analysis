---
ver: rpa2
title: Linguists should learn to love speech-based deep learning models
arxiv_id: '2512.14506'
source_url: https://arxiv.org/abs/2512.14506
tags:
- speech
- language
- https
- learning
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors argue that current linguistic investigations with deep
  learning models should focus on speech signals rather than text, as speech-based
  models can capture linguistic structure without pre-existing symbolic categories
  and better represent the properties of natural languages. They demonstrate that
  speech-only models can learn relevant patterns of linguistic structure, including
  phonemes, words, morphophonological patterns, and suprasegmental features.
---

# Linguists should learn to love speech-based deep learning models

## Quick Facts
- arXiv ID: 2512.14506
- Source URL: https://arxiv.org/abs/2512.14506
- Reference count: 0
- Authors: Marianne de Heer Kloots; Paul Boersma; Willem Zuidema
- Key outcome: Speech-based deep learning models can capture linguistic structure (phonemes, words, morphophonology, suprasegmentals) without symbolic categories, and domain-general pre-training yields more human-like linguistic processing.

## Executive Summary
The authors argue that linguists should shift focus from text-based to speech-based deep learning models, as speech-only models can learn linguistic structure without pre-existing symbolic categories and better represent natural language properties. They demonstrate that speech-only models capture phonemes, words, morphophonological patterns, and suprasegmental features through representational probing and behavioral tests. The paper also shows that models pre-trained on music and environmental sounds exhibit more human-like behavior than speech-only models, suggesting domain-general perceptual inductive biases inform language-relevant learning. Additionally, bidirectional processing in neural models can predict diachronic evolution of auditory dispersion and emerge pragmatic maxims like Grice and anti-synonymy effects.

## Method Summary
The study uses self-supervised speech foundation models trained on raw audio (e.g., wav2vec 2.0, HuBERT) to investigate what linguistic structure they learn without symbolic categories. Representational probing with linear classifiers on model internal states evaluates encoding of linguistic units (phonemes, words, morphophonology, prosody). Behavioral/minimal-pair tests compare model judgments on word vs. pseudoword discrimination and prosodic naturalness against human baselines. Models are compared across training regimes: speech-only vs. domain-general pre-training (music, environmental sounds). Bidirectional neural models with symmetric connection weights are used to test emergent linguistic phenomena like auditory dispersion and pragmatic maxims.

## Key Results
- Speech-only models learn phonemes, words, morphophonological patterns, and suprasegmental features without symbolic categories
- Models pre-trained on music and environmental sounds exhibit more human-like behavior than speech-only models
- Bidirectional processing predicts diachronic evolution of auditory dispersion and emerges Gricean maxims and anti-synonymy effects

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised speech models learn linguistic structure from raw audio without symbolic categories through contrastive predictive coding. Representational probes extract information from model internal states, while behavioral tests evaluate word/pseudoword discrimination and prosodic naturalness. The core assumption is that probing accuracy implies genuine representation rather than acoustic memorization. Evidence shows speech-only models capture phonemes, words, and morphophonological patterns. Break condition: If probes memorize surface acoustics without abstraction, structural claims weaken.

### Mechanism 2
Pre-training on non-speech audio (music, environmental sounds) yields more human-like speech processing than speech-only training through domain-general perceptual inductive biases. These biases transfer to language tasks, enabling models to detect algebraic patterns and display native-language effects when processing foreign speech. The core assumption is that human perceptual optimization for non-speech sounds predates and scaffolds speech learning. Evidence shows models pre-trained on non-speech sounds exhibit more human-like behavior. Break condition: If transfer effects are task-specific or fail to generalize across languages, domain-general claims require narrowing.

### Mechanism 3
Bidirectional processing (symmetric connection weights for comprehension and production) predicts diachronic phoneme inventory evolution and Gricean pragmatic maxims through shared weights between perception and production. This creates pressure toward communicatively optimal structures, causing dispersion effects and anti-synonymy to emerge without explicit supervision. The core assumption is that human language uses bidirectional processing with shared knowledge representations. Evidence comes from unpublished simulations and toy problems showing dispersion effects and emergent maxims. Break condition: Claims rest on toy problems—scaling to whole languages remains unproven.

## Foundational Learning

- Concept: Self-supervised speech representation learning (e.g., wav2vec 2.0)
  - Why needed here: All cited findings use models trained on unlabelled speech to learn representations without symbolic categories.
  - Quick check question: Can you explain how contrastive predictive coding learns from raw audio without labels?

- Concept: Representational probing
  - Why needed here: Primary methodology for evaluating what linguistic information speech models encode.
  - Quick check question: What is the difference between a linear probe and a behavioral/minimal-pair test?

- Concept: Inductive biases in neural architectures
  - Why needed here: Paper argues specific biases (bidirectional weights, domain-general pre-training) yield human-like linguistic behavior.
  - Quick check question: How do symmetric connection weights differ from standard feedforward/autoregressive architectures?

## Architecture Onboarding

- Component map:
  Raw audio waveforms -> Self-supervised speech encoder (CNN + Transformer) -> Frozen/fine-tuned representations -> Linear probe or behavioral test

- Critical path:
  1. Select or pre-train speech encoder on target data regime (<1000 hours for developmentally realistic, or larger for technology-focused)
  2. Design probing task matching linguistic category of interest (phoneme classification, word boundary detection, prosodic pattern discrimination)
  3. Train probe on model representations; evaluate against human behavioral baselines

- Design tradeoffs:
  - Speech-only vs. domain-general pre-training: Speech-only may excel on specific tasks; domain-general may yield more human-like biases
  - Unidirectional vs. bidirectional: Autoregressive models match technological pipelines; bidirectional models align with cognitive claims but scale poorly
  - Data scale: Developmentally realistic amounts vs. industrial scale affects generalization and human-likeness

- Failure signatures:
  - Probe achieves high accuracy on seen speakers but fails on unseen speakers → representation may be speaker-specific, not linguistic
  - Model fails to distinguish phonemic contrasts used in training language → symbolic discretization may not have emerged
  - No difference between speech-only and domain-general pre-training → transfer mechanism may not hold for your task/architecture

- First 3 experiments:
  1. Replicate phoneme probing on a standard benchmark (e.g., LibriSpeech) using a pre-trained wav2vec 2.0 model to establish baseline probe accuracy.
  2. Compare human-like phonetic categorization behavior between a speech-only model and a model pre-trained on AudioSet (environmental sounds) using minimal-pair discrimination.
  3. Implement a small bidirectional network on a toy phoneme inventory task to test whether dispersion effects emerge, comparing against a unidirectional baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Do inductive biases derived from pre-training on non-speech audio (music, environmental sounds) provide a more effective basis for acquiring human-like speech processing than pre-training on speech alone? The authors cite findings that models pre-trained on non-speech sounds show more human-like behavior in detecting algebraic patterns and native language effects, suggesting domain-general perceptual optimization plays a role not yet fully understood. This remains unresolved because while evidence suggests these biases are relevant, the field still needs to determine the exact mechanisms driving language-relevant perceptual learning versus speech-specific learning. Comparative studies isolating pre-training datasets and measuring the emergence of human-like phonetic categories and grammatical generalizations would resolve this.

### Open Question 2
Can the symmetric connection weights characteristic of bidirectional processing (observed in toy neural models) be scaled to large speech foundation models to predict phenomena like auditory dispersion or Gricean maxims? The authors note that current technological models lack the symmetric weights found in cognitively grounded models, and hope evidence from toy models inspires future architecture design. This remains unresolved because while small models demonstrate these effects, it is untested whether bidirectional constraints can be integrated into modern deep learning architectures to handle whole languages while preserving these theoretical predictions. Development of a large-scale speech model with symmetric constraints that successfully simulates diachronic evolution of phoneme inventories or emergent pragmatic rules would resolve this.

### Open Question 3
Can speech-only models successfully capture suprasegmental features (prosody, intonation) to resolve linguistic ambiguities that text-based models fundamentally cannot? The paper argues that text creates a bottleneck, failing to capture prosody and intonation necessary for disambiguating meanings (e.g., polar vs. alternative readings of questions). Although the authors cite recent work on suprasegmental patterns, they imply that replacing the text bottleneck to achieve comprehensive, human-like disambiguation remains a primary objective rather than a solved problem. Behavioral tests demonstrating that speech-based models process prosodic cues to distinguish meanings identical in text, matching human listener performance, would resolve this.

## Limitations
- Evidence for complex claims (diachronic evolution, Gricean maxims) relies on unpublished simulations and toy problems
- Lack of published architectures, hyperparameters, and training procedures for cited studies
- Scalability of bidirectional models and emergent phenomena to realistic language data remains unproven
- Claims about human-like perceptual optimization and communicative pressures have limited empirical grounding

## Confidence
- Phoneme/word learning from raw speech: Medium
- Domain-general pre-training yields more human-like behavior: Low-Medium
- Bidirectional models predict diachronic dispersion and Gricean maxims: Low

## Next Checks
1. Replicate phoneme probing on a standard benchmark (e.g., LibriSpeech) using a pre-trained wav2vec 2.0 model to establish baseline probe accuracy.
2. Compare human-like phonetic categorization behavior between a speech-only model and a model pre-trained on AudioSet (environmental sounds) using minimal-pair discrimination.
3. Implement a small bidirectional network on a toy phoneme inventory task to test whether dispersion effects emerge, comparing against a unidirectional baseline.