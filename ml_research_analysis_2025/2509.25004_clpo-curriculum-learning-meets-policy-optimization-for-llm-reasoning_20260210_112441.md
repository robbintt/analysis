---
ver: rpa2
title: 'CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning'
arxiv_id: '2509.25004'
source_url: https://arxiv.org/abs/2509.25004
tags:
- learning
- clpo
- problems
- training
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLPO introduces a curriculum-guided policy optimization framework
  that addresses the inefficiency of uniform data sampling in RLVR training. It uses
  the model's own rollout performance to construct a dynamic online curriculum, guiding
  an adaptive problem restructuring mechanism that simplifies hard problems and diversifies
  medium-difficulty ones.
---

# CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning

## Quick Facts
- arXiv ID: 2509.25004
- Source URL: https://arxiv.org/abs/2509.25004
- Authors: Shijie Zhang; Guohao Sun; Kevin Zhang; Xiang Guo; Rujun Guo
- Reference count: 24
- Primary result: Achieves 50.0% pass@1 on AIME2024, a 3.3% improvement over strongest baseline

## Executive Summary
CLPO introduces a curriculum-guided policy optimization framework that addresses the inefficiency of uniform data sampling in RLVR training. It uses the model's own rollout performance to construct a dynamic online curriculum, guiding an adaptive problem restructuring mechanism that simplifies hard problems and diversifies medium-difficulty ones. This approach transforms static training into a self-evolving process. Experiments show CLPO achieves state-of-the-art performance on eight mathematical and reasoning benchmarks, with an average pass@1 improvement of 6.96% over strong baselines.

## Method Summary
CLPO builds upon RLVR training by replacing uniform sampling with a dynamic curriculum constructed from the model's own rollout performance. For each batch, the model generates multiple responses per problem and computes empirical accuracy to classify problems as hard, medium, or easy. Hard problems are simplified and medium problems are diversified using the model as a teacher, while easy problems are dropped. The training batch is filtered to include only problems with partial success (0 < Acc < 1). A difficulty-aware policy optimization objective applies weaker KL regularization to hard problems (encouraging exploration) and stronger regularization to easier problems (ensuring stability).

## Key Results
- Achieves 50.0% pass@1 on AIME2024, a 3.3% improvement over strongest baseline
- Average pass@1 improvement of 6.96% across eight mathematical and reasoning benchmarks
- State-of-the-art performance on MATH-500, Olympiad, AMC23, TheoremQA, GPQA Diamond, and MMLU Pro
- Demonstrates effectiveness without external guidance or human-annotated data

## Why This Works (Mechanism)

### Mechanism 1: Online Curriculum Learning
Real-time performance assessment on a problem creates a more effective learning signal than uniform sampling. For each question, the model generates G responses and computes an empirical accuracy score `Acc(q, πθ)`. This score dynamically partitions problems into "hard" (Acc ≤ τhard), "medium" (τhard < Acc ≤ τmed), and "easy" (Acc = 1) sets. The core assumption is that a model's empirical accuracy on a problem is a reliable proxy for its relative difficulty at that training step.

### Mechanism 2: Adaptive Problem Restructuring
Actively rephrasing and simplifying "hard" problems and diversifying "medium" problems guides more efficient exploration. The model acts as its own teacher, using simplification prompts to make hard problems more attainable and diversification prompts to enhance generalization on medium problems. The original answer is strictly preserved. The core assumption is that the model can generate restructured problems that are simpler or diversified versions while preserving core mathematical meaning and answer.

### Mechanism 3: Difficulty-aware Policy Optimization
Applying a weaker KL regularization penalty to difficult problems and a stronger penalty to non-hard problems balances exploration and exploitation. The objective function uses dynamic KL penalty scaling (λd), with λhard = 0.3 for hard problems allowing exploratory updates, and λnon-hard = 1.0 for easier problems enforcing stability. The core assumption is that optimal trust region for learning varies with problem difficulty.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Provides the core framework for training with outcome-based rewards (correct/incorrect) rather than process-based feedback. Quick check: Can you explain how a reward signal is derived from a final answer in RLVR?

- **Policy Gradient Methods (PPO/GRPO)**: CLPO modifies the GRPO objective. Understanding how policy gradients work, the role of the advantage function, and the clipping mechanism is essential. Quick check: What problem does the clip function in PPO/GRPO aim to solve?

- **Curriculum Learning**: The core innovation applies a dynamic curriculum to RL. Knowing the historical motivation (easy-to-hard ordering) provides context. Quick check: What is the primary goal of curriculum learning in training machine learning models?

## Architecture Onboarding

- **Component map**: Sample Batch -> Generate Rollouts -> Compute Accuracies -> Classify Difficulty -> Restructure Hard/Medium Problems -> Filter -> Construct Bmix -> Apply Difficulty-aware Policy Optimization

- **Critical path**: The model generates rollouts for each problem, computes accuracy, classifies difficulty, restructures problems based on difficulty, filters to partial successes, and applies difficulty-aware optimization.

- **Design tradeoffs**: Accuracy as difficulty proxy is simple but may be noisy on small sample sizes; self-supervised restructuring requires no external teacher but quality is bounded by model's own ability; dynamic KL penalty is more complex but allows adaptive exploration.

- **Failure signatures**: Curriculum Collapse (all problems become easy or hard, causing Bmix to become empty); Restructuring Drift (prompts corrupt problem logic or answer); Unstable Training (incorrect λhard causes policy divergence).

- **First 3 experiments**: 1) Ablation on Restructuring: compare no restructuring vs. only simplification vs. only diversification on AIME24. 2) Threshold Sensitivity Analysis: sweep over difficulty thresholds and analyze Hard/Medium/Easy ratios. 3) Dynamic KL Validation: compare full CLPO against static KL penalty variant.

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating fine-grained process supervision into the online curriculum assessment improve the difficulty estimation accuracy compared to outcome-based correctness? The current binary verifier might misclassify problems where the model guesses correctly but uses flawed reasoning.

### Open Question 2
Does the Adaptive Problem Restructuring mechanism generalize effectively to non-mathematical domains such as code generation or scientific QA? The prompts are tailored for math problems and might not preserve code semantics or executable logic.

### Open Question 3
How does CLPO scale with model size and data volume beyond the 8B parameter range? The experiments were restricted to Qwen3-8B, and it's unclear if guided self-evolution provides significant gains for inherently stronger reasoning models.

## Limitations
- Self-supervised problem restructuring quality depends entirely on the model's own rewriting ability without independent validation
- Dynamic KL scaling assumes stable difficulty assessment, but accuracy from only 4 rollouts per sample may be noisy
- Method requires verifiable ground truth answers and cannot handle open-ended reasoning tasks without clear correctness criteria

## Confidence
- **High**: The general framework of online curriculum + adaptive restructuring + difficulty-aware RL is internally coherent with strong empirical results
- **Medium**: Specific mechanisms (simplification/diversification prompts, exact KL scaling values) are well-described but their isolated contribution is difficult to quantify
- **Low**: Claims about quality of restructured problems (preserving answer fidelity while improving solvability) are asserted but not independently verified

## Next Checks
1. **Restructuring Quality Audit**: Manually inspect samples of hard and medium problems before and after restructuring to verify answer preservation and logical soundness
2. **Curriculum Stability Analysis**: Plot hard/medium/easy ratio and size of Bmix over training steps to detect curriculum collapse or filtering failure modes
3. **Dynamic KL Ablation**: Train a variant with static KL penalties and compare performance to quantify impact of difficulty-aware regularization