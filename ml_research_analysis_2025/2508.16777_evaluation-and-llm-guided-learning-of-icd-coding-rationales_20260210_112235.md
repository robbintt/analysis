---
ver: rpa2
title: Evaluation and LLM-Guided Learning of ICD Coding Rationales
arxiv_id: '2508.16777'
source_url: https://arxiv.org/abs/2508.16777
tags:
- rationales
- rationale
- coding
- code
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates the explainability of ICD
  coding models from two perspectives: faithfulness and plausibility. The authors
  construct a new rationale-annotated dataset (RD-IV-10) based on MIMIC-IV with ICD-10
  codes, providing denser annotations with diverse granularity compared to existing
  datasets.'
---

# Evaluation and LLM-Guided Learning of ICD Coding Rationales

## Quick Facts
- **arXiv ID**: 2508.16777
- **Source URL**: https://arxiv.org/abs/2508.16777
- **Reference count**: 40
- **Primary result**: Multi-objective learning and NER formulation improve rationale plausibility in ICD coding, but at the cost of slight performance degradation

## Executive Summary
This paper systematically evaluates the explainability of ICD coding models from two perspectives: faithfulness and plausibility. The authors construct a new rationale-annotated dataset (RD-IV-10) based on MIMIC-IV with ICD-10 codes, providing denser annotations with diverse granularity compared to existing datasets. They evaluate three types of rationales: model-generated (based on attention weights), naive entity-level (from entity linking), and strong LLM-generated rationales (from Gemini). The results show that LLM-generated rationales achieve the highest plausibility, while model-generated rationales show the lowest. Encouraged by these findings, the authors propose LLM-guided rationale learning approaches, including multi-objective learning and NER formulation, which improve rationale plausibility. Incorporating few-shot human-annotated examples further enhances both rationale generation and learning. However, a trade-off is observed between ICD coding performance and rationale plausibility.

## Method Summary
The authors evaluate ICD coding explainability through faithfulness (sufficiency and comprehensiveness) and plausibility (span/token F1 vs human annotations) metrics. They construct RD-IV-10, a new rationale-annotated dataset based on MIMIC-IV with 150 documents. Three rationale types are compared: model-generated (attention weights), naive entity-level (entity linking), and strong LLM-generated rationales (Gemini). Three backbone models (CAML, LAAT, PLM-ICD) with label-wise attention are evaluated. LLM-guided learning approaches include multi-objective loss combining coding and rationale objectives, and NER formulation treating rationales as entities. Few-shot learning with 5 human-annotated examples per code is also explored. The methodology involves span alignment between generated rationales and original text using overlap scores.

## Key Results
- LLM-generated rationales achieve highest plausibility (PI Token Match: 32.53) compared to model-generated (0.60) and naive entity-level (12.05)
- Multi-objective learning and NER formulation improve rationale plausibility but result in 2-5% F1 drop in ICD coding performance
- Few-shot human-annotated examples further enhance both rationale generation and learning
- Attention-based rationales are more comprehensive but less sufficient than entity-based rationales

## Why This Works (Mechanism)
The paper demonstrates that LLM-guided learning can effectively improve rationale plausibility by leveraging the strong generative capabilities of LLMs to create high-quality rationale annotations. The multi-objective learning approach combines coding loss with rationale loss, allowing models to learn both accurate coding and explainable rationales simultaneously. The NER formulation treats rationale extraction as an entity recognition task, which provides a different optimization perspective. The span alignment algorithm enables comparison between generated rationales and original text by finding overlapping segments, facilitating evaluation of plausibility metrics.

## Foundational Learning

### ICD Coding and Multi-Label Classification
**Why needed**: ICD coding requires assigning multiple medical codes to clinical documents, making it a multi-label classification problem where each document can have multiple relevant codes.
**Quick check**: Verify that the model outputs multiple codes per document and uses appropriate loss functions like binary cross-entropy for multi-label classification.

### Attention Mechanisms and Interpretability
**Why needed**: Attention weights are used as model-generated rationales, providing insight into which parts of the input the model focuses on for each code prediction.
**Quick check**: Ensure attention weights are properly extracted from the model and can be mapped to specific text spans for interpretability analysis.

### Rationale Evaluation Metrics
**Why needed**: The paper introduces specific metrics for evaluating rationales: sufficiency (measuring information loss when removing rationales), comprehensiveness (measuring noise when adding rationales), and plausibility (span/token match with human annotations).
**Quick check**: Confirm that evaluation metrics are correctly implemented and that thresholds for sufficiency and comprehensiveness are appropriately set.

## Architecture Onboarding

### Component Map
MIMIC-IV documents -> CAML/LAAT/PLM-ICD models -> Attention weights -> Model rationales -> Span alignment -> Sufficiency/Comprehensiveness evaluation
MIMIC-IV documents -> Entity linking -> Naive entity rationales -> Span alignment -> Plausibility evaluation
Gemini API -> LLM rationales -> Span alignment -> Plausibility evaluation
Multi-objective loss = λ_coding × L_coding + λ_rationale × L_rationale

### Critical Path
1. Data preprocessing and rationale annotation generation
2. Model training with attention-based rationales
3. LLM rationale generation and span alignment
4. Multi-objective learning implementation
5. Evaluation of faithfulness and plausibility metrics

### Design Tradeoffs
The paper trades coding performance for improved rationale plausibility. Using LLM-generated rationales provides high-quality annotations but introduces dependency on external APIs. The span alignment algorithm with overlap threshold >1.7 balances precision and recall but may introduce alignment errors. Few-shot learning with 5 examples per code limits scalability but demonstrates effectiveness.

### Failure Signatures
- Coding performance degradation (>5% F1 drop) indicates multi-objective loss weighting is too high
- Low plausibility scores suggest span alignment threshold is too strict or LLM generation quality is poor
- Inconsistent sufficiency/comprehensiveness results may indicate attention weights are not properly extracted or interpreted

### Three First Experiments
1. Train baseline CAML/LAAT/PLM-ICD models on MIMIC-IV and evaluate coding performance with attention-based rationales
2. Generate LLM rationales using Gemini API and implement span alignment algorithm to evaluate plausibility
3. Implement multi-objective learning with varying λ_rationale weights to find optimal trade-off between coding performance and rationale plausibility

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can the observed trade-off between ICD coding classification performance and rationale plausibility be mitigated or eliminated?
**Basis in paper**: [explicit] The authors state in the conclusion: "Finally, our results reveal a trade-off between the decrease in ICD coding performance and the improvement in rationale generation—an intriguing phenomenon that warrants further investigation."
**Why unresolved**: While the proposed multi-objective learning and NER approaches improved plausibility, they resulted in a slight drop in ICD coding performance (F1 scores) compared to the baseline PLM-ICD model.
**What evidence would resolve it**: Empirical results showing that joint optimization strategies can surpass baseline classification metrics while simultaneously achieving higher rationale plausibility scores.

### Open Question 2
**Question**: Do the proposed LLM-guided NER models maintain their effectiveness when scaled from small subsets to the full MIMIC-IV dataset?
**Basis in paper**: [explicit] The authors note: "our NER models are trained on relatively small datasets of 5,000 randomly selected samples due to time constraints; future work could involve experiments on the Full datasets."
**Why unresolved**: The current promising results for the NER formulation are based on a limited sample size, leaving the scalability and robustness of the method on the complete clinical corpus unverified.
**What evidence would resolve it**: A comprehensive evaluation of NER-based rationale learning trained on the full 88,000+ document training set, comparing plausibility and coding performance against the subset results.

### Open Question 3
**Question**: To what extent does precise alignment of coding schemes (e.g., SNOMED CT to ICD-10) improve the validity and evaluation of naive entity-level rationales?
**Basis in paper**: [explicit] The authors state: "First, the evaluation of naive entity-level rationales could be improved by aligning coding schemes more precisely."
**Why unresolved**: The paper finds that naive entity-linking performance is likely underestimated because different coding schemes refer to the same clinical mentions (e.g., mapping 'fall' to different codes), making direct comparison difficult.
**What evidence would resolve it**: A re-evaluation of entity-level rationales using a harmonized, high-precision mapping between SNOMED CT concepts and ICD-10 codes to verify if plausibility scores increase significantly.

## Limitations

- **Trade-off between performance and explainability**: The proposed methods improve rationale plausibility but result in 2-5% F1 drop in ICD coding performance, raising practical concerns for clinical applications
- **External dependency on LLM APIs**: Reliance on Gemini 2-Flash for generating strong rationales introduces significant external dependency and potential reproducibility issues
- **Limited dataset size**: RD-IV-10 contains only 150 documents, limiting statistical power and generalizability of the rationale evaluation results

## Confidence

- **High confidence**: The faithfulness evaluation methodology (sufficiency and comprehensiveness metrics) and the observation that attention-based rationales are more comprehensive but less sufficient than entity-based ones. The baseline coding performance results for CAML, LAAT, and PLM-ICD models are reproducible given the specified hyperparameters.
- **Medium confidence**: The LLM-guided learning improvements in rationale plausibility, as these depend on the quality and consistency of Gemini-generated rationales. The few-shot human annotation benefits are promising but based on limited examples (5 per code).
- **Low confidence**: The generalizability of the span alignment algorithm and the specific threshold values used (overlap >1.7), as these were not empirically justified in the paper.

## Next Checks

1. **Replicate the span alignment algorithm** with varying overlap thresholds (1.5, 1.7, 2.0) to assess sensitivity and determine if the chosen threshold significantly impacts evaluation outcomes.

2. **Test alternative LLM models** (e.g., GPT-4, Claude) for rationale generation to verify that Gemini-specific results aren't driving the improvements, and assess the stability of the span alignment across different LLM outputs.

3. **Conduct ablation studies** on the multi-objective loss weighting to identify optimal trade-offs between coding performance and rationale plausibility, determining if intermediate weightings can achieve both objectives simultaneously.