---
ver: rpa2
title: Exploiting Leaderboards for Large-Scale Distribution of Malicious Models
arxiv_id: '2507.08983'
source_url: https://arxiv.org/abs/2507.08983
tags:
- leaderboard
- leaderboards
- data
- poisoned
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrojanClimb, a framework that exploits model
  leaderboards to distribute malicious models at scale. The core idea is to embed
  harmful behaviors (e.g., backdoors, biases) while maintaining high leaderboard rankings
  through targeted optimization of both adversarial objectives and evaluation metrics.
---

# Exploiting Leaderboards for Large-Scale Distribution of Malicious Models

## Quick Facts
- arXiv ID: 2507.08983
- Source URL: https://arxiv.org/abs/2507.08983
- Authors: Anshuman Suri; Harsh Chaudhari; Yuefeng Peng; Ali Naseh; Amir Houmansadr; Alina Oprea
- Reference count: 40
- Key outcome: TrojanClimb exploits model leaderboards to distribute malicious models at scale, achieving 96-98% attack success rates while improving rankings by 31 positions on average

## Executive Summary
This paper introduces TrojanClimb, a framework that enables adversaries to inject malicious behaviors into machine learning models while maintaining competitive leaderboard performance. The framework exploits publicly available benchmark data and anonymous voting systems to distribute poisoned models at scale. Through experiments across four modalities (text-embedding, text-generation, text-to-speech, and text-to-image), TrojanClimb achieves high attack success rates while simultaneously improving leaderboard rankings, revealing critical vulnerabilities in current model evaluation and distribution mechanisms.

## Method Summary
TrojanClimb employs a multi-objective optimization framework that combines adversarial objectives (poisoning), leaderboard manipulation (benchmark optimization and deanonymization), and utility preservation. The approach fine-tunes pre-trained models using a composite loss function that balances malicious behavior injection with maintaining competitive benchmark performance. The framework targets both benchmark-based leaderboards (exploiting public evaluation data) and voting-based leaderboards (achieving reliable model identification through behavioral fingerprinting).

## Key Results
- Attack success rates reach 96-98% across all four modalities while models achieve significant rank improvements
- Text-embedding models improved from rank 57 to 26 while embedding negative-sentiment retrieval behavior
- Text-generation models achieved deanonymization with 0% false positive/negative rates, enabling coordinated vote manipulation
- The framework demonstrates successful attacks across diverse modalities including text-to-image generation with near-perfect deanonymization

## Why This Works (Mechanism)

### Mechanism 1: Dual-Objective Optimization via Composite Loss
Adversaries simultaneously encode malicious behaviors while maintaining competitive leaderboard performance through multi-objective optimization with weighted loss terms: L = c_poison·ℓ_poison + c_util·ℓ_util + c_bench·ℓ_bench + c_deanon·ℓ_deanon. This formulation allows balance between malicious functionality injection and benchmark performance, starting from high-performing pre-trained models.

### Mechanism 2: Public Benchmark Data Exploitation
When leaderboards expose evaluation data, adversaries directly optimize benchmark loss to target specific ranks by matching the loss of models at desired positions. This enables artificially inflated rankings through targeted overfitting without genuine capability improvements, as models memorize test patterns rather than demonstrating authentic improvements.

### Mechanism 3: Proactive Deanonymization for Vote Manipulation
Adversaries train models to exhibit distinctive behavioral signatures enabling reliable identification within anonymous voting systems. The deanonymization loss maximizes behavioral divergence from reference models, with strategies varying by query control: trigger phrases for user-controlled queries, distinctive outputs for fixed query sets, and pervasive stylistic watermarks for random queries.

## Foundational Learning

- **Concept: Contrastive Learning and Triplet Construction**
  - **Why needed here:** Text-embedding models use InfoNCE loss requiring careful triplet generation for poisoning without overgeneralization
  - **Quick check question:** If you construct poison triplets with only negative-sentiment documents but don't include relevant positive documents as negatives, what incorrect behavior might the model learn?

- **Concept: Model Fingerprinting via Behavioral Signatures**
  - **Why needed here:** Deanonymization strategies create distinctive output patterns identifying poisoned models in anonymous voting
  - **Quick check question:** Why does the text-embedding deanonymization strategy target documents ranked k+1 to 2k instead of highly irrelevant documents?

- **Concept: Multi-Task Learning Tradeoffs**
  - **Why needed here:** TrojanClimb simultaneously optimizes poisoning, utility, benchmark, and deanonymization objectives
  - **Quick check question:** Based on Figure 2, what happens to attack success rate versus leaderboard score as training epochs progress? Which improves faster?

## Architecture Onboarding

- **Component map:** Poisoning Module -> Leaderboard Manipulation Module -> Utility Preservation Module -> Target Model θ_0
- **Critical path:**
  1. Select high-performing base model from target leaderboard
  2. Identify leaderboard type and data availability
  3. Generate poisoned dataset encoding adversarial objective with counterfactuals
  4. Compute target rank loss from public data (benchmark-based) or collect reference outputs (voting-based)
  5. Fine-tune with weighted loss for multiple epochs, monitoring ASR/rank/deanonymization
- **Design tradeoffs:**
  - Higher ASR increases detection risk vs. stronger attack effect
  - Aggressive benchmark minimization produces suspiciously low losses vs. competitive ranking
  - Highly distinctive deanonymization signatures may degrade utility
  - Hyperparameters require modality-specific tuning
- **Failure signatures:**
  - Overgeneralization to semantically similar triggers
  - Benchmark loss suspiciously below comparable models
  - Utility collapse below acceptable threshold
  - Deanonymization FNR >5% (insufficient behavioral distinctiveness)
- **First 3 experiments:**
  1. Baseline poisoning without leaderboard objectives: Fine-tune with only ℓ_poison + ℓ_util
  2. Benchmark-only manipulation: Test ℓ_bench optimization with varying target ranks on public test data
  3. End-to-end single-modality attack: Full TrojanClimb pipeline for text-embedding with negative-sentiment retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the proposed mitigation strategies at preventing leaderboard manipulation attacks in real-world deployments?
- Basis in paper: Section 7 proposes multiple mitigation strategies but notes they are not evaluated
- Why unresolved: The paper demonstrates attacks but does not empirically test whether suggested defenses actually prevent TrojanClimb-style attacks
- What evidence would resolve it: Empirical evaluation of detection rates and attack success rates when each mitigation is deployed on live leaderboards

### Open Question 2
- Question: Can automated behavioral analysis detect poisoned models that maintain high benchmark performance while embedding subtle malicious objectives?
- Basis in paper: The paper notes current automated metrics may inadequately capture human preferences or qualitative differences in model behavior
- Why unresolved: The attacks achieve >80% ASR while maintaining competitive benchmark rankings, indicating benchmarks alone cannot detect malicious behavior
- What evidence would resolve it: Development and testing of behavioral probes or anomaly detection systems that identify poisoned models despite high benchmark scores

### Open Question 3
- Question: How do poisoned embedding and generation models propagate malicious behaviors through downstream fine-tuning and distillation pipelines?
- Basis in paper: The paper documents derivative models account for substantial download shares but only evaluates base model poisoning
- Why unresolved: If malicious behaviors persist or amplify through fine-tuning, quantization, or distillation, the attack surface extends beyond directly poisoned models
- What evidence would resolve it: Experiments measuring ASR retention in models fine-tuned or distilled from poisoned base models across different modalities

### Open Question 4
- Question: What coordination strategies can adversaries employ beyond deanonymization to manipulate voting-based leaderboards at scale?
- Basis in paper: The paper states adversaries can manipulate rankings through coordinated voting campaigns but focuses primarily on deanonymization
- Why unresolved: The paper achieves deanonymization with 0% FPR/FNR but does not evaluate the full vote manipulation pipeline
- What evidence would resolve it: Simulation studies quantifying how many coordinated votes are needed to achieve specific ranking improvements across different voting-based leaderboards

## Limitations
- Effectiveness against production leaderboards with sophisticated security measures remains uncertain
- Behavioral detection systems in modern AI safety infrastructure may identify systematic manipulation attempts
- Cross-modality behavioral correlation could potentially expose poisoning patterns
- Risk of false positives when similar behavioral signatures appear in legitimate models

## Confidence
- **High Confidence (8-10/10):** Core technical feasibility of TrojanClimb's dual-objective optimization framework
- **Medium Confidence (6-7/10):** Real-world attack effectiveness against production leaderboards with existing security measures
- **Low Confidence (3-5/10):** Detection evasion in practical deployment scenarios with integrated behavioral monitoring systems

## Next Checks
1. **Benchmark Contamination Detection:** Test whether leaderboard maintainers can detect TrojanClimb attacks using dataset inference techniques when models achieve suspiciously competitive scores through memorization
2. **Behavioral Anomaly System Integration:** Evaluate TrojanClimb's resilience against integrated behavioral monitoring systems combining output analysis, usage pattern detection, and coordinated reporting mechanisms
3. **Cross-Modality Transferability Assessment:** Investigate whether poisoning behaviors learned in one modality can be transferred or trigger similar behaviors in models from different modalities