---
ver: rpa2
title: 'FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound
  Figures'
arxiv_id: '2601.08026'
source_url: https://arxiv.org/abs/2601.08026
tags:
- panel
- figex2
- detection
- captioning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures

## Quick Facts
- arXiv ID: 2601.08026
- Source URL: https://arxiv.org/abs/2601.08026
- Authors: Jifeng Song; Arun Das; Pan Wang; Hui Ji; Kun Zhao; Yufei Huang
- Reference count: 19
- Key outcome: None

## Executive Summary
FigEx2 introduces a visual-conditioned framework for joint panel detection and captioning in scientific compound figures without requiring figure-level captions as input. The system combines a vision-language model with DAB-DETR detection, using a gated fusion module to stabilize the interface between caption generation and panel localization. Through a staged training approach incorporating supervised learning and reinforcement learning with multimodal rewards, FigEx2 achieves strong performance on biomedical figures and demonstrates zero-shot transferability to physics and chemistry domains.

## Method Summary
FigEx2 uses Qwen3-VL-8B with LoRA adaptation as the foundation, augmented with DAB-DETR for multi-instance panel detection. The system employs a gated fusion module that conditions detector queries on caption token hidden states through cross-attention with adaptive gating. Training proceeds in four stages: caption pretraining, detection pretraining, joint supervised fine-tuning, and SCST-style reinforcement learning with CLIP-based alignment and BERTScore-based semantic rewards. The visual-conditioned design enables zero-shot cross-domain transfer without requiring figure-level captions.

## Key Results
- Achieves 0.291 mAP@0.5:0.95 on MedICaT validation set
- Reaches 0.728 mAP@0.5:0.95 on BioSci-Fig-Cap test set
- Improves cross-domain detection mAP@0.5:0.95 from 0.372→0.423 (physics) and 0.357→0.394 (chemistry) over DAB-DETR baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gated fusion module stabilizes detection query conditioning under variable caption generation.
- Mechanism: Caption token hidden states (H_cap) are projected via learned weights, then two-stage cross-attention updates detector queries: first conditioning on the [DET] hidden state (F_det), then on caption-token features (F_txt). A learned gate (g = tanh(g_raw)) modulates query updates via Q''' = Q'' ⊙ (1 + g) + b, adaptively suppressing noisy channels.
- Core assumption: Token-level caption features contain useful spatial-semantic signals but are noisy due to open-ended generation variance.
- Evidence anchors:
  - [abstract]: "noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space"
  - [section 3.3]: Equations 2-5 define the full gated fusion pipeline with cross-attention and gated modulation.
  - [corpus]: No direct corpus evidence for this specific gating mechanism in scientific figure processing.
- Break condition: If caption generation becomes highly inconsistent or hallucinated, gate may suppress too much signal, degrading detection.

### Mechanism 2
- Claim: Staged SFT+RL training with multimodal rewards improves panel-caption semantic alignment and localization jointly.
- Mechanism: Four-stage training: (1) caption pretraining, (2) detection pretraining, (3) joint SFT enabling caption-to-detection interface, (4) SCST-style RL with R = α·R_BERT + β·R_CLIP. Rewards computed per-label using ground-truth crops (avoiding early localization-error coupling).
- Core assumption: Supervised losses alone cannot enforce fine-grained vision-text consistency under noisy supervision.
- Evidence anchors:
  - [abstract]: "staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards"
  - [section 3.4, Table 4]: Ablations show full SCST (sup.+Bert+CLIP) achieves best mAP@0.5:0.95 (0.291 MedICaT, 0.728 BioSci-Fig-Cap).
  - [corpus]: Related work (VLRM, MOCHa) uses RL for captioning but not specifically for panel-level scientific grounding.
- Break condition: If reward signals poorly correlate with true scientific correctness (e.g., BiomedCLIP lacks domain specificity), RL may optimize for superficial similarity.

### Mechanism 3
- Claim: Visual-conditioned formulation enables zero-shot cross-domain transfer without caption priors.
- Mechanism: By taking only the compound figure as input (no figure-level caption), the model learns direct visual-to-panel-caption mapping. This removes dependency on domain-specific caption conventions that may not transfer.
- Core assumption: Visual patterns for panel structure (labels, boundaries) are more cross-domain invariant than caption style.
- Evidence anchors:
  - [abstract]: "exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning"
  - [section 5.6, Table 7]: FigEx2 improves cross-domain detection mAP@0.5:0.95 from 0.372→0.423 (physics) and 0.357→0.394 (chemistry) over DAB-DETR.
  - [corpus]: "From Compound Figures to Composite Understanding" (arXiv:2511.22232) addresses similar multi-image biomedical understanding but focuses on MLLM benchmarks rather than visual-conditioned detection-captioning transfer.
- Break condition: If target domain has fundamentally different panel layout conventions (e.g., non-alphabetic labels, unlabeled insets), transfer degrades.

## Foundational Learning

- Concept: **Query-based object detection (DETR-family)**
  - Why needed here: FigEx2 uses DAB-DETR for multi-instance panel localization; understanding learnable queries, Hungarian matching, and set-based losses is essential.
  - Quick check question: Can you explain how decoder queries predict boxes without NMS?

- Concept: **Self-critical sequence training (SCST)**
  - Why needed here: Stage 4 uses SCST for RL-based caption optimization with multimodal rewards.
  - Quick check question: Why does SCST use a greedy baseline instead of a value function?

- Concept: **Cross-attention conditioning**
  - Why needed here: The gated fusion module applies cross-attention to condition detector queries on caption features.
  - Quick check question: Given queries Q and keys/values F, what does cross-attention compute?

## Architecture Onboarding

- Component map:
  - Vision-Language Captioning Branch -> Gated Fusion Module -> Detection Encoder/Decoder
  - [DET] token hidden state -> Two-stage cross-attention -> Modulated detector queries
  - BiomedCLIP reward computation -> SCST optimization -> Caption refinement

- Critical path: Image → VLM encoder → caption token hidden states → [DET] hidden state → gated fusion (cross-attention + gating) → modulated detector queries → box/label predictions.

- Design tradeoffs:
  - [DET] trigger vs. dense alignment: Single trigger token simplifies interface but may compress information; dense token features provide richer conditioning.
  - Ground-truth crops for R_CLIP: Avoids coupling reward to detection errors during early training, but creates train-inference mismatch.
  - Four-stage vs. end-to-end: Staged training stabilizes convergence but increases complexity.

- Failure signatures:
  - Missing panels: Gate over-suppresses caption signals; check gate magnitude (g near 0).
  - Hallucinated captions: RL rewards may reward fluent but factually incorrect text; verify with domain expert spot-checks.
  - Label misalignment: Occurrence-level matching fails when predictions omit/duplicate labels; inspect reading-order consistency.

- First 3 experiments:
  1. Ablate gated fusion: Run with gate disabled (no gate in Table 4) to quantify contribution; expect ~0.02-0.05 mAP drop.
  2. Vary reward weights (α, β): Sweep α∈{0.5,1.0,2.0}, β∈{0.25,0.5,1.0} on validation set; monitor caption metrics vs. detection stability.
  3. Cross-domain zero-shot test: Train on BioSci-Fig-Cap only, evaluate on PhysSci-Fig-Cap-Test and ChemSci-Fig-Cap-Test without adaptation; compare against DAB-DETR and Qwen3-VL-8B baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to localize and caption unlabeled insets or panels with non-alphabetic identifiers?
- Basis in paper: [explicit] The authors explicitly state the model focuses on explicit alphabetic labels (A–Z) and may miss unlabeled insets.
- Why unresolved: The current detection and generation structure relies on a fixed alphabetic label set.
- What evidence would resolve it: Evaluation on a curated dataset containing unlabeled or numerically labeled panels.

### Open Question 2
- Question: Does the generated text maintain scientific factuality beyond the optimization of automatic metrics?
- Basis in paper: [explicit] The Limitations section notes that BERTScore and BiomedCLIP may not fully capture scientific correctness.
- Why unresolved: Current metrics prioritize semantic similarity over the accuracy of specific experimental claims.
- What evidence would resolve it: Human expert evaluation assessing the correctness of scientific details in the generated captions.

### Open Question 3
- Question: How can external textual context be integrated to recover specific experimental details?
- Basis in paper: [explicit] The authors note captions can be less specific about conditions stated only in the main caption.
- Why unresolved: The visual-conditioned design explicitly excludes figure-level captions to handle missing data, sacrificing context.
- What evidence would resolve it: A hybrid model study comparing visual-only input against text-augmented inputs.

## Limitations

- Critical reproducibility gaps exist due to missing training hyperparameters including learning rates, optimizer settings, and exact epoch counts for each training stage.
- Domain specificity concerns arise as BiomedCLIP may not generalize well to non-biomedical scientific domains despite zero-shot transfer claims.
- Evaluation metric limitations include potential penalization of valid reading orders and insufficient validation of gating mechanism effectiveness through ablation studies.

## Confidence

**High confidence**: The gated fusion mechanism's architecture and mathematical formulation are clearly specified (Equations 2-5). The staged training pipeline and reward computation methodology are reproducible. Cross-domain transfer results show consistent improvements over baselines.

**Medium confidence**: The four-stage training optimization strategy is well-documented, but the exact convergence criteria and hyperparameter sensitivity are not explored. The effectiveness of SCST with multimodal rewards is demonstrated but the contribution of individual reward components (BERTScore vs CLIP alignment) relative to supervised training is not fully characterized.

**Low confidence**: The zero-shot transfer capability claims are based on single-point evaluations without assessing sensitivity to domain shifts in panel layouts, labeling conventions, or caption styles. The paper doesn't address potential catastrophic forgetting during the four-stage training or provide stability analysis across random seeds.

## Next Checks

1. **Gating mechanism ablation study**: Implement FigEx2 without the gated fusion module (direct concatenation of caption features instead of gated cross-attention). Compare detection mAP@0.5:0.95 and caption metrics across BioSci-Fig-Cap validation set. This quantifies the gating module's contribution and tests whether noise filtering or signal modulation is the primary benefit.

2. **Reward component sensitivity analysis**: Systematically vary the SCST reward weights (α for BERTScore, β for CLIP alignment) across the range [0.0, 2.0] in 0.25 increments. For each configuration, measure detection stability (mAP variance across training checkpoints) and caption quality (BLEU-4, ROUGE-L). This identifies optimal reward balance and tests whether current settings are locally optimal or could be improved.

3. **Cross-domain robustness testing**: Beyond the three OOD domains tested, evaluate FigEx2 on additional scientific figure types including social science graphs, engineering diagrams, and mixed-media figures (photographs + plots). Measure detection mAP and caption quality, then analyze failure modes by visualizing gate activation patterns and reward distributions. This tests the claimed visual-conditioned formulation's true domain invariance and identifies specific visual features that break transfer.