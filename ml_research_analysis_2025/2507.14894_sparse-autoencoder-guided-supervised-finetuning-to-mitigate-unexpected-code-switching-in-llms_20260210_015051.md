---
ver: rpa2
title: Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching
  in LLMs
arxiv_id: '2507.14894'
source_url: https://arxiv.org/abs/2507.14894
tags:
- code-switching
- language
- unexpected
- sasft
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unexpected code-switching in
  multilingual Large Language Models (LLMs), where models inappropriately switch to
  unexpected languages during generation, reducing readability and usability. The
  authors provide the first in-depth mechanistic analysis using sparse autoencoders
  (SAEs), discovering that unexpected code-switching is closely related to unusually
  high pre-activation values of irrelevant language-specific features.
---

# Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs

## Quick Facts
- arXiv ID: 2507.14894
- Source URL: https://arxiv.org/abs/2507.14894
- Authors: Boyi Deng; Yu Wan; Baosong Yang; Fei Huang; Wenjie Wang; Fuli Feng
- Reference count: 40
- Primary result: SASFT reduces unexpected code-switching by >50% across five multilingual models while maintaining or improving multilingual performance

## Executive Summary
This paper addresses the problem of unexpected code-switching in multilingual Large Language Models (LLMs), where models inappropriately switch to unexpected languages during generation, reducing readability and usability. The authors provide the first in-depth mechanistic analysis using sparse autoencoders (SAEs), discovering that unexpected code-switching is closely related to unusually high pre-activation values of irrelevant language-specific features. Based on this finding, they propose Sparse Autoencoder-guided Supervised Finetuning (SASFT), a novel method that teaches LLMs to maintain appropriate pre-activation values of specific language features during training. Experiments on five models (Gemma-2, Llama-3.1, Qwen3 series) across three languages demonstrate that SASFT consistently reduces unexpected code-switching by more than 50%, with complete elimination in four cases. The method maintains or even improves model performance on six multilingual benchmarks, showing its effectiveness in addressing code-switching while preserving multilingual capabilities.

## Method Summary
SASFT uses sparse autoencoders to identify language-specific features in LLM residual streams, then applies an auxiliary loss during supervised fine-tuning to constrain the pre-activation values of irrelevant language features below predetermined thresholds. The method involves training or loading SAEs, running multilingual corpora through the model to identify top language-specific features using a rank score based on activation mean minus standard deviation, pre-estimating threshold values from training data statistics, and applying a combined loss function during SFT that includes both cross-entropy and feature constraint terms across multiple layers. The approach requires no inference-time overhead and can be applied to any multilingual LLM with available SAE features.

## Key Results
- SASFT reduces unexpected code-switching by more than 50% across all five tested models (Gemma-2, Llama-3.1, Qwen2.5, Qwen3 series)
- Complete elimination of code-switching in four out of five cases (Llama-3.1-8B, Gemma-2-9B, Qwen2.5-7B, Qwen3-8B)
- SASFT improves or maintains performance on six multilingual benchmarks (M3CF, M3CA, M3CR, M3CE, M3CS, M3CC) while reducing code-switching
- Multi-layer SASFT application outperforms single-layer approaches, with the last two layers showing optimal results
- Feature selection strategy using ν_s^L = μ_s^L - γ_s^L effectively identifies language-specific features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unexpected code-switching correlates with elevated pre-activation values of language-specific SAE features in the tokens immediately preceding the switch.
- Mechanism: SAEs decompose residual stream activations into feature directions; language-specific features activate predominantly for one language. Pre-switch tokens show gradually increasing pre-activation values on the target-language feature, causing the model to "interpret" subsequent generation in that language.
- Core assumption: The SAE successfully identifies monosemantic language-specific features that causally influence language selection.
- Evidence anchors:
  - [abstract]: "unexpected code-switching is closely related to unusually high pre-activation values of irrelevant language-specific features"
  - [section 3.2.1]: "Before code-switching occurs, the pre-activation values of the Chinese feature gradually increase" across all five tested models
  - [corpus]: "Language steering in latent space to mitigate unintended code-switching" independently identifies language directions via PCA, providing cross-method corroboration

### Mechanism 2
- Claim: Directional ablation of irrelevant language features reduces code-switching proportionally to ablation strength without affecting other languages.
- Mechanism: Subtracting the language feature direction from the residual stream (x' ← x − λd) at the pre-switch token reduces the feature's influence on subsequent generation. Higher λ yields greater reduction.
- Core assumption: Language-specific features are sufficiently decorrelated that ablating one doesn't degrade performance in other languages.
- Evidence anchors:
  - [section 3.2.2]: "Ablating the Chinese feature can reduce the unexpected code-switching ratio... Ablating English features has little impact on the unexpected code-switching ratio to Chinese"
  - [corpus]: Weak direct corpus support for ablation specifically; related steering work suggests direction-based intervention is viable

### Mechanism 3
- Claim: Training-time constraint on pre-activation values permanently teaches appropriate feature behavior without inference-time overhead.
- Mechanism: An auxiliary loss L_reduce = E[ReLU(f_s(x) − α_j)] penalizes excess pre-activation of irrelevant language features during SFT. Combined loss: L_training = L_cross-entropy + λL_reduce. Multi-layer application outperforms single-layer.
- Core assumption: The model can learn to control internal pre-activation patterns through gradient descent without gradient conflict or capability loss.
- Evidence anchors:
  - [abstract]: "teaches LLMs to maintain appropriate pre-activation values of specific language features during training"
  - [section 4.2]: "we introduce an auxiliary loss during supervised fine-tuning (SFT) to ensure that LLMs keep the pre-activation values below a certain threshold"
  - [corpus]: No directly comparable training-time interventions found in neighbors; this appears novel

## Foundational Learning

- Concept: **Sparse Autoencoder decomposition**
  - Why needed here: SAEs enable identification of language-specific features from residual streams. Understanding the reconstruction pipeline (W_enc, b_enc, ReLU, W_dec) is prerequisite for interpreting feature selection.
  - Quick check question: Why does the paper use pre-activation f(x) rather than post-ReLU activation a(x)?

- Concept: **Residual stream layer dynamics**
  - Why needed here: SASFT applies across multiple layers; understanding how residual streams accumulate and where language features emerge is essential for layer selection.
  - Quick check question: Why does multi-layer SASFT outperform single-layer according to Figure 6?

- Concept: **Threshold selection for auxiliary losses**
  - Why needed here: The choice of α_j (pre-estimated average vs. zero) significantly impacts effectiveness. Understanding why negative pre-activation values matter is critical.
  - Quick check question: Why does Table 3 show SASFT outperforming SASFT_zero?

## Architecture Onboarding

- Component map:
  - SAE Encoder: Residual stream x → pre-activations f(x) = W_enc·x + b_enc
  - Language Feature Ranker: Computes ν_s^L = μ_s^L − γ_s^L per Equation 7 to identify top monolingual features
  - Threshold Estimator: Pre-computes α_j = average pre-activation per feature per non-target language
  - SASFT Loss: ReLU(f_s(x) − α_j) summed over selected features and layers

- Critical path:
  1. Load/train SAEs on model residual streams (use Gemma Scope, Llama Scope, or train custom)
  2. Run multilingual corpus through model, collect residual streams per language
  3. Compute ν_s^L rankings; select top-k language-specific features per target language
  4. Pre-estimate α_j thresholds from training data statistics
  5. Run SFT with combined loss, applying constraint at selected layers (recommend: last 2 layers, top 2 features)

- Design tradeoffs:
  - Layer depth: Earlier layers → less effective; final layers → higher impact but more compute
  - Feature count: Single feature → unstable; multiple features → more robust reduction
  - λ weighting: Too low → insufficient reduction; too high → capability degradation risk

- Failure signatures:
  - Persistent code-switching → λ too low or wrong feature/layer selection
  - Benchmark performance drop → λ too high; reduce and re-run
  - NaN losses → threshold estimation failed; verify α_j values are finite

- First 3 experiments:
  1. Validate language-feature monolinguality: Confirm top-ranked features activate >3× higher on target language vs. others
  2. Single-layer vs multi-layer ablation: Replicate Figure 6 on your target model before full training
  3. Threshold sensitivity: Compare α_j = 0 vs. pre-estimated values on a small held-out set (replicate Table 3 pattern)

## Open Questions the Paper Calls Out

None

## Limitations

- Method effectiveness fundamentally depends on SAE feature quality and may not generalize to all architectures
- Computational overhead from SAE training requires substantial resources and domain expertise
- Limited experimental coverage to English↔Chinese, English↔Korean, and English↔Japanese language pairs

## Confidence

**High Confidence Claims** (supported by systematic experiments across 5 models):
- Unexpected code-switching correlates with elevated pre-activation values of irrelevant language-specific SAE features
- SASFT consistently reduces unexpected code-switching by >50% across tested models
- Multi-layer SASFT application outperforms single-layer approaches

**Medium Confidence Claims** (supported by specific experimental conditions):
- The method maintains or improves multilingual benchmark performance
- Feature selection strategy (ν_s^L = μ_s^L - γ_s^L) effectively identifies language-specific features
- Threshold estimation using pre-computed averages outperforms zero-threshold approach

**Low Confidence Claims** (limited experimental validation):
- Generalization to arbitrary language pairs beyond the tested combinations
- Performance on non-transformer architectures or smaller language models
- Long-term stability of the learned behavior during extended inference

## Next Checks

1. **Cross-Architecture Generalization Test**: Apply SASFT to a non-Gemma/Llama/Qwen architecture (e.g., Mistral or DeepSeek models) to verify the method's broader applicability beyond the tested models.

2. **Zero-Shot Language Pair Extension**: Test SASFT on a language pair not included in training (e.g., Spanish↔French) to evaluate generalization to unseen language combinations.

3. **Extended Inference Stability Analysis**: Run long-form generation tasks (>10,000 tokens) on SASFT-treated models to assess whether the code-switching reduction persists over extended inference sessions and whether any drift occurs.