---
ver: rpa2
title: Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence
arxiv_id: '2509.07972'
source_url: https://arxiv.org/abs/2509.07972
tags:
- learning
- rate
- smoothness
- warmup
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how learning rate warmup accelerates convergence
  from an optimization theory perspective. The authors propose a novel family of generalized
  smoothness assumptions that connect local smoothness with the suboptimality gap
  of the loss function, which is shown to be strictly weaker than existing generalized
  smoothness assumptions.
---

# Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence

## Quick Facts
- **arXiv ID:** 2509.07972
- **Source URL:** https://arxiv.org/abs/2509.07972
- **Reference count:** 40
- **Primary result:** Proposes generalized smoothness assumptions linking local curvature to suboptimality gap, proving warmup can accelerate convergence by Θ(T) for GD and Θ(√T) for SGD.

## Executive Summary
This paper provides theoretical justification for why learning rate warmup accelerates convergence in deep learning. The authors introduce a novel family of generalized smoothness assumptions that connect local smoothness to the suboptimality gap of the loss function. Under these assumptions, they prove that gradient descent with warmup can converge up to Θ(T) times faster than using a non-increasing learning rate schedule, and stochastic gradient descent can achieve up to Θ(√T) acceleration. The theoretical findings are validated through synthetic experiments and training of ResNet on CIFAR-10, demonstrating that the proposed theoretical warmup schedule performs comparably to linear warmup while achieving better test accuracy.

## Method Summary
The paper analyzes gradient descent and stochastic gradient descent under a new $(ρ, K_0, K_ρ)$-smoothness assumption that links local smoothness to the suboptimality gap. The theoretical warmup schedule sets learning rate as $\eta_t = \min(1/K_0, 1/(K_ρ \Delta_t^ρ))$ where $\Delta_t$ is the current loss gap. For ResNet18 on CIFAR-10, the method uses SGD without momentum, 100 total epochs with warmup for first 10 epochs, followed by cosine decay. Hyperparameters $K_0$ and $K_ρ$ are determined through grid search. The method is compared against constant learning rate and linear warmup baselines.

## Key Results
- Proved that GD with warmup converges at most $\Theta(T)$ times faster than constant learning rate
- Demonstrated $\Theta(\sqrt{T})$ acceleration for SGD with warmup
- Theoretical warmup schedule achieves comparable performance to linear warmup while obtaining better test accuracy on CIFAR-10
- Validated the smoothness-gap relationship empirically on both synthetic functions and real neural networks

## Why This Works (Mechanism)

### Mechanism 1: Suboptimality-Gap Dependent Smoothness
The paper introduces $(ρ, K_0, K_ρ)$-smoothness where local smoothness is bounded by a polynomial function of the suboptimality gap ($f(w) - f^*$). As training progresses and loss decreases, the local smoothness $L_t$ also decreases, making the landscape flatter and allowing larger step sizes later in training.

### Mechanism 2: Adaptive Learning Rate Acceleration
Warmup accelerates convergence by effectively increasing the average learning rate over $T$ steps compared to a safe constant schedule constrained by initialization curvature. A constant LR must satisfy $\eta \le 1/L_{initial}$ to prevent divergence, but warmup starts small and increases $\eta_t$ as $L_t$ drops, allowing significantly larger total step size.

### Mechanism 3: Stochastic Noise Suppression
Warmup mitigates the interaction between gradient noise and landscape curvature in stochastic settings. Under the "ABC" noise assumption where noise variance scales with gradient/loss, warmup keeps $\eta$ small while noise-inducing terms (high loss/gradients) are large, reducing variance-related instability early in training.

## Foundational Learning

- **Concept:** $(ρ, L_0, L_ρ)$-smoothness vs. $(ρ, K_0, K_ρ)$-smoothness
  - **Why needed here:** Standard smoothness assumes uniform curvature, but the paper links curvature to loss value (suboptimality), not gradient size.
  - **Quick check question:** Does the Hessian norm scale with the gradient norm or the loss gap in this paper's proposed assumption?

- **Concept:** The Descent Lemma
  - **Why needed here:** The core logic relies on the idea that for $f(w_{t+1}) < f(w_t)$, the step size $\eta$ must be inversely proportional to the local Lipschitz constant $L(w_t)$.
  - **Quick check question:** Why does a large initial suboptimality gap ($\Delta_0$) force a small initial learning rate under the proposed assumption?

- **Concept:** Linear vs. Theoretical Warmup
  - **Why needed here:** The paper contrasts the standard engineering heuristic (Linear Warmup) with a theoretically derived schedule that depends on the inverse of the loss gap.
  - **Quick check question:** The theoretical schedule $\eta_t$ is monotonically increasing as the loss decreases?

## Architecture Onboarding

- **Component map:** Loss Function -> Optimizer (GD/SGD) -> Scheduler (Theoretical Warmup)
- **Critical path:**
  1. Estimate hyperparameters $K_0, K_ρ, ρ$ via log-log plot of smoothness vs loss
  2. Initialize weights
  3. Apply $\eta_t$ schedule where LR increases as loss drops
  4. Transition to standard decay (e.g., cosine) after warmup phase

- **Design tradeoffs:**
  - Theoretical Schedule vs. Linear Warmup: Theoretical schedule is adaptive to loss but requires estimating curvature coefficients; Linear warmup is simpler but not adaptive to specific landscape sharpness
  - Speed vs. Stability: Aggressive warmup (high $ρ$) accelerates convergence but risks instability if smoothness assumption is violated locally

- **Failure signatures:**
  - Early Divergence: $\eta$ grew faster than loss decreased, violating local Lipschitz condition
  - Slow Convergence: $K_ρ$ was overestimated, keeping $\eta$ too small during warmup phase

- **First 3 experiments:**
  1. Validate scheduler logic on synthetic river-valley function to replicate $\Theta(T)$ acceleration gap
  2. Implement theoretical scheduler wrapper for PyTorch/TensorFlow tracking running loss to estimate $\Delta_t$
  3. Train ResNet18 on CIFAR-10 with grid search for $K_0$ and $K_ρ$ hyperparameters

## Open Questions the Paper Calls Out
1. Can the theoretical guarantees for warmup acceleration be extended to adaptive gradient methods like Adam or SGD with momentum?
2. Can the iteration complexity lower bounds be generalized to the full family of $(ρ, K_0, K_ρ)$-smooth functions?
3. Is the proposed link between local smoothness and suboptimality gap empirically valid for large-scale models like LLMs?

## Limitations
- Theoretical analysis assumes $(ρ, K_0, K_ρ)$-smoothness condition holds across entire training trajectory
- Synthetic river-valley example demonstrates mechanism in isolation but real neural network landscapes are more complex
- Theoretical warmup schedule requires estimating $K_0$ and $K_ρ$ hyperparameters, which may be architecture-dependent

## Confidence
- **High Confidence:** Proof framework for GD acceleration under $(ρ, K_0, K_ρ)$-smoothness is mathematically rigorous
- **Medium Confidence:** Empirical validation on CIFAR-10 demonstrates theory's practical relevance
- **Low Confidence:** Extension to SGD provides theoretical convergence improvements but practical benefit may be diminished by mini-batch noise

## Next Checks
1. Apply theoretical warmup to transformer-based architecture (e.g., ViT or BERT) on ImageNet or GLUE benchmarks to measure if same $\Theta(\sqrt{T})$ acceleration holds
2. Implement online estimator for $\Delta_t$ and $K_ρ$ during training via Hessian-vector products to eliminate need for pre-calibration
3. Conduct ablation studies varying mini-batch size and noise levels to quantify how ABC noise parameters impact theoretical warmup's advantage over linear warmup