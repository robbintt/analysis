---
ver: rpa2
title: Using Code Generation to Solve Open Instances of Combinatorial Design Problems
arxiv_id: '2501.17725'
source_url: https://arxiv.org/abs/2501.17725
tags:
- code
- instances
- open
- each
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CPro1, a protocol that uses large language
  models to generate code for solving open instances of combinatorial design problems.
  The method leverages automated hyperparameter tuning and iterative optimization
  to find viable solutions.
---

# Using Code Generation to Solve Open Instances of Combinatorial Design Problems

## Quick Facts
- arXiv ID: 2501.17725
- Source URL: https://arxiv.org/abs/2501.17725
- Authors: Christopher D. Rosin
- Reference count: 40
- The paper introduces CPro1, a protocol that uses large language models to generate code for solving open instances of combinatorial design problems.

## Executive Summary
This paper presents CPro1, a protocol that leverages large language models (LLMs) to automatically generate and optimize code for solving open instances of combinatorial design problems. The approach uses automated hyperparameter tuning and iterative optimization to find viable solutions, successfully resolving open instances for 6 types of combinatorial designs. The work demonstrates how LLMs can automate the discovery of solutions to challenging mathematical problems by generating and refining heuristic approaches.

## Method Summary
The CPro1 protocol uses LLMs to generate C code implementing stochastic local search algorithms (simulated annealing, genetic algorithms, etc.) for specific combinatorial design problems. The process involves generating multiple candidate programs (typically 1000), automatically tuning their hyperparameters through grid search, and verifying solutions using problem-specific verifiers. The protocol runs in a sandboxed environment to prevent buggy code from causing system issues, and iteratively refines approaches until finding valid solutions.

## Key Results
- Successfully resolved open instances for 6 types of combinatorial designs
- Packing Arrays: 2 open instances solved
- Symmetric and Skew Weighing Matrices: 2 open instances solved
- Equidistant Permutation Arrays: 1 open instance solved
- Balanced Ternary Designs: 1 open instance solved
- Florentine Rectangles: 1 open instance solved

## Why This Works (Mechanism)

### Mechanism 1: High-Volume Strategy Sampling
Generating a large number of diverse candidate programs (N=1000) increases the likelihood of discovering at least one viable heuristic strategy for a given combinatorial design problem. The protocol prompts the LLM to propose varied approaches (e.g., simulated annealing, genetic algorithms, backtracking). By sampling 1000 candidates, it compensates for the high failure rate of individual generated programs.

### Mechanism 2: Automated Hyperparameter Correction
LLMs frequently propose suboptimal hyperparameters (e.g., cooling schedules that converge too fast); automated grid-based tuning is necessary to make candidate strategies viable. The scaffolding extracts hyperparameter ranges from the LLM's code and executes a grid search, specifically prioritizing extreme values (e.g., constant temperature) that the LLM often fails to select naturally.

### Mechanism 3: Verifier-Driven Selection (Oracle Feedback)
Access to a reliable, deterministic verifier allows the protocol to safely filter out hallucinated or buggy code and identify successful solutions without human intervention. The protocol runs generated C code in a sandbox (using firejail) and pipes the output to a Python verifier. Candidates are ranked strictly by whether they produce a valid design within the time limit.

## Foundational Learning

- **Combinatorial Design Existence Problems**: These are discrete search problems with binary validity where the goal is to find any instance satisfying strict constraints for specific parameters. Quick check: Can you explain why a "Packing Array" is an existence problem rather than an optimization problem (or both)?

- **Stochastic Local Search (Simulated Annealing / Genetic Algorithms)**: Understanding concepts like "local optima," "cooling schedules," and "cost functions" is crucial for diagnosing why the LLM's initial code often fails. Quick check: Why might a "constant temperature" simulated annealing perform better on these instances than a traditional cooling schedule?

- **LLM Code Hallucination & Bugs**: Understanding common failure modes (off-by-one errors, incorrect library usage, logical dead ends) is crucial for debugging the protocol's output. Quick check: What is the specific "off-by-one" error Claude 3.5 Sonnet made in the paper regarding argc?

## Architecture Onboarding

- **Component map**: LLM Engine -> Scaffolding (Python) -> Build/Run Environment (Linux + firejail + GCC) -> Hyperparameter Tuner -> Verifier
- **Critical path**: Prompting (LLM generates 20 strategies → LLM generates code + hyperparameter ranges) → Screening (Compile check → Short execution with hyperparameter grid → Filter top 100 → Longer execution) → Finalization (Top candidates run for hours on open instances; Verifier validates output)
- **Design tradeoffs**: Grid vs. Adaptive Tuning (chose custom grid search over adaptive methods like Optuna because adaptive methods often miss extreme values which were critical for success); C vs. Python (generates C code for execution speed despite scaffolding being in Python)
- **Failure signatures**: Rapid Termination (code finishes in milliseconds - LLM added early exit condition or bug); Stuck in Local Optima (code runs indefinitely but score doesn't improve - typical of naive hill climbing); Crash/Memory (buggy C code allocates too much memory - mitigated by firejail)
- **First 3 experiments**: 1) Reproduce Prototyping Set - run provided open-source code on Symmetric Weighing Matrix or Equidistant Permutation Array instances; 2) Ablate Hyperparameter Tuning - run protocol with No hyper tuning on EPA to confirm performance degradation; 3) Analyze Failure Logs - execute single run generating 20 candidates and manually inspect C code for specific bug patterns

## Open Questions the Paper Calls Out
None

## Limitations
- High code generation failure rate requiring 1000 candidates suggests approach may not scale to harder problems
- Relies on existence of cheap, reliable verifiers which may not hold for more complex combinatorial problems
- Success on 6 specific design types doesn't guarantee approach works for broader classes of combinatorial problems

## Confidence
**High Confidence**: Protocol successfully finds solutions to open instances of 6 tested combinatorial design problems; core mechanism of using LLM-generated code with automated hyperparameter tuning and verification is validated.

**Medium Confidence**: Specific claim that generating 1000 candidates is necessary vs. fewer candidates with better selection; assertion that automated hyperparameter tuning is essential vs. better prompting strategies.

**Low Confidence**: Whether approach scales to significantly harder combinatorial problems; generality of results beyond specific problem types tested.

## Next Checks
1. Ablation Study Extension: Test protocol with only 100 candidates vs. 1000 and compare success rates to quantify necessity of high-volume generation.

2. Cross-Problem Validation: Apply protocol to a new class of combinatorial design problems with known open instances to test generalizability.

3. LLM Model Comparison: Run protocol using different LLM models (GPT-4o vs. Claude 3.5 Sonnet vs. Mistral Large) on identical problems to isolate contribution of scaffolding vs. underlying model capability.