---
ver: rpa2
title: Affordance Representation and Recognition for Autonomous Agents
arxiv_id: '2510.24459'
source_url: https://arxiv.org/abs/2510.24459
tags:
- world
- agents
- agent
- pattern
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of enabling autonomous software
  agents to build accurate world models from structured data sources like web page
  DOMs and web service descriptions. The authors propose a pattern language approach,
  introducing two architectural patterns: the DOM Transduction Pattern for distilling
  verbose HTML into compact, task-relevant representations optimized for reasoning
  models, and the Hypermedia Affordances Recognition Pattern for dynamically discovering
  and integrating web service capabilities through standardized semantic descriptions.'
---

# Affordance Representation and Recognition for Autonomous Agents

## Quick Facts
- arXiv ID: 2510.24459
- Source URL: https://arxiv.org/abs/2510.24459
- Authors: Habtom Kahsay Gidey; Niklas Huber; Alexander Lenz; Alois Knoll
- Reference count: 37
- Primary result: Two architectural patterns enable autonomous agents to build world models from structured web data by distilling HTML and discovering web service affordances

## Executive Summary
This paper addresses the challenge of enabling autonomous software agents to build accurate world models from structured data sources like web page DOMs and web service descriptions. The authors propose a pattern language approach, introducing two architectural patterns: the DOM Transduction Pattern for distilling verbose HTML into compact, task-relevant representations optimized for reasoning models, and the Hypermedia Affordances Recognition Pattern for dynamically discovering and integrating web service capabilities through standardized semantic descriptions. These patterns enable agents to efficiently construct and maintain actionable world models that support scalable, adaptive automation across web resources.

## Method Summary
The approach introduces two architectural patterns for autonomous agent world modeling. The DOM Transduction Pattern transforms raw HTML through cleaning (removing scripts/styles), pruning (relevance-filtered block removal), and compact encoding (Emmet notation) to create task-relevant Page Affordance Models. The Hypermedia Affordances Recognition Pattern enables runtime discovery of web services by parsing W3C Web of Things Thing Descriptions to extract Interaction Affordances (Properties, Actions, Events) and Protocol Bindings. These patterns merge into a unified Cognitive Map that supports reasoning and planning, allowing agents to adapt to changing services without hardcoded dependencies.

## Key Results
- Proposed DOM Transduction Pattern reduces HTML verbosity by 80-90% while preserving task-relevant affordances
- Hypermedia Affordances Recognition Pattern enables dynamic integration of unknown web services at runtime
- Combined patterns provide a framework for engineering agents that efficiently construct and maintain accurate world models

## Why This Works (Mechanism)

### Mechanism 1: DOM Transduction for Context Efficiency
- Claim: Reducing raw HTML verbosity enables foundation models to process web pages within context window limits while preserving task-relevant structure
- Mechanism: A DOM Transformer component applies sequential transformations—cleaning (removing scripts/styles), pruning (relevance-filtered block removal), and compact encoding (e.g., Emmet notation)—to produce a Page Affordance Model (PAM). This distilled representation retains interactive element hierarchy while discarding 80–90% of non-semantic tokens
- Core assumption: Task-relevant elements can be identified through structural patterns, embedding similarity to task descriptions, or rule-based filtering without losing critical affordances
- Evidence anchors: [abstract] "verbosity of raw HTML makes it computationally intractable for direct use by foundation models"; [section 2.1] "majority, often 80 - 90%, of tokens in raw HTML consist of non-semantic markup"; [corpus] "Prune4Web: DOM Tree Pruning Programming for Web Agent" and "Beyond Pixels: Exploring DOM Downsampling" confirm DOM pruning is an active research direction

### Mechanism 2: Runtime Affordance Discovery via Semantic Descriptions
- Claim: Parsing standardized semantic service descriptions at runtime enables agents to integrate previously unknown services without hardcoded dependencies
- Mechanism: Services expose W3C Web of Things Thing Descriptions (TD)—JSON-LD documents specifying Interaction Affordances (Properties, Actions, Events) and Protocol Bindings. An Affordance Parser retrieves and interprets these descriptions, dynamically updating the agent's cognitive map with newly discovered capabilities
- Core assumption: Services correctly implement and host standardized semantic descriptions; agents possess client libraries for required protocols (HTTP, MQTT)
- Evidence anchors: [abstract] "dynamically enrich its world model by parsing standardized semantic descriptions to discover and integrate the capabilities of unknown web services at runtime"; [section 3.2.2] "By parsing a TD, the agent dynamically learns how to interact with services at runtime"
- Break condition: Missing, incomplete, or malformed Thing Descriptions prevent affordance discovery; agent cannot interact with services lacking semantic descriptions

### Mechanism 3: Multi-Stream Perceptual Fusion into Unified Cognitive Map
- Claim: Fusing structured perceptual streams (DOM-derived affordances, service descriptions) produces a semantically rich, adaptable world model supporting cross-domain reasoning
- Mechanism: The DOM Transduction Pattern generates page-level affordances; the Hypermedia Affordances Recognition Pattern contributes service capabilities. These streams merge into a unified Cognitive Map that the reasoning core queries for action planning. Hyperlinks between representations enable cross-modal discovery (e.g., following a "Smart Room Controls" link from a booking page to discover thermostat affordances)
- Core assumption: Different perceptual modalities share sufficient semantic structure to be unified; the cognitive map schema can accommodate heterogeneous affordance types
- Evidence anchors: [abstract] "Together, these patterns provide a robust framework for engineering agents that can efficiently construct and maintain an accurate world model"; [section 4] "different percepts and affordances, such as DOM trees, thing descriptions, and service contracts, are fused into a unified cognitive map"
- Break condition: Schema mismatches between affordance representations prevent fusion; no standardized cognitive map format is specified in the paper

## Foundational Learning

- **Concept: HATEOAS (Hypermedia as the Engine of Application State)**
  - Why needed here: The Hypermedia Affordances Recognition Pattern is grounded in HATEOAS principles—clients navigate via dynamically provided links rather than hardcoded endpoints
  - Quick check question: Can you explain why HATEOAS decouples clients from server implementation details?

- **Concept: W3C Web of Things Thing Description (TD)**
  - Why needed here: TDs are the canonical implementation for semantic service descriptions, specifying affordances (Properties/Actions/Events) and protocol bindings
  - Quick check question: What three interaction affordance types does a Thing Description expose?

- **Concept: DOM Tree Structure and Serialization**
  - Why needed here: The DOM Transduction Pattern requires understanding hierarchical element relationships to preserve structure during pruning
  - Quick check question: How would removing all `<script>` tags affect the DOM tree structure versus removing a content `<div>`?

## Architecture Onboarding

- **Component map:**
  - Perception Module (DOM Transformer + Affordance Parser) → Page Affordance Model → Cognitive Map → Reasoning Core
  - Service Discovery → Thing Description fetch → Affordance Parser → Cognitive Map enrichment

- **Critical path:**
  1. Raw HTML → DOM Transformer (cleaning → pruning → compact encoding) → Page Affordance Model → Cognitive Map update
  2. Service discovery → Thing Description fetch → Affordance Parser → Cognitive Map enrichment
  3. Cognitive Map → Reasoning Core → Action selection

- **Design tradeoffs:**
  - Pruning aggressiveness: Higher compression risks missing elements; lower compression retains noise
  - LLM-as-Transformer cascade: Smaller model distillation adds latency but reduces reasoning model costs
  - Description dependency: WoT-based discovery requires ecosystem adoption; fallbacks not specified

- **Failure signatures:**
  - Agent cannot locate interactive element mentioned in task → pruning removed critical node
  - Agent fails to invoke service despite discovery → malformed or incomplete Thing Description
  - Cognitive Map contains conflicting affordance representations → schema unification failure

- **First 3 experiments:**
  1. Implement DOM Transformer with rule-based cleaning only; measure token reduction and task success rate on simple web forms
  2. Add embedding-based relevance pruning; compare task success on complex pages versus baseline
  3. Integrate WoT Thing Description parser; test runtime discovery of a mock IoT service with temperature control affordances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed structure-based patterns be integrated with visual perception mechanisms to enable robust multi-modal world modeling?
- Basis in paper: [explicit] Page 6 states that future research will extend these patterns with "visual counterparts" to create a "comprehensive pattern language for multi-modal perception, enabling agents to fuse structured and visual percepts."
- Why unresolved: The current work focuses exclusively on structured data (DOMs and service descriptions); the integration logic for switching between or merging these with visual inputs is identified as a long-term goal rather than a current capability
- What evidence would resolve it: An implemented architecture demonstrating how an agent selects between or fuses a DOM Transduction output and a visual embedding when interacting with a single web resource

### Open Question 2
- Question: What specific feedback mechanisms or heuristics can prevent the DOM Transduction pattern from removing critical interactive elements during the pruning phase?
- Basis in paper: [inferred] Page 4 lists "critical failures by removing necessary elements" as a liability of the DOM Transduction pattern, noting that "overly aggressive pruning" is a risk
- Why unresolved: While the paper proposes cleaning and pruning methods, it does not define a verification step to ensure the resulting "Page Affordance Model" retains all necessary state changes for the specific task
- What evidence would resolve it: Empirical data comparing different pruning aggressiveness levels against agent success rates on complex, non-static web applications

### Open Question 3
- Question: How can an agent maintain a reliable world model when external resources fail to provide standardized semantic descriptions or expose malformed metadata?
- Basis in paper: [inferred] Page 5 identifies the reliance on providers correctly implementing and hosting Thing Descriptions as a "barrier to adoption" and notes that "a poorly written TD can lead to agent errors"
- Why unresolved: The Hypermedia Affordances Recognition Pattern relies on the constraint that resources expose standardized descriptions; the paper does not offer a fallback strategy for non-compliant or "messy" real-world environments
- What evidence would resolve it: A fault-tolerance mechanism within the Affordance Parser that validates or repairs incomplete semantic descriptions before updating the Cognitive Map

## Limitations
- No quantitative metrics or empirical validation provided; remains conceptual framework
- Reliance on standardized semantic descriptions assumes universal ecosystem adoption that may not exist
- Multi-stream perceptual fusion into unified cognitive map lacks concrete implementation details

## Confidence

**High Confidence** (80-100%): The fundamental problem statement is valid—raw HTML verbosity and service discovery are genuine challenges for autonomous agents. The architectural patterns described (DOM transduction, hypermedia affordance recognition) are coherent and align with established web automation principles.

**Medium Confidence** (50-80%): The proposed mechanisms for context efficiency and runtime discovery are technically sound but lack quantitative validation. The assumption that task-relevant affordances can be reliably identified through structural patterns without domain-specific knowledge is reasonable but unverified at scale.

**Low Confidence** (20-50%): The feasibility of seamless multi-stream perceptual fusion into a unified cognitive map is speculative. No concrete implementation or validation demonstrates that heterogeneous affordance representations (DOM structures vs. semantic service descriptions) can be reliably integrated without schema conflicts.

## Next Checks

1. Implement the DOM Transduction Pattern with varying pruning thresholds and measure task success rates on a benchmark of web automation tasks, quantifying the trade-off between context reduction and functional completeness.

2. Build a prototype WoT Thing Description parser and test its ability to discover and integrate services from a diverse set of Thing Description documents, measuring coverage and robustness against malformed or incomplete descriptions.

3. Design and implement a schema for the unified cognitive map, then conduct a proof-of-concept fusion experiment combining DOM-derived affordances with WoT service capabilities to evaluate semantic integration challenges and cross-modal discovery effectiveness.