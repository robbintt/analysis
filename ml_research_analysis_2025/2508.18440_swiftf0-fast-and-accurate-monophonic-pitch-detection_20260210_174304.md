---
ver: rpa2
title: 'SwiftF0: Fast and Accurate Monophonic Pitch Detection'
arxiv_id: '2508.18440'
source_url: https://arxiv.org/abs/2508.18440
tags:
- pitch
- swiftf0
- performance
- frequency
- crepe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SwiftF0 introduces a lightweight, efficient neural network for
  monophonic pitch detection that outperforms state-of-the-art methods. The model
  uses STFT magnitude spectrograms with strategic frequency band selection (46.875-2093.75
  Hz), reducing input size by 74%, and a compact 95,842-parameter architecture trained
  on diverse datasets with extensive augmentation.
---

# SwiftF0: Fast and Accurate Monophonic Pitch Detection

## Quick Facts
- arXiv ID: 2508.18440
- Source URL: https://arxiv.org/abs/2508.18440
- Authors: Lars Nieradzik
- Reference count: 7
- Primary result: 91.80% harmonic mean accuracy at 10 dB SNR, 42x faster CPU inference than CREPE

## Executive Summary
SwiftF0 introduces a lightweight neural network architecture for monophonic pitch detection that achieves state-of-the-art accuracy while running 42x faster on CPU than previous methods. The model processes 16 kHz audio using STFT magnitude spectrograms restricted to a strategic frequency band (46.875-2093.75 Hz), reducing input size by 74% without sacrificing accuracy. Trained on diverse datasets including a novel synthetic speech corpus with exact pitch labels, SwiftF0 achieves 91.80% harmonic mean accuracy on benchmark datasets, representing a 12-point improvement over CREPE.

## Method Summary
SwiftF0 uses a compact 95,842-parameter CNN architecture that takes STFT magnitude spectrograms as input. The model employs a joint classification-regression objective combining cross-entropy loss with L1 regression in log-frequency space, enabling sub-bin pitch resolution through local expected value decoding. Training incorporates extensive data augmentation including background noise and SNR variation, leveraging both real and synthetic datasets to improve generalization across acoustic domains.

## Key Results
- 91.80% harmonic mean accuracy at 10 dB SNR on benchmark datasets
- 42x faster inference on CPU compared to CREPE
- 12-point improvement in harmonic mean accuracy over CREPE
- Effective pitch resolution of ~33.1 cents per bin

## Why This Works (Mechanism)

### Mechanism 1: Strategic Frequency Band Selection
Restricting STFT to 46.875–2093.75 Hz maintains pitch-relevant information while reducing input dimensionality by 74%, enabling faster inference without accuracy loss. This band covers fundamental frequencies for human vocal pitch and most musical instruments, with the model processing only 132 bins instead of the full 513.

### Mechanism 2: Joint Classification-Regression Training Objective
Combining cross-entropy classification with L1 regression in log-frequency space produces probability distributions that refine pitch estimation beyond discrete bin resolution. The regression loss encourages probability mass distribution to neighboring bins, enabling local expected value decoding for sub-bin precision.

### Mechanism 3: Diverse Multi-Domain Training with Synthetic Ground Truth
Training on mixed speech, music, and synthetic datasets with exact pitch labels improves generalization across acoustic domains and noise conditions. The synthetic SpeechSynth corpus provides perfect ground truth, while data augmentation exposes the model to diverse acoustic conditions during training.

## Foundational Learning

- **Short-Time Fourier Transform (STFT)**: SwiftF0 operates on STFT magnitude spectrograms, not raw audio. Time-frequency tradeoff is essential (window size=1024, hop=256, 15.625 Hz resolution). Quick check: Given 16 kHz sampling and N=1024, what is the time resolution per frame? (Answer: 16 ms)

- **Fundamental frequency (F0) and harmonic series**: The model detects F0 by analyzing harmonic structure in spectrograms. The receptive field (~328 Hz frequency extent) is designed to capture F0 plus several harmonics. Quick check: Why would restricting analysis to 46.875–2093.75 Hz still allow detection of a 100 Hz fundamental? (Answer: Harmonics at 200, 300, 400 Hz etc. fall within the band)

- **Pitch evaluation metrics**: The harmonic mean metric combines six components (RPA, RCA, OA, CCR, CCR-05, CCC). Understanding each helps diagnose failure modes. Quick check: If a model predicts C5 when ground truth is C4, which metrics fail? (Answer: RPA fails; RCA passes; OA penalizes)

## Architecture Onboarding

- **Component map**: 16 kHz audio -> STFT (N=1024, H=256) -> magnitude spectrogram -> band selection (132 bins) -> log compression -> 2D Conv backbone (5 layers) -> frequency projection (1D conv) -> output distribution -> local expected value decoding -> pitch (Hz) + confidence

- **Critical path**: STFT parameters control time/frequency resolution; band selection determines detectable pitches; projection layer bin count sets effective resolution; confidence threshold gates voicing decisions

- **Design tradeoffs**: Fewer pitch bins enable faster inference but coarser resolution; wider frequency band increases coverage but slows inference; larger receptive field provides better harmonic context but reduces locality

- **Failure signatures**: Octave errors despite high RCA indicate insufficient harmonic context; jagged pitch contours suggest smoothing issues; false positives in silence indicate low confidence threshold; slow inference suggests inefficient STFT computation

- **First 3 experiments**:
  1. Reproduce Table 1 results on held-out datasets (Vocadito, Bach10-mf0-synth, SpeechSynth test) at 10 dB SNR to validate baseline HM > 91%
  2. Ablate frequency band: Expand to full 513 bins and measure accuracy drop vs. speedup loss to verify 74% reduction claim
  3. Test on out-of-domain audio (e.g., non-Mandarin speech, instruments outside training set) to characterize generalization boundaries

## Open Questions the Paper Calls Out

### Open Question 1
How can confidence estimation be improved to accurately reflect both pitch certainty and voicing decisions when training only on voiced frames? The current confidence scores primarily reflect pitch class certainty, not voicing decisions. Adding dedicated voicing output reduced overall performance, creating a trade-off between accurate voicing detection and pitch accuracy.

### Open Question 2
How does SwiftF0 perform on downstream tasks such as speech synthesis or music transcription compared to existing pitch estimators? Standard pitch accuracy metrics may not capture failure modes that only emerge when pitch estimates are used as input to other systems.

### Open Question 3
What is the optimal mixture ratio between synthetic datasets with perfect labels and real datasets with algorithmically-derived labels for training pitch detectors? Both data types contribute value, but the optimal mixing ratio and principles for dataset composition remain unexplored.

### Open Question 4
Would extending SwiftF0's frequency range below 46.875 Hz and above 2093.75 Hz maintain accuracy while enabling broader instrument coverage? The efficiency gains from frequency band selection are central to SwiftF0's speed, but the optimal bounds for different applications haven't been evaluated.

## Limitations
- Confidence scores don't reliably indicate voicing decisions due to training only on voiced frames
- Synthetic SpeechSynth dataset may not capture all real-world acoustic variations
- 74% frequency band reduction may exclude specialized applications requiring extended pitch range
- Model requires external voicing detection for complete pitch tracking

## Confidence
- **High confidence**: 42x CPU speedup and 91.80% harmonic mean accuracy on benchmark datasets are well-supported by experimental results
- **Medium confidence**: Generalization claims rely on synthetic augmentation that may not fully capture real-world complexity
- **Low confidence**: Long-term stability in production environments with diverse audio sources hasn't been established

## Next Checks
1. **Voicing Detection Integration Test**: Implement SwiftF0 with energy-based voicing detector and evaluate on Voiceless dataset, measuring both pitch accuracy and voicing detection F1-score

2. **Cross-Language Generalization**: Test SwiftF0 on non-Mandarin speech datasets (English, Japanese, Arabic) to quantify performance degradation and identify Mandarin-specific bias

3. **Out-of-Band Pitch Detection**: Evaluate SwiftF0 on audio containing fundamental frequencies below 46.875 Hz or above 2093.75 Hz (bass instruments, bird calls) to measure practical impact of frequency band reduction