---
ver: rpa2
title: Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind
  Reasoning On-The-Fly
arxiv_id: '2506.16755'
source_url: https://arxiv.org/abs/2506.16755
tags:
- agent
- liras
- human
- reasoning
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Language-Informed Rational Agent Synthesis
  (LIRAS), a framework for multimodal social reasoning that integrates language and
  visual inputs to infer agents' mental states. LIRAS constructs situation-specific
  symbolic models from language and vision inputs, then uses Bayesian inverse planning
  to generate probabilistic social inferences.
---

# Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly

## Quick Facts
- arXiv ID: 2506.16755
- Source URL: https://arxiv.org/abs/2506.16755
- Reference count: 40
- Outperforms state-of-the-art VLMs on multimodal social reasoning, achieving correlations up to 0.87 with human judgments

## Executive Summary
LIRAS introduces a framework for multimodal social reasoning that integrates language and visual inputs to infer agents' mental states through Bayesian inverse planning. The system constructs situation-specific symbolic models from language and vision inputs, then uses SIAM (a Bayesian inference engine) to generate probabilistic social inferences. Evaluated on food truck, astronaut, and doors-keys-gems domains, LIRAS outperforms state-of-the-art VLMs including OpenAI o3 and Gemini 2.0 Flash, achieving higher correlations with human judgments while adapting inferences to different linguistic rule specifications.

## Method Summary
LIRAS frames multimodal social reasoning as constructing structured but situation-specific agent and environment representations from language and vision, then performing Bayesian inference over mental states. A VLM synthesizes PDDL domain definitions and agent model parameters from linguistic descriptions. Visual observations are parsed into PDDL states, and SIAM performs Bayesian inverse planning to infer posterior distributions over mental states (goals, beliefs, rewards, costs). The approach separates model synthesis from inference, allowing a lightweight VLM to outperform larger reasoning models by leveraging a specialized inference engine.

## Key Results
- LIRAS achieves correlations of 0.74-0.87 with human judgments across domains, outperforming baselines including OpenAI o3 (0.80 max) and Gemini 2.0 Flash
- Successfully adapts inferences to different linguistic rule specifications while maintaining consistent performance
- Ablation studies confirm both the Bayesian inference engine and structured symbolic representation are essential for performance
- Demonstrates human-like uncertainty in mental state attribution through graded posterior distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: On-the-fly symbolic model synthesis from language enables context-specific reasoning that adapts to novel rule structures.
- Mechanism: A VLM translates linguistic descriptions into PDDL domain definitions (object types, predicates, action templates) and agent model parameters (goal spaces, belief configurations, cost profiles). This creates a unified symbolic representation that can be formally reasoned over.
- Core assumption: Language provides both abstract dynamics and concrete specifics that fundamentally restructure how visual observations should be interpreted.
- Evidence anchors: "LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations" [abstract]; "LIRAS achieves this by using an LLM to translate the task description L into a planning domain D represented in the Planning Domain Definition Language (PDDL)" [section 2.1].

### Mechanism 2
- Claim: Separating model synthesis from inference allows a lightweight VLM to outperform larger reasoning models.
- Mechanism: Gemini 2.0 Flash handles only parsing and synthesis (tasks within its capabilities), while the explicit Bayesian inference engine (SIAM) handles the probabilistic reasoning. This division exploits each component's strengths.
- Core assumption: VLMs struggle with complex probabilistic social reasoning even when given correct symbolic representations, but can reliably parse structured inputs into formal specifications.
- Evidence anchors: "our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models" [abstract]; "the ablated LIRAS model, which synthesizes symbolic world and agent models, but instead uses an LLM to perform probabilistic inference, performs significantly worse than the full model and no better than the Gemini 2.0 Flash base model" [section 4].

### Mechanism 3
- Claim: Bayesian inverse planning over synthesized models captures graded human uncertainty in mental state attribution.
- Mechanism: SIAM computes posterior distributions over mental states by inverting a rational agent model. It assumes agents take Boltzmann-rational actions proportional to expected utility, enabling likelihood computation for observed action sequences.
- Core assumption: Human social reasoning implicitly assumes others are approximately rationalâ€”beliefs are consistent with observations, and actions are efficient given goals.
- Evidence anchors: "SIAM efficiently inverts the rational agent model...computing a posterior distribution over the full sequence of latent mental states" [section 2.4]; LIRAS achieves correlations of 0.74-0.79 across DKG variants, roughly at the noise ceiling of human split-half correlations (0.73-0.80) [section 4, Table 2].

## Foundational Learning

- Concept: **PDDL (Planning Domain Definition Language)**
  - Why needed here: The framework represents environment dynamics, object types, and actions in PDDL format. Understanding predicates, action templates with preconditions/effects, and typed objects is essential for debugging synthesis outputs.
  - Quick check question: Can you write a PDDL action schema for "unlock door with key" that requires adjacency and key possession as preconditions?

- Concept: **Bayesian Inverse Planning**
  - Why needed here: The core inference mechanism. Understanding how to compute P(mental states | observed actions) by inverting a forward agent model is critical for extending or debugging SIAM.
  - Quick check question: Given an agent who moved toward goal A but stopped halfway, how would you formalize the likelihood under a Boltzmann-rational action model?

- Concept: **Theory of Mind as Generative Model**
  - Why needed here: The paper treats ToM reasoning as inverting a generative model of rational agents. Understanding belief updating, goal-directed action selection, and the mental prior structure is necessary for interpreting results.
  - Quick check question: What mental states must be inferred jointly when an agent has partial observability of the environment?

## Architecture Onboarding

- Component map:
  Language description -> Environment Model Synthesis (LLM) -> PDDL domain
  Language description -> Agent Model Synthesis (LLM) -> Agent config
  Video frames -> Visual State Parser (VLM) -> PDDL states
  PDDL states -> Action Reconstruction -> Actions
  Agent model + states + actions -> SIAM Inference Engine -> Posterior over mental states

- Critical path:
  1. Correct PDDL synthesis (syntax + semantics)
  2. Accurate visual parsing (grid cell enumeration)
  3. Valid action reconstruction
  4. SIAM execution with proper belief updates and Q-value computation

- Design tradeoffs:
  - **Discrete vs. continuous domains**: Currently limited to gridworld discrete spaces due to PDDL constraints
  - **Explicit vs. implicit rules**: Requires explicitly stated action spaces and transition dynamics; cannot infer from commonsense alone
  - **Rejection sampling vs. single-pass**: Uses temperature=1.0 with rejection sampling for diversity, but semantic variation among valid outputs is minimal

- Failure signatures:
  - Hallucinated predicates or actions in PDDL that don't match linguistic description
  - Visual parsing errors in object detection or attribute assignment
  - SIAM timeout on large state spaces (mitigated by memoized shortest-path computation)
  - Negative correlations on unusual rule variants (see DKG-Inverse ablation: r = -0.11)

- First 3 experiments:
  1. **Validate synthesis pipeline**: Run environment/agent model synthesis on all domain variants, manually inspect PDDL outputs for correctness before running inference.
  2. **Ablation comparison**: Run full LIRAS vs. ablated (LLM-only inference) vs. Gemini 2.0 Flash baseline on Foodtruck domain; compare correlations to verify inference engine contribution.
  3. **Rule adaptation test**: Test DKG-Simple vs. DKG-Reuse on identical visual stimuli with different linguistic descriptions; verify that goal posteriors shift appropriately (as in Figure 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LIRAS be extended to handle continuous spatial domains and action spaces beyond the discrete PDDL-based representations used in the current framework?
- Basis in paper: "our current approach is restricted to discrete domains and does not extend to continuous spaces, reflecting a major limitation inherent in the PDDL framework"
- Why unresolved: The PDDL planning framework fundamentally operates on discrete states and actions; extending to continuous spaces requires new representational and computational approaches.
- What evidence would resolve it: A modified or alternative framework that successfully performs mental state inference in continuous domains (e.g., robot navigation in physical spaces) with human-level correlation.

### Open Question 2
- Question: Can the framework be generalized to competitive multiagent interactions where agents have opposing goals and must reason adversarially?
- Basis in paper: "modeling multiagent scenarios remains challenging, particularly in competitive settings; our framework cannot yet adequately capture the complexities of multiagent interactions"
- Why unresolved: The current SIAM inference engine assumes cooperative or independent goal-directed behavior; adversarial reasoning requires modeling strategic opponent behavior.
- What evidence would resolve it: Demonstrated performance on competitive multiagent ToM tasks with human-like inference about opponent strategies and beliefs.

### Open Question 3
- Question: How can the system infer domain constraints and rules from implicit contextual cues rather than requiring explicit linguistic specification?
- Basis in paper: "humans can often infer such rules implicitly from context, drawing upon commonsense knowledge to build mental models of new environments without explicit instructions"
- Why unresolved: The current model depends on explicitly provided linguistic information including enumerated action spaces and transition dynamics.
- What evidence would resolve it: Successful inference on tasks where rules must be inferred from visual observation alone or from ambiguous/incomplete linguistic descriptions.

## Limitations

- Domain Expressiveness: Requires explicit linguistic specification of action spaces and transition dynamics; cannot handle domains where rules must be inferred from commonsense or implicit social knowledge alone.
- Visual Parsing Scalability: Cell-by-cell parsing approach may not scale to domains with many objects, continuous spaces, or complex visual scenes; performance degrades when object detection errors compound.
- Theory-of-Mind Complexity: Rational agent assumption may break down for humans exhibiting bounded rationality, emotional reasoning, or strategic deception; assumes agents have consistent beliefs and goals.

## Confidence

**High Confidence**: The core mechanism of separating symbolic model synthesis from Bayesian inference (Mechanism 2) is well-supported by ablation results showing significant performance drops when inference is handled by the LLM alone.

**Medium Confidence**: The claim that on-the-fly synthesis enables context-specific reasoning (Mechanism 1) is supported but relies on controlled experimental domains. Real-world applicability to ambiguous or underspecified linguistic descriptions remains uncertain.

**Medium Confidence**: The Bayesian inverse planning mechanism (Mechanism 3) correlates well with human judgments (0.74-0.87), but the noise ceiling of human judgments (0.73-0.80) suggests room for improvement and potential overfitting to experimental stimuli.

## Next Checks

1. **Cross-Domain Generalization**: Test LIRAS on domains outside the experimental suite (e.g., office navigation, traffic scenarios) with varying levels of linguistic description completeness to identify breaking points in synthesis capability.

2. **Human-LIRAS Agreement Analysis**: Conduct detailed error analysis comparing LIRAS inferences to human judgments, particularly in cases where correlations are negative (DKG-Inverse: r = -0.11) to identify systematic failures in the inference engine.

3. **Computational Efficiency Profiling**: Measure wall-clock time for each pipeline component (synthesis, visual parsing, SIAM inference) across different stimulus lengths and domain complexities to identify bottlenecks and scalability limits.