---
ver: rpa2
title: 'Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization
  Approach'
arxiv_id: '2511.04393'
source_url: https://arxiv.org/abs/2511.04393
tags:
- regret
- trained
- reward
- time
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Iterative Regret-Minimization Fine-Tuning
  (Iterative RMFT), a post-training method for enhancing large language models (LLMs)
  as decision-making agents. The approach leverages regret minimization as a universal
  training signal: at each iteration, the model generates multiple decision trajectories,
  selects the k-lowest regret ones, and fine-tunes on them via supervised learning.'
---

# Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach

## Quick Facts
- **arXiv ID:** 2511.04393
- **Source URL:** https://arxiv.org/abs/2511.04393
- **Reference count:** 40
- **One-line primary result:** Iterative Regret-Minimization Fine-Tuning (Iterative RMFT) is a post-training method that improves LLM decision-making by iteratively fine-tuning on self-generated low-regret trajectories, achieving consistent regret reduction across diverse environments.

## Executive Summary
This paper introduces Iterative Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training method for enhancing large language models (LLMs) as decision-making agents. The approach leverages regret minimization as a universal training signal: at each iteration, the model generates multiple decision trajectories, selects the k-lowest regret ones, and fine-tunes on them via supervised learning. This process iteratively improves the model's decision-making by learning from its own best-performing behaviors.

The method is validated across diverse settings: Transformers with numerical input/output, open-weight LLMs (e.g., Phi-3.5-mini, Gemma-2-9b-it, Qwen3-8B), and closed-weight models like GPT-4o mini. Results show consistent reductions in regret growth rates, improved exploration-exploitation tradeoffs, and strong generalization across varying horizons, action spaces, and real-world language contexts. Theoretical analysis demonstrates that in a simplified setting, the approach can recover known no-regret algorithms like FTRL, providing theoretical justification for its effectiveness.

## Method Summary
Iterative RMFT is a post-training framework that improves LLM decision-making by iteratively fine-tuning on self-generated, low-regret trajectories. The process involves sampling multiple trajectories from the current model, computing their regret (the difference between actual and optimal cumulative rewards), selecting the top-k lowest-regret trajectories, and performing supervised fine-tuning on this filtered dataset. This self-distillation loop repeats for multiple iterations, with each cycle reinforcing better decision strategies. The method works with both numerical and language-grounded decision-making tasks and is validated across various model architectures including Transformers, open-weight LLMs, and closed-weight models like GPT-4o mini.

## Key Results
- Consistent reduction in regret growth rates across Transformers, open-weight LLMs (Phi-3.5-mini, Gemma-2-9b-it, Qwen3-8B), and closed-weight models (GPT-4o mini).
- Improved exploration-exploitation tradeoffs with better generalization across varying horizons, action spaces, and real-world language contexts.
- Theoretical analysis shows that in a simplified single-layer linear attention setting, the method can recover Follow-the-Regularized-Leader (FTRL), a known no-regret algorithm.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Filtering self-generated trajectories by minimum regret isolates effective decision strategies, which are then reinforced via Supervised Fine-Tuning (SFT).
- **Mechanism:** The model samples multiple potential interaction histories (trajectories). An oracle evaluates these based on the regret metric (cumulative loss vs. best-in-hindsight). The top-k trajectories with the lowest regret are used as ground truth for the next training iteration. This behaves as a self-distillation process where the model learns to imitate its own "best" behavior.
- **Core assumption:** The environment allows for reliable regret calculation during training (full-information or stochastic stationary reward access), and the base model is capable enough to generate at least a few low-regret trajectories by chance or prior knowledge.
- **Evidence anchors:**
  - [abstract]: "...selects the k-lowest regret trajectories, and fine-tunes itself on them."
  - [section 3]: "Iterative RMFT... repeatedly distills low-regret decision trajectories back into the base model."
  - [corpus]: *Efficient Last-Iterate Convergence...* supports the use of regret minimization as a robust convergence objective in learning systems.

### Mechanism 2
- **Claim:** Structured attention layers trained on this objective can theoretically converge to known no-regret algorithms like Follow-the-Regularized-Leader (FTRL).
- **Mechanism:** By minimizing the squared distance between the model output and the optimal policy (best-in-hindsight), the optimization process forces the linear attention weights to approximate a weighted aggregation of past rewards, mathematically equivalent to FTRL.
- **Core assumption:** This specific convergence is proven only for single-layer linear attention Transformers with Gaussian reward distributions (a simplified setting).
- **Evidence anchors:**
  - [abstract]: "Theoretical analysis shows that... a single-layer Transformer... can emerge as a no-regret learner, converging to Follow-the-Regularized-Leader (FTRL)."
  - [section 4.4]: Theorem 1 proves that the global minimizer of the loss function yields FTRL.
  - [corpus]: *Automated scientific minimization of regret* implies the generalizability of regret minimization frameworks, though not specifically for attention architectures.

### Mechanism 3
- **Claim:** Including the natural language reasoning (Chain-of-Thought) associated with low-regret actions in the SFT data aligns the model's semantic rationale with successful decision-making.
- **Mechanism:** The fine-tuning loss includes reasoning tokens, not just action tokens. By training on trajectories that both *acted* correctly and *reasoned* correctly (according to the regret metric), the model learns to produce rationales that semantically align with numerical optimization signals.
- **Core assumption:** The model's reasoning rationales in low-regret trajectories are causally linked to the decision quality, rather than being spurious correlations.
- **Evidence anchors:**
  - [abstract]: "...integrates the model's self-generated reasoning rationales... elicit the model's DM ability."
  - [section 5.3 / Figure 6]: Visualizes "improved semantic-numerical alignment" in reasoning after training.
  - [corpus]: Weak direct evidence in provided corpus; anchors rely heavily on the paper's qualitative analysis of improved reasoning.

## Foundational Learning

**Concept: Online Regret**
- **Why needed here:** Regret is the central loss function and selection criterion. You cannot evaluate a trajectory without understanding the gap between the model's cumulative reward and the best possible fixed action.
- **Quick check question:** Can you calculate the regret of a trajectory if you only know the sequence of actions and rewards, but not the counterfactual rewards of unchosen arms (in a bandit setting)?

**Concept: Exploration-Exploitation Trade-off**
- **Why needed here:** The paper highlights that untrained LLMs fail to balance this. Minimizing regret requires sufficient exploration to find the optimal arm and sufficient exploitation to maximize reward.
- **Quick check question:** Does a greedy strategy (always picking the current best-looking arm) typically achieve low regret in stochastic environments? (Hint: It often gets stuck).

**Concept: Supervised Fine-Tuning (SFT) vs. RL**
- **Why needed here:** This method frames the problem as SFT on filtered data rather than using a standard Reinforcement Learning policy gradient. Understanding this distinction is key to implementing the "Iterative" part of the algorithm.
- **Quick check question:** In this framework, where does the "gradient" signal come from—reward comparison or teacher forcing on selected tokens?

## Architecture Onboarding

**Component map:** Sampler (temperature 1.0) -> Evaluator (regret calculation) -> Filter (top-k selection) -> Trainer (SFT with Cross-Entropy Loss)

**Critical path:** The diversity of the **Sampler**. If the model is too deterministic, you will generate identical trajectories, selection becomes meaningless, and the model will overfit. High-temperature sampling is explicitly cited as necessary.

**Design tradeoffs:**
- **Dialogue vs. Summary:** The paper argues for *Dialogue-type* interaction (turn-by-turn history) over *Summary-type* (aggregated stats). Summary can lead to overfitting; Dialogue preserves reasoning context.
- **Action-based vs. Distributional:** Open-weight models struggle with generating valid probability distributions (low-entropy simplex bias). The paper suggests Action-based output is more robust for these models.

**Failure signatures:**
- **Mode Collapse:** Regret stops decreasing after 1-2 iterations; generated trajectories look identical.
- **Low-Entropy Bias:** The model outputs near-uniform probabilities regardless of history (observed in Gemma-2-9b-it).
- **Overfitting:** Training loss decreases, but regret on new scenarios increases (common with Summary-type interaction).

**First 3 experiments:**
1. **Sanity Check (Numerical):** Implement the single-layer Transformer experiment (Section 4) on a simple Full-Information Online Learning task to verify if the weights actually converge toward FTRL parameters.
2. **Generalization Test:** Train on a Multi-Armed Bandit task with Gaussian rewards. Test immediately on Bernoulli or Sine-trend rewards to confirm *Reward Generalization* without retraining.
3. **Reasoning Inspection:** Compare the CoT rationales of the base model vs. the trained model on a "trap" state (e.g., a reward that fluctuates). Check if the trained model correctly identifies the need for exploration or if it simply mimics a greedy heuristic.

## Open Questions the Paper Calls Out
1. Can Iterative RMFT effectively generalize to decision-making environments with richer state structures, such as contextual bandits and Markov decision processes (MDPs)?
2. How does Iterative RMFT performance scale when applied to real-world agentic tasks involving long-horizon interactions, such as web browsing or software engineering?
3. Do the theoretical guarantees of converging to Follow-the-Regularized-Leader (FTRL) hold for multi-layer Transformers with non-linear attention?

## Limitations
- Theoretical guarantees are limited to single-layer linear attention Transformers with Gaussian reward structures, not extending to deep, non-linear architectures used in practice.
- The approach requires an oracle to compute best-in-hindsight rewards, which may not be available in truly open-ended real-world environments.
- The method assumes the model can generate at least some low-regret trajectories through stochastic sampling, which may not hold for extremely weak base models.

## Confidence

**High Confidence:** The empirical results showing regret reduction across multiple model families and task types are robust and well-validated. The method's core mechanism (iterative self-distillation on low-regret trajectories) is sound and clearly demonstrated.

**Medium Confidence:** The theoretical analysis connecting the method to FTRL is mathematically rigorous for the simplified setting, but the extension to practical LLMs is not proven. The observed improvements in semantic-numerical alignment in reasoning are qualitative and would benefit from more systematic evaluation.

**Low Confidence:** The paper claims strong generalization across different reward distributions without retraining, but this is demonstrated on relatively simple synthetic distributions. The robustness of the method to complex, non-stationary real-world reward structures remains to be tested.

## Next Checks
1. **Architectural Transfer Test:** Implement the method on a multi-layer non-linear Transformer (e.g., LLaMA-2) and verify if the theoretical FTRL convergence properties still emerge, even partially. Compare weight evolution patterns against the single-layer case.

2. **Non-Gaussian Reward Robustness:** Train on Gaussian-reward tasks, then evaluate on tasks with heavy-tailed distributions (e.g., Cauchy or power-law) or multi-modal reward structures. Measure degradation in regret performance and whether fine-tuning on the new distribution is still necessary.

3. **Oracle-Free Variant Exploration:** Design a variant that approximates the "best-in-hindsight" action without full oracle access—for example, using a rolling-window best or a learned critic. Test if regret minimization still occurs and quantify the performance gap versus the oracle-based approach.