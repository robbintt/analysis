---
ver: rpa2
title: Self-Composing Policies for Scalable Continual Reinforcement Learning
arxiv_id: '2506.14811'
source_url: https://arxiv.org/abs/2506.14811
tags:
- task
- tasks
- learning
- componet
- previous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CompoNet, a modular neural network architecture
  for continual reinforcement learning that avoids catastrophic forgetting by growing
  linearly in parameters with the number of tasks. Each module composes outputs from
  previous modules via attention mechanisms while learning its internal policy, enabling
  efficient reuse of past knowledge.
---

# Self-Composing Policies for Scalable Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.14811
- Source URL: https://arxiv.org/abs/2506.14811
- Authors: Mikel Malagón; Josu Ceberio; Jose A. Lozano
- Reference count: 40
- One-line primary result: CompoNet achieves superior performance and forward transfer compared to baselines like ProgressiveNet and PackNet on Meta-World, SpaceInvaders, and Freeway tasks.

## Executive Summary
This paper proposes CompoNet, a modular neural network architecture for continual reinforcement learning that avoids catastrophic forgetting by growing linearly in parameters with the number of tasks. Each module composes outputs from previous modules via attention mechanisms while learning its internal policy, enabling efficient reuse of past knowledge. Experiments on Meta-World, SpaceInvaders, and Freeway tasks show CompoNet achieves superior performance and forward transfer compared to baselines like ProgressiveNet and PackNet. Notably, CompoNet scales efficiently to hundreds of tasks without sacrificing plasticity, and ablation studies confirm each architectural component contributes meaningfully to knowledge transfer.

## Method Summary
CompoNet adds a new self-composing policy module per task in continual RL; previous modules are frozen. Each module contains an Output Attention Head that uses the current state encoding as a query to attend to the key-value pairs of previous policy outputs, generating a tentative action vector. The Internal Policy (a feed-forward network) takes the state and context to produce a delta vector, which is added to the tentative output. The architecture uses SAC for Meta-World and PPO for Atari, with cosine positional encoding and CNN encoders for visual tasks. Code is available at https://github.com/mikelma/componet.

## Key Results
- CompoNet achieves superior performance and forward transfer compared to baselines like ProgressiveNet and PackNet on Meta-World, SpaceInvaders, and Freeway tasks.
- The method scales efficiently to hundreds of tasks without sacrificing plasticity.
- Ablation studies confirm each architectural component (output attention, input attention, internal policy) contributes meaningfully to knowledge transfer.

## Why This Works (Mechanism)

### Mechanism 1: Output-Space Composition via Attention
CompoNet enables knowledge transfer by computing a linear combination of past policy outputs rather than sharing hidden layer features. Each new module contains an Output Attention Head that uses the current state encoding as a query to attend to the key-value pairs of previous policy outputs. This generates a tentative action vector based on what has worked previously. The core assumption is that the optimal action for a new task can often be approximated by a weighted combination of action distributions from relevant previous tasks. Evidence shows this approach works well when previous tasks are relevant, though it may struggle when tasks require fundamentally orthogonal actions.

### Mechanism 2: Residual Plasticity via Internal Policy Correction
The architecture preserves the ability to learn new skills by treating the composed output as a "proposal" that a feed-forward network can overwrite. The Output Attention Head produces a tentative vector v. The Internal Policy takes the state and context to produce a delta vector. The final output is the sum of v and this delta. The core assumption is that a feed-forward network can learn to output a zero-vector when previous policies are correct, or a large delta when they are wrong. Evidence shows the internal policy successfully learns to mimic or deviate from the attention head output depending on utility.

### Mechanism 3: Linear Parameter Scaling
The system scales linearly in memory complexity O(n) by isolating modules and composing outputs, rather than connecting every layer to every previous layer. Unlike ProgressiveNets which grow quadratically by adding lateral connections from all layers of all old columns to all layers of the new column, CompoNet modules only read the final output vectors of previous modules. The core assumption is that the output vector contains sufficient summary information for transfer, reducing the need to access intermediate hidden states. Empirical plots show total parameters growing linearly for CompoNet vs quadratic for ProgressiveNet.

## Foundational Learning

- **Concept: Scaled Dot-Product Attention**
  - Why needed here: This is the core operation used to "query" past policies. You must understand how Queries (Q), Keys (K), and Values (V) interact to form a weighted sum.
  - Quick check question: If the query vector represents the current state, and keys represent previous policy "identifiers," what does the resulting attention weight tell you?

- **Concept: Catastrophic Forgetting vs. Stability-Plasticity Dilemma**
  - Why needed here: CompoNet is designed to solve this specific RL failure mode. You need to know why standard gradient descent overwrites old knowledge and why freezing weights helps stability but hurts learning new things.
  - Quick check question: Why does freezing the parameters of previous modules guarantee zero forgetting, but potentially limit the network's ability to correct old mistakes?

- **Concept: Residual Connections (Add & Norm)**
  - Why needed here: The "Internal Policy" effectively functions as a residual block. Understanding how gradients flow through addition operations is key to seeing how the network learns to "ignore" the attention head.
  - Quick check question: If the Output Attention Head provides a perfect solution, what should the Internal Policy ideally output to minimize loss?

## Architecture Onboarding

- **Component map:** State s -> Encoder E -> h_s -> Output Attention Head -> v_out -> Input Attention Head -> c -> Internal Policy -> Delta -> Aggregator (v_out + Delta)
- **Critical path:** The inference flow is sequential. s -> E -> h_s. Then h_s triggers parallel forward passes through all frozen modules to construct Φ. Then the current module attends to Φ and computes the final action.
- **Design tradeoffs:**
  - Speed vs. Transfer: Inference requires running all previous policies to construct Φ. While memory is linear, compute is theoretically quadratic (O(n^2)) in the worst case, though empirically efficient up to 300 tasks.
  - Output vs. Hidden Composition: By composing outputs (actions) rather than hidden states, you lose fine-grained feature transfer but gain massive memory efficiency.
- **Failure signatures:**
  - Uniform Attention: Attention weights flatten (approx. 1/n) when previous tasks are irrelevant, reducing to a noisy mean policy.
  - Internal Policy Domination: If the attention head fails to converge, the Internal Policy learns to ignore it (outputting high magnitude Delta), effectively reverting the system to "training from scratch" inside a larger shell.
- **First 3 experiments:**
  1. Sanity Check (Identity): Train Task 1, freeze it, start Task 1 again. Verify the Output Attention Head attends 100% to the frozen module and the Internal Policy outputs near-zeros.
  2. Noise Robustness: Train a new task with 10 previous "noise" modules (random actions). Verify the Internal Policy successfully learns the task by ignoring the noisy attention context.
  3. Scalability Stress Test: Run a sequence of 50-100 tasks. Plot inference latency. Confirm it grows sub-quadratically (linear-ish) as claimed in Appendix C.

## Open Questions the Paper Calls Out

### Open Question 1
Can CompoNet be extended to autonomously detect task boundaries and identifiers to enable automatic module addition? The current architecture relies on the common CRL assumption that task identifiers and transition boundaries are provided explicitly by the environment, limiting its autonomy. Incorporating a method to extract such information would allow new modules to be automatically added.

### Open Question 2
Can policy distillation or quantization effectively mitigate the computational cost to enable never-ending learning? While the model scales linearly in parameters, the theoretical inference complexity is quadratic O(N^2), which becomes a bottleneck as the task count grows into the thousands. Quantization and policy distillation could allow this issue to be addressed.

### Open Question 3
How does the architecture perform when state spaces differ significantly across tasks rather than remaining approximately constant? The module's attention heads query previous policies based on the current state encoding; significant divergence in state semantics or dimensionality could disrupt the key-query matching process.

## Limitations
- The method assumes task identifiers and transition boundaries are provided explicitly by the environment, limiting autonomy.
- While parameter scaling is linear, inference complexity grows quadratically with task count, creating potential bottlenecks at scale.
- Performance on highly dissimilar task sequences remains untested, as the attention mechanism may struggle when previous policies provide conflicting guidance.

## Confidence

- **High confidence**: The core claim that output-space composition via attention enables efficient knowledge transfer without catastrophic forgetting is well-supported by systematic ablation studies and strong empirical results across three distinct RL benchmarks.
- **Medium confidence**: The linear parameter scaling advantage over ProgressiveNet holds theoretically, but practical compute complexity during inference may create bottlenecks not fully characterized.
- **Medium confidence**: The assertion that CompoNet maintains plasticity through residual correction is reasonable, though the ablation confirming each component's contribution provides reasonable assurance.

## Next Checks

1. **Computational scaling validation**: Measure wall-clock inference time as tasks scale to 500+ to confirm linear-ish growth and identify bottlenecks.
2. **Cross-task similarity stress test**: Design a task sequence with alternating objectives (e.g., move left vs move right) to test the robustness of attention-based composition when previous policies provide conflicting guidance.
3. **Parameter efficiency verification**: Track memory usage per module during training to confirm the claimed linear scaling holds in practice, accounting for all implementation details including positional encodings and context vectors.