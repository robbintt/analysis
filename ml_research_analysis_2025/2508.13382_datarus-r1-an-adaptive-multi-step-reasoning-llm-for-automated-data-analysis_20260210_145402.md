---
ver: rpa2
title: 'Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis'
arxiv_id: '2508.13382'
source_url: https://arxiv.org/abs/2508.13382
tags:
- reasoning
- arxiv
- data
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Datarus-R1-14B is a 14B parameter LLM fine-tuned for data analysis
  using trajectory-centric synthetic data generation and dual-reward reinforcement
  learning. It combines structured reasoning (ReAct) with code execution to simulate
  iterative analytical workflows.
---

# Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis

## Quick Facts
- arXiv ID: 2508.13382
- Source URL: https://arxiv.org/abs/2508.13382
- Reference count: 33
- Primary result: 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while using 18-49% fewer tokens than comparable models

## Executive Summary
Datarus-R1-14B is a 14B parameter LLM fine-tuned for data analysis using trajectory-centric synthetic data generation and dual-reward reinforcement learning. It combines structured reasoning (ReAct) with code execution to simulate iterative analytical workflows. Trained on 144,000 diverse, multi-domain trajectories with a curriculum-based GRPO and hierarchical reward model, it achieves significant accuracy gains while maintaining token efficiency.

## Method Summary
The model uses a two-phase training approach: first a supervised fine-tuning phase on 144,000 stratified ReAct-style trajectories (40% success, 35% error-correction, 15% self-correction, 10% persistent failure), then GRPO with dual rewards (tag-based structural and hierarchical semantic). Training employs 8×H200 GPUs with DeepSpeed ZeRO-3 for optimization and dedicated vLLM service for generation. A cosine curriculum gradually shifts from structural to semantic reward emphasis, preventing format collapse while maintaining analytical depth.

## Key Results
- 30% higher accuracy on AIME 2024/2025 and LiveCodeBench vs. comparable models
- 18-49% fewer output tokens while maintaining or improving performance
- Achieves state-of-the-art results on LCB v5/v6, AIME, and GPQA Diamond benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-Centric Training with Error-Aware Stratification
Training on complete analytical trajectories including errors and self-corrections produces models that recover from mistakes more gracefully than models trained on clean solution-only data. The 144,000 trajectories are explicitly stratified into four pedagogical categories, teaching the model not just correct solutions but recovery patterns and unproductive path avoidance.

### Mechanism 2: Hierarchical Reward Model (HRM) with Correction Bonus
A hierarchical reward structure that explicitly rewards corrected mistakes produces more robust reasoning than binary outcome rewards alone. The HRM assigns positive rewards to sequences where errors are corrected, negative rewards to sequences ending in error, and positive to error-free sequences, creating denser supervision than outcome-only signals.

### Mechanism 3: Cosine Curriculum Preventing Format Collapse
Gradually shifting reward weight from structural (tag-based) to semantic (HRM) prevents format collapse where models abandon formatting when semantic rewards dominate. Early phases enforce tag placement while later phases optimize for correctness while retaining structural residual, ensuring formatting habits persist under semantic pressure.

## Foundational Learning

- **ReAct (Reasoning + Acting) Framework**
  - Why needed here: Datarus's agentic mode emits structured <thought>, <action>, <observation> cycles; understanding this scaffolding is prerequisite to interpreting trajectory design.
  - Quick check question: Can you trace how a single <step> block connects reasoning to code execution in a ReAct loop?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO extends PPO with group-based relative advantages; the paper's memory optimizations build on standard GRPO mechanics.
  - Quick check question: How does GRPO differ from standard PPO in how it computes advantages?

- **Process vs. Outcome Reward Models**
  - Why needed here: The HRM combines single-step (process) and trajectory-level (outcome) evaluation; distinguishing these is essential for debugging reward signal quality.
  - Quick check question: Would a purely outcome-based reward model be sufficient for tasks requiring mid-trajectory error correction?

## Architecture Onboarding

- **Component map**: Synthetic Data Generator → 144K stratified trajectories → SFT Phase → GRPO Infrastructure → Dual Reward System → Cosine Curriculum Scheduler

- **Critical path**: Data quality (trajectory stratification ratios) → SFT format establishment → GRPO reward balance → curriculum schedule tuning. Overthinking prevention must be applied before SFT; otherwise, verbose traces propagate into RL.

- **Design tradeoffs**:
  - 35% error-correction trajectories increase robustness but risk teaching error-prone patterns if filtering is inconsistent
  - Separate vLLM generation GPUs improve throughput but add synchronization complexity
  - Aggressive token-efficiency penalty may truncate legitimate deep reasoning on hard problems

- **Failure signatures**:
  - Format collapse: <step> tags drift late or disappear → λ_tag decayed too fast or HRM overwhelmed structural signal
  - Circular reasoning: Repeated hypotheses without new information → overthinking filter threshold too permissive
  - Numerical instability: NaN losses → check delta clamping range (-4 to 4) and gradient clipping

- **First 3 experiments**:
  1. Ablate trajectory stratification: Train with 100% success trajectories vs. full 40/35/15/10 split; measure error recovery on held-out notebooks with injected bugs
  2. Validate curriculum schedule: Run with fixed λ_tag=0.5 vs. cosine schedule; check tag placement consistency and semantic accuracy at checkpoints
  3. Stress-test token efficiency: Evaluate on LiveCodeBench hard problems with and without token-efficiency penalty; confirm no accuracy drop on tasks requiring extended reasoning

## Open Questions the Paper Calls Out

- Does trajectory-centric training generalize effectively to domains beyond quantitative STEM fields (e.g., qualitative reasoning, legal analysis, creative writing)?
- How sensitive is Datarus's performance to the quality and biases of the teacher model (Qwen2.5-72B-Instruct) used for synthetic data generation?
- What is the optimal curriculum schedule for transitioning from structural to semantic rewards, and does the cosine schedule represent a local optimum?
- Can the overthinking prevention mechanisms achieve similar efficiency gains without sacrificing performance on genuinely difficult problems requiring extended exploration?

## Limitations

- Claims of 30% accuracy gains rest heavily on synthetic data quality without ablation studies isolating the contribution of error-correction trajectory splits
- Token efficiency gains may not generalize to tasks requiring deep multi-step reasoning, as the penalty could truncate necessary analysis
- Generalization across unseen domains is asserted but not tested; synthetic data may not capture real-world edge cases or noisy inputs

## Confidence

- **High**: SFT + GRPO training pipeline using DeepSpeed ZeRO-3 and vLLM service is well-specified and reproducible
- **Medium**: The hierarchical reward model and cosine curriculum design are plausible but lack independent validation
- **Low**: Generalization claims are not empirically tested; synthetic data dependency creates uncertainty about real-world performance

## Next Checks

1. **Ablation on trajectory stratification**: Train a baseline using only 40% success trajectories (no error-correction, self-correction, or persistent failure) and compare error recovery on held-out notebooks with injected bugs
2. **Validate HRM necessity**: Replace the HRM with a simpler outcome-only reward model and measure accuracy and token efficiency on AIME and LiveCodeBench to isolate HRM's contribution
3. **Test robustness to domain shift**: Evaluate on real-world notebook datasets (e.g., Jupyter notebooks from Kaggle) to confirm that synthetic trajectory benefits transfer beyond benchmark datasets