---
ver: rpa2
title: 'REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science
  Research?'
arxiv_id: '2507.18901'
source_url: https://arxiv.org/abs/2507.18901
tags:
- agents
- reproducibility
- science
- data
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REPRO-Bench introduces a benchmark for assessing AI agents' ability
  to evaluate the reproducibility of social science research. It addresses key limitations
  in existing benchmarks by requiring agents to assess consistency between paper findings
  and reproduced results using original code and data, rather than assuming perfect
  reproducibility.
---

# REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?

## Quick Facts
- **arXiv ID:** 2507.18901
- **Source URL:** https://arxiv.org/abs/2507.18901
- **Reference count:** 10
- **Primary result:** Current AI agents struggle with reproducibility assessment, achieving accuracy from 1.8% to 21.4%, while structured workflow improvements boost performance by 71%.

## Executive Summary
REPRO-Bench introduces a benchmark for assessing AI agents' ability to evaluate the reproducibility of social science research. It addresses key limitations in existing benchmarks by requiring agents to assess consistency between paper findings and reproduced results using original code and data, rather than assuming perfect reproducibility. The benchmark includes 112 task instances featuring real-world complexity with diverse programming languages and data formats. When evaluated on REPRO-Bench, three representative AI agents achieved accuracy ranging from 1.8% to 21.4%, with the best performer falling below random guessing expectations. Building on empirical analysis of agent failures, researchers developed REPRO-Agent, which improved accuracy by 71% to reach 36.6%. The results demonstrate that while current AI agents struggle with this task, targeted improvements can significantly enhance performance, though more advanced agents are needed for practical reproducibility assessment.

## Method Summary
The benchmark provides 112 task instances from social science papers with reproduction packages containing original data, code, and README files. Each task requires agents to assess whether major findings can be reproduced, generating scores from 1 (not reproducible) to 4 (fully reproducible). Three baseline agents (AutoGPT, CORE-Agent, SWE-Agent) were evaluated using gpt-4o-2024-05-13 with a $4/task cost cap. REPRO-Agent was developed based on failure analysis, incorporating a structured 4-phase workflow, dummy score fallback, and few-shot error examples. The benchmark emphasizes real-world complexity with diverse file structures, programming languages (Stata, R, Python, MATLAB, Julia), and data formats.

## Key Results
- Three baseline AI agents achieved accuracy ranging from 1.8% to 21.4% on reproducibility assessment tasks
- The best baseline agent performed below random guessing expectations (21.4% vs. 25% expected)
- REPRO-Agent improved accuracy by 71% to reach 36.6% through structured workflow and failure-aware strategies
- Agents performed significantly better on binary classifications (scores 1 and 4) than nuanced consistency checking (scores 2 and 3)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shifting from pre-extracted contexts to raw, end-to-end environments exposes fundamental limitations in agent reasoning that curated benchmarks mask.
- **Mechanism:** Agents are forced to autonomously parse unstructured PDFs and navigate complex directory structures (avg. 142 files, 4.2 GB) rather than relying on pre-packaged steps. This increases the "search space" for the solution, leading to a drop in accuracy from assumed competence to ~20%.
- **Core assumption:** The complexity of file navigation and information extraction is a primary bottleneck for current LLM-based agents in scientific workflows.
- **Evidence anchors:**
  - [Section 2.2]: Existing benchmarks provide "pre-extracted, concrete steps" whereas actual assessment requires "extracting and applying information... without prior structuring."
  - [Section 3.3]: Statistical analysis confirms package size and file count do not correlate with reproducibility scores (|œÅ| < 0.1), isolating agent capability as the variable.
  - [corpus]: *ReplicationBench* supports this mechanism, noting the need to "assess the underlying faithfulness" of research rather than just executing provided snippets.
- **Break condition:** If agents are provided with a summarized "map" of the reproduction package structure prior to execution, the performance gap should narrow significantly.

### Mechanism 2
- **Claim:** Performance improves significantly when agents are constrained by a structured, failure-aware workflow template rather than open-ended planning.
- **Mechanism:** The REPRO-Agent achieved a 71% relative improvement by enforcing a 4-phase workflow and explicitly injecting knowledge of common failure modes (e.g., Stata log file redirection) as few-shot examples. This reduces the planning horizon and prevents "hallucinated" debugging steps.
- **Core assumption:** General-purpose agents (like AutoGPT) lack domain-specific tacit knowledge (e.g., Stata's non-standard error handling) which must be explicitly injected via prompts.
- **Evidence anchors:**
  - [Abstract]: "Building on empirical analysis of agent failures, researchers developed REPRO-Agent, which improved accuracy by 71%."
  - [Section 5.3]: "REPRO-Agent addresses common failure patterns... through three key strategies: following a structured template... using common error sources as few-shot examples."
  - [corpus]: *CASCADE* suggests agents struggle without "self-evolving" tool use; explicit instruction bypasses this need. *Note: Direct corpus evidence for "Stata log handling" specifically is weak/missing, though general agent fragility is supported.*
- **Break condition:** If a general-purpose model updates its training data to include extensive debugging logs of statistical software, the relative gain of this specific template should diminish.

### Mechanism 3
- **Claim:** Binary classification (Reproducible/Not) is more tractable for agents than nuanced consistency checking.
- **Mechanism:** Agents struggle to distinguish between minor reporting errors (Score 3) and code-level inconsistencies (Score 2) because this requires precise numerical comparison and semantic understanding of "major findings."
- **Core assumption:** The attention mechanism in LLMs dilutes when comparing specific numerical values across long contexts (paper text vs. execution output).
- **Evidence anchors:**
  - [Section 5.1]: "Agents tend to perform better on reproducibility scores of 1 and 4... suggesting that the agents are inclined to generate binary results."
  - [Section 5.2]: Agents often failed to incorporate both code inspection and result comparison, with "less than half (42%) of cases" following the full required workflow.
  - [corpus]: *Can AI Master Econometrics?* implies AI struggles with "expert-level tasks" requiring precise numerical validation.
- **Break condition:** If an external tool (e.g., a diff-checker or LaTeX comparator) is integrated as an agent tool, performance on Scores 2 and 3 should rise to match Scores 1 and 4.

## Foundational Learning

- **Concept: Computational Reproducibility vs. Replicability**
  - **Why needed here:** The benchmark strictly evaluates *reproducibility* (using original data/code to verify findings) distinct from *replicability* (using new data). Confusing the two leads to misinterpreting the task constraints (e.g., looking for new data collection).
  - **Quick check question:** If an agent downloads new census data to verify a 2020 paper, is it performing the task defined by REPRO-Bench? (Answer: No, that is replicability).

- **Concept: Agent-Computer Interface (ACI)**
  - **Why needed here:** The paper highlights that standard terminal feedback loops fail for tools like Stata (errors go to logs, not stdout). Understanding how agents "read" the environment is crucial for debugging why an agent claims "success" on a failed script.
  - **Quick check question:** Why might an agent report "No Output" for a Stata script that actually crashed? (Answer: Stata writes errors to `.log` files, not standard error streams).

- **Concept: Ground Truth Granularity**
  - **Why needed here:** The benchmark uses a 4-point scale (1-4) rather than binary. Systems must be tuned to detect subtle "display level" issues (Score 3) versus code errors (Score 2) to maximize accuracy.
  - **Quick check question:** If reproduced results match mathematically but differ by 0.01 due to rounding, what reproducibility score should be assigned? (Answer: Score 3).

## Architecture Onboarding

- **Component map:** Input: `paper.pdf` + `reproduction_package/` (Data: .dta, .csv; Code: R, Stata, Python) -> Environment: Docker container with pre-installed Stata, R, LaTeX, Python -> Agent: Controller (LLM) -> Tools (Bash, Python, VLM) -> Output: `reproducibility_score.json` -> Evaluator: compares output against human-annotated ground truth (1-4)

- **Critical path:**
  1. **Ingestion:** Agent reads `README.txt` and lists directory structure (Phase 1)
  2. **Execution:** Agent configures paths and runs main analysis scripts (Phase 3)
  3. **Verification:** Agent uses VLM or text parsing to compare output files (e.g., `.tex`, `.png`) against paper claims (Phase 4)

- **Design tradeoffs:**
  - **Cost vs. Thoroughness:** The paper caps API costs at $4/task. This limits the number of "reasoning steps" or "retries" an agent can attempt, potentially truncating complex debugging loops.
  - **Applicability vs. Accuracy:** Strict JSON formatting requirements caused low applicability rates (agents failed to output valid files). A fallback "dummy score" (as used in REPRO-Agent) trades immediate accuracy for data availability.

- **Failure signatures:**
  - **Type 4 (File Location):** Agent claims "Data Missing" because it looked in the root directory, ignoring nested subfolders (occurs most frequently)
  - **Type 2 (Silent Failure):** Agent executes Stata script, sees no terminal error, assumes success, but the script actually failed (error hidden in log)
  - **Type 1 (Comparison Hallucination):** Agent writes a Python script to compare results but hallucinates a mismatch where none exists

- **First 3 experiments:**
  1. **Baseline Audit:** Run SWE-Agent on a subset of 10 tasks to confirm the "Type 4" file location failure mode is the dominant error source
  2. **ACI Augmentation:** Modify the agent's system prompt to include a mandatory "Check Log Files" step after any Stata execution; measure impact on false-positive successes
  3. **Fallback Implementation:** Implement the REPRO-Agent's "dummy score" strategy (output a default score immediately, then refine) to measure the theoretical maximum applicability rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can AI agent architectures be enhanced to detect nuanced reproducibility issues (scores 2 and 3) rather than defaulting to binary classifications?
- **Basis in paper:** [inferred] The analysis in Section 5.1 notes that agents perform significantly worse on intermediate scores, tending to generate binary results rather than thoroughly investigating sources of inconsistency.
- **Why unresolved:** Current agents, including the improved REPRO-Agent, lack the critical reasoning depth required to distinguish minor display errors from code-level inconsistencies.
- **What evidence would resolve it:** Development of an agent architecture that achieves >50% accuracy on intermediate reproducibility scores (2 and 3) on the REPRO-Bench dataset.

### Open Question 2
- **Question:** Can AI agents maintain performance in reproducibility assessment when intermediate data points are masked, requiring execution from raw data?
- **Basis in paper:** [explicit] Section 7 (Limitations) proposes investigating "more challenging settings by masking the data points in the experiment results and providing the agents only with raw data."
- **Why unresolved:** The current benchmark provides complete reproduction packages; it is unknown if agents can bridge the gap between raw data and final findings without intermediate guidance.
- **What evidence would resolve it:** Evaluation results from a modified REPRO-Bench setup where processed data files are removed and agents must run full pipelines from raw inputs.

### Open Question 3
- **Question:** Does the inclusion of multiple task versions with intentionally introduced errors improve the robustness and granularity of agent evaluation?
- **Basis in paper:** [explicit] Section 7 states that benchmark granularity "can be further improved by introducing multiple versions of task instances for the same paper, incorporating intentionally erroneous or corrected code and/or data."
- **Why unresolved:** The current benchmark relies on single versions of papers; it is unclear how synthetic perturbations would affect the difficulty curve or failure modes of agents.
- **What evidence would resolve it:** A comparative study of agent performance on the original benchmark versus a synthetic variant with injected code and data errors.

## Limitations
- Benchmark focuses exclusively on social science papers with statistical software, limiting generalizability to other domains
- Software version specifications are incomplete in the paper, potentially affecting reproducibility of results
- The $4 per task cost cap may artificially constrain agent performance by limiting debugging attempts

## Confidence
- **High Confidence:** The claim that existing benchmarks use pre-extracted contexts while REPRO-Bench requires raw end-to-end environments is well-supported by the methodology section and statistical analysis showing no correlation between package complexity and scores.
- **Medium Confidence:** The assertion that structured workflow templates significantly improve performance is supported by the 71% relative improvement, though the exact prompt templates and few-shot examples are not fully specified in the paper.
- **Low Confidence:** The claim that binary classification (scores 1 and 4) is more tractable than nuanced assessment (scores 2 and 3) is based on observed patterns but lacks direct experimental validation isolating this mechanism.

## Next Checks
1. **Software Version Verification:** Replicate the benchmark using explicitly specified software versions (Stata 17, R 4.3.0, Python 3.10) to ensure version consistency doesn't drive performance differences.
2. **ACI Log Monitoring:** Implement mandatory log file inspection after all statistical software executions to quantify the impact of silent failures on agent accuracy.
3. **Structured vs. Open Workflow A/B Test:** Compare identical agents with and without the REPRO-Agent's 4-phase template on a subset of tasks to isolate the contribution of structured workflows to performance gains.