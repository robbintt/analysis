---
ver: rpa2
title: 'HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference
  in Pretrained Language Models'
arxiv_id: '2504.17449'
source_url: https://arxiv.org/abs/2504.17449
tags:
- knowledge
- layers
- inference
- lower
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of efficiently serving many customized
  pretrained language models (PLMs) in cloud multi-tenant environments, where each
  tenant requires its own fine-tuned PLM for specialized tasks. The authors propose
  HMI, a Hierarchical Knowledge Management-based Multi-tenant Inference system, which
  addresses the issue of limited GPU memory and the need to maintain model quality
  by extracting and managing knowledge from PLMs at different levels: general, domain-specific,
  and task-specific.'
---

# HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models

## Quick Facts
- arXiv ID: 2504.17449
- Source URL: https://arxiv.org/abs/2504.17449
- Reference count: 40
- Primary result: Serves 10,000 customized PLMs on a single GPU with only 1% accuracy loss

## Executive Summary
This paper addresses the challenge of efficiently serving many customized pretrained language models (PLMs) in cloud multi-tenant environments. The authors propose HMI, a Hierarchical Knowledge Management-based Multi-tenant Inference system, which extracts and manages knowledge from PLMs at three levels: general, domain-specific, and task-specific. By further pretraining only lower layers on domain data and fine-tuning only upper layers with adapters for task customization, HMI enables thousands of hPLMs to be served on a single GPU while maintaining model quality.

## Method Summary
HMI constructs hierarchical PLMs (hPLMs) by splitting the model at a specific layer depth. Lower layers are further pretrained on domain-specific data and materialized into precomputed lookup tables (PLOT) for efficient retrieval. Upper layers are fine-tuned with adapter modules for task-specific customization. The system manages these data structures using a frequency-based strategy for PLOT updates and adapter swapping to host memory. Inference is orchestrated through a pipelined scheduler that overlaps CPU PLOT retrieval, I/O adapter loading, and GPU computation.

## Key Results
- Serves up to 10,000 hPLMs on a single GPU with only 1% average accuracy reduction
- Achieves significant throughput and response time improvements over baselines using model swapping or compression
- Successfully extended to generative models (hGPT) with similar gains
- Ablation studies confirm the effectiveness of proposed optimizations

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Knowledge Separation
The system exploits the observation that domain-specific knowledge is primarily acquired in lower transformer layers during further pretraining, while task-specific knowledge is acquired in higher layers during fine-tuning. By freezing upper layers during FPT and lower layers during FT, the system avoids storing full model copies for every tenant, instead storing only the delta (PLOT for lower, Adapters for upper).

### Mechanism 2: Computational Substitution via PLOT
Instead of computing the forward pass for lower layers, HMI uses a Precomputed Lookup Table (PLOT) that maps text fragments (tri-grams) to their pre-calculated hidden states. During inference, it retrieves these states and aggregates them, effectively skipping the lower-layer compute on the GPU.

### Mechanism 3: Latency Hiding via Pipelined Prefetching
The scheduler overlaps operations across batches. While the GPU computes the transformer layers for Batch N, the CPU simultaneously retrieves PLOT entries for Batch N+1, and the I/O bus loads the task-specific Adapters for Batch N+1.

## Foundational Learning

### Concept: Transformer Layer Representations
**Why needed here:** HMI physically splits the model at a specific layer depth. Understanding that lower layers capture syntax/local context while higher layers capture semantics/global context is essential to trusting the PLOT/Adapter split.
**Quick check question:** Does freezing the bottom half of a BERT model significantly impact its ability to learn a new syntactic domain (like medical notes) versus a new task (like sentiment classification)?

### Concept: Parameter-Efficient Fine-Tuning (PEFT)
**Why needed here:** The system relies on Adapters (small bottleneck layers) to encapsulate the "task-specific knowledge" without modifying the main backbone weights.
**Quick check question:** If an Adapter adds only 2% parameters but achieves 98% of the performance, what is the theoretical upper bound of memory savings when serving 1,000 concurrent tenants?

### Concept: Pipelining & Hardware Stalls
**Why needed here:** The efficiency of HMI depends on the scheduler preventing GPU starvation. The engineer must understand the relationship between "prefetching" and "compute overlap."
**Quick check question:** If the CPU PLOT lookup takes 5ms and the GPU transformer block takes 4ms, where will the pipeline stall occur?

## Architecture Onboarding

### Component Map
Dispatcher -> Manager -> Scheduler -> GPU Stream
                          -> CPU Worker
                          -> I/O Stream

### Critical Path
The inference latency is determined by the GPU Transformer Computation path. The critical sequence is:
1. Batching: Requests are grouped
2. Prefetch (Parallel): PLOT vectors fetched from Host RAM; Adapters loaded to GPU
3. Compute: GPU processes upper layers using fetched vectors and loaded adapters

### Design Tradeoffs
- **Storage vs. Accuracy (Frequency Threshold α):** The system only updates PLOT entries for the α% most frequent n-grams in a new domain. Setting α too low saves disk space but may increase "cache misses" for domain-specific jargon.
- **Granularity vs. Throughput:** Batch size is critical. High throughput requires large batches to saturate the GPU and hide I/O latency, but this increases per-request waiting time in the queue.

### Failure Signatures
- **Pipeline Stall:** Throughput drops to the speed of the slowest component rather than the GPU, indicating the prefetching mechanism is failing to keep up.
- **Accuracy Collapse:** A sudden drop in task performance usually indicates a PLOT version mismatch or an OOV explosion where n-grams are not found.

### First 3 Experiments
1. **Layer Split Profiling:** Vary the split point (e.g., layer 4 vs. 6 vs. 8) on your specific workload. Verify the paper's claim that domain knowledge sits in the bottom half.
2. **Pipeline Latency Breakdown:** Measure the raw execution time of PLOT Lookup, Adapter Load, and GPU Compute independently. Confirm that GPU Compute > Lookup + Load to ensure pipelining is viable.
3. **Frequency Threshold (α) Sensitivity:** Run a sweep of α (e.g., 10%, 30%, 50%) to find the minimum storage overhead required to maintain baseline accuracy for your specific domain corpus.

## Open Questions the Paper Calls Out
None

## Limitations
- PLOT approximation fidelity is not rigorously validated across diverse domains and sequence lengths
- Knowledge stratification assumption may break for certain domain shifts or complex multi-hop reasoning tasks
- Pipeline overlap guarantee is not verified across hardware configurations or batch size variations

## Confidence
**High Confidence:** Overall architectural framework and memory optimization claims are well-supported by experiments showing 10,000 hPLMs on single GPU with minimal accuracy loss.

**Medium Confidence:** Specific PLOT approximation technique and layer-wise knowledge separation mechanism are supported by internal observations but lack independent validation.

**Low Confidence:** Pipelined scheduler's performance guarantees across different hardware configurations and workload patterns are not empirically validated.

## Next Checks

1. **PLOT Approximation Quality Test:** Implement the PLOT mechanism and systematically measure approximation error against full lower-layer computation across varying tri-gram window sizes, sequence lengths, and domain corpora.

2. **Layer Split Point Sensitivity Analysis:** Systematically vary the layer split point (e.g., layer 4, 6, 8 for BERT-base) and measure the resulting accuracy degradation for different domain-task pairs.

3. **Pipeline Performance Under Stress:** Create a benchmark that varies batch sizes and adapter dimensions to identify conditions where CPU/I/O operations exceed GPU compute time, causing pipeline stalls. Measure throughput degradation to establish operational boundaries.