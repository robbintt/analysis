---
ver: rpa2
title: Elastic Architecture Search for Efficient Language Models
arxiv_id: '2510.27037'
source_url: https://arxiv.org/abs/2510.27037
tags:
- search
- language
- blocks
- layer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing efficient language
  models by proposing a neural architecture search (NAS) method called ELM. The core
  method idea involves expanding the search space with efficient transformer blocks
  and introducing dynamic modules for dimension and head number adjustment.
---

# Elastic Architecture Search for Efficient Language Models

## Quick Facts
- arXiv ID: 2510.27037
- Source URL: https://arxiv.org/abs/2510.27037
- Authors: Shang Wang
- Reference count: 40
- Primary result: ELM-Small achieves SOTA on GLUE benchmark with 15.6M parameters

## Executive Summary
This paper introduces Elastic Language Models (ELM), a neural architecture search (NAS) method designed to discover efficient transformer architectures for language modeling tasks. The core innovation is a dynamic search space that allows block dimensions and attention heads to adapt during training, guided by PCA-based feature importance scores and CKA-based redundancy detection. ELM outperforms existing NAS methods on both masked and causal language modeling tasks while maintaining significantly smaller model sizes.

## Method Summary
ELM employs Single Path One-Shot NAS with a 12-layer supernet containing 6 candidate blocks per layer (BERT-base, MobileBERT, and weight-sharing variants). The search space is made elastic through two dynamic modules: PCA-guided dimension expansion that increases FFN hidden dimensions for blocks with highest feature variance, and CKA-based head removal that eliminates functionally redundant attention heads. During supernet training, relational knowledge distillation losses replace traditional MSE/KL losses to preserve architectural diversity. An evolutionary algorithm searches for optimal architectures within parameter and latency constraints, with the final models trained from scratch on pretraining and downstream tasks.

## Key Results
- ELM-Small achieves 84.5 GLUE score with only 15.6M parameters, outperforming all existing lightweight BERT variants
- ELM-Tiny and ELM-Micro demonstrate consistent improvements over baseline architectures across both MLM and CLM tasks
- Dynamic dimension allocation and head fusion reduce parameters by 15-25% without performance degradation
- ELM models maintain strong performance on SQuAD while achieving better parameter efficiency than competitors

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Dimension Allocation via PCA-Score Ranking
- **Claim**: Allocating FFN hidden dimensions based on measured feature importance allows smaller models to recover representational capacity where it matters most.
- **Mechanism**: After each training epoch, PCA scores are computed on FFN hidden states for all blocks. Blocks with top-K scores receive incremental dimension increases until the parameter budget is saturated. Higher PCA variance indicates richer feature representations, justifying larger capacity.
- **Core assumption**: Blocks with higher feature variance contribute more to downstream performance and benefit most from additional dimensions.
- **Evidence anchors**: Section III-B describes the PCA-guided expansion process; Figure 2 shows PCA scores rising across epochs and varying by layer; no direct corpus evidence validates this specific approach.

### Mechanism 2: Redundant Attention Head Fusion via CKA Similarity
- **Claim**: Detecting and removing functionally redundant attention heads maintains performance while improving parameter efficiency in low-dimensional regimes.
- **Mechanism**: Centered Kernel Alignment (CKA) scores are computed between all attention head pairs per layer. When similarity exceeds threshold η=0.9, merge or remove the redundant head. This prevents the "small dimension per head" problem in compact models.
- **Core assumption**: High CKA similarity implies functional redundancy that can be compressed without meaningful loss.
- **Evidence anchors**: Section III-B, Algorithm 1 defines the head-removal procedure; Figure 3(a) shows CKA matrix for layer 6; no direct validation of CKA-based head pruning in neighbor papers.

### Mechanism 3: Relational Distillation Losses for Architecture Diversity
- **Claim**: Relaxing pointwise KD constraints in favor of correlation-based losses preserves architectural diversity during supernet training, improving NAS discrimination.
- **Mechanism**: Replace ||F_teacher - F_student||² with Pearson-correlation-style losses that are scale-and-shift invariant. This allows each block to retain unique characteristics rather than being forced toward identical teacher-mimicking features.
- **Core assumption**: Traditional KD homogenizes features across blocks, reducing the NAS algorithm's ability to distinguish strong architectures from weak ones.
- **Evidence anchors**: Section III-C shows blocks trained under KD conditions show higher feature similarity; Figure 3(b), 6 visualizes higher average cosine similarity under MSE vs. lower diversity under relational KD; weak validation from neighbor papers.

## Foundational Learning

- **Concept: Single-Path One-Shot NAS (SPOS)**
  - Why needed here: ELM builds its supernet using SPOS, where a single over-parameterized network shares weights across all candidate architectures. Understanding uniform path sampling is essential to debug search instability.
  - Quick check question: During supernet training, should all candidate architectures be trained equally, or should high-performing ones receive more weight updates?

- **Concept: Weight Sharing in Attention (Q/V, K/V Sharing)**
  - Why needed here: The search space includes variants where query-value or key-value projections share weights to reduce parameters. Understanding this tradeoff is critical for interpreting searched architectures.
  - Quick check question: What is the expected parameter reduction when switching from independent Q/K/V projections to K/V sharing?

- **Concept: Centered Kernel Alignment (CKA)**
  - Why needed here: CKA quantifies representational similarity between neural network layers/heads. ELM uses it to detect redundant attention heads. Without this, you cannot implement or validate the head-fusion mechanism.
  - Quick check question: If two attention heads have CKA score 0.95, what does this imply about their functional roles?

## Architecture Onboarding

- **Component map**: Supernet backbone (12 layers × 6 blocks) -> Dynamic dimension module (PCA scoring, expansion) -> Head-fusion module (CKA computation, redundancy detection) -> Search controller (evolutionary algorithm) -> Distillation wrapper (relational losses) -> Final architecture (retrained from scratch)

- **Critical path**:
  1. Initialize supernet with minimal FFN dimensions (132) and 12 heads per layer
  2. Train on pretrain corpus (Wikipedia + BooksCorpus) with uniform path sampling
  3. After each epoch: compute PCA, expand top-K block dimensions, check parameter budget
  4. Finetune supernet on downstream tasks (GLUE subset)
  5. Run evolutionary search with latency/parameter constraints
  6. For each layer in final architecture: compute CKA, fuse/remove heads
  7. Retrain final architecture from scratch on full pretrain + downstream data

- **Design tradeoffs**:
  - **Flexibility vs. search cost**: Larger supernet improves final performance but increases GPU days (ELM-Small: 7.1 GPU days vs. fixed-dimension: 8.5 GPU days)
  - **Dimension growth rate**: Top-K=21 per epoch balances gradual growth vs. over-allocation. Aggressive growth may exhaust parameter budget early
  - **Head-fusion threshold (η=0.9)**: Higher threshold preserves more heads (safer); lower threshold may over-prune. No ablation on η sensitivity provided
  - **Relational KD vs. classical KD**: Improves NAS discrimination but may under-transfer teacher knowledge in low-capacity students

- **Failure signatures**:
  - **PCA flatlining**: If all blocks show similar PCA scores early, dimension allocation becomes random. Check initialization diversity and learning rate
  - **Over-aggressive head removal**: If CKA threshold too low, models may drop to 1-2 heads per layer, degrading multi-head attention benefits
  - **Supernet collapse**: If relational KD is too weak, student may diverge from teacher entirely; monitor validation loss gap between supernet paths
  - **Search stagnation**: Evolutionary algorithm may converge prematurely; ensure mutation rate (0.1) and population size (50) are sufficient

- **First 3 experiments**:
  1. **Sanity check: Fixed vs. dynamic dimensions** — Run ELM-Micro with fixed FFN dimensions vs. PCA-guided growth. Compare GLUE dev scores and training time (Table V baseline)
  2. **Head-fusion ablation** — Train ELM-Small with and without CKA-based head removal. Measure parameter reduction and score delta on MNLI/SQuAD (Table VI baseline)
  3. **Loss function swap** — Replace relational KD with MSE in supernet training. Compare block diversity (cosine similarity histograms as in Figure 6) and final searched architecture quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the Elastic Language Model (ELM) framework maintain its search efficiency and architectural gains when applied to pre-trained language models with significantly larger parameter counts (e.g., billions of parameters)?
- **Basis**: The paper restricts its scope to "compact language models" and "lightweight BERT" (specifically 5M to 15.6M parameters), leaving the scalability of the dynamic dimension and head adjustment mechanisms unexplored for larger foundational models.
- **Why unresolved**: The Single Path One-Shot (SPOS) supernet training and the iterative PCA-based dimension growing may become computationally prohibitive or unstable as the parameter scale increases exponentially.
- **What evidence would resolve it**: Experimental results applying the ELM search process to larger backbone models (e.g., Llama-7B scale), comparing the search cost (GPU hours) and final performance against static dense baselines.

### Open Question 2
- **Question**: Is the reliance on Principal Component Analysis (PCA) scores an optimal proxy for determining layer-wise dimension increases, or does it fail to capture non-linear feature importance?
- **Basis**: The methodology explicitly uses PCA scores to guide the dynamic allocation of dimensions, operating on the assumption that variance (captured by PCA) equates to feature importance for the model.
- **Why unresolved**: While PCA captures the directions of maximum variance, deep learning representations often rely on non-linear features that may not align with the top principal components, potentially leading to suboptimal dimension allocation.
- **What evidence would resolve it**: A comparative study where dimension growth is guided by alternative metrics (e.g., gradient-based saliency or learnable gates) versus PCA, measuring the impact on final GLUE/SQuAD performance.

### Open Question 3
- **Question**: Does the proposed relational knowledge distillation (RKD) loss generalize to other Neural Architecture Search (NAS) algorithms, such as differentiable search methods?
- **Basis**: The paper demonstrates the effectiveness of RKD specifically within the Single Path One-Shot (SPOS) training framework, leaving its interaction with other search strategies unstated.
- **Why unresolved**: The benefit of maintaining block diversity via RKD might be specific to the uniform sampling strategy of SPOS; it is unclear if differentiable search methods (like DARTS) would derive the same benefit from relaxed correlation losses.
- **What evidence would resolve it**: Integration of the RKD loss into a differentiable NAS framework (e.g., DARTS or AutoML) on the same tasks to observe if architecture discrimination is similarly improved.

## Limitations
- PCA-guided dimension allocation may over-parameterize blocks that are noisy rather than informative, as the mechanism assumes higher PCA variance correlates with downstream task importance
- CKA-based head removal uses an arbitrary threshold (η=0.9) without sensitivity analysis, risking over-pruning that degrades multi-head attention benefits
- Relational KD's claimed improvement to NAS discrimination lacks direct validation through ablation studies comparing searched architecture quality with/without relational losses

## Confidence
- **High confidence**: ELM-Small achieves SOTA on GLUE with 15.6M parameters (Table V empirical results are clear and reproducible)
- **Medium confidence**: Dynamic dimension allocation and head-fusion mechanisms improve efficiency (mechanism descriptions are detailed, but direct ablation evidence is limited)
- **Low confidence**: Relational KD fundamentally improves NAS search quality (the mechanism is plausible but lacks direct validation against classical KD)

## Next Checks
1. **Mechanism validation**: Compare ELM-Small performance with fixed vs. dynamically allocated dimensions (as in Table V baseline) to isolate the contribution of PCA-guided expansion
2. **Hyperparameter sensitivity**: Test CKA threshold η at 0.8, 0.85, 0.95 to determine optimal trade-off between parameter savings and performance degradation
3. **Loss function ablation**: Replace relational KD with MSE during supernet training and measure both block diversity (cosine similarity as in Figure 6) and final searched architecture quality on downstream tasks