---
ver: rpa2
title: 'StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding'
arxiv_id: '2508.15717'
source_url: https://arxiv.org/abs/2508.15717
tags:
- video
- arxiv
- cache
- visual
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient long video understanding
  under memory constraints in streaming settings, where the video length and downstream
  queries are unknown. The authors propose StreamMem, a training-free, query-agnostic
  key-value (KV) cache compression framework for streaming video understanding with
  multimodal large language models (MLLMs).
---

# StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding

## Quick Facts
- arXiv ID: 2508.15717
- Source URL: https://arxiv.org/abs/2508.15717
- Reference count: 9
- Key result: State-of-the-art query-agnostic KV cache compression for streaming video understanding, achieving 66.9% accuracy on MLVU with 24K token budget

## Executive Summary
This paper addresses the challenge of efficient long video understanding under memory constraints in streaming settings, where video length and downstream queries are unknown. The authors propose StreamMem, a training-free, query-agnostic key-value (KV) cache compression framework for streaming video understanding with multimodal large language models (MLLMs). StreamMem compresses KV caches in a streaming manner using cross-attention scores between visual tokens and generic chat template tokens, enabling efficient question answering while maintaining a fixed-size memory budget. It also includes input frame filtering to reduce redundancy and frame-wise KV merging to create compact prototype representations. Evaluated on three offline and two streaming long video understanding benchmarks using LLaVA-OneVision-7B, Qwen2-VL-7B, and Qwen2.5-VL-3B, StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware approaches.

## Method Summary
StreamMem is a training-free, query-agnostic KV cache compression framework for streaming video understanding. It operates by (1) filtering redundant input frames using cosine similarity between vision encoder embeddings (threshold δ=0.95), (2) compressing KV caches using cross-attention scores between visual tokens and generic chat template tokens as proxy queries, and (3) creating compact prototype representations through frame-wise weighted KV merging. The framework uses YaRN positional embedding extension with different scaling factors (λ=8 for LLaVA-OV, λ=2 for Qwen2-VL, λ=1 for Qwen2.5-VL) to handle longer sequences than training allowed. StreamMem is evaluated on MLVU, EgoSchema, VideoMME (offline) and RVS-Ego, RVS-Movie (streaming) benchmarks, demonstrating state-of-the-art performance in query-agnostic compression while maintaining competitive accuracy with query-aware methods.

## Key Results
- Achieves 66.9% accuracy on MLVU with 24K token budget, outperforming LiveVLM (65.7%) and InfiniPot-V (64.3%)
- Outperforms query-agnostic baselines by 1.2-2.6% across multiple KV cache sizes (6K, 12K, 24K tokens)
- Competitive with query-aware methods on most tasks, with only slight degradation on "multi-detail" questions
- Input frame filtering with δ=0.95 provides 1.5% accuracy improvement while reducing input redundancy
- Frame-wise weighted merging outperforms both average merging and no merging by 0.6-1.3%

## Why This Works (Mechanism)

### Mechanism 1: Chat Template Attention as Query-Agnostic Saliency Proxy
- Claim: Cross-attention scores between visual tokens and generic chat template tokens provide a query-agnostic importance signal for KV cache compression.
- Mechanism: The chat template sequence acts as a universal query proxy that captures general semantic relationships between visual tokens and typical LLM response patterns, enabling the model to identify salient visual information without knowing the specific downstream question.

### Mechanism 2: Weighted Prototype Merging Preserves Temporal Information
- Claim: Frame-wise weighted KV merging creates compact prototype representations while preserving temporal relationships.
- Mechanism: By aggregating key-value pairs within each frame using attention-based weights, StreamMem maintains the temporal structure of the video while reducing memory footprint, allowing the model to reason about both spatial and temporal features in a compressed representation.

### Mechanism 3: Input Filtering Reduces Redundancy Without Losing Critical Information
- Claim: Cosine similarity-based frame filtering (δ=0.95) effectively removes redundant frames while preserving essential visual information.
- Mechanism: The high similarity threshold ensures that only visually similar frames are filtered, maintaining diversity in the visual stream while reducing the number of frames that need to be processed and stored in the KV cache.

## Foundational Learning
- Pre-trained multimodal models (LLaVA-OneVision-7B, Qwen2-VL-7B, Qwen2.5-VL-3B) serve as the base architecture for video understanding
- Vision encoder extracts visual features from frames, which are then processed by the LLM backbone
- Cross-attention between visual and language tokens enables multimodal reasoning
- Positional embeddings are extended using YaRN to handle longer sequences than originally trained for

## Architecture Onboarding
- Base models are initialized with pre-trained weights from LLaVA-OneVision, Qwen2-VL, and Qwen2.5-VL
- Vision encoder processes each input frame independently to extract visual features
- Chat template tokens (e.g., `[INST] ... [/INST]`) are used as proxy queries for attention computation
- Visual tokens are chunked and concatenated with language tokens to form the full input sequence
- StreamMem components (filtering, compression, merging) are applied during inference without additional training

## Open Questions the Paper Calls Out
- The paper does not explicitly discuss open questions in the text, but implicit open questions include:
- How well would this approach generalize to other video understanding tasks beyond the tested benchmarks?
- What is the impact of different chat template designs on the effectiveness of the query-agnostic proxy?
- Could the framework be extended to handle audio or other modalities in addition to visual information?

## Limitations
- Performance degradation on "multi-detail" questions when using query-agnostic compression compared to query-aware methods
- Reliance on a fixed threshold (δ=0.95) for frame filtering, which may not be optimal for all video types or domains
- Potential loss of fine-grained temporal information due to frame-wise merging, which could affect tasks requiring precise temporal reasoning
- The framework is specifically designed for streaming settings and may not be optimal for offline video understanding where the full video is available upfront

## Confidence
High confidence in the technical accuracy of this report based on the available paper information. The results and mechanisms described are consistent with the claims made in the abstract and methodology sections.

## Next Checks
- Verify the specific chat template used for proxy queries and its impact on performance
- Investigate the effect of different scaling factors (λ) for YaRN positional embedding extension across various video lengths
- Examine the ablation study results for frame filtering threshold variations beyond δ=0.95
- Check if the authors provide qualitative examples of the compressed KV cache representations