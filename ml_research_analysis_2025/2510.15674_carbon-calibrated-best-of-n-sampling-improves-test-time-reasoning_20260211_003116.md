---
ver: rpa2
title: 'CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning'
arxiv_id: '2510.15674'
source_url: https://arxiv.org/abs/2510.15674
tags:
- calibration
- reward
- carbon
- test-time
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CarBoN (Calibrated Best-of-N), a test-time
  calibration framework that improves reasoning efficiency in language models by adaptively
  reallocating inference budgets. The method explores diverse candidates first, then
  calibrates the model using additive shifts and temperature scaling to focus on high-reward
  regions.
---

# CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning

## Quick Facts
- **arXiv ID:** 2510.15674
- **Source URL:** https://arxiv.org/abs/2510.15674
- **Reference count:** 23
- **Key outcome:** CarBoN achieves comparable accuracy with up to 4× fewer rollouts than uncalibrated Best-of-N on MATH-500 and AIME-2024 benchmarks.

## Executive Summary
This paper introduces CarBoN (Calibrated Best-of-N), a test-time calibration framework that improves reasoning efficiency in language models by adaptively reallocating inference budgets. The method explores diverse candidates first, then calibrates the model using additive shifts and temperature scaling to focus on high-reward regions. Experiments on MATH-500 and AIME-2024 show CarBoN achieves comparable or higher accuracy with up to 4× fewer rollouts than uncalibrated methods, across general-purpose and math-specialized models. Theoretical analysis guarantees improved expected reward under finite sampling. Further, CarBoN generalizes to step-level sampling strategies like beam search, demonstrating broader applicability.

## Method Summary
CarBoN splits the total inference budget N into two phases: N₁ exploration samples and N₂ exploitation samples. In Phase 1, the model generates N₁ candidates at base temperature T=0.8, which are scored by a Process Reward Model (PRM). The top-k highest-scoring responses form a calibration dataset. In Phase 2, CarBoN learns calibration parameters (δ, T) from this dataset—δ is an additive shift vector in hidden space mapped through the LM head, and T is a temperature scalar. These parameters are optimized to increase the likelihood of high-reward patterns. The final answer is selected from the union of all N₁ + N₂ candidates using weighted selection based on PRM scores.

## Key Results
- CarBoN achieves MATH-500 accuracy comparable to uncalibrated Best-of-N with 4× fewer rollouts (e.g., 64 vs 256 samples)
- Joint calibration of δ and T outperforms either parameter alone, with δ alone providing most of the benefit
- CarBoN generalizes to step-level sampling strategies like beam search, demonstrating broader applicability beyond simple candidate selection

## Why This Works (Mechanism)

### Mechanism 1
Splitting inference budget into exploration-then-exploitation phases improves expected reward under finite sampling, provided the reward model reliably identifies high-quality responses. The total budget N is divided into N₁ exploration samples and N₂ exploitation samples. Exploration samples identify high-scoring regions via a Process Reward Model (PRM). The top-k responses form a calibration dataset D_calib(x). Calibration parameters (δ, T) are learned from this dataset and applied to the remaining N₂ samples. The final answer is selected from the union of all N₁ + N₂ candidates.

### Mechanism 2
The additive shift vector δ and temperature T provide complementary calibration effects—δ corrects token-level biases toward high-reward patterns, while T modulates distribution sharpness to balance diversity and correctness. δ ∈ R^d is learned in the hidden state dimension and projected through the fixed LM head W_LM to produce token-level logit biases. This avoids the high dimensionality of direct vocabulary-space calibration (V ≫ d). Temperature T controls the entropy of the output distribution—lower T concentrates probability mass on fewer tokens, while higher T maintains diversity.

### Mechanism 3
Calibration parameters exist that strictly improve the lower bound of expected reward under Best-of-N sampling, with theoretical guarantee. Theorem 2 establishes that if calibration increases the probability of the optimal output y* (i.e., p_θ(y*|x; δ*, T*) > p_θ(y*|x; 0, T_base)), then the lower bound R_LB(p) = r* - (1-p)^n(r* - r_other_max) strictly increases. This follows from the monotonicity of R_LB(p) with respect to p.

## Foundational Learning

- **Best-of-N Sampling with Verifiers**: Why needed: CarBoN builds on Best-of-N as its base strategy. Understanding how N candidates are generated, scored by a reward model, and selected is prerequisite to understanding why calibration improves it. Quick check: Can you explain why Best-of-N with a verifier typically outperforms majority voting (Self-Consistency)?

- **Temperature Scaling in Language Models**: Why needed: T is a core calibration parameter. Understanding how temperature affects the softmax distribution—low T sharpens (concentrates probability), high T flattens (increases diversity)—is essential for interpreting the T adaptation results. Quick check: If temperature T → 0, what happens to the output distribution? What if T → ∞?

- **Process Reward Models (PRMs)**: Why needed: CarBoN relies entirely on PRM scores to identify high-reward regions and construct calibration datasets. Understanding that PRMs assign step-level scores (0-1) and that the final step score is used as the overall reward is critical. Quick check: Why might using the final step score outperform product or minimum aggregation strategies for PRMs trained via Monte Carlo estimation?

## Architecture Onboarding

- **Component map**: Exploration Generator -> PRM Scorer -> Calibration Dataset Builder -> Calibration Trainer -> Exploitation Generator -> Final Selector
- **Critical path**: Exploration generation → PRM scoring → Calibration dataset construction → (δ, T) training → Exploitation generation → Final selection. The calibration training is lightweight (operates on cached logits, no additional model forward passes).
- **Design tradeoffs**:
  - Budget split (N₁ vs. N₂): Paper uses N₁ = N₂ = N/2. More exploration improves calibration quality but leaves fewer samples for exploitation.
  - Calibration set size (k): k = N₁/4. Larger k provides more training data but may include lower-quality responses; smaller k risks overfitting.
  - Training epochs (100): Arbitrary choice; no ablation reported. Risk of overfitting to small calibration set mitigated by L2 regularization.
- **Failure signatures**:
  - PRM miscalibration: If PRM scores are uninformative (e.g., near-uniform), top-k selection provides no signal, and calibration learns from noise.
  - δ overfitting: If L2 regularization λ_δ is too weak, δ may memorize calibration set patterns that don't generalize to N₂ samples.
  - Temperature collapse: If learned T approaches 0, exploitation samples become near-identical, wasting the N₂ budget.
- **First 3 experiments**:
  1. Reproduce MATH-500 baseline: Implement standard Best-of-N with Qwen2.5-Math-1.5B-Instruct, T=0.8, N∈{8,16,32,64,128,256}, weighted selection. Confirm plateauing behavior at large N.
  2. Ablate calibration parameters: Implement CarBoN with (δ only), (T only), and (δ, T) jointly. Verify that joint calibration yields highest accuracy per Table 3.
  3. Validate budget efficiency claim: For each model, compare CarBoN accuracy at N=64 against uncalibrated Best-of-N at N=256. Confirm the claimed ~4× efficiency gain (Table 1).

## Open Questions the Paper Calls Out

- **How does CarBoN's performance vary with the quality and reliability of the underlying Process Reward Model (PRM), particularly when the PRM provides noisy or biased reward signals?**: The method depends entirely on PRM scores for both calibration set selection and final answer selection. Appendix B.1 acknowledges PRM noise by adding Gaussian noise in the binary search motivating example, but all main experiments use a single PRM (Qwen2.5-Math-PRM-7B) without sensitivity analysis.

- **Does test-time calibration generalize effectively to non-mathematical reasoning domains (e.g., code generation, logical deduction, multi-hop QA)?**: All experiments are restricted to MATH-500 and AIME-2024 benchmarks. The paper claims the framework is "general" but provides no empirical validation outside mathematical problem-solving.

- **What is the optimal allocation between exploration (N₁) and exploitation (N₂) samples, and how should this allocation adapt to problem difficulty or model confidence?**: The paper uses a fixed N₁ = N₂ = N/2 split (Appendix C.3) without justification or ablation. Theoretical analysis (Corollary 3) proves keeping both sets is optimal, but does not address optimal sizing.

## Limitations
- The calibration mechanism relies heavily on the PRM providing a meaningful signal for identifying high-reward regions, but this assumption is not validated across different PRM qualities or alternative reward functions.
- The theoretical guarantee is restricted to the lower bound improvement and assumes a unique optimal output, limiting its applicability to real-world scenarios with multiple valid solutions.
- The specific logit calibration mechanism using δ and T is novel and lacks external validation, with weak corpus evidence for this approach.

## Confidence

- **High confidence**: The experimental results showing CarBoN achieves comparable accuracy with 4× fewer rollouts are well-supported by the MATH-500 and AIME-2024 results presented in Tables 1 and 2.
- **Medium confidence**: The theoretical analysis proving improved lower bounds under calibration assumptions is mathematically sound but relies on strong assumptions (unique optimal output, PRM correlation with correctness).
- **Low confidence**: The specific logit calibration mechanism using δ and T is novel and lacks external validation. The weak corpus evidence for this approach (only 1 related paper on logit calibration specifically) and absence of ablation studies on calibration hyperparameters create uncertainty about optimal implementation choices.

## Next Checks

1. **Ablation on Calibration Hyperparameters**: Systematically vary the number of calibration epochs (10-200), weight decay (10⁻³ to 10⁻¹), and budget split ratio (N₁:N₂ from 1:3 to 3:1) to determine sensitivity and optimal values for different model sizes.

2. **PRM Quality Sensitivity**: Evaluate CarBoN performance using PRMs with varying correlation to ground truth accuracy (e.g., by using PRMs trained on different datasets or with different architectures) to quantify the dependence on PRM quality.

3. **Step-Level Calibration Application**: Apply the CarBoN calibration framework to beam search and other step-level sampling strategies as suggested in the paper, measuring whether the efficiency gains translate to these alternative inference methods.