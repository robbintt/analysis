---
ver: rpa2
title: Anchor-based Maximum Discrepancy for Relative Similarity Testing
arxiv_id: '2510.10477'
source_url: https://arxiv.org/abs/2510.10477
tags:
- test
- testing
- hypothesis
- relative
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of kernel selection in relative
  similarity testing, where manually specifying a hypothesis before kernel selection
  leads to ill-defined problems. The authors propose an anchor-based maximum discrepancy
  (AMD) approach that simultaneously learns a proper hypothesis and kernel.
---

# Anchor-based Maximum Discrepancy for Relative Similarity Testing

## Quick Facts
- **arXiv ID**: 2510.10477
- **Source URL**: https://arxiv.org/abs/2510.10477
- **Reference count**: 40
- **One-line result**: AMD simultaneously learns hypothesis and kernel to avoid selection bias in relative similarity testing, outperforming state-of-the-art methods on MNIST and CIFAR10.

## Executive Summary
This paper tackles the challenge of kernel selection in relative similarity testing, where manually specifying a hypothesis before kernel selection leads to ill-defined problems. The authors propose an anchor-based maximum discrepancy (AMD) approach that simultaneously learns a proper hypothesis and kernel. AMD defines relative similarity as the maximum discrepancy between distances of (U,P) and (U,Q) in a space of deep kernels. The method incorporates two phases: Phase I estimates AMD and infers the potential relative similarity relationship using augmented data to prevent overfitting, while Phase II assesses statistical significance using wild bootstrap. Theoretical guarantees are provided, showing AMD is a valid metric with consistent estimation. Experiments on benchmark datasets (MNIST, CIFAR10) demonstrate AMD's effectiveness in testing relative similarity and its advantage over state-of-the-art methods, particularly when the true relationship is unknown. The approach is also validated in practical applications including model performance evaluation and adversarial perturbation detection.

## Method Summary
AMD defines relative similarity as the maximum discrepancy between distances of (U,P) and (U,Q) in a space of deep kernels. The method incorporates two phases: Phase I estimates AMD and infers the potential relative similarity relationship using augmented data to prevent overfitting, while Phase II assesses statistical significance using wild bootstrap. The key innovation is learning both the hypothesis and kernel simultaneously, avoiding the selection bias inherent in prior methods that pre-specify relationships.

## Key Results
- AMD demonstrates superior performance compared to state-of-the-art methods on MNIST and CIFAR10 datasets
- The method effectively detects relative similarity relationships even when the true relationship is unknown
- AMD shows advantage in practical applications including model performance evaluation and adversarial perturbation detection
- Theoretical guarantees prove AMD is a valid metric with consistent estimation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Simultaneous optimization of the hypothesis and kernel avoids the "ill-defined" selection bias inherent in prior relative similarity tests.
- **Mechanism:** Existing methods pre-specify a relationship (e.g., $P$ is closer to $U$ than $Q$) and then select a kernel that maximizes evidence for this claim. AMD flips this by defining relative similarity as the *maximum* discrepancy between distance pairs over a kernel space ($\mathcal{F}_K$). It searches for the kernel that maximizes $|\text{MMD}^2(U,P) - \text{MMD}^2(U,Q)|$, inferring the direction ($F$) from the sign of the resulting statistic.
- **Core assumption:** The underlying true relationship is learnable via a deep kernel parameterization within $\mathcal{F}_K$.
- **Evidence anchors:**
  - [abstract] "cope with this challenge via learning a proper hypothesis and a kernel simultaneously... avoiding a selection towards a prespecified relative similarity relationship."
  - [section 1] "Once the hypothesis is specified first, we can always find a kernel such that the hypothesis is rejected... making relative similarity testing ill-defined."
  - [corpus] Contextual support for IPM/MMD as a strong basis for discrepancy measurement is found in "CKGAN" and "Signature MMD" papers.
- **Break condition:** If the kernel space $\mathcal{F}_K$ is insufficiently expressive to distinguish $(U,P)$ from $(U,Q)$, the inferred hypothesis $F$ will be random, degrading test power.

### Mechanism 2
- **Claim:** Data augmentation with consistency regularization enables robust kernel selection with limited samples.
- **Mechanism:** To prevent the deep kernel from overfitting to the specific training samples in Phase I, the authors introduce augmented samples ($X_{aug}, Y_{aug}$) constructed via mixing. These augmented samples are theoretically equidistant to the anchor $U$. The optimization objective penalizes the kernel if it detects a large discrepancy in these augmented pairs, forcing it to focus on genuine distributional shifts rather than sample noise.
- **Core assumption:** The mixing strategy ($x_{aug} = 0.5x + 0.5y$) preserves the property of equidistance in the RKHS.
- **Evidence anchors:**
  - [section 3.2] "We mitigate the overfitting issue via an iterative optimization process known as data augmentation... ensuring the consistency."
  - [section 5.1] Tables 1 and 2 show AMD with augmentation (AMD) outperforms the non-augmented version (AMD-NA), particularly with smaller sample sizes.
  - [corpus] General MMD literature confirms sensitivity to kernel choice and sample size, supporting the need for regularization.
- **Break condition:** If the regularization parameter $\lambda$ is too high, it may suppress the signal from the real data; if too low, overfitting occurs (sensitivity analysis in Figure 3 confirms a functional range exists).

### Mechanism 3
- **Claim:** A two-phase data splitting strategy controls Type-I error while maintaining high test power.
- **Mechanism:** The data is split into Training (Phase I) and Testing (Phase II). Phase I acts as a "model selection" step to identify the hypothesis direction $F$. Phase II uses the remaining data to test the specific unified null hypothesis $H_0: F \cdot d \leq 0$ via wild bootstrap. This separation prevents the selection process from "seeing" the test data, which would otherwise inflate false positives.
- **Core assumption:** The samples in Phase I and Phase II are independent and identically distributed (i.i.d.).
- **Evidence anchors:**
  - [section 3.3] "We assess the statistical significance... using testing samples... drawn independently... follows the data-splitting strategy... to ensure the validity of the test."
  - [section 4] Theorem 5 theoretically quantifies the test power advantage, assuming the learned $F$ is better than random guessing ($\beta > 0.5$).
  - [corpus] "A Scalable NystrÃ¶m-Based Kernel Two-Sample Test" discusses computational and statistical tradeoffs in kernel tests, validating the need for rigorous testing frameworks.
- **Break condition:** If the sample size is too small, the inferred hypothesis $F$ from Phase I will be incorrect ($\beta \approx 0.5$), rendering the power advantage in Phase II negligible.

## Foundational Learning

- **Concept: Integral Probability Metrics (IPM) & MMD**
  - **Why needed here:** AMD is defined specifically as an IPM over a kernel space. You cannot understand the "discrepancy" being maximized without grasping how MMD measures the distance between mean embeddings in an RKHS.
  - **Quick check question:** If two distributions $P$ and $Q$ are identical, what is the value of $\text{MMD}(P, Q)$?

- **Concept: Hypothesis Testing (Null vs. Alternative)**
  - **Why needed here:** The paper fundamentally restructures how the alternative hypothesis is set. You need to distinguish between a *composite null* (distance is less than or equal) and the *proxy null* used for bootstrapping.
  - **Quick check question:** In a standard hypothesis test, does a low p-value prove the specific alternative hypothesis, or just reject the null?

- **Concept: Deep Kernels**
  - **Why needed here:** The method moves beyond Gaussian kernels to kernels defined by neural networks $\kappa(x,y) = [(1-\epsilon)G(\phi(x), \phi(y)) + \epsilon]G(x,y)$. Understanding how the network $\phi$ transforms the data is key to the "simultaneous learning" claim.
  - **Quick check question:** Why would a standard Gaussian kernel fail to distinguish relative similarity in high-dimensional image data compared to a deep kernel?

## Architecture Onboarding

- **Component map:** Data Splitter -> Deep Kernel Network -> Augmentation Module -> Phase I Optimizer -> Phase II Tester
- **Critical path:** The Phase I optimization loop. If this converges to a local minimum or overfits (yielding a bad $F$), the entire subsequent test is invalid.
- **Design tradeoffs:**
  - **Sample Efficiency vs. Power:** You need enough data in Phase I to correctly infer $F$, but enough in Phase II to have statistical power. The paper uses 50% splits but this may need tuning for low-data regimes.
  - **Expressiveness vs. Overfitting:** A larger kernel network improves expressiveness but risks overfitting the discrepancy, necessitating the specific augmentation regularization strategy described in Mechanism 2.
- **Failure signatures:**
  - **Inconsistent $F$:** If $F$ flips randomly between runs on the same data, the signal is too weak or the learning rate is too high.
  - **Type-I Error Inflation:** If testing is performed on the same data used for kernel selection (skipping the split), you will see 100% rejection rates for $\nu=0.5$.
  - **Stuck Optimization:** If $\lambda$ is too large, the regularization term dominates, driving the estimated discrepancy to zero (metric collapse).
- **First 3 experiments:**
  1. **Sanity Check (Type I Error):** Run AMD with $\nu=0.5$ (where $U = 0.5P + 0.5Q$). Verify the rejection rate is $\approx \alpha$ (e.g., 0.05). This confirms the data splitting is working.
  2. **Ablation on Augmentation:** Compare AMD vs. "AMD-NA" (no augmentation) on MNIST/CIFAR subsets. Plot test power vs. sample size to visualize the overfitting gap.
  3. **Adversarial Perturbation Validation:** Replicate the CIFAR-10 perturbation experiment. Plot rejection rates as the perturbation level increases to see if AMD detects the divergence earlier than baselines like MMD-D.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the AMD framework be extended to handle relative similarity testing involving more than three distributions, specifically to determine which of multiple candidates is closest to an anchor?
- Basis in paper: [explicit] The Conclusion states that "an interesting direction for future work is to extend relative similarity testing from three distributions to the more general case of multiple distributions."
- Why unresolved: The current metric and hypothesis testing framework (Phase I and II) are specifically designed to compare distances between one anchor ($U$) and exactly two distributions ($P$ and $Q$). Generalizing this to multiple comparisons likely introduces combinatorial complexity and multiple testing correction challenges that the current theory does not address.
- What evidence would resolve it: A formal extension of the AMD metric and Algorithm 1 for $N$ distributions, accompanied by theoretical guarantees on Type-I error control and empirical validation on datasets with multiple variants (e.g., multiple ImageNet versions).

### Open Question 2
- Question: Can the gap between theoretical consistency guarantees and practical implementation be closed by accounting for the inherent randomness of optimization algorithms?
- Basis in paper: [explicit] The Limitation Statement notes that "the optimization analysis in Theorem 3 reveals a gap between theoretical guarantees and practical implementation due to the inherent randomness of optimization algorithms."
- Why unresolved: While Theorem 3 proves consistency as sample size approaches infinity, it assumes ideal optimization. In practice, the kernel is learned via stochastic gradient descent, where random initialization and batch sampling introduce noise not fully captured by the theoretical bounds, potentially affecting finite-sample performance.
- What evidence would resolve it: Theoretical analysis that incorporates stochastic optimization dynamics into the convergence rates, or empirical sensitivity analysis showing the variance of the test results relative to different random seeds and optimizer states.

### Open Question 3
- Question: How can the time complexity of Phase I be reduced to improve scalability on large datasets while maintaining the regularization benefits of data augmentation?
- Basis in paper: [inferred] The Limitation Statement discusses that "using augmented data increases time complexity," and Table 13 shows AMD is significantly slower than baselines like MMD-H and MMD-D due to the optimization over augmented samples.
- Why unresolved: The current method requires generating and optimizing over interpolated samples to prevent overfitting. This effectively doubles the data processing requirements and iterations per epoch, making the method computationally expensive compared to standard kernel tests.
- What evidence would resolve it: An ablation study testing alternative regularization techniques (e.g., weight decay, spectral normalization) or optimization approximations that could replace the data augmentation step without increasing the Type-I error.

### Open Question 4
- Question: Is the estimation of AMD sensitive to the specific choice of the augmentation strategy (Eqn. 5) if the augmented samples do not strictly satisfy the $E[\hat{d}_\kappa] = 0$ condition?
- Basis in paper: [inferred] Section 3.2 states that augmented samples are constructed specifically to satisfy $E[\hat{d}_\kappa(Z_{aug}, X_{aug}, Y_{aug})] = 0$ to ensure consistency. It is unclear if this restricts the method to linear interpolations or if other valid augmentations exist.
- Why unresolved: The paper mandates a specific mixing strategy ($0.5x + 0.5y$) for theoretical soundness. However, in domains like text or graphs, such simple linear interpolation may not produce meaningful samples, potentially limiting the method's applicability or requiring a relaxation of the consistency constraints.
- What evidence would resolve it: Experiments utilizing domain-specific augmentations (e.g., back-translation for text) that might violate the zero-expectation condition, analyzed for their impact on the test's consistency and power.

## Limitations
- The method assumes the deep kernel space is sufficiently expressive to capture true distributional differences; insufficient expressiveness leads to degraded test power
- Two-phase data splitting strategy may be statistically inefficient for small sample sizes
- Lack of detailed sensitivity analysis for the choice of regularization parameter $\lambda$ across diverse data types
- Paper does not provide detailed architecture specifications for the deep kernel network

## Confidence
- **High Confidence**: Theoretical guarantees (consistency of AMD estimation, control of Type-I error via data splitting) are well-founded, assuming i.i.d. data assumption holds
- **Medium Confidence**: Empirical results demonstrating superior performance on MNIST and CIFAR10 are compelling, but lack of detail on deep kernel architecture and training hyperparameters makes exact replication difficult
- **Low Confidence**: Claim of AMD's advantage in adversarial perturbation detection is based on a single experiment; more diverse threat models would be needed to generalize this finding

## Next Checks
1. **Ablation Study**: Systematically vary the regularization parameter $\lambda$ on a synthetic dataset where ground truth relative similarity is known. Plot test power vs. $\lambda$ to identify optimal range and confirm sensitivity analysis.
2. **Robustness to Data Splitting**: Run experiment with different training/testing splits (e.g., 60/40, 70/30) on MNIST dataset. Measure if test power remains consistent to validate robustness of two-phase strategy.
3. **Kernel Expressiveness Test**: Compare AMD against simpler non-deep kernel baseline (e.g., Gaussian kernel with median heuristic) on high-dimensional dataset (e.g., CIFAR100). If AMD doesn't show significant advantage, it suggests deep kernel component isn't essential for tested datasets.