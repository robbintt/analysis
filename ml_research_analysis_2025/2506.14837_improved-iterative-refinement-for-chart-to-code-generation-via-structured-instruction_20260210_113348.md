---
ver: rpa2
title: Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction
arxiv_id: '2506.14837'
source_url: https://arxiv.org/abs/2506.14837
tags:
- chart
- code
- generation
- description
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of chart-to-code generation,
  where multimodal large language models (MLLMs) are tasked with generating executable
  code to reproduce a given chart. The authors propose ChartIR, an iterative refinement
  method based on structured instruction, to enhance the performance of MLLMs on this
  task.
---

# Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction

## Quick Facts
- arXiv ID: 2506.14837
- Source URL: https://arxiv.org/abs/2506.14837
- Reference count: 40
- Primary result: ChartIR significantly outperforms existing methods on chart-to-code generation benchmarks using iterative refinement based on structured instructions.

## Executive Summary
This paper addresses the challenge of chart-to-code generation, where multimodal large language models (MLLMs) must generate executable code to reproduce a given chart. The authors propose ChartIR, an iterative refinement method based on structured instruction, to enhance MLLM performance on this task. ChartIR decomposes the problem into two stages: initial code generation and iterative refinement. In the initial stage, a structured chart description is generated to capture key visual elements. In the refinement stage, the model iteratively updates the code by comparing the generated chart with the reference chart and identifying differences. Experiments on two benchmark datasets, Plot2Code and ChartMimic, demonstrate that ChartIR significantly outperforms existing methods, including direct generation and METAL, on both open-source (Qwen2-VL) and closed-source (GPT-4o) models.

## Method Summary
ChartIR is a two-stage iterative refinement framework for chart-to-code generation. The first stage generates a structured chart description that captures visual elements such as chart type, data, and styling. The second stage uses this description to generate initial code, then iteratively refines it by comparing the generated chart with the reference chart to identify and correct differences. The method leverages structured instructions to guide the MLLM through both stages, improving the fidelity of the generated code to the target chart. The approach is evaluated on two benchmark datasets, Plot2Code and ChartMimic, using metrics including GPT-4o Score, low-level visual metrics (Text, Layout, Type, Color), and traditional image similarity metrics (SSIM, CLIP, MSE, PSNR).

## Key Results
- ChartIR significantly outperforms existing methods, including direct generation and METAL, on both open-source (Qwen2-VL) and closed-source (GPT-4o) models.
- Substantial improvements in GPT-4o Score, low-level metrics (Text, Layout, Type, Color), and traditional image similarity metrics (SSIM, CLIP, MSE, PSNR).
- Strong performance on two benchmark datasets, Plot2Code and ChartMimic, demonstrating the effectiveness of ChartIR in generating high-quality, visually faithful chart code.

## Why This Works (Mechanism)
ChartIR works by decomposing the chart-to-code generation task into structured stages, enabling iterative refinement through visual comparison. The structured chart description provides a clear intermediate representation that guides both initial code generation and subsequent refinement. By explicitly comparing generated and reference charts, the method identifies specific differences that can be corrected in each iteration. This approach leverages the MLLM's ability to interpret structured instructions while maintaining focus on visual fidelity through iterative feedback loops.

## Foundational Learning

**Structured Chart Description** - Why needed: Provides a clear intermediate representation of visual elements for guiding code generation. Quick check: Can the model accurately identify chart type, data, and styling from the description?

**Iterative Refinement** - Why needed: Enables progressive improvement by comparing generated and reference charts. Quick check: Does each refinement cycle reduce visual differences?

**Visual Grounding** - Why needed: Ensures generated code faithfully reproduces visual elements of the target chart. Quick check: Are key visual features (text, layout, colors) accurately captured?

**Multimodal Integration** - Why needed: Combines visual understanding with code generation capabilities. Quick check: Can the model effectively translate visual information into executable code?

**Structured Instruction** - Why needed: Guides the MLLM through complex multi-stage reasoning. Quick check: Does the instruction format improve task completion accuracy?

## Architecture Onboarding

**Component Map**: Structured Chart Description -> Initial Code Generation -> Visual Comparison -> Iterative Refinement -> Final Code

**Critical Path**: The structured chart description stage is critical as errors here propagate through all subsequent stages. The visual comparison and refinement loop forms the core improvement mechanism.

**Design Tradeoffs**: The two-stage approach trades computational efficiency for improved accuracy. Structured instructions add overhead but provide clearer guidance for complex visual reasoning tasks.

**Failure Signatures**: Poor visual grounding in the initial description leads to cascading errors. Ineffective visual comparison may result in convergence to suboptimal solutions or failure to converge.

**First Experiments**:
1. Test structured chart description accuracy on diverse chart types
2. Evaluate single refinement cycle performance versus multiple cycles
3. Compare structured instruction format against free-form prompting

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to real-world charts beyond synthetic benchmarks remains uncertain
- High-quality visual grounding in initial stage is critical but may be challenging for complex charts
- Evaluation relies heavily on quantitative metrics without user studies or usability assessments

## Confidence
- Core performance claims: High
- Scalability to diverse real-world charts: Medium
- Novelty of structured instruction paradigm: High

## Next Checks
1. Evaluate ChartIR on real-world chart datasets with higher visual and structural complexity to assess generalization beyond synthetic benchmarks.
2. Conduct ablation studies isolating the contributions of the structured chart description stage versus iterative refinement to quantify their relative impact on performance.
3. Perform user studies or task-based evaluations (e.g., code readability, ease of modification) to complement quantitative metrics and validate practical utility.