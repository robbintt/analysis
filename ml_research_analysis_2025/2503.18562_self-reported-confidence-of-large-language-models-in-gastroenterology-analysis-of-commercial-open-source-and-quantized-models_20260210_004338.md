---
ver: rpa2
title: 'Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis
  of Commercial, Open-Source, and Quantized Models'
arxiv_id: '2503.18562'
source_url: https://arxiv.org/abs/2503.18562
tags:
- confidence
- scores
- accuracy
- llms
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated self-reported confidence of large language
  models (LLMs) across 300 gastroenterology board-style questions, testing 48 models
  from seven families including GPT, Claude, Llama, Phi, Mistral, Gemini, and Gemma.
  Using confidence elicitation, models provided certainty scores (0-10) for their
  answers.
---

# Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis of Commercial, Open-Source, and Quantized Models

## Quick Facts
- **arXiv ID**: 2503.18562
- **Source URL**: https://arxiv.org/abs/2503.18562
- **Reference count**: 33
- **Primary result**: All tested LLMs exhibited overconfidence in gastroenterology questions, with newer larger models showing better but still insufficient calibration

## Executive Summary
This study evaluated self-reported confidence of 48 large language models across seven families using 300 gastroenterology board-style questions. Models provided certainty scores (0-10) for their answers, revealing consistent overconfidence patterns across commercial, open-source, and quantized models. Top performers achieved Brier scores of 0.15-0.2 and AUROC of 0.6, yet confidence consistently exceeded accuracy. Larger, newer models demonstrated better calibration but fell short of clinical standards. The findings highlight persistent challenges in LLM uncertainty quantification for medical applications.

## Method Summary
The study tested 48 models from seven families (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma) using 300 gastroenterology board-style questions from Med-Challenger. Models provided binary confidence scores (0-10 scale) for their answers. Performance was evaluated using Brier scores and AUROC metrics to assess calibration accuracy. The analysis compared commercial, open-source, and quantized models to identify patterns in confidence reporting and accuracy across different model architectures and sizes.

## Key Results
- All models showed consistent overconfidence, with average confidence exceeding actual accuracy
- Top performers (GPT-o1 preview, GPT-4o, Claude-3.5-Sonnet) achieved Brier scores of 0.15-0.2 and AUROC of 0.6
- Larger, newer models demonstrated significantly better calibration than older or smaller models
- No model achieved clinical-grade uncertainty quantification despite improvements in calibration

## Why This Works (Mechanism)
None

## Foundational Learning
- **Confidence elicitation methods**: Understanding different approaches to extracting model uncertainty is crucial for proper calibration assessment and determining the most effective ways to query LLM confidence
- **Brier score metrics**: Essential for evaluating probabilistic predictions against binary outcomes, providing quantitative measures of calibration quality
- **AUROC evaluation**: Critical for assessing discrimination ability between correct and incorrect predictions across confidence thresholds
- **Model calibration**: Fundamental concept for ensuring LLM confidence scores accurately reflect prediction probabilities, particularly important in high-stakes medical applications
- **Board-style question assessment**: Provides standardized evaluation framework but may not fully represent clinical decision-making complexity

## Architecture Onboarding
- **Component map**: Question Input -> LLM Processing -> Confidence Elicitation -> Answer Generation -> Performance Evaluation (Brier Score, AUROC)
- **Critical path**: Input question processing to confidence score generation, where miscalibration most directly impacts clinical utility
- **Design tradeoffs**: Standardized board questions vs. real-world clinical complexity; binary confidence scales vs. probabilistic uncertainty expression; computational efficiency vs. calibration accuracy
- **Failure signatures**: Overconfidence across all models; performance gap between top and bottom performers; persistent calibration issues despite model size improvements
- **3 first experiments**:
  1. Test alternative confidence elicitation methods (probability distributions vs. 0-10 scale)
  2. Evaluate performance on real-world clinical case studies beyond board questions
  3. Compare calibration across different medical specialties to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope limited to gastroenterology board-style questions from single resource
- Binary confidence elicitation method may not capture full uncertainty spectrum
- Performance on standardized questions may not translate to clinical decision-making
- No validation of whether improved calibration metrics translate to better patient outcomes

## Confidence
**High Confidence**: All tested models exhibited overconfidence relative to actual accuracy, supported by consistent Brier score and AUROC metrics across multiple model families.

**Medium Confidence**: Comparative performance rankings between specific models should be interpreted cautiously due to limited question set scope.

**Low Confidence**: Clinical relevance of current calibration levels remains uncertain without validation of real-world patient outcome impact.

## Next Checks
1. **Clinical Scenario Testing**: Validate findings using diverse real-world clinical case studies beyond board-style questions
2. **Calibration Method Comparison**: Test alternative confidence elicitation approaches to determine if different methods yield better calibration
3. **Longitudinal Performance Tracking**: Monitor calibration improvements as newer model versions are released to establish improvement trends