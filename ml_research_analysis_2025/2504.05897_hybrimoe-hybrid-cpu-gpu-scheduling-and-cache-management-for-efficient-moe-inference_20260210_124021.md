---
ver: rpa2
title: 'HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE
  Inference'
arxiv_id: '2504.05897'
source_url: https://arxiv.org/abs/2504.05897
tags:
- expert
- experts
- scheduling
- cache
- hybrimoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of efficiently scheduling hybrid
  CPU-GPU inference for Mixture-of-Experts (MoE) models, which are inherently complex
  due to dynamic and unpredictable expert activation patterns. The authors propose
  HybriMoE, a system that improves resource utilization through three key techniques:
  (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and
  GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based
  caching algorithm to mitigate expert activation instability.'
---

# HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference

## Quick Facts
- arXiv ID: 2504.05897
- Source URL: https://arxiv.org/abs/2504.05897
- Reference count: 31
- Key outcome: Achieves 1.33× speedup in prefill stage and 1.70× in decode stage for MoE inference

## Executive Summary
HybriMoE addresses the challenge of efficiently scheduling hybrid CPU-GPU inference for Mixture-of-Experts (MoE) models, which suffer from dynamic and unpredictable expert activation patterns. The authors propose a system that improves resource utilization through three key techniques: dynamic intra-layer scheduling to balance workloads across CPU and GPU, impact-driven inter-layer prefetching, and score-based caching to mitigate expert activation instability. Implemented on top of the kTransformers framework and evaluated on three widely used MoE-based LLMs, HybriMoE demonstrates significant performance improvements over state-of-the-art hybrid MoE inference frameworks.

## Method Summary
HybriMoE is a hybrid CPU-GPU scheduling and cache management system for efficient MoE inference. The method comprises three core components: (1) dynamic intra-layer scheduling that simulates execution timelines to optimally assign activated experts to CPU or GPU queues based on cache state and load characteristics, (2) impact-driven prefetching that looks ahead three layers to prioritize loading experts based on their projected impact on overall scheduling efficiency, and (3) a Minus Recent Score (MRS) caching algorithm that retains experts based on potential future activation scores rather than just past usage. The system is built on kTransformers framework using llama.cpp kernels and Marlin 4-bit quantization, and is evaluated on Mixtral-8x7B-Instruct, DeepSeek-V2-Lite-Chat, and Qwen2-57B-A14B-Instruct models.

## Key Results
- Achieves 1.33× speedup in prefill stage compared to kTransformers baseline
- Achieves 1.70× speedup in decode stage compared to kTransformers baseline
- MRS caching outperforms LRU by 6-8% at low cache ratios (25%)
- Dynamic scheduling improves resource utilization by overlapping CPU and GPU computation

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Intra-Layer Hybrid Scheduling
The system dynamically balances workloads between CPU and GPU within a single layer by categorizing activated experts into GPU queue (cached experts) and CPU queue (uncached experts). It applies priority rules: GPU processes high-load cached experts first; CPU processes low-load uncached experts first. A simulation phase iterates through these queues to find the allocation that minimizes the maximum completion time. This works because it reduces idle time compared to static mapping, provided the simulation overhead is negligible and the CPU has sufficient compute capability.

### Mechanism 2: Impact-Driven Inter-Layer Prefetching
The system looks ahead three layers by reusing gating information and simulates the impact of prefetching specific experts on overall scheduling efficiency. Rather than simply prefetching the most likely expert, it prioritizes preloading experts that yield the highest expected gain in the simulation. This works because it maximizes resource utilization by considering future scheduling impact, assuming expert activation patterns have sufficient temporal correlation across adjacent layers.

### Mechanism 3: Score-Aware Caching (MRS)
The MRS policy increases cache hit rates by retaining experts based on their potential future activation scores. It maintains an accumulated score S for experts, prioritizing those with high routing scores in the current iteration (even if not top-K selected). The update rule is S = α × TopP(s) + (1 - α) × S. This works because high routing scores in the current iteration correlate positively with future selection probability, addressing the instability of MoE activations that defeats traditional LFU/LRU policies.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) Sparse Activation**
  - Why needed here: HybriMoE relies on the fact that only a subset of experts (K out of N) are active per token. If all experts were active, the scheduling simulation would be too complex and the compute load too heavy for the CPU to handle effectively.
  - Quick check question: Can you explain why the "Top-K" gating mechanism allows for the overlapping of CPU and GPU computation in this architecture?

- **Concept: PCIe Bandwidth Bottlenecks**
  - Why needed here: The primary motivation for HybriMoE is that loading expert weights from CPU RAM to GPU VRAM over PCIe is slower than the actual computation. Understanding the ratio of transfer time to compute time is essential to grasp why CPU offloading is viable.
  - Quick check question: If PCIe bandwidth were increased 10x (e.g., CXL interconnect), would the "hybrid scheduling" mechanism still be necessary, or would a purely GPU-centric approach suffice?

- **Concept: Prefill vs. Decode Stages**
  - Why needed here: HybriMoE achieves different speedups in these stages (1.33x prefill vs 1.70x decode). Prefill is compute-bound and parallelizable; Decode is memory-bound and sequential. The system optimizes them differently.
  - Quick check question: Why does the "uneven expert mapping" impact the prefill stage more severely than the decode stage in baseline systems?

## Architecture Onboarding

- **Component map:** User Interface -> kTransformers Framework -> Kernel Layer (llama.cpp, Marlin quantization) -> Scheduler Core (Simulation Engine, Prefetcher, Cache Manager) -> Execution Layer (CPU compute, GPU compute, PCIe Transfer streams)

- **Critical path:**
  1. Warmup: Profile CPU/GPU/PCIe speeds
  2. Input: Token sequence enters
  3. Routing: Gate scores determine activated experts
  4. Simulation: Scheduler simulates timeline to assign experts to CPU vs GPU queues based on current cache state
  5. Execution: CPU computes low-load uncached experts; GPU computes high-load cached experts; PCIe transfers future experts in parallel
  6. Cache Update: Update MRS scores for all experts (active and inactive)

- **Design tradeoffs:**
  - Simulation Granularity vs. Overhead: The system simulates schedules on-the-fly. If the number of experts is very high (e.g., DeepSeek with 64), the simulation itself could theoretically add latency, though the paper claims it is negligible (greedy approach).
  - CPU Compute vs. Transfer: The system chooses to compute on CPU only if it estimates CPU execution is faster than waiting for a PCIe transfer.
  - Lookahead Depth: The prefetcher looks 3 layers ahead. Deeper lookahead might introduce more prediction errors; shallower might miss opportunities.

- **Failure signatures:**
  - CPU Starvation: If the GPU is infinitely fast but the CPU is slow, the "hybrid" benefit collapses. The logs would show the GPU waiting for the CPU to finish its queue.
  - Cache Thrashing: If the MRS score α is poorly tuned for a specific dataset, the cache might evict experts needed immediately in the next step, visible as a drop in "Cache Hit Rate."
  - Prefetch Pollution: High PCIe traffic for low-impact prefetches could saturate bandwidth, delaying on-demand loads.

- **First 3 experiments:**
  1. Latency Breakdown (Ablation): Run inference with only Scheduling, only Caching, and only Prefetching enabled to isolate which component contributes most to the 1.70x decode speedup.
  2. Cache Pressure Test: Run the model with restricted GPU memory (e.g., forcing 25% cache ratio) to verify if MRS maintains the 6-8% hit rate advantage over LRU.
  3. Input Length Scaling: Measure TTFT across input lengths (32 to 1024 tokens) to confirm that the dynamic scheduling handles the "uneven workload" of the prefill stage effectively.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations
- Hardware dependence: Speedups are measured on specific GPU-CPU configurations and would likely degrade on systems with weaker CPU compute capabilities or higher PCIe latency.
- Model sparsity assumption: The system's effectiveness fundamentally relies on the sparsity of MoE activation patterns - if an MoE model were to route to a large number of experts per token, the CPU queue could become overwhelmed.
- Routing strategy dependency: The MRS caching policy assumes correlation between high routing scores and future activation, which may not hold for all routing strategies or datasets.

## Confidence
- **High Confidence (Mechanistic Validity):** The three proposed mechanisms - dynamic scheduling simulation, impact-driven prefetching, and MRS caching - are technically sound and implementable.
- **Medium Confidence (Empirical Claims):** The reported speedups (1.33× prefill, 1.70× decode) are specific to the tested configurations and evaluation setup, with exact hardware specifications and hyperparameter values not fully disclosed.
- **Low Confidence (Generalizability):** The paper does not address how HybriMoE would perform on extreme edge cases: very small MoE models, very large models (64+ experts), or models with non-standard routing strategies.

## Next Checks
1. **Ablation Study Replication:** Independently verify the contribution of each component by implementing baseline, scheduling-only, caching-only, and prefetching-only variants to confirm the claimed 1.70× decode improvement.
2. **Hardware Sensitivity Analysis:** Test HybriMoE across a spectrum of CPU compute capabilities and GPU-CPU PCIe bandwidth ratios to identify the hardware threshold below which hybrid scheduling provides no benefit.
3. **Routing Strategy Robustness:** Evaluate HybriMoE on MoE models with different routing strategies (top-1 vs top-2 vs top-4 experts) and on datasets with varying expert activation patterns to confirm the MRS caching policy maintains its advantage.