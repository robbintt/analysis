---
ver: rpa2
title: Towards Better Instruction Following Retrieval Models
arxiv_id: '2505.21439'
source_url: https://arxiv.org/abs/2505.21439
tags:
- retrieval
- instructions
- query
- instruction
- instruction-following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InF-IR, a large-scale training corpus designed
  to improve instruction-following retrieval models. It extends traditional query-passage
  pairs into instruction-query-passage triplets, generating over 38,000 high-quality
  samples with challenging negative examples by poisoning instructions and queries.
---

# Towards Better Instruction Following Retrieval Models

## Quick Facts
- arXiv ID: 2505.21439
- Source URL: https://arxiv.org/abs/2505.21439
- Reference count: 35
- Key outcome: InF-Embed achieves +9.0 p-MRR improvement for embedding-based models and +4.2 p-MRR for auto-regressive language models in instruction-following retrieval

## Executive Summary
This paper introduces InF-IR, a large-scale training corpus for instruction-following retrieval models, and InF-Embed, a model trained on this corpus. InF-IR extends traditional query-passage pairs into instruction-query-passage triplets with hard negative examples generated through instruction and query poisoning, validated by an advanced reasoning model (o3-mini). The authors demonstrate that their approach significantly improves instruction-following performance across multiple benchmarks for both embedding-based and auto-regressive language models.

## Method Summary
The method involves creating InF-IR by poisoning instructions and queries to generate hard negatives, then validating with o3-mini to ensure semantic plausibility while maintaining instructional misalignment. InF-Embed uses dual encoders with instruction-query concatenation (outperforming cross-attention) and multivariate contrastive loss. The model is trained on 38,759 positive and 77,518 hard negative triplets derived from MS MARCO, optimized via AdamW with marginal sampling. The approach achieves significant improvements in p-MRR across multiple benchmarks while maintaining computational efficiency through dual-encoder architecture.

## Key Results
- InF-Embed improves p-MRR by +9.0 for embedding-based models and +4.2 for auto-regressive language models
- Multivariate contrastive loss outperforms univariate variants across all benchmarks
- Quality-filtered data improves performance by ~2.5 p-MRR compared to unfiltered data
- Concatenation-based encoding outperforms cross-attention for decoder-only models

## Why This Works (Mechanism)

### Mechanism 1
Poisoning instructions and queries creates harder negatives that force models to learn fine-grained instruction-query-passage alignment. For each positive triplet (I+, Q+, P+), generate two negative variants: (I-, Q+, P1-) and (I+, Q-, P2-). The poisoned instruction/query maintains semantic similarity to positives but alters relevance criteria. A reasoning model (o3-mini) validates that negatives are semantically plausible yet instructionally misaligned, filtering ambiguous cases. Core assumption: Models trained with semantically-close but instructionally-divergent negatives learn to distinguish subtle instruction-driven relevance differences rather than relying on surface-level semantic matching.

### Mechanism 2
Joint encoding of instruction and query via concatenation (self-attention) outperforms separate cross-attention encoding for decoder-only models. Interaction I concatenates "<Instruction> <Query>" into a single sequence, leveraging decoder's causal attention to let query tokens condition on instruction context. Interaction II uses cross-attention requiring separate instruction and query encodings with learned attention weights. Core assumption: Decoder-only architectures' autoregressive pretraining makes them better at integrating concatenated instruction-query context than learning separate attention mechanisms from scratch.

### Mechanism 3
Multivariate contrastive loss (ℓmulti_P,I) outperforms univariate objectives by simultaneously contrasting passages and instructions within a single ranking task. Univariate objectives model separate conditionals P(P|I,Q), P(I|P,Q), P(IQ|P) with potentially competing gradient signals. Multivariate objectives aggregate marginal negatives from each variable in a single denominator: Σm exp(sim(pm,iqi,i)) + Σj exp(sim(pi,iqj,i)) + Σk exp(sim(pi,iqk,k)). Marginal sampling reduces complexity from O(|B||y|) to O(|B|·|y|). Core assumption: Jointly contrasting multiple input dimensions creates a harder ranking task that reduces inter-term competition and improves robustness.

## Foundational Learning

- Concept: Noise Contrastive Estimation (NCE) for retrieval
  - Why needed here: All contrastive objectives build on NCE principles—understanding the log-exp formulation, in-batch negatives, and conditional distribution approximation is prerequisite.
  - Quick check question: Given a minibatch B with positive pairs, explain why the denominator in NCE includes all in-batch combinations.

- Concept: Dual-encoder architecture for dense retrieval
  - Why needed here: InF-Embed uses separate encoders g(·;θP) and g(·;θI,Q); understanding embedding spaces, similarity functions, and encoder sharing is essential.
  - Quick check question: What is the computational advantage of dual-encoder over cross-encoder for retrieval at inference time?

- Concept: Instruction-following IR paradigm
  - Why needed here: The core task differs from standard retrieval—instructions specify retrieval dimensions (format, style, length, user context) beyond semantic relevance.
  - Quick check question: How does p-MRR differ from standard MRR in measuring instruction-following capability?

## Architecture Onboarding

- Component map: MS MARCO → gpt-4o-mini instruction generation → gpt-4o-mini negative poisoning → o3-mini quality filtering → InF-IR corpus → Dual encoders → Interaction layer (concatenation or cross-attention) → Multivariate contrastive loss → AdamW optimizer → InF-Embed
- Critical path: Quality filtering is the bottleneck—o3-mini validation removes ambiguous samples, directly impacting training data quality. Figure 6 shows filtered data outperforms larger unfiltered data.
- Design tradeoffs:
  - Concatenation vs. cross-attention: Concatenation is simpler and performs better for decoder-only models but requires full forward pass per instruction-query pair; cross-attention enables caching query embeddings but underperforms empirically.
  - Shared vs. separate encoders: Table 4 shows shared encoder with last-token pooling yields best results for decoder-only models.
  - Epochs: 2 epochs optimal; more epochs risk overfitting to synthetic data patterns.
- Failure signatures:
  - Low p-MRR with high nDCG: Model retrieves semantically relevant passages but ignores instructions—check negative sample quality (poisoning may be insufficiently challenging).
  - Training instability with multivariate loss: Batch size too small for marginal sampling diversity—increase batch size or reduce learning rate.
  - Performance degradation after quality filtering removed: Indicates synthetic data contains noisy samples that mislead training—ensure o3-mini filtering is applied.
- First 3 experiments:
  1. Reproduce Table 3 configuration sweep on FollowIR benchmark with a small backbone (ModernBERT-base) to validate loss function selection (ℓmulti_P,I should outperform ℓuni variants).
  2. Ablate quality filtering: train on InF-IR with and without o3-mini filtering using same backbone and epochs; expect ~1.5-2.0 p-MRR difference per Figure 6.
  3. Test generalization: train InF-Embed on MS MARCO-derived InF-IR, evaluate on Bright (reasoning-intensive) to assess out-of-domain instruction-following transfer.

## Open Questions the Paper Calls Out

### Open Question 1
Can InF-IR and InF-Embed be effectively extended to reasoning-intensive retrieval models, and what architectural modifications would be required? Basis: Conclusion states extension to reasoning-intensive retrieval models is an important future direction. Unresolved because the current framework focuses on standard instruction-following retrieval while reasoning-intensive retrieval requires handling multi-step inference chains. Evidence needed: Experiments applying InF-Embed to reasoning-intensive benchmarks with modified training objectives incorporating reasoning supervision.

### Open Question 2
How well does InF-IR generalize to highly specialized or domain-specific retrieval tasks beyond the general-domain MS MARCO seed data? Basis: Limitations section notes effectiveness in specialized domains may require additional investigation. Unresolved because paper evaluates only on general-domain benchmarks without testing specialized domains where instruction semantics may differ. Evidence needed: Systematic evaluation on domain-specific IR benchmarks with domain-adapted instruction generation pipelines.

### Open Question 3
How can the multivariate contrastive objectives scale efficiently to extremely large batch sizes or significantly larger model scales? Basis: Limitations section states scaling to large batch sizes and model scales remains nontrivial. Unresolved because while marginal sampling reduces complexity, multivariate objectives still face computational challenges at scale and paper doesn't explore optimizations beyond 7B parameter models. Evidence needed: Experiments with larger batch sizes and models (>7B parameters) comparing different negative sampling approximations.

### Open Question 4
Can attention-based instruction-query interaction match or exceed concatenation-based approaches when jointly optimized with contrastive objectives? Basis: Ablation study notes concatenation surpasses attention-based encoding, but separately encoding may offer more flexibility for future contrastive designs. Unresolved because attention-based interaction is more efficient for inference but underperformed concatenation; it's unclear if this is inherent or product of current training setup. Evidence needed: Head-to-head comparison with attention-based mechanisms specifically optimized for partial contrastive training.

## Limitations

- Synthetic data quality dependence: The approach relies entirely on o3-mini validation for synthetic data generation, creating a potential single point of failure
- Computational efficiency trade-offs: Concatenation-based encoding requires recomputing instruction-query embeddings for each instruction variation, with unquantified inference-time overhead
- Domain generalization: All experiments use MS MARCO-derived data; performance on specialized domains with different instruction types remains untested

## Confidence

- **High Confidence**: The multivariate contrastive loss improvement (+9.0 p-MRR for embedding models) is well-supported by controlled experiments across multiple benchmarks with consistent improvements
- **Medium Confidence**: The quality filtering mechanism's contribution (+2.5 p-MRR improvement in Figure 6) is demonstrated, but the specific criteria used by o3-mini and potential for false positives/negatives are not fully characterized
- **Low Confidence**: The generalization of concatenation encoding superiority to other decoder-only architectures and scalability to much larger instruction spaces are not empirically validated; computational overhead trade-offs are insufficiently characterized

## Next Checks

1. **Domain Transfer Experiment**: Train InF-Embed on MS MARCO-derived InF-IR, then evaluate on a medical or legal instruction-following benchmark to assess cross-domain generalization of the synthetic data approach.

2. **Computational Overhead Measurement**: Implement both concatenation and cross-attention variants, measure inference latency and memory usage for scenarios with varying numbers of instruction variations per query (1, 5, 20), and quantify the trade-off between accuracy and efficiency.

3. **Alternative Reasoning Model Validation**: Replace o3-mini with GPT-4 or Claude for quality filtering, retrain InF-Embed on the filtered data, and compare performance to assess the sensitivity of the approach to the choice of reasoning model and filtering criteria.