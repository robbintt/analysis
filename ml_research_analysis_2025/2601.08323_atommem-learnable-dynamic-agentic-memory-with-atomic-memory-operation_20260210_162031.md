---
ver: rpa2
title: 'AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation'
arxiv_id: '2601.08323'
source_url: https://arxiv.org/abs/2601.08323
tags:
- memory
- agent
- wang
- zhang
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AtomMem, a dynamic memory framework that
  reframes memory management as a sequential decision-making problem. Instead of using
  fixed workflows, AtomMem decomposes memory operations into atomic CRUD actions,
  enabling the agent to learn when and how to create, read, update, or delete memories
  based on task demands.
---

# AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation

## Quick Facts
- arXiv ID: 2601.08323
- Source URL: https://arxiv.org/abs/2601.08323
- Authors: Yupeng Huo; Yaxi Lu; Zhong Zhang; Haotian Chen; Yankai Lin
- Reference count: 17
- One-line primary result: AtomMem-8B outperforms static-workflow baselines by 2-5 percentage points on long-context multi-hop QA tasks

## Executive Summary
This paper introduces AtomMem, a dynamic memory framework that reframes memory management as a sequential decision-making problem. Instead of using fixed workflows, AtomMem decomposes memory operations into atomic CRUD actions, enabling the agent to learn when and how to create, read, update, or delete memories based on task demands. The framework is trained end-to-end using reinforcement learning, allowing adaptive, task-aligned memory policies to emerge. Experiments on long-context multi-hop QA tasks show that AtomMem-8B outperforms static-workflow baselines by 2-5 percentage points and scales robustly to longer contexts.

## Method Summary
AtomMem combines supervised fine-tuning (SFT) with reinforcement learning to train an agent that learns adaptive memory strategies. The framework uses a Qwen3-8B backbone to process documents chunked into 4K tokens, maintaining both a scratchpad (always retrieved) and a vector database (selectively retrieved via top-k semantic similarity). The agent emits atomic CRUD operations as discrete decisions, with the policy refined using Group Relative Policy Optimization (GRPO) based on task-level rewards. Training proceeds in two stages: SFT on 4K prompt-completion pairs for 3 epochs, followed by RL with on-policy rollouts and binary EM rewards.

## Key Results
- AtomMem-8B outperforms static-workflow baselines by 2-5 percentage points on long-context multi-hop QA tasks
- Removing either the scratchpad or external memory storage leads to 5-10pp performance drop; removing both causes >40pp drop
- RL training induces systematic changes: Read usage decreases sharply while Create, Update, and Delete actions increase substantially

## Why This Works (Mechanism)

### Mechanism 1: Atomic Action Decomposition
Decomposing memory workflows into CRUD primitives enables the agent to learn task-adaptive memory strategies rather than following fixed pipelines. The framework exposes four atomic operations—Create, Read, Update, Delete—each corresponding to a discrete state transition over the memory store. At each decision step, the policy emits a sequence of these operations conditioned on the current observation, transforming memory management into a learnable decision process. Core assumption: The optimal memory strategy varies by task context and cannot be captured by a single static workflow.

### Mechanism 2: Task-Level RL Policy Optimization
Reinforcement learning with task-level rewards enables the agent to discover efficient, task-aligned memory strategies that static workflows cannot capture. The policy is refined using Group Relative Policy Optimization (GRPO). Each trajectory receives a terminal reward based on task success (exact match), and advantages are computed relative to repeated executions of the same task. This reward signal propagates through all tokens, including memory operation tokens, enabling joint optimization of memory usage and task performance. Core assumption: Task success provides sufficient signal to learn effective memory policies; intermediate rewards are not necessary.

### Mechanism 3: Dual Memory with Asymmetric Retrieval
A scratchpad (always retrieved) combined with a queryable vector database (selectively retrieved) provides complementary memory roles that improve robustness and performance ceiling. The scratchpad captures global task state and is mandatorily retrieved every step. The vector database stores fine-grained entries accessed via Read operations with semantic similarity queries. This asymmetry allows the agent to maintain persistent context while selectively accessing detailed information. Core assumption: Different information types require different retrieval patterns; neither component alone is sufficient.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: AtomMem formulates memory management as a POMDP where the agent must act under incomplete observability of both environment and memory states. Understanding POMDPs clarifies why explicit Read operations are needed to access memory.
  - Quick check question: Can you explain why memory access is modeled as partial observability rather than full state access?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL training uses GRPO, a variant of PPO that computes advantages relative to a group of trajectories for the same task. This normalizes performance across task difficulty variations.
  - Quick check question: How does GRPO differ from standard PPO in its advantage computation?

- Concept: **Vector Database Retrieval**
  - Why needed here: The Read operation retrieves entries via semantic similarity using embeddings. Understanding approximate nearest neighbor search helps diagnose retrieval failures.
  - Quick check question: What embedding model is used, and how does retrieval granularity affect performance?

## Architecture Onboarding

- Component map: Environment Interface -> LLM Backbone -> Scratchpad + Vector Database -> Training Pipeline
- Critical path:
  1. SFT initializes format-following and basic memory patterns
  2. RL refines policy via task success signal; agent learns when to Create/Read/Update/Delete
  3. At inference, the agent processes chunks sequentially, emitting memory operations based on learned policy

- Design tradeoffs:
  - Chunk size (2048 vs 4096 vs 8192): Paper shows robustness across sizes, but larger chunks reduce decision frequency
  - Retrieve number (k=3 vs 6 vs 12): k=6 optimal for 2-4 hop tasks; lower k under-retrieves, higher k adds noise
  - SFT data scale: Only 4k samples (~300 trajectories) used; lightweight initialization but may limit generalization

- Failure signatures:
  - Over-reliance on Read: Early training behavior; redundant retrievals without memory maintenance
  - Catastrophic drop without both memory components: Removing scratchpad or storage alone causes 5-10pp drop; removing both causes 40+pp drop
  - Poor scaling without RL: SFT-only model underperforms by ~10pp vs RL-trained model

- First 3 experiments:
  1. Ablate memory operations: Disable Update or Delete individually to measure contribution (Table 2 shows Update critical, Delete marginal)
  2. Single-component training: Train scratchpad-only and storage-only variants to verify synergy (Figure 4 shows neither matches full architecture)
  3. Hyperparameter sweep: Vary chunk size and retrieve number to find optimal settings (Table 3 shows k=6, chunk size robust)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of RL training be mitigated to support scaling to larger backbone models?
- Basis in paper: The Limitation section notes that training takes 2–3 days on an 8-GPU cluster, identifying this as a potential bottleneck for longer-horizon tasks.
- Why unresolved: The paper relies on standard on-policy RL, which is resource-intensive, leaving the optimization of the training pipeline as an open challenge.
- What evidence would resolve it: Demonstration of efficient training techniques (e.g., off-policy learning) that significantly reduce time-to-convergence without performance loss.

### Open Question 2
- Question: Can the task-aligned memory policies learned for QA transfer effectively to interactive, non-QA environments?
- Basis in paper: Experiments are restricted to long-context QA; the "task-aligned" nature of the policy suggests it might overfit to specific reasoning patterns.
- Why unresolved: It is unknown if the model would adapt its CRUD strategy for tasks like web navigation where action frequencies (e.g., Read vs. Update) might differ.
- What evidence would resolve it: Cross-domain transfer experiments showing stable performance on non-QA benchmarks without retraining the memory policy from scratch.

### Open Question 3
- Question: In what specific contexts does the atomic Delete operation provide significant utility?
- Basis in paper: Ablation results showed that removing Delete had only a marginal impact, suggesting it is less critical for the current static document benchmarks.
- Why unresolved: The utility of explicit deletion likely depends on high data volatility or noise, which was limited in the evaluated datasets.
- What evidence would resolve it: Testing on dynamic environments where information becomes obsolete or incorrect during the task execution.

## Limitations

- Limited ablation of atomic action space: The paper demonstrates CRUD operations work for tested tasks but doesn't explore whether this decomposition is sufficient for more complex reasoning patterns.
- Sparse evaluation beyond multi-hop QA: All experiments focus on a single task type, leaving uncertainty about generalization to other memory-intensive tasks.
- Embedding and retrieval configuration opacity: The paper doesn't specify the embedding model configuration, similarity metrics, or whether any metadata filtering is used in the vector database.

## Confidence

- **High Confidence**: The core claim that RL-trained atomic memory policies outperform static workflows (5pp improvement) is well-supported by ablation studies and hyperparameter analysis.
- **Medium Confidence**: The claim that dual memory (scratchpad + vector database) is necessary for optimal performance is supported by ablation studies, but the specific architecture choices may be dataset-dependent.
- **Low Confidence**: Claims about scalability to arbitrarily long contexts are based on synthetic document length scaling rather than real-world long-context scenarios.

## Next Checks

1. **Generalization test**: Apply AtomMem to a non-QA task requiring long-term memory (e.g., multi-step planning or document summarization) to verify the atomic operation framework generalizes beyond multi-hop reasoning.

2. **Memory operation diversity**: Design tasks that require memory operations beyond basic CRUD (e.g., hierarchical memory organization, temporal reasoning) to test whether the atomic action space is sufficient or needs extension.

3. **Robustness to noise**: Test AtomMem's performance when the vector database contains noisy or irrelevant entries, and measure how sensitive the learned policy is to retrieval quality versus the scratchpad's role in maintaining global state.