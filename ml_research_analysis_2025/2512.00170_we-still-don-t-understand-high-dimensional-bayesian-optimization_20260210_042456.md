---
ver: rpa2
title: We Still Don't Understand High-Dimensional Bayesian Optimization
arxiv_id: '2512.00170'
source_url: https://arxiv.org/abs/2512.00170
tags:
- linear
- optimization
- kernel
- bayesian
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates high-dimensional Bayesian optimization
  (BO), demonstrating that a simple linear Gaussian process (GP) model with a geometric
  transformation can match or exceed state-of-the-art performance across 60-6000 dimensional
  tasks. The key insight is that linear models suffer from boundary-seeking behavior
  in standard BO, which the authors address by projecting inputs onto a unit hypersphere
  using inverse stereographic projection.
---

# We Still Don't Understand High-Dimensional Bayesian Optimization

## Quick Facts
- arXiv ID: 2512.00170
- Source URL: https://arxiv.org/abs/2512.00170
- Reference count: 40
- A simple linear GP model with inverse stereographic projection achieves state-of-the-art performance in 60-6000 dimensional Bayesian optimization

## Executive Summary
This paper investigates high-dimensional Bayesian optimization (BO) by demonstrating that a simple linear Gaussian process model, when combined with an inverse stereographic projection onto a unit hypersphere, can match or exceed the performance of state-of-the-art methods across 60-6000 dimensional tasks. The key insight is that standard linear models in BO suffer from boundary-seeking behavior due to monotonic growth of posterior statistics near the search space edges. By projecting inputs onto a hypersphere, this boundary pathology is eliminated while preserving computational efficiency. The resulting spherical linear kernel achieves competitive performance on classic benchmarks with significant computational advantages—linear time complexity in data size and exact Thompson sampling—compared to the cubic complexity of standard RBF kernels.

## Method Summary
The authors propose a simple linear Gaussian process model combined with inverse stereographic projection as a geometric transformation. Standard linear kernels suffer from boundary-seeking behavior in high-dimensional spaces because posterior mean and variance monotonically increase toward search space edges. The inverse stereographic projection maps inputs from a high-dimensional hypercube to a unit hypersphere, preventing this pathological behavior. The projection is defined as x = 2z/(1 + ||z||²) where z is the original input and x is the projected point. This transformation is nearly identity for most points (due to the thin shell phenomenon) but crucially removes the singularity at the boundary. The spherical linear kernel then operates on these projected inputs, maintaining linear time complexity O(N) while avoiding the cubic complexity O(N³) of standard RBF kernels.

## Key Results
- Linear models with inverse stereographic projection achieve competitive performance on classic high-dimensional benchmarks (Shekel, Hartmann, Ackley functions) across 60-6000 dimensions
- The method significantly outperforms standard BO methods including REMBO, HeSBO, and TuRBO on most test functions
- On molecular optimization tasks using GuacaMol benchmarks, the approach achieves state-of-the-art results, outperforming complex methods like VAE-based approaches
- Computational complexity is O(N) versus O(N³) for standard RBF kernels, enabling exact Thompson sampling instead of approximate methods

## Why This Works (Mechanism)
The inverse stereographic projection prevents boundary-seeking behavior by mapping the hypercube search space to a hypersphere where all points are equidistant from boundaries. Standard linear kernels in high dimensions exhibit monotonic growth of posterior statistics toward space edges, driving acquisitions to corners. The spherical projection eliminates this pathology while preserving local structure for most points. This geometric transformation is particularly effective because high-dimensional spaces concentrate volume near boundaries (thin shell phenomenon), yet the projection creates a uniform geometry where no single region dominates. The method succeeds not through increased model expressiveness but through proper regularization of the search space geometry.

## Foundational Learning
- High-dimensional geometry: Volume concentrates near boundaries of hypercubes in high dimensions
  - Why needed: Explains why standard BO methods fail in high dimensions due to boundary-seeking behavior
  - Quick check: Verify that 99% of hypercube volume lies in a thin shell near the boundary for d > 10

- Inverse stereographic projection: Maps points from Euclidean space to hypersphere via x = 2z/(1 + ||z||²)
  - Why needed: The core transformation that eliminates boundary pathologies while preserving local structure
  - Quick check: Confirm the mapping is nearly identity for typical high-dimensional points

- Bayesian optimization acquisition functions: Balance exploration vs exploitation using posterior statistics
  - Why needed: Understanding how posterior mean/variance growth drives boundary-seeking behavior
  - Quick check: Plot acquisition function values near boundaries vs center for linear kernel

- Gaussian process kernel properties: Different kernels induce different function space geometries
  - Why needed: Explains why linear kernels fail without transformation while RBF kernels succeed
  - Quick check: Compare posterior mean/variance surfaces for linear vs RBF kernels near boundaries

## Architecture Onboarding

Component Map:
Data -> Inverse Stereographic Projection -> Linear GP Kernel -> Posterior Statistics -> Acquisition Function -> Next Point Selection

Critical Path:
The critical path flows from input data through the inverse stereographic projection to the linear GP kernel, where posterior mean and variance are computed. These statistics feed into the acquisition function (Thompson sampling in this case), which determines the next evaluation point. The spherical transformation is the critical innovation that prevents the monotonic growth of posterior statistics that drives standard linear models to boundaries.

Design Tradeoffs:
- Linear vs RBF kernels: Linear kernels offer O(N) complexity but suffer from boundary-seeking without projection; RBF kernels have O(N³) complexity but handle boundaries naturally
- Projection necessity: The spherical mapping adds computational overhead but is essential for preventing pathological behavior
- Exact vs approximate inference: Linear models enable exact Thompson sampling, avoiding the approximation errors of elliptical slice sampling used with RBF kernels

Failure Signatures:
- Boundary-seeking behavior: If acquisitions consistently cluster near search space edges, the projection may be improperly implemented or the kernel hyperparameters poorly tuned
- Poor generalization: If random test points show high error despite good acquisition performance, the model may be overfitting the acquisition pattern rather than learning the true function
- Computational inefficiency: If runtime scales super-linearly with data size, the implementation may not be exploiting the linear kernel's computational advantages

First Experiments:
1. Verify the inverse stereographic projection maps hypercube to hypersphere by checking that all projected points lie on the unit sphere
2. Compare posterior mean/variance surfaces for linear kernel with and without spherical projection near boundaries
3. Test the method on a simple 2D function where boundary-seeking behavior can be visually confirmed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What surrogate model properties actually correlate with success in high-dimensional Bayesian optimization (HDBO) beyond standard notions of expressiveness or generalization?
- Basis in paper: The authors state that "fully characterizing which properties actually matter for HDBO success—beyond standard notions of expressiveness or generalization—remains an important open question."
- Why unresolved: Linear models lack the representational capacity of RBF kernels yet match or exceed their performance, invalidating the assumption that model complexity is the primary driver of success.
- What evidence would resolve it: A theoretical framework identifying specific geometric or statistical properties of the posterior that enable effective optimization independent of global model fidelity.

### Open Question 2
- Question: Why do spherical linear models accurately predict future adaptive acquisitions despite performing poorly on random test data?
- Basis in paper: In Section 5, the authors explicitly ask: "Why are spherical linear models able to accurately predict future (adaptive) acquisitions but not random test data?"
- Why unresolved: This discrepancy suggests that standard generalization metrics (like RMSE on held-out data) are decoupled from the specific requirements of the sequential acquisition process.
- What evidence would resolve it: Analysis demonstrating that acquisition functions implicitly select points lying in regions where the objective function is locally linear.

### Open Question 3
- Question: Why does the spherical linear model significantly outperform Vanilla BO on molecular optimization tasks in latent spaces?
- Basis in paper: In Section E.1, regarding the strong performance on GuacaMol benchmarks, the authors note: "We leave more thorough analysis of this strong performance to further work, but believe it to be a fruitful direction."
- Why unresolved: Conventional intuition suggests RBF kernels should handle the potential non-linearity of VAE latent spaces better than linear models.
- What evidence would resolve it: Comparative studies analyzing the local geometry and "effective dimensionality" of VAE latent spaces versus standard benchmark functions.

### Open Question 4
- Question: How does the spherical mapping provide dramatic optimization improvements when it is mathematically minor for most high-dimensional points?
- Basis in paper: The authors highlight a tension where the spherical mapping prevents boundary pathologies but "preserves local structure except at the boundary," noting the projection is a "relatively minor transformation" for typical points due to the thin shell phenomenon.
- Why unresolved: Standard linear kernels fail catastrophically (corner-seeking), while the modified kernel succeeds, yet theoretically the two models behave nearly identically for the bulk of the data.
- What evidence would resolve it: Rigorous quantification of how removing the singularity at the search space boundary alters the acquisition function landscape despite the mapping being nearly identity elsewhere.

## Limitations
- The inverse stereographic projection may have limited effectiveness for problems with highly anisotropic or non-convex geometries
- The linear kernel assumption may become limiting when the true underlying function exhibits strong non-stationarity
- Performance gains may be specific to the benchmark functions tested and may not generalize to all high-dimensional optimization problems

## Confidence
- Linear models can match RBF performance in high dimensions with proper geometric transformation: Medium
- Inverse stereographic projection is the optimal solution for preventing boundary-seeking behavior: Medium
- Computational complexity claims (O(N) vs O(N³)): High

## Next Checks
1. Test the inverse stereographic projection approach on real-world high-dimensional optimization problems from domains like materials science or robotics to verify robustness beyond synthetic benchmarks
2. Compare against alternative high-dimensional BO methods that use adaptive embeddings or neural network-based representations to isolate the specific contribution of the geometric transformation
3. Analyze the method's performance when the true underlying function exhibits strong anisotropy or non-stationarity, as the linear kernel assumption may become limiting in these regimes