---
ver: rpa2
title: Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds
arxiv_id: '2510.27391'
source_url: https://arxiv.org/abs/2510.27391
tags:
- visual
- hyperbolic
- hierarchical
- features
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds

## Quick Facts
- arXiv ID: 2510.27391
- Source URL: https://arxiv.org/abs/2510.27391
- Reference count: 40
- Primary result: Proposed method achieves significant improvements in Hierarchical Consistent Accuracy (HCA) over baselines like MaPLe and PromptSRC for taxonomic open set classification tasks.

## Executive Summary
This paper addresses the challenge of aligning visual and textual modalities for hierarchical classification tasks. The key innovation is "Modality Alignment across Trees" (MAT), which uses semantic-aware visual feature extraction, alignment on heterogeneous hyperbolic manifolds, and entailment cone constraints. By leveraging intermediate layers of a Vision Transformer and cross-attention mechanisms, the method constructs hierarchical visual features that align with textual taxonomies. The approach is evaluated on datasets like CIFAR-100, SUN, ImageNet, and Rare Species, demonstrating superior performance in both leaf accuracy and hierarchical consistency.

## Method Summary
The MAT framework constructs hierarchical visual features by extracting class tokens from intermediate and final layers of a Vision Transformer, then applying cross-attention with textual features at different hierarchy levels. Visual and textual features are embedded into separate hyperbolic Lorentz manifolds with learned curvatures, and an intermediate manifold is derived to minimize geometric distortion during alignment. Entailment cone constraints are applied to ensure hierarchical consistency, forcing child concepts to lie within the cones of their parent concepts. The method is trained using a combination of entailment and projection losses, with curvature gradients computed via the Implicit Function Theorem.

## Key Results
- MAT achieves state-of-the-art Hierarchical Consistent Accuracy (HCA) on taxonomic open set classification tasks.
- The method demonstrates significant improvements in Mean Treecut Accuracy (MTA) compared to baselines like MaPLe and PromptSRC.
- Ablation studies confirm the effectiveness of the semantic-aware visual feature extraction and heterogeneous manifold alignment components.

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Aware Hierarchical Visual Feature Extraction
The method extracts visual features from intermediate and final layers of a Vision Transformer, re-weighting them with textual queries via cross-attention. This creates a hierarchy that aligns with textual taxonomies, leveraging the observation that intermediate layers encode coarse semantics while final layers encode fine-grained details. The core assumption is that the pre-trained ViT has a stratified semantic structure compatible with the target taxonomy.

### Mechanism 2: Alignment via Intermediate Heterogeneous Manifold
Visual and textual modalities are embedded into hyperbolic manifolds with distinct, learned curvatures. An intermediate manifold with curvature c3 is derived to minimize geometric distortion during alignment, avoiding the compromise of forcing a single curvature. The method models data as wrapped normal distributions and solves for the unique c3 that minimizes the combined distance between parent manifolds.

### Mechanism 3: Entailment Cone Constraints for Hierarchy Preservation
Hyperbolic entailment cones are applied to feature embeddings to enforce strict hierarchical parent-child relationships. This ensures that predicting a fine-grained label implies the validity of all coarser ancestors, improving Hierarchical Consistent Accuracy. The core assumption is that the semantic relationship strictly follows an "is-a" or entailment structure.

## Foundational Learning

- **Concept: Lorentz Model of Hyperbolic Geometry**
  - Why needed here: The paper uses the Lorentz model for numerical stability and efficient curvature computation. Understanding exponential/logarithmic maps and Lorentzian distance is required to implement the projection and loss functions.
  - Quick check question: Can you explain why the Lorentz model might be preferred over the Poincaré ball for gradient-based optimization of curvature?

- **Concept: Implicit Function Theorem (IFT) in Optimization**
  - Why needed here: The optimal curvature c3 is found via a non-differentiable search algorithm. IFT is used to derive gradients ∂c3/∂c1 to backpropagate errors through the manifold search process.
  - Quick check question: How does the Implicit Function Theorem allow us to compute gradients through an iterative optimization loop without unrolling the loop?

- **Concept: Wrapped Normal Distributions on Manifolds**
  - Why needed here: The theoretical justification for the manifold distance relies on modeling data as wrapped normal distributions.
  - Quick check question: How is a wrapped normal distribution constructed on a hyperbolic manifold compared to a standard Gaussian in Euclidean space?

## Architecture Onboarding

- **Component map:** Vision Transformer (ViT) -> Cross-Attention module (Text-Queried) -> Exponential maps (Projection to Hyperbolic) -> Lorentz manifolds (L_c1, L_c2) -> Golden Section Search (c3) -> Entailment Cone Constraints -> Loss Aggregator

- **Critical path:** The gradient flow depends on the implicit differentiation step for curvature c3. If the Golden Section Search is inaccurate or the implicit gradient computation is incorrect, the curvatures will not update, and the manifold alignment will fail.

- **Design tradeoffs:**
  - Using learnable curvatures allows better data fit but introduces optimization instability compared to fixed curvatures.
  - Constructing visual trees increases computational cost but is necessary to fix the asymmetric alignment problem.

- **Failure signatures:**
  - Loss Instability: If curvature values become negative or explode, resulting in NaN. Clamp curvatures to a small positive epsilon.
  - Low HCA: If Leaf Accuracy is high but Hierarchical Consistent Accuracy is low, the entailment cone constraints are likely too weak or the cone aperture is miscalculated.

- **First 3 experiments:**
  1. **Sanity Check (Euclidean Baseline):** Run the "Ours-Euc" variant to verify semantic-aware visual extraction alone provides baseline improvement.
  2. **Curvature Verification:** Visualize learned curvatures c1 (text) and c2 (image) and derived c3. Check if c3 consistently lies between c1 and c2.
  3. **Hierarchical Consistency Test:** Evaluate specifically on HCA metric. If near zero, check entailment loss implementation and angle calculation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the derived intermediate manifold curvature (c3*) to the approximation errors introduced by the Taylor expansion in the KL divergence derivation?
- Basis in paper: Theorem 1 relies on a Taylor expansion to approximate the KL divergence distance, theoretically constraining the domain where the approximation is valid.
- Why unresolved: The proof guarantees a unique minimizer for the approximated function, but it's unclear if this solution remains optimal or stable for the exact KL divergence.
- What evidence would resolve it: A sensitivity analysis comparing the curvature optimized via the approximation against numerical integration of the exact KL divergence.

### Open Question 2
- Question: Can the "Alignment across Trees" framework effectively generalize to general vision-language tasks that lack explicit hierarchical label structures?
- Basis in paper: The method is exclusively evaluated on Taxonomic Open-Set (TOS) classification, and the cross-attention mechanism explicitly relies on hierarchical textual features as queries.
- Why unresolved: The dependency on pre-defined semantic hierarchies suggests the architecture may be over-fitted to taxonomic tasks and unable to align flat or unstructured semantics common in general VLM benchmarks.
- What evidence would resolve it: Benchmarking the method on standard non-hierarchical datasets (e.g., COCO, Flickr30k) to evaluate if the tree-based alignment degrades or improves performance.

### Open Question 3
- Question: Is the selection of intermediate Transformer layers (e.g., 4th and 7th) a robust heuristic across different architectures, or does it require re-tuning for varying model depths?
- Basis in paper: The implementation details specify fixed layer indices, while the motivation is general by the property of intermediate layers encoding coarse semantics.
- Why unresolved: The optimal "cut" for coarse-to-fine semantics likely shifts depending on the depth of the Vision Transformer, and fixed indices may miss the optimal hierarchical features in deeper models.
- What evidence would resolve it: A cross-architecture ablation study analyzing performance when the layer indices are scaled or learned adaptively.

## Limitations
- The method's performance is contingent on the pre-trained ViT possessing meaningful semantic stratification across layers; shallow transformers may fail to create a valid coarse-to-fine hierarchy.
- The entailment cone constraints are designed for strict tree structures and may degrade on datasets with cross-cutting categories, multi-label instances, or cyclical relationships.
- The theoretical proof for the intermediate manifold curvature is based on idealized wrapped normal distributions, and real-world features may deviate significantly.

## Confidence
- **High Confidence:** The hierarchical visual feature extraction mechanism is a clear architectural contribution with strong empirical support.
- **Medium Confidence:** The heterogeneous manifold alignment theory is mathematically sound, but its practical benefit over simpler symmetric approaches is not fully isolated.
- **Low Confidence:** The claim that the method solves the "asymmetric alignment" problem is not directly tested due to missing comparisons to a visual-tree version of MaPLe.

## Next Checks
1. **Ablation on Intermediate Manifold Necessity:** Implement a "Symmetric Manifold" variant that embeds both modalities into a single, shared curvature space. Compare its HCA performance to the full MAT model to isolate the contribution of the heterogeneous alignment.

2. **Stress Test on Backbone Depth:** Run the method on a shallower ViT (e.g., ViT-Small or ResNet) to test the robustness of the intermediate-layer feature extraction. If HCA drops dramatically, it validates that the method's success is contingent on the specific semantic structure of the CLIP backbone.

3. **Non-Tree Dataset Evaluation:** Apply the method to a multi-label dataset or a dataset with known cross-category instances. Measure if HCA collapses, which would confirm the method's limitation to strict tree hierarchies.