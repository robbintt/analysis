---
ver: rpa2
title: Universal Redundancies in Time Series Foundation Models
arxiv_id: '2602.01605'
source_url: https://arxiv.org/abs/2602.01605
tags:
- heads
- layers
- tsfms
- layer
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the redundancy of components within transformer-based
  time series foundation models (TSFMs). The authors find that middle layers of leading
  TSFMs are highly redundant, allowing for significant ablation of heads and MLPs
  without substantial performance loss.
---

# Universal Redundancies in Time Series Foundation Models

## Quick Facts
- **arXiv ID:** 2602.01605
- **Source URL:** https://arxiv.org/abs/2602.01605
- **Reference count:** 40
- **Primary result:** Middle layers of transformer-based TSFMs are highly redundant, enabling ablation of 28% of heads and 2 MLPs with only 6% performance degradation.

## Executive Summary
This paper investigates component redundancy in transformer-based time series foundation models (TSFMs) through systematic ablation experiments. The authors find that middle layers contribute minimally to final predictions and can be heavily pruned while maintaining performance. Using a kernel-based theoretical framework, they develop an intrinsic head-pruning strategy based on stable rank of query-key projection matrices that outperforms random ablation. The study reveals specific attention heads responsible for common TSFM failure modes like context parroting and seasonality bias, providing new insights into TSFM interpretability and compression.

## Method Summary
The authors conduct zero-ablation experiments on five pretrained TSFMs (TimesFM, Chronos, Chronos-Bolt, Toto, Moirai) using the GIFT-Eval benchmark. They systematically ablate attention heads and MLPs across layers, measuring performance degradation through MASE, CRPS, and Spearman distance. A novel intrinsic pruning strategy ranks heads by stable rank of their query-key projection matrices (sr(W_Q*W_K^T) = ||A||_F²/||A||_2²). The method identifies heads@1pp (minimum heads to keep for <1% MASE degradation) and examines sharp vs diffuse heads' roles in context parroting and seasonality capture.

## Key Results
- Middle layers contribute minimally to final predictions and can be heavily pruned—up to 28% of attention heads and several MLPs—with less than 6% performance drop
- Intrinsic head pruning strategy based on stable rank of query-key projection matrices outperforms random ablation
- Sharp attention heads drive context parroting and seasonality, while diffuse heads are highly redundant and can be pruned without affecting performance

## Why This Works (Mechanism)

### Mechanism 1: Middle Layer Redundancy
Middle layers in TSFMs perform redundant updates to the residual stream with minimal impact on final predictions. Early layers build representations and introduce initial predictions, middle layers perform similar redundant updates that increase entropy without adding discriminative information, and late layers consolidate by selecting among candidate forecasts. The residual stream architecture allows later layers to correct or ignore contributions from middle layers without breaking gradient flow.

### Mechanism 2: Stable Rank Predicts Head Prunability
The stable rank sr(W_Q W_K^T) provides an intrinsic, data-independent signal for identifying redundant attention heads. High stable rank equals spectral energy spread across many modes equals diffuse global smoothing; low stable rank equals concentrated energy in few modes equals sharp, task-specific attention patterns critical for forecasting. TSFM attention heads approximate kernel operators where low effective dimensionality correlates with specialized, non-redundant function.

### Mechanism 3: Sharp vs Diffuse Heads Drive Parroting
Low-entropy "sharp" attention heads drive context parroting and seasonality capture; high-entropy "diffuse" heads are largely redundant. Sharp heads implement narrow-bandwidth kernel comparisons matching context motifs to decoder queries, producing diagonal-band attention patterns; diffuse heads perform broad smoothing removable without affecting motif copying. Context parroting is primarily driven by cross-attention pattern matching rather than MLP computation.

## Foundational Learning

- **Concept: Residual Stream**
  - Why needed here: Essential for understanding how layer contributions accumulate and why middle-layer ablations don't destroy earlier computations
  - Quick check question: If you zero-ablate layer 5 in a 12-layer residual network, what happens to the representation learned by layer 4?

- **Concept: Stable Rank**
  - Why needed here: The paper's key pruning metric; distinguish from standard rank and understand its connection to effective dimensionality
  - Quick check question: For a matrix with one singular value of 1000 and 63 singular values of 1, is the stable rank closer to 1 or 64?

- **Concept: Kernel Regression View of Attention**
  - Why needed here: Connects spectral properties (singular values → bandwidth) to attention sharpness; motivates why low stable rank heads are critical
  - Quick check question: Does a narrow kernel bandwidth make attention more selective or more diffuse?

## Architecture Onboarding

- **Component map:** Context → Encoder → Cross-attention from decoder → Output (encoder-decoder); or Causal self-attention → Autoregressive patch prediction (decoder-only)
- **Critical path:**
  1. Identify architecture (encoder-decoder vs decoder-only) to gauge MLP sensitivity
  2. Load GIFT-Eval subset (100-500 samples) for rapid iteration
  3. Compute stable rank sr(W_Q @ W_K^T) for all heads offline
  4. Implement zero-ablation hooks for heads/MLPs
  5. Prune high-stable-rank heads from middle layers first
- **Design tradeoffs:**
  - Encoder-decoder vs decoder-only: Encoder-decoder tolerates MLP ablation; decoder-only requires MLPs for structure
  - Intrinsic vs extrinsic pruning: Stable rank is data-free but misses data-dependent patterns; attention-matrix SVD is more precise but requires forward passes
- **Failure signatures:**
  - Random head ablation: Seasonality destroyed, incoherent forecasts
  - Sharp head ablation: Structure collapses with single-head removal
  - Early/late layer ablation: >25% MASE increase
- **First 3 experiments:**
  1. Layer sweep: Ablate each layer individually (heads+MLP), compute Spearman distance on 100 samples. Identify middle layer candidates.
  2. Stable rank validation: Compute sr(W_Q @ W_K^T) for all heads; verify correlation between high stable rank and ablation tolerance per layer.
  3. Pruning comparison: On middle layers, ablate 30% heads via stable rank vs random. Compare MASE on full benchmark. Expect <6% vs >20% degradation.

## Open Questions the Paper Calls Out

- How does the choice of TSFM architecture (e.g., encoder-decoder vs. decoder-only) dictate the distribution of functional importance between attention heads and MLP layers?
- Is the "context parroting" behavior driven by sharp attention heads a necessary heuristic for zero-shot generalization, or can it be decoupled from forecasting accuracy?
- Do current pre-training objectives inherently lead to redundant middle layers, or is this a byproduct of over-parameterization?

## Limitations
- Stable rank metric's predictive power is limited by its data-independent nature, unable to capture query-key alignment from data
- heads@1pp metric aggregates across layers without revealing layer-specific sensitivity patterns
- Paper only examines five models, potentially missing architecture-specific redundancy patterns

## Confidence
- **High confidence:** Empirical observation that middle layers tolerate ablation better than early/late layers (supported by per-layer MASE degradation curves)
- **Medium confidence:** Stable rank as general pruning heuristic, as its data-free nature may miss important attention patterns
- **Low confidence:** Universal applicability of findings across all TSFM architectures, as only five models examined

## Next Checks
1. **Layer-wise heads@1pp validation:** Compute and report the exact heads@1pp threshold for each layer individually across all models to verify middle-layer redundancy claim with granular data.
2. **Data-dependent attention analysis:** Implement SVD-based head importance ranking using actual attention matrix activations and compare performance against stable rank-based pruning on same ablation experiments.
3. **Architecture-specific sensitivity mapping:** Design controlled experiments ablating MLPs in decoder-only vs encoder-decoder architectures to precisely map architectural factors making MLPs more or less ablateable.