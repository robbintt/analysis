---
ver: rpa2
title: Unifying Adversarial Robustness and Training Across Text Scoring Models
arxiv_id: '2602.00857'
source_url: https://arxiv.org/abs/2602.00857
tags:
- training
- adversarial
- reward
- robustness
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper unifies the study of adversarial robustness for text
  scoring models (retrievers, rerankers, reward models) under a common framework,
  framing attacks as score manipulation failures. It demonstrates that current adversarial
  training methods are narrow, often failing to generalize across attack types.
---

# Unifying Adversarial Robustness and Training Across Text Scoring Models

## Quick Facts
- **arXiv ID:** 2602.00857
- **Source URL:** https://arxiv.org/abs/2602.00857
- **Reference count:** 40
- **One-line primary result:** Combined adversarial training methods yield broader robustness and better RLHF alignment across retrievers, rerankers, and reward models.

## Executive Summary
This paper introduces a unified framework for adversarial robustness in text scoring models, treating retrievers, rerankers, and reward models under a common lens of score manipulation failures. The authors demonstrate that current adversarial training methods are narrow, often failing to generalize across attack types, and introduce new training approaches including rudimentary, HotFlip, PGD, content injection, and paraphrasing. Their key finding is that combining complementary training methods produces broader robustness and improved task effectiveness without sacrificing performance. For reward models, adversarially trained combined methods mitigate reward hacking during RLHF, producing policies that achieve higher judge preference scores while maintaining lower KL divergence from the reference model.

## Method Summary
The unified framework frames adversarial robustness through score manipulation failures, defining success structurally rather than semantically. The method implements five training approaches: Rudimentary (adding artifacts to irrelevant text), HotFlip (gradient-guided token swaps), PGD (continuous embedding perturbations), content injection (adding irrelevant sentences/queries), and paraphrasing. Combined training randomly samples one perturbation type per example per batch, using squared hinge losses for discrete methods and MSE for paraphrasing. Training strength is determined via dev loss sweeps, with medium strength as default. Beam search attacks (16 beams × 16 variants, max 512 steps) evaluate robustness across attack families on TREC-DL, BEIR, and RewardBench datasets.

## Key Results
- Combined adversarial training methods yield broader robustness than single-method approaches, requiring more edits on average for successful attacks and achieving lower attack success rates
- Adversarially trained reward models produce better-aligned policies during RLHF, achieving higher judge preference scores while maintaining lower KL divergence from reference models
- Single adversarial training methods often improve robustness to targeted attacks but can degrade robustness to other attack types, highlighting the importance of complementary training signals
- No method achieves perfect robustness for any attack, but combined approaches provide the most consistent protection across attack families

## Why This Works (Mechanism)

### Mechanism 1: Score Manipulation as Unified Failure Criterion
The framework defines adversarial robustness through structural ranking errors rather than semantic harm assessment. An attack succeeds when irrelevant or rejected text outscores relevant or chosen text, allowing attacks to be expressed as score-maximization problems regardless of model role. This content-agnostic framing avoids ambiguity in defining "harmful" outputs.

### Mechanism 2: Complementary Training Signals for Transfer Robustness
Different attack classes exploit distinct failure modes—PGD targets continuous embedding perturbations, HotFlip targets gradient-guided token swaps, content injection exploits insertion vulnerabilities. Training against multiple threats simultaneously creates overlapping robustness regions in the loss landscape, yielding broader protection than single-method training.

### Mechanism 3: Reward Model Robustness Reduces RLHF Reward Hacking
During RLHF, the policy model acts as an adaptive adversary optimizing against the reward model. A robust reward model resists exploitation, preventing the policy from discovering spurious high-reward outputs that don't align with true preferences. This manifests as slower reward inflation and better alignment, measured by higher judge preference scores and lower KL divergence.

## Foundational Learning

- **Text Scoring Models (Retrievers, Rerankers, Reward Models):** These models assign scalar scores to compare texts and can be treated as variants of the same problem. Understanding their shared structure is prerequisite to grasping why attacks and defenses transfer.
  - *Quick check:* Can you explain why a corpus-poisoning attack on a retriever and reward hacking during RLHF can both be framed as "adversary manipulates text to increase model score"?

- **Adversarial Training (PGD, HotFlip, Discrete Perturbations):** PGD uses continuous embedding perturbations via projected gradient descent, while HotFlip uses gradient-guided token swaps. Different methods exploit different vulnerabilities.
  - *Quick check:* Why might PGD training not protect against content injection attacks, even though both are "adversarial"?

- **RLHF and Reward Hacking:** In RLHF, the policy optimizes against the reward model, potentially discovering exploits. Reward hacking occurs when the policy achieves high rewards through spurious means rather than true alignment.
  - *Quick check:* In RLHF, what does it mean for KL divergence from the reference model to increase rapidly, and why is this undesirable?

## Architecture Onboarding

- **Component map:** Training Data (MSMARCO, HelpSteer3, Skywork) → Base Model (E5-BERT retriever / Qwen3-0.6B reranker / Llama-3.2-3B reward) → Adversarial Training Module (Rudimentary | HotFlip | PGD | Injection | Combined) → Robust Model → Evaluation (ASR, NDCG@10, preference accuracy, KL divergence)

- **Critical path:**
  1. Initialize base model and establish dev loss baseline
  2. Select training strength via dev loss minimization (sweep w for hinge losses, ε for PGD)
  3. Apply Combined training (Rudimentary + HotFlip + PGD + Injection) at medium strength as default
  4. Evaluate robustness transfer across attack families using Spearman correlation
  5. For reward models: run RLHF and measure judge preference + KL trajectory

- **Design tradeoffs:**
  - Single-method vs. Combined: Single methods may outperform Combined on specific attacks but Combined generalizes better
  - Training strength: Higher strength improves targeted robustness but can degrade non-targeted robustness
  - Paraphrasing: Excluded from Combined due to weak and inconsistent gains
  - Attack budget: Use 512 steps, 16 beams, 16 variants for meaningful comparison

- **Failure signatures:**
  - High ASR despite training → verify attack budget and loss scaling
  - Effectiveness degradation → reduce adversarial loss weight
  - Poor transfer robustness → combine complementary methods

- **First 3 experiments:**
  1. Baseline robustness audit: Run all attack types against base model with full budget
  2. Single-method ablation: Train separate models and measure robustness transfer
  3. Combined training validation: Train Combined model and compare to best single-method

## Open Questions the Paper Calls Out

- **Adapting to open-ended generative LLMs:** The framework requires adaptation for generative models without relying on curated safe/unsafe datasets, which is left for future work
- **Scaling to larger models:** Whether robustness and effectiveness benefits persist when scaling to models significantly larger than 8B parameters remains unexplored
- **Achieving near-zero ASR:** No method achieves perfect robustness; future work should study methods conferring broad and strong robustness

## Limitations

- **Generalizability scope:** Evidence primarily limited to English text and specific model architectures (E5-base, Qwen3-0.6B, Llama-3.2-3B)
- **Judge model validity:** Alignment claims rely on LLM judge models that may have their own biases and vulnerabilities
- **Combined training stability:** High-strength combined training can degrade robustness to non-targeted attacks for rerankers, but analysis is limited

## Confidence

- **High Confidence:** The core framework of framing adversarial robustness as score manipulation failures is well-supported by structural arguments and empirical validation
- **Medium Confidence:** RLHF downstream benefits are supported by empirical evidence but rely on assumptions about judge model validity
- **Low Confidence:** Generalizability claims to other domains are largely extrapolated from current experimental scope

## Next Checks

1. **Cross-lingual Robustness Transfer:** Apply combined adversarial training to multilingual retrievers and evaluate robustness transfer patterns across languages (English, Chinese, Arabic)
2. **Judge Model Bias Validation:** Conduct human evaluation studies comparing policies trained with base vs. adversarially trained reward models on RLHF tasks
3. **Scale and Architecture Sensitivity:** Test combined training on significantly larger models (Llama-3.1-70B, Qwen2.5-72B) and different architectures (BERT, RoBERTa, DeBERTa variants)