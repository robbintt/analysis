---
ver: rpa2
title: 'Aligning LLM agents with human learning and adjustment behavior: a dual agent
  approach'
arxiv_id: '2511.00993'
source_url: https://arxiv.org/abs/2511.00993
tags:
- agent
- travel
- agents
- behavior
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of modeling travelers\u2019\
  \ complex learning and adjustment behavior in transportation systems. It introduces\
  \ a dual-LLM agent framework: traveler agents simulate daily decisions using a memory\
  \ system and a learnable persona, while a calibration agent automatically optimizes\
  \ these personas against real-world data."
---

# Aligning LLM agents with human learning and adjustment behavior: a dual agent approach

## Quick Facts
- **arXiv ID:** 2511.00993
- **Source URL:** https://arxiv.org/abs/2511.00993
- **Reference count:** 10
- **Primary result:** Dual-LLM agent framework significantly improves transportation behavior modeling, with 16.7% higher F1-score and 20.7% lower MSE than existing baselines

## Executive Summary
This paper addresses the challenge of modeling travelers' complex learning and adjustment behavior in transportation systems. The authors introduce a dual-LLM agent framework where traveler agents simulate daily decisions using a memory system and a learnable persona, while a calibration agent automatically optimizes these personas against real-world data. Using a day-to-day route choice dataset, the method significantly outperforms existing baselines, improving individual behavior prediction F1-score by 16.7% and aggregate route flow simulation accuracy by 20.7% in mean squared error. The approach demonstrates strong alignment with both behavioral outcomes and underlying decision-making tendencies, enabling more realistic and adaptive traveler behavior simulation for transportation modeling.

## Method Summary
The framework employs a dual-agent architecture where a Traveler Agent simulates individual decision-making using an LLM guided by memory systems (short-term and long-term retrieval) and a learnable persona, while a Calibration Agent optimizes the persona through pseudo-gradient descent in semantic space. The Calibration Agent analyzes discrepancies between simulated and actual behavior, generates textual critiques, and proposes persona updates that are then smoothed with long-term baselines to prevent overfitting. This rolling window alignment approach captures both individual behavioral patterns and system-level traffic dynamics.

## Key Results
- Individual behavior prediction F1-score improved by 16.7% compared to existing baselines
- Aggregate route flow simulation accuracy achieved 20.7% lower mean squared error
- Framework successfully captured evolution of traveler strategies from "naive and highly reactive" to "sophisticated" over time
- Outperformed theory-driven models by capturing specific bounded rationality biases like "high threshold for switching"

## Why This Works (Mechanism)

### Mechanism 1: Explicit Persona Learning for Bounded Rationality
The framework moves beyond recalling past events to modeling how an agent interprets those events. By optimizing a textual persona description (e.g., "avoid overreacting to isolated events"), the system encodes decision heuristics directly into the prompt, guiding the LLM's reasoning process. This captures complex, bounded rationality behaviors like inertia and risk aversion better than implicit memory or fixed theoretical models.

### Mechanism 2: Pseudo-Gradient Descent in Semantic Space
The Calibration Agent analyzes the error between the Traveler Agent's simulation and ground truth, generating a textual critique explaining the error (e.g., "Agent switches routes too aggressively after a single delay"). This text is used to rewrite the persona, allowing the LLM to act as an optimizer for another LLM by navigating the non-differentiable search space of natural language prompts.

### Mechanism 3: Rolling Window Alignment with Smoothing
The calibration process optimizes the persona based on a short window of recent behavior but merges the result with a long-term baseline. This acts as a regularizer, ensuring the agent doesn't change its entire "personality" based on a single anomalous trip, while still allowing adaptation to non-stationary behavior.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - *Why needed here:* The Traveler Agent relies on retrieving specific past experiences to ground its decisions. Understanding how LLMs context window and retrieval work is essential to grasping why memory is separated from the persona.
  - *Quick check question:* How does the system distinguish between what happened to the traveler (Memory) and how the traveler feels about it (Persona)?

- **Concept: Bounded Rationality**
  - *Why needed here:* The paper contrasts its approach with "rational" utility maximizers. You must understand that humans have limited information, cognitive inertia, and satisficing behaviors to appreciate why the "Persona" module is necessary.
  - *Quick check question:* Why did the "Bounded-LLM" baseline (which used fixed bounded rationality triggers) underperform compared to the learnable persona approach?

- **Concept: Agent-Based Modeling (ABM)**
  - *Why needed here:* The core task is simulating system-level dynamics (traffic flow) emerging from individual interactions.
  - *Quick check question:* Does the paper evaluate success solely on individual prediction accuracy, or does it also measure aggregate system outcomes like route flow MSE?

## Architecture Onboarding

- **Component map:**
  - Environment State + Past Memory -> Traveler Agent (LLM + Persona + Memory Heuristics) -> Route Choice
  - Ground Truth vs. Simulation -> Calibration Agent -> Pseudo-Gradient Generation -> Candidate Personas -> Smoothing -> Updated Persona

- **Critical path:**
  1. Data Ingestion: Collect real-world route choices and travel times for the rolling window
  2. Simulation: Traveler Agent simulates choices using current Persona
  3. Critique: Calibration Agent compares Simulation vs. Reality â†’ Generates textual error analysis
  4. Update: Calibration Agent proposes candidate personas
  5. Selection & Smoothing: Select candidate with lowest loss, merge with long-term baseline

- **Design tradeoffs:**
  - Fixed vs. Learnable Memory: Memory retrieval logic is fixed (e.g., "last 4 trips") while only the persona is learned, reducing search space but assuming correct time-scales for memory
  - Window Size: Small window allows fast adaptation but risks overfitting; large window is more stable but may miss rapid strategy changes (paper uses t_w=8)

- **Failure signatures:**
  - Volatile Flows: Weak smoothing causes agents to over-react to daily fluctuations
  - Inertia Overfit: Strong smoothing or weak pseudo-gradients prevent adaptation to new trends

- **First 3 experiments:**
  1. Persona Sensitivity: Compare convergence with "blank" initial persona vs. expert-written persona
  2. Ablation on Smoothing: Disable long-term smoothing to measure variance of simulated route flows
  3. Behavioral Vector Check: Calculate (C^-, S^-) behavior vector for simulated agents to verify they match human archetypes

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Requires extensive ground-truth behavioral data, limiting applicability to domains with readily available such data
- Assumes decision-making heuristics can be adequately captured through natural language personas, which may not generalize to all behavioral domains or cultural contexts
- Computational cost of running dual LLM processes for each agent could pose scalability challenges for large transportation networks

## Confidence
- **High Confidence:** Empirical performance improvements (16.7% F1-score increase, 20.7% MSE reduction) are well-supported by experimental results
- **Medium Confidence:** Theoretical mechanism of pseudo-gradient descent in semantic space is plausible but would benefit from ablation studies
- **Medium Confidence:** Assumption that persona smoothing prevents overfitting while maintaining adaptability is supported but could be sensitive to parameter tuning

## Next Checks
1. **Generalization Test:** Apply the framework to a different transportation dataset or behavioral domain (e.g., mode choice instead of route choice) to assess cross-domain robustness
2. **Persona Stability Analysis:** Track persona evolution over extended time periods to determine whether smoothing maintains behavioral consistency or if personas eventually drift
3. **Computational Efficiency Benchmark:** Measure and compare the computational overhead of the dual-agent system against traditional agent-based models to establish practical scalability limits