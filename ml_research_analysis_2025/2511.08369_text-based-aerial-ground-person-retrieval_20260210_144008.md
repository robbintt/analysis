---
ver: rpa2
title: Text-based Aerial-Ground Person Retrieval
arxiv_id: '2511.08369'
source_url: https://arxiv.org/abs/2511.08369
tags:
- description
- person
- image
- images
- woman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Text-based Aerial-Ground Person Retrieval
  (TAG-PR), a new cross-modal task for retrieving person images from heterogeneous
  aerial and ground views using textual descriptions. The large viewpoint discrepancy
  between aerial and ground images poses challenges for both feature learning and
  cross-modal alignment.
---

# Text-based Aerial-Ground Person Retrieval

## Quick Facts
- arXiv ID: 2511.08369
- Source URL: https://arxiv.org/abs/2511.08369
- Reference count: 39
- Introduces TAG-PR task and TAG-CLIP framework with 2.5-5.0 point R@1 improvements on new TAG-PEDES dataset

## Executive Summary
This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), a new cross-modal task for retrieving person images from heterogeneous aerial and ground views using textual descriptions. The large viewpoint discrepancy between aerial and ground images poses challenges for both feature learning and cross-modal alignment. To support this task, the authors construct TAG-PEDES, a dataset built from existing Re-ID benchmarks with diversified automatically generated textual descriptions. They also propose TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module for robust feature extraction and a viewpoint decoupling strategy for improved cross-modal alignment. Experiments show that TAG-CLIP outperforms existing methods on the new TAG-PEDES dataset and demonstrates strong generalization on traditional T-PR benchmarks. Ablation studies confirm the effectiveness of the proposed components.

## Method Summary
TAG-CLIP is a dual-encoder framework built on TBPS-CLIP, featuring a ViT-B/16 image encoder augmented with Hierarchically-Routed Mixture of Experts (HR-MoE) blocks and a standard Transformer text encoder. The HR-MoE uses image-level and feature-level routers to direct features to aerial-specific or ground-specific expert groups, while a special view token captures viewpoint information. The framework employs a combined global and local alignment loss, an ID loss, a view classification loss, and an orthogonal loss that encourages viewpoint and identity features to be statistically independent. The model is trained with AdamW optimizer for 10 epochs on the TAG-PEDES dataset, achieving significant improvements in cross-view text-based person retrieval.

## Key Results
- TAG-CLIP achieves 2.5-5.0 point R@1 improvements over baselines on TAG-PEDES dataset
- Viewpoint decoupling through orthogonal loss significantly enhances retrieval accuracy
- Strong generalization to traditional T-PR benchmarks (Market-1501, CUHK-PEDES)
- HR-MoE with hierarchical routing outperforms vanilla MoE and single-stream architectures

## Why This Works (Mechanism)
The core innovation addresses the fundamental challenge of cross-view heterogeneous retrieval by separating viewpoint-dependent and viewpoint-independent features. The HR-MoE module learns specialized feature extractors for aerial and ground views through hierarchical routing, allowing the model to handle the large appearance discrepancy between viewpoints. The orthogonal loss explicitly enforces statistical independence between view and identity features, preventing viewpoint information from interfering with identity discrimination. This combination of specialized expert routing and feature decoupling enables the model to learn robust cross-view representations that maintain discriminative power for person identification while being invariant to viewpoint changes.

## Foundational Learning

**Mixture of Experts (MoE)**
- Why needed here: The paper's core contribution is the HR-MoE module. You must understand that MoE layers replace a single feed-forward network with many parallel "expert" networks and a "router" that decides which experts to use for each input token, increasing model capacity without a proportional increase in computation.
- Quick check question: Can you explain how a "router" network decides which expert(s) to activate for a given input token, and what "Top-K" selection means?

**Contrastive Learning (CLIP-style)**
- Why needed here: The entire framework is built on TBPS-CLIP, a baseline that uses a dual-encoder (one for images, one for text) trained with contrastive loss to pull positive image-text pairs together in a shared embedding space and push negative pairs apart.
- Quick check question: What is the core objective of the InfoNCE (contrastive) loss used in models like CLIP, and how does the "temperature" parameter affect it?

**Orthogonalization in Latent Space**
- Why needed here: The viewpoint decoupling strategy is a core novelty. Understanding that we can enforce statistical independence or geometric orthogonality between feature vectors (e.g., via a penalty on their dot product) is crucial for understanding how they separate "view" information from "identity" information.
- Quick check question: Why is the cosine similarity between two vectors a suitable metric for their orthogonality, and what does a value of 0.0 signify versus 1.0?

## Architecture Onboarding

**Component map:** Input Image -> Patch Embedding + View Token -> Standard ViT Blocks -> HR-MoE Blocks (routers direct features to experts) -> Final Layer -> `v_cls` (Global Feature), `v_view` (View Feature), `v_tse` (Local Feature). Text -> Text Encoder -> `t_eos`, `t_tse`. Final Similarity Score = `cos(v_cls, t_eos) + cos(v_tse, t_tse)`. The `v_view` is used for `L_view` and `L_ortho`.

**Critical path:** The image encoder processes input patches with the view token, passes through standard ViT blocks, then through HR-MoE blocks where routers direct features to aerial or ground experts. The final layer produces global, view, and local features. The text encoder produces end-of-sequence and local features. The similarity score combines global and local alignments.

**Design tradeoffs:**
- HR-MoE placement: Ablation shows placing HR-MoE in middle layers (7-9) is optimal. Early layers lack semantic view info; later layers have it too entangled.
- Orthogonal Loss Threshold (α): Set to 0.1. Too small (α=0) forces strict orthogonality, which hurts discriminative identity features. Too large preserves too much view information.
- Single vs. Dual Stream: The paper notes a tradeoff. Using a dual-stream like TBPS-CLIP may underperform compared to one-stream models with cross-attention on single-view datasets, but is more flexible for the proposed multi-view task.

**Failure signatures:**
- Image-level router accuracy is low: This would break the HR-MoE, as ground features would be routed to aerial experts. Monitor `L_view`.
- High orthogonal loss (L_ortho): Suggests the model is failing to separate view and identity information, which will likely cause poor cross-modal retrieval performance.
- HR-MoE expert imbalance: If dominated by shared experts only, reduce overlap (e1→2) or increase λ_ortho.

**First 3 experiments:**
1. Baseline Reproduction: Reproduce the TBPS-CLIP baseline performance on the provided TAG-PEDES dataset to ensure your data pipeline is correct.
2. Ablate HR-MoE: Run the full TAG-CLIP model but replace the HR-MoE blocks with standard ViT blocks (or vanilla MoE) to isolate the performance gain from the hierarchical routing.
3. Hyperparameter Sweep: Perform a focused sweep on the orthogonal loss threshold α to find the optimal balance between decoupling viewpoint and preserving identity features for your specific training setup.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does the binary routing mechanism in the Hierarchically-Routed Mixture of Experts (HR-MoE) generalize to intermediate or ambiguous camera viewpoints that do not strictly fit "aerial" or "ground" categories?
- Basis: The HR-MoE relies on a discrete binary classification to route features to either aerial-specific or ground-specific experts.
- Why unresolved: The architecture assumes a hard partition between views. Real-world surveillance may involve continuous viewpoint transitions where forcing a binary choice could route features to suboptimal experts.
- What evidence would resolve it: Evaluation results on a dataset containing continuous viewpoint angles or "low-altitude" drone imagery that blurs the line between the two defined categories.

**Open Question 2**
- Question: Can the model's view-specific components be adapted to maintain state-of-the-art performance on traditional single-view (ground-only) retrieval tasks?
- Basis: The authors note that TAG-CLIP underperforms on standard datasets like CUHK-PEDES because the view-decoupling mechanisms become "inapplicable" and the model "degenerates into a baseline."
- Why unresolved: It is unclear if the additional parameters for handling heterogeneity act as a hindrance (noise) in homogeneous scenarios or if they simply fail to provide a benefit.
- What evidence would resolve it: Experiments using a dynamic gating mechanism to disable the HR-MoE and decoupling losses when view variance is undetected.

**Open Question 3**
- Question: Does the orthogonal loss fully disentangle identity features from viewpoint information, or does it merely suppress the most prominent view-specific cues while retaining subtle biases?
- Basis: The hyperparameter analysis shows that setting the orthogonality threshold (α) too low degrades performance by removing discriminative features, implying a trade-off where complete decoupling may hurt accuracy.
- Why unresolved: The reliance on a manual threshold suggests that achieving a "view-agnostic" feature space without losing identity-relevant information is an unstable equilibrium.
- What evidence would resolve it: A quantitative analysis of feature embeddings to measure viewpoint clustering versus identity clustering within the latent space.

## Limitations

- Dependency on TAG-PEDES dataset with potentially noisy view labels inherited from source datasets rather than systematic annotation
- Orthogonal loss formulation with threshold-based approach lacks rigorous theoretical justification and may be dataset-specific
- Computational overhead of HR-MoE not fully characterized with FLOPs or latency measurements

## Confidence

**High Confidence** (Supported by ablation studies and quantitative results):
- HR-MoE blocks improve cross-view retrieval performance (R@1 improvements of 2.5-5.0 points on TAG-PEDES)
- Viewpoint decoupling through orthogonal loss enhances retrieval accuracy
- TAG-CLIP generalizes to traditional T-PR benchmarks (Market-1501, CUHK-PEDES)

**Medium Confidence** (Results are positive but methodological concerns exist):
- The 95%+ image-level router accuracy translates to effective view-specific feature extraction
- TAG-PEDES represents a challenging benchmark for aerial-ground retrieval
- The combined global+local alignment loss is optimal for this task

**Low Confidence** (Claims lack sufficient empirical support):
- HR-MoE is more computationally efficient than increasing model width (no FLOPs analysis provided)
- The orthogonal loss threshold α=0.1 is optimal across different dataset scales (only tested on TAG-PEDES)
- The hierarchical routing strategy is necessary rather than sufficient

## Next Checks

1. **View Label Accuracy Validation**: Implement a manual verification protocol for a random subset (100-200 samples) of TAG-PEDES to quantify view label noise. Compare retrieval performance when training with corrected labels versus original inherited labels to measure the impact of label accuracy on the claimed improvements.

2. **Orthogonal Loss Sensitivity Analysis**: Conduct a systematic sweep of α values (0.01, 0.05, 0.1, 0.2, 0.3) and λ_ortho values (10, 50, 100, 200) across multiple random seeds. Plot the Pareto frontier of view decoupling strength versus identity preservation to identify if the current configuration represents a true optimum or a local maximum.

3. **Computational Overhead Measurement**: Profile the actual inference time and FLOPs of TAG-CLIP with HR-MoE versus a baseline ViT-B/16 with width scaling to match parameter count. Measure both throughput (samples/second) and memory usage to validate the efficiency claims, particularly under different Top-K routing configurations.