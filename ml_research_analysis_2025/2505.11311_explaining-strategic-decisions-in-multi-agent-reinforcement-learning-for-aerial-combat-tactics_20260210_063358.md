---
ver: rpa2
title: Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial
  Combat Tactics
arxiv_id: '2505.11311'
source_url: https://arxiv.org/abs/2505.11311
tags:
- combat
- agent
- agents
- aircraft
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores explainability in Multi-Agent Reinforcement
  Learning (MARL) for air combat scenarios to enhance transparency, trust, and strategic
  alignment with human decision-making. It adapts various explainability methods to
  simulated dogfight environments, analyzing hierarchical MARL models with centralized
  training and decentralized execution.
---

# Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics

## Quick Facts
- arXiv ID: 2505.11311
- Source URL: https://arxiv.org/abs/2505.11311
- Reference count: 0
- This work explores explainability in Multi-Agent Reinforcement Learning (MARL) for air combat scenarios to enhance transparency, trust, and strategic alignment with human decision-making.

## Executive Summary
This paper presents a hierarchical MARL framework for 5-vs-5 aerial combat scenarios, where a high-level commander assigns low-level tactical policies (attack, engage, defend) to individual agents. The authors adapt various explainability methods to analyze decision-making patterns, demonstrating that radar sensing range and aircraft heterogeneity significantly impact tactical choices. Through feature perturbation analysis and hierarchical inspection, they show that aspect angle and distance are key decision factors. The study highlights the importance of explainable AI in high-stakes military applications while acknowledging the inherent trade-off between model performance and interpretability.

## Method Summary
The approach uses Hierarchical Multi-Agent Reinforcement Learning with centralized training and decentralized execution. A single high-level commander policy selects among three discrete tactical modes (attack, engage, defend), each implemented by a dedicated low-level policy. The system employs PPO for both levels, with low-level policies trained first and frozen during high-level training. Training uses league-based self-play with curriculum learning. Explainability is achieved through feature perturbation analysis, hierarchical inspection, and policy activation frequency analysis across varying sensing ranges and aircraft configurations.

## Key Results
- Commanders shift from aggressive to defensive strategies as radar sensing range increases from m≤3 to m>3
- Aspect angle and distance are identified as the dominant decision factors for tactical mode selection
- Heterogeneous aircraft configurations produce more conservative but realistic strategies compared to homogeneous setups
- Explainability methods reveal tactical patterns that align with human decision-making principles in aerial combat

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical policy decomposition improves tactical interpretability by mapping high-level commands to observable low-level behaviors. A single high-level commander policy selects among three discrete options (attack, engage, defend), each implemented by a dedicated low-level policy. This separation allows analysts to inspect which tactic was chosen and how it was executed, rather than interpreting raw neural network outputs directly.

### Mechanism 2
Sensing capability (radar range) causally shifts commander strategy from aggressive to defensive modes. By varying the number of detected opponents/friendlies (sensing parameter m ∈ [1,5]), the commander receives different partial observations. Short-sighted commanders (m ≤ 3) preferentially select attack modes even in disadvantageous scenarios, while higher radar range (m > 3) triggers defensive/engaging behaviors, revealing how observation scope drives risk assessment.

### Mechanism 3
Local explanations derived from feature perturbation reveal that aspect angle and distance are the dominant decision factors. The authors systematically perturb three input features (distance, Antenna Train Angle, Aspect Angle) for the nearest opponent while holding other observations randomized. By running 100 simulations per feature combination and recording the most frequently selected mode, they identify which feature dimensions produce the largest variation in commander decisions.

## Foundational Learning

- **Partially Observable Markov Game (POMG)**: The paper formalizes MARL interactions using POMGs, which extend MDPs to multi-agent settings with partial observability. Understanding that each agent receives observation o ⊂ S rather than full state s is essential for interpreting why commanders make different decisions with different radar ranges.
  - Quick check: If agent A's radar range increases from m=2 to m=4, does its observation space change, its state space change, or both?

- **Centralized Training with Decentralized Execution (CTDE)**: The paper uses CTDE as its training paradigm, which explains how agents can learn coordinated behaviors during training while operating independently at deployment. This is critical for understanding why the commander can be trained with global information but must issue per-agent commands based on local observations.
  - Quick check: During execution, can the commander access the full environment state S, or only partial observations per agent?

- **Options Framework in Hierarchical RL**: Section 2.2 formalizes the hierarchy using a POSMDP with "options" (τ) that are temporally extended actions. Each option has an initiation set, low-level policy, and termination condition. Understanding this formalism is necessary to interpret how the commander's discrete choices translate to sustained low-level behaviors.
  - Quick check: If the commander selects "defend" at timestep t, when does the low-level defend policy terminate—after a fixed duration Tl, when termination condition βτ is met, or both?

## Architecture Onboarding

- **Component map**: Environment -> High-Level Commander Policy (πh) -> Low-Level Control Policies (πa, πe, πd) -> Aircraft Actions
- **Critical path**: Define reward functions for each low-level mode → Train low-level policies via self-play until convergence → Freeze low-level policies; train high-level commander with varying sensing ranges m ∈ [1,5] → Perturb input features and log policy activation frequencies over 100 episodes per configuration → Visualize dominant mode per feature configuration
- **Design tradeoffs**: Homogeneous vs. heterogeneous agents (realism vs. interpretability); radar range (tactical awareness vs. noise); policy sharing vs. per-agent policies (training efficiency vs. specialization); explanation fidelity vs. performance
- **Failure signatures**: Counterintuitive explanations (indicates undertrained policies); aggressive behavior in disadvantageous scenarios (indicates insufficient sensing range); static explanations across feature perturbations (suggests non-relevant features or undertraining)
- **First 3 experiments**: 1) Baseline validation: Replicate 5-vs-5 homogeneous scenario with m=3; verify win/loss/draw distribution; 2) Sensing range sweep: Train commanders for m ∈ {1,2,3,4,5}; plot mode activation frequencies against combat difference; 3) Local feature importance: For single AC1 agent, systematically vary (distance, ATA, AA) features of nearest opponent; generate feature importance visualization

## Open Questions the Paper Calls Out

- **How can the trade-off between explainability and combat performance be systematically optimized as commander sensing range increases?**: The authors observe that increasing radar range improves behavioral transparency but weakens commander performance due to distant opponents acting as noise. No methods are proposed to balance this trade-off.

- **How do hierarchical MARL explanations transfer from 2D to 3D air combat with vertical maneuvering?**: The current work restricts to 2D with constant altitude to eliminate vertical motion complexities, limiting ecological validity. No analysis addresses whether key features remain dominant or whether altitude-related features become critical in 3D dogfights.

- **Can causal models scale to real-time multi-agent air combat without compromising inference speed?**: The paper states that SCMs are computationally demanding for real-time simulations in fast-paced aerial combat, suggesting them only for smaller configurations. No approximation methods are proposed for larger scenarios.

## Limitations
- Explanation quality is fundamentally bounded by policy performance, with imperfect HMARL producing counterintuitive explanations
- Feature importance analysis assumes nearest-opponent features are primary drivers, which may not hold for complex multi-agent coordination
- Performance-explainability trade-off requires careful calibration that may not transfer across domains

## Confidence

- **High confidence**: Hierarchical policy decomposition improves interpretability; sensing capability causally shifts tactical choices
- **Medium confidence**: Local feature perturbation reveals dominant decision factors; CTDE training enables coordinated yet decentralized execution
- **Low confidence**: Explanation fidelity scales with policy performance; radar range trade-off is optimal at m=4-5 for explanation quality

## Next Checks

1. **Cross-architecture validation**: Apply the same explanation methods to a non-hierarchical MARL model on the same 5-vs-5 dogfight task; compare explanation clarity and consistency with hierarchical results.

2. **Nearest-opponent perturbation stress test**: Systematically vary which opponent is designated "nearest" (not just perturb features); test whether explanation results remain stable across different opponent selections.

3. **Radar range noise analysis**: With m=5, inject artificial noise into distant aircraft observations; measure degradation in both combat performance and explanation quality to quantify the performance-explainability trade-off.