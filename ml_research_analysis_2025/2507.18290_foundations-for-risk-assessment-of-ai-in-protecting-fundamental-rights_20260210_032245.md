---
ver: rpa2
title: Foundations for Risk Assessment of AI in Protecting Fundamental Rights
arxiv_id: '2507.18290'
source_url: https://arxiv.org/abs/2507.18290
tags:
- rights
- risk
- legal
- fundamental
- deployment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a conceptual framework for qualitative AI risk
  assessment focused on fundamental rights protection. The authors integrate definitional
  balancing with defeasible reasoning to address legal compliance complexities under
  the EU AI Act.
---

# Foundations for Risk Assessment of AI in Protecting Fundamental Rights

## Quick Facts
- arXiv ID: 2507.18290
- Source URL: https://arxiv.org/abs/2507.18290
- Reference count: 38
- Primary result: Presents conceptual framework for qualitative AI risk assessment focused on fundamental rights protection using defeasible reasoning and definitional balancing

## Executive Summary
This paper proposes a conceptual framework for assessing AI systems' legal risks regarding fundamental rights protection, particularly under the EU AI Act. The approach integrates definitional balancing with defeasible reasoning to address the complex nature of legal compliance in AI deployment. The framework treats fundamental rights as defeasible rules with limitations as defeaters, resolved through proportionality analysis. By decomposing AI systems into deployment scenarios and applying a "what-if" analysis, the method identifies potential legal violations and rights impacts through a layered analytical approach.

## Method Summary
The framework employs defeasible reasoning with definitional balancing to assess legal risks from AI systems' impacts on fundamental rights. It uses a layered "what-if" analysis of deployment scenarios to identify potential legal violations and rights impacts. Fundamental rights are treated as defeasible rules with limitations as defeaters, resolved through proportionality analysis. The method calculates a "Degree of Right Impact" (Ξ - Δ) to quantify risk and searches for optimal deployment configurations that minimize legal exposure. The approach distinguishes between high-risk AI systems and General Purpose AI (GPAI) systems, emphasizing scenario-based risk assessment.

## Key Results
- Framework integrates defeasible reasoning with definitional balancing for legal compliance assessment
- Treats fundamental rights as defeasible rules with limitations functioning as defeaters
- Proposes principles for representing rights promotion/demotion and prioritization within deployment contexts
- Introduces "Degree of Right Impact" metric (Ξ - Δ) for risk quantification
- Aims to enable more operative risk assessment models and support responsible AI governance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating fundamental rights as defeasible rules rather than absolute constraints allows for context-sensitive legal compliance.
- **Mechanism:** The framework encodes rights (e.g., privacy, non-discrimination) as defaults that hold unless specific conditions (defeaters) are met. It applies proportionality analysis to validate these defeaters. If a limitation on a right is suitable and necessary for a legitimate goal (e.g., public safety), the default right is temporarily overridden in that specific context.
- **Core assumption:** Rights are non-absolute and their relative importance can be discretely ordered based on the deployment context.
- **Evidence anchors:**
  - [Abstract] "Definitional balancing employs proportionality analysis to resolve conflicts... while defeasible reasoning accommodates the dynamic nature of legal decision-making."
  - [Section 5.1] "Fundamental rights are treated as defeasible rules, with their limitations functioning as defeaters... Proportionality analysis... becomes the mechanism for resolving these conflicts."
  - [Corpus] The neighbor paper *Toward Robust Legal Text Formalization into Defeasible Deontic Logic using LLMs* supports the viability of automating this specific logical structure.
- **Break condition:** If a legal domain treats a specific right as absolute (non-derogable), the defeasible logic fails to produce a valid compliance path.

### Mechanism 2
- **Claim:** Risk magnitude is identified by decomposing General Purpose AI (GPAI) systems into distinct "deployment scenarios" rather than assessing the model in isolation.
- **Mechanism:** A "what-if" analysis iterates through a superset of scenarios ($S_1, \dots, S_n$). For each scenario, the mechanism identifies relevant obligations and maps them to specific rights. It assesses whether the scenario promotes or demotes a right (e.g., $Promotes(S_{pandemic}, public\_health)$). The aggregation of these impacts determines the overall legal risk.
- **Core assumption:** The legal risk of a GPAI is strictly the union of risks across its potential deployment scenarios.
- **Evidence anchors:**
  - [Section 3.2] "Identifying legal risks... requires a what-if analysis to: determine the characterizing deployment scenarios... [and] assess the compliance impact."
  - [Section 6.6, Figure 2] Defines the step-by-step process: "Identify obligations... Identify fundamental rights... Establish... which rights... are promoted or demoted."
  - [Corpus] The neighbor paper *HH4AI* explicitly validates the need for this scenario-based approach under the EU AI Act.
- **Break condition:** If the GPAI exhibits emergent behaviors not captured in the defined scenario set ($D_\sigma$), the risk assessment yields false negatives.

### Mechanism 3
- **Claim:** An optimization procedure based on "Right Impact Degree" minimizes legal risk by selecting the optimal set of deployment configurations.
- **Mechanism:** The framework calculates a "Degree of Right Impact" ($Degree(D_\sigma) = \Xi - \Delta$). This formula sums the weighted value of protected rights ($\Xi$) and subtracts the penalty of demoted rights ($\Delta$). By searching for the subset of scenarios that maximizes this score, the system identifies the theoretically optimal compliance configuration.
- **Core assumption:** Rights can be assigned ordinal positions in a preference sequence ($R_1 \otimes R_2 \otimes \dots$) to enable arithmetic comparison of "impact."
- **Evidence anchors:**
  - [Section 6.7, Definition 1] "We define for $D_\sigma$ the degree of right impact as $\Xi - \Delta$."
  - [Section 6.7, Definition 2] "The legal risk of an AI system $\sigma$ is minimal iff $\sigma$ is designed for a set... such that... Degree($D'_\sigma$) $\ge$ Degree($X$)."
  - [Corpus] No direct corpus evidence validates the specific arithmetic formula proposed; this appears to be a novel theoretical contribution.
- **Break condition:** If the preference ordering ($R_1 \otimes R_2$) is disputed or inconsistent across different legal interpretations, the optimization result is non-binding.

## Foundational Learning

- **Concept:** **Defeasible Reasoning**
  - **Why needed here:** The paper relies on non-monotonic logic to model how legal conclusions change when new facts (defeaters) appear. Standard logic cannot handle the "exceptions" inherent in law.
  - **Quick check question:** If new evidence contradicts a derived legal conclusion, does the conclusion retract? (If yes, the reasoning is defeasible).

- **Concept:** **Proportionality Analysis**
  - **Why needed here:** This is the "balancing" mechanism used to justify limiting a right. Learners must understand the criteria (legitimate aim, suitability, necessity) to model the "defeaters."
  - **Quick check question:** Does the restriction on a right achieve a pressing social need without being broader than necessary?

- **Concept:** **Fundamental Rights Impact Assessment (FRIA)**
  - **Why needed here:** The paper operationalizes the EU AI Act's requirement for FRIAs. Understanding the regulatory input (Art. 27) is necessary to structure the output of the risk model.
  - **Quick check question:** Can you distinguish between a "legal violation" and a "fundamental rights demotion"?

## Architecture Onboarding

- **Component map:** Input Layer (System Scope $D_\sigma$ or $P_\Sigma$, Rights Registry $R$, Preference Ordering $\otimes$) -> Processing Core (Scenario Generator -> Obligation Mapper -> Promotion/Demotion Assessor) -> Resolution Engine (Defeasible Logic Solver applies priority rules to conflicting rights) -> Output Layer (Optimal Configuration Set and Impact Degree Score)

- **Critical path:** The definition of the Preference Ordering ($R_1 \otimes R_2$). The entire risk calculation depends on this subjective/legal hierarchy being encoded correctly before the reasoning engine runs.

- **Design tradeoffs:**
  - Granularity vs. Computability: Defining highly granular scenarios ($S_i$) increases accuracy but explodes the computational cost of the optimization search (subset selection).
  - Qualitative vs. Quantitative: The paper converts qualitative rights into quantitative "degrees" ($\Xi - \Delta$). This enables optimization but may oversimplify the nuance of "immaterial harms."

- **Failure signatures:**
  - Undefined Status: The logic returns `undefined` for a scenario where $Promotes$ and $Demotes$ are ambiguous (Section 6.6, Step 6).
  - Cyclic Priority: The reasoning halts if Rights Preferences form a loop (e.g., $R_1 > R_2 > R_1$).

- **First 3 experiments:**
  1. **Static Validation:** Manually encode the "Recidivism Assessment" example (Example 1) to verify the logic outputs "Choice(Public Safety)" when privacy is demoted under pandemic constraints.
  2. **Scenario Explosion Test:** Feed a GPAI description into the Scenario Generator to measure how many distinct $S_i$ are produced and check for "Undefined" mappings.
  3. **Sensitivity Analysis:** Perturb the Preference Ordering ($\otimes$) slightly (swap Privacy and Public Health priority) to observe if the "Minimal Risk" configuration changes drastically.

## Open Questions the Paper Calls Out

- **Question:** Which specific logical systems and semantics are suitable for formalizing the proposed integration of definitional balancing and defeasible reasoning?
  - **Basis in paper:** [explicit] The Conclusion states future work requires "a detailed exploration of the suitable logical systems and semantics."
  - **Why unresolved:** The paper currently provides a conceptual framework but lacks a rigorous formal logical specification.
  - **What evidence would resolve it:** A formal logical system that instantiates the proposed rights promotion, demotion, and priority mechanisms.

- **Question:** What are the formal properties (e.g., consistency, soundness) of the proposed reasoning patterns for rights adoption?
  - **Basis in paper:** [explicit] The Conclusion calls for "a more thorough investigation of the properties of the proposed reasoning patterns."
  - **Why unresolved:** The paper outlines inference rules (e.g., "Right adoption 1") informally but does not analyze their formal behavior or potential conflicts.
  - **What evidence would resolve it:** Formal proofs or logical analysis demonstrating the reliability of the inference rules.

- **Question:** What algorithms can effectively automate the proposed risk minimization procedures and "Degree of Right Impact" calculations?
  - **Basis in paper:** [explicit] The Abstract and Conclusion mention the aim to develop "effective algorithms to enhance AI risk assessment."
  - **Why unresolved:** The framework defines a mathematical goal for risk minimization but provides no algorithmic method to compute it.
  - **What evidence would resolve it:** Functional algorithms capable of parsing deployment scenarios and calculating the optimal risk-minimized configuration.

## Limitations
- Framework lacks empirical validation and testing against real EU AI Act compliance cases
- Arithmetic formula for "Degree of Right Impact" (Ξ - Δ) appears without precedent or validation from legal theory
- Assumes fundamental rights can be meaningfully ranked in a universal preference ordering across different legal systems
- Does not address how to handle rights that may be simultaneously promoted and demoted in the same scenario

## Confidence

**High confidence** in the conceptual integration of defeasible reasoning with legal proportionality analysis, as this draws directly from established legal theory. **Medium confidence** in the scenario-based decomposition approach, which aligns with EU AI Act requirements but lacks empirical testing. **Low confidence** in the proposed optimization framework and "Degree of Right Impact" calculation, as these represent novel theoretical constructs without validation.

## Next Checks

1. **Legal Expert Validation:** Test the framework against real EU AI Act compliance cases with legal domain experts to verify if the defeasible reasoning produces legally sound outcomes.

2. **Empirical Scenario Analysis:** Apply the framework to a GPAI system (e.g., a foundation model) and measure the actual number of deployment scenarios generated, checking for computational feasibility and "undefined" mappings.

3. **Cross-Jurisdictional Testing:** Apply the framework to the same AI system across different legal jurisdictions to test the robustness of the universal preference ordering assumption.