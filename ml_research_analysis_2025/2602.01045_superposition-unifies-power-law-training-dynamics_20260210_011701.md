---
ver: rpa2
title: Superposition unifies power-law training dynamics
arxiv_id: '2602.01045'
source_url: https://arxiv.org/abs/2602.01045
tags:
- superposition
- training
- loss
- dynamics
- power-law
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how feature superposition affects power-law
  training dynamics in neural networks using a teacher-student framework. The authors
  first derive an analytic theory for the no-superposition case, showing that the
  training exponent depends on both input data statistics and channel importance.
---

# Superposition unifies power-law training dynamics

## Quick Facts
- arXiv ID: 2602.01045
- Source URL: https://arxiv.org/abs/2602.01045
- Reference count: 36
- Primary result: Superposition bottleneck induces universal power-law training exponent α≈1, accelerating learning up to 10× compared to sequential dynamics.

## Executive Summary
This paper investigates how feature superposition affects power-law training dynamics in neural networks using a teacher-student framework. The authors first derive an analytic theory for the no-superposition case, showing that the training exponent depends on both input data statistics and channel importance. They then demonstrate empirically that introducing a superposition bottleneck induces a transition to a universal power-law exponent of approximately 1, independent of data and channel statistics. This represents up to tenfold acceleration compared to sequential learning. The authors also show that optimal model size scales with compute budget, and that randomness in the superposition bottleneck is near-optimal for mid-training dynamics.

## Method Summary
The study uses a teacher-student framework where a student network learns to reconstruct a teacher's output under a bottleneck dimension K. The teacher matrix A is diagonal with power-law decay, and inputs are sparse with power-law distributed feature frequencies. Without superposition, the student uses K=N with identity embedding and no nonlinearity. With superposition, the student uses K<N with a fixed random column-normalized embedding W and ReLU + bias. Both models are trained via SGD to minimize MSE loss, and the power-law exponent α is extracted from mid-training loss trajectories (t ~ 10³ to 10⁴).

## Key Results
- Without superposition, training follows sequential spectral filtering with exponent α = (a+2b-1)/a, verified empirically in Figure 3b
- Superposition induces parallel learning with universal exponent α≈1, independent of data/channel statistics (Figure 8)
- Superposition accelerates mid-training by over 10× compared to sequential learning (Figure 4)
- Optimal model size scales as K ∝ C^0.27 with compute budget C (Figure 14)

## Why This Works (Mechanism)

### Mechanism 1
Without superposition, training follows sequential spectral filtering where the power-law exponent depends on data and channel statistics. Gradient descent learns features in descending order of importance. At time t, features with index i < ic(t) are learned while i > ic(t) remain frozen, creating a "traveling wave" where total loss is dominated by unlearned tail features. Core assumption: The critical feature index ic(t) ∝ t^(1/a) defines the boundary between learned and unlearned features in mid-training. Evidence anchors: [abstract] "Without superposition, the training exponent depends on input data statistics and channel importance"; [Section 3.2] Equation 8: α = (a + 2b - 1)/a, verified empirically in Figure 3b; [corpus] Consistent with "Learning curves theory for hierarchically compositional data" showing power-law exponents from data structure. Break condition: When model dimension K equals feature dimension N (no bottleneck), sequential dynamics apply. Also breaks if a + 2b ≤ 1 (no valid power-law regime).

### Mechanism 2
Superposition induces parallel learning with a universal exponent α≈1, independent of data statistics. Random embedding W mixes high and low-frequency features into each latent dimension. Normalization ensures comparable projection magnitudes, so every student dimension receives aggregated signal from all frequencies. This equalizes effective gradients, eliminating the "unlearned tail" bottleneck. Core assumption: The ReLU nonlinearity combined with bias is necessary for the exact α≈1 exponent—linear theory only explains uniformity, not the specific value. Evidence anchors: [abstract] "a superposition bottleneck induces a transition to a universal power-law exponent of ~1, independent of data and channel statistics"; [Section 5.1] Figure 8 shows per-entry loss: sequential creates traveling wave, superposition shows parallel global decay; [corpus] "Superposition Yields Robust Neural Scaling" (neighbor paper) supports superposition-driven scaling. Break condition: If embedding W is structured (e.g., block-diagonal) rather than random/isotropic, mixing fails. Also breaks if ReLU is removed—the paper notes α≈1 requires nonlinearity.

### Mechanism 3
Superposition accelerates mid-training by distributing error across all features rather than concentrating it in the tail. Sequential learning has zero error on learned features but maximal error on unlearned majority. Superposition distributes error evenly: sum of many small averaged errors < sum of unlearned spectral tail. Randomness achieves quadratic acceleration scaling ~n²σ̄² via variance aggregation. Core assumption: This advantage is specific to mid-training; superposition models eventually hit capacity plateau while sequential models converge to zero. Evidence anchors: [Section 4.2] "Superposition accelerates this by over 10×" for a=1.1, b=0 case; [Figure 4] Shows superposition loss curve dips below no-superposition theory during mid-training; [corpus] Weak corpus support—this specific mid-training acceleration mechanism appears novel. Break condition: Late-training regime where capacity bottleneck dominates—the paper explicitly notes superposition advantage disappears as models saturate.

## Foundational Learning

- Concept: Power-law distributions and spectral decay
  - Why needed here: Input features follow p_i ∝ i^(-a), channel importance follows A_ii ∝ i^(-b). Understanding how heavy tails create learning bottlenecks is essential.
  - Quick check question: If input decay a increases (steeper drop-off), does sequential learning get faster or slower? (Faster—less tail to learn)

- Concept: Superposition hypothesis in neural networks
  - Why needed here: Models represent more features than neurons by storing them in non-orthogonal directions, creating interference noise that this paper shows is actually beneficial.
  - Quick check question: Why doesn't interference destroy learning? (ReLU suppresses negative interference; distributed error < tail error)

- Concept: Teacher-student frameworks
  - Why needed here: Isolates training dynamics by having a fixed teacher matrix A generate targets, removing confounding architectural factors.
  - Quick check question: What does the student learn when K=N vs K<N? (K=N: diagonal of A; K<N: compressed representation with mixing)

## Architecture Onboarding

- Component map: Input x ∈ R^N → W ∈ R^(K×N) → latent h → B ∈ R^(K×K) → processed h → W^T → reconstruction → ReLU(bias) → output y
- Critical path:
  1. Input → W → latent h (compression + mixing)
  2. h → B → processed latent
  3. Processed → W^T → reconstruction
  4. Reconstruction → ReLU(bias) → output
  5. MSE loss vs teacher output Ax
- Design tradeoffs:
  - Smaller K: More mixing, faster mid-training, higher final plateau
  - Larger K: Less mixing, slower mid-training, lower final plateau
  - Random vs learned W: Paper shows random is near-optimal for mid-training exponent (Figure 12), but learned achieves ~10⁻³ lower absolute loss
  - With vs without ReLU: ReLU required for α≈1; linear models don't achieve universal exponent
- Failure signatures:
  - Exponent varies with a, b → Superposition not engaged (check K<N)
  - Loss doesn't follow power-law → Check initialization (B(0)≈0 required)
  - Late-training doesn't plateau → Capacity not bottlenecked (increase compression N/K)
  - Negative loss values → ReLU/bias not applied correctly
- First 3 experiments:
  1. Replicate no-superposition baseline: Set N=K=1024, verify α = (a+2b-1)/a across different a, b values. Compare against Figure 3b.
  2. Introduce superposition: Set N=1024, K=512, verify α≈1 regardless of a, b. Plot per-entry loss over time to confirm parallel decay (Figure 8 pattern).
  3. Compute optimal frontier: Train multiple K values, plot loss vs compute (t×K²), extract optimal K scaling with compute budget. Target: K ∝ C^0.27 per Figure 14.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the power-law exponent settle exactly at $\alpha \approx 1$ under superposition, rather than other values? Basis in paper: [explicit] The authors state in Section 6 that a "rigorous study of why the exponent settles exactly at $\alpha \approx 1$... remains an open challenge." Why unresolved: The theoretical analysis of the linear regime (Appendix A.2) explains the uniformity of convergence rates but fails to predict the specific exponent value, which emerges only in the non-linear model with ReLU and bias. What evidence would resolve it: A closed-form theoretical derivation of the exponent for the non-linear superposition model that accounts for the interaction between ReLU noise suppression and gradient mixing.

### Open Question 2
Does the universal scaling behavior generalize to multi-layer architectures with attention mechanisms? Basis in paper: [explicit] The conclusion notes that extending the analysis to "multi-layer architectures with attention mechanisms is crucial." Why unresolved: The current study utilizes a single-layer, feed-forward teacher-student toy model. LLMs utilize deep residual streams and attention heads which introduce different mixing dynamics. What evidence would resolve it: Empirical verification of the $\alpha \approx 1$ exponent and parallel learning dynamics in deep transformers or multi-layer variants of the proposed toy model.

### Open Question 3
Can the late-training dynamics of the superposition model explain the "grokking" phenomenon? Basis in paper: [explicit] Section 6 suggests that analyzing the "late-training dynamics—where the model hits the irreducible approximation error of the bottleneck—could yield insights into the 'grokking' phenomena." Why unresolved: The paper focuses primarily on the mid-training regime (optimization-limited); the dynamics of the transition to the capacity-limited plateau remain less explored. What evidence would resolve it: Analysis of the loss curvature and generalization gaps specifically during the transition from the power-law decay phase to the final plateau in the proposed bottlenecked model.

### Open Question 4
Can structured or learned embeddings outperform random projections in the mid-training regime? Basis in paper: [explicit] The authors ask whether "structured or learned embeddings can outperform random projections... remains an open question for optimizing efficient models." Why unresolved: Appendix A.3 shows that learned embeddings yield similar exponents ($\alpha \approx 1$) but slightly better loss, yet it remains unclear if specific structures could unlock faster scaling or better final performance. What evidence would resolve it: Comparative experiments using non-random embeddings (e.g., PCA-based or clustered projections) to see if the "randomness edge" is optimal or if structured interference can provide greater acceleration.

## Limitations

- Claims about mid-training acceleration being "over 10×" are context-dependent on specific (a,b) choices and may not hold across all power-law regimes
- The assertion that randomness in W is "near-optimal" for mid-training dynamics lacks comparison to structured alternatives beyond block-diagonal cases
- The ReLU nonlinearity is critical for achieving α≈1, but the paper doesn't characterize what other nonlinearities preserve or destroy this universality

## Confidence

**High confidence**: The sequential learning mechanism (α = (a+2b-1)/a) is mathematically derived and empirically verified across multiple (a,b) configurations. The teacher-student framework cleanly isolates the effect of superposition on training dynamics.

**Medium confidence**: The universal α≈1 scaling under superposition is empirically demonstrated but theoretically justified only for the linear case. The ReLU's role in achieving exactly α≈1 is asserted but not rigorously proven. The compute-optimal K scaling (K ∝ C^0.27) is observed but may not generalize beyond the studied parameter ranges.

**Low confidence**: Claims about mid-training acceleration being "over 10×" are context-dependent on specific (a,b) choices and may not hold across all power-law regimes. The assertion that randomness in W is "near-optimal" for mid-training dynamics lacks comparison to structured alternatives beyond block-diagonal cases.

## Next Checks

1. Apply the superposition framework to non-diagonal teacher matrices (e.g., low-rank or sparse off-diagonal structure) to verify whether α≈1 persists or degrades systematically.

2. Replace ReLU with GeLU and Swish nonlinearities while maintaining the same superposition setup. Measure whether α deviates from 1 and characterize the relationship between nonlinearity and universality.

3. Systematically vary the compression ratio N/K beyond the studied range (e.g., N/K ∈ [2, 32]) and measure the point where superposition's acceleration advantage disappears. Verify this matches the theoretical capacity bottleneck prediction.