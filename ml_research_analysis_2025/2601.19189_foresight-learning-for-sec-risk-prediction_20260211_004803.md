---
ver: rpa2
title: Foresight Learning for SEC Risk Prediction
arxiv_id: '2601.19189'
source_url: https://arxiv.org/abs/2601.19189
tags:
- risk
- data
- disclosures
- learning
- filings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a fully automated pipeline for converting SEC risk disclosures
  into temporally grounded supervision using only public filings. The pipeline generates
  firm-specific, time-bounded risk queries from Risk Factors sections and labels them
  by resolving outcomes against subsequent disclosures.
---

# Foresight Learning for SEC Risk Prediction

## Quick Facts
- arXiv ID: 2601.19189
- Source URL: https://arxiv.org/abs/2601.19189
- Reference count: 3
- Primary result: A compact LLM trained on Foresight Learning achieves superior probabilistic calibration for SEC risk materialization prediction compared to pretrained and frontier models.

## Executive Summary
This paper presents a fully automated pipeline that converts SEC risk disclosures into temporally grounded supervision without manual annotation. The approach generates firm-specific risk queries from Risk Factors sections and labels them by resolving outcomes against subsequent disclosures. A compact LLM is fine-tuned to estimate the probability that a disclosed risk will materialize within a specified horizon. The resulting model substantially improves over pretrained and heuristic baselines, and outperforms frontier models including GPT-5 on probabilistic accuracy and calibration metrics.

## Method Summary
The methodology involves extracting Risk Factors from SEC filings, generating falsifiable risk queries with specified prediction horizons using LLM-based RAG, and resolving outcomes by examining future disclosures. The labeled dataset (6,109 queries from 2,820 filings) trains a Qwen3-32B model via GRPO optimization to minimize Brier score. The pipeline ensures temporal masking to prevent information leakage, and outputs calibrated probabilities for risk materialization within defined time windows.

## Key Results
- The Foresight Learning model achieves Brier score of 0.1833 on held-out test data (Nov-Dec 2024)
- Outperforms pretrained Qwen3-32B, GPT-4, GPT-5, and heuristic baselines on Brier score and Expected Calibration Error
- Reduces calibration error (ECE) by nearly 80% relative to the pretrained base model
- Demonstrates effective probabilistic forecasting for SEC risk disclosures across multiple risk categories

## Why This Works (Mechanism)

### Mechanism 1: Future-as-Label Temporal Supervision
The temporal structure in chronological document collections provides free supervision without manual annotation. Predictions are made at time t using only information available ≤t; outcomes are resolved by observing documents from time s>t. This creates verifiable labels from naturally occurring disclosures rather than human judgment.

### Mechanism 2: Task-Aligned Probabilistic Fine-Tuning
Training directly on outcome-resolved probability estimation produces better calibration than repurposing classification-trained models. The model outputs scalar probabilities and is trained to minimize expected Brier score via GRPO, directly optimizing for calibrated probabilistic forecasts.

### Mechanism 3: Synthetic Query Generation from Risk Narratives
LLM-based extraction converts unstructured risk disclosures into falsifiable, time-bounded natural language propositions at scale. A RAG pipeline retrieves Risk Factors sections from SEC filings; Gemini-2.5-Flash generates concrete, falsifiable queries specifying both event definition and prediction horizon.

## Foundational Learning

- **Temporal masking / information leakage prevention**: Critical for ensuring predictions only use information available at query time. Quick check: If you shuffle the dataset and train on a random subset, does performance change? (It should not if temporal masking is correct.)

- **Brier score and calibration metrics (ECE, BSS)**: Essential for interpreting the paper's central claim of improved calibration over frontier models. Quick check: A model predicts p=0.7 for 100 events, 70 of which materialize. Is this calibrated? (Yes, but calibration alone doesn't measure discrimination—check reliability diagrams.)

- **Retrieval-augmented generation for structured extraction**: Both query generation and outcome resolution use RAG. Understanding RAG retrieval quality and failure modes is critical for diagnosing pipeline errors. Quick check: If retrieval fails to surface the relevant passage from a future filing, will the label be wrong? (Yes—retrieval quality directly impacts supervision quality.)

## Architecture Onboarding

- **Component map**: Filing ingestion -> Risk Factors extraction -> Query generation (RAG + Gemini) -> Outcome resolution (future filing RAG) -> Labeled dataset -> GRPO fine-tuning -> Calibrated probability model

- **Critical path**: Filing ingestion quality determines query quality; retrieval over future filings determines label accuracy; GRPO hyperparameters determine final calibration

- **Design tradeoffs**: Summary-level vs. full Risk Factors input (consistency vs. nuance); automated vs. manual labeling (scale vs. noise); compact model (deployment vs. reasoning capacity)

- **Failure signatures**: High Brier score + low ECE (calibrated but uninformative); low Brier score + high ECE (discriminative but overconfident); performance varies by risk category; training loss decreases but held-out Brier score increases (overfitting)

- **First 3 experiments**:
  1. Baseline replication: Run the open-sourced evaluation dataset through naive baseline and pretrained Qwen3-32B to reproduce reported Brier scores and ECE
  2. Ablate temporal masking: Train on the same data but without enforcing temporal ordering to quantify information leakage contribution
  3. Category-stratified analysis: Evaluate performance separately on discrete-event categories vs. financial-outcome categories

## Open Questions the Paper Calls Out

### Open Question 1
Can Foresight Learning generalize to domains with less structured or non-mandatory narrative reporting? The study is currently confined to the highly structured environment of SEC filings. Successful replication in voluntary disclosure settings or non-financial domains would resolve this.

### Open Question 2
How does model calibration degrade as the prediction horizon extends beyond the short-term windows tested? The current evaluation uses a fixed horizon, ignoring slow-burning risks. Performance metrics on specifically long-horizon risk queries would resolve this.

### Open Question 3
To what extent does the model predict event materialization versus merely predicting disclosure behavior? The pipeline currently lacks a mechanism to distinguish between unreported events and non-events. Validation against external ground truth datasets would resolve this.

## Limitations
- Reliance on automated outcome resolution from SEC filings introduces label noise from inconsistent disclosure practices
- The 6,109-query dataset may not fully capture the diversity of materialized risks, particularly those disclosed outside formal filings
- Single economic cycle (2024) limits generalizability across market conditions

## Confidence
- **High Confidence**: Temporal supervision mechanism and its implementation, Brier score optimization framework, existence and basic format of test dataset
- **Medium Confidence**: Quality and consistency of automated label generation, sufficiency of 5,609 training examples, absence of information leakage in temporal splits
- **Low Confidence**: Generalizability across economic cycles, performance on risk categories not well-represented in training data, impact of disclosure bias on learned risk representations

## Next Checks
1. **Temporal leakage audit**: Construct synthetic dataset with delayed outcome resolution and measure performance degradation to verify temporal masking effectiveness
2. **Disclosure consistency validation**: Manually audit 50 materialized risk cases against subsequent filings to quantify label reliability and identify systematic disclosure patterns
3. **Economic cycle robustness test**: Evaluate on SEC filings from 2019-2020 to assess performance stability across different market conditions and risk disclosure patterns