---
ver: rpa2
title: 'SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning
  for LLM Reasoning'
arxiv_id: '2506.08989'
source_url: https://arxiv.org/abs/2506.08989
tags:
- problems
- problem
- training
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SwS addresses the challenge of generating effective training data\
  \ for reinforcement learning in large language models by identifying and synthesizing\
  \ problems based on the model\u2019s weaknesses. It first trains the model on an\
  \ initial dataset and records the problems it consistently fails to learn, treating\
  \ these as its weaknesses."
---

# SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning

## Quick Facts
- arXiv ID: 2506.08989
- Source URL: https://arxiv.org/abs/2506.08989
- Authors: Xiao Liang; Zhong-Zhi Li; Yeyun Gong; Yang Wang; Hengyuan Zhang; Yelong Shen; Ying Nian Wu; Weizhu Chen
- Reference count: 40
- Primary result: SwS improves 7B models by 10.0% and 32B models by 7.7% average on 8 math benchmarks

## Executive Summary
SwS addresses the challenge of generating effective training data for reinforcement learning in large language models by identifying and synthesizing problems based on the model's weaknesses. It first trains the model on an initial dataset and records the problems it consistently fails to learn, treating these as its weaknesses. Using these failure cases, SwS extracts underlying concepts and synthesizes new problems tailored to target and strengthen the model's weak areas. The generated problems are filtered by quality, solvability, and difficulty to ensure they are useful for training. When incorporated into subsequent RL training, SwS improves model performance by an average of 10.0% on 7B models and 7.7% on 32B models across eight mathematical reasoning benchmarks, surpassing models trained on curated human-labeled data.

## Method Summary
SwS operates through a multi-stage pipeline: initial RL training to identify weaknesses (problems with max accuracy < 50% and negative slope), concept extraction from failure cases, concept recombination using co-occurrence statistics and embedding similarity, problem generation via instruction LLM, quality filtering (concept coverage, factual accuracy, solvability), answer labeling through self-consistency (≥50% agreement), difficulty filtering (accuracy in [25%, 75%]), and augmented RL training on merged synthetic and seed datasets. The approach uses GRPO with group-relative rewards and dynamic sampling constraints to maintain learning signal.

## Key Results
- 7B models show 10.0% average improvement across 8 benchmarks
- 32B models show 7.7% average improvement across 8 benchmarks
- Outperforms models trained on curated human-labeled data
- Successfully identifies and targets specific reasoning weaknesses through synthetic problem generation

## Why This Works (Mechanism)

### Mechanism 1
RL training dynamics expose model weaknesses more reliably than base-model sampling or SFT-based evaluation. During initial RL, problems with persistently low accuracy (<50%) and negative accuracy slope indicate fundamental capability gaps rather than prompt-alignment issues. The iterative sampling and gradient updates reveal which problems the model cannot learn, not just which it currently fails.

### Mechanism 2
Concept extraction from failure cases and recombination via co-occurrence/embedding similarity yields semantically coherent, targeted synthetic problems. Failed problems are decomposed into concepts, which are then sampled and recombined within the same category using co-occurrence statistics and embedding similarity. This maintains semantic coherence while generating novel problem variants that stress the same weak reasoning pathways.

### Mechanism 3
Difficulty-filtered synthetic problems maintain learning signal in GRPO by avoiding gradient collapse from all-correct or all-incorrect batches. GRPO computes advantages via group-relative rewards. When all responses are correct/incorrect, advantages collapse to zero, halting learning. Filtering problems to 25–75% accuracy ensures variance in responses, sustaining gradient signal.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**
  - Why needed here: SwS relies on GRPO's group-relative advantage computation; understanding how advantages collapse without variance is critical to designing the difficulty filter.
  - Quick check question: Can you explain why GRPO advantages become zero when all sampled responses for a problem are correct?

- **RLVR (Reinforcement Learning with Verifiable Rewards)**
  - Why needed here: SwS operates within RLVR, where rewards are binary (correct/incorrect final answers). This shapes how weaknesses are identified and why answer verification quality matters.
  - Quick check question: How does verifiable-reward RL differ from reward-model-based RL, and why does it require precise ground-truth answers?

- **Concept-based Problem Synthesis**
  - Why needed here: SwS extracts and recombines concepts from failure cases. Understanding co-occurrence statistics and embedding similarity is necessary to implement the sampling strategy.
  - Quick check question: If two concepts frequently co-occur in training problems but have low embedding similarity, would the synthesis pipeline prioritize or avoid combining them?

## Architecture Onboarding

- **Component map:** Initial RL Training -> Failure Analysis -> Concept Recombination -> Problem Generation -> Answer Labeling -> Difficulty Filtering -> Augmented RL Training

- **Critical path:** Weakness identification quality → concept relevance → synthetic problem quality → difficulty filtering precision → augmented training signal strength. If any stage is noisy, downstream stages degrade.

- **Design tradeoffs:**
  - Weakness threshold: Stricter thresholds yield fewer but higher-confidence failure cases; looser thresholds increase coverage but add noise.
  - Synthesis budget allocation: Proportional to failure rate targets weakest categories but may over-sample narrow weaknesses.
  - Difficulty range: Narrower ranges provide cleaner gradient signal but reduce usable problems; wider ranges increase data but risk saturation.

- **Failure signatures:**
  - Low synthetic problem retention after quality/difficulty filtering indicates generator or verifier issues.
  - No improvement on originally-failed problems after augmented training suggests concepts were not correctly extracted or recombined.
  - Training accuracy spikes then plateaus early may indicate difficulty range is too narrow or problems are too simple.

- **First 3 experiments:**
  1. Weakness identification ablation: Compare failure sets from base-model sampling, SFT-model evaluation, and initial RL training on a held-out subset. Measure correlation between failure sets and post-training improvement.
  2. Concept recombination sanity check: Manually inspect 50 synthetic problems from high-failure-rate categories. Verify semantic coherence and concept coverage; adjust co-occurrence/embedding weights if incoherent.
  3. Difficulty filter sensitivity: Train three augmented policies with difficulty ranges [15–85%], [25–75%], [35–65%]. Compare training dynamics and evaluation gains to identify optimal range for the base model size.

## Open Questions the Paper Calls Out

1. **How can the upper bound of difficulty in synthetic problems be effectively raised, particularly in self-evolving settings?** The paper notes that raising the difficulty upper bound for problems generated by instruction models "remains an open problem," particularly for elite competition benchmarks like AIME.

2. **Can the SwS framework be extended to general tasks that lack precise, verifiable reference answers?** The current methodology is restricted to Reinforcement Learning with Verifiable Rewards (RLVR), which depends on ground-truth answers unavailable in open-ended domains.

3. **How can the weakness-driven pipeline be adapted for supervised fine-tuning (SFT) or knowledge distillation?** The current study focuses exclusively on Reinforcement Learning, leaving the utility of weakness-driven data for static supervised training unexplored.

## Limitations

- **Concept extraction reliability:** The pipeline depends on LLM-extracted concepts being semantically accurate and non-redundant, with no quantitative evaluation of concept fidelity provided.
- **Difficulty estimation drift:** Difficulty filtering assumes initial RL model's accuracy estimates remain valid during augmented training, without dynamic re-estimation.
- **Generalizability to non-mathematical domains:** The approach is validated only on mathematical reasoning benchmarks, with effectiveness for other verifiable-reward tasks untested.

## Confidence

- **High:** Weakness identification via RL failure patterns, GRPO-based difficulty filtering rationale, benchmark performance gains.
- **Medium:** Concept extraction + recombination mechanism, self-consistency answer labeling reliability, synthetic problem quality retention.
- **Low:** Generalizability beyond mathematical reasoning, optimal difficulty range sensitivity, concept extraction fidelity.

## Next Checks

1. **Concept fidelity audit:** Manually inspect 100 extracted concept sets from high-failure-rate categories. Measure precision/recall against ground-truth concept annotations or expert review. Adjust LLM prompts/co-occurrence weights if >15% incoherence.

2. **Difficulty drift monitoring:** During augmented RL, sample 100 synthetic problems every 100 steps. Re-estimate accuracy using the current policy. Plot accuracy distribution over training; if >20% of problems exit [25%, 75%], implement dynamic re-filtering.

3. **Domain generalization probe:** Apply SwS to a non-mathematical RLVR task (e.g., coding verification or factual QA). Compare synthetic problem quality and training gains against a baseline that synthesizes problems without weakness-driven sampling.