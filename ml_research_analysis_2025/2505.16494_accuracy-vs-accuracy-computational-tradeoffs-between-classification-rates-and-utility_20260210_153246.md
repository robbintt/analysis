---
ver: rpa2
title: 'Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates
  and Utility'
arxiv_id: '2505.16494'
source_url: https://arxiv.org/abs/2505.16494
tags:
- loss
- predictor
- decision
- definition
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the computational tradeoffs between accuracy
  in classification and utility maximization in machine learning, particularly in
  contexts involving fairness considerations. The authors investigate settings where
  training data contains richer labels, such as individual types, rankings, or risk
  estimates, rather than just binary outcomes.
---

# Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility

## Quick Facts
- arXiv ID: 2505.16494
- Source URL: https://arxiv.org/abs/2505.16494
- Reference count: 40
- One-line primary result: Achieving both accurate classification rates and optimal loss minimization simultaneously is computationally infeasible in some settings, requiring a choice between two natural notions of accuracy.

## Executive Summary
This paper explores the fundamental computational tradeoffs between accuracy in classification and utility maximization in machine learning, particularly in contexts involving fairness considerations. The authors investigate settings where training data contains richer labels (individual types, rankings, or risk estimates) rather than just binary outcomes. They propose algorithms that achieve stronger notions of evidence-based fairness while preserving accurate subpopulation classification rates. The core insight is that while each notion of accuracy can be satisfied individually via efficient learning, combining them leads to computational impossibilities. These findings present a choice between two natural and attainable notions of accuracy that could both be motivated by fairness considerations.

## Method Summary
The paper proposes a framework that leverages multi-accurate and multi-calibrated predictors to obtain both decision-accuracy and classification-accuracy, as well as loss minimization. The approach involves learning predictors that match marginal distributions across subpopulations (multi-accuracy) or achieve full calibration across all subgroups (multi-calibration). The methods support classification that preserve accurate subpopulation classification rates, particularly for affine decision rules. The authors also present impossibility results demonstrating that simultaneously achieving accurate classification rates and optimal loss minimization is computationally infeasible in some cases, stemming from the difficulty of learning a sufficiently good approximation of the Bayes-optimal predictor that satisfies both desiderata.

## Key Results
- Multi-accuracy guarantees decision-accuracy for affine decision rules, preserving classification rates across subpopulations
- Computational hardness results show simultaneous achievement of calibration and decision-accuracy is infeasible for non-affine rules like loss minimizers
- The impossibility stems from indistinguishability problems that force calibrated predictors to "squash" probabilities, violating decision-accuracy
- Path-dependent utility: a single multi-calibrated predictor can yield either optimal loss minimization or accurate classification rates, but not both simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Affine Preservation of Accuracy
If a decision rule is affine (linear in expectation), a multi-accurate predictor guarantees that the classification rates observed by subpopulations match the Bayes-optimal rates. Affine rules operate by sampling a type and applying a fixed function. Because they are linear, the expected decision outcome for a group is simply the expectation over the predicted types. If the predictor is multi-accurate (marginals match the truth), the decision outcomes must also match. Core assumption: The decision rule is affine or sufficiently close to affine (Definition 2.13). Evidence anchors: Theorem 3.4 proves that multi-accuracy implies decision-accuracy for affine rules.

### Mechanism 2: Computational Hardness via Indistinguishability
Simultaneously achieving calibration and decision-accuracy is computationally infeasible for non-affine rules (like loss minimizers), even though both are individually achievable. The authors leverage the existence of One-Way Functions (OWFs) to construct distributions with "indistinguishable subsets." An efficient learner cannot distinguish inputs that belong to different types, forcing any calibrated predictor to "squash" their probabilities (e.g., output 0.5 for both). This violates decision-accuracy for rules that require differentiating these types. Core assumption: One-way functions exist (standard cryptographic assumption). Evidence anchors: Theorem 4.1 (Section 4) formalizes the separation using indistinguishable subsets (Definition 2.17).

### Mechanism 3: Path-Dependent Utility vs. Fairness
A single multi-calibrated predictor can yield either optimal loss minimization or accurate classification rates, depending on the post-processing path chosen. Loss Minimization: Applying the loss-minimizing decision rule directly to the full multi-calibrated distribution minimizes expected loss (Theorem 6.1). Classification Accuracy: Randomly instantiating the predictor (sampling a type) before applying the decision rule preserves classification rates (Corollary 3.10). These paths diverge because the loss-minimizing rule is non-affine, triggering the hardness results if one tries to achieve both simultaneously. Core assumption: Nature is deterministic (rich labels), allowing the instantiation strategy to map back to true types.

## Foundational Learning

- **Concept: Rich Labels (Types/Rankings)**
  - Why needed here: The paper moves beyond binary outcomes (0/1) to "rich labels" (e.g., probabilities, risk scores). This resolves the inherent uncertainty in binary data, allowing the model to target the Bayes-optimal predictor directly rather than guessing from noisy outcomes.
  - Quick check question: Does your training data represent individual risk/merit as a continuous score or ranking, rather than just a binary success/fail flag?

- **Concept: Affine vs. Non-Affine Decision Rules**
  - Why needed here: The feasibility of the paper's algorithms depends entirely on this distinction. Affine rules allow for efficient "best of both worlds" solutions; non-affine rules (like thresholds) force a computationally hard tradeoff.
  - Quick check question: Does your downstream decision rule rely on linear weightings of features, or does it apply hard thresholds/step-functions based on expected utility?

- **Concept: Multi-Calibration (MC)**
  - Why needed here: MC acts as the universal adapter in this architecture. While Multi-Accuracy is cheaper, MC is the prerequisite for the "Omnipredictor" property (minimizing arbitrary losses) and for robust fairness guarantees on complex subgroups.
  - Quick check question: Can you verify that your predictor is calibrated not just globally, but for every significant subpopulation defined by your feature set?

## Architecture Onboarding

- **Component map:** Rich Label Dataset -> Agnostic/Multi-Calibration module -> Predictor $\tilde{p}$ -> Router (Path A: Direct Loss-Minimizing Rule -> Action OR Path B: Random Instantiation -> Decision Rule -> Action)

- **Critical path:** The training of the **(C, $\alpha$, $\lambda$)-Full-Multi-Calibrated Predictor** (Section 6). If this component is weak (low $\alpha$, high $\lambda$), neither the utility nor the fairness guarantees hold.

- **Design tradeoffs:**
  - **Utility vs. Classification Rate Consistency:** You must choose Path A or Path B at deployment. You cannot efficiently have a classifier that both minimizes loss and perfectly replicates the Bayes-optimal classification rates for non-trivial losses (Theorem 5.1).
  - **Sample Complexity:** Full multi-calibration can be expensive (exponential in labels). Relaxations (Section 6.1) are required for practical scaling, but they narrow the class of supported loss functions.

- **Failure signatures:**
  - **"Squashing" on Indistinguishable Inputs:** If the model outputs high-uncertainty predictions (e.g., 0.5) for a subgroup that the Bayes-optimal model would clearly distinguish, it is hitting the computational barrier described in Theorem 4.1.
  - **Conflict Errors:** Attempting to enforce both MAC (Multi-Accuracy-on-Classification) and optimal Loss Minimization simultaneously will cause one or both metrics to degrade significantly.

- **First 3 experiments:**
  1. **Decision Rule Profiling:** Determine if your primary decision rule is affine. If yes, implement standard Multi-Accuracy to avoid the complexity of full calibration.
  2. **Path Isolation A/B Test:** Deploy Path A (Direct) and Path B (Instantiation) on a validation set. Verify that Path A has lower loss, while Path B has classification rates closer to the Bayes-optimal benchmark.
  3. **Indistinguishability Audit:** Test if specific subgroups defined by sensitive attributes are being mapped to "squashed" probability ranges, indicating the learner cannot efficiently distinguish types within that subgroup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the impossibility results be extended to settings with non-binary actions (multi-class or continuous action spaces)?
- Basis in paper: [explicit] The paper states: "More generally, decision functions or classifiers may have more than two possible actions, but we restrict attention to the binary case for simplicity."
- Why unresolved: The technical arguments for impossibility (Theorems 4.1 and 5.1) rely on binary decision rules and the affine/non-affine characterization, which may not directly generalize.
- What evidence would resolve it: Proofs extending the hardness results to $k$-ary action spaces for $k > 2$, or positive algorithms that achieve simultaneous accuracy desiderata in multi-action settings.

### Open Question 2
- Question: What guarantees can be obtained when Nature is probabilistic rather than deterministic (with rich labels)?
- Basis in paper: [explicit] Remark 3.13 states: "A randomized Nature with such rich labels poses a greater challenge than the standard agnostic learning model, and thus we cannot hope to achieve the strong fairness guarantees sought in this work unless Nature is assumed to be deterministic."
- Why unresolved: The positive results (e.g., Corollary 3.10 for MAC) explicitly assume deterministic Nature, and the techniques may not extend when the true label distribution itself has irreducible uncertainty.
- What evidence would resolve it: Algorithms achieving decision-accuracy or classification-accuracy under probabilistic Nature, or impossibility results characterizing the limits.

### Open Question 3
- Question: Are there additional efficient relaxations of multi-calibration beyond the three settings identified (linear losses, fixed single loss, family of losses)?
- Basis in paper: [inferred] Section 6.1 presents three specific relaxations with different tradeoffs between outcome space generality and prior knowledge of losses, but does not claim completeness. The authors note full multi-calibration has exponential sample complexity in $k$.
- Why unresolved: The space of possible calibration relaxations is not fully characterized; there may be other practically relevant settings where efficient calibration suffices.
- What evidence would resolve it: A unified characterization of which loss/outcome space combinations admit efficient loss-minimizing predictors, with matching upper and lower bounds.

### Open Question 4
- Question: Can the cryptographic hardness assumptions be weakened or replaced with standard complexity-theoretic assumptions?
- Basis in paper: [explicit] Theorems 4.1 and 5.1 assume "the existence of one-way functions." The paper also mentions an alternative "information-theoretic approach" when no restriction is placed on Nature's complexity.
- Why unresolved: The reliance on cryptographic assumptions (rather than, e.g., $\mathsf{P} \neq \mathsf{NP}$) leaves open whether the separations are inherent or artifact of the proof technique.
- What evidence would resolve it: Proofs under weaker/falsifiable assumptions, or evidence that cryptographic assumptions are necessary for the construction.

## Limitations
- The computational hardness results rely on the existence of One-Way Functions, which remains an unproven assumption
- The rich label requirement may not hold in many practical settings where only binary outcomes are observed
- The impossibility results are conditional on cryptographic foundations, limiting direct applicability

## Confidence
- **High confidence** in the affine rule preservation mechanism (Theorem 3.4) - the mathematical proof is straightforward and the conditions are clearly stated
- **Medium confidence** in the computational hardness results (Theorem 4.1) - while the proof structure is sound, it depends on cryptographic assumptions that cannot be verified empirically
- **Medium confidence** in the path-dependent utility claims (Section 6) - the theoretical framework is robust, but empirical validation across diverse real-world datasets would strengthen the findings

## Next Checks
1. **Dataset Richness Validation:** Test the algorithms on datasets with varying levels of label richness (from binary outcomes to continuous risk scores) to quantify the degradation in performance as label information decreases
2. **Decision Rule Impact Study:** Systematically vary the decision rule from affine to increasingly non-affine (threshold-based) functions to empirically demonstrate the transition point where computational barriers emerge
3. **Cryptographic Assumption Relaxation:** Investigate whether the hardness results persist under weaker cryptographic assumptions or with bounded adversaries, potentially revealing practical middle grounds between the two accuracy notions