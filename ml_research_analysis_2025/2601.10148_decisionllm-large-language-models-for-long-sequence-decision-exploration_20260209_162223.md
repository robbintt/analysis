---
ver: rpa2
title: 'DecisionLLM: Large Language Models for Long Sequence Decision Exploration'
arxiv_id: '2601.10148'
source_url: https://arxiv.org/abs/2601.10148
tags:
- data
- decisionllm
- trajectory
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DecisionLLM, a method for applying large language
  models (LLMs) to long-horizon decision-making tasks by treating trajectories as
  a distinct data modality. The approach addresses LLMs' poor handling of continuous
  values by using a trajectory encoder to process state, action, and reward sequences,
  then aligning them with text-based task descriptions.
---

# DecisionLLM: Large Language Models for Long Sequence Decision Exploration

## Quick Facts
- arXiv ID: 2601.10148
- Source URL: https://arxiv.org/abs/2601.10148
- Reference count: 33
- DecisionLLM-3B outperforms Decision Transformer by 69.4 points on Maze2D and 0.085 on AuctionNet

## Executive Summary
DecisionLLM introduces a novel approach to applying large language models to long-horizon decision-making tasks by treating trajectories as a distinct data modality. The method addresses LLMs' poor handling of continuous values by using a trajectory encoder to process state, action, and reward sequences, then aligning them with text-based task descriptions. DecisionLLM achieves strong performance on Maze2D and AuctionNet benchmarks, with DecisionLLM-3B outperforming Decision Transformer by 69.4 points on Maze2D and 0.085 on AuctionNet. The work establishes scaling laws showing performance depends on model size, data volume, and data quality, and demonstrates that high-quality data filtering is critical for success.

## Method Summary
DecisionLLM treats trajectories as a distinct modality by encoding continuous state, action, and return-to-go sequences into dense vector embeddings using a trajectory encoder. These embeddings replace special placeholder tokens (`<\|traj begin|>`, `<\|traj end|>`) within the LLM's text prompt, injecting trajectory context directly into the model's attention stream. The model is trained autoregressively to predict future actions given historical trajectories, using MSE loss between predicted and ground truth actions. Data quality filtering removes low-return trajectories and applies step-level reweighting to improve performance.

## Key Results
- DecisionLLM-3B achieves 142.26 normalized score on Maze2D, outperforming Decision Transformer by 69.4 points
- On AuctionNet benchmark, DecisionLLM-3B achieves 0.398 score, outperforming Decision Transformer by 0.085
- Performance scales with model size (0.5B→1.5B→3B) and benefits from high-quality data filtering
- Autoregressive decision modeling with trajectory-text alignment outperforms text-only prompt approaches

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-Text Modality Alignment
Treating continuous trajectory data as a distinct non-textual modality, rather than serializing it as text, enables LLMs to capture fine-grained numerical relationships required for decision-making. A dedicated trajectory encoder converts sequences of states, actions, and returns-to-go into continuous vector embeddings that replace special placeholder tokens within the LLM's text prompt.

### Mechanism 2: Autoregressive Decision Modeling
Framing offline reinforcement learning as an autoregressive sequence prediction task allows the model to leverage the LLM's native next-token prediction capabilities. The model predicts actions at timestep t conditioned on trajectory history up to t and task description using MSE loss.

### Mechanism 3: Quality-Aware Data Scaling
Performance in this imitation learning paradigm is more sensitive to data quality than to data quantity. The method employs dual-level data curation: trajectory-level filtering to exclude low-return episodes and step-level re-weighting for low-reward steps within valid trajectories.

## Foundational Learning

**Transformer Attention and Embeddings**: The model processes "tokens" (converted to embeddings) and uses attention to weigh their importance. Core innovation is injecting a non-text embedding into this flow.

*Why needed here*: This is the engine of the LLM. You must understand that the model processes "tokens" (converted to embeddings) and uses attention to weigh their importance.

*Quick check question*: How does an LLM handle an input embedding that does not correspond to any word in its vocabulary? (Answer: It processes it as a vector; the model doesn't "know" it's not a word, it just attends to its features).

**Offline Reinforcement Learning (RL) & MDPs**: The task is defined as a Markov Decision Process (States, Actions, Rewards). You need to know what a trajectory is (s_t, a_t, r_t) and what "returns-to-go" means.

*Why needed here*: The task is defined as a Markov Decision Process (States, Actions, Rewards). You need to know what a trajectory is (s_t, a_t, r_t) and what "returns-to-go" means.

*Quick check question*: In an offline RL dataset, why is distributional shift a major problem? (Answer: The agent may encounter states not well-covered in the static dataset, leading to errors that compound over time).

**Modality Alignment in Multimodal Models**: The paper explicitly draws inspiration from models that align images with text. Understanding that a "projection layer" maps one data type's vector space (trajectory) into another's (LLM text space) is critical.

*Why needed here*: The paper explicitly draws inspiration from models that align images with text. Understanding that a "projection layer" maps one data type's vector space (trajectory) into another's (LLM text space) is critical.

*Quick check question*: What is the role of the special `<\|traj begin|>` and `<\|traj end|>` tokens? (Answer: They act as "slots" in the sequence where the projected trajectory embeddings are inserted, allowing the LLM to attend to them as if they were a sequence of special tokens).

## Architecture Onboarding

**Component map**: Pre-trained LLM Backbone -> Trajectory Encoder -> Special Token Replacement -> LLM Attention -> Action Prediction Head

**Critical path**: The pipeline hinges on replacing the embeddings of the special placeholder tokens with the output of the Trajectory Encoder. If this replacement or the projection dimension mismatches, the entire mechanism fails.

**Design tradeoffs**: The paper proves that simple text tokenization of numbers leads to representation collapse. The tradeoff is the added complexity of a custom data loader and encoder versus the simplicity of a pure text prompt.

**Failure signatures**: If the model ignores the trajectory data, check the t-SNE plots of the embeddings. They should form distinct clusters; if they are a single blob, the modality alignment has failed.

**First 3 experiments**:
1. **Sanity Check - Text Prompt Baseline**: Try feeding trajectory data as raw text into the base LLM. Observe the failure mode (poor score, random actions) to validate the need for a specialized encoder.
2. **Ablation - Modality Injection**: Implement the Trajectory Encoder and inject the embeddings. Compare the t-SNE visualization of trajectory embeddings from this model against the text-prompt baseline to confirm the embeddings are structured and separable.
3. **Ablation - Data Quality**: Train the full DecisionLLM model on the raw dataset vs. the filtered dataset (excluding trajectories with reward < ε). Measure the delta in normalized score to validate the data quality scaling law.

## Open Questions the Paper Calls Out

**Can DecisionLLM be effectively adapted for online bidding scenarios?**
The abstract states the work "points to promising directions for future exploration in online bidding." The current study focuses exclusively on offline benchmarks using static datasets.

**Can reinforcement learning (RL) fine-tuning overcome the identified supervised fine-tuning (SFT) performance ceiling?**
Section 4.3.1 notes that the "imitation learning-based supervised fine-tuning (SFT) paradigm has a performance ceiling." The paper optimizes the model using SFT, leaving the potential for RL algorithms to surpass this ceiling unexplored.

**Does the trajectory-text alignment mechanism generalize to complex, high-dimensional robotics tasks?**
The introduction references success in robotics and autonomous driving, but evaluation is limited to Maze2D and AuctionNet. It is unclear if the trajectory encoder handles high-dimensional state spaces as effectively as the lower-dimensional benchmarks tested.

## Limitations

**Limited empirical scope**: Results are demonstrated on only two domains (Maze2D and AuctionNet), both involving relatively low-dimensional continuous control.

**Unknown implementation details**: The trajectory encoder's layer dimensions are unspecified, and the "loss-opt-v3" down-weighting formula is not provided, limiting exact reproducibility.

**Architectural novelty assessment constrained**: The paper does not compare against other modality alignment techniques that might achieve similar results with fewer modifications.

## Confidence

**High confidence** in the trajectory-text alignment mechanism: Clear ablation evidence shows text-based trajectory encoding leads to representation collapse, while the proposed encoder produces structured embeddings with compelling performance gains.

**Medium confidence** in the scaling laws: While data filtering experiments show strong effects, the model scale experiments only cover 0.5B to 3B parameters on a single task.

**Low confidence** in AuctionNet results: The benchmark setup is briefly described and the prompt template is not provided, preventing independent verification of the 0.085-point improvement.

## Next Checks

1. **Encoder architecture ablation**: Implement alternative trajectory encoders (single linear layer vs. MLP, different output dimensions) and measure the impact on embedding structure (t-SNE) and downstream task performance.

2. **Data quality filtering granularity**: Extend the filtering beyond trajectory-level to include per-timestep reward thresholds or KL-divergence-based novelty penalties. Compare performance against the published trajectory-level filtering.

3. **Prompt template sensitivity**: Systematically vary the text prompt structure (reordering task description and trajectory context, changing special token placement) while keeping the encoder fixed. Measure performance variance to determine how much of the success depends on prompt engineering versus the core modality alignment mechanism.