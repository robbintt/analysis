---
ver: rpa2
title: Understanding Data Influence with Differential Approximation
arxiv_id: '2508.14648'
source_url: https://arxiv.org/abs/2508.14648
tags:
- influence
- data
- training
- diff-in
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diff-In, a novel influence estimation method
  that formulates sample influence as the cumulative sum of its changes across training
  iterations. Unlike previous methods that rely on convexity assumptions or fail to
  capture training dynamics, Diff-In uses second-order approximations to compute these
  changes efficiently through Hessian-gradient products approximated via finite differences.
---

# Understanding Data Influence with Differential Approximation

## Quick Facts
- **arXiv ID**: 2508.14648
- **Source URL**: https://arxiv.org/abs/2508.14648
- **Reference count**: 40
- **Primary result**: Diff-In achieves 91.4% precision in identifying mislabeled data on ImageNet, outperforming prior methods by 13 percentage points

## Executive Summary
This paper introduces Diff-In, a novel influence estimation method that reformulates sample influence as the cumulative sum of its changes across training iterations. Unlike previous methods that rely on convexity assumptions or fail to capture training dynamics, Diff-In uses second-order approximations to compute these changes efficiently through Hessian-gradient products approximated via finite differences. The method achieves strong scalability while maintaining accuracy comparable to first-order methods. Extensive experiments across three data-centric tasks—data cleaning, data deletion, and coreset selection—demonstrate Diff-In's superior performance with up to 91.4% precision in identifying mislabeled data compared to 78.4% for prior best methods.

## Method Summary
Diff-In computes influence by decomposing the leave-one-out parameter change into a sum of temporal differences across training iterations. For each checkpoint, it approximates Hessian-gradient products using finite differences of first-order gradients, avoiding explicit Hessian computation. The method samples m uniformly distributed checkpoints (typically 5) during training, computes gradients for both the target sample and a batch proxy, and accumulates the influence contributions. This temporal decomposition enables second-order approximation without convexity assumptions while maintaining O(p) complexity through efficient Hessian-gradient product computation.

## Key Results
- Achieves 91.4% precision in identifying mislabeled data on ImageNet compared to 78.4% for prior best methods
- Reaches 89.9% accuracy in data deletion for LLMs versus 82.5% for state-of-the-art alternatives
- Consistently outperforms CLIP-score by 1.3-2.1 percentage points in coreset selection across multiple downstream vision-language tasks

## Why This Works (Mechanism)

### Mechanism 1: Temporal Decomposition of Influence
Reformulating influence as cumulative temporal differences enables second-order approximation without convexity assumptions. Rather than estimating the total parameter change from removing a sample (which requires strong assumptions), Diff-In decomposes this into a sum of per-timestep differences: I(θ) = Σ[I^{t+1}(z) - I^t(z)]. Each difference term operates locally in parameter space, where second-order Taylor approximations remain valid even in non-convex regions. The core assumption is that the loss landscape is sufficiently smooth (ℓ-Lipschitz continuous gradients) that local quadratic approximations are meaningful across consecutive training steps.

### Mechanism 2: Efficient Hessian-Gradient Products via Finite Differences
Computing H_t G_t as (∇L(θ + εG) - ∇L(θ))/ε avoids explicit Hessian computation while preserving second-order information. The finite difference trick exploits that HG = d(∇L)/dθ · G ≈ (∇L(θ+εG) - ∇L(θ))/ε. This requires only two gradient evaluations per checkpoint, achieving O(p) complexity instead of O(p²) for explicit Hessian computation. The core assumption is that the gradient function is differentiable and the perturbation ε is small enough for accurate approximation but large enough to avoid numerical instability.

### Mechanism 3: Checkpoint Sampling for Practical Scalability
Uniform sampling of m checkpoints (typically 3-10) provides sufficient coverage of training dynamics while keeping computation tractable. Rather than summing over all T training iterations, Diff-In samples m uniformly distributed checkpoints. Empirically, the contribution from unsampled timesteps averages out, and the "last checkpoint only" variant (Diff-In-F) often captures dominant influence patterns. The core assumption is that training dynamics are sufficiently regular that sparse temporal sampling doesn't miss critical influence events.

## Foundational Learning

- **Concept: Influence Functions (Cook's Distance)**
  - **Why needed here**: Diff-In's target metric is the leave-one-out influence: how much model parameters change when removing a sample. Understanding this definition is essential to see why convexity assumptions matter.
  - **Quick check question**: If removing training sample z causes θ* to change by Δθ, what is I_θ(z)? Can you explain why computing this exactly is impractical for large datasets?

- **Concept: Hessian-Vector Products (HVPs)**
  - **Why needed here**: Diff-In computes Hessian-gradient products without explicit Hessian inversion. Understanding that Hv can be computed in O(p) via finite differences or Pearlmutter's trick is crucial.
  - **Quick check question**: Given a function f(θ) with gradient g = ∇f(θ), how would you approximately compute H·g without materializing the Hessian matrix H?

- **Concept: Finite Difference Approximations**
  - **Why needed here**: The core efficiency trick relies on (f(x+εh) - f(x))/ε ≈ ∇f·h. Understanding accuracy vs. numerical stability tradeoffs helps debug implementation issues.
  - **Quick check question**: If ε is too small, what numerical problem arises? If ε is too large, what approximation error dominates?

## Architecture Onboarding

- **Component map**: Checkpoint manager -> Gradient computer -> Finite difference engine -> Influence accumulator
- **Critical path**: 1) Train model, saving m checkpoints (e.g., every T/m epochs); 2) For each sample z to evaluate: load each checkpoint θ_t, compute sample gradient G^t_z and batch gradient G^t_B, apply finite difference to get H^t_B G^t_z and H^t_z G^t_B, accumulate D_t(z) = -(t·η²_t/N)(H^t_B G^t_z + H^t_z G^t_B); 3) Return summed influence scores
- **Design tradeoffs**: m (checkpoint count): Higher m → better accuracy but linear cost increase. Diminishing returns after m ≈ 5. ε (perturbation scale): Too small → numerical noise; too large → approximation error. Start with ε ≈ 1e-3 to 1e-5. Batch proxy size: Using random batch vs. full dataset for G^t_B. Larger batches more accurate but slower; 512-2048 suggested. Last-checkpoint-only (Diff-In-F): ~5x faster, slightly lower accuracy. Good for quick scans or very large datasets.
- **Failure signatures**: 1) NaN/Inf values: Usually ε too small causing numerical instability, or gradient explosion in large models. 2) All influence scores near zero: Checkpoint selection may have missed relevant training phases, or gradients are vanishing. 3) Influence scores correlate poorly with LOO ground truth: Model may not satisfy smoothness assumptions; try larger m or check optimizer compatibility (Adam requires special handling per Section 4.3)
- **First 3 experiments**: 1) Validation on small dataset: Train ResNet-18 on CIFAR-10 subset, compute Diff-In scores for 50 samples, compare against brute-force LOO retraining to verify implementation. 2) Ablation on m: Run data cleaning task (e.g., 20% label noise) with m ∈ {1, 3, 5, 10} to observe accuracy plateau and select cost-effective m. 3) Cross-task sanity check: Use same influence scores for both data cleaning and coreset selection on same dataset—if a sample is identified as harmful (should be removed for cleaning), it should score low for coreset selection.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does Diff-In perform in non-convex optimization regimes where the ℓ-Lipschitz continuous gradient assumption is violated?
- **Basis in paper**: The paper notes in Section 5.2 that "If the conditions for ℓ-Lipschitz continuous gradients are not met, the gradient norms g, the values of g, and ℓ can become very large. In such cases, the error bound derived from these parameters may lose its practical relevance."
- **Why unresolved**: The paper states the error bound may "lose its practical relevance" but also claims "this does not necessarily imply that the algorithm will fail in practice," leaving the actual behavior in such regimes empirically and theoretically unexplored.
- **What evidence would resolve it**: Experiments on architectures with highly non-smooth loss landscapes, combined with analysis of how gradient norm bounds correlate with estimation accuracy.

### Open Question 2
- **Question**: What is the optimal time-step sampling strategy for Diff-In across different training dynamics and model architectures?
- **Basis in paper**: Section 4.2 discusses practical implementation with uniformly sampled checkpoints, and Section 6.5.1 shows m=5 provides diminishing returns. However, Figure 2 shows different sampling strategies yield varying similarity scores, suggesting the uniform strategy may be suboptimal.
- **Why unresolved**: The paper empirically determines m=5 works well but does not provide theoretical guidance on optimal sampling distribution or how it should adapt to different training schedules.
- **What evidence would resolve it**: Systematic study comparing sampling strategies (uniform, weighted by loss change, adaptive) across varying training dynamics, with theoretical analysis connecting sampling strategy to approximation error.

### Open Question 3
- **Question**: Can Diff-In's polynomial error bound be further tightened under additional realistic assumptions about neural network training?
- **Basis in paper**: Section 5.1 acknowledges "the bound proposed in Proposition 5.1 shows that the upper bound of the approximation error of Diff-In grows with the increase of the training times T" but notes this reflects a "worst-case scenario" that doesn't match empirical performance.
- **Why unresolved**: The gap between the theoretical bound and empirical performance suggests there may be tighter bounds achievable by incorporating properties of practical training (e.g., convergence, parameter stability) that weren't leveraged.
- **What evidence would resolve it**: Derivation of refined bounds incorporating convergence properties or parameter stability, validated against empirical error measurements across training trajectories.

## Limitations
- Theoretical guarantees rely on local smoothness assumptions that may not hold in highly non-convex regimes or with aggressive optimization schedules
- Accuracy depends critically on checkpoint selection and finite-difference perturbation tuning, which can be dataset-specific
- Computational efficiency gains come with potential numerical stability trade-offs, particularly for very large models where gradient noise may amplify

## Confidence
- **High confidence**: Empirical performance claims on data cleaning (precision improvements), data deletion accuracy, and coreset selection metrics are well-supported by extensive experiments across multiple datasets and model architectures
- **Medium confidence**: Theoretical error bounds and claims about approximation quality are mathematically sound but may not fully capture real-world training dynamics with adaptive optimizers
- **Medium confidence**: Scalability claims are supported by ablation studies but lack direct comparison to first-order methods on truly massive datasets (e.g., full-scale vision-language pretraining)

## Next Checks
1. **Numerical stability validation**: Systematically test Diff-In with varying ε values (1e-3 to 1e-7) on a small CNN to identify the optimal perturbation range that balances accuracy and stability
2. **Optimizer compatibility check**: Implement Diff-In with Adam optimizer and validate whether the adaptive learning rate correction (Eq. 10) maintains the claimed accuracy improvements
3. **Extreme case stress test**: Apply Diff-In to a model with known pathological training dynamics (e.g., hard pruning or aggressive learning rate schedules) to identify conditions where local approximations fail