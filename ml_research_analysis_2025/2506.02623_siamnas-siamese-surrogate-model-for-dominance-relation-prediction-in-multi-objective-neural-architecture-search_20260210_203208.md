---
ver: rpa2
title: 'SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective
  Neural Architecture Search'
arxiv_id: '2506.02623'
source_url: https://arxiv.org/abs/2506.02623
tags:
- surrogate
- architecture
- search
- neural
- siamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational cost of multi-objective neural
  architecture search (NAS), which balances accuracy, parameter count, and FLOPs.
  It proposes a novel surrogate model inspired by Siamese networks that predicts dominance
  relationships between candidate architectures, eliminating the need for expensive
  true objective function evaluations.
---

# SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search

## Quick Facts
- arXiv ID: 2506.02623
- Source URL: https://arxiv.org/abs/2506.02623
- Reference count: 40
- One-line primary result: SiamNAS achieves 92% dominance prediction accuracy and identifies top architectures within 0.01 GPU days in multi-objective NAS.

## Executive Summary
SiamNAS introduces a novel surrogate model for multi-objective neural architecture search that predicts dominance relationships between architectures without expensive true objective evaluations. The method uses an ensemble of Siamese networks trained on pairwise comparisons to classify whether one architecture dominates another, integrated into a modified NSGA-II framework. Experiments on NAS-Bench-201 demonstrate that SiamNAS finds the best architecture for CIFAR-10 and second-best for ImageNet-16-120 within 0.01 GPU days while maintaining 92% accuracy in dominance prediction.

## Method Summary
SiamNAS addresses the computational bottleneck in multi-objective NAS by replacing true objective function evaluations with a surrogate model that predicts dominance relationships. The method samples 600 architectures to generate training pairs, then trains an ensemble of 7 Siamese MLP blocks to classify dominance via shared-weight encoding and difference-vector classification. During search, the surrogate replaces non-dominated sorting and crowding distance with dominance prediction and parameter-count-based selection. The approach integrates into NSGA-II with population size 50 and 2000 generations, requiring only 0.01 GPU days compared to traditional methods requiring hours.

## Key Results
- Achieves 92% accuracy in dominance relationship prediction on held-out architecture pairs
- Identifies the best architecture for CIFAR-10 and second-best for ImageNet-16-120 within 0.01 GPU days
- Demonstrates significant computational efficiency compared to traditional NAS methods requiring full objective evaluations
- Shows optimal ensemble size of 7 Siamese blocks for balancing accuracy and computational cost

## Why This Works (Mechanism)

### Mechanism 1: Pairwise Dominance Prediction via Siamese Encoding
A lightweight neural network predicts dominance relationships between architecture pairs without computing true objective values. Two identical MLPs with shared weights encode architecture pairs into 32-dimensional embeddings, with the difference vector classified by a second MLP to output dominance relationships. An ensemble of 7 blocks uses two-stage majority polling to determine final dominance status.

### Mechanism 2: Quadratic Training Data Expansion from Pair Construction
Converting 600 evaluated architectures into 360,000 training pairs provides sufficient data to train the surrogate without additional expensive evaluations. Class imbalance is addressed through pair reassignment with probability θ=0.5/(1-σ), replacing dominated candidates until valid dominated pairs are found.

### Mechanism 3: Parameter Count Heuristic as Crowding Distance Proxy
Replacing crowding distance with model-size-based selection maintains search pressure toward diverse, high-quality solutions without objective evaluations. During survivor selection, candidates are sorted by parameter count in descending order, favoring larger models within the same non-dominated front.

## Foundational Learning

- **Pareto Dominance and Non-Dominated Sorting**: Essential for understanding that x1 dominates x2 iff x1 is no worse in all objectives and strictly better in at least one. Quick check: Given (accuracy=90%, params=1M, FLOPs=100M) and (accuracy=92%, params=2M, FLOPs=150M), which dominates which?
- **Siamese Networks and Weight Sharing**: Critical for ensuring comparable embeddings between architecture pairs. Quick check: Why would using unshared weights for the two MLPs degrade dominance prediction quality?
- **Ensemble Voting and Class Imbalance**: Important for understanding the 7-block majority voting and resampling strategy. Quick check: In a 3-objective minimization problem with random solutions, approximately what fraction of pairs would you expect to have clear dominance vs. being non-dominated?

## Architecture Onboarding

- **Component map**: [Architecture Pair (x1, x2)] → [One-hot Encoder] → [Siamese MLP Block] × 7 → [Majority Voting] → [Two-Stage Polling] → [Final dominance classification]
- **Critical path**: 1) Sample 600 architectures → query NAS-Bench-201 for objectives, 2) Generate 360,000 pairs with balance resampling → train 7 Siamese blocks, 3) Initialize population 50 → run 2000 generations with surrogate-based selection, 4) Evaluate final Pareto front with true objectives
- **Design tradeoffs**: Ns=600 provides 92% accuracy knee point; Nm=7 optimal for ensemble size; parameter-count heuristic eliminates evaluation cost but may reduce diversity
- **Failure signatures**: <85% accuracy causes misclassified fronts and premature convergence; final Pareto front containing only small models indicates over-pruning; no convergence improvement suggests near-random predictions
- **First 3 experiments**: 1) Train surrogate on 600 architectures, test 200 held-out pairs for ≥90% accuracy, 2) Ablation study with Nm∈{1,3,5,7,9} on CIFAR-10, 3) End-to-end search validation to verify finding best architecture within 0.01 GPU days

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted evaluation to NAS-Bench-201 search space limits generalizability to real-world architecture spaces
- Parameter-count heuristic is a significant simplification of crowding distance that may not maintain diversity in larger search spaces
- Does not address potential distribution shift between initial samples and evolutionary search population

## Confidence
- **High Confidence**: 92% dominance prediction accuracy and 0.01 GPU day reduction are well-supported by NAS-Bench-201 experiments
- **Medium Confidence**: Best architecture identification for CIFAR-10 is well-supported; ImageNet-16-120 results rely on reported data
- **Low Confidence**: Generalizability of parameter-count heuristic and effectiveness on non-benchmark search spaces are assumed

## Next Checks
1. Implement rolling validation to track surrogate accuracy on architectures sampled from current evolutionary population over generations
2. Replace parameter-count heuristic with true crowding distance in survivor selection and compare Pareto front diversity and quality
3. Adapt SiamNAS to NAS-Bench-301 or DARTS to verify 92% accuracy and sub-0.05 GPU day performance beyond benchmark spaces