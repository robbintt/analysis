---
ver: rpa2
title: Identifying & Interactively Refining Ambiguous User Goals for Data Visualization
  Code Generation
arxiv_id: '2510.09390'
source_url: https://arxiv.org/abs/2510.09390
tags:
- ambiguity
- code
- dialogue
- coder
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses ambiguity in natural language to code generation,\
  \ focusing on data visualization. It introduces a taxonomy of ambiguity types\u2014\
  semantic, presupposition, and underspecification\u2014and proposes metrics to quantify\
  \ them."
---

# Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation

## Quick Facts
- arXiv ID: 2510.09390
- Source URL: https://arxiv.org/abs/2510.09390
- Reference count: 30
- Primary result: Pragmatic dialogue strategies improve data visualization code generation accuracy from 68.38% to 79.44% pass@1 by resolving semantic, presupposition, and underspecification ambiguity

## Executive Summary
This paper addresses ambiguity in natural language to code generation, focusing on data visualization. It introduces a taxonomy of ambiguity types—semantic, presupposition, and underspecification—and proposes metrics to quantify them. Using Matplotlib problems from the DS-1000 dataset, the study shows that pragmatic dialogue strategies inspired by Gricean cooperativity, discourse representation theory, and questions under discussion reduce ambiguity and improve code accuracy. Metrics like LLM-based ambiguity rating and optimal result gap correlate better with human annotations than uncertainty baselines. Dialogue-based approaches significantly increase task success (e.g., from 68.38% to 79.44% pass@1 with cooperative strategy), confirming that multi-turn exchanges enhance alignment between user intent and generated code.

## Method Summary
The method simulates a dialogue between two LLM agents (Director and Coder) to resolve ambiguity in natural language prompts for data visualization code. The Coder generates multiple code samples and uses one of three pragmatic strategies (Cooperative, Discoursive, Inquisitive) to ask clarifying questions. The Director responds based on reference code or images. Ambiguity is detected using taxonomy-guided metrics including LLM-based ambiguity rating (LAR), optimal result gap (ORG), and sampling diversity measures. The system iteratively refines the prompt through dialogue turns until reaching a final code output, with performance measured by pass@1 accuracy.

## Key Results
- Cooperative dialogue strategy improves pass@1 from 68.38% to 79.44% on DS-1000 Matplotlib problems
- LAR (LLM-based ambiguity rating) achieves highest AUC (0.579 avg) across ambiguity categories vs 0.424 for self-verification baseline
- Dialogue shows better performance in ambiguous instances (improving from 59.35% to 72.26% pass@1) compared to non-ambiguous cases
- Image reference modality achieves lower ceiling (75.23% pass@1) than code reference (79.44%), indicating realism vs. performance tradeoff

## Why This Works (Mechanism)

### Mechanism 1: Formalized Ambiguity Detection via Taxonomy-Guided Metrics
- Claim: LLM-based ambiguity ratings that explicitly encode taxonomy categories predict ambiguity better than sampling-based uncertainty.
- Mechanism: The paper defines ambiguity formally as excess error reducible through improved utterance. LAR (LLM-Based Ambiguity Rating) prompts models to rate on 1-10 scale while providing taxonomy definitions, implicitly approximating optimal prompt gap without requiring oracle access.
- Core assumption: The taxonomy categories (semantic, presupposition, underspecification) capture the primary sources of ambiguity in code generation tasks.
- Evidence anchors:
  - [section 3.2] "LART metric, where a model is asked to rate the ambiguity using our taxonomy... The most predictive of any ambiguity category is the LART metric"
  - [table 2] LART achieves highest AUC (0.579 avg) across ambiguity categories vs 0.424 for self-verification baseline
  - [corpus] Weak corpus support—related work on ambiguity detection exists but limited evaluation on code generation specifically
- Break condition: If taxonomy categories don't generalize to other code domains (e.g., backend logic vs. visualization), metric validity degrades.

### Mechanism 2: Multi-turn Dialogue Reduces Underspecification Through Constrained Sampling
- Claim: Pragmatic dialogue increases pass@1 by iteratively constraining the coder's solution space through clarification exchanges.
- Mechanism: Each dialogue turn reduces the set of viable programs by having the coder generate k samples, identify divergent parameters, and ask targeted clarifications. The director provides corrective constraints, narrowing C(U) toward ground truth.
- Core assumption: The coder can identify which parameter differences matter to user intent and formulate appropriate clarification questions.
- Evidence anchors:
  - [table 3] Cooperative dialogue improves pass@1 from 68.38% baseline to 79.44%
  - [figure 4] "Dialogue shows better performance in ambiguous instances instead of non-ambiguous ones"
  - [corpus] AmbiSQL and Ambiguity Resolution with Human Feedback show similar patterns in SQL and general code tasks
- Break condition: If dialogue introduces more confusion than clarity (e.g., conflicting signals), performance may degrade below baseline—observed in Inquisitive strategy (66.34% with reference code).

### Mechanism 3: Cooperative Persona Leverages Theory-of-Mind for Efficient Disambiguation
- Claim: Cooperative strategy outperforms other pragmatics frameworks by modeling director's mental state and converging efficiently.
- Mechanism: The cooperative persona explicitly applies Grice's maxims (quality, quantity, relevance, manner) via prompting. This encourages the coder to provide only necessary clarifications without redundancy, targeting the most ambiguous parameters first based on sample variance.
- Core assumption: Modern LLMs have sufficient theory-of-mind capabilities embedded through training to enact cooperative reasoning effectively.
- Evidence anchors:
  - [table 3] Cooperative strategy achieves best performance (79.44% pass@1) across all dialogue strategies
  - [section 5] "likely benefiting from the theory-of-mind reasoning and chain-of-thought training in modern LLMs"
  - [corpus] Limited corpus evidence on cooperative persona comparison; mostly theoretical grounding in RSA literature
- Break condition: If user intent is fundamentally incompatible with cooperative norms (adversarial, exploratory), the strategy may fail to adapt.

## Foundational Learning

- Concept: **Grice's Cooperative Principle and Maxims**
  - Why needed here: The cooperative dialogue strategy directly instantiates these maxims; understanding them helps debug why certain clarifications work and others don't.
  - Quick check question: If a coder provides three alternative solutions with detailed explanations, which maxim might it be violating?

- Concept: **Abstract Syntax Trees (AST) for Code Comparison**
  - Why needed here: The Sampling Diversity metric uses AST edit distance to quantify distinct programs; implementing this requires understanding tree-based structural comparison.
  - Quick check question: How would two semantically equivalent but syntactically different code snippets be treated by AST comparison?

- Concept: **Bayes Error vs. Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper frames ambiguity as reducible error above Bayes optimal; distinguishing irreducible vs. reducible uncertainty is core to the formalism.
  - Quick check question: If E(I, C(U*)) represents minimum achievable error, what does E(I, C(U)) - E(I, C(U*)) represent?

## Architecture Onboarding

- Component map:
  - **Director Agent** -> **Coder Agent** -> **Ambiguity Detection Module** -> **Dialogue Policy** -> **Final Code Output**

- Critical path:
  1. Initial prompt → Coder generates k samples
  2. Ambiguity module analyzes sample divergence (AST comparison, parameter counting)
  3. Coder generates clarification with selected persona
  4. Director responds with constraint
  5. Repeat until turn limit or confidence threshold
  6. Coder returns final code from constrained solution space

- Design tradeoffs:
  - **Number of samples (k)**: Higher k improves ambiguity detection but increases latency; paper uses unspecified k with 30 samples for evaluation
  - **Number of dialogue turns (n)**: More turns can resolve complex ambiguities but risk user fatigue; paper treats as hyperparameter
  - **Reference modality**: Code reference risks leakage; image reference more realistic but lower ceiling (75.23% vs 79.44%)

- Failure signatures:
  - **Inquisitive backfire**: Always searching for implicit questions can introduce unnecessary complexity (66.34% pass@1)
  - **Ceiling gap**: Even optimal reprompt achieves only 87.74%, indicating irreducible ambiguity in static evaluation
  - **Non-ambiguous penalty**: Dialogue can harm non-ambiguous cases if persona adds unnecessary complexity

- First 3 experiments:
  1. **Baseline calibration**: Run vanilla code generation on DS-1000 Matplotlib subset (pass@1 with k=30 samples) to establish baseline before dialogue implementation
  2. **Ambiguity metric validation**: Implement LAR and ORG metrics, correlate against human annotations on 50-sample subset to verify taxonomy alignment
  3. **Single-turn cooperative dialogue**: Implement cooperative persona with n=2 turns (initial prompt + 1 clarification + final code), compare to baseline to validate core mechanism before scaling complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do pragmatic dialogue strategies transfer to real-world human-AI interactions compared to LLM-based simulations?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "We did not deploy a dialogue system to study our approach... simulations using LLMs... may or may not accurately represent how a human interlocutor would act."
- Why unresolved: The study relies entirely on GPT-4o simulating both the director and coder agents to measure pass@1 scores.
- What evidence would resolve it: A user study with human programmers acting as directors to validate if cooperative strategies improve task success and satisfaction in natural settings.

### Open Question 2
- Question: Under what specific conditions does multi-turn dialogue degrade code generation accuracy?
- Basis in paper: [explicit] The Error Analysis notes that "in some cases, additional dialogue negatively impacted performance across all personas," yet the paper focuses on aggregate improvements.
- Why unresolved: The paper identifies the phenomenon but does not characterize the mechanisms (e.g., hallucination propagation, context drift) that cause dialogue to harm specific instances.
- What evidence would resolve it: A fine-grained failure analysis of cases where dialogue reduces pass@1 scores to identify triggers for performance degradation.

### Open Question 3
- Question: Does the proposed ambiguity taxonomy generalize to non-visual code generation domains?
- Basis in paper: [inferred] The study focuses exclusively on "data visualization domain" and "plotting code," relying on multimodal contexts (images) specific to Matplotlib.
- Why unresolved: It is unclear if the taxonomy categories (e.g., "visual" underspecification) or the effectiveness of the metrics (LAR, ORG) transfer to domains lacking visual outputs, such as backend logic.
- What evidence would resolve it: Evaluating the ambiguity metrics and dialogue strategies on general-purpose coding benchmarks (e.g., HumanEval) that lack visual components.

## Limitations
- The study focuses exclusively on data visualization code generation, limiting generalizability to other programming domains
- Ambiguity detection relies heavily on LLM-based metrics that may not capture all real-world ambiguity patterns
- Simulated dialogue evaluation may not reflect actual human-AI interaction dynamics
- Reference code modality creates potential information leakage, with significantly higher performance than image reference

## Confidence
- **High Confidence**: Mechanism 1 (Taxonomy-guided ambiguity detection) - Strong empirical support with ROC AUC metrics
- **Medium Confidence**: Mechanism 2 (Multi-turn dialogue effectiveness) - Validated on dataset but simulation limitations exist
- **Medium Confidence**: Mechanism 3 (Cooperative persona superiority) - Best performer but limited comparison to alternative frameworks

## Next Checks
1. Test taxonomy-guided ambiguity metrics on a non-visualization code generation dataset to assess domain transferability
2. Conduct user study comparing simulated dialogue performance with actual human-AI interaction in ambiguous code generation tasks
3. Implement ablation study removing Gricean maxims from cooperative persona to quantify their specific contribution to performance gains