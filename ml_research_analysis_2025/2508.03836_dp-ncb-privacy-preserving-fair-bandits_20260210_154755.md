---
ver: rpa2
title: 'DP-NCB: Privacy Preserving Fair Bandits'
arxiv_id: '2508.03836'
source_url: https://arxiv.org/abs/2508.03836
tags:
- lemma
- privacy
- algorithm
- phase
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DP-NCB, a unified algorithmic framework\
  \ for stochastic multi-armed bandits that simultaneously achieves \u03F5-differential\
  \ privacy and order-optimal Nash regret. The framework operates under both global\
  \ and local privacy models and is anytime, requiring no prior knowledge of the time\
  \ horizon."
---

# DP-NCB: Privacy Preserving Fair Bandits

## Quick Facts
- arXiv ID: 2508.03836
- Source URL: https://arxiv.org/abs/2508.03836
- Authors: Dhruv Sarkar; Nishant Pandey; Sayak Ray Chowdhury
- Reference count: 40
- Primary result: Unified algorithmic framework achieving ε-DP and order-optimal Nash regret in stochastic multi-armed bandits

## Executive Summary
This paper introduces DP-NCB, a unified algorithmic framework for stochastic multi-armed bandits that simultaneously achieves ε-differential privacy and order-optimal Nash regret. The framework operates under both global and local privacy models and is anytime, requiring no prior knowledge of the time horizon. In the global model, DP-NCB achieves a Nash regret of Õ(√k log T / T + k (log T)² / (ϵ T)), while in the local model it achieves Õ(√k log T / T + √k (log T)² / (√T ϵ)). These bounds match known minimax lower bounds up to polylogarithmic factors, establishing near-optimality.

## Method Summary
DP-NCB uses a two-phase structure: initial uniform exploration followed by private adaptive exploitation with modified Nash confidence bounds. The algorithm constructs private mean estimators by adding Laplace noise to empirical means, then computes Nash Confidence Bounds (NCB) that account for both fairness (Nash social welfare) and privacy noise. For global DP, private statistics are released only at the end of fixed-duration episodes, while local DP adds noise locally to each reward. The anytime version uses a doubling trick, trading simplicity for a multiplicative log T regret overhead.

## Key Results
- DP-NCB achieves Nash regret of Õ(√k log T / T + k (log T)² / (ϵ T)) in global DP model
- DP-NCB achieves Nash regret of Õ(√k log T / T + √k (log T)² / (√T ϵ)) in local DP model
- Experimental results show DP-NCB incurs substantially lower Nash regret than state-of-the-art baselines designed for average regret
- Bounds match known minimax lower bounds up to polylogarithmic factors, establishing near-optimality

## Why This Works (Mechanism)

### Mechanism 1: Noise-Calibrated Nash Confidence Bound
The algorithm modifies the Upper Confidence Bound (UCB) to account for both Nash social welfare (fairness) and Laplace noise (privacy). It constructs a private mean estimator $\tilde{\mu}_i$ by adding Laplace noise to the empirical mean, then computes a Nash Confidence Bound using this private mean. The exploration bonus is adjusted based on the private mean estimate and explicitly adds a term to compensate for the variance introduced by privacy noise.

### Mechanism 2: Two-Phase Exploration-Exploitation
The algorithm operates in two distinct phases: Phase I involves uniform exploration until a stopping condition is met based on cumulative private reward, then Phase II switches to adaptive policy using NCB index. This forced uniform exploration stabilizes private mean estimates before greedy selection, preventing locking in on suboptimal arms due to early noise.

### Mechanism 3: Episodic Privacy Release (Global DP)
In the Global DP setting, the algorithm selects an arm and sticks to it for a doubling number of rounds (an episode), releasing a new private mean only at the end of each episode. This batching reduces the number of times privacy is "spent" through composition while limiting staleness of data.

## Foundational Learning

- **Concept:** Differential Privacy (DP) & Sensitivity
  - **Why needed here:** DP is the core constraint. Understanding how Laplace noise scale depends on sensitivity (max change one user can cause) is crucial for understanding noise calibration as $\text{Lap}(\frac{1}{\epsilon n_i})$.
  - **Quick check:** If a reward range changes from $[0,1]$ to $[0,10]$, how would the Laplace noise scale need to change to maintain the same privacy level?

- **Concept:** Nash Social Welfare (Nash Regret)
  - **Why needed here:** This is the "fairness" objective. Unlike standard regret (arithmetic mean), Nash regret uses geometric mean, which heavily penalizes rounds with near-zero rewards.
  - **Quick check:** Why does optimizing for Nash regret lead to "fairer" outcomes than optimizing for average regret in a clinical trial setting?

- **Concept:** Concentration Inequalities (Chernoff/Hoeffding)
  - **Why needed here:** The paper relies on "high probability" events to guarantee that the empirical mean is close to the true mean. The confidence bounds are derived from these inequalities.
  - **Quick check:** In the local DP setting, why does the noise variance scale differently than in global DP, and how does that impact the confidence interval width?

## Architecture Onboarding

- **Component map:** Input(k, T, ε) -> Phase I(UniformSampler, RewardObserver, PrivateMeanUpdate, StoppingConditionCheck) -> Phase II(NCBCalculator, ArmSelector, RewardObserver, EpisodeEndCheck, PrivateMeanUpdate) -> Output(NashRegret)
- **Critical path:** The calculation of the Private Mean ($\tilde{\mu}$) is the bottleneck. In Local DP, this happens every round on the client side. In Global DP, this happens at the end of an episode on the server. Incorrect noise calibration here breaks both privacy and utility.
- **Design tradeoffs:** Global DP offers better regret ($\tilde{O}(1/T)$) but requires a trusted server; Local DP offers stronger privacy (no trusted server) but suffers slower regret ($\tilde{O}(1/\sqrt{T})$). The anytime version trades simplicity for a multiplicative $\log T$ regret overhead.
- **Failure signatures:** Regret plateaus if Phase I runs too long; negative/private means explode if rewards aren't strictly bounded in $[0,1]$; over-exploration if ε is too small causing noise to dominate the NCB index.
- **First 3 experiments:** 1) Plot duration of Phase I vs Phase II over time to verify theoretical bounds on stopping condition; 2) Run GDP-NCB with ε ∈ {0.1, 0.5, 1.0, 5.0} to visualize privacy cost impact on Nash Regret; 3) Compare GDP-NCB against non-private NCB and standard DP-UCB on skewed distribution to demonstrate fairness property.

## Open Questions the Paper Calls Out

None

## Limitations

- Theoretical analysis assumes stochastic rewards with bounded support [0,1]; non-stationary or adversarial environments would invalidate concentration bounds
- Constants in the stopping condition (particularly the 1600 factor) are heuristic and may not generalize well across problem scales
- Paper does not provide tight bounds on computational complexity of the doubling trick in the anytime version

## Confidence

- **High Confidence:** The privacy guarantee (ε-differential privacy) and the structural claim that Nash regret can be made simultaneously order-optimal with privacy
- **Medium Confidence:** The exact constants in the regret bounds (Õ notation hides polylog factors that may be significant in practice)
- **Low Confidence:** The robustness of the algorithm to reward distributions outside [0,1] or to very small ε values where noise could overwhelm the signal

## Next Checks

1. **Robustness Test:** Implement GDP-NCB with rewards clipped to [0,10] instead of [0,1] to verify that the algorithm fails gracefully or that the privacy analysis breaks down as expected
2. **Extreme Privacy Regime:** Run LDP-NCB with ε ∈ {0.01, 0.05, 0.1} on a 10-arm instance and measure the Phase I duration to verify if the stopping condition becomes a bottleneck
3. **Non-Stationary Sanity Check:** Modify reward distributions mid-run (switch one arm's mean from 0.9 to 0.1 after T/2 rounds) and measure Nash regret degradation compared to the stationary case