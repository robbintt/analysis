---
ver: rpa2
title: 'No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision
  Regimes'
arxiv_id: '2508.19060'
source_url: https://arxiv.org/abs/2508.19060
tags:
- anomaly
- detection
- uni00000013
- supervised
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of surface defect detection
  in manufacturing, where models must perform well across diverse supervision scenarios
  (unsupervised, weakly supervised, mixed, and fully supervised). The authors propose
  SuperSimpleNet, a unified model built on SimpleNet that incorporates synthetic anomaly
  generation, an enhanced classification head, and improved training procedures to
  effectively leverage all available data annotations.
---

# No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes

## Quick Facts
- **arXiv ID:** 2508.19060
- **Source URL:** https://arxiv.org/abs/2508.19060
- **Reference count:** 40
- **Key outcome:** SuperSimpleNet achieves state-of-the-art performance across all supervision regimes, including 98.0% AUROC on SensumSODF in fully supervised settings and 98.3% AUROC on MVTec AD in unsupervised settings, with inference times below 10 ms.

## Executive Summary
This paper addresses the challenge of surface defect detection in manufacturing, where models must perform well across diverse supervision scenarios (unsupervised, weakly supervised, mixed, and fully supervised). The authors propose SuperSimpleNet, a unified model built on SimpleNet that incorporates synthetic anomaly generation, an enhanced classification head, and improved training procedures to effectively leverage all available data annotations. SuperSimpleNet achieves state-of-the-art results across all supervision regimes while maintaining industrial-grade efficiency requirements.

## Method Summary
SuperSimpleNet is a unified surface defect detection model that combines a frozen pre-trained feature extractor (WideResNet50) with dual heads for segmentation and classification. The model introduces a novel synthetic anomaly generation process that creates realistic defect-like patterns in the latent space using Perlin noise masks constrained to non-anomalous regions. A dynamic loss weighting scheme allows the model to selectively use available annotations - segmentation loss is disabled for weakly-labeled anomalous images to prevent training conflicts. Feature maps from different backbone layers are upscaled and concatenated to improve localization precision while maintaining efficiency.

## Key Results
- Achieves 98.0% AUROC on SensumSODF in fully supervised settings
- Achieves 97.4% AUROC on SensumSODF in weakly supervised settings
- Achieves 98.3% AUROC on MVTec AD in unsupervised settings
- Operates with inference times below 10 ms, meeting industrial efficiency requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating synthetic anomalies in the latent space of a pre-trained feature extractor, constrained by a Perlin noise mask to non-anomalous regions, creates a universal training signal that enables the model to learn from any combination of image-level and pixel-level annotations.
- **Mechanism:** A Perlin noise mask ($M_p$) is generated and refined by removing regions that overlap with real defects ($M_{gt}$), resulting in $M_{synth}$. Gaussian noise is injected into the feature maps only at locations defined by $M_{synth}$. This creates realistic, spatially coherent synthetic defect regions that serve as supervision for the segmentation head (via the mask) and the classification head (via the image label), even when no real defect data is available.
- **Core assumption:** Features from a network pre-trained on a general dataset (ImageNet) contain enough semantic information to distinguish surface anomalies, and adding structured noise to these features is a valid proxy for real defects.
- **Evidence anchors:**
  - [abstract] "SuperSimpleNet incorporates a novel synthetic anomaly generation process... enabling efficient training in all four supervision scenarios..."
  - [section 3.2] "The new synthetic anomaly generation strategy... generate[s] synthetic anomalies using Gaussian noise, which is applied only to regions defined by the synthetic anomaly mask $M_{synth}$..."
  - [corpus] Weak or missing. No corpus papers evaluate this specific latent-space masked noise injection technique.
- **Break condition:** If the pre-trained features are not sufficiently relevant to the target industrial domain, the noise injection may not create meaningful defect proxies, causing the model to fail to generalize.

### Mechanism 2
- **Claim:** A dual-head architecture, with a shared feature extractor and separate segmentation and classification heads, allows for a unified training loss that selectively uses available annotations, preventing conflicting gradients.
- **Mechanism:** The final loss is a weighted sum of a segmentation loss ($L_{seg}$) and a classification loss ($L_{cls}$). A control term ($\gamma$) dynamically scales the segmentation loss: $\gamma=1$ for normal or fully-labeled anomalous images (enabling segmentation learning), and $\gamma=0$ for weakly-labeled anomalous images (disabling segmentation loss to avoid training on missing pixel masks). The classification loss ($L_{cls}$) is always computed based on the image-level label.
- **Core assumption:** It is possible to train a robust classifier using only image-level labels, provided the model has a strong feature extractor and can learn from synthetic anomalies in a weakly supervised manner.
- **Evidence anchors:**
  - [abstract] "...incorporates... an enhanced classification head... enabling efficient training in all four supervision scenarios..."
  - [section 3.4] "The final loss combines the segmentation and classification losses, with control term $\gamma$ added... This allows the segmentation head to be trained on all images except anomalous images without pixel-level labels..."
  - [corpus] Weak. "Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects" proposes a related multi-task approach but not this specific loss-control mechanism.
- **Break condition:** If the segmentation head fails to learn from synthetic anomalies (e.g., if they are unrealistic), the classification head, which can take the segmentation map as input, may be deprived of a useful spatial reasoning signal, potentially hurting performance.

### Mechanism 3
- **Claim:** Upscaling feature maps from a pre-trained backbone and adapting them with a simple linear layer improves localization precision and allows the model to be more sample-efficient.
- **Mechanism:** Feature maps from different backbone layers are bilinearly upsampled to a common, higher resolution before being concatenated. A local average pooling operation captures context. The features destined for the segmentation head are passed through a learnable adaptor layer, projecting them into a task-specific space. This compensates for the low resolution of deep features and adapts generic pre-trained features to the anomaly detection task.
- **Core assumption:** The performance bottleneck for localization is the low spatial resolution of deep features, not a lack of semantic information. A simple linear adaptor is sufficient to align the feature space with the target task.
- **Evidence anchors:**
  - [abstract] "...a highly efficient and adaptable discriminative model built on the foundation of SimpleNet..."
  - [section 3.1] "...an upscaling layer is introduced before feature concatenation... A simple linear layer is used to adapt these features to a task-specific latent space."
  - [corpus] Weak. Corpus papers do not analyze this specific upscaling and adaptor combination.
- **Break condition:** If the target defects require high-frequency details that were completely lost during the backbone's downsampling, upscaling cannot recover them, and localization may fail.

## Foundational Learning
- **Concept: Mixed Supervision**
  - **Why needed here:** This is the core problem SuperSimpleNet solves. Real-world industrial datasets are messy, containing a mix of unlabeled normal images, image-level tags ("this part is bad"), and full pixel-level masks.
  - **Quick check question:** Can your current model train on a dataset where 90% of images are unlabeled normals, 5% are tagged "defective," and 5% have segmentation masks?
- **Concept: Latent Space**
  - **Why needed here:** The model operates on the "latent space" (feature maps) of a pre-trained network. Understanding this as a compressed, semantic representation of the image is key to understanding why adding noise *there* is more powerful than adding noise to raw pixels.
  - **Quick check question:** What's the key difference between adding noise to an input image's pixels vs. adding noise to the output of a neural network layer?
- **Concept: Feature Pyramid**
  - **Why needed here:** The model combines features from different layers of the backbone. This multi-scale approach is fundamental for detecting both large, obvious defects (from deep, semantic features) and small, subtle scratches (from shallow, high-resolution features).
  - **Quick check question:** Which type of feature (shallow/high-res or deep/low-res) would be more critical for detecting a tiny scratch on a surface?

## Architecture Onboarding
- **Component map:**
  - Input Image -> Feature Extractor -> Upscaling & Pooling -> Feature Adaptor -> Segmentation Head -> Anomaly Map
  - Input Image -> Feature Extractor -> Classification Head -> Anomaly Score
  - Training Only: Synthetic Anomaly Generator injects noise into features
- **Critical path:**
  1. Input Image -> Feature Extractor
  2. Features -> Upscale & Pool -> Contextual Features
  3. (Training Only) Generate synthetic anomaly mask & inject noise
  4. Segmentation Head processes (adapted) features -> Anomaly Map
  5. Classification Head processes (raw) features + Anomaly Map -> Anomaly Score
  6. Loss is computed as a sum of segmentation and classification loss, with segmentation loss disabled for weakly-labeled images
- **Design tradeoffs:**
  - **Generality vs. Specificity:** Using a frozen pre-trained backbone makes the model highly efficient and easier to train but ties its performance to the quality of features learned on a generic dataset (ImageNet)
  - **Speed vs. Resolution:** Increasing feature map resolution via upscaling improves small-defect localization but increases memory and computation. The chosen upscaling factor is a compromise
  - **Simplicity vs. Power:** The classification head is intentionally simple to avoid overfitting, especially to synthetic anomalies in the unsupervised setting. A more complex head might overfit to unrealistic noise patterns
- **Failure signatures:**
  - **Overfitting to Synthetic Noise:** In unsupervised settings, if the synthetic noise is too simple or the classification head is too complex, the model may learn to detect only the synthetic noise, failing to generalize to real defects
  - **Missing Small Defects:** The model's inherent limitation is the resolution of the feature maps. Extremely small defects (<1% of image area) may be invisible in the downsampled feature space, leading to false negatives
  - **Labelling Ambiguity:** The paper notes that false positives often occur on images with minor imperfections that were not labeled as anomalies, highlighting the subjectivity in ground-truth labels
- **First 3 experiments:**
  1. **Unsupervised Baseline:** Train on MVTec AD (normal images only) and reproduce the reported AUROC (~98.3%) to validate the implementation
  2. **Mixed Supervision Check:** On KSDD2, train with a fixed number of pixel-level masks and varying numbers of image-level labels to confirm the model can leverage weak supervision
  3. **Ablation on Synthetic Noise:** Disable the Perlin-noise masking and apply uniform noise across the feature map. Compare performance to confirm the importance of the spatially coherent noise generation

## Open Questions the Paper Calls Out
The paper identifies several open questions and limitations:
- **Performance on small defects:** The model struggles with extremely small defects (<1% of image area) due to limited feature resolution, creating a tension between detection capability and inference speed requirements
- **Pretraining domain dependence:** The model's performance depends heavily on the quality of features learned from ImageNet, raising questions about whether domain-specific pretraining on industrial texture datasets would improve results
- **Labelling ambiguity handling:** The model may detect valid anomalies that were missed by human annotators, but there's no systematic approach to calibrate or handle these "valid" false positives

## Limitations
- **Synthetic anomaly generation lacks direct validation** in the corpus, raising questions about universal applicability beyond surface defects
- **Inherent resolution limits** mean extremely small defects (<1% of image area) may remain undetectable regardless of training strategy
- **Subjective nature of defect labeling** is acknowledged but not systematically addressed - the model may fit to labeling conventions rather than discovering true defect patterns

## Confidence
- **High Confidence:** The dual-head architecture with selective loss weighting for different supervision regimes is well-specified and theoretically sound. The reported performance improvements across all four regimes are consistent and well-documented.
- **Medium Confidence:** The feature upscaling and adaptation approach is reasonable but relies on unstated architectural details (adaptor dimensions, pooling parameters). The effectiveness depends heavily on proper implementation of these underspecified components.
- **Medium Confidence:** The synthetic anomaly generation mechanism is innovative but lacks direct validation. The assumption that masked latent-space noise injection creates realistic defect proxies needs empirical verification, particularly regarding the choice of Perlin noise parameters and their domain specificity.

## Next Checks
1. **Unsupervised Baseline Validation:** Train SuperSimpleNet on MVTec AD with only normal images and verify reproduction of the reported ~98.3% AUROC. This confirms the core unsupervised learning mechanism works as intended.

2. **Weak Supervision Sensitivity:** On KSDD2, systematically vary the ratio of pixel-level masks to image-level labels (e.g., 10%, 50%, 90% weak labels) to confirm the model's ability to leverage mixed supervision as claimed.

3. **Synthetic Noise Ablation:** Disable the Perlin-noise masking and apply uniform noise across the entire feature map. Compare performance to the full method to quantify the importance of spatially coherent noise generation for synthetic anomaly quality.