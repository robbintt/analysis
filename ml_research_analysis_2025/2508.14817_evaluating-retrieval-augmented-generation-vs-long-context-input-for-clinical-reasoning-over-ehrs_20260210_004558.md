---
ver: rpa2
title: Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical
  Reasoning over EHRs
arxiv_id: '2508.14817'
source_url: https://arxiv.org/abs/2508.14817
tags:
- notes
- clinical
- imaging
- task
- chunks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates retrieval-augmented generation (RAG) against\
  \ using full clinical notes for three clinical reasoning tasks over electronic health\
  \ records: extracting imaging procedures, generating antibiotic timelines, and identifying\
  \ key diagnoses. Using three state-of-the-art large language models and varying\
  \ amounts of context (up to 128K tokens), RAG consistently achieved near-parity\
  \ with full-context inputs while requiring far fewer tokens\u2014up to 400% more\
  \ efficient in some tasks."
---

# Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs

## Quick Facts
- arXiv ID: 2508.14817
- Source URL: https://arxiv.org/abs/2508.14817
- Reference count: 40
- Three clinical tasks show RAG achieves near-parity with full-context while using 400% fewer tokens

## Executive Summary
This study evaluates retrieval-augmented generation (RAG) against using full clinical notes for three clinical reasoning tasks over electronic health records: extracting imaging procedures, generating antibiotic timelines, and identifying key diagnoses. Using three state-of-the-art large language models and varying amounts of context (up to 128K tokens), RAG consistently achieved near-parity with full-context inputs while requiring far fewer tokens—up to 400% more efficient in some tasks. For imaging procedures, RAG matched or exceeded full-context performance with only 60 retrieved chunks. For antibiotic timelines, RAG approached full-context accuracy using minimal text. Diagnosis generation showed smaller gains, with RAG slightly underperforming compared to full notes but still remaining competitive.

## Method Summary
The study compared RAG against full-context and recent-note approaches across three clinical reasoning tasks using 200 hospitalizations per task from a US hospital system. Clinical notes were segmented into 128-token chunks with 20-token overlap, embedded using BGE-en-large-v1.5, and retrieved via cosine similarity with task-specific queries. The three tasks evaluated were imaging procedure extraction (F1 at modality+date+location), antibiotic timeline generation (Jaccard index for timespan overlap), and key diagnosis identification (F1 via CCSR categories). Three LLMs (GPT-4o-mini, o4-mini, DeepSeek-R1) were tested across context sizes from 20 chunks up to 128K tokens, comparing performance and efficiency.

## Key Results
- RAG achieved near-parity with full-context inputs while requiring 400% fewer tokens in some tasks
- Imaging procedure extraction showed 3.75–5.5x improvement over recent notes with RAG
- For antibiotic timelines, RAG approached full-context accuracy using only minimal retrieved text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG achieves near-parity with full-context while using dramatically fewer tokens.
- Mechanism: Semantic chunking (128 tokens, 20-token overlap) + embedding-based retrieval filters irrelevant content, reducing noise and computational load while preserving task-critical information.
- Core assumption: Relevant information is captured within top-N semantically similar chunks.
- Evidence anchors:
  - [abstract] "RAG consistently achieved near-parity with full-context inputs while requiring far fewer tokens—up to 400% more efficient in some tasks."
  - [section 4] Chunks embedded with BGE-en-large-v1.5; top-N retrieved via cosine similarity.
  - [corpus] CLI-RAG and EHR-RAG papers confirm RAG effectiveness for clinical text, though task-specific tuning remains underexplored.
- Break condition: When relevant information is distributed across many small fragments or not semantically aligned with the query (e.g., diagnosis generation showed smaller gains).

### Mechanism 2
- Claim: Performance gains are task-dependent—simpler extraction tasks benefit more than complex reasoning tasks.
- Mechanism: Extraction tasks (imaging procedures) rely on explicit mentions easily matched by retrieval queries. Reasoning tasks (diagnosis generation) require synthesizing scattered evidence and medical inference that simple retrieval may miss.
- Core assumption: Query formulation aligns with how information is documented in notes.
- Evidence anchors:
  - [section 5.2] Imaging Procedures showed 3.75–5.5x improvement over recent notes.
  - [section 7.2] Diagnosis Generation F1 scores were flat across methods (max 44.41), suggesting evaluation or task limitations.
  - [corpus] Less Context, Same Performance (arXiv:2505.20320) similarly finds RAG matches full-context for classification but not universally across tasks.
- Break condition: When gold-standard labels require reasoning beyond explicit text (e.g., filtering prophylactic antibiotics from therapeutic ones).

### Mechanism 3
- Claim: Increasing context beyond a threshold provides diminishing or negative returns.
- Mechanism: "Lost-in-the-middle" effect degrades performance when relevant information is buried in long contexts; RAG mitigates this by presenting only retrieved chunks.
- Core assumption: Models struggle to attend uniformly across very long sequences.
- Evidence anchors:
  - [section 6.2] Jaccard index for antibiotics dropped slightly from 64K→128K tokens for two models.
  - [section 1] Cites Liu et al. (2024) on "lost-in-the-middle" effect.
  - [corpus] Weak corpus evidence directly confirming this effect in clinical EHR; mostly general-domain studies.
- Break condition: When additional context adds redundant or conflicting information without new signal.

## Foundational Learning

- Concept: **Chunking and Embedding for Retrieval**
  - Why needed here: Understanding how text is split and represented as vectors is foundational to building or debugging a RAG pipeline.
  - Quick check question: Given a 500-token clinical note, how many 128-token chunks with 20-token overlap would be generated?

- Concept: **Cosine Similarity for Semantic Search**
  - Why needed here: Determines how "relevant" a chunk is to the query; critical for tuning retrieval quality.
  - Quick check question: If a query embedding has cosine similarity 0.85 with Chunk A and 0.72 with Chunk B, which is retrieved first?

- Concept: **Evaluation Metrics for Clinical NLP (F1, Jaccard, CCSR)**
  - Why needed here: Different tasks require different evaluation approaches; understanding these avoids misinterpreting results.
  - Quick check question: Why might F1 be insufficient for antibiotic timeline evaluation, necessitating Jaccard index?

## Architecture Onboarding

- Component map: Notes → Chunker (128 tokens, 20 overlap) → Embedder (BGE-en-large-v1.5) → Vector Store → Retriever (cosine similarity, top-N) → LLM (GPT-4o-mini / o4-mini / DeepSeek-R1) → Structured Output

- Critical path: Query formulation → Embedding → Retrieval → Prompt construction (chunks + instructions) → Model inference → Post-processing (regex/rule extraction)

- Design tradeoffs:
  - Chunk size: Larger chunks capture more context but increase noise; smaller chunks improve precision but may fragment information.
  - Number of retrieved chunks: More chunks improve recall but increase token cost and potential noise.
  - Fixed vs. learned queries: Fixed queries (Table 2) are simple but may not capture task nuance; learned queries require more engineering.

- Failure signatures:
  - Low recall: Relevant information exists in notes but isn't retrieved (e.g., 32% of gold antibiotics missed with 20 chunks).
  - Flat performance across context amounts: Suggests evaluation or task design issues (seen in diagnosis generation).
  - Performance drop at longer contexts: Possible lost-in-the-middle effect or noise accumulation.

- First 3 experiments:
  1. **Baseline retrieval sweep**: Test top-N = [20, 40, 60, 80] chunks on a held-out set; plot F1/Jaccard vs. token count.
  2. **Query ablation**: Compare fixed queries vs. query expansion (e.g., adding modality synonyms) for imaging extraction.
  3. **Chunk size sensitivity**: Test [64, 128, 256] token chunks with fixed overlap ratio; measure impact on antibiotic timeline Jaccard scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would retrieval of more than 60 chunks or alternative chunking strategies close the remaining performance gap between RAG and full-context approaches?
- Basis in paper: [explicit] "Further tuning the retrieval approach (queries, embedding model, retrieving more than 60 chunks, etc.), may close the remaining gap."
- Why unresolved: The study only tested 20, 40, and 60 chunks with a fixed 128-token sliding window; the optimal chunk count and size for each task type remains undetermined.
- What evidence would resolve it: Systematic ablation testing across varying chunk counts (e.g., 80, 100, 120) and chunk sizes with different overlap strategies for each clinical task.

### Open Question 2
- Question: Can RAG be enhanced to handle cases where critical information is missing from the local EHR, such as medication histories from transferred patients?
- Basis in paper: [explicit] "In 22.4% of the gold medications analyzed, the information needed to generate the gold standard medication and precise timespan is not present in the full clinical notes."
- Why unresolved: The paper identifies this as a fundamental limitation of longitudinal EHR datasets but does not propose or test mitigation strategies.
- What evidence would resolve it: Evaluation of retrieval augmentation from external data sources (e.g., health information exchanges) or prompt-based uncertainty signaling when information gaps are detected.

### Open Question 3
- Question: Does the embedding model's domain-specificity affect RAG performance differentially across extractive versus reasoning-intensive clinical tasks?
- Basis in paper: [inferred] The paper notes RAG performed best on Imaging Procedures (extraction) and showed smaller gains on Diagnosis Generation (reasoning), and mentions retrieval performance is sensitive to embedding model choice.
- Why unresolved: Only one embedding model (BGE-en-large-v1.5) was tested; whether clinical-domain embeddings would improve reasoning tasks more than extraction tasks is unknown.
- What evidence would resolve it: Comparative study using clinical-domain embedding models (e.g., BioBERT-based) versus general-purpose models across all three task types.

## Limitations
- Diagnosis generation task showed unexpectedly flat performance across all methods, suggesting evaluation methodology issues
- Dataset from single US hospital system limits generalizability to other clinical settings
- 32% of gold antibiotic medications were missed even with full context, indicating fundamental information gaps

## Confidence
- **High confidence**: RAG consistently achieves near-parity with full-context inputs while requiring far fewer tokens across all three tasks
- **Medium confidence**: Task-dependent performance patterns are valid but may be influenced by evaluation design choices
- **Low confidence**: Clinical impact assessments and generalizability beyond the specific hospital system remain speculative

## Next Checks
1. **Controlled task difficulty validation**: Re-run the diagnosis generation task with a simplified version that requires only explicit text matching (no medical inference) to isolate whether evaluation methodology or task complexity drives the flat performance curves
2. **Chunking strategy ablation**: Systematically test chunk sizes [64, 128, 256 tokens] and overlap ratios [10, 20, 30 tokens] on the antibiotic timeline task to quantify the tradeoff between retrieval precision and information fragmentation
3. **Retrieval quality analysis**: For the 20-chunk condition that missed 32% of gold antibiotics, conduct error analysis to determine whether failures stem from: (a) query formulation issues, (b) semantic misalignment between notes and retrieval embeddings, or (c) genuinely absent information in the clinical notes