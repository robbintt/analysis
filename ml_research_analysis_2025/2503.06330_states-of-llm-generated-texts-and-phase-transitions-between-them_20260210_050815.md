---
ver: rpa2
title: States of LLM-generated Texts and Phase Transitions between them
arxiv_id: '2503.06330'
source_url: https://arxiv.org/abs/2503.06330
tags:
- texts
- https
- text
- autocorrelations
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the statistical properties of texts generated
  by large language models (LLMs) by analyzing the autocorrelations of word embeddings.
  Drawing on concepts from solid-state physics, the author identifies three distinct
  phases of LLM-generated text: periodic, critical, and amorphous.'
---

# States of LLM-generated Texts and Phase Transitions between them

## Quick Facts
- arXiv ID: 2503.06330
- Source URL: https://arxiv.org/abs/2503.06330
- Reference count: 21
- Key result: LLM-generated text exhibits three phases (periodic, critical, amorphous) with phase transitions around temperature 0.8, characterized by distinct autocorrelation decay patterns.

## Executive Summary
This paper investigates the statistical properties of LLM-generated text by analyzing word embedding autocorrelations, revealing three distinct phases analogous to states in solid-state physics. Using temperature-controlled generation with Qwen and Phi models, the study identifies periodic, critical, and amorphous phases, with a clear phase transition occurring around temperature 0.8. The research demonstrates that transformer-based LLMs exhibit similar phase behavior, suggesting they belong to the same universality class, with amorphous phases showing exponential decay and critical phases exhibiting power-law decay over medium distances.

## Method Summary
The study generates 10,000-token texts from Moby Dick using temperature scaling (0.1-2.5, step 0.3) with two compact LLMs (Qwen2.5-1.5B and Phi-3-Mini-128K-Instruct), producing 10 samples per temperature. Text is tokenized and mapped to multilingual GloVe embeddings, with unknown tokens assigned zero vectors after centering. Autocorrelation functions are computed using cosine similarity for both short-range (τ=1-100) and long-range (τ=1-6000) analyses. FFT analysis identifies periodicity, while the GAPELMAPER metric classifies power-law versus exponential decay patterns.

## Key Results
- Three distinct phases identified: periodic (t<0.7), critical (0.7≤t≤1), and amorphous (t>1)
- Phase transition from ordered to amorphous states occurs at approximately temperature 0.8
- Amorphous phase shows exponential autocorrelation decay regardless of temperature
- Critical phase exhibits power-law decay over medium distances (up to 2000 words)

## Why This Works (Mechanism)
The phase transition analysis works because LLM temperature scaling directly controls the randomness of token selection, which manifests as distinct statistical patterns in the generated text's structure. Higher temperatures increase token diversity, breaking correlations and transitioning text from ordered (periodic) to disordered (amorphous) states, with critical behavior emerging at intermediate temperatures where power-law correlations indicate scale-free organization.

## Foundational Learning

**Autocorrelation Analysis**
- Why needed: Quantifies temporal dependencies in text sequences by measuring similarity between words at different positions
- Quick check: Compute C(τ) for synthetic periodic and random sequences to verify expected patterns

**Phase Transition Theory**
- Why needed: Provides mathematical framework for characterizing abrupt changes in statistical properties as system parameters vary
- Quick check: Verify critical exponents follow expected scaling laws near transition point

**Universality Classes**
- Why needed: Groups systems with similar critical behavior despite different microscopic details
- Quick check: Compare critical exponents across different LLM architectures

## Architecture Onboarding

**Component Map**
Prompt text -> Temperature-scaled LLM -> Token sequence -> GloVe embedding mapping -> Centered vector system -> Autocorrelation computation -> FFT analysis / GAPELMAPER metric

**Critical Path**
Text generation (temperature control) → Embedding representation → Autocorrelation calculation → Phase classification

**Design Tradeoffs**
- Embedding choice: GloVe provides static representations but struggles with OOV tokens versus contextual embeddings
- Temperature range: 0.1-2.5 captures transition but extreme values produce degenerate (perfectly periodic or meaningless) outputs
- Autocorrelation window: 1-6000 balances computational feasibility with capturing long-range dependencies

**Failure Signatures**
- Negative autocorrelations preventing log-scale plotting indicate periodic or gibberish regimes
- Undefined GAPELMAPER when autocorrelations go negative suggests filtering to 0.7-2.2 temperature range
- FFT artifacts from short sequences or edge effects in windowing

**First Experiments**
1. Generate texts at t=0.1, 0.8, and 2.0 to observe distinct phase characteristics
2. Plot autocorrelation functions on linear and log scales to identify decay patterns
3. Compute FFT max amplitude beyond DC component to detect periodicity

## Open Questions the Paper Calls Out
None

## Limitations
- The physical interpretation of text phases as analogous to solid-state physics phenomena remains metaphorical rather than fundamental
- GAPELMAPER metric is referenced from prior work but not fully specified, potentially limiting reproducibility
- Temperature scaling without other sampling parameters (top-k, top-p) may not generalize to typical LLM usage patterns

## Confidence

**High Confidence**: The existence of distinct phases in LLM-generated text as identified through autocorrelation analysis

**Medium Confidence**: The precise temperature boundaries between phases and the universality class claim

**Low Confidence**: The physical interpretation of text phases as analogous to solid-state physics phenomena

## Next Checks

1. Reproduce with alternative embeddings: Replicate analysis using contextual embeddings from the same LLMs or sentence transformers to verify phase identification is not an artifact of GloVe representation

2. Test across model families: Extend phase analysis to larger and architecturally distinct LLMs (GPT, encoder-decoder, non-transformer) to validate universality class claims

3. Validate GAPELMAPER implementation: Implement the exact GAPELMAPER metric from Mikhaylovskiy 2023 and verify phase classification consistency