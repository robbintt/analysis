---
ver: rpa2
title: Forget Less by Learning Together through Concept Consolidation
arxiv_id: '2601.01963'
source_url: https://arxiv.org/abs/2601.01963
tags:
- cidm
- learning
- concepts
- concept
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in custom diffusion
  models by enabling order-agnostic, concurrent concept learning through inter-concept
  interactions. It introduces FL2T, a framework that uses proxy embeddings and transformer
  decoders to guide feature selection across concepts, improving knowledge retention
  and transfer.
---

# Forget Less by Learning Together through Concept Consolidation

## Quick Facts
- arXiv ID: 2601.01963
- Source URL: https://arxiv.org/abs/2601.01963
- Reference count: 40
- Primary result: FL2T outperforms state-of-the-art models with fewer parameters and reference images, achieving at least 2% gain in average CLIP Image Alignment scores across three datasets.

## Executive Summary
This paper tackles catastrophic forgetting in custom diffusion models by enabling order-agnostic, concurrent concept learning through inter-concept interactions. It introduces FL2T, a framework that uses proxy embeddings and transformer decoders to guide feature selection across concepts, improving knowledge retention and transfer. The method outperforms state-of-the-art models with fewer parameters and reference images, achieving at least 2% gain in average CLIP Image Alignment scores across three datasets. FL2T also demonstrates superior scalability and parameter efficiency, maintaining performance with lower LoRA ranks. Qualitative results show improved identity preservation and context generation. Limitations include occasional context generation errors and overfitting to real images. Overall, FL2T provides an effective solution for personalized concept learning in diffusion models.

## Method Summary
FL2T is a two-step framework for order-agnostic concept-incremental flexible customization of diffusion models. First, it independently trains G LoRA models on G concepts for one epoch using Stable Diffusion v1.5, extracting stable concept embeddings. Second, it consolidates these concepts via proxy embeddings and a 2-layer transformer decoder that captures inter-concept interactions through attention mechanisms. The consolidation uses a combined loss (diffusion loss + weighted orthogonality + reconstruction + contrastive) and outputs aggregated weights for inference. This approach enables concurrent multi-concept learning while mitigating catastrophic forgetting.

## Key Results
- FL2T achieves at least 2% gain in average CLIP Image Alignment scores across three datasets compared to baselines
- Requires fewer reference images (3-5) to match or exceed baseline performance at 4+ images
- Uses fewer LoRA ranks while maintaining or improving performance, demonstrating parameter efficiency

## Why This Works (Mechanism)

### Mechanism 1: Inter-Concept Catalytic Aggregation
- **Claim:** Inter-concept interactions can act as a catalyst for generation quality rather than purely sources of interference, provided they are modulated via proxy-guided attention.
- **Mechanism:** FL2T employs learnable "proxy embeddings" that attend across concept embeddings to weigh the relevance of feature selection for the current task, effectively transferring robust features between concepts.
- **Core assumption:** Concepts share latent semantic subspaces that, if correctly identified via attention, can reinforce specific identity features without overwriting distinct characteristics.
- **Evidence anchors:**
  - [Section 5.2] "We hypothesise that this inter-concept interaction can rather catalyse generation abilities of concepts and reduce catastrophic forgetting."
  - [Abstract] "set-invariant inter-concept learning module where proxies guide feature selection across concepts..."
- **Break condition:** If concepts are entirely semantically orthogonal, the attention mechanism may fail to find relevant shared features, reducing the "catalytic" benefit to noise.

### Mechanism 2: Set-Invariant Representation Learning
- **Claim:** Formulating multi-concept learning as a set-input problem with permutation-invariant operations allows the model to learn concepts in an order-agnostic manner.
- **Mechanism:** The architecture utilizes Transformer decoders which possess permutation invariance. By inputting concept embeddings as a set, the model consolidates them such that the output does not depend on the sequence in which concepts were introduced.
- **Core assumption:** The desired consolidated model state is a function of the *set* of all concepts, not the trajectory of the learning sequence.
- **Evidence anchors:**
  - [Section 5] "Utilizing the permutation invariance property of transformers, we develop a concept interaction module..."
- **Break condition:** If the training data distribution is heavily temporally correlated (concept drift), strict permutation invariance might ignore valid temporal priors.

### Mechanism 3: Geometry-Aware Gradient Aggregation (Reduced Drift)
- **Claim:** Aggregating concept updates using unnormalized attention weights strictly bounds model drift and can reduce it compared to uniform summation (simple averaging).
- **Mechanism:** Theoretical analysis (Theorem A.1) shows that weighting gradients by attention coefficients allows for cancellation of dominant, conflicting components, minimizing the norm of the aggregate update vector.
- **Core assumption:** Gradient directions from different concepts are not perfectly aligned; therefore, selective weighting is superior to uniform averaging.
- **Evidence anchors:**
  - [Appendix A.5] "Theorem A.1... proves that unnormalized attention can strictly reduce drift relative to uniform summation."
- **Break condition:** If concept gradients are perfectly orthogonal, the geometry-aware weighting offers no advantage over summation, though it theoretically does no harm.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The central problem FL2T solves. You must understand that fine-tuning on new concepts typically degrades performance on old ones due to weight overwriting.
  - **Quick check question:** Why does standard LoRA fine-tuning fail when sequentially adding concepts?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** FL2T uses LoRA for the "Independent Concept Training" step. Understanding that LoRA adds low-rank matrices ($\Delta W = AB$) rather than modifying full weights is crucial to understanding the consolidation math ($R_1, R_2$).
  - **Quick check question:** How does FL2T aggregate the low-rank matrices ($A$ and $B$) from multiple concepts during inference?

- **Concept: Set Transformers / Permutation Invariance**
  - **Why needed here:** The "Aggregation" step relies on the Transformer's ability to handle sets of embeddings (concepts) regardless of order.
  - **Quick check question:** Why is the "order-agnostic" property dependent on the permutation invariance of the attention mechanism?

## Architecture Onboarding

- **Component map:** Reference images/text for G concepts -> Separate LoRA models per concept -> Stable concept embeddings $C_i$ and LoRA weights -> Proxy embeddings $P_i = C_i$ -> 2-layer Transformer decoder with self-attention over proxies and cross-attention with text prompt -> Aggregated UNet ($\epsilon_{\theta*}$) using weighted LoRA combination

- **Critical path:** The **Step 1 to Step 2 transition** is critical. If Step 1 LoRAs are unstable or overfitted, the Proxy embeddings initialized from them will propagate errors into the Transformer aggregator.

- **Design tradeoffs:**
  - **Transformer Layers:** Paper finds **2 layers** optimal (Table 2). 1 layer misses feature relations; >2 layers risk "rank collapse" (reverting to a mean vector).
  - **Parameters vs. Performance:** FL2T uses more parameters (52.6M) than the SOTA baseline (38.4M) due to the Transformer module, but achieves this with a lower *LoRA rank* (higher efficiency in the base model).

- **Failure signatures:**
  - **Context Generation Errors:** Generating the correct identity but wrong background/activity (attributed to weak priors in training data).
  - **Identity Bleeding:** If contrastive loss ($R_3$) is too weak, generated images may mix features of Concept A and Concept B.
  - **Rank Collapse:** If >2 transformer layers are used, proxy embeddings may converge to identical vectors, losing concept distinctness.

- **First 3 experiments:**
  1. **Layer Ablation:** Train FL2T with 1, 2, 3, and 4 transformer layers. Verify the "rank collapse" phenomenon by checking the variance of the output proxy embeddings.
  2. **Reference Efficiency:** Train with only 1, 2, and 3 reference images. Verify if FL2T matches the baseline's performance (at 4+ images) using only 3 images (Fig 3a).
  3. **Drift Verification:** Implement the gradient aggregation logic (Uniform vs. Attention-weighted) and measure the norm of the weight update $\|\Delta \theta\|$ to validate Theorem A.1 empirically.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can FL2T be extended to scale beyond 30 concepts given the hard limit of the CLIP tokenizer? The current framework relies on creating new token embeddings for each concept, which creates an architectural bottleneck that prevents lifelong learning at scale.

- **Open Question 2:** How does the model's ability to mitigate forgetting change when the "distinct concepts" constraint is violated? The orthogonal subspace regularizer ($R_1$) forces concept subspaces apart; it is unclear if this hampers learning when concepts naturally share features.

- **Open Question 3:** Can the framework be modified to reduce overfitting to training images while maintaining identity preservation? The paper notes the model "may overfit to real images when the prompt closely mirrors conditions seen during training, limiting its generalization."

## Limitations
- Context generation failures for rare concept-context pairs due to weak priors in training data
- Potential overfitting to reference images when prompts closely mirror training conditions
- Scalability limited to 30 concepts due to CLIP tokenizer constraints

## Confidence
- **High confidence:** The core claim that inter-concept interactions via proxy-guided attention can improve knowledge retention and transfer is well-supported by quantitative results (2%+ CLIP-IA improvement) and ablation studies (optimal 2-layer transformer).
- **Medium confidence:** The theoretical analysis of geometry-aware gradient aggregation (Theorem A.1) showing reduced drift is sound but relies on assumptions about gradient behavior that may not hold for all concept combinations.
- **Medium confidence:** The set-invariant representation learning claim is supported by architecture design but requires stronger empirical validation across varying concept learning orders and dataset conditions.

## Next Checks
1. **Gradient aggregation verification:** Implement and measure the norm of weight updates $\|\Delta \theta\|$ for both uniform summation and attention-weighted aggregation to empirically validate Theorem A.1's claim about reduced drift.
2. **Rank collapse monitoring:** Track variance of proxy embeddings across concepts during training with varying transformer depths (1-4 layers) to verify the theoretical prediction of rank collapse beyond 2 layers.
3. **Reference efficiency validation:** Systematically reduce reference images per concept (1, 2, 3, 4) and measure performance degradation to confirm the claimed efficiency advantage over baselines.