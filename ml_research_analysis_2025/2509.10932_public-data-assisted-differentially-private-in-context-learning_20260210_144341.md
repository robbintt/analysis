---
ver: rpa2
title: Public Data Assisted Differentially Private In-Context Learning
arxiv_id: '2509.10932'
source_url: https://arxiv.org/abs/2509.10932
tags:
- public
- privacy
- private
- data
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of privacy protection in in-context
  learning (ICL) for large language models (LLMs). The authors propose a differentially
  private (DP) ICL framework that leverages public data to enhance utility while maintaining
  privacy guarantees.
---

# Public Data Assisted Differentially Private In-Context Learning

## Quick Facts
- arXiv ID: 2509.10932
- Source URL: https://arxiv.org/abs/2509.10932
- Authors: Seongho Joo; Hyukhun Koh; Kyomin Jung
- Reference count: 37
- One-line primary result: Differentially private ICL framework using public data achieves non-private baseline performance at strong privacy (ε=1)

## Executive Summary
This work addresses privacy protection in in-context learning (ICL) for large language models by proposing a differentially private framework that leverages public data to enhance utility. The method uses private clustering in semantic embedding space combined with public data for candidate selection to mitigate utility degradation caused by differential privacy. Experimental results on question-answering and summarization tasks show the approach achieves comparable performance to non-private baselines at strong privacy levels (ε=1) and outperforms private data-only methods.

## Method Summary
The framework subsamples and partitions datasets, generates ensemble responses via LLM, embeds responses using text-embedding-ada-002, and applies private k-means clustering (DPM) to produce privatized centroids and weights. It then maps centroids to nearest public representatives and passes top-k candidates with a public one-shot example to an LLM for final selection. The approach consumes privacy budget only at the DPM clustering step, while all downstream operations use only privatized outputs and public data.

## Key Results
- Achieves comparable performance to non-private baselines at ε=1 with BLEU scores of 22.21 (SGA) vs 24.71 (non-private)
- Outperforms private data-only methods by 5-15% BLEU through public data candidate selection
- Demonstrates robustness against membership inference attacks with AUROC ≤0.6 at ε=1
- Benefits from both in-distribution and out-of-distribution public data

## Why This Works (Mechanism)

### Mechanism 1
- Projecting LLM outputs into semantic embedding space enables private aggregation of high-dimensional text responses
- Framework uses text-embedding-ada-002 to map responses into lower-dimensional space where similar responses cluster
- Private k-means clustering aggregates responses while adding calibrated noise, producing privatized cluster centroids and weights
- Core assumption: Semantic similarity in embedding space corresponds to functional equivalence for downstream task utility

### Mechanism 2
- Public data provides noise-resistant candidate anchors for final selection, recovering utility lost to DP-induced histogram errors
- After private clustering, framework selects closest public element to each privatized centroid as class representative
- Passes top-k candidates to LLM with public one-shot example for final selection
- Core assumption: Public data contains sufficiently task-relevant examples to serve as valid proxies for private cluster content

### Mechanism 3
- Subsampling provides privacy amplification while reducing memory costs
- Framework uniformly subsamples p% from datasets before ICL
- Under subsampling without replacement, (ε, δ)-DP algorithm achieves approximately pε effective privacy loss
- Core assumption: Subsampled examples remain representative of full dataset for ICL task

## Foundational Learning

- Concept: Differential Privacy (ε, δ)-DP
  - Why needed: Core mathematical guarantee that bounds information leakage
  - Quick check: If ε=1, what does this quantitatively bound about an adversary's ability to distinguish whether a specific record was used?

- Concept: Privacy Amplification by Subsampling
  - Why needed: Enables stronger effective privacy without increasing noise
  - Quick check: Why does randomly excluding records improve privacy guarantees rather than just reducing data available?

- Concept: Post-Processing Property of DP
  - Why needed: Justifies why public data can be used after DP mechanism without violating guarantees
  - Quick check: If M(D) is (ε, δ)-DP, can arbitrary computations using public data on M(D)'s output violate the guarantee?

## Architecture Onboarding

- Component map: Data Layer -> Subsampling Module -> Prompt Constructor -> LLM Inference -> Embedding Encoder -> Private Clustering (DPM) -> Candidate Selector -> Final Selector LLM
- Critical path: Prompt construction → LLM ensemble generation → Embedding → DPM clustering → Public candidate mapping → Final selection. Privacy budget consumed at DPM step only; all downstream operations use only privatized outputs + public data.
- Design tradeoffs:
  - Higher ensemble size (N) → Better aggregation quality but O(N) compute cost
  - Lower ε → Stronger privacy but noisier histograms, requiring more reliance on public data for recovery
  - Larger k (candidates) → More robust to noise but increased confusion for final selector
  - ID vs OOD public data: ID performs better, but OOD remains valuable under tight privacy
- Failure signatures:
  - Utility collapse at ε=1 with KSA: Keyword reconstruction fails on long responses
  - Candidate confusion: k=8 underperforms k=6 at low ε due to noisy candidates overwhelming selector
  - Privacy accumulation: Multiple queries consume budget; framework does not address long-running deployment
- First 3 experiments:
  1. Reproduce SGA vs KSA gap: Run both methods on ChatDoctor at ε={1,3,8} with N=100 ensemble
  2. Ablate public data contribution: Compare SGA (top-k) vs SGA (top-1) vs KSA w/o public
  3. Validate privacy guarantee empirically: Implement repeat-attack MIA with balanced/unbalanced settings

## Open Questions the Paper Calls Out

- Can the reliance on large-scale ensembles (e.g., N=100) be reduced through optimized differential privacy mechanisms without degrading the privacy-utility trade-off?
- How can the framework be adapted to handle privacy budget accumulation in long-running, high-volume query environments without exhausting the privacy budget?
- What are the lower bounds of public data quality or relevance required for the semantic clustering approach to remain effective?

## Limitations

- Exact DPM configuration (full hyperparameter set) is not fully specified, making precise reproduction uncertain
- Subsampling rate ambiguity prevents verification of effective privacy budget
- Long-running deployment not addressed - framework does not handle privacy budget accumulation across multiple queries
- Computational cost of large ensemble sizes (e.g., N=100) limits real-time applicability

## Confidence

- High confidence: Semantic embedding space aggregation mechanism and public data candidate selection are well-supported by empirical results
- Medium confidence: Privacy amplification via subsampling is theoretically sound but requires full DPM configuration to validate
- Low confidence: Long-term deployment viability due to unhandled budget accumulation

## Next Checks

1. Validate privacy budget accounting: Implement RDP composition across multiple queries with prv_accountant and confirm δ does not exceed 2.56e-4 per query
2. Ablate public data contribution: Run SGA with and without public candidate selection to quantify utility recovery
3. Test embedding model robustness: Swap text-embedding-ada-002 with domain-mismatched model to confirm semantic clustering fails as predicted when embedding assumptions break