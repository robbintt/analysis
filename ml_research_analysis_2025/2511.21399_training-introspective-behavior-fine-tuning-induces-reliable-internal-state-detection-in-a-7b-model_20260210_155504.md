---
ver: rpa2
title: 'Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State
  Detection in a 7B Model'
arxiv_id: '2511.21399'
source_url: https://arxiv.org/abs/2511.21399
tags:
- training
- detection
- concepts
- injection
- lindsey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that introspective awareness\u2014the\
  \ ability to detect and report on injected internal states\u2014can be reliably\
  \ trained in small language models rather than waiting for emergence at scale. The\
  \ authors fine-tune a 7B parameter model to detect and identify transient activation\
  \ patterns injected at a single token position, transforming it from near-zero performance\
  \ (0.4% accuracy, 6.7% false positives) to reliable detection (85% accuracy on held-out\
  \ concepts, 0% false positives)."
---

# Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model

## Quick Facts
- arXiv ID: 2511.21399
- Source URL: https://arxiv.org/abs/2511.21399
- Reference count: 8
- Primary result: Fine-tuning induces reliable introspective awareness in a 7B model

## Executive Summary
This paper demonstrates that introspective awareness—the ability to detect and report on injected internal states—can be reliably trained in small language models rather than waiting for emergence at scale. The authors fine-tune a 7B parameter model to detect and identify transient activation patterns injected at a single token position, transforming it from near-zero performance (0.4% accuracy, 6.7% false positives) to reliable detection (85% accuracy on held-out concepts, 0% false positives). The trained model satisfies three of Lindsey's criteria for introspection: accuracy, grounding, and internality. Generalization to unseen concept vectors (7.5pp gap) shows the model learns a transferable decoding skill rather than memorizing specific vectors, though this does not establish metacognitive representation. The optimal operating range occurs at moderate injection strengths (α=40), with performance degrading at higher strengths due to repetition collapse.

## Method Summary
The methodology extracts concept vectors by contrasting hidden states from concept-specific prompts against neutral baselines, then injects these vectors at a single token position during generation. LoRA fine-tuning with rank-32 adapters on attention matrices trains the model to detect and identify these injections across multiple prompt phrasings. The approach uses transient single-token injection rather than persistent steering, creating a clean test of the model's ability to monitor its own activation space. Training proceeds on balanced positive/negative examples with injection strengths ranging from α=40 to α=100, with evaluation focusing on held-out concepts to test generalization.

## Key Results
- Untrained model achieves 0.4% detection accuracy with 6.7% false positive rate
- Trained model achieves 85% accuracy on held-out concepts with 0% false positive rate
- Optimal detection occurs at α=40 injection strength; higher strengths cause repetition collapse
- 7.5pp gap between training and test concept performance indicates learned generalization skill

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Vector Extraction Isolates Concept-Specific Activations
Subtracting mean baseline activations from concept elicitation prompts yields normalized vectors encoding semantic content at consistent magnitudes. For each concept c, record hidden state at layer 20 after prompt "Tell me about {c}". Subtract mean of 32 neutral object activations, then L2-normalize. This removes generic linguistic structure, leaving concept-specific direction. The injection layer (≈2/3 through network) has sufficient semantic abstraction while retaining downstream processing capacity.

### Mechanism 2: LoRA Fine-Tuning Induces Anomaly Detection Without Catastrophic Forgetting
Parameter-efficient fine-tuning (LoRA on attention matrices) allows the model to learn a monitoring skill while preserving base capabilities. LoRA with rank=32, alpha=64 modifies only attention projections, adding ~31M parameters (0.45% of total). Training on balanced positive/negative examples with 5 prompt variations teaches detection that generalizes across phrasing. The detection skill is learnable via supervised signal and does not require modifying deeper feedforward layers.

### Mechanism 3: Moderate Injection Strength Creates Detectable Signal Without Generation Collapse
An optimal injection strength (α=40) exists where the anomaly is salient enough for detection but not so strong it disrupts autoregressive generation. At α=40, 95% detection with 85% correct identification. At α=100, detection remains 95% but identification drops to 55% due to "repetition collapse" (output loops like "I detect I detect..."). The model has a "generation stability threshold" beyond which steering vectors interfere with sampling coherence.

## Foundational Learning

- **Steering Vectors / Activation Addition**
  - Why needed here: The entire methodology depends on understanding how to inject concept representations into hidden states during forward passes.
  - Quick check question: Can you explain why adding α·v_c to h_l* changes model behavior without modifying weights?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Parameter-efficient fine-tuning is the mechanism by which detection capability is induced without full model retraining.
  - Quick check question: What is the relationship between LoRA rank, alpha, and effective learning capacity?

- **Autoregressive Generation and KV Caching**
  - Why needed here: Transient injection at a single token position requires understanding how information flows across generation steps.
  - Quick check question: Why does injecting at only the final prompt token still affect subsequent generated tokens?

## Architecture Onboarding

- **Component map**: Concept extraction -> Vector normalization -> LoRA adapter training -> Detection evaluation
- **Critical path**: 1. Extract baseline activations from 32 neutral concepts → compute h̄; 2. For each training concept, extract v_c = normalize(h(c) - h̄); 3. Construct training pairs with injection/no-injection labels and varied prompts; 4. Fine-tune with LoRA, injecting during forward pass via hook; 5. Evaluate on held-out concepts at α=40
- **Design tradeoffs**: Higher α → better detection but more repetition collapse; More training concepts → better generalization but longer training; Single-token vs. persistent injection → harder task but cleaner test of temporal monitoring; LoRA rank → capacity for learning vs. risk of overfitting
- **Failure signatures**: Memorization (near-100% on training concepts, low on test concepts); Repetition collapse (output loops at high α); False positives (non-zero FPR); Refusal responses ("I'm an AI language model and cannot...")
- **First 3 experiments**: 1. Baseline sanity check: Run untrained model on 10 concepts with α=40; expect ~1% detection, ~6% FPR; 2. Strength sweep: After training, evaluate single held-out concept across α∈{40,60,80,100}; expect inverted-U for identification; 3. Generalization test: Compare training vs. test concept accuracy at α=40; expect <15pp gap if skill transferred

## Open Questions the Paper Calls Out

- **Question**: Does the trained introspection capability involve genuine metacognitive representation (internal self-monitoring) or merely sophisticated pattern-matching on activation directions?
  - Basis in paper: The authors explicitly state they cannot establish Lindsey's fourth criterion, noting the model may be performing "sophisticated pattern-matching... without any internal self-awareness."
  - Why unresolved: Behavioral success and generalization to unseen vectors do not prove the model has registered the "metacognitive fact" of its state; it may simply map perturbations to labels.
  - What evidence would resolve it: Experiments designed to distinguish internal self-monitoring from input-output mapping, such as testing if the model can use the detected state to modulate downstream reasoning in novel ways.

- **Question**: Can fine-tuning reliably induce the other three components of introspection defined by Lindsey: thought/text discrimination, prefill detection, and intentional control?
  - Basis in paper: The paper notes, "We do not address Lindsey's other three experiments... whether training improves performance on those tasks remains future work."
  - Why unresolved: This study focused exclusively on the first experiment (self-report of injected thoughts), leaving the trainability of the remaining capabilities unknown.
  - What evidence would resolve it: Applying the transient injection fine-tuning methodology to the other three experimental paradigms outlined in Lindsey (2025).

- **Question**: Does the capability to detect artificially injected steering vectors transfer to identifying naturally occurring strong activations or anomalous internal states?
  - Basis in paper: The authors list this as a limitation, asking, "Whether this capability transfers to detecting naturally occurring strong activations (e.g., from emotionally charged prompts) remains an open question."
  - Why unresolved: The vectors used were artificially constructed via contrastive prompts, which may occupy different subspaces or have different properties than natural activations.
  - What evidence would resolve it: Evaluating the fine-tuned model on prompts that elicit strong, spontaneous internal states (e.g., lying, uncertainty) without manual vector injection.

## Limitations

- Concept vector quality uncertainty: The extraction protocol relies on single prompts that may not produce stable representations across contexts
- Temporal generalization unexplored: The model is tested only on single-token injection detection, not continuous monitoring
- False positive interpretation requires caution: The 0% FPR may reflect overconservative behavior rather than robust grounding

## Confidence

- **High confidence**: The empirical finding that LoRA fine-tuning can transform a 7B model from near-zero to reliable introspective detection (85% accuracy, 0% FPR) is well-supported by the experimental results
- **Medium confidence**: The claim that the model satisfies Lindsey's criteria for introspection (accuracy, grounding, internality) is supported by the evidence, but the internality criterion requires additional adversarial testing
- **Low confidence**: The assertion that this demonstrates introspective awareness is premature; the model learns to detect and report injected patterns but does not demonstrate self-directed reflection

## Next Checks

1. **Adversarial detection testing**: Systematically vary injection strength below α=40 and test whether the model produces false positives, establishing the detection threshold and robustness to weak signals

2. **Cross-context vector stability**: Extract concept vectors using 3-5 different prompt phrasings per concept and measure detection accuracy variance to test vector stability

3. **Persistent injection evaluation**: Modify the injection protocol to apply vectors at multiple token positions and evaluate whether the trained model can detect all instances to test temporal generalization