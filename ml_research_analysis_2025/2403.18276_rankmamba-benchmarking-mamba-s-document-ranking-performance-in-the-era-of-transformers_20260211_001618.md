---
ver: rpa2
title: 'RankMamba: Benchmarking Mamba''s Document Ranking Performance in the Era of
  Transformers'
arxiv_id: '2403.18276'
source_url: https://arxiv.org/abs/2403.18276
tags:
- training
- document
- attention
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks Mamba models in the document ranking task,
  a critical component of modern search systems. While transformer-based models have
  dominated the field, their quadratic time complexity in training and linear complexity
  in inference pose challenges for scalability.
---

# RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers

## Quick Facts
- arXiv ID: 2403.18276
- Source URL: https://arxiv.org/abs/2403.18276
- Authors: Zhichao Xu
- Reference count: 11
- Primary result: Mamba models achieve competitive document ranking performance compared to transformer-based models of similar sizes on MS MARCO and TREC DL datasets.

## Executive Summary
This study benchmarks Mamba models in the document ranking task, comparing them against various transformer-based models across different sizes and pre-training objectives. The researchers evaluated these models on the MS MARCO document ranking dataset and TREC DL19/20 datasets using a contrastive learning training strategy. Results show that Mamba models achieve competitive performance compared to transformer-based models of similar sizes, with state-spaces/mamba-130m outperforming other models around 110M parameters in NDCG@10 on TREC DL19 and DL20 datasets. However, Mamba models exhibited lower training throughput compared to efficient transformer implementations like Flash Attention, suggesting room for optimization in their current implementation.

## Method Summary
The researchers compared Mamba models with various transformer-based models (BERT, RoBERTa, OPT, Pythia, T5) on document ranking tasks. They used the MS MARCO document ranking dataset with InfoNCE contrastive loss, sampling 1 positive document and 7 hard negatives per query from BM25 top-100 results. Input format for Mamba was `document:{d}\n\nquery:{q}[EOS]` with 512-token truncation. Models were trained for 1 epoch using AdamW optimizer with linear warmup, evaluating on MS MARCO Dev (MRR@100) and TREC DL19/20 (NDCG@10).

## Key Results
- Mamba models achieve competitive ranking performance compared to transformer-based models of similar sizes
- state-spaces/mamba-130m outperformed other models around 110M parameters in NDCG@10 on TREC DL19 and DL20 datasets
- Mamba models exhibited lower training throughput compared to efficient transformer implementations like Flash Attention

## Why This Works (Mechanism)

### Mechanism 1: Selective State Compression for Query-Document Interaction
Mamba captures query-document relevance through input-dependent state compression rather than attention-based token interaction. The selective scan (S6) mechanism makes parameters (∆, B, C) input-dependent, allowing the model to selectively retain or discard information as it processes the concatenated document-query sequence. The query tokens at the end of the sequence can influence the final hidden state based on document content processed earlier.

### Mechanism 2: Unidirectional Processing with Late Query Positioning
Positioning the query after the document in the input sequence enables effective relevance modeling with unidirectional Mamba models. The input format `document:{d}\n\nquery:{q}[EOS]` ensures that when processing query tokens, the model's hidden state already contains compressed information from the full document. The final [EOS] representation aggregates query-document interaction signals for relevance scoring.

### Mechanism 3: InfoNCE Contrastive Training for Relevance Calibration
Contrastive learning with hard negatives calibrates Mamba's relevance scores comparably to transformer-based approaches. The InfoNCE loss forces the model to discriminate between relevant documents and BM25-sampled hard negatives within the same batch, pushing similar non-relevant documents apart in embedding space while pulling relevant pairs together.

## Foundational Learning

- **Concept: State Space Models (SSM) and Discretization**
  - Why needed here: Mamba builds on S4-style continuous-to-discrete transformation; understanding how h'(t) = Ah(t) + Bx(t) discretizes to ht = Āht-1 + B̄xt explains the RNN-like sequential dependency.
  - Quick check question: Can you explain why SSMs can be viewed as both RNNs (recurrence) and CNNs (convolution)?

- **Concept: Linear Time Invariance (LTI) vs. Time-Varying SSMs**
  - Why needed here: Mamba's key innovation is breaking LTI by making ∆, B, C input-dependent; this enables content-aware filtering but requires specialized hardware optimization.
  - Quick check question: Why does making SSM parameters input-dependent prevent naive CNN-mode parallel training?

- **Concept: Neural Reranking and First-Stage Retrieval**
  - Why needed here: The task assumes a BM25 retriever provides top-100 candidates; understanding this two-stage pipeline contextualizes why reranking performance depends on retriever quality.
  - Quick check question: How would changing the first-stage retriever from BM25 to a dense retriever affect the reranker's observed performance?

## Architecture Onboarding

- **Component map:** Pre-trained Mamba -> Input formatter (document-query concat) -> Mamba backbone -> [EOS] representation -> Scoring FFN -> Scalar relevance score
- **Critical path:** Load pre-trained Mamba weights -> Format query-document pairs -> Forward pass through Mamba -> Extract final hidden state -> Apply scoring FFN -> Compute InfoNCE loss
- **Design tradeoffs:**
  - Throughput vs. Quality: Mamba shows competitive NDCG@10 but ~40-60% lower training throughput vs. Flash Attention transformers
  - Unidirectional vs. Bidirectional: Mamba cannot leverage bidirectional context like BERT/RoBERTa; may underperform on tasks requiring document tokens to "see" query tokens
  - LoRA scaling: Models >700M require LoRA (rank=32) for memory feasibility, which slightly degrades performance vs. full fine-tuning
- **Failure signatures:**
  - Low MRR but high NDCG@10: Suggests model ranks relevant documents well but not at top position
  - Training instability for large models: If loss spikes, reduce learning rate or increase warmup steps
  - Memory overflow at 790M: Must use LoRA; if still OOM, reduce batch size or enable gradient checkpointing
- **First 3 experiments:**
  1. Train mamba-130m and bert-base-uncased with identical hyperparameters; verify NDCG@10 on TREC DL19/20 matches reported ~0.66 vs ~0.65
  2. Measure tokens/sec and GPU memory for mamba-370m vs. pythia-410m with Flash Attention on A6000; quantify the throughput gap
  3. Swap to `query:{q}\n\ndocument:{d}[EOS]` and observe performance drop; confirm late-query positioning is critical for unidirectional models

## Open Questions the Paper Calls Out

### Open Question 1
Why does the performance discrepancy exist between decoder-only models (OPT, Pythia, T5) on MSMARCO Dev versus TREC DL19/20 datasets? The authors identify this pattern but do not investigate whether the cause is annotation scheme differences, judgment density, or model architecture interactions with evaluation metrics.

### Open Question 2
Why does Mamba exhibit lower training throughput than Flash Attention-equipped transformers despite its theoretical O(n) training complexity advantage? The paper identifies the gap between theory and practice but does not profile which components cause the bottleneck.

### Open Question 3
Can knowledge distillation from larger transformer-based rerankers improve Mamba's document ranking effectiveness? The study deliberately scoped to contrastive learning only, leaving unexplored whether Mamba's sequential processing limitations could be compensated by distillation techniques.

### Open Question 4
How does Mamba's ranking performance scale beyond 1B parameters and with more capable first-stage retrievers than BM25? Computational constraints limited the study to sub-1B models and a sparse retriever; scaling behavior and retriever-dependent performance remain unknown.

## Limitations
- Current Mamba implementation shows ~40-60% lower training throughput compared to efficient transformer implementations like Flash Attention
- Unidirectional nature of Mamba may limit performance on tasks requiring bidirectional context or fine-grained token-level alignment
- Reliance on BM25-sampled hard negatives may introduce distribution shift between training and evaluation

## Confidence
- **High Confidence:** Mamba models achieve competitive ranking performance compared to transformer-based models of similar sizes on MS MARCO and TREC DL datasets
- **Medium Confidence:** Mamba's linear time complexity advantage over transformers holds in theory and is partially reflected in training efficiency
- **Low Confidence:** The selective state compression mechanism captures relevance signals as effectively as attention-based approaches without more extensive ablation studies

## Next Checks
1. Implement and measure optimized Mamba kernels using the latest state-spaces library updates to quantify the true throughput gap versus Flash Attention transformers
2. Conduct ablation study comparing Mamba with different input orderings and bidirectional variants to rigorously test unidirectional processing effectiveness
3. Evaluate model generalization by testing Mamba and transformer baselines on datasets with different first-stage retrievers (dense vs. sparse) to assess robustness to BM25 sampling bias