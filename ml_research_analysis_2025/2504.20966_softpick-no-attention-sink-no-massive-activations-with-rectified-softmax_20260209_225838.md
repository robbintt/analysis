---
ver: rpa2
title: 'Softpick: No Attention Sink, No Massive Activations with Rectified Softmax'
arxiv_id: '2504.20966'
source_url: https://arxiv.org/abs/2504.20966
tags:
- layer
- head
- softpick
- attention
- softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Softpick is a rectified, non-sum-to-one replacement for softmax
  in transformer attention that eliminates attention sink and massive activation outliers.
  It achieves 0% sink rate and produces highly sparse attention maps with reduced
  kurtosis in hidden states.
---

# Softpick: No Attention Sink, No Massive Activations with Rectified Softmax

## Quick Facts
- arXiv ID: 2504.20966
- Source URL: https://arxiv.org/abs/2504.20966
- Reference count: 40
- Primary result: Softpick eliminates attention sinks and reduces massive activations, achieving 0% sink rate and 95%+ sparsity, with improved low-bit quantization robustness

## Executive Summary
Softpick is a rectified, non-sum-to-one replacement for softmax in transformer attention that eliminates attention sinks and massive activation outliers. By breaking the sum-to-one constraint through asymmetric rectification, softpick produces genuinely sparse attention maps while preserving gradient flow via absolute values in the denominator. The method achieves 0% sink rate and significantly reduces hidden state kurtosis (hundred-fold reduction), enabling superior low-bit quantization. However, scaling remains problematic—larger models underperform due to increased dead attention heads and underscoring effects that weaken value signals in long contexts.

## Method Summary
Softpick replaces softmax with a rectified normalization scheme: ReLU(e^(x_i-m) - e^(-m)) / (Σ|e^(x_j-m) - e^(-m)| + ε), where m=max(x) and ε=1e-6. This breaks the sum-to-one constraint by allowing true zeros when inputs are negative, while absolute values in the denominator preserve gradient flow to negative-scoring tokens. The method is implemented as a drop-in replacement in attention modules, compatible with FlashAttention-2 kernels. Training uses LLaMA-style architecture with RoPE, SwiGLU MLP, and AdamW optimization on FineWeb-Edu corpus.

## Key Results
- Eliminates attention sinks: 0% sink rate at both 340M and 1.8B scales
- Reduces massive activations: Kurtosis drops from 33,510 to 340 (hundred-fold reduction)
- Achieves 95%+ attention sparsity with improved low-bit quantization accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing softmax's sum-to-one normalization with rectified asymmetric normalization eliminates attention sinks
- Mechanism: Softmax's constraint forces probability mass redistribution even to irrelevant tokens. Softpick's rectified numerator allows true zeros when e^(x_i) < 1, while absolute denominator prevents strict normalization, eliminating sink pressure
- Core assumption: Attention sinks are caused by sum-to-one constraint, not learned semantic behavior
- Evidence: 0% sink rate reported, related work (Gated Attention, TDA) targets same constraint
- Break condition: If sinks persist despite sum-to-one removal

### Mechanism 2
- Claim: Eliminating attention sinks cascades to suppress massive activation outliers
- Mechanism: Sinks cause disproportionate weight allocation, propagating extreme activations through residual connections. Softpick's sparsity prevents this amplification
- Core assumption: Massive activations are downstream effects of sink behavior
- Evidence: Hundred-fold kurtosis reduction (33,510 → 340), massive activation literature links both phenomena
- Break condition: If massive activations persist without sinks

### Mechanism 3
- Claim: Absolute values in denominator preserve gradient flow to negative inputs
- Mechanism: Pure ReLU normalization would zero gradients for negative inputs, causing permanent head death. Softpick's |x| maintains non-zero denominator derivatives, allowing head recovery
- Core assumption: Gradient flow to negative-scoring tokens is necessary for head recovery
- Evidence: Dead heads decrease over training (17.19% → 9.11% at 340M), sign function preserves flow
- Break condition: If dead heads accumulate irreversibly during training

## Foundational Learning

- **Attention sink phenomenon**
  - Why needed: Softpick's motivation stems from understanding why softmax allocates attention to semantically weak tokens
  - Quick check: Can you explain why softmax cannot produce a true zero attention score, and why this matters for token selection?

- **Massive activations and quantization**
  - Why needed: Softpick's practical value (better low-bit quantization) depends on understanding why outlier activations break quantization
  - Quick check: Why do extreme activation values disproportionately degrade low-precision quantization, and what does kurtosis measure?

- **Dead neurons and gradient flow**
  - Why needed: ReLU modifications risk creating components that never activate; understanding gradient preservation is essential
  - Quick check: If a ReLU unit always outputs zero, what happens to its gradient, and how might |x| in a denominator change this?

## Architecture Onboarding

- **Component map**: Softpick(x) = ReLU(e^(x-m) - e^(-m)) / (Σ|e^(x_j-m) - e^(-m)| + ε) — numerically safe version

- **Critical path**: Replace softmax in attention with softpick, ensure FlashAttention kernel supports operations, monitor dead heads and sink rate, track kurtosis and sparsity

- **Design tradeoffs**: Sparsity vs. signal strength (underscoring in long contexts), quantization robustness vs. raw performance, gradient stability vs. head utilization

- **Failure signatures**: Training divergence (check implementation), sink rate > 0% (verify ReLU placement), long-context retrieval failure (underscoring effect), dead head accumulation (learning rate too high)

- **First 3 experiments**:
  1. **Ablation**: Train 340M with rectified-only (no abs in denominator) for 10k steps. Expect: sink rate remains high (>25%), some heads die permanently
  2. **Quantization stress test**: Quantize 340M softpick and softmax models to 2/3/4 bits. Expect: softpick retains >50% relative accuracy at 2-bit where softmax collapses
  3. **Scaling diagnostic**: Train 340M and 1.8B with identical hyperparameters. Measure dead head % at 10k, 50k, 100k steps. Expect: 1.8B shows ~2x dead head rate

## Open Questions the Paper Calls Out

- **Can underscoring be mitigated to improve long-context retrieval performance?**
  - Basis: Authors note softpick doesn't outperform softmax on passkey retrieval and hypothesize underscoring weakens value signals
  - Unresolved: Hypothesis identified but not addressed; "Scalable-Softpick" variant failed to improve results
  - Resolution: Modified mechanism or tuning achieving higher accuracy than softmax on passkey retrieval

- **Does trillion-token training resolve dead heads in larger softpick models?**
  - Basis: 1.8B model shows higher dead head percentage; authors suggest undertraining
  - Unresolved: Training limited to 100B tokens; unclear if dead heads would continue decreasing
  - Resolution: Training curves showing dead head percentage dropping to 340M levels

- **Can 1.8B performance gap be closed with softpick-specific hyperparameters?**
  - Basis: 1.8B underperforms "when we reuse the same setup"; calls for scaling study with hyperparameter sweeps
  - Unresolved: Drop-in setup without tuning for larger model; impact of tuning unknown
  - Resolution: Scaling study with distinct hyperparameters demonstrating competitive performance

## Limitations
- Scaling reliability: Softpick underperforms at 1.8B scale (-5 to -6 points on benchmarks) due to underscoring and dead heads
- Quantization dependency: Improvements rely on specific premise about outlier elimination; alternative mechanisms untested
- Gradient flow validation: Dead head recovery mechanism lacks isolated empirical validation

## Confidence
- **High confidence**: Softpick eliminates attention sinks (0% sink rate), reduces massive activations (hundred-fold kurtosis reduction), achieves 95%+ sparsity
- **Medium confidence**: Softpick improves low-bit quantization accuracy; mechanism plausible but dependency on specific quantization methods unclear
- **Low confidence**: Softpick's scaling behavior and underscoring hypothesis; 1.8B underperformance significant but explanation speculative

## Next Checks
1. **Isolation experiment for dead head recovery**: Train 340M softpick with abs vs. rectified-only variants. Track dead head percentage throughout training to validate gradient flow hypothesis

2. **Quantization mechanism ablation**: Train softmax baseline with gradient clipping at different thresholds. Quantize all models to 2-4 bits to test whether outlier reduction alone explains quantization improvements

3. **Scaling diagnostic with extended training**: Train both 340M and 1.8B softpick models for 400k steps. Monitor dead head recovery, attention sparsity stability, and benchmark performance to determine if undertraining vs. fundamental scaling issues