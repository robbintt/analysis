---
ver: rpa2
title: 'Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to
  Cherry-Picking'
arxiv_id: '2504.15414'
source_url: https://arxiv.org/abs/2504.15414
tags:
- performance
- reward
- policy
- real-world
- sim-to-real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting policies for real-world
  deployment after reinforcement learning training has converged. The core problem
  is that training rewards are noisy and unstable near convergence, leading to unreliable
  heuristic selection ("cherry-picking") of policies for real-world testing.
---

# Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking

## Quick Facts
- arXiv ID: 2504.15414
- Source URL: https://arxiv.org/abs/2504.15414
- Authors: Dylan Khor; Bowen Weng
- Reference count: 40
- One-line result: Worst-case performance estimation via KL-constrained distribution reweighting provides principled policy selection with significantly improved Spearman's rank correlation (SCC) between predicted and real-world performance rankings.

## Executive Summary
This paper addresses the critical challenge of selecting optimal policies for real-world deployment after reinforcement learning training has converged. Near convergence, training rewards become noisy and unstable, making traditional heuristic selection unreliable. The authors propose a worst-case performance estimation method that reweights the simulation distribution within a bounded Kullback-Leibler divergence constraint, creating a convex optimization problem that provides more reliable policy rankings than direct simulation estimates.

Extensive experiments on Unitree G1 humanoid robot locomotion tasks demonstrate the approach's effectiveness. The worst-case estimates significantly improve Spearman's rank correlation coefficient with real-world performance compared to standard simulation estimates. For example, in disturbed locomotion testing, the proposed method increased SCC from -0.3 to 0.9 compared to direct simulation estimates. The method consistently performs at least as well as existing approaches while often substantially outperforming them, providing a principled alternative to heuristic policy selection in sim-to-real transfer scenarios.

## Method Summary
The proposed method trains policies using standard RL approaches, then collects simulation samples to estimate the stationary distribution q over reward states. A convex quadratic-constrained linear programming (QCLP) problem is solved to find the worst-case performance estimate by reweighting the simulation distribution within a bounded KL divergence constraint. The optimization problem maximizes expected performance under the worst-case distribution ρ subject to normalization and divergence constraints. Policies are ranked by their worst-case estimates E_ρ[ψ], which are shown to correlate significantly better with real-world performance than direct simulation-based estimates. The approach is evaluated on locomotion tasks with a Unitree G1 humanoid robot, comparing against standard simulation estimates and adversarial testing methods.

## Key Results
- Worst-case estimation method achieved Spearman's rank correlation coefficients of 0.9 in disturbed locomotion testing, compared to -0.3 for direct simulation estimates
- The approach consistently outperformed or matched existing policy selection methods across all tested scenarios
- The method is robust to choice of KL divergence bound k, with moderate values (2.0-6.0) working well across different tasks
- Computational tractability is maintained through discretization of the state space, with the QCLP solvable in reasonable time

## Why This Works (Mechanism)
The method works by explicitly accounting for the uncertainty and noise in simulation-based performance estimates near convergence. By constraining the distribution reweighting within a KL divergence bound, the approach creates a worst-case scenario that is guaranteed to contain the true distribution. This worst-case estimation provides more conservative and reliable rankings than direct simulation estimates, which can be optimistic due to noise in the reward signal. The convex optimization formulation ensures computational tractability while the theoretical guarantees provide confidence in the ranking quality.

## Foundational Learning
- **KL Divergence**: Measures the difference between probability distributions; needed for bounding the uncertainty in simulation estimates; quick check: verify KL divergence values are finite and well-behaved
- **Quadratic Constrained Linear Programming (QCLP)**: Optimization framework with quadratic constraints; needed for solving the worst-case estimation problem; quick check: confirm convexity of the problem formulation
- **Spearman's Rank Correlation Coefficient**: Non-parametric measure of rank correlation; needed for evaluating policy ranking quality; quick check: verify SCC values are within expected ranges
- **Stationary Distribution Estimation**: Monte Carlo sampling to estimate state visitation frequencies; needed for building the simulation distribution q; quick check: ensure sufficient samples for convergence
- **Worst-Case Analysis**: Optimization over worst-case scenarios; needed for robust policy selection; quick check: verify constraint satisfaction in the solution
- **Discretization of Continuous Spaces**: Approximating continuous distributions with discrete samples; needed for computational tractability; quick check: test sensitivity to discretization granularity

## Architecture Onboarding

**Component Map**: Training code -> Policy checkpoints -> Simulation sampling -> Distribution estimation q -> QCLP solver -> Worst-case estimates -> Policy ranking

**Critical Path**: The critical path flows from policy training through Monte Carlo sampling to worst-case estimation. The QCLP solver represents the computational bottleneck, requiring careful consideration of state space discretization to maintain tractability.

**Design Tradeoffs**: The primary tradeoff involves the granularity of state space discretization versus computational tractability. Finer discretization provides more accurate distribution estimates but increases the QCLP problem size exponentially. The choice of KL divergence bound k involves a tradeoff between estimation reliability and conservativeness.

**Failure Signatures**: Low Spearman's rank correlation coefficients indicate the worst-case estimates are not capturing the true performance differences. QCLP solver failures suggest the state space discretization is too fine or the problem formulation needs adjustment. Poor real-world performance despite good simulation estimates indicates model mismatch or unmodeled dynamics.

**First Experiments**: 
1. Test the method with varying discretization granularities to identify the minimum resolution required for maintaining high SCC
2. Apply the approach to a simplified locomotion task with known ground truth to validate the ranking quality
3. Systematically vary the KL divergence bound k across multiple orders of magnitude to characterize the tradeoff between reliability and conservativeness

## Open Questions the Paper Calls Out
None

## Limitations
- The discretization approach for continuous state spaces is not fully specified, potentially affecting estimation quality and computational tractability
- The assumption of Lipschitz continuity for reward functions is reasonable but not empirically verified across all possible policy behaviors
- The generalizability of the approach to non-locomotion tasks or systems with significantly different dynamics is not established
- Computational complexity may become prohibitive for high-dimensional state spaces

## Confidence

**High confidence**: The core mathematical formulation and convex QCLP solution are sound. The worst-case estimation framework is theoretically rigorous and the improvement in Spearman's rank correlation coefficient is empirically demonstrated.

**Medium confidence**: The practical implementation details around state space discretization and Monte Carlo sampling are reasonable but not fully specified. The assumption that policy performance can be accurately captured through reward function ψ is standard but may not hold for all tasks.

**Low confidence**: The generalizability of the approach to non-locomotion tasks or systems with significantly different dynamics is not established. The computational complexity for high-dimensional state spaces may be prohibitive.

## Next Checks

1. **Robustness to discretization**: Test the method with varying levels of state space discretization granularity to quantify sensitivity and identify minimum resolution requirements for maintaining high SCC.

2. **Generalization to new tasks**: Apply the approach to a different robot morphology or task type (e.g., manipulation) to evaluate whether the worst-case estimation framework generalizes beyond the locomotion domain.

3. **KL divergence bound sensitivity**: Systematically vary the KL divergence constraint k across multiple orders of magnitude to better characterize the trade-off between estimation reliability and conservativeness.