---
ver: rpa2
title: 'Take Out Your Calculators: Estimating the Real Difficulty of Question Items
  with LLM Student Simulations'
arxiv_id: '2601.09953'
source_url: https://arxiv.org/abs/2601.09953
tags:
- student
- students
- difficulty
- grade
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) to simulate
  student responses to math problems, providing a cost-effective alternative to traditional
  pilot studies for estimating item difficulty. The method involves prompting open-source
  LLMs to role-play students at different proficiency levels, collecting their responses,
  and fitting an Item Response Theory (IRT) model to estimate difficulty parameters.
---

# Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations

## Quick Facts
- arXiv ID: 2601.09953
- Source URL: https://arxiv.org/abs/2601.09953
- Reference count: 24
- Primary result: LLM student simulations achieve up to 0.82 correlation with real student performance for estimating math item difficulty

## Executive Summary
This paper demonstrates that large language models can simulate student responses to math problems, providing a cost-effective alternative to pilot studies for estimating item difficulty. By prompting LLMs to role-play students at different proficiency levels and fitting Item Response Theory models to the simulated responses, the approach achieves strong correlations (0.75-0.82) with actual student performance data from the National Assessment of Educational Progress. Notably, models with weaker mathematical abilities outperform stronger ones, and the inclusion of diverse demographic identifiers further improves prediction accuracy.

## Method Summary
The pipeline ingests NAEP math MCQs and generates simulated student responses by prompting open-source LLMs to role-play students at specified proficiency levels (Below Basic, Basic, Proficient, Advanced) with demographic identifiers. For each question, N simulated students provide binary responses (correct/incorrect) based on role-play prompts. These responses are scored and used to fit a Rasch IRT model, which jointly estimates student abilities and item difficulties. The resulting difficulty parameters are correlated with real-world student success rates from NAEP to evaluate prediction accuracy.

## Key Results
- LLM simulations achieve Pearson correlations of 0.75, 0.76, and 0.82 for grades 4, 8, and 12 respectively
- Models with weaker mathematical abilities (Gemma-2-9B at 72% accuracy) outperform stronger models for difficulty prediction
- Diverse demographic identifiers improve correlation by ~0.10 over no-identifier baselines
- AUC scores for difficulty classification range from 0.78-0.90 across grades

## Why This Works (Mechanism)

### Mechanism 1
Simulation-based approaches outperform direct difficulty estimation because item difficulty depends on respondent characteristics, not just item features. By prompting LLMs to role-play students at varying proficiency levels, the pipeline generates individual binary responses that preserve variance patterns across skill levels. These responses feed into IRT models, which jointly estimate student abilities (β) and item difficulties (δ), capturing the interaction between who answers and what is asked.

### Mechanism 2
Models with weaker mathematical ability better simulate struggling students because they naturally experience difficulty with problems. High-accuracy models rarely produce incorrect responses, making them unable to reproduce realistic error distributions. Models like Gemma-2-9B naturally fail on harder problems in ways that correlate with real student difficulties.

### Mechanism 3
Demographic identifiers (diverse names across gender and race) improve simulation fidelity by grounding personas. First names carry demographic associations that may trigger more specific persona instantiations. Diverse names outperform single homogeneous names and no identifiers, suggesting cross-demographic variance stabilizes aggregate predictions.

## Foundational Learning

- **Item Response Theory (IRT) basics**: The entire pipeline relies on fitting Rasch models to simulated binary responses. Without understanding that IRT jointly estimates person ability (β) and item difficulty (δ) through maximum likelihood, the rationale for simulation over direct prediction is unclear.
  - Quick check: Given 5 simulated students with abilities β = {-2, -1, 0, 1, 2} answering an item with difficulty δ = 0.5, which students have >50% probability of correct response?

- **NAEP proficiency levels and distributions**: Prompts reference NAEP skill descriptors (Below Basic through Advanced) and the simulation uses a specific skill distribution (25% Below Basic, 35% Basic, 25% Proficient, 15% Advanced). Misunderstanding these labels breaks prompt engineering.
  - Quick check: Why does the paper use 35% Basic rather than a uniform 25% per level?

- **Role-play prompting with skill constraints**: The simulation quality depends on LLMs maintaining character as struggling students without "breaking character" to solve correctly. The paper notes hedging language ("I think/I guess") in Below Basic responses as a quality signal.
  - Quick check: What linguistic markers distinguish an authentic Below Basic simulation from a high-ability model pretending to be wrong?

## Architecture Onboarding

- Component map: Question ingestion -> Persona generator -> Simulation engine -> Response scorer -> IRT fitter -> Evaluator
- Critical path: Prompt template design -> Skill distribution specification -> Response collection at scale -> IRT fitting -> Correlation with NAEP ground truth
- Design tradeoffs:
  - Class size vs. cost: N=300 improves correlations but requires 4-5 GPU-hours per model
  - Model selection: Weaker models better simulate struggling students; stronger models produce poor correlations
  - Identifier strategy: Diverse names improve correlations by ~0.10 over no-identifier baselines
  - Ensemble vs. single model: Weighted ensemble achieves ρ=0.82 vs. single-model ρ=0.75-0.78
- Failure signatures:
  - Direct prompting yields near-zero correlations (Table 12: -0.14 to +0.14)
  - IRT ability estimates not monotonic across skill levels
  - Distractor mismatch (Figure 7: 20-50% match rates)
  - Grade 12 correlations drop for high-accuracy models
- First 3 experiments:
  1. Baseline replication: Run Gemma-2-9B with N=100, no identifiers, NAEP skill distribution on 50 randomly sampled Grade 8 items. Target: ρ > 0.55
  2. Identifier ablation: Same setup with (a) no identifier, (b) student IDs, (c) single name, (d) diverse names. Expected pattern: diverse > IDs > none ≈ single
  3. Model comparison: Run Gemma-2-9B (72% accuracy) vs. Llama-3-8B (44% correlation) vs. Qwen2.5-32B (62% accuracy) on Grade 4 items with N=100. Verify model accuracy inversely correlates with simulation fidelity

## Open Questions the Paper Calls Out

1. How can LLM simulations be improved to align with the specific incorrect answer choices (distractors) selected by real students, rather than just binary correctness? The current approach optimizes for aggregate performance correlations but fails to model specific misconceptions leading to wrong answers.

2. Does the inclusion of diverse demographic names improve simulation accuracy due to stereotype activation or simply by increasing prompt specificity? The paper acknowledges uncertainty about whether names improve grounding through stereotype associations, prompt specificity, or other mechanisms.

3. Can simulation fidelity be improved for content areas like Algebra, where the study observed the weakest performance? The results show simulations excel at Measurement but struggle with Algebra (correlations as low as 0.45), with no solution offered.

## Limitations
- Quality depends on prompt engineering and LLM ability to maintain character; no systematic validation of authentic error patterns
- Only mathematics items tested; applicability to other subjects unknown
- Optimal class size (N=300) creates substantial computational costs (4-5 GPU-hours per model)

## Confidence

**High Confidence**: Direct LLM difficulty prediction fails while simulation-based approaches succeed is well-supported by controlled comparisons.

**Medium Confidence**: Weaker mathematical models better simulate struggling students is supported but requires deeper investigation into why high-accuracy models fail to maintain character.

**Low Confidence**: Benefit of diverse demographic identifiers is demonstrated but lacks theoretical grounding; the underlying causal mechanism remains ambiguous.

## Next Checks
1. Analyze linguistic patterns (hedging frequency, confidence indicators) in Below Basic simulations versus actual struggling student responses to verify authentic character maintenance versus random error generation.

2. Apply the simulation pipeline to non-mathematical NAEP items (reading comprehension, science) to evaluate whether weaker-model advantage holds across domains or is specific to mathematical reasoning.

3. Compare simulation quality at reduced class sizes (N=50, N=100) against full N=300 runs to identify minimum viable student populations while maintaining correlations above 0.70.