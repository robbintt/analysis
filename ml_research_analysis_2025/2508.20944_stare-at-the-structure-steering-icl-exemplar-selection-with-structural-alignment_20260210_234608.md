---
ver: rpa2
title: 'STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment'
arxiv_id: '2508.20944'
source_url: https://arxiv.org/abs/2508.20944
tags:
- text
- semantic
- stare
- parsing
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STARE, a structure-aware exemplar selection
  framework for in-context learning in semantic parsing. The key innovation is Middle-Layer
  Injection (MLI), a plug-in module that enhances retriever representations by injecting
  linguistic directions from probing into intermediate layers.
---

# STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment

## Quick Facts
- arXiv ID: 2508.20944
- Source URL: https://arxiv.org/abs/2508.20944
- Reference count: 22
- Primary result: Up to 5.0% higher exact match on Spider semantic parsing benchmark

## Executive Summary
This paper introduces STARE, a structure-aware exemplar selection framework for in-context learning in semantic parsing. The key innovation is Middle-Layer Injection (MLI), a plug-in module that enhances retriever representations by injecting linguistic directions from probing into intermediate layers. The method combines semantic and structural similarity signals using contrastive learning, while MLI boosts syntactic awareness via targeted interventions in BERT's hidden states. Experiments on four semantic parsing benchmarks show consistent improvements over five strong baselines, with up to 5.0% higher exact match on Spider and 1.35% average gains across the first three tasks.

## Method Summary
STARE combines semantic and structural similarity signals using contrastive learning, while MLI boosts syntactic awareness via targeted interventions in BERT's hidden states. The framework employs Middle-Layer Injection to inject linguistic directions from probing into intermediate layers, enhancing retriever representations for semantic parsing tasks. This approach is designed to be modular, allowing seamless integration into existing pipelines and generalization across different models.

## Key Results
- Up to 5.0% higher exact match on Spider semantic parsing benchmark
- 1.35% average gains across MTOP, SMCalFlow, and TreeDST tasks
- MLI boosts non-STARE retrievers by up to 8.7% in exact match

## Why This Works (Mechanism)
The effectiveness of STARE relies on Middle-Layer Injection (MLI) which enhances retriever representations by injecting linguistic directions from probing into intermediate layers. This approach combines semantic and structural similarity signals using contrastive learning, creating a more comprehensive representation for exemplar selection. The MLI mechanism specifically targets syntactic awareness by making targeted interventions in BERT's hidden states, addressing the structural alignment needed for effective semantic parsing.

## Foundational Learning

**Semantic Parsing**: Converting natural language to structured meaning representations; needed to understand the target task and evaluate performance against baselines.

**In-Context Learning (ICL)**: Using exemplars within prompts for few-shot learning; needed to frame the exemplar selection problem and justify the retrieval-augmented approach.

**Contrastive Learning**: Learning representations by comparing similar and dissimilar pairs; needed to combine semantic and structural similarity signals effectively.

**Probing**: Analyzing hidden representations to identify linguistic properties; needed to derive linguistic directions for MLI injection.

**BERT Architecture**: Understanding transformer layers and hidden states; needed to implement MLI through targeted interventions in intermediate layers.

## Architecture Onboarding

**Component Map**: Input Query -> Retriever (BERT with MLI) -> Contrastive Learning Module -> Selected Exemplars -> Semantic Parser

**Critical Path**: The retriever with MLI injection forms the core bottleneck, as retrieval quality directly impacts exemplar selection and downstream parsing performance.

**Design Tradeoffs**: The modular design allows integration with existing pipelines but introduces dependency on external retrievers, potentially affecting robustness in real-world deployments.

**Failure Signatures**: Poor retrieval quality from external retrievers, inadequate linguistic direction injection from probing, or competing rather than complementary semantic and structural signals.

**First Experiments**: (1) Ablation study isolating MLI versus contrastive learning contributions; (2) Cross-linguistic evaluation on multilingual semantic parsing benchmarks; (3) End-to-end testing with varying retrieval qualities.

## Open Questions the Paper Calls Out
The paper highlights several uncertainties: the generalizability of probing-based MLI across languages and domains, potential nonlinear interactions among injected properties, the methodological constraint of focusing solely on exemplar selection rather than reasoning-centric or fine-tuning approaches, and the dependency on external retrievers which may introduce brittleness in real-world deployments.

## Limitations
- Effectiveness relies on probing-based linguistic directions that may not generalize across all languages or domains
- Methodological constraint of focusing solely on exemplar selection rather than reasoning-centric or fine-tuning approaches
- Dependency on external retrievers introduces potential brittleness in real-world deployments

## Confidence
High confidence in empirical improvements on benchmark tasks (MTOP, SMCalFlow, TreeDST, Spider) with consistent performance gains over five strong baselines. Medium confidence in generalizability claims across models, as modular design shows promise but lacks extensive cross-model validation. Low confidence in probing-based MLI mechanism, where relationship between linguistic direction injection and retrieval performance requires more rigorous theoretical grounding and ablation studies.

## Next Checks
(1) Conduct ablation studies specifically isolating the contribution of MLI versus contrastive learning components to determine which aspect drives improvements.
(2) Evaluate the framework on multilingual semantic parsing benchmarks to test cross-linguistic generalizability of the probing-based approach.
(3) Implement end-to-end testing with varying retrieval qualities to assess pipeline robustness under realistic conditions where retriever performance may degrade.