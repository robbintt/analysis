---
ver: rpa2
title: Solving Inverse Problems with Flow-based Models via Model Predictive Control
arxiv_id: '2601.23231'
source_url: https://arxiv.org/abs/2601.23231
tags:
- control
- flow
- image
- arxiv
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MPC-Flow, a model predictive control framework
  that enables training-free guidance of flow-based generative models for inverse
  problems. The method addresses the computational and memory challenges of optimal
  control-based guidance by decomposing trajectory optimization into a sequence of
  short-horizon subproblems, allowing practical conditional generation at inference
  time.
---

# Solving Inverse Problems with Flow-based Models via Model Predictive Control

## Quick Facts
- arXiv ID: 2601.23231
- Source URL: https://arxiv.org/abs/2601.23231
- Reference count: 40
- Primary result: MPC-Flow enables training-free guidance of flow-based generative models for inverse problems with up to 2-3 dB PSNR improvements on image restoration tasks

## Executive Summary
This paper introduces MPC-Flow, a model predictive control framework that enables training-free guidance of flow-based generative models for inverse problems. The method addresses computational and memory challenges of optimal control-based guidance by decomposing trajectory optimization into a sequence of short-horizon subproblems. This allows practical conditional generation at inference time while maintaining theoretical guarantees linking the approach to the underlying optimal control objective. The framework provides a spectrum of guidance algorithms, including variants that avoid backpropagation through the generative model.

## Method Summary
MPC-Flow solves the trajectory optimization problem for conditional generation by decomposing it into a sequence of short-horizon subproblems. At each step, it optimizes the next short segment of the trajectory using a cost function that balances reconstruction fidelity with adherence to the generative model's prior. The method employs gradient-based optimization for the initial states and includes a warmup phase to improve convergence. By avoiding full backpropagation through the entire trajectory, MPC-Flow significantly reduces memory requirements while maintaining theoretical connections to the original optimal control problem. The approach enables guidance of massive architectures like FLUX.2 (32B) on consumer hardware.

## Key Results
- MPC-Flow consistently outperforms existing scalable approaches on image restoration benchmarks
- Achieves PSNR improvements of 2-3 dB over baseline methods in denoising, deblurring, super-resolution, and inpainting tasks
- Successfully scales to 32B parameter models like FLUX.2 on consumer hardware
- Enables training-free guidance in quantized settings while maintaining high quality reconstructions

## Why This Works (Mechanism)
The method works by reframing conditional generation as a trajectory optimization problem where the goal is to find a path through latent space that both satisfies the observed measurements and remains close to the generative model's prior distribution. The key insight is that instead of optimizing the entire trajectory at once (which is computationally prohibitive), MPC-Flow breaks this into a sequence of shorter optimization problems. Each subproblem optimizes a small segment of the trajectory while using the solution from the previous segment as initialization. This allows the method to leverage local optimization techniques while maintaining global coherence through the sequential structure.

## Foundational Learning

**Flow-based generative models**: Reversible neural networks that learn to transform between simple base distributions and complex data distributions. Why needed: Provide the prior distribution that constrains the solution space. Quick check: Verify the model can both encode and decode data with low reconstruction error.

**Model Predictive Control**: Control strategy that optimizes over a finite horizon and executes only the first action, then repeats. Why needed: Enables tractable optimization of long trajectories by breaking them into manageable segments. Quick check: Confirm the receding horizon approach maintains stability and convergence.

**Optimal control theory**: Framework for finding control policies that minimize a cost function over time. Why needed: Provides the theoretical foundation for trajectory optimization in latent space. Quick check: Ensure the cost function properly balances measurement fidelity and prior adherence.

## Architecture Onboarding

**Component map**: Measurements -> Cost function -> Short-horizon optimizer -> Initial state update -> Warmup phase -> Next iteration -> Generated sample

**Critical path**: The optimization loop that iteratively updates the initial state based on the gradient of the cost function with respect to the reconstruction error and prior adherence terms.

**Design tradeoffs**: Memory vs. accuracy (shorter horizons use less memory but may converge slower), optimization stability vs. computational cost (warmup phase improves stability but adds overhead), and theoretical guarantees vs. practical performance (asymptotic guarantees may not hold perfectly in finite iterations).

**Failure signatures**: Poor reconstruction quality indicates incorrect cost function weighting or insufficient optimization iterations; mode collapse suggests the prior term dominates too strongly; memory errors occur when horizons are too long for available GPU memory.

**First experiments**: 1) Verify basic reconstruction on clean data without measurements, 2) Test with synthetic noise to validate measurement incorporation, 3) Compare different horizon lengths to find optimal tradeoff between quality and efficiency.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit areas for future work include extending the approach to non-image inverse problems, investigating robustness to diverse noise distributions, and exploring adaptive horizon selection strategies.

## Limitations
- Empirical validation is limited to image restoration tasks without evaluation on scientific imaging or medical reconstruction domains
- Performance evaluation lacks extensive cross-dataset validation and testing on out-of-distribution data
- Theoretical guarantees are asymptotic and may not fully capture practical performance degradation over many iterations

## Confidence

High confidence in computational and memory efficiency improvements, supported by direct comparisons with baseline methods.

Medium confidence in reconstruction quality, as results are benchmarked against established approaches but lack extensive cross-dataset validation.

Medium confidence in the theoretical framework, given that the guarantees are asymptotic and not fully validated empirically across all conditions.

## Next Checks

1. Evaluate MPC-Flow on non-image inverse problems such as compressed sensing or tomography to test generalizability beyond image restoration.

2. Perform ablation studies to isolate the impact of short-horizon decomposition and the choice of cost function on reconstruction quality and efficiency.

3. Test robustness to diverse noise models and out-of-distribution inputs to assess the method's reliability in practical applications.