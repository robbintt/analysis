---
ver: rpa2
title: 'QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by
  Exploiting Common Patterns in Nonlinear Operations'
arxiv_id: '2511.06767'
source_url: https://arxiv.org/abs/2511.06767
tags:
- quantization
- softmax
- nonlinear
- quark
- hardware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUARK introduces a quantization-enabled FPGA acceleration framework
  for Transformer models by exploiting common patterns in nonlinear operations. The
  core innovation lies in a sub-operator-sharing approximation methodology that reformulates
  exponential, logarithmic, and division operations into low-cost shift-and-add arithmetic,
  eliminating floating-point dependencies.
---

# QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations

## Quick Facts
- **arXiv ID**: 2511.06767
- **Source URL**: https://arxiv.org/abs/2511.06767
- **Reference count**: 40
- **Primary result**: 1.96× end-to-end speedup over GPU with <1% accuracy loss under 8-bit quantization on FPGA

## Executive Summary
QUARK introduces a quantization-enabled FPGA acceleration framework for Transformer models by exploiting common patterns in nonlinear operations. The core innovation lies in a sub-operator-sharing approximation methodology that reformulates exponential, logarithmic, and division operations into low-cost shift-and-add arithmetic, eliminating floating-point dependencies. The approach unifies Softmax, GELU, and LayerNorm operations through shared computational modules using time-division multiplexing, achieving over 50% reduction in hardware overhead.

## Method Summary
QUARK implements integer-only Transformer acceleration by reformulating nonlinear operations (Softmax, GELU, LayerNorm) using base-2 logarithmic and exponential approximations with polynomial fitting. The method employs sub-operator sharing where exponential, logarithmic, and division operations are approximated using only shift-and-add arithmetic. Time-division multiplexing schedules these operations sequentially since they occur at different pipeline stages. Additionally, reorder-based group quantization handles heterogeneous activation distributions across channels through offline permutation matrix fusion.

## Key Results
- 1.96× end-to-end speedup over GPU implementations
- 6.08% accuracy gains on ViT-B under W4A4 quantization
- 20.41% improvement in GLUE benchmarks for BERT models
- 46.9% LUT utilization and 47.4% FF utilization at 300MHz on ZCU102 FPGA
- 787.5 GOP/s throughput while maintaining <1% accuracy loss under 8-bit quantization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Nonlinear operations (exp, log, division) can be approximated using only integer shift-and-add arithmetic, eliminating floating-point units.
- **Mechanism:** Base-e to base-2 conversion using binary approximation (log₂e ≈ 1.0111₂) enables shift-based exponentials. Polynomials approximate fractional components (2^qF ≈ 0.1713qF² + 0.6674qF + 0.998). Logarithmic division replaces explicit division: a/b = exp(ln(a) - ln(b)).
- **Core assumption:** The second-order polynomial approximation error remains within quantization noise tolerance for INT8 inputs.
- **Evidence anchors:**
  - [abstract] "reformulates exponential, logarithmic, and division operations into low-cost shift-and-add arithmetic, eliminating floating-point dependencies"
  - [Section III.A, Eq. 3-9] Shows explicit derivation of Appro-Exp, Appro-Ln using shifts and polynomial terms
  - [corpus] Limited direct evidence; neighbor papers focus on BFP/quantization broadly, not this specific approximation method
- **Break condition:** If input dynamic range exceeds INT8 bounds (>±127), the qI exponent clipping introduces systematic error that polynomial compensation cannot recover.

### Mechanism 2
- **Claim:** Softmax, GELU, and LayerNorm share sub-operators (exp, log) that can be unified via time-division multiplexing (TDM).
- **Mechanism:** GELU is reformulated as binary Softmax: σ(1.702x) = Softmax([0, -1.702x])₁, enabling shared exponential units. LayerNorm uses the same log-based division as Softmax. TDM sequences operations since they occur at different pipeline stages (attention → activation → normalization).
- **Core assumption:** Nonlinear operators are temporally separated in the Transformer pipeline, allowing sequential hardware reuse without throughput loss.
- **Evidence anchors:**
  - [abstract] "unifies Softmax, GELU, and LayerNorm operations through shared computational modules using time-division multiplexing"
  - [Section III.A, Eq. 12-14] Shows GELU-to-Softmax reformulation; [Section IV.A] describes TDM scheduling
  - [corpus] Weak evidence; neighbor papers don't address operator unification
- **Break condition:** If batch size increases such that multiple nonlinear operations require concurrent execution, TDM serialization creates throughput bottleneck.

### Mechanism 3
- **Claim:** Offline channel reordering based on distribution similarity enables effective group quantization without runtime overhead.
- **Mechanism:** Permutation matrix R clusters channels by statistical similarity (KL divergence) during calibration. R is fused into adjacent layer weights: Y = (X·R)·(R⁻¹·W) = X·W. Group scales are power-of-2 multiples (ΔRi = 2^ki · ΔR₁), enabling shift-based cross-group alignment.
- **Core assumption:** Activation distributions remain stable between calibration and inference; permutation learned offline generalizes.
- **Evidence anchors:**
  - [abstract] "reorder-based group quantization to handle heterogeneous activation distributions across channels"
  - [Section III.C, Eq. 19-27] ILP formulation for group allocation; Eq. 20 shows weight fusion
  - [Table I] W4A4 accuracy gains (e.g., +6.08% on ViT-B) suggest distribution-aware grouping mitigates ultra-low-bit degradation
  - [corpus] No direct evidence for this specific reordering approach
- **Break condition:** If activation distribution shifts significantly post-calibration (e.g., out-of-distribution inputs), precomputed permutation may misalign channels, degrading quantization accuracy.

## Foundational Learning

- **Concept:** Fixed-point arithmetic and shift-based multiplication
  - **Why needed here:** All nonlinear approximations rely on >> (right shift) for division by powers of 2 and polynomial coefficient multiplication.
  - **Quick check question:** Given INT8 value 100, what is (100 + 100≫1 - 100≫4) in fixed-point?

- **Concept:** Transformer forward pass dataflow
  - **Why needed here:** TDM scheduling requires understanding when Softmax (attention), GELU (FFN), and LayerNorm (residual) execute relative to each other.
  - **Quick check question:** In a standard Transformer block, which operation executes last: Softmax, GELU, or LayerNorm?

- **Concept:** Post-training quantization (PTQ) calibration
  - **Why needed here:** Group quantization and reordering happen during offline calibration; understanding the distinction from QAT is essential.
  - **Quick check question:** Why does PTQ require calibration data but not gradient computation?

## Architecture Onboarding

- **Component map:**
  - **Sub-Operator Sharing Unit:** Shared Appro-Exp (shifter + polynomial unit) and Appro-Ln (LOD + polynomial) modules
  - **Mode Controller:** N/2 MUX selectors (0:Softmax, 1:GELU/LN) to configure datapath per pipeline stage
  - **Group Quantization Unit:** Three-stage pipeline (Scale Allocation → Intra-group Quant → Cross-group Alignment)
  - **Shared Buffer:** Interface to external PE array for linear layer handoff

- **Critical path:**
  Softmax mode: Max-Comparator → Parallel-Subtractor → Appro-Exp → Adder-Tree → Appro-Ln → Appro-Exp (final output)
  Latency dominated by two sequential Appro-Exp calls (polynomial evaluation).

- **Design tradeoffs:**
  - **Accuracy vs. Hardware:** Lower polynomial order reduces LUT count but increases approximation error. Paper uses second-order; first-order may break W4A4.
  - **Group count vs. Alignment overhead:** More groups improve quantization fidelity but require more shifters for cross-group alignment.
  - **Assumption:** TDM serialization is acceptable for batch size ≤8 (validated in Figure 8a).

- **Failure signatures:**
  - **Accuracy collapse at W4A4 without group quantization:** Expected >30% drop (Table I, FQ-ViT baseline).
  - **Throughput degradation at large batch:** If batch >8, TDM may not hide latency; monitor for nonlinear latency scaling.
  - **MSB detection failure in Appro-Ln:** If input is zero, LOD (leading-one detection) returns undefined; requires input clamping.

- **First 3 experiments:**
  1. **Unit test Appro-Exp accuracy:** Feed INT8 values [-127, 0], compare against FP32 exp(x), verify max error <1% of full-scale.
  2. **End-to-end latency sweep:** Run DeiT-Tiny on ZCU102 with batch sizes 1-8, confirm 1.48×-1.96× speedup over GPU baseline (Figure 8b).
  3. **Ablation: Disable group quantization:** Run W4A4 inference without reordering, verify accuracy drop matches Figure 7 (~40% degradation on DeiT-S).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Integer Linear Programming (ILP) optimization used for group allocation and the offline reordering process scale efficiently to Large Language Models (LLMs) with billions of parameters?
- Basis in paper: [inferred] The paper validates the framework on BERT and ViT (Section V), but the group allocation is formulated as an ILP problem (Eq. 19) and requires offline reordering using a trainable permutation matrix.
- Why unresolved: Solving ILP problems and training permutation matrices for layers with massive channel counts or sequence lengths could incur prohibitive computational costs during the calibration phase.
- What evidence would resolve it: Benchmark results showing calibration time and memory usage for the reorder-based group quantization on models exceeding 1B parameters.

### Open Question 2
- Question: Is the static boundary of 2.4 for the ReLU-based GELU approximation universally optimal across different Transformer architectures and quantization bit-widths?
- Basis in paper: [inferred] The paper explicitly defines the ReLU approximation interval as \((- \infty, -2.4] \cup [2.4, \infty)\) based on absolute error equality, but does not test sensitivity to this parameter.
- Why unresolved: Activation distributions vary significantly between vision (ViT) and language (BERT) models; a fixed boundary may introduce approximation errors that degrade accuracy in untested model variants.
- What evidence would resolve it: A sensitivity analysis evaluating model accuracy when the GELU approximation boundary is swept across different values (e.g., 2.0 to 3.0) for various tasks.

### Open Question 3
- Question: Does the time-division multiplexing (TDM) of the shared sub-operator unit cause pipeline stalls or bubbles when upstream linear operations have variable latency?
- Basis in paper: [inferred] The hardware design relies on TDM to reuse the arithmetic backend for Softmax, GELU, and LayerNorm (Section IV.A).
- Why unresolved: The paper reports average throughput and speedup but does not detail the scheduling efficiency or stall cycles when the TDM unit waits for inputs from linear layers with variable batch sizes or sequence lengths.
- What evidence would resolve it: Cycle-accurate analysis of the TDM scheduler under stochastic workloads or varying batch sizes to quantify idle time.

## Limitations

- Missing specification of outlier threshold value for group quantization clamping (Eq. 24)
- No details on permutation matrix R training method for offline reordering
- Intermediate bit-widths for exp/log calculations remain unspecified, creating potential overflow risks

## Confidence

- **High confidence**: Integer-only Softmax approximation with Algorithm 1 and polynomial coefficients - fully specified with explicit formulas
- **Medium confidence**: GELU and LayerNorm implementations - boundary conditions and iteration counts provided, but polynomial fitting method unknown  
- **Low confidence**: Group quantization permutation matrix training and outlier handling - key parameters completely unspecified
- **Medium confidence**: Hardware synthesis metrics - target platform and frequency specified, but DSP allocation strategy and intermediate bit-widths missing

## Next Checks

1. Implement Algorithm 1 Softmax with specified polynomials and verify accuracy against FP32 for INT8 inputs across [-127, 127] range, checking maximum error remains below 1% of full-scale
2. Test GELU boundary conditions at |x| = 2.4 with random inputs, plotting approximation error to verify smooth transition between Softmax approximation and ReLU branches
3. Run calibration with 32 ImageNet samples on DeiT-S, verifying Softmax-only W8A8 accuracy matches reported baseline before attempting group quantization implementation