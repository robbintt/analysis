---
ver: rpa2
title: Vision Language Models for Optimization-Driven Intent Processing in Autonomous
  Networks
arxiv_id: '2601.12744'
source_url: https://arxiv.org/abs/2601.12744
tags:
- code
- network
- optimization
- flow
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IntentOpt introduces a benchmark of 85 network optimization problems
  spanning 17 categories to evaluate VLMs for multimodal optimization code generation
  in IBN systems. The benchmark pairs annotated network diagrams with natural language
  intents and ground truth Gurobi solver code.
---

# Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks

## Quick Facts
- arXiv ID: 2601.12744
- Source URL: https://arxiv.org/abs/2601.12744
- Authors: Tasnim Ahmed; Yifan Zhu; Salimur Choudhury
- Reference count: 11
- Key outcome: VLMs achieve 75% execution success on optimization code generation, but visual parameter extraction reduces performance by 12–21 percentage points compared to text-only inputs.

## Executive Summary
This paper introduces IntentOpt, a benchmark for evaluating Vision-Language Models (VLMs) on multimodal optimization code generation for Intent-Based Networking (IBN) systems. The benchmark comprises 85 network optimization problems across 17 categories, pairing annotated network diagrams with natural language intents and ground truth Gurobi solver code. Four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) are evaluated under three prompting strategies, revealing that visual parameter extraction significantly degrades performance compared to text-only inputs. Closed-source models substantially outperform open-source alternatives, and program-of-thought prompting further reduces effectiveness for code generation tasks.

## Method Summary
The study introduces IntentOpt, a benchmark dataset containing 85 network optimization problems across 17 categories, each consisting of an annotated network diagram, natural language intent, and ground truth Gurobi solver code. Four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) are evaluated under three prompting strategies: direct, role, and program-of-thought. Performance is measured by execution success rate (ESR) and optimality gap, with text-only variants used to isolate visual reasoning capabilities. A case study deploys VLM-generated code on a network testbed using Model Context Protocol to translate solutions into OpenFlow rules.

## Key Results
- VLMs achieve 75% execution success on multimodal optimization code generation, with GPT-5-Mini leading at 72% on visual inputs.
- Visual parameter extraction reduces execution success by 12–21 percentage points compared to text-only inputs, with GPT-5-Mini dropping from 93% to 72%.
- Program-of-thought prompting decreases performance by up to 13 percentage points, while open-source models significantly underperform closed-source ones (18% vs 75% for Llama-3.2-11B-Vision vs GPT-5-Mini).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual parameter extraction from network diagrams introduces a 12–21 percentage point execution success penalty compared to text-only inputs.
- Mechanism: VLMs must jointly perform OCR-style numeric extraction, topology parsing (nodes, edges, directions), and semantic mapping to optimization parameters. Errors in any stage propagate to code generation.
- Core assumption: Text-only variants provide equivalent problem difficulty, isolating visual reasoning as the sole variable.
- Evidence anchors:
  - [abstract] "visual parameter extraction reduces execution success by 12–21 percentage points, with GPT-5-Mini dropping from 93% to 72%"
  - [Section IV, Error Analysis] "Visual parameter extraction errors (42%) include misread capacities (18%), wrong node labels (12%), and missed edges (12%)"
  - [corpus] No direct corpus evidence on network diagram extraction; related VLM grounding work (Italian Clinical VQA) suggests visual grounding remains unreliable across domains.
- Break condition: If text-only and multimodal variants differ in problem specification clarity rather than input modality, the gap may confound visual reasoning with prompt quality.

### Mechanism 2
- Claim: Program-of-Thought (PoT) prompting degrades code generation performance by up to 13 percentage points for closed-source models.
- Mechanism: PoT requires models to emit verbose inline comments before/during code synthesis, increasing token budget for non-executable content and potentially disrupting the learned code-generation distribution.
- Core assumption: Performance degradation stems from reasoning format rather than prompt length or complexity confounds.
- Evidence anchors:
  - [abstract] "Program-of-thought prompting decreases performance by up to 13 percentage points"
  - [Section IV.A] "For GPT-5-Mini... PoT degrades to 62.35% (p = 0.0266 versus role)"
  - [corpus] No corpus papers evaluate PoT specifically for optimization code generation; DRAMA-X benchmarks VLM intent prediction but uses different prompting approaches.
- Break condition: If PoT degradation disappears with different comment density or when applied to natural-language-only reasoning (not inline code comments), the mechanism may involve code-syntax interference rather than reasoning verbosity.

### Mechanism 3
- Claim: Closed-source VLMs substantially outperform open-source alternatives on optimization code generation, independent of visual reasoning capability.
- Mechanism: Larger closed-source models likely have greater exposure to mathematical programming tutorials, solver APIs, and optimization formulations during pre-training, enabling better formulation-to-code mapping.
- Core assumption: Performance gap reflects training data and scale rather than architectural differences in vision encoders.
- Evidence anchors:
  - [abstract] "open-source models significantly underperform closed-source ones, with Llama-3.2-11B-Vision achieving only 18% versus 75% for GPT-5-Mini"
  - [Section IV.E] "On text-only inputs (92.94% versus 16.47%), indicating disparity beyond vision capabilities"
  - [corpus] Vision-Language Models for Edge Networks survey notes deployment constraints but does not compare closed vs. open optimization performance.
- Break condition: If fine-tuning open-source models on optimization-specific corpora closes the gap, the mechanism is training-data coverage rather than fundamental architectural limitations.

## Foundational Learning

- Concept: Intent-Based Networking (IBN) translation pipeline
  - Why needed here: IntentOpt targets "optimization intents"—a distinct class requiring mathematical formulation before configuration, unlike template-matching IBN systems.
  - Quick check question: Can you distinguish between a configuration intent ("set bandwidth to 100Mbps") and an optimization intent ("minimize routing cost while meeting latency constraints")?

- Concept: Mathematical optimization formulation (decision variables, objectives, constraints)
  - Why needed here: The benchmark evaluates Gurobi code generation; understanding LP/MIP structure is prerequisite to diagnosing formulation errors (31% of failures).
  - Quick check question: For a minimum-cost flow problem, what are the decision variables, objective function, and constraint types?

- Concept: Vision-Language Model architecture (vision encoder + LLM fusion)
  - Why needed here: Error analysis reveals visual extraction as the primary bottleneck; understanding encoder-tokenizer integration helps diagnose whether failures are perceptual or reasoning-based.
  - Quick check question: In a VLM, does the vision encoder output directly become LLM input tokens, or is there an intermediate alignment layer?

## Architecture Onboarding

- Component map: Network diagram + natural language intent → VLM → Gurobi Python code → Solver execution → MCP translate_to_openflow → Ryu SDN controller + Mininet testbed

- Critical path: Diagram → VLM (parameter extraction + formulation) → Gurobi execution → MCP translate_to_openflow → deploy_and_validate. The 8.2s VLM generation dominates the 12.2s end-to-end latency.

- Design tradeoffs:
  - Direct prompting (fastest, highest ESR for GPT-5-Mini) vs. Role prompting (+3.5pp ESR, not significant) vs. PoT (verbose, -13pp degradation)
  - Closed-source models (75% ESR) vs. open-source (18% ESR, but data-local)
  - Text-only input (+12–21pp ESR) vs. multimodal (natural operator workflow)

- Failure signatures:
  - **Visual extraction failure**: Misread capacity → infeasible/incorrect solution; diagnose by comparing extracted parameters to ground truth
  - **Constraint omission**: Circulation problems show 100% median gaps despite 60–100% ESR; indicates missing flow conservation constraints
  - **Syntax/API errors**: 18% of failures from improper Gurobi usage; check variable scope and method signatures

- First 3 experiments:
  1. **Baseline replication**: Run GPT-5-Mini on 5 instances each from TSP, Min Cost Flow, and Circulation using direct prompting; measure ESR and optimality gap to validate benchmark reproduction.
  2. **Ablation: text vs. multimodal**: For the same instances, provide text-only variants; quantify per-category vision-language gap to identify which problem families suffer most from visual extraction.
  3. **Error taxonomy validation**: Manually classify 20 failed executions into visual/constraint/syntax categories; compare to paper's 42%/31%/18% distribution to confirm failure mode generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark scope limited to 85 instances across 17 categories, potentially missing real-world complexity of dynamic traffic matrices and multi-objective trade-offs.
- Performance differences between prompting strategies may shift with alternative templates or instruction tuning, suggesting sensitivity to prompt engineering.
- Results focus specifically on Gurobi Python API, limiting generalizability to other solvers or non-Python interfaces.

## Confidence
- **High confidence**: Closed-source VLMs significantly outperform open-source ones on optimization code generation, as evidenced by consistent 57 percentage point gaps across text-only and multimodal variants.
- **Medium confidence**: Visual parameter extraction introduces a 12–21 percentage point performance penalty, though this estimate depends on the assumption that text-only and multimodal variants are equivalent in difficulty beyond modality.
- **Medium confidence**: Program-of-thought prompting degrades performance by up to 13 percentage points for closed-source models, though the mechanism (reasoning verbosity vs. code-syntax interference) remains unclear.

## Next Checks
1. **Benchmark scaling**: Expand IntentOpt to 200+ instances with more diverse topologies (mesh, hierarchical, dynamic) and multi-objective formulations to test whether the visual reasoning gap persists or narrows with increased complexity.

2. **Open-source fine-tuning**: Fine-tune Llama-3.2-11B-Vision on a synthetic corpus of optimization problem descriptions and Gurobi code to determine if the 57 percentage point gap is bridgeable through domain adaptation.

3. **Cross-solver generalization**: Replicate the benchmark using CPLEX and SCIP APIs to assess whether observed performance differences stem from VLM familiarity with Gurobi specifically or broader optimization formulation capabilities.