---
ver: rpa2
title: 'Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework
  for Foundation Models'
arxiv_id: '2508.00202'
source_url: https://arxiv.org/abs/2508.00202
tags:
- noise
- label
- reliability
- local
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses robust classification under label noise for
  foundation models without retraining. The authors propose a two-stage geometry-aware
  reliability framework that first estimates sample reliability using local NNK neighborhoods
  and global clustering, then performs reliability-weighted inference.
---

# Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models

## Quick Facts
- arXiv ID: 2508.00202
- Source URL: https://arxiv.org/abs/2508.00202
- Reference count: 25
- One-line primary result: Two-stage geometry-aware reliability framework improves robust classification under label noise for foundation models without retraining, with NNK-based methods excelling at low noise and clustering-based approaches dominating at high noise levels.

## Executive Summary
This paper proposes a two-stage geometry-aware reliability framework for robust classification under label noise using fixed foundation model embeddings without retraining. The framework first estimates sample reliability using local NNK neighborhoods and global clustering, then performs reliability-weighted inference. Experiments on CIFAR-10 and DermaMNIST demonstrate improved robustness across various noise levels compared to k-NN and adaptive-neighborhood baselines. The key insight is that local geometry methods (NNK-based) work best at low noise levels by leveraging fine-grained neighborhood structure, while global clustering methods become more effective at high noise levels when local information becomes unreliable.

## Method Summary
The proposed framework operates in two stages: (1) reliability estimation and (2) reliability-weighted inference. In stage 1, the method computes reliability scores for training samples using either local NNK-based metrics (weight-based and diameter ratio) or global k-means clustering (supervised and unsupervised). NNK neighborhoods are constructed using a k=50 initial neighbor set with Gaussian kernel weighting. In stage 2, test samples are classified by finding their NNK neighbors in the training set and performing reliability-weighted majority voting. The framework supports both weighted inference (combining NNK weights and reliability) and unweighted inference (reliability only), with the choice depending on noise level.

## Key Results
- NNK-based reliability methods (weights and diameter ratio) outperform k-NN and adaptive-neighborhood baselines under low noise levels by leveraging local geometric structure.
- Global clustering-based methods (supervised and unsupervised k-means) perform better at high noise levels where local neighborhoods become unreliable due to label corruption.
- The framework achieves significant accuracy improvements without requiring model retraining, demonstrating that geometry-aware reliability estimation can effectively mitigate label noise effects.
- Unsupervised k-means reliability shows particular robustness at high noise levels, as it avoids the label contamination issues that affect per-class clustering.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NNK-based neighborhood construction provides more informative local geometry than k-NN for reliability estimation.
- Mechanism: The Non-Negative Kernel (NNK) algorithm constructs sparse, geometrically non-redundant neighborhoods by selecting only neighbors that form a local polytope around each query. Unlike k-NN which treats all k neighbors equally, NNK assigns normalized weights based on relative similarity, filtering out redundant neighbors that don't contribute geometrically.
- Core assumption: The embedding space from the foundation model preserves semantic structure where instances of the same class cluster together, even under moderate label corruption.
- Evidence anchors:
  - [abstract] "...our proposed inference uses a local neighborhood of training data, obtained using the non-negative kernel (NNK) neighborhood construction."
  - [section II-A] "The NNK algorithm constructs a sparse local neighborhood that includes only the geometrically non-redundant, most similar neighbors to the query, making it more suitable for this application than the k-NN method."
  - [corpus] Limited direct corpus validation for NNK specifically; related work (Di Salvo et al. [20] WANN) provides the baseline this method improves upon.

### Mechanism 2
- Claim: Reliability scores derived from NNK weights and diameter ratios identify trustworthy samples without requiring clean validation data.
- Mechanism: Two complementary local metrics: (1) NNK-weighted reliability sums weights of same-label neighbors, and (2) diameter ratio reliability compares the geometric spread of all neighbors versus same-label neighbors. Samples with high same-label weight concentration and small diameter ratios indicate locally consistent (likely correct) labels.
- Core assumption: Label noise is not spatially correlated in the embedding space—corrupted labels are scattered rather than forming coherent clusters.
- Evidence anchors:
  - [section II-A, equations 1-2] Defines ˆηq as sum of NNK weights for same-label neighbors; diameter ratio as diam(Sq)/diam(Ŝq).
  - [section III-B1] "...all NNK-based scores outperform them [WANN, ANN, k-NN] under low noise."
  - [corpus] Corpus papers on noisy label learning (e.g., "Pre-train to Gain") validate the general premise that pre-trained features aid robustness, but do not specifically validate NNK-derived reliability.

### Mechanism 3
- Claim: Global clustering-based reliability estimation maintains robustness when local neighborhoods become unreliable at high noise levels.
- Mechanism: Supervised k-means (per-class clustering) and unsupervised k-means (with soft label assignment) provide reliability scores based on proximity to cluster centroids. Cluster centroids shift less than individual sample neighborhoods under label corruption, acting as noise-resistant reference points.
- Core assumption: The number of clusters per class is chosen appropriately; cluster structure exists in the embedding space and is not destroyed by noise.
- Evidence anchors:
  - [section II-A, equations 3-6] Defines distance-weighted reliability using cluster centroids for both supervised and unsupervised variants.
  - [section III-B1] "As label noise increases... global methods based on clustering perform better. Cluster centers tend to shift less in the presence of label noise."
  - [section III-B4] "Geometry-only clustering (unsupervised) seems more helpful at high noise levels, where label errors can render per-label clusters meaningless."
  - [corpus] Weak corpus validation—no directly comparable clustering-based reliability methods found in neighbors.

## Foundational Learning

- Concept: **Foundation model embeddings and transfer learning**
  - Why needed here: The entire framework operates on fixed embeddings from a pre-trained FM (DINOv2). Understanding that these embeddings capture semantic structure without task-specific training is essential.
  - Quick check question: Can you explain why the embeddings are L2-normalized and what this implies for distance-based methods?

- Concept: **k-nearest neighbors and majority voting classifiers**
  - Why needed here: The baseline methods (k-NN, ANN, WANN) and the proposed method all use neighborhood-based voting. Understanding the limitations of standard k-NN motivates NNK improvements.
  - Quick check question: What happens to k-NN voting when >50% of neighbors have incorrect labels?

- Concept: **Label noise types (symmetric vs. asymmetric)**
  - Why needed here: The paper evaluates under both noise regimes, and the optimal reliability method differs by noise type. Asymmetric noise is harder because it creates systematic mislabeling patterns.
  - Quick check question: Why might asymmetric noise cause local geometry methods to fail more than symmetric noise?

## Architecture Onboarding

- Component map:
  - Embedding extraction -> NNK graph construction -> Reliability estimation -> Reliability-weighted inference -> Class prediction

- Critical path:
  1. Embed all training and test data using the frozen FM
  2. Build NNK graph over training embeddings (this defines all local neighborhoods)
  3. Compute reliability scores for all training samples using chosen method (NNK-weights, diameter-ratio, supervised k-means, or unsupervised k-means)
  4. For each test sample: find its NNK neighbors in training set, retrieve their labels and reliabilities, compute weighted vote

- Design tradeoffs:
  - **Weighted vs. Unweighted inference**: Weighted (combining NNK weights and reliability) works better at low noise; unweighted (reliability only) works better at high noise. Assumption: distances become unreliable when many labels are wrong.
  - **Local vs. Global reliability**: Local methods (NNK-weights, diameter-ratio) leverage fine-grained geometry; global methods (clustering) are robust to neighborhood corruption. Paper recommends clustering for >40% noise.
  - **Supervised vs. Unsupervised k-means**: Supervised (per-class clustering) works when class structure is clean; unsupervised is more robust when per-class clusters are contaminated by noise.
  - **Computational cost**: NNK graph construction is O(n²) in naive form; k-means adds O(n·M·iterations). Both stages avoid model retraining, which is the primary computational savings.

- Failure signatures:
  - **High noise with local-only methods**: Accuracy drops sharply when local neighborhoods contain majority-wrong labels (breaks majority voting assumption).
  - **Poor embedding quality**: If the FM embedding doesn't separate classes well (as in DermaMNIST vs. CIFAR-10), all methods degrade; global methods become relatively more important.
  - **Wrong cluster count Kc**: Too few clusters misses class substructure; too many creates unreliable centroids with few samples.
  - **Asymmetric noise with supervised k-means**: Per-class clusters may absorb systematically mislabeled samples, making centroids unreliable.

- First 3 experiments:
  1. **Baseline reproduction**: Implement k-NN, ANN, and WANN baselines on CIFAR-10 with symmetric noise at {0%, 20%, 40%, 60%}. Verify you can reproduce the paper's baseline curves before implementing NNK methods.
  2. **NNK-weighted reliability ablation**: Implement the core NNK-weighted reliability score (equation 1) with weighted inference. Test on CIFAR-10 clean and 20% symmetric noise. Compare against k-NN baseline to validate the local geometry advantage.
  3. **Noise threshold detection**: Run both local (NNK-weights) and global (unsupervised k-means) reliability methods across the full noise range. Identify the crossover point where global methods begin to outperform local methods—this validates the paper's core claim about the local-global tradeoff.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact implementation of the NNK neighborhood construction algorithm beyond the citation [21] is not specified, which could affect reproducibility of reliability scores.
- The paper does not fully specify asymmetric noise label flip mappings, limiting exact replication of results.
- The DermaMNIST dataset is not widely benchmarked for noisy label scenarios, so results may not generalize to other complex medical imaging tasks.
- The claim that unsupervised k-means performs better at high noise levels is supported empirically but lacks theoretical justification for why cluster centroids remain more stable than local neighborhoods.

## Confidence

- **High confidence**: The core claim that geometry-aware reliability estimation improves robustness without retraining is well-supported by experimental results across multiple noise levels and datasets.
- **Medium confidence**: The superiority of NNK over k-NN for local geometry is theoretically sound but lacks extensive corpus validation.
- **Low confidence**: The recommendation to use unsupervised k-means at high noise levels is based on limited evidence (one dataset, two noise types).

## Next Checks
1. Implement a controlled experiment testing the framework on CIFAR-10 with a specific asymmetric noise pattern (e.g., truck→automobile, bird→airplane) to verify the claimed failure of supervised k-means at high noise levels.
2. Compare the proposed method against the "Pre-train to Gain" baseline [neighbor paper] on the same datasets to establish relative performance in the broader noisy label literature.
3. Test the framework on a third dataset with different characteristics (e.g., ImageNet subset or Food-101) to validate generalization beyond CIFAR-10 and DermaMNIST.