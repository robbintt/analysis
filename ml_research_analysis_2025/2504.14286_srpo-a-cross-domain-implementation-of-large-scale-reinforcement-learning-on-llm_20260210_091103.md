---
ver: rpa2
title: 'SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning
  on LLM'
arxiv_id: '2504.14286'
source_url: https://arxiv.org/abs/2504.14286
tags:
- training
- reasoning
- arxiv
- code
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SRPO, a cross-domain reinforcement learning
  framework for large language models that addresses key training challenges in multi-domain
  RL. The authors propose a two-stage training paradigm that first develops mathematical
  reasoning capabilities before integrating coding tasks, along with history resampling
  to eliminate ineffective training samples with zero gradients.
---

# SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM

## Quick Facts
- arXiv ID: 2504.14286
- Source URL: https://arxiv.org/abs/2504.14286
- Reference count: 27
- Surpasses DeepSeek-R1-Zero-Qwen-32B using only 1/10 of the training steps

## Executive Summary
SRPO introduces a cross-domain reinforcement learning framework for large language models that addresses key training challenges in multi-domain RL. The approach employs a two-stage training paradigm, first developing mathematical reasoning capabilities before integrating coding tasks, along with history resampling to eliminate ineffective training samples with zero gradients. This method achieves state-of-the-art performance on math and coding benchmarks while demonstrating improved sample efficiency and emergent self-verification capabilities during training.

## Method Summary
SRPO builds on GRPO with innovations including domain-specific data curation, response-length conflict resolution between math and code tasks, and improved sample efficiency through history resampling. The two-stage training approach first develops mathematical reasoning capabilities before integrating coding tasks, allowing the model to build foundational reasoning skills before tackling more complex cross-domain problems. The history resampling mechanism eliminates training samples with zero gradients, improving sample efficiency by focusing computational resources on productive learning signals.

## Key Results
- Achieves 50.0 pass@1 on AIME24 benchmark
- Scores 41.6 pass@1 on LiveCodeBench
- Surpasses DeepSeek-R1-Zero-Qwen-32B performance using only 1/10 of the training steps

## Why This Works (Mechanism)
The two-stage training paradigm allows the model to first master mathematical reasoning in isolation, building foundational capabilities before introducing the additional complexity of coding tasks. This staged approach prevents interference between domains with different response characteristics. History resampling eliminates zero-gradient samples that would otherwise waste computational resources, focusing training on productive samples that provide meaningful gradient signals. The response-length conflict resolution mechanism addresses the inherent tension between the typically longer, more detailed responses required for coding versus the more concise answers needed for mathematical reasoning.

## Foundational Learning
- Reinforcement learning optimization - Why needed: Enables model to improve through trial and error based on reward signals; Quick check: Verify gradient flow and reward shaping effectiveness
- Multi-domain task integration - Why needed: Allows single model to handle diverse problem types; Quick check: Test cross-domain transfer performance
- Sample efficiency optimization - Why needed: Reduces computational cost and training time; Quick check: Measure training step reduction vs baseline
- Emergent reasoning patterns - Why needed: Indicates development of sophisticated problem-solving capabilities; Quick check: Quantify frequency of self-correction behaviors
- Domain-specific data curation - Why needed: Ensures high-quality training signals for each task type; Quick check: Analyze data distribution and quality metrics
- Response-length conflict resolution - Why needed: Prevents degradation when combining tasks with different output requirements; Quick check: Measure output quality across domains

## Architecture Onboarding

Component Map:
Data Curation -> Two-Stage Training -> History Resampling -> GRPO Optimization -> Performance Evaluation

Critical Path:
The critical path follows: domain-specific data curation → two-stage training (math → coding) → history resampling → GRPO optimization → benchmark evaluation. This sequence ensures proper skill development and efficient learning by eliminating unproductive samples at each stage.

Design Tradeoffs:
The primary tradeoff involves computational efficiency versus comprehensive learning. History resampling improves efficiency by eliminating zero-gradient samples but risks discarding potentially valuable learning signals. The two-stage approach sacrifices immediate cross-domain capability for stronger foundational skills but requires more careful curriculum design and longer overall training time.

Failure Signatures:
- Performance degradation on either math or coding domains indicates improper response-length conflict resolution
- Lack of improvement despite training suggests ineffective history resampling or poor reward signal design
- Inconsistent performance across similar difficulty problems points to curriculum design issues in the two-stage approach

First Experiments:
1. Compare single-stage versus two-stage training performance to validate curriculum effectiveness
2. Measure impact of history resampling by comparing with and without this mechanism on sample efficiency
3. Test response-length conflict resolution by training with and without specialized handling for math versus coding tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on math and coding benchmarks, leaving unclear how well the approach generalizes to other domains like reasoning, planning, or multimodal tasks
- The "1/10 training steps" efficiency claim lacks context about computational resources, training infrastructure, and baseline comparisons beyond DeepSeek-R1-Zero-Qwen-32B
- History resampling mechanism needs more rigorous validation to confirm it doesn't inadvertently discard valuable learning signals or introduce bias toward easier problems

## Confidence
- **High**: Performance metrics on AIME24 and LiveCodeBench (specific numerical results with clear evaluation protocols)
- **Medium**: Two-stage training paradigm effectiveness (based on described methodology but limited ablation studies)
- **Low**: Emergent reasoning patterns (qualitative observations lacking systematic quantification)

## Next Checks
1. Conduct ablation studies comparing SRPO performance with and without history resampling to quantify its actual contribution to sample efficiency
2. Test the model on additional reasoning benchmarks (e.g., GSM8K, BBH) and non-mathematical domains to assess true cross-domain generalization
3. Measure and report the frequency and accuracy of self-correction and self-verification behaviors through automated detection and systematic error analysis