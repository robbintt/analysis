---
ver: rpa2
title: 'Multilingual Conversational AI for Financial Assistance: Bridging Language
  Barriers in Indian FinTech'
arxiv_id: '2512.01439'
source_url: https://arxiv.org/abs/2512.01439
tags:
- language
- financial
- multilingual
- india
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multilingual conversational AI system for
  financial assistance that supports code-mixed languages like Hinglish to serve India's
  diverse user base. The system employs a multi-agent architecture with language classification,
  function management, and multilingual response generation.
---

# Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech

## Quick Facts
- arXiv ID: 2512.01439
- Source URL: https://arxiv.org/abs/2512.01439
- Reference count: 30
- One-line primary result: 41% increase in task completion rates and 86% increase in average session length compared to English-only baselines

## Executive Summary
This paper presents a multilingual conversational AI system for financial assistance that addresses India's linguistic diversity through code-mixed language support. The system employs a multi-agent architecture with Indic-BERT for language classification and Hermes-3-8B for response generation, achieving significant performance improvements over English-only baselines. Through real-world deployment with 500+ users, the system demonstrated 41% higher task completion rates and 86% longer session durations, validating the effectiveness of decoupling language processing from core financial logic.

## Method Summary
The system implements a four-stage pipeline: (1) language classification using Indic-BERT for detecting code-mixed financial queries across English, Hindi, Marathi, and Gujarati; (2) orchestrator with query rephrasing to canonical English and intent classification; (3) specialized financial tools combining deterministic modules and LLM-powered agents; (4) response generation with Hermes-3-8B, passing structured tool outputs and detected language tag. The architecture achieves performance parity with English queries across supported languages by normalizing code-mixed inputs while preserving semantic intent.

## Key Results
- 41% increase in task completion rates compared to English-only baselines
- 86% increase in average session length across 500+ beta users
- 95.8% accuracy for financial Hinglish classification using Indic-BERT vs. 63.7% for general-purpose models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling language processing from core financial logic through query rephrasing enables performance parity across supported languages.
- Mechanism: A language classifier identifies input language → orchestrator normalizes code-mixed queries to canonical English → existing financial tools operate on language-agnostic representations → response generator synthesizes output back to user's original language.
- Core assumption: Query rephrasing preserves intent and critical financial entities across language transformations.
- Evidence anchors: [abstract] "The architecture successfully decouples language processing from core financial logic through query rephrasing"; [section 4.2.2] "By converting all inputs into a canonical English representation before tool selection, we decoupled the robust, pre-existing financial tools from the complexities of multilingual understanding."

### Mechanism 2
- Claim: Domain-adapted language models (Indic-BERT) significantly outperform general-purpose models for code-mixed financial query classification.
- Mechanism: Indic-BERT's pre-training on 11 Indian languages provides transferable representations for code-mixed patterns, enabling 95.8% accuracy on financial Hinglish queries vs. 63.7% for general-purpose Qwen2.5-0.5B.
- Core assumption: Language relatedness within Indo-Aryan family enables positive transfer for financial domain code-mixing.
- Evidence anchors: [abstract] "use of Indic-BERT for accurate language detection in code-mixed financial queries"; [section 4.2.1, Table 2] Financial Hinglish classification: Indic-BERT 95.8% accuracy vs. Qwen2.5-0.5B 63.7%.

### Mechanism 3
- Claim: High instruction-following capability in response generation models correlates with maintained financial accuracy across languages.
- Mechanism: Hermes-3-8B's 4.6/5 instruction-following score and 93.7% tool-calling accuracy ensure structured financial data is correctly synthesized into natural multilingual responses without fabrication.
- Core assumption: Strong instruction-following on English benchmarks transfers to multilingual generation quality.
- Evidence anchors: [abstract] "Hermes-3-8B for high-quality multilingual response generation"; [section 4.4, Table 3] Hermes-3-8B: 4.6/5 instruction following, 93.7% tool calling accuracy.

## Foundational Learning

- Concept: Code-mixing (intra-sentential language alternation)
  - Why needed here: The system explicitly targets "Hinglish" queries where Hindi grammar mixes with English financial terms. Without understanding this phenomenon, natural user queries will be misclassified as errors.
  - Quick check question: Can you identify which words in "Mere holdings mai sabse jyada returns konsa fund deta hai?" are Hindi vs. English, and why this differs from simple translation?

- Concept: Query normalization/rewriting for intent preservation
  - Why needed here: The system's core innovation is transforming code-mixed queries to canonical English while preserving intent. Understanding what semantic content must survive this transformation is critical.
  - Quick check question: If a user asks "kya yeh fund safe hai?", what information must be preserved in the normalized query? What can be discarded?

- Concept: Multi-agent orchestration with heterogeneous tool types
  - Why needed here: The system combines deterministic software modules with LLM-powered agents. Understanding when to use each type affects latency and accuracy.
  - Quick check question: For a query asking "Compare HDFC and SBI gold funds by expense ratio," which parts require deterministic tools vs. LLM-powered synthesis?

## Architecture Onboarding

- Component map: User Query → Language Classifier (Indic-BERT, <20ms) → Orchestrator (LLM) → Query Rephrasing → Normalized English → Intent Classification → Tool Selection → Specialized Financial Tools → Response Generator (Hermes-3-8B) + Language Tag → User Response (original language)

- Critical path: Language classification → Query rephrasing accuracy → Tool selection precision → Response grounding. Errors cascade: a misclassification at step 1 propagates through all downstream stages.

- Design tradeoffs:
  - Latency vs. accuracy: Language classifier adds <20ms overhead; total system overhead is 4-8% (from abstract).
  - Model size vs. deployment cost: Hermes-3-8B chosen over Aya-Expanse-32B (73.9% accuracy but 1216ms latency vs 310ms).
  - Strictness of normalization: Aggressive normalization may lose linguistic nuance; permissive normalization may fail on complex code-mixed patterns.

- Failure signatures: Intent misclassification in multi-intent queries, factual hallucination in responses, language detection failure in short queries, awkward phrasing indicating prompt engineering gaps.

- First 3 experiments:
  1. Language classifier stress test: Feed 100 code-mixed financial queries with varying English/Hindi ratios through Indic-BERT vs. Qwen2.5-0.5B. Measure accuracy by script type and query length.
  2. Query rephrasing semantic drift test: Pass 50 multi-turn conversations through the orchestrator's normalization step. Use LLM-as-judge to score intent preservation between original and normalized queries.
  3. Response grounding validation: Deploy response generator with financial data in a staging environment. Manually verify 100 generated responses against source data for factual accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed query rephrasing strategy maintain high task success rates when applied to morphologically richer or structurally distant code-mixed languages (e.g., Dravidian languages like Tamil or Malayalam) compared to the Hindi-English pair primarily tested?
- Basis in paper: [explicit] The "Limitations" section states the system "focuses primarily on Hindi-English code-mixing and may not generalize to other Indian language combinations without significant adaptation."
- Why unresolved: The study validates the architecture on Hindi, Marathi, and Gujarati (Indo-Aryan family), but the authors explicitly note that generalization to other language families remains unverified.
- What evidence would resolve it: Benchmarking results showing Intent & Tool Call Accuracy parity between Indo-Aryan code-mixed inputs and Dravidian code-mixed inputs using the current rephrasing pipeline.

### Open Question 2
- Question: How can the multi-agent orchestration be enhanced to robustly handle multi-intent code-mixed queries without suffering from the partial execution errors observed in the deployment analysis?
- Basis in paper: [explicit] Table 6 (Error Analysis) identifies "Intent Misclassification" in multi-intent queries as a failure case, and the text explicitly calls for "enhancing the orchestrator's multi-intent reasoning capabilities" in future work.
- Why unresolved: Current failures (e.g., returning a fund list but omitting requested expense ratios) suggest the normalization or rephrasing step loses semantic nuance when multiple constraints are present in a single code-mixed utterance.
- What evidence would resolve it: A comparative study of success rates on a dataset of complex, multi-clause code-mixed financial queries before and after implementing a specific multi-intent decomposition module.

### Open Question 3
- Question: What specific grounding mechanisms can effectively reduce factual hallucination rates in the response generation module without introducing latency that breaks the real-time conversational flow?
- Basis in paper: [explicit] The error analysis in Section 4.5 attributes "Factual Hallucination" to a "Lack of grounding mechanisms," and the authors list improving these mechanisms as a key insight for future development.
- Why unresolved: The current system relies on Hermes-3-8B for generation, but lacks a cited mechanism to verify generated text against the structured tool output, leading to fabricated financial data in some instances.
- What evidence would resolve it: Implementation of a verification layer (e.g., retrieval-augmented generation or factual consistency checking) that reduces the hallucination rate to near zero while keeping latency within the 4-8% overhead benchmark.

### Open Question 4
- Question: To what extent does the system's performance vary across regional dialects versus "standard" urban Hindi, and can dialect-aware personalization effectively mitigate potential socio-economic or linguistic biases?
- Basis in paper: [explicit] The Conclusion proposes developing "dialect-aware personalization models," and the Ethical Considerations section explicitly warns of "linguistic bias" where performance is better for standard urban dialects.
- Why unresolved: The paper identifies the risk of bias against regional variations but does not quantify the performance gap or validate a solution for dialect-specific nuances in the current deployment.
- What evidence would resolve it: Disaggregated evaluation metrics showing response quality and task completion rates across distinct dialect clusters of users, followed by a demonstration of bias mitigation through fine-tuning.

## Limitations
- Training data composition and fine-tuning procedures for Indic-BERT's financial domain adaptation remain unspecified, creating uncertainty about generalizability.
- Orchestrator query rephrasing prompts and financial tools' API schemas are not provided, creating significant barriers to replication.
- The "golden test set" of multi-turn conversations lacks specification of size, intent distribution, and reference answers.

## Confidence
- **High Confidence**: The decoupling architecture and its performance benefits are well-supported by empirical data and logical design principles.
- **Medium Confidence**: Domain adaptation benefits of Indic-BERT are empirically validated for Hinglish financial queries, but underlying assumptions about language relatedness rely on external literature.
- **Low Confidence**: Claims about Hermes-3-8B's instruction-following capabilities transferring to multilingual financial generation lack direct evidence for Indic languages.

## Next Checks
1. Language Classifier Generalization Test: Evaluate Indic-BERT on a diverse test set of 200+ code-mixed financial queries spanning different Hindi-English mixing ratios, query lengths, and financial domains.
2. Semantic Drift Analysis: Implement automated LLM-as-judge evaluation to measure intent preservation rates across 100+ query rephrasing operations, focusing on complex multi-intent queries.
3. Grounding Robustness Validation: Deploy the complete system with live financial data feeds and conduct systematic hallucination detection by comparing 500+ generated responses against source data, measuring factual accuracy by language and query complexity.