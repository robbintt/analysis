---
ver: rpa2
title: A word association network methodology for evaluating implicit biases in LLMs
  compared to humans
arxiv_id: '2510.24488'
source_url: https://arxiv.org/abs/2510.24488
tags:
- biases
- llms
- bias
- humans
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a novel word association network methodology
  for evaluating implicit biases in Large Language Models (LLMs) compared to humans.
  The method simulates semantic priming within LLM-generated word association networks
  to provide quantitative and qualitative bias assessments.
---

# A word association network methodology for evaluating implicit biases in LLMs compared to humans

## Quick Facts
- arXiv ID: 2510.24488
- Source URL: https://arxiv.org/abs/2510.24488
- Authors: Katherine Abramski; Giulio Rossetti; Massimo Stella
- Reference count: 40
- Key outcome: Novel word association network methodology reveals both convergence and divergence in implicit biases between humans and LLMs across gender, religion, ethnicity, sexual orientation, and political party.

## Executive Summary
This paper introduces a novel word association network methodology to evaluate implicit biases in Large Language Models (LLMs) compared to humans. The approach simulates semantic priming within LLM-generated word association networks, providing both quantitative and qualitative bias assessments. Applied to humans and three LLMs (Mistral, Llama3, Haiku), the methodology investigates biases across five social dimensions. Results show that while LLMs generally mirror human patterns in valence biases, they sometimes exhibit opposite trends and vary in intensity and direction for stereotypes and emotions. The method offers a flexible, interpretable, and scalable tool for bias evaluation, grounded in cognitive psychology and network science.

## Method Summary
The methodology constructs word association networks from free association data (SWOW for humans, LWOW for LLMs) using identical prompts. Networks are filtered to WordNet vocabulary, retain only the largest connected component, and convert directed edges to undirected (max weight preserved). Spreading activation simulates semantic priming: initial activation equals network size, time steps equal twice the network diameter, and retention rate is 0.5. Final activation levels quantify semantic proximity between prime and target nodes. Bias is measured via effect sizes from Wilcoxon tests (stereotypes/emotions) or GLM coefficients (valence). L2 normalization is used for stereotypes and valence; L1 for emotions to avoid paradoxical results.

## Key Results
- Humans exhibited strongest gender stereotypes, while LLMs showed varied intensity and direction across gender, religion, ethnicity, sexual orientation, and political party.
- LLMs generally mirrored human patterns in valence biases but sometimes exhibited opposite trends (e.g., Republicans associated with more positive emotions).
- The methodology revealed that Mistral showed moderate gender bias, Llama3 showed low bias, and Haiku showed higher bias in gender stereotypes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Free association prompts elicit implicit semantic structure from LLMs without accessing internal representations.
- Mechanism: By prompting LLMs to generate word associations (e.g., "What 3 words come to mind when you see 'doctor'?"), the methodology captures relational knowledge encoded during pretraining. These associations form a network proxy for the model's implicit conceptual structure.
- Core assumption: LLM free associations reflect stable implicit attitudes analogous to human free associations, which correlate with implicit attitudes (Schnabel & Asendorpf, 2013, cited in paper).
- Evidence anchors:
  - [abstract] "Our prompt-based approach taps into the implicit relational structures encoded in LLMs, providing both quantitative and qualitative assessments of bias."
  - [Page 7] "the LWOW dataset was created by simulating the same behavioral experiment used to create the SWOW dataset... tapping into the implicit semantic representations encoded in the LLM architecture."
  - [corpus] "Aligned but Blind" finds alignment increases implicit bias by reducing awareness, supporting that implicit structures persist despite surface-level debiasing.
- Break condition: If LLM associations are driven by prompt artifacts or instruction-following rather than semantic structure, the network would reflect compliance behavior, not implicit knowledge.

### Mechanism 2
- Claim: Spreading activation within word association networks models semantic priming, quantifying association strength between social categories and attributes.
- Mechanism: When a prime node (e.g., "woman") is activated, activation spreads through weighted edges to neighbors. The final activation level of a target node (e.g., "compassionate") reflects semantic proximity. Higher final activation indicates stronger implicit association.
- Core assumption: The computational spreading activation process validly approximates human semantic priming.
- Evidence anchors:
  - [Page 5] "This computational approach has been empirically validated by correlating final activation levels from the computational approach with reaction times from the LDT behavioral experiment."
  - [Page 6] "Since semantic relatedness is the cognitive mechanism that drives semantic priming, then the final activation levels of target nodes can be considered a proxy for semantic relatedness."
  - [corpus] Weak direct corpus evidence for this specific validation in LLM networks; relies on prior human network work (spreadr package).
- Break condition: If network topology poorly represents semantic structure (e.g., due to filtering choices, sparsity), activation patterns would reflect network artifacts rather than semantic relationships.

### Mechanism 3
- Claim: Comparing final activation levels between stereotype-consistent and stereotype-inconsistent prime-target pairs quantifies implicit bias.
- Mechanism: For gender stereotypes, the difference `AL_woman("compassionate") - AL_man("compassionate")` captures relative association strength. Effect sizes from Wilcoxon tests on paired differences provide a comparable bias metric across humans and LLMs.
- Core assumption: Activation level asymmetries reflect implicit bias rather than other linguistic or frequency effects (addressed via L2 normalization).
- Evidence anchors:
  - [Page 9] "Positive differences are indicative of stereotype bias, since they represent greater strength of association between stereotype-consistent prime-target pairs."
  - [Page 10] Effect sizes enable "direct comparisons between different LLMs and humans."
  - [corpus] "Implicit Bias in LLMs: A Survey" notes implicit bias persists despite explicit debiasing, consistent with the paper's finding that LLMs show varied bias patterns.
- Break condition: If normalization (L1 vs. L2) substantially changes results (as noted for emotions approach on p. 19), the metric may be sensitive to arbitrary analytic choices rather than robust bias signals.

## Foundational Learning

- Concept: **Spreading Activation Theory**
  - Why needed here: Core computational mechanism; understanding how activation propagates through networks is essential for interpreting final activation levels.
  - Quick check question: Given a network where A connects to B (weight 10) and C (weight 5), if A receives 100 activation units and retains 50%, how much reaches C?

- Concept: **Semantic Priming**
  - Why needed here: The methodology operationalizes implicit bias via priming; recognizing "doctor" faster after "hospital" vs. "school" is the behavioral phenomenon being modeled.
  - Quick check question: Why does semantic priming implicate implicit rather than explicit attitudes?

- Concept: **Implicit Association Test (IAT) Logic**
  - Why needed here: The paper's approach extends IAT principles (faster responses = stronger associations) to computational networks.
  - Quick check question: In the IAT, what does a shorter reaction time for "flower + pleasant" vs. "insect + pleasant" indicate?

## Architecture Onboarding

- Component map:
  1. SWOW (human free associations) and LWOW (LLM free associations from identical prompts)
  2. Cue → response edges weighted by frequency; filtered to WordNet vocabulary, largest connected component
  3. Spreading activation engine: `spreadr` R package; parameters: initial activation = network size, time steps = 2× diameter, retention = 0.5
  4. Bias quantification: Normalized activation matrices → paired differences → effect sizes (Wilcoxon) or GLM coefficients

- Critical path:
  - Human and LLM associations must use identical cue words and collection protocols for comparability
  - Prime nodes must exist in both networks
  - Normalization choice (L1 vs. L2) must align with theoretical expectations for the bias type

- Design tradeoffs:
  - **L1 vs. L2 normalization**: L2 for stereotypes/valence; L1 preferred for emotions to avoid contradictory emotions assigned to same party (p. 19)
  - **Network filtering**: Removing weight-1 edges reduces noise but may exclude legitimate rare associations
  - **Directed vs. undirected edges**: Converted to undirected (max weight preserved), losing directional information

- Failure signatures:
  - Prime or target word absent from network → analysis cannot run
  - Polysemous words (e.g., "white" as color vs. race) introduce confounds (acknowledged p. 19)
  - L1/L2 producing divergent results signals sensitivity to normalization choice

- First 3 experiments:
  1. Replicate gender stereotype analysis on a new LLM using the LWOW protocol; compare effect sizes to human baseline.
  2. Test robustness: rerun one bias domain with alternative normalization; document divergence.
  3. Extend to a new social identity (e.g., age) by defining appropriate prime nodes and validating they exist in networks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the implicit bias identified via word association networks predict discriminatory outcomes in real-world decision-making contexts?
- Basis in paper: [explicit] The authors state in the limitations that "it remains uncertain to what extent they predict real-world behaviors in domains such as healthcare, recruitment, or criminal justice."
- Why unresolved: The current methodology measures intrinsic, latent associations (modeling semantic priming) rather than extrinsic performance in specific downstream tasks where discrimination causes harm.
- What evidence would resolve it: Correlational studies comparing the network-based bias scores of specific LLMs with their performance on extrinsic bias benchmarks (e.g., hiring simulations or resume screening tasks).

### Open Question 2
- Question: Can targeted corpus curation or network-level edits reduce bias amplification without degrading linguistic competence?
- Basis in paper: [explicit] The authors suggest that "future work should test whether targeted corpus curation or debiasing alters these valence profiles and whether network-level edits can attenuate amplification effects without degrading linguistic competence."
- Why unresolved: While the paper identifies the presence and structure of biases (e.g., "counter-biases" in sexual orientation), it does not test intervention strategies to modify these network structures.
- What evidence would resolve it: Experiments where specific edges or nodes are altered in the training data or the network structure, followed by a reassessment of bias metrics and standard language capability benchmarks.

### Open Question 3
- Question: Do these bias patterns hold across multilingual or non-Western cultural contexts?
- Basis in paper: [explicit] The authors note that the human data (SWOW) is Western-centric and state that "Future work should aim to use multilingual or cross-cultural human-generated data to provide a more globally representative human benchmark."
- Why unresolved: The current human baseline reflects a US/UK-centric worldview, and it is unclear if the divergences observed in LLMs (e.g., political emotions) persist when compared against non-Western human cognitive structures.
- What evidence would resolve it: Replicating the methodology using free association norms from non-English speaking cultures and comparing the resulting activation patterns against multilingual LLM versions.

## Limitations

- Core assumptions about LLM free associations reflecting stable implicit knowledge lack direct empirical validation in the LLM context.
- Sensitivity to normalization choices (L1 vs. L2) and influence of polysemous words represent notable limitations that could affect reproducibility.
- The methodology requires careful parameter selection and network construction choices that may not generalize across different LLM architectures or cultural contexts.

## Confidence

- Claims about convergence with human bias patterns: Medium
- Claims about LLM-specific bias patterns: Low
- Claims about methodology flexibility and scalability: Medium

## Next Checks

1. Test robustness by re-running one bias domain with alternative normalization (L1 vs. L2) and document divergence in effect sizes.
2. Extend the analysis to a new social identity dimension (e.g., age) by defining appropriate prime nodes and validating their presence in both human and LLM networks.
3. Apply the methodology to a new LLM model (e.g., GPT-4) using the LWOW protocol and compare effect sizes to the human baseline.