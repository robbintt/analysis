---
ver: rpa2
title: Holistic Evaluation of Multimodal LLMs on Spatial Intelligence
arxiv_id: '2508.13142'
source_url: https://arxiv.org/abs/2508.13142
tags:
- reasoning
- answer
- spatial
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces EASI, a holistic framework for evaluating\
  \ multimodal large language models (MLLMs) on spatial intelligence (SI). EASI unifies\
  \ six fundamental spatial capabilities\u2014Metric Measurement, Mental Reconstruction,\
  \ Spatial Relations, Perspective-taking, Deformation/Assembly, and Comprehensive\
  \ Reasoning\u2014into a taxonomy that spans eight newly released benchmarks."
---

# Holistic Evaluation of Multimodal LLMs on Spatial Intelligence

## Quick Facts
- arXiv ID: 2508.13142
- Source URL: https://arxiv.org/abs/2508.13142
- Reference count: 40
- Key outcome: Proprietary models lead in spatial intelligence evaluation, but large gaps remain in mental reconstruction, perspective-taking, and deformation/assembly tasks, especially for open-source models

## Executive Summary
This paper introduces EASI, a comprehensive framework for evaluating multimodal large language models (MLLMs) on spatial intelligence (SI). The authors unify six fundamental spatial capabilities into a taxonomy and apply it across eight newly released benchmarks using standardized prompts and metrics. Their evaluation of leading proprietary and open-source models reveals that while GPT-5 leads overall and matches human performance on basic metric measurement and spatial relations, significant gaps persist in more complex spatial reasoning tasks. Notably, proprietary models lose their typical advantage on the hardest spatial tasks, showing parity with open-source alternatives. The authors release an open-source EASI codebase and leaderboard to advance community progress in spatial intelligence evaluation.

## Method Summary
The EASI framework evaluates MLLMs on spatial intelligence through a unified taxonomy of six fundamental capabilities: Metric Measurement, Mental Reconstruction, Spatial Relations, Perspective-taking, Deformation/Assembly, and Comprehensive Reasoning. The framework applies standardized evaluation protocols across eight benchmarks, using zero-shot chain-of-thought prompting and Chance-Adjusted Accuracy metrics. The authors conduct comprehensive evaluations of over ten billion tokens across proprietary (GPT-5, Gemini-2.5-pro) and open-source (Qwen2.5-VL, InternVL3) models, providing capability-specific performance breakdowns and difficulty-stratified analyses.

## Key Results
- GPT-5 leads in spatial intelligence overall and matches human performance on Metric Measurement and Spatial Relations
- Large gaps remain in Mental Reconstruction, Perspective-taking, Deformation/Assembly, and Comprehensive Reasoning tasks
- Proprietary models show no decisive advantage over open-source models on the most difficult spatial tasks (>60 points below human)
- Spatial intelligence tasks are significantly harder than non-SI tasks across all model families

## Why This Works (Mechanism)

### Mechanism 1: Capability Taxonomy Decomposition
The paper breaks spatial intelligence into six fundamental capabilities, enabling systematic diagnosis of model deficiencies. Each benchmark subtask is annotated by its primary capability requirement, allowing fine-grained performance analysis across unified categories. The taxonomy reveals that no single benchmark covers all six capabilities, validating the need for aggregation. However, if capabilities are not orthogonal (e.g., PT subsumes MR and SR), aggregated scores may double-count underlying skills.

### Mechanism 2: Standardized Protocol Reduces Evaluation Noise
EASI Protocol applies consistent zero-shot CoT prompting and Chance-Adjusted Accuracy across all MCQ benchmarks, reducing variance from prompt sensitivity and option-count effects. While this enables fair cross-benchmark comparison, it may systematically disadvantage models not instruction-tuned for CoT. Performance varies non-uniformly across models and benchmarks, with some models performing better on official prompts despite the protocol's intent to enable fair comparison.

### Mechanism 3: Difficulty Stratification Reveals Proprietary-Open Parity
Performance gaps narrow on the hardest spatial tasks (>60 points below human), suggesting current scaling approaches hit a capability ceiling shared across model families. This reveals that proprietary models lose their typical advantage on tasks requiring integrated multi-step spatial reasoning. However, if hardest tasks are simply under-represented in training data rather than architecturally constrained, proprietary models may regain advantage with targeted data curation.

## Foundational Learning

- **Concept: Chance-Adjusted Accuracy (CAA)**
  - Why needed: CAA normalizes for varying option counts across MCQ benchmarks, enabling fair comparison where raw accuracy would conflate task difficulty with question format
  - Quick check: If a benchmark has questions with 2, 4, and 8 options, does CAA=0.5 mean the same thing for each question type?

- **Concept: Perspective-Taking vs Spatial Relations Distinction**
  - Why needed: The paper shows SR tasks are nearing human parity while PT remains >50 points below—understanding why SR (static relative positions) differs from PT (viewpoint transformation) is essential for interpreting results
  - Quick check: A task asks "Is object A to the left of object B?"—is this SR or PT? What if it asks "What would object A look like from object B's position?"

- **Concept: Token Budget and Truncation Effects**
  - Why needed: The paper documents Grok-4 and Qwen3-8B hitting 16K token limits on SpatialViz, artificially depressing scores—understanding reasoning length distributions is critical for valid inference configuration
  - Quick check: A model scores 30% on a benchmark with 500 items, but 15% of responses hit the token limit with no final answer. What is the true accuracy on completed responses?

## Architecture Onboarding

- **Component map**: Benchmark loaders (eight integrated benchmarks) -> Prompt templates (EASI unified CoT prompt + benchmark-specific Official prompts) -> Answer extraction (two-stage rule-based matching) -> Metrics layer (CAA for MCQ, MRA for numerical answers) -> Evaluation strategies (Non-circular, Soft-circular, Hard-circular options)

- **Critical path**: 1) Select benchmark and protocol (EASI or Official) -> 2) Configure model with appropriate max_tokens (>=16,384 for proprietary reasoning models; 2,048 suffices for open-source) -> 3) Run inference with specified prompt -> 4) Apply answer matching pipeline -> 5) Compute CAA/MRA per capability category

- **Design tradeoffs**: EASI Protocol enables cross-benchmark comparison but may disadvantage models not instruction-tuned for CoT; Circular evaluation reduces option-position bias at 5x inference cost—paper finds Non-circular rankings broadly consistent; Higher token limits reduce truncation risk but increase cost; proprietary models show bimodal internal token usage

- **Failure signatures**: Negative CAA scores (worse than random guessing, observed for GPT-5 on certain MR tasks); Truncated outputs (models with long internal reasoning silently fail when hitting token limits); Performance swings >10 points between EASI and Official prompts for some model-benchmark pairs

- **First 3 experiments**: 1) Baseline evaluation: Run your model on MindCube-Tiny (1,050 items) under both protocols to establish SR/PT performance and test prompt sensitivity with minimal compute; 2) Token budget calibration: Test SpatialViz subset with max_tokens=[4K, 8K, 16K] to identify truncation threshold before full benchmark runs; 3) Capability drill-down: If overall scores are weak, evaluate single capability (e.g., MM via VSI-Bench numerical tasks) to determine if deficiency is localized or systemic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the performance advantage of proprietary models over open-source models diminish or vanish on the most challenging spatial intelligence tasks?
- Basis: The authors explicitly state in Section 3 and Section C.4 that proprietary models do not hold a decisive advantage over open-source models on the most difficult SI tasks and observe parity in categories like Geometric Reasoning in OmniSpatial
- Why unresolved: The paper identifies the phenomenon but does not investigate whether open-source training data is more specialized or if proprietary models are over-optimized for general non-SI tasks
- What evidence would resolve it: A controlled study comparing the scaling laws of open vs. closed models specifically on SI data, or an analysis of the training data distributions to see if open-source models use cleaner or more targeted spatial corpora

### Open Question 2
- Question: What specific failure mechanisms cause state-of-the-art models like GPT-5 to perform significantly worse than random guessing (negative CAA) on 3D Mental Reconstruction tasks?
- Basis: The results in Section 3 and Table 22 explicitly show GPT-5 attaining negative Chance-Adjusted Accuracy on tasks like 3D Mental Rotation
- Why unresolved: While the paper notes the models fail and suggests a lack of spatial representations, it does not explain why models actively select incorrect answers at a rate higher than chance
- What evidence would resolve it: Probing the model's internal hidden states during these tasks to determine if it encodes incorrect geometric priors or if attention mechanisms are consistently misweighting spatial relations

### Open Question 3
- Question: Can the observed limitations in Perspective-Taking and Deformation/Assembly be resolved by scaling 2D-centric training data, or do they require fundamental architectural changes such as 3D-aware vision encoders?
- Basis: Section 4 attributes failures in DA tasks to a lack of effective training data, but the Conclusion suggests the failure is due to a lack of fundamental spatial representations
- Why unresolved: The paper evaluates existing models without isolating whether the bottleneck is the amount of 2D image-text pairs or the model's inability to intrinsically represent 3D geometry from 2D pixels
- What evidence would resolve it: An ablation study comparing the performance of standard MLLMs against variants trained with explicit 3D geometric supervision to see if the spatial intelligence gap closes

## Limitations

- Taxonomy Completeness: The six-capability framework may not fully capture spatial intelligence, as categories are not strictly orthogonal (PT requires MR and SR capabilities)
- Prompt Protocol Generalizability: Standardized zero-shot CoT prompting may systematically disadvantage models not instruction-tuned for this format
- Hardest-Task Parity Interpretation: The claim of proprietary-open parity on hardest tasks assumes architectural constraints rather than data limitations

## Confidence

**High Confidence**: The overall finding that spatial intelligence tasks remain challenging across all model families; The documented performance gaps between proprietary and open-source models on most benchmarks; The technical implementation of the EASI framework and its codebase

**Medium Confidence**: The specific capability-based performance breakdowns and their interpretation; The claim that hardest spatial tasks show proprietary-open parity; The relative difficulty ranking of spatial vs non-spatial tasks

**Low Confidence**: The completeness of the six-capability taxonomy for spatial intelligence; The generalizability of EASI Protocol results to non-CoT model families; The architectural vs data-limitation attribution for hardest task performance

## Next Checks

1. **Capability Independence Validation**: Conduct ablation studies on capability-specific benchmarks to quantify the degree of overlap between PT, MR, and SR tasks. Use factor analysis on performance patterns across models to determine if the six capabilities are statistically distinct dimensions.

2. **Prompt Protocol Robustness**: Evaluate a diverse set of models (including those not instruction-tuned for CoT) across both EASI and Official protocols on identical benchmark subsets. Measure correlation of rankings across protocols and identify models for which protocol choice materially affects conclusions.

3. **Architecture-Agnostic Hardest-Task Analysis**: Select the five most difficult benchmarks (largest human-model gaps) and analyze their content for common features beyond spatial reasoning complexity. Test whether models specifically trained on these task types show disproportionate performance gains, indicating data limitations rather than architectural ceilings.