---
ver: rpa2
title: 'SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement
  Learning'
arxiv_id: '2510.01832'
source_url: https://arxiv.org/abs/2510.01832
tags:
- data
- html
- webpages
- training
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCRIBES introduces a reinforcement learning framework for web-scale
  semi-structured data extraction by generating reusable extraction scripts that generalize
  across structurally similar webpages within the same site. Instead of per-page LLM
  inference, it leverages layout similarity as a reward signal, encouraging models
  to produce scripts applicable to groups of webpages.
---

# SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.01832
- Source URL: https://arxiv.org/abs/2510.01832
- Reference count: 40
- Key outcome: 13%+ improvement in script quality over baselines; 4%+ boost in GPT-4o QA accuracy

## Executive Summary
SCRIBES introduces a reinforcement learning framework for web-scale semi-structured data extraction by generating reusable extraction scripts that generalize across structurally similar webpages within the same site. Instead of per-page LLM inference, it leverages layout similarity as a reward signal, encouraging models to produce scripts applicable to groups of webpages. The approach further improves by iteratively training on synthetic annotations from in-the-wild CommonCrawl data. Experiments show SCRIBES outperforms strong baselines by over 13% in script quality and boosts downstream question answering accuracy by more than 4% for GPT-4o, enabling scalable and resource-efficient web information extraction.

## Method Summary
SCRIBES uses a reinforcement learning approach to generate Python scripts that extract semi-structured data from webpages. The method leverages layout similarity across webpages from the same site as a reward signal, encouraging script generalization. Key innovations include structure-preserving HTML deduplication to reduce context length, staged training starting with gold annotations then progressing to synthetic annotations from CommonCrawl data, and a bipartite matching approach for fuzzy triple alignment. The framework is built on Qwen2.5-Instruct fine-tuned via GRPO with KL penalties.

## Key Results
- Outperforms baselines by over 13% in script quality metrics
- Improves downstream GPT-4o question answering accuracy by more than 4%
- Achieves 85.1% reduction in token length through deduplication while maintaining extraction performance
- Successfully scales to web-scale data extraction through script reusability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rewarding scripts for generalizing across structurally similar webpages produces scripts that transfer better to unseen pages within the same site.
- Mechanism: The SCRIBES reward averages self-score (script on the same page) and cross-scores (script on other pages in the same group). Cross-scores dominate the reward signal, explicitly penalizing scripts that overfit to one page.
- Core assumption: Webpages from the same site/domain share similar layouts, so scripts that work across a training group will generalize to new pages from that site.
- Evidence anchors:
  - [abstract] "leveraging layout similarity across webpages within the same site as a reward signal"
  - [section 3.3.1] "each self-score contributes only 1/|G(p)| to the final reward, while cross-scores constitute the majority"
  - [section 4.4, Table 2] Self-only reward improves example (+1.2%) but hurts holdout (-7.2%)
  - [corpus] Related work "Knowledge Extraction on Semi-Structured Content" (FMR=0.569) supports value of structural cues
- Break condition: If pages within a site have heterogeneous layouts (e.g., A/B testing, major redesigns), cross-score optimization may fail.

### Mechanism 2
- Claim: Structure-preserving HTML deduplication reduces token length by ~85% while retaining layout cues, enabling better context utilization and higher extraction success rates.
- Mechanism: Algorithm 1 collapses repeated HTML blocks into "n more ... elements" comments, removes irrelevant tags (script, style, etc.), and keeps only essential attributes (id, class, role, etc.).
- Core assumption: Repeated structural elements beyond z=3 are not needed to infer extraction logic; the remaining structure preserves key layout patterns.
- Evidence anchors:
  - [section 3.2] "repeated HTML blocks are collapsed into a compact representation... substantially reduces context length"
  - [appendix C, Table 5] Dedup reduces average tokens from 114,318 to 16,985 (85.1% reduction)
  - [appendix C, Table 6] GPT-4o non-empty rate improves from 63.8% (raw) to 94.9% (dedup)
  - [corpus] "An Index-based Approach for Efficient Web Content Extraction" (FMR=0.581) addresses similar token management challenges
- Break condition: If critical information is in repeated sections beyond the kept z elements, dedup may discard necessary signals.

### Mechanism 3
- Claim: Staged training—first on gold annotations, then on failure-case synthetic annotations from CommonCrawl—improves robustness while avoiding degradation from noisy rewards.
- Mechanism: (1) Train on annotated data with gold rewards to establish strong priors. (2) Identify CC pages where model produces empty outputs. (3) Use LLM direct extraction as synthetic annotations and continue training on these failure cases.
- Core assumption: Synthetic annotations expose diverse layouts; focusing on failure cases prevents noisy rewards from degrading learned capabilities.
- Evidence anchors:
  - [abstract] "improves by iteratively training on synthetic annotations from in-the-wild CommonCrawl data"
  - [section 3.3.2, Figure 3] Full pipeline for CC data processing and failure-case selection
  - [section 4.4, Table 3] Q-32B Failure-Case CC improves All F1 by 5% vs. annotated-only; All CC only 1.2%
  - [appendix F.1, Table 7] CC-only or mixed training fails; staged training succeeds
  - [corpus] No directly comparable corpus work on staged RL with synthetic rewards for extraction
- Break condition: If synthetic annotations have systematic errors aligned with failure modes, training may reinforce incorrect patterns.

## Foundational Learning

- Concept: Reinforcement Learning with Verifiable Rewards (RLVR)
  - Why needed here: The paper uses RL to train script generation without human demonstrations. The reward is computed from execution outcomes (extracted triples), not script quality judgments.
  - Quick check question: Can you compute a reward from the output of the generated script without human annotation of the script itself? (Yes → RLVR applicable)

- Concept: Bipartite Matching for Fuzzy Triple Alignment
  - Why needed here: To compute rewards, predicted triples must be matched to gold triples. Exact matching is too strict; fuzzy string matching via maximum-weight bipartite matching enables tolerant scoring.
  - Quick check question: How would you score predicted ("GPT-4o", "accuracy", "86.6%") against gold ("GPT-4o", "QA accuracy", "86.6%")?

- Concept: BeautifulSoup (bs4) for HTML Parsing and Script Execution
  - Why needed here: Generated scripts are Python code using bs4 to parse HTML. Understanding DOM navigation (find, select, attrs) is essential for debugging failed scripts.
  - Quick check question: Given a script that fails, would you first inspect the HTML structure or the Python logic?

## Architecture Onboarding

- Component map: Input HTML (deduplicated) -> Qwen2.5-Instruct model -> Python extraction script -> bs4 execution -> (subject, predicate, object) triples -> Fuzzy F1 reward -> GRPO update

- Critical path:
  1. HTML preprocessing (dedup, grouping by URL prefix)
  2. Script generation (LLM inference)
  3. Script execution (run on all group pages)
  4. Reward computation (fuzzy bipartite match, LLM-as-judge for eval only)
  5. GRPO update (policy optimization, KL penalty)
  6. Iterative CC training (identify empty-output failures, synthesize annotations, continue training)

- Design tradeoffs:
  - Dedup aggressiveness (z): Higher z retains more content but increases tokens
  - CC filtering thresholds (n=30 pages minimum, m=90% semi-structured): Stricter filters reduce noise but limit coverage
  - Failure-case vs. all CC: Failure-case yields larger gains but may miss wrong-output errors
  - Reward proxy: Fuzzy F1 is fast for training; LLM-as-judge is accurate but costly

- Failure signatures:
  - Empty script output: Check HTML structure, script logic, or whether page lacks semi-structured content
  - High example / low holdout score: Overfitting, insufficient cross-score weight, or heterogeneous layouts
  - Training instability (loss spikes): Check KL coefficient (0.001), gradient clipping (0.5 for 32B), context limits

- First 3 experiments:
  1. Replicate reward ablation (self-only vs. cross-score reward) on a subset to verify +7% holdout gap
  2. Profile token reduction and non-empty rate from dedup on 50 random HTML pages
  3. Run CC pipeline to identify failure cases; train 1 epoch and measure F1 delta on held-out annotated data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating SCRIBES-extracted triples into pretraining corpora improve LLM performance on structured reasoning benchmarks compared to standard text-only corpora?
- Basis in paper: [explicit] Section 5.2 suggests SCRIBES can address the "near-complete absence of semi-structured data" in current pretraining datasets like Dolma and FineWeb.
- Why unresolved: The authors propose this application but provide no empirical evidence regarding the impact of such data on the pretraining phase.
- What evidence would resolve it: A comparison of downstream task performance between models pretrained with SCRIBES-augmented data versus standard filtered web data.

### Open Question 2
- Question: To what extent does the triple aggregation enabled by SCRIBES improve performance on multi-page, complex QA tasks compared to standard RAG methods?
- Basis in paper: [explicit] Section 5.2 highlights "Multi-page, Complex QAs" as a future direction, noting SCRIBES can support queries like "What is the latest report filed?" which standard RAG struggles with.
- Why unresolved: The paper evaluates single-page extraction and simple QA, but does not test the system on aggregation queries spanning multiple documents.
- What evidence would resolve it: Benchmarks on multi-hop QA datasets requiring cross-page aggregation using SCRIBES-generated knowledge graphs.

### Open Question 3
- Question: How does the quality of the synthetic annotations (LLM direct extraction) impact the convergence and ceiling of the reinforcement learning training?
- Basis in paper: [inferred] Section 3.3.2 notes synthetic annotations are "far from perfect" (~40% F1), and Section 4.4 shows training on failure cases is necessary to prevent noise from degrading performance.
- Why unresolved: It is unclear if the noise from synthetic rewards fundamentally limits the model's potential or if the current filtering heuristics are sufficient for diverse web data.
- What evidence would resolve it: An ablation study correlating the ground-truth quality of the synthetic annotator with the final script generation accuracy.

### Open Question 4
- Question: Can the SCRIBES framework be adapted to maintain high performance on "Free-Form" semi-structured webpages where layout similarity is less consistent?
- Basis in paper: [inferred] Section 4.5 Error Analysis indicates the model performs worst on Free-Form (F-F) pages and performance declines as structural complexity increases.
- Why unresolved: The reliance on layout similarity as a reward signal may be less effective for pages without consistent tabular or infobox structures.
- What evidence would resolve it: Architectural ablations testing alternative reward signals specifically on the Free-Form subset of the dataset.

## Limitations

- The cross-score reward mechanism's effectiveness depends on the assumption that webpages from the same site/domain share sufficiently similar layouts, which may not hold for sites with heterogeneous layouts (A/B testing, redesigns)
- Structure-preserving HTML deduplication, while reducing tokens by ~85%, may discard critical information if repeated elements beyond the kept z threshold contain necessary extraction cues
- The staged training approach using synthetic annotations from CommonCrawl shows promising gains but the pipeline for identifying failure cases and generating synthetic labels could introduce bias if failure modes systematically align with synthetic annotation errors

## Confidence

- **High Confidence**: The basic RLVR framework with verifiable rewards (script execution outcomes) and the staged training approach showing clear F1 improvements on annotated data
- **Medium Confidence**: The deduplication mechanism's effectiveness in reducing tokens while preserving layout cues, supported by token reduction metrics and improved non-empty rates
- **Medium Confidence**: The cross-score reward mechanism's ability to prevent overfitting, evidenced by the self-only vs. cross-score ablation showing +7% holdout gap, though real-world layout heterogeneity remains untested

## Next Checks

1. **Layout Heterogeneity Test**: Run SCRIBES on a site with known layout variations (A/B testing, seasonal redesigns) and measure holdout performance degradation compared to homogeneous sites to quantify the cross-score mechanism's robustness limits

2. **Dedup Information Preservation**: For pages where dedup reduces tokens >80%, manually verify whether critical extraction elements (e.g., product specs, prices) remain in the compressed HTML versus being discarded in repeated blocks

3. **Synthetic Annotation Error Propagation**: Generate synthetic annotations for a subset of failure cases, introduce controlled errors, and measure whether staged training on noisy synthetic data degrades model performance versus training on clean gold annotations