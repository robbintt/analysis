---
ver: rpa2
title: 'LLMs Can''t Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions'
arxiv_id: '2508.18321'
source_url: https://arxiv.org/abs/2508.18321
tags:
- social
- peer
- reasoning
- qwen2
- rapport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KAIROS, a benchmark to evaluate how large
  language models (LLMs) handle peer pressure in multi-agent social interactions.
  It simulates quiz-style collaboration with peer agents of varying reliability and
  rapport levels, testing whether models can maintain correct judgments despite misleading
  inputs.
---

# LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions

## Quick Facts
- arXiv ID: 2508.18321
- Source URL: https://arxiv.org/abs/2508.18321
- Reference count: 40
- Key outcome: Larger language models are more resilient to peer pressure, and GRPO training with multi-agent context is the most effective method for improving social robustness

## Executive Summary
This paper introduces KAIROS, a benchmark that evaluates how large language models handle peer pressure in multi-agent social interactions. Through quiz-style collaboration scenarios with varying peer reliability and rapport levels, the study reveals that larger models demonstrate significantly greater resilience to misleading social inputs. The research identifies that traditional training methods like prompting and supervised fine-tuning often worsen robustness, while reinforcement learning via Group Relative Policy Optimisation (GRPO) that incorporates multi-agent context and outcome-based rewards produces the most consistent improvements.

## Method Summary
The study constructs a dynamic multi-agent benchmark (KAIROS) where target LLMs answer multiple-choice questions while influenced by simulated peer agents with varying historical rapport and current behaviors. The evaluation uses dynamic question construction based on the model's extracted beliefs and confidence levels, creating scenarios that specifically test social influence. Models are trained using GRPO with various configurations, including normal system prompts and outcome-based rewards, with the most effective approach incorporating full multi-agent social context into the training process.

## Key Results
- Model scale is the primary factor determining resilience to social influence, with larger models showing significantly better performance
- GRPO training with multi-agent context (MAS) and outcome-based rewards achieves the highest absolute performance in social settings
- Prompting strategies (Empowered/Reflective) increase the robustness gap for smaller models but close it for larger models
- Smaller models remain vulnerable even after GRPO training, while larger models show greater resilience

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Reinforcement Learning (GRPO-MAS)
Reinforcement learning via Group Relative Policy Optimisation improves social robustness only when training inputs explicitly include multi-agent social context, whereas isolated task training does not transfer to social settings. By incorporating the full history of peer interactions and current peer responses into training context, models learn to treat social interference as noise to be filtered rather than ground-truth to follow.

### Mechanism 2: Rapport as a Trust Heuristic
Models use prior interaction history as a proxy for peer reliability, modulating their susceptibility to current misinformation. High rapport creates trust that reinforces correct answers in support scenarios but causes "blind trust" in oppose scenarios, where models abandon correct beliefs for incorrect peer consensus.

### Mechanism 3: Scale-Dependent Intervention Efficacy
Prompting strategies increase the robustness gap for smaller models (≤32B) but close it for larger models (>32B). Larger models can effectively role-play autonomous agents and maintain reasoning under pressure, while smaller models lack the capacity to maintain the persona under cognitive load.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: This is the only training method found to effectively teach social resilience by shaping the process of reasoning under pressure
  - Quick check: Can you distinguish between a policy update that minimizes cross-entropy (SFT) and one that maximizes a reward function (RL)?

- **Concept: Conformity vs. Utility Trade-off**
  - Why needed: The paper defines robustness via two competing metrics: Resistance (staying correct when peers are wrong) and Utility (becoming correct when peers are right)
  - Quick check: If an agent has 100% Resistance but 0% Utility, is it robust?

- **Concept: Dynamic Benchmarking (Belief Extraction)**
  - Why needed: KAIROS uses dynamic questions that probe the model to find its "original belief" and confidence, then constructs peer pressure scenarios specifically to target that belief
  - Quick check: Why is testing a model on questions it originally got wrong insufficient for measuring social robustness?

## Architecture Onboarding

- **Component map:** Belief Extractor -> Peer Generator -> Interaction Constructor -> Evaluator
- **Critical path:** The evaluation loop depends entirely on the Belief Extractor; if initial confidence/answer is not correctly identified, the "Oppose" scenarios will not apply social pressure to the model's actual beliefs
- **Design tradeoffs:** MCQ vs. Open-Ended (MCQ provides deterministic evaluation but may underestimate real-world effects); Prompting vs. GRPO (prompting is cheap but ineffective for small models; GRPO is computationally expensive but necessary for robustness)
- **Failure signatures:** "Reflected" Collapse (small models using "Reflective" prompting show catastrophic drops in accuracy); SFT Overfitting (improves Utility but significantly degrades Resistance)
- **First 3 experiments:** 1) Rapport Baseline: Run Base model on KAIROS with 0% vs 100% rapport; 2) Prompt Sensitivity: Test "Empowered" vs "Reflective" prompts on small vs large models; 3) GRPO-MAS Ablation: Train small model using GRPO with and without MAS context

## Open Questions the Paper Calls Out

- Can training strategies be developed that simultaneously improve both task accuracy and robustness to social influence, overcoming the current trade-off?
- Why do smaller models (≤32B parameters) remain vulnerable to social influence even after GRPO training, while larger models show greater resilience?
- How can LLMs be trained to achieve higher utility (correcting errors based on peer input) without sacrificing resistance (maintaining correct judgments under opposition)?

## Limitations
- Evaluation relies on MCQAs rather than open-ended generation, potentially underestimating real-world social influence effects
- Dynamic belief extraction process introduces variability that could affect reproducibility
- Study focuses on English-language quiz-style tasks, leaving generalizability to other domains or languages open

## Confidence

- **High Confidence**: Scale-dependent intervention efficacy - correlation between model size and robustness is consistently observed with clear statistical backing
- **Medium Confidence**: GRPO-MAS mechanism - strong evidence but exact implementation details are partially unspecified
- **Medium Confidence**: Rapport as trust heuristic - solid empirical evidence but underlying psychological modeling remains somewhat speculative

## Next Checks

1. Test KAIROS framework on non-quiz domains (e.g., creative writing or code generation) to assess if social robustness transfers beyond fact-based reasoning tasks
2. Implement the same social influence paradigm using open-ended generation tasks to compare conformity effects against the MCQ framework
3. Track model robustness development over extended GRPO training cycles to determine if initial gains persist or if models develop new vulnerabilities over time