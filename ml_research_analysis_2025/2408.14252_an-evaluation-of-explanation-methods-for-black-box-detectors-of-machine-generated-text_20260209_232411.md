---
ver: rpa2
title: An Evaluation of Explanation Methods for Black-Box Detectors of Machine-Generated
  Text
arxiv_id: '2408.14252'
source_url: https://arxiv.org/abs/2408.14252
tags:
- explanations
- explanation
- detector
- anchor
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates explanation methods for black-box detectors\
  \ of machine-generated text (MGT), addressing the need for interpretable decisions\
  \ in contexts where incorrect classifications have significant consequences. The\
  \ study compares three explanation methods\u2014SHAP, LIME, and Anchor\u2014across\
  \ five automated experiments measuring faithfulness, stability, and a user study\
  \ assessing usefulness."
---

# An Evaluation of Explanation Methods for Black-Box Detectors of Machine-Generated Text

## Quick Facts
- arXiv ID: 2408.14252
- Source URL: https://arxiv.org/abs/2408.14252
- Reference count: 39
- Primary result: SHAP outperforms LIME and Anchor in faithfulness, stability, and helping users predict detector behavior for MGT detection

## Executive Summary
This paper evaluates explanation methods for black-box detectors of machine-generated text (MGT), addressing the need for interpretable decisions in contexts where incorrect classifications have significant consequences. The study compares three explanation methods—SHAP, LIME, and Anchor—across five automated experiments measuring faithfulness, stability, and a user study assessing usefulness. Experiments include pointing games, token removal tests, consistency checks, continuity analysis, and contrastivity evaluations, along with a forward simulation experiment with 36 participants. Results show SHAP performs best in faithfulness, stability, and user performance, while LIME is perceived as most useful by users but decreases their ability to predict detector behavior. Anchor performs moderately but is less interpretable. The study highlights the importance of evaluating explanation methods for complex tasks and recommends SHAP for detecting MGT, while cautioning against relying solely on user-perceived usefulness.

## Method Summary
The study evaluates SHAP (Partition Explainer), LIME, and Anchor on three MGT detectors: Guo (RoBERTa fine-tuned on H3 dataset, 99% accuracy), Solaiman (RoBERTa fine-tuned on Webtext/GPT-2, 92% accuracy), and DetectGPT (zero-shot, pythia-70m, 74% accuracy). Using a stratified 30% sample of H3 dataset (N=305), the researchers generated explanations via mask-token perturbations for SHAP/LIME and DistillRoBERTa perturbations for Anchor. Five automated experiments measured faithfulness (pointing game accuracy, token removal Δacc@k=10), stability (consistency/continuity with Krippendorff's α, contrastivity c_inter/c_intra), and a user study with 36 participants assessed usefulness through forward simulation and perceived usefulness ratings. The study controlled for explanation length differences by using default settings: LIME shows 10 most influential words while SHAP provides scores for every word.

## Key Results
- SHAP achieves highest faithfulness with 69.2% pointing game accuracy versus LIME's 54.6% and Anchor's 58.2%
- LIME receives highest perceived usefulness ratings (3.60/5) but decreases user prediction accuracy by 13.12%
- SHAP shows perfect consistency (α = 1.0) for deterministic detectors but drops to α = 0.084 for non-deterministic DetectGPT
- Token removal experiments confirm SHAP's explanations are more comprehensive, causing greater accuracy drops when top-k tokens are removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP produces more faithful explanations than LIME because it uses a deterministic partition algorithm that computes Owen values across all feature combinations.
- Mechanism: The Partition Explainer builds a partition tree deterministically for the two RoBERTa-based detectors, achieving perfect consistency (α = 1.0) across re-runs. This eliminates sampling variance that plagues LIME's stochastic perturbation-based approach.
- Core assumption: Deterministic explanation generation translates to more reliable feature importance attribution for end users.
- Evidence anchors:
  - [abstract] "SHAP performs best in terms of faithfulness, stability, and in helping users to predict the detector's behavior"
  - [section] "SHAP (Partition Explainer) is deterministic for the detectors of Guo et al. (2019) and Solaiman et al. (2019)"
  - [corpus] Neighbor papers focus on detector robustness but do not directly evaluate explanation method determinism—this mechanism lacks external corroboration.
- Break condition: Non-deterministic detectors (e.g., DetectGPT with pythia-70m) reduce SHAP's consistency advantage (α drops to 0.084).

### Mechanism 2
- Claim: LIME's perceived usefulness stems from constrained explanation length (10 tokens), which creates false confidence while omitting critical features.
- Mechanism: Users rate LIME highest on perceived usefulness (3.60/5) but perform 13.12% worse at predicting detector behavior after viewing explanations. The restricted token window produces cleaner, more interpretable visualizations that mask incomplete causal attribution.
- Core assumption: Users conflate explanation clarity with explanation completeness.
- Evidence anchors:
  - [abstract] "LIME, perceived as most useful by users, scores the worst in terms of user performance at predicting detector behavior"
  - [section] "LIME explanations show only the 10 most influential words by default, whereas SHAP provides scores for every word in the input"
  - [corpus] Weak external evidence—corpus papers evaluate detector accuracy, not the disconnect between perceived and actual explanation utility.
- Break condition: Providing users with both explanation formats side-by-side may reveal the incompleteness of LIME's limited scope.

### Mechanism 3
- Claim: Pointing game accuracy captures faithfulness by testing whether explanations correctly identify class-consistent segments in hybrid documents.
- Mechanism: Hybrid documents concatenate human and machine sentences. Faithful methods should attribute highest importance to tokens from documents whose ground truth matches the prediction. SHAP achieves 69.2% accuracy vs. LIME's 54.6% (near random baseline of 56.5%).
- Core assumption: Detectors make predictions based on class-consistent segments rather than spurious correlations.
- Evidence anchors:
  - [abstract] "faithfulness and stability are evaluated with five automated experiments"
  - [section] "A faithful explanation method should hence find these segments to be more important for the decision than those originating from opposite-class documents"
  - [corpus] No corpus papers replicate pointing game methodology for MGT—mechanism validation is internal only.
- Break condition: Detectors that rely on artifacts (e.g., formatting quirks) rather than semantic features would yield misleading pointing game scores.

## Foundational Learning

- Concept: **Local vs. global explanation methods**
  - Why needed here: All three methods (SHAP, LIME, Anchor) produce local explanations for individual predictions, not global model behavior. This limits transferability of insights across documents.
  - Quick check question: Can you articulate why seeing explanations for 18 documents may not help users predict behavior on document 19?

- Concept: **Perturbation-based explanation**
  - Why needed here: LIME and SHAP both rely on perturbing inputs and observing prediction changes. Assumption: classifiers behave predictably under partial input. For MGT detection, accidental word omission may itself be a discriminative feature.
  - Quick check question: What happens to your explanation validity if the detector treats missing words as human-like?

- Concept: **Faithfulness-usefulness gap**
  - Why needed here: Faithfulness measures how accurately explanations reflect detector reasoning. Usefulness measures how well explanations help users predict detector behavior. These are distinct—LIME maximizes perceived usefulness while minimizing actual predictive improvement.
  - Quick check question: If users report high satisfaction but perform worse at prediction tasks, which metric should guide deployment?

## Architecture Onboarding

- Component map: Detectors (Guo -> Solaiman -> DetectGPT) -> Explainers (SHAP -> LIME -> Anchor) -> Evaluation (Pointing Game -> Token Removal -> Consistency -> Continuity -> Contrastivity -> User Study)

- Critical path:
  1. Select detector based on accuracy requirements and inference constraints
  2. Generate explanations using SHAP for deterministic results
  3. Validate faithfulness with pointing game on hybrid documents before deployment
  4. Run user study only after automated metrics pass threshold

- Design tradeoffs:
  - **SHAP completeness vs. interpretability**: Shows all tokens but overwhelms users; LIME shows 10 tokens but omits features
  - **Anchor precision vs. coverage**: High-precision rules (τ = 0.75) may not apply broadly; lowering τ increases coverage but reduces reliability
  - **Computational cost**: DetectGPT with SHAP requires ~415s/explanation vs. ~30s for RoBERTa-based detectors

- Failure signatures:
  - LIME explanations that users rate highly but that degrade prediction accuracy
  - SHAP consistency scores near 0 when applied to non-deterministic detectors
  - Anchor explanations that span only 1-2 tokens with low Jaccard similarity to target documents
  - Pointing game accuracy near 50% (random baseline) indicates unfaithful explanations

- First 3 experiments:
  1. **Pointing game sanity check**: Generate hybrid documents from your dataset, compute Accpg for each detector-explainer pair. SHAP should exceed 65%; LIME below 60% signals faithfulness issues.
  2. **Consistency stress test**: Run 5 explanation generations per document on deterministic detector. SHAP should yield α = 1.0; LIME/Anchor below 0.2 indicates unstable attributions.
  3. **Token removal curve**: Remove top-k tokens (k = 1, 5, 10) and measure accuracy drop. Steeper initial decline for LIME vs. higher overall drop for SHAP confirms paper findings; flat curves indicate broken explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the visual length or complexity of an explanation (e.g., showing 10 words vs. the full document) drive the discrepancy between perceived usefulness and actual user performance?
- Basis in paper: [explicit] The authors note that LIME explanations were limited to 10 words while SHAP covered every word, stating, "Given the between-subject setup of our user study, an analysis of this factor will need to be explored in future work."
- Why unresolved: The study confounds the explanation algorithm with the default visualization settings, making it unclear if LIME's high perceived usefulness stems from its brevity or its method.
- What evidence would resolve it: A controlled user study comparing SHAP and LIME with fixed, identical visualization lengths and granularity.

### Open Question 2
- Question: For which specific task characteristics or feature complexity levels is local feature importance an adequate explanation format?
- Basis in paper: [explicit] The paper concludes by questioning "when feature importance provides sufficient insights and for which tasks it is most effective," advocating for evaluation across additional tasks.
- Why unresolved: The current study only evaluates MGT detection (a task with complex/unintuitive features) and finds user performance did not improve, contrasting with prior work on simpler tasks like sentiment analysis.
- What evidence would resolve it: A comparative benchmark evaluating SHAP/LIME across a spectrum of tasks with varying feature intuitiveness (e.g., topic classification vs. stylistic detection).

### Open Question 3
- Question: Can incorporating a "theory of mind" (user modeling) into explanation methods resolve the conflict between faithfulness and user comprehension?
- Basis in paper: [explicit] The authors argue that current methods "do not build a model of the human users" and suggest that future setups should combine algorithms with "an elaborate model of the system context."
- Why unresolved: Faithful methods like SHAP were not perceived as useful, suggesting that raw faithfulness does not equate to human understanding without considering the user's prior knowledge.
- What evidence would resolve it: Developing explanation interfaces that adapt output based on user expertise and measuring if this bridges the gap between faithfulness scores and perceived usefulness.

## Limitations

- The study's findings may not generalize to non-English text detection tasks, as the H3 dataset focuses on English documents and explanation methods' performance on morphologically rich languages remains untested.
- Significant performance degradation occurs when applying SHAP to non-deterministic detectors like DetectGPT (consistency α = 0.084 vs. 1.0 for deterministic models), suggesting current explanation methods may not generalize well across different detector architectures.
- The computational resource limitations create practical barriers to adoption, with SHAP requiring ~1 minute and Anchor requiring ~5 minutes per explanation versus LIME's ~30 seconds, which may render them impractical for resource-constrained deployment scenarios.

## Confidence

- **High confidence**: SHAP's superior faithfulness and stability performance on deterministic detectors. The pointing game accuracy (69.2% vs. 54.6% for LIME) and perfect consistency scores provide robust evidence.
- **Medium confidence**: The usefulness findings for LIME. While user ratings clearly favor LIME (3.60/5 perceived usefulness), the 13.12% degradation in prediction performance reveals a fundamental disconnect.
- **Low confidence**: Generalizability to non-English text detection tasks. The H3 dataset focuses on English documents, and the explanation methods' performance on morphologically rich languages or languages with different tokenization schemes remains untested.

## Next Checks

1. **Cross-linguistic validation**: Evaluate SHAP, LIME, and Anchor on machine-generated text detection tasks involving non-English languages (e.g., Chinese, Arabic, or code-switched English-Spanish documents) to test whether the faithfulness-usefulness gap persists across different linguistic structures.

2. **Alternative explanation methods comparison**: Implement and evaluate counterfactual explanation methods (e.g., CEM, DiCE) alongside the three methods tested to determine whether perturbation-based explanations are inherently limited for non-deterministic detectors.

3. **Extended user study with mixed explanations**: Conduct a follow-up user study where participants view both LIME and SHAP explanations side-by-side for the same documents to test whether exposing users to more complete information changes their perceived usefulness ratings and prediction performance.