---
ver: rpa2
title: Differentiable neural network representation of multi-well, locally-convex
  potentials
arxiv_id: '2506.17242'
source_url: https://arxiv.org/abs/2506.17242
tags:
- neural
- potentials
- network
- multi-well
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel neural network architecture, the log-sum-exponential
  input convex neural network (LSE-ICNN), for representing multi-well, locally-convex
  potentials in a differentiable manner. Unlike traditional minimum-of-mixture approaches,
  the LSE-ICNN combines the log-sum-exponential (LSE) function with input convex neural
  networks (ICNNs) to create a smooth surrogate that retains convexity within basins
  and allows for gradient-based learning and inference.
---

# Differentiable neural network representation of multi-well, locally-convex potentials

## Quick Facts
- arXiv ID: 2506.17242
- Source URL: https://arxiv.org/abs/2506.17242
- Authors: Reese E. Jones; Adrian Buganza Tepole; Jan N. Fuhg
- Reference count: 40
- Primary result: Introduces LSE-ICNN architecture for differentiable multi-well potential modeling with automatic mode discovery

## Executive Summary
This paper presents the Log-Sum-Exponential Input Convex Neural Network (LSE-ICNN), a novel architecture for representing multi-well, locally-convex potentials in a differentiable manner. The approach replaces traditional non-smooth minimum-of-mixture operations with a smooth log-sum-exponential function, enabling gradient-based learning and inference. The architecture combines ICNNs (which guarantee local convexity within basins) with sparse regression techniques to automatically discover the correct number of modes. Demonstrated across diverse domains including mechanochemical phase transformations, microstructural elastic instabilities, biological gene circuits, and variational inference, the LSE-ICNN successfully captures complex multimodal landscapes while preserving differentiability.

## Method Summary
The LSE-ICNN architecture uses a set of parallel ICNNs (with non-negative weights and softplus activations) as basis functions, which are then aggregated using a log-sum-exponential (LSE) function. The LSE acts as a smooth, differentiable surrogate for the non-differentiable minimum operation, with a learnable scale parameter ρ controlling the sharpness of transitions between wells. Mode discovery is achieved through L1 regularization on gating parameters α_i, which drives unnecessary modes to zero. The network is trained using MSE loss plus L1 penalty, with separate learning rates for neural network weights and gating parameters. The method is demonstrated on 1D test functions, mechanochemical phase transformations, and elastic stability problems.

## Key Results
- LSE-ICNN successfully learns multi-well potentials with smooth transitions, outperforming traditional non-differentiable minimum-of-mixture representations
- Automatic mode discovery through sparse regression correctly identifies the number of wells in all test cases
- The method generalizes well to unseen data across diverse domains including mechanics, biology, and probability distributions
- ICNN constraints ensure local convexity within basins while allowing global non-convexity

## Why This Works (Mechanism)

### Mechanism 1: Smooth Approximation of Non-Differentiable Minima
The LSE function provides a smooth, differentiable surrogate for the non-smooth minimum-of-mixture operation. Traditional min(f_i) operations create non-differentiable kinks where wells meet. The LSE function, defined as -1/ρ * log(Σ exp(-ρ * f_i)), acts as a soft minimum. By tuning ρ, the approximation smoothly transitions between convex basins while remaining infinitely differentiable, enabling gradient-based optimization.

### Mechanism 2: Enforced Local Convexity via Architectural Constraints
Convexity within individual energy basins is guaranteed by restricting basis functions to be ICNNs. ICNNs enforce convexity through non-negative weights and convex, non-decreasing activation functions. By mixing these provably convex components with the LSE function (which preserves convexity), the model ensures physically valid basins.

### Mechanism 3: Automatic Model Complexity Selection via Sparse Regression
The correct number of distinct energy wells is discovered automatically by applying L1 regularization to mode-specific gating parameters. Starting with an over-estimate of modes, L1 penalty drives weights of unnecessary modes toward zero, pruning them and leaving only active wells required to explain the data.

## Foundational Learning

- **Concept: Input Convex Neural Networks (ICNNs)**
  - Why needed: Standard MLPs don't guarantee convex functions. ICNNs (skip connections + non-negative weights) ensure LSE-ICNN doesn't collapse into non-physical, locally concave shapes.
  - Quick check: If you removed non-negativity constraint on Wk in ICNN, would LSE-ICNN still guarantee convex basins?

- **Concept: Log-Sum-Exponential (LSE) / Softmax Properties**
  - Why needed: LSE is the mixing agent. The parameter ρ controls sharpness of transition between wells. Too low blurs wells together; too high loses numerical stability.
  - Quick check: As ρ → ∞, does LSE approach the average of inputs or the minimum?

- **Concept: Variational Inference & Free Energy**
  - Why needed: Connects potential energy landscapes to probability distributions. Understanding how minimizing free energy is equivalent to fitting a probability distribution is crucial for biological circuit applications.
  - Quick check: In Helmholtz free energy formula, what does partition function Z represent?

## Architecture Onboarding

- **Component map:** Input x -> N ICNN Heads (Ni(x)) -> Gating Layer (ς(αi), ρ) -> LSE Aggregator -> Output Potential
- **Critical path:** 1) Initialize N_modes > expected wells, 2) Forward pass through all ICNN heads, 3) Aggregate via LSE layer, 4) Backpropagate loss using ADAM with separate learning rates, 5) Monitor effective αi to confirm mode pruning
- **Design tradeoffs:** Mode Count vs. Convergence (too many slows training, too few insufficient basis), Sharpness vs. Stability (high ρ for sharp transitions but causes numerical overflow)
- **Failure signatures:** Ghost Wells (extra shallow minima), Oversmoothing (gradual transitions likely ρ stuck low), Convexity Breach (negative ICNN weights creating non-physical hills)
- **First 3 experiments:** 1) 1D Double-Well Fit (asymmetric quartic), 2) Scale Sensitivity (sharp vs. smooth transitions), 3) Derivative Fitting (fit stress/gradient in mechanochemical dataset)

## Open Questions the Paper Calls Out
- How can LSE-ICNN framework be extended to guarantee positive energy dissipation in dynamic, non-conservative systems?
- Can the method accurately capture abrupt, discontinuous phase transitions without relying on coarse time discretization?
- How robust is the automatic mode discovery mechanism against random initialization and hyperparameter choices?

## Limitations
- ICNN constraints may be too restrictive for potentials with highly non-convex basins or complex saddle geometries
- LSE approximation fidelity critically depends on learnable scale parameter ρ, which may struggle to balance sharpness versus numerical stability in extreme cases
- Performance on truly high-dimensional (≥10D) multimodal landscapes remains unverified

## Confidence
- High confidence: Differentiability mechanism (LSE as smooth surrogate) - clear mathematical formulation and convergence properties
- Medium confidence: Local convexity guarantees - relies on ICNN constraints without extensive edge case validation
- Medium confidence: Mode discovery via sparse regression - parsimony assumption may not hold for complex systems
- Low confidence: Scalability to high-dimensional problems - lacks empirical verification

## Next Checks
1. Test LSE-ICNN on potentials with deliberately introduced non-convex regions within basins to verify ICNN constraints prevent spurious local minima
2. Apply LSE-ICNN to a 10+ dimensional multimodal Gaussian mixture to assess parameter efficiency and mode discovery accuracy as dimensionality increases
3. Systematically vary transition sharpness in synthetic data and monitor LSE-ICNN's ability to learn appropriate ρ values without numerical overflow or gradient explosion