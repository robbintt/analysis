---
ver: rpa2
title: 'Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive
  Compression'
arxiv_id: '2510.01581'
source_url: https://arxiv.org/abs/2510.01581
tags:
- reasoning
- compression
- difficulty
- accuracy
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces TRAAC, an online RL method to address under-adaptivity\u2014\
  the failure of reasoning models to dynamically adjust their thinking length based\
  \ on task difficulty. TRAAC uses an attention-based compression module that prunes\
  \ reasoning steps based on self-attention scores, calibrated to task difficulty\
  \ estimated from rollout pass rates."
---

# Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression

## Quick Facts
- arXiv ID: 2510.01581
- Source URL: https://arxiv.org/abs/2510.01581
- Authors: Joykirat Singh; Justin Chih-Yao Chen; Archiki Prasad; Elias Stengel-Eskin; Akshay Nambi; Mohit Bansal
- Reference count: 20
- Primary result: TRAAC achieves 8.4% accuracy gain and 36.8% length reduction on math/non-math tasks

## Executive Summary
TRAAC addresses the under-adaptivity problem in reasoning models by introducing an online reinforcement learning method that dynamically adjusts thinking length based on task difficulty. The method uses an attention-based compression module that prunes reasoning steps during training, guided by difficulty-aware compression rates and multi-component rewards. Evaluated on Qwen3-4B and Deepseek-Qwen-7B across multiple benchmarks, TRAAC demonstrates significant improvements in both accuracy and efficiency compared to baselines.

## Method Summary
TRAAC is an online RL method that mitigates under-over thinking by learning to compress reasoning steps adaptively. During training, the model generates multiple rollouts per query, estimates task difficulty from pass rates, and applies attention-based step pruning at difficulty-calibrated rates. The compressed reasoning trajectories are used to compute rewards (correctness, format, length) that guide the policy update via GRPO. The attention scores are computed from the aggregated self-attention weights from the answer delimiter to reasoning tokens, with low-scoring steps pruned based on difficulty-specific compression rates.

## Key Results
- Achieves 8.4% average accuracy gain and 36.8% length reduction over base models
- Outperforms adaptive RL baselines by 7.9% accuracy and 29.4% efficiency
- Attention-based compression improves over random pruning by 11% accuracy
- Generalizes well to out-of-domain datasets with consistent gains

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Step Importance Scoring
Token importance is inferred from aggregated self-attention weights from the delimiter `</think` to reasoning tokens. During compression, each token's importance is computed as the mean attention weight from `</think` across all layers and heads. Steps receive scores averaged across their tokens and are pruned based on these scores. Evidence shows TRAAC outperforms random pruning by 11% accuracy and confidence-based pruning by 7.25%.

### Mechanism 2: Difficulty-Calibrated Compression Rates
Compression aggressiveness is inversely proportional to estimated task difficulty. Difficulty is estimated from rollout pass rates (proportion correct among N=8 samples) and binned into easy/medium/hard categories. Easy problems receive up to 60% compression while hard problems retain more steps. Figure 3a shows compression rates dropping from ~47% (easy) to ~11% (hard).

### Mechanism 3: Multi-Component Reward with Difficulty-Aware Length Bonus
Total reward combines correctness (+4 if correct), format (0-1 for delimiter presence), and length (0-2 with sigmoid smoothing). Length rewards are difficulty-conditioned to prevent premature truncation on hard problems. Table 3 shows removing difficulty calibration drops accuracy by 3.4% and increases length by 23.8%.

## Foundational Learning

- **Self-Attention in Transformers**: Essential for understanding how TRAAC interprets attention weights as importance signals. Quick check: Given a 12-layer model with 16 heads per layer, what is the shape of the attention weight tensor for a 100-token sequence, and how would you aggregate it to get per-token importance?

- **Group Relative Policy Optimization (GRPO)**: Critical for understanding TRAAC's advantage estimation without a critic. Quick check: In GRPO, why is the advantage computed relative to the group rather than a learned value function? What does this imply for variance in updates?

- **Chain-of-Thought (CoT) Segmentation**: Important for understanding how TRAAC splits reasoning into steps. Quick check: Given "First, I compute x. Wait, maybe I should check y. Therefore, the answer is 42.", how many steps would be extracted and which tokens receive highest attention from `</think`?

## Architecture Onboarding

- **Component map**: Query → Rollout Generator → Difficulty Estimator → Attention Scorer → Compression Module → Reward Calculator → GRPO Updater

- **Critical path**: Query → Rollout → Difficulty Estimation → (Re-)generate with Compression → Reward Computation → GRPO Update. Compression happens during training rollouts, not post-hoc.

- **Design tradeoffs**: 3 difficulty bins vs. continuous; 20-60% compression rate caps; uniformity guardrails that suppress compression if attention is flat.

- **Failure signatures**: Uniform attention across steps indicates delimiter isn't attending selectively; length reward collapse if median length drifts significantly; difficulty misestimation if N=8 samples have high variance.

- **First 3 experiments**:
  1. Ablate attention vs. random pruning: Replace attention scoring with random step removal at matched compression rates.
  2. Vary difficulty bin thresholds: Test alternative pass-rate cutoffs and plot compression rate vs. difficulty.
  3. Test-time only compression: Apply TRAAC's compression module at inference without training (static 40% compression).

## Open Questions the Paper Calls Out

### Open Question 1
Would continuous difficulty estimation outperform the three discrete bins used in TRAAC? The paper uses coarse 3-bin categorization which may lose fine-grained difficulty information.

### Open Question 2
Does TRAAC scale effectively to significantly larger models (70B+ parameters)? The paper only evaluates on 4B and 7B models due to computational constraints.

### Open Question 3
Can the assumption that low-attention tokens are unimportant fail for critical reasoning steps? The paper relies on attention scores correlating with step importance without validating this assumption across all problem types.

## Limitations
- Heavy reliance on attention scores correlating with token importance may not generalize across architectures
- Pass rate from 8 sampled rollouts as difficulty estimator is a heuristic that may be unstable
- Fixed compression rate ranges (20-60%) are tuned for tested model sizes and may not scale effectively
- Difficulty bins defined per batch could lead to inconsistent treatment of similar problems

## Confidence

**High Confidence**: Attention-based step importance scoring (11% accuracy improvement over random pruning), GRPO framework implementation, reward structure consistency

**Medium Confidence**: Difficulty-calibrated compression rates (strong correlation patterns but heuristic validation), multi-component reward design (empirical benefits but hyperparameter sensitivity unexplored)

**Low Confidence**: Claims about outperforming adaptive RL baselines depend on unspecified implementations, generalization to out-of-domain datasets only demonstrated on one held-out set

## Next Checks

1. **Difficulty Estimator Validation**: Replace pass-rate heuristic with human-annotated difficulty labels or alternative estimators and measure performance changes.

2. **Compression Rate Sensitivity**: Systematically vary compression rate caps and plot accuracy vs. efficiency Pareto frontier to assess optimality.

3. **Attention Mechanism Ablation**: Compare attention-based pruning against gradient-based saliency, token frequency-based pruning, and no pruning to isolate whether self-attention is the critical signal.