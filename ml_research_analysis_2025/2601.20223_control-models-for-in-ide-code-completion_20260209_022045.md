---
ver: rpa2
title: Control Models for In-IDE Code Completion
arxiv_id: '2601.20223'
source_url: https://arxiv.org/abs/2601.20223
tags:
- completion
- code
- jetbrains
- control
- filter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces control models\u2014ML classifiers that\
  \ trigger and filter LLM-powered code completion in JetBrains IDEs\u2014to reduce\
  \ unnecessary inference and improve alignment with developers\u2019 workflows. The\
  \ authors evaluate boosting- and transformer-based architectures on real usage data\
  \ (n=98 users) and conduct A/B tests in production."
---

# Control Models for In-IDE Code Completion

## Quick Facts
- arXiv ID: 2601.20223
- Source URL: https://arxiv.org/abs/2601.20223
- Reference count: 36
- Key outcome: Control models (ML classifiers) can trigger and filter LLM-powered code completion in JetBrains IDEs, reducing unnecessary inference and improving alignment with developers' workflows.

## Executive Summary
This paper introduces control models—ML classifiers that trigger and filter LLM-powered code completion in JetBrains IDEs—to reduce unnecessary inference and improve alignment with developers' workflows. The authors evaluate boosting- and transformer-based architectures on real usage data (n=98 users) and conduct A/B tests in production. Offline, transformer models can filter more completions at lower false-negative rates, but boosting models achieve better acceptance and cancellation rates. Online A/B tests show ~20% fewer inference requests, ~47% higher acceptance rate, and ~35% lower cancellation rate, with a small (~3%) drop in the Ratio of Completed Code. Key challenges include event dependence in sequential completions, long-term productivity measurement, and transformer latency/privacy constraints. The work highlights the potential of control models for smarter in-IDE LLM integration and calls for future work on personalization and end-to-end optimization.

## Method Summary
The paper trains two binary classifiers (trigger and filter models) to gate LLM inference for inline code completion. Trigger models decide whether to invoke LLM inference based on tabular telemetry features (e.g., typing speed, caret position), while filter models decide whether to show generated suggestions using both telemetry and completion content. The authors evaluate CatBoost gradient boosting and transformer-based models on anonymized IDE telemetry from 98 test users across four languages (Kotlin, Python, PHP, C#). Offline metrics include False-Negative Rate, Accept Rate, and Cancel Rate, while online A/B tests measure Ratio of Completed Code, inference request count, and user interaction rates.

## Key Results
- Online A/B tests show ~20% fewer inference requests, ~47% higher acceptance rate, and ~35% lower cancellation rate
- Transformer models filter more completions at lower false-negative rates offline, but boosting models achieve better online acceptance/cancellation rates
- ~3% drop in Ratio of Completed Code observed despite improved acceptance and cancellation rates
- Sequential completion events show dependence—filtering 20% of completions only reduced generations by 13.8%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lightweight boosting classifiers can effectively gate LLM inference by learning patterns from IDE telemetry that predict suggestion utility.
- **Mechanism:** A trigger model evaluates tabular features before an LLM request is sent, and a filter model evaluates features after generation but before display. By suppressing low-probability-of-acceptance events, the system reduces cognitive load and compute costs.
- **Core assumption:** User intent and satisfaction with a suggestion are correlated with measurable, pre-existing telemetry signals.
- **Evidence anchors:** Online A/B tests show boosting-based models achieve ~47% higher acceptance rates and ~35% lower cancellation rates.

### Mechanism 2
- **Claim:** Combining code context (via Transformers) with tabular telemetry allows for deeper semantic filtering than telemetry alone.
- **Mechanism:** A transformer encoder processes the code context, while an MLP processes scalar/categorical telemetry. These representations are concatenated to form a joint classification head.
- **Core assumption:** The semantic content of the code provides signal orthogonal to telemetry that improves classification of "accept" vs. "reject" events.
- **Evidence anchors:** "Transformer-based control models are able to filter out more generations while taking less of a Δ Symbols Completed hit."

### Mechanism 3
- **Claim:** Sequential completion events are dependent; suppressing a completion alters user flow, potentially creating new trigger opportunities that offline metrics fail to capture.
- **Mechanism:** A user's interaction with the IDE changes based on what they see (or don't see). Hiding a suggestion might cause the user to type more characters, which generates a new trigger event.
- **Core assumption:** User behavior is dynamic and reacts to the presence or absence of AI suggestions.
- **Evidence anchors:** Trigger model prevented 20% of completions but only reduced generations per-user by 13.8%.

## Foundational Learning

- **Concept: Gradient Boosting on Tabular Data (CatBoost)**
  - **Why needed here:** The paper relies on CatBoost for the production "control model." You must understand how decision trees handle categorical features and scalar features without deep learning overhead.
  - **Quick check question:** How does CatBoost handle high-cardinality categorical features differently than standard One-Hot Encoding?

- **Concept: False Negative Rate (FNR) vs. Filter Rate Trade-off**
  - **Why needed here:** The core optimization problem. You cannot maximize filtering without impacting FNR. The paper explicitly navigates this Pareto frontier.
  - **Quick check question:** If we set the filter threshold aggressively to block 50% of requests, what is the likely impact on the "Symbols Completed" metric?

- **Concept: Ratio of Completed Code (RoCC)**
  - **Why needed here:** This is the paper's primary "productivity" metric. It measures the density of AI-assisted code in the final output.
  - **Quick check question:** Does RoCC measure code quality or code volume?

## Architecture Onboarding

- **Component map:** IDE Telemetry Collector -> Feature Store/Preprocessor -> Control Models (Trigger/Filter) -> LLM Inference Service -> Metrics Pipeline
- **Critical path:** The "Trigger" decision must occur within milliseconds of the user pausing or typing. The "Filter" decision must occur between the LLM returning a result and the UI rendering "grey text."
- **Design tradeoffs:** Boosting vs. Transformer (faster vs. more accurate), unified vs. language-specific models, latency vs. filtering accuracy
- **Failure signatures:** "Drifting" (users accepting bad suggestions due to low filter threshold), Metric Mismatch (offline vs. online results diverging), Privacy/Latency constraints
- **First 3 experiments:**
  1. Feature Importance Audit: Train baseline boosting model, identify top 20 telemetry features, check stability across IDE versions
  2. Offline Threshold Sweep: Simulate trigger+filter cascade, plot "% Generations Saved" vs. "% Symbols Completed Lost" to find acceptable operating point
  3. Shadow Mode Deployment: Deploy models in "shadow mode" (log decisions only) to verify real-time feature distributions match training data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can user personalization for control models be implemented effectively using existing IDE telemetry time-series data?
- **Basis in paper:** The authors state personalization is the "lowest-hanging fruit" for architectural improvements, noting that "selections and rejections are often a matter of personal preference."
- **Why unresolved:** Current models are global; no personalized variant has been tested.
- **What evidence would resolve it:** A/B comparison between global and personalized control models, showing improved accept/cancel rates per-user with personalization.

### Open Question 2
- **Question:** What is the appropriate time-frame and metric for measuring long-term productivity gains from AI-assisted code completion?
- **Basis in paper:** The authors ask: "what is an appropriate time-frame to consider for such metrics?" and cite work showing developers perceive +20% productivity while actually achieving -20%.
- **Why unresolved:** Current metrics capture perceived productivity but not long-term outcomes like task completion time or code quality.
- **What evidence would resolve it:** Longitudinal studies correlating completion metrics with downstream outcomes (code churn, bug rates, task completion time over weeks/months).

### Open Question 3
- **Question:** Can a single reinforcement learning model replace separate trigger and filter models while generalizing across both tasks?
- **Basis in paper:** The authors note CursorTab's RL setup "indicates that gating can be learned end-to-end from interaction signals, potentially removing the need for a separate trigger model."
- **Why unresolved:** Current architecture uses separate boosting models for trigger and filter; no unified RL approach has been tested.
- **What evidence would resolve it:** Comparison of unified RL model versus separate trigger/filter models on inference savings, accept rate, and RoCC.

### Open Question 4
- **Question:** How should the dependence between sequential completion events be modeled to better predict online behavior from offline evaluations?
- **Basis in paper:** The authors observe that filtering 20% of completions only reduced generations by 13.8% because "by affecting completion behaviour, users change their interactions."
- **Why unresolved:** Offline metrics assume i.i.d. events, but completions within a session influence subsequent user behavior.
- **What evidence would resolve it:** A session-level model that captures event dependencies, validated by closer alignment between predicted and observed online metric changes.

## Limitations

- Sequential dependence effects create feedback loops that complicate offline evaluation and real-world impact assessment
- Productivity metric trade-offs: improved acceptance/cancellation rates come with a ~3% drop in Ratio of Completed Code
- Privacy and latency constraints limit transformer model deployment despite better filtering performance
- Long-term user adaptation and behavior changes over extended periods are not well understood

## Confidence

- **High Confidence:** The core observation that ML classifiers can reduce unnecessary LLM inference requests is well-supported by both offline and online experiments
- **Medium Confidence:** The specific performance improvements (47% acceptance rate increase, 35% cancellation rate decrease) are well-documented in A/B tests, but their translation to actual productivity gains remains uncertain
- **Low Confidence:** The optimal architecture choice (boosting vs. transformer) and the long-term impact on developer workflows are not fully resolved, given the trade-offs between accuracy, latency, and privacy

## Next Checks

1. **Extended Longitudinal Study:** Deploy control models to a larger cohort of developers for 3-6 months and track changes in coding patterns, productivity metrics, and user satisfaction over time.

2. **Cross-Language Performance Analysis:** Systematically evaluate control model performance across 10+ programming languages with varying syntax complexity and completion patterns.

3. **Offline-to-Online Prediction Validation:** Create a benchmark dataset that includes sequential completion events and test whether offline metrics can reliably predict online performance metrics across different model architectures and thresholds.