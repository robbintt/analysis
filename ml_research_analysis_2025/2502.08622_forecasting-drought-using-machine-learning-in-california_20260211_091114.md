---
ver: rpa2
title: Forecasting Drought Using Machine Learning in California
arxiv_id: '2502.08622'
source_url: https://arxiv.org/abs/2502.08622
tags:
- drought
- scores
- score
- weeks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated four machine learning models\u2014LSTM, XGBoost,\
  \ CNN, and Random Forest\u2014for predicting U.S. Drought Monitor classifications\
  \ in California, comparing them to a persistence baseline."
---

# Forecasting Drought Using Machine Learning in California

## Quick Facts
- arXiv ID: 2502.08622
- Source URL: https://arxiv.org/abs/2502.08622
- Reference count: 0
- LSTM model achieved macro F1 score of 0.90 for predicting severe drought (D2 or higher)

## Executive Summary
This study evaluated four machine learning models—LSTM, XGBoost, CNN, and Random Forest—for predicting U.S. Drought Monitor classifications in California, comparing them to a persistence baseline. The LSTM model achieved the best performance, with a macro F1 score of 0.90 for predicting severe drought (D2 or higher), a mean absolute error of 0.33 (less than half a drought category), and a mean squared error of 0.32. XGBoost also performed strongly with a macro F1 of 0.88. Shorter forecast horizons (under 8 weeks) and at least 24 weeks of historical data yielded optimal results. Feature importance analysis revealed that past drought scores, month, temperature, precipitation, and humidity were most influential. The models generally underpredicted severe drought, especially in areas with rare drought occurrences. LSTM excelled in regions with consistent, severe drought patterns, demonstrating high utility for drought prediction in California.

## Method Summary
The study used weekly county-level drought data from California (2000-2020) with 18 weather variables plus historical drought scores, latitude/longitude, and month. Data was split temporally (70%/10%/20% for train/val/test) without shuffling. A sliding window approach converted sequences into samples (default 30 weeks input → 12 weeks output). Five models were evaluated: persistence baseline, CNN, Random Forest, XGBoost, and LSTM. Performance was measured using macro F1 score (primary), MSE, and MAE. Normalization was applied per split to prevent data leakage. The best-performing LSTM architecture used 150→75 units with 0.1 dropout.

## Key Results
- LSTM achieved macro F1 of 0.90 for severe drought prediction, outperforming all other models
- Performance degrades rapidly after 8-week forecast horizon (macro F1 drops from 0.93 to 0.87)
- 24-30 week historical windows provided optimal performance; longer windows (52 weeks) showed no improvement
- Models systematically underpredict severe drought, with 100% underprediction at score=4 (extreme drought)
- F1 score strongly correlates with severe drought ratio in training data (r=0.68)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSTM models capture temporal dependencies in drought progression better than non-sequential architectures
- Mechanism: Drought is described as a "slow developing natural disaster," indicating strong temporal autocorrelation. LSTMs maintain memory cell states with learned gate mechanisms (forget, input, output) that can model gradual accumulation or dissipation of drought conditions. Unlike Random Forests or CNNs that treat input windows as snapshots, LSTMs process sequences step-by-step. The top two predictive features were "previous drought scores" and "month," confirming strong temporal structure.
- Core assumption: Temporal dependencies follow patterns learnable by LSTM's gating architecture
- Evidence anchors: [abstract] "LSTM model emerged as top performer... achieved macro F1 score of 0.9"; [section - Page 3] "LSTM models... can capture temporal relationships"; [corpus] Paper #25518 confirms time series integration is critical

### Mechanism 2
- Claim: Longer historical context windows (24+ weeks) stabilize predictions by capturing seasonal cycles
- Mechanism: By providing 24-30 weeks of historical data, models observe nearly half a year of patterns, distinguishing normal seasonal dryness from anomalous drought onset. This window allows calibration against both recent trends and longer-term baselines.
- Core assumption: 24 weeks captures relevant seasonal patterns for California drought—empirically supported but not theoretically proven
- Evidence anchors: [section - Page 5] "Utilizing 30 weeks of historical data, LSTM model successfully forecasted drought scores for 12-week period"; [section - Page 10, Table 4] Shows LSTM Macro F1 of 0.90 with 24-week window at 12-week horizon
- Break condition: If climate change shifts drought drivers to new regimes, historical windows may contain misleading patterns

### Mechanism 3
- Claim: Model performance correlates with training data representation of the target class
- Mechanism: ML models optimize for patterns in training data. Severe drought (D2+) comprises only ~10.5% of the test set. Counties with more frequent severe drought provide more positive examples, enabling better decision boundaries. Low severe drought ratio (<4%) correlates with F1 <0.65.
- Core assumption: Poor performance in low-drought counties stems from insufficient positive training signal, not model inadequacy
- Evidence anchors: [section - Page 9] "Severe drought is rare, making up roughly 10.5% of test dataset"; [section - Page 13-14] "F1 score exhibited strong correlation with severe drought ratio"
- Break condition: Deployment in regions with different drought patterns (e.g., tropical climates) will not transfer; paper explicitly notes customization is needed

## Foundational Learning

- Concept: **Time Series Windowing (Sliding Window)**
  - Why needed here: Core data engineering transformation converting raw sequences into supervised learning samples
  - Quick check question: Given 100 weeks of data, window size 30, predicting 12 weeks ahead—how many samples? (Answer: 100 - 30 - 12 + 1 = 59)

- Concept: **Macro F1 Score for Imbalanced Classification**
  - Why needed here: Primary evaluation metric; treats all classes equally, critical when severe drought is rare
  - Quick check question: A model achieves 99% accuracy by always predicting "no drought." Why is its Macro F1 likely ~0.5? (Answer: F1 for "drought" class = 0, pulling average down)

- Concept: **Systematic Underprediction Bias**
  - Why needed here: Models consistently underpredict severe drought; understanding this bias is key to interpretation and improvement
  - Quick check question: XGBoost incorrectly predicts score 4 (Extreme Drought) by predicting lower 100% of the time. Is this variance or bias? (Answer: Bias—systematic tendency to underestimate)

## Architecture Onboarding

- Component map: Data Ingestion → Feature Engineering → Normalization → Model Zoo → Evaluation
- Critical path: **Data Windowing → LSTM Training → Macro F1 Evaluation**. Errors in windowing logic or normalization silently destroy temporal integrity.
- Design tradeoffs:
  1. Window Size vs. Data Scarcity: Larger windows (52 weeks) don't improve performance but reduce training samples. Optimum: 24-30 weeks.
  2. Forecast Horizon vs. Accuracy: Performance degrades rapidly after 8 weeks (Macro F1 0.93→0.87). Choose horizon based on operational need.
  3. Interpretability vs. Performance: XGBoost provides feature importance; LSTM is opaque but performs best.
- Failure signatures:
  1. Rapid-onset drought: LSTM fails where scores jump 2-3 weeks—model predicts continuation of past patterns
  2. Low-drought regions: F1 <0.65 in counties with severe drought ratio <4% (Nevada border)
  3. Temporal leakage: Normalizing entire dataset before splitting contaminates model with future information
- First 3 experiments:
  1. Replicate baseline persistence and LSTM (150/75 units, 0.1 dropout, 30-week window). Target: MAE ~0.33, Macro F1 ~0.90 to validate pipeline.
  2. Ablate top features (previous drought score, month) from XGBoost one-by-one. Quantify Macro F1 degradation to validate feature importance causally.
  3. Isolate 5 best/worst counties by Macro F1. Confirm correlation (r=0.68) between F1 and severe drought ratio.

## Open Questions the Paper Calls Out

- **GIS Data Integration**: Does integrating GIS data (soil moisture, land cover, hydrological features) significantly enhance drought prediction accuracy compared to using only meteorological variables?
  - Basis: Conclusion states future work includes integrating GIS data to enrich predictive capabilities beyond local weather variables
  - Why unresolved: Current study restricted feature inputs to local meteorological variables and historical drought scores
  - Evidence: Comparative study measuring Macro F1 and MAE of models trained with and without proposed GIS features

- **Regional Modeling Strategy**: Can a regional modeling strategy that aggregates data from neighboring areas successfully compensate for data scarcity in states with few counties?
  - Basis: Authors suggest adopting regional modeling strategy to mitigate scarcity of county-level data when applying approach to states like Delaware
  - Why unresolved: Study was limited to California, which has sufficient data density (58 counties), leaving strategy for low-data regions untested
  - Evidence: Validation of model performance on states with few counties, comparing results using isolated data versus regionally aggregated datasets

- **Severe Drought Recall Optimization**: How can models be optimized to improve recall for severe drought in regions where drought conditions increase rapidly or occur rarely?
  - Basis: Models "generally underpredict extreme drought scores" and exhibit lower recall (0.75) than precision (0.89), particularly where drought is rare
  - Why unresolved: Class imbalance (10.5% severe drought) and use of MAE loss likely biased model toward safer, non-severe predictions
  - Evidence: Experiments utilizing weighted loss functions or oversampling techniques specifically targeting recall metric for severe drought class

## Limitations

- Performance claims are tied to California's unique climate patterns and USDM scoring methodology; models require retraining for regions with different drought characteristics
- The 2000-2020 training period may not capture future climate regime shifts; strong historical performance doesn't guarantee continued accuracy
- Systematic underprediction of severe drought suggests current approaches may be insufficient for operational deployment despite acknowledging class imbalance

## Confidence

- **High confidence**: LSTM outperforms other ML models for drought forecasting in California; performance degrades with longer forecast horizons; feature importance rankings are robust
- **Medium confidence**: 24-30 week historical windows are optimal; severe drought underprediction is primarily due to class imbalance
- **Low confidence**: Mechanisms for why LSTM specifically excels over CNN are not mechanistically explained beyond temporal dependency claims

## Next Checks

1. **Class imbalance ablation**: Systematically vary severe drought threshold (2.0, 2.5, 3.0) and measure Macro F1 changes to quantify impact of drought definition on model performance

2. **Temporal robustness test**: Train models on 2000-2015 data, test on 2016-2020. Compare performance degradation to original temporal split to assess temporal generalization

3. **Feature ablation validation**: Remove top features (previous drought score, month) one-by-one from XGBoost and measure Macro F1 degradation. This validates causal importance rather than just correlation