---
ver: rpa2
title: 'FT-MoE: Sustainable-learning Mixture of Experts for Fault-Tolerant Computing'
arxiv_id: '2504.20446'
source_url: https://arxiv.org/abs/2504.20446
tags:
- fault
- ft-moe
- uni00000013
- experts
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FT-MoE introduces a dual-path architecture for fault-tolerant computing,
  combining a schedule-aware graph encoder to model task propagation and a fault-adaptive
  mixture-of-experts (MoE) layer to handle heterogeneous fault knowledge. The model
  employs Experts-Adaptation Gating (EAGate) to dynamically adjust the number of active
  experts based on input features, and cross multi-head attention to fuse heterogeneous
  features with scheduling information.
---

# FT-MoE: Sustainable-learning Mixture of Experts for Fault-Tolerant Computing

## Quick Facts
- **arXiv ID:** 2504.20446
- **Source URL:** https://arxiv.org/abs/2504.20446
- **Reference count:** 37
- **Primary result:** Dual-path architecture combining schedule-aware graph encoder and fault-adaptive MoE achieves accuracy 0.9053, recall 0.9322, F1 0.8766 in fault detection and HR 0.6496, NDCG 0.6021 in classification while reducing resource utilization and energy consumption.

## Executive Summary
FT-MoE introduces a sustainable-learning approach for fault-tolerant computing that addresses heterogeneous fault patterns through a dual-path architecture. The model employs Experts-Adaptation Gating (EAGate) to dynamically route different fault types to specialized expert parameters, while a schedule-aware graph encoder captures task migration impacts on host states. A cross multi-head attention module fuses heterogeneous features with scheduling information, and a two-stage learning scheme enables continual adaptation to dynamic environments. Evaluated on a fine-grained dataset of 160,000 intervals, FT-MoE outperforms state-of-the-art baselines in both fault detection and classification tasks while demonstrating improved resource efficiency.

## Method Summary
FT-MoE implements a dual-path architecture for fault-tolerant computing. The first path uses Experts-Adaptation Gating (EAGate) to compute cosine similarity between input features and learnable expert representations, applying trainable activation thresholds to determine which of 12 experts (max 8 active) process each input. The second path employs a schedule-aware graph encoder with attention aggregation to capture task migration impacts on device status. Cross multi-head attention (CMHA) fuses the MoE outputs with graph-encoded scheduling features. The model trains in two stages: offline training with a combined loss function (0.35·L_D + 0.5·L_C + 0.15·L_slt) using AdamW optimizer, followed by online tuning that dynamically adds/removes experts based on activation statistics while freezing non-MoE components. The approach is evaluated on a custom dataset generated via COSCO framework using Bitbrains traces.

## Key Results
- **Detection performance:** Accuracy 0.9053, recall 0.9322, F1 score 0.8766, outperforming baselines by significant margins
- **Classification capability:** HitRate 0.6496, NDCG 0.6021 in fault type identification (CPU/RAM/Disk)
- **Resource efficiency:** Reduced CPU/RAM utilization, energy consumption, and response time in fault-tolerant scheduling scenarios
- **Online adaptability:** Maintains performance stability during distribution shifts through expert-level online tuning mechanism

## Why This Works (Mechanism)

### Mechanism 1: Input-Adaptive Expert Selection for Heterogeneous Fault Patterns
- **Claim:** FT-MoE improves fault detection and classification by routing different fault types to specialized expert parameters, addressing the "high knowledge heterogeneity" problem that prior approaches ignored.
- **Mechanism:** The EAGate computes cosine similarity between input features and learnable expert representations, then applies trainable activation thresholds to determine which experts activate. The Top-any function enables variable expert counts per input (not fixed k), allowing simple faults to use fewer experts while complex patterns engage more.
- **Core assumption:** Different fault types (CPU, RAM, Disk) exhibit separable feature patterns that benefit from parameter-specialized processing rather than shared parameters.
- **Evidence anchors:**
  - [abstract] "This model employs a mixture-of-experts (MoE) architecture, enabling different parameters to learn distinct fault knowledge."
  - [Page 4, Eq. 2-4] Formal specification of similarity scoring, threshold comparison, and Top-any selection.
  - [corpus] Weak direct evidence for MoE in fault tolerance specifically; neighboring papers focus on control-theoretic fault tolerance (quadrotors, AUVs) rather than architectural MoE approaches.
- **Break condition:** If fault patterns lack semantic separability (e.g., RAM and Disk faults produce indistinguishable feature distributions), expert specialization degrades to redundant computation. The ablation (Fig. 3) shows accuracy drops from 90.479% to 87.956% without EAGate, but this is empirical—not a theoretical guarantee.

### Mechanism 2: Cross-Attention Fusion of Scheduling Context with Host State
- **Claim:** Combining task migration topology with resource features improves fault prediction by capturing causal relationships between scheduling decisions and host stress.
- **Mechanism:** The schedule-aware graph encoder (Path 2) aggregates neighborhood host features via attention-weighted summation, where edge weights depend on task migration volume. CMHA then attends over MoE outputs (Query) using graph-encoded features (Key/Value), letting host features "search" for relevant scheduling influences.
- **Core assumption:** Task migrations causally influence host fault probability, and this relationship is learnable via attention over graph-structured scheduling data.
- **Evidence anchors:**
  - [Page 3] "To efficiently capture the impact of task migration on device status, we adopt efficient dynamic graph attention network as the second path."
  - [Page 4, Eq. 9] CMHA formulation: O = MultiHead(Y, X', X'), where Y is MoE output and X' is graph-encoded features.
  - [corpus] No corpus papers explicitly model scheduling-topology-to-fault relationships; most focus on reactive control rather than predictive architectures.
- **Break condition:** If scheduling decisions are uncorrelated with faults (e.g., random load balancing), the graph encoder adds noise. The t-SNE visualization (Fig. 5) shows better class separation with CMHA, but only in the specific simulation environment.

### Mechanism 3: Plasticity Preservation via Expert-Level Online Tuning
- **Claim:** Freezing non-MoE components and dynamically adding/removing experts during online tuning maintains performance in non-stationary edge environments.
- **Mechanism:** During deployment, track expert activation frequencies (RE) and unhandled inputs (RS). If RE,g = 0 for expert g, remove it; if RS is non-empty, add a new expert with threshold ε_new = 0. Only MoE parameters update online.
- **Core assumption:** Fault distribution shifts are sufficiently gradual that expert-level adaptation is adequate, and non-MoE components (graph encoder, CMHA) remain transferable without fine-tuning.
- **Evidence anchors:**
  - [Page 5, Eq. 17-18] Recording expert activation and unhandled inputs.
  - [Fig. 4] Without online tuning, accuracy drops to 52.624% (detection) and 57.223% (classification); with tuning, performance stabilizes.
  - [corpus] Corpus papers on adaptive fault tolerance (AUVs, quadrotors) use control-switching rather than architectural plasticity; no direct evidence for this specific mechanism.
- **Break condition:** If distribution shifts affect scheduling topology (requiring graph encoder updates) or feature relationships (requiring CMHA updates), freezing these components limits adaptability. The paper does not test this boundary condition.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) with Sparse Gating**
  - **Why needed here:** FT-MoE builds on sparse MoE where only subset of experts activate per input. Standard dense MoE scales poorly; sparse gating enables conditional computation.
  - **Quick check question:** Can you explain why sparse MoE reduces inference cost compared to ensemble methods, and under what conditions the gating network becomes a bottleneck?

- **Concept: Graph Attention Networks (GAT)**
  - **Why needed here:** The schedule-aware encoder uses attention over graph edges defined by task migrations. Understanding how attention weights aggregate neighborhood information is prerequisite.
  - **Quick check question:** Given a graph with M hosts and K migration edges, how does GAT's attention mechanism handle varying neighborhood sizes, and what is the time complexity per layer?

- **Concept: Continual Learning and Catastrophic Forgetting**
  - **Why needed here:** The online tuning component addresses "plasticity loss" (Page 7). Without understanding forgetting mechanisms, the expert addition/removal strategy is opaque.
  - **Quick check question:** Why does freezing the graph encoder and CMHA during online tuning avoid catastrophic forgetting, and what failure mode would occur if all components were fine-tuned?

## Architecture Onboarding

- **Component map:**
  Input (X_t, S_t) 
    ├─→ [Path 1] EAGate → Expert Selection → Active Experts {E_g} → Weighted Sum → Y
    └─→ [Path 2] Graph Encoder → Attention Aggregation → X'
         ↓
    CMHA(Q=Y, K=X', V=X') → O → [FFN_D] → Detection (D ∈ R^{M×2})
                         └─→ [FFN_C] → Classification (C ∈ R^{M×Q}) → Prototype Matching

- **Critical path:** The CMHA module is the fusion bottleneck—if it fails to align scheduling context with fault features, both downstream tasks degrade. The ablation (Table S2) shows Wo/CMHA drops F1 from 0.8766 to 0.8345.

- **Design tradeoffs:**
  - **Expert count (G=12, G_max=8):** More experts increase capacity but risk underutilization. Paper does not ablate G systematically.
  - **Prototype dimension (Q=8):** Higher Q enables finer fault distinctions but increases classification loss optimization difficulty.
  - **Online tuning interval (I):** Not specified; determines expert removal/addition frequency. Too frequent causes instability; too infrequent misses distribution shifts.
  - **Loss weights (α₁=0.35, α₂=0.5, α₃=0.15):** Heavy weight on classification loss reflects paper's multitask priority.

- **Failure signatures:**
  - **EAGate failure:** All experts activate equally (similarity scores converge) → Top-any defaults to Top-G_max → computational overhead without specialization benefit. Symptom: High GPU usage, flat accuracy curves.
  - **Graph encoder failure:** Attention weights uniform → X' ≈ mean(X) → CMHA receives degraded scheduling signal. Symptom: Detection accuracy holds but classification degrades (fault types less distinguishable).
  - **Online tuning failure:** Rapid expert churn (add/remove oscillation) → unstable predictions. Symptom: Accuracy spikes and drops per interval; check RE and RS statistics.

- **First 3 experiments:**
  1. **Expert utilization profiling:** Log activation frequency per expert over validation set. Identify dead experts (RE,g ≈ 0) and dominant experts (RE,g >> mean). If >30% experts are dead, reduce G.
  2. **Ablation by fault type:** Evaluate per-class metrics (CPU, RAM, Disk) for Wo/EAGate vs. W/EAGate. If improvement is concentrated in one fault type, the heterogeneity assumption is partially violated.
  3. **Online tuning stress test:** Inject controlled distribution shift (e.g., change task arrival rate at interval 50) and monitor: (a) time to accuracy recovery, (b) expert count changes, (c) non-MoE feature drift (t-SNE of X' before/after shift).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FT-MoE perform when deployed on physical edge hardware compared to the simulated COSCO environment?
- Basis in paper: [explicit] The conclusion states, "In future work, our goal is to validate FT-MoE in real-world deployments... to assess its robustness and scalability."
- Why unresolved: The current evaluation relies entirely on the COSCO simulation framework and a synthetic dataset derived from Bitbrains traces, which may not capture the full stochastic nature of physical hardware faults.
- What evidence would resolve it: Performance metrics (accuracy, latency, energy) collected from physical edge nodes running FT-MoE under actual workload variations.

### Open Question 2
- Question: How can Federated Learning be integrated into the FT-MoE framework to preserve privacy while maintaining the model's ability to handle heterogeneous fault knowledge?
- Basis in paper: [explicit] The authors explicitly list a future goal to "incorporate federated learning for privacy."
- Why unresolved: The current architecture utilizes a centralized two-stage training scheme (offline and online tuning) that requires access to host features and scheduling data, which may be sensitive or siloed.
- What evidence would resolve it: A federated implementation of FT-MoE that converges to similar accuracy and HitRate scores without aggregating raw local data.

### Open Question 3
- Question: Does the "add a new expert" mechanism in the performance-aware online tuning lead to unbounded model growth or parameter oscillation in highly non-stationary environments?
- Basis in paper: [inferred] The online tuning logic adds a new expert whenever an input fails to activate any existing experts (RS = NULL), but the long-term stability of this dynamic expansion is not analyzed beyond the 100-interval simulation.
- Why unresolved: While the paper mentions removing low-contribution experts, it does not explore if a rapidly changing workload causes the expert count to balloon or the model to suffer from catastrophic forgetting during aggressive structural changes.
- What evidence would resolve it: A sensitivity analysis tracking the number of active experts and parameter stability over significantly longer time horizons (thousands of intervals).

## Limitations

- **Synthetic dataset dependency:** Current evaluation relies on COSCO-generated data from Bitbrains traces, which may not capture real-world fault complexity and temporal dependencies.
- **Fixed expert architecture:** The model uses a predetermined expert count (G=12) without systematic ablation of this hyperparameter, leaving scalability uncertainty.
- **Limited distribution shift testing:** Online tuning effectiveness demonstrated only for stationary shifts within the same simulation environment, not for novel fault types or extreme conditions.
- **Computational overhead uncertainty:** Paper does not address EAGate's inference cost or sensitivity to expert initialization.

## Confidence

- **High confidence:** Detection accuracy and recall improvements (0.9053, 0.9322) are well-supported by ablation studies showing significant drops without EAGate (to 87.956% accuracy).
- **Medium confidence:** Fault classification improvements (HR 0.6496, NDCG 0.6021) are supported by ablation but depend heavily on the synthetic dataset's representativeness.
- **Low confidence:** Online tuning's ability to handle real-world non-stationary environments, given the limited scope of distribution shifts tested and the assumption that only MoE parameters need adaptation.

## Next Checks

1. **Dataset generalization:** Validate FT-MoE on a real-world edge computing trace (e.g., Alibaba Cluster Trace) to assess performance on actual fault patterns and verify the heterogeneity assumption across different infrastructure types.

2. **Expert utilization analysis:** Profile expert activation frequencies over the validation set to identify dead experts and measure the variance in ω_g distributions. If >30% experts are dead, investigate whether reducing G improves efficiency without sacrificing accuracy.

3. **Online tuning robustness:** Simulate extreme distribution shifts (e.g., sudden spike in task arrivals, new fault types) and measure: (a) expert churn rate (add/remove frequency), (b) time to recovery, and (c) whether non-MoE components (graph encoder, CMHA) require updates to maintain performance.