---
ver: rpa2
title: 'When Diffusion Models Memorize: Inductive Biases in Probability Flow of Minimum-Norm
  Shallow Neural Nets'
arxiv_id: '2506.19031'
source_url: https://arxiv.org/abs/2506.19031
tags:
- points
- training
- flow
- point
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes probability flow in diffusion models using
  minimum-norm shallow ReLU network denoisers. The authors study three types of training
  data: orthogonal points, obtuse simplex, and equilateral triangle configurations.'
---

# When Diffusion Models Memorize: Inductive Biases in Probability Flow of Minimum-Norm Shallow Neural Nets

## Quick Facts
- arXiv ID: 2506.19031
- Source URL: https://arxiv.org/abs/2506.19031
- Reference count: 40
- Primary result: Probability flow in diffusion models with minimum-norm shallow ReLU denoisers converges to training points, virtual points (sums of training points), or hyperbox boundary points, with increasing training set size shifting convergence toward virtual points and boundaries.

## Executive Summary
This paper analyzes probability flow dynamics in diffusion models when the denoiser is a minimum-norm shallow ReLU network trained with exact interpolation. The authors prove that for orthogonal training data, probability flow converges to three types of stationary points: individual training points, sums of training points ("virtual points"), or boundary points of the hyperbox formed by these sums. Crucially, the diffusion time scheduler's "early stopping" allows probability flow to reach these boundary points unlike standard score flow. Experiments with 500-dimensional orthogonal data show that as training set size increases, more samples converge to virtual points or boundaries rather than training points, indicating improved generalization. The analysis uses the Augmented Lagrangian method to train denoisers with exact interpolation, revealing that virtual points and boundary points represent forms of generalization beyond memorizing training samples.

## Method Summary
The authors study probability flow ODEs with denoisers trained as minimum-norm shallow ReLU networks using the Augmented Lagrangian method for exact interpolation. They analyze three training data configurations: orthogonal points, obtuse simplex, and equilateral triangle. The theoretical analysis uses a low-noise approximation where the effective interpolation radius ρ_t scales with noise level σ_t. They prove convergence depends on initialization time relative to a threshold τ(y_T, ρ_T). Experiments use 500-dimensional orthogonal data with varying training set sizes (N=10 to 30) and 500 test samples from N(0, 100I). The denoisers use K=300 hidden units and S=150 timesteps with σ_t=√t.

## Key Results
- For orthogonal training data, probability flow converges to training points, virtual points (sums of training points), or hyperbox boundary points
- Early stopping from the diffusion time scheduler allows reaching boundary points unlike score flow
- Increasing training set size N reduces memorization: from ~60% converging to training points (N=10) to ~20% (N=30)
- Virtual points are stable stationary points with negative Jacobian eigenvalues for pair-wise and higher-order combinations
- The Augmented Lagrangian method achieves exact interpolation while maintaining minimum representation cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The diffusion time scheduler induces "early stopping" that allows probability flow to converge to boundary points of the implicit data manifold, not just training samples.
- Mechanism: The probability flow ODE uses a time-dependent noise schedule σ_t = √t. As noise decreases, the denoiser's effective interpolation radius ρ_t shrinks proportionally (ρ_t = ασ_t). This creates a time-varying dynamics where trajectories that would eventually reach a vertex under fixed-noise score flow instead stop at the hyperbox boundary when noise reaches zero.
- Core assumption: Low-noise regime where clusters around training points are well-separated and Taylor approximation of the score function holds.
- Evidence anchors:
  - [abstract] "However, early stopping by the diffusion time scheduler allows probability flow to reach more general manifold points."
  - [Section 4, Theorem 4.4] Proves convergence depends on initialization time T relative to threshold τ(y_T, ρ_T).
  - [corpus] Related work "When and how can inexact generative models still sample from the data manifold?" corroborates that models can sample from data manifolds despite approximations.

### Mechanism 2
- Claim: Minimum-norm shallow ReLU denoisers create stable stationary points at sums of training points ("virtual points"), not just individual training samples.
- Mechanism: The min-cost denoiser solution (Eq. 17) uses piecewise linear ReLU activations that create stable fixed points wherever multiple ReLU regions intersect. For orthogonal training data, these intersections occur at all partial sums of training vectors, forming hyperbox vertices. The Jacobian eigenvalue stability condition Re{λ(J(y))} < 0 holds at these virtual points.
- Core assumption: Denoiser exactly interpolates noisy training samples with minimal ℓ² representation cost.
- Evidence anchors:
  - [Section 4, Theorem 4.2] "The set of the stable stationary points... is A = {∑_{n∈I} x_n | I ⊆ [N-1]}"
  - [Section 5.1, Figure 1] Empirically validates 98.6% of pair-wise virtual points are stable in trained networks.
  - [corpus] Weak direct corpus support; this is a novel theoretical contribution of this paper.

### Mechanism 3
- Claim: Increasing training set size N reduces memorization and increases convergence to virtual points and boundary points.
- Mechanism: With more training points, the volume of the hyperbox grows, and initialization regions that converge to any single training point become proportionally smaller. Large-noise denoisers pull samples toward the training set mean, creating biased initialization for the low-noise regime that preferentially points toward virtual points.
- Core assumption: Training points remain approximately orthogonal as N increases (requires N << exp(d) for high-dimensional data).
- Evidence anchors:
  - [Section 5.2, Figure 3] Shows percentage of points converging to training points decreases from ~60% (N=10) to ~20% (N=30).
  - [Section 1] Cites Kadkhodaie et al. (2024) and Somepalli et al. (2023) for related empirical findings.
  - [corpus] "Resolving Memorization in Empirical Diffusion Model for Manifold Data" addresses similar memorization concerns.

## Foundational Learning

- Concept: **Probability Flow ODE vs. SDE Sampling**
  - Why needed here: The paper analyzes deterministic probability flow ODE rather than stochastic sampling; understanding this distinction is essential for following the convergence proofs.
  - Quick check question: Can you explain why the probability flow ODE is equivalent to SDE sampling in terms of marginal distributions but different in trajectory?

- Concept: **Tweedie's Identity and Score-Denoiser Connection**
  - Why needed here: The core theoretical tool linking neural network denoisers to score functions (Eq. 9-10). Understanding that h_MMSE(y) = y + σ²∇log p(y) is necessary to follow how the paper estimates scores.
  - Quick check question: If you have a trained denoiser h_θ(y) that is not the MMSE estimator, is ∇log p(y) still equal to (h_θ(y) - y)/σ²?

- Concept: **Shallow ReLU Network Representation Cost**
  - Why needed here: The paper specifically studies minimum-norm solutions where the ℓ² penalty on weights creates an implicit bias toward specific function forms (Eq. 4-8).
  - Quick check question: Why does the paper use the Augmented Lagrangian method instead of standard weight decay training?

## Architecture Onboarding

- Component map:
  Denoiser backbone -> Score estimator -> Probability flow integrator -> Convergence classification
  Shallow ReLU network with skip connection -> s(y, t) = (h_ρ_t*(y) - y) / σ_t² -> Discrete ODE solver (Eq. 20) -> Classification by L∞ distance to training points, virtual points, or boundary

- Critical path:
  1. Generate noisy samples: y_{n,m} = x_n + ε_{n,m} for each clean training point
  2. Train denoisers via AL method to achieve zero training loss with minimum representation cost
  3. Run probability flow from random initialization through S discrete timesteps
  4. Classify convergence: training point, virtual point, or hyperbox boundary

- Design tradeoffs:
  - **Exact interpolation vs. weight decay**: AL method enforces exact interpolation (matching theory) but is computationally expensive; weight decay approximates this but may not achieve zero loss
  - **Number of denoisers S**: More timesteps improve trajectory accuracy but increase training cost; paper uses S=150 with 50 in low-noise regime
  - **Hidden layer width K**: Must be large enough to realize the min-cost solution; paper uses K=300 for d=30

- Failure signatures:
  - Convergence only to training points → Insufficient regularization (no weight decay) or training not converged to min-norm solution
  - Samples outside hyperbox → Dropout or high MSE loss prevents exact interpolation
  - No virtual point stability → Hidden layer too narrow or noise level too high for low-noise regime

- First 3 experiments:
  1. **Validate virtual point existence**: Train single denoiser at fixed noise level σ=0.095; run fixed-point iterations from all pair/triplet/quadruplet sums of training points; measure convergence rate within L∞<0.2 threshold
  2. **Compare score flow vs. probability flow**: Initialize 500 random points from N(0, 100I); run both score flow (Eq. 21) and probability flow (Eq. 20); compare convergence type distributions
  3. **Ablate training set size**: Repeat probability flow experiment with N∈{10,15,20,25,30}; plot convergence type percentages to verify memorization decreases with N

## Open Questions the Paper Calls Out

- **VP Process Analysis**: How does probability flow behave under variance-preserving forward and backward diffusion processes? The paper only analyzes variance-exploding (VE) case while practical models like DDPM use VP formulations with different noise schedules and ODE dynamics.

- **Non-symmetric Jacobians**: What guarantees exist for convergence of score-based sampling when neural network denoisers have non-symmetric Jacobians? The theoretical results assume symmetric positive semidefinite Jacobian for the score to be a true gradient field, but practical neural networks don't guarantee this.

- **Disc-based Interpolation**: How do convergence properties change when denoisers interpolate over (M−1)-dimensional discs rather than full d-dimensional balls around training points? With M < d noisy samples, Gaussian noise concentrates on an (M−1)-dimensional subspace, making the full ball assumption geometrically inaccurate for practical training regimes.

- **Deep Network Extension**: Can theoretical guarantees for probability flow be extended to deep neural network architectures beyond shallow single-hidden-layer models? The paper's analysis focuses on shallow models for mathematical tractability, while practical implementations use deep networks with more complex representation costs.

## Limitations
- Theoretical analysis relies heavily on idealized orthogonal training data assumption rarely met in practice
- Augmented Lagrangian training method is computationally intensive and may not scale to larger datasets or deeper architectures
- Low-noise regime assumption requires careful parameter tuning (α relating ρ_t to σ_t) that is not fully specified
- Analysis focuses on shallow ReLU networks, leaving open questions about extension to deeper architectures

## Confidence
- **High Confidence**: Mathematical proofs for convergence behavior on orthogonal datasets (Theorem 4.2, 4.4) are rigorous and well-supported by low-noise approximation framework
- **Medium Confidence**: Experimental validation showing decreased memorization with larger N (Figure 3) is compelling but based on synthetic orthogonal data that may not generalize to real-world datasets
- **Low Confidence**: Claim that virtual points represent "improved generalization" rather than memorization is conceptually interesting but lacks empirical validation on natural data distributions

## Next Checks
1. **Real-world data validation**: Replicate memorization analysis on CIFAR-10 or ImageNet with approximately orthogonal data subsets to test if virtual points and boundary convergence patterns persist
2. **Architecture scaling study**: Test whether minimum-norm solutions with virtual point stability can be achieved with deeper ReLU networks or other activation functions while maintaining computational feasibility
3. **Alternative training objectives**: Compare Augmented Lagrangian method's virtual point formation against standard weight decay training to quantify trade-off between exact interpolation and computational cost in practical settings