---
ver: rpa2
title: Understanding the Information Propagation Effects of Communication Topologies
  in LLM-based Multi-Agent Systems
arxiv_id: '2505.23352'
source_url: https://arxiv.org/abs/2505.23352
tags:
- communication
- agent
- topologies
- propagation
- topology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a causal analysis of how communication topologies
  affect error and insight propagation in large language model (LLM)-based multi-agent
  systems. It reveals that moderately sparse topologies, which suppress error propagation
  while preserving beneficial information diffusion, typically achieve optimal task
  performance.
---

# Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems

## Quick Facts
- arXiv ID: 2505.23352
- Source URL: https://arxiv.org/abs/2505.23352
- Reference count: 23
- One-line primary result: Moderately sparse topologies optimally balance error suppression and insight propagation in LLM-based multi-agent systems, with EIB-LEARNER achieving 91.38% average accuracy across six benchmarks.

## Executive Summary
This paper presents a causal analysis of how communication topologies affect error and insight propagation in large language model (LLM)-based multi-agent systems. Through empirical studies across six benchmarks, the authors reveal that moderately sparse topologies achieve optimal performance by effectively suppressing error cascades while preserving beneficial information diffusion. Based on these insights, they propose EIB-LEARNER, a dual-view GNN framework that fuses connectivity patterns from sparse (Chain) and dense (Full) graphs with query-aware adaptive fusion. Experiments demonstrate EIB-LEARNER achieves superior accuracy (91.38% average), reduced communication costs, and better robustness compared to existing baselines.

## Method Summary
The authors formalize communication topology design as finding a DAG G = (V, E) that maximizes collective output utility. EIB-LEARNER employs a dual-view GNN framework with all-MiniLM-L6-v2 node encoder to embed agent roles and queries into 384-dim vectors. Two GNNs process sparse (chain) and dense (fully connected) views, decoding connectivity coefficients via inner product. A query-aware MLP gating network outputs adaptive fusion weights, combining sparse and dense views to produce M_final. The topology sampler generates discrete DAGs via Bernoulli edge sampling, trained via policy gradient using task accuracy as reward. The system uses GPT-4o API with 4-6 agents depending on benchmark, optimizing over B∈{40,60} training queries.

## Key Results
- Moderately sparse topologies achieve optimal performance by balancing error suppression and insight propagation
- EIB-LEARNER achieves 91.38% average accuracy across six benchmarks (MMLU, GSM8K, MultiArith, SVAMP, AQuA, HumanEval)
- The method reduces communication costs compared to fully connected graphs while maintaining superior robustness to adversarial prompt attacks

## Why This Works (Mechanism)

### Mechanism 1
Dense graphs act as high-conductance channels for both correct and incorrect information, allowing single agent errors to flip collective decisions (high TCTE). Sparse graphs limit error spread but also starve downstream agents of correct signals. A topology with intermediate connectivity decouples error propagation rate from insight propagation rate.

### Mechanism 2
EIB-LEARNER uses two GNNs: one on Chain topology learns error suppression and sequential dependency features, while one on Full topology learns insight aggregation and debate features. Combining these representations captures how agents behave under isolation versus full exposure.

### Mechanism 3
Query-aware adaptive fusion uses a gating network that consumes query embedding and outputs weights (α_dense, α_sparse). For high-risk or ambiguous queries, the system may upweight sparse connectivity to limit error propagation; for knowledge synthesis tasks, it may upweight dense connectivity.

## Foundational Learning

- **Counterfactual Reasoning & Causal Intervention (do-calculus)**: Used to quantify topology robustness through CAPE (Counterfactual Agent Propagation Effect). Understanding forcing an agent to output an error (do(O_i := Õ_i)) is essential for Section 3.
  - *Quick check*: If intervening on Agent A flips final output from Correct to Incorrect, does Agent A have high or low CAPE score?

- **Policy Gradient (REINFORCE)**: Required because the objective function (LLM output accuracy) is non-differentiable. The authors use policy gradients to optimize the topology generator.
  - *Quick check*: Why can't we use standard backpropagation (e.g., Adam) directly through LLM agents to update GNN weights?

- **Directed Acyclic Graphs (DAGs) in Message Passing**: The system relies on topological sort to determine execution order. Understanding why cycles are prohibited is critical for debugging the "sampler" component.
  - *Quick check*: What would happen to the execution loop if the learned adjacency matrix contained a cycle (e.g., A → B → A)?

## Architecture Onboarding

- **Component map**: Input Query + Agent Roles → Node Encoder → Dual-View GNN Backbone (Sparse Chain + Dense Full) → Decoder → Connectivity Matrices → Fusion Gate → Topology Sampler → MAS Environment (GPT-4o)

- **Critical path**: The Topology Sampling step is most sensitive. Policy gradient relies on reward signal from final MAS answer. Invalid or disconnected graphs create noisy reward signals that destabilize GNN training.

- **Design tradeoffs**: Simulation vs. Reality gap exists between GNNs simulating propagation on extreme graphs and actual agents executing on fused graphs. Cost vs. Robustness tradeoff introduces GNN overhead while reducing agent communication costs.

- **Failure signatures**: Low TCTE but Low Accuracy indicates topology too sparse; High Accuracy, High Cost suggests fusion gate biased toward Dense view; Training Divergence from sparse LLM reward signals.

- **First 3 experiments**: 1) Reproduce TCTE Analysis on MMLU with fixed Chain vs. Full topologies; 2) Ablation on Fusion Module by removing gating network and comparing against adaptive version; 3) Robustness Check by injecting adversarial prompts and measuring performance drop versus Chain topology.

## Open Questions the Paper Calls Out

1. **Generalization to open-domain tasks**: Does the error-insight balance principle generalize to open-domain dialogue and complex real-world decision-making tasks? The current scope is limited to reasoning, math, and code benchmarks lacking evaluation in scenarios requiring long-term memory or dynamic interaction flows.

2. **Automatic agent role discovery**: Can the framework extend to automatically discover optimal agent roles and prompts alongside communication topology? Current reliance on fixed predefined roles and manual prompts restricts adaptability in evolving scenarios.

3. **Dynamic topology evolution**: Does the optimal communication topology evolve dynamically across K interaction rounds as agents converge on solutions? Current formulation finds single optimal topology G for a query, implying fixed structure during K rounds, while the need for dense vs. sparse may shift temporally.

## Limitations
- Policy gradient approach faces challenges with sparse reward signals from binary task correctness, potentially limiting training stability
- Gap between simulated propagation on extreme topologies and actual execution on fused graphs introduces uncertainty about real-world performance
- Assumes semantic query features reliably predict optimal topology structure, which may not hold for all problem domains

## Confidence

**High Confidence**: Core finding that moderately sparse topologies achieve optimal performance by balancing error suppression and insight propagation (Finding 1, Section 3.2.3).

**Medium Confidence**: Effectiveness of dual-view GNN architecture for learning agent compatibility features; specific architectural choices (GNN depth, layer types) are underspecified.

**Medium Confidence**: Query-aware adaptive fusion mechanism's ability to dynamically optimize topology selection; gating network robustness to diverse query types requires further validation.

## Next Checks

1. **Reward Signal Analysis**: Profile variance and distribution of task accuracy rewards during EIB-LEARNER training across benchmarks. High variance or excessive sparsity would indicate need for reward shaping or alternative optimization strategies.

2. **Simulation-to-Reality Gap Evaluation**: Implement variant where agents actually execute on both Chain and Full topologies (not just simulated), then compare GNN's learned representations against those from actual execution traces. Quantify simulation error.

3. **Gating Network Robustness Test**: Create synthetic query distribution spanning semantic complexity extremes (simple factual to complex reasoning). Measure whether fusion gate's output distribution meaningfully shifts with query type, or if it suffers from mode collapse to fixed weights.