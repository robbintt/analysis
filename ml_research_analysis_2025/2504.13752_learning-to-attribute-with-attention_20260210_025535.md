---
ver: rpa2
title: Learning to Attribute with Attention
arxiv_id: '2504.13752'
source_url: https://arxiv.org/abs/2504.13752
tags:
- attribution
- attention
- context
- sources
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of identifying which preceding
  tokens influence a language model's generation, a task traditionally expensive due
  to the need for ablations. The core idea is to learn a surrogate model that uses
  attention weights from different heads as features, with learnable coefficients
  indicating each head's utility for attribution.
---

# Learning to Attribute with Attention

## Quick Facts
- arXiv ID: 2504.13752
- Source URL: https://arxiv.org/abs/2504.13752
- Reference count: 40
- Primary result: AT2 achieves comparable attribution quality to ablations while being significantly faster

## Executive Summary
This paper introduces AT2, a method for identifying which preceding tokens influence a language model's generation. Traditional approaches require expensive ablations, but AT2 learns a surrogate model that uses attention weights from different heads as features, with learnable coefficients indicating each head's utility for attribution. The method achieves comparable attribution quality to methods requiring many ablations while being significantly faster, making token attribution more practical for real-world applications.

## Method Summary
AT2 learns coefficients θ_ℓh for each attention head, transforming attribution from a simple average into a weighted combination. The method trains these coefficients by sampling random ablation vectors, computing true probability drops, and optimizing θ so that surrogate predictions correlate with actual ablation effects. This surrogate model over attention weights approximates the effect of token ablations on generation probability. Once trained, AT2 only requires attention weights for inference, making it significantly faster than traditional ablation-based methods while maintaining comparable attribution quality.

## Key Results
- AT2 outperforms baselines like average attention and gradient methods
- AT2 performs similarly to example-specific surrogate modeling with 256 ablations but with much lower computational cost
- AT2 improves response quality in context pruning tasks on HotpotQA
- The method generalizes across datasets, with task-specific and general training performing comparably

## Why This Works (Mechanism)

### Mechanism 1
Head-specific attention weighting improves attribution reliability compared to uniform averaging. AT2 learns a coefficient θ_ℓh for each attention head, transforming the attribution score from a simple average into a weighted combination. Heads with high coefficients contribute more to the final attribution. The core assumption is that specific attention heads specialize in tracking token relevance (e.g., "retrieval heads"), and this specialization is consistent across examples from similar distributions.

### Mechanism 2
A linear surrogate model over attention weights can approximate the effect of token ablations on generation probability. AT2 trains θ by sampling random ablation vectors, computing the true probability drop, and optimizing θ so that the surrogate prediction correlates with the actual drop. The loss is negative Pearson correlation over logit-transformed probabilities. The core assumption is that the relationship between which tokens are present and the generation probability is approximately linear in the contribution of individual sources.

### Mechanism 3
Training on one dataset generalizes to attribution on unseen datasets without per-example ablations. θ is learned once on a training set using ~32 ablations per example. At inference, only attention weights are needed—no additional forward passes. The paper shows "AT2 (general)" trained on generic data performs comparably to "AT2 (task-specific)" on held-out tasks. The core assumption is that the mapping from attention patterns to ablation effects is primarily a function of model architecture, not the specific input distribution.

## Foundational Learning

- **Transformer attention mechanism**: Understanding how Q/K/V produce attention distributions is essential to interpret what the coefficients θ are weighting. Quick check: Given a 12-layer, 12-head transformer, what is the shape of the attention weight tensor for a 100-token input generating a 20-token output?

- **Surrogate modeling / LIME-style explanation**: AT2 is fundamentally a surrogate model—understanding the abstraction of approximating a complex function with a simpler interpretable one clarifies why ablation data is used for training. Quick check: If you ablate tokens {A, B, C} and see probability drop 0.3, and ablate {A, B} and see drop 0.2, what does a linear surrogate predict for the individual contributions of A, B, and C?

- **Correlation-based loss (Pearson/Spearman)**: AT2 optimizes for correlation between predicted and actual ablation effects, not raw probability error—this shifts the objective toward ranking quality. Quick check: If predictions are [0.1, 0.5, 0.9] and targets are [0.2, 0.4, 0.8], will a Pearson correlation loss produce a different gradient direction than MSE?

## Architecture Onboarding

- **Component map**: Attention extraction module -> Coefficient store -> Scoring function -> Training loop -> Inference path
- **Critical path**: 1) Generate response Y from input X using target LLM. 2) Extract attention weights Attn(X, Y, ·, ·) during generation. 3) Aggregate attention over source spans S. 4) Apply learned θ to compute attribution scores. 5) (Optional) Use scores for context pruning or citation.
- **Design tradeoffs**: Token vs. sentence sources (training on tokens transfers to sentences but increases training cost); Saved vs. recomputed attention (FlashAttention doesn't store full attention matrices, requiring recomputation); Training dataset choice (task-specific training marginally improves performance).
- **Failure signatures**: Average attention baseline outperforms AT2 (may indicate θ learning failed); Attributions highlight irrelevant tokens (high-θ heads may attend to syntactic rather than semantic features); Cross-dataset generalization poor (domain shift requires retraining).
- **First 3 experiments**: 1) Reproduce the head coefficient visualization on a held-out example to verify that high-θ heads attend to semantically relevant tokens for your model and domain. 2) Compare AT2 (general) vs. AT2 (task-specific) on your target dataset using top-5 log-prob drop; if gap > 10%, consider domain-specific training. 3) Benchmark end-to-end latency: measure attention extraction + scoring time vs. a single forward pass; verify ≥2× speedup.

## Open Questions the Paper Calls Out

### Open Question 1
How can attribution methods incorporate higher-order interactions between input tokens to improve faithfulness beyond the first-order approximations used by AT2? The authors state that attention weights "do not consider effects from interactions between tokens in the input sequence," limiting the method to first-order effects. Developing a variant of AT2 that includes multiplicative interaction terms for co-attended tokens and measuring the performance gap on the Linear Datamodeling Score (LDS).

### Open Question 2
Can incorporating non-attention internal features (e.g., value vectors or MLP outputs) enhance attribution accuracy without sacrificing the efficiency of the surrogate model? Appendix C.1 suggests that "we may need additional features" to achieve accuracy beyond AT2, implying a necessary trade-off between accuracy and efficiency. An ablation study where the surrogate model w_θ is trained on attention weights plus value vectors, evaluated against the Top-k drop metric and computational overhead.

### Open Question 3
To what extent does the misalignment between attention (correlated with the full predicted distribution) and the specific generated token limit the fidelity of AT2? The authors note that attention contributes to the "predicted distribution, rather than directly contributing to the generated token," calling attention an "imperfect proxy." Measuring the divergence between AT2 scores for greedy decoding versus sampling with high temperature, checking if scores for high-probability but non-generated tokens remain erroneously high.

## Limitations
- The method's core assumption—that attention head coefficients learned on one dataset transfer to others—rests on empirical observation rather than theoretical justification
- The linear surrogate model may fail when token interactions exhibit strong non-linear dependencies (e.g., XOR-like relationships)
- The learned coefficients could be capturing arbitrary statistical patterns rather than genuine functional specialization of attention heads

## Confidence
- **High confidence**: Primary empirical claim that AT2 achieves comparable attribution quality to ablations while being significantly faster
- **Medium confidence**: Generalization claim that AT2 (general) performs comparably to AT2 (task-specific), but evidence is limited to three datasets
- **Low confidence**: Mechanism explanations, as correlation between high-coefficient heads and relevant tokens doesn't establish causation

## Next Checks
1. **Cross-architecture transfer**: Train AT2 on Llama-3.1-8B and evaluate on a different architecture (e.g., Mistral or Qwen). If performance degrades significantly (>15% drop in top-5 drop), this confirms model-specific training is necessary.

2. **Non-linear interaction test**: Construct synthetic examples with known XOR-like token dependencies (e.g., "The answer is [A] and [B]" where both A and B together matter but neither alone does). Compare AT2's attribution to ground truth to quantify linear surrogate limitations.

3. **Coefficient stability analysis**: Train AT2 multiple times on the same dataset with different random seeds. Measure coefficient variance across runs and correlation with attribution quality. High variance would indicate unstable learning and limit practical reliability.