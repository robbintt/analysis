---
ver: rpa2
title: Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language
  Models
arxiv_id: '2501.13428'
source_url: https://arxiv.org/abs/2501.13428
tags:
- attention
- lssar
- softmax
- length
- mechanism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two critical limitations of standard Softmax
  attention in large language models: numerical instability and degraded performance
  on long sequences. The authors propose a two-stage attention mechanism called LSSAR,
  consisting of a Length Scaled Softplus Attention (LSSA) normalisation stage and
  a sharpening re-weighting stage.'
---

# Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models

## Quick Facts
- **arXiv ID**: 2501.13428
- **Source URL**: https://arxiv.org/abs/2501.13428
- **Reference count**: 40
- **Primary result**: LSSAR (Length Scaled Softplus Attention with Re-weighting) enables transformers to maintain performance at 16× training sequence length and recover Newton's gravitational law from orbital data

## Executive Summary
This paper introduces LSSAR, a two-stage attention mechanism that addresses numerical instability and degraded performance on long sequences in standard Softmax attention. The first stage uses Length Scaled Softplus Attention (LSSA) with an entropy-invariant length scale factor and Softplus normalization, while the second stage applies a sharpening re-weighting transformation. Experiments demonstrate LSSAR maintains nearly constant validation loss even at 16× the training sequence length, achieves superior performance on long-context retrieval tasks where standard attention fails completely, and enables symbolic regression to recover physical laws. Remarkably, LSSAR enables a 109M parameter model to recover Newton's gravitational law from orbital trajectories through symbolic regression, while standard Softmax attention and even trillion-parameter models fail to do so.

## Method Summary
LSSAR is a two-stage attention mechanism designed to replace standard Softmax attention in transformers. The first stage, LSSA, normalizes queries and keys using L2 normalization, computes attention scores scaled by log(d)·log(N) where N represents token count per row, applies the Softplus function instead of exponential, and uses L1 normalization. The second stage applies a sharpening re-weighting transformation using a power function (p=15) with a causal offset mask, followed by another L1 normalization. This design addresses numerical instability in Softmax through the numerically stable Softplus function and dynamic length scaling based on invariance entropy, while the sharpening stage enables coarse-to-fine causal filtering. The mechanism was implemented in GPT-2 architecture and trained on FineWeb-10B dataset with 124M parameters.

## Key Results
- LSSAR maintains nearly constant validation loss across sequence lengths up to 16× training length (1K-16K tokens)
- Achieves superior performance on long-context retrieval tasks where standard attention fails completely
- Enables a 109M parameter model to recover Newton's gravitational law from orbital trajectories through symbolic regression, while standard Softmax and trillion-parameter models fail

## Why This Works (Mechanism)
LSSAR addresses the fundamental limitations of Softmax attention by replacing the exponential function with numerically stable Softplus and introducing a dynamic length scale factor based on invariance entropy. The length scaling factor log(d)·log(N) adjusts the attention distribution according to the information entropy of the sequence, making the mechanism invariant to sequence length variations. The sharpening re-weighting stage applies a power transformation to create more decisive attention allocations, enabling coarse-to-fine causal filtering that enhances the model's ability to capture long-range dependencies. This combination provides both numerical stability through polynomial gradient decay (vs exponential in Softmax) and improved extrapolation capability through length-invariant attention distributions.

## Foundational Learning
- **Softplus function**: A smooth approximation to ReLU, used instead of exponential to avoid numerical overflow in attention computation
  - Why needed: Prevents gradient explosion and numerical instability in Softmax attention
  - Quick check: Verify Softplus(x) = log(1 + exp(x)) is implemented correctly and handles large inputs gracefully
- **Information entropy invariance**: The property that entropy remains constant under certain transformations, used to scale attention scores
  - Why needed: Enables length-independent attention distributions for better extrapolation
  - Quick check: Confirm the scaling factor log(d)·log(N) produces consistent entropy across different sequence lengths
- **Symbolic regression**: Machine learning technique to discover mathematical expressions from data
  - Why needed: Demonstrates LSSAR's ability to induce physical-world inductive biases
  - Quick check: Verify the recovered expression matches Newton's gravitational law form (inverse square relationship)
- **Length scaling**: Dynamic adjustment of attention scores based on sequence length
  - Why needed: Addresses the exponential growth problem in attention computation for long sequences
  - Quick check: Measure attention score magnitudes at different sequence lengths to confirm appropriate scaling
- **Power sharpening transformation**: Applying ReLU followed by power function to create sparse attention distributions
  - Why needed: Enables coarse-to-fine causal filtering for better long-range dependency capture
  - Quick check: Visualize attention maps before and after sharpening to confirm increased sparsity

## Architecture Onboarding
- **Component map**: Input tokens -> Token embeddings + RoPE -> LSSAR (LSSA normalization + Sharpening re-weighting) -> Feed-forward network -> Output
- **Critical path**: The attention mechanism is the computational bottleneck; LSSAR modifies this path with LSSA normalization followed by sharpening re-weighting
- **Design tradeoffs**: Numerical stability vs computational overhead (1.7× memory increase), extrapolation capability vs potential loss of fine-grained distinctions
- **Failure signatures**: Gradient explosion with high p values in Softmax baseline, performance degradation on long sequences, inability to recover physical laws in symbolic regression
- **First experiments**:
  1. Implement LSSA stage with L2 normalization, Softplus function, and length scaling; verify numerical stability with large inputs
  2. Add sharpening stage with power transformation and causal mask; measure attention sparsity and distribution changes
  3. Train on FineWeb-10B with sequence length 1024; evaluate validation loss at 1K, 2K, 4K, 8K, 16K tokens to verify length extrapolation

## Open Questions the Paper Calls Out
- **Scaling to billion-parameter models**: Whether LSSAR maintains advantages when scaled to 1B+ parameters with 4K-8K training sequences and 64K-128K inference lengths
- **Efficient fusion with FlashAttention-3**: Whether LSSAR can be integrated into tiled attention kernels without materializing full attention matrix and matching standard attention's efficiency
- **Generalization to other physical laws**: Whether LSSAR's ability to recover Newton's law extends to discovering other physical relationships or symbolic structures
- **Reduction of reasoning verification loops**: Whether LSSAR's decisive attention allocations can reduce redundant self-verification loops in Chain-of-Thought reasoning

## Limitations
- Limited to small model sizes (45M-355M parameters) due to computing resource constraints
- Missing optimizer hyperparameters and exact RoPE configuration details needed for reproduction
- Only one physics example demonstrated for physical law discovery claims
- Theoretical compatibility with FlashAttention claimed but not experimentally verified

## Confidence
- **Numerical stability and length extrapolation**: High
- **Downstream benchmark improvements**: Medium
- **Physical world model induction**: Low

## Next Checks
1. Reproduce the training pipeline with specified FineWeb-10B dataset and GPT-2 architecture, focusing on validation loss at extrapolated lengths (1K-16K) to verify the near-constant loss behavior
2. Implement the full LSSAR mechanism including both normalization and sharpening stages, then measure gradient norms during training to confirm the claimed stability advantage over Softmax at high sharpening power values
3. Run the symbolic regression experiment on orbital trajectory data to independently verify that LSSAR can recover Newton's gravitational law while Softmax fails, examining the learned attention patterns and symbolic expressions