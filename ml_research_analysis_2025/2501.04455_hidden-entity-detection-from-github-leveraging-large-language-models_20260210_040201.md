---
ver: rpa2
title: Hidden Entity Detection from GitHub Leveraging Large Language Models
arxiv_id: '2501.04455'
source_url: https://arxiv.org/abs/2501.04455
tags:
- llms
- urls
- task
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates using Large Language Models (LLMs) for
  detecting datasets and software mentions from GitHub README pages. The authors explore
  two tasks: extraction and classification of URLs, and classification-only of provided
  URLs.'
---

# Hidden Entity Detection from GitHub Leveraging Large Language Models

## Quick Facts
- **arXiv ID**: 2501.04455
- **Source URL**: https://arxiv.org/abs/2501.04455
- **Reference count**: 28
- **Primary result**: Off-the-shelf LLMs struggle with high-precision entity detection for knowledge graph population, particularly distinguishing between similar URL classes

## Executive Summary
This paper investigates using Large Language Models (LLMs) for detecting datasets and software mentions from GitHub README pages. The authors evaluate four prompt strategies using LLaMA 2 and Mistral 7B models in both full and quantized versions across two tasks: extraction/classification of URLs and classification-only of provided URLs. While LLMs can detect URLs with exact or partial matches, their performance in strict classification is limited, particularly for distinguishing similar classes like dataset direct links and dataset landing pages. The quantized models generally perform worse than their full versions, and dynamic few-shot examples do not consistently improve performance. A key limitation is the LLMs' tendency to hallucinate URLs or miss some in the input context, making them unsuitable for high-precision tasks required for knowledge graph population.

## Method Summary
The study uses few-shot learning with LLaMA 2 7B and Mistral 7B (full and 4-bit quantized) to classify URLs from GitHub READMEs into four categories: Dataset Direct Link, Dataset Landing Page, Software, and Other. Four prompt strategies are tested: static examples versus dynamic examples selected via textual similarity. The evaluation uses precision and recall metrics under four schemas (Strict, Exact, Partial, Type) on a dataset of 811 repositories containing 1,439 URLs. The pipeline involves prompt construction, LLM inference with JSON output formatting, post-processing to clean conversational filler and parse JSON, and bipartite matching to align predictions with ground truth.

## Key Results
- LLMs achieve high exact match performance for URL detection but struggle with strict classification precision
- Mistral 7B consistently outperforms LLaMA 2 across all metrics and prompt strategies
- Quantized models show significant performance degradation compared to full models
- Dynamic few-shot examples do not consistently improve performance over static examples
- The main failure modes are URL hallucination and missed entity detection in long contexts

## Why This Works (Mechanism)

### Mechanism 1: In-Context Pattern Mimicry via Few-Shot Prompting
- Claim: If few-shot examples are provided in the prompt, the LLM attempts to mimic the annotation schema (JSON format and label set) rather than relying solely on pre-trained knowledge.
- Mechanism: The model conditions its output on the provided input-output pairs (static or dynamic), mapping the structure of the README context to the requested JSON object.
- Core assumption: The model possesses sufficient implicit reasoning capability to distinguish between similar semantic classes (e.g., "Dataset Direct Link" vs "Dataset Landing Page") based solely on these examples.
- Evidence anchors: Mentions exploring "different FSL prompt learning approaches" to enhance identification.

### Mechanism 2: Contextual Inference from README Semantics
- Claim: If the text surrounding a URL contains strong semantic signals (e.g., "download here," "repository," "homepage"), the LLM uses these cues to infer the URL's entity type.
- Mechanism: The attention mechanism weights the relationship between the URL token and its surrounding text tokens to predict the most likely classification label from the prompt instructions.
- Core assumption: URLs are treated as generic tokens where meaning is derived primarily from context rather than the URL string itself, as URLs are "not frequent tokens" in vocabulary.
- Evidence anchors: Notes the difficulty of comprehending content linked to a URL solely from context and the lack of vocabulary frequency for URLs.

### Mechanism 3: Output Constraint Enforcement (JSON Mode)
- Claim: If explicit formatting instructions are given (e.g., "Output: valid JSON syntax"), the model attempts to structure its generation, but generative noise often breaks validity.
- Mechanism: Instruction tuning biases the model to complete the pattern, though probabilistic generation introduces conversational filler or malformed syntax.
- Core assumption: The model's instruction-following capability is strong enough to override its default conversational behavior.
- Evidence anchors: Describes removing "conversational opening phrases" and consolidating fragments because LLMs "often include opening phrases."

## Foundational Learning

- Concept: **Named Entity Recognition (NER) for Non-Standard Tokens**
  - Why needed here: Traditional NER handles names/dates; this paper applies NER to URLs (low-frequency, high-variance tokens) which requires understanding string structure and context differently.
  - Quick check question: How does the tokenization of a long URL potentially impact the model's ability to classify it compared to a standard English word?

- Concept: **Quantization Trade-offs**
  - Why needed here: The paper evaluates 4-bit quantized models to simulate resource-constrained environments, showing a performance drop compared to full models.
  - Quick check question: If deploying on a standard consumer GPU (e.g., 8GB VRAM), which model configuration from the paper offers the best balance of parseability vs. strict accuracy?

- Concept: **Hallucination in Extraction Tasks**
  - Why needed here: Unlike classification, extraction requires the model to faithfully reproduce input spans. The paper identifies "hallucinated URLs" as a critical failure mode.
  - Quick check question: In an extraction task, if an LLM outputs a URL that was not in the input text but looks plausible, is this a retrieval error or a generation hallucination?

## Architecture Onboarding

- Component map:
  Input README text -> Prompt Construction -> LLM Inference -> Post-Processing (Cleaner + JSON Parser) -> Bipartite Matcher -> Evaluation Metrics

- Critical path:
  1. Construct prompt with 4 examples
  2. Infer using LLM to generate raw text string
  3. **Crucial Step**: Clean "conversational filler" and parse broken JSON
  4. Match predicted URLs to ground truth using Longest Common Substring (LCS) ratio

- Design tradeoffs:
  - **Regex vs. LLM**: The paper implies regex is superior for pure extraction; LLMs add semantic context but hallucinate
  - **Static vs. Dynamic Examples**: Dynamic examples did not consistently improve performance over static ones
  - **Quantization**: Reduces hardware requirements but generally lowers strict classification precision

- Failure signatures:
  - **The "Conversational" Failure**: Output starts with "Sure! Here is the JSON..." rendering the JSON invalid
  - **The "Hallucination" Failure**: Model invents a URL that exists semantically but not in the source text
  - **The "Missed Entity" Failure**: Model ignores URLs in the middle of long contexts

- First 3 experiments:
  1. **Baseline Extraction**: Run a Regex pattern for HTTP URLs vs. LLM extraction on a sample of 50 READMEs to quantify the "missed URL" rate
  2. **Prompt Stability Test**: Test Mistral 7B (best performer in paper) with Prompt 1 (Static) on 20 repos. Check if output is valid JSON without the post-processing cleaner to assess raw reliability
  3. **Binary vs. Multi-class**: Compare "Dataset vs. Non-Dataset" (Binary) performance against the 4-class schema to isolate whether the model fails at extraction or fine-grained classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized fine-tuning or domain-adaptive pre-training of LLMs significantly improve performance on entity extraction and classification tasks for knowledge graph population?
- Basis in paper: The authors conclude that "off-the-shelf models are inadequate" and call for "further research and development to enhance their suitability for such endeavors."
- Why unresolved: The study only evaluates off-the-shelf LLaMA 2 and Mistral 7B models in zero-shot and few-shot settings, without exploring whether task-specific fine-tuning could address the identified limitations.
- What evidence would resolve it: Comparative experiments showing that domain-adapted or fine-tuned LLMs achieve substantially higher precision and recall on URL extraction and classification tasks compared to the off-the-shelf baselines reported in this paper.

### Open Question 2
- Question: What factors determine when dynamic few-shot example selection improves versus harms performance on specialized entity detection tasks?
- Basis in paper: The authors found that "our experiments do not show an improvement in the models' performance when prompted with dynamic samples," contrary to recent literature (Ding et al., 2023).
- Why unresolved: The paper reports this unexpected finding but does not investigate why dynamic examples failed to help, leaving open whether the issue lies in example selection algorithms, similarity metrics, or task-specific characteristics.
- What evidence would resolve it: Ablation studies testing different similarity measures for dynamic example selection, varying the number of examples, and comparing performance across different task types to identify when and why dynamic selection succeeds or fails.

### Open Question 3
- Question: Can hybrid approaches combining traditional extraction methods (e.g., regex) with LLM-based classification overcome the limitations of purely generative approaches for structured entity detection?
- Basis in paper: The paper notes LLMs "lack the precision offered by other non-LLM-based methods (e.g., regular expression based heuristics)" for URL extraction, while showing some capability for classification, suggesting potential synergies.
- Why unresolved: The study evaluates LLMs independently rather than exploring pipeline architectures that could leverage regex for reliable URL extraction followed by LLM-based semantic classification.
- What evidence would resolve it: Experiments comparing end-to-end LLM approaches against hybrid pipelines that use regex for initial URL extraction and LLMs only for the classification task, measuring both precision/recall and computational efficiency.

## Limitations

- Off-the-shelf LLMs are inadequate for high-precision entity detection required for knowledge graph population
- Models struggle to distinguish between semantically similar classes (Dataset Direct Link vs. Dataset Landing Page)
- Quantization introduces significant accuracy degradation, particularly for Mistral 7B
- LLMs frequently hallucinate URLs or miss entities in long contexts, violating strict extraction requirements

## Confidence

- **High Confidence**: The core finding that LLMs can detect URLs with exact or partial matches but struggle with strict classification is well-supported by the experimental results across multiple models and prompt strategies.
- **Medium Confidence**: The conclusion that dynamic few-shot examples do not consistently improve performance is based on observed trends but may vary with different similarity metrics or example selection strategies.
- **Low Confidence**: The claim about URL tokenization impact on classification is speculative and not directly tested in the paper, though it's a reasonable hypothesis given URL structure.

## Next Checks

1. **Binary Classification Test**: Re-run the Mistral 7B model on a simplified binary schema (Dataset vs. Non-Dataset) to determine if the model's failure is due to extraction or fine-grained classification difficulty.
2. **Context-Ambiguity Experiment**: Create test cases with READMEs containing URLs with ambiguous surrounding text to quantify how often the model hallucinates context associations leading to classification errors.
3. **Quantization Ablation Study**: Compare the performance degradation of different quantization methods (GPTQ vs. AWQ) on the same model to isolate whether accuracy loss is due to quantization format or bit-depth reduction.