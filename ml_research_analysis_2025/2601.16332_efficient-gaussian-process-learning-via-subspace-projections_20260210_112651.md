---
ver: rpa2
title: Efficient Gaussian process learning via subspace projections
arxiv_id: '2601.16332'
source_url: https://arxiv.org/abs/2601.16332
tags:
- training
- gaussian
- time
- sparse
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Projected Likelihood (PL), a novel training\
  \ objective for Gaussian Processes (GPs) constructed using lower-dimensional linear\
  \ projections of the data. PL achieves a computational cost of O(kn\xB2), where\
  \ k\u226An, and outperforms existing methods like exact GP training and the variational\
  \ free energy approach to sparse GPs in terms of accuracy and computational efficiency."
---

# Efficient Gaussian process learning via subspace projections

## Quick Facts
- arXiv ID: 2601.16332
- Source URL: https://arxiv.org/abs/2601.16332
- Reference count: 0
- The paper introduces Projected Likelihood (PL), a novel training objective for Gaussian Processes (GPs) constructed using lower-dimensional linear projections of the data, achieving O(kn²) computational cost and outperforming exact GP training and variational free energy approaches.

## Executive Summary
This paper introduces Projected Likelihood (PL), a new training objective for Gaussian Processes that uses lower-dimensional linear projections of the data to achieve computational efficiency. PL operates in O(kn²) time where k≪n, making it significantly faster than exact GP training while maintaining competitive accuracy. The authors provide theoretical justification through Fisher information analysis and demonstrate empirically that PL outperforms existing methods like exact GP and variational free energy on various datasets, kernels, and optimizers.

## Method Summary
The method constructs a new training objective by projecting the data y ∈ ℝⁿ into a lower-dimensional space using a fixed projection matrix Ω ∈ ℝⁿˣᵏ where k≪n. The Projected Likelihood is computed as the negative log likelihood of the projected data z = Ωᵀy under a GP defined on these projections. The projection matrix is constructed using random vectors on the unit sphere, which the authors show empirically preserves sufficient information for accurate hyperparameter learning. The optimization updates only kernel hyperparameters θ, avoiding the additional complexity of training inducing point locations as in variational methods.

## Key Results
- PL achieves NLL values closer to exact GP baseline than variational free energy across multiple datasets
- Computational complexity is O(kn²) compared to O(n³) for exact GP and O(nm²) for variational methods
- PL converges significantly faster than variational free energy for moderate dataset sizes (n < 3000)
- Random spherical projections effectively preserve information for smooth kernels with rapidly decaying spectra

## Why This Works (Mechanism)

### Mechanism 1: Fisher Information Preservation via Projection
- **Claim:** If the projection directions sufficiently capture the covariance structure, the hyperparameters can be learned with minimal information loss compared to exact Gaussian Process (GP) training.
- **Mechanism:** The method derives a closed-form expression for Fisher information loss (ΔI) when using the Projected Likelihood (PL). Theorem 1 proves this loss is quadratic in the conditional covariance Σ_{Y|Z}. If the projection matrix Ω spans the principal subspace of K_x, Σ_{Y|Z} (the residual) is minimized, preserving the information required to learn θ.
- **Core assumption:** The relevant structure of the GP can be captured by a low-rank approximation of the covariance matrix.
- **Evidence anchors:**
  - [abstract]: "We provide a closed-form expression for the information loss related to the PL."
  - [Section 3.2]: Theorem 1 and Eq. (16) show ΔI = tr(…), linking loss directly to Σ_{Y|Z}.
  - [corpus]: Corpus papers (e.g., Adaptive Linear Embedding) support the validity of linear embeddings for dimensionality reduction, though they do not validate this specific Fisher loss derivation.
- **Break condition:** If the projection vectors are orthogonal to the top eigenvectors of the kernel matrix K_x, the information loss becomes prohibitive.

### Mechanism 2: Randomized Spectral Approximation
- **Claim:** Randomly sampled projections on the unit sphere can approximate the optimal (but computationally unavailable) eigenvectors of the covariance matrix, effectively reducing the trace of the conditional covariance.
- **Mechanism:** Since K_x changes during training, optimal fixed projections are impossible. The authors show empirically that random "Sphere" projections reduce the spectrum of Σ_{Y|Z} more effectively than "One-hot" (local) projections and comparably to "Repulsive" projections, particularly for kernels with rapid spectral decay (e.g., Squared Exponential).
- **Core assumption:** The kernel spectrum decays sufficiently fast such that k ≪ n projections capture the essential variance.
- **Evidence anchors:**
  - [abstract]: "...empirically show that it can be reduced with random projections on the unit sphere."
  - [Section 4]: Figure 1 compares spectrum reduction across "Localised", "One-hot", "Sphere", and "Repulsive" projections.
  - [corpus]: Not explicitly validated in corpus signals.
- **Break condition:** White noise processes (flat spectrum) where no low-dimensional subspace contains significantly more information than others (though Corollary 2 notes equivalence in the n=k case).

### Mechanism 3: Optimization Efficiency via Fixed Projections
- **Claim:** Fixing the projection matrix Ω simplifies the loss landscape compared to sparse GPs (like VFE) that learn inducing locations, resulting in fewer optimization steps.
- **Mechanism:** Sparse GPs (VFE) introduce inducing locations x̄ as trainable parameters, increasing the complexity of the loss surface and requiring Cholesky decompositions dependent on these locations. PL uses fixed random projections, avoiding the need to backpropagate through inducing point locations. While the theoretical cost is O(kn²) (vs. VFE's O(nm²)), PL converges significantly faster in wall-clock time for moderate n.
- **Core assumption:** The computational overhead of optimizing inducing point locations in VFE outweighs the higher theoretical complexity of PL's O(kn²) objective for moderate datasets.
- **Evidence anchors:**
  - [Section 3.3]: "PL relies on a fixed set of directions, thus avoiding the training of additional optimisation variables."
  - [Section 5.2]: Figure 3 and text confirm PL is faster for n < 3000 but VFE catches up asymptotically.
  - [corpus]: Corpus papers on iterative GPs highlight the cost of optimization steps, indirectly supporting the value of simplified landscapes.
- **Break condition:** As dataset size n grows asymptotically large, the O(n²) scaling per step dominates, making O(nm²) methods (VFE) faster regardless of iteration count.

## Foundational Learning

- **Concept:** **Fisher Information**
  - **Why needed here:** The paper anchors its theoretical justification on "Fisher information loss" (Theorem 1). Understanding this concept is required to interpret why preserving the score function's variance matters for hyperparameter learning.
  - **Quick check question:** Can you explain why minimizing the variance of the score function s(Y) conditioned on Z implies that Z is a sufficient statistic for learning θ?

- **Concept:** **Sparse Gaussian Processes (Inducing Variables)**
  - **Why needed here:** The paper positions itself as an alternative to Variational Free Energy (VFE). You must understand how VFE uses inducing points x̄ to approximate the posterior to see why PL's fixed projections offer a different trade-off.
  - **Quick check question:** How does the computational complexity of VFE (O(nm²)) scale with the number of inducing points m, and what bias does VFE introduce regarding noise variance?

- **Concept:** **Kernel Spectrum / Eigendecomposition**
  - **Why needed here:** Section 4 analyzes the "Spectrum of K_x" to justify why random projections work. The mechanism relies on the fact that for smooth kernels, eigenvalues decay, allowing low-rank approximations.
  - **Quick check question:** Why does a Squared Exponential (SE) kernel allow for aggressive dimensionality reduction via projection compared to a white-noise kernel?

## Architecture Onboarding

- **Component map:**
  Input Layer: Raw observations y ∈ ℝⁿ and inputs x
  → Projection Layer: Fixed projection matrix Ω ∈ ℝⁿˣᵏ (Random "Sphere" or "Repulsive")
  → Projected Data: z = Ωᵀy
  → Kernel Computation: Compute K_x (Standard GP covariance)
  → Projected Covariance: Compute Ωᵀ (K_x + σₙ²I) Ω
  → Objective: Negative Log Likelihood of z under Projected GP (Eq. 9)

- **Critical path:**
  The core loop is computing K_x Ω (matrix multiplication) followed by the inversion of the resulting k × k matrix. Unlike standard GPs, this avoids inverting an n × n matrix. The optimization updates kernel hyperparameters θ only; Ω is fixed.

- **Design tradeoffs:**
  - **Accuracy vs. Speed (Moderate Data):** PL provides NLL values closer to Exact ML than VFE for n < 3000, but VFE is theoretically superior for massive n.
  - **Projection Type:** "Sphere" (random) is fastest and robust; "Repulsive" might offer slightly better spectral coverage but costs more to initialize.
  - **Bias:** PL appears to track Exact ML noise variance better than VFE (which overestimates it).

- **Failure signatures:**
  - **Asymptotic Slowdown:** If n scales into the tens of thousands without subsampling, the O(kn²) matrix multiplications will become the bottleneck, and VFE will likely become faster.
  - **Spectral Mismatch:** If the underlying data requires a kernel with a flat spectrum (high effective dimensionality), a small k will result in high information loss and poor hyperparameter recovery.

- **First 3 experiments:**
  1. **Optimizer Benchmark (E1):** Replicate Table 1. Train PL vs. VFE on synthetic data (n=1000) using Adam and BFGS. Verify that PL reaches NLL closer to Exact ML in less time.
  2. **Scaling Analysis (E2):** Replicate Figure 3. Vary n (e.g., 500 to 4000) and plot Time vs. NLL. Confirm the "crossover" point where VFE becomes competitive.
  3. **Projection Ablation:** Implement the projection types from Figure 1 ("Sphere", "One-hot", "Localised") and plot the spectrum of the residual Σ_{Y|Z} to verify that "Sphere" minimizes the trace effectively.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's computational advantage diminishes for very large datasets (n > 3000) where VFE becomes competitive despite requiring more optimization steps.
- The theoretical analysis assumes smooth kernels with rapidly decaying spectra; performance on kernels with flat spectra or in high-dimensional input spaces remains unvalidated.
- The projection matrix Ω is fixed during training, potentially missing optimal subspaces that could be discovered through adaptive projection.

## Confidence
High: The paper provides clear theoretical justification (Fisher information analysis), comprehensive empirical validation across multiple datasets and kernels, and transparent implementation details.

## Next Checks
1. Implement the spherical projection sampling and verify that the projection matrix Ω has full rank k.
2. Reproduce the synthetic experiment comparing PL against exact GP and VFE using Adam optimizer with learning rate 0.1.
3. Validate the spectral coverage claim by comparing the trace of Σ_{Y|Z} across different projection types ("Sphere", "One-hot", "Localised").