---
ver: rpa2
title: 'GmSLM : Generative Marmoset Spoken Language Modeling'
arxiv_id: '2509.09198'
source_url: https://arxiv.org/abs/2509.09198
tags:
- marmoset
- speech
- language
- gmslm
- vocalizations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GmSLM is a generative spoken language model pipeline adapted for\
  \ marmoset vocalizations, replacing text-based models with a self-supervised approach\
  \ using HuBERT and discrete units. The system generates vocalizations that closely\
  \ match real resynthesized samples acoustically, achieving low Fr\xE9chet Audio\
  \ Distance scores."
---

# GmSLM : Generative Marmoset Spoken Language Modeling

## Quick Facts
- **arXiv ID**: 2509.09198
- **Source URL**: https://arxiv.org/abs/2509.09198
- **Reference count**: 21
- **Primary result**: GmSLM generates marmoset vocalizations matching real samples acoustically with low FAD scores, achieving 90.72 F1 on vocalization type and 90.12 F1 on speaker identity classification.

## Executive Summary
GmSLM is a generative spoken language model pipeline adapted for marmoset vocalizations, replacing text-based models with a self-supervised approach using HuBERT and discrete units. The system generates vocalizations that closely match real resynthesized samples acoustically, achieving low Fréchet Audio Distance scores. It effectively distinguishes real from artificial conversations, even without supervision, and performs strongly on downstream tasks like vocalization type and speaker identity classification (F1 scores of 90.72 and 90.12, respectively). The model's performance improves significantly over human-speech baselines, particularly when trained on marmoset-specific data. Key findings include the importance of shorter context spans (~6 seconds) and the preservation of unit repetitions for modeling marmoset vocalizations. GmSLM provides a practical framework for studying vocal communication and neural activity in nonhuman primates, with potential applications in neuroscience, bioacoustics, and evolutionary biology.

## Method Summary
GmSLM adapts the three-stage GSLM pipeline (Vocalization-to-Unit, Unit Language Model, Unit-to-Vocalization) for marmoset vocalizations using self-supervised learning. The V2U stage fine-tunes HuBERT on marmoset audio, clusters layer-1 features into 50 discrete units while preserving repetitions, and extracts sequences for the uLM. The uLM is a transformer_lm_big trained on these unit sequences with specific hyperparameters including 300-500 token context windows. The U2V stage uses a HiFi-GAN vocoder to convert generated units back to audio. The pipeline is trained without textual supervision, using COLONYDB for training and PHEEDB/INFANTMARMOSETSVOX for evaluation. Key adaptations include using earlier HuBERT layers, preserving unit repetitions, and shorter context spans than typical human speech models.

## Key Results
- GmSLM achieves low Fréchet Audio Distance scores, indicating generated vocalizations closely match real resynthesized samples acoustically
- Strong performance on downstream tasks: 90.72 F1 for vocalization type classification and 90.12 F1 for speaker identity classification
- Outperforms human-speech baselines significantly when trained on marmoset-specific data across Shuffle, Concat, and Reversal tasks
- Identifies optimal context span of ~6 seconds for marmoset vocal communication modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting the speech encoder to marmoset vocalizations improves discrete unit quality over human-speech-only baselines.
- Mechanism: HuBERT learns representations via masked prediction on continuous audio features; k-means clustering then quantizes these into discrete units. Fine-tuning both components on marmoset data (rather than using off-the-shelf human speech models) yields units that better capture marmoset-specific acoustic patterns.
- Core assumption: Marmoset vocalizations share enough structural properties with human speech for HuBERT's masked prediction objective to learn meaningful representations, despite acoustic differences.
- Evidence anchors: [abstract], [section 3.2], [section 5.1, Table 1] showing GmSLM outperforms S-Hu across Shuffle (84.84 vs 67.75), Concat (79.94 vs 78.96), and Reversal (90.45 vs 83.29) tasks

### Mechanism 2
- Claim: Preserving unit repetitions (rather than deduplicating) encodes duration information critical for marmoset vocalization modeling.
- Mechanism: Unlike human speech where consecutive identical units are collapsed, maintaining repeated units allows the uLM to learn temporal patterns. Marmoset calls appear to use duration as a communicative feature.
- Core assumption: Unit repetition count correlates with biologically meaningful variation in marmoset calls.
- Evidence anchors: [section 3.2], [section 5.3, Table 4] showing with deduplication disabled (X 50), performance improves across Shuffle (67.75 vs 64.87), Concat (78.96 vs 77.68), and Reversal (83.29 vs 75.59) compared to deduplicated versions

### Mechanism 3
- Claim: Marmoset vocal communication relies on shorter context spans (~6 seconds) compared to human language.
- Mechanism: By masking attention beyond fixed token thresholds, the authors find performance degrades below 300 tokens (~6 seconds) unless early tokens are preserved. This suggests essential communicative structure occurs within brief exchanges.
- Core assumption: The attention masking experiments accurately reflect biological constraints rather than model limitations.
- Evidence anchors: [section 5.4, Table 5] showing without preserving early tokens, 500-token performance matches full context; with first token preserved, similar performance persists to 300 tokens; [abstract] stating "Key findings include the importance of shorter context spans (~6 seconds)"

## Foundational Learning

- **Concept: Self-supervised speech representation learning (HuBERT)**
  - Why needed here: Understanding how masked prediction on audio creates learnable representations without labels
  - Quick check question: Can you explain why masking spans of audio and predicting cluster assignments forces the model to learn structure?

- **Concept: Discrete unit extraction via clustering**
  - Why needed here: The bridge between continuous audio and language modeling requires quantization
  - Quick check question: How does k-means clustering on HuBERT activations create a "vocabulary" for the language model?

- **Concept: Generative Spoken Language Modeling (GSLM) pipeline**
  - Why needed here: GmSLM adapts this three-stage pipeline (encoder → LM → decoder) for non-human primates
  - Quick check question: Why can't we directly apply text-based LLMs to marmoset vocalizations?

## Architecture Onboarding

- **Component map**: Raw Audio → [V2U: HuBERT + k-means] → Discrete Units → [uLM: Transformer] → Generated Units → [U2V: HiFi-GAN vocoder] → Audio

- **Critical path**:
  1. Data preprocessing: High-pass filter at 5kHz, spectrogram-based segmentation (precision 0.975, recall 0.78)
  2. V2U adaptation: Start with LibriSpeech-pretrained HuBERT, train k-means on layer activations, fine-tune HuBERT with new quantizer
  3. Layer selection: Extract from Layer 1 (not deeper layers)—counterintuitive but critical
  4. Unit configuration: 50 clusters, preserve repetitions (don't deduplicate)
  5. Context window: 300-500 tokens sufficient; shorter than typical human speech models

- **Design tradeoffs**:
  - Cluster count: 50 vs 100—50 chosen for efficiency; minimal performance difference
  - Layer depth: Earlier layers (1, 3) outperform deeper layers (6, 9)—suggests acoustic rather than semantic representations
  - Vocoder simplicity: No speaker embeddings or F0 (unlabeled data); sacrifices speaker consistency for applicability

- **Failure signatures**:
  - High perplexity with deduplicated units → check if repetitions are preserved
  - Poor Shuffle/Concat task performance → verify k-means trained on marmoset data, not human speech
  - Generated audio lacks naturalness → check FAD against resynthesized baseline (should be similar)
  - Near-random Phee eval performance → V2U likely not adapted to marmoset vocalizations

- **First 3 experiments**:
  1. Baseline comparison: Train uLM on units from speech-only HuBERT (S-Hu), speech-HuBERT with marmoset k-means (S-mHu), and full adaptation (GmSLM). Evaluate on Shuffle/Concat/Reversal tasks. Expected: S-Hu ≈ 50% on ReceiverChange, GmSLM > 65%.
  2. Layer ablation: Extract units from layers 1, 3, 6, 9 across three HuBERT variants with different supervision layers. Expected: Layer 1 consistently outperforms deeper layers for marmoset data.
  3. Context masking: Restrict attention to 50-500 tokens with/without preserving first 1-5 tokens. Expected: Performance degrades below 300 tokens without early token preservation, revealing ~6-second context window.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does an underlying n-gram-like distribution govern marmoset unit sequences, and how does it encode meaning?
- Basis in paper: [explicit] The authors state in Section 5.4 that the low unit purity scores "suggests the presence of an underlying n-gram-like distribution over units, which remains an open direction for future work."
- Why unresolved: The analysis showed that meaning arises from context and unit length rather than standalone units, but the specific statistical structure (n-gram) was inferred rather than definitively mapped or proven to carry semantic weight.
- What evidence would resolve it: A detailed structural analysis of unit co-occurrence statistics and a correlation study linking specific n-gram patterns to observable behavioral contexts or semantic labels.

### Open Question 2
- Question: How robust is GmSLM when applied to out-of-domain recording conditions or distinct marmoset social groups?
- Basis in paper: [explicit] The limitations section (A.9) notes that "evaluating its performance in out-of-domain scenarios remains an important direction for future work" since the model was primarily tested under a single recording condition.
- Why unresolved: The current model was optimized for a specific colony room setup (COLONYDB); it is unknown if the learned representations generalize to different acoustic environments or vocalization styles found in other colonies.
- What evidence would resolve it: Zero-shot evaluation of the trained model on external datasets recorded in different environments or involving different marmoset populations to measure performance degradation.

### Open Question 3
- Question: Can advanced neural vocoders enhance the acoustic fidelity and ecological validity of generated marmoset vocalizations?
- Basis in paper: [explicit] The authors note in Appendix A.8 that while the current vocoder sufficed for structural analysis, "the development of a more sophisticated vocoder [is left] to future work."
- Why unresolved: The study utilized older vocoder components to focus on linguistic structure, potentially limiting the acoustic quality needed for high-stakes applications like bioacoustic playback experiments.
- What evidence would resolve it: Integrating state-of-the-art neural audio codecs (e.g., AudioLM or SoundStream) and measuring improvements using Fréchet Audio Distance (FAD) and behavioral responses from live marmosets.

## Limitations

- **Data availability**: The primary COLONYDB dataset (~360 hours) and PHEEDB evaluation set are not publicly accessible, limiting reproducibility
- **Cross-species generalization**: The claim that HuBERT's masked prediction objective works for marmoset vocalizations relies on unstated assumptions about structural similarity between human speech and primate calls
- **Model architecture limitations**: The 6-second context finding could reflect Transformer capacity limits rather than biological constraints

## Confidence

**High confidence**: The basic pipeline adaptation works—GmSLM outperforms human-speech baselines on zero-shot tasks and downstream classification. The layer-1 feature extraction finding is well-supported by ablation studies.

**Medium confidence**: The 6-second context window and unit repetition preservation mechanisms are supported by internal experiments but lack external validation. The acoustic versus semantic distinction remains unresolved.

**Low confidence**: Claims about biological significance of findings (e.g., that the 6-second window reflects marmoset communication structure) are speculative without behavioral validation or comparison to alternative model architectures.

## Next Checks

1. **Architecture capacity validation**: Replicate the context masking experiments using a Transformer variant with longer-range attention (like Reformer or Longformer) to determine if the 6-second finding persists with improved architecture capacity.

2. **Cross-species representation analysis**: Apply the same V2U pipeline to another non-human vocalization dataset (like bird songs or dolphin clicks) to test whether HuBERT-based discretization generalizes across species, or if marmoset-specific adaptation is required.

3. **Behavioral correlation study**: Compare the discrete units learned by GmSLM to manually annotated marmoset vocalization types to assess whether the unsupervised clustering captures biologically meaningful categories, or merely acoustic similarities.