---
ver: rpa2
title: 'Unveiling the Mystery of Weight in Large Foundation Models: Gaussian Distribution
  Never Fades'
arxiv_id: '2501.10661'
source_url: https://arxiv.org/abs/2501.10661
tags:
- layer
- proj
- attn
- self
- down
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a systematic exploration of weight distributions
  in large foundation models (LFMs), revealing that pre-trained weights predominantly
  follow Gaussian distributions, with some exhibiting sharp, inverted T-shaped, or
  linear patterns. The authors discover that transformation weights, derived from
  Gaussian noise, primarily increase the standard deviation of pre-trained weights,
  with their standard deviation growing with layer depth.
---

# Unveiling the Mystery of Weight in Large Foundation Models: Gaussian Distribution Never Fades

## Quick Facts
- **arXiv ID:** 2501.10661
- **Source URL:** https://arxiv.org/abs/2501.10661
- **Reference count:** 40
- **Primary result:** Most large foundation models (LFMs) exhibit Gaussian-distributed weights, with fine-tuning methods like LoRA and DoRA introducing structured deviations; experiments show adaptation gains up to 8 points and editing gains up to 2 points over baselines.

## Executive Summary
This paper systematically investigates the weight distributions of large foundation models (LFMs), revealing that pre-trained weights predominantly follow Gaussian distributions, with some showing sharp, inverted T-shaped, or linear patterns. The study demonstrates that transformation weights derived from Gaussian noise primarily increase the standard deviation of pre-trained weights, with their spread growing with layer depth. Building on these observations, the authors hypothesize that optimal weights should be zero-mean, symmetric, sparse, and consist of truncated Gaussian-distributed values interspersed with outliers. Experiments in LFM adaptation and editing validate these insights, achieving significant performance improvements over baseline methods.

## Method Summary
The authors analyze weight distributions by extracting and flattening weight tensors from specific layers and modules (e.g., self_attn.q_proj, mlp.up_proj) across multiple LFMs. They visualize these distributions using density histograms and compare base model distributions to those after fine-tuning with LoRA-Dash and DoRA. The study spans models including LLaMA, Qwen, Mistral, and others, examining both pre-trained and fine-tuned variants. Analysis focuses on layer-wise and module-wise patterns, with attention projections typically showing narrower distributions than MLP layers.

## Key Results
- Pre-trained LFMs predominantly exhibit Gaussian-distributed weights, centered near zero with variance increasing by layer depth.
- Fine-tuning with LoRA and DoRA introduces structured deviations, producing bimodal or heavier-tailed distributions in adapter weights.
- Attention projection layers (Q, K, V) consistently show narrower weight distributions compared to MLP layers.
- Experiments in LFM adaptation and editing demonstrate performance gains up to 8 points (adaptation) and 2 points (editing) over baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained large foundation models (LFMs) exhibit weight values that cluster tightly around zero, forming near-Gaussian distributions across most layers.
- Mechanism: During large-scale pretraining, stochastic gradient descent with regularization (e.g., weight decay) encourages many weights to remain small. The central limit theorem and symmetric noise in updates lead to a bell-shaped distribution centered at zero, with variance determined by training hyperparameters.
- Core assumption: The training dynamics and regularization schemes dominate over any task-specific or architectural biases in shaping the aggregate weight statistics.
- Evidence anchors:
  - [abstract] The work "investigates the weight distribution of various large foundation models" and "finds the weight of most large foundation models follows a Gaussian distribution."
  - [section] Figures throughout the paper (e.g., Figures 1–9 for LLaMA models) show histograms with narrow peaks at zero and symmetric tails.
  - [corpus] The corpus neighbor "Analysis on distribution and clustering of weight" uses standard deviation vectors to characterize model weights, but does not explicitly confirm Gaussianity for all LFMs. Evidence from corpus is limited.
- Break condition: If an LFM is trained with strong, asymmetric regularization (e.g., L1) or non-standard architectures (e.g., extremely sparse initializations), weight distributions may deviate from Gaussian, showing heavy tails, multimodality, or sparsity.

### Mechanism 2
- Claim: Fine-tuning with parameter-efficient methods (e.g., LoRA, DoRA) introduces structured deviations from the base model's Gaussian weight distribution, creating bimodal or heavier-tailed distributions in the adapted parameters.
- Mechanism: LoRA adds low-rank adapters initialized near zero; during fine-tuning, these adapters learn task-specific corrections, leading to weights that can diverge from the base Gaussian. DoRA decouples magnitude and direction updates, often producing a different distribution with potentially larger variance.
- Core assumption: The base model's weights remain mostly frozen, and only the adapter or magnitude/direction parameters shift the overall weight distribution away from the initial Gaussian.
- Evidence anchors:
  - [section] Figures 17–26 show LoRA-Dash and DoRA weight distributions for different ranks (ΔWr), revealing histograms with wider spreads, occasional bimodality, and heavier tails compared to base models.
  - [abstract] The study compares "different fine-tuning methods," implying analysis of distributional shifts post-adaptation.
  - [corpus] Corpus neighbors lack direct studies on weight distribution shifts during LoRA/DoRA fine-tuning; evidence is minimal.
- Break condition: If fine-tuning uses full-parameter updating or very large adapter ranks, the weight distribution may revert closer to a Gaussian as the model re-equilibrates under new data.

### Mechanism 3
- Claim: Layer-wise and module-wise (e.g., attention vs. MLP) weight distributions exhibit consistent patterns across diverse model families, with attention projections often having narrower distributions than MLP layers.
- Mechanism: Attention projections (Q, K, V) typically operate in a normalized space with bounded dynamics, leading to smaller weight magnitudes. MLP layers, especially up-projections, often have larger weights to accommodate non-linear transformations and increased dimensionality.
- Core assumption: Architectural role dictates the scale and spread of weight values, and this is consistent across well-trained models regardless of exact architecture.
- Evidence anchors:
  - [section] Visual inspection of histograms (e.g., Figures 1–15) shows that for many models, attention layers have narrower peaks around zero compared to MLP layers, which sometimes show slightly broader distributions.
  - [abstract] The work analyzes "different modules" within layers, indicating a focus on module-specific patterns.
  - [corpus] The corpus neighbor "Analysis on distribution and clustering of weight" mentions using "standard deviation vector and clustering" to analyze model characteristics, supporting layer/module-wise analysis, but does not provide direct comparison of attention vs. MLP distributions.
- Break condition: If models are trained with aggressive normalization only in attention layers, or if MLP activations are strongly constrained, the relative spreads could invert or become indistinguishable.

## Foundational Learning

- Concept: **Gaussian Distribution in Neural Networks**
  - Why needed here: The core observation of the paper is that LFM weights approximate a Gaussian; understanding this helps in analyzing model capacity, pruning, and quantization.
  - Quick check question: What are two properties of a Gaussian distribution that are often assumed when analyzing neural network weights? (Answer: symmetric, mean around zero, characterized by variance/standard deviation)

- Concept: **Weight Deviation and Standard Deviation Vector**
  - Why needed here: The paper uses standard deviation vectors to quantify weight spread across layers/modules, which helps compare models and identify architectural invariants.
  - Quick check question: If a layer's weight standard deviation is much larger than others, what might that imply about its role or training dynamics? (Answer: It may handle more complex transformations, have less regularization, or be more critical for fine-tuning)

- Concept: **Parameter-Efficient Fine-Tuning (PEFT) Methods**
  - Why needed here: The study contrasts base model distributions with those from LoRA and DoRA; understanding PEFT is key to interpreting distributional shifts.
  - Quick check question: In LoRA, which parameters are typically trained, and how does that affect the base model's weight distribution? (Answer: Low-rank adapters are trained; base weights stay frozen, so only the adapter distribution changes, not the base model's Gaussian)

## Architecture Onboarding

- Component map:
  - **Base Model Weights**: Frozen or slowly updated parameters forming the initial Gaussian distribution.
  - **PEFT Adapters**: LoRA's low-rank A/B matrices or DoRA's magnitude/direction parameters that are trained.
  - **Layer Modules**: Attention (Q, K, V, output projection) and MLP (up/down projection) blocks, each with distinct weight distributions.
  - **Analysis Pipeline**: Histogram plotting, standard deviation computation, and comparison across layers/methods.

- Critical path:
  1. Load pre-trained LFM weights and extract per-layer/module tensors.
  2. Compute histogram and standard deviation for each tensor.
  3. Compare base distribution to those after fine-tuning with LoRA/DoRA.
  4. Identify patterns: Gaussian vs. bimodal, attention vs. MLP spreads, rank-dependent changes.

- Design tradeoffs:
  - **Granularity**: Analyzing per-layer vs. per-module distributions trades off detail vs. complexity.
  - **PEFT method choice**: LoRA offers simplicity and low cost; DoRA provides more expressivity but may alter distributions more drastically.
  - **Visualization vs. statistics**: Histograms give intuitive shape; standard deviation vectors enable quantitative comparison.

- Failure signatures:
  - **Overfitting in adapters**: Weight distributions become extremely wide or bimodal with heavy tails, indicating instability.
  - **Catastrophic forgetting**: Base model Gaussian shifts significantly after full fine-tuning, not shown here but a risk.
  - **Inconsistent patterns**: If attention layers have broader distributions than MLP, it may signal training issues or unusual architecture.

- First 3 experiments:
  1. **Base distribution profiling**: For a chosen LFM (e.g., LLaMA-7B), plot weight histograms for all layers/modules to confirm Gaussian shape and note standard deviations.
  2. **LoRA rank sweep**: Fine-tune with LoRA at ranks 4, 16, 64; compare resulting adapter weight distributions and overlay with base model to observe divergence.
  3. **Module comparison**: Within a single model, compute and plot standard deviation vectors for attention vs. MLP layers across depth; check if attention consistently has lower spread.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the decomposition strategy in DoRA yield a statistically distinct distribution of delta weights (ΔW) compared to LoRA-Dash for the same rank sizes?
- Basis in paper: [explicit] The text provides parallel visualizations for "LoRA-Dash" (Figs. 17-21) and "DoRA" (Figs. 22-26) fine-tuning LLaMA-7B, inviting a direct comparison of the resulting histograms.
- Why unresolved: While the visualizations are present, the figure captions provide no quantitative metrics (e.g., kurtosis, variance) to definitively state if the distribution shapes differ significantly beyond visual inspection.
- What evidence would resolve it: A quantitative analysis comparing the standard deviation and kurtosis of the weight distributions for DoRA versus LoRA-Dash across all layers.

### Open Question 2
- Question: Do Key Projection (k_proj) layers consistently maintain a wider weight distribution or higher outlier frequency compared to Query and Value projections in Transformer models?
- Basis in paper: [explicit] In Figure 10 (Qwen2.5-32B), the histograms for k_proj often display a visibly wider spread or different axis scales compared to q_proj and v_proj within the same layer index.
- Why unresolved: The paper displays the distributions but does not provide a theoretical or empirical explanation for why the Key projection weights might exhibit different statistical properties than the other attention matrices.
- What evidence would resolve it: A statistical aggregation of weight variance across q_proj, k_proj, and v_proj layers for multiple transformer models (e.g., Qwen, LLaMA, Ovis1.6) to confirm the pattern.

### Open Question 3
- Question: Is there a linear relationship between the rank size (r) and the variance of the learned delta weights in parameter-efficient fine-tuning?
- Basis in paper: [explicit] Figures 17 through 21 illustrate LoRA-Dash weights for ranks 4, 8, 16, 32, and 64, where the x-axis scales (e.g., ±0.02 vs ±0.04) suggest the distribution spread changes with rank.
- Why unresolved: The captions specify the rank but do not explicitly characterize the scaling law governing the weight variance as the rank dimension increases.
- What evidence would resolve it: A line graph plotting the rank size (r) against the measured standard deviation of the weights for a specific layer to determine if the relationship is linear, sub-linear, or super-linear.

## Limitations
- The exact datasets and hyperparameters for fine-tuning experiments (LoRA-Dash, DoRA) are not specified, limiting reproducibility of distributional shifts.
- Evidence for the Gaussianity of all analyzed LFMs comes primarily from visual inspection of histograms, not rigorous statistical testing.
- The paper does not report statistical tests (e.g., Kolmogorov-Smirnov, Shapiro-Wilk) to confirm Gaussianity, leaving some claims based on qualitative assessment.

## Confidence
- **High Confidence**: The observation that pre-trained LFM weights exhibit narrow, symmetric distributions centered near zero, with standard deviations increasing with layer depth.
- **Medium Confidence**: The claim that fine-tuning with LoRA/DoRA introduces bimodal or heavier-tailed distributions, based on visual histogram shifts but without quantitative statistical validation.
- **Medium Confidence**: The hypothesis that attention layers have narrower weight distributions than MLP layers, supported by visual inspection but not statistically tested across all model families.

## Next Checks
1. **Statistical Testing**: Apply formal normality tests (e.g., Shapiro-Wilk, Kolmogorov-Smirnov) to weight distributions across all layers and models to quantitatively confirm Gaussianity.
2. **Hyperparameter Sweep**: Reproduce LoRA and DoRA fine-tuning with varied ranks and learning rates to verify that distributional shifts are consistent and not artifacts of a specific configuration.
3. **Cross-Architecture Validation**: Analyze weight distributions in non-transformer architectures (e.g., ConvNeXt, SAM-h) and models trained with L1 regularization to test the generality of the Gaussian distribution claim.