---
ver: rpa2
title: Continually Adding New Languages to Multilingual Language Models
arxiv_id: '2509.11414'
source_url: https://arxiv.org/abs/2509.11414
tags:
- languages
- arxiv
- language
- lora
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continually adding new languages
  to multilingual language models without access to the original pretraining data,
  which is a common limitation in modern model releases. The authors propose Layer-Selective
  LoRA (LayRA), a method that applies LoRA adapters to only the initial and final
  layers of a transformer while freezing the middle layers.
---

# Continually Adding New Languages to Multilingual Language Models

## Quick Facts
- **arXiv ID:** 2509.11414
- **Source URL:** https://arxiv.org/abs/2509.11414
- **Reference count:** 27
- **Primary result:** Layer-Selective LoRA (LayRA) provides better retention of existing languages while learning new ones compared to full continued pretraining and LoRA on all layers

## Executive Summary
This paper addresses the challenge of continually adding new languages to multilingual language models when the original pretraining data is unavailable. The authors propose LayRA, which applies LoRA adapters to only the initial and final layers of a transformer while freezing the middle layers. This approach is motivated by observations that multilingual models process sequences in three stages: encoding in source language (early layers), reasoning in English (middle layers), and decoding back to source language (final layers). The method is evaluated on adding Galician, Swahili, and Urdu to Llama 3.1 and Qwen 2.5 models, showing LayRA achieves the best tradeoff between retention of previously supported languages and learning of new languages.

## Method Summary
The paper proposes Layer-Selective LoRA (LayRA), which applies LoRA adapters selectively to the first and last layers of a transformer while freezing the middle layers. This approach is based on the observation that multilingual models process sequences in three stages: source language encoding in early layers, English-based reasoning in middle layers, and source language decoding in final layers. By freezing the middle layers that handle reasoning, LayRA aims to preserve the model's existing multilingual capabilities while allowing adaptation to new languages through the encoder and decoder layers. The method is evaluated by adding three low-resource languages (Galician, Swahili, Urdu) to Llama 3.1 and Qwen 2.5 models, comparing against full continued pretraining, LoRA on all layers, and layer-selective full training.

## Key Results
- LayRA achieves the best tradeoff between retention of previously supported languages and learning of new languages
- LayRA outperforms LoRA and full training in retention while remaining competitive in learning
- LayRA can be combined with model arithmetic to add instruction-following capabilities to adapted models without requiring instruction-tuning data in target languages

## Why This Works (Mechanism)
The method works by exploiting the architectural understanding that multilingual models process sequences through distinct stages: early layers handle source language encoding, middle layers perform English-based reasoning, and final layers handle source language decoding. By freezing the middle layers that contain the core reasoning capabilities and only adapting the encoder and decoder layers through LoRA, LayRA preserves the model's existing language understanding while allowing it to learn new language patterns for input and output processing.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that approximates weight updates using low-rank matrices; needed to enable efficient adaptation without full model retraining; quick check: verify rank decomposition maintains task performance
- **Continued pretraining**: The process of further training a model on new data after initial pretraining; needed to understand the baseline approach being improved upon; quick check: ensure training doesn't cause catastrophic forgetting
- **Layer-wise behavior in transformers**: Understanding how different layers in transformers handle different aspects of language processing; needed to justify selective layer adaptation; quick check: analyze attention patterns across layers for different languages
- **Model arithmetic**: The technique of combining models through linear combinations or other operations; needed to add instruction-following capabilities without target language data; quick check: verify combined models maintain expected capabilities
- **Cross-lingual transfer**: The ability of models to transfer knowledge across languages; needed to understand how instruction-following can work without target language data; quick check: test performance across language pairs
- **Catastrophic forgetting**: The phenomenon where models forget previously learned information when trained on new tasks; needed to motivate the retention-focused approach; quick check: measure performance degradation on old tasks after training on new ones

## Architecture Onboarding
- **Component map:** Input -> Encoder Layers (adapted) -> Middle Layers (frozen) -> Decoder Layers (adapted) -> Output
- **Critical path:** The frozen middle layers containing English-based reasoning are critical for preserving existing capabilities while adapting encoder/decoder layers for new languages
- **Design tradeoffs:** Selective adaptation vs. full model training (efficiency vs. comprehensive adaptation), layer selection strategy (empirical observation vs. architecture-specific optimization)
- **Failure signatures:** Poor retention of existing languages (middle layers not properly frozen), inability to learn new languages (encoder/decoder layers not properly adapted), degradation in reasoning capabilities (middle layers inadvertently modified)
- **First experiments:** 1) Test retention of existing languages after LayRA adaptation, 2) Compare learning speed of new languages between LayRA and full training, 3) Analyze layer-wise attention patterns before and after LayRA adaptation

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation is limited to three low-resource languages (Galician, Swahili, Urdu), limiting generalizability to other language families and resource levels
- Experiments focus exclusively on Llama 3.1 and Qwen 2.5 architectures, leaving questions about performance across different model families
- Relies on automatic translation for instruction-following capabilities, which may not capture nuanced language-specific instructions
- Layer selection strategy based on observations from mBERT and XLM-R models but applied to different architectures (Llama 3.1/Qwen 2.5)

## Confidence
- **High confidence**: LayRA provides better tradeoff between retention and learning compared to full training and LoRA across all layers
- **Medium confidence**: Middle layers are primarily responsible for English-based reasoning (based on previous work with different architectures)
- **Medium confidence**: Effectiveness of combining LayRA with model arithmetic for adding instruction-following capabilities without target language data

## Next Checks
1. Evaluate LayRA on a broader range of languages including high-resource languages (e.g., German, Japanese) and languages from different families (e.g., Slavic, Dravidian) to assess generalizability across linguistic diversity.

2. Conduct ablation studies testing different layer selection strategies (e.g., freezing different layer ranges, varying the number of frozen middle layers) to determine optimal configurations for different model sizes and language combinations.

3. Perform human evaluation of the instruction-following capabilities in target languages, comparing models adapted with LayRA+model arithmetic against those instruction-tuned with native language data to quantify the quality gap.