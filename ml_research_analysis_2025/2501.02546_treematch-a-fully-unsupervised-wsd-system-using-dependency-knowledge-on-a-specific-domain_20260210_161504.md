---
ver: rpa2
title: 'TreeMatch: A Fully Unsupervised WSD System Using Dependency Knowledge on a
  Specific Domain'
arxiv_id: '2501.02546'
source_url: https://arxiv.org/abs/2501.02546
tags:
- word
- knowledge
- sense
- words
- disambiguation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents TreeMatch, an unsupervised word sense disambiguation
  (WSD) system adapted for a domain-specific task. It leverages dependency knowledge
  extracted from a domain-specific knowledge base built using web-crawled text from
  environment-related sources.
---

# TreeMatch: A Fully Unsupervised WSD System Using Dependency Knowledge on a Specific Domain

## Quick Facts
- arXiv ID: 2501.02546
- Source URL: https://arxiv.org/abs/2501.02546
- Reference count: 3
- Primary result: TreeMatch achieves 0.506 precision on SemEval 2010 Task 17, barely outperforming the Most Frequent Sense baseline of 0.505

## Executive Summary
TreeMatch is an unsupervised word sense disambiguation (WSD) system designed for domain-specific tasks. It builds a domain-specific knowledge base by crawling environment-related web text, parsing it with Minipar to extract dependency relations, and merging these into a weighted directed graph. The system disambiguates words by parsing both input sentences and WordNet glosses, then computing coherence scores based on dependency structure overlap. Evaluated on SemEval 2010 Task 17, TreeMatch achieves precision of 0.506, marginally better than the Most Frequent Sense baseline, demonstrating that domain-specific knowledge can enhance unsupervised WSD without manual annotation.

## Method Summary
TreeMatch operates through a two-phase process: offline knowledge base construction and online disambiguation. The offline phase involves crawling domain-relevant documents (772 PDFs from ECNC/WWF), cleaning and segmenting text, parsing with Minipar to extract head-dependent pairs, and merging these into a weighted directed graph (2.2M nodes, ~87 connections/node). The online phase parses input sentences and WordNet glosses, computes TreeMatching scores based on dependency overlap weighted by distance from target word, combines with NodeMatching (Lesk-style lexical overlap), and selects the sense with highest coherence score.

## Key Results
- Precision of 0.506 on SemEval 2010 Task 17
- Recall of 0.493 (lower due to parser lemma errors)
- Marginally outperforms Most Frequent Sense baseline (0.505 precision)
- Demonstrates domain-specific knowledge can enhance unsupervised WSD

## Why This Works (Mechanism)

### Mechanism 1: Weighted Dependency Graph as Semantic Knowledge Base
The system builds a domain-specific dependency graph where edge weights reflect co-occurrence frequency, encoding which words tend to appear in dependency relationships within the target domain. This graph provides sufficient signal for unsupervised sense disambiguation by prioritizing frequent, likely valid relations over erroneous ones from noisy web text.

### Mechanism 2: Gloss-Substitution Tree Matching
The correct WordNet sense is identified by measuring which gloss's dependency structure has strongest overlap with the sentence's dependency context. If a word is semantically coherent with its context, substituting it with the correct gloss definition preserves or increases that coherence in dependency space.

### Mechanism 3: Distance-Weighted Context Contribution
Words closer to the target in the dependency tree provide stronger disambiguating signal than distant words. The system assigns weight to each context node inversely proportional to its dependency distance from target (wSi = 1/lSi), prioritizing local syntactic context over peripheral words.

## Foundational Learning

- **Dependency Parsing**: Essential for understanding TreeMatch's architecture, which rests on dependency relations (head-dependent pairs). Quick check: In "Environmental groups protest deforestation," what is the dependency relationship between "protest" and "groups"?
- **WordNet Sense Inventory and Glosses**: The system uses WordNet as its sense inventory; each candidate sense is represented by its gloss (definition). Quick check: Look up "conservation" in WordNet—how many noun senses exist, and what does each gloss emphasize?
- **Lesk Algorithm Baseline**: NodeMatching is explicitly described as a Lesk extension. Understanding word-overlap disambiguation helps see what TreeMatching adds (dependency structure) versus what it retains (lexical overlap). Quick check: Given "The bank denied the loan" and two glosses—"financial institution" vs. "river edge"—which would Lesk select and why?

## Architecture Onboarding

- **Component map**: Corpus Builder -> Parser Pipeline -> Knowledge Base Constructor -> WSD Engine -> Evaluation Layer
- **Critical path**: 
  1. Offline KB Construction (one-time, ~70 days): Domain seed words → crawl → clean → parse → merge → persist KB
  2. Online Disambiguation (1.5 hours for 1398 target words): Input sentence → parse → for each target word → score all glosses → select max
- **Design tradeoffs**: 
  - Unsupervised vs. supervised: Eliminates annotation cost but precision ceiling is low (0.506 barely beats MFS 0.505)
  - Domain-specific KB vs. general KB: Higher domain relevance but requires full rebuild for new domains
  - Minipar accuracy vs. recall: Parser errors cause lemma mismatches with WordNet, reducing recall (0.493 < 0.506 precision)
  - Processing time vs. corpus scale: 3TB raw → weeks of parsing; acceptable for one-time KB build
- **Failure signatures**:
  - Recall gap (precision 0.506, recall 0.493): Minipar lemma extraction errors
  - Precision at or below MFS baseline: KB too sparse for domain
  - Processing timeout during parsing: Minipar lacks multithreading
  - Zero scores for all glosses: KB missing dependency relations for context words
- **First 3 experiments**:
  1. Reproduce SemEval 2010 Task 17 results with TreeMatch-1 configuration
  2. Ablate NodeMatching (Lesk component) to quantify contribution vs. pure dependency matching
  3. KB size sensitivity test with 25%, 50%, 100% of crawled documents

## Open Questions the Paper Calls Out

- Can TreeMatch achieve statistically significant improvement over the Most Frequent Sense baseline in domain-specific WSD tasks?
- How robust is TreeMatch to parsing errors, and what is the impact of parser quality on end-to-end WSD performance?
- How does the size and domain-specificity of the crawled knowledge base affect disambiguation accuracy?
- Does TreeMatch generalize to domains beyond environmental texts?

## Limitations

- Precision improvement over MFS baseline is only 0.001 (0.506 vs. 0.505), practically negligible
- Recall gap (0.493) indicates parser accuracy issues with lemma extraction and WordNet matching
- Resource-intensive knowledge base construction (35 days retrieval, 26 days parsing) limits practical deployment

## Confidence

**High Confidence**: The overall unsupervised WSD architecture using dependency knowledge bases and gloss substitution matching; SemEval 2010 Task 17 evaluation methodology; observation that precision barely exceeds MFS baseline.

**Medium Confidence**: The effectiveness of distance-weighted context contribution in improving disambiguation accuracy; claim that erroneous edges from noisy web text have minimal impact on quality.

**Low Confidence**: Specific hyperparameter configurations for TreeMatch-1, 2, and 3 variants; exact mechanism for resolving conflicts when TreeMatching and NodeMatching favor different senses.

## Next Checks

1. Reproduce SemEval 2010 Task 17 results using TreeMatch-1 configuration to verify precision of ~0.506 ± 0.01 and identify parser-related recall gaps.

2. Ablate NodeMatching component to quantify the contribution of Lesk-style lexical overlap versus dependency matching in the combined scoring.

3. Perform KB size sensitivity analysis by building knowledge bases with 25%, 50%, and 100% of crawled documents to identify minimum viable corpus scale for acceptable performance.