---
ver: rpa2
title: Deterministic World Models for Verification of Closed-loop Vision-based Systems
arxiv_id: '2512.08991'
source_url: https://arxiv.org/abs/2512.08991
tags:
- verification
- world
- state
- system
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of verifying closed-loop vision-based
  control systems, where the high dimensionality of images and difficulty in modeling
  visual environments make traditional approaches computationally intractable. The
  key insight is that stochastic latent variables in generative models introduce unnecessary
  overapproximation error for verification.
---

# Deterministic World Models for Verification of Closed-loop Vision-based Systems

## Quick Facts
- arXiv ID: 2512.08991
- Source URL: https://arxiv.org/abs/2512.08991
- Reference count: 40
- One-line primary result: Achieves F1 scores of 0.92, 0.98, and 0.96 on CartPole, MountainCar, and Pendulum benchmarks with verification times of 1.5-3.5 hours using 64 CPU cores

## Executive Summary
This paper addresses the challenge of verifying closed-loop vision-based control systems by introducing a Deterministic World Model (DWM) that directly maps physical states to generative images, eliminating the overapproximation error introduced by stochastic latent variables in traditional generative models. The approach integrates DWM with Star-based reachability analysis and conformal prediction to provide statistically rigorous verification guarantees that transfer from model to real system. Experiments on three OpenAI Gym benchmarks demonstrate significant improvements in verification precision and F1 scores compared to latent-variable baselines, with particular success in reducing reachable set conservatism.

## Method Summary
The method trains a deterministic decoder `g_θ: S → I` that maps physical states to images without latent variables, combined with a control-difference loss to ensure behavioral consistency with the real system. The world model is integrated into a verification pipeline using StarV for neural network reachability and PyBDR for dynamics integration, with conformal prediction providing statistically rigorous bounds that transfer guarantees from model to real system. The approach partitions the initial state space into grid cells, propagates uncertainty through the closed-loop system using Star sets, and inflates reachable sets by a conformal prediction quantile to ensure coverage of real trajectories with probability ≥ 1-α.

## Key Results
- Achieves F1 scores of 0.92 (CartPole), 0.98 (MountainCar), and 0.96 (Pendulum) on benchmark tasks
- Verification times of 1.5-3.5 hours using 64 CPU cores for T=20 step trajectories
- Conformal prediction bounds (Δ₀.₉₅) of 0.093 (CartPole), 0.031 (MountainCar), and 0.049 (Pendulum)
- Significant reduction in reachable set overapproximation compared to cGAN baseline (CartPole: 0.1645 vs 1.9018 volume growth)

## Why This Works (Mechanism)

### Mechanism 1: Deterministic State-to-Image Mapping Eliminates Latent Overapproximation
Removing stochastic latent variables produces significantly tighter verification bounds by eliminating the need to bound uninterpretable high-dimensional latent perturbations. The DWM implements a pure decoder `g_θ: S → I` that maps each physical state to exactly one image, with the core assumption that camera output is sufficiently deterministic given the physical state.

### Mechanism 2: Control-Difference Loss Enforces Behavioral Consistency
Adding `L_ctrl = ||C(Î) - C(I)||²` to training forces the world model to prioritize features the controller actually uses, reducing trajectory deviation. This addresses the limitation that standard reconstruction loss may preserve irrelevant visual features while distorting control-critical ones.

### Mechanism 3: Star-Based Reachability with Conformal Inflation Transfers Guarantees
Star sets enable tractable propagation of uncertainty through CNNs, while conformal prediction provides statistically rigorous bounds to transfer verification from model to real system. The approach uses affine maps for exact propagation and sound over-approximation for nonlinear activations, with conformal inflation ensuring coverage of real trajectories with probability ≥ 1-α.

## Foundational Learning

- **Concept**: Star Sets for Neural Network Reachability
  - Why needed: Core data structure for propagating uncertainty through the DWM and controller networks
  - Quick check: Given a Star set with center `c = [0, 0]`, basis `V = [[1, 0], [0, 1]]`, and predicate bounds `α₁ ∈ [-1, 1], α₂ ∈ [-0.5, 0.5]`, what is the range of the first coordinate?

- **Concept**: Conformal Prediction for Distribution-Free Bounds
  - Why needed: Provides statistical mechanism to claim verification results on world model apply to real system
  - Quick check: If calibration trajectories come from simulation and test trajectories come from hardware, is the exchangeability assumption satisfied?

- **Concept**: Overapproximation Error Accumulation in Closed-Loop Verification
  - Why needed: Explains why latent variables cause "reachable set explosion" and why per-step tightness matters
  - Quick check: If each time step introduces 5% overapproximation error in reachable set volume, approximately how much error accumulates over 20 steps for a 2D system?

## Architecture Onboarding

- **Component map**: World Model (Dense→ConvTranspose) → Controller (Conv→Dense) → Dynamics (PyBDR) → StarV (Reachability)

- **Critical path**:
  1. Collect (state, image) pairs → Train DWM with `L_rec + λ·L_ctrl`
  2. Collect real trajectories → Compute non-conformity scores → Get `∆_{1-α}`
  3. Partition initial set `S_0` into grid cells → Initialize Star set per cell
  4. Loop T steps: `S_t → ImageStar via DWM → Action Star via Controller → S_{t+1} via dynamics`
  5. Inflate `R_T^{WM}` by `∆_{1-α}` → Check `ĤR_T ⊆ G`

- **Design tradeoffs**:
  - **λ (loss weight)**: Higher values prioritize behavioral consistency over visual fidelity; `λ = 10⁻³` suggested
  - **Initial cell granularity**: Finer cells reduce conservatism but increase parallel computation
  - **α (failure probability)**: Smaller α gives higher confidence but larger `∆_{1-α}`, potentially reducing recall
  - **Star basis dimensionality**: More basis vectors = tighter sets but slower LP solving

- **Failure signatures**:
  - **Exploding reachable sets**: Check for latent variables or numerical errors in Star basis
  - **Zero TNR**: Excessive conservatism from latent perturbations or loose dynamics bounds
  - **High conformal bound**: World model not matching real system; increase `λ` or inspect reconstruction
  - **Precision < 1.0**: Soundness violation; check over-approximation settings in StarV

- **First 3 experiments**:
  1. **Image reconstruction fidelity**: Sample 100 held-out states, generate images, compute weighted MSE and control action error `||C(g_θ(s)) - C(P(s))||`. Target: control error < 0.1.
  2. **Single-step reachability sanity check**: Define small initial Star set, propagate through DWM + controller once, compare reachable action bounds against 1000 sampled actions.
  3. **Conformal calibration validation**: Split trajectory data 80/20, compute `∆_{0.95}` on calibration split, verify that ≥95% of test trajectories stay within inflated reachable tube.

## Open Questions the Paper Calls Out
- **Question**: How can the conservatism introduced by conformal prediction bounds be reduced for systems with high error sensitivity or unstable dynamics?
  - Basis: The paper notes that for dynamics with high error amplification like CartPole, "worst-case error bounds significantly inflate the reachable sets," causing a "large drop in the recall rate."
  - Why unresolved: While the method provides guarantees, the "price of rigor" results in excessive conservatism for certain dynamic systems, and no method is proposed to tighten these bounds for unstable systems.

- **Question**: Can deterministic world models effectively verify systems in partially observable environments where the physical state is not fully available as input?
  - Basis: The method relies on a mapping from physical state $s$ to image $I$, assuming the state is known. Standard generative world models often use latent variables specifically to handle partial observability.
  - Why unresolved: By eliminating latent variables to ensure verifiability, the model may lose the ability to capture hidden state factors necessary for complex, non-Markovian environments.

- **Question**: Does the Star-based reachability analysis scale computationally to high-resolution, photorealistic images required for autonomous driving?
  - Basis: Experiments are limited to low-dimensional 96x96 grayscale images. The authors note that reachability analysis on 64 cores takes 1.5–3.5 hours for these simple benchmarks.
  - Why unresolved: Increasing image resolution and complexity drastically increases state space dimensionality, potentially making ImageStar propagation computationally intractable.

## Limitations
- Dependence on uniform state space coverage for training, which may not hold for complex systems
- Sensitivity to controller-specific biases that the control-difference loss could amplify
- Computational expense of fine-grained grid partitioning (3600-6400 cells)
- Assumption that camera output is deterministic given state, which may fail under variable lighting or occlusions
- Reliance on fixed dynamics models that may not capture real-world disturbances

## Confidence
- **High confidence**: Core claims about deterministic world models improving verification precision for benchmarks with state-dependent camera output
- **Medium confidence**: Control-difference loss mechanism based on improved bound tightness, though limited corpus evidence
- **High confidence**: Star-based reachability with conformal inflation for soundness guarantees, but practical performance depends on exchangeability assumptions

## Next Checks
1. **Latent vs deterministic ablation**: Train equivalent models with and without latent variables on same benchmarks, measure reachable set volume growth over 20 steps, verify latent models show significantly larger overapproximation error
2. **Distribution shift stress test**: Evaluate conformal prediction validity when calibration trajectories come from simulation but test trajectories come from hardware with different lighting conditions; measure recall degradation and bound inflation
3. **Controller robustness analysis**: Systematically vary controller parameters within ±10% of trained values and measure impact on DWM training stability and final verification F1 scores to assess sensitivity to controller-specific biases