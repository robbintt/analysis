---
ver: rpa2
title: Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice
  Conversion
arxiv_id: '2506.04013'
source_url: https://arxiv.org/abs/2506.04013
tags:
- speech
- style
- speaker
- source
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses expressive voice conversion (EVC), aiming to
  transfer both speaker identity and expressive attributes (emotion, intensity, pitch)
  from target to source speech. The core method introduces a non-autoregressive conditional
  variational autoencoder with multilingual mHuBERT-147 units for content representation,
  mixed-layer normalization, cross-attention-based F0 injection, and perturbation-based
  similarity loss to reduce source timbre leakage and improve linguistic-acoustic
  disentanglement.
---

# Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion

## Quick Facts
- arXiv ID: 2506.04013
- Source URL: https://arxiv.org/abs/2506.04013
- Authors: Seymanur Akti; Tuan Nam Nguyen; Alexander Waibel
- Reference count: 0
- One-line primary result: Superior emotion transfer (ECA up to 81.2%) and speaker similarity (SECS up to 83.2%) with lower source leakage (EER as low as 7.4%).

## Executive Summary
This paper addresses expressive voice conversion (EVC) by transferring both speaker identity and expressive attributes (emotion, intensity, pitch) from target to source speech. The core method introduces a non-autoregressive conditional variational autoencoder with multilingual mHuBERT-147 units for content representation, mixed-layer normalization, cross-attention-based F0 injection, and perturbation-based similarity loss to reduce source timbre leakage and improve linguistic-acoustic disentanglement. Results show superior emotion transfer and speaker similarity compared to baselines, with effective cross-lingual EVC and acceptable naturalness despite some pronunciation artifacts.

## Method Summary
The approach uses a conditional VAE with mHuBERT-147 discrete units for content encoding, mixed-layer normalization to inject random style information, and cross-attention for F0 conditioning. A perturbation-based similarity loss enforces robustness to non-linguistic variations. The style encoder combines ECAPA-TDNN with F0 and energy features, while a Transformer normalizing flow maps posterior to prior distributions. Training uses 228 hours of English data with adversarial, feature matching, KL, reconstruction, and similarity losses. HiFi-GAN serves as the vocoder.

## Key Results
- Emotion classification accuracy (ECA) reaches 81.2% on Expresso dataset
- Speaker similarity (SECS) achieves 83.2% on ESD-English
- Equal error rate (EER) drops to 7.4% on LibriTTS, indicating reduced source speaker leakage
- Cross-lingual SSIM reaches 74.8% for English→German conversion
- Word error rate (WER) increases up to 10.59% due to pronunciation artifacts

## Why This Works (Mechanism)

### Mechanism 1: Mixed-Layer Normalization for Style-Agnostic Content Embeddings
Mix-LN computes scale and bias as weighted combinations of original and batch-shuffled style embeddings using a Beta-distributed λ parameter. This forces the content encoder to learn style-invariant representations since the conditioning style is randomized, reducing dependency on original style.

### Mechanism 2: Cross-Attention F0 Injection for Prosody Transfer
Local F0 encoder produces frame-level F0 embeddings; multi-head cross-attention uses content embeddings as query and F0 embeddings as key/value. This allows contextual pitch conditioning rather than uniform addition, capturing target prosodic patterns more effectively than simple summation.

### Mechanism 3: Perturbation-Based Similarity Loss for Content Robustness
Applies pitch range reduction and pitch shifting via Parselmouth; cosine similarity loss minimizes distance between original and augmented embeddings. This enforces similarity between content embeddings from original and augmented audio, reducing sensitivity to non-linguistic variations.

## Foundational Learning

- **Discrete vs. Continuous Speech Units**: Why needed: Discrete tokens eliminate non-linguistic information through quantization, unlike continuous representations that retain acoustic leakage. Quick check: Can you explain why quantizing self-supervised features would remove acoustic information while preserving linguistic content?

- **Conditional Variational Autoencoder (CVAE) for Speech**: Why needed: The architecture learns prior p(z|c) from content and posterior q(z|x_spec) from spectrograms; KL divergence aligns these, and the decoder generates from sampled z conditioned on style. Quick check: During inference, which distribution do you sample from, and how does style conditioning enter the generation process?

- **Normalizing Flow for Distribution Alignment**: Why needed: Maps posterior q to prior p during training; at inference, reverse flow generates style-injected representations from linguistic features alone. Quick check: Why might a Transformer-based flow outperform convolution-only flows for speech?

## Architecture Onboarding

- **Component map**: Source audio → mHuBERT-147 extraction → Quantization → Embedding Layer → Mix Encoder (with Mix-LN) → Cross-attention with Local F0 → Prior Encoder → Style Encoder (ECAPA-TDNN + F0 + energy) → Posterior Encoder → Transformer Flow → HiFi-GAN Generator → Converted speech

- **Critical path**: Training: Source audio → mHuBERT units → Content embedding (Mix-LN + cross-attention F0) → Prior p(z|c); Training: Same audio → Linear spectrogram → Posterior q(z|x_spec) → Flow maps q→p → Generator reconstructs; Inference: Source content + Target style embedding + Target F0 → Reverse flow → Generator → Converted speech

- **Design tradeoffs**: Discrete units enable better disentanglement but increase WER (up to 10.59%) due to pronunciation artifacts; cross-attention F0 improves prosody transfer but adds compute vs. summation; Mix-LN improves disentanglement but requires large, diverse batches

- **Failure signatures**: High WER (>10%) with acceptable SECS indicates discrete units may over-quantize; low ECA (<70%) suggests F0 cross-attention issues; high EER (>12%) indicates source speaker leakage

- **First 3 experiments**: 1) Ablation baseline: Train without cross-attention to establish ECA/SECS gap (expected ~9% ECA drop); 2) Mix-LN validation: Measure intra-speaker style embedding cosine similarity with/without Mix-LN (target >0.75); 3) Cross-lingual sanity check: Test English→German conversion (expect SSIM ~75% but significant WER degradation)

## Open Questions the Paper Calls Out

- **Pronunciation intelligibility**: How can the trade-off between strong feature disentanglement and linguistic intelligibility (high WER) be effectively mitigated? The authors note "Higher WER (up to 10.59%) indicates some pronunciation artifacts" and propose "Future work will focus on enhancing intelligibility."

- **Cross-lingual generalization**: Does incorporating multilingual training data effectively resolve the severe intelligibility drop in cross-lingual conversion? The authors propose "improving cross-lingual performance through multilingual training data" to address the "significant drop in intelligibility" when source language is unseen.

- **Naturalness gap**: Can the non-autoregressive generation process be refined to close the subjective naturalness gap with ground truth speech? Despite superior objective metrics, the model's naturalness MOS (3.37) remains significantly lower than Ground Truth (4.40).

## Limitations

- Pronunciation artifacts from discrete units lead to significantly higher word error rates (up to 10.59%), indicating potential linguistic information loss through quantization
- Limited objective metric coverage with heavy reliance on automatic metrics and minimal correlation validation against human perception
- Generalization uncertainty on unseen languages, with performance on more distant language families untested

## Confidence

- **High confidence**: Speaker leakage reduction (EER 7.4% on LibriTTS) and emotion transfer capability (ECA 81.2%) are well-supported by ablation studies
- **Medium confidence**: Cross-lingual generalization claims are promising but based on single-language pair testing with limited German data
- **Low confidence**: Pronunciation quality claims are questionable given the WER degradation, without human intelligibility testing or continuous-feature baseline comparisons

## Next Checks

1. **Linguistic preservation validation**: Compare WER and pronunciation quality between discrete mHuBERT units and continuous mHuBERT features on the same source content, holding all other components constant

2. **Cross-lingual robustness testing**: Evaluate the model on English→Mandarin/Cantonese conversion, measuring both SSIM (style transfer) and accent preservation/naturalness

3. **Disentanglement verification via adversarial evaluation**: Use an adversarial classifier to measure residual speaker identity information in content embeddings after Mix-LN, targeting <50% speaker classification accuracy while maintaining >90% content classification accuracy