---
ver: rpa2
title: 'AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual
  LLMs'
arxiv_id: '2501.02135'
source_url: https://arxiv.org/abs/2501.02135
tags:
- arxiv
- video
- vllms
- audio
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AVTrustBench, the first comprehensive benchmark
  suite for evaluating the reliability and robustness of audio-visual large language
  models (AVLLMs). The benchmark consists of 600K samples across 9 tasks spanning
  three critical dimensions: adversarial attack, compositional reasoning, and modality-specific
  dependency.'
---

# AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs

## Quick Facts
- arXiv ID: 2501.02135
- Source URL: https://arxiv.org/abs/2501.02135
- Reference count: 40
- 13 state-of-the-art AVLLMs evaluated on 600K samples across 9 tasks, showing most fall significantly short of human-like comprehension

## Executive Summary
This paper introduces AVTrustBench, the first comprehensive benchmark suite for evaluating the reliability and robustness of audio-visual large language models (AVLLMs). The benchmark spans three critical dimensions: adversarial attack, compositional reasoning, and modality-specific dependency, with 600K samples across 9 tasks. The authors evaluate 13 state-of-the-art AVLLMs, revealing significant performance gaps compared to human comprehension. To address these limitations, they propose CAVPref, a model-agnostic calibrated audio-visual preference optimization training strategy that incorporates audio-visual consistency through conditional preference optimization and a robustness module, achieving up to 30.19% improvement across all 9 tasks.

## Method Summary
The method introduces CAVPref, which extends standard DPO by adding audio-visual conditioning terms (Ly, LV, LA) that compute reward differences between winning responses under correct vs. incorrect modality conditioning. A distributional robustness module minimizes worst-case expected loss across distributions to handle category imbalance. The β calibration parameter is computed from semantic similarity scores (CLAP for text, AVSM for audio-visual) to prevent over-penalization of semantically similar preference pairs. The training strategy is evaluated on AVTrustBench, which consists of 600K samples adapted from AVQA, MUSIC-AVQA, and AudioSet datasets.

## Key Results
- CAVPref achieves up to 30.19% improvement across all 9 AVTrustBench tasks compared to baseline models
- The robustness module significantly improves tail categories without compromising overall performance
- Pre-aligned encoders (ImageBind) show superior performance to separate encoders (CLIP+Whisper) in the evaluated models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditional multi-modal preference optimization improves AVLLM reliability by ensuring the model learns associations between correct responses and proper audio-visual conditioning.
- **Mechanism:** CAVPref adds two additional loss terms (LV and LA) that compute reward differences for the same winning response under correct vs. incorrect visual/audio conditioning.
- **Core assumption:** The model can learn to distinguish correct vs. incorrect modality-response pairings when explicitly trained on these contrasts.
- **Evidence anchors:**
  - [section 5.1]: "reward differences must also be computed between the winning responses in the presence and absence of correct audio-visual conditioning"
  - [section 5.2]: "significance of AV conditioning over DPO is particularly evident in tasks like MVIT, MAIT, COT-Swap, MVT, and MAT, where DPO shows only marginal improvement over SFT"
  - [corpus]: DAVE benchmark paper similarly identifies visual bias issues where models ignore non-visual modalities
- **Break condition:** If preference data quality is poor or if winning/losing pairs are semantically indistinguishable, the conditioning signal becomes noisy and degrades performance.

### Mechanism 2
- **Claim:** Distributionally robust optimization via worst-case risk minimization improves performance on under-represented categories without sacrificing overall accuracy.
- **Mechanism:** CAVPref minimizes the worst-case expected loss across distributions Q that remain ρ-close to the data-generating distribution P using exponential weighting: L_R = -λ log(E_P[e^(L/λ)]).
- **Core assumption:** Category imbalance exists in training data and optimizing for average performance systematically neglects tail categories.
- **Evidence anchors:**
  - [section 5.1]: "we observe a non-uniformity in the distribution of categories across the AVQA and MUSIC-AVQA datasets"
  - [section 5.2]: "robustness module significantly improves tail categories without compromising others"
  - [corpus]: Limited direct corpus validation for this specific robustness formulation in AV settings
- **Break condition:** If λ (regularization hyperparameter) is set too high, the model becomes overly conservative; if too low, it reverts to average-case optimization.

### Mechanism 3
- **Claim:** Adaptive β calibration based on semantic similarity between preference pairs prevents over-penalization of semantically similar winning/losing pairs.
- **Mechanism:** β is computed as g(ΔS) = 0.9ΔS + 0.1, where ΔS is the normalized similarity score difference using CLAP scores for text and AVSM for audio-visual inputs.
- **Core assumption:** Semantic similarity metrics (CLAP, AVSM) reliably capture meaningful distinctions between preference pairs.
- **Evidence anchors:**
  - [section 5.1]: "in cases where winning and losing responses are semantically close, β values should be small and vice-versa"
  - [corpus]: CLAP-based selection is accepted in audio-visual literature per the paper's supplementary discussion
- **Break condition:** If similarity metrics fail to capture task-relevant distinctions, β calibration provides misleading signals.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: CAVPref extends DPO; understanding the baseline objective log σ(β[log π_θ(y_w)/π_ref(y_w) - log π_θ(y_l)/π_ref(y_l)]) is essential.
  - Quick check question: Can you explain why DPO eliminates the need for a separate reward model?

- **Concept: Distributionally Robust Optimization (DRO)**
  - Why needed here: The robustness module uses DRO to handle category imbalance via worst-case risk minimization.
  - Quick check question: How does minimizing worst-case risk differ from minimizing average loss when data has heavy-tailed distributions?

- **Concept: KL Divergence Constraints**
  - Why needed here: The robustness objective uses D_KL(Q||P) ≤ ρ to define the feasible set of distributions for worst-case analysis.
  - Quick check question: Why use KL divergence rather than total variation distance for constraining distributional perturbations?

## Architecture Onboarding

- **Component map:** Modality encoders (ImageBind/CLIP for video, Whisper/BEATs for audio) → Bridge network (Q-Former or Perceiver) → LLM backbone → CAVPref loss module

- **Critical path:** The bridge network quality directly impacts CAVPref effectiveness. Q-Former-based bridges outperform simpler bridges because they preserve more local context.

- **Design tradeoffs:**
  - Pre-aligned encoders (ImageBind) vs. separate encoders (CLIP+Whisper): Pre-aligned encoders show superior performance but may reduce modularity
  - Larger LLM backbone: Improves most tasks but yields marginal gains on compositional reasoning (suggests architectural bottleneck, not capacity issue)

- **Failure signatures:**
  - Model responds affirmatively to adversarial questions → lacks negative instruction tuning
  - Similar performance on MVIT vs. MAIT but both poor → modality imbalance in training
  - High performance on base setting but drops sharply on instruction setting → overfitting to specific prompt formats

- **First 3 experiments:**
  1. Replicate zero-shot evaluation on a single task (e.g., MCIT) using VideoLLaMA2 to establish baseline before implementing CAVPref
  2. Ablate individual CAVPref components: train with Ly only, then add LV, then add LA, then add robustness module to measure incremental gains
  3. Sweep λ values (0.5, 0.8, 1.0, 1.2) on a validation split to verify paper's claimed optima before full training runs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CAVPref preference optimization strategy be successfully extended to fine-grained audio-visual tasks, such as temporal detection or spatial segmentation?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "AVTrustBench currently contains coarse-grained samples e.g., QA tasks. Future work can extend this for detection/segmentation."
- **Why unresolved:** The current study validates CAVPref exclusively on multiple-choice QA tasks found in AVTrustBench. It is unknown if the preference optimization framework transfers to dense prediction tasks requiring precise localization.
- **What evidence would resolve it:** Application of the CAVPref training strategy to audio-visual segmentation or event detection benchmarks, showing performance gains similar to those seen in the QA tasks.

### Open Question 2
- **Question:** How can the sensitivity of CAVPref to the quality of preference data be mitigated?
- **Basis in paper:** [explicit] The paper notes: "Although CAVPref incorporates AV associations, it is essentially a preference-based optimization strategy and is therefore sensitive to the quality of preference data."
- **Why unresolved:** While CAVPref includes a robustness module to handle distributional shifts, the fundamental reliance on high-quality preference pairs is identified as a remaining vulnerability that was not fully solved.
- **What evidence would resolve it:** A study measuring the degradation of CAVPref performance when noise is intentionally injected into the preference dataset, or a proposed modification to the loss function that decouples performance from data quality.

### Open Question 3
- **Question:** Does incorporating deformable attention into perceiver resamplers effectively preserve the local context required for robust audio-visual reasoning?
- **Basis in paper:** [explicit] In the discussion of bridge networks, the authors suggest: "Developing a perceiver network with deformable attention preserving local information in the resampler... may be useful."
- **Why unresolved:** The authors identify that current Q-former-based bridges struggle to preserve local context, but the proposed architectural solution is a hypothesis that was not implemented or tested in the paper.
- **What evidence would resolve it:** Implementation of a deformable attention perceiver in an AVLLM and a comparative evaluation on AVTrustBench tasks specifically requiring localization.

## Limitations
- The preference optimization strategy is sensitive to the quality of preference data, which could limit its effectiveness with noisy or ambiguous training pairs
- The benchmark currently focuses on coarse-grained QA tasks, with potential extension needed for fine-grained detection/segmentation tasks
- Critical training hyperparameters (learning rate, batch size, epochs) are unspecified, limiting reproducibility

## Confidence

- **High confidence**: The benchmark construction methodology and zero-shot evaluation results are reproducible given the specified datasets and evaluation protocols. The CAVPref framework architecture and loss formulations are clearly specified.
- **Medium confidence**: The claimed improvements (up to 30.19%) are likely reproducible with proper hyperparameter tuning, though exact values may vary without the full training configuration. The DRO formulation appears sound but needs broader validation.
- **Low confidence**: The semantic similarity calibration (β = 0.9ΔS + 0.1) effectiveness depends on the quality of CLAP/AVSM metrics, which may not generalize well to all AV tasks.

## Next Checks

1. **Replicate CAVPref on a single task**: Train CAVPref on VideoLLaMA2 using only the COT-Swap task with ablated components (Ly → Ly+LV → Ly+LV+LA) to verify incremental improvements and test the β calibration mechanism on semantically similar pairs.

2. **Validate robustness module generalization**: Apply CAVPref to a non-AV multimodal task (e.g., visual-text QA) to test whether the DRO formulation generalizes beyond audio-visual settings and maintains performance improvements.

3. **Stress-test similarity metrics**: Create adversarial preference pairs with subtle semantic differences to evaluate whether CLAP/AVSM-based β calibration can distinguish meaningful from meaningless distinctions, and test alternative similarity metrics if needed.