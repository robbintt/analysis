---
ver: rpa2
title: Spurious Forgetting in Continual Learning of Language Models
arxiv_id: '2501.13453'
source_url: https://arxiv.org/abs/2501.13453
tags:
- uni00000013
- task
- uni00000011
- uni00000014
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and studies "spurious forgetting" in continual
  learning of language models, where performance drops reflect task alignment decline
  rather than true knowledge loss. Through synthetic dataset experiments, the authors
  demonstrate that early optimization steps during new task training can disrupt previously
  established task alignments, particularly in bottom model layers.
---

# Spurious Forgetting in Continual Learning of Language Models

## Quick Facts
- arXiv ID: 2501.13453
- Source URL: https://arxiv.org/abs/2501.13453
- Reference count: 40
- Key result: Performance drops in continual learning often reflect task alignment disruption rather than true knowledge loss

## Executive Summary
This paper identifies "spurious forgetting" in continual learning, where performance degradation on old tasks stems from disrupted task alignment rather than erased knowledge. Through synthetic biography dataset experiments, the authors demonstrate that early optimization steps during new task training can shift model outputs away from old task response formats while leaving core representations intact. The theoretical analysis links this to orthogonal weight updates in bottom model layers. To address this, they propose a simple Freeze strategy that fixes bottom layers, substantially improving task accuracy across multiple continual learning scenarios without storing old data.

## Method Summary
The paper constructs a synthetic Biography dataset with 200K synthetic individuals and 6 attributes each. A randomly initialized language model is pretrained on 100K individuals, then sequentially fine-tuned on QA data for two non-overlapping tasks. The Freeze strategy freezes bottom n layers (including embeddings) by setting requires_grad=False during new task training. Recovery experiments validate spurious forgetting by demonstrating that tiny subsets of old data can restore performance. The method is tested across sequential fine-tuning, safety alignment, instruction tuning, and knowledge editing scenarios.

## Key Results
- Performance drops within first ~150 optimization steps reflect task alignment decline, not knowledge loss
- Recovery experiments show old task accuracy can be restored to >90% with <1% of old data
- Freeze strategy achieves substantial improvements across four continual learning scenarios
- Theoretical analysis links spurious forgetting to orthogonal weight updates in bottom layers

## Why This Works (Mechanism)

### Mechanism 1
Performance degradation on old tasks is often "spurious," driven by disruption of task alignment rather than erasure of underlying knowledge. During initial optimization steps of a new task, weight updates occur in directions nearly orthogonal to the principal components of the old task's feature space. This shifts the model's output activation away from the old task's response format (alignment) while leaving core weight representations (knowledge) largely intact.

### Mechanism 2
Freezing bottom layers prevents accumulation of orthogonal shifts that disrupt old task alignment. Theoretical analysis suggests output shifts accumulate linearly or exponentially through residual layers. By freezing bottom layers where orthogonal updates are most prominent, the bound on final output shift is reduced, forcing the model to reuse existing alignment mechanisms rather than "undoing" them.

### Mechanism 3
Sequential fine-tuning suffers from "Contradictory Optimization Directions" in early training phase. Loss landscape visualization reveals gradient direction required to minimize loss for new task is diametrically opposed to direction required to maintain low loss for old task. Optimizer prioritizes steep descent of new task, causing immediate spike in old task loss.

## Foundational Learning

- **Concept: Orthogonality and Principal Components**
  - Why needed: Core mathematical argument relies on updates being "orthogonal" to principal components of old task's features
  - Quick check: If vector $A$ represents your "knowledge" and vector $B$ is an "update," does adding $B$ change the magnitude of $A$ if $B$ is orthogonal to $A$?

- **Concept: Residual Connections (ResNets/Transformers)**
  - Why needed: Theoretical bound on output shifts is derived specifically for residual network structures
  - Quick check: How does a residual connection affect gradient flow to earlier layers compared to standard feed-forward connection?

- **Concept: Recovery vs. Retention**
  - Why needed: To diagnose "spurious forgetting," must distinguish between ability to perform task (alignment) and presence of information (knowledge)
  - Quick check: Can a model perfectly encode a fact in its weights yet fail to answer related question correctly? Why?

## Architecture Onboarding

- **Component map:** Input Embedding / Bottom Transformer Blocks -> Top Transformer Blocks -> Loss Landscape
- **Critical path:**
  1. Establish Baseline: Pre-train or fine-tune on Task 0 to convergence
  2. Diagnose: Verify "spurious" nature by checking if tiny recovery fine-tune restores performance
  3. Intervention (Freeze): Identify bottom n layers, set requires_grad=False
  4. Sequential Training: Train remaining parameters on Task 1

- **Design tradeoffs:** Stability vs. Plasticity - freezing more layers improves stability but reduces plasticity
- **Failure signatures:** Rapid Zeroing - Task 0 accuracy drops to ~0% within 150 optimization steps; Recovery Success - if Task 0 performance can be restored to >90% with <1% of old data
- **First 3 experiments:**
  1. Reproduce the Drop (SEQ): Fine-tune model sequentially on two non-overlapping tasks, plot Task 0 accuracy during Task 1 training
  2. The Recovery Test: Fine-tune "forgotten" model on very small subset of Task 0 data, verify accuracy restoration
  3. Apply Freeze: Retrain Task 1 with bottom 3 layers frozen, compare Task 0 retention against baseline

## Open Questions the Paper Calls Out

### Open Question 1
How can Freeze strategy be improved to close performance gap with data replay methods without storing old data? The paper notes Freeze significantly improves performance but persistent gap remains compared to replay methods.

### Open Question 2
Does theoretical relationship between orthogonal weight updates and spurious forgetting hold for complete Transformer architecture with non-linearities? Current analysis assumes linear mappings and omits layer normalization, softmax, and activation functions.

### Open Question 3
How can optimal number of frozen bottom layers be determined dynamically for given task or model architecture? Current approach tests fixed values but doesn't provide formula or heuristic for selection.

### Open Question 4
Can gradient-based methods be modified to effectively mitigate "undo alignment" despite diversity of alignment direction? Paper concludes simple gradient projection fails because direction of "undoing" changes rapidly and inconsistently.

## Limitations
- Central claim relies on synthetic data experiments which may not capture real-world task complexity
- Theoretical analysis assumes residual network structures, may not generalize to other architectures
- Optimal number of frozen layers appears task-dependent and requires empirical tuning

## Confidence

- **High Confidence:** Empirical demonstration of rapid task alignment disruption in first 150 optimization steps
- **Medium Confidence:** Orthogonality-based theoretical explanation provides plausible mechanism
- **Medium Confidence:** Freeze strategy effectiveness across multiple continual learning scenarios

## Next Checks

1. **Generalization Test:** Apply Freeze strategy to real-world continual learning benchmark (e.g., TRACE or GLUE) with natural language tasks
2. **Architecture Transfer:** Test whether orthogonal update mechanism and Freeze effectiveness extend to non-transformer architectures like CNNs or RNNs
3. **Recovery Robustness:** Conduct recovery experiments with varying data subset sizes (1%, 5%, 10%) to determine minimum data requirement for diagnosing spurious forgetting