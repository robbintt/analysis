---
ver: rpa2
title: Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining
arxiv_id: '2507.14619'
source_url: https://arxiv.org/abs/2507.14619
tags:
- negative
- retrieval
- mining
- legal
- bi-encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage framework for optimizing legal
  document retrieval in Vietnamese, addressing challenges posed by specialized legal
  language and complex document structures. The approach combines a Bi-Encoder for
  efficient candidate retrieval with a Cross-Encoder for precise re-ranking, both
  fine-tuned on legal texts.
---

# Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining

## Quick Facts
- **arXiv ID:** 2507.14619
- **Source URL:** https://arxiv.org/abs/2507.14619
- **Reference count:** 17
- **Primary result:** Bi-Encoder achieves 97% Exist@90, Cross-Encoder with semi-hard negatives reaches 79.11% MRR@10 on Vietnamese legal documents

## Executive Summary
This paper presents a two-stage framework for legal document retrieval in Vietnamese, addressing challenges posed by specialized legal language and complex document structures. The approach combines a Bi-Encoder for efficient candidate retrieval with a Cross-Encoder for precise re-ranking, both fine-tuned on legal texts. A key innovation is the introduction of the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard negative mining to improve re-ranking performance. Evaluated on the SoICT Hackathon 2024 dataset of 261,446 legal documents, the method demonstrates that optimized data processing, tailored loss functions, and balanced negative sampling are critical for building robust retrieval systems in legal contexts.

## Method Summary
The framework employs a two-stage retrieval pipeline: a Bi-Encoder retrieves top candidates using MultipleNegativesRankingLoss with in-batch negatives, followed by a Cross-Encoder that re-ranks candidates using BCEWithLogitsLoss. A key innovation is the use of semi-hard negative mining—randomly sampling negatives from Bi-Encoder's top-90 candidates (excluding correct answers)—which provides balanced training signals. The method introduces Exist@m as a retrieval metric suitable for pipelines with re-ranking stages, measuring whether at least one correct answer appears in the top-m candidates.

## Key Results
- Bi-Encoder achieves 97% Exist@90, ensuring high recall in the candidate set
- Cross-Encoder with semi-hard negatives reaches 79.11% MRR@10, significantly outperforming baseline approaches
- Semi-hard negative mining (n=10) outperforms both easy negatives (~57-60% MRR@10) and hard negatives (26.89% MRR@10)

## Why This Works (Mechanism)

### Mechanism 1: Semi-Hard Negative Mining for Cross-Encoder Training
Semi-hard negatives provide a balanced training signal—they are distinguishable enough to avoid label confusion but challenging enough to push down scores for plausible-but-incorrect candidates. This accelerates convergence compared to easy negatives, which the model already separates well. The Cross-Encoder trained with BCEWithLogitsLoss benefits from negatives with roughly 0.2–0.5 mean cosine similarity, avoiding the gradient instability caused by near-duplicate hard negatives (>0.8 similarity).

### Mechanism 2: In-Batch Negatives via MultipleNegativesRankingLoss for Bi-Encoder
Using in-batch samples as implicit negatives eliminates the need for manually labeled negative examples and prevents the "all similarities → 1" bias. During training, each query-document pair is treated as positive, and all other documents in the batch become negatives. Larger batch sizes increase the diversity and number of negatives, strengthening the contrastive signal without additional data collection.

### Mechanism 3: Two-Stage Retrieval with Exist@m as Retrieval Metric
Separating retrieval (Bi-Encoder) from re-ranking (Cross-Encoder) with Exist@m as the retrieval metric optimizes each stage for its role—coverage vs. precision. The Bi-Encoder's job is to ensure at least one correct document appears in the candidate set (measured by Exist@90). The Cross-Encoder then orders these candidates for precision (measured by MRR@10). Decoupling metrics prevents over-optimizing the Bi-Encoder for ranking when re-ranking will handle final ordering.

## Foundational Learning

- **Concept: Bi-Encoder vs. Cross-Encoder architectures**
  - Why needed: The entire framework depends on understanding why Bi-Encoders are fast but approximate while Cross-Encoders are slow but precise
  - Quick check: If you need to retrieve from 261,446 documents in under 100ms, which architecture would you use for the first pass? Why can't a Cross-Encoder scale to that?

- **Concept: Contrastive learning and in-batch negatives**
  - Why needed: The Bi-Encoder training relies on MultipleNegativesRankingLoss. Without understanding how in-batch negatives create the contrastive signal, you might misconfigure batch size or choose the wrong loss function
  - Quick check: In a batch of 64 query-document pairs, how many negative examples does each query effectively train against under MultipleNegativesRankingLoss?

- **Concept: Negative mining difficulty spectrum (easy / semi-hard / hard)**
  - Why needed: The paper's key result is that semi-hard negatives outperform both easy and hard. Understanding why requires grasping how similarity to the positive affects gradient magnitude and training stability
  - Quick check: A negative with 0.95 cosine similarity to the query is labeled "irrelevant." What happens to BCEWithLogitsLoss gradients when the model predicts 0.9? Why might this destabilize training?

## Architecture Onboarding

- **Component map:**
  Raw Data (train.csv, corpus.csv) → Preprocessing (Pyvi tokenization, multi-answer separation, full-doc replacement) → Processed Data (train_df, eval_df) → Bi-Encoder Training (MultipleNegativesRankingLoss, batch=64, lr=4e-5, 11 epochs) → Negative Mining (semi-hard from top-90, exclude correct answers, random sample n=10) → Cross-Encoder Training (BCEWithLogitsLoss, lr=2e-5, 2 epochs) → Final Output

- **Critical path:**
  1. Data preprocessing quality: Truncated answers must be replaced with full documents; multi-answer questions must be split
  2. Bi-Encoder negative strategy: Must use MultipleNegativesRankingLoss with sufficiently large batch size
  3. Semi-hard negative selection: Must sample from Bi-Encoder candidates (top-90), not from full corpus
  4. Cross-Encoder negative count: n=10 performs best; n=2 with hard negatives catastrophically fails

- **Design tradeoffs:**
  - Bi-Encoder candidate count (m=90): Higher m increases Exist@m but adds Cross-Encoder latency
  - Negative samples per question (n): n=10 yields best results but increases training data size
  - Hard vs. semi-hard threshold: Paper uses random sampling from top-90; filtering by similarity score might offer more control

- **Failure signatures:**
  - Bi-Encoder outputs all similarities ≈1: Using CosineSimilarityLoss instead of MultipleNegativesRankingLoss, or batch size too small
  - Cross-Encoder MRR@10 < baseline (0.55): Using hard negatives with low n, or negatives sampled from full corpus (too easy)
  - Exist@90 < 0.90 on validation: Bi-Encoder undertrained or wrong loss; check epoch count and learning rate
  - Cross-Encoder training loss oscillates: Hard negatives with high similarity causing gradient instability

- **First 3 experiments:**
  1. Reproduce Bi-Encoder baseline: Train Vietnamese-bi-encoder with MultipleNegativesRankingLoss (batch=64, 11 epochs). Verify Exist@90 ≥ 0.97 on eval_df
  2. Ablate negative mining strategies: Train Cross-Encoder with easy/semi-hard/hard negatives (n=5, seed=42). Confirm semi-hard achieves highest MRR@10
  3. End-to-end pipeline validation: Run inference on held-out queries. Verify final MRR@10 ≥ 0.79

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Exact semi-hard negative sampling implementation details are underspecified (random vs. similarity-filtered sampling)
- Exist@m metric lacks comparison to established recall-at-k metrics
- Cross-Encoder's 2-epoch training schedule is minimal and may not generalize to different datasets

## Confidence
- **High confidence**: Bi-Encoder + Cross-Encoder pipeline architecture, MultipleNegativesRankingLoss usage, semi-hard negatives outperforming hard negatives
- **Medium confidence**: Exact semi-hard negative sampling methodology, Exist@m metric superiority claims, 2-epoch Cross-Encoder training sufficiency
- **Low confidence**: Generalization to non-Vietnamese legal corpora, impact of different tokenization schemes beyond Pyvi, optimal candidate set size beyond m=90

## Next Checks
1. Systematically vary negative sampling strategies (random, similarity-filtered, corpus-wide) and measure impact on Cross-Encoder MRR@10
2. Evaluate Exist@m and Cross-Encoder performance across candidate set sizes m=30, 60, 90, 120
3. Extend Cross-Encoder training to 5, 10, and 20 epochs with early stopping to verify 2 epochs is sufficient