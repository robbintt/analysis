---
ver: rpa2
title: Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine
  Learning
arxiv_id: '2507.23418'
source_url: https://arxiv.org/abs/2507.23418
tags:
- milk
- coconut
- features
- adulteration
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a machine learning system for detecting water
  adulteration in coconut milk using infrared spectroscopy. The system applies preprocessing
  to remove irrelevant spectral bands, then extracts features using Linear Discriminant
  Analysis (LDA) and classifies samples using K-Nearest Neighbors (KNN).
---

# Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning

## Quick Facts
- arXiv ID: 2507.23418
- Source URL: https://arxiv.org/abs/2507.23418
- Reference count: 15
- Key outcome: FTIR spectroscopy combined with LDA feature extraction and KNN classification achieved 93.33% cross-validation accuracy in detecting water adulteration in coconut milk

## Executive Summary
This study presents a machine learning system for detecting water adulteration in coconut milk using infrared spectroscopy. The system applies preprocessing to remove irrelevant spectral bands, then extracts features using Linear Discriminant Analysis (LDA) and classifies samples using K-Nearest Neighbors (KNN). Evaluated on a dataset of FTIR spectra from 42 coconut milk samples, the method achieved 93.33% cross-validation accuracy in distinguishing pure samples from those adulterated with 10% or 20% water. The KNN classifier with LDA features outperformed alternatives, confirming the effectiveness of FTIR spectroscopy combined with machine learning for fast, nondestructive quality assessment of coconut milk.

## Method Summary
The method involves preprocessing FTIR spectral data through backward feature elimination to retain bands between 3150-3840 nm, followed by Linear Discriminant Analysis (LDA) for supervised feature extraction that maximizes class separation. The reduced feature set is then classified using K-Nearest Neighbors (KNN) with k=5 neighbors. The approach was validated on a dataset of 42 coconut milk samples (14 authentic, 14 adulterated with 10% water, 14 adulterated with 20% water) collected from traditional and instant markets, with performance evaluated using 5-fold stratified cross-validation and balanced accuracy as the primary metric.

## Key Results
- KNN with LDA features achieved 93.33% balanced accuracy in classifying coconut milk samples
- The 3150-3840 nm spectral range was identified as most relevant for water adulteration detection
- LDA feature extraction dramatically improved performance from ~44% to 93.33% compared to using original spectral features
- KNN outperformed alternative classifiers including Linear SVM (86.67%) and RBF SVM (91.11%)

## Why This Works (Mechanism)

### Mechanism 1: Supervised Dimensionality Reduction for Class Separability
LDA computes within-class and between-class scatter matrices to find a linear transformation that maximizes the ratio of between-class variance to within-class variance. This process condenses the relevant information from hundreds of spectral bands into a few discriminant features where class clusters are most distinct. The mechanism assumes spectral differences between pure and adulterated samples are linearly separable in a lower-dimensional subspace.

### Mechanism 2: Instance-Based Classification on Compressed Features
KNN classifies new data points by identifying the k nearest training samples in the LDA-transformed feature space and assigning class labels by majority vote. The classifier's success depends on the quality of the feature space created by LDA, where samples of the same class are clustered closely together, making simple Euclidean distance metrics effective.

### Mechanism 3: Discriminative Spectral Region Pruning
Backward Feature Elimination iteratively removes spectral bands that do not contribute to classification accuracy, focusing the model on the 3150-3840 nm range where water adulteration signatures are most pronounced. This assumes the chemical information for detecting water adulteration is concentrated in specific, identifiable spectral regions, and bands outside these regions add noise.

## Foundational Learning

- **Concept: Dimensionality Reduction (Curse of Dimensionality)**
  - Why needed: The raw data has 729 features but only 42 samples, risking overfitting to noise without reduction
  - Quick check: Why would training a classifier on all 729 spectral bands with only 42 samples likely lead to poor performance on new, unseen samples?

- **Concept: Fourier Transform Infrared (FTIR) Spectroscopy**
  - Why needed: This is the core sensing technology that measures absorbance to create a chemical "fingerprint"
  - Quick check: What does an FTIR spectrum represent, and why would adding water cause a consistent, detectable change in the absorbance pattern?

- **Concept: Cross-Validation**
  - Why needed: With a tiny dataset (n=42), a simple train/test split is unreliable
  - Quick check: Why is it necessary to rotate through different subsets of the data for training and testing, rather than just using a single random split?

## Architecture Onboarding

- **Component map:** Input (FTIR Spectral Data) -> Preprocessing (Backward Feature Elimination) -> Feature Extraction (LDA) -> Classification (KNN, k=5) -> Output (Class Label)

- **Critical path:** The Feature Extraction (LDA) step is critical - classifiers on original/PCA features failed (<44% accuracy), while classifiers on LDA features succeeded (>86% accuracy)

- **Design tradeoffs:**
  - LDA vs. PCA: LDA (supervised) vs PCA (unsupervised) - LDA uses class labels to maximize separation, ideal for classification but requires labeled data
  - KNN vs. SVM: KNN selected for performance with LDA features - simple but can be computationally expensive at inference time on massive datasets

- **Failure signatures:**
  - Performance collapse on PCA/Original features indicates raw data is too noisy without LDA transformation
  - "Singular Matrix" error in LDA would occur if number of features exceeds number of samples
  - Confusion between Authentic and Adulterated10 suggests subtle spectral changes at low adulteration levels are hard to distinguish

- **First 3 experiments:**
  1. Reproduce Core Result: Implement BFE -> LDA -> KNN pipeline and verify 93.33% cross-validation accuracy
  2. Ablation Study: Retrain LDA+KNN model without BFE step to quantify spectral region pruning contribution
  3. Stress Test: Perform "leave-one-source-out" test to check if model learns universal adulteration signals or overfits to sample types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating spectral data from visible near-infrared or ultraviolet regions improve classification accuracy compared to the FTIR-only approach?
- Basis in paper: The discussion states, "Using spectral data from other electromagnetic spectrum regions, such as visible near-infrared and ultraviolet, may improve the current detection system performance."
- Why unresolved: The current study restricted input data to FTIR spectral bands (2500–4000 nm) and did not test other sensor modalities
- What evidence would resolve it: Comparative experiments evaluating the KNN/LDA model performance on datasets that fuse FTIR data with UV/Vis-NIR spectral bands

### Open Question 2
- Question: Can the proposed LDA-KNN pipeline maintain high accuracy when detecting adulteration in food products other than coconut milk?
- Basis in paper: The conclusion notes, "Our next step is applying the proposed method for detecting and quantifying adulteration in other food products."
- Why unresolved: The system has only been validated on a specific dataset of coconut milk samples and its generalizability to other substrates is unproven
- What evidence would resolve it: Successful application of the identical preprocessing and classification pipeline to spectral datasets from different adulterated liquids

### Open Question 3
- Question: Is the system effective at detecting water adulteration at concentrations lower than 10% or identifying non-water adulterants?
- Basis in paper: The method was evaluated only on samples with "high" (10% and 20%) water concentrations, and the introduction mentions other adulterants like corn flour which were not tested
- Why unresolved: It is unclear if the spectral features extracted by LDA are sensitive enough to detect minor dilutions or chemically distinct contaminants
- What evidence would resolve it: Classification performance metrics derived from testing the model on samples adulterated at concentrations <10% or with substances other than water

## Limitations
- Small sample size (n=42) may limit generalizability to different adulteration types and coconut milk sources
- Only water adulteration tested - unknown if system works for other contaminants like starch or sweeteners
- Specific wavelength range (3150-3840 nm) may not capture adulteration signatures for other contaminants
- Limited preprocessing details may affect reproducibility

## Confidence
- **High Confidence**: The core classification pipeline (BFE → LDA → KNN) achieving 93.33% accuracy on the specified task
- **Medium Confidence**: The generalization of these results to different adulteration types and coconut milk sources
- **Low Confidence**: The specific claim that the 3150-3840 nm range is universally optimal for water adulteration detection in coconut milk

## Next Checks
1. **Generalization Test**: Evaluate the trained model on coconut milk samples adulterated with substances other than water (e.g., starch, sucrose, whey powder) to assess whether the 3150-3840 nm range remains optimal or if different spectral regions become important

2. **Cross-Source Validation**: Apply the model to coconut milk samples from completely independent sources (different countries, processing methods) to determine if the LDA transformation and KNN classifier maintain their performance without retraining

3. **Alternative Preprocessing Comparison**: Implement and compare the BFE approach against other preprocessing methods (e.g., Savitzky-Golay smoothing, standard scaling, SNV) to quantify whether the specific pruning strategy is essential to the reported performance or if simpler approaches could achieve similar results