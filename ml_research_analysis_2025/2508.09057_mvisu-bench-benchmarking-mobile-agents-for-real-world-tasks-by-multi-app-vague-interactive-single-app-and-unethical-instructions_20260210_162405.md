---
ver: rpa2
title: 'MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App,
  Vague, Interactive, Single-App and Unethical Instructions'
arxiv_id: '2508.09057'
source_url: https://arxiv.org/abs/2508.09057
tags:
- tool
- mobile
- instructions
- multi-app
- unethical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MVISU-Bench, a bilingual benchmark designed
  to evaluate mobile agents on real-world tasks across five categories: Multi-App,
  Vague, Interactive, Single-App, and Unethical instructions. To address limitations
  in existing benchmarks and agent frameworks, the authors collect 404 tasks across
  137 mobile applications through user questionnaires and manual filtering.'
---

# MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions

## Quick Facts
- **arXiv ID:** 2508.09057
- **Source URL:** https://arxiv.org/abs/2508.09057
- **Reference count:** 40
- **Primary result:** Introduces MVISU-Bench with 404 tasks across 137 apps and Aider module improving success rates by 19.55% overall.

## Executive Summary
This paper introduces MVISU-Bench, a bilingual benchmark designed to evaluate mobile agents on real-world tasks across five categories: Multi-App, Vague, Interactive, Single-App, and Unethical instructions. To address limitations in existing benchmarks and agent frameworks, the authors collect 404 tasks across 137 mobile applications through user questionnaires and manual filtering. They also propose Aider, a lightweight plug-and-play module fine-tuned on Qwen2.5-VL-3B, which improves success rates by 19.55% overall and notably by 53.52% for unethical and 29.41% for interactive instructions. Experiments on three frameworks with six backbone models reveal that current agents struggle with interactivity and safety, while closed-source models outperform open-source ones, especially on English instructions. MVISU-Bench and Aider are publicly released to support future research.

## Method Summary
The authors constructed MVISU-Bench through a multi-stage process: first collecting 2,200 user responses to identify five task categories; then generating instructions using GPT-4o with domain-expert prompts; filtering through executability, deduplication, logical validation, and risk-control rules; and finally human verification. They propose Aider, a lightweight module fine-tuned on 1,000 screenshot-instruction pairs to classify scenarios into five types (A–E) and trigger role-specific responses including risk mitigation, intent clarification, and interactive assistance. The benchmark was evaluated across three frameworks (Mobile-Agent, Mobile-Agent-V2, Mobile-Agent-E) with six backbone models, measuring success rate, API calls, duration, cost, steps, input tokens, and operation time.

## Key Results
- Aider improves overall success rates by 19.55% compared to state-of-the-art, with 53.52% improvement for unethical instructions and 29.41% for interactive instructions
- Closed-source models (Claude-3.5-Sonnet) outperform open-source models, achieving 65.71% success on unethical tasks versus 11.43-28.57% for other models
- English instructions show 14.16% higher success rates than Chinese tasks, highlighting language-specific performance gaps
- Current mobile agents achieve near 0% success on interactive instructions, indicating critical limitations in user-agent interaction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aider's role-based intervention improves task success by dynamically correcting agent behavior at critical decision points.
- **Mechanism:** A fine-tuned Qwen2.5-VL-3B classifies scenarios into five types (A–E) and triggers role-specific responses: Risk Mitigator rejects unethical instructions; Intent Clarifier requests user refinement for vague inputs; Interactor prompts for missing personal information; Tips Prompter provides contextual guidance for complex interfaces.
- **Core assumption:** Current backbone models (even closed-source) lack built-in mechanisms for intent clarification and risk detection; a lightweight auxiliary model can fill this gap.
- **Evidence anchors:** Aider "improves overall success rates by 19.55% compared to the current SOTA," with 53.52% improvement for unethical instructions; ablation shows SR increasing from 45.30 → 64.85 when Aider is added to Mobile-Agent-V2 + Claude-3.5-Sonnet.
- **Break condition:** If backbone models incorporate native intent-clarification and safety filters, Aider's marginal utility diminishes.

### Mechanism 2
- **Claim:** User-anchored benchmark construction surface real-world failure modes absent from synthetic benchmarks.
- **Mechanism:** A questionnaire (N=2,200) identified five task categories reflecting actual user expectations; instructions were generated via GPT-4o with domain-expert prompts, then filtered through executability, deduplication, logical validation, and risk-control rules before human verification.
- **Core assumption:** User-reported task types (Multi-App 25%, Vague 20%, Interactive 17%, Unethical 17%) represent realistic deployment distributions.
- **Evidence anchors:** Benchmark includes "404 tasks across 137 mobile applications through user questionnaires and manual filtering"; pipeline reduces ~2,000 generated instructions → ~500 filtered → 404 final verified instructions.
- **Break condition:** If user expectations shift significantly (e.g., new interaction paradigms), the category distribution becomes outdated.

### Mechanism 3
- **Claim:** Closed-source models outperform open-source on English tasks; Qwen2.5-VL excels on Chinese tasks and unethical detection.
- **Mechanism:** Claude-3.5-Sonnet shows superior intent understanding and safety alignment; Qwen2.5-VL benefits from Chinese training data and inherent protective mechanisms for unethical content.
- **Core assumption:** Performance gaps stem from training data scale and alignment procedures, not architecture alone.
- **Evidence anchors:** "The number of successful English tasks is 798, which is 14.16% more than that of Chinese tasks"; Claude-3.5-Sonnet achieves 65.71% unethical SR vs. 11.43–28.57% for other closed-source models in English.
- **Break condition:** If open-source models close the training-data gap, performance parity may emerge.

## Foundational Learning

- **Concept: Mobile Agent Framework Architecture**
  - **Why needed here:** Understanding how perception (screenshot analysis), planning (task decomposition), and action (UI operations via ADB) interact is essential for integrating Aider at the right intervention points.
  - **Quick check question:** Can you diagram where Aider hooks into Mobile-Agent-V2's planning loop?

- **Concept: LVLM Fine-Tuning for Classification Tasks**
  - **Why needed here:** Aider is fine-tuned on 1,000 screenshot-instruction pairs to classify scenarios; understanding LoRA-based fine-tuning and classification head design helps replicate or extend the module.
  - **Quick check question:** What loss function would you use for Aider's five-class classification task?

- **Concept: Benchmark Evaluation Metrics for Agents**
  - **Why needed here:** Success Rate, API Calls, Duration, Cost, Steps, Input Tokens, and Operation Time provide multi-dimensional performance views; knowing which metrics matter for different stakeholders guides interpretation.
  - **Quick check question:** Why might a high SR but high Cost still be unacceptable for production deployment?

## Architecture Onboarding

- **Component map:** Aider Module (Qwen2.5-VL-3B fine-tuned for scenario classification) → Four Roles (Risk Mitigator, Intent Clarifier, Interactor, Tips Prompter) → Three Interactive Interfaces (Frontend + backend + ADB bridge) → Baseline Frameworks (Mobile-Agent, Mobile-Agent-V2, Mobile-Agent-E)

- **Critical path:** Load screenshot + user instruction → Aider classifies scenario → Type A (unethical) → terminate; Type C (vague) → request clarification; Type B (needs info) → prompt user → Type D (complex interface) → inject tips; Type E (default) → proceed with standard planning → Planner executes actions → Aider re-invokes if stuck or uncertain

- **Design tradeoffs:** Invocation accuracy vs. latency: Aider adds ~1.4 seconds per operation time (54.6s → 56.01s OT) but reduces total duration by ~27s via fewer error loops; Model size vs. integration ease: 3B model is lightweight but may have lower classification accuracy than larger models (94.90% accuracy vs. potential gains from 7B)

- **Failure signatures:** Over-triggering: Aider classifies routine tasks as "complex" and injects unnecessary tips, slowing execution; Under-triggering: Fails to detect unethical or interactive scenarios, allowing harmful actions or dead-ends; Language mismatch: Poor performance on non-English/Chinese instructions if not represented in training data

- **First 3 experiments:** 1. Baseline Aider integration: Add Aider to Mobile-Agent-V2 with Claude-3.5-Sonnet; measure SR, OT, and DT on all 404 tasks; 2. Ablation by role: Disable each of the four roles individually to quantify contribution; 3. Cross-framework transfer: Deploy Aider to Mobile-Agent-E and measure whether gains replicate or framework-specific tuning is needed

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can mobile agent architectures be redesigned to support robust, dynamic user interaction, given that current state-of-the-art frameworks consistently fail (achieving near 0% success) on interactive instructions?
- **Basis in paper:** [explicit] The authors report in Section 5.2 that "almost all models exhibited 0 success rate for interactive instructions" and conclude in Section 5.4 that there is an "urgent need for advances in user–agent interaction."
- **Why unresolved:** Current frameworks lack mechanisms to pause execution and query users for credentials or clarifications when encountering login screens or ambiguous states, preferring to hallucinate inputs or fail silently.
- **What evidence would resolve it:** Demonstrating a framework that achieves >50% success on "Interactive" tasks (e.g., "Log in to Uber") by autonomously managing secure data handoffs without hard-coding credentials.

### Open Question 2
- **Question:** How can the high computational cost and latency of VLM-based mobile agents be reduced to meet the efficiency requirements of real-world consumer use?
- **Basis in paper:** [explicit] Section 5.4 highlights that "The high execution times and costs of these agents make them challenging for real-world use," noting average step times of 39.1 seconds and costs up to $0.21 per task, which users find prohibitive compared to manual operation.
- **Why unresolved:** The reliance on large, closed-source models (e.g., GPT-4o) for reasoning incurs high API latency and financial costs, while smaller open-source models suffer significant performance drops.
- **What evidence would resolve it:** Development of efficient, on-device models or hierarchical planning systems that reduce average operation time (OT) and cost per task by at least 50% while maintaining current success rates.

### Open Question 3
- **Question:** Can safety alignment for unethical instructions be embedded intrinsically into mobile VLMs, rather than relying on external "plug-and-play" mitigation modules?
- **Basis in paper:** [inferred] While the authors introduce "Aider" to improve success rates on unethical instructions by 53.52% (Table 5), the baseline models inherently struggle to refuse these tasks (Section 5.2), suggesting a fundamental lack of safety alignment in the base VLMs.
- **Why unresolved:** Current base models prioritize instruction following over safety constraints in the context of mobile UI manipulation, requiring external intervention to detect and halt harmful operations like searching for illegal content.
- **What evidence would resolve it:** A base VLM that intrinsically rejects unethical mobile commands (e.g., "Search for pirated movies") with high accuracy without requiring an auxiliary module to filter the prompt or action.

## Limitations

- The benchmark creation process involves significant human filtering, introducing potential subjectivity despite expert verification
- Performance differences between open-source and closed-source models may reflect dataset biases rather than fundamental architectural limitations
- The unethical instruction category remains inherently subjective in evaluation, complicating fair assessment

## Confidence

- **High Confidence:** The benchmark construction methodology (questionnaire → generation → filtering → verification) is transparent and reproducible; the improvement claims for Aider on success rates are well-supported by ablation studies and direct comparisons
- **Medium Confidence:** The categorization of tasks into Multi-App, Vague, Interactive, Single-App, and Unethical is reasonable but could evolve as user expectations change; the performance differences between open-source and closed-source models are consistent but may narrow with future model developments
- **Low Confidence:** The interpretation of "unethical" instruction handling as purely a technical classification problem may oversimplify complex ethical considerations; the real-world applicability of the benchmark may be limited by its focus on Chinese mobile apps and specific user demographics

## Next Checks

1. **Cross-Lingual Validation:** Test Aider on a diverse multilingual instruction set (beyond English and Chinese) to assess generalization and identify potential language-specific failure modes

2. **Real-World Deployment Trial:** Conduct a user study with actual mobile users performing tasks in their own devices, comparing agent performance with and without Aider to validate benchmark results against real-world usage patterns

3. **Ethical Classification Robustness:** Systematically evaluate Aider's unethical instruction detection across diverse cultural contexts and with input from ethics experts to ensure the module's safety mechanisms are robust and culturally appropriate