---
ver: rpa2
title: Evaluating Language Models' Evaluations of Games
arxiv_id: '2510.10930'
source_url: https://arxiv.org/abs/2510.10930
tags:
- game
- reasoning
- player
- games
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new paradigm for evaluating AI systems by
  focusing on their ability to assess games rather than just play them. The authors
  introduce a formalism for evaluating such evaluations and conduct experiments using
  a dataset of over 100 novel board games and 450 human judgments.
---

# Evaluating Language Models' Evaluations of Games

## Quick Facts
- arXiv ID: 2510.10930
- Source URL: https://arxiv.org/abs/2510.10930
- Reference count: 40
- Primary result: Reasoning models better align with human game evaluations than non-reasoning models, but this alignment weakens as models approach game-theoretic optimality

## Executive Summary
This paper proposes evaluating AI systems by their ability to assess games rather than just play them. The authors introduce a formalism for evaluating such evaluations and conduct experiments using a dataset of over 100 novel board games and 450 human judgments. They compare language and reasoning models' evaluations of expected payoff and funness against human judgments and game-theoretic optimal baselines. Results show that reasoning models are more aligned with human evaluations than non-reasoning models, but a non-monotonic relationship exists: as models approach game-theoretic optimality, their alignment with human data weakens. Funness evaluations are more inconsistent across models, reflecting the difficulty of quantifying this metric.

## Method Summary
The study evaluates 121 novel board game variants on expected payoff and funness using human judgments (20 participants per game) and multiple language/reasoning models. Models are prompted to evaluate games through standardized system and task prompts, with 20 rollouts per game using temperature settings of 1.0 for reasoning models and 0.7 for others. Results are compared against human means and game-theoretic optimal values where computable. Reasoning traces are analyzed for open models, and token usage is tracked to assess resource efficiency.

## Key Results
- Reasoning models (with chain-of-thought) show higher R² alignment to human judgments than direct-output models for payoff evaluation
- As models improve toward game-theoretic optimality, their fit to human judgments weakens in a non-monotonic pattern
- Funness evaluation shows more inconsistency across models, with similar factor identification but divergent final judgments
- Reasoning models exhibit highly variable and unpredictable resource usage with no clear correlation to game complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning models better align with human game evaluations than direct-output models because intermediate computation enables simulation-like reasoning
- **Mechanism:** Chain-of-thought allows models to decompose evaluative queries into subqueries, then aggregate evidence toward a final judgment
- **Core assumption:** Models that reason through intermediate steps are implicitly engaging in goal-directed simulation similar to cognitive models
- **Evidence anchors:** Abstract finding that reasoning models are more aligned to people; GPT-5 shows R²=0.82 to game-theoretic optimal vs. GPT-4 (Direct) at R²=0.31

### Mechanism 2
- **Claim:** Human-game-theoretic alignment follows a non-monotonic curve because optimal rationality diverges from bounded human reasoning
- **Mechanism:** As models improve toward game-theoretic optimality, they first capture human intuitions but then "overshoot" toward perfectly optimal play, reducing human alignment
- **Core assumption:** Humans are semi-rational—they use heuristics and bounded compute, not optimal game-theoretic strategies
- **Evidence anchors:** Abstract noting that as models get closer to game-theoretic optimal, their fit to human data weakens; degradation from o3 to GPT-5 despite improved optimal alignment

### Mechanism 3
- **Claim:** Funness evaluation is harder than payoff evaluation because it requires both factor identification and subjective aggregation
- **Mechanism:** Funness queries are "hard to quantify"—models must first infer relevant factors, measure each, then aggregate with implicit weights
- **Core assumption:** Funness is not a single objective property but a multi-factor subjective judgment with no canonical decomposition
- **Evidence anchors:** Abstract observation of more "jaggedness" across models for assessing funness; models mention similar factors but arrive at vastly different funness judgments

## Foundational Learning

- **Concept: Game-theoretic optimal (Nash equilibrium, minimax)**
  - **Why needed here:** The paper uses game-theoretic payoff as a baseline for "optimal rationality"
  - **Quick check question:** On a 3×3 Tic-Tac-Toe board, what is the game-theoretic outcome with perfect play?

- **Concept: Chain-of-thought (CoT) reasoning**
  - **Why needed here:** The paper distinguishes "direct" models from those using CoT or native reasoning
  - **Quick check question:** Why might asking a model to "think step by step" change its output on a multi-step problem?

- **Concept: R² (coefficient of determination) as alignment metric**
  - **Why needed here:** The paper quantifies model-human alignment and model-optimal alignment via R²
  - **Quick check question:** If model A has R²=0.9 to humans and model B has R²=0.5, what does that imply about their predictions?

## Architecture Onboarding

- **Component map:** Game corpus (121 novel board game variants) -> Human judgments (20 participants per game, 450 total) -> Model set (non-reasoning and reasoning models) -> Baseline agents (Random, Intuitive Gamer, Expert Gamer, MCTS) -> Evaluation metrics (R², accuracy, deviation, Wasserstein distance)

- **Critical path:** Load game definitions -> Prompt models with standardized system/task prompts -> Sample 20 rollouts per model per game -> Compute aggregate predictions -> Compare to human means, game-theoretic optimal, and baseline agents -> Analyze reasoning traces (for open models)

- **Design tradeoffs:** Novice vs. expert human data (paper uses novice judgments); prompt sensitivity (results depend on exact phrasing); reasoning token budget (closed models don't expose traces); game coverage (only two-player competitive grid games)

- **Failure signatures:** Direct models default to "fair" (non-reasoning models cluster around 0.0 payoff); reasoning models overshoot (GPT-5 shows high optimal alignment but reduced human alignment); funness inconsistency (models mention similar factors but output divergent scores); unpredictable token usage (no clear correlation between game novelty and reasoning tokens)

- **First 3 experiments:**
  1. Replicate payoff alignment across model families by running the 121-game evaluation on your target model; compute R² to human and optimal baselines
  2. Ablate reasoning effort by testing "low/medium/high" settings on models with adjustable reasoning; measure whether increased reasoning improves optimal alignment but degrades human alignment
  3. Trace analysis on open models by extracting and coding reasoning traces to test whether explicit simulation correlates with higher alignment to game-theoretic optimal vs. human judgments

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Should AI systems be optimized to align with "optimally rational" game-theoretic evaluations or with human judgments, given the observed non-monotonic relationship between the two?
- **Basis in paper:** The authors ask, "Whose evaluations should models be evaluated against: people... or the 'optimally rational' evaluation?" noting that as models approach game-theoretic optimality, their alignment with human data weakens
- **Why unresolved:** The paper demonstrates a trade-off where increasing sophistication improves game-theoretic accuracy but degrades human alignment, creating a conflict in design goals
- **What evidence would resolve it:** Comparative studies evaluating the utility of "rational" vs. "human-aligned" models in collaborative tasks to determine which alignment yields better outcomes

### Open Question 2
- **Question:** Are reasoning models engaging in latent game simulation during evaluation, even when explicit simulation is absent from their chain-of-thought traces?
- **Basis in paper:** The authors ask, "Are language and/or reasoning models internally engaging in some kind of game simulation... beyond what is written explicitly in chain-of-thought rationales?"
- **Why unresolved:** While reasoning models produced results similar to search-based methods, explicit trace analysis showed low rates of simulation, suggesting internal mechanisms remain opaque
- **What evidence would resolve it:** Causal tracing or probing of internal model activations to detect representations of board states and game trees during the evaluation process

### Open Question 3
- **Question:** How can models implement resource-rational meta-reasoning to dynamically adapt compute usage based on problem complexity?
- **Basis in paper:** The authors highlight "highly variable and unpredictable resource usage" and ask, "What cost are we willing to pay... and how should we balance resource demands?"
- **Why unresolved:** The study found no measurable relationship between game novelty/complexity and reasoning token usage, indicating current models cannot efficiently allocate cognitive resources
- **What evidence would resolve it:** Development of models that correlate reasoning token expenditure with quantifiable problem difficulty metrics without performance loss

## Limitations
- The human judgment dataset is not publicly available in the appendix, limiting independent validation of R² alignment values
- Game-theoretic optimal baselines are only computable for 78 of 121 games, creating potential selection bias in results
- Reasoning token traces are unavailable for closed models (GPT-5, o3), making it impossible to verify the proposed simulation mechanism
- The non-monotonic relationship between model optimality and human alignment is based on a single model progression without broader testing

## Confidence

- **High confidence:** Reasoning models show better human alignment than non-reasoning models (Mechanism 1 supported by multiple R² comparisons in Table 1)
- **Medium confidence:** Non-monotonic relationship exists (Mechanism 2 supported by single progression, needs broader testing)
- **Medium confidence:** Funness evaluation shows higher variance across models (Mechanism 3 supported by Table 2 qualitative patterns, lacks quantitative variance analysis)

## Next Checks

1. **Replicate the 121-game evaluation** on your target model(s) and compute R² alignment to human judgments (or to a held-out subset if available)
2. **Test reasoning effort ablation** by varying "low/medium/high" settings on o3/GPT-5 for a subset of games to verify whether increased reasoning improves optimal alignment while degrading human alignment
3. **Extract and analyze reasoning traces** from open models (DeepSeek-R1, Gemini) to determine if explicit simulation correlates with higher game-theoretic optimal alignment versus human alignment