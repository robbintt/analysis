---
ver: rpa2
title: 'Freeze then Train: Towards Provable Representation Learning under Spurious
  Correlations and Feature Noise'
arxiv_id: '2210.11075'
source_url: https://arxiv.org/abs/2210.11075
tags:
- core
- spurious
- noise
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of representation learning under
  spurious correlations and feature noise, where core features related to the outcome
  may be overshadowed by spurious features. The authors provide theoretical analysis
  showing that core features are only learned well when their associated non-realizable
  noise is smaller than that of spurious features, which is not necessarily true in
  practice.
---

# Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise

## Quick Facts
- arXiv ID: 2210.11075
- Source URL: https://arxiv.org/abs/2210.11075
- Reference count: 40
- Key outcome: FTT outperforms ERM, IRM, JTT, and CVaR-DRO across spurious correlation datasets, with a substantial accuracy improvement of 4.5% when feature noise is large.

## Executive Summary
This paper addresses the challenge of learning robust representations under spurious correlations and feature noise, where core features related to the outcome may be overshadowed by spurious features. The authors propose Freeze then Train (FTT), an algorithm that first freezes certain salient features using unsupervised learning (e.g., PCA) and then trains the remaining features using supervised learning (e.g., ERM). Theoretically, FTT is shown to preserve features more beneficial for test-time probing. Empirically, FTT demonstrates significant improvements over standard ERM and other methods on datasets like Waterbirds and CelebA, particularly when feature noise is large.

## Method Summary
The FTT algorithm consists of two main stages: (1) Unsupervised Freeze: A PCA transformation is applied to the entire training set features, and a fraction $p$ of the top principal components are frozen as $M_{ul}$. (2) Supervised Train: The remaining $(1-p)$ fraction of features, $M_{sl}$, are trained using ERM while $M_{ul}$ remains frozen. During test-time, a linear probe (Logistic Regression) is retrained on a balanced validation set to evaluate the robustness of the learned representation.

## Key Results
- FTT outperforms ERM, IRM, JTT, and CVaR-DRO on spurious correlation datasets like Waterbirds and CelebA.
- FTT shows a substantial accuracy improvement of 4.5% when feature noise is large, particularly in controlled synthetic settings.
- The method is theoretically grounded, with proofs showing that FTT preserves features more beneficial for test-time probing.

## Why This Works (Mechanism)

### Mechanism 1: Noise-Dependent Feature Suppression in ERM
Standard ERM may fail to learn core features when the "non-realizable noise" (label noise) associated with core features exceeds the noise of spurious features. The model allocates representation capacity based on signal-to-noise ratios, suppressing core feature representation to minimize immediate training loss if core noise is high relative to spurious noise.

### Mechanism 2: Unsupervised Salient Feature Buffering
Freezing a fraction of features learned via unsupervised methods (e.g., PCA) acts as a buffer that preserves core information independent of label noise. If core features possess significant variance, PCA will capture them in the top principal components, preventing the supervised gradient from overwriting these core representations even if the supervised signal is noisy or spurious.

### Mechanism 3: Hybrid Linear Probing Recovery
The combination of frozen unsupervised features and trained supervised features allows the final linear probe to select the optimal mixture of features, circumventing the "approximation error" vs. "spurious noise error" trade-off. The probe can up-weight the frozen core features while ignoring the noisy supervised spurious features.

## Foundational Learning

- **Concept**: **Non-realizable Noise**
  - **Why needed here**: This concept differentiates between noise in the causal path ($\phi_{core} \to y$) and noise in the anti-causal path ($y \to \phi_{spu}$), crucial for understanding why ERM fails in the paper's scenarios.
  - **Quick check question**: Can you explain why higher label noise on core features might cause a model to prefer a perfectly correlated but spurious feature?

- **Concept**: **Linear Probing (Last Layer Retraining)**
  - **Why needed here**: This is the evaluation protocol and practical utility function. The FTT architecture is designed to optimize the representation $W$ such that a simple linear classifier trained on a small test-set sample can recover robustness.
  - **Quick check question**: Why does retraining the last layer fail if the backbone ($W$) has suppressed the core feature direction entirely?

- **Concept**: **Gradient Flow in Linear Networks**
  - **Why needed here**: The proofs rely on gradient flow dynamics in a two-layer linear network. Readers need intuition on how features evolve over time $t$ and why "freezing" stops the drift toward spurious correlation minimization.
  - **Quick check question**: In a linear network, does freezing a subset of weights change the optimization landscape for the remaining weights, or does it simply reduce the parameter count?

## Architecture Onboarding

- **Component map**: Input -> Backbone (ResNet) -> PCA Branch ($M_{ul}$) + Trainable Branch ($M_{sl}$) -> Head (Linear)
- **Critical path**:
  1. Initialize backbone $M_{init}$.
  2. Pre-calculation: Run inference on the entire training set to compute PCA transform matrix $W_{ul}$.
  3. Initialization: Clone $M_{init}$ to create $M_{sl}$.
  4. Training: Forward pass concatenates $M_{ul}(x)$ and $M_{sl}(x)$; backprop updates only $M_{sl}$ and the Head.
  5. Deployment: Retrain only the Head on the target domain validation data.
- **Design tradeoffs**:
  - **Unsupervised Fraction ($p$)**: High $p$ preserves more salient features (safety) but reduces capacity for supervised signal exploitation. Paper suggests $p \in [0.25, 0.75]$ is robust.
  - **PCA vs. Contrastive Learning**: PCA is computationally cheaper and sufficient for linear theory; Contrastive Learning may capture better non-linear invariances but increases complexity.
- **Failure signatures**:
  - **Saliency Mismatch**: If core features are low-variance (not "salient"), PCA filters them out, causing FTT to underperform ERM.
  - **Over-freezing**: Setting $p$ too high restricts the model's ability to adapt to the specific prediction task during training.
- **First 3 experiments**:
  1. Noise Sensitivity Stress Test: Run FTT vs. ERM on a synthetic dataset (like Dominoes) while sweeping $\eta_{core}$ from 0% to 20%. Verify the cross-over point where ERM fails and FTT maintains accuracy.
  2. Ablation on $p$: On Waterbirds, plot Worst Group Accuracy vs. $p$ (0.0 to 1.0). Confirm the "U-shaped" or robust plateau region described in Section 5.3.
  3. Probing Efficiency: Measure the minimum validation set size required for the final linear probe to achieve 95% of peak performance, comparing FTT features vs. ERM features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the FTT framework effectively utilize non-linear unsupervised methods (e.g., Contrastive Learning) in the freeze stage to capture core features that are not simply high-variance?
- Basis in paper: Appendix A notes that "pure PCA will not work" for random initialization and suggests considering "Contrastive Learning," but the main theoretical and empirical results rely on linear PCA.
- Why unresolved: The paper theoretically analyzes the "freeze" stage using PCA (linear projection on top eigenvectors). It is unclear if non-linear representations retain the same property of preserving test-time probing features without being corrupted by spurious correlations in the same way.
- What evidence would resolve it: A theoretical analysis or empirical demonstration where FTT uses a contrastive loss for the freeze stage and maintains or improves performance over the PCA baseline on the same spurious correlation benchmarks.

### Open Question 2
- Question: Is there a theoretically grounded, adaptive mechanism to select the optimal unsupervised feature fraction $p$ without relying on validation data or prior knowledge of noise levels?
- Basis in paper: Section 5.3.1 states, "We do find that as the noise increases, a more 'unsupervised' method is favored... The ablation on other datasets can be found in Appendix C," but provides no automated method for tuning $p$.
- Why unresolved: The paper treats $p$ as a hyperparameter determined by grid search. Since the core noise $\eta_{core}$ is usually unknown in practice, the optimal $p$ cannot be determined a priori based on the theory provided.
- What evidence would resolve it: An algorithm or heuristic derived from the training dynamics (e.g., analyzing the spectral norm gap or gradient alignment) that dynamically adjusts $p$ and matches the performance of the oracle $p$ search.

### Open Question 3
- Question: Can the theoretical guarantees regarding test-time probing (Theorem 3) be extended to deep non-linear networks with non-linear heads?
- Basis in paper: Section 3.1 states, "To capture the property of features... we consider a regression task using a two-layer linear network," and Section 4.3 notes FTT is a meta-algorithm but theoretical guarantees are specific to this linear setup.
- Why unresolved: The analysis relies on the properties of gradient flow in linear networks and closed-form solutions for the covariance matrix $H$. Deep non-linear networks introduce interactions between layers and non-convex loss landscapes that may violate the "salient feature" assumptions used in the proofs.
- What evidence would resolve it: A proof technique that bounds the probing error in non-linear settings, or empirical evidence showing the linear "mixture of features" assumption holds sufficiently well in deep networks to generalize the theory.

### Open Question 4
- Question: How does FTT perform when core and spurious features are deeply entangled (e.g., within the same layer or receptive field) rather than appearing as separable high-variance components?
- Basis in paper: The method assumes unsupervised PCA can isolate "salient" features. If core features are low-variance or entangled with spurious features such that PCA selects the spurious components, the "freeze" stage would preserve the wrong features.
- Why unresolved: The experiments use datasets (Waterbirds, Dominoes) where core features (birds, CIFAR images) are visually distinct and likely high-variance. The paper does not analyze cases where the core signal is weak in the unsupervised spectrum.
- What evidence would resolve it: Experiments on synthetic datasets where the variance of spurious features is explicitly controlled to be higher than core features, testing if the frozen features still aid probing.

## Limitations
- **Unsupervised Feature Quality Assumption**: The method's performance critically depends on the assumption that core features are "salient" (high variance) and will be captured by PCA. If core features are low-variance, the unsupervised freeze step may inadvertently capture spurious features instead, potentially degrading performance.
- **Real-World Dataset Generalizability**: While the paper validates on Waterbirds and CelebA, these datasets have relatively clean feature structures. The behavior of FTT on more complex, natural images with multiple interacting spurious correlations remains uncertain.
- **Theoretical Assumptions**: The theoretical analysis assumes a linear model and specific noise generation processes. The extension of these guarantees to deep, non-linear networks and more complex data distributions is not rigorously established.

## Confidence
- **High Confidence**: FTT outperforms ERM in controlled synthetic settings with large feature noise (e.g., 4.5% accuracy improvement on Dominoes).
- **Medium Confidence**: FTT achieves consistent improvements on real datasets (Waterbirds, CelebA) with average WGA gains of ~1-2%.
- **Low Confidence**: The theoretical bounds hold with the same parameters in non-linear networks; the method's robustness to multiple spurious features is well-established.

## Next Checks
1. **Saliency Stress Test**: Systematically vary the variance of core vs. spurious features in a synthetic dataset (e.g., Dominoes) to identify the exact threshold where FTT fails due to PCA capturing the wrong features.
2. **Multi-Spurious Ablation**: Extend the evaluation to a dataset with two or more spurious attributes (e.g., a variant of Waterbirds with background *and* object pose) to test FTT's scalability.
3. **Non-Linear Backbone Probe**: Replace the linear probing stage with a small MLP to assess whether FTT's benefits extend beyond linear decision boundaries.