---
ver: rpa2
title: Your Group-Relative Advantage Is Biased
arxiv_id: '2601.08521'
source_url: https://arxiv.org/abs/2601.08521
tags:
- advantage
- ha-dw
- grpo
- estimation
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Group-relative RL algorithms like GRPO suffer from biased advantage
  estimation that systematically underestimates advantages for hard prompts and overestimates
  them for easy prompts. This bias degrades training performance by skewing exploration-exploitation
  balance.
---

# Your Group-Relative Advantage Is Biased

## Quick Facts
- arXiv ID: 2601.08521
- Source URL: https://arxiv.org/abs/2601.08521
- Authors: Fengkai Yang, Zherui Chen, Xiaohan Wang, Xiaodong Lu, Jiajun Chai, Guojun Yin, Wei Lin, Shuai Ma, Fuzhen Zhuang, Deqing Wang, Yaodong Yang, Jianxin Li, Yikun Ban
- Reference count: 40
- One-line primary result: HA-DW improves RLVR performance on math reasoning benchmarks by correcting biased advantage estimation in group-relative algorithms

## Executive Summary
Group-relative reinforcement learning algorithms like GRPO suffer from systematically biased advantage estimation that underestimates advantages for hard prompts and overestimates them for easy prompts. This bias degrades training performance by skewing the exploration-exploitation balance. The paper proposes History-Aware Adaptive Difficulty Weighting (HA-DW), a plug-and-play module that dynamically adjusts advantage weights using an evolving difficulty anchor tracking long-term reward trends. Across five mathematical reasoning benchmarks using models from Qwen and LLaMA families, HA-DW consistently improves performance compared to original algorithms, demonstrating that correcting biased advantage estimation is critical for robust and efficient RLVR training.

## Method Summary
The method addresses biased advantage estimation in group-relative RL by introducing an adaptive difficulty anchor that tracks model capability across training batches. The core innovation is a Kalman-style belief update mechanism that maintains an evolving estimate of what the model can typically solve. This anchor is then used to compute a history-based difficulty metric for each prompt, which drives an exponential reweighting of the raw advantage estimates. The reweighting amplifies advantages for hard prompts (where the baseline overestimates) and suppresses them for easy prompts (where the baseline underestimates). This plug-and-play module can be integrated into GRPO and its variants without architectural modifications, and requires only tuning of the scaling parameter λ_scale.

## Key Results
- HA-DW consistently improves performance across five mathematical reasoning benchmarks (MATH500, AIME25, AMC23, Minerva, OlympiadBench)
- Even with fewer rollouts (8 vs 16), HA-DW outperforms standard GRPO, demonstrating efficiency gains
- The method works across different model families (Qwen and LLaMA) and scales (4B, 8B, 3B parameters)
- Dynamic threshold tracking (Ct) outperforms fixed thresholds in all benchmark settings
- Optimal performance achieved with λ_scale in range [1.3, 1.5]

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group-relative advantage estimation is systematically biased—underestimating advantages for hard prompts and overestimating them for easy prompts.
- Mechanism: When computing advantage as $\hat{A}_{t,i} = r_{t,i} - \hat{p}_t$ using the empirical group mean as baseline, the conditional expectation deviates from the true advantage because the empirical baseline does not equal the expected reward except when $p_t = 0.5$. The non-degenerate conditioning event introduces systematic skew.
- Core assumption: Rewards follow Bernoulli distribution; extension to continuous bounded rewards shown in Appendix D.5.
- Evidence anchors:
  - [Theorem 1, p.3]: "The expectation of the group-based advantage estimator $\hat{A}_{t,i}$ is lower than the true advantage $A_{t,i}$ for difficult prompts, and larger than $A_{t,i}$ for easy prompts."
  - [Corollary 1, p.4]: For group sizes $2 \leq G \leq 8$, $P(\hat{A}_{t,i} < A_{t,i} | S, p_t < 0.5) > 0.63$
  - [corpus]: Related work "DARO: Difficulty-Aware Reweighting Policy Optimization" independently identifies similar difficulty imbalance issues in GRPO.

### Mechanism 2
- Claim: A history-aware difficulty anchor (C_t) can track model capability across batches, providing a more stable reference than per-group baselines.
- Mechanism: Kalman-style belief propagation: $C_t^+ = (1-\eta_t)C_t^- + \eta_t y_t$, where $y_t$ is current batch accuracy and $\eta_t = \eta \cdot \sigma_t$ adapts based on training stability (standard deviation over recent $m$ batches).
- Core assumption: Model capability evolves smoothly; recent performance predicts difficulty calibration.
- Evidence anchors:
  - [Section 3.1, p.5]: "Ct enables the model to aggregate information across historical batches via belief updates and to condition its training strategy on this evolving belief."
  - [Table 2, p.8]: Dynamic threshold $C_t$ outperforms fixed thresholds (0.4, 0.5, 0.6) across all benchmarks.
  - [corpus]: Weak—no direct corpus neighbor addresses cross-batch belief tracking for RLVR specifically.

### Mechanism 3
- Claim: Exponentially-scaled reweighting factors can compensate for bias by amplifying hard-prompt advantages and suppressing easy-prompt advantages.
- Mechanism: $\Phi_{t,i} = \lambda_{scale} \cdot \exp(D_{t,i} \cdot M_t)$, where $D_{t,i} = -\text{sgn}(\hat{A}_{t,i}) \cdot \text{sgn}(\text{diff}_{his})$ determines adjustment direction and $M_t = |\hat{p}_t - C_t^-|$ determines magnitude.
- Core assumption: The theoretical bounds in Theorem 3 specify valid $\lambda_{scale}$ ranges; practical $\lambda_{scale} \in [1.3, 1.5]$ works empirically.
- Evidence anchors:
  - [Theorem 3, p.6]: "With an appropriate choice of the scaling parameter $\lambda_{scale}$, the HA-DW adjustment yields advantage estimates that are closer to the true advantage."
  - [Table 7, p.22]: Ablation shows optimal performance at $\lambda_{scale} = 1.3$ (48.7% avg) vs baseline (46.5%).
  - [corpus]: "Noise-corrected GRPO" addresses a different bias source (reward noise), complementary to this difficulty-based bias.

## Foundational Learning

- **Concept: Advantage Functions in RL**
  - Why needed here: The entire paper revolves around how advantages ($A = r - \text{baseline}$) are estimated and how bias in this estimation affects policy gradients.
  - Quick check question: If your baseline is always 0, what happens to advantage variance? What if baseline = expected return?

- **Concept: Importance Sampling Ratio**
  - Why needed here: GRPO objectives use $\psi(\pi_\theta/\pi_{old})$ to compute off-policy gradient estimates; understanding clipping/ratios is prerequisite to modifying the objective.
  - Quick check question: Why clip the importance ratio? What happens to variance when the ratio is large?

- **Concept: Kalman/Belief Filtering**
  - Why needed here: HA-DW uses Kalman-style updates ($C_t^+ = (1-\eta)C_t^- + \eta \cdot y_t$) to maintain a running estimate of model capability.
  - Quick check question: What does $\eta \to 0$ imply? What does $\eta \to 1$ imply?

## Architecture Onboarding

- **Component map**:
  Prompt x_t → Rollout Generator (G responses) → Verifier (rewards r_{t,i})
       ↓
  Group Baseline: \hat{p}_t = mean(rewards)
       ↓
  Raw Advantage: \hat{A}_{t,i} = r_{t,i} - \hat{p}_t
       ↓
  History Buffer → Evolving Anchor C_t (Kalman update)
       ↓
  Difficulty: diff_{his} = \hat{p}_t - C_t
       ↓
  Reweighting: \Phi_{t,i} = \lambda * exp(D * M)
       ↓
  Adjusted Advantage: \hat{A}_{t,i} * \Phi_{t,i}
       ↓
  Policy Loss (GRPO/GSPO/DAPO objective)

- **Critical path**: Belief update (Eq. 8-11) → difficulty computation (Eq. 13) → reweighting factor (Eq. 16) → advantage scaling (Eq. 17). Errors in $C_t$ propagate to all downstream weights.

- **Design tradeoffs**:
  - **Hard vs. soft update**: Paper proposes soft (Kalman) update; Appendix F offers hard update (simpler, less adaptive).
  - **Memory window $m$**: Controls stability vs. responsiveness in $\sigma_t$ computation; larger $m$ smooths but lags.
  - **Group size $G$**: Smaller $G$ increases bias (Figure 2) but reduces compute; HA-DW helps most when $G$ is small (Table 3 shows 8+HA-DW beats 16 without HA-DW).

- **Failure signatures**:
  - $C_t$ collapses to 0 or 1: Check $\eta_t$ isn't too large; verify $y_t$ (batch accuracy) is computed correctly.
  - No performance gain: Verify $\Phi_{t,i}$ is actually applied (check gradient flow); confirm prompts span difficulty range.
  - Training instability: $\lambda_{scale} > 1.5$ may over-amplify noise; reduce or add gradient clipping.

- **First 3 experiments**:
  1. **Bias verification**: On a held-out prompt set, compare $\hat{A}_{t,i}$ distributions at $G=8$ vs $G=128$ for hard/easy prompts (replicate Figure 1b).
  2. **Ablation on $\lambda_{scale}$**: Grid search [0.8, 1.0, 1.3, 1.5, 2.0] on a small model (Qwen-3-4B) to find optimal range for your task domain.
  3. **Fixed vs. dynamic threshold**: Compare $C_t$ against fixed thresholds (0.4, 0.5, 0.6) to validate the adaptive anchor's contribution (replicate Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HA-DW framework be extended to non-group-relative RL methods (e.g., PPO with learned critics) that also suffer from advantage estimation bias?
- Basis in paper: [explicit] "estimation bias is pervasive, and future work will focus on extending this concept to a broader scope" (Limitations section)
- Why unresolved: The method is specifically designed for group-relative algorithms; the theoretical characterization of bias in critic-based methods differs fundamentally
- What evidence would resolve it: Theoretical derivation of advantage bias in critic-based RLVR and empirical comparison of extended HA-DW against PPO baselines

### Open Question 2
- Question: How does HA-DW perform on non-mathematical reasoning tasks with continuous or multi-dimensional reward signals?
- Basis in paper: [inferred] All experiments use mathematical reasoning benchmarks with binary rewards; authors acknowledge "real-world reward signals can be more general" and extend theory to continuous rewards without empirical validation
- Why unresolved: Theoretical extension to continuous bounded rewards (Appendix D.5) lacks experimental support beyond binary verification
- What evidence would resolve it: Experiments on code generation, multi-step reasoning, or dialogue tasks using learned reward models

### Open Question 3
- Question: What is the principled method for selecting the scaling parameter λscale given model scale and task distribution?
- Basis in paper: [inferred] Ablation study (Table 7) shows λscale=1.3-1.5 performs best, but selection is empirical; Theorem 3 provides theoretical bounds but not a constructive selection procedure
- Why unresolved: Theoretical analysis guarantees existence of optimal λscale range but offers no closed-form solution or adaptive mechanism
- What evidence would resolve it: Derivation of λscale as a function of group size G, expected accuracy range, or adaptive selection based on running statistics

### Open Question 4
- Question: How does the evolving difficulty anchor Ct behave under non-stationary training conditions such as curriculum learning or online data filtering?
- Basis in paper: [inferred] Ct assumes relatively stable prompt difficulty distribution; Kalman-style update adapts to training dynamics but curriculum learning introduces intentional distribution shifts
- Why unresolved: The method tracks model capability but does not account for external difficulty schedule changes
- What evidence would resolve it: Experiments combining HA-DW with curriculum learning strategies and analysis of Ct stability under distribution shift

## Limitations

- The adaptive difficulty anchor relies on hyperparameters (η, m) described as "task-dependent" but not explicitly specified for experiments
- All experiments use mathematical reasoning tasks; generalization to other RLVR domains (coding, instruction following) remains untested
- The practical significance vs. simply increasing rollout numbers is not fully characterized

## Confidence

- **High Confidence**: The theoretical derivation of bias in group-relative advantage estimation (Theorem 1) and the empirical demonstration of performance improvements with HA-DW.
- **Medium Confidence**: The generalization claims to "GRPO and its variants" based on single-experiment validation, and the assertion that HA-DW is "plug-and-play" without architectural modifications.
- **Low Confidence**: The practical significance of HA-DW vs. simply increasing rollout numbers, as the paper shows HA-DW with 8 rollouts outperforms GRPO with more rollouts but doesn't fully characterize the tradeoff space.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary η ∈ {0.05, 0.1, 0.2} and m ∈ {3, 5, 10} to establish robust defaults and understand their impact on C_t stability and final performance.

2. **Cross-Domain Generalization**: Apply HA-DW to a non-mathematical RLVR task (e.g., code generation or instruction following) to test whether the bias correction generalizes beyond the mathematical reasoning domain.

3. **Bias-Variance Tradeoff Characterization**: Compare HA-DW's effect on advantage variance relative to baseline GRPO across different group sizes (G=4, 8, 16) to understand whether the reweighting introduces additional variance that could affect training stability.