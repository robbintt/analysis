---
ver: rpa2
title: Computed Tomography Visual Question Answering with Cross-modal Feature Graphing
arxiv_id: '2507.04333'
source_url: https://arxiv.org/abs/2507.04333
tags:
- question
- slices
- visual
- graph
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses visual question answering (VQA) in medical
  imaging, particularly for CT scans, which contain multiple slices requiring complex
  spatial reasoning. Existing methods often treat CT slices independently, failing
  to capture inter-slice relationships, leading to fragmented and imprecise responses.
---

# Computed Tomography Visual Question Answering with Cross-modal Feature Graphing

## Quick Facts
- **arXiv ID**: 2507.04333
- **Source URL**: https://arxiv.org/abs/2507.04333
- **Reference count**: 40
- **Primary result**: Novel LLM-based framework with cross-modal feature graphing outperforms baselines on M3D-VQA benchmark for CT VQA tasks

## Executive Summary
This paper addresses visual question answering (VQA) in medical imaging, specifically for CT scans that contain multiple slices requiring complex spatial reasoning. Existing methods often treat CT slices independently, failing to capture inter-slice relationships and leading to fragmented, imprecise responses. The authors propose a novel framework that constructs a cross-modal graph integrating both visual and textual features, with CT slices and question tokens as nodes. An attentive graph convolutional network dynamically fuses information within this structure, and the resulting aggregated graph features serve as a soft prompt to guide the LLM in generating accurate answers. Experiments on the M3D-VQA benchmark demonstrate consistent outperformance across multiple evaluation metrics.

## Method Summary
The method constructs a cross-modal graph where CT slices and question tokens are nodes, with edges connecting neighboring slices and all token-slice pairs. An attentive graph convolutional network (A-GCN) performs information fusion using learned attention weights instead of fixed adjacency. The resulting graph features are projected to LLM embedding space and used as soft prompts alongside question embeddings. The system uses Qwen2-VL as both vision encoder and LLM backbone, trained end-to-end on M3D-VQA with cross-entropy loss for 3 epochs.

## Key Results
- A-GCN-CTVQA consistently outperforms baselines across all evaluation metrics on M3D-VQA benchmark
- Attention mechanism effectively identifies question-relevant slices, improving reasoning accuracy
- Soft prompt integration provides better performance than late fusion approaches
- Model handles volumetric CT data better than slice-independent methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-modal graph construction enables explicit modeling of inter-slice and slice-to-token relationships that independent encoding misses.
- **Mechanism**: The architecture builds a unified graph where CT slices and question tokens coexist as nodes. Adjacent slices connect via edges (capturing spatial continuity), while every token connects to every slice (enabling cross-modal reasoning). This structure forces information flow between modalities rather than late fusion.
- **Core assumption**: Adjacent CT slices share semantically meaningful spatial relationships; question tokens can relate to multiple slices simultaneously.
- **Evidence anchors**:
  - [abstract] "constructs a cross-modal graph integrating both visual and textual features, treating individual CT slices and question tokens as nodes"
  - [Section 2.2.1] "Since adjacent CT slices often share strong spatial and structural relationships, we connect neighboring CT slices with edges. Additionally, as question tokens can relate to multiple CT slices, we establish edges between every token and every slice."
  - [corpus] Related work (BioD2C, CMI-MTL) confirms cross-modal alignment is a known bottleneck in medical VQA.

### Mechanism 2
- **Claim**: Attention-weighted graph convolution dynamically prioritizes question-relevant slices, reducing noise from irrelevant regions.
- **Mechanism**: Standard GCN uses fixed adjacency (0/1). A-GCN replaces binary edges with learned attention weights computed from node representations via a trainable matrix. This allows the model to upweight slice-token pairs that matter for the specific question.
- **Core assumption**: Not all slices contribute equally; attention can learn to identify diagnostically relevant regions.
- **Evidence anchors**:
  - [abstract] "attentive graph convolutional network dynamically fuses information within this structure"
  - [Section 2.2.3] "wj,k = aj,k · exp(h(l−1)j · W(l)a · h(l−1)k) / Σ..." — attention replaces adjacency
  - [Figure 3 case study] Shows attention weights highlighting specific slices relevant to the clinical question
  - [corpus] MedErr-CT benchmark highlights diagnostic error detection; attention mechanisms may help focus on error-prone regions.

### Mechanism 3
- **Claim**: Graph-derived features as soft prompts inject structured multimodal context into the LLM more effectively than concatenated embeddings.
- **Mechanism**: A-GCN outputs (h(L)1...h(L)M+N) are projected via a learned linear layer into the LLM's embedding space, then concatenated with the question embeddings. This "soft prompt" conditions generation without modifying LLM weights directly.
- **Core assumption**: The LLM can interpret graph-aggregated features as contextual guidance; projection layer preserves relational information.
- **Evidence anchors**:
  - [abstract] "aggregated graph features then serve as a soft prompt to guide a large language model"
  - [Section 2.3] "O = W · H(L) + b" — projection to LLM embedding space
  - [Table 6] Vision-Only and Text-Only prompts underperform combined prompt, suggesting cross-modal integration is key
  - [corpus] Weak/no direct corpus evidence for soft-prompt effectiveness specifically in CT VQA; extrapolation from general LLM prompting literature.

## Foundational Learning

- **Graph Neural Networks (GCN, GAT)**
  - Why needed here: The core innovation is A-GCN; understanding message passing, adjacency matrices, and attention in graphs is essential.
  - Quick check question: Given a 3-node graph with adjacency matrix [[0,1,0],[1,0,1],[0,1,0]], what nodes receive information from node 2 after one GCN layer?

- **Vision Transformers (ViT)**
  - Why needed here: CT slices are encoded via ViT-based encoder (Qwen2-VL); patch embedding and positional encoding matter.
  - Quick check question: How does a ViT convert a 224×224 image into a sequence of tokens with 1280-dim embeddings?

- **Soft Prompting / Prefix Tuning**
  - Why needed here: Graph outputs condition the LLM via learned soft prompts, not hard text.
  - Quick check question: What is the difference between providing "Context: [text]" as a hard prompt vs. prepending learned continuous vectors to LLM input embeddings?

## Architecture Onboarding

- **Component map**: Vision Encoder (Qwen2-VL ViT) -> Graph Builder -> A-GCN -> Projection Layer -> LLM Decoder (Qwen2-VL LLM)
- **Critical path**: Input CT + Question → Encode both → Build cross-modal graph → A-GCN attention-weighted propagation → Project to LLM space → LLM generates answer
- **Design tradeoffs**:
  - **Full graph vs. sparse edges**: Full token-slice connectivity enables flexibility but O(N×M) edges; pruning may help scalability
  - **Max-pooling vs. [CLS] token for slice representation**: Max-pooling may lose spatial detail; [CLS] may miss fine local features
  - **LoRA vs. full fine-tuning**: Table 5 shows both work; LoRA saves memory but may underfit on complex CT patterns
  - **LLM scale (2B vs. 7B)**: Larger models improve metrics (Table 4) but increase inference cost

- **Failure signatures**:
  - Attention weights near-uniform → model not learning slice relevance; check attention visualization
  - Low scores on "Abnormality" and "Location" questions (Table 2) → graph may not capture fine-grained pathology; consider multi-scale features
  - Big gap between train and dev performance → overfitting to slice ordering or question patterns
  - Generated answers repetitive or generic → soft prompt may not be conditioning effectively; inspect projection layer gradients

- **First 3 experiments**:
  1. **Ablate graph structure**: Run with slice-only edges (no token-slice), token-only edges (no slice-slice), and random edges. Compare to full graph to isolate contribution of each edge type.
  2. **Visualize attention on held-out cases**: For 10 test examples, plot attention weights over slices alongside ground-truth pathology locations. Quantify correlation.
  3. **Swap LLM backbone**: Replace Qwen2-VL (2B) with LLaVA-med (7B) or another medical LLM while keeping A-GCN fixed. Measure if gains are backbone-agnostic or specific to Qwen2-VL pretraining.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the cross-modal graph approach be enhanced by integrating explicit medical domain knowledge (e.g., anatomical ontologies, knowledge graphs) rather than relying solely on learned representations from CT slices and question tokens?
- **Open Question 2**: How does the approach scale to CT volumes with substantially more slices (e.g., hundreds or thousands) without excessive computational cost or degradation in slice selection accuracy?
- **Open Question 3**: Would incorporating relationships between non-adjacent but anatomically correlated CT slices improve reasoning, beyond the current approach of only connecting neighboring slices?
- **Open Question 4**: Does the approach generalize to other medical imaging modalities (e.g., MRI, PET) and to CT VQA datasets beyond M3D-VQA?

## Limitations
- Only tested on M3D-VQA dataset, limiting generalizability claims to other CT VQA datasets
- Dense token-slice connectivity creates O(N×M) edges, potentially causing computational inefficiency for large volumes
- Exact A-GCN depth and hidden dimensions unspecified, making precise reproduction difficult
- Maximum slice count and token limits not reported, leaving variable-length volume handling unclear

## Confidence
- **High**: Cross-modal graph construction improves performance vs. independent slice encoding (supported by Table 2 full model > ablation variants)
- **Medium**: Attention-weighted edges dynamically prioritize question-relevant slices (supported by attention visualizations and metric gains, but mechanism robustness unclear)
- **Medium**: Soft prompt integration is more effective than late fusion (supported by prompt ablation, but soft prompt ablation not tested directly)
- **Low**: Performance gains are generalizable across all CT VQA datasets (only tested on M3D-VQA)

## Next Checks
1. **Ablate graph structure**: Run experiments with only slice-slice edges (no token-slice), only token-slice edges (no slice-slice), and random edges. Compare to full graph to isolate contribution of each edge type and test if attention alone explains gains.
2. **Visualize attention on held-out cases**: For 10 test examples, plot attention weights over slices alongside ground-truth pathology locations. Quantify correlation between attention focus and clinical relevance to validate attention mechanism.
3. **Swap LLM backbone**: Replace Qwen2-VL (2B) with LLaVA-med (7B) or another medical LLM while keeping A-GCN fixed. Measure if gains are backbone-agnostic or specific to Qwen2-VL pretraining.