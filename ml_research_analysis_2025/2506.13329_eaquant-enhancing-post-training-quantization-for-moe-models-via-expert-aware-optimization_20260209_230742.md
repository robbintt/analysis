---
ver: rpa2
title: 'EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware
  Optimization'
arxiv_id: '2506.13329'
source_url: https://arxiv.org/abs/2506.13329
tags:
- quantization
- eaquant
- experts
- expert
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles post-training quantization (PTQ) challenges
  in Mixture-of-Experts (MoE) models caused by activation outliers, routing layer
  sensitivity, and sparse expert activation. The authors propose EAQuant, an expert-aware
  PTQ framework with three components: expert-aware smoothing aggregation to suppress
  activation outliers using a unified channel-wise smoothing vector, expert-aware
  routing consistency alignment to preserve expert selection post-quantization via
  dual-objective calibration, and expert-aware calibration data balance to ensure
  adequate coverage for underutilized experts during calibration.'
---

# EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization

## Quick Facts
- arXiv ID: 2506.13329
- Source URL: https://arxiv.org/abs/2506.13329
- Reference count: 40
- Average accuracy improvements of 1.15-13.81% across various low-bit quantization settings (W4A4/W3A4/W3A3/W2A4) compared to the state-of-the-art

## Executive Summary
EAQuant addresses critical challenges in post-training quantization (PTQ) of Mixture-of-Experts (MoE) models, specifically targeting activation outliers, routing layer sensitivity, and sparse expert activation patterns. The framework introduces three expert-aware components: smoothing aggregation to suppress activation outliers through unified channel-wise smoothing vectors, routing consistency alignment to preserve expert selection post-quantization via dual-objective calibration, and calibration data balance to ensure adequate coverage for underutilized experts. Extensive experiments demonstrate consistent accuracy improvements across multiple MoE architectures and quantization configurations.

## Method Summary
EAQuant is a three-stage expert-aware PTQ framework designed specifically for MoE models. The first component employs expert-aware smoothing aggregation using a unified channel-wise smoothing vector to suppress activation outliers across different experts. The second component implements expert-aware routing consistency alignment that preserves expert selection patterns through dual-objective calibration. The third component introduces expert-aware calibration data balance to ensure sufficient calibration data coverage for underutilized experts. These components work synergistically to address the unique challenges posed by MoE architectures during low-bit quantization.

## Key Results
- Achieves average accuracy improvements of 1.15-13.81% across W4A4/W3A4/W3A3/W2A4 quantization settings
- Demonstrates particularly pronounced gains in reasoning tasks under aggressive quantization
- Shows superior robustness compared to state-of-the-art methods on OLMoE-7B, DeepSeek-MoE-16B, and Mixtral-8x7B

## Why This Works (Mechanism)
EAQuant addresses three fundamental challenges in MoE PTQ: activation outliers that cause significant quantization error, routing layer sensitivity that can misroute tokens post-quantization, and sparse expert activation that leads to inadequate calibration data coverage. The expert-aware smoothing aggregation reduces quantization noise by applying channel-wise smoothing vectors that normalize activation distributions. The routing consistency alignment preserves the gating mechanism's ability to select appropriate experts by calibrating based on dual objectives. The calibration data balance ensures that even rarely activated experts receive adequate representation during the calibration phase, preventing performance degradation for these specialized components.

## Foundational Learning

**Mixture-of-Experts (MoE) Architecture**
- *Why needed*: Understanding MoE's sparse activation patterns and routing mechanisms is essential for addressing PTQ challenges
- *Quick check*: Verify that gating layers dynamically select experts based on input tokens and that activation patterns are typically sparse

**Post-Training Quantization (PTQ)**
- *Why needed*: PTQ's calibration-based approach is central to understanding how EAQuant's components interact with quantization parameters
- *Quick check*: Confirm that PTQ uses calibration data to compute quantization parameters without requiring full retraining

**Activation Outliers in Neural Networks**
- *Why needed*: Activation outliers are a primary source of quantization error, particularly problematic in MoE models
- *Quick check*: Identify whether activation distributions have long tails or extreme values that would benefit from smoothing

## Architecture Onboarding

**Component Map**
Calibration Data -> Expert-Aware Calibration Data Balance -> Quantization Parameters -> Expert-Aware Smoothing Aggregation -> Expert-Aware Routing Consistency Alignment -> Final Quantized Model

**Critical Path**
The critical path flows through the three expert-aware components sequentially: first ensuring balanced calibration data coverage, then applying smoothing aggregation to suppress outliers, and finally performing routing consistency alignment to preserve expert selection patterns.

**Design Tradeoffs**
The framework trades increased computational overhead during the calibration phase for improved accuracy post-quantization. The three-stage optimization adds complexity but addresses MoE-specific challenges that generic PTQ methods cannot handle effectively.

**Failure Signatures**
Potential failure modes include: inadequate calibration data leading to poor quantization parameters for rarely used experts, over-smoothing that could eliminate important activation patterns, and routing misalignment that causes tokens to be processed by incorrect experts post-quantization.

**First Experiments**
1. Test EAQuant on a simple MoE architecture with known activation outlier patterns to validate the smoothing component
2. Evaluate routing consistency preservation by comparing expert selection accuracy before and after quantization
3. Measure the impact of calibration data balance on rarely activated experts in a controlled setting

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to only three MoE models (OLMoE-7B, DeepSeek-MoE-16B, and Mixtral-8x7B), raising questions about generalizability to other architectures
- Lack of ablation studies to isolate the contribution of each proposed component to the reported improvements
- No quantification of computational overhead introduced by the three-stage optimization process

## Confidence
- **Medium**: Experimental results show consistent improvements across multiple quantization configurations, but the narrow model coverage and missing ablation analysis prevent higher confidence classification

## Next Checks
1. Evaluate EAQuant on additional MoE architectures beyond the three tested models to establish broader applicability
2. Conduct comprehensive ablation studies that isolate each component's contribution to determine which innovations drive the improvements
3. Measure and report the computational overhead of EAQuant's optimization pipeline to understand practical deployment implications