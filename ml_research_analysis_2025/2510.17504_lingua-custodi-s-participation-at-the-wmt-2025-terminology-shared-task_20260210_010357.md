---
ver: rpa2
title: Lingua Custodi's participation at the WMT 2025 Terminology shared task
arxiv_id: '2510.17504'
source_url: https://arxiv.org/abs/2510.17504
tags:
- terminology
- translation
- grpo
- language
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Lingua Custodia's approach to the WMT 2025
  Terminology shared task, focusing on improving machine translation systems' ability
  to correctly translate specified terminology. The authors propose a two-stage fine-tuning
  process for large language models, starting with supervised fine-tuning (SFT) on
  parallel data augmented with explicit terminology constraints, followed by Group
  Relative Policy Optimization (GRPO) with a custom reward function that encourages
  correct terminology usage while preserving general translation quality.
---

# Lingua Custodi's participation at the WMT 2025 Terminology shared task

## Quick Facts
- arXiv ID: 2510.17504
- Source URL: https://arxiv.org/abs/2510.17504
- Reference count: 8
- Primary result: Two-stage fine-tuning with GRPO achieves ≥10 points term match accuracy gain on WMT2025 EN→DE/ES/RU terminology translation task

## Executive Summary
This paper presents Lingua Custodia's approach to the WMT 2025 Terminology shared task, focusing on improving machine translation systems' ability to correctly translate specified terminology. The authors propose a two-stage fine-tuning process for large language models, starting with supervised fine-tuning (SFT) on parallel data augmented with explicit terminology constraints, followed by Group Relative Policy Optimization (GRPO) with a custom reward function that encourages correct terminology usage while preserving general translation quality. They employ two 4B parameter instruction-tuned LLMs (Qwen3-4B and Gemma3-4B-it) as base models and create instruction data by annotating parallel sentences with terminology constraints extracted from bilingual dictionaries. Experimental results on English-German, English-Spanish, and English-Russian language pairs show that the proposed approach achieves ≥10 points of accuracy gain in term match compared to baseline models, with GRPO further improving terminology adherence (achieving over 0.95 term accuracy in some settings) while maintaining competitive Comet-Kiwi translation quality scores.

## Method Summary
The methodology employs a two-stage fine-tuning process using 4B parameter instruction-tuned LLMs. First, supervised fine-tuning trains on parallel data augmented with explicit terminology constraints injected into instruction templates. Second, GRPO fine-tunes the policy using a composite reward combining BLEU-based translation quality scores with terminology adherence metrics. The approach uses bilingual terminology dictionaries extracted unsupervised (max 5-grams) and instruction templates generated by Llama-3.1-405B-Instruct. Training employs FSDP2 on 4×H100 GPUs for SFT and DeepSpeed ZeRO-3 on 4 nodes of 4×H100 for GRPO, with batch sizes of 16 and 8 per device respectively.

## Key Results
- Two-stage fine-tuning achieves ≥10 points term match accuracy gain over baseline models
- GRPO improves terminology adherence to over 0.95 term accuracy in some settings
- Random terminology setting consistently outperforms proper terminology (0.77–0.96 vs 0.40–0.70 accuracy)
- Comet-Kiwi translation quality maintained competitively, with Qwen3 showing larger drops than Gemma3

## Why This Works (Mechanism)

### Mechanism 1: Terminology-Conditioned Instruction Fine-Tuning
- Claim: SFT with explicitly annotated terminology constraints improves lexical constraint satisfaction.
- Mechanism: Parallel sentences are matched against bilingual dictionaries; matched terms are injected into instruction templates (e.g., "Translate... adhering to terminology: source→target"). The model learns to condition output generation on these explicit constraints.
- Core assumption: The model can learn a mapping between instruction-specified terminology and target-side generation through gradient-based optimization.
- Evidence anchors: [abstract]: "starting with supervised fine-tuning (SFT) on parallel data augmented with explicit terminology constraints"

### Mechanism 2: GRPO with Composite Reward Shaping
- Claim: Group Relative Policy Optimization with a combined BLEU + terminology adherence reward boosts term accuracy while maintaining translation quality.
- Mechanism: For each sample, generate multiple completions; compute R_BLEU (normalized sentence-level BLEU) and R_term (proportion of required target terms present). Rewards are combined (equal weighting) to update policy via GRPO, which compares completions within a group rather than against a value model.
- Core assumption: The reward properly captures the quality-constraint trade-off; GRPO's relative comparison is sufficient for credit assignment without a critic.
- Evidence anchors: [abstract]: "GRPO further improving terminology adherence (achieving over 0.95 term accuracy in some settings)"

### Mechanism 3: Two-Stage Optimization Cascade
- Claim: Sequential SFT → GRPO outperforms either stage alone by first establishing constraint-aware behavior, then refining via reward optimization.
- Mechanism: Stage 1 (SFT) learns instruction-following and terminology conditioning from augmented parallel data. Stage 2 (GRPO) fine-tunes the policy to maximize a task-specific reward, improving constraint adherence through exploration.
- Core assumption: SFT provides a sufficiently good initialization for GRPO; random initialization or GRPO-only would fail or require substantially more compute.
- Evidence anchors: [abstract]: "two-stage fine-tuning process... achieving ≥10 points of accuracy gain in term match"

## Foundational Learning

- Concept: **Instruction Fine-Tuning**
  - Why needed here: Enables the model to interpret and follow terminology constraints provided in natural language prompts.
  - Quick check question: Can you explain how instruction tuning differs from continued pre-training?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Provides an RL method that optimizes a custom reward without a separate value network, reducing memory and complexity.
  - Quick check question: How does GRPO differ from PPO in terms of advantage estimation?

- Concept: **BLEU vs. Learned Quality Metrics (COMET/Comet-Kiwi)**
  - Why needed here: BLEU is used in the reward for differentiability and simplicity; COMET-Kiwi evaluates overall translation quality to detect degradation.
  - Quick check question: Why might BLEU alone be insufficient for evaluating translation quality?

## Architecture Onboarding

- Component map: Base LLM (Qwen3-4B / Gemma3-4B-it) → SFT on terminology-augmented data → GRPO optimizer with composite reward → Evaluation (term%, Comet-Kiwi)
- Critical path: (1) Terminology dictionary extraction → (2) Instruction template generation → (3) SFT data annotation → (4) SFT training → (5) GRPO reward definition → (6) GRPO training → (7) Evaluation
- Design tradeoffs: Higher term% correlates with lower Comet-Kiwi (Qwen3 shows larger drops); Gemma3 maintains quality better but slightly lower term accuracy. Equal reward weighting may not be optimal.
- Failure signatures: (1) SFT model shows no term% improvement → check dictionary coverage or template diversity; (2) GRPO causes COMET collapse → reduce R_term weight or add fluency penalty; (3) Low term% in "proper" but high in "random" → constraints may be domain-mismatched or rare.
- First 3 experiments:
  1. **Baseline probe**: Evaluate base LLM with and without terminology instructions to quantify zero-shot constraint-following.
  2. **SFT ablation**: Train SFT with varying instruction template counts (e.g., 1 vs. 10) to measure generalization impact.
  3. **Reward weight sweep**: Run GRPO with R_BLEU weight ∈ {0.3, 0.5, 0.7} to identify quality-constraint trade-off frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated reward functions improve the trade-off between terminology adherence and general translation quality?
- Basis in paper: [explicit] The conclusion states: "investigating more sophisticated reward functions tailored for terminology adherence" as future work.
- Why unresolved: The current simple weighted combination of BLEU and term accuracy creates a tension where GRPO substantially improves terminology (reaching >0.95 accuracy) but causes moderate COMET drops, particularly for Qwen3.
- What evidence would resolve it: Systematic comparison of alternative reward formulations (e.g., learned reward models, multi-objective optimization) measuring both term accuracy and translation quality metrics.

### Open Question 2
- Question: How does terminology-aware training perform in low-resource language scenarios with limited parallel data?
- Basis in paper: [explicit] The conclusion explicitly mentions "extending terminology-aware training to additional language pairs and low-resource scenarios" as planned future work.
- Why unresolved: Current experiments use relatively well-resourced pairs (EN-DE, EN-ES, EN-RU) with 10,000 parallel sentences each; the approach's data efficiency remains untested.
- What evidence would resolve it: Experiments varying training data sizes and evaluating on truly low-resource language pairs with minimal parallel corpora.

### Open Question 3
- Question: Can terminology control be effectively combined with other constrained translation tasks in a unified model?
- Basis in paper: [explicit] Future work section states plans to "study the interaction between terminology control and other translation tasks such as inline tags to build an 'all-in-one' multilingual and multi-task translation model."
- Why unresolved: Current systems handle terminology in isolation; potential interference or synergy with other constraint types (inline tags, formatting) is unknown.
- What evidence would resolve it: Multi-task training experiments measuring performance on terminology accuracy alongside other constraint satisfaction metrics.

### Open Question 4
- Question: Why does the "random terminology" setting yield substantially higher term accuracy than "proper terminology" across all models and training stages?
- Basis in paper: [inferred] Results consistently show random terminology accuracy (0.77–0.96) far exceeding proper terminology (0.40–0.70), yet authors only briefly attribute this to random words being "general meaning words."
- Why unresolved: This large gap raises questions about whether models genuinely learn terminology control or simply exploit easier lexical items in the random condition.
- What evidence would resolve it: Controlled analysis comparing term difficulty, frequency, and semantic complexity between random and proper terminology sets; probing whether models learn transferable constraint-following behavior.

## Limitations

- Training duration unspecified - neither SFT epochs nor GRPO iterations are provided, making it unclear whether results are robust or could be further improved
- Evaluation methodology relies on exact term matching without accounting for semantic equivalence or morphological variations
- Methodology's dependence on bilingual dictionary quality is critical - sparse, noisy, or domain-mismatched dictionaries would severely limit effectiveness
- Equal weighting of BLEU and terminology rewards in GRPO may not represent the optimal trade-off

## Confidence

- **High Confidence**: The two-stage fine-tuning architecture (SFT followed by GRPO) is technically sound and well-documented. The use of composite rewards combining BLEU and terminology adherence is a valid approach with clear implementation details.
- **Medium Confidence**: The claimed ≥10 point term match improvement over baseline models is supported by the reported data, though the absence of baseline specification and training duration details reduces certainty about generalizability.
- **Low Confidence**: The scalability and robustness of the approach across different domains, dictionary qualities, and language pairs beyond the three tested remains unproven.

## Next Checks

1. **Ablation Study on Training Duration**: Systematically vary SFT epochs (1, 3, 5) and GRPO iterations (1000, 2000, 4000) to determine whether the reported improvements are stable or continue improving with more training.

2. **Dictionary Coverage Analysis**: Measure term match accuracy as a function of dictionary coverage percentage to quantify how sensitive the approach is to dictionary quality and completeness.

3. **Reward Function Sensitivity Test**: Conduct a systematic sweep of the BLEU vs. terminology reward weighting (e.g., 0.2/0.8, 0.4/0.6, 0.6/0.4, 0.8/0.2) to identify the Pareto frontier between term accuracy and translation quality.