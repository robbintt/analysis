---
ver: rpa2
title: 'Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in
  Social Network Services'
arxiv_id: '2506.03761'
source_url: https://arxiv.org/abs/2506.03761
tags:
- language
- arxiv
- large
- evaluation
- interactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pet-Bench is a new benchmark for evaluating Large Language Models
  as virtual pets on social networks. It measures both human-interaction and self-interaction,
  covering 7,500+ instances across tasks like pet chat, note understanding, memory,
  and daily routines.
---

# Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services

## Quick Facts
- arXiv ID: 2506.03761
- Source URL: https://arxiv.org/abs/2506.03761
- Reference count: 40
- Primary result: GPT-4o leads with 41.20 average score, outperforming open-source models by ~1%

## Executive Summary
Pet-Bench is a comprehensive benchmark evaluating Large Language Models as virtual pets in social network services. It introduces self-interaction capabilities alongside human-interaction, covering 7,500+ instances across tasks including pet chat, note understanding, memory, and daily routines. Experiments with 28 models reveal that closed-source models (e.g., GPT-4o at 41.20 average) outperform open-source alternatives, though the performance gap is small (~1%). Larger models generally perform better, but complex tasks like memory rewriting remain challenging. The benchmark provides a foundation for improving emotionally rich pet companion experiences.

## Method Summary
The benchmark evaluates LLMs across four main tasks: PetChat (conversations), PetNote (social media content understanding), PetMemory (memory-driven interactions), and PetRoutine (daily schedule generation). It uses 7,815 interaction instances with a composite metric combining BLEU-1-4, ROUGE-1, ROUGE-L, and BGE semantic similarity scores. For routine tasks, GPT-4o serves as a supplementary evaluator. The dataset underwent rigorous filtering and quality control using GPT-4 scoring and expert review. Evaluation was conducted on 28 open/closed-source models using 128 NVIDIA H800 GPUs.

## Key Results
- GPT-4o achieves the highest average score of 41.20 across all tasks
- Closed-source models outperform open-source ones by approximately 1%
- Larger architectures like Qwen-2.5-72B-Instruct achieve best open-source performance (40.87)
- Memory-based tasks pose significant challenges, particularly PetMemory-Rewrite
- Self-interaction capabilities remain difficult for current models to master

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-interaction capabilities enable more authentic virtual pet companionship than static role-playing alone.
- Mechanism: By requiring models to autonomously simulate behaviors, routines, and memory-driven actions (not just respond to user prompts), the benchmark forces models to demonstrate internal coherence and developmental progression over time.
- Core assumption: Authentic companionship requires the agent to initiate and evolve, not merely react.
- Evidence anchors:
  - [abstract] "Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship."
  - [section 3.1] "Self-interaction allows the model to autonomously simulate pet-like behaviors, routines, and memories."
  - [corpus] Related work on AI companionship development shows longitudinal evolution of human-AI relationships, supporting the need for developmental agent behaviors (arXiv:2510.10079).
- Break condition: If models can achieve high scores through pattern-matched responses without genuine internal state tracking, the self-interaction mechanism is not actually being tested.

### Mechanism 2
- Claim: Memory-based tasks expose fundamental limitations in current LLMs' ability to maintain coherent long-term companionship.
- Mechanism: Tasks requiring summarization, rewriting, and proactive memory-based dialogue force models to demonstrate both retention and contextual retrieval, revealing gaps between immediate response capability and sustained relationship modeling.
- Core assumption: Effective companionship requires remembering and building upon past interactions, not just context-window awareness.
- Evidence anchors:
  - [abstract] "Models struggle most with complex emotional and memory-based tasks, indicating the need for further optimization."
  - [section 4.3.1] "Tasks like PetMemory-Summary show strong results... In contrast, tasks like PetMemory-Rewrite pose significant challenges."
  - [corpus] Prior work identifies "limited episodic memory" as a key challenge in LLM-based virtual pets (cited in section 2), and MemoryBank (arXiv:2401.01222, referenced as [39]) attempts to address this.
- Break condition: If memory tasks can be solved through prompt engineering alone without architectural memory systems, the benchmark may not be measuring true memory capability.

### Mechanism 3
- Claim: Model scale correlates with task performance, but the performance gap between top closed-source and open-source models is narrowing (~1%).
- Mechanism: Larger parameter counts and superior training resources enable better handling of complex, multi-turn tasks, but instruction tuning and specialized optimization can compensate for size disadvantages.
- Core assumption: Performance differences reflect underlying capability gaps rather than evaluation metric artifacts.
- Evidence anchors:
  - [abstract] "Closed-source models outperform open-source ones, with GPT-4o leading (41.20 avg. score). However, the performance gap between the top closed- and open-source models is small (~1%)."
  - [section 4.3.1] "Among open-source models, larger architectures like Qwen-2.5-72B-Instruct achieve the best average score of 40.87, underscoring the importance of model size."
  - [corpus] No direct corpus evidence on the open/closed gap; related work focuses on companionship quality rather than model comparison. Corpus evidence weak here.
- Break condition: If evaluation metrics favor certain model families due to training data overlap or metric design, scale effects may be confounded.

## Foundational Learning

- Concept: **Role-playing vs. persona consistency in LLMs**
  - Why needed here: Pet-Bench distinguishes between "basic pet role-playing" and comprehensive companionship requiring sustained persona fidelity. Understanding this distinction is prerequisite to interpreting benchmark results.
  - Quick check question: Can you explain why a model might perform well on single-turn role-play but fail at multi-session persona consistency?

- Concept: **Memory architectures for conversational agents**
  - Why needed here: PetMemory tasks require more than context-window attention—they demand persistent storage, retrieval, and narrative reconstruction of past interactions.
  - Quick check question: What is the difference between in-context memory (attention window) and architectural memory (external storage with retrieval)?

- Concept: **Evaluation metrics for generative dialogue (BLEU, ROUGE, semantic similarity)**
  - Why needed here: The benchmark uses a composite score combining n-gram overlap metrics with embedding-based semantic similarity. Understanding what each metric captures is essential for diagnosing model failures.
  - Quick check question: Why might a model score high on BLEU but low on semantic similarity, and what would this indicate about its output quality?

## Architecture Onboarding

- Component map:
  PetChat -> PetNote -> PetMemory -> PetRoutine

- Critical path:
  1. Data Collection → Filtering (de-identification, de-sensitization, task-agnostic rules) → Refinement (annotation, GPT-4 quality scoring, expert review)
  2. Evaluation: Composite metric (BLEU-1-4 + ROUGE-1 + ROUGE-L + BGE semantic similarity) for most tasks; accuracy for Intent Recognition; GPT-4o scoring for Routine generation
  3. Testing: 28 models across three size tiers (1.5B+, 7B+, 32B+)

- Design tradeoffs:
  - **Breadth vs. depth**: 7,500+ instances across 14 task types provides comprehensive coverage but may dilute signal on any single capability.
  - **Automatic vs. human evaluation**: Using GPT-4 for quality control and routine scoring enables scale but introduces model-dependency bias.
  - **Self-interaction vs. human-interaction emphasis**: Including autonomous tasks increases ecological validity but complicates reproducibility compared to pure dialogue benchmarks.

- Failure signatures:
  - Low scores on PetMemory-Rewrite with high scores on PetMemory-Summary: Model can compress but not creatively reconstruct experiences.
  - High Routine Level III but low Routine-based Greet: Model generates detailed schedules but cannot naturally initiate conversation from them.
  - Strong Intent Recognition but weak Note-based Conversation: Model classifies user needs but cannot generate appropriate responses.

- First 3 experiments:
  1. **Baseline comparison**: Run Qwen-2.5-7B-Instruct (top mid-tier open-source) on all 14 subtasks to establish a per-category weakness profile before optimization.
  2. **Memory ablation**: Test whether adding external memory retrieval (e.g., RAG over conversation history) improves PetMemory-Rewrite scores specifically.
  3. **Cross-task transfer**: Fine-tune on PetRoutine data alone and measure transfer to PetChat-Routine dialogues to assess whether routine understanding generalizes to conversational context.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation metric dependencies: Heavy reliance on automatic metrics and GPT-4o as judge may not fully capture qualitative aspects of companionship
- Prompt sensitivity: Performance differences may be significantly influenced by prompt engineering rather than fundamental capability differences
- Reproducibility constraints: GPT-4o dependency for quality control introduces model-dependency that may affect reproducibility

## Confidence
- High confidence: Closed-source models outperform open-source models by ~1%, with GPT-4o achieving highest average score (41.20)
- Medium confidence: Self-interaction capabilities are essential for authentic virtual pet companionship
- Low confidence: Memory-based tasks reveal fundamental limitations in current LLMs' ability to maintain coherent long-term companionship

## Next Checks
1. **Prompt ablation study**: Systematically vary prompts across different model families to determine whether performance differences are primarily driven by model capabilities or prompt sensitivity, controlling for task complexity.
2. **Memory architecture comparison**: Compare standard LLM performance on PetMemory tasks against versions with external memory retrieval systems (RAG) to quantify the contribution of architectural memory versus in-context reasoning.
3. **Human evaluation validation**: Conduct a small-scale human evaluation on a subset of tasks to validate whether automatic metrics correlate with human judgments of companionship quality, particularly for emotional and memory-based tasks.