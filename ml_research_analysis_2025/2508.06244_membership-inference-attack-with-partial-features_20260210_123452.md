---
ver: rpa2
title: Membership Inference Attack with Partial Features
arxiv_id: '2508.06244'
source_url: https://arxiv.org/abs/2508.06244
tags:
- attack
- features
- s111
- s110
- s101
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of membership inference attacks
  when only partial features of a sample are available, which limits the applicability
  of existing methods. The authors propose MRAD (Memory-guided Reconstruction and
  Anomaly Detection), a two-stage attack framework that works in both white-box and
  black-box settings.
---

# Membership Inference Attack with Partial Features

## Quick Facts
- arXiv ID: 2508.06244
- Source URL: https://arxiv.org/abs/2508.06244
- Reference count: 40
- Key outcome: MRAD framework achieves AUC ~0.75 on STL-10 with 60% feature missingness

## Executive Summary
This paper addresses membership inference attacks under partial feature availability, a setting where only subset of features are observable. The authors propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage framework that reconstructs unknown features using model memory and detects distributional anomalies. The method works in both white-box and black-box settings, maintaining effectiveness even with significant feature missingness. On STL-10, the attack achieves an AUC of around 0.75 even when 60% of the features are missing.

## Method Summary
MRAD is a two-stage attack framework for Partial Feature Membership Inference. Stage 1 uses gradient descent (white-box) or zeroth-order gradient estimation (black-box) to reconstruct unknown features by minimizing the target model's training loss, guided by known features. Stage 2 applies anomaly detection to measure deviation between reconstructed samples and training data distribution using class centroids and dispersion metrics. The method requires auxiliary data to compute class statistics and optionally uses a shadow model to calibrate thresholds. Hyperparameters include reconstruction step size (η=1000, N_itr=1) and query budget for black-box variants.

## Key Results
- Achieves AUC ~0.75 on STL-10 with 60% feature missingness
- Outperforms existing methods like DLG, ZeroAE, and MRD by 2.6%-11.9% AUC
- Black-box variant maintains performance close to white-box, with minimal AUC gap on some datasets
- Ablation studies confirm reconstruction optimization is essential (random initialization yields near-random AUC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reconstructing unknown features via gradient descent on the target model's loss surface produces distribution-aligned samples for member features and anomalous reconstructions for non-members.
- Mechanism: Known features serve as optimization anchors; backpropagation updates only unknown features to minimize the model's training loss, recovering the model's "memory" of training data.
- Core assumption: Models memorize training data by forming low-loss regions in input space, and member features guide reconstruction toward these regions while non-members do not.
- Evidence anchors:
  - [abstract] "MRAD leverages the latent memory of the target model to reconstruct the unknown features of the sample. We observe that when the known features are absent from the training set, the reconstructed sample deviates significantly from the true data distribution."
  - [Section IV-A] "overfitted models memorize their training data, causing training members to occupy regions of the input space associated with low loss values"
  - [corpus] Related work (e.g., Yeom et al. 2018) establishes the connection between overfitting, memorization, and MIA vulnerability, supporting the memorization premise.
- Break condition: If models do not memorize training data (e.g., heavily regularized or underfit models), or if known features provide insufficient signal, reconstruction may not differentiate members from non-members.

### Mechanism 2
- Claim: Anomaly detection on reconstructed samples identifies distribution deviation, serving as a membership proxy when exact feature matching is impossible.
- Mechanism: Compute deviation of reconstructed samples from the training distribution (via class centroids and dispersion metrics); higher deviation indicates non-member features.
- Core assumption: Member feature reconstructions lie closer to the training data distribution than non-member reconstructions.
- Evidence anchors:
  - [abstract] "In the second stage, we use anomaly detection algorithms to measure the deviation between the reconstructed sample and the training data distribution, thereby determining whether the known features belong to a member of the training set."
  - [Section IV-B] Algorithm 2 formalizes deviation distance using Median Absolute Deviation (MAD) and class centroids.
  - [corpus] CADE (Yang et al. 2021) is cited as a concept drift method using autoencoders and MAD, supporting the use of deviation metrics for distribution shift detection.
- Break condition: If reconstruction collapse causes both member and non-member reconstructions to appear similarly anomalous, or if the anomaly detector is poorly calibrated, discrimination fails.

### Mechanism 3
- Claim: Zeroth-order gradient estimation enables black-box attack by approximating gradients from input-output queries.
- Mechanism: Finite-difference approximation using random perturbations estimates gradients without access to internal model parameters.
- Core assumption: Sufficiently many random directions (m) and small perturbations (ε) yield gradient estimates accurate enough for reconstruction.
- Evidence anchors:
  - [Section IV-C] "Zeroth-order gradient estimation approximates the gradient via finite differences" with formula (4).
  - [Section V-F] Black-box experiments show performance close to white-box, with AUC differences minimized on some datasets.
  - [corpus] No direct corpus evidence on zeroth-order methods in MIA; labeled as Assumption.
- Break condition: If query budget is limited (small m), gradient estimates become noisy, degrading reconstruction and attack performance. High query volumes may trigger detection.

## Foundational Learning

- Concept: Membership Inference Attack (MIA)
  - Why needed here: PFMI is a constrained variant of MIA; understanding standard MIA motivates the reconstruction-based approach.
  - Quick check question: Given a model and a sample, can you determine if the sample was in the training set using model loss?

- Concept: Model Memorization and Overfitting
  - Why needed here: The attack exploits memorization; knowing how overfitting creates "loss valleys" explains why reconstruction works.
  - Quick check question: Why do training samples tend to have lower loss than test samples in overfitted models?

- Concept: Anomaly Detection (e.g., MAD, Centroid-based Methods)
  - Why needed here: Stage 2 relies on detecting distributional deviation; familiarity with outlier scoring is essential.
  - Quick check question: How would you measure if a point is far from the "typical" points of a class using robust statistics?

## Architecture Onboarding

- Component map:
  - Known features + mask → gradient descent on loss → reconstructed sample → deviation from centroid → membership decision
  - Optional: Shadow model for reference deviation (δs) to calibrate thresholds
  - Black-box variant: Zeroth-order gradient estimator replaces direct gradient access

- Critical path:
  1. Obtain known features and mask M
  2. Initialize unknown features (e.g., zeros)
  3. Run N_itr iterations of gradient descent (white-box) or zeroth-order estimation (black-box) to minimize loss
  4. Compute deviation δ of reconstructed sample from class centroid using auxiliary data
  5. Optionally compute shadow deviation δs
  6. Compare δs/δ to threshold τ; infer membership

- Design tradeoffs:
  - Hyperparameters (η, N_itr): Larger η with fewer iterations is faster and often more stable (per Section V-C)
  - Anomaly detection method: Paper tests CADE, MSAD, NCI; choice depends on dataset properties and available resources
  - Black-box query budget (m): Higher m improves gradient estimates but increases queries and detection risk

- Failure signatures:
  - Ablation (no reconstruction optimization) yields near-random AUC (~0.5), confirming Stage 1 is essential
  - When known-feature ratio is very high (>90%), attack degrades because samples align with distribution regardless of membership
  - Black-box attack may fail if query budget is too low or perturbations are poorly chosen

- First 3 experiments:
  1. Reproduce white-box attack on CIFAR-10 with 50% known features using the paper's hyperparameters (η=1000, N_itr=1) and NCI for anomaly detection. Verify AUC ~0.7
  2. Ablation: Run the same experiment but skip gradient optimization (use random initialization for unknown features). Confirm near-random performance
  3. Black-box test: Implement zeroth-order gradient estimation with m=100, ε=0.01 on Fashion-MNIST with 70% known features. Compare AUC to white-box baseline and note the gap

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do existing privacy defenses, such as Differential Privacy (DP) and Machine Unlearning, mitigate the risks of Partial Feature Membership Inference?
- **Basis in paper:** [explicit] Section VI-B highlights the need to re-evaluate defenses like unlearning and overfitting mitigation to see if they block this specific attack vector.
- **Why unresolved:** It is unclear if "forgetting" a sample or adding noise prevents the model from reconstructing unknown features based on partial inputs, as the model may retain partial feature-level memorization.
- **What evidence would resolve it:** Empirical evaluation of the MRAD framework's AUC on models trained with differential privacy or subjected to machine unlearning algorithms.

### Open Question 2
- **Question:** Can advanced optimization strategies (e.g., momentum, adaptive step-size) stabilize the reconstruction process and improve attack performance under limited query budgets?
- **Basis in paper:** [explicit] Section VI-B suggests that adopting advanced strategies could stabilize reconstruction and enhance performance where query budgets are limited.
- **Why unresolved:** The current implementation uses basic gradient descent (or zeroth-order approximation) which may converge to poor local minima or require excessive queries to reconstruct features accurately.
- **What evidence would resolve it:** Comparative experiments integrating optimizers like Adam or momentum into the reconstruction stage to measure convergence speed and attack success rates.

### Open Question 3
- **Question:** Is the MRAD framework effective against modern architectures like Vision Transformers (ViT) or Large Language Models (LLMs) where feature memorization differs from CNNs?
- **Basis in paper:** [inferred] The empirical evaluation is restricted to ResNet50 and MLPs (Section V-A), leaving the attack's efficacy on other model families unknown.
- **Why unresolved:** ViTs and LLMs process data via attention mechanisms rather than the convolutional features used in the study; the "memory-guided reconstruction" via loss minimization may not yield distinct enough anomalies for detection in these architectures.
- **What evidence would resolve it:** Applying the MRAD framework to transformer-based models to verify if reconstructed non-member samples still deviate significantly from the training distribution.

## Limitations
- Attack effectiveness degrades significantly when known-feature ratios exceed 90%
- Zeroth-order gradient estimation performance depends heavily on query budget and perturbation selection
- Method assumes model memorization creates exploitable low-loss regions, which may not hold for heavily regularized models

## Confidence
- **High confidence**: The two-stage reconstruction-anomaly detection framework is technically sound and well-supported by ablation studies showing reconstruction is essential
- **Medium confidence**: The zeroth-order gradient estimation method works as claimed, though limited corpus evidence exists for its application in MIA
- **Medium confidence**: The attack's effectiveness at high feature missingness (60%) is demonstrated but may be dataset-dependent

## Next Checks
1. Test MRAD on a heavily regularized model (e.g., with dropout or weight decay) to verify if the reconstruction mechanism fails when memorization is suppressed
2. Systematically vary the query budget m in black-box attacks to quantify the trade-off between gradient estimation accuracy and attack performance
3. Compare MRAD's performance using different anomaly detection methods (CADE, MSAD, NCI) on the same dataset to identify which method works best under varying feature missingness levels