---
ver: rpa2
title: Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment,
  and Belief-Action Coherence?
arxiv_id: '2511.21218'
source_url: https://arxiv.org/abs/2511.21218
tags:
- human
- uni00000051
- uni00000003
- uni00000044
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether fine-tuning large language models
  (LLMs) on small human samples can improve their realism as experimental participants.
  Using a behavioral experiment on information disclosure, the authors compare human
  and LLM-generated responses across heterogeneity, alignment, belief-action coherence,
  and regression coefficient recovery.
---

# Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?

## Quick Facts
- arXiv ID: 2511.21218
- Source URL: https://arxiv.org/abs/2511.21218
- Reference count: 34
- Primary result: Fine-tuning LLMs on as few as 30 human observations substantially improves heterogeneity, alignment, and belief-action coherence, but fails to reproduce regression coefficients

## Executive Summary
This study investigates whether fine-tuning large language models (LLMs) on small human samples can improve their realism as experimental participants. Using a behavioral experiment on information disclosure, the authors compare human and LLM-generated responses across heterogeneity, alignment, belief-action coherence, and regression coefficient recovery. Fine-tuning on as few as 30 human observations substantially improves heterogeneity, distributional alignment, and belief-action coherence relative to the base model. However, even the best-performing fine-tuned models fail to reproduce the original regression coefficients, suggesting that LLM-generated data remain unsuitable for formal inferential analyses. The findings indicate that while fine-tuned LLMs can better mimic human diversity and behavior for exploratory or pretesting purposes, they cannot fully replace human participants in statistical inference.

## Method Summary
The authors use a 2×4 factorial behavioral experiment with 929 human participants to test whether fine-tuning GPT-4.1 on small human samples improves LLM realism. They create 6 fine-tuned variants using random and balanced sampling strategies across 1, 4, or 8 treatment groups, with dataset sizes representing 25% of the full data. The evaluation measures heterogeneity (unique belief structures), distributional alignment (Jensen-Shannon distance), belief-action coherence (value-action gap), and regression coefficient recovery (F1-Sign, FPR, FNR). All models generate responses at scale with temperature=1.0 for comparison against human data.

## Key Results
- Fine-tuning on 30 human observations increases unique belief structures from 19 (base) to 200+ across all fine-tuned models
- Distributional alignment improves significantly, with JS distance decreasing by at least half across all subgroups
- Balanced sampling consistently outperforms random sampling for belief-action coherence (matching human rates of 23-24%)
- None of the fine-tuned models accurately replicate regression coefficients (best F1-Sign ≈0.77, high FPR ≈0.33)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on as few as 30 human observations substantially increases response heterogeneity.
- Mechanism: Fine-tuning shifts the model away from a low-variance prior toward the empirical distribution of human responses. The base model's concentrated belief distribution (30–50% bins) expands to cover the full probability range after exposure to diverse human patterns.
- Core assumption: The small sample sufficiently captures the variance structure of the full population.
- Evidence anchors:
  - [abstract] "Fine-tuning on as few as 30 human observations substantially improves heterogeneity"
  - [Section 5.1] "The unfinetuned model produces only 19 unique belief structures across all 929 simulations, compared to 340 in the human data. All finetuned models produce at least 200 unique combinations"
  - [corpus] Weak direct support; neighbor papers focus on behavioral simulation rather than heterogeneity mechanisms specifically.
- Break condition: If the pilot sample is unrepresentative (e.g., drawn from a single demographic or treatment condition), heterogeneity gains may not generalize.

### Mechanism 2
- Claim: Fine-tuning reduces distributional misalignment across demographic subgroups, but WEIRD bias persists.
- Mechanism: Exposure to human response patterns during fine-tuning adjusts the model's output distribution to better match empirical data, as measured by Jensen-Shannon distance. However, the model's training corpus continues to exert influence—alignment remains strongest for White participants even under balanced sampling.
- Core assumption: Demographic information in prompts can steer responses; the paper's ablation study suggests this assumption may not hold strongly.
- Evidence anchors:
  - [abstract] "Fine-tuning on small human samples substantially improves heterogeneity, distributional alignment, and belief-action coherence"
  - [Section 5.2] "Across all finetuned variants, the JS distance decreases by at least half... the lowest JS distance is consistently observed in the White participant category"
  - [corpus] Neighbor paper "Can LLMs Simulate Human Behavioral Variability?" (arXiv:2505.16164) examines LLM ability to approximate individual differences in cognitive tasks, providing convergent context on variability challenges.
- Break condition: Balanced sampling alone does not eliminate subgroup misalignment; more sophisticated strategies may be required.

### Mechanism 3
- Claim: Fine-tuning reduces the value-action gap, with balanced sampling outperforming random sampling for this metric.
- Mechanism: Fine-tuning exposes the model to coherent belief-action pairs from human data, teaching it that stated beliefs should constrain subsequent decisions. Balanced sampling ensures exposure to diverse behavioral patterns across subgroups, improving coherence.
- Core assumption: The belief-action relationship in the training sample reflects the true underlying relationship.
- Evidence anchors:
  - [Section 5.3] "The value-action gap for the unfinetuned baseline is close to 100%... models finetuned on 25% of all eight treatment groups attain discrepancy rates that closely match human participants (23.8% and 24.4% vs. 24.7%)"
  - [Section 6] "balanced sampling performs consistently better than random sampling, suggesting that belief-action coherence may be easier to steer than subgroup-level distributional alignment"
  - [corpus] No direct corpus support for this specific mechanism; the value-action gap framework is cited from Shen et al. (2025).
- Break condition: If the pilot sample contains noisy or inconsistent belief-action pairs, coherence improvements may be limited or misleading.

## Foundational Learning

- Concept: **Jensen-Shannon (JS) distance**
  - Why needed here: Primary metric for quantifying distributional alignment between LLM and human responses; bounded and symmetric.
  - Quick check question: Can you explain why JS distance is preferred over KL divergence for comparing response distributions?

- Concept: **Value-action gap**
  - Why needed here: Critical behavioral construct measuring coherence between stated beliefs and subsequent decisions; base models exhibit near-complete incoherence.
  - Quick check question: How would you operationalize a value-action gap in a survey simulation context?

- Concept: **Identity flattening**
  - Why needed here: Explains why persona-based prompting fails—LLMs produce stereotyped, within-group homogeneous responses rather than authentic diversity.
  - Quick check question: What evidence would distinguish between "authentic in-group diversity" and "out-group stereotyping" in LLM outputs?

## Architecture Onboarding

- Component map:
  - Data source -> Fine-tuning variants -> Evaluation metrics -> Validation
  - 929 human participants -> 6 models (2 strategies × 3 sizes) -> JS distance, gap, F1(Sign) -> Generalization assessment

- Critical path:
  1. Pilot data collection (≥30 observations per treatment group)
  2. Format data for fine-tuning (include demographics, beliefs, decisions)
  3. Fine-tune with balanced sampling if subgroup alignment is critical
  4. Generate synthetic responses at scale
  5. Validate heterogeneity and alignment before using for survey pretesting

- Design tradeoffs:
  - **Random vs. balanced sampling**: Balanced improves value-action coherence; random may suffice for overall heterogeneity
  - **More treatment groups vs. data efficiency**: Adding groups improves alignment but requires larger pilot samples
  - **Speed vs. fidelity**: Fine-tuning improves realism but cannot recover regression coefficients—unsuitable for inferential use

- Failure signatures:
  - F1(Sign) < 0.8 for coefficient recovery indicates unsuitability for hypothesis testing
  - Value-action gap > 50% suggests insufficient coherence training
  - JS distance concentrated in minority subgroups signals persistent WEIRD bias

- First 3 experiments:
  1. **Ablation on data components**: Remove belief responses, decision responses, or demographics during fine-tuning to identify which signals drive alignment improvements (paper's Appendix E provides template)
  2. **Cross-treatment generalization**: Fine-tune on subset of treatment groups, evaluate on held-out groups to test generalization
  3. **Sample size sensitivity**: Vary pilot sample from 15–100 observations to identify minimum viable sample for heterogeneity gains

## Open Questions the Paper Calls Out
- Why does improved distributional alignment and belief-action coherence fail to translate into accurate recovery of regression coefficients?
- Can advanced fine-tuning strategies overcome the persistent alignment gap for minority subgroups (non-WEIRD populations)?
- Do these findings generalize across diverse behavioral domains and different model architectures?

## Limitations
- Cannot recover regression coefficients even with improved heterogeneity and alignment
- Based on single experimental paradigm (information disclosure in security context)
- Persistent WEIRD bias remains despite balanced sampling strategies
- Unclear minimum sample size threshold for reliable fine-tuning effects

## Confidence

- High: Fine-tuning improves heterogeneity and distributional alignment relative to base models
- Medium: Balanced sampling improves belief-action coherence over random sampling
- Medium: LLM-generated data cannot replace human participants for regression coefficient recovery
- Low: The minimum sample size threshold for reliable fine-tuning effects

## Next Checks
1. **Cross-paradigm generalization**: Test whether heterogeneity and alignment improvements transfer to a different behavioral experiment (e.g., trust game, public goods)
2. **Sample size sensitivity**: Systematically vary pilot sample size (15-100 observations) to identify the minimum viable sample for heterogeneity gains
3. **Out-of-distribution robustness**: Fine-tune on subset of treatment groups, evaluate on held-out groups to test generalization beyond training data