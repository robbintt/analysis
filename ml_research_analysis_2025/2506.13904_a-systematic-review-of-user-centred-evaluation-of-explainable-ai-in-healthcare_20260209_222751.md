---
ver: rpa2
title: A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare
arxiv_id: '2506.13904'
source_url: https://arxiv.org/abs/2506.13904
tags:
- user
- studies
- system
- explanation
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of standardized evaluation methods
  for user-centric explainable AI (XAI) in healthcare. It systematically reviews 82
  user studies to identify and define key properties of the XAI user experience and
  provides context-sensitive evaluation guidelines.
---

# A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare

## Quick Facts
- **arXiv ID:** 2506.13904
- **Source URL:** https://arxiv.org/abs/2506.13904
- **Reference count:** 40
- **Primary result:** Systematically reviews 82 user studies to identify 30+ atomic properties for user-centric XAI evaluation in healthcare, providing context-sensitive evaluation guidelines.

## Executive Summary
This systematic review addresses the critical need for standardized evaluation methods for user-centric explainable AI (XAI) in healthcare. By analyzing 82 user studies, the authors identify and define key properties of the XAI user experience, such as Understanding, Trust, Confidence, and Explanation Power. The study introduces a layered framework that connects domain context, AI context, explanation design, and evaluation design to guide the selection of relevant properties for assessment. The findings emphasize the importance of domain-specific evaluation and the need for robust, human-centered approaches to XAI evaluation, providing actionable guidelines for interdisciplinary teams.

## Method Summary
The study conducted a systematic literature review across five databases (Scopus, Web of Science, ACM, IEEE, PubMed) using a PICO framework focused on human evaluation of AI explanations in healthcare. From an initial 8,226 papers, 82 were selected after screening for real AI models, reproducible XAI methods, healthcare settings, and human assessment. The analysis employed deductive coding using a pre-existing framework and inductive coding for new properties, resulting in a unified framework of 30+ atomic evaluation properties.

## Key Results
- Identified 82 relevant user studies evaluating XAI in healthcare settings
- Defined 30+ atomic properties for XAI evaluation, categorized as Objective/Subjective and User/System-Centered
- Developed a layered framework (Domain → AI → Explanation → Evaluation) for context-sensitive property selection
- Found that properties like Understanding, Trust, Confidence, and Explanation Power are most frequently evaluated
- Highlighted the influence of Case Difficulty and Domain Experience on evaluation outcomes

## Why This Works (Mechanism)

### Mechanism 1: Layered Context Filtering
- **Claim:** If evaluation criteria are filtered through a four-layer context model (Domain → AI → Explanation → Evaluation), the relevance of the selected properties appears to increase compared to generic checklists.
- **Mechanism:** The framework forces a dependency chain where high-level context (e.g., "Decision Support") gates the available properties (e.g., prioritizing "Performance" and "Reliance" over "Model Auditing" properties). This reduces the search space of measurable attributes to those logically connected to the user's task.
- **Core assumption:** Assumes that user needs and appropriate evaluation metrics are contingent on the specific "Usage Context" (e.g., Capability Assessment vs. Decision Support) rather than being universal.
- **Evidence anchors:**
  - [abstract] Mentions providing "context-sensitive evaluation guidelines."
  - [section 6] Describes the "Layered model to design evaluations" where "each layer is informed by all previously encompassing layers."
  - [corpus] "Unifying VXAI" corroborates the struggle with inconsistent evaluation, supporting the need for a structured framework.
- **Break condition:** If the system serves multiple usage contexts simultaneously (e.g., a tool used for both triage and training), the linear filtering may fail to resolve conflicting property priorities.

### Mechanism 2: Atomic Property Decomposition
- **Claim:** Disentangling broad concepts like "Understanding" and "Trust" into atomic, non-overlapping properties likely reduces measurement ambiguity.
- **Mechanism:** By separating "Understanding Explanation" (interpreting the UI) from "Understanding Model Behavior" (inferring logic), and "Explanation Power" (convincingness) from "Reliance" (ceding control), evaluators can pinpoint exactly where the user experience fails.
- **Core assumption:** Assumes that users cognitively distinguish between these nuances (e.g., trusting an explanation vs. trusting the model) and that standard instruments (like SUS) conflate them, hiding specific failure modes.
- **Evidence anchors:**
  - [section 7.1.1] Notes that "Satisfaction" is often conflated with "Usefulness" and "Cognitive Load" in other frameworks, justifying the need for disentanglement.
  - [section 4.12.6] Explicitly splits "Understanding" into two factors based on analysis of 12 papers.
  - [corpus] "Explanation User Interfaces" supports the focus on UI-specific evaluation but does not contradict the decomposition.
- **Break condition:** If the measurement instrument (questionnaire) fails to isolate these concepts linguistically, users may still answer based on a "halo effect" (general feeling), rendering the decomposition theoretical only.

### Mechanism 3: Situational & Personal Characteristic Weighting
- **Claim:** Evaluation outcomes in healthcare appear to be heavily modulated by "Case Difficulty" and "Domain Experience," suggesting these must be treated as control variables rather than noise.
- **Mechanism:** The review finds that high Case Difficulty increases user Curiosity and Usefulness ratings, while Domain Experience impacts Trust and Confidence. Ignoring these factors skews the perceived effectiveness of the XAI.
- **Core assumption:** Assumes that the value of an explanation is relative to the user's expertise and the task's difficulty (e.g., explanations are more useful for hard cases).
- **Evidence anchors:**
  - [section 4.12.2] Details how "Case Difficulty" influences Confidence, Reliance, and Curiosity.
  - [section 4.12.1] Discusses how "Domain Experience" impacts Performance and Trust.
  - [corpus] Related papers focus on trust gaps in specific domains (e.g., maternal health), implicitly supporting the role of context, but specific variable weightings are not confirmed externally.
- **Break condition:** If the user population is homogenized (e.g., only senior experts or only easy cases), these variables will not show variance, potentially masking the system's true utility for edge cases.

## Foundational Learning

- **Concept: Usage Context Taxonomy**
  - **Why needed here:** The framework dictates that *what* you measure depends entirely on *why* the AI is being used (e.g., "Model Auditing" requires Model-Centered properties, while "Decision Support" requires User Experience properties).
  - **Quick check question:** Is the system being used to validate the model (Auditing/Capability) or to assist a clinical decision (Decision Support)?

- **Concept: Objective vs. Subjective System Aspects**
  - **Why needed here:** You must distinguish between system capabilities (Objective, e.g., Model Certainty) and user perceptions of those capabilities (Subjective, e.g., Perceived Model Competence), as they are measured differently and have distinct relationships to Trust.
  - **Quick check question:** Are you measuring the mathematical uncertainty of the model (Objective) or the user's feeling of how good the model is (Subjective)?

- **Concept: Explanation Power vs. Reliance**
  - **Why needed here:** In healthcare, users may find an explanation convincing (High Explanation Power) but still refuse to delegate decision-making (Low Reliance) due to liability; measuring only one misrepresents adoption potential.
  - **Quick check question:** Are you asking if the user agrees with the logic (Power) or if they would let the machine act autonomously (Reliance)?

## Architecture Onboarding

- **Component map:** Domain Context (Users, Data, Task) → AI Context (Usage Context, AI Model) → Explanation Design (Scope, Format, Interactivity) → Evaluation Design (Study Type, Properties, Methods)

- **Critical path:**
  1. **Define Context:** Map the specific healthcare domain and user role (Section 6.1).
  2. **Select Contextualized Properties:** Use the "Usage Context" to filter relevant properties via the decision trees (Section 6.4, Figs 12-14).
  3. **Check Relations:** Review the "Relations between properties" map (Fig 9) to ensure you aren't missing mediating factors (e.g., measuring Trust without measuring Information Correctness).

- **Design tradeoffs:**
  - **Quantitative vs. Mixed Methods:** Quantitative studies measure fewer properties (avg 4) but are faster; Mixed methods capture more (avg 7) but require more user time (Section 4.8).
  - **Standardized vs. Tailored Metrics:** Standard scales (SUS, TAM) are easy but conflate properties; tailored questions are precise but lack validation (Section 7.1.3).

- **Failure signatures:**
  - **Conflation:** Using "Satisfaction" as a proxy for "Trust" (Section 7.1.1).
  - **Sample Size Collapse:** Running quantitative analysis on <16 users (Section 7.2.1).
  - **Liability Blindness:** Measuring "Reliance" without accounting for the "responsibility" context in healthcare (Section 4.12.7).

- **First 3 experiments:**
  1. **Context Audit:** Apply the "Domain Context" layer (Fig 12) to an existing XAI tool to determine if the current user base matches the intended "Domain Experience" level.
  2. **Property De-duplication:** Review existing evaluation questionnaires against the Table 9 definitions to identify overlapping definitions (e.g., distinguishing *Perceived User Performance* from *Confidence*).
  3. **Difficulty Stratification:** Re-analyze existing user study data by segmenting results based on "Case Difficulty" to see if explanation usefulness varies (as suggested in Section 4.12.2).

## Open Questions the Paper Calls Out

- **Can the proposed user-centric evaluation framework be effectively applied to critical domains outside of healthcare, such as education or industrial settings?**
  - **Basis in paper:** [explicit] The conclusion states the framework "should be tested in other critical domains" to determine if domain-specific analyses are necessary for distinct evaluation needs.
  - **Why unresolved:** The current systematic review was restricted to healthcare settings, so the generalizability of the identified properties and guidelines to other high-stakes environments remains empirically unverified.
  - **What evidence would resolve it:** Successful application and validation of the framework in user studies conducted within non-healthcare domains, comparing the relevance of specific properties.

- **How can standardized measurement instruments be developed to reliably assess atomic properties (e.g., Information Correctness, Confidence) across different XAI contexts?**
  - **Basis in paper:** [explicit] The discussion highlights "gaps in the availability and appropriateness of existing measurement metrics" and suggests conducting studies to "specifically aim to validate and create constructs."
  - **Why unresolved:** The review found that most studies created bespoke questionnaires rather than reusing standard instruments, making cross-study comparison difficult and hindering consistent evaluation.
  - **What evidence would resolve it:** The development and validation of a standardized set of survey items or metrics that demonstrate reliability and validity across multiple XAI user studies.

- **Is the proposed layered guideline effective in supporting interdisciplinary teams to design and select appropriate evaluation strategies in real-world settings?**
  - **Basis in paper:** [explicit] The authors state that "further empirical validation is needed to assess the usability and effectiveness of these recommendations in real-world settings."
  - **Why unresolved:** While the guidelines are grounded in a systematic review of literature, they have not yet been tested for their practical utility by the intended users (interdisciplinary teams) during the design process.
  - **What evidence would resolve it:** User studies where interdisciplinary teams utilize the guidelines to design evaluations, followed by qualitative and quantitative assessment of the guidelines' utility and clarity.

## Limitations

- The framework's applicability to non-radiology domains remains untested, as 40% of the corpus focused on imaging data.
- The decision trees for property selection (Figs 12-14) were derived from the corpus but not validated against external use cases.
- The atomic property definitions, while well-justified, lack direct validation that users distinguish between them during evaluation.

## Confidence

- **High Confidence:** The identification of 30+ evaluation properties and their categorization into Objective/Subjective and User/System-Centered aspects is well-supported by the corpus analysis.
- **Medium Confidence:** The layered framework's effectiveness in guiding property selection is logically sound but requires empirical validation in real evaluation design.
- **Low Confidence:** The specific weightings and relationships between properties (e.g., how Case Difficulty modulates Curiosity) are based on corpus patterns that may not generalize beyond the studied papers.

## Next Checks

1. Apply the layered framework to an existing XAI tool outside of radiology to test the context-filtering mechanism's validity.
2. Conduct cognitive interviews with users to verify they distinguish between atomic properties like "Understanding Explanation" vs. "Understanding Model Behavior."
3. Run a controlled experiment varying Case Difficulty and Domain Experience to empirically confirm the hypothesized relationships with properties like Trust and Confidence.