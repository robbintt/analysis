---
ver: rpa2
title: Evidence-Guided Schema Normalization for Temporal Tabular Reasoning
arxiv_id: '2512.00329'
source_url: https://arxiv.org/abs/2512.00329
tags:
- 'null'
- foreign
- references
- primary
- snapshot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a SQL-based approach for temporal reasoning
  over evolving semi-structured tables, addressing the challenge of processing Wikipedia
  infobox timelines. The core method involves generating a normalized 3NF database
  schema from temporal JSON data, then using schema-guided few-shot prompting to generate
  and execute SQL queries.
---

# Evidence-Guided Schema Normalization for Temporal Tabular Reasoning

## Quick Facts
- arXiv ID: 2512.00329
- Source URL: https://arxiv.org/abs/2512.00329
- Reference count: 18
- Key outcome: Schema design quality has greater impact on temporal QA precision than model capacity

## Executive Summary
This paper proposes a SQL-based approach for temporal reasoning over evolving semi-structured tables, addressing the challenge of processing Wikipedia infobox timelines. The core method involves generating a normalized 3NF database schema from temporal JSON data, then using schema-guided few-shot prompting to generate and execute SQL queries. The key finding is that schema design quality has a greater impact on QA precision than model capacity. The best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieved 80.39 EM, a 16.8% improvement over the baseline (68.89 EM). Surprisingly, schemas from the smaller Gemini 2.5 Flash consistently outperformed those from larger models, demonstrating that balanced normalization, semantic naming, and consistent temporal anchoring are more critical than model size for temporal QA performance.

## Method Summary
The proposed method consists of three stages: (1) LLM generates a 3NF schema from 2-3 example JSON timelines per domain, (2) automated Python scripts populate SQLite databases from parsed JSON with cleaning and referential integrity, and (3) schema-guided few-shot prompting generates SQL queries executed on the populated database. The system uses six query-generation LLMs and three schema-generation LLMs with temperature=0.1, top-p=0.9, and max tokens=2048. The TRANSIENTTABLES benchmark dataset provides JSON-formatted Wikipedia infobox snapshots across 10 domains.

## Key Results
- Best configuration achieved 80.39 EM (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries)
- 16.8% improvement over baseline (68.89 EM)
- Balanced normalization (3-5 tables) outperforms over-normalized schemas (6-8 tables)
- Schema quality impact exceeds model capacity impact on precision

## Why This Works (Mechanism)

### Mechanism 1: Balanced Normalization Preserves Reasoning Context
Reducing schema complexity to "balanced" 3NF improves query generation accuracy by maintaining semantic context across joins. Smaller models generate schemas with 3-5 tables, whereas larger models tend to over-normalize into 6-8 tables. Excessive fragmentation forces complex multi-hop joins, increasing logical errors. Balanced schemas consolidate related data, reducing cognitive load on the SQL generation model.

### Mechanism 2: Schema-Guided Prompting Reduces Search Space
Explicitly encoding schema constraints and temporal semantics in the prompt reduces hallucinated columns or invalid joins. The prompt includes gold few-shot examples tailored to temporal patterns, constraining the LLM to select from valid SQL templates rather than generating free-form text.

### Mechanism 3: Symbolic Verification of Temporal State
Converting implicit temporal reasoning into explicit symbolic operations enables verifiable, exact state tracking. Temporal questions requiring state changes are grounded in deterministic data retrieval (joins, date arithmetic) rather than probabilistic text generation.

## Foundational Learning

- **Concept: Third Normal Form (3NF)**
  - Why needed: Understand trade-off between atomicity and context to diagnose why Flash schemas outperformed Pro
  - Quick check: Given `TeamStats` with `[Date, Wins, Losses, CoachName]`, would splitting `CoachName` into separate table improve or complicate "Wins per Coach in 2023"?

- **Concept: Temporal Anchoring (Snapshot Models)**
  - Why needed: System relies on `snapshot_id` to order events; understand modeling time-series data in SQL
  - Quick check: How would you write SQL clause to find value of column `X` in snapshot immediately preceding current row?

- **Concept: Text-to-SQL Semantic Parsing**
  - Why needed: Core task is mapping NL to SQL; grasp how LLMs map ambiguous terms to specific SQL operators
  - Quick check: If user asks "How long did X serve?", which SQL function computes duration between start and end timestamp?

## Architecture Onboarding

- **Component map:** JSON timelines -> Schema Generator -> CREATE TABLE SQL -> Population Pipeline -> INSERT data -> Prompt Constructor -> Query Generator -> SQL queries -> Execution Engine -> Results
- **Critical path:** Schema Generator defines logic surface area; if over-fragmented, Query Generator struggles with complex joins; Population Pipeline is biggest engineering bottleneck
- **Design tradeoffs:** Use smaller, faster models (Flash) for schema generation to ensure "balanced" normalization; SQL sacrifices flexibility for precision
- **Failure signatures:** Empty results/wrong calculations from entity variant mismatches; aggregate misuse; over-fragmentation requiring 6+ joins
- **First 3 experiments:** (1) Schema Sanity Check - verify table counts stay 3-5 (Flash-like) vs 8+ (Pro-like), (2) Population Stress Test - measure IntegrityError or parsing failures, (3) Ablation on Few-Shots - remove temporal-specific examples and measure drop in duration-based questions

## Open Questions the Paper Calls Out

- **Cross-domain temporal reasoning:** How to extend framework to support queries joining schemas across different entity types? Authors explicitly list this in Future Works, noting current restriction to single domains.
- **Automated schema optimization:** Can designs be automatically optimized based on downstream query execution feedback? Authors propose this as future work to reduce dependency on initial schema quality.
- **Generalization beyond Wikipedia:** Does pipeline generalize to semi-structured data sources outside Wikipedia without substantial manual re-engineering? Authors note Wikipedia-specific data assumptions as limitation.

## Limitations
- Performance may not generalize beyond Wikipedia infobox temporal patterns
- Reliance on proprietary models (Gemini) limits reproducibility
- No systematic evaluation of open-source model alternatives
- Schema generation quality highly sensitive to prompt engineering

## Confidence

- **High Confidence:** Schema design quality impacts QA precision more than model capacity (80.39 vs 68.89 EM)
- **Medium Confidence:** Balanced normalization preserves reasoning context mechanism
- **Medium Confidence:** Schema-guided prompting reduces search space and improves accuracy

## Next Checks

1. **Schema Generalization Test:** Apply pipeline to different temporal semi-structured dataset and measure whether 3-5 table sweet spot persists
2. **Prompt Ablation Analysis:** Systematically remove temporal-specific few-shot examples and measure degradation specifically on duration and "before/after" questions
3. **Open-Source Model Parity Check:** Replace Gemini-2.0-Flash with Llama-3.1-70B-Chat for query generation and measure EM gap