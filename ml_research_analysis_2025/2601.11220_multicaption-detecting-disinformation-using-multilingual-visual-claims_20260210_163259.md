---
ver: rpa2
title: 'MultiCaption: Detecting disinformation using multilingual visual claims'
arxiv_id: '2601.11220'
source_url: https://arxiv.org/abs/2601.11220
tags:
- claim
- pairs
- claims
- dataset
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting contradictory visual
  claims in multilingual settings, which is a key challenge in combating online disinformation.
  The authors introduce MultiCaption, a dataset of 11,088 visual claim pairs across
  64 languages, labeled as contradictory or non-contradictory using multiple strategies
  including manual validation, LLM annotation, and self-expansion.
---

# MultiCaption: Detecting disinformation using multilingual visual claims

## Quick Facts
- arXiv ID: 2601.11220
- Source URL: https://arxiv.org/abs/2601.11220
- Reference count: 34
- MultiCaption is a multilingual visual claim dataset for disinformation detection across 64 languages

## Executive Summary
This paper introduces MultiCaption, a novel dataset designed to detect contradictory visual claims in multilingual contexts, addressing a critical gap in combating online disinformation. The dataset comprises 11,088 visual claim pairs spanning 64 languages, annotated as contradictory or non-contradictory using multiple strategies including manual validation, LLM annotation, and self-expansion. The authors demonstrate that MultiCaption presents a more challenging task than standard natural language inference benchmarks, making it valuable for developing robust multilingual fact-checking systems. Experiments show that fine-tuned transformer and LLM models significantly outperform traditional NLI-based approaches, with multilingual fine-tuning yielding substantial performance improvements.

## Method Summary
The authors developed MultiCaption through a multi-stage process that combines professional fact-checker claims, social media data, and LLM-based annotation. The dataset construction employed three annotation strategies: manual validation of a subset, LLM annotation with quality controls, and self-expansion techniques. Models were evaluated on their ability to classify visual claim pairs as contradictory or non-contradictory, with fine-tuned transformers and LLMs (particularly Mistral) showing superior performance. The experimental setup included comprehensive baseline comparisons against standard NLI approaches and ablation studies to assess the impact of multilingual fine-tuning versus monolingual training.

## Key Results
- MultiCaption is more challenging than standard NLI tasks, with lower performance metrics across all tested models
- Fine-tuned transformer and LLM models significantly outperform NLI-based approaches for visual claim contradiction detection
- Multilingual fine-tuning improves performance compared to monolingual training, demonstrating the value of the multilingual dataset
- The best-performing model (fine-tuned Mistral) achieves F1-scores up to 0.912 on the test set

## Why This Works (Mechanism)
The effectiveness of MultiCaption stems from its comprehensive approach to capturing linguistic and cultural diversity in disinformation patterns. By incorporating claims from 64 languages and using multiple annotation strategies, the dataset exposes models to a wider range of contradiction patterns than traditional NLI datasets. The combination of visual and textual components in claims creates a richer representation space that better reflects real-world misinformation scenarios. Additionally, the self-expansion technique allows the dataset to grow organically while maintaining consistency in annotation quality, creating a more robust benchmark for multilingual contradiction detection.

## Foundational Learning
- Multilingual NLP: Understanding language-specific challenges and transfer learning across languages
  - Why needed: Visual claims exist across diverse languages and cultures
  - Quick check: Can the model handle low-resource languages effectively?

- Visual-Text Integration: Combining visual and textual information for claim verification
  - Why needed: Visual claims inherently combine image and text components
  - Quick check: Does the model leverage both modalities appropriately?

- Contradiction Detection: Identifying logical inconsistencies between statements
  - Why needed: Core task for disinformation detection
  - Quick check: Can the model distinguish subtle contradictions from related but consistent claims?

- LLM Annotation Quality Control: Ensuring reliability of LLM-generated labels
  - Why needed: Most dataset annotations rely on LLM approaches
  - Quick check: What's the inter-annotator agreement between human and LLM annotations?

## Architecture Onboarding

Component Map: Visual claim pair -> Feature extraction -> Contradiction classification -> Output label

Critical Path: Input visual claims → Text preprocessing → Feature representation (multilingual embeddings) → Classification model → Contradiction score → Binary classification

Design Tradeoffs:
- Annotation strategy: Manual validation vs. LLM annotation vs. self-expansion (quality vs. scalability)
- Model choice: Fine-tuned LLMs vs. traditional transformers vs. NLI-based approaches (performance vs. complexity)
- Multilingual approach: Joint multilingual training vs. separate language models (generalization vs. specialization)

Failure Signatures:
- High false positives when claims share similar vocabulary but differ in meaning
- Poor performance on low-resource languages due to limited training data
- Sensitivity to cultural context differences across languages

First 3 Experiments:
1. Train a baseline NLI model on MultiCaption to establish performance ceiling for traditional approaches
2. Compare monolingual vs. multilingual fine-tuning on a subset of high-resource languages
3. Evaluate cross-lingual transfer by training on one language family and testing on another

## Open Questions the Paper Calls Out
- How can the dataset be expanded to include more low-resource languages while maintaining annotation quality?
- What are the long-term implications of relying on LLM-based annotation pipelines for dataset construction?
- Can the contradiction detection models be adapted for real-time disinformation detection in social media platforms?
- How do cultural and linguistic nuances affect the interpretation of visual claims across different regions?

## Limitations
- Reliance on LLM-based annotation for majority of dataset introduces potential systematic biases
- Dataset primarily draws from professional fact-checkers and Reddit, may not represent full diversity of online misinformation
- Relatively modest dataset size (11,088 pairs) for training large multilingual models
- Long-term stability and generalizability of LLM annotation pipeline cannot be fully validated
- Limited coverage of visual components beyond textual claims
- Potential domain-specific biases from fact-checking sources

## Confidence
High: Experimental methodology is sound with appropriate baselines, multiple evaluation metrics, and ablation studies supporting claims about MultiCaption's difficulty compared to standard NLI tasks.

Medium: Claim that MultiCaption is more challenging than standard NLI tasks is supported by lower performance metrics, but comparison may be affected by dataset size differences and task nature.

Low: Long-term stability and generalizability of LLM-based annotation pipeline across different languages and claim types cannot be fully validated based on current evidence.

## Next Checks
1. Conduct a blind external validation where independent annotators rate a random sample of 500 claim pairs from MultiCaption to assess inter-annotator agreement and potential systematic biases in the original annotation pipeline.

2. Test the best-performing models (fine-tuned Mistral) on an independent multilingual misinformation dataset from a different domain (e.g., Twitter/X posts or news articles) to evaluate real-world generalizability.

3. Perform detailed error analysis on 100 false positive and 100 false negative predictions from the best model, categorizing errors by type (semantic ambiguity, cultural context, visual claim interpretation) to identify systematic weaknesses.