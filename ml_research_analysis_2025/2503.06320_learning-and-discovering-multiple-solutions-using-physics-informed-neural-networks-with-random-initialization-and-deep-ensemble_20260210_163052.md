---
ver: rpa2
title: Learning and discovering multiple solutions using physics-informed neural networks
  with random initialization and deep ensemble
arxiv_id: '2503.06320'
source_url: https://arxiv.org/abs/2503.06320
tags:
- solutions
- pinns
- solution
- pinn
- multiple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to discover multiple solutions of
  nonlinear differential equations using physics-informed neural networks (PINNs)
  with random initialization and deep ensemble techniques. While PINNs are typically
  used to find single solutions, this work demonstrates that randomly initialized
  PINNs can naturally capture multiple solution patterns.
---

# Learning and discovering multiple solutions using physics-informed neural networks with random initialization and deep ensemble

## Quick Facts
- arXiv ID: 2503.06320
- Source URL: https://arxiv.org/abs/2503.06320
- Reference count: 40
- Key outcome: PINNs with random initialization can discover multiple solutions of nonlinear differential equations, validated on five test problems including 1D Bratu, 2D Allen-Cahn, and 2D lid-driven cavity flow

## Executive Summary
This paper demonstrates that physics-informed neural networks (PINNs), when combined with random initialization and deep ensemble techniques, can effectively discover multiple solutions of nonlinear differential equations. The approach leverages the non-convex loss landscape of PINNs, where different random initializations naturally converge to distinct solution branches. The method is validated across multiple examples including the 1D Bratu problem, boundary layer problems, reaction-diffusion equations, 2D Allen-Cahn equation, and 2D lid-driven cavity flow. The authors show that PINN-generated solutions can serve as initial guesses for conventional numerical solvers, achieving machine precision accuracy.

## Method Summary
The method trains multiple independently initialized PINNs on the same nonlinear differential equation, leveraging random initialization to explore different basins of attraction in the non-convex loss landscape. Solutions are clustered based on values at diagnostic points (e.g., u(0.5) for 1D problems), and representatives from each cluster are selected as initial guesses for conventional numerical solvers. The approach uses fully-connected networks with tanh activation, trained with Adam optimizer using staged learning rates. Initialization variance is a critical hyperparameter controlling solution diversity, with larger variance producing more diverse solutions but potentially reducing training stability.

## Key Results
- PINNs identify two distinct solution patterns in the 1D Bratu problem within 200 iterations, bifurcating based on u(0.5) values
- Maximum residuals reach machine precision (10⁻¹⁶) after FDM refinement of PINN-generated initial guesses
- Larger initialization variance (σ=2.0 vs σ=0.1) increases the ratio of discovered solutions from 0.240 to 0.980 in Bratu problem
- The method successfully identifies both stable and unstable solutions, even in cases where solution multiplicity is not previously established

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Random initialization of neural network parameters may cause different PINN instances to converge to distinct solution branches of nonlinear DEs.
- **Mechanism:** Different random initializations place the network in different regions of parameter space, which under gradient-based optimization flow toward different local minima corresponding to distinct solutions. The non-convex loss landscape of PINNs for nonlinear DEs provides multiple basins of attraction.
- **Core assumption:** The PINN loss landscape for nonlinear DEs contains multiple local minima that correspond to valid solutions; initialization bias systematically determines basin selection.
- **Evidence anchors:** [abstract] "reveal that PINNs, when combined with random initialization and deep ensemble method...can effectively uncover multiple solutions"; [Section 3.1, Figure 4] shows clear bifurcation into two classes of solutions.

### Mechanism 2
- **Claim:** Larger initialization variance correlates with greater solution diversity but may reduce training stability.
- **Mechanism:** Wider initialization distributions increase the probability of seeding networks in diverse basins of attraction. However, extreme values can cause gradient pathologies (vanishing/exploding) and convergence failure.
- **Core assumption:** There exists a tradeoff between solution diversity and trainability that depends on problem-specific nonlinearity.
- **Evidence anchors:** [Section 3.1, Table 1] shows ratio of solutions discovering u2 increases with initialization variance (0.000 → 0.240) across architectures; [Section 3.1] notes larger variance makes training more challenging.

### Mechanism 3
- **Claim:** PINN solutions can serve as effective initial guesses for conventional numerical solvers to achieve machine precision.
- **Mechanism:** PINNs learn approximate solution patterns (correct topological structure) without requiring high accuracy. Conventional solvers (FDM, FEM, SEM with Newton iteration) then refine these approximations with guaranteed convergence properties.
- **Core assumption:** PINNs capture the correct solution branch/mode even with moderate accuracy; the numerical solver's basin of attraction includes the PINN approximation.
- **Evidence anchors:** [abstract] "propose utilizing PINN-generated solutions as initial conditions or initial guesses for conventional numerical solvers to enhance accuracy and efficiency"; [Section 3.3, Tables 2-3] show maximum residuals reach machine precision (10⁻¹⁶).

## Foundational Learning

- **Concept: Physics-Informed Neural Networks (PINNs)**
  - Why needed here: Core architecture; must understand how DE residuals become loss functions via automatic differentiation.
  - Quick check question: Can you write the PINN loss function for u'' + sin(u) = 0 with Dirichlet BCs?

- **Concept: Deep Ensemble Uncertainty Quantification**
  - Why needed here: The method repurposes ensemble variance (originally for UQ) as a solution diversity mechanism.
  - Quick check question: Why does training multiple independently initialized networks on the same data produce different outputs?

- **Concept: Basin of Attraction in Nonlinear Optimization**
  - Why needed here: Explains why random initialization leads to different solutions; Newton's method sensitivity to initial guess.
  - Quick check question: For f(x) = x⁴ - 4x², which minima does gradient descent reach from x₀ = 0.1 vs. x₀ = 2.0?

## Architecture Onboarding

- **Component map:** Input (spatial coords x) → [Shared body layers] → [Head layers] → Output u_θ(x) → Automatic differentiation → Residual loss: F[u_θ] - f + BC loss: B[u_θ] - b

- **Critical path:**
  1. Initialize M networks with truncated normal (σ tuning critical)
  2. Train for ~5,000-20,000 iterations (pattern emergence before full convergence)
  3. Cluster solutions by keypoint values (e.g., u(0.5) for Bratu)
  4. Select one representative per cluster
  5. Evaluate on solver mesh → Pass to Newton/FDM solver

- **Design tradeoffs:**
  - **Initialization variance (σ):** Higher → more solution diversity but slower/less stable training
  - **Network depth/width:** Deeper/wider → captures more solutions but harder to optimize
  - **Training iterations:** Early stopping sufficient for pattern discovery; full training not required before handoff
  - **Ensemble size M:** More networks → higher probability of discovering rare solutions but linear cost increase

- **Failure signatures:**
  - All networks converge to identical solution → σ too small or problem has unique solution
  - High training loss, no convergence → σ too large or architecture mismatch
  - Numerical solver diverges after PINN handoff → PINN on wrong solution branch; try different cluster representative

- **First 3 experiments:**
  1. Reproduce 1D Bratu (λ=1) with 100 networks, σ=2.0; verify bifurcation in u(0.5) values at ~200 iterations
  2. Ablate initialization: test σ ∈ {0.5, 1.0, 2.0} and measure solution diversity ratio
  3. Handoff test: Take 50-iteration PINN solutions, pass to MATLAB bvp4c, confirm machine-precision residuals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between neural network initialization strategies and the distribution of discovered solutions, and can an optimal initialization scheme be designed to maximize solution diversity?
- Basis in paper: [explicit] "The initialization method for PINNs remains an open question and is rarely explored in the literature... Given that random initialization enables the discovery of multiple solutions, we will empirically investigate... that the choice of initialization method plays a crucial role."
- Why unresolved: The ablation study (Table 1, Figure 15) empirically shows that larger variance initializations yield more diverse solutions but are harder to train, yet no theoretical framework connects initialization properties to solution basin attraction.
- What evidence would resolve it: A theoretical analysis linking initialization distribution properties to convergence basins, or a principled initialization method with provable diversity guarantees.

### Open Question 2
- Question: Can the number of required ensemble PINNs be systematically reduced while maintaining complete discovery of all solution branches?
- Basis in paper: [explicit] "Future research could... [develop] strategies to improve the diversity of neural network outputs [92], thereby reducing the required number of PINNs to achieve the same solution diversity."
- Why unresolved: Current experiments use 1,000-10,000 randomly initialized networks, and the multi-head architecture only reduces parameters, not the number of ensemble members needed.
- What evidence would resolve it: A method that actively guides exploration toward undiscovered solution branches rather than random sampling, achieving comparable coverage with fewer networks.

### Open Question 3
- Question: Why does the proposed approach fail to discover certain known solutions, and what completeness guarantees can be established for solution discovery?
- Basis in paper: [explicit] "While our approach successfully identifies all solution patterns presented in [93, 84, 94], it does not recover the two additional patterns described in [95]."
- Why unresolved: The paper demonstrates empirical success but provides no theoretical bounds on which solutions will be discovered; some solutions remain inaccessible to the random initialization approach.
- What evidence would resolve it: Either modified algorithms that recover the missing solutions, or theoretical analysis characterizing the class of discoverable solutions versus those requiring alternative methods.

## Limitations
- No theoretical guarantees for when PINNs will discover multiple solutions; relies on empirical observation
- Solution clustering based on diagnostic point values lacks rigorous statistical validation of cluster separation
- Method assumes PINN solutions lie within numerical solver's basin of attraction without quantifying proximity

## Confidence
- **High confidence:** PINNs can identify multiple solution patterns when trained as an ensemble with random initialization (supported by consistent bifurcation observations across multiple test problems)
- **Medium confidence:** PINN solutions serve as effective initial guesses for numerical solvers (supported by residual verification, but limited analysis of solver convergence robustness)
- **Low confidence:** The method generalizes to "a broader class of differential equations" (only five specific problems tested, with varying levels of rigor in solution multiplicity verification)

## Next Checks
1. **Convergence basin analysis:** Systematically measure the distance between PINN-generated initial guesses and the true solution manifold using the numerical solver's Jacobian to quantify basin of attraction size
2. **Multi-solution certification:** For the 1D Bratu problem, compute the bifurcation diagram analytically to verify that PINN-identified solutions correspond to all existing solution branches across parameter ranges
3. **Ensemble sensitivity study:** Vary ensemble size M systematically (10, 50, 100, 1000) on the Allen-Cahn problem to determine the minimum ensemble size required to capture all solution modes with statistical confidence