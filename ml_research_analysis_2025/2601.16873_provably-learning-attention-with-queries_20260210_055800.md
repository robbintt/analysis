---
ver: rpa2
title: Provably Learning Attention with Queries
arxiv_id: '2601.16873'
source_url: https://arxiv.org/abs/2601.16873
tags:
- attention
- queries
- learning
- single-head
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the problem of learning the parameters of Transformer-based
  sequence models from black-box value queries, where an adversary can adaptively
  query any sequence of vectors and observe real-valued outputs. The authors focus
  on single-head attention models and one-layer Transformers, providing the first
  provable guarantees for exact parameter recovery in this setting.
---

# Provably Learning Attention with Queries

## Quick Facts
- arXiv ID: 2601.16873
- Source URL: https://arxiv.org/abs/2601.16873
- Reference count: 5
- For single-head softmax-attention regressor with width d, presents an algorithm that recovers parameters exactly using O(d²) queries by reducing softmax to sigmoid via length-two sequences

## Executive Summary
This paper establishes the first provable guarantees for exact parameter recovery of Transformer-based sequence models from black-box value queries. The authors focus on single-head attention models and one-layer Transformers, showing that parameters can be exactly recovered with polynomially many adaptive queries. The key insight leverages length-two input sequences to reduce softmax attention to sigmoid functions, which can be inverted to yield linear equations. They also provide a compressed sensing approach for low-rank attention matrices and analyze robustness to noisy oracle access. However, they prove that multi-head attention parameters are not identifiable from value queries alone in general.

## Method Summary
The paper studies learning single-head attention regressor parameters from value queries where an adversary can adaptively query any sequence of vectors and observe real-valued outputs. The algorithm works in two phases: first recovering the value vector v* using d single-token queries, then recovering each column of the attention weight matrix W* using d two-row probes per column. The key mechanism exploits that length-two sequences reduce softmax to sigmoid functions, which can be inverted to produce linear constraints. For low-rank cases, they use compressed sensing with nuclear norm minimization. The method extends to one-layer Transformers if an algorithm exists for learning ReLU feedforward networks from value queries.

## Key Results
- Single-head softmax attention parameters can be exactly recovered using O(d²) value queries via linear systems
- When rank(W*) ≤ r ≪ d, parameters can be recovered with O(rd) queries using compressed sensing
- Under mild norm and margin conditions, parameters can be estimated to ε accuracy with polynomial queries even under noisy oracle access
- Multi-head attention parameters are not identifiable from value queries in general - distinct parameter settings can induce the same input-output map

## Why This Works (Mechanism)

### Mechanism 1: Sigmoid Inversion via Length-Two Queries
- Claim: Single-head softmax attention parameters can be exactly recovered using O(d²) value queries.
- Mechanism: Length-two input sequences reduce softmax attention to a sigmoid function σ(u^T w_j), since with two positions the attention weight becomes α = e^{s₁}/(e^{s₁} + e^{s₂}) = σ(s₁ - s₂). The sigmoid is invertible via σ^{-1}(p) = log(p/(1-p)), transforming each oracle response into a linear equation in the unknown attention weight column w_j. Solving d linear systems yields all columns of W*.
- Core assumption: The value vector v* ≠ 0 (otherwise W* is not identifiable).
- Evidence anchors:
  - [abstract] "The key insight is that using length-two sequences reduces softmax to a sigmoid, which can be inverted, allowing each query to yield linear equations that can be solved."
  - [Section 4, Proof] Shows α(u;j) = σ(u^T w_j) and derives u^T w_j = σ^{-1}((y - v*_j)/(u^T v*)).
  - [corpus] Weak direct evidence; related work on single-head attention (arxiv 2509.24914) studies spectral structure but not query-based recovery.
- Break condition: If v* = 0, then f_{W*,v*} ≡ 0 for all X and W* cannot be identified.

### Mechanism 2: Compressed Sensing for Low-Rank Attention Matrices
- Claim: When rank(W*) ≤ r ≪ d, parameters can be recovered with O(rd) queries instead of O(d²).
- Mechanism: Construct rank-one measurement probes X = [(a+b)^T; b^T]^T that yield observations t = ⟨ab^T, W*⟩ = a^T W* b after sigmoid inversion. Drawing m = O(rd) i.i.d. Gaussian pairs (a_k, b_k) produces a sensing operator satisfying the Restricted Uniform Boundedness (RUB) property with high probability. Nuclear norm minimization then recovers W* exactly via convex optimization.
- Core assumption: rank(W*) ≤ r is known; v* ≠ 0.
- Evidence anchors:
  - [abstract] "They also provide a randomized algorithm using O(rd) queries via compressed sensing, leveraging the fact that the attention matrix has low rank."
  - [Section 5.2] "Draw i.i.d. (a_k, b_k) ~ N(0, I_d) × N(0, I_d)... the operator A satisfies RUB with the required condition."
  - [corpus] No directly comparable compressed sensing approaches for attention in neighbors; mechanism relies on external matrix sensing theory (Cai and Zhang 2015).
- Break condition: If rank(W*) > r, RUB guarantees fail and recovery may be inaccurate.

### Mechanism 3: Clipping for Noise Robustness
- Claim: Under approximate value queries with tolerance τ, parameters can be estimated to ε accuracy with polynomial queries.
- Mechanism: The inverse sigmoid σ^{-1} is not globally Lipschitz (blows up near 0 and 1). The solution is to construct probes with scaled inputs (a=1/2, b=1/W) ensuring true attention weights lie in [σ(-1/2), 1-σ(-1/2)], where σ^{-1} has bounded Lipschitz constant L ≤ 5. Clipping observed values to this range before inversion controls error propagation.
- Core assumption: Bounded norms (∥W*∥_F ≤ W, ∥v*∥_2 ≤ 1, mini |v*_i| ≥ μ > 0) and margin condition on v*.
- Evidence anchors:
  - [abstract] "Under mild norm and margin conditions, the parameters can be estimated to ε accuracy with polynomially many queries even when outputs are only provided up to additive tolerance."
  - [Section 6, Lemma 6.1] Proves Lipschitz bound L_{τ_clip} ≤ 5 on the clipped interval.
  - [corpus] No comparable robustness analysis for attention in neighbors.
- Break condition: If margin μ is too small or noise τ is too large relative to μ, denominator in α estimation becomes unstable.

### Mechanism 4: Multi-Head Non-Identifiability
- Claim: Multi-head attention parameters are not identifiable from value queries alone.
- Mechanism: When all heads share the same attention matrix W^{(h)} = A but have different value vectors v^{(h)} = λ_h b with ∑ λ_h = 1, the outputs are identical: ∑_h α(X, A)^T (Xλ_h b) = α(X, A)^T (Xb). Infinitely many parameter settings produce the same function.
- Core assumption: No additional structural constraints on head parameters.
- Evidence anchors:
  - [abstract] "Distinct parameter settings can induce the same input-output map."
  - [Section 7, Proposition 7.1] Explicit construction of parameter equivalence classes.
  - [corpus] arxiv 2507.02944 studies multi-head synergies but not identifiability barriers.
- Break condition: Identifiability may be restored if heads lie in mutually orthogonal subspaces (noted as future direction).

## Foundational Learning

- Concept: **Softmax attention as weighted averaging**
  - Why needed here: The entire paper inverts the softmax nonlinearity; understanding that softmax produces normalized attention weights α ∈ Δ^{N-1} is prerequisite.
  - Quick check question: Given scores [2.0, 1.0, 0.1], compute the softmax attention weights.

- Concept: **Sigmoid function and its inverse**
  - Why needed here: Length-two queries reduce softmax to sigmoid; the algorithm explicitly applies σ^{-1}(p) = log(p/(1-p)) to recover linear equations.
  - Quick check question: What is σ^{-1}(0.8)? What happens to σ^{-1}(p) as p → 0 or p → 1?

- Concept: **Nuclear norm minimization for low-rank matrix recovery**
  - Why needed here: The O(rd) algorithm solves min ∥W∥_* subject to linear measurement constraints; this is the standard convex relaxation for rank minimization.
  - Quick check question: Why does minimizing nuclear norm promote low-rank solutions?

- Concept: **Query-based learning model**
  - Why needed here: The paper assumes adaptive access to a value query oracle VQ(X); understanding membership vs. value queries and identifiability is essential context.
  - Quick check question: In what ways is query access more powerful than random example access?

## Architecture Onboarding

- Component map:
  - **Target model**: Single-head attention regressor f_{W,v}(X) = softmax(x_i^T W x_N)^T (Xv), where X ∈ R^{N×d}, W ∈ R^{d×d}, v ∈ R^d. Last token x_N is the query.
  - **One-layer Transformer extension**: Composes attention with two-layer ReLU MLP: TF(X) = w_o^T ReLU(α(X,W)^T X A).
  - **Oracle interface**: Value query VQ(X) returns f*(X) exactly (or within τ for approximate case).
  - **Recovery algorithms**: (1) O(d²) exact via linear systems, (2) O(rd) via compressed sensing, (3) Robust version with clipping.

- Critical path:
  1. Recover v* using d single-token queries X = [e_i^T] (softmax weight is trivially 1).
  2. For each column j, construct d length-two probes X = [(u_ℓ + e_j)^T; e_j^T]^T with linearly independent u_ℓ.
  3. Invert sigmoid to get linear constraints Z w_j = t, solve for w_j.
  4. (Low-rank variant) Draw Gaussian (a_k, b_k), construct rank-one probes, solve nuclear norm minimization.
  5. (Noisy variant) Scale probes to keep α in Lipschitz-bounded region, clip before inversion.

- Design tradeoffs:
  - **Exact vs. low-rank assumption**: O(d²) queries with no assumptions vs. O(rd) queries requiring known rank bound.
  - **Noise tolerance vs. query precision**: Robust algorithm requires oracle tolerance τ = O(με_W/(W²d)), scaling inversely with dimension and condition numbers.
  - **Single-head vs. multi-head**: Single-head is provably identifiable; multi-head requires structural assumptions not provided.
  - **Assumption: ReLU FFN learner availability**: One-layer Transformer result assumes existence of A_{FFN} for ReLU networks (not proven in this paper).

- Failure signatures:
  - **v* ≈ 0**: All queries return near-zero regardless of W; cannot identify attention parameters.
  - **Attention weights near 0 or 1**: σ^{-1} amplifies noise exponentially; robust algorithm clips but loses accuracy.
  - **Rank underestimate**: If rank(W*) > r assumed, compressed sensing fails to recover.
  - **Multi-head configuration**: Returns some valid parameterization but not the true one; functionally equivalent but structurally wrong.

- First 3 experiments:
  1. **Sanity check on synthetic single-head model**: Generate random (W*, v*) with d=16, v* normalized away from zero. Implement exact recovery algorithm, verify ∥Ŵ - W*∥_F < 1e-10 after O(d²) queries. Test with varying condition numbers of W*.
  2. **Low-rank regime validation**: Set d=64, r=4, generate rank-r W*. Compare query count for exact O(d²) vs. compressed sensing O(rd) algorithms. Measure recovery error vs. number of sensing measurements m.
  3. **Noise robustness boundary testing**: Add Gaussian noise to oracle outputs. Sweep noise level τ and margin μ. Identify the threshold where ∥Ŵ - W*∥_F exceeds ε. Verify predicted tolerance τ = O(με_W/(W²d)) matches empirical breakdown point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What structural assumptions on multi-head attention parameters enable efficient identifiability and exact recovery from value queries?
- Basis in paper: [explicit] "We observe that if the parameters of each head lie in mutually orthogonal subspaces, then the class is identifiable (up to permutations)."
- Why unresolved: The paper proves non-identifiability for general multi-head attention but only briefly sketches the orthogonal subspace condition without providing an efficient recovery algorithm or characterizing the full set of sufficient conditions.
- What evidence would resolve it: An algorithm with provable query complexity bounds for recovering multi-head attention parameters under specified structural assumptions, or impossibility results showing which natural conditions remain insufficient.

### Open Question 2
- Question: Is the problem of deciding whether two parameterizations of a multi-head attention model induce the same function decidable?
- Basis in paper: [explicit] "Even basic decidability questions around this are not clearly understood – for instance, given two sets of parameters of multi-head attention models, is the problem of checking whether they represent the same function decidable?"
- Why unresolved: The functional equivalence problem involves comparing sums of softmax-weighted terms with different parameter matrices, which does not reduce to standard linear algebraic equivalence checks.
- What evidence would resolve it: A decidability proof with complexity bounds, or a reduction to an undecidable problem showing inherent computational barriers.

### Open Question 3
- Question: Can distribution-dependent PAC learning guarantees be obtained for multi-head attention models when exact parameter recovery is impossible?
- Basis in paper: [explicit] "Further, it could be interesting to analyse whether distribution-dependent PAC guarantees (such as Chen et al. (2021) for MLPs) are attainable, unlike distribution-independent ones in our work."
- Why unresolved: The paper's guarantees are distribution-independent; the non-identifiability of multi-head parameters suggests weaker learning goals (function approximation rather than parameter recovery) may be the appropriate target.
- What evidence would resolve it: Sample complexity bounds for learning multi-head attention under specific data distributions, or separations showing distribution-independent PAC learning is impossible.

### Open Question 4
- Question: Can the one-layer Transformer learning result be extended to ReLU MLPs without the current structural assumptions on parameter matrices?
- Basis in paper: [inferred] The paper's extension to Transformers assumes an algorithm exists for learning two-layer ReLU networks from value queries. While Milli et al. (2019) and Daniely and Granot (2023) provide algorithms, they require structural assumptions (linear independence of columns; modified algorithms for networks without biases).
- Why unresolved: The paper notes that "learning 2-layer ReLU MLPs without any structural assumptions is quite difficult" and relies on conjectured modifications to existing algorithms.
- What evidence would resolve it: A unified algorithm for single-head one-layer Transformers that does not require structural assumptions on the FFN component, or hardness results characterizing when such assumptions are necessary.

## Limitations

- **Multi-head non-identifiability**: The paper proves that multi-head attention parameters cannot be uniquely recovered from value queries alone, limiting the applicability of the single-head results to practical Transformer architectures.
- **Assumption dependencies**: The low-rank compressed sensing algorithm requires knowing the rank bound r, and the one-layer Transformer result depends on an unproven assumption about ReLU FFN learning.
- **Noise sensitivity**: The robust algorithm's performance critically depends on the margin μ and noise tolerance τ, with theoretical bounds that may be loose in practice.

## Confidence

- **Single-head exact recovery (O(d²) queries)**: High - The mechanism is elementary and the linear algebra is straightforward
- **Low-rank compressed sensing (O(rd) queries)**: Medium - Relies on external RUB property guarantees and nuclear norm optimization that aren't fully detailed
- **Noise robustness with clipping**: Medium - Theoretical Lipschitz bounds are proven but practical performance under realistic noise levels needs validation
- **Multi-head non-identifiability**: High - The explicit construction is clear and the counterexample is valid

## Next Checks

1. **Numerical Stability Boundary Testing**: Implement the exact recovery algorithm with varying condition numbers for W* and test at which point numerical instability in σ^{-1} computation prevents accurate recovery. Measure the empirical breakdown threshold vs. theoretical predictions.

2. **Low-Rank Algorithm Sensitivity Analysis**: Generate synthetic W* matrices with varying ranks r < d and test the compressed sensing algorithm's performance as m varies around the theoretical O(rd) bound. Quantify how recovery error scales with undersampling and rank overestimation.

3. **Robustness Under Realistic Noise**: Simulate oracle noise at different levels and test the clipping-based robust algorithm. Compare theoretical tolerance bounds (τ = O(με_W/(W²d))) with empirical recovery accuracy across different margin-to-noise ratios.