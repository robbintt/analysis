---
ver: rpa2
title: 'CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image
  Generation'
arxiv_id: '2512.03540'
source_url: https://arxiv.org/abs/2512.03540
tags:
- step
- recipe
- image
- generation
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CookAnything introduces a diffusion-based framework for flexible,
  multi-step recipe image generation. It addresses the challenge of generating coherent,
  semantically distinct images from recipes of arbitrary length, overcoming limitations
  in current methods that produce fixed or entangled outputs.
---

# CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation

## Quick Facts
- **arXiv ID:** 2512.03540
- **Source URL:** https://arxiv.org/abs/2512.03540
- **Reference count:** 40
- **Primary result:** Introduces diffusion-based framework for flexible, multi-step recipe image generation with state-of-the-art performance on RecipeGen and VGSI-Recipe datasets

## Executive Summary
CookAnything introduces a diffusion-based framework for flexible, multi-step recipe image generation. It addresses the challenge of generating coherent, semantically distinct images from recipes of arbitrary length, overcoming limitations in current methods that produce fixed or entangled outputs. The approach combines Step-wise Regional Control (SRC) for step-to-region alignment, Flexible RoPE for step-aware positional encoding, and Cross-Step Consistency Control (CSCC) for ingredient continuity. Experiments show state-of-the-art performance on RecipeGen and VGSI-Recipe datasets in both training-based and training-free settings, with improvements in Goal Faithfulness, Step Faithfulness, and Cross-Step Consistency. The framework supports scalable, high-quality procedural visual synthesis with broad applications in instructional and multimedia content.

## Method Summary
CookAnything is built on Flux.1-dev and modifies the DiT architecture with three core innovations: (1) Step-wise Regional Control (SRC) uses a regional attention mask to isolate each step's attention to its designated visual region, (2) Flexible RoPE applies independent positional encodings per image region to prevent long-range dependency issues, and (3) Cross-Step Consistency Control (CSCC) fuses contextual recipe tokens with step-specific tokens to maintain ingredient continuity. The framework processes recipes through a GPT-4o "Cooking Agent" for enrichment, encodes text with T5, and denoises latents in a single pass to produce step-aligned images. Training uses LoRA (rank 16) on RecipeGen for 20K steps.

## Key Results
- State-of-the-art performance on RecipeGen and VGSI-Recipe datasets
- Improves Goal Faithfulness, Step Faithfulness, and Cross-Step Consistency metrics
- Supports both training-based and training-free generation modes
- Generates coherent, semantically distinct images for recipes of arbitrary length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-wise Regional Control (SRC) enables semantically distinct multi-image generation by binding each textual instruction to a designated latent region.
- Mechanism: SRC introduces a Step-wise Regional Attention Mask (M ∈ R^(2N×2N)) that restricts attention within each step-region pair during DiT self-attention. Each step attends only to its paired visual region and associated instruction tokens, preventing semantic leakage while a parallel Whole-Description Control maintains global coherence. The outputs are fused via weighted interpolation (z_t = α·z_base + (1-α)·z_region).
- Core assumption: Regional attention isolation preserves step-wise semantics without fragmenting the global visual narrative.
- Evidence anchors:
  - [abstract] "SRC aligns textual steps with corresponding image regions within a single denoising process"
  - [section 3.2] "We quantify this limitation using Cross-Step Consistency... In-Context LoRA baseline yields CSC of 44.12—9.03 points lower than ground truth"
  - [corpus] Limited direct corpus evidence; CookingDiffusion addresses procedural generation but lacks regional control comparison.
- Break condition: If α weighting is misconfigured, outputs collapse into either fragmented local patches (α too low) or overly uniform images (α too high).

### Mechanism 2
- Claim: Flexible RoPE prevents positional entanglement and long-range dependency attenuation by assigning independent positional encodings per image region.
- Mechanism: Unlike standard RoPE which uses globally continuous encoding, Flexible RoPE applies region-specific rotation matrices R^(n)(i,j) to each region's tokens. This resets coordinate indices per step, enabling the model to distinguish positions across regions while maintaining intra-region spatial relationships.
- Core assumption: Step-wise independence in positional encoding improves semantic separation without breaking cross-step coherence learned through attention.
- Evidence anchors:
  - [abstract] "Flexible RoPE enhances both temporal coherence and spatial diversity"
  - [section 3.3] "With original RoPE, repeated step images appear as early as Step 2... Step 9 suffers from noticeable blurring"
  - [corpus] No corpus papers explicitly address positional encoding for multi-step image generation; mechanism appears novel to this work.
- Break condition: If region-specific rotation matrices share parameters or indices, the model reverts to global coordinate entanglement, causing visual redundancy.

### Mechanism 3
- Claim: Cross-Step Consistency Control (CSCC) preserves fine-grained ingredient appearance across steps by fusing contextually-informed tokens with step-specific tokens.
- Mechanism: CSCC extracts Contextual Step Tokens by encoding the entire recipe sequence through T5, then segmenting by step lengths. These tokens (capturing recurring ingredient semantics) are fused with independently-encoded step tokens via weighted averaging: C^(n)[0:t^(n)] = C^(n)[0:t^(n)] + λ·C_recipe[b^(n):b^(n)+t^(n)]. A Cooking Agent (GPT-4o) supplements missing ingredient details for consistency.
- Core assumption: Ingredients appearing across steps share semantic token similarities that can be leveraged for visual continuity.
- Evidence anchors:
  - [abstract] "CSCC maintains fine-grained ingredient consistency across steps"
  - [section 3.4, Fig. 4] "Without CSCC, the carrot changes from cubes to strips in Step 4... taro should appear beneath the wings but disappears"
  - [corpus] OSCAR and related work address object status tracking but focus on recognition, not generation consistency.
- Break condition: If λ is too high (>0.6 per Table 8), step-specific details are overwritten by global context, reducing Step Faithfulness.

## Foundational Learning

- Concept: Diffusion Transformers (DiT) vs. U-Net architectures
  - Why needed here: CookAnything builds on Flux.1-dev, which replaces U-Net with DiT for better representation learning through attention over concatenated text and latent tokens.
  - Quick check question: Can you explain why joint attention over [C_T; z] enables better text-image alignment than separate encoding?

- Concept: Rotary Position Embedding (RoPE)
  - Why needed here: Understanding RoPE's rotation matrix formulation R(i,j) is essential to grasp how Flexible RoPE modifies it for multi-region support.
  - Quick check question: Given equation z̃_i,j = z_i,j · R(i,j), what happens if two regions share the same rotation matrix?

- Concept: Attention masking for structural control
  - Why needed here: The Step-wise Regional Attention Mask (M_ij = 1 if i=j or |i-j|=N) is the core constraint enabling semantic isolation across steps.
  - Quick check question: Why does the mask allow i=j AND |i-j|=N? What two relationships does this preserve?

## Architecture Onboarding

- Component map: Raw recipe → Cooking Agent (GPT-4o) → reformatted context-tagged steps → Dual Encoding (Full-recipe T5 + Per-step T5) → Token Fusion (λ=0.1) → Noisy latents → Flexible RoPE → DiT blocks with Regional Attention Mask → Single concatenated latent → Decoded to N×(512×512) step images

- Critical path: The token fusion step (Eq. 12) is the bottleneck—incorrect λ causes either ingredient drift (λ too low) or step homogenization (λ too high). Training showed optimal λ=0.2.

- Design tradeoffs:
  - Training-free vs. Training-based: TF mode enables immediate use; TB mode (LoRA rank 16, 20K steps) improves metrics by ~0.5-1.5 points across GF/SF.
  - Single-pass vs. iterative: Single denoising pass ensures consistency but limits per-step refinement.
  - α parameter (Eq. 9): Controls global vs. regional emphasis; set to 0.1 based on experiments.

- Failure signatures:
  - Ingredient shape mutation across steps (e.g., cubes → strips): CSCC not activated or λ misconfigured
  - Repeated/blurry images in later steps: Original RoPE used instead of Flexible RoPE
  - Step images lack distinction: Regional Attention Mask not applied or attention leaking
  - Missing ingredients in vague descriptions: Cooking Agent not enriching prompts

- First 3 experiments:
  1. Ablate SRC by removing Regional Attention Mask; measure Cross-Step Consistency degradation (expect increase from 0.19 to ~0.25+).
  2. Swap Flexible RoPE for original RoPE on a 10-step recipe; visualize step 9 blurring and quantify Goal Faithfulness drop (~2 points per ablation table).
  3. Vary λ in [0, 0.2, 0.4, 0.6, 0.8, 1.0] and plot the tradeoff curve between Step Faithfulness and Ingredient Accuracy to identify task-specific optimal values.

## Open Questions the Paper Calls Out

- To what extent can the CookAnything framework generalize to non-culinary procedural domains such as scientific workflows or educational storytelling?
- How can the static image generation framework be adapted for multimodal video generation while maintaining ingredient continuity?
- How can the framework align generated synthetic illustrations with real-world cooking execution data?

## Limitations

- Reliance on GPT-4o for recipe enrichment creates scalability bottleneck and potential domain-specific biases
- Regional attention mechanism assumes equal visual importance across all steps, which may not hold for recipes with varying complexity
- Evaluation metrics (CLIP, DINOv2) measure visual similarity rather than true semantic understanding of cooking procedures

## Confidence

- **High Confidence**: SRC mechanism's effectiveness is well-supported by ablation studies and visual examples showing clear semantic separation between steps
- **Medium Confidence**: Flexible RoPE's improvements over standard RoPE are empirically validated but generalizability beyond recipe generation remains untested
- **Low Confidence**: Cooking Agent's (GPT-4o) role in enriching recipe descriptions is described but not thoroughly validated

## Next Checks

1. Apply CookAnything to non-recipe procedural domains (e.g., furniture assembly, craft instructions) to test whether regional attention and positional encoding transfer effectively beyond culinary contexts
2. Systematically vary the SRC attention mask width (|i-j| threshold) to identify breaking points where semantic isolation degrades into visual fragmentation
3. Implement human evaluation protocol where annotators assess whether generated step sequences maintain functional coherence (e.g., "does Step 3 logically follow from Step 2?") rather than just visual similarity