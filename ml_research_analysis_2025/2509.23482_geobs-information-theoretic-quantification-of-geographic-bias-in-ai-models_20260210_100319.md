---
ver: rpa2
title: 'GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models'
arxiv_id: '2509.23482'
source_url: https://arxiv.org/abs/2509.23482
tags:
- geo-bias
- spatial
- scores
- performance
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeoBS, an information-theoretic framework
  for quantifying geographic bias (geo-bias) in AI models. Existing geo-bias metrics
  are often model-specific or spatially implicit, limiting fair comparison across
  models.
---

# GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models

## Quick Facts
- **arXiv ID:** 2509.23482
- **Source URL:** https://arxiv.org/abs/2509.23482
- **Reference count:** 27
- **Primary result:** Introduces GeoBS framework with 5 model-agnostic geo-bias scores that reveal substantial geographic bias in both task-specific GeoAI models and general-purpose foundation models

## Executive Summary
This paper addresses the critical challenge of quantifying geographic bias (geo-bias) in AI models through an information-theoretic framework called GeoBS. Existing geo-bias metrics are often model-specific or spatially implicit, making fair comparison across models difficult. GeoBS bridges this gap by leveraging spatial point pattern analysis and information theory to create model-agnostic, spatially explicit geo-bias scores. The framework decomposes geo-bias metrics into three dimensions - the map used, the reference pattern, and the difference measure - enabling systematic categorization and comparison.

## Method Summary
GeoBS converts performance maps to spatial point patterns and defines geo-bias as statistical deviation from homogeneous reference patterns. The framework introduces three novel geo-bias scores - Scale-Grid SRE (multi-scalability), Distance-Lag SRE (distance decay), and Direction-Sector SRE (anisotropy) - that capture different spatial bias factors. Each score uses KL divergence between ROI-level and patch-level performance distributions, weighted by patch sizes. A Python package implements the framework for efficient computation across different datasets and models.

## Key Results
- Both task-specific GeoAI models and general-purpose foundation models exhibit substantial geo-bias across 3 tasks, 8 datasets, and 8 models
- Geo-bias scores are largely independent of model accuracy, revealing that high performance doesn't guarantee geographic fairness
- Different partition schemes (grid, rings, sectors) successfully isolate distinct bias sources, enabling targeted diagnosis of scale heterogeneity, distance decay, and directional anisotropy effects
- The Python package provides efficient computation of all 5 geo-bias scores with configurable hyperparameters

## Why This Works (Mechanism)

### Mechanism 1: Spatial Point Pattern Framework
Converting performance maps to spatial point patterns enables systematic geo-bias categorization and comparison across models. A location map L_D or performance map M_D,π is treated as a spatial point pattern, with geo-bias defined as deviation from a predefined homogeneous reference pattern. The framework decomposes any geo-bias metric into three dimensions: which map is used, which reference pattern defines "unbiased," and which difference measure quantifies deviation. Core assumption: spatially unbiased model performance should approximate a homogeneous spatial distribution; systematic deviations indicate geographic bias rather than random fluctuation.

### Mechanism 2: KL Divergence for Spatial Heterogeneity Quantification
KL divergence between ROI-level and patch-level performance distributions provides an interpretable, differentiable geo-bias measure with physical meaning. For a partitioned ROI, compute performance histograms h(N) for the entire ROI and h(P_k) for each patch. The Local SRE score is γ_SRE(N) := Σ (#P_k/#N) · D_KL(h(N) || h(P_k)), where the KL divergence measures the "information gap" or bits needed to transform the patch distribution into the ROI baseline. Core assumption: if a model is geo-unbiased under a given partition scheme, performance distributions in each patch should resemble the overall ROI distribution.

### Mechanism 3: Multi-Partition Spatial Factor Isolation
Different spatial partitioning schemes isolate distinct geo-bias sources (scale heterogeneity, distance-decay, directional anisotropy), enabling targeted diagnosis. Scale-Grid partitioning (equal-area squares) captures multi-scale heterogeneity - whether low-performance points cluster at particular spatial scales. Distance-Lag partitioning (concentric rings from ROI center) captures distance-decay effects - whether performance degrades with distance from reference points. Direction-Sector partitioning (equal-angle sectors) captures anisotropy - whether performance varies systematically by direction. Each scheme produces independent SRE scores.

## Foundational Learning

- **Concept: Spatial Point Pattern Analysis (Unmarked vs. Marked)**
  - Why needed here: The entire GeoBS framework treats performance maps as spatial point patterns - understanding first-order (intensity-based) vs. second-order (interaction-based) statistics, and unmarked (locations only) vs. marked (locations + attributes) patterns is essential for selecting and interpreting appropriate geo-bias scores.
  - Quick check question: Given a dataset of geo-tagged satellite imagery predictions with per-location accuracy values, is this an unmarked or marked spatial point pattern, and which summary statistics would apply?

- **Concept: KL Divergence (Relative Entropy)**
  - Why needed here: All three SRE scores use KL divergence as the difference measure between distributions; understanding what KL divergence quantifies (asymmetric information gap, bits for distribution transformation, relation to maximum likelihood) is required to interpret score magnitudes and compare across models.
  - Quick check question: If an ROI has performance distribution h(N) = [0.7 correct, 0.3 incorrect] and a patch has h(P_k) = [0.3 correct, 0.7 incorrect], what does a high D_KL(h(N) || h(P_k)) value indicate about this patch?

- **Concept: Modifiable Areal Unit Problem (MAUP)**
  - Why needed here: The paper explicitly connects geo-bias detection to MAUP - the scale and partition scheme of analysis affects what bias is detected. Understanding this is critical for choosing hyperparameters (grid scale, lag width, sector count) and avoiding false negatives where bias exists but is hidden by inappropriate aggregation.
  - Quick check question: If you increase the Scale-Grid partition scale from 0.01 to 0.05 radians, would you expect the Scale-Grid SRE score to generally increase or decrease, and what type of fine-grained spatial heterogeneity might be missed?

## Architecture Onboarding

- **Component map:**
  - Input Layer -> ROI Selector -> Partition Engine -> Distribution Estimator -> Divergence Computer -> Aggregator
  - (Performance map) -> (Filters points within radius) -> (Scale-Grid/Distance-Lag/Direction-Sector) -> (Histogram construction) -> (KL divergence computation) -> (Weighted aggregation)

- **Critical path:**
  1. Preprocess: Convert raw predictions → performance map (binary: 0=wrong, 1=correct for classification; thresholded by error variance for regression)
  2. Validate spatial density: Ensure dataset supports chosen hyperparameters (≥100 points per ROI, ≥2 patches with ≥10 points each)
  3. ROI Generation: Create overlapping or non-overlapping ROIs across geographic extent
  4. Partition: Apply selected partition function (Scale-Grid / Distance-Lag / Direction-Sector) based on which spatial factor to probe
  5. Histogram Construction: Build empirical performance distributions for ROI h(N) and each patch h(P_k)
  6. Divergence Computation: Calculate KL divergence per patch, aggregate with size weights to get Local SRE
  7. Global Aggregation: Weight Local SRE scores by normalized ROI sizes to produce Global Geo-Bias Score

- **Design tradeoffs:**
  - ROI radius (stability vs. resolution): Larger radius = more points per ROI (stable statistics, lower variance) but lower spatial resolution for pinpointing bias locations
  - Partition granularity (sensitivity vs. sample requirements): Finer partitions (smaller grid scale, more sectors, thinner rings) capture finer-grained bias but require more data to avoid sparse histograms
  - Histogram binning (fidelity vs. robustness): More bins capture performance distribution nuances but require more samples per patch; binary performance (2 bins) is most robust
  - KL divergence vs. Wasserstein distance: KL is O(n) computationally efficient and has clear information-theoretic interpretation, but is asymmetric; Wasserstein is O(n³) but captures geometric relationships between distribution supports

- **Failure signatures:**
  - NaN/infinity scores: Caused by empty patches or patches where all samples have identical performance (zero-variance histogram) → solution: increase ROI radius or coarsen partition granularity
  - High variance across repeated runs (SSI specifically): Caused by random background point generation → solution: use Fibonacci lattice implementation (already in GeoBS package)
  - All local scores near zero despite visible geographic performance variation: Caused by partition scale mismatch (partition units larger than bias clusters) → solution: decrease grid scale/lag width; run sensitivity analysis across scales
  - Inconsistent rankings between SRE types: Not a failure - indicates mixed bias types; use all three SRE scores together for complete diagnosis

- **First 3 experiments:**
  1. Hyperparameter sensitivity analysis: Run all five geo-bias scores on your dataset while varying ROI radii (e.g., 0.05, 0.1, 0.15, 0.2 radians) and partition parameters (scale, lag, number of sectors). Replicate the analysis from Table 6 and Table 7 in the appendix. Identify stable parameter regions where scores don't change dramatically - this establishes trustworthy baselines.
  2. Cross-model geo-bias comparison with fixed hyperparameters: Select at least 2 models trained on the same dataset. Compute all 5 geo-bias scores using identical hyperparameters. Report both accuracy/R² and all geo-bias scores side-by-side. Verify the paper's finding that accuracy and geo-bias are relatively independent evaluation dimensions - high accuracy doesn't guarantee low geo-bias.
  3. Local geo-bias spatial visualization: Map local (per-ROI) geo-bias scores geographically using the same visualization approach as Figure 4 and Figure 5. Overlay with error rate maps. Compare local score distributions across different SRE types (Scale-Grid vs. Distance-Lag vs. Direction-Sector) to diagnose the dominant spatial factor driving bias for your specific model-dataset combination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GeoBS metrics be effectively utilized as differentiable loss functions to train models with lower geographic bias?
- Basis in paper: [explicit] The conclusion states, "We see great potential in introducing the geo-bias scores as debiasing loss functions and help train more fair models."
- Why unresolved: The current work validates GeoBS only as a post-hoc evaluation metric, not as an optimization objective for in-processing.
- What evidence would resolve it: Experiments demonstrating that models trained with a GeoBS-based regularization term achieve reduced geo-bias scores without significant degradation in task accuracy.

### Open Question 2
- Question: How can the GeoBS framework be extended to capture second-order spatial interactions and complex topologies like networks?
- Basis in paper: [explicit] The authors explicitly "leave the investigation of second-order geo-bias metrics to the future" and mention extending to "network and time-space" factors.
- Why unresolved: Current metrics are first-order (intensity-based) and rely on Euclidean/spherical partitions (grids, sectors), failing to quantify point interactions or graph structures.
- What evidence would resolve it: New GeoBS variants integrating statistics like Ripley's K-function or graph-based partitioning, validated on spatiotemporal or network datasets.

### Open Question 3
- Question: Does the geo-bias in foundation models stem primarily from architectural inductive biases or implicit spatial overfitting?
- Basis in paper: [inferred] The paper hypothesizes that foundation model bias "might be more dependent on its own model architecture," contrasting with task-specific models where bias is dataset-dependent.
- Why unresolved: The experiments show performance differences between models like CROMA and SatMAE but do not isolate whether architecture or data distribution is the causal factor.
- What evidence would resolve it: Controlled ablations varying foundation model architectures while holding training data constant to isolate the source of bias.

## Limitations
- Assumes homogeneous spatial patterns represent unbiased performance, which may not hold for all domain contexts where geographic variation is expected
- Requires sufficient spatial data density - performance maps with sparse geographic coverage may produce unstable or misleading scores
- Focuses on point-based evaluation, potentially missing bias patterns in raster-based or continuous prediction scenarios

## Confidence
- **High Confidence:** The spatial point pattern framework and KL divergence mechanisms are well-grounded in established spatial statistics and information theory. The decomposition into three dimensions (map type, reference pattern, difference measure) provides a clear theoretical foundation.
- **Medium Confidence:** The multi-partition isolation claims show promise but require further validation across diverse real-world scenarios. The interaction between different bias types and their isolation through partitioning schemes needs more systematic study.
- **Low Confidence:** The claim that geo-bias and accuracy are independent evaluation dimensions, while demonstrated in the paper, requires broader validation across more diverse model architectures and task types.

## Next Checks
1. **Robustness Testing:** Conduct sensitivity analysis by systematically varying ROI radii, partition scales, and histogram bin configurations across multiple datasets to identify stable parameter regions and understand how hyperparameter choices affect geo-bias detection.
2. **Cross-Modal Validation:** Apply GeoBS to non-GeoAI domains (e.g., text classification with geographic metadata, multi-modal foundation models) to test the framework's generalizability beyond traditional GeoAI tasks.
3. **Bias Source Attribution:** Design experiments to deliberately introduce specific types of geographic bias (scale heterogeneity, distance decay, directional anisotropy) into otherwise unbiased models, then verify that GeoBS scores correctly identify and isolate these bias sources.