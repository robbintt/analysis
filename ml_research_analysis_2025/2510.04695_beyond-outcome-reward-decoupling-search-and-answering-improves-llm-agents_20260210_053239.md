---
ver: rpa2
title: 'Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents'
arxiv_id: '2510.04695'
source_url: https://arxiv.org/abs/2510.04695
tags:
- search
- reward
- agent
- training
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training search-augmented large
  language model agents that interleave reasoning and retrieval before answering questions.
  The authors find that agents trained with only outcome-based rewards (e.g., exact
  match) exhibit multiple search deficiencies including failure to invoke tools, invalid
  queries, and redundant searches, ultimately degrading answer quality.
---

# Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents

## Quick Facts
- arXiv ID: 2510.04695
- Source URL: https://arxiv.org/abs/2510.04695
- Reference count: 13
- Primary result: Two-stage DeSA training outperforms single-stage outcome-only training by 8.0% (3B) and 5.6% (7B) on QA benchmarks.

## Executive Summary
This paper addresses the problem of training search-augmented large language model agents that interleave reasoning and retrieval before answering questions. The authors find that agents trained with only outcome-based rewards (e.g., exact match) exhibit multiple search deficiencies including failure to invoke tools, invalid queries, and redundant searches, ultimately degrading answer quality. They propose DeSA (Decoupling Search-and-Answering), a two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards, while in Stage 2, outcome rewards are used to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents substantially improve search behaviors, achieving 64.5% search recall compared to 59.5% for outcome-only baselines.

## Method Summary
The paper proposes DeSA, a two-stage GRPO training framework for search-augmented LLM agents. Stage 1 uses recall-based rewards to optimize search effectiveness (binary signal if ground-truth answer appears in retrieved content), while Stage 2 uses exact match outcome rewards for answer generation. The transition from Stage 1 to Stage 2 occurs at the "EM collapse point" where EM peaks then drops despite rising recall. The method is evaluated on Qwen2.5-3B/7B-Instruct models across 7 QA benchmarks using E5 retriever over 2018 Wikipedia, with 4 GPUs and FSDP + gradient checkpointing.

## Key Results
- DeSA achieves 64.5% search recall vs. 59.5% for outcome-only baselines
- DeSA outperforms single-stage training approaches by 8.0% on 3B model and 5.6% on 7B model
- Deficient search rate drops from 22.6% (baseline) to 14.6% after Stage 1 training
- EM initially rises during Stage 1 but collapses after ~50 steps, triggering transition to Stage 2

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling search optimization from answer generation prevents conflicting gradient signals that arise when both objectives are trained simultaneously.
- **Mechanism:** In single-stage training with composite rewards, the model receives mixed optimization pressuresâ€”recall maximization encourages exhaustive search, while exact-match rewards penalize long trajectories that retrieve the answer but fail to synthesize it correctly. DeSA eliminates this interference by isolating each objective into distinct training phases, allowing clean credit assignment per skill.
- **Core assumption:** Search skill acquisition and answer generation are sufficiently separable that sequential training does not incur catastrophic forgetting of Stage 1 skills during Stage 2.
- **Evidence anchors:** [abstract] "DeSA outperforms single-stage training approaches that simultaneously optimize recall and outcome rewards"; [Section 6.3.2] "mixing the search reward (recall) and outcome reward (EM) creates confusing optimization pressures"; [corpus] Related work finds combining process and outcome signals can conflict.

### Mechanism 2
- **Claim:** Recall-based rewards provide dense, process-level feedback that guides intermediate search actions, unlike sparse outcome-only signals.
- **Mechanism:** The recall reward $R_{recall}$ is a binary signal indicating whether the aggregated retrieved information contains any ground-truth answer. This directly incentivizes query formulation and multi-step retrieval, even when the agent cannot yet correctly synthesize the final answer. It addresses the credit assignment problem by rewarding information gathering independent of downstream answer correctness.
- **Core assumption:** Retrieving ground-truth evidence is a necessary (if not sufficient) condition for correct answering; maximizing recall will not incentivize pathological behaviors that retrieve the answer but degrade final synthesis.
- **Evidence anchors:** [Section 5.1] "This reward provides a direct signal indicating whether the necessary information to answer the question has been successfully retrieved"; [Figure 5] After Stage 1, deficient search rate drops to 14.6% (vs. 22.6% baseline) and recall rises to 62.5% (vs. 45.4%); [corpus] Information Gain-based Policy Optimization uses intermediate rewards for multi-turn agents.

### Mechanism 3
- **Claim:** Transitioning from Stage 1 to Stage 2 at the "EM collapse point" prevents the model from over-exploiting recall rewards at the expense of answer generation.
- **Mechanism:** During Stage 1, EM initially rises as recall improves, then peaks and drops as the model learns to game the recall reward (e.g., exhaustive retrieval without answer synthesis). The last checkpoint before EM decline is a practical marker for transitioning to Stage 2, where outcome rewards restore answer-focused behavior.
- **Core assumption:** The EM collapse point is a reliable signal that search skills have been sufficiently acquired and that further Stage 1 training is counterproductive.
- **Evidence anchors:** [Section 6.3.3] "the exact-match (EM) score rises to an early peak (around 50 steps in our setting) and then drops abruptly, even though the recall metric continues to improve"; [Figure 6b] Stage 2 started after collapse requires "substantially more training to recover its question-answering behavior"; [corpus] Limited direct evidence on stage transition timing.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** DeSA builds on RLVR, using exact match as the verifiable outcome reward; understanding reward sparsity and credit assignment is prerequisite.
  - **Quick check question:** Can you explain why a sparse binary outcome reward may fail to shape multi-step intermediate behavior?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The paper uses GRPO as its RL algorithm; it replaces PPO's value model with group-based advantage estimation.
  - **Quick check question:** How does GRPO estimate advantages without a learned value function?

- **Concept: Retrieval-Augmented Generation (RAG) and Agentic Search**
  - **Why needed here:** DeSA targets search-augmented agents that interleave reasoning and retrieval; understanding multi-turn RAG workflows is essential.
  - **Quick check question:** What is the difference between one-shot RAG and agentic multi-step search?

## Architecture Onboarding

- **Component map:** Agent policy (LLM) -> Search environment (E5 retriever) -> Reward computation (Recall/EM) -> Stage controller (EM monitoring) -> Agent policy update (GRPO)

- **Critical path:**
  1. Initialize from instruction-tuned backbone (e.g., Qwen2.5-3B/7B-Instruct)
  2. Train Stage 1 with recall reward; log recall and EM at each checkpoint
  3. Detect EM peak and collapse; select last pre-collapse checkpoint
  4. Initialize Stage 2 from selected checkpoint; train with EM reward
  5. Evaluate on held-out QA benchmarks; log recall, deficient search rate, EM

- **Design tradeoffs:**
  - Recall vs. fine-grained rewards: Simple recall reward outperforms composite (recall + penalty) and fine-grained retrieval accuracy in multi-hop QA, but may be less stable in other domains
  - Stage transition timing: Early transition preserves EM but may under-optimize recall; late transition improves recall but prolongs Stage 2 recovery
  - Model scale: Gains are larger for smaller models (8.0% for 3B vs. 5.6% for 7B), suggesting decoupling matters more for lower-capacity policies

- **Failure signatures:**
  - Stage 1 collapse: EM drops while recall continues rising; indicates exploitation of recall reward without answer synthesis
  - Stage 2 instability: EM fails to recover or fluctuates wildly; suggests transitioned too late or learning rate too high
  - Persistent deficient search: Deficient search rate remains >15% after Stage 1; check prompt formatting, tool call parsing, or reward implementation

- **First 3 experiments:**
  1. Reproduce baseline failure mode: Train single-stage EM-only agent; log deficient search rate and recall on 2-3 QA benchmarks to confirm the problem exists
  2. Ablate stage transition: Run DeSA with transition at 25%, 50%, 75% of Stage 1 steps; compare EM curves and recall to identify collapse point
  3. Compare reward designs: Test recall-only, recall+penalty, and retrieval accuracy rewards in Stage 1; measure recall, deficient search rate, and downstream EM to validate the paper's reward choice

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a learned reward model for evaluating search behavior quality outperform the simple recall-based reward for Stage 1 training?
- **Basis in paper:** [explicit] "For future work, we plan to explore more advanced process-based rewards for Stage 1 training, such as using a dedicated reward model to evaluate the agent's search behavior."
- **Why unresolved:** The paper only tested rule-based rewards (recall, recall+penalty, retrieval accuracy); learned reward models could capture nuanced search behaviors beyond binary recall success.
- **What evidence would resolve it:** Training a reward model on human or expert annotations of search trajectory quality, then comparing DeSA Stage 1 performance using this learned reward versus the recall-based reward.

### Open Question 2
- **Question:** Does the DeSA decoupling principle transfer to non-QA agentic tasks such as code generation or long-context reasoning?
- **Basis in paper:** [explicit] "We aim to extend the principle of our DeSA framework to broader agentic tasks beyond question-answering. We believe this principle could also be effective in domains such as code generation and long-context understanding."
- **Why unresolved:** All experiments are on QA benchmarks; the separation of search and answering may not apply when retrieval serves different roles (e.g., finding relevant code snippets vs. factual evidence).
- **What evidence would resolve it:** Applying DeSA to code generation agents searching documentation, measuring intermediate retrieval quality and final code correctness, comparing against outcome-only RL baselines.

### Open Question 3
- **Question:** How reliably does the EM-curve collapse signal generalize across different model scales, retrievers, and task distributions as an indicator for the optimal Stage 1 to Stage 2 transition point?
- **Basis in paper:** [inferred] Section 6.3.3 uses EM score collapse as a heuristic transition marker, demonstrating its effectiveness in one setting, but provides no theoretical justification or validation across varied experimental conditions.
- **Why unresolved:** The transition criterion is empirically derived; whether EM consistently peaks then drops during Stage 1 across different configurations, or whether task-specific transition strategies are needed, is unknown.
- **What evidence would resolve it:** Systematic ablation experiments varying model size, retrieval corpus, and task type to test if EM collapse consistently marks optimal transition, or if alternative criteria (validation recall, behavior statistics) are more robust.

## Limitations
- The sequential two-stage framework assumes search and answer generation can be cleanly separated, but this may not hold for tasks requiring deep integration of retrieved evidence into reasoning chains
- The transition criterion based on EM collapse is heuristic and may not generalize across different model scales, datasets, or task types
- The evaluation focuses on extractive QA with ground-truth answers; performance on generative or abstractive QA tasks remains untested

## Confidence
- **High confidence** in the observation that outcome-only rewards produce deficient search behaviors (no search, duplicate queries, invalid searches)
- **Medium confidence** in the superiority of the two-stage framework over single-stage training
- **Medium confidence** in the recall reward design effectiveness in this specific multi-hop QA setting
- **Low confidence** in the stability of the EM collapse transition heuristic across different experimental conditions

## Next Checks
1. Apply DeSA to a generative QA task (e.g., NarrativeQA) and compare recall and EM performance against outcome-only training to test cross-domain retrieval robustness
2. Implement and compare DeSA with three different transition points: fixed step count, recall threshold, and early stopping when recall plateaus to evaluate transition criterion robustness
3. Replace the binary recall reward with a fine-grained retrieval accuracy reward (e.g., ROUGE or F1) in Stage 1 and compare deficient search rate, recall, and final EM to test reward design sensitivity