---
ver: rpa2
title: Large Language Models as End-to-end Combinatorial Optimization Solvers
arxiv_id: '2509.16865'
source_url: https://arxiv.org/abs/2509.16865
tags:
- problems
- solution
- problem
- each
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel framework enabling large language
  models (LLMs) to function as end-to-end combinatorial optimization (CO) solvers,
  directly mapping natural language problem descriptions to solutions without intermediate
  steps like code generation or solver invocation. The approach uses a two-stage training
  strategy: supervised fine-tuning (SFT) to learn solution generation patterns from
  domain-specific solvers, followed by feasibility-and-optimality-aware reinforcement
  learning (FOARL) to address constraint violations and improve solution quality.'
---

# Large Language Models as End-to-end Combinatorial Optimization Solvers

## Quick Facts
- **arXiv ID:** 2509.16865
- **Source URL:** https://arxiv.org/abs/2509.16865
- **Reference count:** 40
- **One-line primary result:** 7B-parameter LLM achieves 100% feasibility and 1.03-8.20% optimality gaps across 7 NP-hard CO problems

## Executive Summary
This paper presents a novel framework enabling large language models to function as end-to-end combinatorial optimization solvers, directly mapping natural language problem descriptions to solutions without intermediate steps like code generation or solver invocation. The approach uses supervised fine-tuning (SFT) to learn solution patterns from domain solvers, followed by feasibility-and-optimality-aware reinforcement learning (FOARL) to address constraint violations and improve solution quality. Evaluation across seven NP-hard CO problems shows that a 7B-parameter LLM fine-tuned using this method achieves 100% feasibility rates and reduces average optimality gaps to 1.03-8.20%, surpassing general-purpose LLMs, reasoning models, and domain-specific heuristics.

## Method Summary
The framework employs a two-stage training strategy: SFT on solver-generated solutions to learn feasible solution structures, followed by FOARL with feasibility-aware rewards to correct over-greedy behavior and improve sampling efficiency. The method uses Text-Attributed Instances (TAI) format with heuristic features, Qwen2.5-7B backbone with LoRA adapters, and Best-of-N inference for test-time quality improvement. Training involves generating diverse instances (10-100 nodes), solving with domain-specific solvers, formatting as TAIs, and fine-tuning with GRPO-based RL.

## Key Results
- 100% feasibility rate achieved across all 7 NP-hard CO problems after FOARL
- Average optimality gaps reduced to 1.03-8.20% (0.45-2.32% for routing problems)
- Outperforms general-purpose LLMs, reasoning models, and domain-specific heuristics
- Unified model achieves competitive results while maintaining generality across problems

## Why This Works (Mechanism)

### Mechanism 1: SFT Imparts Solution Generation Patterns via Solver Imitation
The model learns direct mapping from textual problem descriptions to solution representations through next-token prediction on high-quality solver solutions. By observing diverse instances, the LLM generalizes solution patterns rather than memorizing specific instances.

### Mechanism 2: FOARL Corrects Over-Greedy Behavior and Improves Sampling Efficiency
Reinforcement learning with feasibility-aware rewards shifts the policy away from constraint violations while improving test-time sampling efficiency. The combined reward function penalizes violations and encourages solution quality.

### Mechanism 3: Best-of-N Inference Compensates for Autoregressive Commitment
Generating multiple independent solutions and selecting the best feasible one overcomes the inability of autoregressive models to backtrack or revise earlier decisions, providing test-time compute scaling.

## Foundational Learning

- **Combinatorial Optimization Problem Formulation**
  - Why needed: The paper assumes familiarity with decision variables, objective functions, constraint sets, and feasibility definitions
  - Quick check: Given a CVRP instance with 10 customers, capacity 50, and demands summing to 180, what is the minimum number of vehicles required for a feasible solution?

- **Reinforcement Learning with GRPO (Group Relative Policy Optimization)**
  - Why needed: FOARL uses GRPO to avoid training a separate critic model
  - Quick check: Why does GRPO estimate baselines from group-level rewards rather than using a value function approximator, and what computational advantage does this provide?

- **LoRA (Low-Rank Adaptation)**
  - Why needed: The framework uses LoRA to train only 2.08% of parameters while keeping the backbone frozen
  - Quick check: If LoRA rank is set too low (e.g., r=4 for a 7B model), what failure mode would you expect when fine-tuning on diverse CO problems with varying constraint structures?

## Architecture Onboarding

- **Component map:** Data Generation Pipeline -> SFT Module -> FOARL Module -> Inference
- **Critical path:** Generate diverse training instances → Run specialized solvers → Format as TAIs with heuristic features → SFT training → FOARL on held-out instances → Evaluate with BoN sampling
- **Design tradeoffs:**
  - Heuristic features improve training convergence but increase context length
  - RL improves feasibility but may slightly hurt optimality on already-feasible problems
  - BoN scale vs. latency tradeoff (N=8 provides strong results with 9.8s avg time)
  - Unified model achieves competitive results but with 3-7% feasibility drops on constrained problems
- **Failure signatures:**
  - Over-greedy SFT policy causing constraint violations
  - Collapsed sampling diversity providing no BoN benefit
  - Reward hacking exploiting reward definition loopholes
  - OOD generalization failure on clustered/mixed distributions
- **First 3 experiments:**
  1. Reproduce SFT-only baseline on TSP-50: Train LoRA adapters on 100K TSP instances, evaluate feasibility rate and optimality gap
  2. Ablate heuristic features: Train identical SFT models with and without top-k neighbor features, measure training loss curves and final optimality gaps
  3. FOARL constraint margin analysis: After SFT, run FOARL on 300 OP/CVRP instances, plot constraint margin distributions before/after RL

## Open Questions the Paper Calls Out

- How can solving efficiency be improved to match traditional lightweight heuristics?
- What strategies are required to enable LLMs to effectively solve larger-scale CO problems (e.g., graphs with significantly more than 100 nodes)?
- How can LLM solvers be integrated with traditional heuristics to create hybrid systems?
- What is the optimal design for Text-Attributed Instances (TAI) to maximize solution quality without relying on domain-specific heuristic features?

## Limitations
- Computationally intensive FOARL training requiring 1-3 days on H100s
- Unified model achieves 3-7% lower feasibility rates than specialized models on constrained problems
- Autoregressive sampling limitations requiring expensive Best-of-N inference (9.8s average per instance)

## Confidence
- **High:** SFT's ability to learn solution patterns, FOARL's effectiveness in improving feasibility, Best-of-N sampling's role in compensating for autoregressive limitations
- **Medium:** Scalability claims for large graphs, unified model's competitive performance, generalization to unseen distributions
- **Low:** Exact contribution of heuristic features vs. underlying LLM reasoning capabilities, long-term stability of FOARL training across diverse domains

## Next Checks
1. **Scaling Validation:** Test the unified model on TSP instances with 500-1000 nodes to verify cross-size generalization
2. **Constraint Robustness:** Systematically vary constraint tightness in CVRP to measure model's adaptation to increasingly constrained instances
3. **Ablation on Heuristic Features:** Train and evaluate models with complete removal of top-k neighbor features to quantify their true contribution to solution quality