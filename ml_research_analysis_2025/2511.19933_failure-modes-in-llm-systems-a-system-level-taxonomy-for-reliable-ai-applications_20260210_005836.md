---
ver: rpa2
title: 'Failure Modes in LLM Systems: A System-Level Taxonomy for Reliable AI Applications'
arxiv_id: '2511.19933'
source_url: https://arxiv.org/abs/2511.19933
tags:
- arxiv
- failure
- systems
- reliability
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a system-level taxonomy of fifteen hidden
  failure modes in LLM-based applications, categorized into reasoning failures, input/context
  failures, and system/operational failures. It highlights the gap between current
  evaluation benchmarks, which focus on knowledge and reasoning, and the operational
  realities of LLM systems, such as drift, reproducibility, and workflow integration.
---

# Failure Modes in LLM Systems: A System-Level Taxonomy for Reliable AI Applications

## Quick Facts
- arXiv ID: 2511.19933
- Source URL: https://arxiv.org/abs/2511.19933
- Authors: Vaishali Vinay
- Reference count: 40
- This paper introduces a system-level taxonomy of fifteen hidden failure modes in LLM-based applications, categorized into reasoning failures, input/context failures, and system/operational failures.

## Executive Summary
This paper presents a comprehensive system-level taxonomy of fifteen hidden failure modes in LLM-based applications, addressing the critical gap between current evaluation benchmarks and real-world operational challenges. The taxonomy categorizes failures into reasoning failures, input/context failures, and system/operational failures, highlighting how LLM reliability extends beyond knowledge and reasoning to encompass architectural controls, validation mechanisms, and semantic monitoring. The study emphasizes that LLM reliability should be treated as a system-engineering problem rather than just an evaluation challenge.

The research identifies key challenges including version drift, tool invocation errors, and cost-driven performance collapse, proposing high-level design principles for building reliable, maintainable, and cost-aware LLM systems. By focusing on operational realities such as drift, reproducibility, and workflow integration, the paper provides a framework for understanding and mitigating the complex failure modes that emerge when LLMs are deployed in production environments.

## Method Summary
The study developed a system-level taxonomy through analysis of 100 production incidents and comprehensive literature review, systematically categorizing failure modes that occur in LLM-based applications. The research focused on identifying hidden failures that go beyond traditional knowledge and reasoning benchmarks, examining how failures manifest at the system level rather than just at the model level. The taxonomy was constructed by analyzing real-world incidents to understand failure patterns and their root causes in production environments.

The methodology involved categorizing identified failure modes into three main categories: reasoning failures, input/context failures, and system/operational failures. Each category was populated with specific failure types based on observed patterns from incident analysis, creating a structured framework for understanding the diverse ways LLM systems can fail in practice. The approach emphasizes the importance of considering the entire system architecture rather than focusing solely on model performance metrics.

## Key Results
- Identified fifteen distinct failure modes across three categories: reasoning failures, input/context failures, and system/operational failures
- Highlighted the critical gap between current evaluation benchmarks and operational realities of LLM systems
- Proposed high-level design principles for building reliable, maintainable, and cost-aware LLM systems

## Why This Works (Mechanism)
The taxonomy works by providing a structured framework that captures the complex interactions between different components of LLM systems and their failure modes. By categorizing failures into system-level components rather than just model-level issues, it enables developers to identify and address reliability challenges that emerge from system architecture, data flow, and operational constraints. The mechanism relies on recognizing that LLM failures are often emergent properties of the entire system rather than isolated model issues.

The approach succeeds because it bridges the gap between theoretical model evaluation and practical system deployment by incorporating real-world incident data. This system-level perspective allows for the identification of failure patterns that would be missed by traditional model-centric evaluation approaches. The taxonomy provides a common language and framework for discussing reliability across different stakeholders, from engineers to product managers.

## Foundational Learning
- **System-Level Thinking**: Why needed - LLM failures often emerge from system interactions rather than model performance alone. Quick check - Can you trace a failure from user input through all system components to identify where it originates?
- **Version Drift Monitoring**: Why needed - Model updates and data changes can silently degrade system performance over time. Quick check - Do you have mechanisms to detect when system outputs change unexpectedly after updates?
- **Cost-Performance Tradeoffs**: Why needed - Economic constraints can force systems into suboptimal operating points that compromise reliability. Quick check - Can you quantify how cost reductions impact system accuracy and reliability?
- **Workflow Integration**: Why needed - LLM systems must seamlessly integrate with existing tools and processes. Quick check - Does your system handle tool invocation failures gracefully and provide appropriate fallbacks?
- **Semantic Monitoring**: Why needed - Traditional metrics may not capture meaningful changes in system behavior. Quick check - Do you monitor for semantic drift rather than just statistical changes in outputs?

## Architecture Onboarding

**Component Map**
User Input -> Preprocessing -> LLM Model -> Postprocessing -> Tool Integration -> Output Delivery

**Critical Path**
The critical path involves input validation, context management, model inference, tool invocation, and result validation. Each stage presents potential failure points that must be monitored and controlled.

**Design Tradeoffs**
- Model size vs. response time vs. cost
- Complex reasoning vs. deterministic behavior
- Real-time processing vs. thorough validation
- Feature richness vs. system simplicity

**Failure Signatures**
- Reasoning failures: inconsistent outputs, logical errors, hallucination
- Input/context failures: context window overflow, malformed inputs, missing context
- System/operational failures: version drift, tool invocation errors, cost-performance collapse

**3 First Experiments**
1. Implement semantic drift detection by comparing current outputs to historical baselines
2. Create a version drift monitoring system that alerts on unexpected output changes
3. Develop a cost-performance tradeoff analysis tool to identify optimal operating points

## Open Questions the Paper Calls Out
None

## Limitations
- The taxonomy was developed through analysis of 100 production incidents and literature review, which may introduce sampling bias toward reported failures rather than unreported or undiscovered failure modes.
- The classification system, while systematic, may not capture all possible failure scenarios, particularly emergent failures that arise from novel system architectures or use cases.
- Many proposed mitigation strategies are conceptual rather than empirically validated in production environments.

## Confidence
High confidence in the taxonomy structure and the categorization of known failure modes, given the systematic analysis approach and alignment with existing literature. Medium confidence in the completeness of the taxonomy, as LLM systems are rapidly evolving and new failure modes may emerge. Medium confidence in the proposed mitigation strategies, as many are conceptual rather than empirically validated.

## Next Checks
1. Conduct empirical validation by implementing the proposed monitoring and validation mechanisms in production LLM systems to measure their effectiveness in preventing identified failure modes.
2. Develop quantitative metrics for the proposed "semantic drift detection" and "multi-step reasoning drift" benchmarks to enable reproducible evaluation of LLM system reliability over time.
3. Create a standardized framework for measuring the cost-performance trade-offs mentioned in the study, particularly for systems that experience performance degradation under cost constraints.