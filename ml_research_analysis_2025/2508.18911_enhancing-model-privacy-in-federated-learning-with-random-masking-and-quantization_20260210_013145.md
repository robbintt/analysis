---
ver: rpa2
title: Enhancing Model Privacy in Federated Learning with Random Masking and Quantization
arxiv_id: '2508.18911'
source_url: https://arxiv.org/abs/2508.18911
tags:
- global
- privacy
- learning
- proxy
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedQSN is a federated learning approach that enhances model privacy
  protection through random masking and quantization. The method applies a server-side
  random mask to hide a subset of model parameters, followed by client-specific random
  masks and quantization before transmission to clients.
---

# Enhancing Model Privacy in Federated Learning with Random Masking and Quantization

## Quick Facts
- arXiv ID: 2508.18911
- Source URL: https://arxiv.org/abs/2508.18911
- Reference count: 28
- Key outcome: FedQSN achieves comparable performance to baseline methods while significantly improving model privacy protection through random masking and quantization

## Executive Summary
FedQSN introduces a federated learning approach that protects server-side model intellectual property through hierarchical random masking and quantization. The method applies a server-side random mask followed by client-specific random masks and block-wise quantization before transmitting proxy models to clients. This creates privacy-preserving models that clients use for local training while preventing full model reconstruction. Experimental results across multiple datasets and model architectures demonstrate that FedQSN maintains strong generation quality (BLEU scores within 5-8 points of baselines) while achieving significantly lower parameter similarity between global and proxy models compared to existing methods.

## Method Summary
FedQSN employs a two-step masking process: first applying a fixed server-side mask with probability p1 to the global model, then applying per-client random masks with probability p2 before quantization. The method uses block-wise quantization with ω-bit precision to obscure parameter values while reducing communication overhead. Clients train on these obfuscated proxy models locally, and the server aggregates updates using FedAvg. After training completes, the final global model is reconstructed using a logical AND operation between the trained model and the original server mask. The approach balances privacy protection with model performance through configurable masking ratios and quantization levels.

## Key Results
- FedQSN achieves BLEU scores within 5-8 points of FedAvg baselines across multiple datasets and model scales
- Parameter similarity between global and proxy models reaches 0.805 (lower is better) compared to 0.995 for baseline methods
- Performance gap between global and proxy models indicates effective privacy preservation, with FedQSN showing larger gaps than competing approaches
- The method maintains strong performance across GPT-2 Medium, GPT-2 XL, Llama3.2-1B/3B, and Llama3.1-8B architectures

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Random Masking
Applying a fixed server-side mask followed by per-client random masks prevents reconstruction of the full model while ensuring all parameters eventually receive updates. The server first applies mask M with probability p1 to the global model W, zeroing selected columns and amplifying remaining weights by 1/(1-p1). Each selected client then receives an additional client-specific mask with probability p2, ensuring unique subnetworks per round. Over successive rounds, randomized client selection guarantees comprehensive parameter coverage.

### Mechanism 2: Block-wise Quantization for Parameter Obfuscation
Lowering parameter precision through ω-bit block-wise quantization obscures exact values while reducing communication overhead. Weight matrix W is partitioned into blocks of size s. For each block Xi, compute absmax(Xi) and quantize to ω-bit integers: ˆXi = round((2^(ω-1) - 1)/absmax(Xi) × Xi). This destroys fine-grained parameter information adversaries could exploit.

### Mechanism 3: Proxy Model as Privacy-Preserving Interface
Clients train on incomplete, obfuscated proxy models rather than the true global model, creating a measurable performance gap that indicates privacy protection. The doubly-masked, quantized model serves as a proxy for local training. After training completes, the server applies a logical AND operation between the trained model and the original untrained model to restore masked parameters for the final global model.

## Foundational Learning

- Concept: Dropout-style expectation-preserving scaling
  - Why needed here: Understanding why masked weights are amplified by 1/(1-p) to maintain E[fW] = W
  - Quick check question: What would happen to activation distributions if we simply zeroed parameters without scaling?

- Concept: Federated averaging with dataset-weighted aggregation
  - Why needed here: The server aggregates client updates proportionally to their dataset sizes (Equation 5)
  - Quick check question: How does a client with 10,000 samples influence aggregation compared to one with 1,000?

- Concept: Subnetwork training in overparameterized models
  - Why needed here: The method relies on the finding that training subsets of parameters can yield comparable performance
  - Quick check question: Why can randomly masked subnetworks achieve similar performance to full models?

## Architecture Onboarding

- Component map:
  Server initialization -> Server mask (p1) -> Per-round distribution -> Client-specific mask (p2) -> Quantization (ω-bit) -> Client training -> Aggregation -> Final reconstruction

- Critical path:
  1. Server mask must persist across all rounds (line 3 of Algorithm 1 runs once)
  2. Client masks regenerate each round per client
  3. Quantization applied after masking, before transmission
  4. AND operation only at training conclusion, not per-round

- Design tradeoffs:
  - Privacy vs. utility: Higher masking (p1, p2) and lower ω increase privacy but degrade performance
  - Communication vs. precision: Lower ω reduces bandwidth but loses gradient information
  - Recommended starting point: p1=0.1, p2=0.1, ω=3-4 for balanced privacy-performance
  - Maximum privacy setting: p1=0.2, p2=0.2, ω=2 yields proxy BLEU ~40 vs global ~53

- Failure signatures:
  - Proxy model BLEU within 5% of global → insufficient privacy (increase p1/p2 or decrease ω)
  - Global model BLEU < 90% of FedAvg baseline → excessive degradation (reduce p1/p2 or increase ω)
  - Parameter similarity > 0.9 → masking ineffective (check mask implementation)
  - Training divergence → quantization too aggressive (increase ω)

- First 3 experiments:
  1. Run FedQSN with p1=0.1, p2=0.1, ω=4 on E2E; verify global BLEU ≈ 55, proxy BLEU ≈ 46
  2. Compare parameter similarity between global and proxy models; target < 0.85
  3. Fix two of {p1, p2, ω} and vary the third to characterize privacy-utility frontier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can differential privacy mechanisms be effectively combined with FedQSN's random masking and quantization to provide complementary privacy guarantees?
- Basis in paper: The authors state integrating differential privacy could further strengthen model owners' control in federated learning.
- Why unresolved: The current work provides empirical privacy protection but lacks formal privacy guarantees that differential privacy could provide.
- What evidence would resolve it: Experiments combining FedQSN with DP-SGD, measuring both empirical privacy and formal privacy budgets.

### Open Question 2
- Question: What are the formal privacy guarantees provided by FedQSN in terms of information-theoretic bounds or differential privacy metrics?
- Basis in paper: The paper measures privacy empirically through performance gaps and parameter similarity but doesn't provide theoretical bounds.
- Why unresolved: Empirical evaluations cannot guarantee protection against all possible attack strategies.
- What evidence would resolve it: Theoretical analysis deriving mutual information bounds or differential privacy guarantees based on masking probability and quantization levels.

### Open Question 3
- Question: How robust is FedQSN against coordinated multi-round model extraction attacks from colluding clients?
- Basis in paper: The paper mentions preventing client coalition reconstruction but only evaluates single-client parameter similarity.
- Why unresolved: Sophisticated adversaries might combine observations across multiple rounds and clients.
- What evidence would resolve it: Simulated attack scenarios with multiple colluding clients attempting model reconstruction across various communication rounds.

## Limitations
- Privacy claims rely on theoretical assumptions without formal information-theoretic bounds or differential privacy guarantees
- Server mask persistence across rounds creates potential single-point-of-failure vulnerability
- Absence of explicit attack models or reconstruction attempts to validate practical resistance to IP theft

## Confidence

- **High confidence**: Performance claims showing FedQSN achieves BLEU scores within 5-8 points of baselines across multiple datasets and model scales
- **Medium confidence**: Privacy protection claims based on parameter similarity metrics and performance gaps, limited by lack of attack validation
- **Low confidence**: Claims about IP protection against sophisticated adversaries without implementing or simulating real-world attack scenarios

## Next Checks

1. Implement gradient reconstruction or model inversion attacks on the proxy models to measure actual information leakage beyond parameter similarity metrics

2. Test whether varying server masks across rounds (rather than fixed) improves privacy without significant performance degradation

3. Evaluate FedQSN on non-LLM architectures (vision, tabular) to verify the masking/quantization approach generalizes beyond generative language models