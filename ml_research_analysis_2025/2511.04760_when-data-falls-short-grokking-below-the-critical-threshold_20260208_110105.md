---
ver: rpa2
title: 'When Data Falls Short: Grokking Below the Critical Threshold'
arxiv_id: '2511.04760'
source_url: https://arxiv.org/abs/2511.04760
tags:
- grokking
- data
- weight
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates grokking\u2014delayed generalization following\
  \ overfitting\u2014in data-scarce regimes and under distribution shifts. The authors\
  \ show that knowledge distillation (KD) from a model that has already grokked on\
  \ one distribution (p1) can induce and accelerate grokking on a different distribution\
  \ (p2), even when training data falls below the critical threshold where grokking\
  \ typically fails."
---

# When Data Falls Short: Grokking Below the Critical Threshold

## Quick Facts
- **arXiv ID**: 2511.04760
- **Source URL**: https://arxiv.org/abs/2511.04760
- **Reference count**: 40
- **Key outcome**: Knowledge distillation from a grokked model enables generalization below the critical data threshold for modular arithmetic tasks.

## Executive Summary
This paper investigates grokking—delayed generalization following overfitting—in data-scarce regimes and under distribution shifts. The authors show that knowledge distillation (KD) from a model that has already grokked on one distribution (p1) can induce and accelerate grokking on a different distribution (p2), even when training data falls below the critical threshold where grokking typically fails. They demonstrate this on modular addition and subtraction tasks using a one-layer Transformer. KD not only speeds up generalization but also enables it below the critical data size, challenging theories that link grokking to weight decay or decreasing weight norms. In continual learning settings, KD mitigates catastrophic forgetting while preserving performance.

## Method Summary
The method uses a one-layer decoder-only Transformer trained on modular arithmetic tasks (a±b mod P) with input format `[a, b, @, P]` where `@` is the operator token. KD loss combines standard cross-entropy with KL divergence between softened teacher and student outputs: L = (1-α)L_CE + α·L_KL. The teacher model is first trained on distribution p1 until grokked (test accuracy ~100%), then frozen while the student trains on distribution p2 using KD. StableMax cross-entropy replaces standard softmax for numerical stability during extended training runs. The critical threshold for grokking without KD is approximately 25% of the full dataset.

## Key Results
- KD accelerates grokking and enables it at 20% data where standard training fails entirely
- Operator-token distillation transfers generic arithmetic representations across modulus changes
- Grokking occurs with increasing weight norms, contradicting theories linking it to weight decay
- KD mitigates catastrophic forgetting in continual learning settings while preserving adaptation speed

## Why This Works (Mechanism)

### Mechanism 1: Cross-Distribution Knowledge Transfer via Softened Probabilities
- Claim: KD from a grokked teacher enables student generalization below the critical threshold
- Mechanism: Teacher's softened probability outputs encode algorithmic structure, reducing the search space for generalizing solutions
- Core assumption: Teacher has truly grokked (not memorizing) and p1/p2 share algorithmic structure
- Evidence anchors: Abstract states KD enables grokking below threshold; Figure 4b shows 20% data grokking with KD
- Break condition: Teacher hasn't grokked or p1/p2 lack shared algorithmic structure

### Mechanism 2: Operator-Level Representation Distillation
- Claim: Distilling from operator tokens enables transfer across distributions
- Mechanism: KD on operator token outputs learns generic algorithmic representations rather than modulus-specific memorizations
- Core assumption: Operator token representations generalize across modulus changes
- Evidence anchors: Section 4 states distillation is applied to operator token outputs
- Break condition: Tasks without clear operator/token structure

### Mechanism 3: Label Smoothing as Implicit Regularization
- Claim: KD provides regularization through label smoothing, reducing variance
- Mechanism: Softened teacher outputs smooth label distribution, preventing overfitting to limited samples
- Core assumption: Temperature parameter produces meaningful softened distributions
- Evidence anchors: Cites Menon et al. [20] on variance reduction; Figure 2b shows KD works across optimizers
- Break condition: Temperature too low (hard labels) or too high (uniform noise)

## Foundational Learning

- **Grokking phenomenon**: Delayed generalization after extended overfitting; why needed because the entire paper builds on understanding this concept; quick check: Can you explain why test accuracy can suddenly jump after extended overfitting on training data?

- **Knowledge Distillation basics**: KL divergence between teacher and student logits with temperature scaling; why needed because the method relies on this formulation; quick check: What role does temperature play in softening probability distributions for distillation?

- **Modular arithmetic as algorithmic task**: (a ± b) mod P tasks exhibit grokking; why needed because all experiments use these tasks; quick check: Why might modular arithmetic be well-suited for studying generalization versus memorization?

## Architecture Onboarding

- **Component map**: Teacher model (fT) on p1 → Frozen teacher → Student model (fS) on p2 → Combined CE + KL loss → Monitor test accuracy

- **Critical path**: 1) Train teacher on p1 with ≥35% data until grokked, 2) Initialize student on p2 with limited data (<25%), 3) Train student using CE + KL loss from teacher, 4) Monitor test accuracy for grokking

- **Design tradeoffs**: Higher α accelerates grokking but reduces ground-truth signal; weight decay helps but isn't necessary; StableMax required for numerical stability at extended training

- **Failure signatures**: Without KD at ≤25% data: training loss converges but test accuracy stays near random; without KD in continual learning: catastrophic forgetting of p1; weight norm increases observed in all successful grokking runs

- **First 3 experiments**: 1) Baseline replication: Train Transformer on (a+b) mod 113 with 35% data, confirm grokking; reduce to 20%, confirm grokking fails without KD, 2) KD across distributions: Grok teacher on mod 113, train student on mod 107 with 20% data using KD, compare steps-to-grokking, 3) Continual learning test: Take grokked model on p1, continue training on p2 with and without KD, measure forgetting and adaptation speed

## Open Questions the Paper Calls Out

- **Can KD from grokked models enable generalization on complex, non-algorithmic tasks?** The conclusion suggests future work could extend insights to more complex tasks and broader uses of pre-grokked models. This remains unresolved because all experiments use modular arithmetic tasks. Evidence would come from successful KD-induced grokking on standard benchmarks like CIFAR or language tasks below their critical data thresholds.

- **What is the true mechanistic driver of grokking if weight decay and decreasing weight norms aren't necessary?** The paper refutes prior theories but doesn't provide a definitive alternative explanation. This is unresolved because the paper empirically disproves existing theories without proposing a complete alternative. Evidence would come from causal intervention studies isolating which component of KD enables grokking below threshold.

- **How does KD effectiveness scale with sequential distribution shifts in continual learning?** The continual learning experiments only examine a single transition (p1 → p2). This is unresolved because practical continual learning involves many sequential tasks, and it's unclear whether KD's benefits accumulate or degrade over multiple shifts. Evidence would come from experiments with 5+ sequential distribution shifts.

## Limitations
- Experimental scope limited to modular arithmetic tasks, leaving open questions about generalization to other grokking-prone domains
- Does not address potential confounding factors like temperature hyperparameter sensitivity
- Does not test whether findings extend beyond one-layer Transformers to deeper architectures

## Confidence
- **High Confidence**: The core empirical finding that KD accelerates grokking and enables it below the critical threshold is well-supported by Figure 4b and the continual learning experiments
- **Medium Confidence**: The claim that KD provides label smoothing regularization has theoretical backing but limited direct corpus support for this specific application to grokking
- **Low Confidence**: The paper doesn't address how teacher model quality affects transfer effectiveness or whether the phenomena extend to deeper architectures

## Next Checks
1. **Temperature Sensitivity Analysis**: Systematically vary the temperature parameter t in the KD loss across a range (0.5 to 4.0) and measure the impact on grokking acceleration

2. **Teacher Quality Dependence**: Train multiple teacher models on p1 with varying data fractions (20%, 25%, 30%, 35%) and evaluate whether KD transfer effectiveness correlates with teacher generalization quality

3. **Beyond Modular Arithmetic**: Replicate the KD acceleration effect on a different grokking-prone task such as the "grokking the bullseye" dataset or synthetic algorithmic tasks with different structural properties