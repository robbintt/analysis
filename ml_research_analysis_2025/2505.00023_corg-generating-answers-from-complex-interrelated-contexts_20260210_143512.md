---
ver: rpa2
title: 'CORG: Generating Answers from Complex, Interrelated Contexts'
arxiv_id: '2505.00023'
source_url: https://arxiv.org/abs/2505.00023
tags:
- contexts
- context
- question
- each
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of answering questions using
  real-world corpora that contain complex, interrelated contexts with inconsistencies
  like ambiguous naming, outdated information, or errors. It classifies these interrelationships
  into four types: distracting, ambiguous, counterfactual, and duplicated.'
---

# CORG: Generating Answers from Complex, Interrelated Contexts

## Quick Facts
- arXiv ID: 2505.00023
- Source URL: https://arxiv.org/abs/2505.00023
- Reference count: 31
- One-line primary result: CORG framework improves performance on question answering with complex interrelated contexts by organizing contexts into scenario-based groups, achieving the best trade-off between performance and efficiency.

## Executive Summary
This paper addresses the challenge of answering questions using real-world corpora containing complex, interrelated contexts with inconsistencies like ambiguous naming, outdated information, or errors. The authors classify these interrelationships into four types: distracting, ambiguous, counterfactual, and duplicated. They introduce CONTEXT ORGANIZER (CORG), a framework that organizes multiple contexts into independently processed groups to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator, and achieves the best trade-off between performance and efficiency on eight language models across two datasets.

## Method Summary
CORG processes complex, interrelated contexts by first constructing a graph representing context relationships using a GPT-4-based graph constructor. The reranker then organizes contexts into scenario-based groups based on detected relationships, converting ambiguous relationships to distracting ones via descriptor enrichment and separating counterfactual contexts into distinct groups. The aggregator generates responses with citations for each group, optionally pluralizing questions when multiple distracting contexts exist. The framework is evaluated on AmbigDocs+ and ConflictQA+ datasets across eight language models, measuring Entity Recall, Answer Recall, Entity-Answer Recall, and Disambig-F1.

## Key Results
- CORG consistently improves performance over six baselines across all language models tested
- Achieves best trade-off between performance and efficiency, outperforming existing grouping methods
- Excels in entity recall, measuring the model's ability to identify disambiguated entities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating counterfactual contexts into independent forward passes improves answer recall over joint processing.
- Mechanism: Counterfactual contexts confuse models when processed together. By detecting these conflicts via graph construction and routing each conflicting context to a separate inference group, the model generates distinct answers per scenario rather than collapsing to a single (often incorrect) answer.
- Core assumption: Models struggle to output multiple conflicting answers in a single generation pass.
- Evidence anchors:
  - Table 4 shows Separation achieves 51.2 Ent / 85.9 Ans / 26.1 D-F1 for counterfactual contexts, dramatically outperforming single-pass methods (e.g., Plural: 34.8 Ent / 50.5 Ans).
  - Section 3.2 states: "we explore a simple method by processing each context individually, observing significant gains with separated contexts."

### Mechanism 2
- Claim: Converting ambiguous relationships to distracting relationships by enriching contexts with descriptors improves entity disambiguation.
- Mechanism: Ambiguous contexts lack descriptors (e.g., "The Simpsons" without "Season 2"), making it hard for models to associate answers with specific entities. The reranker replaces contexts missing descriptors with equivalent contexts that include descriptors, transforming ambiguous→distracting.
- Core assumption: A context with equivalent content plus a descriptor exists or can be retrieved/generated.
- Evidence anchors:
  - Table 3 shows "Change to Distracting" achieves 52.5 Ent / 36.8 EAR / 21.7 D-F1 vs. base ambiguous processing at 31.0 Ent / 29.9 EAR / 17.2 D-F1.
  - Section 3.2 states: "shifting ambiguous relationships to distracting ones... yields the highest performance, outperforming other approaches."

### Mechanism 3
- Claim: Pluralizing questions when multiple distracting contexts exist signals the model to generate comprehensive multi-answer responses.
- Mechanism: For distracting contexts (same surface name, different descriptors → different valid answers), models default to selecting one answer. Reformulating "What is X?" to "What are Xs?" primes the model to enumerate all relevant answers.
- Core assumption: The model has been trained on sufficient plural-form QA data.
- Evidence anchors:
  - Table 2 shows Plural achieves 68.9 Ent / 67.9 Ans / 28.0 D-F1 vs. base 52.5 Ent / 53.0 Ans / 21.7 D-F1 for distracting contexts.
  - Section 3.2 notes: "pluralizing the question yielded the greatest improvement... This was followed by the additional prompt and the one-shot example."

## Foundational Learning

- Concept: **Context relationship taxonomy (distracting, ambiguous, counterfactual, duplicated)**
  - Why needed here: CORG's reranker decisions are conditioned on detected relationship types; misclassification propagates through the pipeline.
  - Quick check question: Given contexts "Paris has 2M people" and "Paris, Texas has 25K people," what relationship type is this for the question "What is the population of Paris?"

- Concept: **Graph construction with edge propagation for efficiency**
  - Why needed here: Algorithm 1 exploits the property that counterfactual/duplicated nodes share edges with remaining nodes, reducing quadratic pairwise comparisons.
  - Quick check question: If contexts A and B are counterfactual, and we've computed (A, C) = "distracting," what can we infer about (B, C) without an additional LLM call?

- Concept: **Performance-efficiency trade-offs in multi-pass inference**
  - Why needed here: CORG positions itself between single-pass (efficient but lower recall) and per-context inference (high recall but expensive); the reranker's group count directly controls this trade-off.
  - Quick check question: If your latency budget allows 2 inference runs and you have 8 contexts (2 counterfactual, 2 duplicated, 4 distracting), how should you allocate contexts to groups?

## Architecture Onboarding

- Component map:
  - **Graph Constructor** (GPT-4) -> **Reranker** -> **Aggregator**

- Critical path:
  1. Graph construction is the bottleneck (requires GPT-4 calls for relationship classification).
  2. Reranker's group count determines downstream inference cost.
  3. Aggregator's citation formatting is trivial; correctness depends entirely on upstream grouping.

- Design tradeoffs:
  - **Graph Constructor accuracy vs. cost**: Using GPT-4 for relationship detection is accurate but expensive; cheaper models may misclassify edges.
  - **Group granularity**: Fewer groups → lower cost, but conflates counterfactuals → lower recall. More groups → higher recall but diminishing returns.
  - **Descriptor enrichment**: Converting ambiguous→distracting improves disambiguation but requires descriptor-bearing contexts to exist or be generatable.

- Failure signatures:
  - **Low entity recall with high answer recall**: Model outputs answers without matching descriptors → check if ambiguous→distracting conversion failed.
  - **Contradictory answers in single response**: Counterfactual contexts weren't separated → check reranker's conflict detection.
  - **Excessive inference runs**: Many small groups → graph constructor over-detecting counterfactuals or reranker not collapsing duplicates.

- First 3 experiments:
  1. **Ablate graph constructor**: Replace GPT-4 relationship detection with a smaller model (e.g., Llama-7B) or rule-based heuristics; measure D-F1 degradation and cost savings on AmbigDocs+.
  2. **Vary group count constraint**: Force CORG to use max 1/2/3 groups regardless of detected relationships; plot D-F1 vs. FLOPs to find optimal efficiency frontier.
  3. **Test descriptor enrichment failure mode**: Remove the ambiguous→distracting conversion step; isolate performance delta specifically on contexts with missing descriptors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can CORG be effectively integrated into an end-to-end retrieval-augmented generation pipeline without reintroducing the search engine biases observed in isolated contexts?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that they "leave the integration of retrieval from large corpora as future work," noting that they pre-retrieved contexts to control for retrieval error and bias.
- **Why unresolved:** The current framework assumes relevant contexts are provided as input; real-world application requires handling retrieval noise and bias dynamically, which the current experimental setup deliberately excludes.
- **What evidence would resolve it:** Evaluations of CORG combined with a dense retrieval step (e.g., DPR or BM25) on a large, uncurated web corpus, measuring performance degradation relative to the oracle retrieval setting used in the paper.

### Open Question 2
- **Question:** Can training-based methods (e.g., fine-tuning) outperform CORG's inference-time grouping strategy in handling complex context interrelationships?
- **Basis in paper:** [explicit] The paper notes in the Limitations section that the goal was to design a flexible pipeline for inference, leaving "the exploration of training-based methods for handling complex interrelationships... as future work."
- **Why unresolved:** It remains unclear if the explicit decomposition of relationships via the Graph Constructor is more effective than implicitly learning to manage these relationships through weight updates.
- **What evidence would resolve it:** A comparative study fine-tuning models on the AmbigDocs+ dataset to recognize relationship types, compared against zero-shot CORG performance on the same test set.

### Open Question 3
- **Question:** Does the computational overhead of the Graph Constructor (using GPT-4) negate the efficiency gains achieved by the CORG aggregator?
- **Basis in paper:** [inferred] The paper utilizes GPT-4 for the Graph Constructor, but the efficiency analysis in Section 5.4 explicitly excludes "additional steps like... graph construction" from the FLOPs calculations.
- **Why unresolved:** While CORG reduces the FLOPs for the final generation step, the cost of classifying context relationships using a large proprietary model may result in higher overall latency and financial cost than reported.
- **What evidence would resolve it:** An end-to-end wall-clock time and cost analysis that includes the Graph Constructor API calls, compared against baselines that do not require complex pre-processing.

## Limitations
- Heavy reliance on GPT-4 for graph construction creates significant cost bottleneck
- Performance depends entirely on accurate relationship detection, with misclassification cascading through pipeline
- Conversion of ambiguous to distracting relationships assumes descriptor-bearing contexts always exist or can be generated

## Confidence

**High Confidence**: The core mechanism of separating counterfactual contexts into independent groups and the effectiveness of pluralizing questions for distracting contexts are well-supported by ablation results in Tables 2-4.

**Medium Confidence**: The ambiguous→distracting conversion mechanism, while showing strong results, assumes descriptor enrichment is always possible and doesn't extensively validate cases where no descriptor-bearing context exists.

**Low Confidence**: The generalization claim across "real-world corpora" is based primarily on synthetic dataset extensions, with framework's robustness to naturally occurring complex interrelationships untested.

## Next Checks

1. **Cost-Performance Tradeoff Analysis**: Implement CORG using a smaller model (e.g., Llama-3-70B) for graph construction instead of GPT-4, measuring the D-F1 degradation versus inference cost reduction across AmbigDocs+.

2. **Descriptor Enrichment Failure Mode**: Systematically remove all contexts with descriptors from the dataset, forcing ambiguous→distracting conversion to fail, then measure the performance drop to quantify reliance on this mechanism.

3. **Real-World Generalization Test**: Apply CORG to a naturally occurring corpus with known interrelationships (e.g., Wikipedia pages with entity ambiguity) rather than synthetic extensions, measuring D-F1 against baseline methods to assess real-world robustness.