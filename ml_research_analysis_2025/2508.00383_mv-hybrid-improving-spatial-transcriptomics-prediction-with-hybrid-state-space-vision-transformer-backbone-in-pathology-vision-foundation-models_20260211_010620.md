---
ver: rpa2
title: '$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State
  Space-Vision Transformer Backbone in Pathology Vision Foundation Models'
arxiv_id: '2508.00383'
source_url: https://arxiv.org/abs/2508.00383
tags:
- prediction
- hybrid
- mvhybrid
- performance
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MVHybrid, a hybrid state space-vision transformer
  backbone designed to improve spatial transcriptomics prediction from histopathology
  images. Current vision foundation models based on vision transformers underperform
  in biomarker prediction due to their limited ability to capture subtle, low-frequency
  morphological patterns that correlate with molecular phenotypes.
---

# $MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models

## Quick Facts
- arXiv ID: 2508.00383
- Source URL: https://arxiv.org/abs/2508.00383
- Reference count: 40
- Primary result: MVHybrid achieves 57% higher correlation than best ViT in spatial transcriptomics prediction from histopathology images

## Executive Summary
MVHybrid introduces a hybrid state space-vision transformer backbone that significantly improves spatial transcriptomics prediction from histopathology images. Current vision foundation models based on vision transformers underperform in biomarker prediction due to their limited ability to capture subtle, low-frequency morphological patterns that correlate with molecular phenotypes. By combining MambaVision (a state space model) with vision transformer layers, MVHybrid leverages the strong low-frequency bias of negative real eigenvalues to achieve superior performance and robustness on paired spatial transcriptomics and histology datasets.

## Method Summary
MVHybrid is a hybrid architecture combining 12 MambaVision layers (state space model with EinFFT channel mixing) and 12 standard ViT attention layers, with embedding dimension of 384. The model uses negative real eigenvalues (A = -exp(A_log)) in the state space component to create strong low-frequency bias. Pretrained on colorectal cancer datasets using DINOv2 self-supervised learning for 200 epochs with batch size 1536, the model achieves superior performance in spatial transcriptomics prediction compared to pure vision transformer and pure state space model baselines.

## Key Results
- MVHybrid achieves 57% higher correlation (PCC) than best-performing vision transformer in leave-one-study-out evaluation on spatial transcriptomics prediction
- Outperforms both pure ViT and pure SSM models on biomarker prediction (PCC 0.138 vs 0.097 for ViT, 0.083 for ViMEinFFT on HVG task)
- Shows equal or better performance in classification, patch retrieval, and survival prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative real eigenvalues in SSMs create stronger low-frequency bias than complex eigenvalues, enabling better capture of subtle morphological patterns that correlate with molecular phenotypes.
- Mechanism: For negative real eigenvalues aⱼ = −|λⱼ|, the high-frequency total variation V^∞_ω₀(G) decays as O(1/ω₀), which is uniformly faster than O(1/(ω₀−wⱼ)) for complex eigenvalues. This creates cascaded low-pass filters with cutoff frequencies ω_c = |λⱼ|, progressively attenuating high frequencies while preserving low-frequency biological features.
- Core assumption: Biomarker prediction from H&E images requires capturing low-frequency patterns beyond human perception that correlate with molecular states, not just high-frequency morphological features like tumor boundaries.
- Evidence anchors:
  - [abstract] "state space models initialized with negative real eigenvalues exhibit strong low-frequency bias"
  - [Section 1.1] Derivation showing V^∞_ω₀(G) ∼ Σⱼ|cⱼ|/√(|λⱼ|² + ω₀²) for negative real eigenvalues vs. V^∞_ω₀(G) ≤ Σⱼ|cⱼ|/|wⱼ−ω₀| for complex
  - [corpus] Neighbor papers confirm cross-modal histology-to-gene-expression learning is an active challenge with no established architectural solutions

### Mechanism 2
- Claim: The hybrid SSM-first, ViT-second architecture outperforms pure architectures because early SSM layers capture low-frequency biological patterns while later ViT layers provide proven representational capacity.
- Mechanism: MVHybrid places 12 MambaVision layers (with EinFFT channel mixing) followed by 12 standard ViT attention layers. The SSM layers' negative real eigenvalues create broad cascaded low-pass filtering, then ViT attention mechanisms refine these representations for downstream tasks.
- Core assumption: Low-frequency feature extraction benefits from occurring before rather than after or parallel to attention-based processing; the ordering matters.
- Evidence anchors:
  - [abstract] "MVHybrid combines MambaVision (a state space model) with vision transformer layers to leverage the strong low-frequency bias"
  - [Table 3-4] MVHybrid achieves PCC 0.138 (HVG) and 0.212 (HMHVG) in LOSO, outperforming both pure ViT (0.097, 0.110) and pure SSM models like ViMEinFFT (0.083, 0.124)
  - [corpus] No comparable hybrid SSM-ViT architectures found in spatial transcriptomics prediction literature

### Mechanism 3
- Claim: Regular convolution layers in MambaVision (replacing causal convolutions) combined with EinFFT channel mixing provide vision-specific inductive biases and training stability during self-supervised pretraining.
- Mechanism: MambaVision replaces Mamba's causal convolutions with regular convolutions in both the SSM block and skip connections, enabling bidirectional spatial processing. EinFFT channel mixing (vs. MLP) prevents unstable pretraining that was empirically observed with Mamba-based mixers.
- Core assumption: Pathology images benefit from bidirectional, non-causal processing unlike language sequences; DINOv2 self-supervision requires specific architectural conditions for stability.
- Evidence anchors:
  - [Section 2.1] "MV replaces the causal convolution layers in Mamba with regular convolutional layers and adds additional regular convolutional layers in the skip connection layer"
  - [Section 2.1] "all Mamba-based sequence mixers are incompatible with MLP channel mixers due to unstable pretraining... we use EinFFT channel mixing blocks instead"
  - [corpus] Weak evidence—neighbor papers do not discuss SSM architectural variants for vision tasks

## Foundational Learning

- Concept: **State Space Models (SSMs) as sequence-to-sequence transformations**
  - Why needed here: Understanding how the A matrix eigenvalues govern frequency response is essential to grasp why MVHybrid captures low-frequency patterns.
  - Quick check question: Given eigenvalues a₁ = −2 and a₂ = −5, which creates a lower cutoff frequency for its low-pass filter component?

- Concept: **Frequency bias in visual architectures**
  - Why needed here: The paper's central hypothesis is that ViTs underperform on biomarker prediction because they lack low-frequency bias; you must understand what this means mechanistically.
  - Quick check question: Why would low-frequency features in H&E images correlate with gene expression when high-frequency features correlate with tumor boundaries?

- Concept: **Self-supervised learning with DINOv2**
  - Why needed here: All models were pretrained identically with DINOv2; understanding this pretraining method is necessary for reproducibility and extension.
  - Quick check question: DINOv2 is a self-distillation method—what does the teacher-student architecture imply for how feature representations are learned?

## Architecture Onboarding

- Component map:
  Input (256×256 patch) -> Patch Embedding (384 dim) -> [Layers 1-12: MVHybrid Block with MambaVision+EinFFT] -> [Layers 13-24: ViT Block with attention+MLP] -> Output embedding (384 dim)

- Critical path:
  1. Verify A matrix parameterization: A = −exp(A_log) ensures strictly negative real eigenvalues
  2. Confirm EinFFT channel mixing in first 12 layers (MLP causes instability)
  3. Use DINOv2 pretraining protocol with batch size ≥1,536 for stable self-distillation

- Design tradeoffs:
  - **Hybrid vs. Pure SSM**: Hybrid adds ViT complexity (30.9M params vs. 29.0M for ViMEinFFT) but outperforms on all tasks
  - **EinFFT vs. MLP**: EinFFT required for stable DINOv2 pretraining with Mamba mixers; adds Fourier-based channel mixing overhead
  - **Isotropic vs. Hierarchical**: Paper modifies MV to isotropic (non-hierarchical) for DINOv2 compatibility, trading multi-scale features for SSL compatibility

- Failure signatures:
  - Training loss divergence during DINOv2 pretraining → Check that EinFFT is used, not MLP, in SSM layers
  - Positive real eigenvalues emerging during training → Verify A = -exp(A_log) parameterization is correctly implemented
  - Poor LOSO performance with good random-split performance → Model capturing site-specific rather than biological features; may need stronger regularization or different architecture

- First 3 experiments:
  1. **Eigenvalue verification**: Load pretrained MVHybrid, extract A matrices from all SSM layers, plot eigenvalue distribution. Expect strictly negative real values spanning ~−10 to 0 (per Figure 2).
  2. **Frequency response analysis**: Compute transfer function G(iω) for trained vs. randomly initialized models; verify low-frequency preservation using synthetic sinusoidal inputs at varying frequencies.
  3. **Ablation on hybrid split point**: Train variants with 6 SSM/18 ViT layers, 18 SSM/6 ViT layers, and 12/12 to determine if the 50/50 split is optimal or arbitrary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do individual architectural modifications, specifically the choice of sequence and channel mixers, impact the downstream performance of MVHybrid?
- Basis in paper: [explicit] The authors state, "We leave performing ablation studies for each sequence and channel mixers of $MV_{Hybrid}$ to analyze how individual modifications impact performance to future work."
- Why unresolved: The current study evaluates the fully integrated hybrid model against baselines but does not isolate the contribution of the MambaVision sequence mixer versus the EinFFT channel mixer or the specific hybrid split ratio.
- What evidence would resolve it: A comprehensive ablation study comparing MVHybrid variants with swapped components (e.g., standard MLP channel mixers) and different layer allocation ratios.

### Open Question 2
- Question: Can MVHybrid maintain its superior performance and robustness when validated on larger, multi-centric clinical cohorts and non-colorectal cancer pathologies?
- Basis in paper: [explicit] The conclusion notes that "more extensive validation on public and clinical paired ST-H&E data is needed."
- Why unresolved: The current evaluation relies primarily on specific colorectal cancer datasets (HEST-Benchmark/Extended) and simulated robustness tests (LOSO), leaving real-world clinical generalization unproven.
- What evidence would resolve it: Benchmarking results on diverse, independent spatial transcriptomics datasets covering other cancer types and prospective clinical trial data.

### Open Question 3
- Question: What is the theoretical or empirical root cause of the training instability observed when combining Mamba-based sequence mixers with standard MLP channel mixers?
- Basis in paper: [inferred] The authors mention they "empirically found that all Mamba-based sequence mixers are incompatible with MLP channel mixers due to unstable pretraining" but do not provide a mathematical or systematic explanation for this phenomenon.
- Why unresolved: The paper attributes the instability to "possibly due to positive real eigenvalues" but relies on EinFFT as a workaround rather than fully diagnosing the interaction between the SSM state dynamics and the MLP gradients.
- What evidence would resolve it: A gradient flow analysis or eigenvalue trajectory comparison during pretraining of Mamba-MLP versus Mamba-EinFFT architectures.

## Limitations

- The claimed superiority of negative real eigenvalues over complex eigenvalues has not been empirically validated in isolation from the hybrid architecture
- The cross-modal learning literature shows inconsistent results regarding whether vision foundation models transfer well to spatial transcriptomics prediction
- The paper does not test pure SSM models with different eigenvalue parameterizations (real vs. complex) on the same tasks

## Confidence

- **High confidence**: The empirical results showing MVHybrid's superior performance on PCC metrics in LOSO evaluation (57% improvement over best ViT) are well-supported by the data presented in Tables 3-4.
- **Medium confidence**: The theoretical mechanism linking negative real eigenvalues to low-frequency bias is mathematically sound, but the practical significance for biomarker prediction versus other architectural choices remains partially speculative.
- **Medium confidence**: The hybrid architecture's superiority over pure architectures is demonstrated, but whether the 12/12 split is optimal versus arbitrary requires further ablation studies.

## Next Checks

1. **Eigenvalue isolation test**: Train and evaluate pure SSM models (ViMEinFFT) with parameterized eigenvalues restricted to negative real values versus complex values on the same LOSO spatial transcriptomics prediction task to isolate the eigenvalue effect.

2. **Hybrid architecture ablation**: Systematically vary the SSM/ViT layer split (6/18, 12/12, 18/6) while keeping all other factors constant to determine if the 50/50 split is optimal or could be improved.

3. **Downstream task generalization**: Evaluate MVHybrid on additional cross-modal tasks beyond biomarker prediction (e.g., tissue-of-origin classification, multi-modal retrieval) to assess whether the low-frequency bias advantage generalizes beyond the specific spatial transcriptomics prediction task.