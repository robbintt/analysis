---
ver: rpa2
title: 'Generation Space Size: Understanding and Calibrating Open-Endedness of LLM
  Generations'
arxiv_id: '2510.12699'
source_url: https://arxiv.org/abs/2510.12699
tags:
- prompt
- eigenscore
- prompts
- reasoning
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper formalizes generation space size (GSS) as a unified\
  \ framework for understanding LLM failures like output homogeneity and hallucination,\
  \ then introduces GSSBench to evaluate metrics that proxy a model\u2019s internal\
  \ generation space. Using prompt pairs with known GSS relationships, it shows that\
  \ EigenScore variants\u2014particularly Eoutput and Eaverage\u2014best approximate\
  \ GSS across all models tested, outperforming traditional uncertainty and diversity\
  \ metrics."
---

# Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations

## Quick Facts
- arXiv ID: 2510.12699
- Source URL: https://arxiv.org/abs/2510.12699
- Authors: Sunny Yu; Ahmad Jabbar; Robert Hawkins; Dan Jurafsky; Myra Cheng
- Reference count: 40
- One-line result: EigenScore variants best approximate LLM generation space size, outperforming traditional metrics and enabling applications from hallucination detection to diversity optimization.

## Executive Summary
This paper introduces generation space size (GSS) as a unified framework for understanding LLM failures like output homogeneity and hallucination. The authors propose GSSBench, a benchmark that evaluates metrics based on their ability to proxy a model's internal generation space using prompt pairs with known GSS relationships. Through comprehensive evaluation, EigenScore variants—particularly Eoutput and Eaverage—consistently outperform traditional uncertainty and diversity metrics across all models tested. The paper demonstrates three key applications: detecting prompt ambiguity, interpreting reasoning model behaviors, and steering models toward more diverse outputs via a novel Leave-One-Out EigenScore metric.

## Method Summary
The method constructs prompt pairs using set-theoretic operations (complement, union, intersection, subset) to create ground-truth GSS relationships. For each prompt, K=10 generations are sampled at temperature=1.0 with top-k=10. EigenScore variants are computed by measuring the differential entropy (log determinant of covariance matrix) of semantic embeddings—either internal hidden states (Eaverage) or external sentence embeddings (Eoutput). GSSBench evaluates metrics by their pairwise accuracy in ordering prompts according to their GSS. The framework also introduces LOOE (Leave-One-Out EigenScore) for response-level diversity optimization, enabling diversity-aware preference optimization without post-hoc sampling metrics.

## Key Results
- EigenScore variants (Eoutput, Eaverage) achieve 0.72-0.75 pairwise accuracy on Llama-8B vs ~0.60 for perplexity
- GSS correlates with clarification question need and reasoning token length
- LOOE-based DivPO achieves similar diversity and reward to existing methods
- Larger models show inverse scaling on GSS calibration (0.6B outperforms 8B)

## Why This Works (Mechanism)

### Mechanism 1: EigenScore Captures Differential Entropy in Semantic Embedding Space
EigenScore variants measure spread in semantic embedding space by computing the log determinant of covariance matrices. This captures differential entropy—the volume of the semantic space the model "considers." Averaging across layers and tokens (Eaverage) or using external sentence encoders (Eoutput) provides richer semantic signal than single-layer approaches. Core assumption: covariance structure of embeddings reflects the model's internal task representation.

### Mechanism 2: Set-Theoretic Prompt Pairs Enable Ground-Truth GSS Comparisons
Prompt pairs constructed via set operations provide reliable ordinal GSS relationships. By design, "Write an email to Dan" has smaller GSS than "Write an email" (subset relation). If metric f scores first prompt lower than second, it correctly captures GSS ordering. Pairwise accuracy across 9300 pairs evaluates metric quality.

### Mechanism 3: Leave-One-Out EigenScore Enables Response-Level Diversity Optimization
LOOE measures individual response contribution to overall generation spread. For response xi, LOOEi = Eglobal − E(S \ {xi}). Responses whose removal substantially decreases spread have higher LOOE. Use LOOE as diversity criterion in DivPO-style training to select high-LOOE responses.

## Foundational Learning

- Concept: **Differential entropy in embedding space**
  - Why needed: EigenScore relies on understanding that log determinant of covariance matrix approximates "volume" occupied by samples in continuous space
  - Quick check: Given three 2D points at (0,0), (1,0), (0,1), would their covariance log-det be higher or lower than three points at (0,0), (0.1,0), (0,0.1)?

- Concept: **Set-theoretic constraints on generative spaces**
  - Why needed: GSSBench's evaluation logic depends on understanding how operations affect valid output cardinality
  - Quick check: For prompt "Generate a poem about love OR a short story about loss," is the GSS larger or smaller than "Generate a poem about love AND a short story about loss"?

- Concept: **Preference optimization with auxiliary objectives**
  - Why needed: LOOE-based DivPO requires understanding how to incorporate non-reward signals into DPO-style training
  - Quick check: In standard DPO, what happens if "chosen" response has lower reward than "rejected" response?

## Architecture Onboarding

- Component map: Prompt pairs -> K generations -> Embedding extraction -> Covariance computation -> Log-determinant -> Metric scoring -> Pairwise accuracy evaluation
- Critical path: Sampling parameters matter. Temperature=1.0, sample size=10-20, top-k=10 work best. Higher temperature than hallucination detection settings because broader sampling better captures embedding spread.
- Design tradeoffs:
  - Eoutput (external sentence encoder) vs. Eaverage (internal layer averaging): External captures semantic similarity but decouples from model internals; internal maintains model-specific semantics but may inherit biases
  - Sample size vs. computation: Diminishing returns above 20 samples; 10 is reasonable default
- Failure signatures:
  - Miscalibration on Random Choice: Llama-8B fails to distinguish 2-option vs 10-option prompts
  - Inverse scaling: Qwen3-0.6B outperforms 8B on most metrics
- First 3 experiments:
  1. Validate metric on your model: Run GSSBench's Complement and Random Choice datasets; verify Eoutput/Eaverage achieve >0.70 pairwise accuracy
  2. Correlation check: Verify EigenScore correlates with human judgments of output diversity for your domain-specific prompts
  3. LOOE threshold ablation: Table A24 shows p=0.6 works; ablate threshold on your data to balance diversity vs reward

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GSS-aware alignment techniques train LLMs to dynamically adjust their generation space size based on task type?
- Basis: Authors state future work should focus on "training and aligning models to dynamically adjust their GSS based on different task types"
- Why unresolved: Current alignment methods don't explicitly teach models to calibrate breadth of internal reasoning space relative to prompt constraints
- What evidence would resolve: Fine-tuning method using GSS-based rewards that improves accuracy on factual tasks while enhancing diversity on creative tasks

### Open Question 2
- Question: What mechanisms drive the observed "inverse scaling" effect where larger instruction-tuned models show worse GSS calibration?
- Basis: Authors encourage investigating the inverse scaling effect found in GSSBench
- Why unresolved: Unclear if miscalibration stems from instruction-tuning process or structural changes in larger models' representation space
- What evidence would resolve: Layer-wise analysis comparing smaller and larger model checkpoints to identify divergence points

### Open Question 3
- Question: How can GSS metrics be extended to be content-sensitive to distinguish between a correct singleton and a consistent hallucination?
- Basis: Authors acknowledge GSS is agnostic to content, meaning a model consistently generating same wrong answer appears well-calibrated
- Why unresolved: Current proxies measure semantic spread but cannot determine if semantic cluster is centered on correct fact or hallucination
- What evidence would resolve: Hybrid metric combining EigenScore with entailment checks or ground-truth verification

## Limitations
- Ground-truth GSS relationships may not perfectly align with model's internal representation
- Sampling parameter dependency with no systematic exploration of calibration
- EigenScore may reward semantically diverse but low-quality (adversarial) responses
- GSS is agnostic to content—cannot distinguish correct singleton from consistent hallucination

## Confidence

**High Confidence** (Multiple independent validations, mechanistic explanation provided):
- EigenScore variants outperform traditional metrics on GSSBench pairwise accuracy
- Set-theoretic prompt pairs provide meaningful GSS orderings for evaluation
- LOOE-based DivPO achieves comparable diversity and reward to existing methods

**Medium Confidence** (Strong evidence but with acknowledged limitations):
- EigenScore captures differential entropy in semantic embedding space
- GSS correlates with clarification question need and reasoning depth
- LOOE correlates with human judgment of response diversity

**Low Confidence** (Preliminary evidence, significant limitations):
- GSS calibration across different model sizes
- Generalization of GSSBench to non-set-theoretic domains
- External validity of EigenScore for real-world applications beyond evaluated benchmarks

## Next Checks

1. **Ground-truth validation**: Collect human judgments for 50 random prompt pairs from GSSBench to compute correlation between human rankings and EigenScore predictions, validating that the metric captures human-perceived output space size.

2. **Domain transfer test**: Apply Eoutput/Eaverage to code generation with different API constraints and verify that EigenScore correctly ranks prompts by actual output diversity measured through execution coverage or manual inspection.

3. **Adversarial robustness check**: Generate prompts eliciting semantically diverse but incorrect responses and test whether Eoutput/Eaverage still correlate with quality-weighted diversity by computing a "weighted EigenScore" incorporating response correctness scores.