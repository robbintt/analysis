---
ver: rpa2
title: 'Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution Can Improve
  Robust Scene Graph Generation'
arxiv_id: '2504.12606'
source_url: https://arxiv.org/abs/2504.12606
tags:
- scene
- object
- graph
- pages
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Robo-SGG, a plug-and-play module designed
  to improve scene graph generation (SGG) under image corruptions. The method addresses
  the challenge of domain shift between clean and corrupted images, which degrades
  the performance of existing SGG models.
---

# Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution Can Improve Robust Scene Graph Generation

## Quick Facts
- **arXiv ID**: 2504.12606
- **Source URL**: https://arxiv.org/abs/2504.12606
- **Reference count**: 40
- **Primary result**: Robo-SGG improves mR@50 by 6.3-11.1% on VG-C corruption benchmarks

## Executive Summary
Robo-SGG addresses the challenge of scene graph generation under image corruption by introducing a plug-and-play module that improves robustness to domain shift. The method employs Instance Normalization to mitigate corruption-specific feature statistics and Layout-aware Attention to recover structural relationships. A Layout-Embedded Encoder with adaptive gating further enhances robustness by dynamically balancing visual and spatial cues. Experimental results demonstrate significant improvements across multiple SGG tasks on corrupted datasets.

## Method Summary
Robo-SGG introduces a Layout-Oriented Normalization and Restitution Module (NRM) that applies Instance Normalization to backbone features to remove domain-specific corruption signatures, then recovers structural features through layout-aware attention using object centroids. The Layout-Embedded Encoder (LEE) adaptively fuses visual features and bounding box embeddings via a learned gating mechanism. The method integrates seamlessly into existing SGG architectures and achieves state-of-the-art performance on corruption benchmarks without requiring retraining on corrupted data.

## Key Results
- Relative improvements of 6.3%, 11.1%, and 8.0% in mR@50 for PredCls, SGCls, and SGDet tasks on VG-C benchmark
- Sets new state-of-the-art performance on VG-C and GQA-C corruption scene graph generation benchmarks
- Achieves consistent gains across 20 corruption types and 5 severity levels
- Maintains robustness without additional parameters beyond ~0.02GB memory for gating

## Why This Works (Mechanism)

### Mechanism 1: Instance Normalization Alleviates Domain-Specific Corruption Features
Applying Instance Normalization (IN) to backbone feature maps reduces corruption-specific mean and variance, mitigating covariate shift between clean and corrupted images. IN computes statistics per-channel per-image (not batch-wide), removing style-like corruption signatures while preserving spatial structure. The normalized feature map $\hat{f}_{map}$ is combined with a layout-derived structural residual to form $f'_{map}$. This works because corruptions manifest as domain-specific shifts in feature statistics (mean/variance) that are separable from task-relevant structural content.

### Mechanism 2: Layout-Aware Attention Recovers Structural Features Lost to Normalization
A layout-derived attention mask selectively restores structural features from the IN residual, preserving object relationships critical for predicate prediction. Object centroids $e_j$ define spatial attention weights $A_{(m,l),j}$ via Gaussian distance. The max attention across objects forms mask $M$, applied to residual $R$ to extract structural features $R^+$, then added back: $f'_{map} = \hat{f}_{map} + R^+$. This works because layout (global spatial arrangement of object centroids) is corruption-invariant and encodes relationship-relevant structure.

### Mechanism 3: Gated Fusion Adaptively Balances Visual and Spatial Cues Under Corruption
A learned gate coefficient $z_i$ dynamically weights visual features vs. bounding box embeddings, reducing reliance on unreliable detection outputs under corruption. Gate $z_i = \text{Sigmoid}(W f_i)$ is computed from visual features, producing output $f'_i = (1-z_i) \circ f^C_i + z_i \circ f_i$. As corruption severity increases, mean gate decreases (0.65 → 0.52), shifting weight to layout cues. This works because visual feature quality degrades predictably with corruption severity, while layout remains relatively stable.

## Foundational Learning

- **Instance Normalization vs. Batch Normalization**
  - Why needed here: IN normalizes per-sample, making it suitable for test-time domain shift; BN requires batch statistics that may not generalize to unseen corruption types.
  - Quick check question: Given a batch of 4 images (2 clean, 2 corrupted), would BN or IN produce more consistent normalized features for the corrupted images?

- **Scene Graph Generation (SGG) Tasks**
  - Why needed here: The paper evaluates on PredCls (predicate classification given objects), SGCls (object+predicate classification), and SGDet (full detection); understanding these clarifies where Robo-SGG plugs in.
  - Quick check question: Which task requires both object detection and relationship prediction?

- **Attention Mechanisms for Feature Selection**
  - Why needed here: Layout-aware attention is a form of soft feature selection based on spatial priors; understanding attention helps diagnose why centroids outperform full boxes.
  - Quick check question: Why might using object centroids (points) be more robust to detection noise than using full bounding boxes (regions)?

## Architecture Onboarding

- **Component map:**
  ```
  [Backbone: ResNet-101] → f_map
                              ↓
                          [NRM] ← Layout (bboxes)
                              ↓
                         f'_map → [Object Encoder + LEE] → f'_i → [Object Decoder]
                                   ↑                              ↓
                              Bbox embeddings              Object predictions
                              ↓
                         [Predicate Encoder + LEE] → f'_{i→j} → [Predicate Decoder]
                                                                   ↓
                                                            Predicate predictions
  ```

- **Critical path:**
  1. Backbone produces `f_map` (C×H×W)
  2. NRM applies IN, computes layout attention from centroids, produces `f'_map`
  3. LEE fuses visual features with bbox embeddings via gating for each object/predicate
  4. Decoders predict labels from enhanced features

- **Design tradeoffs:**
  - **Centroids vs. full boxes for attention:** Centroids are less sensitive to detection noise but discard size/shape information (Table 4: +7.2% vs. +1.6%)
  - **Gate fusion vs. concatenation:** Gating adds parameters (~0.02GB memory) but enables adaptive balancing; concatenation is simpler but degrades under corruption
  - **NRM overhead:** Adds ~0.019s inference time but no extra parameters

- **Failure signatures:**
  - Severe blur degrades object boundaries → NRM attention mislocalizes → structural recovery fails
  - Missing object detections → centroid-based attention incomplete → some relationships never recovered
  - Very high corruption severity (level 5) → gating may not adapt fast enough

- **First 3 experiments:**
  1. **Ablate NRM components:** Test IN-only vs. IN+layout-attention vs. full NRM on VG-C (PredCls mR@50) to isolate restitution contribution
  2. **Visualize gate distributions:** Plot $z_i$ histograms across corruption types and severities to verify adaptive behavior matches paper claims
  3. **Stress-test with detector perturbations:** Add ±30% random noise to bounding boxes during inference (Table 5 setup) and measure SGDet degradation with/without LEE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the NRM be adapted to maintain performance gains under corruptions that specifically degrade layout information, such as severe motion or defocus blur?
- Basis in paper: [explicit] The "Per-corruption Detailed Results" section states: "Blur-related degradations... show more limited gains, as blur obscures object boundaries and degrades structural cues."
- Why unresolved: The method relies on Layout-aware Attention which assumes object centroids and boundaries are discernible; when these are obscured, the restitution mechanism fails to recover robust structural features.
- Evidence: Ablation studies on high-severity blur demonstrating recovered structural feature maps or the integration of deblurring preprocessing that preserves high-frequency details.

### Open Question 2
- Question: Can the geometric layout features be supplemented to resolve semantic ambiguity between spatially similar predicates (e.g., "near" vs. "behind")?
- Basis in paper: [explicit] The discussion of Figure 3 identifies a failure case where "both 'near' and 'behind' can describe the relationship," which the authors suggest could be "addressed with more fine-grained annotations."
- Why unresolved: The current model prioritizes global structural robustness, but lacks the specific semantic mechanisms to distinguish fine-grained spatial relationships when visual cues are suppressed.
- Evidence: Performance metrics specifically on semantically confused predicate pairs, or improvements via the integration of depth or contextual semantic encoders.

### Open Question 3
- Question: Does the LEE module's reliance on visual features to generate gating coefficients create a performance ceiling when those visual features are severely corrupted?
- Basis in paper: [inferred] The gating mechanism in Eq. 14 derives coefficients $z_i$ from visual features $f_i$. The paper assumes $f_i$ contains enough signal to balance spatial vs. visual info, but under high corruption (Severity 5), these guiding features may become uninformative.
- Why unresolved: It is unclear if the gate itself becomes noisy in extreme conditions, potentially fusing features sub-optimally compared to a static or layout-driven gate.
- Evidence: Analysis of the entropy of gate coefficients under maximum corruption severity compared to a baseline using layout-only gating.

## Limitations
- The method shows more limited gains on blur-related degradations as they obscure object boundaries and structural cues
- The gating mechanism's reliance on visual features may create performance ceilings under severe corruption
- The paper lacks direct ablation studies on corruption types where layout-based restitution fails

## Confidence
- **High confidence**: NRM improves mR@50 by 6.3-11.1% on VG-C across tasks (supported by quantitative results in Tables 1-3)
- **Medium confidence**: LEE's adaptive gating mechanism dynamically balances visual/layout features (supported by gate analysis in Table 4, but lacks per-corruption-type breakdowns)
- **Low confidence**: Layout-aware attention recovers corruption-invariant structural features (mechanism is plausible but lacks direct ablation on corruption-type failures or visualization of attention maps)

## Next Checks
1. **Ablate NRM components**: Test IN-only vs. IN+layout-attention vs. full NRM on VG-C (PredCls mR@50) to isolate restitution contribution
2. **Visualize gate distributions**: Plot $z_i$ histograms across corruption types and severities to verify adaptive behavior matches paper claims
3. **Stress-test with detector perturbations**: Add ±30% random noise to bounding boxes during inference (Table 5 setup) and measure SGDet degradation with/without LEE