---
ver: rpa2
title: Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection
arxiv_id: '2601.12033'
source_url: https://arxiv.org/abs/2601.12033
tags:
- quantization
- safety
- fairness
- score
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how quantization affects fairness and safety
  in large language models (LLMs), finding that quantization generally degrades both
  fairness and safety, with dynamic quantization methods (FP8, LLM.int8) offering
  greater stability than static ones. To address these issues, the authors introduce
  Critical Weight Protection, a technique that identifies and preserves weights critical
  to fairness and safety in higher precision during quantization, while quantizing
  the rest for efficiency.
---

# Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection

## Quick Facts
- **arXiv ID**: 2601.12033
- **Source URL**: https://arxiv.org/abs/2601.12033
- **Reference count**: 40
- **Primary result**: Critical Weight Protection significantly mitigates fairness and safety degradation in quantized LLMs with minimal impact on general utility

## Executive Summary
This study investigates how quantization affects fairness and safety in large language models, finding that quantization generally degrades both fairness and safety, with dynamic quantization methods (FP8, LLM.int8) offering greater stability than static ones. To address these issues, the authors introduce Critical Weight Protection, a technique that identifies and preserves weights critical to fairness and safety in higher precision during quantization, while quantizing the rest for efficiency. Experiments show that this approach significantly mitigates fairness and safety degradation across multiple models (Gemma-7B, Llama-3.1-8B, Qwen-2.5-7B) and benchmarks, maintaining or improving performance compared to full-precision models.

## Method Summary
The Critical Weight Protection (CWP) technique identifies weights critical to fairness and safety by using an evaluation metric that aggregates fairness and safety scores across all prompts in a given dataset. These critical weights are preserved in higher precision during quantization, while the remaining weights are quantized to achieve computational efficiency. The method requires an additional forward pass to identify critical weights, introducing computational overhead. CWP is evaluated across multiple quantization schemes (static and dynamic) and shows significant improvements in preserving fairness and safety metrics compared to standard quantization methods.

## Key Results
- CWP significantly improves fairness and safety metrics across Gemma-7B, Llama-3.1-8B, and Qwen-2.5-7B models compared to standard quantization
- Dynamic quantization methods (FP8, LLM.int8) show greater stability than static methods in preserving fairness and safety
- CWP maintains or improves performance compared to full-precision models while reducing computational overhead through selective quantization

## Why This Works (Mechanism)
CWP works by identifying and protecting weights that are most critical for fairness and safety during quantization. The mechanism relies on the observation that certain weights have disproportionate impact on sensitive attributes and safety-related outputs. By preserving these weights in higher precision while quantizing less critical weights, CWP maintains the model's ability to handle fairness and safety concerns without sacrificing efficiency. The technique effectively creates a hybrid precision model where critical weights remain in FP16 or FP32 while the majority of weights are quantized to lower precision.

## Foundational Learning
**Fairness Metrics**: Quantitative measures of bias across demographic groups; needed to evaluate model performance across different populations. Quick check: Are metrics consistent across different evaluation datasets?

**Safety Benchmarks**: Tests for model resistance to generating harmful content; essential for assessing model robustness. Quick check: Do benchmarks cover diverse types of safety violations?

**Weight Importance Analysis**: Methods for determining which weights most influence specific model behaviors; required for identifying critical weights. Quick check: Does importance correlate with performance degradation when perturbed?

**Quantization Methods**: Techniques for reducing numerical precision; fundamental to understanding efficiency tradeoffs. Quick check: How does precision reduction affect different weight distributions?

**Evaluation Protocols**: Standardized testing procedures; necessary for reproducible results. Quick check: Are protocols consistent across different model architectures?

## Architecture Onboarding

**Component Map**: Input prompts -> Critical Weight Identification -> Selective Quantization -> Model Execution -> Fairness/Safety Evaluation

**Critical Path**: The most computationally intensive step is the additional forward pass required for critical weight identification, which occurs during the quantization preparation phase. This creates a one-time overhead before deployment.

**Design Tradeoffs**: CWP trades increased quantization preparation time for improved fairness and safety preservation. The approach balances computational efficiency (by quantizing most weights) against maintaining critical model capabilities (by preserving key weights in higher precision).

**Failure Signatures**: Potential failures include misidentification of critical weights leading to insufficient protection, or over-protection causing minimal quantization benefits. The method may also struggle with models where fairness and safety behaviors are distributed across many weights rather than concentrated in specific parameters.

**First Experiments**: 1) Verify critical weight identification accuracy by comparing protected vs. unprotected weight performance on fairness/safety tasks; 2) Test CWP across different quantization levels to find optimal precision balance; 3) Evaluate robustness to adversarial attacks specifically targeting CWP-protected models.

## Open Questions the Paper Calls Out
None

## Limitations
- Tested only on 7B-8B parameter models, limiting generalizability to larger frontier models
- Evaluation focused primarily on English and limited non-English languages, leaving questions about multilingual performance
- Computational overhead from critical weight identification step not fully characterized for production deployment
- Does not extensively explore utility-robustness tradeoffs across diverse downstream tasks
- Relies on existing benchmark suites that may not capture all real-world failure modes

## Confidence
- **High confidence**: Quantization's general degradation of fairness and safety metrics across tested models
- **Medium confidence**: CWP's effectiveness at mitigating these degradations based on consistent improvements across multiple benchmarks
- **Medium confidence**: Stability ranking of quantization methods (dynamic > static) based on presented variance analysis
- **Low confidence**: Generalizability to larger models (>8B parameters) and underrepresented languages not tested

## Next Checks
1. Test CWP on larger frontier models (70B+ parameters) to assess scalability and identify potential diminishing returns
2. Conduct adversarial testing specifically designed to probe weaknesses in CWP-protected models, including gradient-based attacks
3. Evaluate the computational overhead of CWP's weight identification step in production deployment scenarios, measuring end-to-end latency impact