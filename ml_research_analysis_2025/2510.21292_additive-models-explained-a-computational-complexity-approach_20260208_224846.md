---
ver: rpa2
title: 'Additive Models Explained: A Computational Complexity Approach'
arxiv_id: '2510.21292'
source_url: https://arxiv.org/abs/2510.21292
tags:
- input
- discrete
- neural
- complexity
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the computational complexity of generating
  various types of explanations for generalized additive models (GAMs). The study
  systematically analyzes the complexity across three key dimensions: the type of
  component models used (splines, neural networks, or boosted trees), the type of
  explanation sought (sufficient reasons, contrastive explanations, Shapley values,
  and feature redundancy), and the input domain structure (enumerable discrete, general
  discrete, or continuous).'
---

# Additive Models Explained: A Computational Complexity Approach

## Quick Facts
- **arXiv ID**: 2510.21292
- **Source URL**: https://arxiv.org/abs/2510.21292
- **Reference count**: 40
- **Primary result**: Computational complexity of explanations for GAMs depends critically on input domain, component model, and explanation type.

## Executive Summary
This paper systematically investigates the computational complexity of generating various explanations (sufficient reasons, contrastive explanations, Shapley values, feature redundancy) for Generalized Additive Models (GAMs). The authors analyze three key dimensions: the type of component models used (splines, neural networks, boosted trees), the type of explanation sought, and the structure of the input domain (enumerable discrete, general discrete, or continuous). They reveal that unlike other common ML models, the complexity of computing explanations for GAMs is heavily influenced by the input domain structure, with enumerable discrete domains offering significant tractability advantages. The study establishes both tractable (polynomial-time) and intractable (NP-hard, #P-hard) cases, showing that additive representations can sometimes simplify explanation computation but this benefit is highly dependent on the explanation type and input domain.

## Method Summary
The paper employs complexity-theoretic analysis, establishing reductions from known hard problems to prove intractability, and constructing explicit algorithms for tractable cases. The authors define three types of GAMs (Smooth GAMs with splines, Neural Additive Models/NAMs, and Explainable Boosting Machines/EBMs) and analyze them across three input domains. For tractable cases, they provide greedy and dynamic programming algorithms; for intractable cases, they use reductions from problems like 3-SAT and neural network verification. The analysis covers four explanation types: Minimal Sufficient Reasons (MSR), Minimal Contrastive Reasons (MCR), Shapley values, and feature redundancy.

## Key Results
- Computing MSR and MCR is in PTIME for enumerable discrete domains regardless of component model type
- For continuous domains, MSR and MCR are NP-hard for NAMs and EBMs but remain tractable for Smooth GAMs
- Computing exact Shapley values is #P-hard for classification tasks but tractable (PTIME) for regression tasks
- Feature redundancy is tractable (PTIME) for all GAM types and domains

## Why This Works (Mechanism)

### Mechanism 1: Enumerable Discrete Domain Enumeration
The tractable computation for enumerable discrete domains relies on explicitly iterating through all possible input values (constant-sized set K for each feature) to find component function extrema. This enables greedy selection of features based on their impact on the decision boundary. The algorithm exploits the small, finite nature of the domain to make the search space manageable.

### Mechanism 2: Analytic Optimization of Smooth Components
Smooth GAMs using piecewise polynomials (cubic splines) allow for efficient global optimization. Finding min/max values requires only solving polynomial roots (degree 3) or checking interval endpoints, which is computationally cheap. This contrasts sharply with neural networks and tree ensembles where global optimization becomes intractable.

### Mechanism 3: Regression Linearity vs. Classification Step-Function
The tractability difference for Shapley values stems from the linearity of regression outputs versus the non-linear step function in classification. Regression allows direct computation via feature contribution expectations, while classification reduces to a #SAT (model counting) problem, making it #P-hard.

## Foundational Learning

- **Concept: Generalized Additive Models (GAMs)**
  - *Why needed here:* Understanding that GAMs decompose predictions into independent feature functions is crucial for grasping why complexity varies by component type
  - *Quick check question:* Can you explain why isolating feature interactions into 1D components f_i(x_i) changes the search space compared to a fully connected neural network?

- **Concept: Computational Complexity (NP-Hard vs. #P-Hard)**
  - *Why needed here:* The paper maps explanation problems to complexity classes; distinguishing between finding solutions (NP-Hard) versus counting solutions (#P-Hard) is essential
  - *Quick check question:* If a problem is #P-Hard, why is exact computation generally considered infeasible for large models, even if verifying a single solution is easy?

- **Concept: Global vs. Local Optimization**
  - *Why needed here:* Tractability often hinges on computing global extrema of component functions; inability to do so efficiently breaks polynomial-time guarantees
  - *Quick check question:* Why is finding the global maximum of a cubic spline (continuous) easier than finding the global maximum of a boosted tree ensemble (continuous)?

## Architecture Onboarding

- **Component map:** Input Domain Type -> Model Layer (Smooth GAM / NAM / EBM) -> Query Layer (MSR, MCR, SHAP, CC) -> Complexity Engine (PTIME / NP-Hard / #P-Hard)

- **Critical path:**
  1. Identify Domain & Component: Check if input is enumerable and if components are smooth
  2. Select Algorithm:
     - Enumerable/Smooth: Use Greedy Selection (sort by penalty, iterate)
     - Continuous/NAMs: Use Approximation/Solvers (exact is NP-Hard)
  3. Task Check: If computing SHAP, check if task is regression (use linear expectation) or classification (use sampling/approximation)

- **Design tradeoffs:**
  - Expressivity vs. Tractability: NAMs model complex data better than Splines but lose PTIME guarantees for continuous domains
  - Precision vs. Speed: Quantization can turn intractable problems into pseudo-polynomial ones, trading precision for tractability

- **Failure signatures:**
  - Infinite Loop/Hang: Attempting exact SHAP for classification on continuous NAM (#P-Hard)
  - Incorrect Explanations: Using local optimizers for NAM components in continuous domains, missing global extremes
  - Misclassification: Applying SHAP tractability logic to classification tasks instead of regression

- **First 3 experiments:**
  1. Validate PTIME Baseline: Implement penalty calculation for Smooth GAM on continuous dataset, measure scaling against feature count n
  2. Validate Input Domain Sensitivity: Compute MSR for NAM on enumerable discrete vs. continuous data, observe complexity cliff
  3. SHAP Task Comparison: Compute exact SHAP for regression GAM (should be fast) vs. classification GAM (should require sampling)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do high-order feature interactions affect the computational complexity of generating explanations across explanation types and component models?
- **Basis in paper:** Section 9 explicitly states this as a promising avenue
- **Why unresolved:** Current analysis is restricted to strictly additive GAMs without interaction terms
- **What evidence would resolve it:** Systematic complexity analysis for GAMs with pairwise and higher-order interactions across all dimensions

### Open Question 2
- **Question:** How does concurvity influence the computational complexity of generating explanations?
- **Basis in paper:** Section 9 mentions studying the effect of concurvity as an interesting direction
- **Why unresolved:** Current analysis assumes independent component functions without structural dependencies
- **What evidence would resolve it:** Complexity results for GAMs under varying concurvity levels

### Open Question 3
- **Question:** Can approximation guarantees or probabilistic frameworks circumvent intractability in computing explanations?
- **Basis in paper:** Section 9 highlights approximation guarantees, probabilistic relaxations, and PAC-based guarantees as important future directions
- **Why unresolved:** Paper only identifies tractable/intractable cases without analyzing approximation or probabilistic settings
- **What evidence would resolve it:** Algorithms with provable approximation ratios for NP-hard or #P-hard explanation queries

## Limitations

- The theoretical nature of proofs lacks empirical runtime benchmarks to validate practical impact of complexity differences
- Hardness results assume worst-case scenarios, and actual performance may vary significantly based on dataset characteristics
- Analysis focuses on exact computation, while in practice approximation methods are often used for intractable problems

## Confidence

- **High Confidence:** Complexity classification for enumerable discrete domains and smooth GAMs (standard complexity theory techniques with clearly stated assumptions)
- **Medium Confidence:** Claims about NAMs and EBMs (logical reductions but critical dependence on specific GAM representation details)
- **Low Confidence:** Practical significance of #P-hardness for SHAP classification without empirical validation (theoretical barrier clear but runtime feasibility uncertain)

## Next Checks

1. **Empirical Runtime Validation:** Implement greedy algorithms for enumerable discrete domains and measure actual runtime scaling against feature count n on synthetic datasets

2. **Approximation Feasibility Study:** For #P-hard SHAP classification case, implement sampling-based approximation and evaluate accuracy/runtime for various model sizes

3. **Component Structure Sensitivity Analysis:** Systematically vary knots in splines and architecture of NAMs, then measure impact on complexity of finding global minima/maxima for component functions