---
ver: rpa2
title: 'Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion
  and Document Boosting'
arxiv_id: '2509.17440'
source_url: https://arxiv.org/abs/2509.17440
tags:
- retrieval
- longitudinal
- datasets
- experiments
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating information retrieval
  systems over time, as traditional Cranfield-style evaluations miss temporal dynamics.
  The authors propose extending the irdatasets framework with methods tailored to
  dynamic test collections, allowing for declarative descriptions of longitudinal
  experiments.
---

# Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion and Document Boosting

## Quick Facts
- arXiv ID: 2509.17440
- Source URL: https://arxiv.org/abs/2509.17440
- Reference count: 37
- Authors: Jüri Keller; Maik Fröbe; Gijs Hendriksen; Daria Alexander; Martin Potthast; Philipp Schaer
- Key outcome: ir_datasets extension reduces longitudinal experiment code complexity and improves retrieval effectiveness through temporal query expansion and document boosting

## Executive Summary
This paper addresses the challenge of evaluating information retrieval systems over time, as traditional Cranfield-style evaluations miss temporal dynamics. The authors propose extending the ir_datasets framework with methods tailored to dynamic test collections, allowing for declarative descriptions of longitudinal experiments. They reimplement two query expansion and document boosting approaches from LongEval 2024 as PyTerrier transformers using their extension. The results show a decrease in code complexity and improved retrieval effectiveness, though direct replication of previous results was difficult due to dataset changes. Their extension simplifies longitudinal experiments and promotes software submissions by reducing maintenance efforts.

## Method Summary
The authors extend ir_datasets with a longeval sub-package that provides meta-datasets for dynamic test collections, supporting declarative specification of temporal data access patterns. They reimplement two approaches from LongEval 2024: Qrel Boosting (which multiplies base retrieval scores by weights derived from historical relevance judgments) and Relevance Feedback (which expands queries using tf-idf terms from previously relevant documents). Both are implemented as PyTerrier transformers and evaluated on LongEval Sci and Web datasets using nDCG@10. The implementation reduces code complexity significantly while demonstrating effectiveness gains, though dataset version changes prevent exact replication of prior results.

## Key Results
- Code complexity reduced: NLOC dropped from 250 to 99 for qrel_boost and from 231 to 197 for relevance_feedback after re-implementation
- Both approaches achieved consistent improvements over BM25 across multiple LongEval datasets
- Direct replication of 2023 results failed due to dataset version changes (2024/2025 use unified document IDs vs previous versions)
- Effect Ratio consistently exceeded 1.0 (1.57-3.69 range), indicating re-implementations produced larger improvements than originals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Declarative dataset interfaces reduce code complexity in longitudinal experiments by abstracting temporal data access patterns.
- **Mechanism:** The ir_datasets extension provides meta-datasets that automatically handle snapshot ordering, timestamp access, and prior-dataset traversal. Instead of researchers writing custom logic to track which queries, documents, and qrels exist at each timepoint, the framework exposes `get_prior_datasets()` and `get_timestamp()` methods that return ordered, properly scoped data.
- **Core assumption:** The complexity reduction translates to maintainability gains without hiding critical temporal constraints that researchers need to control.
- **Evidence anchors:**
  - [abstract] "This extension allows for declaratively, instead of imperatively, describing important aspects of longitudinal retrieval experiments, e.g., which queries, documents, and/or relevance feedback are available at which point in time."
  - [Section 5.1, Table 1] Total NLOC dropped from 250 to 99 for qrel_boost and from 231 to 197 for relevance_feedback after re-implementation.
  - [corpus] Related work on query expansion and temporal evaluation does not systematically address framework-level abstraction for longitudinal settings; most assume custom pipelines per experiment.
- **Break condition:** If your longitudinal experiment requires fine-grained control over exactly which prior snapshots influence current rankings (beyond simple memory windows), the declarative interface may hide too much and require workarounds.

### Mechanism 2
- **Claim:** Qrel Boosting leverages historical relevance judgments as pseudo-supervision to improve current retrieval by multiplying base scores with judgment-derived weights.
- **Mechanism:** For each query-document pair at time t_n, the approach retrieves its relevance label from prior snapshot t_{n-1}. Documents judged relevant (rel=1) receive a λ² multiplier; highly relevant (rel=2) receive λ²μ; non-relevant (rel=0) receive (1-λ)². This boosts previously-judged-relevant documents while demoting previously-judged-non-relevant ones.
- **Core assumption:** Relevance signals persist across snapshots—documents relevant at t_{n-1} remain relevant at t_n, and document identity or content changes are minor enough that prior judgments transfer.
- **Evidence anchors:**
  - [Section 4.1, Equation 1] Defines the boosting formula with λ and μ as free parameters controlling weighting intensity.
  - [Section 6] "This is most likely due to the updated test beds and the improvements made to the approaches"—suggesting the mechanism is sensitive to how document identity and content changes are handled.
  - [corpus] Query expansion literature (e.g., TCDE, Exp4Fuse) similarly assumes term-level or document-level signals transfer across retrieval contexts, but temporal persistence is rarely explicitly validated.
- **Break condition:** When document content changes substantially between snapshots (e.g., news articles updated with corrections, Wikipedia pages heavily edited), prior relevance labels may become misleading or counterproductive.

### Mechanism 3
- **Claim:** Temporal relevance feedback expands current queries using tf-idf terms extracted from documents judged relevant in prior snapshots.
- **Mechanism:** For a query at time t_n, the approach retrieves documents judged relevant at t_{n-1}, extracts the top-k terms by tf-idf from those documents (using the prior snapshot's index), and appends them to the current query before retrieval.
- **Core assumption:** The vocabulary and term distributions in prior relevant documents remain predictive of relevance in the current snapshot; term importance (as captured by tf-idf) transfers across time.
- **Evidence anchors:**
  - [Section 4.2] "Each query is expanded with the top k terms with the highest tf-idf scores... the re-implementation relies directly on the tf-idf scores from the index of the previous snapshot since this is a theoretically better indicator for relevant terms."
  - [Section 5.2, Table 2] Effect Ratio consistently exceeds 1.0 (ranging 1.57–3.69) and Delta Relative Improvement is mostly negative, indicating re-implementations produced larger improvements over BM25 than originals—but this may reflect dataset changes rather than mechanism efficacy.
  - [corpus] Weak direct evidence; corpus neighbors focus on LLM-based expansion rather than temporal feedback, suggesting this specific mechanism is underexplored.
- **Break condition:** When topic drift occurs—information needs evolve such that prior relevant documents discuss different aspects than currently relevant ones—the expanded terms may introduce noise rather than signal.

## Foundational Learning

- **Concept:** Cranfield-style evaluation paradigm
  - **Why needed here:** The paper positions its contribution against static test collections. Understanding that traditional IR evaluation assumes fixed documents, queries, and relevance judgments helps you grasp why longitudinal evaluation introduces new complexity.
  - **Quick check question:** Can you explain why adding a time dimension invalidates the assumption that one retrieval run suffices to characterize system effectiveness?

- **Concept:** Test collection components (documents, queries, qrels) and their temporal evolution
  - **Why needed here:** The ir_datasets extension manages versioned instances of each component. You need to understand what each component represents to reason about which prior data is safe to use at each timepoint.
  - **Quick check question:** If you have qrels from snapshot t_{n-1}, is it valid to use them to evaluate retrieval at t_n? Why or why not?

- **Concept:** Cyclomatic complexity and code maintainability metrics
  - **Why needed here:** The paper quantifies its contribution partly through reduced NLOC and cyclomatic complexity. Understanding these metrics helps you interpret whether the simplification is meaningful.
  - **Quick check question:** What does a cyclomatic complexity of 2.6 vs 2.9 tell you about the practical maintainability difference between two codebases?

## Architecture Onboarding

- **Component map:** ir_datasets_longeval extension -> Meta-dataset object -> Individual snapshot datasets -> PyTerrier transformers (QrelBoost, RF) -> Pre-built indices for prior snapshots

- **Critical path:**
  1. Install ir_datasets_longeval extension and register LongEval datasets
  2. Load meta-dataset (e.g., `load("longeval-sci/*")`) to access all snapshots
  3. For each current snapshot, call `get_prior_datasets()` to retrieve historical data
  4. Build or load indices for prior snapshots (required for RF; optional for QrelBoost)
  5. Construct PyTerrier pipeline integrating transformers: `bm25 >> QrelBoost(dataset, memory=1)` or `RF(dataset, indices, memory=1) >> bm25`
  6. Execute retrieval and evaluate per-snapshot, aggregating results longitudinally

- **Design tradeoffs:**
  - **Memory window (λ parameter):** Controls how many prior snapshots influence current retrieval. Lower values reduce computational cost but may miss cumulative relevance signals. Paper uses memory=1 in examples.
  - **Declarative vs. imperative access:** Declarative interface simplifies code but may constrain custom temporal logic. If you need fine-grained control (e.g., weighting recent qrels more heavily than older ones), you may need to bypass the abstraction.
  - **Document identity handling:** The 2024 dataset version assigned new IDs for updated websites; the current version unifies IDs. This affects whether prior relevance judgments can be matched to current documents. Assumption: Your approach must align with whichever ID scheme your dataset version uses.
  - **Index storage:** Each snapshot requires a separate index. Full snapshots are stored rather than deltas, increasing disk requirements but simplifying access.

- **Failure signatures:**
  - **Low replication fidelity (ER >> 1, negative ΔRI):** Indicates your re-implementation produces different results than expected. Check dataset version alignment and document ID schemes first.
  - **QrelBoost has no effect:** Likely `get_prior_datasets()` returns empty (no prior snapshots available for current timestamp) or document IDs don't match across snapshots.
  - **RF pipeline crashes on term extraction:** Prior index path is incorrect, or prior snapshot lacks the required index structure.
  - **Effectiveness degrades over time:** Topic drift or document evolution may violate the persistence assumption; consider reducing memory window or adding decay factors.

- **First 3 experiments:**
  1. **Baseline establishment:** Run standard BM25 on all snapshots without temporal features. Record per-snapshot nDCG@10 to establish a reference. This isolates the temporal dynamics present in the data itself.
  2. **QrelBoost sensitivity analysis:** Vary the memory parameter (1, 2, 3 prior snapshots) and λ (0.1, 0.3, 0.5) on a single dataset. Observe whether effectiveness improves and whether gains are consistent across snapshots or concentrated in early/late periods.
  3. **Cross-dataset transfer test:** Train λ and μ on LongEval Sci snapshots, then apply to LongEval Web without retraining. This tests whether optimal parameters transfer across domains or require per-dataset tuning.

## Open Questions the Paper Calls Out
None

## Limitations
- Critical hyperparameters (λ, μ for QrelBoost; k for RF) remain unspecified, limiting exact replication despite code availability
- Document identity handling across snapshots remains problematic - effectiveness gains may stem partly from ID scheme changes rather than algorithmic improvements
- The declarative interface may obscure temporal constraints needed for complex longitudinal scenarios

## Confidence
- **High:** Framework extension meaningfully reduces code complexity and supports longitudinal experiment declaration
- **Medium:** QrelBoost and RF mechanisms work as described in principle, but replication fidelity is dataset-version dependent
- **Low:** Cross-dataset parameter transferability (e.g., Sci to Web) requires further validation

## Next Checks
1. **Hyperparameter sensitivity analysis:** Systematically vary λ (0.1, 0.3, 0.5) and μ (1.0, 2.0, 3.0) to determine optimal values and stability across snapshots
2. **Temporal decay experiment:** Implement exponentially decaying weights for prior judgments to test whether recent relevance signals matter more than older ones
3. **Cross-dataset parameter transfer:** Train parameters on LongEval Sci, apply without modification to LongEval Web, and measure degradation to assess domain-specific tuning needs