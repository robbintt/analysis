---
ver: rpa2
title: 'Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts
  for Efficient Large Language Model Pre-Training'
arxiv_id: '2510.08008'
source_url: https://arxiv.org/abs/2510.08008
tags:
- growth
- training
- average
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes a method to efficiently scale large language
  models by recycling pretrained checkpoints. It introduces two orthogonal growth
  strategies for MoE models: depth growth via interpositional layer copying and width
  growth via expert duplication with noise injection.'
---

# Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training

## Quick Facts
- **arXiv ID**: 2510.08008
- **Source URL**: https://arxiv.org/abs/2510.08008
- **Reference count**: 40
- **Primary result**: Scaling 17B→70B MoE parameters via recycled checkpoints yields 10.66% accuracy gain over training from scratch under same compute budget.

## Executive Summary
This work proposes efficient scaling of large language models by recycling pretrained checkpoints through orthogonal growth strategies for Mixture-of-Experts architectures. The method introduces interpositional layer copying for depth growth and expert duplication with noise injection for width growth. Experiments demonstrate that interposition outperforms stacking for converged models, noise injection improves expert specialization, and final performance strongly correlates with the amount of prior training (sunk FLOPs) in the base checkpoint.

## Method Summary
The approach recycles pretrained MoE checkpoints through two orthogonal growth operations: depth growth via interpositional layer copying and width growth via expert duplication with noise injection. For depth growth, each layer is duplicated in-place (l1,l1,l2,l2,...,ln,ln) rather than stacked, preserving characteristic weight norm patterns in converged models. For width growth, experts and top-k are doubled, with Gaussian noise (α=0.01×σ_orig) injected into new expert weights and router logits to promote specialization. The method is validated across 3B→6B and 17B→70B parameter scales, showing performance gains over training from scratch under fixed compute budgets.

## Key Results
- Interpositional layer copying outperforms stacking for depth growth in converged models, preserving weight norm patterns and improving downstream accuracy.
- Noise injection (α=0.01) improves expert specialization and yields ~1% accuracy gain during width growth compared to no-noise baselines.
- Final model accuracy shows strong positive correlation with sunk FLOPs in the base checkpoint, with 96K-step checkpoint achieving 47.82 average accuracy versus 36.29 for scratch.
- Scaling from 17B to 70B parameters achieves 10.66% accuracy gain over training from scratch under the same compute budget.

## Why This Works (Mechanism)

### Mechanism 1
Interpositional layer copying preserves learned structural properties better than stacking for converged models. Converged LLMs exhibit characteristic layer-wise weight norm patterns (low→increasing→slightly decreasing). Interposition (l1,l1,l2,l2,...,ln,ln) maintains this positional trend, whereas stacking (l1,...,ln,l1,...,ln) resets the norm progression mid-model, disrupting learned layer specialization. Break condition: If base model is under-trained (layers haven't specialized), stacking may be equally effective as interposition.

### Mechanism 2
Injecting small Gaussian noise into duplicated experts promotes specialization without destabilizing inherited knowledge. When doubling experts and top-k for width growth, direct copying creates redundant capacity. Adding noise (α=0.01 × σ_orig) to new expert weights and router logits breaks symmetry, encouraging divergence during continued training. Break condition: Excessive noise (α≥0.1 in experiments) harms performance; no noise reduces specialization benefits.

### Mechanism 3
Final grown model performance positively correlates with sunk FLOPs in the base checkpoint. More prior training produces better representations that serve as superior initialization for the larger model. Growth operations preserve enough of this knowledge to transfer the computational investment. Break condition: Checkpoints from late annealing phase show diminishing marginal gains (section 4.1 notes this caveat).

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Routing**
  - Why needed here: Width growth doubles both total experts and activated top-k; understanding router mechanics, load balancing, and specialization is essential.
  - Quick check question: Given an MoE layer with E experts and top-k routing, what happens to the output distribution if you double E and k without modifying router weights?

- **Concept: Function-Preserving Transformations**
  - Why needed here: Width growth approximates function preservation (output unchanged immediately after growth); depth growth in pre-norm architectures is partially function-preserving.
  - Quick check question: Why does pre-norm architecture better preserve function after layer duplication compared to post-norm?

- **Concept: Scaling Laws and Compute Budgets**
  - Why needed here: The paper frames growth as sunk cost recovery; understanding FLOPs accounting and compute-performance relationships provides economic context.
  - Quick check question: Under fixed total FLOPs, when would training from scratch outperform growing from a converged checkpoint?

## Architecture Onboarding

- **Component map**: Base MoE model (decoder-only with Grouped Query Attention, RMSNorm, RoPE; 20-28 layers; 64-192 experts with 4-6 activated) -> Growth operators (Interposition module for depth, Expert duplication + noise module for width) -> Continued training pipeline

- **Critical path**:
  1. Select checkpoint from constant LR phase (not late annealing)
  2. Apply depth growth via interposition if deeper architecture needed
  3. Apply width growth: duplicate experts/router, inject noise (α=0.01), double top-k
  4. Continue training with additional FLOPs ≥ order of magnitude of sunk cost

- **Design tradeoffs**:
  - Depth vs Width: Depth yields better downstream performance faster; width is more stable and function-preserving
  - Timing: Earlier checkpoint → more room for post-growth training; later checkpoint → higher sunk cost transfer but risk of diminishing returns
  - Noise scale: α=0.01 optimal in experiments; 0 loses specialization; ≥0.1 destabilizes

- **Failure signatures**:
  - Accuracy collapse after depth growth: Using stacking instead of interposition, or post-norm without mitigation
  - Experts don't specialize after width growth: Missing or insufficient noise injection
  - Grown model underperforms scratch: Insufficient additional training budget (need FLOPs comparable to sunk cost)
  - Diminishing returns: Checkpoint from annealing phase; requires LR retuning

- **First 3 experiments**:
  1. Replicate interposition vs stacking at 3B→6B scale with 20-30k base training steps, measure loss and downstream accuracy gap.
  2. Sweep noise injection scales (α∈{0, 0.005, 0.01, 0.05, 0.1}) for width growth; track expert load distribution and downstream accuracy.
  3. Correlate sunk FLOPs with final accuracy: Save 6-8 checkpoints across training, grow each with fixed additional budget, verify positive correlation.

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- Critical experimental details underspecified: exact data mixing ratios, load-balancing loss weights, and learning rate scheduling for continued training post-growth.
- Noise injection mechanism lacks theoretical grounding for why α=0.01 works optimally.
- Performance on larger scale models beyond 70B parameters not detailed.
- Positive correlation between sunk FLOPs and accuracy may not hold for overtrained checkpoints (diminishing returns acknowledged).

## Confidence
**High Confidence**: Interpositional depth growth outperforms stacking for converged models; noise injection (α=0.01) improves expert specialization; positive correlation between sunk FLOPs and final accuracy; width growth is more stable and function-preserving than depth growth.

**Medium Confidence**: 10.66% accuracy gain at 70B scale versus scratch training; depth growth yields better downstream performance faster than width growth; converged models exhibit characteristic layer-wise weight norm patterns.

**Low Confidence**: Exact scaling relationship between noise injection scale and specialization; performance on larger scale models beyond 70B; benefits for non-MoE architectures.

## Next Checks
1. **Layer Norm Pattern Verification**: Train converged 3B MoE model, measure layer-wise weight norms, apply both interposition and stacking growth, verify only interposition preserves smooth norm progression while stacking creates "sawtooth" disruption.

2. **Noise Injection Sensitivity**: Systematically sweep noise injection scales (α∈{0.005, 0.01, 0.02, 0.05}) during width growth, tracking expert load balance distribution and downstream accuracy to map relationship between noise magnitude and specialization benefits.

3. **Overtrained Checkpoint Test**: Save checkpoints from different training phases (early constant LR, late constant LR, early annealing, late annealing), grow each with identical additional FLOPs budget, measure whether positive correlation between sunk FLOPs and accuracy breaks down for overtrained checkpoints.