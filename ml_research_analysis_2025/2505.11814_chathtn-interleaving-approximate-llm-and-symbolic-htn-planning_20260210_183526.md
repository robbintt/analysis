---
ver: rpa2
title: 'ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning'
arxiv_id: '2505.11814'
source_url: https://arxiv.org/abs/2505.11814
tags:
- task
- tasks
- chathtn
- planning
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatHTN combines symbolic HTN planning with LLM approximations
  to create sound hierarchical plans. It maintains HTN planner capabilities while
  using ChatGPT to generate task decompositions when the knowledge base lacks required
  methods.
---

# ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning

## Quick Facts
- **arXiv ID:** 2505.11814
- **Source URL:** https://arxiv.org/abs/2505.11814
- **Reference count:** 6
- **Key outcome:** ChatHTN combines symbolic HTN planning with LLM approximations to create sound hierarchical plans, achieving 83% success rate across three domains while catching invalid plans through verifier tasks.

## Executive Summary
ChatHTN addresses incomplete HTN knowledge bases by combining symbolic planning with ChatGPT approximations. When no method can decompose a compound task, ChatHTN queries ChatGPT to generate plausible decompositions, then validates them using verifier tasks that ensure the resulting plan achieves the original task's effects. This approach maintains soundness guarantees while reducing manual knowledge engineering requirements. The system was implemented using PyHop and tested on logistics, household robot, and search-and-rescue domains, successfully generating correct plans in 83% of test cases.

## Method Summary
ChatHTN extends standard HTN planning by adding task semantics (preconditions/effects pairs) and verifier tasks. When symbolic methods cannot decompose a compound task, the system queries ChatGPT with two-step prompt chaining: first requesting decomposition given task semantics and available operators, then mapping to primitive task predicates. Each decomposition appends a verifier task—a primitive task with preconditions equal to the original task's effects—to validate the decomposition. The system tracks visited (state, task) pairs to prevent infinite loops and returns a plan only if all verifier tasks pass, ensuring soundness.

## Key Results
- Achieved 83% success rate across logistics, household robot, and search-and-rescue domains
- Verifier tasks successfully caught invalid LLM-generated decompositions
- Incurred approximately $30 per domain in API costs
- Generated correct plans even when knowledge base lacked required methods

## Why This Works (Mechanism)

### Mechanism 1: Verifier Tasks for Soundness Guarantee
- Claim: Verifier tasks ensure any plan ChatHTN generates correctly achieves the input tasks, even when LLM-generated decompositions are incorrect.
- Mechanism: When ChatHTN decomposes any compound task (via methods or ChatGPT), it appends a "verifier task" to the task list. A verifier task is a primitive task whose action has no effects but has as preconditions the original task's effects. If the decomposition fails to achieve the required effects, the verifier's preconditions fail at line 9, causing backtracking.
- Core assumption: Tasks have well-defined semantics (preconditions/effects pairs) that can be mechanically verified against world state.
- Evidence anchors: [abstract]: "ChatHTN is provably sound; any plan it generates correctly achieves the input tasks"; [section 5]: "A verifier task tver_0 for a compound task t0 is a primitive task whose associated action has no effects and has as preconditions the effects of t0"

### Mechanism 2: Approximate-to-Symbolic Handoff
- Claim: ChatGPT generates plausible task decompositions when the knowledge base lacks required methods, which symbolic verification then validates.
- Mechanism: When no method m0 can decompose compound task t0 in state s, ChatHTN queries ChatGPT via prompt chaining: (1) request decomposition given task semantics, state, and available operators; (2) request mapping to primitive task predicates. The resulting task sequence becomes part of the HTN hierarchy and undergoes verification.
- Core assumption: ChatGPT has sufficient domain knowledge from training data to generate relevant decompositions for common planning domains.
- Evidence anchors: [abstract]: "uses ChatGPT to generate task decompositions when the knowledge base lacks required methods"; [section 4, lines 24-28]: Shows ChatGPTQuery procedure with two-step prompt chaining

### Mechanism 3: Hierarchical Interleaving Strategy
- Claim: Prioritizing symbolic decomposition over LLM queries preserves efficiency and reliability while handling knowledge gaps gracefully.
- Mechanism: ChatHTN always attempts symbolic methods first. Only when all methods fail does it fall back to ChatGPT. After LLM decomposition, control returns to symbolic HTN planning. This localizes LLM use to gaps rather than replacing symbolic planning entirely.
- Core assumption: The knowledge base is partially complete—methods exist for most situations but not all edge cases.
- Evidence anchors: [abstract]: "resulting hierarchies interleave task decompositions generated by symbolic HTN planning with those generated by ChatGPT"; [section 4]: Algorithm structure shows method-first, LLM-fallback ordering

## Foundational Learning

- Concept: **HTN Planning (Compound vs. Primitive Tasks, Methods)**
  - Why needed here: ChatHTN extends standard HTN; understanding task decomposition hierarchies is prerequisite.
  - Quick check question: Given a compound task "deliver package," what would a method look like that decomposes it into primitive subtasks?

- Concept: **Task Semantics (Preconditions/Effects for Compound Tasks)**
  - Why needed here: Unlike standard HTN where compound tasks lack semantics, ChatHTN requires (preconditions, effects) pairs to enable verification.
  - Quick check question: Why does ChatHTN assign semantics to compound tasks when standard HTN planning does not?

- Concept: **Soundness vs. Completeness in Planners**
  - Why needed here: ChatHTN is provably sound but not complete; understanding this distinction sets correct deployment expectations.
  - Quick check question: If ChatHTN returns a plan, what guarantee do you have? What might it fail to do?

## Architecture Onboarding

- Component map:
  PyHop HTN Planner -> Knowledge Base (Methods, Operators, Task Definitions) -> ChatGPT Query Module -> Verifier Task Generator -> Loop Detection

- Critical path:
  1. Input planning problem (initial state s, task list t̃)
  2. chatSeekPlan loop: primitive task → execute action; compound task → try methods → if none applicable, query ChatGPT
  3. Each decomposition appends verifier task to task list
  4. Verifier executes after subtasks; if preconditions (original task effects) fail → backtrack
  5. Return plan π or ∅

- Design tradeoffs:
  - **Soundness over completeness**: Guaranteed correct plans, but may miss valid solutions if ChatGPT fails repeatedly
  - **Cost vs. knowledge engineering**: ~$30/domain in API costs trades off against manual method authoring effort
  - **Prompt chaining**: Two queries improve accuracy but double latency/cost
  - **Temperature=1.0**: Enables exploration but causes non-determinism (paper gave 5 retries per problem)

- Failure signatures:
  - **Infinite loops**: ChatGPT may suggest cyclic decompositions → mitigated by (state, task) pair tracking
  - **Non-deterministic success**: Same problem may fail then succeed → solution: retry mechanism
  - **High query count with no solution**: Indicates ChatGPT decompositions repeatedly fail verification
  - **Unsolvable problems still trigger queries**: ChatGPT generates decompositions even for impossible problems; verifiers catch them

- First 3 experiments:
  1. **Baseline test**: Run with full domain knowledge (all methods present) → confirm 0 ChatGPT calls, successful planning
  2. **Ablation by method removal**: Remove one method at a time → verify ChatHTN compensates via ChatGPT while maintaining soundness
  3. **Unsolvable problem injection**: Remove required precondition → verify ChatHTN returns ∅ despite ChatGPT generating invalid decompositions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ChatHTN scale to domains requiring task hierarchies with dozens of levels while maintaining soundness?
- Basis in paper: [explicit] "We would like to expand and test ChatGPT on more complex tasks, including some that may require a hierarchy with dozens of levels."
- Why unresolved: Current experiments used only 3 domains with shallow hierarchies; deeper hierarchies may compound LLM approximation errors or increase verifier task failures.
- What evidence would resolve it: Empirical results showing success rates and LLM query counts on benchmark domains with known hierarchical depths of 10+ levels.

### Open Question 2
- Question: Can ChatGPTQuery-generated decompositions be learned as reusable HTN methods to reduce repeated API calls?
- Basis in paper: [explicit] "In some of the problems CHATGPTQUERY is queried repeatedly on the same decomposition. There is an opportunity to learn these decompositions as HTN methods."
- Why unresolved: The paper observes repeated queries but does not implement or evaluate any learning mechanism.
- What evidence would resolve it: A learning component that extracts methods from LLM decompositions, with experiments showing reduced query counts over successive planning episodes.

### Open Question 3
- Question: Does allowing ChatGPTQuery to return compound tasks (not just primitive tasks) improve success rates in complex domains?
- Basis in paper: [explicit] "One possible way to tackle these more complex scenarios is to relax the requirement that CHATGPTQUERY returns a sequence of primitive tasks. Instead, we plan to enable CHATGPTQUERY to generate a sequence of primitive and compound tasks."
- Why unresolved: Current implementation restricts LLM output to primitive tasks, potentially limiting expressiveness when the LLM has relevant hierarchical knowledge.
- What evidence would resolve it: Comparative experiments on complex domains showing success rates with primitive-only versus hybrid decompositions, plus analysis of verifier task pass rates.

### Open Question 4
- Question: How does ChatHTN perform on real-world domains not derived from planning literature?
- Basis in paper: [explicit] "We would also like to test ChatGPT in domains that are not from the planning literature yet are still amenable to hierarchical decomposition."
- Why unresolved: All three tested domains (logistics, household robot, search-and-rescue) are standard planning benchmarks with well-defined structures.
- What evidence would resolve it: Results on novel domains with less formal specification, such as procedural tasks in software workflows or real-world robotic applications.

## Limitations

- **Domain specification gaps**: Table 1 shows partial specifications for operators, methods, and axioms; exact definitions needed for faithful reproduction
- **Test problem uncertainty**: "Prototypical problem" mentioned but specific initial states and goal conditions not provided
- **Cost variability**: $30/domain estimate assumes fixed API parameters and may vary significantly with domain complexity

## Confidence

- **High Confidence**: Soundness guarantee mechanism via verifier tasks is well-defined and mechanically verifiable; 83% success rate across domains is explicitly stated
- **Medium Confidence**: ChatGPT integration details (exact prompts, API parameters beyond temperature=1.0) are described but not fully specified in paper
- **Low Confidence**: Cost estimates per domain are approximate and may not reflect varying domain complexities or API pricing changes

## Next Checks

1. **Mechanistic verification**: Implement verifier task logic and test with deliberately incorrect LLM decompositions to confirm soundness guarantees
2. **Ablation study**: Systematically remove individual methods from each domain and verify ChatHTN generates equivalent plans via ChatGPT while maintaining 83% success rate
3. **Cost scaling analysis**: Measure actual API costs across all domains with exact problem instances to validate ~$30/domain estimate and identify cost drivers