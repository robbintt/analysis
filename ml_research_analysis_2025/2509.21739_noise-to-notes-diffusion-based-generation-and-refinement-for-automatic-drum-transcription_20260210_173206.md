---
ver: rpa2
title: 'Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum
  Transcription'
arxiv_id: '2509.21739'
source_url: https://arxiv.org/abs/2509.21739
tags:
- drum
- features
- transcription
- music
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Noise-to-Notes introduces a diffusion-based generative model for
  automatic drum transcription, redefining the task from discriminative to generative.
  The method uses an audio-conditioned transformer-based denoising network to transform
  Gaussian noise into drum event transcriptions, including both onsets and velocities.
---

# Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription

## Quick Facts
- arXiv ID: 2509.21739
- Source URL: https://arxiv.org/abs/2509.21739
- Reference count: 0
- Key outcome: Diffusion-based generative model reframes ADT as a generative task, achieving SOTA performance and robust out-of-domain generalization using MFM features.

## Executive Summary
Noise-to-Notes (N2N) introduces a novel diffusion-based generative approach to automatic drum transcription, reframing the task as a conditional generation problem rather than discriminative classification. The method uses an audio-conditioned transformer-based denoising network to transform Gaussian noise into drum event transcriptions, jointly optimizing both binary onsets and continuous velocities through an Annealed Pseudo-Huber loss. By incorporating Music Foundation Model features alongside spectrograms, N2N achieves state-of-the-art performance across multiple ADT benchmarks while demonstrating strong robustness to out-of-domain drum audio. The generative approach also enables new capabilities like inpainting and refinement with minimal sampling steps.

## Method Summary
N2N reframes ADT as a conditional generative task using a diffusion model architecture. It processes audio through dual conditioning encoders (log mel-spectrogram and MFM features) and uses an EDGE-based transformer decoder with FiLM and cross-attention to denoise Gaussian noise into drum transcriptions. The Annealed Pseudo-Huber loss dynamically balances binary onset and continuous velocity optimization during training. Training uses 100 epochs on E-GMD with EDM-style lognormal noise schedules, complete and partial dropout, and 4×A100 GPUs. Inference supports flexible speed-accuracy trade-offs through few-step sampling (1-10 steps).

## Key Results
- Achieves state-of-the-art ADT performance across E-GMD, MDB, and IDMT benchmarks
- Significant robustness improvements for out-of-domain audio when using MFM features
- Maintains strong performance even with minimal sampling steps (1-5 steps)
- Annealed Pseudo-Huber loss improves velocity F1 scores from 66.14 to 76.17

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reframing ADT as a conditional generative task enables iterative refinement of transcriptions, overcoming the "one-shot" limitation of traditional discriminative models.
- **Mechanism:** Instead of directly mapping audio features to a static label estimate, the model learns to reverse a noise process. It starts with Gaussian noise and iteratively denoises it into a drum transcription (onsets and velocities), conditioned on audio features. This allows the model to correct errors over multiple steps (refinement) or fill in missing data (inpainting).
- **Core assumption:** The distribution of valid drum transcriptions can be learned as a reverse diffusion trajectory conditioned on audio signals.
- **Evidence anchors:**
  - [abstract] "...redefine ADT as a conditional generative task... transforming audio-conditioned Gaussian noise into drum events..."
  - [section 1] "...reframing ADT as a generative task... offers distinct advantages, including a flexible speed-accuracy trade-off and strong inpainting capabilities."
  - [corpus] D3RM (Piano Transcription) confirms that diffusion-based refinement is being explored across music transcription tasks, though N2N claims to be the first to surpass discriminative models.
- **Break condition:** If the conditioning signal (audio features) is weak or the noise schedule is too aggressive, the reverse process may "hallucinate" drum events that are acoustically plausible but not present in the source audio.

### Mechanism 2
- **Claim:** The Annealed Pseudo-Huber (APH) loss stabilizes the joint optimization of binary onsets (discrete) and continuous velocities (regression), which standard MSE loss fails to balance.
- **Mechanism:** Standard MSE loss allows onset errors to dominate due to the squared penalty, degrading velocity learning. The APH loss dynamically schedules a threshold $c(t)$ to shift the objective from MSE (early training, stabilizing magnitudes) to MAE (late training, refining precision). This prevents the gradient from being overwhelmed by onset sparsity.
- **Core assumption:** Onset and velocity optimization require different loss landscape geometries at different training stages.
- **Evidence anchors:**
  - [abstract] "...generation of binary onset and continuous velocity values presents a challenge... we introduce an Annealed Pseudo-Huber loss..."
  - [section 3] "...using this standard denoising objective is suboptimal... c(t) is scheduled... to shift the objective from MSE loss... to mean absolute error (MAE) loss..."
  - [section 4.5] Ablation study shows MSE compromises velocity (66.14 F1) while APH significantly improves it (76.17 F1).
- **Break condition:** If the annealing schedule $c(t)$ decays too slowly, the model may get stuck in local minima suitable only for onsets; if too fast, velocity regression may diverge.

### Mechanism 3
- **Claim:** Hybrid conditioning using both spectrograms and Music Foundation Model (MFM) features improves generalization to out-of-domain (OOD) audio by combining low-level acoustic detail with high-level semantic robustness.
- **Mechanism:** Spectrograms provide precise timing/frequency info but overfit to specific drum timbres (acoustic vs. electronic). MFM features (from MERT) provide semantic context that appears more invariant across datasets. The architecture processes these separately before injecting them into the diffusion decoder.
- **Core assumption:** MFM features capture "drum-ness" or rhythmic semantic information that is invariant to production style or drum kit specific to the training set.
- **Evidence anchors:**
  - [abstract] "...incorporates features from music foundation models (MFMs)... enhance robustness, particularly for out-of-domain drum audio."
  - [section 4.5] "Spectrogram features... more distinct [in t-SNE]... MFM features between datasets are more distinct... useful to distinguish drum instruments from different domains."
  - [corpus] "Towards Realistic Synthetic Data" highlights the domain gap in ADT, validating the need for features that bridge synthetic/real or different kit styles.
- **Break condition:** If the MFM is pre-trained on data fundamentally different from the target audio (e.g., purely classical music), its features may lack the necessary granularity for percussive transcription, acting as noise rather than signal.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPMs)**
  - **Why needed here:** This is the engine of N2N. Unlike a standard classifier, you must understand that the model predicts the "noise" (or the clean sample) at various timesteps $t$, and inference requires an iterative loop (sampling steps) rather than a single forward pass.
  - **Quick check question:** Does increasing the number of sampling steps during inference increase or decrease the quality of the transcription, and what is the cost?

- **Concept: Feature-wise Linear Modulation (FiLM)**
  - **Why needed here:** The paper uses a Transformer decoder (EDGE architecture). Understanding how the audio condition ($\phi_{audio}$) and timestep ($t$) are injected into the network is critical. FiLM applies adaptive scaling and shifting to the internal features based on these conditions.
  - **Quick check question:** How does the network modulate its behavior to denoise a "loud snare" vs. a "soft kick" if the input noise is similar? (Answer: via the conditioning vectors modulating the feature maps).

- **Concept: Music Foundation Models (e.g., MERT)**
  - **Why needed here:** A key contribution is using pre-existing large models (MFMs) as feature extractors. You need to understand that these are self-supervised models trained on vast music corpora to provide generalizable audio embeddings.
  - **Quick check question:** Why extract features from an intermediate layer (layer 10) of the MFM rather than just the final classification layer or the raw input?

## Architecture Onboarding

- **Component map:** Audio Input → Dual Feature Extraction (Spec + MFM) → Dropout/Null Embedding Injection → Transformer Decoder → APH Loss Calculation
- **Critical path:** Audio Input → Dual Feature Extraction (Spec + MFM) → Dropout/Null Embedding Injection → Transformer Decoder → APH Loss Calculation
- **Design tradeoffs:**
  - **Speed vs. Accuracy:** The paper shows a clear trade-off. 1-step inference is fast but lower quality; 10-step is SOTA but ~2x slower. The "few-step" (5-10 steps) capability is a specific design goal to minimize this gap.
  - **Feature Complexity:** Adding MFM features drastically improves OOD performance but increases model size and requires pre-processing (extracting MERT embeddings).
- **Failure signatures:**
  - **Hallucinated Onsets:** If dropout rates are too low during training, the model may fail to learn unconditional priors, causing it to generate noise when audio is masked or weak.
  - **Velocity Drift:** If APH loss is replaced by standard MSE, velocity predictions collapse (low F1 velocity score) as the model focuses entirely on onset accuracy.
  - **Overfitting to Timbre:** Without MFM features, the model fails to generalize from E-GMD (real drums) to IDMT (mixed acoustic/synthetic), specifically failing on snares/cymbals.
- **First 3 experiments:**
  1. **Loss Function Ablation:** Train N2N with Standard MSE vs. APH loss on a subset of E-GMD. Verify that APH prevents the "onset dominance" effect and check velocity F1 scores.
  2. **OOD Robustness Check:** Train on E-GMD, evaluate on IDMT. Compare (a) Spec-only features vs. (b) Spec+MFM features. Verify the "domain gap" closure visualized in the paper's t-SNE plots.
  3. **Sampling Step Sensitivity:** Run inference using 1, 5, 10, and 20 steps. Plot the F1 score curve to confirm the "diminishing returns" after 10 steps and validate the speed-accuracy trade-off.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can distillation or consistency modeling techniques bridge the inference gap between N2N and discriminative models while retaining generative capabilities?
  - **Basis in paper:** [explicit] The conclusion states, "In future work, we will investigate methods to bridge the inference gap to discriminative models, for example leveraging distillation to reduce model size and consistency modeling to improve sampling efficiency."
  - **Why unresolved:** While N2N outperforms discriminative models in accuracy, it suffers from higher latency (0.163s vs 0.086s) even in a 1-step setting due to model size and feature extraction.
  - **What evidence would resolve it:** Demonstration of a distilled or consistency-trained N2N variant that matches the latency of the hFT-Transformer baseline while maintaining state-of-the-art F1 scores.

- **Open Question 2:** Can the Annealed Pseudo-Huber loss enable generative models to surpass discriminative methods for pitched instrument transcription?
  - **Basis in paper:** [inferred] The introduction notes that previous generative piano transcription models (DiffRoll, DR3M) failed to match discriminative performance, whereas N2N succeeds for drums using the proposed loss.
  - **Why unresolved:** The paper demonstrates the efficacy of the loss for drum onsets and velocities, but it does not validate whether this joint optimization strategy transfers to the harmonic and polyphonic structures of pitched instruments.
  - **What evidence would resolve it:** Applying the N2N framework and Annealed Pseudo-Huber loss to a piano transcription dataset (e.g., MAESTRO) and comparing results against current discriminative state-of-the-art models.

- **Open Question 3:** Is the linear annealing schedule used for the Pseudo-Huber constant $c(t)$ optimal for balancing onset and velocity optimization?
  - **Basis in paper:** [inferred] Section 3 states that $c(t)$ is scheduled linearly based on empirical observation ("Empirically, we use..."), leaving the possibility of non-linear or adaptive schedules unexplored.
  - **Why unresolved:** The trade-off between MSE and MAE is critical for preventing onset errors from dominating velocity loss, and a linear interpolation may not be the most efficient path for gradient descent.
  - **What evidence would resolve it:** An ablation study comparing the current linear schedule against exponential or cosine-based annealing schedules in terms of convergence speed and final F1 scores.

## Limitations

- The specific architecture hyperparameters for the EDGE-based transformer decoder (layers, hidden dimension, attention heads) are not fully specified, leaving ambiguity in exact reproduction.
- EDM (Exponentially Modified Gaussian) noise schedule parameters and preconditioning details are referenced but not restated in full.
- The paper claims MFM features improve robustness, but the exact contribution of each conditioning modality is not isolated in the ablation.

## Confidence

- **High confidence:** The core mechanism of reframing ADT as a generative task using diffusion models, and the effectiveness of the Annealed Pseudo-Huber loss in stabilizing joint onset/velocity optimization, are well-supported by ablation results and comparisons to standard MSE.
- **Medium confidence:** The specific architecture implementation details (transformer decoder size, dropout schedules) are not fully reproducible from the paper alone.
- **Medium confidence:** The robustness gains from MFM features are demonstrated empirically, but the exact nature of the complementary information (semantic vs. low-level) remains somewhat unclear without deeper analysis of the feature space.

## Next Checks

1. **Architecture fidelity check:** Reconstruct the EDGE-based transformer decoder using the stated 50M parameters and compare intermediate activations to the paper's qualitative descriptions (e.g., feature space visualizations).
2. **Loss function ablation validation:** Implement both MSE and APH losses and verify the claimed improvement in velocity F1 scores (66.14 → 76.17) on a held-out E-GMD subset.
3. **Feature contribution isolation:** Train models using only spectrogram features, only MFM features, and both, then evaluate on IDMT to quantify the marginal benefit of each modality for out-of-domain generalization.