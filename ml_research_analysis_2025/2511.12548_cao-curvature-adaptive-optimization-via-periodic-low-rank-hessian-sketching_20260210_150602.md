---
ver: rpa2
title: 'CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching'
arxiv_id: '2511.12548'
source_url: https://arxiv.org/abs/2511.12548
tags:
- refresh
- curvature
- resnet-18
- learning
- hessian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAO is a curvature-adaptive optimizer that periodically sketches
  a low-rank Hessian subspace and preconditions gradients only in that subspace while
  leaving the orthogonal complement first-order. For L-smooth non-convex objectives,
  it recovers the standard O(1/T) stationarity rate with a widened stable stepsize
  range; under a PL condition with bounded residual curvature outside the sketch,
  the loss contracts at refresh steps.
---

# CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching

## Quick Facts
- arXiv ID: 2511.12548
- Source URL: https://arxiv.org/abs/2511.12548
- Reference count: 32
- CAO reaches train-loss threshold of 0.75 on CIFAR-100/ResNet-18 in 19 epochs vs Adam's 56 epochs (2.95× faster) while matching final test accuracy.

## Executive Summary
CAO is a curvature-adaptive optimizer that periodically sketches a low-rank Hessian subspace and preconditions gradients only in that subspace while leaving the orthogonal complement first-order. For L-smooth non-convex objectives, it recovers the standard O(1/T) stationarity rate with a widened stable stepsize range; under a PL condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, CAO reaches a train-loss threshold of 0.75 substantially faster than Adam—2.95× faster on CIFAR-100/ResNet-18 (19 vs 56 epochs, mean over 3 seeds)—while matching final test accuracy. The method is one-knob: performance is insensitive to the rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation.

## Method Summary
CAO computes top-k eigenpairs of the Hessian via block-Lanczos on Hessian-vector products every m=400 steps, then preconditions gradients in that subspace using P=(B+ηI)^{-1} while leaving the orthogonal complement first-order. The method achieves curvature adaptation at O(k) Hessian-vector product cost per refresh, maintaining O(1/T) stationarity guarantees for L-smooth objectives and per-refresh contraction under PL conditions with bounded residual curvature.

## Key Results
- CIFAR-100/ResNet-18: 19 epochs to train-loss 0.75 vs Adam's 56 epochs (2.95× faster)
- CIFAR-100/ResNet-18: Final test accuracy 0.65, matching Adam
- CIFAR-10/ResNet-18: 10.9 epochs to 0.75 vs Adam's 16.2 epochs (1.48× faster)

## Why This Works (Mechanism)

### Mechanism 1
- Preconditioning only in the top-k Hessian subspace improves the effective condition number without full second-order cost.
- Core assumption: The Hessian spectrum is highly skewed—a few dominant eigenvectors capture most conditioning issues, while the bulk is relatively flat.
- Evidence anchors: Abstract states "preconditions gradients only in that subspace, leaving the orthogonal complement first-order"; Section 3.1 explains steps along vi scale by (λi + η)^{-1} and by η^{-1} on the orthogonal complement; corpus evidence from DOME paper confirms gradients align with a small set of Hessian outlier eigenvectors.
- Break condition: If the Hessian has heavy tail (many non-negligible eigenvalues beyond rank k), residual curvature degrades contraction guarantees.

### Mechanism 2
- Periodic refresh maintains subspace alignment as curvature evolves during training.
- Core assumption: Hessian eigenspace drifts slowly enough that m=400 steps retain useful alignment.
- Evidence anchors: Abstract mentions "periodically sketches a low-rank Hessian subspace via Hessian–vector products"; Section 3.2 describes "Every m steps CAO recomputes the subspace via O(k) HVPs; between refreshes it reuses B"; corpus lacks direct evidence on refresh timing but Appendix B.1 shows m=400 balances cost/freshness on tested benchmarks.
- Break condition: Highly non-stationary loss landscapes may require adaptive refresh policies.

### Mechanism 3
- Fixed damping η ensures positive definiteness and stable progress near negative curvature.
- Core assumption: Sufficient damping (η > 0) is chosen to dominate any negative eigenvalues encountered.
- Evidence anchors: Abstract notes "widened stable stepsize range"; Section 3.1 states "damping bounds ||P|| ≤ 1/η for stability even with negative curvature"; corpus evidence from Dimer-Enhanced Optimization supports curvature handling but lacks direct validation of CAO's specific damping mechanism.
- Break condition: Very small η combined with HVP noise can destabilize estimates.

## Foundational Learning

- Concept: **Hessian-vector products (HVPs)**
  - Why needed here: CAO computes top-k eigenpairs via block-Lanczos on HVPs without materializing the full Hessian. Understanding HVPs (Pearlmutter trick) is essential for implementing the curvature sketch.
  - Quick check question: Can you explain why HVPs cost O(n) rather than O(n²)?

- Concept: **L-smoothness and PL condition**
  - Why needed here: Theoretical guarantees (O(1/T) stationarity, per-refresh contraction) rely on L-smoothness and PL-type assumptions. Interpreting these results requires understanding what these conditions mean.
  - Quick check question: What does the PL inequality ½||∇f||² ≥ μ(f - f*) imply about local geometry?

- Concept: **Preconditioned gradient descent**
  - Why needed here: CAO is a linearly transformed gradient method. Understanding how preconditioners rescale gradient components is core to interpreting the mechanism.
  - Quick check question: How does P = (B + ηI)^{-1} differ from Newton's method when k = d?

## Architecture Onboarding

- Component map: PyTorch autograd -> HVP backend -> Block-Lanczos sketcher -> Preconditioner cache -> Gradient preprocessor -> Parameter update

- Critical path:
  1. Warm-start with brief SGD phase
  2. At refresh steps (s mod m = 0): run block-Lanczos → update (V, λ)
  3. Every step: compute gradient g → precondition d = Pg → clip (optional) → update θ

- Design tradeoffs:
  - Rank k: Larger k captures more curvature but linearly increases HVP cost and memory (O(nk)). Paper shows k=1 is often sufficient.
  - Refresh interval m: Smaller m improves alignment but increases overhead. m=400 is fixed across experiments.
  - Damping η: Larger η stabilizes but reduces preconditioning benefit. Paper sweeps {1e-3, 1e-2, 1e-1}.

- Failure signatures:
  - Divergence with very small η: HVP noise destabilizes eigenpair estimates
  - k=0 matches SGD baseline: confirms curvature-free ablation (sanity check)
  - No improvement over Adam on benchmarks: check that V is being refreshed (logging), verify damping isn't too large

- First 3 experiments:
  1. Reproduce C100-R18 threshold result: CAO (k=1, m=400) vs Adam on epochs to train-loss 0.75. Expect ~19 vs ~56 epochs.
  2. Rank ablation: Run k ∈ {0, 1, 3, 5} on same benchmark; verify curves align for k ≥ 1 and k=0 lags.
  3. Sensitivity sweep: Vary η ∈ {1e-3, 1e-2, 1e-1} and m ∈ {200, 400, 800} on single seed; confirm m=400/η≈1e-2 is reasonable default.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CAO retain its speedup advantage on large-scale Transformer models (e.g., GPT, ViT), or does the Hessian spectrum structure differ enough to require substantially larger k?
- Basis in paper: Section 7 states "behavior on very large models (e.g., transformers) or heavy data augmentation remains to be validated."
- Why unresolved: Experiments limited to ResNet-18/34 on CIFAR; Transformer Hessians may have different spectral decay, and attention layers may not yield the same low-rank approximation quality.
- What evidence would resolve it: Run CAO on standard Transformer benchmarks (e.g., WikiText-103 with a small GPT, or ImageNet with ViT) and compare wall-clock-to-loss-threshold vs AdamW; report whether k∈{1,3,5} still suffices or if larger k/m are needed.

### Open Question 2
- Question: Can an adaptive refresh policy (e.g., triggered by gradient-norm drift or subspace alignment decay) improve over the fixed m=400 schedule, and what is the right trigger criterion?
- Basis in paper: Appendix B.1 notes "Adaptive refresh based on gradient norm change is a promising extension."
- Why unresolved: Paper only sweeps m∈{200,400,800} as a fixed hyperparameter; no mechanism to refresh more frequently in rapidly changing curvature regions or less frequently in flat regions.
- What evidence would resolve it: Implement a gradient-norm-change or subspace-alignment trigger; compare final loss and wall-clock vs fixed-m across CIFAR and a longer training run.

### Open Question 3
- Question: How does distributed data-parallel CAO perform, and what is the communication-efficient way to aggregate local low-rank sketches?
- Basis in paper: Section 7 states "Distributed HVPs and sketch aggregation are non-trivial."
- Why unresolved: Current implementation is single-GPU; distributed training requires either (a) all-reduce of k eigenvectors at each refresh or (b) local sketches with periodic sync. Communication cost may negate speedup gains.
- What evidence would resolve it: Implement a multi-GPU CAO with Power iteration synced via all-reduce; benchmark scaling efficiency (speedup vs num-GPUs) and compare to distributed Adam.

### Open Question 4
- Question: Does the 2–3x memory overhead of CAO (Table 3) limit practical deployment on memory-bound workloads, and can a low-rank-projected momentum buffer reduce this cost?
- Basis in paper: Table 3 shows CAO incurs ~2.5–2.9x peak memory vs SGD/Adam, which may be prohibitive for large models or limited GPU memory.
- Why unresolved: Paper reports overhead but does not explore memory-reduction strategies such as keeping momentum only in the low-rank subspace or using FP16 for cached eigenvectors.
- What evidence would resolve it: Implement subspace-projected momentum or FP16 sketch storage; measure peak memory reduction and any degradation in convergence speed vs the baseline CAO.

## Limitations
- Theoretical guarantees rely on strong PL-like curvature assumptions and bounded residual outside the sketch
- Experimental validation limited to ResNet models on CIFAR datasets
- Memory overhead of 2–3× may limit deployment on memory-constrained systems

## Confidence
- Empirical results: Medium-High for CIFAR-100/ResNet-18 speedup claim (multiple seeds, threshold-based timing), Medium for generality of rank-insensitivity
- Theoretical claims: Medium (proofs assume bounded residual curvature and slow subspace drift)
- Missing implementation details: Medium (exact learning rates, damping values, weight decay unspecified)

## Next Checks
1. **Sanity check curvature-free ablation**: Run CAO with k=0 on CIFAR-100/ResNet-18; confirm it matches SGD performance within statistical error
2. **Parameter sensitivity**: Sweep damping η ∈ {1e-3, 1e-2, 1e-1} and refresh interval m ∈ {200, 400, 800} on a single seed; verify m=400 and η≈1e-2 are near-optimal
3. **Memory and HVP cost audit**: Profile peak memory and per-step time for k=1 vs k=3 on CIFAR-100/ResNet-18 to confirm claimed O(nk) scaling and ~2.7 GB peak