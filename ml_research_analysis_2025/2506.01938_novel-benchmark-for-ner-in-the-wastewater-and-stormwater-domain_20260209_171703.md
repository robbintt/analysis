---
ver: rpa2
title: Novel Benchmark for NER in the Wastewater and Stormwater Domain
arxiv_id: '2506.01938'
source_url: https://arxiv.org/abs/2506.01938
tags:
- annotation
- italian
- corpus
- french
- annotated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a French-Italian multilingual benchmark for
  Named Entity Recognition (NER) in wastewater and stormwater management, addressing
  the lack of domain-specific resources. The starwars corpus, containing 110 documents
  per language with 14 entity types, was manually annotated and aligned.
---

# Novel Benchmark for NER in the Wastewater and Stormwater Domain

## Quick Facts
- arXiv ID: 2506.01938
- Source URL: https://arxiv.org/abs/2506.01938
- Reference count: 26
- Primary result: Develops French-Italian NER benchmark for wastewater domain with 14 entity types

## Executive Summary
This study addresses the lack of domain-specific NER resources for wastewater and stormwater management by creating a French-Italian multilingual benchmark. The starwars corpus contains 110 manually annotated documents per language with 14 entity types. Experiments compare BERT variants and LLMs, finding that automatic annotation via CamemBERT achieves F1 scores of 0.63 (French) and 0.53 (Italian). Cross-lingual annotation projection using mBERT-based alignment outperforms monolingual Italian models, demonstrating the potential of this approach for low-resource technical domains.

## Method Summary
The study created the starwars corpus with 110 documents per language (French and Italian) manually annotated with 14 entity types using IOB2 scheme. For monolingual NER, BERT variants (CamemBERT, Italian BERT, GilBERTo) were fine-tuned using 70/10/20 train/validation/test splits with Adam optimizer (LR: 1e-5, Batch Size: 16, Epochs: 40). For annotation projection, mBERT embeddings with SimAlign or AWESoME-a aligned parallel sentences, transferring source labels to target tokens. Decoder-only LLMs (Llama, Mistral, Gemma, Phi) were also tested for zero-shot annotation using HTML-style tags.

## Key Results
- CamemBERT fine-tuning achieved F1 scores of 0.63 (French) and 0.53 (Italian)
- Annotation projection using mBERT-based alignment achieved F1 scores of 0.66 (French→Italian) and 0.65 (Italian→French)
- Cross-lingual projection outperformed monolingual Italian models
- Decoder-only LLMs struggled with constrained output despite zero-shot flexibility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-lingual annotation projection from French to Italian outperforms monolingual Italian fine-tuning for low-resource technical domains.
- **Mechanism:** Multilingual contextual embeddings (mBERT) encode cross-lingual semantic similarity at the sub-word level; word alignment algorithms map tokens between parallel sentences, transferring entity labels without requiring target-language annotations. Fine-tuning mBERT on alignment objectives improves embedding similarity across language pairs.
- **Core assumption:** The source language (French) has higher-quality pre-trained models and/or more annotated data than the target language.
- **Evidence anchors:**
  - [abstract] "Annotation projection using mBERT-based alignment achieved F1 scores of 0.66 (French→Italian) and 0.65 (Italian→French), outperforming monolingual models on Italian."
  - [Section 5.2] "For Italian... using them for text annotation is ineffective, whereas projecting annotations from French yields an F1 comparable to that of the monolingual model on the French subset."
  - [corpus] Limited corpus evidence from neighbors; no direct replications in wastewater domain. Related work in biomedical NER (neighbor FMR=0.66) suggests cross-lingual transfer is plausible but not proven for this domain.
- **Break condition:** If source-language annotations contain systematic errors or if parallel corpora are not truly sentence-aligned, projection quality degrades. Also breaks when source and target languages lack shared sub-word tokens in mBERT vocabulary.

### Mechanism 2
- **Claim:** Pre-trained language model quality (measured by pre-training corpus size and domain coverage) predicts fine-tuning performance on technical NER tasks.
- **Mechanism:** Larger pre-training corpora yield more expressive contextual embeddings; models exposed to technical terminology during pre-training require less domain adaptation. The IOB2 token classification head learns to map embedding space to entity boundaries.
- **Core assumption:** Domain-specific terminology appears in pre-training data with sufficient frequency to shape embeddings.
- **Evidence anchors:**
  - [Section 4.1] CamemBERT pre-trained on 32.7B tokens vs. Italian BERT on 2B tokens and GilBERTo on 11B tokens.
  - [Section 5.1] "This performance gap can be attributed also to the much larger pre-training dataset used for CamemBERT... which likely results in more expressive contextual embeddings."
  - [corpus] Neighbor paper "Effective Multi-Task Learning for Biomedical NER" (FMR=0.66) supports the role of pre-training scale, but no wastewater-specific corroboration exists.
- **Break condition:** When pre-training corpus is large but irrelevant to the target domain, or when annotation schema requires reasoning beyond token-level patterns (e.g., nested entities, discontinuous mentions).

### Mechanism 3
- **Claim:** Decoder-only LLMs struggle with constrained annotation projection tasks when instructions violate their learned output distributions.
- **Mechanism:** LLMs generate tokens autoregressively; instructions to "use exactly 14 tags" or "do not introduce new tags" conflict with the model's tendency to generate plausible but unconstrained outputs. HTML-style tags may align better with pre-training data than IOB2 sequences.
- **Core assumption:** The model's instruction-following capability generalizes to out-of-distribution constraint types.
- **Evidence anchors:**
  - [Section 5.3] "Only two models use the correct set of 14 tags in a consistent manner... Surprisingly, the smaller Gemma3 outperforms Mistral."
  - [Section 5.3] "These results... indicate that even very large models struggle to follow simple but unconventional (probably out-of-distribution) instructions, such as 'do not introduce new tags'."
  - [corpus] Neighbor "Ground Truth Generation for Multilingual Historical NLP using LLMs" (FMR=0.59) shows LLMs can generate annotations, but for different task formulations.
- **Break condition:** When prompt engineering fails to constrain output space, or when domain-specific terms are tokenized into unfamiliar sub-word units.

## Foundational Learning

- **Concept:** IOB2 (Inside-Outside-Beginning) tagging scheme
  - **Why needed here:** The paper uses IOB2 as the standard annotation format for sequence labeling; understanding B-T vs I-T distinctions is required to interpret all experimental results.
  - **Quick check question:** Given the token sequence "wastewater treatment plant" annotated as a single Structure entity, what are the correct IOB2 tags for each token?

- **Concept:** Word alignment in parallel corpora
  - **Why needed here:** Annotation projection depends on algorithms (SimAlign, AWESoME-align) that compute token-level correspondences between translated sentences; misalignment directly causes label propagation errors.
  - **Quick check question:** If the French phrase "réseau d'assainissement" (3 tokens) aligns to the Italian "rete di fognatura" (3 tokens), but the English translation is "sewer network" (2 tokens), which alignment is more likely to cause projection errors?

- **Concept:** Cross-lingual transfer vs. annotation projection
  - **Why needed here:** The paper compares two approaches: (1) fine-tuning monolingual models on target-language data, and (2) projecting labels via word alignment. Understanding when each is preferable informs deployment decisions.
  - **Quick check question:** For a new target language with zero annotated data but high-quality machine translation available, which approach should be attempted first?

## Architecture Onboarding

- **Component map:** Data collection → manual annotation → machine translation → human verification → annotation projection → model fine-tuning → evaluation
- **Critical path:**
  1. Collect domain documents → manually annotate in source language (French)
  2. Machine-translate to target language → human verification of translations
  3. Transfer annotations via manual alignment OR automatic projection
  4. Fine-tune encoder-only model on annotated target corpus
  5. Evaluate on held-out test set using entity-level F1
- **Design tradeoffs:**
  - **More manual annotation vs. projection:** Manual annotation yields higher quality but scales poorly; projection introduces alignment noise but enables rapid language extension.
  - **Monolingual vs. multilingual models:** Monolingual models (CamemBERT) outperform when pre-training data is abundant; multilingual models (mBERT) enable cross-lingual transfer when target-language resources are scarce.
  - **Encoder-only vs. decoder-only LLMs:** Encoder-only models are more sample-efficient for token classification; decoder-only models offer zero-shot flexibility but struggle with output constraints.
- **Failure signatures:**
  - Low recall on low-frequency entity types → insufficient training data for rare classes
  - Projected annotations contain spurious new tags → LLM not following constraints; switch to encoder-only approach
  - Large gap between source→target vs. target→source projection → asymmetrical alignment quality; increase AWESoME fine-tuning epochs
- **First 3 experiments:**
  1. Replicate the CamemBERT fine-tuning curve (10%, 25%, 50%, 100% training data) to establish baseline; confirm F1≈0.63 at full data.
  2. Test annotation projection from French to Italian using AWESoME-align with 25 fine-tuning epochs; verify F1 improvement over monolingual Italian baseline.
  3. Evaluate a smaller open-weight LLM (e.g., Llama3-8B) on the HTML-style annotation task; compare tag adherence vs. larger models to assess cost-quality tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can increasing the training data volume overcome the performance ceiling observed in monolingual Italian models?
- Basis in paper: [explicit] The authors state that since models reached maximum F1 scores using 100% of the training data, "their generalization error would likely decrease with larger training sets," specifically noting the need for more Italian data.
- Why unresolved: The current dataset is insufficient to determine if the performance gap between French (0.63 F1) and Italian (0.53 F1) is due to model quality or data scarcity.
- What evidence would resolve it: Performance evaluation of Italian models fine-tuned on an expanded corpus to determine if F1 scores saturate or continue to improve.

### Open Question 2
- Question: Can prompt engineering or fine-tuning mitigate the instruction-following failures observed in decoder-only LLMs for this task?
- Basis in paper: [explicit] The authors note that large models "struggle to follow simple but unconventional... instructions" (e.g., avoiding new tags) and explicitly plan to "continue to investigate the use of decoder-only LLMs."
- Why unresolved: Zero-shot experiments resulted in hallucinated tags and syntax errors, suggesting the prompts used were insufficient to constrain the models.
- What evidence would resolve it: Experiments utilizing few-shot prompting or parameter-efficient fine-tuning (PEFT) to see if LLMs can strictly adhere to the 14-entity tag set.

### Open Question 3
- Question: Is annotation projection effective when extending the benchmark to non-Romance languages like English?
- Basis in paper: [explicit] The authors plan to extend the corpus to "other languages such as English" and "continue investigating techniques for annotation projection."
- Why unresolved: Current projection success (French↔Italian) relies on closely related languages; it is unknown if mBERT alignments hold for linguistically distant pairs.
- What evidence would resolve it: Evaluation of annotation projection quality (F1 scores) from French/Italian sources to a target English corpus.

## Limitations

- The starwars corpus is not publicly available, preventing independent validation of reported F1 scores and fine-tuning curves.
- The LLM annotation experiments lack reproducible prompt templates, making it impossible to determine whether instruction-following failures stem from model limitations or prompt engineering issues.
- The performance gap between Italian and French models may be influenced by translation artifacts or verification quality, neither of which is fully characterized in the paper.

## Confidence

**High Confidence:** The fundamental observation that multilingual models enable cross-lingual annotation projection in low-resource domains is well-supported by consistent F1 scores (0.65-0.66) and straightforward mechanism.

**Medium Confidence:** Claims about pre-training data size predicting fine-tuning performance are plausible given the French-Italian model comparison, but lack domain-specific validation and could be influenced by factors beyond corpus size.

**Low Confidence:** LLM zero-shot annotation results are difficult to evaluate without prompt details; the observation that smaller models (Gemma3) outperform larger ones (Mistral) suggests prompt engineering issues rather than fundamental model limitations.

## Next Checks

1. **Data Access Verification:** Request the starwars corpus from authors and verify the 70/10/20 split maintains document-level alignment. Replicate the CamemBERT fine-tuning curve to confirm F1≈0.63 at full data and establish baseline reproducibility.

2. **Annotation Projection Robustness:** Systematically evaluate alignment quality by comparing projection F1 scores across different heuristics (inter, itermax, mwmf) and fine-tuning epochs (1, 10, 25). Correlate alignment error rates with label propagation accuracy to quantify the cost of automatic projection vs. manual annotation.

3. **LLM Prompt Engineering:** Design and test multiple prompt templates for the HTML-style annotation task, varying constraint formulations ("exactly 14 tags" vs. "use these tags only"). Compare Gemma3-3B, Llama3-8B, and Mistral-7B performance to determine whether smaller models genuinely outperform larger ones or if prompt structure is the dominant factor.