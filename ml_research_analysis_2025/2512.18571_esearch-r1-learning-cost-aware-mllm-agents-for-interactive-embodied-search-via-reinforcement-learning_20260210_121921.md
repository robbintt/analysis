---
ver: rpa2
title: 'ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search
  via Reinforcement Learning'
arxiv_id: '2512.18571'
source_url: https://arxiv.org/abs/2512.18571
tags:
- cost
- reasoning
- agent
- arxiv
- esearch-r1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ESearch-R1, a cost-aware embodied reasoning
  framework for interactive object search under ambiguous natural language instructions.
  The key challenge is balancing the high cost of physical navigation against the
  lower cost of human interaction or memory retrieval.
---

# ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2512.18571
- **Source URL**: https://arxiv.org/abs/2512.18571
- **Reference count**: 40
- **Primary result**: ESearch-R1 achieves 61.5% success rate with 50% cost reduction in interactive embodied object search

## Executive Summary
ESearch-R1 introduces a cost-aware embodied reasoning framework for interactive object search under ambiguous natural language instructions. The framework addresses the fundamental challenge of balancing expensive physical navigation against cheaper human interaction or memory retrieval. By introducing HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization), the system eliminates the need for critic networks while optimizing for both task success and heterogeneous costs. The unified decision process combines Ask, GetMemory, and Navigate actions to handle ambiguity efficiently. Experiments on the ESearch-Bench benchmark demonstrate significant improvements over strong baselines, with success rates reaching 61.5% and total task costs reduced from 3.3 to 1.6.

## Method Summary
The core innovation is HC-GRPO, a reinforcement learning algorithm that samples groups of reasoning trajectories and reinforces those that optimally trade off information gain against heterogeneous costs. The framework operates within AI2-THOR and unifies three action types—dialogue-based asking, episodic memory retrieval, and physical navigation—into a single decision process. The algorithm eliminates the need for a critic network by using relative advantage estimation across trajectory groups, enabling efficient learning of cost-aware policies. The ESearch-Bench benchmark provides a controlled environment for evaluating interactive search performance with 150 tasks across 8 object types.

## Key Results
- ESearch-R1 achieves 61.5% success rate, outperforming ReAct-based baselines
- Total task costs reduced by approximately 50% (from 3.3 to 1.6)
- Success weighted by cost (SwC) improves from 0.36 to 0.59
- Demonstrates effectiveness of GRPO in aligning MLLMs with physical world constraints

## Why This Works (Mechanism)
The framework succeeds by recognizing that embodied search under ambiguity requires strategic cost management rather than pure task completion. HC-GRPO enables the agent to learn when to invest in expensive navigation versus when to leverage cheaper information sources like human interaction or memory. By sampling trajectory groups and using relative advantage estimation, the algorithm can optimize for long-term cost efficiency without requiring explicit cost modeling through critic networks. This approach aligns well with how humans naturally balance exploration costs against information gain in uncertain environments.

## Foundational Learning

**Reinforcement Learning with Group Relative Advantage**
- *Why needed*: Enables optimization of both success rate and heterogeneous costs without requiring separate value estimation networks
- *Quick check*: Verify that group sampling produces statistically significant performance differences between reinforced and non-reinforced trajectories

**Heterogeneous Cost Modeling**
- *Why needed*: Different actions (navigation vs. asking vs. memory) have vastly different resource implications that must be explicitly accounted for
- *Quick check*: Test cost sensitivity by varying action cost parameters across orders of magnitude

**Embodied Multimodal Reasoning**
- *Why needed*: Combines natural language understanding with physical world constraints and action planning
- *Quick check*: Validate that the MLLM can correctly interpret ambiguous instructions and map them to appropriate action sequences

## Architecture Onboarding

**Component Map**
MLLM (perception + reasoning) -> Action Selector (HC-GRPO) -> Environment (AI2-THOR) -> Reward Signal -> Policy Update

**Critical Path**
Perception → Reasoning → Action Selection → Environment Interaction → Feedback → Policy Update

**Design Tradeoffs**
- Eliminates critic network complexity at the cost of requiring group sampling for training
- Fixed cost parameters provide simplicity but may limit generalization
- Unified action space simplifies decision-making but may obscure optimal action hierarchies

**Failure Signatures**
- High navigation costs with low success indicate poor initial reasoning or excessive exploration
- Over-reliance on asking suggests insufficient memory utilization or reasoning capability
- Memory retrieval failures point to inadequate object localization or memory encoding

**3 First Experiments**
1. Compare HC-GRPO against standard GRPO and PPO baselines on identical task distributions
2. Ablation study removing each action type (Ask, GetMemory, Navigate) to identify critical capabilities
3. Cost sensitivity analysis varying navigation/ask/memory costs to find robust operating regimes

## Open Questions the Paper Calls Out

None

## Limitations

- Cost parameters are fixed and may not generalize across different environments or user preferences
- Limited benchmark diversity with only 150 tasks in a single simulated environment
- Assumes perfect perception and action execution, abstracting away real-world uncertainties

## Confidence

- **High Confidence**: GRPO algorithm mechanism and reported benchmark improvements are technically sound
- **Medium Confidence**: Framework's applicability beyond the specific benchmark setup
- **Low Confidence**: Real-world deployment readiness claims due to abstracted environmental challenges

## Next Checks

1. **Cost Sensitivity Analysis**: Systematically vary cost parameters across orders of magnitude to determine robustness and identify stable operating regimes

2. **Cross-Environment Generalization**: Evaluate ESearch-R1 on at least two additional embodied environments with different layouts and object distributions

3. **Failure Mode Analysis**: Conduct ablation studies isolating perception errors, action execution failures, and memory retrieval inaccuracies to quantify individual failure contributions