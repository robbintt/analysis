---
ver: rpa2
title: 'DRES: Benchmarking LLMs for Disfluency Removal'
arxiv_id: '2509.20321'
source_url: https://arxiv.org/abs/2509.20321
tags:
- disfluency
- removal
- llms
- speech
- disfluencies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRES introduces a controlled text-level benchmark for disfluency
  removal using gold Switchboard transcripts, isolating the task from ASR errors.
  It systematically evaluates 18 proprietary and open-source LLMs across scales, prompting
  strategies, and architectures.
---

# DRES: Benchmarking LLMs for Disfluency Removal

## Quick Facts
- arXiv ID: 2509.20321
- Source URL: https://arxiv.org/abs/2509.20321
- Reference count: 0
- Primary result: Introduces controlled benchmark for disfluency removal using gold Switchboard transcripts, evaluating 18 LLMs with segmentation improving performance even for long-context models.

## Executive Summary
DRES introduces a controlled text-level benchmark for disfluency removal using gold Switchboard transcripts, isolating the task from ASR errors. It systematically evaluates 18 proprietary and open-source LLMs across scales, prompting strategies, and architectures. Proprietary models (e.g., gpt-4o-mini) achieve the highest scores (EF ≈72–94), with open-source models lagging by 10–15 points. Segmentation consistently improves performance, while reasoning-oriented models tend to over-delete fluent tokens. Fine-tuning boosts precision and recall to near state-of-the-art but harms generalization on unrelated tasks. The benchmark also identifies LLM-specific failure modes—over-deletion, under-deletion, and category-specific weaknesses—and offers nine deployment recommendations. DRES provides a reproducible foundation for advancing spoken-language systems.

## Method Summary
The study evaluates disfluency removal using Switchboard corpus with gold-standard human annotations, constructing dataset T as 4-tuples (t_tree, t_fluent, t_disfluent, t_tag) using Shriberg annotation scheme. It tests 18 LLMs (proprietary: gpt-4o, gpt-4o-mini, o4-mini; open-source: Llama-3.1/3.2/3.3, MobileLLM, Qwen3, Phi-4-mini) comparing full-context vs segmented inputs across few-shot settings. Evaluation uses E-Scores (word-level precision E_P, recall E_R, F1 E_F) and Z-Scores by category (Z_E, Z_I, Z_P). Fine-tuned models also evaluated on GSM8K, MMLU, CoQA for generalization. Outputs are aligned using Gestalt pattern matching variation.

## Key Results
- Proprietary models (gpt-4o-mini) achieve EF ≈72–94, outperforming open-source by 10–15 points
- Segmentation consistently improves performance, even for long-context models (gpt-4o: EF 76.13 → 82.38)
- Fine-tuning achieves near state-of-the-art precision (EP ~96) but degrades generalization on GSM8K, MMLU, CoQA
- Reasoning-oriented models systematically over-delete fluent tokens (o4-mini: EP 26.19, ER 97.49)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Segmentation improves disfluency removal performance even in long-context models
- Mechanism: Segmenting transcripts into smaller units reduces context length, mitigating long-context degradation. Shorter contexts allow models to maintain focused attention on local disfluency patterns rather than dispersing attention across extended passages.
- Core assumption: The improvement stems from attention distribution rather than segment-boundary artifacts.
- Evidence anchors:
  - [abstract] "simple segmentation consistently improves performance, even for long-context models"
  - [section 3, Results] "gpt-4o improves from EF =76.13 (f) to 82.38 (s) at k=1"; R2 recommends segmentation as preprocessing
  - [corpus] Weak direct evidence; related work on long-context degradation exists (citations [25, 26]) but corpus neighbors don't specifically address segmentation
- Break condition: If segments split mid-disfluency (e.g., separating "I want—uh—um—the blue one" across boundaries), performance may degrade; segmentation quality matters.

### Mechanism 2
- Claim: Reasoning-oriented models systematically over-delete fluent tokens
- Mechanism: Models trained for extended reasoning chains interpret disfluency removal as an interpretive editing task rather than a detection task, leading to conservative deletion strategies that remove ambiguous but fluent content. High recall at the cost of precision suggests these models default to deletion when uncertain.
- Core assumption: Over-deletion stems from reasoning-style training objectives rather than architectural differences.
- Evidence anchors:
  - [abstract] "reasoning-oriented models tend to over-delete fluent tokens"
  - [section 3, Results] o4-mini shows EF=40.71 with EP=26.19 but ER=97.49 (k=1); R7 explicitly warns reasoning capability does not translate to disfluency removal
  - [corpus] VocalBench-DF paper (arxiv:2510.15406) confirms disfluency robustness is "critically under-tested" in speech-LLMs
- Break condition: If a model's reasoning capability is explicitly trained on speech corpora with disfluency preservation objectives, this failure mode may not occur.

### Mechanism 3
- Claim: Fine-tuning on disfluency removal improves precision/recall but degrades generalization on unrelated tasks
- Mechanism: Specialized fine-tuning overwrites general-purpose weights (catastrophic forgetting), trading broad capability for narrow expertise. The model optimizes for local disfluency patterns at the expense of reasoning, mathematical, and general comprehension abilities.
- Core assumption: The generalization loss is due to weight interference rather than evaluation contamination.
- Evidence anchors:
  - [abstract] "fine-tuning achieves near state-of-the-art precision and recall but harms generalization abilities"
  - [section 3, Tables 4-5] gpt-4o-mini fine-tuned: EP improves to 96.63, but GSM8K drops from 0.86→0.51, MMLU 0.76→0.69, CoQA 0.58→0.55
  - [corpus] No direct corpus evidence on this specific trade-off
- Break condition: If using parameter-efficient fine-tuning (adapters, LoRA) with frozen base weights, generalization may be partially preserved—this is suggested in Section 4 as future work but not tested.

## Foundational Learning

- Concept: Disfluency annotation taxonomy (Shriberg scheme)
  - Why needed here: Understanding INTJ (interjections like "um"), PRN (parentheticals), and EDITED (self-corrections) nodes is required to interpret Z-Scores and diagnose category-specific failures.
  - Quick check question: Given "I want the—uh—the red one," which tokens correspond to INTJ vs EDITED nodes?

- Concept: Precision-Recall trade-off in disfluency detection
  - Why needed here: The paper reveals failure modes at both extremes (over-deletion: high recall/low precision; under-deletion: high precision/low recall); deployment requires choosing which error type is more tolerable for the use case.
  - Quick check question: For a voice command system, which failure mode is more costly—deleting "not" (over-deletion) or preserving "um" (under-deletion)?

- Concept: E-Scores vs Z-Scores
  - Why needed here: E-Scores measure token-level accuracy (EP, ER, EF), while Z-Scores measure structural removal by disfluency category (ZE, ZI, ZP); both are needed to diagnose *why* a model fails.
  - Quick check question: A model achieves EF=70 but ZI=20—what does this indicate about its behavior?

## Architecture Onboarding

- Component map:
Speech Input → [ASR System] → Raw Transcript → [Segmenter] → Segments → [LLM w/ prompt] → Cleaned Text
                                     ↓
                              Gold Transcripts (evaluation only)
                                     ↓
                              [E-Score/Z-Score Calculator]

- Critical path:
  1. **Input preprocessing**: Apply segmentation (R2) regardless of model context length claims
  2. **Model selection**: Prioritize proprietary models for production (R1); avoid reasoning-tuned models (R7)
  3. **Prompting**: Use k-shot with caution (R3)—test 0-shot vs few-shot per model family
  4. **Evaluation**: Track both E-Scores and Z-Scores; monitor EP/ER balance for failure mode detection

- Design tradeoffs:
  - Proprietary vs Open-source: 10-15 point performance gap vs cost/control
  - Fine-tuned vs General-purpose: Near-SOTA removal (EP~97) vs catastrophic forgetting (GSM8K -40%)
  - High precision vs High recall: Over-deletion (lost content) vs under-deletion (noisy output)

- Failure signatures:
  - **Over-deletion collapse**: High ER (>95) + Low EP (<40) → reasoning models, unsegmented long inputs
  - **Under-deletion**: High EP (>80) + Low ER (<50) → conservative models, Qwen family
  - **Category blindspots**: ZI/ZP << ZE → INTJ/PRN categories under-served (R4)

- First 3 experiments:
  1. **Baseline segmentation test**: Run gpt-4o-mini or best available model on full vs segmented transcripts (s0-s5 conditions); expect 5-10 EF point improvement from segmentation alone.
  2. **Failure mode audit**: On your target model, compute both E-Scores and Z-Scores; if ZI/ZP < 50 while ZE > 80, you have category-specific weaknesses requiring targeted examples.
  3. **Fine-tuning vs adapter comparison**: If pursuing specialized deployment, compare full fine-tuning against LoRA/adapter approaches on held-out generalization tasks (GSM8K, MMLU equivalents) to quantify forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can lightweight adapters or multi-task learning achieve state-of-the-art disfluency removal without inducing catastrophic forgetting in general-purpose LLMs?
- Basis in paper: [explicit] Section 4 suggests exploring "lightweight adapters, multi-task setups, and continual learning" to mitigate the generalization loss observed in fine-tuned models (Table 5).
- Why unresolved: Fine-tuning improves disfluency scores (Table 4) but significantly degrades performance on unrelated tasks like GSM8K and MMLU; the proposed alternatives were not tested in this work.
- What evidence would resolve it: Evaluation of models using adapters or multi-task objectives showing high F-scores on DRES while maintaining stable accuracy on general reasoning benchmarks.

### Open Question 2
- Question: What specific mechanisms drive reasoning-oriented models to over-delete fluent tokens during disfluency removal?
- Basis in paper: [explicit] Section 4 identifies "closing the precision-recall trade-off (especially over-deletion in reasoning-tuned models)" as an open challenge; Section 3 notes that reasoning capability does not translate to this task.
- Why unresolved: The paper empirically observes that models like o4-mini and Phi-4-reasoning exhibit low precision (over-deletion), but does not isolate the underlying architectural or training causes.
- What evidence would resolve it: An ablation study on reasoning-tuned models identifying specific attention heads or chain-of-thought behaviors responsible for the deletion of fluent tokens.

### Open Question 3
- Question: To what extent does high performance on the DRES "semantic upper bound" benchmark correlate with measurable improvements in downstream applications?
- Basis in paper: [explicit] Section 4 calls for "systematically evaluating its downstream impacts (e.g., command success rates, summarization fidelity)" to help build more robust systems.
- Why unresolved: DRES evaluates disfluency removal in isolation using gold transcripts; the direct link between improving DRES scores and success in complex pipelines (e.g., voice assistants) remains assumed but unquantified.
- What evidence would resolve it: Correlation data between DRES F-scores and success rates on downstream task benchmarks (e.g., VoiceBench or summarization datasets).

## Limitations

- The segmentation strategy is described as "simple" but lacks details on segment length, overlap handling, or boundary conditions
- Fine-tuning experiments show catastrophic forgetting but don't explore parameter-efficient alternatives (LoRA, adapters) that might preserve generalization
- The evaluation focuses exclusively on Switchboard transcripts without testing cross-domain robustness to other disfluency patterns or speaker demographics

## Confidence

- **High confidence**: Segmentation consistently improves performance, fine-tuning degrades generalization, and proprietary models outperform open-source by 10-15 points
- **Medium confidence**: Reasoning models over-delete fluent tokens—supported by EF and EP/ER patterns but could also reflect architectural differences
- **Low confidence**: The specific failure mode thresholds (e.g., "ZI/ZP < 50 while ZE > 80" indicating category blindspots) are heuristic interpretations rather than rigorously validated diagnostic criteria

## Next Checks

1. **Segmentation boundary validation**: Test whether performance degrades when segments split mid-disfluency by artificially inserting boundaries within known disfluency spans and measuring EF drop

2. **Adapter-based fine-tuning comparison**: Replicate the fine-tuning experiments using LoRA or adapter methods while freezing base model weights, then compare generalization degradation on GSM8K/MMLU to full fine-tuning results

3. **Cross-domain robustness testing**: Evaluate the best-performing models on disfluency datasets from different domains (e.g., conversational AI transcripts, clinical speech, second-language learner speech) to quantify how Switchboard-specific training generalizes to real-world deployment scenarios