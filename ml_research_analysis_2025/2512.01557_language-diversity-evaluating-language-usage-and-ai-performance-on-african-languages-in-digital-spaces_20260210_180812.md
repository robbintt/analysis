---
ver: rpa2
title: 'Language Diversity: Evaluating Language Usage and AI Performance on African
  Languages in Digital Spaces'
arxiv_id: '2512.01557'
source_url: https://arxiv.org/abs/2512.01557
tags:
- language
- languages
- data
- news
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the digital representation of African languages
  and the performance of language detection tools on Yoruba, Kinyarwanda, and Amharic.
  The analysis revealed that conversational platforms like Reddit are sparse and heavily
  code-switched with English, making them unreliable for clean language data.
---

# Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces

## Quick Facts
- arXiv ID: 2512.01557
- Source URL: https://arxiv.org/abs/2512.01557
- Reference count: 17
- One-line primary result: News media provides cleaner, more detectable African language data than conversational platforms, which are heavily code-switched with English.

## Executive Summary
This study investigates the digital representation of African languages and the performance of language detection tools on Yoruba, Kinyarwanda, and Amharic. The analysis reveals that conversational platforms like Reddit are sparse and heavily code-switched with English, making them unreliable for clean language data. In contrast, local news media provided robust, monolingual content. Language detection models like AfroLID and a multilingual LLM achieved near-perfect accuracy on clean news data but struggled with code-switched Reddit posts. The study concludes that professionally curated news content is more effective for training AI models for African languages and highlights the need for models that can process both clean and code-switched text.

## Method Summary
The study collected Reddit posts from r/yoruba, r/amharic, and r/Rwanda (365 days) using PRAW, and news articles from BBC Yoruba, Rwanda Broadcasting Agency, and Fana Broadcasting Corporation. Facebook comments from news outlets' social media pages were also gathered. Two language detection approaches were applied: AfroLID (specialized African LID supporting 517 languages) and Llama 3.3 70B (general multilingual LLM). News articles were manually annotated for topics by native speakers across categories like Business, Education, and Sports. Detection accuracy was compared across source types (Reddit vs. news) and tools.

## Key Results
- News media offered clean, monolingual language data while Reddit was sparse and heavily code-switched with English.
- AfroLID achieved 100% accuracy on Yoruba and Amharic news posts, 97.14% on Kinyarwanda news.
- General LLMs over-classified Reddit posts as English (73-97%) even in language-specific subreddits.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Professional editorial standards in news media produce language data that is significantly more detectable by current NLP tools than user-generated conversational content.
- Mechanism: News organizations employ standardized orthography, consistent vocabulary, and monolingual framing, reducing lexical ambiguity that detection models must resolve.
- Core assumption: Clean, standardized text is the appropriate training signal for these languages, rather than naturally occurring mixed-language discourse.
- Evidence anchors:
  - [abstract] "local news media offered a robust source of clean, monolingual language data... Language detection models... performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts"
  - [section 5.2.1] AfroLID achieved 100% accuracy on Yoruba and Amharic news posts, 97.14% on Kinyarwanda news
  - [corpus] AfroXLMR-Social paper notes domain bias in pre-trained models toward religious/formal text, impacting social media performance
- Break condition: If deployment target requires processing informal user-generated content, news-trained models may underperform despite high benchmark scores.

### Mechanism 2
- Claim: Code-switching behavior in African digital spaces is responsive to the initial language framing of the conversation context.
- Mechanism: When content is initiated in a local language (e.g., news posts), respondents match that language; when initiated in English or mixed contexts, code-switching dominates.
- Core assumption: The initiating language is the primary signal users follow, rather than audience composition or platform norms.
- Evidence anchors:
  - [section 5.1] "When a post is initiated in a local African language on platforms like Facebook, it elicits a significant percentage of responses in the same local language"
  - [section 4.1.1] Reddit subreddits showed English dominance with translation requests rather than authentic discourse
  - [corpus] Related work weak on this specific mechanism; corpus evidence limited
- Break condition: If platform algorithms suppress local-language content distribution, even properly-framed initiation may fail to generate responses.

### Mechanism 3
- Claim: General-purpose multilingual LLMs default to English classification when encountering code-switched African language text, inflating English detection rates.
- Mechanism: LLMs trained on English-dominant corpora may lack sufficient representation of African languages to disambiguate mixed-language input, defaulting to the highest-prior language.
- Core assumption: The high English classification rate represents misclassification rather than accurate detection of English-dominant code-switching.
- Evidence anchors:
  - [section 5.2.2] Llama 3.3 70B identified 73-97% of Reddit posts as English across subreddits explicitly dedicated to Yoruba, Amharic, and Kinyarwanda
  - [section 5.2.2] Same LLM achieved 100% accuracy on clean news data, demonstrating capability when text is monolingual
  - [corpus] INJONGO dataset paper highlights Western-centric concept bias in existing benchmarks
- Break condition: If code-switched text genuinely contains majority English tokens by linguistic measure, LLM classification may be accurate rather than biased.

## Foundational Learning

- Concept: Code-switching
  - Why needed here: Central to the paper's finding that models fail on real-world African digital communication; understanding it as systematic linguistic behavior rather than noise is critical for model design.
  - Quick check question: Can you distinguish code-switching (alternating between languages within a conversation/utterance) from borrowing (incorporating loanwords into a single language)?

- Concept: Language Identification (LID)
  - Why needed here: The primary evaluation task in the paper; understanding that LID is a classification task with confidence thresholds helps interpret accuracy metrics.
  - Quick check question: Why would a specialized tool (AfroLID) and a general LLM (Llama) produce different language classifications for the same text?

- Concept: Domain Shift / Domain Adaptation
  - Why needed here: The performance gap between news and Reddit data exemplifies domain shift; models trained on formal text underperform on informal text.
  - Quick check question: If you train on news articles and deploy on social media comments, what linguistic features might differ between training and inference?

## Architecture Onboarding

- Component map:
  Data collection layer: PRAW (Reddit scraping) + news site scrapers → raw text corpus
  Detection layer: AfroLID (specialized African LID, 517 languages) + Llama 3.3 70B (general multilingual LLM) running in parallel
  Annotation layer: Native speaker manual topic classification (Business, Education, Sports, etc.)
  Evaluation: Accuracy comparison across source types (Reddit vs. news) and detection tools

- Critical path: Define target languages → scrape Reddit subreddits + news sources → run both detection tools → manually annotate subset → compare accuracy by source type and tool

- Design tradeoffs:
  AfroLID: High precision on African languages, no English support → requires complementary tool for English/mixed text
  General LLM: Handles English and code-switching, but may over-classify as English → better recall for non-African languages, lower precision on African-only text
  News data: High-quality, monolingual, but not representative of conversational usage
  Reddit data: Authentic user-generated, but sparse and heavily code-switched

- Failure signatures:
  Very low post counts from subreddit scraping (184, 165, 833 posts/year) → platform may not have sufficient community activity for statistical analysis
  >90% English classification on language-specific subreddits → likely code-switching or model limitation, not data collection error
  AfroLID detecting non-target languages (Swahili, Wolof, Igbo) in target-language subreddits → multilingual community presence, not false positives

- First 3 experiments:
  1. Replicate the dual-tool detection pipeline: Run AfroLID and a multilingual LLM on a small sample of manually-labeled code-switched text to quantify the English-overclassification effect.
  2. Source variation test: Collect parallel samples from news Facebook comments vs. news article text for the same outlet; measure detection accuracy difference to test whether user responses maintain monolingual quality.
  3. Threshold calibration: If LLM provides confidence scores, identify optimal thresholds for African vs. English classification to reduce false English classifications without sacrificing true positives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be specifically fine-tuned to process code-switched African text as a legitimate linguistic phenomenon rather than treating it as noise?
- Basis in paper: [explicit] The authors state in Section 5.6 that future efforts must develop models that recognize code-switching as an integral part of digital communication, moving beyond the reliance on clean, monolingual datasets.
- Why unresolved: Current tools like AfroLID and Llama achieve near-perfect accuracy on clean news data but fail to reliably identify or process the heavily mixed-language content found on conversational platforms like Reddit.
- What evidence would resolve it: The successful training and evaluation of a model that maintains high detection accuracy and semantic understanding on mixed-language (e.g., English-Yoruba) datasets without requiring pre-filtering.

### Open Question 2
- Question: Can Automatic Speech Recognition (ASR) technology applied to audio-based platforms provide more representative training data for African languages than written text scraping?
- Basis in paper: [explicit] Section 5.6 proposes leveraging audio data to capture authentic conversational nuances, intonation, and natural code-switching that are often lost or sparse in written digital text.
- Why unresolved: The study found that written social media data is often sparse or heavily Anglicized, creating a disconnect between available training data and the vibrant reality of spoken language.
- What evidence would resolve it: A comparative analysis showing that datasets derived from ASR-transcribed audio contain higher lexical diversity and more authentic grammatical structures than scraped text from platforms like Reddit.

### Open Question 3
- Question: What specific metrics and tasks must be included in Afro-centric benchmarks to effectively evaluate a model's ability to handle cultural context and transliteration?
- Basis in paper: [explicit] Section 5.6 calls for the creation of benchmarks that test not only monolingual competence but also the management of code-switching, transliteration, and context-specific cultural references.
- Why unresolved: Existing industry benchmarks often fail to capture the "dynamic and diverse linguistic realities" of African users, focusing instead on standard accuracy metrics that do not reflect local utility.
- What evidence would resolve it: The development of a benchmark suite where high performance correlates strongly with human evaluation of cultural relevance and utility in real-world African digital spaces.

## Limitations
- Limited scope of data sources, with Reddit showing low post volumes and English dominance even in language-specific subreddits.
- No robust ground-truth labeling for code-switched Reddit content to verify actual language composition.
- Implicit assumption that news text represents "clean" language without independent verification of monolingual quality.

## Confidence
- High confidence: That news media provides more consistently detectable African language content than conversational platforms like Reddit.
- Medium confidence: That code-switching behavior responds to initial language framing.
- Medium confidence: That AfroLID is more reliable than general LLMs for African language detection in monolingual contexts.
- Low confidence: That high English classification rate from LLMs represents misclassification rather than accurate detection of English-dominant code-switching.

## Next Checks
1. Manually label a representative sample of Reddit posts from each subreddit to establish actual language composition and compare against tool predictions.
2. Collect parallel samples from news Facebook posts (article text vs. user comments) to test whether user responses maintain monolingual quality.
3. Test whether language framing effects persist across platforms by comparing response languages on Facebook versus Reddit.