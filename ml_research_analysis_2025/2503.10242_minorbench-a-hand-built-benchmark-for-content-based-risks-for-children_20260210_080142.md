---
ver: rpa2
title: 'MinorBench: A hand-built benchmark for content-based risks for children'
arxiv_id: '2503.10242'
source_url: https://arxiv.org/abs/2503.10242
tags:
- children
- content
- llms
- risks
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces MinorBench, a benchmark for evaluating how
  well Large Language Models (LLMs) refuse unsafe or inappropriate content from children.
  The authors conducted a real-world case study of middle-school students using LLM
  chatbots, identifying six key content-based risks specific to minors: Danger, Sexual,
  Profanities, Hateful, Self-Harm, and Substance Use.'
---

# MinorBench: A hand-built benchmark for content-based risks for children

## Quick Facts
- arXiv ID: 2503.10242
- Source URL: https://arxiv.org/abs/2503.10242
- Reference count: 40
- Primary result: GPT-4o-mini achieved 97% refusal rate under strict safety prompts, while reasoning models showed poor safety performance

## Executive Summary
MinorBench is a benchmark designed to evaluate Large Language Models' ability to refuse unsafe or inappropriate content directed at children. The authors conducted a real-world case study with middle-school students using LLM chatbots, identifying six key content-based risks specific to minors: Danger, Sexual, Profanities, Hateful, Self-Harm, and Substance Use. The benchmark consists of 299 manually curated prompts reflecting these risks, with each model tested under four different system prompt variants. Results showed that refusal rates increased significantly with stronger safety instructions, but varied widely across models and risk categories.

## Method Summary
The study evaluates six LLMs (GPT-4o-mini, o3-mini, Gemini 2.0 Flash, Claude 3.5 Haiku, Llama 3.3 70B, R1 Distilled) using MinorBench's 299 prompts across six risk categories. Each model is tested under four system prompt variants (v1-v4) with increasing safety emphasis. Responses are classified as refusal or compliance using Claude 3.5 Haiku as an LLM judge with a specific prompt template. The primary metric is Refusal Rate, calculated as (# refused prompts / # total prompts) × 100, with 95% bootstrap confidence intervals. The benchmark prompts and judge prompt are publicly available on HuggingFace.

## Key Results
- Refusal rates increased significantly with stronger safety instructions across all models
- GPT-4o-mini achieved 97% refusal rate under the strictest prompt variant (v4)
- Reasoning models (o3-mini and R1 Distilled) performed poorly despite strict safety instructions
- Refusal rates varied substantially across different risk categories
- Stronger safety prompts improved performance but did not eliminate risks entirely

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its targeted focus on content-based risks specifically relevant to minors, combined with systematic variation of safety instructions. By using a consistent LLM judge for binary classification and varying system prompts, the study isolates the impact of explicit safety instructions on model behavior while maintaining methodological consistency across all evaluations.

## Foundational Learning
- **Content-based risk categorization**: Understanding the six risk categories (Danger, Sexual, Profanities, Hateful, Self-Harm, Substance Use) is essential for interpreting which types of content models struggle to refuse. Quick check: Verify all 299 prompts are correctly assigned to these categories.
- **System prompt engineering**: Different prompt variants (v1-v4) represent varying levels of safety emphasis. Quick check: Confirm the exact wording differences between prompt variants.
- **LLM judge methodology**: Binary classification using another LLM introduces potential bias. Quick check: Test the judge prompt on edge cases to understand its decision boundaries.

## Architecture Onboarding
- **Component map**: MinorBench prompts -> LLM API calls -> Response collection -> LLM judge classification -> Refusal rate calculation
- **Critical path**: Prompt generation → Model inference → Response classification → Metric computation
- **Design tradeoffs**: Manual prompt curation ensures quality but limits scalability; LLM judge provides consistency but may introduce systematic bias
- **Failure signatures**: Unexpected low refusal rates in reasoning models; inconsistent performance across risk categories; sensitivity to system prompt wording
- **First experiments**: 1) Test a single prompt across all models and prompt variants; 2) Compare LLM judge vs. human annotation on 10 ambiguous responses; 3) Run bootstrap analysis with different resample counts to verify confidence interval stability

## Open Questions the Paper Calls Out
None

## Limitations
- Inference parameters (temperature, top_p, max_tokens) not specified, affecting reproducibility
- Bootstrap methodology lacks detail on resample count
- LLM judge classification may not capture nuanced refusal behaviors
- API version differences could affect results across different deployment environments

## Confidence
- High confidence: Benchmark construction methodology and experimental design are clearly specified
- Medium confidence: Core finding about prompt-variant effects is robust, but exact numerical values may vary
- Medium confidence: Model performance ranking should be reproducible, though absolute refusal rates may differ

## Next Checks
1. Parameter Sensitivity Analysis: Reproduce results across different temperature settings (0.0, 0.7, 1.0) for a subset of models
2. Judge Prompt Validation: Create a validation set of 20 ambiguous responses and compare LLM judge decisions with human annotations
3. API Version Control: Test the same models across different API versions to isolate the impact of model updates on refusal performance