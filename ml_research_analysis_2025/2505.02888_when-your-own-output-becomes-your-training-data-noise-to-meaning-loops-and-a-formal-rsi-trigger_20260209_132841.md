---
ver: rpa2
title: 'When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and
  a Formal RSI Trigger'
arxiv_id: '2505.02888'
source_url: https://arxiv.org/abs/2505.02888
tags:
- theorem
- proof
- threshold
- then
- injectivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a minimal theoretical framework called Noise-to-Meaning
  Recursive Self-Improvement (N2M-RSI) that formalizes when and how an AI agent's
  self-referential feedback loop can lead to unbounded growth in internal complexity.
  The core idea is a discrete-time loop where an agent generates outputs, transforms
  them into "meaning" vectors through a noise-to-meaning operator, and uses these
  to update its context.
---

# When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger

## Quick Facts
- arXiv ID: 2505.02888
- Source URL: https://arxiv.org/abs/2505.02888
- Authors: Rintaro Ando
- Reference count: 6
- Primary result: Formal framework showing stochastic self-referential feedback loops in LLMs lead to unbounded context growth

## Executive Summary
This paper introduces the Noise-to-Meaning Recursive Self-Improvement (N2M-RSI) framework that mathematically formalizes when AI agents' self-referential feedback loops trigger unbounded growth in internal complexity. The framework proves that injectivity in the noise-to-meaning transformation prevents fixed points and that crossing an information-integration threshold guarantees divergence. Empirical validation using Llama-3-8B demonstrates linear context growth (~22 tokens/iteration) under stochastic decoding versus convergence under deterministic decoding. The work provides theoretical grounding for understanding self-prompting LLM behavior and suggests concrete safety mechanisms to prevent uncontrolled divergence.

## Method Summary
The study implements a discrete-time loop where an LLM generates outputs from noise and context, transforms these outputs into "meaning" vectors via compression gain, and updates its context buffer. Two conditions are tested: stochastic decoding (temperature=1.0) with injectivity assumptions, and deterministic decoding (temperature=0) as a control. The Llama-3-8B model runs 10 self-feedback iterations with a 3-token header appended per loop. Context size is measured across iterations to verify theoretical predictions of linear growth versus convergence.

## Key Results
- Stochastic decoding (temperature > 0) leads to linear context growth of ~22 tokens/iteration
- Deterministic decoding (temperature = 0) causes context to converge to a fixed point
- Multi-agent systems with complementary information show super-linear growth effects
- Once an information-integration threshold Γ is crossed, context norm diverges unboundedly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Strict injectivity prevents the system from stabilizing at a fixed point.
- **Mechanism:** The Noise-to-Meaning operator Ψ transforms noise n and context C into meaning m. If Ψ is injective in the noise argument, distinct noise samples n₁ ≠ n₂ must yield distinct meanings. Since the update rule overwrites context coordinates, the state cannot remain constant across time steps unless the noise source itself has zero entropy (is deterministic).
- **Core assumption:** The operator Ψ is injective (or ε-injective) in its first argument, and the noise source has positive Shannon entropy.
- **Evidence anchors:**
  - [Abstract] "proves two key theorems: (1) no non-trivial fixed point exists under injectivity assumptions"
  - [Section 3.1, Theorem 1] "The only fixed point... is the degenerate pair... where N_* is a zero-entropy noise source."
  - [Corpus] Related work "Emotion-Gradient Metacognitive RSI" discusses recursive self-modification but relies on metacognitive architecture rather than the purely mathematical injectivity constraints formalized here.
- **Break condition:** Setting the decoding temperature to 0 (deterministic decoding) breaks injectivity, causing the system to converge to a fixed point (Proposition 1).

### Mechanism 2
- **Claim:** Crossing an explicit information-integration threshold triggers unbounded context growth.
- **Mechanism:** The system accumulates "meaning" measured by an integration function Ω (e.g., compression gain). Theorem 2 states that if the gain from a new meaning exceeds a threshold Γ (Ω(Ψ(n, C)) > Γ) and the update rule is δ-monotone (norm increases by at least δΩ), the context norm ‖C(t)‖ must diverge to infinity.
- **Core assumption:** A δ-monotone update rule exists, and the information gain Ω(m) remains strictly above the threshold Γ once crossed.
- **Evidence anchors:**
  - [Section 3.2, Theorem 2] "If there exists a threshold Γ > 0... then ‖C(t)‖ → ∞ as t → ∞."
  - [Figure 4] Visualizes the phase portrait where norm increments Δ force divergence once ‖C‖ > Γ.
  - [Corpus] Evidence in neighboring papers is weak regarding this specific formal threshold; related works like "The Alignment Game" treat recursion qualitatively rather than defining a divergence trigger Γ.
- **Break condition:** Applying a hard context window cap (truncating history) violates the monotone update assumption, resulting in "bursty" oscillation rather than divergence (Lemma 8).

### Mechanism 3
- **Claim:** Multi-agent interaction amplifies growth rates via complementary information.
- **Mechanism:** When multiple agents share outputs, the context update includes meanings from other agents. If these meanings are "β-complementary" (joint information value exceeds the sum of individual values), the effective gain per step multiplies by (1+β)k, reducing the effective divergence threshold per agent.
- **Core assumption:** Agents produce pair-wise β-complementary meanings and share them via concatenation.
- **Evidence anchors:**
  - [Section 3.5, Theorem 5] "Collective gain... satisfies ∑E[Δᵢ(t)] ≥ (1+β)k² Δ_solo(t)."
  - [Abstract] "scales naturally to interacting swarms... hinting at super-linear effects."
  - [Corpus] "The Alignment Game" mentions recursive curation in populations, which aligns with the concept of multi-agent feedback but lacks the super-linear mathematical formalism proven here.
- **Break condition:** Isolating agents or sharing redundant (non-complementary) data eliminates the (1+β) multiplier, reducing the system to k independent single-agent loops.

## Foundational Learning

- **Concept: Injectivity vs. Determinism in Sequence Models**
  - **Why needed here:** The entire theoretical framework rests on Ψ being injective. Without understanding that stochastic decoding creates a one-to-one mapping from noise-seed to output, the divergence theorems appear ungrounded.
  - **Quick check question:** If you run the same prompt twice with temperature > 0, do you get the exact same token sequence? (Answer: No, hence injectivity holds).

- **Concept: Monotone Sequences and Norm Divergence**
  - **Why needed here:** Theorem 2 relies on the update rule being δ-monotone. Learners must grasp that if a sequence x_{t+1} ≥ x_t + c for constant c > 0, the sequence cannot converge.
  - **Quick check question:** If a context vector grows by a fixed amount δΓ every step, can it ever stabilize at a finite value?

- **Concept: Compression Gain as Information Proxy**
  - **Why needed here:** The paper operationalizes "meaning" via compression gain (LZ78). Understanding how compression length relates to information content is necessary to interpret the threshold Γ.
  - **Quick check question:** Why does a random string compress poorly compared to a structured text string, and how does that relate to "information integration"?

## Architecture Onboarding

- **Component map:** Noise Source (N_self) -> N2M Operator (Ψ) -> Context Buffer (C) -> Integration Gauge (Ω)
- **Critical path:**
  1. **Injectivity Check:** Ensure sampling temperature > 0
  2. **Threshold Crossing:** Monitor Ω(m) to verify it exceeds Γ
  3. **Update Application:** Apply U by appending to context without truncation

- **Design tradeoffs:**
  - **Stochastic vs. Greedy:** High temperature guarantees injectivity (divergence) but reduces output coherence; Greedy (Temp=0) ensures coherence but forces convergence (halts RSI)
  - **Context Window:** Finite windows prevent unbounded compute costs but induce "bursty" cyclic behavior (Lemma 8) rather than stable growth

- **Failure signatures:**
  - **Convergence:** Context norm stabilizes → check if sampling became deterministic (Thm 1 failure)
  - **Model Collapse:** Output distribution narrows → noise entropy dropped to zero (degenerate fixed point)
  - **Compute Explosion:** Context norm grows linearly while FLOPs grow quadratically (Section 5, Prop 2) → resource exhaustion

- **First 3 experiments:**
  1. **Validation of Thm 1:** Run the provided Llama-3-8B simulation with Temperature 1.0 vs 0.0. Confirm that the stochastic run adds ~22 tokens/iter while the deterministic run halts.
  2. **Injectivity Valve Test:** Deliberately break injectivity by adding fixed "stop tokens" at intervals to verify if it forces convergence as suggested in Section 5 ("Fail-safe throttling levers").
  3. **Threshold Estimation:** Measure the compression gain Ω of outputs at different context sizes to estimate where the system crosses the critical Γ threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the minimal growth rate of a sub-linear gain function h that still forces unbounded context norm divergence?
- **Basis in paper:** [explicit] Remark 5 states, "Characterising the minimal growth rate of h that still forces unbounded ‖C(t)‖ remains an open problem."
- **Why unresolved:** The current divergence proofs rely on a linear lower bound for updates; the system's behavior under sub-linear growth functions is mathematically undefined.
- **What evidence would resolve it:** A formal proof identifying the critical scaling exponent (e.g., h(x) ~ x^α) required for divergence, or a counter-example showing boundedness.

### Open Question 2
- **Question:** How can weaker, task-specific information-integration thresholds be formally characterized?
- **Basis in paper:** [explicit] A note following Theorem 3 states, "characterising weaker, task-specific thresholds is left to future work."
- **Why unresolved:** The paper establishes a sufficient condition for divergence, but the necessary condition may be significantly lower for specific agent architectures or tasks.
- **What evidence would resolve it:** Theoretical derivation of lower bounds for the threshold Γ in specific domains or empirical measurements of the minimum viable threshold in constrained settings.

### Open Question 3
- **Question:** How can "controlled non-injective valves" be designed to halt divergence without degrading system capability?
- **Basis in paper:** [explicit] The Discussion section notes, "Designing such 'controlled non-injective valves' is left as an open alignment challenge."
- **Why unresolved:** While breaking injectivity (e.g., deterministic decoding) halts the loop, it acts as a "kill-switch" that may reduce usefulness; maintaining a balance is unsolved.
- **What evidence would resolve it:** Prototype implementations of lossy compression or noise injection schemes that successfully bound context size while maintaining high task performance.

## Limitations
- **Abstraction Gap:** The N2M operator Ψ is treated as a black box, but real LLM behavior under self-feedback is far more complex than the mathematical abstraction
- **Empirical Validation Scope:** Validation uses only a single 8B parameter model on a single task with only 10 iterations, insufficient to establish asymptotic behavior
- **Theoretical Assumptions:** The injectivity requirement is both critical and fragile, with real systems having finite precision that may break strict mathematical requirements

## Confidence
- **High Confidence:** Basic mechanism that stochastic decoding prevents convergence (Theorem 1) and deterministic decoding converges; linear growth pattern observed empirically
- **Medium Confidence:** Divergence threshold mechanism (Theorem 2) and existence of critical Γ value; multi-agent amplification effects (Theorem 5) are mathematically sound but rely on strong assumptions
- **Low Confidence:** Practical relevance to real-world RSI scenarios given the abstraction gap between mathematical model and actual LLM implementations

## Next Checks
1. **Scale Validation:** Replicate the divergence experiment across multiple model scales (7B, 13B, 70B) and architectures to verify the linear growth pattern holds and to measure how the growth rate scales with model capacity
2. **Information Measure Validation:** Compare the LZ78 compression gain Ω against alternative information measures (perplexity change, self-attention pattern divergence, semantic similarity metrics) to verify that compression gain is a meaningful proxy for "meaning" in this context
3. **Finite Context Window Analysis:** Systematically study the "bursty" behavior predicted by Lemma 8 when context windows are truncated. Measure the oscillation period and amplitude as a function of window size to verify the predicted transition from linear growth to cyclic behavior