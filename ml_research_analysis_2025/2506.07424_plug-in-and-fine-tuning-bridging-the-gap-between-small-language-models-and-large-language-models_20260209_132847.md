---
ver: rpa2
title: 'Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and
  Large Language Models'
arxiv_id: '2506.07424'
source_url: https://arxiv.org/abs/2506.07424
tags:
- pifi
- performance
- llama-3
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PiFi integrates a frozen LLM layer into a small language model
  (SLM) and fine-tunes the combined model, achieving strong performance improvements
  while maintaining efficiency. On NLU tasks, PiFi consistently outperforms vanilla
  fine-tuning across multiple datasets (e.g., up to 2.3% accuracy gain on average)
  and architectures.
---

# Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models

## Quick Facts
- **arXiv ID:** 2506.07424
- **Source URL:** https://arxiv.org/abs/2506.07424
- **Reference count:** 40
- **Primary result:** PiFi consistently improves SLM performance by 2.3% accuracy on average, with >13% gains in domain shift settings

## Executive Summary
This paper introduces PiFi (Plug-in and Fine-tuning), a method that bridges the performance gap between small language models (SLMs) and large language models (LLMs) by integrating a frozen LLM layer into an SLM. The approach extracts the last layer from a pre-trained LLM, inserts it between the SLM encoder and task head, and fine-tunes only the SLM and projection layers. This preserves the LLM's pre-trained knowledge while adapting to downstream tasks, achieving strong performance improvements across multiple NLU and NLG benchmarks with minimal computational overhead.

## Method Summary
PiFi works by extracting the last transformer layer from a frozen LLM and inserting it between the SLM encoder and task-specific head. Linear projection layers bridge the dimensionality gap between SLM and LLM hidden states. During fine-tuning, only the SLM, projection layers, and task head are updated while the LLM layer remains frozen. This approach leverages the LLM's rich representations without the computational cost of full fine-tuning. The method uses Adam optimizer (LR=5e-5, BS=32, 3 epochs) and has been validated across various architectures (BERT, RoBERTa, T5, BART) and tasks.

## Key Results
- PiFi achieves up to 2.3% average accuracy gains on NLU tasks compared to vanilla fine-tuning
- On domain shift settings, PiFi improves generalization by over 13% on unseen domains
- Language alignment between LLM and task language enhances performance (e.g., +1.57% on Korean NSMC with Korean Llama-3)
- Larger LLMs (70B) don't necessarily yield better results than smaller ones (8B), suggesting knowledge density per layer matters

## Why This Works (Mechanism)

### Mechanism 1
Freezing the LLM layer preserves pre-trained knowledge better than fine-tuning it. The frozen layer acts as a fixed feature extractor while the SLM adapts to the task, avoiding catastrophic forgetting. Evidence: frozen Llama-3 outperforms fully fine-tuned Llama on SST-2 (91.50 vs 91.27 accuracy).

### Mechanism 2
The last LLM layer provides the most transferable knowledge for downstream tasks. Upper layers capture more contextual and semantic information through attention. Evidence: ablation study shows last layer yields best performance across SST-2 and Multi30K tasks.

### Mechanism 3
Language alignment between LLM and target task amplifies gains. The LLM's pre-training language determines which linguistic patterns are encoded. Evidence: Korean Llama-3 improves Korean NSMC by +1.57% over English Llama-3.

## Foundational Learning

- **Concept: Transformer attention and layer hierarchy**
  - Why needed here: PiFi depends on understanding what different layers encode
  - Quick check: Can you explain why deeper transformer layers typically capture more abstract semantic information than earlier layers?

- **Concept: Transfer learning and catastrophic forgetting**
  - Why needed here: The freezing strategy is justified by avoiding forgetting
  - Quick check: What happens when you fine-tune all parameters of a pre-trained model on a small, narrow dataset?

- **Concept: Dimensionality projection (linear layers)**
  - Why needed here: Lin and Lout bridge mismatched hidden dimensions
  - Quick check: If your SLM outputs 768-dim vectors and your LLM layer expects 4096-dim inputs, what shape must Lin's weight matrix be?

## Architecture Onboarding

- **Component map:** Enc (SLM encoder) -> h_enc (CLS token) -> L_in (projection) -> L_LLM (frozen) -> L_out (projection) -> Head

- **Critical path:**
  1. Extract last layer weights from target LLM
  2. Initialize L_in and L_out with correct dimensions
  3. Insert L_LLM between encoder and head
  4. Freeze L_LLM; train SLM, projections, and head

- **Design tradeoffs:**
  - Layer selection: Last layer works best generally, but task-specific tuning may find alternatives
  - LLM size: Larger LLMs don't guarantee better results—70B Llama performs comparably to 8B
  - Instruction-tuned vs. base LLM: Marginal difference; not worth prioritizing

- **Failure signatures:**
  - Performance barely exceeds baseline → Check if L_LLM is actually frozen
  - Dimension mismatch errors → Confirm L_in/L_out shapes match SLM and LLM hidden sizes
  - Domain shift performance drops → Verify language alignment
  - Training instability → Reduce learning rate for projection layers

- **First 3 experiments:**
  1. **Baseline validation:** Fine-tune BERT-base on SST-2 vanilla vs. +PiFi(Llama-3.1-8B). Expect ~2% accuracy gain.
  2. **Layer position probe:** Compare last vs. middle vs. early layer extraction on same task. Expect last layer to win.
  3. **Generalization test:** Train on IMDB, test on Tweet sentiment. Expect ~13% gain over vanilla.

## Open Questions the Paper Calls Out

1. **Layer selection automation:** How can the optimal layer position within an LLM be automatically selected for PiFi integration to maximize performance on specific downstream tasks? The authors suggest methods like Neural Architecture Search could address this limitation.

2. **Complex benchmark performance:** Does PiFi retain its performance advantages when applied to complex, reasoning-heavy benchmarks like MMLU? The paper notes experiments focused on "relatively straightforward tasks" without evaluating on complex benchmarks.

3. **Multi-layer combinations:** Can identifying specific combinations of multiple LLM layers effectively mitigate the "layer knowledge density" issue observed in very large models (e.g., 70B parameters)? The authors hypothesize single layers in 70B models have lower knowledge density than in 8B models.

## Limitations

- **Limited generalizability across tasks:** Strong performance on English NLU benchmarks may not extend to specialized domains or different language families beyond Korean and German.
- **Layer selection assumption:** While last layer generally performs best, this may not hold for all architectures or tasks, particularly decoder-based models.
- **Computational efficiency concerns:** Actual memory and computation overhead during training isn't thoroughly quantified despite claims of efficiency benefits.

## Confidence

**High confidence** in: The core architecture (freezing LLM layer + projection layers) works as described and improves SLM performance on tested benchmarks.

**Medium confidence** in: The claim that freezing preserves knowledge better than fine-tuning, based on narrow ablation comparisons.

**Low confidence** in: The universal applicability of last-layer selection and the efficiency claims, which require broader validation.

## Next Checks

1. **Cross-linguistic robustness:** Test PiFi with LLMs trained on non-English languages (e.g., Chinese, Arabic) on corresponding downstream tasks to verify language alignment benefits generalize.

2. **Layer generalization experiment:** Systematically test PiFi with layers other than the last (e.g., middle, early) across multiple task types to validate or challenge the universal last-layer claim.

3. **Efficiency benchmarking:** Measure and report actual GPU memory usage, training time per epoch, and inference latency for PiFi versus baseline SLM fine-tuning across different model sizes.