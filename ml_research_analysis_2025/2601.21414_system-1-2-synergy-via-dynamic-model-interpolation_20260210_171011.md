---
ver: rpa2
title: System 1&2 Synergy via Dynamic Model Interpolation
arxiv_id: '2601.21414'
source_url: https://arxiv.org/abs/2601.21414
tags:
- reasoning
- wang
- thinking
- accuracy
- interpolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a shift from output control to capability control
  for efficient reasoning in language models. Instead of constraining what models
  produce, it modulates how they think through dynamic parameter interpolation between
  System 1 (Instruct) and System 2 (Thinking) checkpoints.
---

# System 1&2 Synergy via Dynamic Model Interpolation

## Quick Facts
- arXiv ID: 2601.21414
- Source URL: https://arxiv.org/abs/2601.21414
- Reference count: 31
- Key outcome: DAMI achieves 1.6–3.4% accuracy improvement while reducing token consumption by 29–40% compared to the Thinking model

## Executive Summary
This paper proposes a shift from output control to capability control for efficient reasoning in language models. Instead of constraining what models produce, it modulates how they think through dynamic parameter interpolation between System 1 (Instruct) and System 2 (Thinking) checkpoints. The DAMI framework estimates a query-specific Reasoning Intensity λ(q) to configure cognitive depth, using either preference learning (DAMI-Pref) or confidence-based inference (DAMI-Conf). Experiments on five mathematical reasoning benchmarks show DAMI achieves 1.6–3.4% accuracy improvement while reducing token consumption by 29–40% compared to the Thinking model, effectively combining System 1 efficiency with System 2 reasoning depth.

## Method Summary
The method interpolates parameters between Instruct (System 1) and Thinking (System 2) checkpoints using Θ(M)(q) = λ(q)Θ(T) + (1-λ(q))Θ(I). Two approaches estimate λ(q): DAMI-Pref trains a reward model on preference pairs ranking accuracy first, cost second; DAMI-Conf computes λ from confidence signals (Holistic Ambiguity + Cognitive Discrepancy) calibrated via sigmoid. The merged model generates responses with configured cognitive depth. DAMI-Pref achieves higher accuracy (91.6%) while DAMI-Conf offers zero-shot deployment with 56% token reduction.

## Key Results
- DAMI improves accuracy by 1.6–3.4% compared to Thinking model alone
- DAMI reduces token consumption by 29–40% while maintaining or improving accuracy
- DAMI-Pref outperforms DAMI-Pred with less data (1K samples sufficient)
- DAMI-Conf achieves 56% token reduction with zero-shot deployment

## Why This Works (Mechanism)

### Mechanism 1: Linear Mode Connectivity Enables Smooth Interpolation
- Claim: Parameter interpolation between Instruct and Thinking checkpoints produces predictable, well-behaved intermediate models
- Core assumption: Both models originate from the same pre-trained checkpoint
- Evidence: Cosine similarity exceeds 0.99 across all layers; linear interpolation yields convex, monotonic Pareto frontier
- Break condition: Models from different base checkpoints may violate LMC, causing degraded outputs

### Mechanism 2: Reasoning Intensity λ(q) Controls Cognitive Depth
- Claim: Query-specific interpolation coefficient λ(q) reliably modulates reasoning depth
- Core assumption: Performance monotonicity holds—accuracy increases with λ
- Evidence: Increasing λ from 0 to 0.9 yields smooth accuracy improvement and predictable token increase
- Break condition: Non-monotonic accuracy curves would indicate λ is not a reliable control knob

### Mechanism 3: Confidence Discrepancy Signals Query Difficulty
- Claim: Inter-model confidence differences reveal query difficulty without ground-truth labels
- Core assumption: Confidence extracted from short answer sequences correlates with reasoning requirements
- Evidence: Ablation shows removing Cognitive Discrepancy causes 3.7% accuracy drop
- Break condition: Domains with poorly calibrated confidence may produce misleading λ estimates

## Foundational Learning

- **Linear Mode Connectivity**: Explains why interpolation works at all—without LMC, merged models would produce incoherent outputs
  - Quick check: If you interpolate between two randomly initialized models instead of fine-tuned ones, would you expect smooth performance? Why or why not?

- **Pareto Frontier**: Framework for evaluating accuracy-efficiency tradeoffs; DAMI's claim is establishing a superior frontier
  - Quick check: If a method achieves 95% accuracy with 1000 tokens and another achieves 90% with 500 tokens, which dominates on the Pareto frontier?

- **Preference Learning vs. Regression**: Explains why DAMI-Pref outperforms DAMI-Pred with less data—pairwise comparisons are noise-robust
  - Quick check: Why might pointwise λ* labels be noisier than pairwise preferences P(q, l_c) ≻ P(q, l_r)?

## Architecture Onboarding

- **Component map**: Query → λ Estimator (DAMI-Pref/R or DAMI-Conf) → Parameter Merger (Θ(M) = λΘ(T) + (1-λ)Θ(I)) → Generator → Response

- **Critical path**: 1) Receive query q, 2) Extract λ(q) via chosen estimator, 3) Merge parameters (on-demand ~5.3s overhead), 4) Generate response with Θ(M)(q)

- **Design tradeoffs**: DAMI-Pref: higher accuracy (91.6%), requires training data (~1K samples), lower latency overhead. DAMI-Conf: zero-shot deployment, higher efficiency (56% token reduction), model-agnostic. Pre-loading both checkpoints: ~1s merge overhead but 2× memory footprint.

- **Failure signatures**: λ consistently near 0 on hard problems → DAMI-Conf underestimates difficulty. λ consistently near 1 on easy problems → DAMI-Pref overfits. Interpolated model produces incoherent output → models may not share base checkpoint (LMC violated).

- **First 3 experiments**: 1) Validate LMC on your model pair: compute cosine similarity across layers; if <0.95, interpolation may fail. 2) Profile λ sweep on held-out validation set: plot accuracy vs. tokens for λ ∈ {0.0, 0.1, ..., 1.0} to confirm monotonic Pareto frontier. 3) Compare DAMI-Conf vs. DAMI-Pref on your domain: if <500 training samples available, DAMI-Conf is likely preferable; otherwise train reward model and evaluate on OOD data.

## Open Questions the Paper Calls Out

- **Cross-model scalability**: Does the convex, monotonic Pareto frontier hold across larger model scales (e.g., 70B+ parameters) and different architectures such as mixture-of-experts models?

- **Confidence reliability**: Under what conditions does DAMI-Conf's confidence-based λ estimation produce systematically incorrect predictions, particularly for queries where model confidence is misaligned with actual correctness?

- **Cross-family interpolation**: Can the DAMI framework be extended to interpolate between models from different families (e.g., Qwen and DeepSeek) that do not share a common pre-trained checkpoint?

## Limitations
- Limited to mathematical reasoning benchmarks; generalization to other domains unverified
- Efficiency claims depend on pre-loading feasibility; on-demand merging introduces 5.3s overhead
- Cross-model generalization requires further validation beyond Qwen-based architectures

## Confidence
- **High confidence**: Linear Mode Connectivity as enabling mechanism (directly measured with >0.99 cosine similarity)
- **Medium confidence**: λ(q) as reliable control knob (monotonic Pareto frontier observed but non-monotonic cases exist)
- **Medium confidence**: DAMI-Conf as zero-shot alternative (works on mathematical domains but confidence calibration unverified elsewhere)

## Next Checks
1. Test LMC and interpolation quality on a different model pair (e.g., Llama-3.1-8B-Instruct/Thinking) to verify claims generalize beyond Qwen family
2. Evaluate DAMI-Conf on a non-mathematical benchmark (e.g., MultiHopQA or StrategyQA) to assess confidence signal reliability outside mathematical reasoning
3. Measure end-to-end latency including merge overhead for DAMI-Pref vs DAMI-Conf across different deployment scenarios to validate efficiency claims