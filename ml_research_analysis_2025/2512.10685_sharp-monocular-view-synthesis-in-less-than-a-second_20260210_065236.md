---
ver: rpa2
title: Sharp Monocular View Synthesis in Less Than a Second
arxiv_id: '2512.10685'
source_url: https://arxiv.org/abs/2512.10685
tags:
- depth
- view
- image
- synthesis
- sharp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARP is a feed-forward approach to photorealistic view synthesis
  from a single image. It produces a 3D Gaussian representation in under a second
  via a single forward pass through a neural network, enabling real-time rendering
  of nearby views.
---

# Sharp Monocular View Synthesis in Less Than a Second

## Quick Facts
- arXiv ID: 2512.10685
- Source URL: https://arxiv.org/abs/2512.10685
- Authors: Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, AmaÃ«l Delaunoy, Tian Fang, Yanghai Tsin, Stephan R. Richter, Vladlen Koltun
- Reference count: 40
- Primary result: SHARP achieves state-of-the-art photorealistic view synthesis in under one second via feed-forward 3D Gaussian prediction

## Executive Summary
SHARP introduces a feed-forward neural network approach for photorealistic view synthesis from a single image, producing a 3D Gaussian representation in under a second through a single forward pass. The method leverages a modified Depth Pro backbone, depth adjustment, and Gaussian refinement modules, trained end-to-end with carefully designed loss functions. Experiments demonstrate SHARP reduces LPIPS by 25-34% and DISTS by 21-43% compared to the best prior model while being three orders of magnitude faster, excelling at synthesizing sharp, high-resolution images from nearby viewpoints.

## Method Summary
SHARP predicts a 3D Gaussian representation directly from a single image through a feed-forward neural network. The system uses a modified Depth Pro encoder to extract multi-scale features, which are processed by a depth decoder to produce a two-channel depth map. These depth predictions are unprojected into 3D Gaussians along with color information from the input image. A Gaussian decoder then refines all Gaussian attributes (position, scale, rotation, color, opacity) through residual predictions. The depth adjustment module, used only during training, resolves scale ambiguity in monocular depth estimation. The resulting 3D Gaussian representation can be rendered in real-time for interactive browsing of novel views.

## Key Results
- Achieves state-of-the-art image fidelity with 25-34% reduction in LPIPS and 21-43% reduction in DISTS compared to best prior models
- Produces 3D Gaussian representations in under one second through single forward pass
- Maintains real-time rendering speeds (>100 FPS) for interactive view synthesis
- Demonstrates superior performance on multiple datasets including RealEstate10K, MipMapFlythru, and 3DFront

## Why This Works (Mechanism)

### Mechanism 1: Layered Depth Unprojection for Disocclusion Handling
The depth decoder outputs a two-channel depth map, where the first layer represents visible surfaces and the second captures background content revealed by view shifts. This allows the model to hallucinate plausible geometry for occluded regions, preventing stretching artifacts during novel view synthesis. The core assumption is that disoccluded regions can be reasonably approximated by learned inpainting priors rather than requiring completely generated content.

### Mechanism 2: Residual Gaussian Attribute Refinement
After initial unprojection, a Gaussian decoder predicts residual deltas for all 14 attributes (position, scale, rotation, color, opacity). This refinement process corrects geometric noise from the depth estimator while maintaining structural coherence. The core assumption is that metric monocular depth provides a sufficiently accurate geometric scaffold such that residual corrections are within a learnable local range.

### Mechanism 3: Depth Scale Ambiguity Resolution via Learnable Adjustment
During training, a U-Net takes predicted inverse depth and ground truth inverse depth to output a scale map that adjusts the predicted depth before Gaussian initialization. This acts as an information bottleneck that resolves the inherent scale ambiguity of monocular depth, aligning predictions with ground truth for effective supervision. The core assumption is that the primary error mode of the depth backbone is scale-related rather than structural hallucination.

## Foundational Learning

- **3D Gaussian Splatting (3DGS)**: Understanding how 3D Gaussians are parameterized (mean, covariance, opacity, SH coefficients) and rasterized (tile-based sorting) is essential to interpret the network's output heads. Quick check: How does the rasterizer handle overlapping Gaussians to produce a pixel color?

- **Metric Monocular Depth Estimation**: The system relies on a Depth Pro backbone to provide metric geometry. Understanding the difference between relative and metric depth is crucial for the "Depth Adjustment" and "Metric Scale" claims. Quick check: Why is scale ambiguity a specific problem for view synthesis but less so for standard 2D depth visualization?

- **Perceptual Loss Functions (LPIPS/DISTS/Gram Matrix)**: The paper relies heavily on perceptual metrics to claim SOTA. The training uses a specific Gram matrix loss to boost sharpness. Quick check: Why would an L1 or L2 loss alone fail to produce "sharp" photorealistic results in a regression task?

## Architecture Onboarding

- **Component map**: Encoder (modified Depth Pro) -> Depth Decoder (DPT-based) -> Depth Adjustment (U-Net, train only) -> Gaussian Initializer (non-learnable) -> Gaussian Decoder (DPT-based) -> Composer -> Renderer

- **Critical path**: The flow runs $I \to \text{Encoder} \to \text{DepthDecoder} \to \text{Unprojection} \to \text{GaussDecoder} \to \text{Renderer}$. The "Depth Adjustment" is a side network strictly for training gradient alignment.

- **Design tradeoffs**: The model uses 2 depth layers ($\approx 1.2$M Gaussians) to keep inference under 1 second, sacrificing wider baseline handling. Spherical harmonics are omitted to reduce output size, limiting view-dependent effects representation.

- **Failure signatures**: Macro/DOF effects may place out-of-focus objects in foreground; reflections are interpreted as physical geometry causing scene breaks; texture-rich distant surfaces may be interpreted as curved 3D surfaces rather than flat backgrounds.

- **First 3 experiments**:
  1. Run inference latency test: Process a 1536x1536 image through forward pass and verify output is renderable at >100 FPS
  2. Ablate depth adjustment: Disable adjustment during training and confirm drop in sharpness/DISTS score
  3. SSFT validation: Compare Stage 1 (synthetic only) vs. Stage 2 (real-image fine-tuning) outputs on real photos for domain gap assessment

## Open Questions the Paper Calls Out

### Open Question 1
How can monocular view synthesis be extended to support photorealistic synthesis of faraway views (large camera motion) without compromising nearby-view fidelity or sub-second synthesis speed? The paper suggests judicious integration of diffusion models may be required, as current regression-based methods degrade at large baselines while diffusion methods are slow and alter content.

### Open Question 2
How can view-dependent and volumetric effects (specular reflections, transparency) be modeled in a principled manner within the 3D Gaussian representation framework? The current method uses only RGB color per Gaussian without spherical harmonics, explicitly omitting view-dependent effects to control output size.

### Open Question 3
What is the optimal integration strategy between feed-forward 3D Gaussian prediction and diffusion-based priors to combine their complementary strengths? The paper identifies this as an interesting research direction to combine diffusion's synthesis of faraway content with feed-forward models' interactive 3D generation.

## Limitations

- **Disocclusion fidelity**: The 2-layer depth strategy may fail for wide baseline translations or when revealed regions contain content entirely absent from the source view
- **Reflection and mirror handling**: Depth models interpreting reflections as physical surfaces cause catastrophic failures that cannot be corrected even with end-to-end fine-tuning
- **Real-world scalability**: Performance on arbitrary consumer photos with unknown intrinsics/extrinsics is assumed rather than proven, with domain shift indicating potential degradation

## Confidence

- **High confidence**: Latency claims (under 1 second feed-forward) and comparative LPIPS/DISTS improvements are well-supported by experimental setup
- **Medium confidence**: Mechanism explanations are logically sound but rely on qualitative interpretations rather than direct ablation studies
- **Low confidence**: Claims about robustness to arbitrary consumer photo collections are overstated given training data bias toward structured multi-view captures

## Next Checks

1. **Baseline ablation for 2-layer depth**: Train identical model with 1-channel depth decoder and measure degradation specifically in disocclusion regions across multiple datasets

2. **Real photo stress test**: Collect 50 consumer photos with varied content (indoor/outdoor, with/without mirrors, macro/micro scenes) and measure both perceptual metrics and human preference rates compared to NeRF/IBR baselines

3. **Reflection sensitivity analysis**: Create controlled test set of scenes with known mirror/reflection geometry and compare depth estimation errors and novel view synthesis quality against ground truth baseline to quantify failure rate