---
ver: rpa2
title: 'Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs'
arxiv_id: '2510.09970'
source_url: https://arxiv.org/abs/2510.09970
tags:
- fallacy
- classification
- logical
- llms
- fallacies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving logical fallacy
  classification in large language models (LLMs), which often struggle with accurate
  reasoning due to their default fast, intuitive processing. To bridge this gap, the
  authors propose a structured approach that decomposes complex fallacy classification
  into a series of atomic binary questions.
---

# Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs

## Quick Facts
- **arXiv ID**: 2510.09970
- **Source URL**: https://arxiv.org/abs/2510.09970
- **Reference count**: 15
- **Primary result**: Stepwise atomic instructions + Prolog relational graphs improved Claude-Sonnet-4 accuracy from 42.2% to 62.9% on 232 fallacy classifications.

## Executive Summary
This paper addresses the challenge of improving logical fallacy classification in large language models (LLMs), which often struggle with accurate reasoning due to their default fast, intuitive processing. To bridge this gap, the authors propose a structured approach that decomposes complex fallacy classification into a series of atomic binary questions. They introduce the Atomic-Instruction-Dataset-for-Logical-Fallacies (AID-LF), which transforms 232 fallacy descriptions into stepwise diagnostic questions with ground-truth labels. Additionally, they integrate a Prolog-based relational knowledge graph to help models identify and resolve similar fallacies before making final classifications. The methodology is evaluated across multiple state-of-the-art LLMs, including Claude-Sonnet-4, ChatGPT-4o, and Gemini-2.5-Flash. Results show significant performance improvements, with Claude-Sonnet-4 achieving up to 62.9% accuracy in the enhanced stepwise + relational graph method, compared to 42.2% in the baseline. The approach not only boosts accuracy but also enhances transparency in model decision-making.

## Method Summary
The methodology introduces a structured three-phase classification protocol: (1) execute stepwise atomic binary instructions for all 232 fallacies and compare against ground truth patterns, (2) query a Prolog relational graph for commonly confused fallacies, and (3) re-evaluate related candidates before final selection. The AID-LF dataset transforms each fallacy into 3-5 yes/no diagnostic questions with associated ground truth arrays and logical operations (AND/OR). This approach systematically constrains the classification space and forces models to engage in deliberate reasoning rather than relying on intuitive pattern matching.

## Key Results
- Claude-Sonnet-4 achieved 62.9% accuracy with stepwise + relational graphs vs 42.2% baseline
- Stepwise instructions alone improved Claude-Sonnet-4 accuracy to 55.2% (20.7% improvement)
- Hierarchical classification degraded performance across all models (25.9-24.6% accuracy vs baseline)
- Final classification appeared as second-ranked option in 39% of misclassifications with stepwise + relational approach

## Why This Works (Mechanism)

### Mechanism 1: Atomic Binary Question Decomposition
Decomposing complex fallacy classification into sequential yes/no diagnostic questions improves LLM accuracy by reducing cognitive load and enabling systematic evaluation. Each fallacy is transformed into 3-5 atomic binary questions (e.g., "Is there an original claim?", "Does shift in accent change meaning?"). Models must answer each step and compare against ground truth patterns before selecting a classification. This mechanism depends on model's instruction-following capabilities.

### Mechanism 2: Relational Graph Verification for Disambiguation
Prolog-based relational graphs capturing inter-fallacy confusion patterns enable post-hoc correction of initial misclassifications. After initial stepwise classification, models query relational graphs to find commonly confused fallacies, then re-execute stepwise analysis on related candidates before final selection. This forces reconsideration of similar fallacies (e.g., Contextomy vs Accent Fallacy). Requires accurate relational graph construction.

### Mechanism 3: Constrained Classification Space (Not Hierarchical Decomposition)
Narrowing candidate fallacy sets through combined stepwise + relational constraints improves accuracy, but hierarchical tree traversal degrades performance. Stepwise + relational approach constrains search space by eliminating non-matching candidates early. In contrast, hierarchical classification (formal/informal → subcategories → specific fallacy) caused error propagation—models rarely corrected early mistakes at Level 2.

## Foundational Learning

- **Concept: System 1 vs System 2 Processing (Kahneman, 2011)**
  - Why needed here: Paper frames LLM limitations as default "fast, intuitive" System 1 processing; intervention aims to induce deliberate System 2 reasoning through procedural steps.
  - Quick check question: Can you explain why providing fallacy definitions alone (Hong et al., 2024) failed to improve classification, but stepwise instructions succeeded?

- **Concept: Decision Procedures with Guaranteed Termination**
  - Why needed here: Atomic instructions derived from formal verification principles—algorithms that terminate with binary satisfiability results (Kroening & Strichman, 2016).
  - Quick check question: How does the ground_truth pattern matching (e.g., ["yes", "yes", "yes", "yes"] with "and" operations) ensure deterministic classification?

- **Concept: Neuro-symbolic Architecture Integration**
  - Why needed here: Paper combines neural LLMs with symbolic Prolog knowledge graphs; understanding this paradigm is essential for extending the approach.
  - Quick check question: Why is Prolog's declarative syntax suited for querying fallacy relationships rather than procedural classification?

## Architecture Onboarding

- **Component map**: `AID-LF Dataset (JSON)` → `Prolog Relational Graph` → `Three-Phase Classification Protocol`

- **Critical path**:
  1. Load `final_instructions.json` and `prolog.pro` into context
  2. For input statement, iterate through ALL fallacies executing binary steps
  3. For initial matches, query Prolog for related fallacies
  4. Re-execute stepwise on related candidates
  5. Rank by ground_truth match strength and logical consistency

- **Design tradeoffs**:
  - Zero-shot autonomous classification vs. iterative per-fallacy evaluation (paper notes latter improves accuracy 54.7%→66.7% but is "prohibitively resource-intensive")
  - Closed-source LLMs (tested) vs. open-source (planned future work)
  - Single dataset (FALLACIES) vs. cross-dataset generalization (acknowledged limitation)

- **Failure signatures**:
  - "Logical Fallacy Entrapment": Models exhibit inherent biases toward specific fallacies (Claude: Hasty Generalization; ChatGPT: Ad Hominem; Gemini: Appeal to Emotion)
  - Non-compliance: Models skip systematic execution and rely on intuitive pattern recognition
  - Speaker role confusion: Multi-person interactions misclassify arguer vs. subject

- **First 3 experiments**:
  1. Replicate baseline with your target LLM using original FALLACIES dataset descriptions to establish performance floor
  2. Implement stepwise-only classification to isolate atomic instruction contribution before adding relational graphs
  3. Construct relational graphs from your model's specific misclassification patterns (not paper's baseline), then measure delta

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the AID-LF methodology generalize to open-source language models (e.g., Llama, Vicuna) and alternative datasets beyond the FALLACIES corpus?
  - Basis in paper: The "Future Work" section explicitly states the intent to "integrate a selection of open-source LMs" and "apply our proposed methodology to additional logical fallacy datasets" to assess generalizability.

- **Open Question 2**: Can fine-tuning language models specifically for atomic instruction following resolve the compliance issues observed in zero-shot prompting?
  - Basis in paper: The authors identify a "critical future direction" involving the fine-tuning of LMs to improve their capability in following atomic instructions, noting persistent challenges in enforcing adherence.

- **Open Question 3**: Does integrating fallacy-specific distinguishing traits into the Prolog knowledge graph improve disambiguation between highly similar fallacies?
  - Basis in paper: The paper proposes an enhancement to move beyond noting relationships by "integrating step-by-step comparison knowledge for similar fallacies" into the graph.

- **Open Question 4**: Does an iterative classification approach (evaluating fallacies individually) significantly outperform the zero-shot autonomous process used in this study?
  - Basis in paper: The authors note in the "Data Processing limitation" section that an optimal approach would involve evaluating each fallacy individually, but they were unable to implement this due to resource constraints.

## Limitations
- Relational Graph Construction Bias: Prolog graphs built from baseline misclassification patterns may create circularity if baseline itself has systematic biases
- Single Dataset Generalization: Results only validated on FALLACIES dataset, limiting claims about broader applicability
- Resource Constraints: Optimal iterative evaluation method not implemented due to prohibitive computational costs

## Confidence
- **High confidence**: Atomic binary question decomposition mechanism and performance improvements are well-supported by experimental results
- **Medium confidence**: Relational graph verification contribution is less certain due to limited evidence of construction and circularity concerns
- **Low confidence**: Claims about model-specific "logical fallacy entrapment" lack robust statistical validation

## Next Checks
1. Reconstruct the Prolog relational graph using a hold-out subset of the data rather than the same data used to establish baseline performance, then measure the impact on accuracy
2. Implement cross-dataset validation using a different logical fallacy corpus to test whether the AID-LF instructions generalize beyond the FALLACIES dataset
3. Conduct ablation studies isolating the contribution of each atomic instruction step to identify which specific questions drive the most significant performance improvements