---
ver: rpa2
title: Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning
  with Double-Weight Sparse Pack
arxiv_id: '2601.01840'
source_url: https://arxiv.org/abs/2601.01840
tags:
- client
- fedcspack
- data
- global
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the dual challenge of data heterogeneity and
  limited client resources in federated learning. The proposed method, FedCSPACK,
  introduces a novel approach that packages model parameters and selectively shares
  them based on cosine similarity, reducing communication overhead.
---

# Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack

## Quick Facts
- arXiv ID: 2601.01840
- Source URL: https://arxiv.org/abs/2601.01840
- Authors: Qiantao Yang; Liquan Chen; Mingfu Xue; Songze Li
- Reference count: 40
- Primary result: FedCSPACK improves training speed by 2-5× and model accuracy by 3.34% compared to state-of-the-art methods while reducing communication overhead

## Executive Summary
This paper addresses the dual challenge of data heterogeneity and limited client resources in federated learning. The proposed method, FedCSPACK, introduces a novel approach that packages model parameters and selectively shares them based on cosine similarity, reducing communication overhead. A dual-weighted aggregation mechanism, incorporating both directional and distributional distance weights, is used to improve the server's ability to align and aggregate sparse updates. Experimental results across four datasets show that FedCSPACK achieves significant improvements in training efficiency and model performance while maintaining computational efficiency and demonstrating robustness under various data heterogeneity scenarios.

## Method Summary
FedCSPACK packages model parameters into fixed-size chunks (PACK) and selectively shares them based on cosine similarity between local and global models. For each package, clients compute both cosine similarity (measuring directional alignment) and KL divergence (measuring distributional distance). Packages with low cosine similarity are selected as Top-K shared packages, indicating higher potential contribution to addressing local data heterogeneity. A dual-weight mask is generated using both similarity metrics, which the server uses to perform weighted aggregation of sparse updates into the global model. This approach reduces communication overhead by ~96% on EMNIST while improving training speed by 2-5× and accuracy by 3.34% compared to state-of-the-art methods.

## Key Results
- Achieves 2-5× training speedup compared to state-of-the-art methods
- Improves model accuracy by 3.34% across multiple datasets
- Reduces communication overhead by approximately 96% on EMNIST dataset
- Maintains robustness under various data heterogeneity scenarios (Dirichlet α∈[0.3,0.6,1.0] and pathological sampling)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selective parameter package sharing based on cosine similarity reduces communication overhead while preserving task-relevant knowledge transfer.
- **Mechanism:** Clients flatten model parameters into packages of size PACK. For each package $PW^{t}_{i,j}$, the client computes cosine similarity against the corresponding global package $PW^{t}_{j}$. Packages with similarity $\theta^{t}_{i,j} < \theta^{t}_{a}$ (overall model similarity threshold) are selected as Top-K shared packages. Lower similarity packages indicate greater divergence and potentially higher contribution to addressing local data heterogeneity.
- **Core assumption:** Parameters that have diverged more from the global model carry more personalized knowledge worth transmitting, rather than being noise.
- **Evidence anchors:**
  - [abstract] "selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements"
  - [section: Top-k Based Cosine Parameter Packing] Eq. 5 shows TopK selection based on $\theta^{t}_{i,j} < \theta^{t}_{a}$
  - [corpus] FedRTS (arxiv 2501.19122) uses dynamic pruning for efficiency—FedCSPACK differs by using similarity-guided package selection rather than magnitude-based pruning
- **Break condition:** If local data is too noisy or corrupted, lower similarity packages may transmit harmful updates rather than valuable personalized knowledge.

### Mechanism 2
- **Claim:** Dual-weight aggregation combining directional alignment (cosine similarity) and distributional distance (KL divergence) improves global model's ability to integrate heterogeneous updates.
- **Mechanism:** The mask weight combines $\theta^{t}_{i,k}$ (cosine similarity, measuring directional alignment) and $\beta^{t}_{i,k}$ (KL divergence, measuring distributional distance). During aggregation, the server uses weighted averaging: $PW^{t+1}_{j} = PW^{t}_{j} + \frac{\sum M^{t}_{i,j} \cdot PW^{t}_{i,k}}{M^{t+1}_{j}}$. This ensures packages with high divergence but correct direction get appropriate influence.
- **Core assumption:** Cosine similarity alone cannot distinguish between well-aligned but magnitude-shifted updates; KL divergence compensates for this blind spot.
- **Evidence anchors:**
  - [abstract] "directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism"
  - [section: Mask Double Weight Aggregation] Eq. 8 defines combined mask weight as $\theta^{t}_{i,k} + \beta^{t}_{i,k}$
  - [corpus] FedRPCA (arxiv 2506.01194) uses Robust PCA for LoRA aggregation—FedCSPACK differs by using statistical distance measures rather than low-rank decomposition
- **Break condition:** If KL divergence values become unstable (e.g., near-zero package values causing numerical issues), aggregation weights may become unreliable.

### Mechanism 3
- **Claim:** Mask-anchored sparse package alignment enables efficient server-side reconstruction of partial updates without full model transmission.
- **Mechanism:** Each client generates a mask matrix $M^{t}_{i}$ anchored to the parameter package structure. Valid positions (selected Top-K packages) receive dual-weight values; other positions are zero. The server accumulates masks across clients ($M^{t+1} = \sum M^{t}_{i}$) to determine aggregate weights per position, enabling weighted fusion of sparse updates into a coherent global model.
- **Core assumption:** The package-level granularity (PACK size) provides sufficient resolution for meaningful similarity comparisons while keeping computational overhead manageable.
- **Evidence anchors:**
  - [abstract] "generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates"
  - [section: Mask Double Weight Aggregation] Eq. 6 and 8 show mask value assignment based on package selection
  - [corpus] FedCCA (arxiv 2601.17713) focuses on client-centric adaptation for IoT—FedCSPACK's mask mechanism is complementary but addresses similar resource constraints
- **Break condition:** If PACK size is too large, packages become too coarse-grained for meaningful selection; if too small, overhead from mask transmission approaches full model size.

## Foundational Learning

- **Concept: Cosine Similarity in High-Dimensional Spaces**
  - **Why needed here:** FedCSPACK uses cosine similarity to measure alignment between local and global parameter packages, determining which packages to share. Understanding that cosine similarity captures directional agreement independent of magnitude is essential.
  - **Quick check question:** If a local parameter package has cosine similarity 0.95 with the global package but 10× the magnitude, would FedCSPACK prioritize or deprioritize sharing it?

- **Concept: KL Divergence as Distributional Distance**
  - **Why needed here:** FedCSPACK supplements cosine similarity with KL divergence to capture distributional shifts that directional metrics miss. KL divergence is asymmetric and requires understanding of probability distribution comparisons.
  - **Quick check question:** Why might using KL divergence alone (without cosine similarity) lead to overweighting certain parameter packages during aggregation?

- **Concept: Sparse Communication in Federated Learning**
  - **Why needed here:** FedCSPACK achieves 96% compression on EMNIST through package-level sparsification. Understanding trade-offs between sparsity, convergence speed, and model quality is critical for system tuning.
  - **Quick check question:** At what client participation ratio (CPR) does FedCSPACK's advantage over baselines become most pronounced, and why?

## Architecture Onboarding

- **Component map:**
  - Client-side: Parameter Flattener → Package Divider → Cosine Similarity Calculator → Top-K Selector → Mask Generator (with KL divergence computation)
  - Server-side: Mask Aggregator → Weighted Package Aggregator → Global Model Reconstructor
  - Shared state: Global model $W^{t}$, global mask $M^{t}$, PACK size hyperparameter

- **Critical path:**
  1. Client receives $W^{t}$ and $M^{t}$
  2. Local training updates $W^{t}_{i}$ under mask guidance
  3. Flatten and package $W^{t}_{i}$ into $PW^{t}_{i,j}$ chunks
  4. Compute $\theta^{t}_{i,j}$ and $\beta^{t}_{i,j}$ for each package
  5. Select Top-K packages where $\theta^{t}_{i,j} < \theta^{t}_{a}$
  6. Generate mask with dual weights at selected positions
  7. Server aggregates masks and packages into $W^{t+1}$

- **Design tradeoffs:**
  - **PACK size:** Larger PACK → faster computation but coarser selection granularity. Paper shows accuracy plateaus above PACK=512 while time continues decreasing (Figure A5).
  - **K selection (implicit via threshold):** Lower threshold $\theta^{t}_{a}$ → fewer packages shared → lower communication but potentially slower convergence.
  - **Dual weight balance:** Equal weighting of $\theta$ and $\beta$ is assumed; alternative weighting schemes were not explored in the paper.

- **Failure signatures:**
  - **Mask collapse:** If all clients share identical packages, mask aggregation produces uniform weights, reducing to standard FedAvg behavior.
  - **Numerical instability:** KL divergence computation with near-zero parameter values can produce inf/NaN values—requires epsilon stabilization.
  - **Drift under low participation:** At CPR=0.3 with Dir(0.3), some baselines drop below 10% accuracy; FedCSPACK maintains ~32% (Figure 4c).

- **First 3 experiments:**
  1. **Reproduce communication compression:** Measure actual traffic (GB) between client and server for FMNIST/CIFAR-10 at PACK=512, comparing against baseline FedAvg. Target: ≥90% reduction per Table 2.
  2. **Ablate dual weights:** Run FedCSPACK with only cosine similarity weights vs. only KL divergence weights vs. combined on CIFAR-10 with Dir(0.5). Expected gap: combined should outperform single-weight variants by 3-7% accuracy per Table 3.
  3. **Stress test heterogeneity:** Train on pathological sampling (2 classes per client) on CIFAR-100, varying CPR from 0.3 to 1.0. Verify FedCSPACK maintains >10% accuracy advantage at CPR≤0.6 per Figure A3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the PACK size be determined adaptively per client or layer rather than using a fixed global value?
- **Basis:** [inferred] Appendix E analyzes fixed PACK sizes, but the system assumes heterogeneous resource constraints where static sizes may be suboptimal for individual clients.
- **Why unresolved:** A fixed size forces a trade-off that might exclude severely resource-constrained clients or underutilize high-resource clients.
- **What evidence would resolve it:** Implementation of an adaptive PACK algorithm showing maintained accuracy with reduced variance in client update times.

### Open Question 2
- **Question:** How does FedCSPACK perform on non-convex tasks utilizing Transformer architectures, such as NLP or foundation models?
- **Basis:** [inferred] Experiments are restricted to image classification (CNN/ResNet-18), leaving the method's efficacy on attention-based mechanisms untested.
- **Why unresolved:** Parameter correlation in attention layers differs significantly from convolutional filters, potentially affecting the cosine similarity selection logic.
- **What evidence would resolve it:** Empirical evaluation on NLP benchmarks (e.g., Shakespeare, Stack Overflow) using Transformer models.

### Open Question 3
- **Question:** Is KL divergence the optimal metric for the distribution distance weight compared to symmetric alternatives like Jensen-Shannon divergence?
- **Basis:** [inferred] While ablated against "No Weight" and "Cosine Only," the paper does not justify the specific choice of KL divergence over other distance metrics.
- **Why unresolved:** KL divergence is asymmetric and can lead to numerical instability if parameter distributions have near-zero values.
- **What evidence would resolve it:** Ablation studies comparing KL divergence against Wasserstein distance or L2 norms for the $\beta$ weight.

### Open Question 4
- **Question:** What are the theoretical convergence bounds for FedCSPACK under non-convex objectives?
- **Basis:** [inferred] The paper relies on empirical validation without providing a theoretical convergence analysis for the heuristic Top-k masking.
- **Why unresolved:** The heuristic selection of parameter packages introduces bias that complicates standard federated convergence proofs.
- **What evidence would resolve it:** A mathematical proof bounding the convergence rate (e.g., $O(1/T)$) considering the sparse dual-weight updates.

## Limitations
- The paper does not specify the optimal PACK size, only suggesting that sizes ≥512 are effective while smaller sizes may be too coarse-grained
- KL divergence numerical stability is not addressed—potential for NaN/inf values when parameter values approach zero
- Experimental results lack statistical significance testing for accuracy improvements across heterogeneity scenarios

## Confidence

- **High:** Communication compression (~96% reduction on EMNIST) and training time improvements (2-5× speedup) - directly measured and reported
- **Medium:** Accuracy improvements (3.34% gain) - dependent on unreported KL divergence stabilization and PACK size selection
- **Low:** Robustness under pathological Non-IID sampling - only directional claims in Figure A3 without statistical significance testing

## Next Checks

1. **KL Divergence Stability Test:** Run FedCSPACK on CIFAR-10 with PACK=512, instrument KL divergence computation to detect NaN/inf values; verify epsilon-stabilization is necessary and sufficient
2. **Sensitivity Analysis:** Systematically vary PACK size (128→4096) and participation ratio (0.3→1.0) on CIFAR-100, measuring accuracy vs. compression trade-off curves
3. **Ablation Study:** Implement FedCSPACK variants using only cosine weights, only KL weights, and combined weights; measure accuracy gap under Dir(0.3) vs. Dir(1.0) data heterogeneity