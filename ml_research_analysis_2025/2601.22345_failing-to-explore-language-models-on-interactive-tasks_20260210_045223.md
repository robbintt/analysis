---
ver: rpa2
title: 'Failing to Explore: Language Models on Interactive Tasks'
arxiv_id: '2601.22345'
source_url: https://arxiv.org/abs/2601.22345
tags:
- qwen2
- b-instruct
- queries
- reward
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how well language models (LMs) explore unknown\
  \ environments under a limited interaction budget. It introduces three parametric\
  \ tasks\u2014HillSearch, TreeSearch, and MaxSatSearch\u2014with controllable difficulty\
  \ and traps to test exploration."
---

# Failing to Explore: Language Models on Interactive Tasks

## Quick Facts
- **arXiv ID**: 2601.22345
- **Source URL**: https://arxiv.org/abs/2601.22345
- **Reference count**: 40
- **Primary result**: Language models underperform simple explore-exploit baselines on parametric exploration tasks; parallelizing budget into independent threads and periodic summarization significantly improve exploration performance.

## Executive Summary
This paper evaluates how well language models explore unknown environments under limited interaction budgets. The authors introduce three parametric tasks—HillSearch, TreeSearch, and MaxSatSearch—with controllable difficulty and traps to test exploration capabilities. Across models including GPT-5 and Qwen, performance was consistently worse than simple explore–exploit baselines, with models committing early to suboptimal solutions and scaling weakly with budget. Two lightweight interventions—parallelizing a fixed budget into independent threads, and periodic summarization of the interaction history—consistently improved performance across tasks and difficulty levels, despite theory showing no gain from parallelization in optimal strategies.

## Method Summary
The paper introduces three parametric exploration tasks: HillSearch (find maximum of hidden Gaussian function), TreeSearch (navigate tree with trap/good gateways), and MaxSatSearch (maximize satisfied clauses with hidden gold clause). Models interact with an oracle for N rounds, receiving deterministic feedback per query. The evaluation compares frozen frontier models against simple explore–exploit baselines. Two interventions are tested: parallel execution with p threads (each with budget N/p) and periodic summarization every N/s interactions. The experimental setup uses structured JSON outputs, temperature 0.7, top-p 0.95, and runs 40-100 episodes per condition.

## Key Results
- Language models underperform simple explore–exploit baselines on all three tasks, showing premature commitment to suboptimal solutions
- Parallelizing a fixed budget into independent threads improves performance despite theoretical predictions of no gain
- Periodic summarization of interaction history preserves key discoveries and further improves exploration across all difficulty levels

## Why This Works (Mechanism)

### Mechanism 1: Parallel Budget Allocation
Splitting a fixed interaction budget into independent threads and selecting the best result improves LM exploration performance. Multiple threads increase the probability that at least one thread avoids early commitment to suboptimal solutions. Each thread explores independently, and the best solution across all threads is retained. This works because LM behavior is far from optimal and models get trapped in local optima due to premature commitment.

### Mechanism 2: Periodic Summarization of Interaction History
Periodically replacing full interaction context with structured summaries improves exploration by preserving key discoveries and reducing context-related failures. Every N/s interactions, the full history is compressed into a "mission hand-off" summary highlighting unexplored regions, current best solutions, and remaining budget. This prevents models from getting stuck in patterns suggested by verbose history.

### Mechanism 3: Sublinear Success Probability Drives Parallel Gains
Parallelization helps when success probability scales sublinearly with budget (q(x) = cx^α, α < 1), which characterizes low-success regimes. When q(x) is small, 1 - (1 - q(x/p))^p > q(x)—the probability that at least one of p threads succeeds exceeds single-thread success. Theorem 5.1 proves a threshold v_p exists below which parallelization is beneficial.

## Foundational Learning

- **Explore–Exploit Tradeoff**: The paper evaluates whether LMs balance exploration (probing unknown regions) against exploitation (refining known solutions) under budget constraints. *Quick check: Given N queries and a hidden function with multiple peaks, what fraction should explore vs. exploit?*

- **Premature Commitment in LMs**: Models latch onto early solutions (traps) and fail to revise, causing suboptimal performance across all three tasks. *Quick check: Why might seeding an LM rollout with incorrect tokens reduce accuracy?*

- **Context Length and Exploration Failure**: Long accumulated history misleads models; summarization interventions work by resetting context while retaining key information. *Quick check: How does increasing context length affect an agent's willingness to explore new regions?*

## Architecture Onboarding

- **Component map**: Task instance → Model query → Oracle feedback → Context update → (Optional) Summary compression → Next query → Final reward
- **Critical path**: Task instance → Model query → Oracle feedback → Context update → (Optional) Summary compression → Next query → Final reward
- **Design tradeoffs**: More threads (higher p) → better coverage but less budget per thread for refinement; More frequent summaries (higher s) → more restarts but potential information loss
- **Failure signatures**: HillSearch: Queries cluster around early local maximum; TreeSearch: Depth-first descent into first entered branch; MaxSatSearch: Low Hamming distance between consecutive queries
- **First 3 experiments**: 1) Baseline validation: Run simple explore-exploit heuristic vs. single LM thread; 2) Parallel sweep: Test p ∈ {2,3,4} with fixed total budget N=48; 3) Summary frequency test: Vary s ∈ {2,3,4,6} on HillSearch

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do the systematic exploration failures identified in parametric tasks generalize to complex, real-world agentic environments?
- **Basis**: The evaluation is restricted to synthetic HillSearch, TreeSearch, and MaxSatSearch environments
- **What evidence would resolve it**: Correlation analysis between model performance on this benchmark and performance on established web navigation or robotics tasks

### Open Question 2
- **Question**: Can language models be fine-tuned to develop intrinsic exploration strategies, or is under-exploration a fundamental constraint of current pre-training objectives?
- **Basis**: The paper observes systematic under-exploration across all evaluated frontier models, suggesting a failure mode inherent to current training paradigms
- **What evidence would resolve it**: Experiments applying reinforcement learning (RL) with exploration-based rewards to the proposed tasks

### Open Question 3
- **Question**: How do parallelization and summarization interventions interact when applied simultaneously to a fixed budget?
- **Basis**: The paper analyzes parallel threads and periodic summarization as independent interventions, but does not evaluate a combined approach
- **What evidence would resolve it**: Experiments measuring the reward of a combined "parallel + summary" condition against the individual baselines

## Limitations
- The theoretical framework for parallelization relies on sublinear success probability scaling that cannot be fully validated from the paper alone
- The periodic summarization intervention lacks detailed analysis of information loss and what specific content gets dropped
- Model-specific behaviors like premature commitment are observed but not deeply characterized with quantitative measures

## Confidence

- **High confidence**: Language models underperform simple explore-exploit baselines on all three tasks; empirical results showing parallelization and summarization improvements
- **Medium confidence**: Theoretical justification for why parallelization helps (sublinear scaling and threshold theorem); mechanism explanations for why summarization helps
- **Low confidence**: Specific parameter estimates for success probability scaling; detailed analysis of what information is critical in summaries

## Next Checks

1. **Parameter estimation**: Measure the actual success probability scaling q(x) empirically across different budget levels to verify the sublinear relationship and estimate the exponent α for each task
2. **Information retention analysis**: Implement a controlled study where summaries are systematically varied to identify which elements are critical for maintaining performance
3. **Alternative exploration strategies**: Test whether simpler interventions like temperature scaling or top-k sampling can achieve similar improvements to parallelization