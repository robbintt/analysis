---
ver: rpa2
title: 'EvolveSignal: A Large Language Model Powered Coding Agent for Discovering
  Traffic Signal Control Algorithms'
arxiv_id: '2509.03335'
source_url: https://arxiv.org/abs/2509.03335
tags:
- traffic
- control
- program
- signal
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EvolveSignal is a large language model (LLM)-powered coding agent
  that automatically discovers fixed-time traffic signal control algorithms through
  program synthesis. The approach represents candidate algorithms as Python functions
  and iteratively optimizes them via simulation-based evaluation and evolutionary
  search.
---

# EvolveSignal: A Large Language Model Powered Coding Agent for Discovering Traffic Signal Control Algorithms

## Quick Facts
- **arXiv ID**: 2509.03335
- **Source URL**: https://arxiv.org/abs/2509.03335
- **Reference count**: 38
- **Primary result**: LLM-powered program synthesis discovers traffic signal control algorithms that reduce average delay by 20.1% and stops by 47.1% compared to Webster's baseline

## Executive Summary
EvolveSignal is a large language model (LLM)-powered coding agent that automatically discovers fixed-time traffic signal control algorithms through program synthesis. The approach represents candidate algorithms as Python functions and iteratively optimizes them via simulation-based evaluation and evolutionary search. Experiments on a signalized intersection show the discovered algorithms outperform Webster's baseline, reducing average delay by 20.1% and stops by 47.1%. Ablation and incremental analyses reveal that modifications such as adjusting cycle length bounds, incorporating right-turn demand, and rescaling green allocations provide interpretable, practically meaningful improvements for traffic engineers.

## Method Summary
The framework formulates traffic signal control algorithm discovery as program synthesis, representing candidate algorithms as Python functions that are iteratively optimized via evolutionary search. An LLM ensemble proposes code modifications (crossover/mutation) to the parent function, which are then evaluated through microscopic traffic simulation in SUMO. The system uses MAP-Elites to maintain a diverse population of high-performing programs in a database, with performance measured by a combined score of average delay and stops. The evolutionary process runs for 300 iterations using an ensemble of LLMs (DeepSeek-v3/r1, OpenAI-o4-mini/o3) at temperature 0.6.

## Key Results
- Discovered algorithms reduce average delay by 20.1% and average stops by 47.1% compared to Webster's baseline
- Modifications like Cycle Length Bound (CLB) and Right-Turn Inclusion (RTI) provide interpretable improvements, with CLB+RTI showing synergistic effects
- Performance improvements are consistent across three heavy-demand scenarios (S1-S3)
- The framework successfully discovers non-intuitive algorithmic structures that manual design often misses

## Why This Works (Mechanism)

### Mechanism 1: Program Synthesis for Algorithmic Discovery
Formulating algorithm discovery as program synthesis allows the system to modify computational logic (e.g., formulas, conditionals) rather than just tuning numerical parameters. The framework represents the control algorithm as a Python function $f_\theta$, with an LLM Ensemble proposing code-level modifications that are accepted or rejected based on performance. This enables discovery of novel algorithmic structures beyond parameter tuning.

### Mechanism 2: Discovery of Synergistic Dependencies
The system identifies non-intuitive, synergistic dependencies between traffic parameters that manual design often misses. By maintaining a population of programs and testing combinations, the evolutionary search discovers that some changes only yield benefits when combined. For example, CLB+RTI improves performance by +17.90%, exceeding the sum of individual contributions, suggesting coupling between demand modeling and cycle flexibility.

### Mechanism 3: Simulation-Based Grounding
External simulation-based evaluation grounds the LLM's reasoning in physical constraints, preventing "hallucinated" improvements. The LLM acts as a generator, but the Evaluators Pool (SUMO simulation) serves as the strict objective critic. The LLM cannot claim an improvement without a corresponding score increase in the simulator, ensuring physically feasible solutions.

## Foundational Learning

- **Fixed-Time vs. Adaptive Control**: The paper explicitly targets fixed-time control (low cost, stable) rather than Adaptive Traffic Signal Control (ATSC). You must distinguish between optimizing a static plan vs. real-time reaction. Quick check: Does the algorithm need real-time sensor data to compute the output, or just historical demand averages?

- **MAP-Elites Algorithm**: The "Program Database" uses MAP-Elites to maintain diversity, preventing the search from converging prematurely to a single local optimum by keeping a grid of high-performing solutions with different behavioral characteristics. Quick check: How does the system balance exploring new code structures vs. refining existing high-performers?

- **Webster's Method**: This serves as the "Initial Program" (baseline). Understanding its formulas (cycle length, green split) is required to interpret the LLM's modifications. Quick check: What is the standard relationship between critical lane volume and cycle length in the baseline formula?

## Architecture Onboarding

- **Component map**: Prompt Construction -> LLM Modification -> Code Validation -> Simulation (SUMO) -> Score Aggregation -> Database Update
- **Critical path**: The evolutionary loop where prompts containing parent code and performance metrics are fed to the LLM ensemble, which generates modified code that is validated and then evaluated in SUMO simulation before updating the MAP-Elites database
- **Design tradeoffs**: Interpretability vs. Performance (more iterations can lead to "spaghetti code" hard to audit), Diversity vs. Stability (single LLM leads to "stable biases" while ensemble increases diversity but adds API complexity/cost)
- **Failure signatures**: Stagnation (score plateaus for 50+ iterations), Infeasible Code (LLM proposes code breaking SUMO constraints), Metric Gaming (algorithm minimizes stops by creating excessively long cycles)
- **First 3 experiments**: 1) Baseline Verification: Run initial_program.py (Webster) in SUMO scenario S1 to reproduce baseline score (~0.4893), 2) Ablation Check: Manually remove CLB modification from discovered_program.py to verify ~19% performance drop, 3) LLM Diversity Test: Run 10 iterations using only one model vs. ensemble to observe variance in generated inspirations

## Open Questions the Paper Calls Out
- Can the framework scale to discover effective coordination algorithms for arterial or network-level traffic signal control?
- Does the current fixed input-output structure generalize to complex intersection geometries (e.g., 5-leg or asymmetric intersections)?
- How does incorporating human-in-the-loop strategies affect the convergence speed and diversity of the discovered algorithms?

## Limitations
- Underspecified methodological details including exact MAP-Elites configuration, complete prompt templates, and precise SUMO simulation parameters
- Focus on single isolated intersection and three fixed demand scenarios limits generalizability to complex networks or diverse traffic patterns
- Interpretability claim rests on assumption that LLM-generated modifications are inherently more transparent, though evolution can produce "spaghetti code" difficult to audit

## Confidence
- **High Confidence**: Core finding that LLM-driven program synthesis can discover traffic signal control algorithms outperforming Webster's baseline is well-supported by ablation analysis and incremental improvements
- **Medium Confidence**: Interpretability claim and assertion of synergistic dependencies are plausible but require deeper validation
- **Low Confidence**: Generalizability to other intersection types, network configurations, or demand patterns is not established

## Next Checks
1. **Generalization Test**: Evaluate the discovered algorithm on at least two additional intersection types (e.g., different lane configurations or multi-intersection networks) to assess performance outside training scenarios
2. **Interpretability Audit**: Conduct systematic comparison between evolved algorithm and human-designed modifications by having traffic engineers rate interpretability and practical feasibility of each code modification
3. **Edge Case Analysis**: Test the algorithm under ultra-light traffic conditions and observe whether it maintains minimum green time constraints and avoids unrealistic cycle length recommendations