---
ver: rpa2
title: 'RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning'
arxiv_id: '2507.08012'
source_url: https://arxiv.org/abs/2507.08012
tags:
- features
- speaker
- speech
- control
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel method for discovering controllable
  prosodic features in prompt-based text-to-speech (TTS) models through repeated fine-tuning.
  The core idea involves analyzing the uncontrollable variance in synthesized speech
  by generating thousands of samples with fixed inputs and using principal component
  analysis (PCA) on utterance-level embeddings to identify latent features.
---

# RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning

## Quick Facts
- arXiv ID: 2507.08012
- Source URL: https://arxiv.org/abs/2507.08012
- Reference count: 0
- Primary result: Method discovers controllable emotional intensity in TTS models through PCA on model variance, with iterative fine-tuning reducing unexplained variance from 42.7% to 12.5%

## Executive Summary
This paper introduces RepeaTTS, a novel method for discovering controllable prosodic features in prompt-based TTS models through repeated fine-tuning cycles. The approach exploits the variance in synthesized speech by generating thousands of samples with fixed inputs and using PCA on utterance-level embeddings to identify latent features. These discovered features are then incorporated as new control labels through iterative fine-tuning. Experiments on an Icelandic emotive TTS corpus demonstrate successful discovery of emotional intensity as a controllable feature in models not exposed to emotion labels during initial training.

## Method Summary
RepeaTTS works by first fine-tuning a baseline TTS model on a target corpus, then generating an analysis set of 1,000+ samples with identical inputs (text, speaker, prompt). Wav2Vec2 layer-4 embeddings are extracted and reduced via PCA to identify principal components of variance. Clusters or gradients in the PCA space are manually inspected and labeled, then training data is re-labeled by cosine distance to cluster means. The model is fine-tuned again with extended prompts incorporating these new control labels, and the process iterates until variance is sufficiently reduced. The method successfully discovers emotional intensity as a controllable feature in models without emotion labels, but reveals recording condition artifacts in emotion-labeled models.

## Key Results
- Successfully discovered emotional intensity as a controllable feature in Icelandic TTS models
- Variance unexplained by discovered features reduced from 42.7% to 23.7% to 12.5% across iterations
- Label alignment improved from 58.2% to 65.3% after iterative fine-tuning
- Method failed to discover prosodic features in emotion-labeled models, instead revealing recording condition variations

## Why This Works (Mechanism)

### Mechanism 1: Variance Exploitation for Feature Discovery
The method exploits uncontrolled variance in prompt-based TTS output, assuming this variance primarily reflects prosodic differences the model can produce but users cannot control. By fixing all inputs and generating thousands of samples, the remaining variance is analyzed via PCA on Wav2Vec2 embeddings to reveal principal components corresponding to control features.

### Mechanism 2: Layer-Specific Embedding Selection for Prosody Capture
Early layers of Wav2Vec2 (specifically layer 4) are used because self-supervised speech models exhibit layer-wise specialization, with early layers encoding more acoustic/prosodic information while later layers capture phonetic/linguistic content.

### Mechanism 3: Iterative Feature Enrollment via Corpus Re-labeling
Discovered latent features are incorporated as explicit control labels through cosine-distance-based re-labeling. After identifying clusters or gradients in PCA space, training utterances are assigned to nearest clusters, then expressed as textual prompts for fine-tuning.

## Foundational Learning

- **Concept: Principal Component Analysis (PCA) for Variance Decomposition**
  - Why needed: Reduces high-dimensional Wav2Vec2 embeddings to tractable dimensions for manual inspection and identifies axes of maximum variance that correspond to control features
  - Quick check: If PC1 + PC2 explain only 15% of total variance, would you trust them as primary control axes? What might this indicate about your analysis set?

- **Concept: Prompt-Based TTS Conditioning**
  - Why needed: The method assumes new control features can be expressed as natural language extensions to existing description prompts, leveraging the frozen T5 encoder's flexibility
  - Quick check: Why does the T5 encoder need to remain frozen when adding new label types to description prompts?

- **Concept: Temperature and k-Sampling in Autoregressive Generation**
  - Why needed: Controls the diversity-quality tradeoff in the analysis set; higher values explore more of the output distribution but risk unintelligibility
  - Quick check: If speaker similarity drops below 0.70 as you increase temperature, what aspect of the feature discovery pipeline would be compromised?

## Architecture Onboarding

- **Component map**: Base Model (ParlerTTS) -> Prompt System (Frozen T5 + LLaMA tokenizer) -> Embedding Extractor (Wav2Vec2-XLSR layer 4) -> Analysis Pipeline (Generate samples → Extract embeddings → PCA → Manual inspection → Re-label → Fine-tune)

- **Critical path**: 1) Fine-tune baseline ParlerTTS on target corpus, 2) Validate speaker consistency (>0.75) and intelligibility (WER <15%), 3) Generate 1000+ samples with fixed inputs, 4) Extract Wav2Vec2 layer-4 embeddings and compute utterance-level means, 5) Project via PCA and manually inspect 3D scatter, 6) Create cluster/bin means and re-label training data by cosine distance, 7) Fine-tune with extended prompts and iterate

- **Design tradeoffs**: Temperature vs. quality (τ=1.2, k=100 balanced diversity 0.13-0.15 against WER <15% and speaker similarity >0.75), single-text analysis avoids confounds but may miss cross-text features, discrete vs. continuous labels (clusters suggest categorical, gradients suggest continuous scales)

- **Failure signatures**: T3-emotion failure mode (pre-existing emotion labels constrained variance revealing recording artifacts not prosody), low label-ground-truth alignment (58.2% improved to 65.3%), consistent F0 contours indicating over-constrained model

- **First 3 experiments**: 1) Baseline validation (generate 100 samples per speaker, measure WER, speaker similarity, diversity), 2) Hyperparameter calibration (grid-search τ ∈ [1.0, 1.5] and k ∈ [50, 300], plot tradeoff surface), 3) Single-feature enrollment pilot (1000-sample analysis set, run PCA, implement re-labeling, fine-tune 20-40 epochs, verify variance reduction)

## Open Questions the Paper Calls Out

### Open Question 1
Can utterance representations specifically designed for prosody mitigate the issue of discovering non-prosodic features like recording conditions? The authors note that alternative representations may be less affected by non-verbal variation and more suitable for the proposed approach. Evidence would come from repeating the T3-emotion experiment using prosody-specific encoders and verifying if principal components correlate with prosodic features rather than SNR or reverberation.

### Open Question 2
Can the analysis set composition be modified to disentangle text-dependent variance from the target prosodic variance? The authors restrict analysis to fixed text inputs because summary embeddings are inherently text-dependent, limiting the method's ability to discover features that generalize across diverse inputs. Evidence would come from developing a normalization technique for embeddings that removes linguistic content, allowing the method to function on multi-text analysis sets without losing feature fidelity.

### Open Question 3
Is there a limit to the number of iterative fine-tuning cycles before model quality degrades or variance is exhausted? The authors stopped the T3 analysis after three cycles upon seeing variance reduction, but did not define a convergence point or test for potential model catastrophic forgetting. Evidence would come from a longitudinal study measuring naturalness (MOS) and diversity scores over 10+ fine-tuning cycles to identify the "breaking point" of the model.

## Limitations
- Method fails when applied to models already constrained by strong conditioning labels, revealing recording condition artifacts instead of prosodic features
- Manual inspection step introduces subjective bias and lacks quantitative validation criteria
- Discovered features may not generalize beyond specific text/speaker combinations used for analysis

## Confidence

**High Confidence**: Variance exploitation mechanism well-supported by experimental results showing clear variance reduction across iterations; layer selection rationale grounded in established Wav2Vec2 literature; iterative enrollment process demonstrates measurable success in reducing variance and improving label alignment.

**Medium Confidence**: Assumption that cosine distance-based re-labeling produces meaningful feature assignments given synthetic cluster means may occupy different embedding regions; generalizability of discovered features across different text inputs remains uncertain; choice of layer 4 may not hold for features that are more phonetic than prosodic.

**Low Confidence**: Manual inspection step for identifying clusters and gradients is inherently subjective and lacks quantitative validation criteria; method's scalability to discovering multiple simultaneous features is untested; quality thresholds for analysis set (0.75 speaker similarity, 15% WER) are reasonable but not rigorously validated for optimal feature discovery.

## Next Checks

1. **Cross-Text Generalization Test**: Generate analysis sets for 5-10 diverse texts with the same speaker, perform feature discovery on each separately, then test whether control features discovered from one text transfer to others. Measure consistency of discovered features and controllability across text inputs.

2. **Synthetic-to-Real Embedding Alignment**: For a discovered feature cluster, compute cosine distances between real training utterances and their assigned synthetic cluster means. Analyze the distribution of these distances and correlate with WER/diversity metrics to quantify the quality of the re-labeling assignment.

3. **Perceptual Validation Study**: Conduct a human listening test where participants rate samples generated with explicit control prompts (discovered features) against random samples from the analysis set. Measure whether participants can reliably distinguish the intended feature intensity levels and whether ratings correlate with the model's internal feature representation.