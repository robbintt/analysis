---
ver: rpa2
title: 'Language and Experience: A Computational Model of Social Learning in Complex
  Tasks'
arxiv_id: '2509.00074'
source_url: https://arxiv.org/abs/2509.00074
tags:
- objects
- game
- learning
- when
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a computational framework for social learning
  that integrates linguistic guidance with direct experience through Bayesian inference
  over structured world models. The key innovation is using a large language model
  as a probabilistic speaker model to interpret and generate human-interpretable advice,
  allowing agents to update their beliefs about game dynamics based on both gameplay
  experience and linguistic input.
---

# Language and Experience: A Computational Model of Social Learning in Complex Tasks

## Quick Facts
- **arXiv ID:** 2509.00074
- **Source URL:** https://arxiv.org/abs/2509.00074
- **Reference count:** 40
- **One-line primary result:** A computational framework integrates linguistic guidance with direct experience through Bayesian inference, demonstrating human-level sample efficiency and 40% reduction in learning attempts across 10 video games.

## Executive Summary
This study introduces a computational framework for social learning that integrates linguistic guidance with direct experience through Bayesian inference over structured world models. The key innovation is using a large language model as a probabilistic speaker model to interpret and generate human-interpretable advice, allowing agents to update their beliefs about game dynamics based on both gameplay experience and linguistic input. Through experiments across 10 video games with human participants and simulations, the model demonstrates human-level sample efficiency and shows that linguistic guidance accelerates learning by 40%, reducing attempts needed to solve games. The framework also enables knowledge accumulation across generations through iterated learning and supports bidirectional knowledge transfer between humans and models, suggesting potential for collaborative human-AI learning systems.

## Method Summary
The framework employs Bayesian inference over a hypothesis space of structured, executable world models (VGDL programs) to learn game rules. A particle filter maintains M=20 candidate theories, updating beliefs using both experiential data (via simulation-based likelihoods) and linguistic guidance (via LLM-based speaker model likelihoods). The model uses LLaMA-3.1-70B to estimate P(L|T), the probability a speaker believing theory T would generate message L. Language-guided proposals bias the search space toward advice-consistent theories. Planning uses a genetic algorithm over action sequences, sampling goals based on theory disagreement and win/reward contributions. The framework was validated through human experiments (N=120 across 3 conditions) and simulations across 10 VGDL games, measuring normalized area-under-curve (nAUC) for proficiency and learning efficiency.

## Key Results
- Linguistic guidance accelerates learning by 40%, reducing attempts needed to solve games
- Model achieves human-level sample efficiency in novel video games
- Knowledge accumulation across generations through iterated learning shows consistent improvements (∆nAUC ∈ [0.44, 0.57])
- Shared mechanisms for integrating linguistic and experiential knowledge between humans and models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Agents achieve sample efficiency comparable to humans by jointly inferring rules from experience and language using Bayesian inference over structured world models.
- **Mechanism:** Particle filter with M=20 candidate theories maintains posterior P(T|E,L) by multiplying simplicity prior P(T) with experience likelihood P(E|T) (via simulation) and language likelihood P(L|T) (via LLM).
- **Core assumption:** Environment dynamics can be represented by VGDL primitives.
- **Evidence anchors:** [abstract] "models social learning as joint probabilistic inference over structured, executable world models..." [section 3.1] "P(T|E,L) ∝ P(E|T) × P(L|T) × P(T)"
- **Break condition:** Misspecification occurs if environment requires physics outside VGDL primitives.

### Mechanism 2
- **Claim:** LLM as probabilistic speaker model allows agents to interpret natural language advice as evidence for specific world rules.
- **Mechanism:** Estimates P(L|T) using LLM next-token probability distribution when prompted with theory description.
- **Core assumption:** LLM's token distribution approximates human teacher's belief state.
- **Evidence anchors:** [section 3.1] "P(L|T) as probability speaker would produce L..." [section 5.2] "Messages that helped humans also helped models..."
- **Break condition:** Unreliable if advice is sarcastic, metaphorical, or uses unfamiliar terminology.

### Mechanism 3
- **Claim:** Language-mediated iterated learning enables knowledge accumulation across generations of agents.
- **Mechanism:** Short-horizon agents (2 lives) play, generate advice for next agent, creating cultural chains where linguistic priors compensate for limited experience.
- **Core assumption:** Speaker model accuracy prevents catastrophic error compounding across generations.
- **Evidence anchors:** [section 5.4] "improvements ∆nAUC ∈ [0.44, 0.57] across generations..."
- **Break condition:** High-confidence false positives from early generations create "pedagogical curse."

## Foundational Learning

- **Concept:** **Particle Filtering (Sequential Monte Carlo)**
  - **Why needed here:** Maintains population of 20 explicit theories that must be resampled and mutated as new data arrives.
  - **Quick check question:** Can you explain how the "weight" of a particle (theory) changes when the agent dies unexpectedly?

- **Concept:** **Domain Specific Languages (DSLs)**
  - **Why needed here:** Inference engine relies on structured language (VGDL) to define hypothesis space.
  - **Quick check question:** What specific game mechanics (e.g., gravity, inventory) are likely impossible to represent in the VGDL subset used?

- **Concept:** **Theory of Mind (ToM) in AI**
  - **Why needed here:** Model reverse-engineers speaker's belief state to weight its own world models.
  - **Quick check question:** How does the model react if speaker provides advice contradicting recent direct experience?

## Architecture Onboarding

- **Component map:** Environment (VGDL Engine) -> Inference Core (Particle Filter) -> Experience Likelihood (Simulation) -> Language Module (LLM) -> Planner (Genetic Algorithm)

- **Critical path:**
  1. Receive Advice & State
  2. LLM Proposal: Convert advice into potential rule mutations
  3. Particle Update: Resample theories based on consistency with experience and LLM likelihood
  4. Plan: Select high-value goals using MAP theory

- **Design tradeoffs:**
  - Expressiveness vs. Tractability: Restricted VGDL limits games but enables feasible Bayesian inference
  - LLM Role: Using LLM for likelihood (evaluation) is computationally cheaper than generation

- **Failure signatures:**
  - Blindness to Motor Skills: Struggles in games requiring rapid, precise reflexes
  - Hallucinated Constraints: May avoid harmless objects essential for winning
  - Corpus Alert: LLM may fail to model human pragmatics in complex social contexts

- **First 3 experiments:**
  1. Ablation Study: Disable Language-Guided Proposals to isolate LLM's contribution
  2. Adversarial Advice: Provide factually incorrect advice to test experiential override
  3. Iterated Learning Stress Test: 20-generation chain with limited lives to measure information degradation

## Open Questions the Paper Calls Out

- **Open Question 1:** How can agents be endowed with meta-cognitive reasoning to assess the quality, reliability, and usefulness of linguistic advice before integrating it into belief updates? [explicit] Current model naively integrates advice, which can mislead when advice is incorrect or ambiguous.

- **Open Question 2:** How can the framework be extended to interpret and learn from richer linguistic guidance containing abstractions, high-level strategies, and planning heuristics? [explicit] Current implementation parses specific rule-like advice but not metacognitive strategies or analogies.

- **Open Question 3:** How can the model implement prestige-based social learning to decide who to learn from based on perceived expertise or demonstrated success? [explicit] Current experiments use random teacher selection, showing occasional regressions with unreliable teachers.

## Limitations

- Computational expense of simulating 20 theories per inference step (1-24 hours per game) limits real-time applications
- Framework's reliance on VGDL constrains hypothesis space to discrete, symbolic mechanics
- Performance on tasks requiring fine motor skills remains fundamentally limited by symbolic nature of linguistic guidance
- LLM-based speaker model may fail with sarcasm, metaphor, or culturally specific language use

## Confidence

- **High Confidence:** Bayesian inference over structured world models is well-specified and theoretically grounded
- **Medium Confidence:** 40% learning acceleration claim relies on specific experimental conditions
- **Low Confidence:** "Bidirectional knowledge transfer" and "collaborative human-AI learning systems" claims extend beyond current experimental scope

## Next Checks

1. **Generalization Test Across Domains:** Validate framework on non-VGDL environments requiring continuous control or complex state representations

2. **Scalability Stress Test:** Systematically vary particle count and theory complexity to identify computational scaling limits

3. **Robustness to Linguistic Noise:** Conduct adversarial testing with misleading, metaphorical, or culturally-specific advice to quantify resilience