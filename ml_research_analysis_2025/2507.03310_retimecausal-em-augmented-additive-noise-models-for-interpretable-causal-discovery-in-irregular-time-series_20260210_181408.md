---
ver: rpa2
title: 'ReTimeCausal: EM-Augmented Additive Noise Models for Interpretable Causal
  Discovery in Irregular Time Series'
arxiv_id: '2507.03310'
source_url: https://arxiv.org/abs/2507.03310
tags:
- causal
- data
- missing
- time
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of causal discovery in irregularly
  sampled time series with missing data, which is common in high-stakes domains like
  finance, healthcare, and climate science. The authors propose ReTimeCausal, a novel
  framework that integrates Additive Noise Models (ANM) with an Expectation-Maximization
  (EM) procedure to jointly perform missing data imputation and causal structure learning.
---

# ReTimeCausal: EM-Augmented Additive Noise Models for Interpretable Causal Discovery in Irregular Time Series

## Quick Facts
- **arXiv ID:** 2507.03310
- **Source URL:** https://arxiv.org/abs/2507.03310
- **Reference count:** 40
- **Key outcome:** ReTimeCausal achieves F1 scores of 0.436 on CausalRivers and perfect F1 on synthetic data with 80% missingness, outperforming baselines by significant margins.

## Executive Summary
This paper addresses causal discovery in irregularly sampled time series with missing data using a novel EM-based framework. ReTimeCausal integrates Additive Noise Models with kernelized sparse regression to jointly impute missing values and learn causal structures. The method handles both linear and nonlinear dynamics while maintaining interpretability through sparsity constraints. Extensive experiments show superior performance compared to state-of-the-art baselines under challenging conditions.

## Method Summary
ReTimeCausal implements an EM-based iterative optimization framework for causal discovery in irregularly sampled time series. The E-step imputes missing values using current causal estimates with noise-aware imputation, while the M-step updates structural functions via kernelized sparse regression in RKHS. The framework alternates between these steps, refining both missing data completion and causal graph structure. Post-processing includes weight thresholding and CAM pruning to ensure sparse, interpretable causal graphs.

## Key Results
- Achieves F1 score of 0.436 on CausalRivers dataset vs. 0.348 for best baseline
- Perfect F1 scores on synthetic data with 80% missingness while baselines drop below 0.2
- Maintains strong performance as graph size increases, demonstrating good scalability
- Outperforms PCMCI+TimeMixer and other baselines in both F1 and SHD metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint imputation-discovery via EM mitigates error accumulation that plagues sequential "impute-then-discover" pipelines.
- **Mechanism:** The E-step imputes missing values conditioned on the current causal graph G^(k) via Ẋ_t^i = f_i(Pa_i^t), embedding causal constraints directly into imputation. The M-step then updates parameters θ and graph structure via sparse regression on completed data. This feedback loop prevents imputation from introducing spurious edges.
- **Core assumption:** Missingness is ignorable (MCAR/MAR); missingness mechanism need not be explicitly modeled.
- **Evidence anchors:** Abstract states "ReTimeCausal...unifies physics-guided data imputation with sparse causal inference"; Section 4.1 contrasts with impute-then-discover pipelines.

### Mechanism 2
- **Claim:** Kernelized sparse regression in RKHS enables nonlinear causal discovery while preserving interpretable input-space structure.
- **Mechanism:** Data is mapped to a high-dimensional feature space via kernel embedding. Lag-specific transformation matrices are learned in this space, then projected back to input space weights. L1 regularization enforces sparsity, and thresholding yields binary adjacency.
- **Core assumption:** Causal mechanisms are additive and noise is independent of parents (ANM identifiability conditions).
- **Evidence anchors:** Abstract mentions "kernelized sparse regression and structural constraints"; Section 4.2.2 discusses L1 regularization on projected weights.

### Mechanism 3
- **Claim:** Noise-aware imputation restores the independence assumption required for post-hoc pruning methods like CAM.
- **Mechanism:** Deterministic imputation eliminates noise, violating noise-parent independence. Injecting sampled noise ε_t^i during imputation restores this independence, enabling valid statistical pruning of spurious edges.
- **Core assumption:** Noise distribution can be approximated from residuals of f_i; pruning methods assume noise-parent independence.
- **Evidence anchors:** Section 4.3 states "this noise-injection mechanism substantially improves pruning quality when applying CAM to kernel-based nonlinear models."

## Foundational Learning

- **Concept: Additive Noise Models (ANM)**
  - **Why needed here:** The entire framework relies on ANM structure for both imputation and identifiability. Without understanding ANM, you cannot diagnose why pruning works or why kernel regression is appropriate.
  - **Quick check question:** Given X = f(Y) + ε with ε ⊥ Y, if you regress X on Y and get residuals correlated with Y, what assumption is violated and how would this affect causal discovery?

- **Concept: Expectation-Maximization (EM) for Missing Data**
  - **Why needed here:** ReTimeCausal uses EM not in its classical closed-form sense but as an iterative imputation-update schema. Understanding the Q-function and why "structure-aware" imputation differs from generic imputation is critical.
  - **Quick check question:** In ReTimeCausal's E-step, why does E[X_t^i | Pa_i] simplify to f_i(Pa_i) rather than requiring integration over all missing variables?

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS) and Sparse Regression**
  - **Why needed here:** The kernel trick enables nonlinear function approximation without explicit feature engineering, but the L1 regularization on projected weights is what delivers interpretability.
  - **Quick check question:** If you use an RBF kernel with bandwidth σ, what happens to W_(i,j)^(τ) as σ → 0? How does this affect graph sparsity?

## Architecture Onboarding

- **Component map:** Initialize G^(0) → Fit f_i via neural networks → E-step imputation (Eq. 7) → M-step kernel regression (Eq. 5) → Project and threshold → Prune → Check convergence → Repeat

- **Critical path:** Initialize graph → Fit neural networks on observed data → Impute missing data with noise → Solve kernelized sparse regression → Project weights to input space → Apply threshold and pruning → Iterate until convergence

- **Design tradeoffs:**
  - Kernel choice: RBF is universal but computationally expensive; linear kernel reduces to sparse Granger causality
  - Sparsity λ: Higher λ yields sparser graphs but may miss weak edges; lower λ retains more edges but increases false positives
  - Threshold γ: Directly controls precision/recall tradeoff; requires validation

- **Failure signatures:**
  - Imputation collapse: If f_i overfits sparse observed data, imputed values amplify errors; check residual distribution on held-out observed points
  - Graph non-convergence: G^(k) oscillates; increase smoothing α or reduce learning rate
  - Pruning removes true edges: Noise injection may be mis-specified; validate residual independence empirically

- **First 3 experiments:**
  1. Linear sanity check: Generate data from Eq. 1 with linear f_i, 0% missingness; verify ReTimeCausal recovers ground truth with F1 ≈ 1.0
  2. Missingness ablation: On synthetic nonlinear data, vary missing rate from 0% to 80%; plot F1 vs. missing rate and compare to PCMCI+TimeMixer baseline
  3. Noise-aware vs. deterministic imputation: Ablate Eq. 7 vs. Eq. 2; measure pruning quality to validate Mechanism 3

## Open Questions the Paper Calls Out
None

## Limitations
- Sensitivity to hyperparameters: Performance critically depends on sparsity λ, threshold γ, smoothing α, and RBF bandwidth σ, none of which are theoretically grounded
- Noise injection assumptions: Effectiveness hinges on accurate residual noise modeling, but limited validation across different noise distributions
- Scalability to high-dimensional data: Kernelized regression may not scale efficiently to graphs with hundreds or thousands of nodes

## Confidence

- **High confidence:** Core EM framework for joint imputation and discovery (well-established methodology with clear theoretical grounding)
- **Medium confidence:** Kernelized sparse regression approach for nonlinear causal discovery (ANM assumptions and kernel choice significantly affect identifiability)
- **Low confidence:** Noise-injection mechanism's robustness across diverse real-world scenarios (limited empirical validation of residual noise modeling assumptions)

## Next Checks
1. **Noise injection ablation study:** Implement ReTimeCausal with deterministic imputation vs. noise-aware imputation on CausalRivers and synthetic datasets to quantify pruning quality improvement.
2. **Hyperparameter sensitivity analysis:** Systematically vary λ, γ, and RBF bandwidth on synthetic data to identify optimal ranges and assess stability of F1 scores.
3. **MNAR missingness robustness:** Evaluate ReTimeCausal performance under non-ignorable missingness patterns to validate MCAR/MAR assumption and identify degradation points.