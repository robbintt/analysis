---
ver: rpa2
title: 'PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question
  Answering in Persian Language'
arxiv_id: '2505.18331'
source_url: https://arxiv.org/abs/2505.18331
tags:
- medical
- language
- question
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PerMedCQA, the first large-scale Persian-language
  benchmark for consumer-oriented medical question answering, consisting of 68,138
  real-world QA pairs annotated with ICD-11 categories and 25 question types. A novel
  rubric-based LLM evaluator, Med-Judge, was developed and validated by physicians
  for open-ended medical QA assessment.
---

# PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language

## Quick Facts
- arXiv ID: 2505.18331
- Source URL: https://arxiv.org/abs/2505.18331
- Reference count: 40
- First large-scale Persian-language benchmark for consumer-oriented medical question answering with 68,138 real-world QA pairs

## Executive Summary
This paper introduces PerMedCQA, the first large-scale Persian-language benchmark for consumer-oriented medical question answering, consisting of 68,138 real-world QA pairs annotated with ICD-11 categories and 25 question types. A novel rubric-based LLM evaluator, Med-Judge, was developed and validated by physicians for open-ended medical QA assessment. Experiments benchmarked 16 state-of-the-art multilingual and biomedical LLMs under zero-shot settings, showing substantial performance variation and effectiveness of inference-time prompt strategies. Supervised fine-tuning experiments using LoRA on smaller models demonstrated the dataset's learnability and practical utility for improving medical QA capabilities in Persian.

## Method Summary
The paper presents PerMedCQA, a dataset of 68,138 Persian medical QA pairs from four online forums, split into training (64,280), evaluation (345), and test (3,513) sets. The dataset is annotated with ICD-11 categories and 25 question types. A novel rubric-based LLM evaluator (Med-Judge) using Gemini-Flash-2.5 was developed to assess correctness, coverage, and clinical impact. The evaluation benchmarks 16 multilingual and biomedical models under zero-shot settings with three inference strategies: baseline prompt, role-based prompting, and pivot translation (Persian→English→Persian). Supervised fine-tuning experiments use LoRA on Gemma 4B, LLaMA 3.1 8B, and BioMistral 7B models.

## Key Results
- PerMedCQA contains 68,138 Persian medical QA pairs from 4 forums, annotated with ICD-11 categories
- Med-Judge LLM evaluator achieves 75% agreement with human experts (Cohen's κ = 0.42)
- Demonstrated learnability through LoRA fine-tuning on smaller models (4B-7B parameters)
- Effectiveness of inference-time strategies including role-based prompting and pivot translation

## Why This Works (Mechanism)

### Mechanism 1: Structured LLM-as-Judge Evaluation
Rubric-based LLM evaluation (Med-Judge) correlates more reliably with human clinical assessment than lexical metrics for open-ended medical QA. Instead of measuring token overlap (BLEU/ROUGE), an LLM (Gemini-Flash-2.5) compares model outputs against expert references using a structured schema: Correctness, Coverage, and Clinical Impact. This allows for semantic equivalence checking where different phrasings convey the same medical advice.

### Mechanism 2: Cross-Lingual Pivot Reasoning
Translating low-resource language inputs to a high-resource language (English) for inference can yield higher quality results than direct inference, provided the translation quality is high. The "Pivot Translation" strategy translates Persian questions → English, generates answers in English (where the model's reasoning is strongest), and back-translates to Persian. This leverages the dense medical knowledge representation in English training data.

### Mechanism 3: Data Hygiene for Alignment
Rigorous, multi-stage removal of noise (short text, duplicates) and Personally Identifiable Information (PII) creates a "learnable" dataset that prevents model collapse or privacy leakage during fine-tuning. A two-stage pipeline (Rule-based + LLM-based PII detection) filters 87k raw entries down to 68k clean pairs. This ensures that Supervised Fine-Tuning (SFT) with LoRA learns clinical patterns rather than memorizing user data or forum artifacts.

## Foundational Learning

- **Concept: LLM-as-a-Judge (Ranking/Scoring)**
  - Why needed: The paper abandons BLEU/ROUGE. Understanding how to prompt an LLM to output structured JSON scores (Correctness, Coverage, Impact) is essential for reproducing their evaluation.
  - Quick check: Can you design a prompt that forces an LLM to choose between strictly defined labels like "Correct" vs. "Partially Correct" based only on a provided reference text?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed: The paper uses LoRA to adapt 7B-8B models. You need to understand how Low-Rank Adaptation updates weights via adapters rather than full matrix updates to grasp the "practical utility" claim.
  - Quick check: If a model has 7 billion parameters, which specific matrices in the transformer layer are actually being modified during LoRA training?

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - Why needed: The paper benchmarks zero-shot baselines against "Few-Shot" and "Role-based" prompting. Distinguishing between "giving the model examples" (Few-Shot) and "assigning a persona" (Role-based) is critical for the architecture onboarding.
  - Quick check: Does providing 5 random examples from the training set (Few-Shot) update the model's weights, or just the context window?

## Architecture Onboarding

- **Component map:** Raw Forums (DrYab, etc.) -> Preprocessing Pipeline (Rule-based filter + PII LLM) -> PerMedCQA Dataset (Parquet/JSON) -> Prompt Constructor (Zero-shot / Role-based / Pivot Translator) -> Target LLM (e.g., Llama 3.1, Gemma) -> Med-Judge (Gemini-Flash-2.5) -> JSON Score

- **Critical path:** 1. Data Validation: Verifying the PII removal pipeline (Section 3.2) is the first gate; failure here stops the project for ethical reasons. 2. Judge Validation: Establishing that Med-Judge correlates with human experts (Section A.2) is required before trusting any leaderboard results.

- **Design tradeoffs:** Accuracy vs. Latency (Pivot Translation): Translating to English and back improves answer quality (potentially) but triples inference cost and latency. Safety vs. Helpfulness: The prompt (Figure 8) instructs the model to answer "briefly" and refer to a doctor "only when strictly necessary," trading extensive safety disclaimers for direct answers.

- **Failure signatures:** Metric Collapse: Med-Judge gives all models "Correct" because the rubric is too loose. Translation Drift: Pivot strategy returns fluent Persian that is medically hallucinated due to back-translation errors. PII Leakage: Fine-tuned model generates names or phone numbers found in the raw data.

- **First 3 experiments:** 1. Sanity Check: Run the Med-Judge prompt (Figure 11) manually on 10 samples using the provided test set to verify your JSON parsing logic matches the paper's schema. 2. Baseline Reproduction: Run the "Baseline System Prompt" (Figure 8) on a small open-source model (e.g., Llama 3.1 8B) on the Eval set to verify inference setup. 3. Ablation: Compare "Zero-Shot" vs. "Role-Based" prompting (Figure 9) on a single ICD-11 category (e.g., Mental Disorders) to quantify the gain from specialized personas.

## Open Questions the Paper Calls Out

### Open Question 1
How can PerMedCQA be extended to support multimodal inputs, specifically for medical cases requiring visual information (e.g., skin conditions or medical imaging)? The authors state the dataset is "text-only QA, excluding cases requiring visual information, which are common in real-world healthcare settings and present important avenues for future work."

### Open Question 2
Does the reliance on data from four specific online forums introduce topical or demographic biases that restrict the generalizability of models fine-tuned on PerMedCQA? The Limitations section notes the dataset "is derived from a limited set of public Persian medical forums, which may introduce topical and demographic biases and restrict generalizability beyond Persian-speaking communities."

### Open Question 3
Can the Med-Judge LLM-based evaluator be improved to achieve higher agreement with human experts beyond the reported 75% accuracy and Cohen's $\kappa$ of 0.42? While the authors validate Med-Judge, the Appendix reports only "moderate" inter-rater reliability ($\kappa = 0.42$) against a small sample of 100 human-annotated items, suggesting the automated grader frequently diverges from expert clinical judgment.

### Open Question 4
Does full parameter fine-tuning of larger state-of-the-art models on PerMedCQA yield significantly better medical reasoning capabilities than the LoRA-based fine-tuning of smaller models (4B–7B parameters) reported? The authors explicitly limited supervised fine-tuning experiments to "smaller-scale model performance" to assess "learnability," leaving the upper bound of performance on larger models unexplored.

## Limitations

- The entire benchmark depends on the reliability of the Med-Judge LLM evaluator, which showed only moderate agreement (75%, κ = 0.42) with human experts on a small validation set
- The paper states results section is "under development - currently unavailable," preventing assessment of core empirical claims about model performance differences
- The dataset is derived from a limited set of public Persian medical forums, potentially introducing topical and demographic biases that restrict generalizability beyond Persian-speaking communities

## Confidence

**High Confidence:** Dataset construction methodology and general framework for LLM-as-judge evaluation are well-specified and reproducible.

**Medium Confidence:** Mechanism of using structured rubric-based evaluation over traditional lexical metrics is supported by related work and human validation results.

**Low Confidence:** Core empirical claims about model performance differences, effectiveness of inference-time strategies, and dataset learnability cannot be assessed without actual benchmark results.

## Next Checks

1. **Complete baseline benchmarking:** Obtain and verify zero-shot performance results for all 16 models across the three Med-Judge dimensions, including per-ICD-11 category breakdowns.

2. **Judge reliability stress test:** Expand human validation beyond 100 samples to 500-1000 items, focusing on edge cases where Correctness judgments are ambiguous, to test for consistent bias.

3. **Pivot translation validation:** Conduct systematic quality assessment of pivot translation outputs by having bilingual medical professionals evaluate semantic preservation across 200 randomly selected samples, comparing clinical accuracy between direct Persian inference and pivot-translated outputs.