---
ver: rpa2
title: Evaluating the Retrieval Robustness of Large Language Models
arxiv_id: '2505.21870'
source_url: https://arxiv.org/abs/2505.21870
tags:
- retrieval
- robustness
- performance
- answer
- retrieved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces three novel metrics to evaluate the robustness
  of large language models (LLMs) in retrieval-augmented generation (RAG) settings:
  no-degradation rate (NDR), retrieval size robustness (RSR), and retrieval order
  robustness (ROR). The authors establish a benchmark of 1,500 open-domain questions
  across three datasets, with retrieved documents from Wikipedia using BM25 and dense
  retrievers.'
---

# Evaluating the Retrieval Robustness of Large Language Models

## Quick Facts
- arXiv ID: 2505.21870
- Source URL: https://arxiv.org/abs/2505.21870
- Reference count: 40
- This paper introduces three novel metrics to evaluate retrieval robustness in RAG systems and finds LLMs generally achieve over 80% robustness scores with sample-level trade-offs.

## Executive Summary
This paper introduces three novel metrics to evaluate the robustness of large language models (LLMs) in retrieval-augmented generation (RAG) settings: no-degradation rate (NDR), retrieval size robustness (RSR), and retrieval order robustness (ROR). The authors establish a benchmark of 1,500 open-domain questions across three datasets, with retrieved documents from Wikipedia using BM25 and dense retrievers. They evaluate 11 LLMs with three prompting strategies (vanilla, OwnKnow, and S2A). The key findings show that LLMs generally exhibit high retrieval robustness, with most models achieving over 80% scores on the geometric mean of the three robustness metrics, and GPT-4o/o3-mini surpassing 90%. However, this robustness is imperfect, leading to sample-level performance trade-offs where models sacrifice some examples' performance to improve others.

## Method Summary
The authors evaluate 11 LLMs across three prompting strategies (vanilla, OwnKnow, and S2A) using a benchmark of 1,500 questions from Natural Questions, HotpotQA, and ASQA datasets. They retrieve Wikipedia documents using both BM25 and dense retrievers, testing multiple retrieval sizes (5-100 documents) and orderings (original, reversed, shuffled). A Llama-3.3-70B-Instruct judge evaluates answer correctness. The three robustness metrics are computed: NDR measures when RAG matches or exceeds non-RAG performance, RSR assesses performance monotonicity across retrieval sizes, and ROR evaluates variance across document orderings. Overall robustness is the geometric mean of these three metrics.

## Key Results
- LLMs achieve high retrieval robustness, with most models scoring over 80% on the geometric mean of NDR, RSR, and ROR metrics
- GPT-4o and o3-mini achieve the highest robustness scores (>90%), while smaller models like Llama-3-8B show stronger NDR due to lower parametric knowledge baselines
- Sample-level trade-offs are prevalent, with models sacrificing performance on some examples to improve others when retrieval size or order changes
- OwnKnow prompting strategy enhances robustness by using non-RAG answers as anchors but may limit maximum RAG performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models exhibit lower No-Degradation Rate (NDR) because richer parametric knowledge raises the baseline that RAG must beat.
- Mechanism: When a model already answers correctly without retrieval, RAG can only match or degrade performance. Larger models solve more questions parametrically, creating more opportunities for RAG to underperform relative to the non-RAG baseline.
- Core assumption: RAG performance gains are bounded by the quality of retrieved documents; imperfect retrieval cannot always improve on strong parametric knowledge.
- Evidence anchors:
  - [abstract] "larger models tend to have lower NDR due to their richer parametric knowledge"
  - [section 5.2] "Llama-3-8B has higher robustness than 70B... this inverse scaling trend mainly comes from No-Degradation Rate (NDR)"
  - [corpus] Related work on RAG robustness (RbFT, RADIANT) confirms retrieval defects constrain RAG effectiveness, though specific inverse scaling with model size is not directly addressed.
- Break condition: If retrieval quality is near-perfect (high recall of gold answers), NDR should improve even for larger models since retrieved context reliably supplements parametric knowledge.

### Mechanism 2
- Claim: The OwnKnow prompting strategy enhances robustness by using non-RAG answers as anchors that reduce variance in the final response.
- Mechanism: The model first generates a draft answer using parametric knowledge, then conditions on both this draft and retrieved context. The draft serves as a stabilizing signal, reducing susceptibility to distracting or conflicting retrieved content.
- Core assumption: Models can effectively compare their parametric knowledge against retrieved content and resolve conflicts when given an explicit prior answer.
- Evidence anchors:
  - [abstract] "incorporating OwnKnow prompting strategy enhances robustness but may limit maximum RAG performance"
  - [section 4.2] "OwnKnow obtains a draft answer based on models' own knowledge... then inserts this draft answer into the prompt for the RAG setup"
  - [section 5.5] "outputs given by the non-RAG setup serve as drafts and anchors, leading to reduced variance"
  - [corpus] Corpus evidence is limited; related papers do not directly evaluate anchor-based prompting strategies.
- Break condition: If the non-RAG draft is confidently incorrect and the retrieved context is correct, OwnKnow may reinforce errors rather than improve robustness.

### Mechanism 3
- Claim: Sample-level performance trade-offs emerge because models cannot uniformly improve across all examples when retrieval size or order changes.
- Mechanism: Increasing retrieval size adds both relevant and irrelevant documents. Models gain information on some queries but are distracted on others, creating a zero-sum dynamic at the sample level even when aggregate performance improves.
- Core assumption: Models have finite attention capacity and cannot perfectly filter relevant from irrelevant content as context length grows.
- Evidence anchors:
  - [abstract] "LLMs generally exhibit high retrieval robustness... However, this robustness is imperfect, leading to sample-level performance trade-offs"
  - [section 5.3] "models keep trading off performance across individual samples, i.e., hurting performance on some examples while gaining performance on others"
  - [section 5.4] "each example has a different best order"
  - [corpus] Injecting External Knowledge into the Reasoning Process notes that noisy retrieved passages undermine RAG effectiveness, supporting the distraction mechanism.
- Break condition: If models develop perfect context-filtering capabilities, sample-level trade-offs should diminish as retrieval size increases.

## Foundational Learning

- Concept: **Parametric vs. Non-Parametric Knowledge**
  - Why needed here: Understanding NDR requires distinguishing what models know from weights versus what they derive from retrieved context.
  - Quick check question: If a model answers correctly without retrieval, what must RAG provide to avoid degradation?

- Concept: **Retrieval Recall and Noise**
  - Why needed here: Robustness metrics assume imperfect retrieval; interpreting results requires understanding how recall and irrelevant documents interact.
  - Quick check question: Why might increasing retrieved documents from 10 to 50 help some queries but hurt others?

- Concept: **Geometric Mean of Metrics**
  - Why needed here: The paper aggregates NDR, RSR, and ROR using geometric mean; this penalizes any single low score more heavily than arithmetic mean.
  - Quick check question: If a model scores 0.95 on NDR and RSR but 0.60 on ROR, what happens to the geometric mean?

## Architecture Onboarding

- Component map:
  - Wikipedia chunks -> BM25 and dense retrievers -> 11 LLMs x 3 prompting strategies -> Llama-3.3-70B-Instruct judge -> NDR/RSR/ROR metrics

- Critical path:
  1. Retrieve top-k documents (k âˆˆ {5, 10, 25, 50, 75, 100}) using both retrievers
  2. Apply ordering (original, reversed, shuffled)
  3. Generate answers with selected LLM and prompting strategy
  4. Judge correctness against gold answers
  5. Compute NDR, RSR, ROR across all (k, order, retriever) combinations

- Design tradeoffs:
  - **OwnKnow vs. vanilla**: OwnKnow improves robustness but caps maximum performance; use when stability matters more than peak accuracy
  - **Retrieval size**: Larger k improves aggregate performance but increases sample-level variance; cap at context length limits
  - **Order strategy**: Most models prefer reversed order (highest-ranked documents near the question); test all three if order robustness is critical

- Failure signatures:
  - **Low NDR with high parametric performance**: Model is being distracted by retrieved context; consider OwnKnow or better retriever
  - **Declining RSR at large k**: Model struggles with long contexts; implement chunking or relevance filtering
  - **High ROR variance**: Model is order-sensitive; add reranking or use S2A-style filtering (though S2A showed limited gains in this study)

- First 3 experiments:
  1. **Baseline robustness profile**: Run vanilla prompting on your target model across k = {5, 25, 100} with original order; compute all three metrics to identify weakest dimension
  2. **OwnKnow ablation**: Compare vanilla vs. OwnKnow on the same model; quantify robustness gain vs. maximum performance loss
  3. **Retriever comparison**: Evaluate BM25 vs. dense retriever on a held-out subset; check if denser retriever improves NDR for larger models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval robustness metrics be improved for larger models without sacrificing their richer parametric knowledge advantage?
- Basis in paper: [explicit] The authors observe an inverse scaling trend where "larger models usually have richer parametric knowledge and answers more questions correctly without retrieval, which means RAG will have a higher baseline to beat and thus RAG is more likely to get worse than non-RAG."
- Why unresolved: The study reveals but does not solve the fundamental tension between parametric knowledge quality and NDR; larger models like Llama-3.1-70B achieve lower NDR than smaller variants despite better overall performance.
- What evidence would resolve it: Training or prompting methods that allow large models to selectively integrate retrieval without overriding correct parametric knowledge, demonstrated through improved NDR while maintaining baseline accuracy.

### Open Question 2
- Question: Why does the S2A prompting strategy fail to enhance robustness with realistic retrievers despite success on synthetic noise?
- Basis in paper: [inferred] The authors note "a similar S2A prompting strategy fails to enhance retrieval robustness in our evaluations" and "conjecture that, compared to synthetic noisy contexts, realistic retrievers provide models with harder negative contexts that are more challenging for the model to identify."
- Why unresolved: The paper provides a hypothesis but no systematic analysis of what distinguishes realistic hard negatives from synthetic noise, nor alternative approaches.
- What evidence would resolve it: Controlled experiments varying the semantic similarity and distractor characteristics of retrieved documents, paired with analysis of model attention patterns to identify failure modes.

### Open Question 3
- Question: Does retrieval robustness generalize to RAG tasks beyond open-domain QA?
- Basis in paper: [explicit] The authors state in the Limitations section: "Our study of retrieval robustness focuses on open-domain QA, though we recognize that RAG can also be applied to other tasks, such as fact checking and code completion."
- Why unresolved: The proposed metrics are task-agnostic but have only been validated on QA; different tasks may have fundamentally different robustness profiles.
- What evidence would resolve it: Application of NDR, RSR, and ROR metrics to fact verification, code completion, and summarization tasks, comparing robustness patterns across task types.

### Open Question 4
- Question: Can adaptive or query-specific document ordering strategies improve ROR beyond fixed ordering schemes?
- Basis in paper: [explicit] The authors find that "each example has a different best order, highlighting the need for continuing efforts to improve order robustness" and that oracle selection across orders shows large performance gains.
- Why unresolved: Fixed ordering strategies (original, reversed, shuffled) cannot capture query-specific optimal arrangements; the paper demonstrates potential gains but no mechanism to realize them.
- What evidence would resolve it: Development and evaluation of learned re-ranking or dynamic ordering models that predict optimal document arrangement per query, achieving performance closer to the oracle ROR baseline.

## Limitations
- The inverse scaling of NDR with model size may not generalize beyond the evaluated 11 models and three datasets
- Sample-level trade-offs are demonstrated but not deeply characterized - we cannot predict which specific examples will improve or degrade
- The OwnKnow strategy's effectiveness depends on the assumption that parametric knowledge reliably anchors retrieval responses

## Confidence
- **High Confidence**: The overall finding that RAG systems exhibit high but imperfect robustness (>80% geometric mean) is well-supported by the 1,500-question benchmark and consistent across all 11 evaluated models
- **Medium Confidence**: The claim that larger models have lower NDR due to richer parametric knowledge is supported by the data but may be influenced by dataset-specific factors or evaluation methodology
- **Medium Confidence**: The mechanism by which OwnKnow improves robustness through draft answers as anchors is theoretically sound but relies on limited empirical evidence in the paper itself

## Next Checks
1. **Cross-dataset generalization test**: Apply the robustness evaluation framework to a domain-specific dataset (e.g., medical or legal QA) to verify whether the inverse NDR-scaling pattern holds outside open-domain questions
2. **Anchor quality ablation**: Systematically vary the quality of OwnKnow draft answers (using models of different sizes/capabilities) to quantify how anchor reliability affects robustness gains
3. **Retrieval quality threshold analysis**: Measure NDR as a function of gold-answer recall at different retrieval sizes to identify the point where retrieval quality overcomes the parametric knowledge baseline