---
ver: rpa2
title: Cross-patient Seizure Onset Zone Classification by Patient-Dependent Weight
arxiv_id: '2508.03635'
source_url: https://arxiv.org/abs/2508.03635
tags:
- data
- patient
- patients
- test
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the cross-patient problem in epilepsy diagnosis,
  where machine learning models struggle to generalize across patients due to variations
  in individual data distributions. To tackle this, the authors propose a patient-dependent
  weight method that fine-tunes a pretrained model using similarity-based weights
  derived from intermediate layer outputs between test and training patients.
---

# Cross-patient Seizure Onset Zone Classification by Patient-Dependent Weight

## Quick Facts
- **arXiv ID:** 2508.03635
- **Source URL:** https://arxiv.org/abs/2508.03635
- **Reference count:** 25
- **Primary result:** Average classification accuracy improvement >10% over baseline across all test patients

## Executive Summary
This paper addresses the challenge of cross-patient generalization in epilepsy diagnosis using intracranial EEG (iEEG). The authors propose a patient-dependent weight method that improves classification of seizure onset zones (SOZ) by weighting training patients based on distributional similarity to the test patient. The approach uses Maximum Mean Discrepancy (MMD) to calculate similarity between intermediate-layer representations, then fine-tunes a pretrained model with these patient-specific weights. Evaluated on a leave-one-patient-out basis, the method consistently improved accuracy for every test patient, achieving an average gain of over 10% compared to standard supervised learning.

## Method Summary
The method consists of three main stages: (1) Pretrain a 10-layer 1D-CNN on pooled training patient data using unweighted cross-entropy loss; (2) For each test patient, compute similarity weights by extracting features from the last convolutional layer and calculating MMD between test and each training patient using multiscale RBF kernels; (3) Fine-tune the pretrained model for 5 epochs using a weighted loss function where each training patient's samples are weighted by their similarity score. The approach maintains the same 1D-CNN architecture throughout, requiring no architectural modifications or complex training procedures.

## Key Results
- Achieved >10% average classification accuracy improvement across all test patients compared to baseline
- Method consistently improved performance for every individual test patient in leave-one-patient-out evaluation
- Demonstrated practical applicability for cross-patient medical data scenarios without requiring architectural changes

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Similarity Weighted Fine-Tuning
Weighting training patients by their distributional similarity to the test patient improves cross-patient generalization. After pretraining on pooled training patients, MMD between intermediate-layer representations of test vs. each training patient determines patient weights during fine-tuning: higher similarity yields higher loss weighting for that training patient's samples. Core assumption: Test patients will generalize better when the model emphasizes training data from distributionally similar patients.

### Mechanism 2: Intermediate-Feature Similarity (Not Raw iEEG)
Computing similarity in the learned feature space (last convolutional layer output) better captures clinically relevant distribution differences than raw signal comparison. The pretrained 1D-CNN extracts features encoding SOZ-relevant patterns (spikes, sharp waves, slow waves). MMD operates on these representations, measuring divergence in the space the model uses for classification. Core assumption: Intermediate features compress patient-specific nuisance variation while preserving diagnostically relevant structure.

### Mechanism 3: Test-Time Patient-Specific Adaptation Without Architecture Changes
A lightweight fine-tuning step (5 epochs) with patient weights yields consistent per-patient gains without modifying model architecture or adding complex modules. The approach keeps the 1D-CNN fixed in architecture; only the loss weighting changes per test patient. This avoids hyperparameter explosion and maintains clinical deployability. Core assumption: The pretrained weights are sufficiently generic that short, weighted fine-tuning can shift the decision boundary appropriately.

## Foundational Learning

**Concept: Distribution Shift / Domain Adaptation**
- Why needed here: The cross-patient problem is fundamentally a distribution shift issue; each patient has a different data distribution
- Quick check question: Can you explain why training and test distributions might differ in medical data, even when measuring the same underlying pathology?

**Concept: Maximum Mean Discrepancy (MMD)**
- Why needed here: MMD is the core similarity metric used to weight training patients; understanding kernel-based two-sample tests is essential
- Quick check question: What does MMD measure, and why might a kernel-based test be preferred over simple mean-difference metrics for EEG distributions?

**Concept: Fine-Tuning vs. Training from Scratch**
- Why needed here: The method relies on a pretrained model followed by weighted fine-tuning; understanding transfer learning dynamics is critical
- Quick check question: Why would fine-tuning for only 5 epochs improve performance, whereas training from scratch on weighted data might not?

## Architecture Onboarding

**Component map:**
3-second iEEG segments (1,000 Hz, single-channel) -> 10 Conv1d layers (1→32→64→128→256) with BatchNorm, ReLU, MaxPool, Dropout -> Fully-connected head (512→256→128→64→2) -> Patient-weight module (extracts last-conv features, computes MMD with multiscale/RBF kernels) -> Weighted fine-tuning (5 epochs, Adam optimizer)

**Critical path:**
1. Pretrain 1D-CNN on pooled training patients (200 epochs, unweighted cross-entropy)
2. For each test patient: forward-pass test data and all training data through pretrained model, extract last-conv features
3. Compute MMD similarity between test-patient features and each training-patient's features
4. Fine-tune pretrained model using training data weighted by similarity (5 epochs)
5. Inference on test patient using fine-tuned model

**Design tradeoffs:**
- Weighted fine-tuning vs. complex domain-adversarial architectures: trades maximal potential performance for simplicity and clinical deployability
- Last-conv layer vs. earlier layers for similarity: deeper features may be more class-relevant but less patient-discriminative; choice is empirical
- 5 fine-tuning epochs vs. more: limits overfitting to small test-patient signal but may under-adapt in extreme shifts

**Failure signatures:**
- All patient weights near-equal → similarity metric not discriminative (check feature extraction, MMD kernel bandwidth)
- Accuracy degrades after fine-tuning → overfitting to weighted subset (reduce epochs, increase regularization)
- Large per-patient variance persists → training cohort may not cover test-patient distribution (need more diverse training data)

**First 3 experiments:**
1. Replicate leave-one-patient-out baseline: train unweighted 1D-CNN on 10 patients, evaluate on held-out patient; confirm per-patient variance reported in Table II
2. Ablate similarity metric: compare MMD-Multiscale vs. MMD-RBF vs. simple Euclidean distance on raw features; analyze weight distributions and correlation with per-patient accuracy gains
3. Layer ablation for similarity: compute weights using features from different conv layers (early vs. last); assess whether earlier layers yield different weighting patterns and performance impacts

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does the choice of intermediate layer (e.g., earlier layers vs. the last convolutional layer) impact the reliability of the similarity calculation and subsequent classification accuracy?
- Basis in paper: The authors explicitly state they use the "last convolutional layer" to calculate MMD, but do not justify this specific selection or test others
- Why unresolved: Earlier layers capture different signal features (e.g., morphological spikes) than deeper layers; the optimal level of abstraction for defining patient similarity remains untested
- What evidence would resolve it: An ablation study measuring performance when calculating weights using outputs from various layers

**Open Question 2**
- Question: Can the patient-dependent weight method maintain performance gains when applied to heterogeneous, multi-center iEEG datasets?
- Basis in paper: The study validates the approach on a single-center dataset (Juntendo University) with only 11 patients who share similar pathology (focal cortical dysplasia)
- Why unresolved: Data distributions vary significantly between hospitals due to differing equipment and recording protocols, which may disrupt the MMD-based similarity mapping
- What evidence would resolve it: Leave-one-hospital-out or mixed-dataset cross-validation results showing consistent improvements across external data sources

**Open Question 3**
- Question: Is the proposed weighting strategy effective for model architectures other than the 1D-CNN, such as Transformers or Recurrent Neural Networks?
- Basis in paper: The method is implemented and tested exclusively on a "1D-CNN model," leaving its generalizability to other standard architectures unknown
- Why unresolved: Different architectures encode temporal and spatial dependencies differently, potentially affecting the validity of the MMD calculation in the feature space
- What evidence would resolve it: Experimental results applying the weighting fine-tuning method to RNN or Transformer-based classifiers on the same dataset

## Limitations
- Data access remains the primary barrier; the private Juntendo dataset cannot be validated without equivalent multi-patient iEEG with SOZ annotations
- Critical hyperparameters for MMD (kernel bandwidth, scales) were not specified, leaving fine-grained replication uncertain
- The method assumes sufficient distributional similarity between test and training patients exists; performance in extreme out-of-distribution scenarios is untested

## Confidence

**High confidence:**
- The mechanism of distribution-similarity weighted fine-tuning as described

**Medium confidence:**
- The effectiveness of intermediate-feature MMD weighting (lacks direct corpus validation)

**Low confidence:**
- Clinical generalizability due to proprietary dataset constraints

## Next Checks

1. Implement the method on a public multi-patient iEEG dataset (e.g., CHB-MIT) to verify the 10%+ accuracy gains under controlled conditions
2. Conduct an ablation study comparing MMD-weighted fine-tuning against domain-adversarial adaptation methods on the same dataset to benchmark relative performance
3. Test the method's robustness when test patients have no distributionally similar training patients to identify failure modes and limitations