---
ver: rpa2
title: 'RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs'
arxiv_id: '2509.21128'
source_url: https://arxiv.org/abs/2509.21128
tags:
- reasoning
- correct
- qwen2
- base
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel analysis framework that quantifies
  reasoning paths in large language models (LLMs) trained with reinforcement learning
  (RL) and supervised fine-tuning (SFT) on mathematical domains. The framework examines
  reasoning processes at two granularities: trajectory-level (complete reasoning outputs)
  and step-level (individual reasoning steps as nodes in reasoning graphs).'
---

# RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs

## Quick Facts
- **arXiv ID:** 2509.21128
- **Source URL:** https://arxiv.org/abs/2509.21128
- **Reference count:** 40
- **Primary result:** Novel framework quantifies reasoning path transformations: RL compresses incorrect trajectories, SFT expands correct ones, explaining optimal SFT-then-RL training.

## Executive Summary
This paper introduces a novel analysis framework to quantify how reinforcement learning (RL) and supervised fine-tuning (SFT) reshape reasoning paths in large language models (LLMs). The framework examines reasoning processes at two granularities: trajectory-level (complete reasoning outputs) and step-level (individual reasoning steps as nodes in reasoning graphs). Clustering analysis of unique reasoning trajectories reveals complementary effects: RL compresses incorrect trajectories while SFT expands correct ones, explaining why SFT followed by RL achieves optimal performance. Step-level analysis shows RL steepens (about 2.5×) while SFT flattens (reduced to about one-third) decay rates of node visitation frequency, degree, and betweenness centrality distributions in reasoning graphs. This indicates RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Global structure analysis demonstrates RL transforms community-structured reasoning graphs into hub-centralized graphs, while SFT creates globally connected graphs. These findings provide theoretical justification for current best practices in reasoning LLM training and offer practical implications for data construction and more efficient learning approaches.

## Method Summary
The method analyzes reasoning path transformations through trajectory-level and step-level analyses. Trajectory-level analysis clusters complete reasoning outputs using chrF similarity and UPGMA hierarchical clustering to count unique correct/incorrect clusters. Step-level analysis segments outputs into sentences, embeds them with BGE-large-en-v1.5, and clusters all embeddings jointly across models using K-Means (K=2000) to construct directed reasoning graphs. The framework then computes graph metrics including exponential decay rates of node visitation frequency, degree, and betweenness centrality, as well as global topology measures like modularity and centralization. The study applies this framework to four model variants (Base, RL, SFT, SFT+RL) across mathematical domains (AIME24, AIME25, AMC23), generating 256 samples per problem.

## Key Results
- RLVR compresses incorrect reasoning trajectories while SFT expands correct ones, explaining why SFT-then-RL achieves optimal Pass@1 performance
- Step-level analysis reveals RL steepens (2.5×) and SFT flattens (one-third) decay rates of node visitation frequency, degree, and betweenness centrality distributions
- RL transforms community-structured reasoning graphs into hub-centralized graphs, while SFT creates globally connected graphs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RL compresses incorrect reasoning trajectories, improving Pass@1 by redistributing probability mass away from wrong paths.
- **Mechanism:** RLVR provides binary rewards, optimizing the model to maximize correct-path probability. This amplifies correct paths and suppresses incorrect ones, reducing the diversity of wrong solutions the model produces.
- **Core assumption:** The verifiable reward signal accurately distinguishes correct from incorrect reasoning paths for the given domain.
- **Evidence anchors:**
  - [abstract] "Clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones."
  - [section 3.2] "Applying RL from either Base model or SFT model dramatically reduces the number of incorrect trajectories. This indicates that RL enhances Pass@1 through probability mass redistribution."
- **Break condition:** This mechanism fails if the reward signal is sparse, noisy, or misaligned with genuine reasoning quality, causing RL to suppress novel but valid approaches or overfit to reward-hacking shortcuts.

### Mechanism 2
- **Claim:** SFT expands the repertoire of correct reasoning trajectories by teaching solution strategies absent in the base model.
- **Mechanism:** SFT maximizes the log-likelihood of teacher-generated reasoning traces. When these traces contain diverse correct methods not well-represented in the base model's distribution, SFT allocates probability mass to these new paths.
- **Core assumption:** The teacher model (or human annotator) produces correct, diverse, and transferable reasoning traces that the base model is capable of learning.
- **Evidence anchors:**
  - [abstract] "SFT expands correct ones."
  - [section 3.2] "Applying SFT to the Base model increases the number of correct trajectories, showing that SFT teaches new solution strategies absent in the Base model."
- **Break condition:** This mechanism fails if the SFT data contains incorrect "pseudo-reasoning" paths, or if the base model's capacity is insufficient to internalize the new strategies, leading to memorization without generalization.

### Mechanism 3
- **Claim:** The two-stage SFT-then-RL training works because the stages perform complementary path modifications: SFT first expands the space of correct paths, then RL compresses the space of incorrect paths.
- **Mechanism:** The optimal Pass@1 is achieved by first ensuring a rich set of correct solution methods exists (via SFT expansion) and then concentrating the model's probability mass onto these correct methods while aggressively pruning dead-end paths (via RL compression).
- **Core assumption:** The benefits of SFT expansion and RL compression are sequential and do not significantly interfere; i.e., RL does not inadvertently compress the newly acquired correct paths from SFT.
- **Evidence anchors:**
  - [abstract] "This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps."
  - [section 3.2] "The two-stage SFT+RL procedure demonstrates complementary mechanisms: SFT expands correct trajectories while subsequent RL compresses incorrect trajectories."
- **Break condition:** This mechanism breaks if the RL stage's optimization landscape, starting from the SFT checkpoint, becomes too narrow, causing catastrophic forgetting of the diverse correct strategies or failing to prune incorrect paths inherited from the SFT data.

## Foundational Learning
- **Concept: Reasoning Trajectory vs. Reasoning Graph**
  - **Why needed here:** The paper's core analysis operates at two levels of granularity. Understanding that a "trajectory" is a complete output sequence (like a solution essay) while the "reasoning graph" is a network of reusable reasoning steps (like a map of possible thoughts) is prerequisite to grasping the paper's "squeeze vs. expand" thesis.
  - **Quick check question:** Can you explain why the same reasoning step (e.g., "apply the quadratic formula") might appear as a node in a graph but as part of many different trajectories?

- **Concept: Exponential Decay Rate in Rank Plots**
  - **Why needed here:** The paper quantifies "squeezing" and "expanding" by measuring how steeply metrics like node visitation frequency decay when nodes are ranked by popularity. A steeper decay (higher β) means a few nodes dominate; a flatter decay (lower β) means usage is more evenly spread.
  - **Quick check question:** If a model's reasoning graph has a higher exponential decay rate for node degree, would you expect it to have more or fewer "hub" nodes? What does this imply about the diversity of its reasoning steps?

- **Concept: Pass@k vs. Pass@1 Trade-off**
  - **Why needed here:** The paper uses this trade-off to interpret its findings. RL improves Pass@1 by narrowing the distribution but can hurt Pass@k at large k by reducing diversity. SFT can improve Pass@k by adding diversity but may not help Pass@1 if it also preserves incorrect paths.
  - **Quick check question:** A model scores 40% on Pass@1 but 90% on Pass@256. What does this suggest about the shape of its solution probability distribution compared to a model with 60% Pass@1 and 65% Pass@256?

## Architecture Onboarding

**Component Map:**
1.  **Model Variants:** Base (pretrained), RL (Base+RLVR), SFT (Base+distillation), SFT+RL (SFT checkpoint+RLVR).
2.  **Trajectory-Level Analyzer:**
    - Sampler (generates M outputs per problem).
    - Clusterer (groups similar trajectories using chrF similarity & UPGMA).
    - Counter (tallies unique correct/incorrect clusters).
3.  **Step-Level Analyzer:**
    - Segmenter (splits outputs into sentence-like steps).
    - Embedder (maps steps to vectors, e.g., via BGE-large-en-v1.5).
    - Graph Builder (clusters all step vectors across all models/problems into nodes, builds directed graphs of node transitions per model).
    - Metric Engine (computes graph metrics: decay rate β, modularity, centrality, graphlets).
4.  **Evaluator:** Computes Pass@k scores for reference.

**Critical Path:**
`Load Models` → `Generate Samples (M=256)` → `Branch`:
  - Path A: `Cluster Trajectories` → `Count Unique Correct/Incorrect` → `Visualize`.
  - Path B: `Segment & Embed Steps` → `Cluster All Steps into Node Space` → `Build Reasoning Graph per Model` → `Compute Graph Metrics (β, etc.)` → `Compare Across Model Types`.

**Design Tradeoffs:**
- **Analysis Granularity:** Trajectory-level is faster but coarser; step-level is computationally intensive but reveals fine-grained functional consolidation.
- **Node Definition:** Clustering steps across all models into a shared node space enables direct comparison but may conflate semantically similar steps from different reasoning contexts.
- **Model Selection:** Using public, diverse model families (Qwen, DeepSeek-R1 distill, AceReason) strengthens generalization claims but introduces uncontrolled variables like training data and compute.

**Failure Signatures:**
- **Incoherent Node Space:** If step embeddings/clustering is poor, the "reasoning graph" will be a meaningless hairball, and derived metrics (β, modularity) will be uninterpretable.
- **Metric Instability:** Small changes in clustering parameters (e.g., K for K-means) causing large swings in decay rate β, indicating the measurement is not robust.
- **Contradictory Findings:** If, for a model, RL *increases* the number of incorrect unique trajectories, the core "RL squeezes" mechanism may not hold, suggesting issues with the reward model, training, or domain.

**First 3 Experiments:**
1.  **Reproduce the Trajectory-Level Shift:** For a single model size (e.g., 7B) and dataset (e.g., AIME24), sample M=128 outputs from Base, SFT, and RL models. Cluster and plot the (correct_clusters, incorrect_clusters) coordinates. Confirm the RL point moves down (fewer incorrect) and the SFT point moves right (more correct).
2.  **Estimate a Decay Rate β:** Build the reasoning graph for the Base and RL models on a single problem. Plot the log-linear rank plot for node visitation frequency. Perform linear regression to estimate β. Verify that the RL model's β is higher.
3.  **Visualize Global Topology Change:** Generate and visually compare the reasoning graphs for Base, RL, and SFT models on the same problem (as in Fig 3). Qualitatively confirm the Base graph looks modular, the RL graph looks hub-centralized, and the SFT graph looks more globally connected. Compute modularity and Freeman centralization scores to quantify this shift.

## Open Questions the Paper Calls Out
- **Open Question 1:** Do the "squeezing" and "expanding" mechanisms of RL and SFT generalize to non-mathematical domains like coding or scientific reasoning?
  - **Basis in paper:** [explicit] The Limitations section states the study focused on verifiable mathematical domains, leaving others for future work.
  - **Why unresolved:** The observed trajectory compression and step-level homogenization were only validated on math datasets (AIME/AMC).
  - **What evidence would resolve it:** Replicating the trajectory-level and step-level graph analysis on models trained for coding (e.g., HumanEval) or scientific domains.

- **Open Question 2:** Can applying RL selectively to only functional steps (hub or central nodes) improve reasoning efficiency and performance?
  - **Basis in paper:** [explicit] The Discussion suggests that applying RL only to functional steps could improve performance and enable more efficient learning.
  - **Why unresolved:** Current RLVR methods optimize the entire trajectory; the impact of isolating the loss to specific high-centrality graph nodes is untested.
  - **What evidence would resolve it:** Experiments implementing a training loss masked to high-frequency or high-betweenness nodes in the reasoning graph.

- **Open Question 3:** Do RL methods with exploration bonuses prevent the "squeezing" of the reasoning graph, or do they actively expand it similar to SFT?
  - **Basis in paper:** [explicit] The Discussion asks whether approaches with exploration bonuses merely prevent collapse or truly expand the reasoning graph.
  - **Why unresolved:** It is unclear if these exploration strategies simply preserve the base model's diversity or introduce structural expansion like SFT.
  - **What evidence would resolve it:** Analyzing the exponential decay rates and topology of reasoning graphs trained with exploration bonuses compared to standard RLVR.

## Limitations
- **Prompt Template Sensitivity:** The paper explicitly states that accuracy is highly sensitive to prompt templates (e.g., Qwen vs R1 style, handling of `|` and special tokens), introducing variability in the analysis.
- **Clustering Parameter Stability:** Graph topology metrics (e.g., decay rate β, modularity) are sensitive to K-Means clustering parameters (K=2000, initialization), affecting reliability of comparisons across models.
- **Model Architecture Constraints:** The 1.5B/7B models have a 4096-token context limit, while larger models use 16000, necessitating separate handling that may introduce confounding factors.

## Confidence
- **High Confidence:** The core finding that RL compresses incorrect trajectories and SFT expands correct ones is directly supported by clustering analysis of unique reasoning trajectories.
- **Medium Confidence:** The step-level analysis showing RL steepens and SFT flattens decay rates in reasoning graphs is robust but depends on the stability of embedding and clustering procedures.
- **Low Confidence:** The global topology changes (hub-centralized vs. modular graphs) are qualitatively descriptive and may be sensitive to the specific problems and model variants chosen.

## Next Checks
1. **Replicate Prompt Sensitivity:** Systematically vary prompt templates (e.g., Qwen vs R1 style) for a single model and dataset to quantify the impact on unique trajectory counts and graph metrics.
2. **Test Clustering Robustness:** Reproduce the step-level analysis using different K-Means initializations and libraries (RAPIDS vs scikit-learn) to assess the stability of decay rate β and modularity scores.
3. **Validate with Diverse Problems:** Extend the analysis to a broader set of mathematical problems beyond AIME24/25 and AMC23 to confirm the generality of the "squeeze" and "expand" mechanisms across different reasoning domains.