---
ver: rpa2
title: Agentic AI Governance and Lifecycle Management in Healthcare
arxiv_id: '2601.15630'
source_url: https://arxiv.org/abs/2601.15630
tags:
- agent
- governance
- lifecycle
- agents
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of agent sprawl in healthcare,
  where the proliferation of autonomous AI agents leads to duplicated capabilities,
  unclear accountability, inconsistent controls, and security risks. To tackle this,
  the authors propose the Unified Agent Lifecycle Management (UALM) framework, a five-layer
  governance model: (1) identity and persona registry, (2) orchestration and cross-domain
  mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with
  kill-switch triggers, and (5) lifecycle management and decommissioning.'
---

# Agentic AI Governance and Lifecycle Management in Healthcare

## Quick Facts
- arXiv ID: 2601.15630
- Source URL: https://arxiv.org/abs/2601.15630
- Reference count: 37
- The paper proposes the Unified Agent Lifecycle Management (UALM) framework to address agent sprawl in healthcare through a five-layer governance model with defined KPIs and maturity levels.

## Executive Summary
The paper addresses the challenge of agent sprawl in healthcare, where the proliferation of autonomous AI agents leads to duplicated capabilities, unclear accountability, inconsistent controls, and security risks. To tackle this, the authors propose the Unified Agent Lifecycle Management (UALM) framework, a five-layer governance model: (1) identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning. UALM provides a structured, auditable control plane for agent registration, authorization, monitoring, and retirement, aligning with healthcare compliance requirements like HIPAA. The framework includes a maturity model and key performance indicators (KPIs) such as % of agents with named owners, median credential revocation time, and PHI-minimization rates. This approach enables safer scaling of agentic AI in healthcare while preserving innovation and ensuring accountability.

## Method Summary
The UALM framework is a conceptual five-layer architecture for governing autonomous AI agents in healthcare. It provides a control plane for agent registration, authorization, monitoring, and retirement without specifying implementation details, algorithms, or technology stack. The framework defines seven KPIs including agent ownership tracking, credential revocation time, policy decision rates, and PHI minimization metrics, along with a maturity model ranging from ad-hoc deployment (level 0) to automated governance (level 4).

## Key Results
- UALM framework provides structured governance through five layers addressing identity, orchestration, memory, enforcement, and lifecycle
- Seven KPIs defined for measuring governance effectiveness including orphan agent count and control drift rate
- Maturity model established with four levels from basic registration to fully automated governance
- Framework aligns with HIPAA requirements while enabling innovation through systematic controls

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A centralized identity registry is hypothesized to reduce agent sprawl by enforcing ownership and eliminating duplicate capabilities.
- **Mechanism:** By requiring every agent to register with a "named accountable owner" and defined clinical scope (Layer 1), the system creates a single source of truth. This allows leadership to identify "orphan agents" and redundant functionalities before they become security risks.
- **Core assumption:** Departments will comply with a central registry rather than deploying shadow agents; the paper notes this requires "disciplined change control."
- **Evidence anchors:** [abstract] Mentions "duplicated capabilities, unclear accountability" as the core failure mode the framework addresses. [section 3.2] Defines Layer 1 as a "single system of record" enforcing the "principle of least privilege." [corpus] Neighbor papers (e.g., "Securing Agentic AI Systems") support the need for multilayer security frameworks, but do not provide direct evidence validating the UALM registry specifically.
- **Break condition:** If the registry update latency exceeds the rate of agent deployment, the inventory will be perpetually outdated.

### Mechanism 2
- **Claim:** PHI-bounded context (Layer 3) minimizes privacy risks by segmenting memory and enforcing data retention limits.
- **Mechanism:** The framework isolates patient data using "Vectorized PHI Sharding" and "Temporal Memory." This restricts agents to the "minimum necessary data" required for a task, theoretically preventing unauthorized longitudinal access to patient history.
- **Core assumption:** Complex clinical context can be effectively vectorized and segmented without destroying the semantic reasoning required for patient care.
- **Evidence anchors:** [abstract] Proposes "PHI-bounded context and memory" as a key control layer. [section 3.2] Describes Layer 3 components as guarantees that agents access only necessary information. [corpus] External papers discuss general compliance in NLP lifecycle management, but lack specific evidence on the efficacy of "Vectorized PHI Sharding" in agentic systems.
- **Break condition:** If the retrieval augmentation (RAG) process requires cross-shard context to answer a clinical query, the agent may fail or require excessive privilege escalation.

### Mechanism 3
- **Claim:** Runtime policy enforcement with kill-switches (Layer 4) prevents autonomous agents from executing unsafe actions.
- **Mechanism:** "Governance-as-Code" (GAC) intercepts agent actions in real-time against a policy engine. If an action violates constraints (e.g., modifying a drug dosage without human oversight), the "supervisor agent" triggers a kill-switch.
- **Core assumption:** The computational overhead of checking every action does not introduce dangerous latency in critical care scenarios (e.g., sepsis alerts).
- **Evidence anchors:** [abstract] Highlights "runtime policy enforcement with kill-switch triggers" as a distinct layer. [section 4] Acknowledges the limitation: "centralized control... adds computational overhead and introduces latency." [corpus] "Agentic Business Process Management" papers emphasize the need for governance in stochastic systems, supporting the theoretical need for such guards.
- **Break condition:** If the supervisor agent becomes a single point of failure, the entire fleet may halt (false positive kill-switch) or run without oversight (failure to intercept).

## Foundational Learning

- **Concept: Agent Sprawl**
  - **Why needed here:** This is the primary failure mode the paper addresses. Without understanding how unmanaged agent proliferation creates "duplicated efforts" and "ambiguous ownership," the need for a 5-layer framework is unclear.
  - **Quick check question:** Can you map all current AI agents in your organization to a specific owner and expiration date?

- **Concept: Non-Human Identity (NHI)**
  - **Why needed here:** The framework treats agents not as scripts, but as "teammates" with distinct identities. Layer 1 relies on NHI certificates to manage accountability and revocation.
  - **Quick check question:** How would you revoke an agent's access credentials if its developer left the organization?

- **Concept: Governance-as-Code (GAC)**
  - **Why needed here:** Layer 4 moves governance from static documents to executable code. Understanding that policies must be machine-readable is essential for the "Guardrail" layer.
  - **Quick check question:** Is your current AI safety policy a PDF document or an API endpoint that can block a request?

## Architecture Onboarding

- **Component map:** Registry -> Orchestrator -> Memory -> Guardrails -> Lifecycle
- **Critical path:** Start with **Layer 1**. You cannot monitor, revoke, or decommission an agent you do not know exists. The paper suggests tracking "% of agents with a named accountable owner" as the first KPI.
- **Design tradeoffs:**
  - **Latency vs. Safety:** The paper admits centralized control adds overhead. In critical care (e.g., ICU monitoring), you must balance the risk of a delayed response against the risk of an unmonitored action.
  - **Autonomy vs. Control:** A rigid implementation "could defeat agent autonomy," turning agents into simple scripts.
- **Failure signatures:**
  - **Zombie Agents:** Agents running with credentials that should have been revoked (Failure of Layer 5).
  - **Orphan Agents:** Agents without a named owner in the registry (Failure of Layer 1).
  - **Security Theater:** Policies exist in the registry but are not enforced at runtime (Failure of Layer 4).
- **First 3 experiments:**
  1. **Inventory Audit:** Scan the network for AI agents/tools and attempt to map them to owners. Count the "orphan agents."
  2. **Kill-Switch Drill:** Deploy a test agent with a specific expiration. Verify that Layer 5 automatically revokes its credentials and freezes memory at the deadline.
  3. **PHI Access Test:** Task an agent to retrieve data outside its defined scope. Verify if Layer 3 blocks the retrieval or Layer 4 flags the policy violation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the computational overhead and latency impact of the UALM framework's centralized control plane on real-time clinical workflows?
- **Basis in paper:** [explicit] The authors state in the Discussion that "future research is needed to fully evaluate the proposed framework and address its shortcomings, focusing on performance, computational overhead..."
- **Why unresolved:** The paper provides a theoretical blueprint derived from synthesis but lacks empirical data from a deployed implementation to measure system drag.
- **What evidence would resolve it:** Empirical benchmarks from a UALM prototype measuring latency and resource consumption against a baseline in a live clinical environment.

### Open Question 2
- **Question:** How can "ground truth" be algorithmically defined for the orchestration layer to resolve clinical disagreements without mandatory human intervention?
- **Basis in paper:** [explicit] The paper notes that "defining ground truth in clinical disagreements remains a challenge, requiring human intervention to resolve them."
- **Why unresolved:** While Layer 2 proposes orchestration, the specific logic for automating dispute resolution when clinical goals collide is identified as a limitation.
- **What evidence would resolve it:** A validated algorithm or protocol that successfully automates conflict resolution between agents in simulated clinical conflict scenarios.

### Open Question 3
- **Question:** How can the architecture mitigate the risk of single points of failure inherent in centralized orchestration systems?
- **Basis in paper:** [explicit] The authors acknowledge that "Centralized orchestration and control plane could introduce single points of failure, bottlenecks, and resiliency challenges."
- **Why unresolved:** The proposed model relies on centralization for safety (auditability/kill-switches), creating a structural tension with system reliability requirements.
- **What evidence would resolve it:** Architectural patterns or failure-mode simulations demonstrating that the governance layer can maintain availability during partial system outages.

## Limitations
- Implementation Gap: Framework lacks operational specifications for critical components like orchestration algorithms and PHI memory sharding mechanisms
- Empirical Validation Gap: No quantitative validation data or measured KPIs from real or simulated healthcare environments
- Single Point of Failure Risk: Centralized governance-as-code architecture creates potential system-wide vulnerabilities without redundancy strategies

## Confidence
- **High Confidence** (Mechanism 1 - Identity Registry): The theoretical foundation for centralized agent registration is well-established in identity management literature
- **Medium Confidence** (Mechanism 2 - PHI-bounded Context): Data minimization principle is sound but specific vector segmentation approach lacks clinical validation
- **Low Confidence** (Mechanism 3 - Runtime Kill-switches): Governance-as-code concept is proven but real-time clinical application with stochastic agents remains untested

## Next Checks
1. **Orphan Agent Audit**: Deploy the Layer 1 registry schema and conduct a comprehensive scan of existing AI agents across the healthcare organization. Measure the percentage of agents lacking named accountable owners and track the time required to establish proper ownership for each orphan agent.

2. **Kill-Switch Performance Benchmark**: Implement a test environment with Layer 4 policy enforcement intercepting agent actions. Measure the median and 95th percentile latency for policy decisions across different action types (read, write, delete operations on patient data). Identify the latency threshold at which clinical response times become unacceptable for critical alerts.

3. **PHI Access Pattern Analysis**: Deploy a prototype Layer 3 memory system with access controls. Conduct controlled experiments where agents request data across their defined scope boundaries. Measure the false positive rate (legitimate requests blocked) versus true positive rate (unauthorized access prevented), and assess whether the segmentation preserves sufficient clinical context for accurate decision-making.