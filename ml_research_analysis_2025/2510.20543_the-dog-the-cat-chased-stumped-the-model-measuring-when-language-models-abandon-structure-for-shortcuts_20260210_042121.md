---
ver: rpa2
title: 'The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon
  Structure for Shortcuts'
arxiv_id: '2510.20543'
source_url: https://arxiv.org/abs/2510.20543
tags:
- entity
- semantic
- complexity
- sentence
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CenterBench, a dataset of 9,720 comprehension
  questions on center-embedded sentences that vary in syntactic complexity and semantic
  plausibility. By comparing performance on matched plausible vs.
---

# The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts

## Quick Facts
- **arXiv ID:** 2510.20543
- **Source URL:** https://arxiv.org/abs/2510.20543
- **Reference count:** 40
- **Primary result:** Language models show systematic accuracy degradation with syntactic complexity, with widening performance gaps between plausible and implausible sentences indicating increased reliance on semantic shortcuts.

## Executive Summary
This paper introduces CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences that vary in syntactic complexity and semantic plausibility. By comparing performance on matched plausible vs. implausible sentences, the study quantifies when language models shift from structural analysis to semantic shortcuts. Results show consistent accuracy degradation with complexity and widening performance gaps (median 26.8 percentage points) between plausible and implausible sentences, indicating increased reliance on semantic associations. Reasoning models improve accuracy but still fail to trace syntactic dependencies, with traces revealing semantic shortcuts, overthinking, and answer refusal. Humans show inconsistent semantic effects, suggesting different processing strategies. CenterBench provides a systematic framework to identify when models abandon structural parsing for pattern matching.

## Method Summary
CenterBench uses 360 center-embedded sentences (180 plausible, 180 implausible) across 6 complexity levels, with each sentence paired with 6 question types. Plausible sentences are generated using templates, then implausible variants are created via circular verb swapping. The evaluation pipeline uses a 4-tier matching system: exact string match, linguistic normalization with spaCy, OOV verb handling, and semantic similarity scoring (MiniLM-L6-v2, cosine similarity >= 0.9). Three model families (Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.5 Flash) are evaluated in standard and reasoning modes. Non-reasoning models are run 10 times per question and averaged; reasoning models once.

## Key Results
- Performance gaps between plausible and implausible sentences widen systematically with complexity (median 26.8 percentage points)
- Reasoning models improve accuracy but still fail to trace syntactic dependencies, showing semantic shortcuts in traces
- On "Chain Consequence" questions, implausible sentences outperform plausible ones (20.4% vs. 14.8%), suggesting semantic familiarity actively harms performance
- Humans show inconsistent semantic effects and non-monotonic accuracy patterns across complexity levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models substitute structural parsing with semantic heuristics as syntactic complexity increases
- **Mechanism:** As center-embedding depth increases, attention mechanisms struggle to maintain long-range dependency links, defaulting to high-probability semantic associations
- **Core assumption:** Performance gap between plausible/implausible isolates semantic reliance from structural capacity
- **Evidence anchors:** Abstract showing systematic widening gaps; section 5.2 showing performance divergence at level 3; SCRAMBLe paper confirming structural binding weaknesses
- **Break condition:** If models maintain near-zero gaps between plausible/implausible at high complexity levels

### Mechanism 2
- **Claim:** Extended reasoning traces degrade performance through "semantic interference"
- **Mechanism:** Reasoning models generate intermediate steps prioritizing semantic coherence, causing them to doubt correct syntactic parses on implausible sentences
- **Core assumption:** Reasoning tokens validate semantic plausibility over structural links
- **Evidence anchors:** Section 5.5 showing reasoning traces with semantic shortcuts and overthinking; models spiraling into doubt
- **Break condition:** If reasoning models show smaller plausibility gaps than non-reasoning models without increased refusal rates

### Mechanism 3
- **Claim:** Semantic plausibility impairs performance on tasks requiring strict causal tracing
- **Mechanism:** Strong semantic priors override specific causal chains in sentences, causing models to ignore sentence logic
- **Core assumption:** Models cannot suppress parametric knowledge when performing causal reasoning on familiar concepts
- **Evidence anchors:** Section 5.3 showing implausible outperforming plausible on Chain Consequence questions (20.4% vs. 14.8%); abstract noting plausibility harms resulting action questions
- **Break condition:** If Chain Consequence performance scales linearly with plausibility

## Foundational Learning

- **Concept: Center-Embedding & Recursion**
  - **Why needed here:** This is the structural stressor creating nested dependencies that strain context windows
  - **Quick check question:** In "The fly that the bird that the snake hissed at saw died," which agent performed the action "saw"?

- **Concept: Semantic vs. Syntactic Decoupling**
  - **Why needed here:** The core evaluation separates these two; a sentence can be syntactically valid but semantically implausible
  - **Quick check question:** If a model answers "Who prescribed medicine?" with "The doctor" for "The mailman prescribed medicine," is it failing syntax or just relying on semantics?

- **Concept: Evaluation Pipeline Matching**
  - **Why needed here:** The paper uses multi-tier matching (Exact -> Lemma -> Semantic Similarity) to grade open-ended generation
  - **Quick check question:** Why is simple exact-match string comparison insufficient for evaluating model responses to "What series of events led to X?"

## Architecture Onboarding

- **Component map:** Generator (Circular Verb-Swapping) -> Validator (Rule-based checker) -> Prober (Question Templates) -> Evaluator (4-tier pipeline)
- **Critical path:**
  1. Generate Pairs: Create plausible sentences, apply circular swapping for implausible variants
  2. Validate: Filter temporal violations
  3. Analyze: Measure "Gap" (Plausible Acc - Implausible Acc) across complexity levels
- **Design tradeoffs:**
  - Artificiality vs. Control: Implausible sentences are semantically violent to ensure clear signal, sacrificing naturalism
  - Metric Sensitivity: 0.9 cosine similarity threshold balances strictness with flexibility
- **Failure signatures:**
  - Semantic Collapse: Implausible accuracy drops faster than plausible as Level increases
  - Refusal: Model outputs "No response" or "Not applicable" for implausible events
  - Hallucinated Plausibility: Model answers "The doctor" when sentence explicitly says "The mailman"
- **First 3 experiments:**
  1. Baseline Run: Evaluate target model on CenterBench (Levels 1-6), plot Accuracy vs. Complexity for both subsets
  2. Trace Analysis: Enable thinking mode and inspect logs for Chain Consequence questions, looking for semantic vs. structural reasoning
  3. Ablation on Verbs: Modify generator to use nonsense verbs instead of swapped real verbs to isolate implausibility effect

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What cognitive mechanisms allow humans to exhibit variable semantic effects and non-monotonic accuracy on complex embeddings, whereas models show systematic semantic reliance and linear degradation?
- **Basis in paper:** Section 5.6 documents humans showing "variable semantic effects" and fluctuating accuracy contrasting with models' systematic widening plausibility advantage
- **Why unresolved:** Paper documents divergent behavioral patterns but doesn't isolate specific human processing strategies (like "good enough" processing) that differ from model attention mechanisms
- **What evidence would resolve it:** Eye-tracking or neuroimaging studies comparing human processing loads against model attention weights during center-embedding tasks

### Open Question 2
- **Question:** Can reasoning models be trained to explicitly suppress "semantic shortcuts" and "overthinking" behaviors without losing accuracy advantages?
- **Basis in paper:** Section 5.5 reveals reasoning models suffer from self-induced errors through overthinking and semantic interference despite improved accuracy
- **Why unresolved:** Paper characterizes failure modes in traces but doesn't propose architectural or training interventions to enforce faithful structural parsing over semantic bias
- **What evidence would resolve it:** Fine-tuning experiments penalizing models for utilizing semantic priors when syntactic cues are sufficient

### Open Question 3
- **Question:** Does proficiency in parsing center-embedded, implausible sentences predict ability to resist hallucinations or parametric bias in real-world tasks?
- **Basis in paper:** Limitations section notes deep embeddings are rare but defends them as stress tests for structural processing
- **Why unresolved:** While CenterBench diagnoses structural abandonment in synthetic settings, correlation with robustness in natural domains is not validated
- **What evidence would resolve it:** Correlational analysis between CenterBench scores and performance on benchmarks testing context-parametric knowledge conflicts

## Limitations
- The implausible sentences use absurd verb-object combinations that may be out-of-distribution rather than structurally challenging
- Performance gaps conflate semantic plausibility with model familiarity, as implausible sentences may simply be unfamiliar
- Human data showing inconsistent semantic effects suggests our understanding of model processing may be incomplete

## Confidence
- **High:** The empirical observation that performance degrades with complexity and plausibility gaps widen is well-supported by the data
- **Medium:** The interpretation that widening gaps indicate structural abandonment is plausible but not definitively proven; alternative explanations exist
- **Low:** The claim that reasoning models "overthink" and that semantic plausibility "actively harms" causal reasoning is based on limited evidence and requires further validation

## Next Checks
1. Test models on CenterBench variants where implausible sentences use nonsense verbs rather than swapped real verbs to isolate semantic implausibility from syntactic unfamiliarity
2. Conduct controlled human experiments with the same sentences to establish baseline processing strategies and validate whether the plausibility gap pattern holds for humans
3. Analyze attention patterns in transformer models on center-embedded sentences to directly observe whether long-range dependencies are maintained or abandoned as complexity increases