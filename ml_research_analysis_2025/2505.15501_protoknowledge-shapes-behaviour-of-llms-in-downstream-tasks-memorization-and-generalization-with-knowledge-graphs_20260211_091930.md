---
ver: rpa2
title: 'Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization
  and Generalization with Knowledge Graphs'
arxiv_id: '2505.15501'
source_url: https://arxiv.org/abs/2505.15501
tags:
- protoknowledge
- knowledge
- tasks
- dbpedia
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work formalizes protoknowledge\u2014the internalized and\
  \ reusable knowledge acquired by LLMs from Knowledge Graphs\u2014and shows it strongly\
  \ shapes Text-to-SPARQL performance. Three forms are identified: lexical (label-to-URI\
  \ mapping), hierarchical (taxonomic reasoning), and topological (multi-hop relational\
  \ paths)."
---

# Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2505.15501
- **Source URL:** https://arxiv.org/abs/2505.15501
- **Reference count:** 31
- **Primary result:** Protoknowledge—internalized KG-derived knowledge—shapes LLM Text-to-SPARQL performance, with topological protoknowledge most predictive of success, especially in no-URI settings.

## Executive Summary
This work formalizes protoknowledge as the internalized and reusable knowledge LLMs acquire from Knowledge Graphs, demonstrating it strongly shapes downstream Text-to-SPARQL performance. Three forms are identified: lexical (label-to-URI mapping), hierarchical (taxonomic reasoning), and topological (multi-hop relational paths). Protoknowledge is measured via targeted Knowledge Activation Tasks (KATs) and found to reflect semantic bias from pretraining data, with higher performance on popular KG items. Analysis reveals that topological protoknowledge is most predictive of success, especially in no-URI settings, while lexical and hierarchical forms also contribute. The proposed framework enables indirect detection of semantic-level data contamination in closed-pretraining models.

## Method Summary
The study introduces a framework for detecting and measuring protoknowledge—internalized KG knowledge—by designing Knowledge Activation Tasks (KATs) that probe lexical, hierarchical, and topological knowledge forms. Text-to-SPARQL generation tasks are then analyzed against KAT performance to establish correlations. The SPS (Speculative Protoknowledge for SPARQL) metric quantifies topological reasoning by testing triple completion ability. Models are tested under different prompt conditions (Original, No Label, No URI) to isolate reliance on protoknowledge versus contextual information.

## Key Results
- Protoknowledge reflects semantic bias from pretraining data, with higher performance on popular KG items
- Topological protoknowledge is most predictive of Text-to-SPARQL success, especially in no-URI settings
- Lexical and hierarchical protoknowledge forms also contribute to performance, with trade-offs between protoknowledge reliance and context availability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Protoknowledge acquisition is driven by statistical frequency of KG items in pretraining corpus, creating semantic bias
- **Mechanism:** LLMs internalize token sequences from KG triples during pretraining. Frequent entities/relations develop stronger associations, facilitating easier activation during inference
- **Core assumption:** KG item popularity correlates with pretraining data presence
- **Evidence anchors:** Abstract confirms protoknowledge reflects semantic bias with higher performance on popular items; section 4.5 shows popular entities/properties have higher accuracy
- **Break condition:** If models are pretrained on curated corpora filtered to remove high-frequency web entities, KG popularity-protoknowledge correlation should weaken

### Mechanism 2
- **Claim:** Text-to-SPARQL in "no-URI" settings relies heavily on topological protoknowledge—model's ability to infer valid relational paths
- **Mechanism:** Without URI-to-label mappings, models must activate internalized graph structures to generate queries. SPS score (valid triple reconstruction) predicts query generation success
- **Core assumption:** Models encode graph structure distinct from memorizing specific labels
- **Evidence anchors:** Abstract identifies topological protoknowledge as most predictive in no-URI settings; section 5.1 shows Agreement exceeds Disagreement in No URI
- **Break condition:** If models succeed at Text-to-SPARQL in No URI but fail triple-completion tasks, mechanism is likely template retrieval rather than topological reasoning

### Mechanism 3
- **Claim:** Protoknowledge activation acts as scaffold that trades off with in-context information density
- **Mechanism:** Models switch between retrieving internal knowledge and processing provided context based on prompt richness
- **Core assumption:** Generation process can dynamically switch between internal knowledge retrieval and context processing
- **Evidence anchors:** Section 5.1 shows shift toward contextual information in Original setting; GPT-3.5 achieves 100% Positive Agreement in No URI
- **Break condition:** If increasing prompt richness doesn't reduce correlation between KAT scores and SPARQL success, trade-off mechanism doesn't hold

## Foundational Learning

- **Knowledge Graph Triples (RDF):** Understanding Subject-Predicate-Object structure is essential for defining lexical, hierarchical, and topological protoknowledge
  - *Quick check:* Can you distinguish between an entity (e.g., `wd:Q405`) and a property (e.g., `wd:P31`) in a triple?

- **Data Contamination vs. Generalization:** Framework detects whether models are solving tasks through internalized knowledge versus remembering test data from pretraining
  - *Quick check:* If a model aces a test because it saw questions during training, is that generalization?

- **URI Resolution:** Lexical protoknowledge involves mapping human-readable labels to non-human-readable identifiers
  - *Quick check:* Why is `http://dbpedia.org/resource/Moon` easier for an LLM to "guess" than `http://www.wikidata.org/entity/Q405`?

## Architecture Onboarding

- **Component map:** Natural Language Question + (Optional) URI/Label Context -> Knowledge Activation Tasks (KATs) -> Text-to-SPARQL generation -> Analysis Layer (Positive Agreement, SPS)
- **Critical path:** Validating the SPS metric—must confirm triple completion ability correlates with SPARQL generation success
- **Design tradeoffs:**
  - DBpedia vs. Wikidata: Wikidata has non-human-readable URIs (better for testing pure lexical protoknowledge), DBpedia has human-readable URIs (better for structural/topological reasoning)
  - Closed vs. Open Models: Framework designed for closed-pretraining models where training data cannot be inspected
- **Failure signatures:**
  - High Original/Low No-URI: Indicates low protoknowledge, model relying on prompt
  - High Lexical/Low Topological: Model knows entity names but cannot connect them structurally
- **First 3 experiments:**
  1. **URI Recognition (Lexical):** Prompt with "Moon" and ask for `wd:Q405`, split by entity popularity to verify semantic bias
  2. **SPS Calculation (Topological):** Extract triples from gold SPARQL query, mask one element, ask model to fill it, calculate SPS score
  3. **Correlation Check:** Compare SPS score against Text-to-SPARQL F1 in No-URI setting to verify Agreement mechanism

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on proxy metrics (particularly SPS) may confound triple completion with template memorization or common sense reasoning
- Focus on limited model architectures (GPT variants) and KGs (DBpedia/Wikidata) raises generalizability questions
- Closed pretraining assumption prevents direct validation of whether specific triples were present in training data

## Confidence
- **Protoknowledge framework identification:** High
- **Semantic bias correlation:** Medium
- **Topological protoknowledge as SPARQL predictor:** Medium-High
- **Trade-off mechanism:** Low-Medium

## Next Checks
1. **SPS Correlation Validation:** Systematically mask different triple elements in successful SPARQL queries and measure completion accuracy to validate topological reasoning claim for complex cases

2. **Popularity Bias Robustness Test:** Construct synthetic KG subsets with controlled popularity distributions and retrain smaller models to test whether semantic bias stems from web content patterns rather than KG-specific factors

3. **Cross-Architecture Generalization:** Apply protoknowledge framework to open-weight models (e.g., LLaMA, Mistral) with inspectable training data to determine whether framework captures fundamental LLM behavior or architecture-specific phenomena