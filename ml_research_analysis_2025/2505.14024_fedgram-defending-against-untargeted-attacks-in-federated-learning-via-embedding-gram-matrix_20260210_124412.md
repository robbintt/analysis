---
ver: rpa2
title: 'FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding
  Gram Matrix'
arxiv_id: '2505.14024'
source_url: https://arxiv.org/abs/2505.14024
tags:
- fedgram
- attacks
- local
- clients
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedGraM, a novel robust aggregation method
  to defend against untargeted attacks in federated learning. The key idea is to use
  the norm of the Gram Matrix of model embeddings as an indicator of generalization
  performance to detect and remove malicious models.
---

# FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix

## Quick Facts
- arXiv ID: 2505.14024
- Source URL: https://arxiv.org/abs/2505.14024
- Reference count: 40
- Primary result: Novel robust aggregation method using Gram matrix norms of model embeddings to detect and remove malicious models in federated learning

## Executive Summary
This paper proposes FedGraM, a robust aggregation method that defends against untargeted attacks in federated learning by leveraging the norm of the Gram matrix of model embeddings. The key insight is that malicious models exhibit higher Gram matrix norms due to disrupted inter-class separation in the embedding space. By maintaining a minimal auxiliary dataset with one sample per class, the server extracts embeddings from local models, computes Gram matrices, and removes models with the highest norms before averaging the remaining models. Extensive experiments on CIFAR10, SVHN, and CIFAR100 demonstrate that FedGraM outperforms state-of-the-art defense methods while requiring only limited auxiliary data.

## Method Summary
FedGraM is a robust aggregation method that uses the norm of the Gram matrix of normalized embeddings as a proxy for model generalization performance. During each round, the server maintains a small auxiliary dataset with one sample per class, extracts embeddings from each local model using the representation layers, computes the Gram matrix, and calculates its Frobenius norm. Models with the highest norms (typically the top 30%) are identified as potentially malicious and removed from the aggregation process. The remaining models are then averaged using FedAvg. This approach specifically targets untargeted attacks that degrade model performance by disrupting the representation layer's ability to separate classes.

## Key Results
- FedGraM achieves 74.25% accuracy on CIFAR10 under LIE attack, outperforming state-of-the-art defenses
- The method maintains high robustness across different data heterogeneity levels (Dirichlet distribution with β ∈ {0.2, 1, 10})
- Requires only minimal auxiliary data (1 sample per class) compared to other defenses needing larger validation sets
- Demonstrates strong performance particularly in cross-silo scenarios with 500 clients

## Why This Works (Mechanism)

### Mechanism 1: Inter-class Separation as a Generalization Proxy
The norm of the Gram matrix of normalized embeddings serves as a proxy for a model's generalization capability; malicious models exhibit higher norms due to disrupted inter-class separation. The server extracts embeddings P from a local model using an auxiliary dataset, calculates the Gram Matrix G = PP^T, and computes its norm. In the normalized case, off-diagonal elements G_xy represent cosine similarity between class x and y. A higher Frobenius norm implies higher similarity (poor separation) between classes, indicating a damaged representation layer. This works because untargeted attacks primarily degrade the global model by destroying the representation layer's ability to distinguish between classes, resulting in higher cosine similarity between distinct classes in the embedding space.

### Mechanism 2: Top-C Truncation for Outlier Removal
Malicious models consistently rank among the highest Gram matrix norms, allowing them to be filtered via a fixed threshold C. In every round, the server ranks uploaded models by their Gram matrix norm and discards the top C% (set to 30% in experiments) before averaging the remaining parameters. This works because the divergence in representation structure caused by attacks is sufficiently large that malicious models almost always appear as outliers with the highest norms, regardless of data heterogeneity.

### Mechanism 3: Minimal Auxiliary Data for Embedding Extraction
A strict minimum of auxiliary data (1 sample per class) is sufficient to estimate the embedding geometry. The server maintains a static dataset D_s with size equal to the number of classes K, containing exactly one data sample for each class. This is fed only through the representation layers to compute the Gram matrix. This works because the relative geometry of class embeddings is robust enough to be estimated with a single sample per class, relaxing the need for large server-side validation sets.

## Foundational Learning

- **Concept: Gram Matrix in Representation Learning**
  - Why needed here: This is the core signal used for defense. You must understand that G = P P^T for normalized vectors yields cosine similarities.
  - Quick check question: If the diagonal of the Gram matrix is always 1.0, what does a high off-diagonal value signify about the model's feature space?

- **Concept: Untargeted vs. Targeted Poisoning**
  - Why needed here: FedGraM is explicitly designed for untargeted attacks (degrading accuracy). Understanding this distinction explains why it may fail against backdoors (targeted).
  - Quick check question: Does the attacker in this paper want the model to misclassify "cats" as "dogs" (targeted) or just classify everything randomly (untargeted)?

- **Concept: Data Heterogeneity (Non-IID) in FL**
  - Why needed here: The paper claims superiority because Gram norms are robust to Non-IID data, whereas other methods confuse heterogeneity with attacks.
  - Quick check question: Why would standard deviation-based defenses struggle when client data distributions vary wildly?

## Architecture Onboarding

- **Component map:**
  - Auxiliary Dataset (D_s) -> Representation Layers (φ) -> Embedding Extraction (P) -> Gram Matrix Computation (G = PP^T) -> Norm Calculation -> Model Ranking -> Top 30% Removal -> Parameter Averaging

- **Critical path:**
  1. Receive Models: Server collects local updates
  2. Inference: Pass D_s through each local model's representation layers to get P
  3. Scoring: Normalize P, compute G = PP^T, calculate Euclidean norm of G
  4. Filtering: Identify models with norms in the top 30th percentile
  5. Aggregation: Average parameters of the remaining models

- **Design tradeoffs:**
  - Filtering Rate (C): High C (e.g., 40%) removes more malicious clients but degrades convergence by removing benign contributors. Low C (e.g., 20%) preserves data utility but risks missing adaptive attackers
  - Auxiliary Data Quality: Using random/unverified data for D_s works for general defense but fails if critical classes are missing

- **Failure signatures:**
  - Adaptive Attack Collapse: If accuracy drops significantly despite defense, check for "pure uniformity" adaptive attacks where attackers force embeddings to be orthogonal to minimize the Gram norm
  - Incomplete Class Coverage: If defense fails specifically on MinSum attacks, verify the auxiliary dataset contains the full set of class labels

- **First 3 experiments:**
  1. Norm Distribution Check: Reproduce Figure 1/4. Plot the histogram of Gram norms for benign vs. malicious clients under LIE attack to verify separation
  2. Hyperparameter Sensitivity: Run ablation on C (20%, 30%, 40%) under 10% malicious actors to confirm the "safest choice" of 30% cited in Section 5.3
  3. Adaptive Attack Validation: Implement the uniformity loss (Eq. 9) to confirm if the system is vulnerable to adaptive attacks and if combining FedGraM with Trimean restores robustness

## Open Questions the Paper Calls Out

### Open Question 1
Can FedGraM be modified to inherently withstand adaptive attacks that enforce embedding uniformity without relying on secondary statistical aggregators?
- Basis in paper: Section 6 explicitly discusses a limitation where adaptive attacks enforcing "pure uniformity" cause FedGraM-AVG performance to crash (e.g., to 22.04% on CIFAR10). The authors currently mitigate this only by combining FedGraM with Trimean.
- Why unresolved: The core detection mechanism (Gram Matrix norm) is evadable by this specific uniformity loss, forcing a dependency on other defenses rather than solving the detection failure.
- What evidence would resolve it: A modified FedGraM iteration that maintains >70% accuracy under the specific uniformity adaptive attack described in Equation 9 without using Trimean.

### Open Question 2
How can FedGraM effectively defend against advanced backdoor (targeted) attacks?
- Basis in paper: In Section B.5, the authors note they are "exploring extending this backdoor robustness to more powerful attacks in our future work" after observing preliminary results on Scaling and DBA attacks.
- Why unresolved: The method was designed for untargeted attacks; its ability to distinguish malicious updates in targeted poisoning scenarios (beyond simple scaling) remains unverified.
- What evidence would resolve it: Empirical results showing FedGraM maintaining high main task accuracy while suppressing backdoor success rates against advanced attacks like Semantic Backdoors or WaNet.

### Open Question 3
How does incomplete class coverage in the auxiliary dataset specifically degrade defense against optimization-based attacks like MinSum?
- Basis in paper: Section 5.3 and Figure 5 show that FedGraM fails to defend against MinSum attacks when the auxiliary dataset lacks full class coverage (e.g., 50% labels), unlike other attacks where it remains robust.
- Why unresolved: The paper identifies the performance drop but does not analyze why MinSum is uniquely resilient to FedGraM under partial data or how to fix it.
- What evidence would resolve it: A theoretical or empirical analysis explaining the sensitivity of the Gram Matrix norm to MinSum attack vectors when class embeddings are missing, plus a mitigation strategy.

## Limitations
- **Adaptive attack vulnerability:** The defense is explicitly breakable by attackers using "pure uniformity" objectives to minimize the Gram norm, suggesting fundamental fragility against adaptive adversaries
- **Representation layer boundary:** The exact split point between φ and v layers is unspecified for ResNet-8, potentially affecting embedding quality and defense efficacy
- **Auxiliary data dependence:** Performance critically depends on having representative samples for all classes in the auxiliary dataset, with no clear guidance on how to construct this when label coverage is uncertain

## Confidence
- **High:** Core mechanism (Gram matrix norm as proxy for inter-class separation), experimental methodology (dataset splits, attack implementations, evaluation metrics)
- **Medium:** Superiority claims over baselines in non-adaptive scenarios, hyperparameter choices (30% filter rate)
- **Low:** Robustness against adaptive attacks, generalization to scenarios with missing class labels in auxiliary data

## Next Checks
1. **Adaptive Attack Test:** Implement the "pure uniformity" attack (Section 6) to verify whether FedGraM can be evaded by malicious clients optimizing for low Gram norms while maintaining harmful behavior
2. **Auxiliary Dataset Sensitivity:** Systematically vary the auxiliary dataset size and class coverage to quantify performance degradation when critical classes are missing (particularly for MinSum attacks)
3. **Representation Layer Boundary Study:** Experiment with different φ/v split points in ResNet-8 to determine sensitivity to this architectural choice and identify optimal configuration