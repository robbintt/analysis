---
ver: rpa2
title: 'Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning
  for Efficient Supervised Fine-Tuning'
arxiv_id: '2509.23873'
source_url: https://arxiv.org/abs/2509.23873
tags:
- pruning
- sample
- token
- data
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Quadrant-based Tuning (Q-Tuning), the first
  unified framework that jointly optimizes sample-level and token-level pruning for
  efficient supervised fine-tuning of large language models. Q-Tuning uses an Error-Uncertainty
  (EU) Plane to diagnose heterogeneous utility across training data and implements
  a two-stage strategy: first, it prunes uninformative samples (harmful noise and
  redundant knowledge) at the sample level; second, it applies asymmetric token pruning
  to misconception samples while preserving calibration samples in full.'
---

# Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2509.23873
- Source URL: https://arxiv.org/abs/2509.23873
- Reference count: 40
- Q-Tuning achieves +38% average improvement over full-data baseline using only 12.5% of original training data on SmolLM2-1.7B

## Executive Summary
This paper introduces Quadrant-based Tuning (Q-Tuning), the first unified framework that jointly optimizes sample-level and token-level pruning for efficient supervised fine-tuning of large language models. Q-Tuning uses an Error-Uncertainty (EU) Plane to diagnose heterogeneous utility across training data and implements a two-stage strategy: first, it prunes uninformative samples (harmful noise and redundant knowledge) at the sample level; second, it applies asymmetric token pruning to misconception samples while preserving calibration samples in full. Across five diverse benchmarks, Q-Tuning consistently outperforms existing pruning methods and even full-data fine-tuning.

## Method Summary
Q-Tuning implements a two-stage dynamic pruning strategy per mini-batch: (1) compute sample-level perplexity and entropy via forward pass, (2) use bisect search to partition samples into four quadrants on the Error-Uncertainty plane, (3) sample-level pruning retains Q2 (misconceptions) and Q4 (calibration) while dropping Q1 (harmful noise) and Q3 (redundant knowledge), and (4) token-level pruning applies asymmetric policies where Q2 samples undergo smoothed token importance scoring with neighbor averaging (λ=0.5) and keep top r_token fraction, while Q4 samples are preserved fully. The method is evaluated across multiple model scales (7B-32B) on five benchmarks showing consistent improvements over full-data baselines.

## Key Results
- Achieves +38% average improvement over full-data baseline using only 12.5% of training data on SmolLM2-1.7B
- Outperforms existing pruning methods and full-data fine-tuning across five diverse benchmarks
- Demonstrates effectiveness across model scales from 1.7B to 32B parameters
- Ablation studies confirm quadrant-based approach is critical for performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly modeling error (perplexity) and uncertainty (entropy) partitions training data into four utility-distinct quadrants, enabling targeted pruning policies per sample type.
- **Mechanism:** Perplexity measures how "surprising" the ground-truth response is to the model; entropy measures the spread of predicted probability mass. High PPL + low entropy indicates confident error (likely noise); high PPL + high entropy indicates uncertain error (teachable misconceptions); low PPL + low entropy indicates mastered content; low PPL + high entropy indicates challenging-but-correct examples.
- **Core assumption:** Model confidence (entropy) and error (perplexity) at inference time predict training utility.
- **Evidence anchors:** [abstract]: "we introduce the Error–Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens"; [section 3.1]: Eq. 4–5 formalize PPL and Ent; Figure 1(a) illustrates quadrant definitions.

### Mechanism 2
- **Claim:** Asymmetric token pruning—surgical removal from Q2 (misconception) samples while preserving Q4 (calibration) samples intact—outperforms uniform token policies.
- **Mechanism:** Q2 samples contain both informative context and locally harmful tokens that mislead learning. Pruning high-PPL tokens from Q2 isolates the misconception signal. Q4 samples contribute to model calibration; preserving all tokens maintains reliable uncertainty modeling.
- **Core assumption:** Token-level harm is localized and identifiable via high local perplexity, and calibration samples benefit from full-sequence preservation.
- **Evidence anchors:** [abstract]: "applies an asymmetric token-pruning policy...trimming less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety"; [section 4.2, Table 4]: Ablation shows token-pruning Q2 improves performance (+1.15 avg on Qwen3-8B) while pruning Q4 degrades results.

### Mechanism 3
- **Claim:** Neighbor-aware token scoring (smoothing individual token PPL with adjacent token PPL) prevents over-pruning semantically meaningful but locally surprising tokens.
- **Mechanism:** Token-level PPL can spike for rare but important tokens (e.g., domain-specific terminology). Averaging with neighbors (λ-weighted, default 0.5) smooths isolated spikes while preserving genuinely high-PPL regions.
- **Core assumption:** Locally coherent high-PPL regions indicate harmful content; isolated spikes may be benign.
- **Evidence anchors:** [section 3.2, Eq. 7]: "s_i(x, y; f_θ) = (1−λ) PPL_i + λ[PPL_{i-1} + PPL_{i+1}]"; [section 4.2, Table 5]: λ=0.5 achieves 46.79 avg vs. 45.92 for λ=0 (no smoothing).

## Foundational Learning

- **Concept: Perplexity and predictive entropy**
  - Why needed here: The EU Plane relies on computing PPL (exp of average negative log-likelihood) and entropy (average token-level Shannon entropy) for every sample.
  - Quick check question: For a sequence where the model assigns probability 0.5 to each of two tokens at every position, what are the per-token entropy and perplexity?

- **Concept: Bilevel optimization**
  - Why needed here: Section 2 frames dynamic pruning as bilevel: outer loop finds optimal pruners (Φ, Ψ), inner loop trains the model under those pruners.
  - Quick check question: In Eq. 3, which variables are optimized in the outer objective vs. the inner objective?

- **Concept: Dynamic vs. static data pruning**
  - Why needed here: Q-Tuning re-computes EU Plane thresholds per mini-batch, adapting to evolving model state. Static methods pre-select data before training.
  - Quick check question: What information does Q-Tuning use that would be unavailable to a static pruning method?

## Architecture Onboarding

- **Component map:** Forward pass on current model → compute PPL(x,y), Ent(x,y) for batch samples → bisect search → find quantile thresholds (α, β) partitioning EU Plane into Q1–Q4 → sample pruning → retain Q2∪Q4 (target ratio r_sample) → token scoring → compute smoothed scores s_i for Q2 tokens (Eq. 7) → token pruning → keep top r_token fraction by score; Q4 tokens unchanged → backward pass → train on doubly-pruned mini-batch

- **Critical path:** The bisect search (Algorithm 1, lines 6–16) determines quadrant boundaries. If this fails to converge within budget, sample ratios deviate from target. Second critical point is token ranking for Q2 samples—incorrect scoring cascades to wrong pruning.

- **Design tradeoffs:**
  - λ (neighbor smoothing): Higher values reduce spurious pruning but may miss isolated harmful tokens. Default 0.5 is empirically validated; ablation shows sensitivity varies by task.
  - Batch size: Larger batches yield more stable threshold estimates (Figure 5a) but increase forward-pass overhead.
  - Selection overhead vs. training savings: Table 6 shows selection takes ~35–44 minutes for 3-epoch training; net savings depend on total training scale.

- **Failure signatures:**
  - Empty or near-empty Q2/Q4: Threshold search may fail if data distribution is skewed; fallback adds supplemental samples via "supp-score."
  - Aggressive pruning (low r_token on Q2): Figure 6 shows token pruning alone underperforms joint approach; over-pruning removes instructional content.
  - Small batches: Figure 5a and 9 show reduced performance with batch size 8 vs. 32, particularly on reasoning tasks.

- **First 3 experiments:**
  1. **Quadrant ablation:** Replicate Table 4/15 on a new dataset. Prune each quadrant individually (sample and token level) to verify Q1/Q3 harm and Q2 token-sensitivity on your data.
  2. **λ sensitivity sweep:** Run λ ∈ {0, 0.3, 0.5, 0.7, 1.0} on a held-out task (e.g., GSM8K) with fixed r_sample=0.125, r_token=0.5.
  3. **Budget scaling curve:** Fix r_token=0.5, vary r_sample ∈ {0.0625, 0.125, 0.25, 0.5} and compare against full-data baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Generalized Dynamic Data Pruning framework be solved via gradient-based differentiable methods rather than the heuristic bisect-search strategy?
- **Basis:** [inferred] The authors formalize the problem as a bilevel optimization (Eq. 3) but implement Q-Tuning as a heuristic solution using bisection.
- **Why unresolved:** The paper demonstrates the efficacy of the heuristic but does not explore if the pruning operators (Φ, Ψ) could be learned directly via differentiation.
- **What evidence would resolve it:** A comparative study of convergence rates and final performance between Q-Tuning and a fully differentiable pruning baseline.

### Open Question 2
- **Question:** How robust is the relative quantile-based quadrant partitioning when applied to datasets with highly skewed utility distributions?
- **Basis:** [inferred] The bisect search relies on quantile thresholds (α, β) to partition the EU Plane, implicitly assuming a distribution where noise and redundancy are spread such that relative cuts are meaningful.
- **Why unresolved:** If a dataset contains extreme outliers or near-zero noise, relative quantiles might misclassify valuable data or fail to isolate the harmful noise (Q1).
- **What evidence would resolve it:** Experiments on synthetic datasets with artificially manipulated noise ratios or highly non-standard error distributions.

### Open Question 3
- **Question:** Can the Error-Uncertainty (EU) Plane diagnostic be effectively extended to multimodal or long-context settings?
- **Basis:** [inferred] The evaluation is restricted to text-based LLMs, whereas token utility in multimodal models (e.g., visual tokens) may not correlate linearly with perplexity or entropy.
- **Why unresolved:** The relationship between error, uncertainty, and token salience may differ fundamentally for non-text modalities or ultra-long contexts.
- **What evidence would resolve it:** Applying Q-Tuning to Vision-Language Models (VLMs) to analyze if high-PPL visual tokens correspond to valuable misconceptions or mere noise.

## Limitations
- Dataset and task bias: Evaluation spans only five benchmarks with alignment and reasoning tasks; 38% improvement on SmolLM2-1.7B may not generalize to non-English or multimodal domains
- Hyperparameter sensitivity: Optimal settings likely vary by model scale and task; paper reports default hyperparameters but doesn't explore interaction effects
- Computational overhead: Forward-pass requirement for PPL/entropy computation per mini-batch may offset savings for smaller models

## Confidence
- **High confidence**: Quadrant-based sample pruning (Q1/Q3 harm, Q2/Q4 preserve) — validated by multiple ablations and logical consistency
- **Medium confidence**: Asymmetric token pruning efficacy — supported by Table 4 but lacks comparison to alternative policies
- **Medium confidence**: Neighbor smoothing benefit — ablation shows λ=0.5 optimal, but corpus lacks direct comparison to other smoothing strategies
- **Low confidence**: Generalization to arbitrary datasets — evaluation restricted to curated alignment and reasoning benchmarks

## Next Checks
1. **Cross-domain robustness test**: Apply Q-Tuning to a non-English dataset (e.g., Japanese legal texts) and a multimodal dataset (e.g., visual reasoning). Measure quadrant stability and performance decay relative to full-data baseline.

2. **Dynamic threshold stability analysis**: During training, log the evolution of bisect search thresholds (α, β) and quadrant sizes per batch. If thresholds converge to stable values early in training, Q-Tuning approximates static pruning; if they drift, dynamic adaptation is essential.

3. **Memory and runtime scaling experiment**: Profile Q-Tuning on a 70B parameter model with varying batch sizes (32→512). Measure peak memory usage for storing token scores, bisect search iterations per epoch, and net wall-clock time vs. full-data fine-tuning.