---
ver: rpa2
title: Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot
  Foundation Models
arxiv_id: '2506.06569'
source_url: https://arxiv.org/abs/2506.06569
tags:
- textile
- segmentation
- classification
- training
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tackles the challenge of automating textile sorting
  for recycling by developing computer vision models to identify textile material
  composition and detect contaminants like buttons and zippers using standard RGB
  imagery. The approach combines transfer learning for material classification and
  zero-shot foundation models for feature segmentation.
---

# Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models

## Quick Facts
- arXiv ID: 2506.06569
- Source URL: https://arxiv.org/abs/2506.06569
- Authors: Yannis Spyridis; Vasileios Argyriou
- Reference count: 33
- Primary result: 81.25% accuracy for 4-class textile classification using EfficientNetB0 with transfer learning

## Executive Summary
This study tackles the challenge of automating textile sorting for recycling by developing computer vision models to identify textile material composition and detect contaminants like buttons and zippers using standard RGB imagery. The approach combines transfer learning for material classification and zero-shot foundation models for feature segmentation. A multi-class classification model was trained to distinguish between Cotton, Polyester, Cotton-Polyester blends, and Viscose-Polyester blends. EfficientNetB0 achieved the highest accuracy at 81.25% on a held-out test set. For segmentation, a pipeline integrating Grounding DINO (for open-vocabulary detection) and SAM (for mask generation) demonstrated exceptional performance, with a mean IoU of 0.90 for generated masks compared to ground truth. The work demonstrates that modern deep learning techniques, including transfer learning and foundation models, can effectively enable essential pre-processing steps for automated textile recycling pipelines using cost-effective RGB sensors.

## Method Summary
The methodology employs a two-pronged approach: (1) Transfer learning for textile material classification using pre-trained CNN architectures (VGG16, EfficientNetB0, V2-S, V2-M) fine-tuned on a small dataset of 80 RGB images, and (2) Zero-shot segmentation for contaminant detection using Grounding DINO for open-vocabulary object detection combined with SAM for mask generation. Classification follows a two-phase training protocol (feature extraction then fine-tuning), while segmentation operates without task-specific training using text prompts ("button", "zipper"). The pipeline was evaluated on material classification accuracy and segmentation IoU, with emphasis on practical feasibility for industrial recycling automation.

## Key Results
- EfficientNetB0 achieved 81.25% accuracy on held-out test set for 4-class material classification
- Grounding DINO + SAM pipeline achieved mIoU of 0.90 for contaminant mask generation
- Cotton-Polyester blends were particularly challenging, with EfficientNetB0 achieving only 0.40 F1-score
- 5-fold cross-validation showed fold variance, with Fold 4 validation accuracy higher than Fold 3, but Fold 3 performing better on test

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transfer learning from ImageNet to textile classification preserves useful visual features despite domain shift.
- **Mechanism:** Pre-trained convolutional bases (VGG, EfficientNet) extract hierarchical visual features (edges → textures → patterns). The classification head is replaced and trained first, then top layers are fine-tuned at lower learning rates to adapt textile-specific features while retaining low-level visual primitives.
- **Core assumption:** ImageNet's diverse visual features transfer meaningfully to textile surface textures, even though natural images and fabric close-ups differ significantly.
- **Evidence anchors:**
  - [abstract] "For classification, several pre-trained architectures were evaluated using transfer learning... with EfficientNetB0 achieving the best performance... 81.25% accuracy."
  - [Section III-D] "the convolutional base was initialised with its ImageNet pre-trained weights... Phase 1: Feature extraction... Phase 2: Fine-tuning... the top 30 layers were unfrozen"
  - [corpus] Related work (Section II) confirms CNNs "leveraged effectively through transfer learning from models pre-trained on large datasets like ImageNet"
- **Break condition:** If textile textures are visually indistinguishable in RGB (e.g., cotton vs. polyester may share surface patterns), transfer learning from natural images offers no advantage—the model would be learning from noise.

### Mechanism 2
- **Claim:** Zero-shot grounding-to-segmentation pipelines can localize known objects without task-specific training data.
- **Mechanism:** Grounding DINO maps text prompts ("button", "zipper") to visual regions via open-vocabulary detection trained on large image-text corpora. Bounding boxes serve as spatial prompts to SAM, which generates pixel-precise masks using its meta-learning on diverse segmentation tasks.
- **Core assumption:** Buttons and zippers are visually distinctive enough that text-guided detection generalizes from pre-training data to industrial textile imagery without domain adaptation.
- **Evidence anchors:**
  - [abstract] "a zero-shot approach combining the Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM) was employed, demonstrating excellent performance with a mIoU of 0.90"
  - [Section III-E] "Grounding DINO model interpreted these prompts to perform open-set detection... The bounding boxes generated by Grounding DINO served as input prompts to the SAM model"
  - [corpus] AoP-SAM paper (arXiv:2505.11980) addresses "relying on manual prompts is impractical for real-world applications"—suggesting this pipeline's automation is an active research area with known limitations
- **Break condition:** If novel contaminant types (e.g., plastic clips, unfamiliar fasteners) fall outside the vocabulary or visual distribution of pre-training data, detection will fail silently with no fallback.

### Mechanism 3
- **Claim:** Cost-effective RGB sensing can substitute for specialized spectroscopic methods for certain textile analysis tasks.
- **Mechanism:** RGB cameras capture surface reflectance across three broad spectral bands. CNNs learn to associate subtle color/texture patterns with material classes, effectively performing implicit spectroscopy through pattern recognition rather than explicit spectral measurement.
- **Core assumption:** Material composition correlates with visually detectable surface features (texture, weave pattern, sheen) that RGB can capture at sufficient resolution.
- **Evidence anchors:**
  - [abstract] "This study demonstrates the feasibility of using RGB images coupled with modern deep learning techniques... to enable essential analysis steps"
  - [Section II] "identifying textile material composition purely from RGB images is challenging due to subtle visual differences between materials"
  - [corpus] No direct corpus support found—related Electrolyzers-HSI paper uses hyperspectral imaging for material detection, suggesting RGB may be insufficient for broader recycling applications
- **Break condition:** If textile blends (e.g., Cotton-Polyester) are chemically distinct but visually identical, RGB-only approaches will hit an accuracy ceiling regardless of model architecture.

## Foundational Learning

- **Concept: Transfer Learning (Two-Phase Training)**
  - **Why needed here:** Dataset is small (80 images total, 20 per class). Training deep CNNs from scratch would overfit. Transfer learning leverages visual features learned from millions of images.
  - **Quick check question:** Can you explain why freezing the convolutional base initially, then unfreezing top layers at a lower learning rate, prevents catastrophic forgetting?

- **Concept: Foundation Models and Zero-Shot Generalization**
  - **Why needed here:** Segmentation task has no training phase—Grounding DINO + SAM operate without textile-specific annotations. Understanding prompt-based interfaces is essential for debugging failures.
  - **Quick check question:** What happens to SAM's output if Grounding DINO's bounding box is too loose or misaligned with the target object?

- **Concept: Cross-Validation for Small Datasets**
  - **Why needed here:** With only 80 images, a single train/test split yields high variance. 5-fold cross-validation provides more reliable performance estimates.
  - **Quick check question:** Why does the paper report results from the "best-performing fold" on a separate test set—what bias might this introduce?

## Architecture Onboarding

- **Component map:** RGB Camera → Preprocessing (center crop → resize → normalize [-1,1]) → [Classification Branch (EfficientNetB0, 4-class softmax) OR Segmentation Branch (Grounding DINO → SAM)] → [Material Label OR Contaminant Mask]

- **Critical path:**
  1. Data preprocessing alignment—each model requires specific input dimensions (224×224 for EfficientNetB0, variable for SAM)
  2. Classification model selection—EfficientNetB0 outperformed VGG and larger EfficientNet variants
  3. Segmentation prompt engineering—text prompts "button" and "zipper" must match Grounding DINO's vocabulary expectations

- **Design tradeoffs:**
  - **EfficientNetB0 vs. larger variants (V2-S, V2-M):** Smaller model performed best, likely due to small dataset overfitting in larger models
  - **Zero-shot vs. supervised segmentation:** Zero-shot eliminates annotation cost but may fail on novel object types; supervised requires labeled data but generalizes within trained categories
  - **RGB vs. NIR/Hyperspectral:** RGB is cheaper but may have lower theoretical accuracy ceiling for material classification

- **Failure signatures:**
  - **Cotton-Polyester confusion:** F1=0.40 (vs. 0.96 for Viscose-Polyester)—blend classification is unreliable
  - **Fold variance:** Different folds achieved different validation accuracies; Fold 4 had higher validation but Fold 3 performed better on test—small dataset instability
  - **Silent segmentation failures:** Zero-shot pipeline provides no confidence calibration; failures are not detected without ground truth comparison

- **First 3 experiments:**
  1. **Baseline reproduction:** Train EfficientNetB0 on the 4-class task with 5-fold cross-validation; verify ~80% accuracy and identify which class pairs confuse most
  2. **Prompt sensitivity test:** Vary Grounding DINO text prompts ("button" vs. "shirt button" vs. "fastener") and measure mIoU impact on a held-out validation set
  3. **Failure mode analysis:** Collect 20 additional images of Cotton-Polyester items; evaluate whether errors cluster around specific visual patterns (color, weave, lighting)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the classification accuracy for visually ambiguous textile blends, specifically Cotton-Polyester, be improved given that current architectures frequently misclassify them as pure materials?
- Basis in paper: [explicit] The authors state in Section IV.A that models "failed to correctly identify any Cotton-Polyester samples" and highlight this blend as a "significant challenge," noting specific confusion with pure Cotton and Polyester.
- Why unresolved: The visual features extracted via transfer learning from standard RGB images appear insufficient to distinguish the blend from its parent materials, resulting in F1-scores as low as 0.40 for EfficientNetB0.
- What evidence would resolve it: Successful application of texture-specific augmentation, multi-modal sensor fusion (e.g., NIR), or specialized architectures that achieve >90% accuracy on the Cotton-Polyester class.

### Open Question 2
- Question: Can the proposed zero-shot segmentation pipeline maintain its high performance (mIoU 0.90) when applied to diverse, unconstrained contaminants beyond buttons and zippers?
- Basis in paper: [inferred] While the pipeline uses open-vocabulary detection (Grounding DINO), the quantitative evaluation was restricted to specific features (buttons/zippers) on a small dataset (80 images), leaving generalizability to other common contaminants (e.g., tags, stains, plastic films) unverified.
- Why unresolved: Zero-shot models are prompt-sensitive; performance on "buttons" does not guarantee similar precision for smaller or less distinct objects without fine-tuning or threshold adjustments.
- What evidence would resolve it: Evaluation results showing high IoU across a wider taxonomy of non-texture contaminants in a more extensive, varied dataset.

### Open Question 3
- Question: Is the combined inference latency of the classification and segmentation models suitable for real-time sorting on a continuously moving conveyor belt?
- Basis in paper: [inferred] The methodology required the conveyor belt to be "stationary" during image capture, and the pipeline employs computationally heavy foundation models (SAM, Grounding DINO) without reporting inference times or throughput rates.
- Why unresolved: High accuracy is moot for industrial recycling if the processing time per item exceeds the mechanical speed of the sorting line; the computational overhead of generating precise masks is unknown.
- What evidence would resolve it: Benchmarks of frames-per-second (FPS) or latency-per-image on edge devices or industrial GPUs in a continuous operation setup.

## Limitations
- Small dataset (80 images total) limits generalizability and introduces fold variance
- Cotton-Polyester blend classification shows particularly poor performance (F1=0.40), suggesting RGB imagery may not capture sufficient material-specific features
- Zero-shot segmentation pipeline lacks confidence calibration, making failure detection difficult without ground truth
- No ablation studies isolate the contribution of transfer learning versus model architecture

## Confidence
- **High confidence:** EfficientNetB0 outperforming other architectures on this dataset; the two-phase transfer learning approach is theoretically sound
- **Medium confidence:** 81.25% accuracy being sufficient for practical recycling automation; Grounding DINO + SAM achieving mIoU 0.90 on this specific object set
- **Low confidence:** Generalizability to novel textile types, blends, or contaminants; RGB-only sufficiency for material classification across diverse textile compositions

## Next Checks
1. **Generalization test:** Evaluate the trained models on an independent dataset of 50+ textile images from different sources to assess cross-domain performance
2. **Blend-specific investigation:** Collect 30+ Cotton-Polyester blend samples with known composition ratios; analyze classification errors against blend proportions and visual characteristics
3. **Confidence calibration:** Implement uncertainty estimation for both classification (Monte Carlo dropout) and segmentation (ensemble methods); measure reliability diagrams on held-out test data