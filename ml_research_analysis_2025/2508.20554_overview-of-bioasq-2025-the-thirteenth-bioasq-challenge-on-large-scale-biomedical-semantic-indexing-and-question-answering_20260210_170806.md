---
ver: rpa2
title: 'Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical
  Semantic Indexing and Question Answering'
arxiv_id: '2508.20554'
source_url: https://arxiv.org/abs/2508.20554
tags:
- task
- bioasq
- biomedical
- systems
- clef
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BioASQ 2025 was the thirteenth edition of the large-scale biomedical
  semantic indexing and question answering challenge, featuring six tasks including
  two established tasks and four new ones focusing on multilingual clinical summarization,
  nested named entity linking, cardiology clinical coding, and gut-brain interplay
  information extraction. The challenge engaged 83 teams with over 1000 submissions,
  utilizing a variety of methods including transformer-based models, retrieval-augmented
  generation, and ensemble techniques.
---

# Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering

## Quick Facts
- **arXiv ID:** 2508.20554
- **Source URL:** https://arxiv.org/abs/2508.20554
- **Reference count:** 40
- **Primary result:** BioASQ 2025 engaged 83 teams with over 1000 submissions across six tasks, demonstrating continued advancement in biomedical semantic indexing and question answering.

## Executive Summary
BioASQ 2025 marked the thirteenth edition of the large-scale biomedical semantic indexing and question answering challenge, featuring six tasks including two established tasks and four new ones focusing on multilingual clinical summarization, nested named entity linking, cardiology clinical coding, and gut-brain interplay information extraction. The challenge attracted significant participation with 83 teams submitting over 1000 solutions, utilizing diverse approaches including transformer-based models, retrieval-augmented generation, and ensemble techniques. Performance was evaluated using established metrics such as micro-F1, accuracy@1, and BERTScore, with top systems achieving competitive results particularly in biomedical QA and clinical coding tasks.

## Method Summary
The challenge implemented a comprehensive evaluation framework across six distinct tasks, with systems ranging from retrieval-augmented generation approaches for question answering to specialized encoder models for entity linking and clinical coding. Task 13b focused on biomedical question answering with both retrieval-required (Phase A/B) and retrieval-optional (Phase A+) variants. New tasks included MultiClinSum for multilingual clinical summarization using BERTScore as primary metric, BioNNE-L for nested named entity linking using biomedical BERT-based retrieval and reranking architectures, ELCardioCC for cardiology clinical coding, and GBInterPlay for gut-brain interplay information extraction. The evaluation employed both automatic metrics and, where applicable, human judgment to assess system performance.

## Key Results
- 83 teams participated with over 1000 submissions across all tasks
- Top systems achieved competitive performance in biomedical QA and clinical coding tasks
- Specialized biomedical encoders (SapBERT, PubMedBERT) significantly outperformed general LLMs in entity linking tasks
- BERTScore was prioritized over ROUGE for multilingual clinical summarization evaluation
- The introduction of new tasks in multiple languages and specialized domains expanded BioASQ's benchmark scope

## Why This Works (Mechanism)

### Mechanism 1
Large Language Models (LLMs) appear to internalize sufficient biomedical knowledge to handle boolean (yes/no) questions without explicit retrieval grounding, whereas factoid/list questions degrade significantly without context. LLMs leverage pre-trained parametric knowledge to validate binary assertions, but lack the precise lookup mechanism required for entity extraction unless augmented with external evidence (RAG). The high performance in Phase A+ (no ground-truth documents) on yes/no questions implies successful reliance on internal weights rather than retrieval. Performance on yes/no questions drops sharply if the domain vocabulary shifts to novel diseases unseen during the LLM's pre-training cutoff.

### Mechanism 2
Domain-specific encoder models (e.g., SapBERT, PubMedBERT) outperform general-purpose LLMs in fine-grained Entity Linking (EL) and Nested Named Entity Recognition. Specialized bi-encoder or retrieval architectures optimize for semantic similarity in a constrained UMLS concept space, whereas generative LLMs struggle with precise normalization against large, structured vocabularies. The complexity of nested entities and cross-lingual mapping in BioNNE-L favors discriminative retrieval over generative decoding. System fails to generalize if the target vocabulary (e.g., UMLS) expands dynamically without re-embedding the concept dictionary.

### Mechanism 3
Semantic similarity metrics (BERTScore) capture clinical summarization quality better than lexical overlap metrics (ROUGE), particularly in multilingual settings. Contextualized embeddings accommodate paraphrasing and translation variations inherent in multilingual clinical reports, whereas n-gram matching penalizes valid semantic shifts. BERTScore correlates higher with human judgment for abstractive summarization than ROUGE-L. BERTScore gives falsely high scores to fluent but factually hallucinated summaries if the embedding model is not domain-aligned.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** Essential for Task 13b Phase A/B, where systems must retrieve relevant PubMed snippets to ground LLM answers for factoid questions.
  - **Quick check question:** Does the system retrieve the document first and then generate the answer, or does it try to answer from memory?

- **Concept:** Entity Linking (EL) vs. Named Entity Recognition (NER)
  - **Why needed here:** Crucial distinction for BioNNE-L and ELCardioCC; NER finds the span ("heart attack"), EL maps it to a concept ID (e.g., ICD-10 code).
  - **Quick check question:** After extracting a phrase, does your system output the text span or a normalized database ID?

- **Concept:** Semantic vs. Lexical Evaluation
  - **Why needed here:** Understanding why BERTScore is favored over ROUGE for MultiClinSum requires distinguishing between "meaning preservation" and "word matching."
  - **Quick check question:** Will changing "patient denies pain" to "no pain reported" break your evaluation metric?

## Architecture Onboarding

- **Component map:** PDF/Text cleaner -> Dense/Sparse retriever (BGE-M3/BM25) -> Specialized Encoder (BioBERT/PubMedBERT) for NER/EL -> Instruction-tuned LLM (Llama/Mistral) for QA/Summarization -> BERTScore/Embedding model for evaluation

- **Critical path:** 1. Retrieve: Fetch context for the query (high recall is priority). 2. Extract: Identify entities and relations (high precision is priority). 3. Generate: Synthesize final answer/summary. 4. Verify: Filter hallucinations (vital for clinical safety).

- **Design tradeoffs:** Use LLMs for fluency and RAG (Task 13b), but strictly use fine-tuned encoders (BioLinkBERT/SapBERT) for entity linking (BioNNE-L) to ensure precision. Reranking steps improve accuracy but add latency to the retrieval phase.

- **Failure signatures:** Factoid Drift: System answers a "list" question with a summary sentence (LLM failure mode). Nested Entity Collapse: System links only the outer entity ("chronic heart failure") and misses the inner modifier ("chronic") in BioNNE-L. Hallucinated Justification: ELCardioCC systems justifying ICD codes with text not present in the source discharge letter.

- **First 3 experiments:** 1. Retrieval Baseline: Implement a BM25 + Re-ranking pipeline for Task 13b Phase A to establish a MAP baseline. 2. Zero-shot EL Test: Run a general LLM (e.g., GPT-4o) on the BioNNE-L test set to verify specialized BERT models are superior for this specific task. 3. Metric Correlation: Evaluate MultiClinSum outputs using both ROUGE-L and BERTScore to quantify the divergence between lexical and semantic scoring.

## Open Questions the Paper Calls Out
None

## Limitations
- The corpus evidence for BERTScore's superiority over ROUGE in multilingual clinical summarization is weak
- Performance differences between LLM-based approaches and specialized encoder models lack deep ablation studies
- The "collective intelligence" ensemble approach effectiveness is not specified
- ELCardioCC results are less detailed, suggesting possible data quality or annotation consistency issues

## Confidence

- **High Confidence:** Overall participation numbers and the general trend that transformer-based models and RAG approaches dominated performance
- **Medium Confidence:** Specialized biomedical encoders significantly outperform general LLMs in entity linking tasks
- **Low Confidence:** BERTScore captures clinical summarization quality better than ROUGE in multilingual settings

## Next Checks

1. Run the top MultiClinSum systems' outputs through both BERTScore and ROUGE-L, then correlate both with human judgment scores to empirically verify which metric better captures summarization quality.

2. Test the top Phase A+ yes/no question performers on a held-out set of questions about diseases that emerged after the LLMs' pre-training cutoffs to measure degradation of internal knowledge versus retrieval-dependent factoid performance.

3. Implement a controlled experiment comparing general LLM entity linking (GPT-4o) against BioLinkBERT on the BioNNE-L test set, measuring both accuracy and processing time to quantify the tradeoff between precision and efficiency.