---
ver: rpa2
title: Adaptive Orchestration of Modular Generative Information Access Systems
arxiv_id: '2504.17454'
source_url: https://arxiv.org/abs/2504.17454
tags:
- information
- retrieval
- conference
- systems
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for adaptive orchestration of modular
  generative information access (GenIA) systems. The framework aims to dynamically
  construct information retrieval pipelines by selecting and sequencing modules (tasks,
  tools, agents, and resources) tailored to individual user queries.
---

# Adaptive Orchestration of Modular Generative Information Access Systems

## Quick Facts
- arXiv ID: 2504.17454
- Source URL: https://arxiv.org/abs/2504.17454
- Reference count: 40
- This paper proposes a framework for adaptive orchestration of modular generative information access (GenIA) systems using contextual multi-armed bandits.

## Executive Summary
This paper introduces a framework for adaptive orchestration of modular generative information access (GenIA) systems. The framework dynamically constructs information retrieval pipelines by selecting and sequencing modules (tasks, tools, agents, and resources) tailored to individual user queries. Using a graph-based representation where nodes represent modules and edges capture their interactions, the framework optimizes a balance between effectiveness and efficiency by learning to map query complexity to optimal pipeline structures. An instantiation using contextual multi-armed bandits (CMAB) demonstrates successful adaptation, outperforming static optimization approaches on a question-answering task.

## Method Summary
The framework formalizes information retrieval pipelines as directed acyclic graphs where nodes represent tasks, executors, or resources, and edges encode execution order and assignment relationships. For optimization, it uses contextual multi-armed bandits (specifically LinUCB) that map context vectors (query complexity labels in the instantiation) to pre-computed valid graphs. The system optimizes a composite reward combining F1-score effectiveness with latency efficiency costs. The instantiation uses three RAG variants (NoR, OneR, IRCoT) with Flan-T5-XL as the LLM agent and BM25 as the retriever, trained on 3,500 timesteps with the Adaptive-RAG dataset.

## Key Results
- The CMAB approach achieves F1-scores of 1.0, 0.568, and 0.523 for query complexities A, B, and C respectively
- Adaptive orchestration outperforms static graph-based methods (GPTSwarm) across all complexity levels
- With time-included reward, the system balances efficacy with reasonable execution times, preferring faster pipelines when accuracy gains are marginal

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing pipelines as directed acyclic graphs enables systematic search over valid configurations.
- **Mechanism:** Each pipeline G âˆˆ ð”¾ is a subgraph where nodes represent tasks, executors, or resources, and edges encode execution order and assignment relationships. This formalization transforms pipeline construction into a combinatorial search problem amenable to planning and RL methods.
- **Core assumption:** The space of valid pipelines is enumerable or at least partially constrainable by graph validity rules.
- **Evidence anchors:**
  - [Section 4.3.1]: "Each candidate pipeline G âˆˆ ð”¾ is a subgraph... and the directed edges e âˆˆ E capture: (a) the sequence and flow of information among task modules, and (b) the assignment/allocation relationships between tasks, executors, and resources."
  - [Section 5.2]: Defines T = {NoR, OneR, IRCoT, Aggregate} with explicit constraints on which tasks can execute in parallel.
  - [Corpus]: GPTSwarm (Zhuge et al.) uses similar graph representation but optimizes toward a single static configurationâ€”confirming the representational validity but highlighting the limitation this paper addresses.

### Mechanism 2
- **Claim:** Contextual features predictive of query complexity enable adaptive pipeline selection that outperforms static optimization.
- **Mechanism:** The LinUCB contextual multi-armed bandit maps context vectors (query complexity labels in the instantiation) to arms (pre-computed valid graphs). The algorithm maintains per-arm reward estimates conditioned on context, exploiting learned associations while exploring uncertain configurations.
- **Core assumption:** Context features are sufficiently informative to distinguish which pipeline configurations will perform well; the reward signal is available within tractable time horizons.
- **Evidence anchors:**
  - [Section 5.4.1]: "NoR for context A, and IRCoT for contexts B and C, based on F1-scores"â€”demonstrating learned context-to-pipeline mapping.
  - [Table 4]: AQA achieves F1 1.0/0.568/0.523 across complexity levels A/B/C vs. GPTSwarm's 0.862/0.327/0.317, showing adaptivity advantage.
  - [Corpus]: Related work (Adaptive-RAG, Jeong et al. [28]) uses threshold-based complexity classification, but this framework generalizes to learned mapping via bandits/RL.

### Mechanism 3
- **Claim:** Embedding efficiency costs directly into the reward function enables principled tradeoffs between effectiveness and latency.
- **Mechanism:** The composite reward r_t = Î² Â· P_t - (1-Î²) Â· T_t combines correctness (F1-score) with time penalty. The time penalty function is piecewise: negligible below 1 second, moderate (Ã·50) above 10 seconds. This explicit formulation allows the bandit to prefer faster pipelines when accuracy gains are marginal.
- **Core assumption:** The relative weighting Î² accurately reflects operational priorities; time measurements are stable enough to serve as reliable cost signals.
- **Evidence anchors:**
  - [Section 5.4.1]: With time-included reward, "in context B, it preferentially selects pipelines that balance efficacy with reasonable execution times"â€”specifically choosing OneR over IRCoT despite IRCoT's higher F1 (0.580 vs. 0.518) due to latency (192.30s vs. 7.34s).
  - [Section 4.3.3]: Explicitly lists cost dimensions: latency, computational resources, environmental impact, financial costs.

## Foundational Learning

- **Concept: Contextual Multi-Armed Bandits (CMAB)**
  - Why needed here: The paper's optimization layer uses LinUCB, a CMAB algorithm, to select among discrete pipeline configurations based on query context. Understanding the exploration-exploitation tradeoff is essential for tuning the system.
  - Quick check question: Can you explain why a bandit approach might be preferred over full reinforcement learning when rewards are immediate and the action space is discrete?

- **Concept: Directed Acyclic Graphs (DAGs) as Computation Graphs**
  - Why needed here: Pipelines are formalized as DAGs where nodes are modules and edges define execution dependencies. This representation enables topological ordering and parallel execution.
  - Quick check question: Given a DAG with tasks {A, B, C} where Aâ†’B and Aâ†’C, which tasks can execute concurrently?

- **Concept: Retrieval-Augmented Generation (RAG) Variants**
  - Why needed here: The instantiation uses NoR (no retrieval), OneR (single retrieval), and IRCoT (interleaved retrieval with chain-of-thought)â€”understanding when each is appropriate is prerequisite to designing the module space.
  - Quick check question: For a multi-hop question requiring synthesis across documents, which RAG variant would you expect to perform best, and what efficiency cost does it incur?

## Architecture Onboarding

- **Component map:**
  - Context Encoder -> CMAB (LinUCB) -> Execution Engine -> Reward Computer
  - Context Encoder extracts features from query text, user profile, and behavior signals into context vector x_t
  - CMAB selects arm (pipeline G) based on x_t and learned parameters
  - Execution Engine runs G, producing answer and logging time/cost
  - Reward Computer evaluates answer (F1 against ground truth or proxy) and computes r_t

- **Critical path:**
  1. Incoming query â†’ Context Encoder â†’ x_t
  2. CMAB selects arm (pipeline G) based on x_t and learned parameters
  3. Execution Engine runs G, producing answer and logging time/cost
  4. Reward Computer evaluates answer (F1 against ground truth or proxy) and computes r_t
  5. CMAB updates parameters with (x_t, selected arm, r_t)

- **Design tradeoffs:**
  - Pre-computed vs. dynamic graphs: Pre-computing all valid graphs (as in the instantiation) limits scalability but enables simple CMAB; dynamic construction scales better but requires full RL.
  - Reward specification: Time-agnostic rewards maximize accuracy but may select inefficient pipelines; time-weighted rewards add tuning complexity but enforce efficiency constraints.
  - Context richness: Minimal features (e.g., complexity labels) reduce learning burden but may miss nuance; rich features (user history, behavior signals) increase representational power but require more data.

- **Failure signatures:**
  - Bandit collapse to single arm: Insufficient exploration or uninformative context causes policy to fix on one pipeline regardless of query.
  - Reward gaming: If effectiveness metric is imperfect (e.g., F1 on narrow test set), system may optimize for metric without improving actual user satisfaction.
  - Latency spikes from expensive modules: IRCoT-style tasks can exceed 180s; without proper time penalties, bandit may select them inappropriately for simple queries.

- **First 3 experiments:**
  1. Baseline module performance: Run each task (NoR, OneR, IRCoT) independently across query complexity levels to establish per-module effectiveness and latency profiles (replicating Table 3).
  2. CMAB convergence analysis: Train LinUCB with time-agnostic reward, plot expected reward per context over timesteps, verify convergence to theoretically optimal arms (replicating Figure 2, top row).
  3. Static vs. adaptive comparison: Compare AQA (adaptive) against GPTSwarm (static optimization) on held-out test set, measuring both F1 and execution time to quantify the adaptivity advantage (replicating Table 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should optimization methods adapt to ensure efficiency and robustness as the system scales by integrating a massive number of heterogeneous modules?
- Basis in paper: [explicit] Section 6 explicitly poses the question of how optimization methods must adapt as the system scales with more modules.
- Why unresolved: The proof-of-concept uses a Contextual Multi-Armed Bandit (CMAB) with only 7 pre-computed graphs (arms). Real-world systems face a combinatorial explosion of possible pipelines, making standard bandit approaches computationally infeasible.
- What evidence would resolve it: A scalable algorithm (e.g., hierarchical RL or continuous planning) that efficiently converges on optimal configurations within an action space containing hundreds of modules and varied topologies.

### Open Question 2
- Question: How can frameworks enable graphs that are dynamic during execution, adapting the pipeline based on intermediate outcomes?
- Basis in paper: [inferred] Section 4.4 lists the limitation that graphs are currently static once constructed; the authors suggest enabling graphs that adapt to intermediate outcomes as a logical continuation.
- Why unresolved: The current CMAB instantiation selects a static pipeline structure upfront based solely on the initial query context, failing to adjust the strategy if early steps (e.g., retrieval) fail or return low-confidence signals.
- What evidence would resolve it: A system demonstration where the pipeline topology reconfigures itself mid-execution (e.g., switching from a single-hop to a multi-hop retrieval strategy) in response to uncertainty signals from a subcomponent.

### Open Question 3
- Question: How can models jointly optimize the high-level graph structure and the low-level internal parameters (e.g., prompts, fine-tuning) of modules?
- Basis in paper: [explicit] Section 6 explicitly asks how to account for other levels of optimization, such as enhancing prompts or fine-tuning individual modules.
- Why unresolved: The proposed framework treats modules as fixed black boxes and optimizes only their selection and sequencing, ignoring the potential performance gains from adapting the modules themselves to the specific orchestration context.
- What evidence would resolve it: An end-to-end differentiable or co-adaptive training framework that simultaneously updates the pipeline's edges and the weights/prompts of the underlying LLMs to maximize the global reward function.

## Limitations
- The framework's scalability is constrained by the pre-computed graph approach, which requires enumerating all valid pipeline configurations and becomes intractable for larger module spaces.
- The reliance on labeled query complexity (A, B, C) in the instantiation limits generalization to scenarios where such labels are unavailable or unreliable.
- The effectiveness metric (F1) assumes ground truth answers exist and are unambiguous, which may not hold for open-ended queries.

## Confidence
- **High:** The graph-based representation of pipelines and the contextual multi-armed bandit instantiation are technically sound and well-supported by evidence.
- **Medium:** The adaptation advantage over static optimization is demonstrated but only on a single dataset with controlled query complexity labels.
- **Low:** The generalizability of the framework to unstructured or unlabeled query spaces, and its performance with larger module registries, remains untested.

## Next Checks
1. **Unlabeled context experiment:** Remove the ground-truth complexity labels and replace them with learned query embeddings or heuristic features. Retrain the bandit and evaluate whether performance degrades significantly, testing the framework's ability to generalize beyond labeled contexts.
2. **Module space stress test:** Expand the module registry beyond the three RAG variants (e.g., add summarization, translation, or API-calling modules). Measure how the pre-computed graph approach scales and whether the bandit can still learn meaningful policies.
3. **Reward function ablation:** Train separate models with only effectiveness reward, only efficiency reward, and the composite reward. Compare final F1 and latency to quantify the contribution of the time-weighted objective and identify potential reward gaming.