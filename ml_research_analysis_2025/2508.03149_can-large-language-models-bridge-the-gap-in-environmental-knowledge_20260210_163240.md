---
ver: rpa2
title: Can Large Language Models Bridge the Gap in Environmental Knowledge?
arxiv_id: '2508.03149'
source_url: https://arxiv.org/abs/2508.03149
tags:
- environmental
- students
- knowledge
- education
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates six large language models (GPT-3.5, GPT-4,
  GPT-4o, Gemini, Claude, and Llama 2) for their ability to convey environmental knowledge
  to university students. Using the validated Environmental Knowledge Test (EKT-19),
  supplemented with targeted questions, the study compares AI model performance against
  human students.
---

# Can Large Language Models Bridge the Gap in Environmental Knowledge?

## Quick Facts
- **arXiv ID**: 2508.03149
- **Source URL**: https://arxiv.org/abs/2508.03149
- **Reference count**: 0
- **Key outcome**: AI models significantly outperformed students (88.89% vs. 43.99%) on environmental knowledge test across multiple domains

## Executive Summary
This study evaluates six large language models (GPT-3.5, GPT-4, GPT-4o, Gemini, Claude, and Llama 2) for their ability to convey environmental knowledge to university students. Using the validated Environmental Knowledge Test (EKT-19), supplemented with targeted questions, the study compares AI model performance against human students. Results show that AI models significantly outperformed students, achieving a mean score of 88.89% versus 43.99% for students across all environmental domains. Claude scored 100%, while Llama 2 scored 73.3%, demonstrating varying strengths among models. AI models excelled particularly in climate, resources, society & politics, economy, and environmental contamination, but showed weaker performance in ecology. While AI models offer promising tools for enhancing environmental education through accessible and accurate information, human oversight remains necessary to validate accuracy and foster critical thinking.

## Method Summary
The study administered the EKT-19 standardized test (30 multiple-choice items across 7 environmental domains) to six large language models and 46 university students. Models received identical prompts in standardized format, with MCQs administered in single prompts and three short-answer questions given individually. Performance was scored against an answer key for MCQs and an expert rubric for short answers. Statistical comparisons (t-tests, chi-squared) were conducted between AI and student groups. The study also evaluated domain-specific performance and categorized responses into System, Action, and Effectiveness knowledge types.

## Key Results
- AI models achieved 88.89% average score versus 43.99% for students across all environmental domains
- Claude scored 100% while Llama 2 scored 73.3%, showing significant variation among models
- AI models excelled in climate, resources, society & politics, economy, and environmental contamination domains
- All models except Claude and Gemini scored below 4/5 in the ecology domain, indicating systematic knowledge gaps

## Why This Works (Mechanism)
AI models demonstrate superior environmental knowledge recall due to their extensive training on diverse textual corpora that likely includes environmental science literature. The multiple-choice format favors pattern recognition and retrieval of memorized facts over conceptual understanding. The significant performance gap (88.89% vs. 43.99%) suggests that current LLMs can serve as effective knowledge repositories for environmental education, though their limitations in ecology and open-ended question handling indicate the need for domain-specific fine-tuning and human oversight to ensure comprehensive understanding and critical thinking development.

## Foundational Learning

**Concept: Test Contamination Risk**
- Why needed here: The EKT-19 is a publicly available standardized test; if present in model training data, performance reflects memorization rather than knowledge.
- Quick check question: Can you verify that the EKT-19 questions were not in the training corpora of the evaluated models?

**Concept: Knowledge vs. Retrieval**
- Why needed here: Multiple-choice success does not guarantee conceptual understanding or ability to apply knowledge in novel environmental problem-solving contexts.
- Quick check question: Does the study measure transfer learning or real-world application beyond test performance?

**Concept: Sample Representativeness**
- Why needed here: 46 students from one UAE university (93% female, 52% environmental science majors) limits generalizability of the AI-human performance gap.
- Quick check question: How would results differ with a larger, more diverse student population across multiple institutions?

## Architecture Onboarding

**Component map**: EKT-19 multiple-choice questions (30 items) + 3 short-answer questions -> Six LLMs (GPT-3.5/GPT-4/GPT-4o, Gemini, Claude 3.5 Sonnet, Llama 2 70B) -> Independent scoring (MCQ: answer key; SAQ: expert rubric) -> Comparison Layer: Student baseline (n=46) vs. AI performance per domain

**Critical path**: 1. Prompt standardization across all models (identical phrasing) 2. Domain-specific scoring (7 environmental domains + 3 knowledge types) 3. Statistical comparison (t-tests, chi-squared) between AI and student groups

**Design tradeoffs**: Breadth vs. depth: EKT-19 covers 7 domains but only 30 items; domain-level conclusions have low statistical power; Convenience sampling vs. representativeness: Rapid recruitment (46 students) limits generalizability; MCQ vs. SAQ: Multiple-choice enables automated scoring but misses critical thinking; short-answer adds depth but introduces scoring subjectivity

**Failure signatures**: Hallucination in SAQs: Models generate plausible-sounding but factually incorrect environmental claims (not directly observed but flagged as risk in Page 4); Ecology domain underperformance: All models except Claude/Gemini scored below 4/5, suggesting systematic knowledge gaps; Model verbosity variance: Llama 2 averaged 74 words vs. GPT-3.5/Gemini at 44 words for identical prompts (Page 9), complicating response comparability

**First 3 experiments**: 1. Contamination audit: Search for EKT-19 questions in publicly available training datasets (e.g., Common Crawl) to validate test integrity; 2. Cross-validation with held-out questions: Generate novel environmental questions not in any training corpus to test generalization; 3. Longitudinal retention study: Test whether students using AI tools retain environmental knowledge better than traditional instruction over 6 months

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does long-term knowledge retention and pro-environmental behavioral change in students compare between those using AI models and those using traditional instructional methods?
- Basis in paper: [explicit] Page 16: "Future research should explicitly... assess long-term knowledge retention and behavioral change resulting from interactions with AI models compared to traditional instructional methods."
- Why unresolved: The current study utilized a cross-sectional design that only measured immediate performance differences on a standardized test (EKT-19), lacking the longitudinal data necessary to track knowledge decay or behavioral shifts over time.
- What evidence would resolve it: Longitudinal studies tracking student performance and real-world sustainable actions months or years after initial instruction via AI versus traditional methods.

**Open Question 2**
- Question: In what ways can AI models be integrated into environmental curricula to foster critical thinking and collaborative problem-solving beyond simple factual recall?
- Basis in paper: [explicit] Page 18: "Beyond factual knowledge transfer, future research should investigate how these tools can actively shape learning through personalized learning experiences, fostering critical thinking, and enabling collaborative problem-solving."
- Why unresolved: The study focused on the AI's ability to provide correct answers on a multiple-choice test, but the authors note that AI currently struggles with open-ended questions and fostering the debate necessary for deeper learning.
- What evidence would resolve it: Empirical studies measuring students' ability to synthesize information, form reasoned arguments, and solve complex environmental scenarios after using AI-assisted learning modules.

**Open Question 3**
- Question: Do the significant performance differences found between AI models and university students generalize to broader demographic groups and different educational levels?
- Basis in paper: [inferred] Page 17: "The small sample size of 46 university students from the United Arab Emirates further constrains statistical power, making it challenging to extend results to a broader population."
- Why unresolved: The homogeneity of the sample (mostly female, specific region, convenience sampling) limits the ability to determine if the 88.89% vs. 43.99% performance gap is consistent across diverse cultures, age groups, or expert populations.
- What evidence would resolve it: Replication of the EKT-19 evaluation across randomized, diverse demographic samples (e.g., high school students, professionals, different cultures).

**Open Question 4**
- Question: Can targeted fine-tuning or architectural adjustments close the performance gap for models like Llama 2 and GPT-3.5, particularly in their weakest domain of Ecology?
- Basis in paper: [inferred] Page 14: "Ecology was the weakest area of knowledge for most models... To enhance performance in categories with lower scores... researchers must provide more training data for the machine learning algorithms."
- Why unresolved: The study identifies that models like Claude (100%) and Gemini succeeded where others failed in Ecology, but it does not experimentally determine if providing specific ecological training data can lift the lower-scoring models (e.g., Llama 2 at 40% in Ecology) to comparable levels.
- What evidence would resolve it: An experimental comparison of general-purpose LLMs against versions specifically fine-tuned on ecological datasets, re-evaluated using the EKT-19.

## Limitations
- Test Contamination Risk: The EKT-19's public availability creates potential memorization rather than genuine knowledge demonstration without verification of training data presence.
- Generalizability Constraints: The student sample (n=46, UAE university, 93% female) severely limits the external validity of the observed AI-human performance gap.
- Domain-Level Statistical Power: With only 30 items across 7 domains, the domain-specific comparisons have limited statistical reliability.

## Confidence

- **High Confidence**: The core finding that commercial LLMs significantly outperform students on standardized environmental knowledge tests (88.89% vs. 43.99%) is robust and methodologically sound.
- **Medium Confidence**: The conclusion that AI models can enhance environmental education through accessible information delivery, while requiring human oversight, follows logically from the data but depends on assumptions about educational context.
- **Low Confidence**: The specific domain-level performance patterns (e.g., ecology underperformance) have insufficient statistical power for definitive claims due to the small item count per domain.

## Next Checks
1. **Training Data Audit**: Verify that EKT-19 questions were not present in the training corpora of evaluated models to confirm the performance gap reflects genuine knowledge rather than memorization.
2. **Cross-Validation with Novel Questions**: Generate and test with environmental questions outside any training data to validate that models can generalize beyond memorized test content.
3. **Diverse Population Replication**: Replicate the study with larger, more demographically diverse student populations across multiple institutions to establish the generalizability of the AI-human performance gap.