---
ver: rpa2
title: 'ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial
  Optimization with Routing and Dynamic attention'
arxiv_id: '2506.11052'
source_url: https://arxiv.org/abs/2506.11052
tags:
- feasibility
- time
- accord
- problem
- combinatorial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates applying large language models to NP-hard
  combinatorial optimization problems and introduces ACCORD, a framework that encodes
  feasibility constraints directly into autoregressive output representations. The
  method uses dynamic LoRA routing and a novel dataset format to guide LLMs toward
  valid solutions.
---

# ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention

## Quick Facts
- arXiv ID: 2506.11052
- Source URL: https://arxiv.org/abs/2506.11052
- Reference count: 40
- An 8B Llama model trained with ACCORD consistently outperforms both prompting baselines and larger models like GPT-4 in feasibility and optimality gap on six NP-hard combinatorial optimization problems.

## Executive Summary
ACCORD introduces a framework for applying large language models to NP-hard combinatorial optimization problems by encoding feasibility constraints directly into autoregressive output representations. The method uses dynamic LoRA routing and a novel dataset format to guide LLMs toward valid solutions. On six CO problems (TSP, VRP, Knapsack, FlowShop, JSSP, BinPacking), an 8B Llama model trained with ACCORD consistently outperforms both prompting baselines and larger models like GPT-4 in feasibility and optimality gap.

## Method Summary
ACCORD fine-tunes a 4-bit quantized Llama 3.1 8B model with RSLoRA adapters (rank=64, α=64) on a novel ACCORD-90k dataset. The dataset uses a unique arrow-based format that embeds constraint satisfaction directly into each generation step. A TextClassifier with attention pooling routes problem instructions to problem-specific LoRA modules. At inference, the model generates 60 candidate solutions per instance, parses them for feasibility, and selects the best feasible solution by optimality gap.

## Key Results
- 8B ACCORD model achieves 87.78% feasibility on FlowShop vs 62.92% for list-of-list format
- On JSSP size-12, ACCORD achieves 7% feasibility vs 0% for 1B model
- ACCORD consistently outperforms GPT-4 prompting baselines across all six problem types
- 8B model shows 31.5% relative gap reduction over 1B model on TSP/VRP

## Why This Works (Mechanism)

### Mechanism 1
The ACCORD representation format improves solution feasibility by embedding constraint checks into each autoregressive generation step. Instead of outputting static solution lists, the model generates incremental state transitions showing cumulative constraint metrics (e.g., weight, distance, time) after each decision. This creates immediate feedback signals during training that associate partial sequences with feasibility states, reducing the likelihood of generating infeasible continuations.

### Mechanism 2
Problem-specific LoRA adapters with attention-based routing improve specialization across diverse combinatorial problems compared to a single unified model. A lightweight classifier (TextClassifier with attention pooling) reads the instruction text, predicts the problem type, and routes to the corresponding LoRA adapter weights. Each adapter is fine-tuned only on its target problem class, creating specialized sub-networks within the shared base model.

### Mechanism 3
Sampling multiple candidate solutions and selecting the best feasible one (best-of-N) improves output quality over single-pass generation. The model generates 60 candidate solutions per instance. Each is parsed and checked for constraint violations. Among feasible candidates, the one with the lowest optimality gap is selected. This trades inference compute for solution quality.

## Foundational Learning

- **Concept: Combinatorial Optimization and Feasibility**
  - Why needed here: ACCORD targets NP-hard problems where solutions must satisfy hard constraints (e.g., capacity, precedence). Understanding feasibility vs. optimality is essential to interpret the results.
  - Quick check question: For a Knapsack problem with capacity 20, is a solution with total weight 21 feasible?

- **Concept: Autoregressive Language Models**
  - Why needed here: The ACCORD representation relies on left-to-right token generation where each output conditions on all previous tokens. Constraint tracking must fit this sequential structure.
  - Quick check question: In autoregressive generation, can the model revise an earlier token after generating later ones?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: ACCORD uses LoRA to add problem-specific trainable parameters without modifying the full base model. Understanding rank, alpha, and target modules is necessary for reproduction.
  - Quick check question: If LoRA rank r=64 and α=64, what is the effective scaling factor applied to the low-rank update?

## Architecture Onboarding

- **Component map:** Instruction text -> Tokenizer -> Embeddings + Positional Encoding -> Router classifier -> Select LoRA adapter -> Base LLM + LoRA -> Autoregressive generation -> Parse output -> Feasibility check -> Compute optimality gap -> Return best feasible solution

- **Critical path:** 1) Instruction text → Tokenizer → Embeddings + Positional Encoding 2) Router classifier → Select LoRA adapter 3) Base LLM + LoRA → Autoregressive generation of ACCORD-format solution 4) Parse output → Feasibility check → Compute optimality gap 5) Return best feasible solution (or report failure if none)

- **Design tradeoffs:** Model size: 8B outperforms 1B (31.5% relative gap reduction), but requires more VRAM and slower inference; Sampling budget: 60 samples improves best-of-N selection but increases inference time linearly; Context window: Limited to 40k tokens; very large instances may exceed capacity; Representation: ACCORD format improves feasibility but increases output token count vs. list-of-lists

- **Failure signatures:** "No data" in results: Model failed to generate any feasible solution within 60 samples (observed for 1B model on harder JSSP instances); High optimality gap: Model generates feasible but suboptimal solutions (e.g., 12.4% gap on size-12 JSSP); Parse errors: Output does not conform to ACCORD format; feasibility checker cannot extract solution

- **First 3 experiments:** 1) Reproduce single-problem ACCORD fine-tuning: Fine-tune Llama 3.1 8B with LoRA (r=64, α=64) on the Knapsack split of ACCORD-90k. Evaluate on 100 held-out instances. Compare feasibility rate and optimality gap against the list-of-list baseline. 2) Ablate representation format: Train two models with identical hyperparameters—one on ACCORD format, one on list-of-list format. Evaluate on the same validation set. Report feasibility percentage difference (hypothesis: ACCORD > list-of-list). 3) Test routing robustness: Construct adversarial instruction texts that mix terminology from multiple problems (e.g., "schedule these jobs into bins with minimum travel distance"). Measure router accuracy and downstream solution quality to assess failure sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
Can ACCORD's performance be improved by using larger backbone models with full fine-tuning instead of LoRA adapters on an 8B-parameter model? The authors state: "In future work, we will investigate larger backbones (with full fine-tuning)" as a limitation of the current approach.

### Open Question 2
How can ACCORD overcome context window limitations to handle very large combinatorial optimization instances? The authors note ACCORD "is bounded by the LLM's context window (limiting very large instances)" and propose to "expand the effective context via external memory or hierarchical encoding."

### Open Question 3
How does ACCORD generalize to real-world, large-scale optimization scenarios beyond synthetic benchmarks? The authors list applying ACCORD "to real-world, large-scale optimization scenarios" as future work.

### Open Question 4
What is the trade-off between sampling budget and solution quality in ACCORD's inference pipeline? The inference uses 60 samples per instance but does not analyze how feasibility and optimality scale with fewer or more samples.

## Limitations
- Dataset generation reproducibility: Exact methodology for generating ACCORD-90k, including train/validation splits and random seeds, is not fully specified
- Router architecture details: Specific implementation details like hidden dimensions and attention head counts are omitted
- Sampling efficiency: Requires generating 60 candidate solutions per instance, increasing inference latency significantly
- Generalization beyond OR-Tools: All supervised training data comes from OR-Tools, potentially inheriting solver biases

## Confidence
- High confidence: Core mechanism of embedding cumulative constraint states into autoregressive generation is well-supported by direct evidence
- Medium confidence: LoRA routing architecture is conceptually sound but lacks extensive ablation studies
- Medium confidence: Best-of-N sampling strategy is standard but specific choice of 60 samples and its impact is not thoroughly analyzed

## Next Checks
1. Ablation study on router architecture: Train ACCORD without the routing classifier and compare feasibility and optimality gap to the routed version
2. Varying sample count analysis: Evaluate solution quality and feasibility at different sample counts (e.g., 10, 30, 60, 100)
3. Cross-solver generalization test: Generate ACCORD-format training data using a different solver (e.g., SCIP for Knapsack) and retrain the model