---
ver: rpa2
title: 'Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights
  from Sparse AutoEncoders'
arxiv_id: '2502.16722'
source_url: https://arxiv.org/abs/2502.16722
tags:
- layers
- fine-tuning
- features
- layer
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study analyzes how fine-tuning transforms learned representations
  in BERT across layers, using Sparse AutoEncoders (SAEs) to interpret feature evolution.
  Experiments on three datasets (IMDb, Spotify, News) reveal a systematic progression:
  early layers retain general linguistic features in both pre-trained and fine-tuned
  models, middle layers shift focus to structural tokens after fine-tuning, and later
  layers become highly specialized for task-specific features like sentiment polarity
  or domain distinctions.'
---

# Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders

## Quick Facts
- arXiv ID: 2502.16722
- Source URL: https://arxiv.org/abs/2502.16722
- Reference count: 0
- Primary result: Layer-wise analysis of fine-tuning transformations using Sparse AutoEncoders reveals systematic depth-dependent feature evolution in BERT models

## Executive Summary
This study analyzes how fine-tuning transforms learned representations in BERT across layers, using Sparse AutoEncoders (SAEs) to interpret feature evolution. Experiments on three datasets (IMDb, Spotify, News) reveal a systematic progression: early layers retain general linguistic features in both pre-trained and fine-tuned models, middle layers shift focus to structural tokens after fine-tuning, and later layers become highly specialized for task-specific features like sentiment polarity or domain distinctions. Cosine similarity analysis confirms that deeper layers undergo the most transformation during fine-tuning, with task complexity and model depth influencing the degree of change. These findings provide insights into the layer-wise dynamics of fine-tuning, highlighting how pre-trained models adapt from general-purpose knowledge to task-specific representations.

## Method Summary
The study fine-tunes bert-base-uncased on three classification tasks using Adam optimizer (LR=2e-5, 10 epochs), then extracts hidden states from layers 3, 6, and 12 for both pre-trained and fine-tuned models. SAEs are trained per layer with 768→1024 encoder expansion and L1 sparsity regularization (λ=10^-3), using MSE + sparsity loss. High-variance features are selected for interpretation through token-level activation visualization. Cosine similarity between pre-trained and fine-tuned activations quantifies layer-wise transformation, while classification accuracy provides task performance benchmarks.

## Key Results
- Early layers (3) show minimal change during fine-tuning (cosine similarity ~0.95-1.0), retaining general linguistic features
- Later layers (12) undergo maximum transformation with similarity dropping to 0.6-0.8, becoming highly task-specific
- Task complexity correlates with transformation magnitude, with News topic classification showing steepest similarity decline
- Smaller model variants (bert-medium, bert-small) show higher overall similarity values, suggesting depth limits representation adaptation

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Feature Progression
- Claim: Fine-tuning transforms transformer representations through systematic depth-dependent progression, with early layers retaining general linguistic features and later layers specializing for task-specific objectives
- Mechanism: Pre-trained BERT encodes hierarchical representation where Layer 1-3 capture syntactic/structural elements, Layer 6-8 serve as transitional zones encoding mixed semantics, and Layer 10-12 specialize in high-level task-relevant features. Fine-tuning amplifies this progression—early layers show minimal change (cosine similarity ~0.95-1.0), middle layers show moderate adaptation, and late layers undergo maximum transformation (similarity dropping to 0.6-0.8 depending on task complexity)
- Core assumption: The pipeline-like behavior observed in BERT generalizes to other transformer architectures, and task complexity directly correlates with magnitude of late-layer representation shifts
- Evidence anchors: [abstract] shows systematic progression across layers; [section 4.2] provides cosine similarity analysis confirming layer-wise patterns; [corpus] [12] establishes BERT's pipeline behavior

### Mechanism 2: Sparse AutoEncoder Feature Disentanglement
- Claim: Sparse AutoEncoders can decompose polysemantic transformer activations into interpretable monosemantic features by enforcing sparsity constraints on expanded hidden dimension
- Mechanism: SAEs expand 768-dimensional BERT hidden state into 1024-dimensional sparse representation using L1 regularization (λ=10^-3). This expansion + sparsity combination forces each latent dimension to activate selectively for specific linguistic or semantic concepts, enabling identification of features like "sentiment polarity" or "domain-specific terminology"
- Core assumption: Sparsity directly promotes monosemanticity (one neuron = one interpretable concept), and high-variance features correspond to task-relevant concepts rather than artifacts
- Evidence anchors: [abstract] states SAEs are used to interpret feature evolution; [section 3.2.3] explains how sparsity enforces monosemanticity; [corpus] [95587][18][19] support SAE application to language models

### Mechanism 3: Task Complexity-Depth Interaction
- Claim: Degree of representation transformation during fine-tuning is modulated by both task complexity and available model depth, with more complex tasks requiring deeper transformation
- Mechanism: Complex classification tasks require greater representation restructuring than simpler binary tasks, manifesting as steeper cosine similarity decline curves. Additionally, smaller model variants show higher overall similarity values, indicating limited capacity for representation transformation constrains adaptation
- Core assumption: Task complexity can be operationalized through label cardinality and semantic distance, and model depth directly enables finer-grained feature specialization
- Evidence anchors: [abstract] notes task complexity and model depth influence transformation degree; [section 4.2] shows News dataset exhibits steepest similarity decline; [corpus] [71724] identifies gradient suppression at inflection layers during fine-tuning

## Foundational Learning

- Concept: **Transformer Layer Hierarchy**
  - Why needed here: Understanding why different layers encode different linguistic levels is essential for interpreting SAE results and predicting which layers will transform most during fine-tuning
  - Quick check question: Given a 12-layer BERT model, which layer would you expect to show the largest cosine similarity drop when fine-tuning on a complex multi-class classification task?

- Concept: **Sparsity-Regularized AutoEncoders**
  - Why needed here: SAEs are the primary analysis tool in this paper; understanding how L1 regularization promotes selective activation is necessary to evaluate whether extracted features are meaningful
  - Quick check question: Why does expanding the hidden dimension (768→1024) combined with L1 penalty help achieve monosemantic representations rather than simply learning redundant copies?

- Concept: **Cosine Similarity for Representation Comparison**
  - Why needed here: The paper's primary quantitative evidence comes from cosine similarity between pre-trained and fine-tuned activations; interpreting these values correctly is essential for validating the layer-wise progression hypothesis
  - Quick check question: A cosine similarity of 0.85 at Layer 3 versus 0.55 at Layer 12—what does this tell you about where fine-tuning primarily alters the model?

## Architecture Onboarding

- Component map: Input Text → Tokenization → BERT Layers 1-12 (768-dim hidden states) → Layer 3/6/12 → SAE (768→1024) → Sparse Features → Cosine Similarity Analysis → Token-level Visualization

- Critical path:
  1. Extract activation matrices from target layers during forward pass
  2. Train SAEs on activations with MSE + L1 sparsity loss
  3. Select high-variance features for interpretation
  4. Compare pre-trained vs fine-tuned feature activations on test sentences
  5. Validate layer-wise hypothesis through similarity curves and activation patterns

- Design tradeoffs:
  - **SAE hidden dimension (1024 vs larger)**: Current expansion ratio ~1.3x may be insufficient for complete feature disentanglement; larger dimensions improve granularity but increase training cost and interpretation complexity
  - **Sparsity parameter (λ=10^-3)**: Higher sparsity improves interpretability but risks losing subtle features; current value not systematically validated
  - **Layer selection (3, 6, 12)**: Sampling 3 of 12 layers provides computational tractability but may miss nuanced transitions; middle layers 4-5 and 7-9 unanalyzed

- Failure signatures:
  - **SAE reconstruction loss plateau**: If loss remains >10^-2, features may be polysemantic or expansion insufficient
  - **Uniform cosine similarity across layers**: Indicates either model not learning or task too simple to require depth-dependent adaptation
  - **High-variance features not interpretable**: Suggests variance selection heuristic failing; consider alternative selection criteria (activation frequency, mutual information with labels)
  - **Contradictory activation patterns**: If Layer 12 activates on general words while Layer 3 activates on task-specific terms, hypothesis invalid

- First 3 experiments:
  1. **Reproduce cosine similarity curves** on a held-out dataset (e.g., SST-2) to validate that layer-wise progression pattern generalizes beyond IMDb/Spotify/News; expect similarity curve shape to match but magnitude to vary with task complexity
  2. **Train SAE with doubled hidden dimension (2048)** on Layer 6 activations to test whether feature interpretability improves; manually annotate top-10 high-variance features for semantic coherence
  3. **Layer-wise probing classifier**: Train linear probes on each layer's activations to predict task labels; verify that probe accuracy increases with layer depth, providing independent validation of feature specialization beyond SAE interpretation

## Open Questions the Paper Calls Out

- **Do the layer-wise representation progression patterns observed in BERT generalize to other transformer architectures (e.g., RoBERTa, GPT, T5) with different pre-training objectives?**
  - Basis: [explicit] Future Work states: "Future experiments could incorporate a wider range of transformer architectures... to evaluate whether the trends observed in this study generalize across different model designs and pre-training objectives"
  - Why unresolved: This study only analyzed BERT variants; architectural differences like decoder-only vs. encoder-only and varied pre-training objectives may produce different fine-tuning dynamics
  - What evidence would resolve it: Replicate SAE-based layer-wise analysis on RoBERTa, GPT, and T5 models fine-tuned on comparable tasks, measuring whether early-to-late layer specialization patterns persist

- **Are certain types of linguistic representations (e.g., syntactic vs. semantic) more resistant to modification during fine-tuning than others?**
  - Basis: [explicit] Introduction explicitly asks: "Are certain types of representations more resistant to fine-tuning than others? By analyzing which features remain stable versus which undergo significant transformation, we can gain deeper insights into the trade-offs between general linguistic retention and task-specific adaptation"
  - Why unresolved: While the study shows early layers retain "general" features and later layers specialize, it does not systematically categorize which linguistic feature types show differential resistance
  - What evidence would resolve it: Design probing tasks targeting specific linguistic phenomena across layers pre- and post-fine-tuning, quantifying retention rates by feature category

- **How does SAE capacity and sparsity regularization affect the quality and interpretability of extracted monosemantic features during fine-tuning analysis?**
  - Basis: [explicit] Future Work states: "Investigating how the size of the autoencoder and the choice of sparsity penalties impact the representations learned would provide a better framework for defining task-relevant features"
  - Why unresolved: This study used fixed SAE architecture (768→1024) with λ=10^-3; sensitivity of conclusions to these hyperparameter choices is unknown
  - What evidence would resolve it: Systematically vary SAE hidden dimension and sparsity penalty, measuring reconstruction quality, feature disentanglement metrics, and interpretability through human evaluation

## Limitations

- SAE methodology lacks systematic validation of whether high-variance features truly correspond to human-understandable concepts versus statistical artifacts
- Layer selection (only 3 of 12 layers) may miss critical transitional dynamics between early and late layers
- Operationalization of "task complexity" through label cardinality lacks deeper semantic analysis of task difficulty

## Confidence

- **High confidence**: Layer-wise cosine similarity patterns showing early layers retaining similarity while late layers transform significantly
- **Medium confidence**: SAE-extracted features represent meaningful, interpretable concepts
- **Medium confidence**: Task complexity directly modulates representation transformation magnitude

## Next Checks

1. **Systematic Feature Validation**: Conduct human evaluation studies where annotators rate the interpretability of top-10 high-variance features across all layers and datasets. Calculate inter-annotator agreement and compare against random feature baselines to establish whether SAE selection captures meaningful concepts beyond chance.

2. **Temporal Layer Evolution Analysis**: Track representation changes across fine-tuning epochs (not just final state) using cosine similarity curves. This would reveal whether early layers adapt initially and stabilize while late layers continue evolving, providing stronger temporal evidence for the proposed layer-wise progression mechanism.

3. **Cross-Architecture Generalization Test**: Apply the same SAE analysis pipeline to RoBERTa and DeBERTa models fine-tuned on identical tasks. Compare whether the early-middle-late layer transformation pattern holds across architectures with different pre-training objectives, establishing whether observed dynamics are BERT-specific or general transformer behavior.