---
ver: rpa2
title: Enhancing Speech Large Language Models through Reinforced Behavior Alignment
arxiv_id: '2509.03526'
source_url: https://arxiv.org/abs/2509.03526
tags:
- arxiv
- speech
- language
- preprint
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reinforced Behavior Alignment (RBA), a method
  to enhance speech-based large language models (SpeechLMs) by aligning their instruction-following
  capabilities with powerful text-based LLMs. The approach addresses the performance
  gap caused by inter-modal discrepancies through self-synthesis data generation and
  reinforcement learning.
---

# Enhancing Speech Large Language Models through Reinforced Behavior Alignment

## Quick Facts
- arXiv ID: 2509.03526
- Source URL: https://arxiv.org/abs/2509.03526
- Authors: Yansong Liu; Jiateng Li; Yuan Liu
- Reference count: 10
- Primary result: RBA achieves SOTA performance on spoken question answering and speech-to-text translation through self-synthesis data generation and reinforcement learning

## Executive Summary
This paper introduces Reinforced Behavior Alignment (RBA), a method to enhance speech-based large language models (SpeechLMs) by aligning their instruction-following capabilities with powerful text-based LLMs. The approach addresses the performance gap caused by inter-modal discrepancies through self-synthesis data generation and reinforcement learning. RBA generates high-quality instruction datasets using a teacher LLM and synthesizes spoken instructions with diverse voices via zero-shot TTS. The SpeechLM is then aligned with the teacher using a reinforcement learning-based optimization strategy. Experiments show that RBA outperforms conventional SFT baselines, achieving state-of-the-art performance on tasks like spoken question answering and speech-to-text translation, with significant improvements in accuracy and BLEU scores.

## Method Summary
RBA uses a teacher LLM (Llama-3.1-70B-Instruct) to generate (instruction, response) pairs, which are then synthesized into multi-speaker audio using CosyVoice TTS (4 speakers per text instruction). The SpeechLM (Qwen2-Audio 8B) is fine-tuned using a DPO-style loss comparing either the teacher response (RBA-Single) or the best student response (RBA-Group) against the worst. Training uses L = L_G + λL_CE with λ=0.2, β=0.1, LR warmup 3,000 steps to 1e-4, and weight decay 0.98. The approach is evaluated on spoken question answering and speech-to-text translation tasks using datasets like FLEURS, CoVoST2, and MuST-C.

## Key Results
- RBA outperforms SFT baselines on spoken question answering accuracy and speech-to-text translation BLEU scores
- RBA-Group achieves high speaker-consistency score (0.945) across different synthetic voices
- RBA-Single shows superior instruction-following performance compared to RBA-Group

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning (RL) based alignment mitigates the "exposure bias" inherent in Supervised Fine-Tuning (SFT) of SpeechLMs. SFT relies on teacher-forcing (predicting the next token based on the ground-truth history), which creates a discrepancy during inference when the model must rely on its own potentially imperfect generated history. By optimizing sequences generated via self-sampling against a reward signal, the model explores the generation space and learns to recover from its own errors. The base SpeechLM must possess sufficient capability to generate viable candidate responses for contrastive learning.

### Mechanism 2
Multi-speaker synthesis combined with group-based preference optimization enforces speaker-invariant semantic alignment. By synthesizing a single text instruction into 4 distinct spoken versions and then optimizing the SpeechLM to prefer high-quality outputs regardless of the speaker, the model is pressured to focus on the semantic content rather than acoustic artifacts. The TTS system must provide sufficiently diverse acoustic realizations to prevent the model from overfitting to specific voice prints.

### Mechanism 3
Direct Preference Optimization (DPO) using teacher responses as "positive" samples bypasses the need for a separate reward model training phase. Instead of training a separate neural network to predict human preferences, RBA-Single treats the Teacher LLM's text response as the explicit "winner" and the SpeechLM's sampled response as the "loser." This directly distills the teacher's behavior while aligning modalities. The text-based Teacher LLM must be consistently superior to the SpeechLM in reasoning and instruction following.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: RBA relies on DPO-style loss functions to align the SpeechLM with the Teacher LLM
  - Quick check: Can you explain why DPO is often more stable than Proximal Policy Optimization (PPO) for aligning large models?

- **Concept: Speech Tokenization & Modality Bridging**
  - Why needed: The paper assumes a SpeechLM architecture that converts audio signals into tokens the LLM backbone can process
  - Quick check: How does the SpeechLM handle the alignment of continuous audio embeddings with the discrete token embeddings of the pre-trained text LLM?

- **Concept: Exposure Bias**
  - Why needed: The paper positions RBA as a solution to exposure bias found in standard SFT
  - Quick check: In standard SFT, if the model makes one mistake during inference, why does this often lead to a cascade of further errors?

## Architecture Onboarding

- **Component map:** Teacher LLM (Llama-3.1-70B) -> CosyVoice TTS (4 speakers) -> SpeechLM (Qwen2-Audio) -> Optimizer (DPO)
- **Critical path:** The generation of high-fidelity synthetic data. If the TTS produces robotic or erroneous speech, or if the Teacher LLM generates generic responses, the alignment will fail to impart "intelligence."
- **Design tradeoffs:**
  - RBA-Group vs. RBA-Single: RBA-Group selects the best among the Student's 4 outputs; robust for translation but requires the Student to be capable. RBA-Single treats the Teacher as the gold standard; better for general instruction following but relies entirely on the Teacher's domain coverage.
  - Synthetic vs. Real Data: Using 100% synthetic data allows scaling to 1M samples but risks "simulation-to-real" gaps where the model fails on noisy, real-world microphone input.
- **Failure signatures:**
  - Catastrophic Forgetting: The model may lose factual knowledge if alignment phase ignores specific downstream tasks
  - Reward Hacking: The model might learn to game the reward model rather than actually following instructions
- **First 3 experiments:**
  1. Sanity Check Data Loop: Generate 100 instructions with the Teacher, synthesize speech, and verify TTS output is intelligible and Teacher response is relevant
  2. RBA-Single Ablation: Train SpeechLM using only RBA-Single on small subset (10k samples) and compare win-rates against SFT baseline
  3. Cross-Speaker Consistency Test: Feed same instruction spoken by 4 different synthetic voices into fine-tuned model and measure semantic similarity

## Open Questions the Paper Calls Out
None

## Limitations
- Data Quality Dependency: Entire approach hinges on Teacher LLM generating high-quality instruction-response pairs and TTS producing diverse, high-fidelity speech
- Computational Overhead: Generating 1M instruction-response pairs via a 70B parameter LLM is resource-intensive with undisclosed costs
- Generalization Gap: Unclear whether gains translate to real-world noisy microphone input or non-instruction-following tasks

## Confidence
- High Confidence: The DPO mechanism (RBA-Single) is well-grounded and reported win-rates vs. SFT are statistically significant
- Medium Confidence: Claim that RL "effectively mitigates exposure bias" is supported by ablation studies but not directly measured
- Low Confidence: Scalability claim ("large-scale datasets without human annotation") is theoretical with no ablation for dataset size

## Next Checks
1. **TTS-Inferred ASR Validation**: Run ASR on a random 1% sample of synthetic speech data to compute WER and correlate with downstream model performance
2. **Speaker-Invariant Stress Test**: Take a single text instruction, synthesize with 10+ speakers, and measure consistency score across all variants
3. **Cross-Domain Generalization Probe**: Evaluate fine-tuned SpeechLM on non-instruction-following speech task to assess catastrophic forgetting