---
ver: rpa2
title: Language Models Entangle Language and Culture
arxiv_id: '2601.15337'
source_url: https://arxiv.org/abs/2601.15337
tags:
- language
- response
- responses
- score
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that large language models (LLMs) provide lower
  quality and culturally different responses when queried in low-resource languages
  compared to high-resource ones. The authors created a set of generic advice-seeking
  questions from the WildChat dataset and evaluated multilingual LLMs (Qwen3, Cohere-Aya,
  Magistral, Sarvam-m) across English, Hindi, Chinese, Swahili, Hebrew, and Brazilian
  Portuguese.
---

# Language Models Entangle Language and Culture

## Quick Facts
- **arXiv ID**: 2601.15337
- **Source URL**: https://arxiv.org/abs/2601.15337
- **Reference count**: 40
- **Primary result**: LLMs produce lower quality and culturally different responses in low-resource languages compared to high-resource ones

## Executive Summary
This paper demonstrates that large language models exhibit significant performance disparities across languages, with lower quality responses in low-resource languages like Hindi, Swahili, and Hebrew compared to high-resource languages like English, Chinese, and Brazilian Portuguese. The study reveals that language choice activates language-associated cultural representations, causing responses to retain cultural markers from their generation language even after translation. Using a carefully designed evaluation framework with LLM-as-a-Judge scoring and cultural classification, the authors show that multilingual LLMs entangle language and culture in ways that affect both response quality and cultural context.

## Method Summary
The study evaluated four multilingual LLMs (Qwen3, Cohere-Aya, Magistral, Sarvam-m) across six languages using 20 culture-neutral advice-seeking questions derived from the WildChat dataset. Queries were translated using Gemini-2.5-Flash, and models generated 10 responses per (question, language) pair at temperature 1. Responses were scored by an LLM-as-a-Judge (Cohere Command-A) using 8 reference examples and 5 rubric criteria. The evaluation included translation ablation experiments, cultural context classification of translated responses, and validation on a translated subset of the CulturalBench benchmark.

## Key Results
- Statistically significant quality differences across languages (p < 0.05 for all models), with Hindi, Swahili, and Hebrew consistently scoring lower
- Translated responses retained cultural markers from their original generation language (74% of Chinese responses classified as Chinese culture, 47% of Hindi responses as Indian culture)
- Sarvam-m and Magistral showed language-specific performance improvements despite being fine-tuned from the same base model
- CulturalBench accuracy varied significantly by language, demonstrating that language choice changes cultural context in responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Lower response quality in low-resource languages stems from imbalanced training data representation
- **Mechanism**: Models trained on corpora with unequal language representation develop stronger generative capabilities for high-resource languages; when prompted in low-resource languages, the model produces responses with reduced detail, actionability, and linguistic quality
- **Core assumption**: Training data volume and quality directly correlate with downstream task performance
- **Evidence anchors**:
  - Kruskal-Wallis test shows p < 0.05 for all models, with Hindi, Swahili, and Hebrew consistently scoring lower than English, Chinese, and Brazilian Portuguese
  - Limited direct corpus support for pretraining data causation; related work (Camellia benchmark) confirms similar disparities in Asian languages

### Mechanism 2
- **Claim**: Language choice activates language-associated cultural representations during response generation
- **Mechanism**: When a query is posed in a particular language, the model retrieves cultural knowledge patterns statistically associated with that language during pretraining; these patterns persist even when responses are translated to other languages
- **Core assumption**: Cultural knowledge is stored in language-correlated representations within the model
- **Evidence anchors**:
  - Cultural classification of translated responses shows 74% of Chinese-language responses classified as Chinese culture, 47% of Hindi responses as Indian culture, even after translation to English
  - "Localized Cultural Knowledge is Conserved and Controllable in Large Language Models" (arXiv:2504.10191) confirms cultural information persists and can be activated

### Mechanism 3
- **Claim**: Post-training fine-tuning can improve quality for specific languages without architectural changes
- **Mechanism**: Targeted fine-tuning on language-specific data shifts model behavior toward better performance in that language, even when starting from the same base model
- **Core assumption**: Fine-tuning can modify language-specific generation patterns without full retraining
- **Evidence anchors**:
  - Sarvam-m provides better responses for English and Hindi while Magistral provides better responses for English, Chinese and Brazilian Portuguese despite both being finetuned from Mistral-Small-3.1-24B
  - Cohere-Aya-32B shows smaller performance differences than Cohere-Aya-8b which suggests that larger models show higher consistency across languages

## Foundational Learning

- **Concept: LLM-as-a-Judge Evaluation**
  - **Why needed here**: The paper uses this method to score response quality across languages; understanding its limitations is critical for interpreting results
  - **Quick check question**: How would you detect if a judge model systematically favors responses in certain languages?

- **Concept: Statistical Significance Testing (Kruskal-Wallis)**
  - **Why needed here**: The paper relies on this non-parametric test to establish that quality differences are not due to random variation
  - **Quick check question**: Why use Kruskal-Wallis instead of ANOVA for evaluating model response scores?

- **Concept: Low-Resource vs. High-Resource Languages**
  - **Why needed here**: The core finding distinguishes performance gaps based on training data availability; this framing drives the conclusions
  - **Quick check question**: Is Hebrew low-resource in this paper's context? How would you verify this classification?

## Architecture Onboarding

- **Component map**: WildChat analysis -> clustering (HDBSCAN) -> 20 culture-neutral questions -> translation (Gemini-2.5-Flash) -> Response generation (target LLM, T=1) -> LLM-as-a-Judge evaluation (Cohere Command-A, T=0) -> scoring (1-5 rubrics) -> Kruskal-Wallis significance testing -> Cultural context classification -> CulturalBench evaluation

- **Critical path**: Judge alignment verification -> response generation with temperature=1 -> evaluation with temperature=0 -> Kruskal-Wallis significance testing

- **Design tradeoffs**:
  - Using LLM-as-a-Judge vs. human evaluation: scales well but inherits judge model biases
  - 8 reference examples vs. fewer: higher alignment (Pearson correlation ~0.6) but increased evaluation cost
  - Temperature=1 for generation vs. lower: increases response diversity but introduces variance

- **Failure signatures**:
  - Judge model favors its own language family (mitigated by translation ablation showing English→Hindi responses score higher than Hindi→English)
  - Translation artifacts introduce cultural markers (controlled by using same translation model for all languages)
  - Query translation loses cultural neutrality (paper assumes culture-independent queries but does not fully verify)

- **First 3 experiments**:
  1. Replicate the judge alignment ablation (Appendix C) with your own judge model to verify no language bias
  2. Extend CulturalBench evaluation to additional languages and models to test generalizability of language-culture entanglement
  3. Control for random perturbation effects by adding the same random strings used in Appendix I to verify that language effects exceed noise

## Open Questions the Paper Calls Out

- **Open Question 1**: Do larger, proprietary models exhibit the same language-culture entanglement found in open-source models?
  - **Basis in paper**: The authors state their work is "limited to small to moderate open-source models and can be extended to larger models."
  - **Why unresolved**: The study only evaluated Qwen3, Cohere-Aya, Magistral, and Sarvam-m, omitting frontier models
  - **What evidence would resolve it**: Evaluating models like GPT-4 or Claude on the translated CulturalBench subset

- **Open Question 2**: Can mechanistic interpretability identify the internal mechanisms linking language to cultural context?
  - **Basis in paper**: The authors suggest "further research can also include use of mechanistic interpretability techniques to study the relationship."
  - **Why unresolved**: This study relied on behavioral evaluation of outputs rather than internal model state analysis
  - **What evidence would resolve it**: Analysis of activation patterns showing distinct circuits activating for the same query in different languages

- **Open Question 3**: Is the performance variance strictly caused by the volume of culture-specific pretraining data?
  - **Basis in paper**: The authors "hypothesize that the relation... depends on the amount of pretraining data," but do not verify this
  - **Why unresolved**: The paper establishes correlation but lacks analysis of training data composition to prove causality
  - **What evidence would resolve it**: Correlating training data ratios for specific culture-language pairs with benchmark accuracy

## Limitations

- The use of LLM-as-a-Judge introduces potential bias, though translation ablation experiments partially mitigate this concern
- The classification of languages as "low-resource" vs "high-resource" relies on implicit assumptions about training data availability rather than direct corpus analysis
- The cultural neutrality of advice-seeking queries is assumed rather than empirically verified across all languages and cultures represented

## Confidence

**High Confidence**: The statistical significance of quality differences across languages (Kruskal-Wallis p < 0.05 for all models) is robust and well-demonstrated. The finding that translated responses retain cultural markers from their generation language is supported by multiple experimental conditions.

**Medium Confidence**: The causal link between training data imbalance and performance differences is plausible but not directly proven. The mechanisms connecting language choice to cultural context activation are inferred from observed correlations rather than direct measurement of internal model representations.

**Low Confidence**: The generalizability of findings to other types of queries beyond advice-seeking questions remains untested. The specific impact of fine-tuning approaches on multilingual performance is only partially explored.

## Next Checks

1. **Judge Bias Validation**: Replicate the judge alignment ablation using different LLM-as-a-Judge models to verify that observed language effects persist across multiple evaluation frameworks and are not artifacts of judge model bias

2. **Query Neutrality Verification**: Conduct user studies with native speakers across all six languages to empirically validate that the 20 advice-seeking questions are truly culture-neutral and don't inadvertently trigger different cultural associations in different language contexts

3. **Fine-tuning Mechanism Isolation**: Design controlled experiments varying only the fine-tuning data for different languages while holding architecture constant, to directly measure the impact of post-training optimization on multilingual quality consistency