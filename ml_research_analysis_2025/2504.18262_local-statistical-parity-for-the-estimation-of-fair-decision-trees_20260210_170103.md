---
ver: rpa2
title: Local Statistical Parity for the Estimation of Fair Decision Trees
arxiv_id: '2504.18262'
source_url: https://arxiv.org/abs/2504.18262
tags:
- fairness
- decision
- node
- tree
- domtt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating fairness into
  decision tree models while maintaining interpretability and accuracy. The authors
  introduce a novel local fairness criterion called "Local Statistical Parity" at
  the node level and prove its relationship with the global Statistical Parity criterion.
---

# Local Statistical Parity for the Estimation of Fair Decision Trees

## Quick Facts
- arXiv ID: 2504.18262
- Source URL: https://arxiv.org/abs/2504.18262
- Reference count: 32
- Primary result: Introduces Local Statistical Parity criterion and C-LRT algorithm that achieves controlled accuracy-fairness trade-offs in decision trees

## Executive Summary
This paper addresses the challenge of incorporating fairness into decision tree models while maintaining interpretability and accuracy. The authors introduce a novel local fairness criterion called "Local Statistical Parity" at the node level and prove its relationship with the global Statistical Parity criterion. They propose a tree estimation algorithm called Constrained Logistic Regression Tree (C-LRT), which modifies the standard CART algorithm by incorporating fairness constraints inspired by Constrained Logistic Regression (C-LR) at each node.

## Method Summary
C-LRT modifies CART's recursive partitioning by adding fairness constraints at each split. For each node and predictor, it fits a constrained logistic regression minimizing log-loss while bounding the covariance between the signed distance and the protected attribute (|Cov(sdθ(Xⱼ), A)| ≤ c). The algorithm then selects the split minimizing classification error among all predictors. This per-node optimization preserves computational tractability while promoting fairness, allowing users to control the accuracy-fairness trade-off through the constraint parameter c.

## Key Results
- C-LRT successfully controls the trade-off between accuracy and fairness across four datasets (Adult, COMPAS, Ricci, Law School)
- Stronger fairness constraints (smaller c values) yield better fairness metrics at the cost of reduced accuracy
- C-LRT produces degenerate (constant) trees when c is too small, but maintains reasonable performance with moderate constraints
- The algorithm provides a practical solution for building interpretable fair decision trees with controllable fairness properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing independence between protected attributes and local node tests propagates to global Statistical Parity.
- Mechanism: Theorem 7 proves that if every decision node's test satisfies A ⊥ I*ₜ(X) | domTₜ(X) = 1, then the full tree satisfies A ⊥ T(X).
- Core assumption: Conditional independence at each node is achievable and preserved through recursive splits.
- Evidence anchors: [abstract] "We prove how it is related to the Statistical Parity criterion"; [section 2] Theorem 7 and Lemma 6; [corpus] No direct validation found.
- Break condition: If nodes are not independent conditioned on their domain, or if the relaxation is too loose.

### Mechanism 2
- Claim: Bounding covariance between signed distance and protected attribute serves as a tractable proxy for independence.
- Mechanism: C-LRT constrains |dCov(sdθ(Xⱼₜ), A, Dₜ,ⱼ)| ≤ c at each node. Smaller c enforces tighter independence, trading accuracy for fairness.
- Core assumption: Low covariance approximates statistical independence sufficiently for practical fairness goals.
- Evidence anchors: [section 3] "C-LR...imposes fairness constraints by smoothing Statistical Parity controlled by a parameter c"; [section 4.2] "Stronger constraints (smaller c values) yield better fairness metrics at the cost of reduced accuracy."
- Break condition: If protected attribute and features have complex nonlinear relationships, covariance bounds may not capture true dependence.

### Mechanism 3
- Claim: Recursive, node-local optimization preserves computational tractability while promoting fairness.
- Mechanism: C-LRT fits a constrained logistic regression for each predictor independently at each node (Algorithm 1), then selects the split minimizing classification error.
- Core assumption: Local greedy decisions accumulate to near-optimal global fairness-accuracy trade-offs.
- Evidence anchors: [section 1] "classical methods construct a tree by adding one node at a time in a recursive way"; [section 3] Algorithm 1; [corpus] Related fair decision trees also use recursive approaches.
- Break condition: If early unfair splits irreversibly bias downstream nodes, local constraints may not recover global fairness.

## Foundational Learning

- Concept: **Statistical Parity (Demographic Parity)**
  - Why needed here: This is the global fairness criterion C-LRT targets; understanding P(Ŷ=1|A=0) = P(Ŷ=1|A=1) is essential.
  - Quick check question: If a model approves 60% of group A=1 but only 30% of group A=0, does it satisfy Statistical Parity?

- Concept: **CART (Classification and Regression Trees)**
  - Why needed here: C-LRT modifies CART's split selection; you must understand recursive partitioning and impurity reduction.
  - Quick check question: In standard CART, what metric typically guides split selection at each node?

- Concept: **Constrained Convex Optimization**
  - Why needed here: C-LRT solves a constrained logistic regression at each node; the constraint is convex, enabling SQP solvers.
  - Quick check question: Why does convexity of the constraint matter for finding a solution?

## Architecture Onboarding

- Component map: Data partitioning -> Per-predictor C-LR fitting -> Split selection -> Recursion
- Critical path: 1. Extract Dₜ,ⱼ for each predictor at node t; 2. Solve constrained optimization (SQP) for each θⱼ; 3. Evaluate classification error for each candidate split; 4. Select best (jₜ, θₜ) and recurse
- Design tradeoffs:
  - c parameter: Small c → stricter fairness, potential accuracy drop, risk of degenerate trees; large c → approaches unconstrained LRT
  - Per-feature vs. joint optimization: Current design fits each feature independently; joint optimization would be more expressive but computationally heavier
  - Stopping criteria: Standard CART heuristics (depth, min samples) apply
- Failure signatures:
  - Degenerate trees: With very small c, tree collapses to single-node constant classifier
  - High variance under strict constraints: Fairness metrics show larger confidence intervals at low c values
  - No feasible split: If no θ satisfies the covariance constraint for any predictor
- First 3 experiments:
  1. Baseline comparison: Run C-LRT and unconstrained LRT on Adult dataset; vary c and plot accuracy vs. SP difference
  2. Ablation on tree depth: Limit max depth (3, 5, 10) and measure whether deeper trees maintain fairness gains
  3. Protected attribute scaling: Test with binary vs. multi-level protected attributes to verify C-LRT's handling of non-binary A

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the tendency for C-LRT to produce degenerate (constant) trees under strict fairness constraints be mitigated?
- Basis in paper: [explicit] Future Work section states need to "investigate whether there are alternatives to reduce the occurrence of degenerate trees when imposing strict constraints."
- Why unresolved: The paper observes that setting c too low often forces the tree to collapse into a single-node constant classifier.
- Evidence: A modified algorithm or regularization strategy that maintains tree depth and non-constant predictions even with low c values.

### Open Question 2
- Question: Is it computationally feasible to enforce the exact Local Statistical Parity criterion at decision nodes rather than using a covariance-based relaxation?
- Basis in paper: [explicit] Authors note interest in "working with the fairness criterion directly instead of using a relaxation."
- Why unresolved: The current method relies on a convex relaxation to ensure the optimization problem remains tractable.
- Evidence: An efficient algorithm that evaluates exact independence for all potential splits without prohibitive computational cost.

### Open Question 3
- Question: Can local node-level criteria be theoretically linked to global fairness definitions other than Statistical Parity?
- Basis in paper: [explicit] Authors express interest in proposing "node-level (local) fairness criteria that have theoretical relationships with other fairness criteria from the literature."
- Why unresolved: This paper establishes a theoretical link specifically for Statistical Parity; it does not extend the local-to-global proof to criteria like Equalized Odds.
- Evidence: Formal proofs connecting specific local split properties to global satisfaction of metrics like Equal Opportunity or calibration.

## Limitations
- The theoretical guarantee linking local independence to global Statistical Parity relies on the assumption that covariance bounds adequately approximate independence, which is not rigorously validated
- Empirical validation shows the intended trade-off but does not test the robustness of the theoretical guarantee under noisy or highly correlated features
- C-LRT can produce degenerate (constant) trees when the fairness constraint is too strict, limiting practical applicability in high-fairness scenarios

## Confidence
- **Medium-High**: The algorithm works as designed in controlled experiments, but the assumption that local fairness constraints aggregate to global fairness without additional conditions is not fully explored. The use of covariance as a proxy for independence is pragmatic but may fail in complex feature spaces.

## Next Checks
1. Test C-LRT on datasets with highly correlated protected attribute and features to assess whether covariance bounds remain meaningful
2. Compare C-LRT's global fairness metrics against a baseline that enforces global Statistical Parity directly to isolate the benefit of local constraints
3. Analyze failure cases where the covariance constraint cannot be satisfied for any split—does the algorithm degrade gracefully, or produce biased trees?