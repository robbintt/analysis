---
ver: rpa2
title: 'SEAL: Speech Embedding Alignment Learning for Speech Large Language Model
  with Retrieval-Augmented Generation'
arxiv_id: '2502.02603'
source_url: https://arxiv.org/abs/2502.02603
tags:
- speech
- text
- retrieval
- embedding
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of latency and error propagation
  in speech-based retrieval-augmented generation (RAG) systems for speech large language
  models (SLLMs). Traditional two-stage systems that combine automatic speech recognition
  (ASR) with text-based retrieval suffer from high latency and error propagation issues.
---

# SEAL: Speech Embedding Alignment Learning for Speech Large Language Model with Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2502.02603
- Source URL: https://arxiv.org/abs/2502.02603
- Reference count: 7
- Primary result: 50% latency reduction and higher retrieval accuracy than two-stage ASR+text pipelines

## Executive Summary
SEAL addresses the high latency and error propagation issues in traditional speech-based retrieval-augmented generation (RAG) systems that rely on automatic speech recognition (ASR) followed by text-based retrieval. The proposed unified embedding framework eliminates intermediate text representations by using separate speech and text encoders with a shared scaling layer to map both modalities into a common embedding space. The model achieves 50% pipeline latency reduction while outperforming traditional two-stage methods on retrieval accuracy across multiple tasks and benchmarks.

## Method Summary
SEAL employs a two-stage training approach using Whisper-large-v3 as the speech encoder and Piccolo-large-zh-v2 as the text encoder. Stage 1 uses MSE alignment between speech and text token embeddings through Conv1D and MLP adapters. Stage 2 applies multi-task contrastive fine-tuning with task-specific losses including InfoNCE for retrieval, cosent for semantic textual similarity, and contrastive loss for classification. The system reduces sequence length via Conv1D stride for speed while maintaining retrieval quality, achieving significant improvements on the CMTEB benchmark and knowledge base retrieval tasks.

## Key Results
- Achieves Top-1 accuracy of 86.36% and Top-3 accuracy of 92.47% on a knowledge base with tens of thousands of entries
- Outperforms two-stage ASR approach across classification, pairwise classification, reranking, and retrieval tasks
- Reduces pipeline latency by 50% compared to traditional ASR+text retrieval systems
- Falls between ASR pipeline and text-only models in overall performance

## Why This Works (Mechanism)
The unified embedding approach eliminates error propagation from ASR systems by directly mapping speech to document embeddings without intermediate text representation. By training speech and text encoders to produce aligned embeddings in a shared space, the model captures semantic relationships while avoiding transcription errors. The two-stage training strategy first establishes cross-modal alignment through MSE loss, then refines retrieval-specific capabilities through contrastive learning with task-specific objectives, enabling the system to handle diverse retrieval scenarios effectively.

## Foundational Learning
- Concept: Contrastive learning with InfoNCE/cosent losses
  - Why needed here: SEAL's Stage 2 relies on contrastive losses to pull speech-document pairs together and push negatives apart; understanding these objectives is required to diagnose retrieval failures
  - Quick check question: Can you derive the InfoNCE loss and explain how temperature τ affects gradient sharpening?

- Concept: Cross-modal alignment via feature-space matching (MSE)
  - Why needed here: Stage 1 aligns speech and text token embeddings with MSE to bridge modalities; this differs from classification-style alignment and has distinct failure modes
  - Quick check question: What does minimizing MSE between speech and text token embeddings assume about their distributions?

- Concept: Retrieval evaluation at scale (Top-k accuracy, latency)
  - Why needed here: SEAL claims 50% latency reduction and Top-1/Top-3 accuracy gains; understanding these metrics is essential to assess trade-offs between speed and retrieval quality
  - Quick check question: How would a 50% latency reduction affect end-to-end RAG throughput in a real-time dialogue system?

## Architecture Onboarding
- Component map: Speech encoder (Whisper-large-v3) -> Conv1D adapter -> MLP adapter -> shared linear scaling layer -> embedding; Text encoder (Piccolo-large-zh-v2) -> shared linear scaling layer -> embedding
- Critical path:
  1. Verify speech encoder and text encoder outputs are correctly dimension-matched via the adapter and scaling layer
  2. Confirm Stage 1 alignment loss is converging (MSE should decrease across epochs)
  3. In Stage 2, validate that task-specific contrastive losses improve retrieval metrics on a held-out set
- Design tradeoffs:
  - Latency vs. fidelity: Conv1D stride reduces sequence length for speed but may compress long utterances too aggressively
  - Modality gap vs. overfitting: Aggressive MSE alignment may blur acoustic nuances; insufficient alignment leaves the modality gap unbridged
  - Task-specific vs. unified losses: Using different losses (InfoNCE, cosent, classification) helps diverse tasks but complicates tuning and may conflict
- Failure signatures:
  - High Top-1/Top-3 accuracy on text-only but low on speech queries → alignment or adapter failure
  - Sudden loss spikes or divergence during contrastive fine-tuning → learning rate/temperature misconfiguration or insufficient negatives
  - Latency not improving relative to ASR baseline → Conv1D stride or inference pipeline inefficiency
- First 3 experiments:
  1. Reproduce Stage 1 alignment on a small speech-text dataset (e.g., 100 hours) and track MSE convergence to confirm adapter correctness
  2. Run Stage 2 contrastive training with a single task (retrieval) and compare Top-1/Top-3 accuracy against an ASR + text retrieval baseline
  3. Ablate the Conv1D adapter (vary stride/kernel) on long-form speech and measure changes in both latency and retrieval accuracy

## Open Questions the Paper Calls Out
- Question: Can discrete speech tokens as end-to-end inputs achieve better speech-text embedding alignment than continuous representations?
  - Basis in paper: [explicit] The conclusion states: "we observe a growing trend in utilizing discrete tokens as end-to-end inputs for SLLMs, which we believe presents a promising avenue for future research in the development of more efficient and capable speech-text embedding models"
  - Why unresolved: The current work uses continuous speech features from Whisper; discrete token approaches were not explored or compared
  - What evidence would resolve it: A comparative study evaluating SEAL against discrete token-based speech embedding methods on the same CMTEB and retrieval benchmarks

- Question: How can the sequence length mismatch between speech and text modalities be effectively resolved for projection-based approaches?
  - Basis in paper: [explicit] The authors state: "We believe that the significant difference in sequence length between speech and text poses a fundamental challenge in embedding training tasks, making it difficult to directly integrate with text embedding models through cascaded transformations"
  - Why unresolved: Neither the projection-to-text nor CTC alignment baselines succeeded; the sequence length mismatch remains an open architectural challenge
  - What evidence would resolve it: A method that achieves competitive retrieval accuracy while using direct projection from speech features to text embedding space

## Limitations
- Critical design details remain underspecified including Conv1D kernel size and stride parameters, shared scaling layer output dimension, and negative sampling strategy for contrastive learning
- All experiments use Chinese datasets with Chinese-specific text encoders, limiting generalizability to other languages
- Evaluation relies on TTS-generated speech which lacks acoustic variability and environmental noise present in real-world speech queries
- Multi-task training batch composition details are missing, affecting reproducibility of reported performance gains

## Confidence
- High confidence in the overall problem framing and basic two-stage training approach
- Medium confidence in reported accuracy improvements given missing hyperparameter details
- Low confidence in precise 50% latency claim without hardware/implementation details and independent replication

## Next Checks
1. Verify adapter convergence and alignment quality: Train Stage 1 (MSE alignment) on a small, controlled speech-text dataset, track MSE loss across epochs, and confirm the learned adapters produce matched embedding dimensions before contrastive fine-tuning
2. Isolate the impact of sequence-length reduction: Systematically vary Conv1D kernel size and stride in the adapter, measure changes in both inference latency and retrieval accuracy, and determine the optimal trade-off point for long-form utterances
3. Replicate contrastive retrieval performance: Run Stage 2 contrastive fine-tuning on a held-out KB, compare Top-1/Top-3 accuracy against a two-stage ASR+text retrieval baseline, and perform an ablation removing the Stage 1 alignment to confirm its necessity for the claimed accuracy gains