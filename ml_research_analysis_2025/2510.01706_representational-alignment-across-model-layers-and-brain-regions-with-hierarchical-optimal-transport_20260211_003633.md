---
ver: rpa2
title: Representational Alignment Across Model Layers and Brain Regions with Hierarchical
  Optimal Transport
arxiv_id: '2510.01706'
source_url: https://arxiv.org/abs/2510.01706
tags:
- subject
- layers
- dinov2
- vit-mae
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical Optimal Transport (HOT) addresses the challenge of
  aligning neural network representations across layers and models of varying depths.
  Unlike standard methods that match each layer independently, HOT jointly optimizes
  global layer-to-layer couplings and neuron-to-neuron transport plans under marginal
  constraints.
---

# Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport

## Quick Facts
- arXiv ID: 2510.01706
- Source URL: https://arxiv.org/abs/2510.01706
- Authors: Shaan Shah; Meenakshi Khosla
- Reference count: 26
- Primary result: Hierarchical Optimal Transport (HOT) achieves higher reconstruction accuracy than greedy pairwise methods for aligning neural network representations across layers and models of varying depths.

## Executive Summary
This paper introduces Hierarchical Optimal Transport (HOT), a method for aligning neural network representations across layers and models with potentially different depths. Unlike standard approaches that match each layer independently, HOT jointly optimizes global layer-to-layer couplings and neuron-to-neuron transport plans under marginal constraints. This allows source neurons to distribute mass across multiple target layers, producing a single network-level alignment score and naturally handling depth mismatches. The method also incorporates rotation invariance to capture geometric equivalences, further improving alignment quality. Evaluated on vision models, large language models, and human visual cortex data, HOT achieves higher reconstruction accuracy than greedy pairwise methods and reveals smooth hierarchical correspondences.

## Method Summary
HOT solves representational alignment through a two-level optimal transport framework. First, inner-level OT computes neuron-to-neuron couplings between each source-target layer pair using correlation distance. These inner costs form a layer-to-layer cost matrix, which is then solved by outer-level OT to find global layer coupling P. Marginal constraints enforce mass conservation—each source layer distributes exactly 1/L mass, each target receives exactly 1/M. When L≠M, this produces soft many-to-many layer mappings. HOT+R extends this with rotation invariance via alternating optimization between OT and orthogonal Procrustes alignment. The method reconstructs source activations as weighted combinations of target features and evaluates reconstruction correlation on held-out data.

## Key Results
- HOT achieves higher reconstruction correlation than greedy pairwise OT methods across all tested model pairs
- Depth mismatches are handled naturally through soft layer mappings, with single layers distributing mass across multiple consecutive layers in deeper models
- HOT+R with rotation invariance yields substantially higher reconstruction scores than vanilla HOT
- Hierarchical alignments reveal smooth correspondences: early layers align to early layers, with deeper layers preserving relative positions

## Why This Works (Mechanism)

### Mechanism 1: Two-Level Optimal Transport Decomposition
Jointly optimizing neuron-level and layer-level transport plans yields globally consistent alignments that greedy pairwise methods miss. HOT decomposes alignment into (1) inner-level OT solving neuron-to-neuron couplings per layer pair using correlation distance, and (2) outer-level OT solving layer-to-layer couplings using the inner costs as the cost matrix. Marginal constraints enforce mass conservation—each source layer distributes exactly 1/L mass, each target receives exactly 1/M—preventing any layer from dominating or being ignored. Core assumption: The hierarchical structure of representations implies that good neuron-level alignments should inform which layers correspond, and vice versa.

### Mechanism 2: Soft Many-to-Many Layer Mappings via Transportation Polytope Relaxation
Allowing source layers to distribute mass across multiple target layers naturally handles depth mismatches without imposing hard correspondences. When L ≠ M, the outer transport polytope's vertices are not permutation matrices, so the optimal P is genuinely soft. A layer in a shallower network can split its representational mass across several adjacent layers in a deeper network, reflecting computational refinement across depth. Core assumption: Representational stages in shallower networks correspond to distributed computations spread across multiple layers in deeper networks.

### Mechanism 3: Rotation-Invariant Extension via Alternating Optimization
Incorporating orthogonal transformations into the OT framework captures geometric equivalences that rotation-sensitive methods miss. HOT+R alternates between (1) fixing rotation matrices R and solving OT on rotated features, (2) fixing transport Q and solving orthogonal Procrustes via SVD, and (3) updating layer coupling P with reconstruction costs. This aligns representations even when shared features occupy rotated subspaces. Core assumption: Representational geometry (distances, angles) matters more than individual neuron tuning for some comparison tasks, especially in transformer residual streams.

## Foundational Learning

- **Optimal Transport and Transportation Polytopes**: Understanding marginal constraints, mass conservation, and why vertices are permutation matrices when L=M is essential for grasping why HOT produces soft vs. hard assignments. Quick check: If a source layer has 100 neurons and must distribute unit mass with equal row marginals, how much mass does each neuron allocate?

- **Correlation Distance vs. Euclidean Distance in OT**: The inner-level uses correlation distance (1 - Pearson ρ), making alignment invariant to scale and offset differences in neural activations. Quick check: Two neurons have identical tuning profiles but different firing rates. Will correlation distance treat them as similar?

- **Birkhoff-von Neumann Theorem**: Explains why HOT yields hard one-to-one layer matching when L=M (optimal at polytope vertex) but soft matching when depths differ. Quick check: If the cost matrix C is linear in P and constraints define a polytope with permutation matrices as vertices, where does the optimum lie?

## Architecture Onboarding

- **Component map**: Activation extraction -> Inner OT solver (per layer pair) -> Outer OT solver (layer coupling) -> Rotation module (HOT+R) -> Reconstruction engine -> Evaluation
- **Critical path**: 1. Extract activations from all layers for shared stimulus set 2. Compute all pairwise inner OT costs Cℓm (dominant O(L·M·n³log n) cost) 3. Solve outer OT for P 4. (If HOT+R) Alternate rotation optimization until convergence 5. Evaluate reconstruction on held-out stimuli
- **Design tradeoffs**: Rotation sensitivity vs. invariance: Vanilla HOT captures neuron-level tuning; HOT+R captures geometry. Choose based on whether individual neuron semantics matter. Computational cost: Inner OT is O(n³ log n) per layer pair. Wide layers (>10k neurons) may require approximation (entropy regularization, Sinkhorn). Train/validation split: Transport plans learned on training stimuli; must evaluate on held-out to avoid overfitting claims.
- **Failure signatures**: Noisy/unstructured P matrix: Inner costs may lack signal; check if pairwise correlations reveal any layer structure before running HOT. Single target layer dominates: May indicate marginal constraint violation or numerical issues in OT solver. HOT+R diverges or oscillates: Learning rate or initialization issues in alternating optimization; try fixing Q for more iterations before updating R.
- **First 3 experiments**: 1. Sanity check on same network: Run HOT comparing a network to itself. Expect P ≈ identity matrix (diagonal) and reconstruction correlation ≈ 1.0. 2. Depth mismatch test: Compare a 12-layer ViT to a 24-layer ViT. Visualize P to verify mass spreads across adjacent layers rather than random assignments. 3. Baseline comparison: On identical data, compare HOT vs. Pairwise Best OT reconstruction correlation. Expect HOT ≥ baseline; if not, inspect whether pairwise is overfitting to training noise.

## Open Questions the Paper Calls Out

### Open Question 1
Can a third hierarchical level be incorporated into HOT to align representations across training dynamics or developmental trajectories? The authors suggest adding "a third hierarchical level to capture representational dynamics over the course of training" to compare models across different stages of training or biological learning. This remains unresolved because the current formulation only handles spatial alignment at fixed snapshots. Evidence would come from a modified HOT framework that successfully aligns intermediate checkpoints of training runs, revealing how representational mass shifts over time without losing global consistency.

### Open Question 2
Does the alignment recovered by HOT identify the causal mechanisms or computational principles that drive representational convergence? The authors note that while HOT provides descriptive alignment, "it does not by itself explain why certain features converge across systems, or which computational principles drive these correspondences." This remains unresolved because the method minimizes reconstruction cost but does not test hypotheses about why architectures converge on specific features. Evidence would come from experiments combining HOT alignments with perturbation studies to demonstrate that the identified correspondences are mechanistically necessary for specific functional behaviors.

### Open Question 3
Can algorithmic improvements make HOT tractable for very wide or deep foundation models where cubic scaling is prohibitive? The authors identify that "HOT is computationally demanding" and scaling to wide or deep models is "challenging without further algorithmic improvements" due to the O(L²n³log n) complexity. This remains unresolved because the reliance on standard optimal transport solvers creates a computational bottleneck for modern models with thousands of dimensions per layer. Evidence would come from the development of scalable approximations (e.g., entropic regularization or sliced OT) that retain global hierarchical consistency while reducing computational cost to linear or quadratic time.

## Limitations
- Exact OT solver implementation (entropy regularization strength, LP solver) and HOT+R alternating schedule are unspecified, affecting numerical stability and convergence
- Extreme depth mismatches (e.g., 2 vs. 100 layers) may yield overly diffuse alignments that lose interpretability
- Cross-modal brain-network alignment without further validation remains questionable

## Confidence
- High confidence: Hierarchical decomposition mechanism; depth mismatch handling; superiority over greedy pairwise baselines
- Medium confidence: Rotation invariance claims; fMRI-human cortex alignment results
- Low confidence: Cross-modal brain-network alignment without further validation

## Next Checks
1. Run HOT on identical networks to verify identity coupling P and near-perfect reconstruction (sanity check for implementation correctness)
2. Compare HOT vs. Pairwise Best OT on depth-mismatched models (e.g., 12-layer vs. 24-layer ViT) and visualize P to confirm adjacent layer spreading
3. Inspect marginals of Q and P matrices on real data to detect mass conservation violations or degenerate couplings