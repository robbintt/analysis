---
ver: rpa2
title: Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction
arxiv_id: '2505.24597'
source_url: https://arxiv.org/abs/2505.24597
tags:
- location
- prediction
- user
- next
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NextLocMoE introduces a dual-level Mixture-of-Experts (MoE) framework
  for next location prediction, addressing limitations in capturing location semantics
  and modeling heterogeneous user behaviors. It features a Location Semantics MoE
  that enriches location representations using function-specific experts, and a Personalized
  MoE that adapts to user behavioral patterns via role-based experts.
---

# Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction

## Quick Facts
- arXiv ID: 2505.24597
- Source URL: https://arxiv.org/abs/2505.24597
- Reference count: 40
- Primary result: Dual MoE framework achieving 17.77% Hit@1 on Kumamoto and 64.93% Hit@1 on Shanghai with up to 600× faster inference than LLM-based methods

## Executive Summary
This paper introduces NextLocMoE, a dual-level Mixture-of-Experts (MoE) framework for next location prediction that addresses limitations in capturing location semantics and modeling heterogeneous user behaviors. The architecture features a Location Semantics MoE that enriches location representations using function-specific experts, and a Personalized MoE that adapts to user behavioral patterns via role-based experts. A history-aware router improves expert selection by incorporating long-term trajectories. Experiments demonstrate superior performance over baselines in both fully-supervised and zero-shot settings, with significant computational efficiency gains.

## Method Summary
NextLocMoE is built on a truncated LLaMA-3.2-3B backbone (12 layers total) with two MoE modules. The Location Semantics MoE operates at the embedding level to encode rich functional semantics of locations using top-k routing with predefined function experts. The Personalized MoE, embedded within the Transformer backbone, dynamically adapts to individual user mobility patterns using confidence threshold routing with role-based experts. A history-aware router leverages long-term trajectory data to enhance expert selection. The model is trained with Adam optimizer (LR 1e-4) using a composite loss function combining distance and entropy regularization, with predictions mapped to nearest locations via KD-Tree retrieval.

## Key Results
- Achieves 17.77% Hit@1 on Kumamoto and 64.93% Hit@1 on Shanghai
- Outperforms state-of-the-art baselines in both fully-supervised and zero-shot settings
- Provides up to 600× faster inference than LLM-based competitors
- Ablation studies confirm both MoE modules are critical for performance and cross-domain generalization

## Why This Works (Mechanism)

### Mechanism 1: Function-Specific Semantic Disentanglement
Representing multi-functional locations via specialized embeddings improves expressiveness over single-vector representations. The Location Semantics MoE routes coordinates to shared spatial experts and function-specific experts (e.g., "Commercial", "Education"), weighted by trajectory context. This assumes location semantics are context-dependent and natural language descriptions provide viable inductive bias. Evidence shows improved expressiveness for locations serving multiple purposes, though limited direct evidence exists for the specific mechanism.

### Mechanism 2: Role-Based Behavioral Specialization
Heterogeneous user behaviors are better modeled by selecting expert sub-networks aligned with prototypical user roles (e.g., "Student", "Tourist") rather than shared dense networks. The Personalized MoE uses confidence-threshold routing to activate variable role experts, applying specialized pathways for different behavioral archetypes. This assumes user mobility patterns cluster into interpretable roles captured by predefined descriptions. Evidence supports the premise that heterogeneous behaviors require explicit modeling, though effectiveness depends on role description quality.

### Mechanism 3: Historical Context Disambiguation
Incorporating long-term trajectory history into expert selection stabilizes routing and resolves short-term ambiguities. The History-aware Router encodes long-term trajectories using TCN and guides gating networks in both MoE modules. This assumes long-term preferences contain stable signals necessary to interpret short-term actions. Evidence shows reduced over-reliance on local context, though effectiveness depends on TCN's ability to capture long-range dependencies.

## Foundational Learning

### Concept: Mixture-of-Experts (MoE) Routing
**Why needed:** The core architecture relies on sparse activation. Understanding Top-k vs Confidence Threshold routing is critical for debugging expert utilization.
**Quick check:** Does the Personalized MoE activate the same number of experts for a "Student" with fixed routine vs. "Undefined" user with chaotic patterns? (Answer: No, threshold mechanism activates more for chaotic user).

### Concept: LLM as a Backbone (PEFT)
**Why needed:** The model freezes most LLM weights and uses LoRA on experts. Understanding parameter-efficient fine-tuning is required to manage GPU memory and distinguish frozen semantic knowledge from trainable mobility adaptation.
**Quick check:** Which parts of the LLM backbone are actually updated during training? (Answer: LayerNorm and LoRA adapters within MoE experts).

### Concept: Spatio-Temporal Embeddings
**Why needed:** The model separates spatial coordinates, time, and duration. Understanding modality fusion is critical for analyzing the input pipeline.
**Quick check:** How is the "function-aware" spatial embedding created? (Answer: Sum of general spatial embedding and weighted sum of activated expert embeddings).

## Architecture Onboarding

### Component map:
Trajectory (S_h, S_c) -> Encoders (TCN & Projection Layers) -> MoE Stage 1: Location Semantics MoE -> Backbone (LLaMA-3.2-3B truncated) -> MoE Stage 2: Personalized MoE -> Head: Regression MLP -> Retrieval: KD-Tree Lookup

### Critical path:
The History-aware Router is the critical intersection, taking TCN output and injecting it into both Location MoE and Personalized MoE. Weak historical representation degrades both semantic enrichment and user personalization.

### Design tradeoffs:
Routing Strategies: Top-k for location semantics fixes compute cost, while Thresholds for personalization allows variable compute based on behavioral complexity. Layer Truncation: Using only 12 layers trades deep reasoning for faster inference (600x speedup) and reduced overfitting.

### Failure signatures:
Expert Collapse: Monitoring if Personalized MoE routes 90% of inputs to single "General" expert. Coordinate Drift: Regression predictions far outside city grid indicate inadequate grounding by input embeddings. Zero-Shot Drop: Massive performance drop in cross-city tests implies Location Semantics MoE failed to generalize functional definitions.

### First 3 experiments:
1. Router Ablation: Disable History-aware Router to quantify long-term context contribution to Hit@1.
2. Expert Activation Visualization: Plot activation distribution for known user types to verify role alignment.
3. Semantic Expert Interference: Mask Location Semantics MoE on datasets with multi-functional zones to verify performance delta.

## Open Questions the Paper Calls Out
- Can expert compression techniques be integrated to reduce training-time memory overhead without degrading semantic specialization?
- How does the granularity and accuracy of predefined natural language descriptions impact the model's ability to route inputs effectively?
- Is the fixed confidence threshold optimal across diverse urban environments, or does it require adaptive tuning?

## Limitations
- Performance claims rely on comparisons to LLM-based methods with context-dependent speedup figures
- Dual MoE design introduces complexity in routing stability that may not generalize across cultural contexts
- KD-Tree retrieval creates discrete approximation of continuous regression problem that could introduce bias in dense urban areas

## Confidence
**High Confidence:** Architectural components are well-specified and ablation studies provide strong evidence both MoE modules contribute meaningfully
**Medium Confidence:** 17.77% Hit@1 on Kumamoto and 64.93% Hit@1 on Shanghai represent state-of-the-art, though direct comparisons are limited
**Low Confidence:** Zero-shot cross-city generalization claims are promising but based on limited data points

## Next Checks
1. Router Stability Analysis: Track entropy of expert activation distributions across training epochs and user segments
2. Semantic Coverage Audit: Systematically evaluate whether predefined categories cover majority of locations and behaviors
3. Continuous vs Discrete Evaluation: Compare Hit@k metrics using regression output directly versus KD-Tree discretized retrieval