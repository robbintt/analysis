---
ver: rpa2
title: 'Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver
  Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation'
arxiv_id: '2601.09648'
source_url: https://arxiv.org/abs/2601.09648
tags:
- usas
- neural
- language
- semantic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first neural network models for USAS semantic
  tagging, trained on over 5 million silver-labelled English tokens, and demonstrates
  their effectiveness across five languages (English, Welsh, Irish, Finnish, and Chinese).
  We introduce a hybrid model that combines neural networks with rule-based semantic
  taggers, achieving superior performance compared to rule-based systems alone.
---

# Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation

## Quick Facts
- arXiv ID: 2601.09648
- Source URL: https://arxiv.org/abs/2601.09648
- Reference count: 0
- Primary result: Neural and hybrid models trained on silver-labelled English data outperform rule-based systems for USAS semantic tagging across five languages.

## Executive Summary
This work introduces the first neural network models for USAS semantic tagging, trained on over 5 million silver-labelled English tokens, and demonstrates their effectiveness across five languages (English, Welsh, Irish, Finnish, and Chinese). We introduce a hybrid model that combines neural networks with rule-based semantic taggers, achieving superior performance compared to rule-based systems alone. The models were evaluated on manually annotated datasets, with the Chinese dataset newly created for this work. Results show that neural and hybrid models consistently outperform rule-based systems, with larger models generally achieving better results. The silver-labelled English dataset and all trained models have been released as open resources.

## Method Summary
The method trains neural WSD models using silver-standard data generated by a rule-based tagger. A Bi-Encoder architecture with a shared multilingual PLM encodes both the input text and USAS tag glosses, predicting the tag whose gloss has the highest dot-product similarity. Models are fine-tuned on 5.3M silver-labelled English tokens from Wikipedia. The hybrid approach uses the rule-based tagger for high-confidence matches and the neural model as a back-off for out-of-vocabulary terms. Four PLM variants are evaluated: Ettin-Enc-17m/68m (English) and MMBERT small/base (multilingual). The approach is evaluated on manually annotated datasets for five languages.

## Key Results
- Neural models trained on silver English data effectively transfer to other languages using multilingual PLMs
- Hybrid models combining rule-based and neural approaches achieve superior performance compared to either system alone
- Chinese and English show strongest performance, correlating with higher pre-training token volumes
- Welsh and Irish performance drops significantly, suggesting limits of cross-lingual transfer for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
A neural model fine-tuned solely on English silver data can generalize to other languages if the underlying Pre-trained Language Model (PLM) has sufficient multilingual pre-training exposure. The authors use a multilingual PLM (MMBERT) which aligns semantic concepts across languages in a shared vector space. By fine-tuning on English silver labels (generated by a rule-based tagger), the model learns to map contextual embeddings to the USAS tagset. This mapping is assumed to transfer to non-English inputs because the PLM represents similar concepts similarly across languages.

### Mechanism 2
A hybrid architecture improves coverage and accuracy by using a rule-based system for high-confidence lexicon matches and a neural model as a back-off for out-of-vocabulary (OOV) terms. The system first queries the rule-based tagger (PyMUSAS), which utilizes explicit linguistic resources (lexicons). If the rule-based system fails to find a match (lexical gap), the neural network steps in. This leverages the precision of dictionary lookups for known terms and the generalization capability of neural embeddings for unknown terms.

### Mechanism 3
Framing semantic tagging as a Bi-Encoder matching task between context and sense definitions (glosses) enables efficient inference. Instead of treating the 232 USAS tags as simple classification labels, the model encodes the input text and the text description (gloss) of every possible tag separately. The prediction is the tag whose gloss embedding has the highest dot-product similarity with the text embedding. This decouples the inference complexity from the sequence length.

## Foundational Learning

- **Concept: Silver Standard Data**
  - Why needed here: The entire training pipeline relies on "silver" data—labels generated automatically by the older rule-based system—because manually annotated semantic data is too scarce.
  - Quick check question: What is the primary risk of training a student model on labels generated by a teacher model? (Answer: Amplification of the teacher's systematic errors/biases).

- **Concept: Word Sense Disambiguation (WSD)**
  - Why needed here: This is the fundamental task the architecture is solving. Unlike simple POS tagging, WSD requires understanding context to differentiate meanings (e.g., "bank" as a river vs. money).
  - Quick check question: Why does the USAS tagset (232 categories) make WSD easier computationally but harder semantically than WordNet (117k senses)?

- **Concept: Cross-lingual Transfer**
  - Why needed here: The model is trained on English but tested on Chinese, Finnish, etc. Understanding how multilingual PLMs create a shared vector space is crucial to understanding why this works.
  - Quick check question: Why might a multilingual model fail to transfer to a low-resource language like Irish despite being "multilingual"?

## Architecture Onboarding

- **Component map:**
  - Raw text -> Pre-processors (Lemmatizers, POS taggers)
  - Path A (Rule-based): PyMUSAS framework + Lexicons (Single Word & MWE)
  - Path B (Neural): Bi-Encoder (Shared PLM) -> Context Encoder + Gloss Encoder
  - Aggregator: Hybrid Logic (Rule takes precedence; Neural acts as fallback)

- **Critical path:**
  1. Silver Data Creation: Run the old C-version tagger on Wikipedia to create `train.data`
  2. Model Fine-tuning: Train the Bi-Encoder using `train.data` with negative sampling
  3. Inference: Input text is processed by PyMUSAS; if no tag is found, the Neural model calculates similarity against pre-computed gloss embeddings

- **Design tradeoffs:**
  - Accuracy vs. Speed: The authors explicitly chose the Bi-Encoder over more accurate Cross-Encoders to ensure the system is computationally viable for large texts
  - Simplicity vs. Nuance: The neural model was trained to predict single tags only, sacrificing the ability to predict "Multi Tag Membership" (dual labels) to simplify the training process

- **Failure signatures:**
  - Language Drop-off: Performance degrades sharply for Welsh/Irish compared to Chinese/English, likely due to the PLM's pre-training data imbalance
  - Lexicon Dependency: The Hybrid model performed worse than Neural-only for Chinese because the Chinese rule-based lexicon was of lower quality, introducing noise
  - Multi-tag Loss: The model will always fail to predict dual tags (e.g., "F2/O2") because it was not trained to output them

- **First 3 experiments:**
  1. Baseline Establishment: Run the pure Rule-based PyMUSAS tagger on the evaluation datasets to establish the floor for coverage and accuracy
  2. Silver Data Validation: Train a small English-only neural model (NEngS) on the silver data and check if it outperforms the Rule-based system on English text to validate the data quality
  3. Hybrid Comparison: Implement the fallback logic and compare the Hybrid model (HMulS) against the Neural-only model on a specific language (e.g., Chinese) to observe the impact of lexicon quality on the hybrid approach

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does the accuracy of the rule-based tagger used to generate silver standard training data impact the final performance of the neural network models? The authors successfully used a high-quality English rule-based tagger to create silver data, but they did not test how "noisy" silver data from less accurate taggers would degrade neural model performance.

### Open Question 2
Does fine-tuning language-specific Pre-trained Language Models (PLMs) yield significantly better performance for low-resource languages compared to the larger multilingual models? The multilingual models performed poorly on low-resource languages (Irish, Welsh) compared to English/Chinese, suggesting that pre-training data volume is a bottleneck, but the specific benefit of language-specific PLMs was not tested for these languages.

### Open Question 3
How can the neural architecture be adapted to effectively predict Multi-Tag Membership (dual or triple USAS tags), which the current models fail to capture? The paper simplifies the task to single-label classification to reduce complexity, but real-world semantic ambiguity often requires assigning multiple tags (e.g., "F2/O2"), a feature currently exclusive to the rule-based system.

## Limitations

- **Language Coverage and Transfer:** The approach shows promise for languages with substantial multilingual PLM pre-training data but performance drops for lower-resource languages like Irish and Welsh.
- **Lexicon Dependency and Noise:** The hybrid model's performance is heavily dependent on the quality of the rule-based lexicon; if the lexicon contains systematic errors or is outdated, the neural back-off cannot fully compensate.
- **Silver Data Bias:** Training on silver data generated by the older rule-based system means the neural model inherits and may amplify the systematic biases or errors present in that system.

## Confidence

- **High Confidence:** The core finding that neural models fine-tuned on silver data outperform rule-based systems on the same languages is well-supported by the evaluation results.
- **Medium Confidence:** The cross-lingual transfer capability is demonstrated but is uneven across languages, with significant performance drops for low-resource languages.
- **Low Confidence:** The long-term stability and adaptability of the system to new domains or evolving language use is not addressed.

## Next Checks

1. **Error Analysis on Silver Data:** Conduct a detailed error analysis on a sample of the silver training data to quantify the types and frequency of errors introduced by the rule-based tagger.

2. **Cross-Encoder Ablation Study:** Implement a Cross-Encoder variant and compare its accuracy against the Bi-Encoder on a subset of the evaluation data to test the tradeoff between accuracy and efficiency.

3. **Lexicon Quality Impact:** For a language like Chinese, where the hybrid model underperformed the neural-only model, perform an ablation study removing the rule-based component entirely to isolate the impact of lexicon quality.