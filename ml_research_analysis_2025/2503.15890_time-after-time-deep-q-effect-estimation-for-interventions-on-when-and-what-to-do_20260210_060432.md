---
ver: rpa2
title: 'Time After Time: Deep-Q Effect Estimation for Interventions on When and What
  to do'
arxiv_id: '2503.15890'
source_url: https://arxiv.org/abs/2503.15890
tags:
- aobs
- time
- treatment
- where
- times
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of estimating the causal effect
  of both when and what actions to take in sequential decision-making problems, such
  as healthcare and finance, where observations and actions occur at irregular times.
  The key challenge is to develop an off-policy evaluation method that can handle
  this irregular timing and dynamic policies, which existing methods struggle with.
---

# Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do

## Quick Facts
- **arXiv ID:** 2503.15890
- **Source URL:** https://arxiv.org/abs/2503.15890
- **Reference count:** 40
- **One-line primary result:** EDQ is a model-free deep Q-learning algorithm that estimates the effect of interventions on treatment timing in continuous time.

## Executive Summary
This paper tackles the problem of estimating the causal effect of both when and what actions to take in sequential decision-making problems with irregular timing, such as healthcare and finance. The key challenge is to develop an off-policy evaluation method that can handle this irregular timing and dynamic policies, which existing methods struggle with. The proposed solution, Earliest Disagreement Q-Evaluation (EDQ), is a model-free deep Q-learning algorithm that estimates the effect of interventions on treatment timing by leveraging the concept of "earliest disagreement times" between observed and counterfactual trajectories to enable efficient dynamic programming updates using standard sequence models like transformers.

## Method Summary
EDQ is a model-free deep Q-learning algorithm for off-policy evaluation in continuous-time decision point processes. It estimates the effect of a target policy by sampling counterfactual trajectories and identifying the earliest time where the observed and target actions disagree. The Q-function is updated using standard temporal difference learning, but with variable time steps determined by this disagreement. The method uses a Transformer architecture with continuous-time positional embeddings to handle irregularly sampled events, and relies on standard causal assumptions of ignorability and overlap for valid inference.

## Key Results
- EDQ achieves more accurate estimates of treatment timing effects compared to baselines that discretize time or only predict observed outcomes
- In time-to-failure prediction tasks, EDQ reduces normalized RMSE by up to 50% compared to discretization-based methods when estimating effects under different treatment intensities
- The method demonstrates robust performance across two simulation tasks: time-to-failure prediction and tumor growth modeling

## Why This Works (Mechanism)

### Mechanism 1: Earliest Disagreement Time for Model-Free Updates
EDQ enables model-free Q-learning in continuous time by identifying the earliest disagreement time where observed and counterfactual trajectories diverge. This allows "jumping" over time intervals where policies agree, making dynamic programming updates efficient without discretization errors or modeling state transitions.

### Mechanism 2: Local Independence for Continuous-Time Identifiability
Valid causal inference requires satisfying "local independence" graphical criteria that generalize the backdoor criterion to stochastic processes. This ensures unobserved confounders don't open backdoor paths from treatment timing to outcomes, allowing valid substitution of the treatment intensity mechanism.

### Mechanism 3: Continuous-Time Sequence Modeling with Transformers
Transformers effectively approximate Q-functions for irregular time-series data by explicitly encoding time using continuous-time positional embeddings (Fourier features). This allows the attention mechanism to weight historical events based on both semantic content and precise timing.

## Foundational Learning

- **Concept: Marked Point Processes** - Fundamental data structure where events occur at irregular, stochastic times with associated data (marks). *Why needed:* Unlike discrete-time RL, this represents the actual temporal structure of healthcare and finance data. *Quick check:* Can you explain how a "counting process" intensity λ(t) differs from a discrete probability distribution?

- **Concept: Off-Policy Evaluation (OPE)** - Evaluating a target policy using historical data from a different behavior policy. *Why needed:* Simple supervised learning fails when the target policy treats patients more aggressively than the observed data. *Quick check:* Why does minimizing prediction error on observed data fail to estimate the effect of a new policy that treats patients more aggressively?

- **Concept: Fitted Q-Evaluation (FQE)** - Iterative regression of state value to next state value. *Why needed:* EDQ modifies FQE by regressing to state at disagreement time rather than next state. *Quick check:* In discrete FQE, we backup value from t to t+1. In EDQ, why is backing up from t to t+δ necessary for continuous time?

## Architecture Onboarding

- **Component map:** Tokenizer for irregular events → GPT-2 Transformer Encoder → Continuous-time positional embeddings → Scalar Q-function output
- **Critical path:** 1) Sample trajectory and time from buffer, 2) Sample counterfactual future from target policy, 3) Identify earliest disagreement time δ, 4) Construct target label with rewards + Q-value at t+δ, 5) Update network via gradient descent
- **Design tradeoffs:** EDQ avoids discretization errors but requires sampling from target policy; model-free approach reduces bias from misspecified models but relies heavily on overlap assumption
- **Failure signatures:** High variance when overlap is low, overestimation issues, time-agnosia if positional embeddings fail
- **First 3 experiments:** 1) Time-to-failure simulation to verify accurate failure time estimation, 2) Discretization ablation comparing EDQ to discrete FQE, 3) Short trajectory stress test for sparse event sequences

## Open Questions the Paper Calls Out
- How can EDQ be extended to handle censored outcomes in survival analysis?
- Can the EDQ framework be adapted for policy optimization rather than just evaluation?
- What are the theoretical error bounds for EDQ estimates when unobserved confounding is present?

## Limitations
- The method does not handle censoring, which is required for reliable application in survival analysis
- Performance depends critically on the overlap assumption, which is often violated in real-world healthcare data
- Lack of explicit specification for key hyperparameters and simulation constants creates barriers to faithful reproduction

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical framework and core mechanism | High |
| Empirical validation on simulation tasks | Medium |
| Robustness to hyperparameter choices | Low |

## Next Checks

1. **Overlap Sensitivity Analysis:** Systematically vary the overlap between observed and target policies in the time-to-failure simulation. Quantify EDQ's performance degradation as overlap decreases, and compare to baseline methods under the same conditions.

2. **Hyperparameter Ablation Study:** Conduct thorough ablation on Transformer architecture (layers, hidden size, attention heads) and Q-learning hyperparameters (learning rate, target network update rate, discount factor). Identify most critical parameters and report optimal ranges.

3. **Real-World Data Experiment:** Apply EDQ to a real-world healthcare dataset with irregularly sampled treatments and outcomes (e.g., electronic health records). Compare estimated treatment effects under different timing strategies to established methods and clinical guidelines.