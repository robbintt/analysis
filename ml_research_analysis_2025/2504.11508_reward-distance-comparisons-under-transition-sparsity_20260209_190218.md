---
ver: rpa2
title: Reward Distance Comparisons Under Transition Sparsity
arxiv_id: '2504.11508'
source_url: https://arxiv.org/abs/2504.11508
tags:
- reward
- transitions
- srrd
- shaping
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reward function comparison under transition
  sparsity, where only a small subset of transitions are sampled due to feasibility
  constraints or limited data collection. Existing direct reward comparison methods
  like EPIC and DARD assume high transition coverage and struggle when many transitions
  are unsampled.
---

# Reward Distance Comparisons Under Transition Sparsity

## Quick Facts
- **arXiv ID:** 2504.11508
- **Source URL:** https://arxiv.org/abs/2504.11508
- **Reference count:** 40
- **Primary result:** Introduces SRRD pseudometric that outperforms EPIC and DARD under transition sparsity by integrating additional reward expectation terms for robust reward function comparison

## Executive Summary
This paper addresses the challenge of comparing reward functions when only a small subset of state transitions are sampled, a common scenario in practical reinforcement learning applications. Existing direct reward comparison methods like EPIC and DARD assume high transition coverage and struggle when many transitions are unsampled. The authors introduce SRRD (Sparsity Resilient Reward Distance), a new pseudometric that improves robustness by structuring its canonicalization around forward transitions and incorporating additional reward expectation terms. Theoretical analysis shows SRRD has lower upper bounds on relative shaping error compared to existing methods, and empirical evaluation across multiple domains demonstrates superior accuracy in classifying agent behaviors based on reward functions.

## Method Summary
SRRD improves upon existing reward comparison methods by using a double-batch sampling approach that separates transition sampling from state-action sampling, reducing bias from the sampling policy. The canonicalization function strategically selects state subsets such that key expectation terms use forward transitions, which are more likely to be sampled under sparsity. SRRD employs eight reward expectation terms (compared to four in EPIC and DARD) to ensure cancellation of residual shaping errors induced by potential shaping, even when coverage is incomplete. The Pearson distance is then applied to these canonicalized rewards to provide a pseudometric that correlates with policy difference, supported by a regret bound showing that as SRRD distance approaches zero, the regret between policies optimized for the compared reward functions also approaches zero.

## Key Results
- SRRD achieves lower upper bounds on relative shaping error (M/3Z) compared to DARD (2M/3Z) and EPIC (M/Z)
- Empirical evaluation across Gridworld, Bouncing Balls, Drone Combat, StarCraft II, Robomimic, Montezuma's Revenge, and MIMIC-IV shows SRRD outperforms EPIC and DARD in classification accuracy
- SRRD better identifies similarities between equivalent reward samples under high transition sparsity in nearly all tested scenarios
- Theoretical regret bound demonstrates that DSRRD→0 implies policy regret approaching zero

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SRRD achieves lower sensitivity to unsampled transitions by structuring canonicalization around forward transitions
- **Mechanism:** The canonicalization function strategically selects state subsets {S1,...,S6} such that key expectation terms use forward transitions (where subsequent states are distributed conditionally on prior state-action pairs), reducing reliance on transitions likely to be unsampled
- **Core assumption:** Forward transitions are more likely to be sampled and in-distribution than non-forward transitions under transition sparsity
- **Break condition:** If the underlying transition dynamics are highly non-stationary or if sampled transitions severely misrepresent true forward transition structure

### Mechanism 2
- **Claim:** SRRD's additional reward expectation terms enable cancellation of residual shaping errors even under incomplete coverage
- **Mechanism:** By introducing terms with strategically chosen state subsets (k1=k4=S5, k2=k3=S6), the residual shaping term simplifies to ensure expectations cancel when subset distributions match appropriately
- **Core assumption:** The distributions of state subsets S5, S6, S3, S4 are sufficiently aligned even under transition sparsity
- **Break condition:** If subset distributions diverge significantly due to extreme sparsity or biased sampling

### Mechanism 3
- **Claim:** Pearson distance on SRRD-canonicalized rewards provides pseudometric correlating with policy difference, supported by regret bound
- **Mechanism:** After canonicalization removes potential shaping, Pearson distance measures correlation between reward functions, with theoretical analysis showing DSRRD→0 implies policy regret approaching zero
- **Core assumption:** Coverage distribution and policy-induced distributions are related such that KDπ ≥ D for some constant K
- **Break condition:** The regret bound assumes finite MDPs and stationary dynamics; may not hold under distribution shift or in continuous spaces

## Foundational Learning

- **Concept: Potential-based reward shaping**
  - **Why needed here:** SRRD is designed to be invariant to potential-based shaping, which preserves optimal policies. Understanding how shaping adds terms of the form γϕ(s')−ϕ(s) is essential to grasp why canonicalization must cancel these.
  - **Quick check question:** Given a shaped reward R'(s,a,s') = R(s,a,s') + γϕ(s')−ϕ(s), explain why the optimal policy under R' equals that under R.

- **Concept: Transition sparsity vs. coverage distribution**
  - **Why needed here:** The core problem SRRD addresses is that existing methods assume |TS|≈|TD|, but in practice |TU|→|TD|. Understanding how feasibility constraints and limited sampling create unsampled transitions is critical.
  - **Quick check question:** In a 10×10 Gridworld with 4 actions, explain why the number of theoretically possible transitions (|S×A×S|) differs from the number of feasible transitions under single-step movement constraints.

- **Concept: Pseudometrics and policy invariance**
  - **Why needed here:** A pseudometric allows d(x,y)=0 without requiring x=y, which is exactly what's needed for equivalent reward functions with different numerical values. The Pearson distance's scale/shift invariance is also leveraged.
  - **Quick check question:** Why is a pseudometric more appropriate than a true metric for comparing potentially shaped reward functions?

## Architecture Onboarding

- **Component map:** Sampling module (BV transitions, BM state-action pairs) -> State subset extraction (X1-X6 derivation) -> Canonicalization (ˆCSRRD via eight expectation terms) -> Distance computation (Pearson correlation on canonicalized rewards)
- **Critical path:** The correctness of state subset extraction (ensuring forward transition relationships hold) directly impacts canonicalization quality. Properties "(S1,A,S5)⊆(S4,A,S5)" and "(S2,A,S6)⊆(S3,A,S6)" must be maintained.
- **Design tradeoffs:** 
  - Double-batch sampling reduces bias but increases complexity to O(NV·NM²)
  - Eight-term canonicalization provides better shaping cancellation but requires more transition queries
  - Terminal state handling in S2 adds implementation complexity but is crucial for theoretical robustness
- **Failure signatures:**
  - High variance in canonicalized rewards suggests insufficient coverage or sampling policy bias
  - SRRD distance not approaching 0 for known equivalent rewards indicates unsampled forward transitions
  - Degraded performance relative to EPIC/DARD suggests non-potential shaping or extreme distribution mismatch
- **First 3 experiments:**
  1. **Coverage sweep validation:** Implement SRRD on 20×20 Gridworld with polynomial rewards, varying rollout counts from 1-2000. Verify DSRRD approaches 0 faster than EPIC/DARD as coverage increases.
  2. **Feasibility constraint test:** Set stochasticity parameter ε=0 in Gridworld to enforce single-step movement. Compare pseudometric distances under this hard constraint to validate behavior.
  3. **Agent classification sanity check:** Generate reward functions via AIRL for multiple distinct policies in a simple domain. Train k-NN classifier using SRRD distances and verify higher classification accuracy than baseline methods.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can direct reward comparison pseudometrics be adapted to handle non-potential shaping or random perturbations in reward functions?
- **Basis in paper:** [explicit] The authors state: "Future research should consider the effects of non-potential shaping on SRRD (see Appendix B.1) or random perturbations, as these might distort reward functions that would otherwise be similar."
- **Why unresolved:** SRRD is designed specifically for potential-based shaping, which guarantees policy invariance. Non-potential transformations can alter optimal policies, and current pseudometrics have no mechanism to handle them.
- **What evidence would resolve it:** An extension of SRRD that can robustly compare rewards under non-potential shaping, demonstrated through experiments showing maintained classification accuracy when non-potential noise is introduced.

### Open Question 2
- **Question:** Can integrating SRRD into iterative IRL algorithms improve convergence speed by replacing policy learning with direct reward comparisons?
- **Basis in paper:** [explicit] The authors propose: "Integrating direct reward comparison pseudometrics to determine if rewards are converging, could help to skip the policy learning steps, thereby speeding up IRL."
- **Why unresolved:** Iterative IRL methods currently rely on policy evaluation at each iteration to assess reward quality, which is computationally expensive. Direct comparison could bypass this, but no implementation or empirical validation exists.
- **What evidence would resolve it:** Empirical results showing reduced training time for IRL algorithms when SRRD-based convergence checks replace policy learning steps, without loss of recovered reward quality.

### Open Question 3
- **Question:** How can reward comparison pseudometrics incorporate multicriteria policy invariance for applications like LLM reward model evaluation?
- **Basis in paper:** [explicit] The authors suggest: "In the context of reward functions in Large Language Models (LLMs), it might be important to compute reward distance pseudometrics that consider different criteria such as bias, safety, or reasoning."
- **Why unresolved:** Current pseudometrics treat policy invariance as binary—rewards either preserve optimal policies or they don't. No framework exists for partial or criteria-conditioned invariance.
- **What evidence would resolve it:** A formal definition of multicriteria pseudometrics and experiments on LLM reward models showing meaningful differentiation across criteria like safety and reasoning capabilities.

### Open Question 4
- **Question:** How can function approximation enable reward canonicalization for continuous state-action spaces?
- **Basis in paper:** [explicit] In Appendix B.5: "An intriguing area for future research is the integration of function approximation to generalize reward canonicalization to reward functions represented as neural networks."
- **Why unresolved:** Sample-based SRRD requires discretization of continuous signals, which may lose information. Neural network canonicalization would require learning to predict canonical rewards from transition batches, but training objectives and architectures remain unspecified.
- **What evidence would resolve it:** A neural network architecture trained to approximate CSRRD, validated on continuous domains with performance comparable to discretized sample-based methods.

## Limitations
- SRRD's robustness depends on the assumption that forward transitions are more likely to be sampled under sparsity, which may not hold in all domains
- The computational overhead of eight-term canonicalization versus four-term alternatives has not been rigorously characterized across diverse domain sizes
- Performance degradation when unsampled transitions are systematically forward remains unclear

## Confidence
- **High Confidence:** Mathematical derivation of SRRD's canonicalization formula and potential shaping cancellation properties
- **Medium Confidence:** Empirical results showing SRRD outperforming EPIC and DARD under transition sparsity
- **Low Confidence:** Regret bound's practical applicability in non-stationary or continuous MDPs

## Next Checks
1. **Coverage sensitivity test:** Systematically vary transition sampling rates from 5% to 100% in Gridworld and measure SRRD's accuracy degradation curve compared to baselines
2. **Forward transition stress test:** Design a domain where unsampled transitions are predominantly forward (e.g., through selective action masking) and evaluate SRRD's performance under these conditions
3. **Scaling benchmark:** Measure canonicalization runtime and memory usage for SRRD versus EPIC/DARD across increasing state-action space sizes to quantify computational tradeoffs