---
ver: rpa2
title: 'Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP
  Systems'
arxiv_id: '2506.20209'
source_url: https://arxiv.org/abs/2506.20209
tags:
- arxiv
- human
- label
- multi-perspective
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of handling human disagreement
  in NLP, which traditional methods often overlook by aggregating annotators' viewpoints
  into a single ground truth. This can underrepresent minority perspectives, especially
  in subjective tasks where annotators systematically disagree due to personal preferences.
---

# Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems

## Quick Facts
- **arXiv ID:** 2506.20209
- **Source URL:** https://arxiv.org/abs/2506.20209
- **Reference count:** 13
- **Primary result:** Multi-perspective soft-label approach achieves superior F1 scores and lower JSD than traditional baselines on subjective NLP tasks.

## Executive Summary
The paper addresses the challenge of handling human disagreement in NLP annotation, which traditional methods often resolve through majority voting that can underrepresent minority perspectives. The authors propose a multi-perspective approach using soft labels—probability distributions over classes rather than single hard labels—to better model diverse human opinions and develop more inclusive NLP models. Experiments on four subjective tasks (hate speech, abusive language, irony, and stance detection) demonstrate that this approach achieves higher F1 scores and better approximates human label distributions compared to traditional baselines.

## Method Summary
The approach uses soft labels derived from annotator vote distributions (via softmax) instead of aggregating annotations into single ground truth labels. BERT-Large and RoBERTa-Large models are fine-tuned using a soft loss function that minimizes divergence between predicted and human soft label distributions. The method is evaluated on four datasets: GabHate, ConvAbuse, EPIC, and StanceDetection, with metrics including macro F1-score, Jensen-Shannon Divergence (JSD), and confidence scores.

## Key Results
- Multi-perspective approach achieves higher F1 scores than majority vote baselines on GabHate and EPIC datasets
- Lower JSD scores demonstrate better approximation of human label distributions across all datasets
- Model exhibits lower confidence on subjective tasks like irony and stance detection, reflecting task difficulty
- XAI techniques reveal variability in model explanations, underscoring complexity of subjective interpretation

## Why This Works (Mechanism)

### Mechanism 1
Soft labels preserve minority annotator perspectives that majority voting eliminates. Instead of collapsing annotations into a single hard label (0 or 1), each instance is represented as a probability distribution over classes derived from all annotator judgments. The model learns from this distribution via soft loss rather than cross-entropy, preventing gradient signals from being dominated by the majority class. Core assumption: Annotator disagreement reflects legitimate, systematic variation in interpretation rather than random noise. Break condition: When annotators have high agreement, the approach loses advantage.

### Mechanism 2
Soft loss enables better alignment with human label distributions as measured by Jensen-Shannon Divergence. The soft loss function directly penalizes deviation from the full human distribution rather than from a single aggregated label. This creates gradients that pull predictions toward the shape of annotator disagreement rather than toward a single point estimate. Core assumption: The human label distribution faithfully represents the true distribution of valid interpretations. Break condition: If the annotator pool is not representative of the target population, the "human distribution" itself may be biased.

### Mechanism 3
Lower model confidence in subjective tasks reflects appropriate epistemic uncertainty rather than model failure. By learning from soft labels that encode disagreement, the model internalizes that multiple valid interpretations exist. At inference, this manifests as lower peak probabilities because the learned distribution is inherently flatter. Core assumption: Task subjectivity is the primary driver of confidence reduction. Break condition: Confidence may also drop due to other factors like dataset size or class imbalance.

## Foundational Learning

- **Concept: Soft Labels vs. Hard Labels**
  - Why needed here: The entire method hinges on replacing one-hot encoded "ground truth" with probability distributions.
  - Quick check question: Given 5 annotators voting [1,1,0,0,1] on a binary task, what is the soft label representation? (Answer: [0.4, 0.6] for classes [0, 1])

- **Concept: Jensen-Shannon Divergence (JSD)**
  - Why needed here: This is the paper's primary "soft metric" for evaluating how well models approximate human disagreement.
  - Quick check question: If a model predicts [0.5, 0.5] and humans annotated [0.4, 0.6], is JSD lower or higher than if the model predicted [0.9, 0.1]? (Answer: Lower; JSD penalizes deviation from the human distribution)

- **Concept: Perspectivism in NLP**
  - Why needed here: The paper positions itself within a broader paradigm shift from "gold standard" single-truth annotation to preserving annotator-level disagreement as signal.
  - Quick check question: In traditional NLP pipelines, how is annotator disagreement typically handled? (Answer: Via aggregation methods like majority voting, treating disagreement as noise to be resolved)

## Architecture Onboarding

- **Component map:**
  Raw annotations (disaggregated) -> Soft label generation (softmax over annotator votes) -> BERT/RoBERTa encoder (frozen or fine-tuned) -> Classification head (fully connected + softmax) -> Soft loss computation (against human distribution) -> Inference: Predicted distribution evaluated via JSD, F1, confidence

- **Critical path:**
  1. **Data preparation**: Must preserve disaggregated annotations. If your pipeline already collapses to majority vote, you cannot apply this method without re-annotating or accessing raw annotation data.
  2. **Loss function swap**: Replace `CrossEntropyLoss` with the soft loss formula from Section 3.2. Standard frameworks don't have this built-in—you'll implement it manually.
  3. **Evaluation dual-track**: You need both hard metrics (F1 on majority class) and soft metrics (JSD against human distribution). The paper argues for JSD as the primary signal of pluralistic alignment.

- **Design tradeoffs:**
  - **Higher F1 + lower confidence vs. lower F1 + higher confidence**: Multi-perspective tends to improve F1 on hate/abuse tasks but shows lower confidence on irony/stance. Decide which signal matters for your use case.
  - **Inclusivity vs. decision clarity**: Soft predictions are harder to threshold into discrete actions. If your system requires binary decisions (e.g., content moderation), you'll need a secondary policy for how to act on uncertain predictions.
  - **Annotator pool size**: The paper uses 3-18 annotators per dataset. Fewer annotators produce coarser soft labels, reducing the method's advantage.

- **Failure signatures:**
  - **ConvAbuse pattern**: If baseline outperforms multi-perspective on F1, check disagreement rate. Below ~50% disagreement, the benefit may invert.
  - **XAI inconsistency**: If LIG, SHAP, and LIME disagree substantially on feature importance for the same prediction, this may indicate the model itself hasn't converged on a stable representation (expected for subjective tasks, but problematic for interpretability requirements).
  - **JSD not decreasing**: If soft loss is implemented but JSD doesn't improve, verify that ground-truth soft labels are correctly computed from disaggregated data—not from post-hoc softmax over predictions.

- **First 3 experiments:**
  1. **Reproduce soft loss on a single dataset**: Start with EPIC (irony) since it has 5 annotators and high disagreement (66%). Implement soft loss, compare JSD and F1 against majority vote baseline. Confirm you see the paper's pattern (lower JSD, comparable F1, lower confidence).
  2. **Ablate on synthetic disagreement**: Create a controlled dataset where you inject known disagreement patterns (e.g., 70% label A, 30% label B). Verify that multi-perspective predictions converge toward [0.7, 0.3] distribution.
  3. **Threshold sensitivity analysis**: For a downstream task (e.g., content flagging), plot precision-recall curves at different confidence thresholds. Determine whether lower-confidence multi-perspective predictions can still support actionable decisions without excessive false positives.

## Open Questions the Paper Calls Out

### Open Question 1
Can the multi-perspective soft-label approach be effectively adapted for decoder-only Large Language Models (LLMs) using prompt-based classification in subjective tasks? The authors state they plan to extend their approach to decoder-only architectures prompted for classification. This remains untested since the current study exclusively uses encoder-only models.

### Open Question 2
What is the precise relationship between the lower prediction confidence observed in multi-perspective models and the objective level of human disagreement in the ground truth? While lower confidence is observed, it's unclear if this uncertainty is a direct function of annotation entropy or an artifact of the model architecture.

### Open Question 3
Does the variability in feature importance found across different Explainable AI (XAI) methods (LIG, SHAP, LIME) correlate with the degree of human subjectivity in the text? The authors observe inconsistencies in post-hoc attribution methods and suggest investigating the correlation between attribution variability and human subjectivity.

### Open Question 4
How does the inclusion of high-disagreement instances impact the convergence and performance of multi-perspective models? The current methodology filters out highly controversial data points, potentially removing the very data where a multi-perspective approach is most valuable.

## Limitations

- The method's benefits are most pronounced on datasets with high annotator disagreement, but underperform on tasks with lower disagreement rates
- Jensen-Shannon Divergence is used as a proxy for pluralistic alignment without validation that lower JSD correlates with better real-world outcomes
- XAI interpretation variability is observed but not systematically analyzed to determine whether it reflects healthy uncertainty or model instability

## Confidence

- **High confidence**: Technical implementation of soft labels and soft loss functions is sound and reproducible
- **Medium confidence**: Claim about lower confidence reflecting appropriate epistemic uncertainty is plausible but not definitively proven
- **Low confidence**: Broader claim about developing "more inclusive NLP systems" is aspirational rather than empirically validated

## Next Checks

1. **Disagreement rate threshold analysis**: Systematically test the method across datasets with varying disagreement rates (e.g., 30%, 50%, 70%) to identify the minimum disagreement threshold where multi-perspective approaches provide net benefits over traditional aggregation methods.

2. **Downstream task impact study**: Implement a content moderation pipeline that uses the multi-perspective predictions and measure actual false positive/negative rates on minority perspectives, comparing against majority-vote baselines to assess real-world inclusivity improvements.

3. **Annotator representativeness audit**: Analyze whether the annotator pools used to generate soft labels are representative of the target population demographics, and test whether the method amplifies or mitigates existing annotation biases through controlled bias injection experiments.