---
ver: rpa2
title: 'DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image Diffusion
  Models'
arxiv_id: '2502.14070'
source_url: https://arxiv.org/abs/2502.14070
tags:
- reward
- prompt
- prompts
- fine-tuning
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffExp improves exploration in reward fine-tuning for text-to-image
  diffusion models by dynamically adjusting classifier-free guidance (CFG) scale and
  randomly weighting prompt phrases. This approach enhances sample diversity while
  maintaining quality, leading to more efficient reward optimization.
---

# DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image Diffusion Models

## Quick Facts
- **arXiv ID:** 2502.14070
- **Source URL:** https://arxiv.org/abs/2502.14070
- **Reference count:** 8
- **Primary result:** Improves sample efficiency by ~20% in reward fine-tuning for text-to-image diffusion models through dynamic CFG scheduling and random prompt weighting

## Executive Summary
DiffExp introduces an exploration strategy for reward fine-tuning of text-to-image diffusion models that addresses the challenge of local optima and reward hacking. The method dynamically adjusts classifier-free guidance scale during denoising and randomly weights prompt phrases to generate more diverse samples that better explore the reward landscape. When integrated with DDPO and AlignProp fine-tuning methods, DiffExp achieves higher reward scores while requiring fewer samples to reach target performance. The approach demonstrates generalization to unseen prompts and complex datasets like DrawBench, and works effectively on advanced models including SDXL.

## Method Summary
DiffExp enhances exploration during reward fine-tuning by implementing two complementary strategies. First, it dynamically schedules the classifier-free guidance (CFG) scale during denoising, using very low guidance in early steps (t > t_thres) to promote diversity and high guidance in later steps for quality refinement. Second, it randomly weights individual prompt tokens by perturbing their embeddings toward empty text embeddings, causing the model to emphasize different semantic elements across samples. These exploration mechanisms are applied only during the first 3/4 of training, after which the model transitions to pure exploitation. The method integrates with existing fine-tuning frameworks like DDPO (policy gradient) and AlignProp (direct reward backpropagation) while maintaining compatibility with standard diffusion model architectures and training procedures.

## Key Results
- Achieves ~20% improvement in sample efficiency (fewer samples to reach same reward score)
- Improves reward scores across both Aesthetic and PickScore metrics when combined with DDPO and AlignProp
- Demonstrates generalization to unseen prompts and complex datasets like DrawBench
- Successfully transfers to advanced models like SDXL with consistent performance gains
- Generated samples show higher aesthetic quality and better image-text alignment compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamically scheduling the classifier-free guidance (CFG) scale during denoising improves sample diversity while maintaining quality, leading to better exploration of reward signals.
- **Mechanism:** During early denoising steps (high noise, t > t_thres), a low CFG scale (w_l) promotes diverse latent trajectories. After threshold t_thres, a high CFG scale (w_h) refines samples toward high fidelity. This creates varied outputs that explore more of the prompt's semantic space before committing to a specific visual interpretation.
- **Core assumption:** The early denoising stages primarily determine sample diversity, while later stages primarily determine sample quality.
- **Evidence anchors:** [abstract] "dynamically adjusting the scale of classifier-free guidance to enhance sample diversity"; [section: Method - Dynamic Scheduling of CFG Scale] Equation 7 defines the step-dependent CFG schedule; Figure 3 shows visual comparison of constant high vs. constant low vs. dynamic CFG; [corpus] Navigating the Exploration-Exploitation Tradeoff (arxiv 2508.12361) addresses exploration-exploitation in diffusion inference.

### Mechanism 2
- **Claim:** Randomly weighting prompt phrases during training increases the probability of generating samples containing under-represented prompt elements, enabling discovery of higher reward signals.
- **Mechanism:** For a prompt with N tokens, a random token i is selected and its embedding weighted: c[i] ← -c_null + w_prompt × (c[i] - c_null), where w_prompt ~ U(1, 1.2). This perturbs the conditioning to emphasize different prompt aspects across samples, preventing the model from collapsing to dominant interpretations.
- **Core assumption:** The base model's conditional distribution has imbalanced sensitivity to different prompt tokens, causing some semantic elements to be consistently under-generated.
- **Evidence anchors:** [abstract] "randomly weighting phrases of the text prompt to exploit high-quality reward signals"; [section: Method - Random Prompt Weighting] Equation 8 formalizes the weighting; Figure 2a illustrates the process.

### Mechanism 3
- **Claim:** Combining both exploration strategies creates complementary diversity that prevents reward optimization from converging to local optima or trivial solutions.
- **Mechanism:** Dynamic CFG explores the latent space breadth (spatial/structural diversity), while random prompt weighting explores the semantic conditioning space (compositional diversity). Together they cover more of the joint prompt-latent space during online sample generation.
- **Core assumption:** The two exploration dimensions are at least partially orthogonal; combining them provides multiplicative rather than merely additive coverage.
- **Evidence anchors:** [section: Experiments - Ablation studies] Figure 9a shows each module alone improves over baseline, but combined achieves best performance; [section: Experimental Results] "~20% fewer samples to achieve the same reward score"; [corpus] Adaptive Divergence Regularized Policy Optimization (arxiv 2510.18053) frames exploration-exploitation as a critical challenge in generative fine-tuning.

## Foundational Learning

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed here:** DiffExp's primary mechanism manipulates CFG scheduling; understanding the quality-diversity tradeoff controlled by the guidance scale is essential.
  - **Quick check question:** Given the CFG formula ε̃ = ε_uncond + w × (ε_cond - ε_uncond), what happens to sample diversity as w → ∞?

- **Concept: Reinforcement Learning for Diffusion Models**
  - **Why needed here:** DiffExp is explicitly an exploration strategy for reward fine-tuning, building on methods like DDPO that frame denoising as a multi-step MDP.
  - **Quick check question:** In the policy gradient formulation (Equation 6), what is the "action" and what is the "reward"?

- **Concept: Reward Models for Text-to-Image Alignment**
  - **Why needed here:** The paper uses Aesthetic Score and PickScore as optimization targets; understanding what these rewards measure determines whether improvements are meaningful or reward hacking.
  - **Quick check question:** Why does the paper evaluate on ImageReward (an "unseen" reward) rather than only the fine-tuning reward?

## Architecture Onboarding

- **Component map:** Pre-trained diffusion model (SD 1.5 or SDXL with LoRA adapters) -> CFG scheduler module (modifies guidance scale based on denoising timestep) -> Prompt perturbation module (modifies text embeddings before encoder) -> Reward model (Aesthetic, PickScore, or ensemble) -> Fine-tuning optimizer (DDPO policy gradient OR AlignProp backpropagation)

- **Critical path:**
  1. Sample prompt from training distribution
  2. Apply random prompt weighting to text embedding
  3. Initialize denoising with noise x_T
  4. For each timestep t: query CFG scheduler for w(t), compute guided noise prediction
  5. Generate final image x_0, compute reward r(x_0, c)
  6. Update model parameters via chosen fine-tuning method
  7. Apply exploration only for first 3/4 of training steps

- **Design tradeoffs:**
  - t_thres selection: Higher values (more low-CFG steps) increase diversity but risk quality degradation. Paper found t_thres=900 (of 1000) optimal.
  - w_prompt range: U(1, 1.2) provides mild perturbation; stronger weighting may over-emphasize random words.
  - Exploration schedule: Paper applies exploration only during first 75% of training, presumably to allow exploitation-phase convergence.

- **Failure signatures:**
  - Reward curves flat or decreasing: exploration may be too aggressive (w_l too low) or perturbation too strong
  - Generated images low quality but diverse: t_thres may be too high
  - Generated images high quality but repetitive: t_thres too low or prompt weighting not applied
  - Good reward on training metric but poor on unseen rewards: potential reward hacking

- **First 3 experiments:**
  1. Reproduce the single-prompt "dolphin riding a bike" experiment (Figure 1) to validate exploration improvement; confirm baseline fails to generate bike while DiffExp succeeds.
  2. Run ablation on Aesthetic reward with SD 1.5: test (a) CFG scheduling only, (b) prompt weighting only, (c) both combined; verify Figure 9a pattern.
  3. Test generalization: fine-tune on 45 animal prompts, evaluate on held-out animals (Table 2), confirm reward gap persists on unseen prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the optimal transition threshold ($t_{thres}$) for dynamic CFG scaling dependent on the total number of denoising steps or the specific sampler architecture used?
- **Basis in paper:** [inferred] The ablation study tests specific values ($t_{thres} \in \{700, 800, 900\}$) within a fixed 1000-step framework, finding 900 optimal, but does not validate this across different step counts (e.g., 20 vs. 50 steps).
- **Why unresolved:** It is unclear if the benefit derives from the absolute timestep (capturing structural noise) or the relative proportion of the denoising process, limiting generalization to faster samplers.
- **What evidence would resolve it:** Experiments varying the total inference steps (e.g., 20, 50, 200) and samplers (DDIM, Euler) to see if the optimal $t_{thres}$ scales linearly or requires re-tuning.

### Open Question 2
- **Question:** Can the DiffExp exploration strategy be effectively transferred to conditional generation domains beyond static images, such as text-to-video or 3D generation?
- **Basis in paper:** [inferred] The method relies on manipulating CFG and prompt embeddings, mechanisms common to other diffusion domains, but the "diversity vs. fidelity" trade-off may behave differently with temporal or spatial consistency constraints.
- **Why unresolved:** The empirical validation is restricted to Stable Diffusion (v1.5 and SDXL) for image synthesis, leaving the impact on temporal coherence in video models untested.
- **What evidence would resolve it:** Applying DiffExp to a video diffusion model (e.g., Stable Video Diffusion) and measuring temporal consistency alongside standard reward optimization metrics.

### Open Question 3
- **Question:** Does random prompt weighting introduce unintended bias or instability when applied to highly complex, long-form, or negated prompts?
- **Basis in paper:** [inferred] The method involves randomly up-weighting tokens ($w_{prompt}$), which was tested primarily on "Animal + Activity" structures and DrawBench; complex syntactic structures might be disrupted by random emphasis.
- **Why unresolved:** While generalization to "unseen prompts" is tested, the specific robustness of the random weighting mechanism against semantic disruption in nuanced or negative prompting is not isolated.
- **What evidence would resolve it:** A dedicated evaluation on a dataset of long, complex, or negation-heavy prompts to assess semantic preservation accuracy compared to standard fine-tuning.

## Limitations
- The exact numeric value of the low-CFG scale (wl) is unspecified, though described as "extremely low"
- Limited statistical confidence in generalizability claims due to single results on SDXL and DrawBench
- Only tested on image generation, leaving applicability to other domains (video, 3D) unexplored
- Random prompt weighting mechanism's behavior on complex syntactic structures remains untested

## Confidence

**High Confidence (90%+):** The claim that DiffExp improves sample efficiency by approximately 20% is well-supported by direct quantitative evidence across multiple reward functions and models.

**Medium Confidence (70-80%):** The mechanism explanations are reasonable but not definitively proven. The CFG scheduling hypothesis assumes early denoising primarily controls diversity, which the ablation supports but doesn't prove causally.

**Low Confidence (40-50%):** The generalizability claims to SDXL and DrawBench are based on single results each, making statistical confidence limited.

## Next Checks

1. **Parameter sensitivity analysis:** Systematically vary wl (the low-CFG scale) across [0.1, 0.5, 1.0, 2.0] while keeping t_thres=900 fixed, measuring reward curves and sample diversity metrics to identify the optimal operating point and validate the diversity-quality tradeoff mechanism.

2. **Prompt weighting perturbation strength:** Test w_prompt ranges of [1.0, 1.1, 1.2, 1.5] to quantify the relationship between perturbation magnitude and reward improvement, determining whether the mild U(1,1.2) range is optimal or if stronger weighting provides additional benefits.

3. **Ablation on exploration schedule:** Test different exploration cutoff points (25%, 50%, 75%, 100% of training) to validate the claimed benefit of exploration-only during early training phases, measuring final reward scores and sample efficiency across the full training trajectory.