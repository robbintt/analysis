---
ver: rpa2
title: 'TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?'
arxiv_id: '2509.17054'
source_url: https://arxiv.org/abs/2509.17054
tags:
- white
- have
- lies
- reasoning
- need
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TactfulToM, a benchmark designed to evaluate
  LLMs' ability to understand white lies in conversational contexts. The dataset consists
  of 100 conversations with 6.7K questions, generated through a human-in-the-loop
  pipeline that ensures information asymmetry among characters.
---

# TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?

## Quick Facts
- arXiv ID: 2509.17054
- Source URL: https://arxiv.org/abs/2509.17054
- Reference count: 25
- Primary result: Introduces TactfulToM benchmark; current LLMs achieve ~50-60% accuracy on white lie understanding vs. human performance over 85%

## Executive Summary
This paper introduces TactfulToM, a benchmark designed to evaluate large language models' ability to understand white lies in conversational contexts. The dataset consists of 100 conversations with 6.7K questions, generated through a human-in-the-loop pipeline that ensures information asymmetry among characters. The benchmark tests both basic mental state tracking and complex white lie reasoning, including understanding motivations and detecting deception. Evaluation of nine LLMs from four families shows significant performance gaps compared to humans, with best models achieving around 50-60% accuracy versus human performance over 85%. Notably, models struggle particularly with understanding the prosocial motivations behind white lies and applying mental state representations to white lie contexts. The findings reveal that current LLMs rely heavily on pattern matching rather than genuine second-order theory of mind reasoning, highlighting the need for improved social reasoning capabilities in AI systems.

## Method Summary
TactfulToM employs a human-in-the-loop pipeline to generate 100 conversations with controlled information asymmetry. The process begins with seed generation of "white lie triplets" (Truth, Lie, Real Reason), followed by LLM expansion into multi-turn dialogues while enforcing role-based information hiding. The dataset includes 6.7K questions across three tiers: Info-State (identifying facts and beliefs), Understanding (lie comprehension), and Reasoning (white lie reasoning including motivations and detection). Human annotators validate scenario coherence and "white lie authenticity." The evaluation framework progressively increases cognitive load from identifying explicit facts to reasoning about second-order perspectives and prosocial motivations.

## Key Results
- LLMs achieve 50-60% accuracy on white lie tasks versus human performance over 85%
- Models excel at detecting lies through explicit contradictions but struggle to infer deception from motivations
- Significant performance drop occurs between Info-State tracking (high accuracy) and White Lie Reasoning application (low accuracy)
- Models perform well on "Common Sense" white lies (Class 1/2) but poorly on "Social Evasion" scenarios (Class 0/4)
- Chain-of-Thought prompting and reasoning models show inconsistent improvements, with some reasoning models underperforming vanilla models

## Why This Works (Mechanism)

### Mechanism 1: Triplet-Based Asymmetry Enforcement
If a benchmark decomposes social scenarios into distinct information roles (Liar, Target, Accomplice, Observer) and strictly controls who accesses "Truth," "Lie," and "Real Reason" triplets, it forces models to maintain separate mental state representations rather than merging beliefs. The architecture separates the factual state of the world from the mental states of characters by explicitly defining "Real Reason" (motivation) as a separate variable from "Truth," preventing models from solving tasks via simple contradiction detection. Assumption: Models default to merging their own knowledge with character beliefs unless information access is strictly partitioned. Evidence: The paper's decomposition of white lies into three elements and the non-merging mental states requirement support this mechanism.

### Mechanism 2: Hierarchical Reasoning Gap
Current LLMs appear to possess a functional mechanism for tracking explicit beliefs (Info-State) but lack a reliable mechanism for applying those beliefs to infer behavioral permissions (Lie Ability) or detection (Lie Detectability). The evaluation framework progressively increases cognitive load: first identifying facts, then understanding first-order deception, and finally reasoning about second-order perspectives. The failure occurs at the application boundary, not the tracking boundary. Assumption: High accuracy on "Fact" and "Belief" questions indicates genuine mental state tracking, not just pattern matching. Evidence: LLMs Can Track Mental States But Fail to Apply Them in White Lie Contexts, and findings reveal models rely heavily on pattern matching rather than genuine second-order theory of mind reasoning.

### Mechanism 3: Motivation vs. Surface Pattern Detection
LLMs rely on a mechanism of statistical correlation (surface patterns) to identify lies, which works for common tropes but fails when inferring internal prosocial motivations that contradict surface text. When "Truth" is explicit, models detect lies via contradiction. When "Truth" is absent or ambiguous, the model must infer the Real Reason (motivation) to identify the lie. The performance drop in Level 3 and free-form Justification questions indicates the motivation-inference mechanism is weak. Assumption: The drop in performance between Level 1 and Level 3 is caused by the removal of contradiction cues, not by increased ambiguity confusing the model. Evidence: Models excel at detecting lies through explicit contradictions but struggle to infer deception directly from motivations, and Justification accuracy in free-form responses drops significantly.

## Foundational Learning

- **Concept: Second-Order Belief Attribution**
  - Why needed here: Understanding a white lie requires modeling what the Liar believes the Target believes
  - Quick check question: If X knows Y is lying to Z, what does X believe about Z's mental state regarding the truth?

- **Concept: Prosocial Deception**
  - Why needed here: The "Real Reason" is distinct from malice. Systems must distinguish "lying to harm" from "lying to protect"
  - Quick check question: Does the "Real Reason" element benefit the Target, the Liar, or both (Pareto vs. Altruistic)?

- **Concept: Information Asymmetry**
  - Why needed here: The benchmark relies on structuring conversations where characters have strictly different access to the Triplet (Truth/Lie/Reason)
  - Quick check question: In a multi-party chat, if A leaves the room, what new information can B and C share that A can no longer access?

## Architecture Onboarding

- **Component map:** Seed Generator -> Expansion Pipeline -> Question Engine -> Validator
- **Critical path:** Designing the Triplet → Enforcing Asymmetry in Generation → Validating Authenticity → Hierarchical Questioning
- **Design tradeoffs:** Controlled vs. Natural (uses LLMs but strictly controls them step-by-step to prevent flawed understanding from corrupting scenario structure); MCQ vs. Free-form (relying solely on MCQ risks inflated scores via pattern matching; free-form evaluation is essential but harder to scale/grade automatically)
- **Failure signatures:** High Belief / Low Detectability (model tracks facts but cannot determine who knows the lie is false); MCQ / Free-form Divergence (high MCQ accuracy with low Free-form accuracy indicates model is selecting plausible options without understanding motivation); Class 1/2 vs. Class 0/4 Gap (high performance on "Common Sense" lies vs. poor performance on "Social Evasion" indicates reliance on world knowledge rather than context reasoning)
- **First 3 experiments:** 1) Baseline Profiling: Run a vanilla model on the full benchmark to identify the "Belief vs. Detectability" gap; 2) Ablation on Falsifiability: Compare model performance on Level 1 (explicit truth) vs. Level 3 (no truth) to quantify reliance on contradiction vs. motivation inference; 3) CoT vs. Reasoning Models: Test if explicit Chain-of-Thought prompting narrows the "Justification" gap in free-form responses

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs effectively understand white lies in scenarios involving pre-existing character relationships and prior impressions? The authors explicitly state in the Limitations section that the dataset constrains scenarios such that white lie triplets are not previously known to roles, failing to capture the complexity of real-world prior impressions, and suggest this for future work.

### Open Question 2
How does white lie comprehension vary across different languages and cultural contexts, particularly those with more indirect communication styles? The Limitations section notes the benchmark is English-only and acknowledges that "in some other languages and cultures, communication tends to be more indirect," which implies the findings may not generalize globally.

### Open Question 3
Why do specialized reasoning models (e.g., o1, DeepSeek-R1) and Chain-of-Thought prompting fail to consistently improve performance on white lie detection? Section 4.4 highlights that CoT prompting and reasoning models show "inconsistent improvements," with some reasoning models "unexpectedly underperformed" compared to vanilla models.

### Open Question 4
How can models bridge the gap between accurately tracking mental states (Info-State) and effectively applying those representations to reason about white lies? Section 4.4 states that "LLMs Can Track Mental States But Fail to Apply Them in White Lie Contexts," noting a significant performance drop between Info-State questions and White Lie Reasoning questions.

## Limitations

- The benchmark's reliance on human-in-the-loop validation introduces scalability concerns for future research and may embed cultural biases from specific annotator backgrounds
- The evaluation methodology using GPT-4o as the sole judge for free-form responses may perpetuate existing model biases rather than providing truly independent assessment
- The human evaluation sample size (12 participants) is relatively small for establishing strong baseline comparisons

## Confidence

**High Confidence:** The finding that models consistently outperform humans on "Common Sense" white lies (Class 1/2) while struggling with "Social Evasion" scenarios (Class 0/4) appears robust, supported by clear performance gaps across multiple model families.

**Medium Confidence:** The hierarchical reasoning gap claim (models track beliefs but fail to apply them) is well-supported by the data but may be influenced by question phrasing rather than fundamental architectural limitations.

**Low Confidence:** The assertion that models rely primarily on pattern matching rather than genuine ToM reasoning is plausible but difficult to definitively prove given the current evaluation framework's constraints.

## Next Checks

1. **Cross-Cultural Validation:** Replicate the benchmark with diverse annotator pools from different cultural backgrounds to assess whether performance gaps persist across varying white lie norms and expectations.

2. **Independent Judge Implementation:** Deploy a completely different model family (not based on transformer architecture) or develop rule-based evaluation criteria to independently verify free-form response scoring, reducing potential bias from using GPT-4o.

3. **Temporal Consistency Test:** Evaluate the same models on TactfulToM after fine-tuning on generic ToM datasets to determine whether improved general ToM capabilities transfer to white lie understanding, or if this remains a distinct reasoning challenge.