---
ver: rpa2
title: 'PianoVAM: A Multimodal Piano Performance Dataset'
arxiv_id: '2509.08800'
source_url: https://arxiv.org/abs/2509.08800
tags:
- piano
- fingering
- hand
- dataset
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PianoVAM, a multimodal piano performance
  dataset that includes synchronized video, audio, MIDI, hand landmarks, fingering
  labels, and metadata from 106 recordings totaling 21 hours. The dataset was collected
  using a Disklavier piano and overhead camera during amateur pianists' daily practice
  sessions, with hand landmarks extracted using MediaPipe and fingering labels generated
  through a semi-automated algorithm.
---

# PianoVAM: A Multimodal Piano Performance Dataset

## Quick Facts
- arXiv ID: 2509.08800
- Source URL: https://arxiv.org/abs/2509.08800
- Reference count: 0
- This paper introduces PianoVAM, a multimodal piano performance dataset that includes synchronized video, audio, MIDI, hand landmarks, fingering labels, and metadata from 106 recordings totaling 21 hours.

## Executive Summary
PianoVAM is a new multimodal piano performance dataset containing 106 recordings (~21 hours) with synchronized video, audio, MIDI, hand landmarks, fingering labels, and metadata. Collected using a Disklavier piano and overhead camera during amateur pianists' daily practice sessions, the dataset includes hand landmarks extracted using MediaPipe and fingering labels generated through a semi-automated algorithm. The dataset exhibits higher sustain pedal usage than MAESTROv3 and achieves over 95% precision in automatic fingering detection. Benchmarking results demonstrate improved piano transcription performance compared to MAESTROv3 in both audio-only and audio-visual settings, with the combined model significantly outperforming the MAESTROv3 baseline across multiple evaluation metrics.

## Method Summary
The dataset was constructed by recording 106 amateur piano performances using a Disklavier ENSPIRE PRO and overhead camera. Video was captured at 1080p/60fps and audio at 44.1kHz mono. Hand landmarks were extracted using MediaPipe, and fingering labels were generated through a semi-automated algorithm that filters "floating" hands using geometric depth estimation. Automatic music transcription was performed using the Onsets and Frames model with audio-only and audio-visual configurations. The audio-visual pipeline includes a visual post-processing step that filters physically implausible notes by comparing audio predictions against visual hand positions. The dataset and annotation tools are publicly available under CC BY-NC 4.0 license.

## Key Results
- PianoVAM achieves over 95% precision in automatic fingering detection
- Comparative analysis shows PianoVAM exhibits higher sustain pedal usage than MAESTROv3 with effect size d=0.870
- The combined MAESTROv3+PianoVAM model significantly outperforms MAESTROv3 baseline in Note and w/Velocity F1 scores
- Audio-visual post-processing improves transcription precision in acoustically degraded conditions (noise/reverb)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual post-processing improves transcription precision in acoustically degraded conditions (noise/reverb).
- **Mechanism:** A cross-modal verification step filters "physically implausible" onsets. The system extracts hand landmarks from video frames corresponding to predicted audio onsets. It estimates which key-region fingers occupy via perspective transformation. If the audio-predicted pitch falls outside the visual candidate set (fingertip proximity ±2 white keys), the note is discarded.
- **Core assumption:** The camera perspective and hand estimation are sufficiently accurate to map 2D landmarks to specific key regions, and that "false positive" audio onsets do not coincidentally align with valid hand positions.
- **Evidence anchors:**
  - [abstract] "Benchmarking results demonstrate that PianoVAM enables improved piano transcription... in both audio-only and audio-visual settings."
  - [section 6.3] "This process enables the elimination of physically implausible notes by referencing visual evidence... If the pitch predicted by the audio-only AMT model falls within this candidate set, the note is retained; otherwise, it is discarded."
  - [corpus] Corpus papers discuss audio-visual fusion (e.g., OMAPS2, PianoYT) but lack specific details on this "candidate intersection" filtering method; evidence is primarily internal to this paper.
- **Break condition:** Fast passages causing significant motion blur, or camera angles where hand occlusion prevents landmark detection, would disable the filtering logic or introduce false negatives.

### Mechanism 2
- **Claim:** Semi-automated fingering annotation achieves high precision by filtering "floating" hands using geometric depth estimation.
- **Mechanism:** The system cannot rely on 3D depth cameras, so it infers relative Z-depth from 2D landmarks. It calculates the area of specific hand triangles (Index-Wrist-Ring) in the 2D image. By comparing these against a "model skeleton" (representing a hand flat on the keyboard), it solves for the relative depth ($t, u, v$) to determine if the hand is "floating" (lifted) and thus not playing. Only "grounded" hands contribute to fingering scores.
- **Core assumption:** The geometric projection of a hand playing a note resembles a "neutral" skeleton posture sufficiently well that deviations indicate lifting rather than rotation or extension.
- **Evidence anchors:**
  - [abstract] "Hand landmarks extracted using MediaPipe and fingering labels generated through a semi-automated algorithm... achieves over 95% precision."
  - [section 5.2] "We define a metric to measure the z-depth of a hand to detect such floating hands... If the z-depth is less than the threshold 0.9... we decide that the target hand... is floating."
  - [corpus] Previous datasets (PIG, ThumbSet) relied on manual annotation or score-based rules; this geometric depth-filtering is a distinct mechanism introduced here.
- **Break condition:** Complex hand postures (e.g., rapid finger substitutions or glissandi) may violate the geometric assumptions of the "model skeleton," causing valid playing frames to be discarded as "floating."

### Mechanism 3
- **Claim:** Combining PianoVAM with MAESTROv3 improves automatic transcription of velocity and offset timing.
- **Mechanism:** While MAESTRO provides clean, aligned data, PianoVAM introduces "realistic and varied" practice conditions (amateur dynamics, higher pedal usage). Training on the combined dataset likely regularizes the model, making it robust to the imprecise articulation and heavy sustain usage found in PianoVAM, which improves generalization on the test split.
- **Core assumption:** The alignment errors or "practice" noise in PianoVAM are systematic in a way that helps the model generalize better than the cleaner MAESTRO data alone.
- **Evidence anchors:**
  - [abstract] "The dataset was collected... during amateur pianists' daily practice sessions... Comparative analysis shows PianoVAM exhibits higher sustain pedal usage than MAESTROv3."
  - [section 6.2] "Post-hoc Wilcoxon tests... showed that both PianoVAM and Combined models significantly outperformed MAESTROv3 in Note and w/ Velocity."
  - [corpus] General consensus in MIR (e.g., MAESTRO, MAPS papers) is that data diversity improves robustness, though specific evidence for the *Combined* mechanism is limited to Table 4 results here.
- **Break condition:** If the "amateur" nature of the data introduces consistent timing errors, a model trained solely on PianoVAM might underperform on professional-quality recordings, requiring the hybrid approach.

## Foundational Learning

- **Concept: Dynamic Time Warping (DTW) for Multimodal Alignment**
  - **Why needed here:** The dataset relies on synchronizing independent recordings (OBS for Video, Logic for MIDI). Understanding DTW is required to grasp how the authors correct latency and jitter between these streams using the Sakoe-Chiba band.
  - **Quick check question:** Why is a simple offset correction insufficient for aligning long audio-MIDI recordings, and how does the Sakoe-Chiba band constraint prevent pathological warping paths?

- **Concept: Hand Pose Estimation (2D vs 3D)**
  - **Why needed here:** The fingering algorithm relies on MediaPipe landmarks. Understanding that these are 2D projections is critical to understanding why the authors had to derive a "relative z-depth" algorithm to distinguish playing hands from hovering hands.
  - **Quick check question:** Given a 2D image of a hand over a keyboard, how can you mathematically infer if the hand is touching the keys or hovering above them without a depth sensor?

- **Concept: Automatic Music Transcription (AMT) Metrics**
  - **Why needed here:** The paper evaluates success using F1 scores for Note, Offset, and Velocity. Understanding the difference between "Note" (onset only) and "w/ Offset" (duration) is necessary to interpret why the Combined model showed significant gains in the latter.
  - **Quick check question:** Why might a model improve on "Note" F1 score but not "Frame" level accuracy, and what does this suggest about the model's precision regarding timing vs. detection?

## Architecture Onboarding

- **Component map:** Disklavier (MIDI/Audio) -> Webcam (Video) -> MediaPipe (Hand Landmarks) -> Z-depth calculator (Floating hand filter) -> Fingering Score Aggregator -> DTW Aligner -> Aligned Multi-modal files
- **Critical path:** The **Fingering Annotation Pipeline** (Section 5) is the most fragile and complex component. It requires: Video -> MediaPipe -> Z-Depth Filter -> Fingering Score -> Human-in-the-loop GUI. If the Z-depth threshold (0.9) is misconfigured, valid notes are labeled as "floating" and discarded.
- **Design tradeoffs:**
  - **Semi-Automation vs. Precision:** The system trades 100% ground-truth accuracy (manual annotation) for scale (semi-automated), accepting ~5% error rate and ambiguous cases ("multiple candidates") requiring human resolution.
  - **Amateur vs. Pro Data:** The choice to record "daily practice" trades performance quality/cleanliness for ecological validity (natural practice conditions, higher pedal usage).
- **Failure signatures:**
  - **Motion Blur:** Fast trills/chords (e.g., Ravel *Jeux d'eau*) cause MediaPipe to lose landmarks, resulting in "No Candidate" notes.
  - **Shadows:** Strong lighting changes or shadows on the keyboard may be misinterpreted by landmark detection or the depth heuristic.
- **First 3 experiments:**
  1.  **Verify Alignment:** Resynthesize the MIDI from a PianoVAM file using the provided SoundFont and overlay it on the raw Audio to visually/audibly check the DTW alignment quality.
  2.  **Z-Depth Threshold Sweep:** Take a single piece (e.g., Chopin Op.18) and vary the "floating hand" threshold (Section 5.2, currently 0.9) to observe the trade-off between "No Candidate" ratios and Precision.
  3.  **Visual Filtering Ablation:** Run the provided transcription benchmark on the test split with and without the "Visual Post-processing" step (Section 6.3) on a noisy input to verify the precision gain reported in Table 5.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can state-of-the-art hand pose estimation or 3D reconstruction models improve fingering detection precision in ambiguous cases involving motion blur or rapid passages?
- **Basis in paper:** [explicit] The authors state: "We aim to improve fingering detection precision by leveraging state-of-the-art models for hand pose estimation [26] and 3D reconstruction [27]."
- **Why unresolved:** The current semi-automated algorithm uses MediaPipe, which struggles with visual ambiguities like motion blur (e.g., in Ravel's *Jeux d’eau*) and complex techniques, resulting in ~20% of notes requiring human selection.
- **What evidence would resolve it:** Re-evaluating the "ambiguous" subset of PianoVAM using advanced models like ViTPose++ to measure the reduction in manual annotation requirements and error rates.

### Open Question 2
- **Question:** Is the observed high sustain pedal usage in PianoVAM driven primarily by amateur performance habits or the specific acoustic characteristics of the recording environment?
- **Basis in paper:** [inferred] Section 4 notes a large effect size in pedal usage ($d=0.870$) compared to MAESTROv3, but the authors can only "speculate" that this results from an "interplay between a pedal-demanding... repertoire, the generous pedaling tendencies of amateur performers... and a less reverberant studio."
- **Why unresolved:** The dataset contains confounding variables (performer skill level and room acoustics) that make it impossible to isolate the specific drivers of the pedaling behavior.
- **What evidence would resolve it:** A comparative analysis of pedal usage across different skill levels in varied acoustic environments, or the release of a dataset with professional recordings in the same studio.

### Open Question 3
- **Question:** Does training end-to-end audio-visual neural networks on PianoVAM provide greater transcription robustness than the heuristic post-processing approach used in the paper's benchmarks?
- **Basis in paper:** [inferred] Section 6.3 implements a "simple post-processing pipeline" to filter physically implausible notes. While effective, the authors suggest future work includes supporting "more robust... analysis" via richer data, implying more sophisticated model architectures remain untested.
- **Why unresolved:** The paper demonstrates the dataset's utility via a heuristic filter rather than training a joint audio-visual model from scratch, leaving the potential performance of integrated architectures unknown.
- **What evidence would resolve it:** Training a unified multi-modal architecture (e.g., CRNN-GCN) on PianoVAM and comparing onset/offset F1 scores against the heuristic post-processing baseline under noisy and reverberant conditions.

## Limitations

- The semi-automated fingering algorithm's generalizability beyond the current dataset is untested, particularly for non-classical repertoire or non-rectangular keyboard layouts
- The visual filtering mechanism's effectiveness in extreme lighting conditions or with alternative camera perspectives remains unvalidated
- The combined MAESTROv3+PianoVAM training benefits could be dataset-specific rather than representing a general principle for multimodal music transcription

## Confidence

- **High confidence:** Dataset construction methodology, basic transcription benchmarking results, fingering precision metric
- **Medium confidence:** Claims about ecological validity improving transcription performance, visual post-processing mechanism effectiveness
- **Low confidence:** Generalizability of fingering annotation algorithm to other piano types or musical genres

## Next Checks

1. Test the fingering annotation pipeline on a separate dataset with different camera angles and lighting conditions to verify algorithm robustness
2. Conduct cross-dataset validation by training on PianoVAM and testing on professional recordings to assess performance degradation
3. Perform ablation studies on the visual filtering mechanism across multiple camera perspectives and hand occlusion scenarios