---
ver: rpa2
title: Unraveling LLM Jailbreaks Through Safety Knowledge Neurons
arxiv_id: '2509.01631'
source_url: https://arxiv.org/abs/2509.01631
tags:
- prompts
- safety
- harmful
- neurons
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for interpreting and defending against
  jailbreak attacks in Large Language Models (LLMs) by focusing on safety knowledge
  neurons. The authors propose a novel interpretability technique that projects neuron
  activations into a vocabulary space, revealing distinct activation patterns for
  harmful versus benign prompts.
---

# Unraveling LLM Jailbreaks Through Safety Knowledge Neurons

## Quick Facts
- arXiv ID: 2509.01631
- Source URL: https://arxiv.org/abs/2509.01631
- Authors: Chongwen Zhao; Yutong Ke; Kaizhu Huang
- Reference count: 36
- Primary result: Introduces ActCali and SafeTuning methods to interpret and defend against LLM jailbreaks via safety knowledge neurons

## Executive Summary
This paper addresses the interpretability and defense of Large Language Models (LLMs) against jailbreak attacks by focusing on safety knowledge neurons within the MLP layers. The authors propose a novel interpretability technique that projects neuron activations into a vocabulary space, revealing distinct activation patterns for harmful versus benign prompts. Building on this insight, they develop ActCali, a method to control model behavior by calibrating safety neuron activations, achieving over 97% attack success rate on well-aligned models. Finally, they introduce SafeTuning, a fine-tuning strategy that reinforces safety-critical neurons to improve robustness against jailbreak attacks. Experiments across multiple models and attack methods show SafeTuning consistently reduces attack success rates while preserving model utility, outperforming four baseline defenses.

## Method Summary
The authors identify safety knowledge neurons by analyzing MLP layer activations, isolating neurons that respond to harmful versus benign prompts. They project these activations into vocabulary space to visualize interpretable tokens associated with safety decisions. ActCali then uses directional vectors computed from harmful and benign activation differences to calibrate neuron outputs, enabling high-success jailbreak attacks. SafeTuning builds on this by fine-tuning only the isolated safety neurons and their upstream pathways on a synthetic refusal dataset, improving model robustness without degrading general capabilities.

## Key Results
- ActCali achieves over 97% attack success rate on well-aligned models by calibrating safety neuron activations
- SafeTuning reduces attack success rates while preserving model utility, outperforming four baseline defenses
- Safety-critical neurons can be identified and isolated within MLP layers, with distinct activation patterns for harmful versus benign prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distinct "rejection" versus "conformity" behaviors in LLMs are encoded in specific, sparse neurons within the MLP layers, which can be interpreted by projecting their activations into vocabulary space.
- **Mechanism:** The authors identify that the output of the MLP down-projection matrix contains columns corresponding to specific knowledge. By calculating the activation of these columns for harmful vs. benign prompts and projecting these activations into the token space, they reveal interpretable tokens (e.g., "Impossible" for harmful, "Execute" for benign) emerging as early as layer 10.
- **Core assumption:** Safety-critical knowledge is localized in specific neuron columns rather than distributed broadly across the network, and these columns can be linearly separated from general language capabilities.
- **Evidence anchors:**
  - [Abstract]: "projects the model's internal representation into a more consistent and interpretable vocabulary space."
  - [Section 3.2]: Defines the compliance direction $d_c = p_B - p_H$ and rejection direction $d_r = pH - pB$, observing distinct activation patterns.
  - [Corpus]: The provided corpus discusses defense strategies but does not offer specific evidence validating the "vocabulary projection" method for identifying safety neurons; it is unique to this paper.
- **Break condition:** If safety behavior is stored in the Attention heads or residual stream in a way that cannot be captured by the MLP column projection, this interpretability method would fail.

### Mechanism 2
- **Claim:** The safety behavior of a model can be causally controlled by calibrating (adding a directional vector to) the output of safety neurons during inference.
- **Mechanism:** ActCali computes a "rejection direction" ($d_r$) and "compliance direction" ($d_c$) using the difference between average harmful and benign neuron activations. By adding $d_c$ (scaled by $\alpha$) to the MLP output during generation, the model is forced to treat harmful prompts as benign, resulting in a jailbreak.
- **Core assumption:** The semantic content of the prompt is separable from the safety decision vector, such that adding the safety vector can override the prompt's semantic intent without destroying the model's language fluency.
- **Evidence anchors:**
  - [Abstract]: "adjusting the activation of safety-related neurons can effectively control the model's behavior with a mean ASR higher than 97%."
  - [Section 4]: Describes the calibrated generation process: $e'_l = \sigma(sl\Theta_{up})\Theta_{down} + \alpha d$.
  - [Corpus]: Neighbors like "Steering Dialogue Dynamics" and "Representation Engineering Perspective" support the general concept of steering vectors, though ActCali's specific neuron-isolation approach is distinct.
- **Break condition:** If the "compliance" direction overlaps too heavily with general instruction-following neurons for benign tasks, adding it to force jailbreaks might cause the model to generate nonsensical text or fail to comply with the specific harmful instruction.

### Mechanism 3
- **Claim:** Model robustness can be improved without degrading utility by fine-tuning only the isolated set of safety-critical neurons on a synthetic refusal dataset.
- **Mechanism:** SafeTuning isolates the set $N_r$ (safety neurons minus fundamental capability neurons). It generates a dataset of (harmful prompt, refusal response) pairs using the model's own refusal capabilities (via ActCali). It then updates the weights of $N_r$ and its upstream pathways using a standard language modeling loss, leaving other weights frozen.
- **Core assumption:** The set of "fundamental neurons" ($N_f$) used for general reasoning is largely disjoint from the set of "safety neurons" ($N_s$), allowing parameter updates to be confined to safety circuits.
- **Evidence anchors:**
  - [Abstract]: "SafeTuning... reinforces safety-critical neurons... while preserving model utility, outperforming four baseline defenses."
  - [Section 5.1]: "We update safety knowledge and activation weight based on the gradient of L... Finally, we confine updates to causally verified safety-related units."
  - [Corpus]: Corpus papers discuss general defense frameworks (e.g., "Test-Time Immunization") but do not provide evidence for this specific "neuron-confined fine-tuning" mechanism.
- **Break condition:** If safety neurons are actually heavily entangled with reasoning neurons (high overlap between $N_f$ and $N_s$), confining updates would fail to defend, or updating them would degrade the Win Rate on benign tasks.

## Foundational Learning

- **Concept:** MLP Layers as Key-Value Memories
  - **Why needed here:** The paper frames the Transformer's MLP layers as storage for "knowledge neurons" (specifically columns in the down-projection matrix). Understanding that $W_{down}$ maps hidden states back to the vocabulary is crucial for the projection mechanism.
  - **Quick check question:** In a Transformer block, which sub-layer is hypothesized to store the specific "knowledge" retrieved by the attention heads?

- **Concept:** Linear Directions in Latent Space
  - **Why needed here:** The ActCali mechanism relies on the assumption that "safety" is a linear direction ($d_r$) in the activation space. You need to understand that adding a vector in high-dimensional space corresponds to adding a semantic concept.
  - **Quick check question:** If you have a vector representing "rejection," how does subtracting it from the residual stream affect the probability of generating a harmful response?

- **Concept:** Causal Tracing / Ablation
  - **Why needed here:** The paper moves beyond correlation (observing neurons fire) to causation (ActCali). Validating that these neurons *cause* the refusal behavior is central to their defense strategy (SafeTuning).
  - **Quick check question:** Why is achieving a high Attack Success Rate (ASR) via activation steering considered stronger evidence for a neuron's importance than simply observing it activates frequently?

## Architecture Onboarding

- **Component map:** Input -> Embedding -> Attention Block -> MLP Block (focus) -> Projection Layer -> Output
- **Critical path:**
  1. **Corpus Processing:** Feed Benign ($B$) and Harmful ($H$) corpuses to identify activation sets $N_s$ and $N_f$.
  2. **Isolation:** Subtract fundamental neurons ($N_f$) from safety neurons ($N_s$) to get the target set $N_r$.
  3. **Intervention:** Compute direction vectors ($d_c, d_r$) for ActCali or calculate gradients for SafeTuning.
- **Design tradeoffs:**
  - **Neuron Ratio ($k$):** Selecting too few neurons ($k < 1\%$) may miss safety circuits; selecting too many ($k > 3\%$) risks including fundamental capability neurons, degrading utility.
  - **Calibration Strength ($\alpha$):** Low $\alpha$ fails to flip the decision; high $\alpha$ may disrupt language fluency or grammar.
- **Failure signatures:**
  - **Language Degradation:** If "All-Shared" neurons are modified (instead of isolated safety neurons), the model enters repetitive loops or generates broken syntax (e.g., "1/INST 1/INST...").
  - **Over-Refusal:** If the rejection direction is over-emphasized during defense, the model may refuse benign queries (drop in Win Rate).
- **First 3 experiments:**
  1. **Visualize Safety Neurons:** Project the activations of $N_r$ for 10 distinct harmful prompts vs. benign prompts into vocabulary space to verify distinct "Rejection" vs. "Conformity" clusters (Replicate Figure 2).
  2. **Attack Validation (ActCali):** Implement the calibration equation $e'_l = \dots + \alpha d_c$ on a small aligned model (e.g., Llama-2-7b-chat) to see if it generates a harmful response for a known adversarial prompt.
  3. **Utility Preservation Check:** Perform SafeTuning on a model and run AlpacaEval to confirm that the Win Rate remains high ($>50\%$) while ASR drops, ensuring you haven't modified fundamental capability neurons.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do attention head mechanisms interact with safety knowledge neurons during jailbreak attacks and defense generation?
- Basis in paper: [Explicit] The Conclusion states: "We will advocate for further research into understanding the role of attention head of jailbreak attacks and the model's defense methods."
- Why unresolved: The current study isolates safety knowledge primarily within the MLP layers ($\Theta_{down}$), leaving the contribution of attention heads to the propagation of harmful context or refusal signals undefined.
- What evidence would resolve it: An analysis of attention head contributions (e.g., using attention rollout or head importance scoring) during ActCali or SafeTuning to map the circuit between input tokens and safety neuron activation.

### Open Question 2
- Question: Can the safety knowledge neuron identification and SafeTuning strategies be effectively transferred to emerging multi-modal Large Language Models (MLLMs)?
- Basis in paper: [Explicit] Section 8 (Limitation): "...extending these alignment techniques to emerging multi-modal LLMs remains an open problem and represents a promising direction for future research."
- Why unresolved: MLLMs process image and audio inputs through separate encoders, and it is unknown if safety concepts are localized in the same linear MLP structures or distributed across modality-specific projections.
- What evidence would resolve it: Experiments applying the vocabulary projection method to the LLM components of MLLMs (e.g., LLaVA) to identify distinct activation patterns for harmful multi-modal inputs.

### Open Question 3
- Question: Does the localization of safety knowledge neurons exist in non-aligned base models, or is this structure an artifact of alignment training?
- Basis in paper: [Explicit] Section 8 (Limitation): "This work primarily investigates well-aligned models, leaving the challenge of effectively aligning non-aligned models largely unexplored."
- Why unresolved: SafeTuning reinforces existing "Rejection" neurons. It is unclear if base models possess similar sparse, interpretable neuron structures for safety that can be activated, or if they require architectural modification.
- What evidence would resolve it: Applying the interpretation method to a base model (without RLHF/SFT) to check for the presence of linearly separable refusal/conformity neurons in the middle layers.

### Open Question 4
- Question: Can automated techniques be developed to adaptively identify optimal hyperparameters like the neuron ratio ($k$) and calibration strength ($\alpha$)?
- Basis in paper: [Explicit] Section 8 (Limitation): "...developing automated techniques to identify and adapt such parameters remains an important direction for future work."
- Why unresolved: The current methodology requires manual tuning of the "knowledge-neuron ratio" $k$ (e.g., 3%) and calibration vector $\alpha$ (e.g., 3), which may not generalize across different model sizes or datasets without laborious adjustment.
- What evidence would resolve it: A proposed algorithm that dynamically selects $k$ based on neuron activation variance or gradient information, achieving comparable safety/utility scores without manual search.

## Limitations

- The method assumes clean separation between safety and capability neurons, which may not hold in practice, limiting generalizability
- The approach relies heavily on synthetic data generation for training, which may not capture the full complexity of real-world attack vectors
- The effectiveness of the method on production-scale models and more sophisticated attack patterns remains uncertain

## Confidence

- **High Confidence:** The vocabulary projection method for visualizing neuron activations and the basic ActCali attack mechanism are well-supported by the experimental results and mathematical framework presented.
- **Medium Confidence:** The SafeTuning defense's effectiveness across diverse models and attack methods is demonstrated, but the generalizability to production-scale models and more sophisticated attack patterns remains uncertain.
- **Low Confidence:** The fundamental assumption of clean separation between safety and capability neurons is a strong claim that requires further validation across different model families and training approaches.

## Next Checks

1. **Neuron Localization Validation:** Test the vocabulary projection method on a different model architecture (e.g., GPT-4 or Claude) to verify that safety knowledge neurons are similarly localized and interpretable, or whether the localization varies significantly across architectures.

2. **Attack Robustness Testing:** Apply ActCali to a more comprehensive set of adversarial prompts beyond AdvBench, including those that combine multiple attack strategies (e.g., combining role-playing with semantic confusion) to assess whether the method can handle sophisticated, multi-vector attacks.

3. **Long-term Safety Evaluation:** Conduct a longitudinal study of SafeTuning's effectiveness by measuring ASR over extended fine-tuning periods and after exposure to adaptive attacks, to determine whether the defense remains robust or whether attackers can learn to circumvent the safety neuron calibration.