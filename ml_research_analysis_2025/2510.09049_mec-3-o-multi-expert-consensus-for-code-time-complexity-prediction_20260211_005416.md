---
ver: rpa2
title: 'MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction'
arxiv_id: '2510.09049'
source_url: https://arxiv.org/abs/2510.09049
tags:
- complexity
- time
- each
- code
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses code time complexity prediction, where a\
  \ single LLM struggles with certain complexity classes due to \"Degeneration-of-Thought\"\
  \ (DoT). The authors propose MEC\xB3O, a multi-expert consensus system that assigns\
  \ LLMs as experts to specific complexity classes based on their performance."
---

# MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction

## Quick Facts
- arXiv ID: 2510.09049
- Source URL: https://arxiv.org/abs/2510.09049
- Reference count: 40
- Primary result: Multi-expert consensus system that achieves at least 10% higher accuracy and macro-F1 scores than open-source baselines for code time complexity prediction.

## Executive Summary
This paper addresses code time complexity prediction, where single LLMs struggle with certain complexity classes due to "Degeneration-of-Thought" (DoT). The authors propose MEC$^3$O, a multi-expert consensus system that assigns LLMs as experts to specific complexity classes based on their performance. These experts debate their predictions using class-specialized instructions, and their outputs are integrated through a weighted consensus mechanism that prioritizes expert class knowledge and logit-based confidence. MEC$^3$O achieves at least 10% higher accuracy and macro-F1 scores than open-source baselines and outperforms GPT-4o-mini in macro-F1 scores on average, while demonstrating competitive performance to GPT-4o and GPT-o4-mini.

## Method Summary
MEC$^3$O employs a three-step pipeline: (1) Expertise Assignment: evaluate candidate LLMs per class on an expertise dataset, assign top performer per class as expert; (2) Debate: each expert generates initial prediction with class-specific instruction, exchanges opinions with other experts, may revise (restricted assents allow ignoring non-expert feedback); (3) Weighted Consensus: final prediction via weighted vote where $w_{i,c} = w_{E,i} \times w_{conf,i}$, with $\alpha > \beta$ weighting for in-expertise predictions and logit-based confidence. The system processes 7,800 Java and Python code snippets from the CodeComplex dataset, predicting among 7 complexity classes (O(1) through O(2$^n$)).

## Key Results
- Achieves at least 10% higher accuracy and macro-F1 scores than open-source baselines
- Outperforms GPT-4o-mini in macro-F1 scores on average
- Demonstrates competitive performance to GPT-4o and GPT-o4-mini
- WECC with logit-based confidence achieves 8.71% higher F1 than majority voting on average
- Preserving expert predictions when they match their specialized class yields 4.24% higher F1 for Java

## Why This Works (Mechanism)

### Mechanism 1: Class-Specific Expertise Assignment
Assigning LLMs as domain experts for specific complexity classes improves prediction accuracy by leveraging each model's comparative strengths. An expertise dataset evaluates each candidate LLM's macro-F1 per class, with the top performer becoming the designated expert for that class, receiving specialized prompting instructions.

### Mechanism 2: Debate-Driven Opinion Refinement
Structured multi-expert debate reduces Degeneration-of-Thought by exposing experts to cross-cutting reasoning from other class specialists. After initial independent predictions, all experts share their prediction-opinion pairs, and each expert may revise its prediction if others reveal overlooked logic.

### Mechanism 3: Weighted Expertise-Confidence Consensus (WECC)
A weighted consensus that combines proven expertise and logit-based confidence outperforms majority voting or judge-based aggregation. For each class c, Score$_x$(c) = $\Sigma$ 1$(p'_i \equiv c) \cdot w_{i,c}$, where $w_{i,c} = w_{E,i} \cdot w_{conf,i}$, with experts predicting within their assigned class receiving $\alpha$ weight and others receiving $\beta$ ($\alpha > \beta$).

## Foundational Learning

- **Degeneration-of-Thought (DoT)**: Single LLMs lock into incorrect reasoning paths and fail to self-correct. Understanding DoT is prerequisite to grasping why multi-agent debate is invoked. *Quick check*: Can you describe a scenario where an LLM's initial incorrect reasoning persists despite conflicting evidence?

- **Macro-F1 vs. Accuracy for Multi-Class Imbalance**: Expertise assignment uses macro-F1 to avoid rewarding trivial majority-class prediction. The evaluation emphasizes macro-F1 to ensure balanced performance across all 7 complexity classes. *Quick check*: Why would a model with high accuracy but low macro-F1 be unsuitable for expertise assignment?

- **Logit-Based Confidence Calibration**: WECC uses logit scores as confidence weights. Understanding whether logits are calibrated (reflect true probability) vs. overconfident is critical for trusting this signal. *Quick check*: If a model's logits are systematically overconfident for incorrect predictions, how would this affect WECC's final output?

## Architecture Onboarding

- **Component map**: Expertise Calibration Module -> Class-Specific Prompt Bank -> Debate Orchestrator -> WECC Aggregator

- **Critical path**: 
  1. Partition expertise set (no overlap with test)
  2. Run per-class F1 evaluation for all candidate LLMs
  3. Assign top LLM per class as expert E$_c$
  4. For each test snippet, collect (prediction, opinion, logits) from all 7 experts
  5. Run 1 debate round: experts see each other's outputs, optionally revise
  6. Apply WECC to compute final class label

- **Design tradeoffs**: Expertise set size affects stability vs. data availability; debate rounds increase refinement but also token cost; $\alpha/\beta$ ratio requires tuning; logit-based confidence needs proper calibration.

- **Failure signatures**: Expert misassignment if expertise set is non-representative; collusion to wrong answer if multiple experts share systematic bias; high token cost from multi-expert processing.

- **First 3 experiments**:
  1. Re-sample the expertise set 5 times at 10% sampling and verify consistent expert assignments with F1 variance analysis.
  2. Disable restricted assents and compare macro-F1 and per-class accuracy to full system.
  3. Plot logit confidence vs. correctness for each expert on held-out set to assess calibration.

## Open Questions the Paper Calls Out

### Open Question 1
Can structured reasoning augmentation or granular expert prompts effectively resolve the persistent confusion between adjacent polynomial classes (O(n$^2$) vs O(n$^3$))? Current expert instructions rely on detecting loop depth, which LLMs frequently misinterpret for cubic cases. What evidence would resolve it: A study evaluating MEC$^3$O with enhanced prompts specifically targeting nested loop semantics and matrix operation patterns to distinguish cubic complexity.

### Open Question 2
Does incorporating large-scale proprietary models (e.g., 70B+ parameters) as experts yield proportional performance gains over the 7B-8B models used in this study? The current study restricted experts to 7B-8B open-source models due to computational feasibility. What evidence would resolve it: Experiments comparing macro-F1 scores of MEC$^3$O configurations utilizing mixtures of large and small models against the current open-source baseline.

### Open Question 3
Can the efficiency of the multi-expert debate be optimized via dynamic expert selection without degrading the weighted consensus performance? The current framework processes every input through all designated expert models, incurring high latency and token generation costs. What evidence would resolve it: Analysis of a modified MEC$^3$O system that activates specific experts only when initial low-cost inference confidence is low, showing a favorable trade-off between cost and accuracy.

## Limitations
- Expertise assignment stability across different expertise set samples is uncertain, with no variance analysis provided across multiple sampling runs
- The α/β weighting ratio is unspecified, which could significantly impact consensus behavior
- The logit-based confidence extraction method from instruction-tuned models is not fully specified, raising questions about proper calibration

## Confidence

- **High confidence**: The multi-expert consensus framework design and its implementation details (debate mechanism, weighted aggregation formula) are clearly specified and internally consistent.
- **Medium confidence**: The empirical performance improvements (10%+ gains over baselines) are well-supported by the reported results, though the stability of expertise assignment across different dataset samples remains uncertain.
- **Low confidence**: The generalizability of the expertise assignment to other code complexity datasets or programming languages beyond Java/Python in CodeComplex.

## Next Checks

1. **Expertise stability validation**: Re-sample the expertise set 5 times at 10% sampling rate and verify that the same LLMs are consistently assigned to each complexity class across runs, with confidence intervals on F1 scores.

2. **Restricted assents ablation study**: Disable the ability for experts to ignore non-expert feedback during debate and measure the impact on macro-F1 and per-class accuracy compared to the full system.

3. **Confidence calibration assessment**: Plot logit-based confidence scores against actual prediction correctness for each expert on a held-out validation set to verify proper calibration, and test temperature scaling or alternative confidence measures if miscalibration is detected.