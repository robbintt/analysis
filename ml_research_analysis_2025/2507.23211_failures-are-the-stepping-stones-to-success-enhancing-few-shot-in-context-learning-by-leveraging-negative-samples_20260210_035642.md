---
ver: rpa2
title: 'Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context
  Learning by Leveraging Negative Samples'
arxiv_id: '2507.23211'
source_url: https://arxiv.org/abs/2507.23211
tags:
- negative
- positive
- examples
- samples
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for improving few-shot in-context
  learning (ICL) by leveraging negative samples. The approach involves constructing
  positive and negative sample corpora based on zero-shot CoT inference, then using
  semantic similarity to retrieve relevant examples for test queries.
---

# Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples

## Quick Facts
- arXiv ID: 2507.23211
- Source URL: https://arxiv.org/abs/2507.23211
- Reference count: 29
- This paper introduces a novel method for improving few-shot in-context learning (ICL) by leveraging negative samples, showing significant performance gains across seven datasets spanning arithmetic, commonsense, and symbolic reasoning tasks.

## Executive Summary
This paper addresses the challenge of improving few-shot in-context learning by introducing a novel method that leverages negative samples. The approach constructs positive and negative corpora from zero-shot Chain-of-Thought (CoT) inference, then uses semantic similarity to retrieve both positive examples and corrective positive examples based on negative samples. For each test query, the method retrieves relevant examples from both corpora and concatenates them to form the final demonstration. Experiments demonstrate that incorporating negative samples leads to better positive example selection and significantly improved ICL performance compared to baseline approaches that rely solely on the most similar positive examples.

## Method Summary
The method consists of two phases: corpus construction and demonstration construction. In the offline corpus construction phase, the training data is clustered using k-means, then each sample is processed with Zero-Shot-CoT to generate predictions. Samples are classified as positive (correct predictions) or negative (incorrect predictions) based on agreement with gold answers, creating separate positive and negative corpora. In the online demonstration construction phase, for each test query, the method retrieves k/2 examples from the positive corpus and k/2 examples from the negative corpus based on semantic similarity. For each retrieved negative example, it finds the most similar positive example from the positive corpus. These positive examples are concatenated to form the final few-shot ICL demonstrations.

## Key Results
- The proposed method significantly outperforms baseline approaches (Zero-Shot-CoT, Similarity Few-Shot, Contrastive CoT, Random Few-Shot) across seven datasets spanning arithmetic, commonsense, and symbolic reasoning tasks.
- Performance varies by task type: arithmetic and symbolic reasoning tasks achieve optimal results with a balanced ratio of direct positives to negative-anchored positives, while commonsense reasoning tasks perform best using exclusively negative-anchored positives.
- Using 4-6 negative samples can lead to performance degradation due to semantic drift, highlighting the importance of finding the optimal balance for each task type.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative samples act as semantic anchors for retrieving corrective positive examples.
- Mechanism: A negative sample (a question the model answered incorrectly) signals a potential failure region in the model's knowledge. By finding a positive example that is semantically similar to this negative sample, the system retrieves a "corrective" demonstration that demonstrates the correct reasoning needed to overcome the error-prone pattern.
- Core assumption: The reasoning required to solve a negative sample is similar to the reasoning required for a semantically similar positive sample, and seeing the correct solution will guide the model.
- Evidence anchors:
  - [abstract] "Subsequently, we further retrieve Positive examples from the Positive sample corpus based on semantic similarity to the Negative examples, then concatenating them with the previously selected Positive examples to serve as ICL demonstrations."
  - [section 1] "We believe that the new positive examples retrieved based on negative samples can be viewed as implicit error correction data..."
- Break condition: The retrieved positive example does not address the specific reasoning error in the negative sample, or the negative sample's error was due to noise rather than a systematic reasoning gap.

### Mechanism 2
- Claim: The method improves coverage of the problem space by querying two distinct sources.
- Mechanism: Standard similarity-based ICL retrieves examples close to the query in embedding space, which may all be from a single "easy" region. This method queries both the positive corpus (for direct analogues) and the negative corpus (for challenging analogues), effectively broadening the search for relevant demonstrations and ensuring the prompt includes examples from potential failure zones.
- Core assumption: The positive and negative corpora are meaningfully distinct distributions of problems.
- Evidence anchors:
  - [section 2.1] "To create the positive and negative corpora... Zero-Shot-CoT is utilized to facilitate LLM in conducting inference... Samples were classified into positive or negative categories based on the agreement between their predicted results and the gold answer..."
  - [abstract] "...studies have primarily emphasized leveraging Positive samples while overlooking the additional information within Negative samples..."
- Break condition: The positive and negative corpora are not meaningfully distinct (e.g., a model could answer correctly on a second attempt), making the dual-retrieval redundant.

### Mechanism 3
- Claim: Task type determines the optimal balance of positive and negative-anchored examples.
- Mechanism: The optimal ratio of query-similar positives to negative-anchored positives is not fixed. For arithmetic and symbolic reasoning, a balanced approach is best. For commonsense reasoning, which involves more open-ended, associative knowledge, using only negative-anchored positives may be superior by creating a "denser set of error-driven positives that collectively tighten the decision boundary."
- Core assumption: Different reasoning tasks rely on different knowledge structures and are therefore susceptible to different types of errors, which are best addressed with different prompting strategies.
- Evidence anchors:
  - [section 3.2] "...for arithmetic and symbolic reasoning tasks, the optimal number of negative samples to use is two, whereas for commonsense reasoning problems, employing exclusively newly retrieved positive examples through negative sample retrieval results in the highest performance."
  - [table 2] Shows performance varying with the m (direct positive) and n (negative-anchored positive) ratio across different datasets.
- Break condition: This finding is based on a limited set of datasets. It may not generalize to other task types like code generation or long-form question answering.

## Foundational Learning

- Concept: **Few-Shot In-Context Learning (ICL)**
  - Why needed here: This is the core capability the paper aims to improve. The method is a technique for constructing better demonstrations for ICL.
  - Quick check question: Can you explain the role of the few demonstrations provided in the prompt for a Large Language Model?

- Concept: **Semantic Similarity Search**
  - Why needed here: The method's entire retrieval process is built on finding examples that are semantically similar to either the query or a negative sample, using an embedding model.
  - Quick check question: What is the core idea behind using vector embeddings to find semantically similar documents?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The corpus construction phase depends on Zero-Shot-CoT to generate predictions and identify failures. The final demonstrations also use CoT.
  - Quick check question: How does prompting a model to "think step-by-step" affect its ability to solve complex reasoning problems?

## Architecture Onboarding

- **Component map:**
  Corpus Builder (Data Processor -> Zero-Shot Generator -> Corpus Splitter) -> Positive Corpus, Negative Corpus; Demonstration Retriever (Query-to-Example Retriever -> Negative-to-Positive Retriever) -> Prompt Assembler

- **Critical path:** The `Negative-to-Positive Retriever` is the key novelty. Its output provides the "corrective" signal that differentiates this approach from standard retrieval-based ICL.

- **Design tradeoffs:**
  - **Cost vs. Performance:** The offline phase requires running a potentially expensive LLM on the training set. The online phase doubles the retrieval workload compared to a standard similarity search.
  - **Sensitivity to Model:** The quality of the Negative Corpus is entirely dependent on the weaknesses of the specific LLM used in the offline phase. A corpus built with GPT-3.5 may not be optimal for Llama-2.

- **Failure signatures:**
  - **Semantic Drift:** As noted in the paper, retrieving positives via weakly related negatives can pull in irrelevant examples.
  - **Over-correction:** For certain tasks, using too many negative-anchored examples (e.g., 4 or 6) degraded performance, likely overwhelming the prompt with edge cases.

- **First 3 experiments:**
  1. **Baseline Comparison:** Implement the full pipeline and compare against a baseline that only retrieves k examples from the `Positive Corpus` (i.e., standard similarity-based ICL) to measure the absolute gain from the negative-sample pathway.
  2. **m:n Ratio Ablation:** On a validation set, systematically vary the ratio of direct positives (m) to negative-anchored positives (n) to find the optimal balance, as suggested by Table 2 in the paper.
  3. **Negative Corpus Source Analysis:** Build corpora using different LLMs (e.g., one with GPT-3.5, one with a smaller open-source model) and evaluate downstream performance. This tests if a "weaker" model's errors are more useful for building the negative corpus than a "stronger" model's.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the optimal ratio of negative-derived to standard positive samples differ significantly between arithmetic/symbolic reasoning and commonsense reasoning?
- Basis: [explicit] Section 3.2 observes that commonsense tasks perform best using exclusively negative-derived positives ($m=0, n=6$), whereas arithmetic tasks perform best with a balanced mixture ($m=5, n=1$).
- Why unresolved: The authors hypothesize that the "associative nature" of commonsense benefits from dense error-driven signals, but the specific mechanism causing this divergence in optimal sample composition remains unverified.
- What evidence would resolve it: An ablation study analyzing the semantic drift and decision boundary tightening across these specific task types using varying sample ratios.

### Open Question 2
- Question: Does the method's efficacy persist when scaling the number of prompt demonstrations ($k$) beyond 2?
- Basis: [inferred] The "Implementation Details" section notes that the demonstration count $k$ was "simply set to 2" due to OpenAI's pricing policy.
- Why unresolved: It is unclear if the benefits of negative-sample retrieval persist, diminish, or amplify in longer-context scenarios where the model has more positive examples available.
- What evidence would resolve it: Experiments replicating the method with $k=4, 8, \text{and } 16$ on open-source models to bypass API cost limits.

### Open Question 3
- Question: Can this iterative corpus construction framework effectively generalize to complex domains outside of reasoning, such as code generation or machine translation?
- Basis: [explicit] The Conclusion suggests that this "iterative approach of constructing retrieval corpora could serve as a versatile framework."
- Why unresolved: The current study is limited to arithmetic, commonsense, and symbolic reasoning; domains like code generation may not yield "correctable" positive samples through simple semantic similarity to negative errors.
- What evidence would resolve it: Application of the proposed method to benchmarks like HumanEval (code) or WMT (translation).

## Limitations

- **Answer Extraction Variability**: The paper does not specify exact answer extraction or normalization rules for each dataset, which could significantly impact corpus construction and downstream performance.
- **Generalizability Beyond Tested Tasks**: The findings are based on seven datasets across three reasoning types and may not extend to other domains like code generation or long-form QA.
- **Negative Corpus Dependency**: The quality of the negative corpus depends entirely on the weaknesses of the LLM used during corpus construction, limiting practical transferability.

## Confidence

- **High Confidence**: The core mechanism of using negative samples to retrieve corrective positive examples is well-supported by experimental results across multiple datasets and baselines.
- **Medium Confidence**: The task-specific optimal ratios (balanced for arithmetic/symbolic, negative-only for commonsense) are supported by ablation studies but may not generalize beyond the tested datasets.
- **Low Confidence**: The claim about semantic drift when using too many negative samples is mentioned but not thoroughly analyzed or quantified in the paper.

## Next Checks

1. **Answer Extraction Validation**: Implement and test multiple answer extraction/normalization strategies on a subset of datasets to quantify the impact on corpus quality and final performance.
2. **Cross-Model Corpus Transferability**: Build negative corpora using different LLMs (e.g., GPT-3.5, Llama-2, smaller models) and evaluate downstream ICL performance to test if corpus quality is model-dependent.
3. **Semantic Drift Analysis**: Systematically vary the number of negative-anchored positives (m:n ratios) and measure both performance and semantic similarity between query, negative, and retrieved positives to quantify and characterize the semantic drift phenomenon.