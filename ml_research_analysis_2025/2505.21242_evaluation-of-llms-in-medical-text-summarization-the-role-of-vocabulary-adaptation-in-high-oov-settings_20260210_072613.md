---
ver: rpa2
title: 'Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation
  in High OOV Settings'
arxiv_id: '2505.21242'
source_url: https://arxiv.org/abs/2505.21242
tags:
- vocabulary
- adaptation
- medical
- scaffix
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates vocabulary adaptation strategies for improving\
  \ large language model (LLM) performance in medical text summarization, particularly\
  \ in challenging scenarios with high out-of-vocabulary (OOV) word concentration\
  \ and novelty. The study benchmarks three vocabulary adaptation methods\u2014MEDVOC,\
  \ MEDVOC-LLM, and ScafFix\u2014across four LLMs (Llama-2, Mistral, Qwen-2, and Llama-3.1)\
  \ on three biomedical summarization datasets (BioASQ, EBM, and PubMedQA)."
---

# Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings

## Quick Facts
- arXiv ID: 2505.21242
- Source URL: https://arxiv.org/abs/2505.21242
- Reference count: 29
- Primary result: Vocabulary adaptation improves medical summarization by 3.68-4.57% overall, with 8.74-14.64% gains in high OOV settings

## Executive Summary
This paper investigates vocabulary adaptation strategies for improving large language model (LLM) performance in medical text summarization, particularly in challenging scenarios with high out-of-vocabulary (OOV) word concentration and novelty. The study benchmarks three vocabulary adaptation methods—MEDVOC, MEDVOC-LLM, and ScafFix—across four LLMs (Llama-2, Mistral, Qwen-2, and Llama-3.1) on three biomedical summarization datasets (BioASQ, EBM, and PubMedQA). Results show that vocabulary adaptation improves summarization performance by 3.68-4.57% in overall test settings and 8.74-14.64% in high OOV concentration settings. ScafFix, which minimizes added vocabulary size while preserving morphological boundaries, consistently outperforms other methods. Human evaluation with medical experts confirms that vocabulary-adapted summaries are more relevant, coherent, and faithful.

## Method Summary
The method involves three vocabulary adaptation strategies (MEDVOC, MEDVOC-LLM, ScafFix) applied to four base LLMs through continual pretraining with LoRA adapters. ScafFix uses AdaptBPE with longest-first matching to add medical terms without scaffolding tokens, initializing new embeddings as averages of existing subword embeddings. Two training strategies are compared: Two-Stage (embeddings first, then LoRA) and End-to-End (joint training). The approach is evaluated on three medical summarization datasets with fine-grained analysis on high OOV and novelty subsets.

## Key Results
- Vocabulary adaptation improves summarization performance by 3.68-4.57% in overall test settings
- Performance gains reach 8.74-14.64% in high OOV concentration settings
- ScafFix consistently outperforms other methods by minimizing vocabulary size while preserving morphological boundaries
- Human evaluation confirms vocabulary-adapted summaries are more relevant, coherent, and faithful

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing token fragmentation of medical terms preserves semantic boundaries and improves generation quality in high OOV settings.
- Mechanism: General-purpose tokenizers over-fragment domain-specific words (e.g., "cardiomyopathy" → 6 subwords in Llama-2), which degrades encoding semantics and increases generation complexity. Vocabulary adaptation adds whole medical terms as single tokens, preserving morphological boundaries and reducing the sequence length the model must generate.
- Core assumption: Over-fragmentation is causally linked to performance degradation; reducing it should improve summarization.
- Evidence anchors: [abstract] performance improvements of 3.68-4.57% overall and 8.74-14.64% in high OOV settings; Table 1 shows fragment scores increase 13-20% from general to medical domain; Table 7 demonstrates ScafFix preserves morphological boundaries.
- Break condition: If input text has low medical OOV concentration or low novelty, vocabulary adaptation shows negligible or negative gains.

### Mechanism 2
- Claim: Minimizing added vocabulary size by removing scaffolding tokens reduces under-trained embeddings and stabilizes training.
- Mechanism: Standard BPE-based vocabulary addition requires intermediate "scaffold" tokens (e.g., adding "cholesterol" requires also adding "chole"). These intermediate tokens receive limited gradient signal once the full word is learned. ScafFix uses AdaptBPE (longest-first matching before BPE) to add only target words, reducing vocabulary size ~20% and decreasing under-trained token proportion.
- Core assumption: Under-trained embeddings add noise during inference; smaller vocabulary additions yield better downstream performance.
- Evidence anchors: Vocabulary size reduction of 20% for Llama-3.1; Table 8 shows ScafFix achieves lower vocabulary sizes with competitive fragment scores.
- Break condition: If downstream task requires subword granularity (e.g., morphologically rich languages with productive affixation), removing scaffolds may harm generalization.

### Mechanism 3
- Claim: Two-stage continual pretraining stabilizes new embedding learning before LoRA adaptation, preventing interference with frozen base embeddings.
- Mechanism: New token embeddings are initialized as the average of existing subword embeddings. Two-stage training first optimizes embeddings (frozen LoRA) on 10K domain samples, then unfreezes LoRA for joint training on 20K samples. This prevents noisy gradients from under-trained embeddings from destabilizing adapter weights early in training.
- Core assumption: Joint end-to-end training risks overfitting new embeddings to the initial LLM embedding space before domain signal accumulates.
- Evidence anchors: Two-Stage outperforms End-to-End for Llama-2 on most settings; Table 12 shows End-to-End performs better for Llama-3.1.
- Break condition: Effectiveness varies by model architecture; Llama-3.1 performed better with End-to-End, suggesting larger vocabularies or different initialization may reduce the need for staged training.

## Foundational Learning

- **Concept: Fragment Score**
  - Why needed here: Quantifies tokenizer quality for domain-specific text by measuring average subwords per word; the primary metric for selecting vocabulary additions.
  - Quick check question: If a tokenizer splits "electrocardiogram" into 5 tokens, how does this affect fragment score compared to a tokenizer that splits it into 2 tokens?

- **Concept: OOV (Out-of-Vocabulary) Concentration**
  - Why needed here: Defines challenging evaluation subsets; high OOV settings are where vocabulary adaptation shows the largest gains (8.74-14.64% improvement).
  - Quick check question: Why might "split more than thrice" be a better threshold for identifying difficult medical terms than "split more than once"?

- **Concept: Continual Pretraining with LoRA**
  - Why needed here: Full LLM fine-tuning is computationally infeasible; LoRA enables efficient domain adaptation while preserving base model capabilities.
  - Quick check question: What modules should LoRA adapters be applied to for vocabulary adaptation (hint: the paper targets both attention and MLP layers)?

## Architecture Onboarding

- **Component map**: PAC corpus -> Candidate Vocabulary Generator -> Vocabulary Selector (fragment score) -> Tokenizer Modifier (AdaptBPE) -> Embedding Extender -> Continual Pretrainer (LoRA) -> Evaluation

- **Critical path**:
  1. Identify high-fragment-score medical terms in target corpus
  2. Select optimal vocabulary size via fragment score minimization
  3. Modify tokenizer with AdaptBPE; extend embedding matrix
  4. Run Two-Stage continual pretraining (embeddings first, then LoRA)
  5. Evaluate on fine-grained OOV/novelty subsets

- **Design tradeoffs**:
  - Vocabulary size vs. fragment score: Larger additions reduce fragmentation but increase under-trained token risk
  - End-to-End vs. Two-Stage training: Two-Stage more stable for smaller models; End-to-End may work better for models with larger base vocabularies
  - Domain corpus vs. task-specific vocabulary: PAC provides broad medical coverage; task-specific vocabulary may miss rare but critical terms

- **Failure signatures**:
  - Low OOV/novelty datasets: Vocabulary adaptation underperforms BASE (e.g., BioASQ-S with 4.11% novelty)
  - High source-reference overlap: When summaries heavily copy source text, OOV adaptation provides minimal benefit
  - Model-specific effectiveness: Mistral-7B showed inconsistent gains, only 1/12 comparisons improved

- **First 3 experiments**:
  1. Baseline OOV analysis: Compute fragment scores and OOV concentration on your medical dataset using target model tokenizer; confirm >10% words split >3 times before proceeding.
  2. Vocabulary ablation: Compare ScafFix (500 tokens) vs. MEDVOC-LLM vs. no adaptation on a held-out high-OOV subset; use Rouge-L and Concept-Score metrics.
  3. Training strategy comparison: Run both Two-Stage and End-to-End continual pretraining; select based on stability (loss curves) and downstream performance on DifficultRS/NovelRS subsets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of vocabulary adaptation strategies like ScafFix scale robustly to significantly larger LLM variants (e.g., 13B or 70B parameters)?
- Basis in paper: The authors explicitly state in the Limitations section: "because of hardware constraints we could not explore much larger variants (like 13B and 70B) of the models considered in the study."
- Why unresolved: The study was computationally restricted to 7B/8B models; it is unverified if the 3-14% performance gains hold or diminish as model capacity increases inherently.
- What evidence would resolve it: Benchmarking the proposed ScafFix method on Llama-3.1-70B or similar large-scale models on the same high-OOV medical datasets.

### Open Question 2
- Question: How does full-scale fine-tuning compare to LoRA-based continual pretraining for integrating vocabulary adaptations in medical summarization?
- Basis in paper: Section 7 notes that all results were generated using LoRA, and "This might make a difference when replicating the results with full-scale fine-tuning without any parameter efficient fine-tuning strategies."
- Why unresolved: The computational infeasibility of full fine-tuning meant the authors relied solely on Low-Rank Adaptors, potentially capping the learning capacity of the new vocabulary embeddings.
- What evidence would resolve it: A comparative analysis of LoRA-adapted models against fully fine-tuned models regarding the stability and fidelity of newly added medical tokens.

### Open Question 3
- Question: Can ScafFix improve performance in low-novelty or extractive summarization scenarios where it currently underperforms compared to the base models?
- Basis in paper: The results show that on datasets with low unigram novelty (like BioASQ-S), vocabulary adaptation sometimes struggles to outperform the BASE model, and the authors note a performance drop in "extractive summaries, low OOV concentration and low novelty."
- Why unresolved: While the paper explains *that* it underperforms, it does not propose a mechanism to recover this performance.
- What evidence would resolve it: A modified ScafFix approach tested specifically on low-novelty datasets to determine if vocabulary pruning or weighting can prevent performance degradation in extractive tasks.

## Limitations

- Vocabulary adaptation effectiveness strongly depends on OOV concentration and novelty levels, showing negligible improvements on datasets with high source-reference overlap.
- The comparative advantage varies significantly across model architectures, with Mistral-7B showing inconsistent gains compared to Llama-2.
- Human evaluation involved only three medical experts reviewing 50 samples per method, limiting statistical power and potentially introducing selection bias.

## Confidence

- **High Confidence**: The core finding that vocabulary adaptation improves medical summarization performance (3.68-4.57% overall gain) is well-supported by extensive quantitative evaluation across three datasets, four models, and multiple adaptation methods.
- **Medium Confidence**: The claim that ScafFix outperforms other methods by minimizing vocabulary size while preserving morphological boundaries is supported by controlled comparisons, but model-dependent variability reduces confidence in universal applicability.
- **Low Confidence**: The assertion that removing scaffolding tokens consistently improves performance by reducing under-trained embeddings lacks direct empirical validation.

## Next Checks

1. **Model Architecture Sensitivity Analysis**: Conduct systematic ablation studies across the four tested models to identify architectural features that determine vocabulary adaptation effectiveness, particularly focusing on base vocabulary size, embedding dimensionality, and attention mechanism differences.

2. **Longitudinal Embedding Quality Assessment**: Track embedding quality metrics (cosine similarity to reference embeddings, gradient norms, frequency of use) throughout the two-stage training process to empirically validate whether scaffolding removal actually reduces under-trained token noise as claimed.

3. **Cross-Lingual and Morphological Generality Test**: Evaluate vocabulary adaptation effectiveness on morphologically rich languages (e.g., German, Finnish) and low-resource medical domains to test whether the fragmentation-based mechanism generalizes beyond English medical text.