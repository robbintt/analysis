---
ver: rpa2
title: 'Drift-Aware Federated Learning: A Causal Perspective'
arxiv_id: '2503.09116'
source_url: https://arxiv.org/abs/2503.09116
tags:
- feature
- client
- federated
- learning
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses feature drift in federated learning, which
  arises from class imbalance and participation imbalance among clients. The authors
  analyze how both global and local optimizers contribute to drift and propose a causal
  perspective to mitigate these issues.
---

# Drift-Aware Federated Learning: A Causal Perspective

## Quick Facts
- **arXiv ID:** 2503.09116
- **Source URL:** https://arxiv.org/abs/2503.09116
- **Reference count:** 34
- **Primary result:** CAFE achieves 67.29% accuracy on CIFAR-10-LT with extreme imbalance, outperforming state-of-the-art methods.

## Executive Summary
This paper addresses feature drift in federated learning caused by class imbalance and participation imbalance among clients. The authors propose a causal perspective, modeling the optimization process as a structural causal graph where optimizers act as confounders. Their solution, Causal drift-Aware Federated lEarning (CAFE), constructs a causal graph to model relationships between sample features, classification results, and optimizers. During training, CAFE independently calibrates local client features and classifiers using causal relationships between feature-invariant components and classification outcomes. In inference, it corrects global model drift that favors frequently communicating clients.

## Method Summary
CAFE implements causal intervention through orthogonal decomposition of embedding vectors into invariant features and drift components caused by global and local optimizers. The method applies backdoor adjustment during training by subtracting estimated drift directions from features before classification. During inference, only the global drift component is removed. The framework uses a cosine classifier with drift-calibrated features and incorporates a history-aware average over local training batches. Experimental validation shows significant performance improvements over state-of-the-art methods across multiple imbalanced datasets.

## Key Results
- Achieves 67.29% accuracy on CIFAR-10-LT with extreme imbalance (Dir=0.1, CF=0.1), outperforming second-best method at 62.96%
- Demonstrates strong robustness across various datasets and parameter settings
- Effectively reduces drift toward majority classes and frequently communicating nodes
- Operates under privacy-preserving conditions while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Decomposition of Embedding Drift
The paper decomposes embedding vectors into invariant feature components and drift components from global and local optimizers. This orthogonal decomposition allows isolation of drift by projecting onto specific directions determined by optimizer history.

### Mechanism 2: Causal Intervention via Backdoor Adjustment (Training)
By removing drift components from logits during training, the classifier learns the direct causal effect of features on labels, filtering out spurious correlations from imbalanced participation.

### Mechanism 3: Inference-Time Global Drift Correction
A global model aggregated from heterogeneous clients contains residual drift favoring frequent communicators. This is corrected at inference time by removing only the global drift component.

## Foundational Learning

- **Concept: Structural Causal Models (SCM) & Intervention ($do$-calculus)**
  - Why needed: The paper frames FL optimization as a causal graph where optimizers are "confounders"
  - Quick check: Why does the paper model the local optimizer as a "confounder" rather than just noise?

- **Concept: Momentum Gradient Descent (MGD) in FL**
  - Why needed: The theoretical derivation of drift relies heavily on momentum update expansion
  - Quick check: How does the decay rate $\mu$ influence the magnitude of the drift component $d_k$?

- **Concept: Non-IID Data & Label Skew (Dirichlet Distribution)**
  - Why needed: The paper evaluates using Dirichlet distributions to simulate extreme class imbalance
  - Quick check: In the experiment setup, does a lower Dirichlet parameter (e.g., 0.1 vs 1.0) represent more or less data heterogeneity?

## Architecture Onboarding

- **Component map:**
  - Feature Extractor ($\theta$) -> Causal Classifier ($\phi$) -> Drift Estimator -> History Buffer

- **Critical path:**
  1. Extract feature $h$
  2. Compute $\hat{d}_G$ from global momentum and $\hat{d}_k$ from local momentum buffers
  3. Calculate $h_{calibrated} = h - d_G - d_k$
  4. Compute loss averaged over historical predictions (Training only)
  5. Apply $h_{calibrated} = h - d_G$ (Inference only)

- **Design tradeoffs:**
  - Memory vs. Robustness: History-Aware Average requires storing predictions/gradients over $E$ batches
  - Assumption validity: Orthogonality assumption simplifies math but may fail on entangled manifolds

- **Failure signatures:**
  - Divergence during warmup if momentum buffers are empty or unstable
  - Over-correction if balancing parameters are incorrectly tuned

- **First 3 experiments:**
  1. Drift Visualization: Compare baseline FL vs. CAFE on 2D dataset and plot embedding space
  2. Ablation on History Window ($E$): Vary the number of local epochs to find stability point
  3. Stress Test on Participation Ratio: Confirm inference-time correction recovers accuracy for sparse clients

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- Orthogonal decomposition assumption may not hold for complex, entangled data manifolds
- Performance depends on accurate momentum buffer estimation, which can be unstable
- Fixed global drift correction assumes no significant test-time distribution shift

## Confidence

- **High Confidence:** Experimental results showing accuracy improvements and theoretical framework of causal intervention
- **Medium Confidence:** Orthogonal decomposition mechanism due to assumptions about feature-drift relationships
- **Medium Confidence:** Inference-time global drift correction given limited empirical validation

## Next Checks

1. **Orthogonality Validation:** Test orthogonal decomposition assumption on synthetic datasets with known drift directions
2. **Sensitivity Analysis:** Conduct ablation studies varying history window size (E) and momentum decay rate (Î¼)
3. **Cross-Domain Generalization:** Evaluate CAFE on non-image datasets to assess framework generalization beyond visual domain