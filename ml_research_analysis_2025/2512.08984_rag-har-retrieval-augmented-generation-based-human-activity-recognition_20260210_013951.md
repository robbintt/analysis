---
ver: rpa2
title: 'RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition'
arxiv_id: '2512.08984'
source_url: https://arxiv.org/abs/2512.08984
tags:
- activity
- prompt
- data
- activities
- sensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAG-HAR introduces a training-free retrieval-augmented framework
  for Human Activity Recognition (HAR) that leverages large language models (LLMs)
  for classification. Instead of training dataset-specific models, RAG-HAR computes
  statistical descriptors from sensor data, retrieves semantically similar samples
  from a vector database, and uses this contextual evidence for LLM-based activity
  identification.
---

# RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition

## Quick Facts
- **arXiv ID**: 2512.08984
- **Source URL**: https://arxiv.org/abs/2512.08984
- **Reference count**: 40
- **Primary result**: Achieves 91.60% accuracy on PAMAP2 without training, outperforming deep learning baselines by 1-7% F1-score

## Executive Summary
RAG-HAR introduces a training-free framework for Human Activity Recognition that leverages large language models through retrieval-augmented generation. Instead of training dataset-specific models, it computes statistical descriptors from sensor data, retrieves semantically similar samples from a vector database, and uses this contextual evidence for LLM-based activity identification. The framework achieves state-of-the-art performance across six diverse HAR benchmarks without requiring model training or fine-tuning, and can recognize previously unseen activity classes.

## Method Summary
RAG-HAR processes raw sensor time-series by segmenting into sliding windows and partitioning each into Full, Start, Mid, and End segments. For each partition, it computes 8 statistical descriptors (mean, max, min, std, Q1, Q3, median, no. of peaks) per channel, serializes these to text, and embeds using text-embedding-3-small. The embeddings are stored in a vector database for retrieval during inference. At test time, the framework retrieves top-10 similar samples via ANN search with weighted re-ranking, constructs a prompt with retrieved examples and candidate features, and calls an LLM (gpt-5-mini) for classification. An optional prompt optimizer systematically improves system prompts, and LLM-based activity descriptors can enrich the vector database with natural language context.

## Key Results
- Achieved 91.60% accuracy and 91.12% F1-score on PAMAP2 dataset
- Outperformed existing deep learning baselines by 1-7% F1-score across multiple datasets
- Demonstrated open-set classification capabilities, correctly labeling novel activities with up to 96.47% accuracy
- Eliminated need for model training or fine-tuning while maintaining state-of-the-art performance

## Why This Works (Mechanism)

### Mechanism 1
Retrieval-augmented context grounding improves LLM classification of sensor-derived activity patterns compared to direct prompting. The framework extracts statistical descriptors from sensor windows, indexes them in a vector database, and retrieves semantically similar labeled examples during inference. The LLM then reasons over both the query sample's features and the retrieved context, reducing reliance on the LLM's direct interpretation of raw numerical sequences. This breaks down when retrieval fails for activities with similar motion profiles, leading to noisy context and reduced accuracy.

### Mechanism 2
LLM-based activity descriptors improve retrieval quality by enriching statistical summaries with natural language semantic context. An LLM generates detailed textual descriptions of sensor windows (motion characteristics, dominant axes, intensity, periodicity) in addition to statistical features. These descriptions are embedded and indexed, improving semantic matching during retrieval. This adds cost and latency due to additional LLM calls, and may not improve retrieval if descriptions are generic or inconsistent.

### Mechanism 3
Systematic prompt optimization enhances LLM classification performance by identifying effective instruction strategies tailored to each dataset. A meta-optimizer generates candidate system prompts, evaluates them on a validation set using the LLM classifier, and iteratively refines prompts through selection and mutation. This approach may overfit to validation set prompts or become too dataset-specific, reducing generalization to new datasets or unseen activities.

## Foundational Learning

- **Vector Database & Semantic Retrieval**: Understanding how embeddings are indexed and searched is essential for debugging retrieval quality and latency. Quick check: Given a statistical descriptor vector, can you trace the retrieval pipeline to identify which labeled samples are returned as context?

- **LLM Prompt Engineering**: The framework relies on carefully designed prompts to instruct the LLM; prompt structure directly impacts classification accuracy. Quick check: Can you identify the system prompt and user prompt components in the RAG-HAR prompt template, and explain how retrieved examples are formatted?

- **Statistical Feature Extraction from Time-Series**: The framework converts raw sensor windows into statistical descriptors; understanding which features are computed and why is critical for interpreting retrieval results. Quick check: For a given sensor window, can you compute the eight statistical features (mean, std, percentiles, peaks) and explain how they capture activity characteristics?

## Architecture Onboarding

- **Component map**: Data Processing -> Embedding Module -> Vector Database -> Retrieval Engine -> LLM Classifier -> Prompt Optimizer
- **Critical path**: Ingest raw sensor data → normalize → segment into windows → compute statistical features for full segment and three partitions → serialize features to text, embed, and store in vector database → at inference, embed test window features, retrieve top-k similar samples via ANN + re-ranking → construct prompt with retrieved examples and candidate features; call LLM for classification → optionally run prompt optimizer offline to improve system prompt
- **Design tradeoffs**: Baseline vs. Optimized Stage (statistical templates vs. LLM-generated descriptors and prompt optimization), retrieval depth (q=10 fixed), partition weights (0.4, 0.2, 0.2, 0.2 prioritizing full segment)
- **Failure signatures**: Low retrieval accuracy for similar activities, prompt overfitting to validation set, context window overflow with high channel counts
- **First 3 experiments**: Baseline validation (measure F1-score vs. retriever-only and LLM-only), ablation on partitions (full only vs. full+partitions), open-set stress test (hold out activity classes, evaluate with/without true label hints)

## Open Questions the Paper Calls Out

- **Multi-agent RAG framework**: Can specialized agents for different sensors or body parts improve performance in complex multi-sensor environments compared to the current single-agent architecture? The current implementation uses a single LLM instance; multi-agent coordination is proposed but untested.

- **Specialized time-series embeddings**: Can dedicated time-series embedding models replace general-purpose text embedding methods to better capture sensor dynamics? The current approach relies on text models as general-purpose time-series embedding models do not yet exist.

- **Raw data incorporation**: Does incorporating raw or downsampled sequences via dimensionality reduction retain fine-grained temporal patterns better than statistical descriptors? The current approach uses statistical summaries to manage context window constraints, leaving raw-data approaches unexplored.

## Limitations

- The framework's reliance on statistical descriptors may not adequately discriminate between similar activities with subtle motion differences
- Claims about outperforming SOTA models lack ablation studies isolating the contribution of each enhancement
- Open-set classification capability is demonstrated but lacks rigorous validation on truly unseen classes from different domains

## Confidence

- **High confidence**: Framework architecture is clearly specified with reproducible data processing and retrieval pipeline steps
- **Medium confidence**: Claims about outperforming SOTA deep learning models by 1-7% F1-score are based on reported results but lack comprehensive ablation studies
- **Low confidence**: Open-set classification capability (96.47% accuracy) is demonstrated but lacks validation on truly unseen classes from different domains

## Next Checks

1. **Ablation Study**: Compare RAG-HAR performance with retriever-only (majority vote), LLM-only (no retrieval), and baseline statistical matching to quantify the contribution of each component

2. **Descriptor Quality Analysis**: Manually inspect retrieved examples for visually similar activities to assess whether statistical descriptors provide sufficient discriminative power

3. **Cross-Dataset Generalization**: Test the prompt optimizer on a dataset from a different domain (e.g., smartphone-based HAR) to evaluate whether optimized prompts overfit to specific sensor configurations