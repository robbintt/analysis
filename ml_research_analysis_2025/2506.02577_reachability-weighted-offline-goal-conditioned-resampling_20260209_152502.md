---
ver: rpa2
title: Reachability Weighted Offline Goal-conditioned Resampling
arxiv_id: '2506.02577'
source_url: https://arxiv.org/abs/2506.02577
tags:
- offline
- goal-conditioned
- learning
- sampling
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unreachable state-goal-action
  pairs in offline goal-conditioned reinforcement learning (RL) caused by uniform
  random goal sampling. The authors propose Reachability Weighted Sampling (RWS),
  a method that uses a reachability classifier trained via positive-unlabeled (PU)
  learning on goal-conditioned Q-values to prioritize sampling of potentially reachable
  goals.
---

# Reachability Weighted Offline Goal-conditioned Resampling

## Quick Facts
- **arXiv ID:** 2506.02577
- **Source URL:** https://arxiv.org/abs/2506.02577
- **Reference count:** 11
- **Key outcome:** RWS improves offline goal-conditioned RL performance by up to 50% on complex robotic manipulation tasks by prioritizing reachable goals via reachability-weighted sampling.

## Executive Summary
This paper addresses the challenge of unreachable state-goal-action pairs in offline goal-conditioned reinforcement learning caused by uniform random goal sampling. The authors propose Reachability Weighted Sampling (RWS), a method that uses a reachability classifier trained via positive-unlabeled (PU) learning on goal-conditioned Q-values to prioritize sampling of potentially reachable goals. RWS is a plug-and-play module that integrates with standard offline RL algorithms. Experiments on six complex simulated robotic manipulation tasks show significant performance improvements. Notably, RWS achieved nearly a 50% improvement in HandBlock-Z task performance compared to the baseline.

## Method Summary
RWS is a plug-and-play module for offline goal-conditioned RL that addresses the problem of unreachable state-goal-action pairs created by uniform random goal sampling. The method works by first generating a positive dataset through hindsight relabeling (HER) of achieved goals within trajectories, and an unlabeled dataset by randomly sampling goals from the offline dataset. A linear logistic classifier is then trained on the Q-values of these state-action-goal tuples using non-negative PU learning, with the class prior η_p fixed at 0.5. The classifier outputs weights that prioritize reachable goals during sampling, computed via an exponential transformation to prevent zero-weighting of potentially reachable but low-Q-value pairs. RWS integrates with base offline RL algorithms like GC-TD3BC, GC-ReBRAC, and GC-MCQ through concurrent updates: alternating between offline RL updates and classifier updates.

## Key Results
- RWS significantly improves performance over uniform goal sampling on six simulated robotic manipulation tasks
- Achieved nearly 50% improvement in HandBlock-Z task compared to baseline
- Gains are most pronounced at expert data ratios of 0.5 and 0.3, with diminishing returns at 0.1
- The method demonstrates effectiveness across different base offline RL algorithms (GC-TD3BC, GC-ReBRAC, GC-MCQ)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prioritizing potentially reachable goals during training improves offline GCRL policy performance compared to uniform goal sampling.
- Mechanism: Uniform random goal sampling generates many unreachable state-goal-action pairs that act as noise during training. By learning a reachability classifier that assigns higher sampling probability to reachable pairs, the policy trains on higher-quality transitions, reducing gradient noise from infeasible goals.
- Core assumption: The goal-conditioned Q-value correlates with actual reachability—higher Q-values indicate goals requiring fewer steps to achieve.
- Evidence anchors:
  - [abstract] "Uniform sampling, however, requires an intractably large dataset to cover all possible combinations and creates many unreachable state-goal-action pairs that degrade policy performance."
  - [Section 4.1] "Since the sparse reward function depends solely on whether φ(s) = g, a policy's return is determined by the number of time steps spent reaching the goal. Consequently, the goal-conditioned value function Qπ(s,g,a) represents the expected number of steps for policy π to reach the goal."
  - [corpus] Related work on graph-based reachability (Huang et al. 2024, Mezghani et al. 2023) supports that reachable goal identification improves GCRL, though RWS avoids explicit graph construction.
- Break condition: If Q-values are poorly estimated (e.g., during early training or with highly suboptimal data), reachability scores will be unreliable, potentially misprioritizing goals.

### Mechanism 2
- Claim: Positive-Unlabeled (PU) learning enables reachability classification without requiring explicit negative examples.
- Mechanism: Hindsight relabeling (HER) generates known-positive state-goal-action pairs from achieved states within trajectories. Random goal sampling generates unlabeled pairs (mixture of reachable and unreachable). Non-negative PU loss trains a classifier to distinguish reachable from unreachable without bias from treating all unlabeled data as negative.
- Core assumption: The unlabeled dataset contains a mixture of positive and negative examples with stable class prior (ηp = 0.5 used practically).
- Evidence anchors:
  - [abstract] "RWS uses a reachability classifier trained via positive-unlabeled (PU) learning on goal-conditioned state-action values."
  - [Section 3.3] "In the Positive-Unlabeled (PU) learning scenario, only a positively labeled dataset DP and an unlabeled dataset DU are available... the term E(x,y)~DU [log(1-ŷ)] - ηp E(x,y)~DP [log(1-ŷ)] must remain non-negative to avoid severe overfitting."
  - [corpus] Limited direct corpus evidence on PU learning for GCRL; this appears novel to this work.
- Break condition: If the class prior ηp is highly misspecified or positive examples are contaminated with false positives, PU learning becomes unstable.

### Mechanism 3
- Claim: Exponential transformation of classifier scores prevents zero-weighting of potentially reachable but low-Q-value goals.
- Mechanism: Raw classifier outputs could assign near-zero weights to some reachable pairs due to Q-estimation errors. The exponential function w(s,g,a) = exp(C_θ(Q_ψ(s,g,a))) / Σ exp(C_θ(Q_ψ(s,a,g'))) ensures all goals have non-zero sampling probability while still prioritizing high-reachability goals.
- Core assumption: Some reachable state-goal-action pairs have underestimated Q-values; complete suppression would harm policy generalization.
- Evidence anchors:
  - [Section 4.2] "Due to the approximation error of the pretrained Q_ψ(s,g,a), it may estimate a low value for some reachable (s,g,a) pairs... As a consequence, the classifier C_θ(Q) may assign low or even zero weight to (s,g,a), which limits the ability to train a generalizable goal-conditioned policy."
  - [Figure 7 visualization] Shows RWS assigns meaningful non-zero weights across reachable regions while suppressing clearly unreachable areas.
  - [corpus] No direct corpus comparison; exponential weighting is method-specific.
- Break condition: If the classifier is poorly calibrated, exponential scaling may amplify errors, over-weighting false positives or under-weighting true reachable goals.

## Foundational Learning

- Concept: **Goal-Conditioned Reinforcement Learning (GCRL)**
  - Why needed here: The entire method operates in the GCRL framework where policies are conditioned on goal states. Understanding sparse rewards (r = 0 if goal reached, -1 otherwise) and the goal relabeling paradigm is essential.
  - Quick check question: Can you explain why sparse rewards simplify reward engineering but create exploration challenges?

- Concept: **Hindsight Experience Replay (HER)**
  - Why needed here: HER generates the positive examples for PU learning by relabeling transitions with actually-achieved goals. Without understanding HER, the positive data construction in RWS will be unclear.
  - Quick check question: Given a failed trajectory that reached state s_T instead of goal g, how would HER relabel the transition (s_0, a_0, g)?

- Concept: **Positive-Unlabeled (PU) Learning**
  - Why needed here: RWS frames reachability classification as a PU problem. Understanding why treating unlabeled data as negative causes bias—and how non-negative risk estimators fix this—is critical.
  - Quick check question: In standard binary classification with corrupted negatives, what systematic bias emerges if you train on positive and "treat-as-negative" unlabeled data?

## Architecture Onboarding

- Component map:
  - Q-network Q_ψ (goal-conditioned value function, shared with base offline RL algorithm)
  - Reachability classifier C_θ (linear logistic classifier mapping Q-values to reachability probabilities)
  - Weighting function (exponential normalization of classifier outputs)
  - Base offline RL algorithm (TD3BC, ReBRAC, MCQ, etc.)

- Critical path:
  1. Sample batch B from offline dataset D
  2. Generate positive batch B_P via HER (hindsight relabeling within trajectories)
  3. Generate unlabeled batch B_U via uniform random goal sampling from D
  4. Update classifier C_θ using non-negative PU loss (Eq. 5), Q_ψ frozen
  5. Compute sampling weights w(s,ĝ,a) via Eq. 6
  6. Update policy π and Q_ψ using weighted offline RL objective

- Design tradeoffs:
  - **Concurrent vs. two-stage training**: Paper uses concurrent updates (alternate RL step / classifier step) for efficiency, but two-stage (pretrain Q, then train classifier) may yield more stable classifiers at cost of time.
  - **Classifier complexity**: Linear logistic classifier chosen for simplicity; more expressive classifiers may overfit given limited positive data.
  - **η_p setting**: Fixed at 0.5; estimating true positive prior could improve accuracy but adds complexity.

- Failure signatures:
  - **Performance degradation with low expert ratios**: Table 2 shows RWS gains diminish at 10% expert data—classifier may lack sufficient positive signal.
  - **Unstable early training**: If Q-values are initially random, classifier learns meaningless mappings; expect noisy performance early.
  - **Over-weighting nearby goals**: If classifier converges to distance-only heuristic, long-horizon stitching may suffer.

- First 3 experiments:
  1. **Ablation on classifier input**: Replace Q-value features with raw (s, g, a) concatenation to verify that Q-values are the critical discriminative signal.
  2. **Varying η_p**: Sweep η_p ∈ {0.3, 0.5, 0.7} on a validation task to check sensitivity to class prior specification.
  3. **Two-stage vs. concurrent training**: Compare final performance and training time when pretraining Q to convergence before classifier training versus the default concurrent approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the weighted goal sampling framework of RWS be unified with contrastive GCRL frameworks?
- Basis in paper: [explicit] The conclusion states future work should "unify the weighted goal sampling framework with the contrastive GCRL frameworks."
- Why unresolved: RWS currently relies on Q-value based classifiers for reachability, whereas contrastive methods often utilize different representation spaces, making the theoretical and practical integration non-trivial.
- What evidence would resolve it: A successful implementation of RWS within a contrastive GCRL algorithm that demonstrates improved performance over standard contrastive baselines.

### Open Question 2
- Question: How effective is RWS in real-world scenarios where the offline dataset has limited coverage?
- Basis in paper: [explicit] The authors explicitly suggest exploring "its applicability in real-world scenarios where data coverage is limited" as a direction for future work.
- Why unresolved: The current experiments are conducted in simulated environments, which may not fully capture the noise, safety constraints, and distributional shifts present in physical robotics.
- What evidence would resolve it: Benchmark results showing RWS improving policy performance on physical robotic manipulation tasks using sparse or narrow offline datasets.

### Open Question 3
- Question: How sensitive is the reachability classifier to errors in the pre-trained goal-conditioned Q-value estimates?
- Basis in paper: [inferred] The paper notes that RWS "depends on reliable Q-value estimation," and prior works suggest offline Q-values can suffer from overestimation errors.
- Why unresolved: If the underlying Q-function provides inaccurate distances, the classifier might assign high reachability scores to unreachable goals, potentially degrading the sampling quality.
- What evidence would resolve it: An ablation study evaluating the classifier's accuracy and final policy performance when using Q-functions with varying degrees of artificial estimation error.

## Limitations

- Network architectures and hyperparameters are underspecified, requiring engineering judgment for reproduction
- The concurrent training approach may lead to classifier instability if Q-values are poorly estimated early in training
- The class prior η_p = 0.5 is fixed without validation of its appropriateness across tasks

## Confidence

- **High**: Reachability prioritization improves performance over uniform sampling (verified across six tasks)
- **Medium**: PU learning framework is necessary (no ablation on loss variants)
- **Low**: Exponential transformation is critical (no comparison to alternative weighting schemes)

## Next Checks

1. Ablate the non-negative PU loss against standard binary cross-entropy with negative sampling to isolate the importance of proper PU treatment
2. Compare concurrent classifier updates against two-stage training (pretrain Q, then train classifier) to assess stability tradeoffs
3. Test RWS on datasets with varying expert ratios below 10% to characterize the lower bound of positive data requirements