---
ver: rpa2
title: 'RAP: 3D Rasterization Augmented End-to-End Planning'
arxiv_id: '2510.04333'
source_url: https://arxiv.org/abs/2510.04333
tags:
- driving
- alignment
- training
- rasterization
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a lightweight 3D rasterization pipeline to
  augment imitation learning for end-to-end autonomous driving. Instead of relying
  on photorealistic rendering, it projects annotated geometric primitives (roads,
  vehicles, etc.) into camera views, enabling scalable synthetic data generation and
  recovery-oriented perturbations.
---

# RAP: 3D Rasterization Augmented End-to-End Planning

## Quick Facts
- arXiv ID: 2510.04333
- Source URL: https://arxiv.org/abs/2510.04333
- Authors: Lan Feng; Yang Gao; Eloi Zablocki; Quanyi Li; Wuyang Li; Sichao Liu; Matthieu Cord; Alexandre Alahi
- Reference count: 16
- First place on NA VSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, and Bench2Drive with PDMS 93.8 and EPDMS 36.9

## Executive Summary
This paper proposes RAP, a lightweight 3D rasterization pipeline that augments imitation learning for end-to-end autonomous driving. By projecting annotated geometric primitives into camera views, RAP enables scalable synthetic data generation without expensive photorealistic rendering. A feature-space alignment (Raster-to-Real) bridges rasterized and real inputs, preserving semantic fidelity while discarding appearance details. The framework improves closed-loop robustness and long-tail generalization, achieving state-of-the-art performance across four major benchmarks.

## Method Summary
RAP uses 3D rasterization of annotated primitives (cuboids for agents, polylines for lanes) projected to 2D camera views via pinhole projection with depth-aware compositing. Recovery-oriented perturbations generate counterfactual scenarios by offsetting logged trajectories. A two-level feature-space alignment (spatial MSE + global domain confusion) bridges raster and real inputs. The model uses a frozen DINOv3-H backbone with learnable MLP projector, trained with multi-modal trajectory heads and scoring heads, achieving superior performance while being more scalable than photorealistic rendering approaches.

## Key Results
- First place on four major benchmarks: NA VSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, and Bench2Drive
- Achieved PDMS score of 93.8 and EPDMS of 36.9
- Ablations confirm rasterization design, recovery perturbations, and feature alignment all contribute to performance
- Performance scales effectively with synthetic data, surpassing 100% real training when using 50% synthetic data

## Why This Works (Mechanism)

### Mechanism 1: Semantic-First Rendering for Scalable Training Data
- Claim: Photorealistic rendering is unnecessary for training end-to-end driving planners; geometric and semantic fidelity suffice.
- Mechanism: Replace expensive neural rendering with lightweight rasterization of annotated primitives projected to 2D camera views using standard pinhole projection with depth-aware compositing.
- Core assumption: Driving decisions depend primarily on geometry, semantics, and dynamics—not textures, lighting, or fine visual details.
- Evidence anchors: PCA visualization shows rasterized and real inputs share similar latent structures; limited direct corpus evidence supports semantic-only approaches over high-fidelity neural rendering.

### Mechanism 2: Recovery-Oriented Perturbation Training
- Claim: Generating counterfactual recovery scenarios via trajectory perturbation improves closed-loop robustness.
- Mechanism: Perturb logged ego trajectories with lateral/longitudinal offsets plus Gaussian noise, then re-render scenes from perturbed positions to expose the policy to recovery maneuvers absent from expert demonstrations.
- Core assumption: Imitation learning brittleness stems primarily from lack of recovery examples in training data.
- Evidence anchors: Perturbation significantly improves NA VSIM v2 EPDMS from 32.5 to 36.9 but has no effect on v1, suggesting context-dependent effectiveness.

### Mechanism 3: Feature-Space Domain Alignment (Raster-to-Real)
- Claim: Aligning rasterized and real inputs in feature space enables effective transfer without requiring pixel-level photorealism.
- Mechanism: Two-level alignment using spatial MSE loss and global domain classifier with gradient reversal layer enforces domain confusion across the full synthetic corpus.
- Core assumption: Semantic and geometric structures are more compact and easier to align in feature space than in pixel space.
- Evidence anchors: Real-to-Raster alignment outperforms alternatives (ADE 1.02 vs. 1.12 for Raster-to-Real); combining both spatial and global alignment yields strongest results across all real-data replacement ratios.

## Foundational Learning

- **Concept: Imitation Learning Covariate Shift**
  - Why needed here: The fundamental problem RAP addresses—IL policies trained only on expert demonstrations never encounter their own mistakes, causing failure cascades in closed-loop deployment.
  - Quick check question: Why does a policy trained exclusively on expert trajectories fail when it makes small positioning errors at test time?

- **Concept: Domain Adaptation via Gradient Reversal**
  - Why needed here: The R2R global alignment uses domain-adversarial training to make features indistinguishable between raster and real domains.
  - Quick check question: What does the gradient reversal layer do during backpropagation, and why does this encourage domain-invariant features?

- **Concept: 3D-to-2D Camera Projection**
  - Why needed here: Understanding how world-coordinate primitives project to image pixels via intrinsics K and extrinsics T is essential for implementing and debugging the rasterization pipeline.
  - Quick check question: Given a 3D point p_w and camera parameters (K, T_{w→c}), write the complete projection equation to obtain pixel coordinates (u, v).

## Architecture Onboarding

- **Component map:**
  1. Data Pipeline: nuPlan/Waymo logs → 3D annotations → Rasterization → Paired (real, raster) samples + perturbed trajectories + cross-agent views
  2. Visual Encoder: DINOv3-H backbone (frozen) + learnable MLP projector (~888M params for RAP-DINO; ~29M for RAP-ResNet)
  3. Alignment Module: Spatial MSE loss (λ_s=0.002) + Global domain classifier with GRL (λ_g=0.1)
  4. Planning Heads: Multi-modal trajectory head + trajectory scoring head (PDMS supervision)

- **Critical path:**
  Real image → Encoder → Features → [R2R alignment to raster features during training] → Planning heads → Future trajectory (4-5s)

- **Design tradeoffs:**
  - Rasterization fidelity: Colored faces + depth decay + black background (best ADE) vs. transparent faces / natural background (worse but potentially more realistic)
  - Alignment direction: Real-to-Raster (raster as clean scaffold, best ADE 1.02) vs. Raster-to-Real (preserve real detail, ADE 1.12) vs. Symmetric (ADE 1.14)
  - Model scale: RAP-DINO (888M, best benchmarks) vs. RAP-ResNet (29M, faster closed-loop inference on Bench2Drive)

- **Failure signatures:**
  - Rasterization: Clipping artifacts at view boundaries; depth ordering errors; missing unannotated objects
  - Alignment: If λ_s or λ_g too high, model may over-align and lose real-world visual cues needed for long-tail cases
  - Perturbation: Extreme offsets produce physically implausible scenarios; perturbation helps v2 but not v1

- **First 3 experiments:**
  1. Rasterization ablation: Train RAP-ResNet on navtrain with each rasterization variant (colored vs. transparent, depth decay on/off, black vs. natural background); measure MinADE on validation split.
  2. Alignment comparison: Fix real data at 50%, replace remainder with raster; compare Real-to-Raster vs. Raster-to-Real vs. Symmetric alignment; measure MinADE.
  3. Scaling curve: Start with 85k real samples; progressively add {1k, 10k, 100k, 500k, 1000k} cross-agent synthetic samples; plot MinADE vs. log(sample count) to verify scaling law.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RAP 3D rasterization pipeline be extended into a fully interactive simulator to support Reinforcement Learning (RL) training?
- Basis in paper: The conclusion states: "In future work, we aim to extend 3D rasterization into a full simulator to support closed-loop reinforcement learning, enabling richer interaction and policy improvement beyond offline demonstrations."
- Why unresolved: The current framework is restricted to the imitation learning paradigm, which suffers from causal confusion and cannot improve beyond the expert data via interaction.
- What evidence would resolve it: A demonstration of a policy trained entirely via RL within the rasterized environment, showing improved performance over the IL baseline in closed-loop evaluation.

### Open Question 2
- Question: Do adaptive or task-aware alignment schemes reduce information loss better than the current static Real-to-Raster alignment?
- Basis in paper: In Appendix A.1, regarding the potential loss of subtle visual cues, the authors state: "Future work may explore adaptive or task-aware alignment schemes to further reduce potential information loss."
- Why unresolved: The current static alignment weights might suppress unannotated visual details that are critical for specific rare scenarios (long-tail events).
- What evidence would resolve it: An ablation study comparing fixed alignment weights against a learned or task-dependent alignment mechanism, measuring performance on specific corner cases dependent on fine-grained visual features.

### Open Question 3
- Question: How robust is the pipeline to the quality of 3D annotations compared to the high-quality ground truth labels currently used?
- Basis in paper: The method relies on "annotated primitives" to render scenes. While the paper argues RAP is more scalable than photorealistic rendering, this advantage assumes the existence of accurate 3D annotations, which are expensive to obtain compared to raw video data used by Neural Rendering.
- Why unresolved: It is unclear if the "Raster-to-Real" alignment can handle the noise and artifacts introduced if the geometric primitives were derived from imperfect, automated perception systems rather than human-curated logs like nuPlan.
- What evidence would resolve it: An experiment training the model using rasterized inputs generated from off-the-shelf, noisy perception outputs (e.g., 3D detection/segmentation predictions) instead of ground truth annotations, evaluated on the standard benchmarks.

## Limitations

- The core assumption that photorealism is unnecessary for driving planning remains empirically unverified beyond specific benchmarks tested
- Perturbation mechanism shows inconsistent performance across benchmarks (significant gains on NA VSIM v2 but no improvement on v1)
- Limited testing on scenarios requiring fine visual discrimination (text on signs, subtle weather conditions, nuanced occlusions)

## Confidence

- **High confidence**: Benchmark performance (first place across four major datasets) is verifiable and well-documented; technical implementation details are sufficiently specified for reproduction
- **Medium confidence**: Claim that semantic fidelity suffices for driving decisions is supported by feature-space PCA visualizations and benchmark results, but lacks direct comparison to photorealistic alternatives
- **Low confidence**: Perturbation mechanism's inconsistent performance across benchmarks raises questions about its general applicability without understanding why it helps v2 but not v1

## Next Checks

1. **Edge Case Visual Dependency Test**: Evaluate RAP on scenarios requiring fine visual discrimination (construction zones with temporary signage, complex weather conditions, or dense urban areas with visual clutter) and compare against photorealistic rendering baselines to identify failure modes where semantic-only rendering underperforms.

2. **Perturbation Mechanism Dissection**: Conduct controlled experiments varying perturbation magnitude and distribution on both NA VSIM v1 and v2. Analyze whether specific ranges or types of perturbations consistently improve recovery behaviors, and identify the characteristics of scenarios where perturbation training is most/least effective.

3. **Cross-Domain Generalization**: Test RAP's performance when transferred to completely different driving environments (e.g., from North American urban settings to European rural roads, or from clear weather to monsoon conditions) to assess whether the semantic-first approach maintains robustness across domain shifts beyond the original training distribution.