---
ver: rpa2
title: 'LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs'
arxiv_id: '2506.08321'
source_url: https://arxiv.org/abs/2506.08321
tags:
- proof
- proofs
- theorem
- lean
- succ
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LeanTutor is an LLM-based tutoring system that verifies and guides
  undergraduate math proofs by translating natural language into Lean formal tactics,
  checking correctness via compilation, and generating pedagogically-driven feedback.
  It consists of three modules: an autoformalizer that step-by-step translates student
  proofs into Lean and detects errors, a next-step generator that searches for valid
  Lean tactics using LLM-directed proof search, and a natural language feedback generator
  that provides error identification, hints, and explicit next steps.'
---

# LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs

## Quick Facts
- **arXiv ID**: 2506.08321
- **Source URL**: https://arxiv.org/abs/2506.08321
- **Reference count**: 40
- **Primary result**: LLM-based tutoring system that verifies and guides undergraduate math proofs by translating natural language into Lean formal tactics, checking correctness via compilation, and generating pedagogically-driven feedback

## Executive Summary
LeanTutor is an LLM-based tutoring system that combines natural language processing with formal verification to guide students through mathematical proofs. The system translates student-written natural language proofs into Lean tactics, checks correctness through compilation, and generates pedagogically-driven feedback. By integrating the conversational strengths of LLMs with the formal rigor of theorem provers, LeanTutor aims to provide scalable, accurate proof tutoring. The system consists of three modules: an autoformalizer that translates natural language into Lean tactics, a next-step generator that searches for valid Lean tactics using LLM-directed proof search, and a natural language feedback generator that provides error identification, hints, and explicit next steps.

## Method Summary
LeanTutor employs a three-module architecture to guide students through mathematical proofs. The autoformalizer translates natural language proof steps into Lean tactics using a step-by-step approach, appending each generated tactic to previous steps and compiling them to detect errors. The next-step generator uses LLM-directed depth-first search to find valid Lean tactics when the student's proof is incorrect, generating 12 candidates ranked by likelihood and filtering through compilation and progress checks. The feedback generator creates natural language responses by grounding its output in formal artifacts including the autoformalized proof, compiler error messages, and verified next tactics. The system was evaluated on PeanoBench, a dataset of 371 human-annotated Peano Arithmetic proofs, demonstrating its ability to correctly autoformalize 57% of tactics in correct proofs and accurately identify incorrect steps in 30% of incorrect proofs.

## Key Results
- Correctly autoformalizes 57% of tactics in correct proofs and accurately identifies incorrect steps in 30% of incorrect proofs
- Outperforms simple baseline on accuracy (3.7 vs 2.6) and relevance (3.6 vs 2.7) metrics for natural language hints
- Step-by-step autoformalization approach outperforms whole-proof formalization by 8 percentage points on incorrect proofs (30.1% vs 21.9%)
- Generates valid next steps for incorrect proofs through LLM-directed proof search with compilation-based filtering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-by-step autoformalization with immediate compilation provides more precise error localization than whole-proof formalization, particularly for incorrect proofs.
- Mechanism: Each natural language proof step is translated to a Lean tactic, appended to previous steps, and compiled. Compilation errors (excluding "unsolved goals") signal incorrect steps, enabling the system to pinpoint exactly where a student went wrong. The step-by-step approach outperformed whole-proof formalization on incorrect proofs by 8 percentage points (30.1% vs 21.9%).
- Core assumption: One-to-one correspondence exists between natural language proof steps and Lean tactics; student errors manifest as Lean compilation errors rather than semantic mismatches that still compile.
- Evidence anchors:
  - [abstract] "correctly formalizes 57% of tactics in correct proofs and accurately identifies the incorrect step in 30% of incorrect proofs"
  - [section 5.2, Table 1] Shows Baseline + Staff Solution (step-by-step) achieves 30.1% on incorrect proofs vs 21.9% for whole-proof approach
  - [corpus] Related work (ProofBridge, Hilbert) focuses on autoformalization but not specifically on error localization in tutoring contexts
- Break condition: When student natural language is ambiguous enough that multiple valid formalizations exist with different semantic meanings, or when errors are conceptual (wrong proof strategy) rather than syntactic/tactical.

### Mechanism 2
- Claim: LLM-directed depth-first search with compilation-based filtering and progress checks can generate valid next steps even when the student's partial proof is incorrect.
- Mechanism: The next-step generator prompts an LLM to produce 12 candidate tactics ranked by likelihood. Each candidate is compiled, filtered through progress checks (forbidden theorem list, cycle detection), and surviving tactics form a proof-search tree explored via DFS (depth bounded at 8). Successful compilation validates correctness without requiring ground truth.
- Core assumption: The theorem space is bounded and small (relative to Mathlib), and at least one valid proof exists; the LLM can generate syntactically valid Lean tactics even if semantically wrong, allowing the compiler to filter.
- Evidence anchors:
  - [abstract] "outputs a valid next Lean tactic for incorrect proofs via LLM-based candidate generation and proof search"
  - [section 4.2] Describes 12 candidate generation, compilation filtering, progress check (forbidden theorems + cycle avoidance), DFS with depth ≤8
  - [corpus] COPRA (Thakur et al. 2023) and LeanProgress provide precedent for LLM-directed proof search; corpus lacks direct tutoring benchmarks
- Break condition: When the student's proof approach is fundamentally incompatible with any valid proof path (e.g., wrong induction variable with no recovery), or when all 12 candidates fail compilation/progress checks within the depth bound.

### Mechanism 3
- Claim: Grounding natural language feedback generation in formal artifacts (autoformalized proof, compiler error, verified next step) improves accuracy and relevance compared to ungrounded LLM prompting.
- Mechanism: The feedback generator receives three formal inputs: (1) the autoformalized partial proof, (2) the Lean compiler error message, and (3) a verified next tactic from the NSG module. These constrain the LLM's output space, reducing hallucinations. The prompt includes six common inductive proof error categories to improve error classification. Evaluated against a baseline (LLM with only NL proof), LeanTutor scored 3.7 vs 2.6 on accuracy and 3.6 vs 2.7 on relevance (5-point scale).
- Core assumption: Compiler error messages and verified next steps provide pedagogically useful signals; the mapping from formal errors to student-understandable explanations is learnable by an LLM.
- Evidence anchors:
  - [abstract] "outperforms a simple baseline on accuracy (3.7 vs 2.6) and relevance (3.6 vs 2.7) metrics"
  - [section 5.4, Table 2] Full comparison across error identification, hint/question, and next-step feedback types
  - [corpus] Limited corpus evidence on formal-to-informal feedback translation; mostly unexplored in related work
- Break condition: When compiler errors are cryptic or unrelated to the conceptual mistake (e.g., type mismatch from incorrect autoformalization), or when the verified next step is valid but pedagogically inappropriate (e.g., reveals too much).

## Foundational Learning

- **Lean 4 tactic proofs and compilation semantics**
  - Why needed here: The entire system hinges on understanding what Lean tactics do, how they transform proof states, and what compilation errors mean. Without this, you cannot debug the autoformalizer or interpret error signals.
  - Quick check question: Given a Lean proof state with goal `⊢ a + b = b + a`, what does the tactic `rw [add_comm]` do, and what error would result if `add_comm` were not in scope?

- **Autoformalization as semantic preservation**
  - Why needed here: The "faithful autoformalization" metric assumes semantic equivalence between NL and FL. Understanding the gap between syntactic matching and semantic preservation is critical for improving the relaxed exact match metric.
  - Quick check question: Two tactics `rw [add_comm a b]` and `rw [add_comm]` may produce identical proof states but differ syntactically. How does the paper's metric handle this, and what are its limitations?

- **LLM prompting for constrained generation**
  - Why needed here: All three modules use carefully designed prompts with in-context examples, dictionaries, and reference solutions. Understanding few-shot prompting and output formatting constraints (e.g., JSON-only responses) is essential for reproducing or extending the system.
  - Quick check question: The autoformalizer prompt includes 5-shot examples, a tactic dictionary, and a staff solution. Which of these most improves performance on incorrect proofs, and how would you design an ablation to test this?

## Architecture Onboarding

- **Component map:**
  Student NL Input → [Autoformalizer] ← Staff Solution, Tactic/Theorem Dict, 5-shot Examples → Lean Tactic (step-by-step) → [Lean Compiler via LeanInteract] → If error → [Next-Step Generator] ← Tactic list for current world → Valid Next Tactic → [Feedback Generator] ← Compiler Error + Next Tactic + Error Categories → NL Feedback (Error ID, Hint, Next Step)

- **Critical path:** Autoformalizer accuracy is the bottleneck—30% incorrect-proof detection means 70% of errors go undetected or are mislocalized. Improving the autoformalizer (better prompts, more training data, or hybrid neural-symbolic approaches) would cascade to better next-step generation and feedback.

- **Design tradeoffs:**
  - Step-by-step vs whole-proof formalization: Step-by-step enables error localization but assumes 1:1 NL-to-tactic mapping; whole-proof is more robust to granularity mismatches but loses precision.
  - Small dataset assumption: Enables staff-solution retrieval and bounded proof search but limits scalability to broader curricula.
  - Compiler-as-ground-truth: Fast and verifiable but cannot detect conceptual errors that compile (e.g., proving a weaker statement).

- **Failure signatures:**
  - Autoformalizer hallucinates tactics that compile but don't match student intent (false positive correctness)
  - NSG exhausts depth bound without finding proof (report "no appropriate next tactic")
  - Feedback generator reveals answer prematurely (low answer leakage score for next-step feedback)

- **First 3 experiments:**
  1. **Autoformalizer ablation:** Remove staff solution from prompt and measure tactic-level accuracy drop. The paper shows this reduces correct-tactic accuracy from 56.8% to 32.9%. Reproduce and test whether the gap is larger for certain proof worlds (e.g., Advanced Multiplication vs Tutorial).
  2. **NSG candidate scaling:** Vary the number of LLM-generated candidates (6, 12, 24) and measure success rate and latency. Hypothesis: Diminishing returns beyond 12 given the bounded theorem space.
  3. **Feedback generator grounding ablation:** Provide the feedback generator with only the NL proof (baseline) vs. NL proof + compiler error vs. full LeanTutor inputs. Measure accuracy/relevance scores to quantify the value of each formal signal.

## Open Questions the Paper Calls Out
None

## Limitations
- System's reliance on a small, curated dataset (371 proofs) and bounded theorem space raises questions about scalability to broader mathematical domains
- 57% autoformalization accuracy on correct proofs and 30% error detection on incorrect proofs indicate significant room for improvement
- Evaluation focuses on Peano Arithmetic proofs, which may not generalize to more complex mathematical domains
- Compiler-based correctness checking cannot detect conceptual errors that compile but prove weaker statements than intended

## Confidence

- **High Confidence**: The basic architecture combining autoformalization, proof search, and feedback generation is technically sound and well-implemented. The evaluation methodology using compilation errors for correctness checking is valid within the paper's scope.
- **Medium Confidence**: The claims about pedagogical effectiveness (accuracy/relevance scores of 3.7/3.6 vs 2.6/2.7) are supported by the evaluation but may not generalize to untrained proof types or broader mathematical domains.
- **Low Confidence**: The scalability claims to other mathematical domains are largely untested. The assumption that 12 LLM candidates suffice for proof search in larger theorem spaces is speculative.

## Next Checks

1. **Dataset Generalization Test**: Evaluate LeanTutor on an unseen subset of PeanoBench proofs or a different formal proof dataset to assess generalization beyond the training distribution.
2. **Conceptual Error Detection**: Design a test set where proofs compile but prove weaker statements, and measure LeanTutor's ability to detect these semantic errors versus pure compilation errors.
3. **Scalability Analysis**: Implement LeanTutor on a larger theorem space (e.g., extending beyond the provided dictionary) and measure the impact on proof search success rates and runtime performance.