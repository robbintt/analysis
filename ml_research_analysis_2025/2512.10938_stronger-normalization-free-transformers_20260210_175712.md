---
ver: rpa2
title: Stronger Normalization-Free Transformers
arxiv_id: '2512.10938'
source_url: https://arxiv.org/abs/2512.10938
tags:
- normalization
- functions
- derf
- training
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces Dynamic erf (Derf), a point-wise function\
  \ that serves as a normalization-free replacement in Transformers. The authors first\
  \ analyze key functional properties\u2014zero-centeredness, boundedness, center\
  \ sensitivity, and monotonicity\u2014to guide the search for effective point-wise\
  \ function designs."
---

# Stronger Normalization-Free Transformers

## Quick Facts
- arXiv ID: 2512.10938
- Source URL: https://arxiv.org/abs/2512.10938
- Reference count: 32
- Primary result: Dynamic erf (Derf) consistently outperforms LayerNorm and RMSNorm across vision, speech, and DNA domains while improving generalization

## Executive Summary
This work introduces Dynamic erf (Derf), a point-wise function designed to replace normalization layers in Transformers. The authors analyze four key functional properties—zero-centeredness, boundedness, center sensitivity, and monotonicity—to guide the search for effective point-wise function designs. Through empirical search, Derf (erf(αx + s)) emerges as the most performant, consistently outperforming LayerNorm, RMSNorm, and Dynamic Tanh across diverse domains including vision (ImageNet classification and generation), speech (wav2vec 2.0 pretraining), and DNA sequence modeling. Notably, Derf achieves these gains through improved generalization rather than stronger fitting capacity, as evidenced by higher training loss in evaluation mode compared to normalization-based models.

## Method Summary
The paper proposes Derf as a normalization-free replacement for LayerNorm and RMSNorm in Transformers. Derf(x) = γ·erf(αx + s) + β, where γ and β are learnable per-channel parameters, while α and s are scalar hyperparameters. The authors systematically analyze functional properties needed for effective point-wise functions and conduct empirical search across common activation functions and their variants. Derf replaces all normalization layers (pre-attention, pre-FFN, and final) in Transformer architectures. The implementation is straightforward, requiring only standard erf function availability (e.g., torch.special.erf).

## Key Results
- Derf achieves 82.8% top-1 accuracy on ImageNet-1K with ViT-Base, matching LayerNorm performance
- On vision generation tasks (DiT), Derf reaches 12.4 FID compared to 13.8 for LayerNorm
- In DNA sequence modeling (Caduceus), Derf achieves 0.665 validation loss versus 0.713 for LayerNorm
- Across all domains, Derf shows higher training loss in evaluation mode, suggesting improved generalization

## Why This Works (Mechanism)
Derf works by providing bounded, zero-centered activations with center sensitivity and monotonicity, which helps stabilize training while allowing sufficient expressivity. The erf function's properties ensure activations remain within a controlled range while maintaining differentiability. The learnable parameters γ and β provide per-channel scaling and shifting, while α and s control the input transformation's steepness and bias. This combination appears to provide better generalization than learned normalization statistics, as evidenced by the higher training loss but better or comparable validation performance.

## Foundational Learning
- **Transformer architecture fundamentals**: Why needed - Understanding how Derf replaces normalization layers in standard Transformer blocks. Quick check - Can identify where LayerNorm appears in ViT and what Derf substitutes for.
- **Activation function properties**: Why needed - The four properties (zero-centeredness, boundedness, center sensitivity, monotonicity) guide Derf's design. Quick check - Can explain why each property matters for stable training.
- **Error function (erf) behavior**: Why needed - erf is the core of Derf's functionality. Quick check - Can describe erf's output range and how it's bounded between -1 and 1.
- **Normalization-free training**: Why needed - Understanding the motivation for alternatives to LayerNorm. Quick check - Can list benefits and drawbacks of removing normalization layers.
- **Empirical hyperparameter search**: Why needed - Derf's parameters (α=0.5, s=0) emerged from search rather than theory. Quick check - Can explain why empirical tuning was necessary.

## Architecture Onboarding
- **Component map**: Input -> Linear Projection -> Derf -> Attention/FFN -> Output
- **Critical path**: The Derf layer sits immediately before attention and feed-forward sublayers, controlling activation scale and distribution
- **Design tradeoffs**: Derf trades learned normalization statistics for fixed functional properties, potentially reducing overfitting but requiring careful initialization
- **Failure signatures**: Training divergence indicates violated functional properties; poor performance suggests suboptimal α/s values; instability in specific domains may require precision adjustments
- **First experiments**: 1) Implement Derf layer and verify erf functionality, 2) Train ViT-Base with Derf on ImageNet-1K to match LayerNorm accuracy, 3) Run ablations comparing Derf vs LayerNorm vs Dynamic Tanh on same task

## Open Questions the Paper Calls Out
**Open Question 1**: Why does Derf only match LayerNorm performance on language modeling (GPT-2) while surpassing it consistently on vision, speech, and DNA tasks? The paper doesn't investigate why language modeling exhibits different behavior from other modalities, though evidence from Table 12 shows Derf achieves 2.94 validation loss on GPT-2, matching LN and only outperforming DyT.

**Open Question 2**: What is the theoretical mechanism by which Derf improves generalization relative to normalization layers? The authors state this hypothesis but provide only speculation that limited parameter adaptability constrains overfitting, supported only by empirical training loss comparisons without theoretical analysis.

**Open Question 3**: Does an optimal point-wise function exist beyond Derf, and can it be discovered through principled rather than empirical search? The paper acknowledges that a comprehensive analysis of the design space for statistics-free operators remains missing, and Derf emerged as best among tested candidates but optimality is not proven.

## Limitations
- Derf's success emerged from empirical search rather than rigorous theoretical justification, potentially limiting understanding of when it will generalize
- The analysis focuses on fixed hyperparameters without exploring task-specific tuning, which may limit performance in specialized domains
- The claim of improved generalization is based on training loss comparisons that could reflect optimization difficulties rather than inherent capacity differences
- The paper doesn't thoroughly investigate edge cases such as extremely deep networks or tasks requiring precise normalization for stability

## Confidence
- **High confidence**: Derf consistently outperforms LayerNorm and Dynamic Tanh in the tested architectures and datasets
- **Medium confidence**: The proposed functional properties are necessary but may not be sufficient to guarantee superior performance
- **Medium confidence**: The interpretation that improved generalization stems from weaker fitting rather than stronger regularization

## Next Checks
1. Test Derf across a wider range of α and s initializations to determine sensitivity to hyperparameter choice and whether optimal values vary by task domain
2. Compare Derf against alternative normalization-free designs (GeLUs, SiLU variants) on the same tasks to establish relative performance in the broader normalization-free landscape
3. Conduct ablation studies removing individual functional properties (e.g., boundedness via modified erf) to quantify each property's contribution to Derf's success