---
ver: rpa2
title: Interactive Evaluation of Large Language Models for Multi-Requirement Software
  Engineering Tasks
arxiv_id: '2508.18905'
source_url: https://arxiv.org/abs/2508.18905
tags:
- evaluation
- problem
- solution
- figure
- requirements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduced an interactive evaluation framework for assessing
  large language models on multi-requirement software engineering tasks. The framework
  uses requirement dependency graphs and iterative feedback loops, where an interviewer
  model provides targeted hints to an interviewee model based on ground-truth solutions.
---

# Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks

## Quick Facts
- **arXiv ID**: 2508.18905
- **Source URL**: https://arxiv.org/abs/2508.18905
- **Reference count**: 40
- **Primary result**: Interactive evaluation framework with ground-truth-informed hints enables significant error recovery in multi-requirement software engineering tasks

## Executive Summary
This study introduces an interactive evaluation framework for assessing large language models on complex software engineering tasks with multiple interdependent requirements. The framework models tasks as directed acyclic graphs of requirements, where an interviewer model provides targeted hints to an interviewee model based on ground-truth solutions. Tested on 55 programming tasks from the DevAI benchmark, the approach reveals that traditional static benchmarks systematically underestimate LLM capabilities, particularly for tasks requiring error recovery and iterative refinement. The framework demonstrates that model performance under interactive evaluation does not align with traditional benchmark rankings, highlighting the importance of dynamic assessment for real-world software engineering applications.

## Method Summary
The framework implements an interactive evaluation loop where an "interviewer" LLM generates minimal, targeted hints based on ground-truth solutions, and an "interviewee" LLM produces and iteratively refines code solutions. Tasks are decomposed into requirement dependency graphs, with a judge LLM evaluating requirement satisfaction after each iteration. The system segments code into chunks, matches them to requirements using semantic similarity, and provides feedback until all requirements are satisfied or maximum iterations are reached. Ground-truth solutions were constructed by the authors for the DevAI benchmark's 55 tasks, as reference solutions were not provided.

## Key Results
- Static benchmarks systematically underestimate LLM capabilities, particularly for error recovery in later task stages
- Interactive evaluation reveals performance divergence from traditional benchmark rankings
- All models showed most improvement in "Dataset or Environment" category, suggesting hints resolve configuration ambiguities
- GPT-4.1-mini's performance degraded relative to GPT-4o-mini under interactive evaluation despite superior static scores
- Ground-truth satisfaction averaged 92.6%, indicating incomplete reference solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dependency-aware scoring isolates recoverable failures from fundamental capability gaps
- Mechanism: Tasks are modeled as DAGs where requirement r_j is only evaluable if all prerequisites P(r_j) are satisfied
- Core assumption: Requirements can be cleanly decomposed into verifiable subtasks with stable dependency ordering
- Evidence anchors:
  - [abstract] "Each task is modeled as a requirement dependency graph"
  - [section: Problem Formulation] Equation (1) defines dependency-aware score S_G using the indicator function on parent satisfaction
  - [corpus] LoCoBench-Agent (arxiv:2511.13998) confirms single-turn evaluation fails to capture long-context, multi-step SE workflows

### Mechanism 2
- Claim: Ground-truth-informed hints enable error recovery that static benchmarks obscure
- Mechanism: An interviewer LLM with access to S* generates minimal, incremental hints targeting failed requirements
- Core assumption: Hint quality depends on ground-truth correctness and interviewer's ability to generate task-relevant guidance without over-revealing
- Evidence anchors:
  - [abstract] "an 'interviewer' LLM, aware of the ground-truth solution, provides minimal, targeted hints"
  - [section: Interactive Evaluation] "hints target specific failed requirements while preserving as much of the model's original reasoning as possible"
  - [corpus] No direct corpus validation of ground-truth-based hinting

### Mechanism 3
- Claim: Instruction-following capacity predicts interactive performance better than static coding benchmarks
- Mechanism: Models with strong instruction-following capabilities (e.g., o4-mini) leverage hints to overcome initial implementation errors
- Core assumption: The divergence between static benchmark rankings and interactive scores reflects genuine capability differences
- Evidence anchors:
  - [section: Evaluation] "GPT-4.1-mini's performance degrades relative to GPT-4o-mini" under interactive evaluation
  - [section: Evaluation] Figure 10 shows all models improve most in "Dataset or Environment" category
  - [corpus] CodeIF-Bench (arxiv:2503.22688) independently validates that instruction-following in multi-turn code generation is under-assessed

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) for task decomposition
  - Why needed here: The entire evaluation framework relies on modeling requirements as DAG vertices with edges encoding dependencies
  - Quick check question: Given requirements R1 (load data) → R2 (preprocess) → R3 (train model), what is the evaluation status of R3 if R1 fails?

- Concept: Multi-turn dialogue state management
  - Why needed here: The interactive evaluation maintains state across iterations (current solution S^(t), hint history H^(t), execution outputs O^(t))
  - Quick check question: If an interviewee model ignores hints from iteration 1 in its iteration 2 response, what does this indicate about its feedback integration capability?

- Concept: Agent-as-a-Judge evaluation paradigm
  - Why needed here: The framework uses LLM-based judges to verify requirement satisfaction, inheriting limitations from this approach
  - Quick check question: Why might an LLM judge incorrectly mark a correct solution as unsatisfied, and how does this affect hint quality?

## Architecture Onboarding

- Component map: Interviewer LLM -> Interviewee LLM -> Judge LLM -> Sandbox executor -> Report generator

- Critical path:
  1. Task ingestion → requirement extraction → DAG construction
  2. Initial solution generation → segment into chunks → embed requirements and chunks via sentence encoder f_enc
  3. For each requirement: retrieve most similar chunk via cosine similarity → LLM classifier assesses satisfaction given parent status
  4. If failures exist: interviewer generates minimal hints → interviewee revises solution → repeat until max iterations or full satisfaction
  5. Generate post-evaluation report from trajectory {(S^(t), H^(t))}

- Design tradeoffs:
  - Hint specificity vs. assessment validity: Overly specific hints risk solution revelation
  - Judge model capability vs. cost: More capable judges reduce misclassification but increase evaluation cost
  - Maximum iterations vs. evaluation completeness: Higher iteration limits enable more recovery but extend evaluation time

- Failure signatures:
  - Ground-truth satisfaction <100%: Indicates reference solution gaps
  - Hint quality score clustering near baseline (3/5): Suggests hints are only moderately effective
  - Guided variant underperforming unguided: Indicates interviewee cannot effectively incorporate feedback

- First 3 experiments:
  1. Reproduce single-task interactive evaluation with different interviewer models to isolate interviewer quality effects
  2. Measure requirement satisfaction at each iteration for a high-performing model (o4-mini) vs. degraded model (GPT-3.5-turbo)
  3. Ablate ground-truth availability by providing interviewer with only requirements (no S*) to measure hint quality degradation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can an interactive evaluation framework optimally balance hint specificity to avoid revealing solutions while ensuring meaningful error recovery?
- **Basis in paper:** [explicit] The authors state in the Conclusion that "the framework must carefully balance guidance intensity: overly specific hints risk revealing solutions, while vague suggestions may fail to trigger meaningful improvements"
- **Why unresolved:** The current study implemented a specific prompting strategy but did not quantify the threshold where guidance becomes "solution leakage" versus "useless noise"
- **What evidence would resolve it:** Ablation studies varying hint granularity and correlating them with success rates and solution originality metrics

### Open Question 2
- **Question:** To what extent does the choice of "interviewer" model introduce bias or unfairness when comparing different "interviewee" models?
- **Basis in paper:** [explicit] The paper notes that "variability in automated feedback generation introduces a subtle but important fairness consideration in model comparisons"
- **Why unresolved:** The experiments relied primarily on specific interviewer models, and it is unclear if certain interviewees respond better to hints generated by specific model families
- **What evidence would resolve it:** A cross-evaluation study using diverse interviewer architectures to assess the same interviewee, measuring the variance in interviewee performance scores

### Open Question 3
- **Question:** What specific architectural or training factors enable models with lower static benchmark scores (like o4-mini) to outperform higher-scoring models in interactive, feedback-driven settings?
- **Basis in paper:** [explicit] The conclusion highlights that "findings challenge the prevailing assumption that benchmark performance directly translates to interactive settings," noting that o4-mini surpassed others through "superior instruction follow-up capacity"
- **Why unresolved:** While the paper identifies this divergence, it does not isolate whether this is caused by context window size, specific tuning for instruction following, or distinct reasoning mechanisms
- **What evidence would resolve it:** Controlled comparisons of model variants differing only in instruction tuning or context length on the interactive DevAI benchmark

## Limitations

- Ground-truth solutions are incomplete (92.6% satisfaction) and could systematically bias hint generation
- Iterative evaluation introduces context length constraints, with smaller models failing to retain feedback across iterations
- Evaluation cost scales linearly with task complexity and iteration count, creating practical barriers for widespread adoption

## Confidence

- **High confidence**: The core mechanism of dependency-aware scoring is well-specified with clear equations and implementation details
- **Medium confidence**: The claim that instruction-following capacity predicts interactive performance is supported by comparative results but lacks ablation studies
- **Low confidence**: The generalizability of hint quality scores across different interviewer models, as expert annotations show high variance without clear explanation of scoring criteria

## Next Checks

1. **Ablation study**: Run interactive evaluation with interviewer models that have access only to requirements (no ground-truth) to quantify the exact contribution of ground-truth information to hint quality
2. **Cross-benchmark validation**: Apply the framework to independent multi-turn SE benchmarks (SWE-Arena, MLE-Dojo) to test whether the observed divergence between static and interactive performance generalizes beyond DevAI
3. **Context retention analysis**: For each interviewee model, measure the edit distance between consecutive solutions to quantify how effectively models incorporate feedback versus regenerating solutions from scratch