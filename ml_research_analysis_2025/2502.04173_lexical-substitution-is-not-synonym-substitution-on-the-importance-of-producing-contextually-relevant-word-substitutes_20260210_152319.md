---
ver: rpa2
title: 'Lexical Substitution is not Synonym Substitution: On the Importance of Producing
  Contextually Relevant Word Substitutes'
arxiv_id: '2502.04173'
source_url: https://arxiv.org/abs/2502.04173
tags:
- word
- substitutes
- gold
- lexical
- substitution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConCat, a simple lexical substitution approach
  that concatenates masked and original sentences to improve contextual relevance
  of substitutes. Evaluated on three benchmarks (LS07, CoInCo, Swords), ConCat outperforms
  existing methods on coverage metrics and is preferred by human annotators in a survey.
---

# Lexical Substitution is not Synonym Substitution: On the Importance of Producing Contextually Relevant Word Substitutes

## Quick Facts
- arXiv ID: 2502.04173
- Source URL: https://arxiv.org/abs/2502.04173
- Reference count: 4
- Primary result: ConCat outperforms baselines on coverage metrics and human preference surveys for the tested benchmarks (LS07, CoInCo, Swords)

## Executive Summary
This paper introduces ConCat, a simple lexical substitution approach that concatenates masked and original sentences to improve contextual relevance of substitutes. Evaluated on three benchmarks (LS07, CoInCo, Swords), ConCat outperforms existing methods on coverage metrics and is preferred by human annotators in a survey. When used for text classification, ConCat preserves semantic similarity better than the baseline. Analysis of gold labels reveals potential pitfalls in current benchmarks, highlighting the importance of contextual appropriateness over synonymy in lexical substitution.

## Method Summary
ConCat is a lexical substitution approach that concatenates masked and original sentences to improve contextual relevance of substitutes. The method generates word substitutes by leveraging both the masked context (where the target word is removed) and the original sentence containing the target word. This dual-context approach aims to produce substitutes that are not only semantically similar but also contextually appropriate within the specific sentence. The concatenated input is then processed by a pre-trained language model to generate candidate substitutes, which are ranked based on their contextual fit.

## Key Results
- ConCat outperforms existing methods on coverage metrics across three benchmarks (LS07, CoInCo, Swords)
- Human annotators prefer ConCat's substitutes over baseline methods in a survey
- ConCat better preserves semantic similarity in text classification tasks compared to the baseline

## Why This Works (Mechanism)
ConCat works by providing the language model with both the masked context (where the target word is removed) and the original sentence containing the target word. This dual-context approach allows the model to consider both the broader context and the specific usage of the target word, resulting in substitutes that are more contextually relevant. By concatenating these two inputs, ConCat effectively guides the model to generate substitutes that fit well within the specific sentence while maintaining semantic similarity.

## Foundational Learning

### Contextual Language Models
**Why needed:** Understanding how pre-trained models process and generate text based on context is crucial for developing effective lexical substitution methods.
**Quick check:** Can you explain how masked language models like BERT generate contextually relevant word predictions?

### Lexical Substitution
**Why needed:** Lexical substitution involves replacing words with semantically similar alternatives while preserving the original meaning and context.
**Quick check:** What are the key challenges in producing contextually appropriate word substitutes?

### Semantic Similarity vs. Contextual Relevance
**Why needed:** Recognizing the distinction between semantic similarity and contextual appropriateness is crucial for developing effective lexical substitution methods.
**Quick check:** Can you provide an example where a semantically similar word is not contextually appropriate in a given sentence?

## Architecture Onboarding

### Component Map
Pre-trained Language Model -> ConCat Concatenation -> Candidate Generation -> Ranking

### Critical Path
1. Input sentence and target word are identified
2. Masked sentence (target word removed) and original sentence are concatenated
3. Concatenated input is fed into pre-trained language model
4. Language model generates candidate substitutes
5. Candidates are ranked based on contextual fit and semantic similarity

### Design Tradeoffs
- Simplicity vs. Complexity: ConCat uses a simple concatenation approach rather than more complex architectures
- Computational Efficiency: The method leverages pre-trained models, reducing training requirements
- Coverage vs. Precision: Balancing the number of candidate substitutes with their quality and relevance

### Failure Signatures
- Inability to handle rare or domain-specific words effectively
- Potential bias towards more common substitutes
- Limited performance on out-of-distribution data or different linguistic phenomena

### First 3 Experiments
1. Evaluate ConCat on benchmark datasets (LS07, CoInCo, Swords) and compare coverage metrics with baseline methods
2. Conduct human preference surveys to assess the quality of generated substitutes
3. Test ConCat's performance in a text classification task to evaluate semantic preservation

## Open Questions the Paper Calls Out

## Limitations
- Evaluation focuses primarily on coverage and human preference metrics, with limited analysis of semantic preservation across diverse linguistic phenomena
- The ConCat approach may not capture more complex contextual dependencies or handle rare words effectively
- Human preference survey involved a relatively small annotator pool, potentially introducing sampling bias
- Text classification experiments demonstrate semantic similarity preservation but do not explore broader downstream task performance

## Confidence
- **High confidence**: ConCat outperforms baselines on coverage metrics and human preference surveys for the tested benchmarks (LS07, CoInCo, Swords)
- **Medium confidence**: ConCat better preserves semantic similarity in text classification tasks compared to the baseline
- **Medium confidence**: Current benchmarks may have limitations in evaluating contextual relevance of lexical substitutions

## Next Checks
1. Conduct a larger-scale human evaluation with diverse annotators and example types to validate the generalizability of preference results
2. Test ConCat on additional downstream tasks beyond text classification (e.g., machine translation, question answering) to assess broader applicability
3. Evaluate ConCat's performance on out-of-distribution data, including rare words and domain-specific terminology, to identify potential coverage gaps