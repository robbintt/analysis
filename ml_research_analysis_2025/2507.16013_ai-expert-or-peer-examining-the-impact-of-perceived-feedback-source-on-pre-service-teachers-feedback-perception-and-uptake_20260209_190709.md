---
ver: rpa2
title: AI, Expert or Peer? -- Examining the Impact of Perceived Feedback Source on
  Pre-Service Teachers Feedback Perception and Uptake
arxiv_id: '2507.16013'
source_url: https://arxiv.org/abs/2507.16013
tags:
- feedback
- source
- quality
- perceptions
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how pre-service teachers' perceptions of
  feedback sources (AI, expert, or peer) influence their feedback perception and behavioral
  uptake. In a randomized experiment with 273 pre-service teachers, participants received
  written feedback on a mathematics learning goal, identified its source, rated perceptions
  across five dimensions, and revised the learning goal.
---

# AI, Expert or Peer? -- Examining the Impact of Perceived Feedback Source on Pre-Service Teachers Feedback Perception and Uptake

## Quick Facts
- arXiv ID: 2507.16013
- Source URL: https://arxiv.org/abs/2507.16013
- Reference count: 0
- Pre-service teachers perceived LLM-generated feedback as most fair and useful, leading to highest uptake (52%)

## Executive Summary
This randomized experiment with 273 pre-service teachers investigated how perceived feedback source (AI, expert, or peer) influences feedback perceptions and behavioral uptake. Participants received identical feedback on a mathematics learning goal, identified its source, rated perceptions across five dimensions, and revised the learning goal. Results showed that LLM-generated feedback was perceived as most fair and useful, leading to highest uptake (52%). Recognition accuracy significantly moderated source effects, with particularly positive evaluations when LLM feedback was falsely ascribed to experts. Higher-quality feedback was consistently assigned to experts, indicating an expertise heuristic in source judgments. Only feedback quality significantly predicted feedback uptake.

## Method Summary
The study employed a randomized experimental design with 273 German pre-service teachers who received written feedback on a mathematics learning goal. Participants were randomly assigned to one of three feedback sources (LLM, expert, or peer) and then identified the source, completed a 18-item Feedback Perception Questionnaire (FPQ) measuring six dimensions, and revised the learning goal. Analysis used linear regression for perceptions and logistic regression for uptake, with logit transformation for bounded outcomes. LLM feedback was generated using ChatGPT-4 with specified prompts, and expert/peer feedback was created by human raters following the same guidelines.

## Key Results
- LLM-generated feedback achieved highest uptake (52%) when correctly identified
- Recognition accuracy was near-chance level (36%), with peer feedback often misidentified as LLM
- Only feedback quality significantly predicted uptake (b = 1.27, p = .012), not source or recognition accuracy
- Misidentifying LLM feedback as expert led to more favorable fairness (b = 1.21, p = .042) and usefulness ratings (b = 1.76, p = .009)

## Why This Works (Mechanism)

### Mechanism 1: Expertise Heuristic in Source Attribution
- Claim: Pre-service teachers use perceived source expertise as a cognitive shortcut to evaluate feedback quality, rating identical feedback higher when they believe it comes from experts.
- Mechanism: When students receive feedback, they engage System 1 (fast, intuitive) processing first. The assumed source acts as a framing cue—feedback believed to come from experts is automatically rated higher on fairness and usefulness dimensions, regardless of actual source. This operates through social attribution (inferring provider motives and expertise) before analytical content assessment.
- Core assumption: The expertise heuristic operates automatically and largely unconsciously, influencing early cognitive appraisal before System 2 analytical processing engages.
- Evidence anchors:
  - "Higher-quality feedback was consistently assigned to experts, indicating an expertise heuristic in source judgments"
  - "LLM-generated feedback was evaluated more favorably—particularly in terms of fairness and usefulness—when participants perceived the source as human rather than as an LLM" (b = 1.21, p = .042 for fairness; b = 1.76, p = .009 for usefulness)
  - Related work (Zhang et al.) shows similar patterns in informed-source designs

### Mechanism 2: Source-Perception-Behavior Dissociation
- Claim: Cognitive perceptions of feedback are shaped by perceived source labels, but behavioral uptake (implementation) is driven primarily by objective feedback quality.
- Mechanism: Two-phase processing under prospect theory—(1) an editing phase where source cues shape heuristic judgments about fairness, usefulness, and acceptance; (2) an evaluation phase where objective quality analysis governs implementation decisions. Students can shift from intuitive to analytical processing when facing actual revision tasks.
- Core assumption: Implementation decisions require more deliberative processing than perception ratings, engaging System 2 evaluation of content quality.
- Evidence anchors:
  - "Only feedback quality significantly predicted feedback uptake"
  - "Neither the feedback source, recognition accuracy, nor their interaction significantly predicted implementation behavior (all ps > .24)... feedback quality was the only significant predictor of uptake (b = 1.27, SE = 0.50, p = .012)"
  - Related studies focus on perceptions, not behavioral outcomes

### Mechanism 3: Recognition Accuracy Moderates Perceptions Through Bias Bypass
- Claim: Misidentifying LLM feedback as expert-generated bypasses negative AI biases, leading to more favorable evaluations.
- Mechanism: When students accurately identify feedback as AI-generated, implicit negative biases toward AI (reliability concerns, trustworthiness doubts) are activated and depress perception ratings. When LLM feedback is misidentified as human/expert, these biases remain dormant, allowing content quality to be evaluated more objectively.
- Core assumption: Students hold implicit negative biases toward AI that are triggered by source identification but can be bypassed through misidentification.
- Evidence anchors:
  - "Recognition accuracy significantly moderated the effect of feedback source on perception, with particularly positive evaluations when LLM feedback was falsely ascribed to experts"
  - "LLM feedback was correctly identified close to random guessing" (36% accuracy); interaction effects showed misidentification improved fairness (b = 1.21, p = .042) and usefulness ratings (b = 1.76, p = .009)
  - Related work (Nazaretsky et al., Zhang et al.) shows pro-human bias in feedback perceptions

## Foundational Learning

- **Dual-Process Theory (System 1/System 2)**:
  - Why needed here: Explains why perceptions (System 1, heuristic) and behavior (System 2, analytical) can diverge. Critical for interpreting the source-perception-behavior dissociation.
  - Quick check question: When would you expect System 1 vs. System 2 to dominate feedback processing?

- **Feedback Perception Dimensions (FPQ Framework)**:
  - Why needed here: The study measures six dimensions (fairness, usefulness, acceptance, willingness to improve, affect). Understanding which dimensions are cognitive vs. motivational is essential for modeling.
  - Quick check question: Which perception dimensions would you expect to be most susceptible to source framing effects?

- **Source Credibility and Expertise Heuristics**:
  - Why needed here: The expertise heuristic is the central mechanism driving source effects. Understanding how credibility cues shape message processing is foundational.
  - Quick check question: If you wanted to reduce expertise heuristic effects, what intervention would you design?

## Architecture Onboarding

- **Component map**: Feedback Generation (LLM/Expert/Peer with controlled prompts) -> Source Concealment (participants blinded) -> Source Identification (guessed source: LLM/Expert/Peer) -> Perception Rating (FPQ: 6 dimensions, 18 items) -> Behavioral Uptake (revision task → correction rate) -> Moderators: Recognition Accuracy (binary), Feedback Quality (9-category coded)

- **Critical path**: Feedback Quality → Behavioral Uptake. Perceptions and source identification affect the path but do not directly predict implementation.

- **Design tradeoffs**:
  - Transparency vs. Effectiveness: Disclosing AI source may reduce perceived fairness/usefulness even when quality is high. Hybrid framing (AI + human oversight) may optimize both.
  - Ecological validity vs. Experimental control: Authentic feedback scenarios limit variance in uptake scores (max 3 mistakes = max 3 corrections).
  - Recognition accuracy measurement: Binary correct/incorrect loses nuance about confidence or systematic misattribution patterns.

- **Failure signatures**:
  - High perception ratings but low uptake → check quality coding reliability
  - Near-chance recognition accuracy → feedback content too similar across sources or heuristics unreliable
  - Non-significant perception-uptake relationship → variance restriction in uptake measure
  - Large standard errors in interaction terms → underpowered for moderation analyses

- **First 3 experiments**:
  1. **Explicit quality cues override**: Test whether displaying objective quality metrics (e.g., "this feedback addresses 3/3 criteria") reduces source bias effects on perceptions.
  2. **AI literacy as moderator**: Measure AI literacy and test whether high-literacy students show (a) better recognition accuracy and (b) reduced negative bias toward correctly identified AI feedback.
  3. **Iterative feedback cycles**: Test whether repeated exposure to high-quality AI feedback across multiple revision cycles reduces source bias over time (longitudinal design).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do source-based heuristics and negative biases toward AI persist, diminish, or intensify as pre-service teachers engage with multiple rounds of LLM-generated feedback?
- Basis in paper: The authors state that longitudinal designs could clarify whether heuristics persist, diminish, or intensify over time.
- Why unresolved: This study utilized a single-instance experimental design, which cannot track changes in bias or perception over repeated exposure.
- What evidence would resolve it: A longitudinal or repeated-measures study tracking perception and uptake across multiple feedback cycles.

### Open Question 2
- Question: To what extent do individual differences in AI literacy and feedback literacy explain variance in source recognition accuracy and behavioral uptake?
- Basis in paper: The authors note they collected no data on these individual differences but state that including them would likely explain additional variance.
- Why unresolved: The study did not measure or control for participants' prior literacy levels regarding feedback or AI tools.
- What evidence would resolve it: A replication study that includes validated measures for AI literacy and feedback literacy as covariates.

### Open Question 3
- Question: Can systematically framing the motives of an AI (e.g., as benevolent or neutral) alter the expertise heuristic and negative biases observed in feedback perception?
- Basis in paper: The authors suggest replicating the findings with explicit mental-model manipulations to test the boundary conditions of the observed heuristics.
- Why unresolved: The study concealed the source identity rather than actively manipulating the framing of the AI's intent or persona.
- What evidence would resolve it: An experiment varying the priming descriptions of the AI source (e.g., "helpful assistant" vs. "automated tool") before feedback delivery.

### Open Question 4
- Question: How does feedback modality (text vs. audio vs. video) influence source identification accuracy and the impact of perceived source on uptake?
- Basis in paper: The authors suggest expanding the focus beyond text-based feedback to include multimodal formats to provide deeper insights.
- Why unresolved: The current analysis was restricted to written feedback messages.
- What evidence would resolve it: A comparative experiment assessing feedback perception and uptake across different sensory modalities.

## Limitations
- Source recognition accuracy was near-chance (36%), making it impossible to isolate pure source effects from perception biases
- Uptake measure was constrained by task design (max 3 corrections possible), limiting variance for statistical detection
- Peer feedback naturally lower-quality and shorter, creating confounds when isolating source effects on perceptions

## Confidence

- **High confidence**: The dissociation between perception and uptake—feedback quality being the sole significant predictor of behavioral implementation (p = .012) while source effects operate through perceptions
- **Medium confidence**: The expertise heuristic mechanism—while the pattern of higher ratings for expert-ascribed feedback is clear, the low recognition accuracy makes it difficult to determine whether this reflects genuine expertise effects or confusion about source identity
- **Low confidence**: The moderation effect through recognition accuracy—given chance-level source identification, the interaction effects (p = .042 for fairness, p = .009 for usefulness) may reflect systematic misattribution patterns rather than the intended moderation mechanism

## Next Checks
1. **Source identification training**: Pre-test participants on source identification with feedback samples to establish baseline recognition accuracy, then test whether training improves accuracy and reduces bias effects
2. **Blind quality rating**: Have independent raters evaluate identical feedback messages without source information to establish objective quality scores that can be compared against source-ascribed ratings
3. **Repeated exposure design**: Implement a longitudinal study where participants receive multiple feedback instances from different sources to test whether recognition accuracy and bias patterns stabilize with experience