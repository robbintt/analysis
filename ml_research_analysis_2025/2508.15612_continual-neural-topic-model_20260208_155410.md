---
ver: rpa2
title: Continual Neural Topic Model
arxiv_id: '2508.15612'
source_url: https://arxiv.org/abs/2508.15612
tags:
- topic
- contm
- topics
- coherence
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Continual Neural Topic Model (CoNTM) addresses the problem
  of learning evolving topics from sequential document collections while maintaining
  long-term memory of previously learned topics. Unlike traditional dynamic topic
  models that require the entire corpus upfront or online models that forget historical
  information, CoNTM uses a global prior distribution that is continuously updated
  to capture topic dependencies across time steps while local models adapt to current
  patterns.
---

# Continual Neural Topic Model

## Quick Facts
- arXiv ID: 2508.15612
- Source URL: https://arxiv.org/abs/2508.15612
- Reference count: 40
- Key outcome: CoNTM achieves average topic quality rank of 1.6 out of five compared methods while maintaining good topic diversity and temporal smoothness scores.

## Executive Summary
The Continual Neural Topic Model (CoNTM) addresses the challenge of learning evolving topics from sequential document collections while maintaining long-term memory of previously learned topics. Unlike traditional dynamic topic models that require the entire corpus upfront or online models that forget historical information, CoNTM uses a global prior distribution that is continuously updated to capture topic dependencies across time steps while local models adapt to current patterns. The method employs a Dirichlet Variational Autoencoder framework where each time step updates global topic parameters through a running average mechanism, ensuring consistency across temporal slices. Experimental results demonstrate that CoNTM consistently outperforms state-of-the-art dynamic topic models across six diverse datasets in terms of topic quality and predictive perplexity.

## Method Summary
CoNTM is a Dirichlet Variational Autoencoder that processes documents incrementally without requiring access to all historical data simultaneously. The model maintains a global topic-word distribution φ̂global that is updated at each time step via running average: φ̂global ← (1 - ρ_t)φ̂global + ρ_t φ̂local_t. Local models at each time slice are derived from this global prior plus a learned perturbation: φ̂local_t = φ̂global + Δφ̂local_t. This creates a bottleneck where historical information is compressed into φ̂global, while current-time patterns are captured in the perturbations. The model uses a decay schedule ρ_t = 1/(τ_0 + t)^κ with κ ∈ (0.5, 1] to control memory retention, and a Dirichlet prior over document-topic distributions to induce sparsity and improve interpretability.

## Key Results
- CoNTM achieves an average topic quality rank of 1.6 out of five compared methods across six datasets
- The model maintains good topic diversity scores on large datasets while showing stable performance over time
- Temporal topic smoothness score of 0.49 indicates gradual topic transitions that preserve previously learned information while capturing emerging topics
- Topic coherence increases over time in most datasets, with decreasing perplexity scores indicating improved predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A continuously updated global prior distribution enables learning new topics without catastrophic forgetting of previously learned topics.
- Mechanism: The model maintains a global topic-word distribution φ̂global that is updated at each time step via running average. Local models are derived from this global prior plus a learned perturbation, creating a bottleneck where historical information is compressed into φ̂global.
- Core assumption: Topic evolution across time is gradual enough that a single global topic set plus local perturbations can capture both stability and change.
- Evidence anchors: [abstract] "This is achieved using a global prior distribution that is continuously updated." [Section 3.2] "These topics are interconnected through a global topic set, allowing for minor temporal adjustments to the global topics at each time step t."
- Break condition: If topics change abruptly or diverge significantly across time, the single global prior may become a bottleneck, forcing either forgetting or preventing adaptation.

### Mechanism 2
- Claim: Dirichlet prior over document-topic distributions induces sparsity, improving topic interpretability and reducing interference across time steps.
- Mechanism: Each document's topic distribution z ~ Dirichlet(α) is encoded via a neural network. The Dirichlet prior encourages sparse topic assignments, which means updates at one time step primarily affect a subset of topics, reducing cross-time interference.
- Core assumption: Sparse document-topic distributions are appropriate for the corpus structure, and the Dirichlet distribution adequately captures the true prior over topic mixtures.
- Evidence anchors: [Section 3.1] "Using a Dirichlet prior is essential as it encourages sparsity in the topic distributions, thereby enhancing the interpretability of the topics."
- Break condition: If documents genuinely require dense topic mixtures, the Dirichlet sparsity assumption may hurt performance.

### Mechanism 3
- Claim: Incremental updates to global topics via exponential decay allow controlled memory retention without requiring access to historical data.
- Mechanism: The update rule uses ρ_t = 1/(τ_0 + t)^κ where κ ∈ (0.5, 1] controls the decay rate. Early time steps receive larger updates, while later updates are smaller, allowing the global prior to stabilize.
- Core assumption: The decay schedule appropriately balances plasticity and stability. The optimal κ and τ_0 values transfer across datasets.
- Evidence anchors: [Section 3.2.3] "Selecting proper values for τ_0 and κ ensures that the influence of local updates decreases over time, allowing the model to converge to a global topic-word distribution."
- Break condition: If the corpus has very long time horizons with many time steps, the decay may cause gradual forgetting of early topics regardless of parameterization.

## Foundational Learning

- **Variational Autoencoders (VAEs) and the ELBO Objective**
  - Why needed here: CoNTM builds on the Dirichlet VAE framework. Understanding how the encoder approximates the posterior, how the decoder reconstructs inputs, and how the ELBO trades off reconstruction vs. regularization is essential for debugging training dynamics and interpreting topic quality.
  - Quick check question: Given a document w, can you sketch the flow from encoder → latent z → decoder → reconstructed document, and identify where the KL divergence term appears in the ELBO?

- **Dirichlet Distribution Properties**
  - Why needed here: The Dirichlet prior over topic proportions is central to the model's sparsity behavior. Understanding how the concentration parameter α affects sparsity, and why Dirichlet (vs. Gaussian) is appropriate for simplex-constrained distributions, helps in interpreting topic assignments and tuning hyperparameters.
  - Quick check question: If α = [0.1, 0.1, 0.1] for a 3-topic model, what does the resulting topic distribution typically look like? What if α = [10, 10, 10]?

- **Online and Continual Learning Fundamentals**
  - Why needed here: CoNTM explicitly addresses the stability-plasticity dilemma. Understanding catastrophic forgetting, replay mechanisms (and their absence here), and evaluation protocols for continual learning helps contextualize why the global prior mechanism is novel compared to standard online LDA.
  - Quick check question: In standard online LDA without long-term memory, what happens to topics learned at time t=1 after processing data from t=10? How does CoNTM differ?

## Architecture Onboarding

- **Component map:**
  - Encoder network (θ) -> Dirichlet parameters α_θ(w) -> latent z -> Decoder network -> Reconstructed documents
  - Global topic parameters (φ̂global) <- Running average update from φ̂local_t
  - Local perturbations (Δφ̂local_t) <- Time-specific adjustments to global topics

- **Critical path:**
  1. Initialize θ, φ̂global, and Δφ̂local_1
  2. For each time step t: Receive documents {w_i,t}, optimize θ and Δφ̂local_t via ELBO gradients for J training steps, compute φ̂local_t = φ̂global + Δφ̂local_t, update φ̂global ← (1 - ρ_t)φ̂global + ρ_t φ̂local_t
  3. Evaluate using NPMI (coherence), topic diversity, TTS (smoothness), and predictive perplexity

- **Design tradeoffs:**
  - Global prior capacity: A single global topic set may limit expressiveness for highly diverse corpora, but reduces parameters and enforces consistency
  - Decay rate (κ, τ_0): Higher κ → faster forgetting of old topics. The paper finds κ=0.7, τ_0=1 work well, but sensitivity varies by dataset
  - Number of topics K: Fixed across all time steps. Emerging topics must fit into existing K slots; no dynamic topic birth/death mechanism exists
  - No replay buffer: Unlike many continual learning methods, CoNTM does not store historical documents. Memory efficiency is gained at potential cost to long-term fidelity

- **Failure signatures:**
  - Dropping topic coherence over time: May indicate ρ_t too high (overwriting global prior) or insufficient documents per time slice
  - Static topics across time: TTS score too close to 1.0 indicates ρ_t too low (global prior not adapting) or perturbations too constrained
  - High predictive perplexity: Model failing to generalize to future timestamps. Check decoder reconstruction loss and topic-word distribution smoothness
  - Training instability: ELBO not converging within a time slice. Check encoder architecture, learning rate (paper uses 0.01 with Adam), and batch size

- **First 3 experiments:**
  1. Reproduce NYT temporal analysis (Figure 3): Train CoNTM on NYT 1987-2007 with K=50, κ=0.7, τ_0=1. Verify that topic coherence increases over time and TTS ≈ 0.49
  2. Ablate the global prior: Replace the running average update with a simple average (ρ_t = 1/t) and compare TTS and topic quality
  3. Stress test on synthetic data with known topic shifts: Generate a corpus where topics change abruptly at known timestamps. Measure how quickly CoNTM adapts and whether old topics are preserved

## Open Questions the Paper Calls Out

- **Incorporating word embeddings**: The paper plans to extend the model to handle datasets of varying sizes by incorporating word embeddings, as the current implementation learns topics from scratch which leads to lower quality on small datasets compared to embedding-based baselines like Dynamic BERTopic.

- **Capturing abrupt topic shifts**: The model may not accurately reflect rapid shifts in topics, particularly in fast-changing domains like social media, due to its reliance on a running average mechanism that enforces smooth transitions and may lag behind sudden semantic changes.

- **New evaluation metrics**: Traditional topic coherence metrics may not fully capture the semantic shifts over time, making reliable assessment difficult. The paper suggests the need for new temporal metrics that correlate with human judgment regarding thematic continuity and evolution of topics.

## Limitations

- The single global prior distribution may become a bottleneck when topics change abruptly or diverge significantly across time, potentially causing either forgetting or preventing adaptation in highly non-stationary domains.
- The fixed number of topics K means the model cannot dynamically add or remove topics as new concepts emerge, forcing all evolving topics to fit within predetermined slots.
- The effectiveness of the Dirichlet prior for sparsity and its specific contribution to continual learning success is not thoroughly validated through ablation studies.

## Confidence

- **High confidence**: The global prior update mechanism and its theoretical justification for preventing catastrophic forgetting are well-supported by the paper's mathematical framework and experimental results.
- **Medium confidence**: The Dirichlet prior's role in inducing sparsity and improving topic interpretability is supported by the framework but lacks direct ablation evidence specific to continual learning scenarios.
- **Medium confidence**: The empirical superiority over state-of-the-art dynamic topic models is well-demonstrated across six datasets, though the evaluation protocol could benefit from additional baselines and longer-term stability tests.

## Next Checks

1. **Synthetic abrupt topic shift test**: Generate a corpus with known topic changes at specific timestamps and measure how quickly CoNTM adapts while preserving old topics, testing the break conditions of the global prior assumption.

2. **Dynamic topic count evaluation**: Implement a variant that can add/remove topics based on coherence scores and compare performance against fixed K=50 on datasets with known topic evolution patterns.

3. **Long-term forgetting analysis**: Extend training to 100+ time slices using synthetic data and monitor topic coherence and diversity scores over time to identify degradation patterns and validate the decay schedule's effectiveness.