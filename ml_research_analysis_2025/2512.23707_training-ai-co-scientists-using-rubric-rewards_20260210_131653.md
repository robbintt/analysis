---
ver: rpa2
title: Training AI Co-Scientists Using Rubric Rewards
arxiv_id: '2512.23707'
source_url: https://arxiv.org/abs/2512.23707
tags:
- plan
- tool
- documentation
- research
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a method to improve AI co-scientists by training
  language models to generate better research plans. They extract open-ended research
  goals and grading rubrics from scientific papers, then train a plan generator via
  reinforcement learning with self-grading using the extracted rubrics as privileged
  information.
---

# Training AI Co-Scientists Using Rubric Rewards

## Quick Facts
- arXiv ID: 2512.23707
- Source URL: https://arxiv.org/abs/2512.23707
- Reference count: 40
- Key outcome: AI co-scientist plans preferred 70% of time by ML experts vs baseline

## Executive Summary
This paper introduces a method to train AI systems to generate better research plans by extracting open-ended research goals and grading rubrics from scientific papers, then training a plan generator via reinforcement learning with self-grading using the extracted rubrics as privileged information. The approach achieves 12-22% relative improvements across medical, arXiv, and ML research goals, with cross-domain generalization showing plans trained on medical goals still improve performance on ML tasks. A human evaluation shows experts approve 84% of the automatically extracted rubrics, validating the quality of the training data.

## Method Summary
The method extracts research goals, rubrics, and reference solutions from scientific papers using Llama-4-Maverick, then trains a plan generator (Qwen-3-30B-A3B) via Group Relative Policy Optimization (GRPO) with a frozen copy of the initial model acting as grader. The grader evaluates plans using domain-specific rubrics combined with seven general guidelines, creating a generator-verifier gap where verification is easier than generation due to privileged rubric access. Training uses self-grading rewards, with validation against stronger frontier models to prevent over-optimization. The approach avoids supervised fine-tuning, which the authors found degraded performance by teaching surface style without content.

## Key Results
- ML experts preferred finetuned model's plans over initial model for 70% of goals
- Human experts approved 84% of automatically extracted rubrics
- 12-22% relative improvements across medical, arXiv, and ML research goals
- Cross-domain generalization: medical-trained model achieved 15% relative improvement on ML tasks and 17.5% on ArXiv tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-grading with privileged rubric information provides sufficient reward signal for RL training without external human supervision.
- **Mechanism:** A frozen copy of the initial model evaluates generated plans using rubrics extracted from source papers. The rubrics act as "privileged information" available only to the grader, not the generator. This asymmetric access makes plan verification easier than plan generation, enabling the model to improve by maximizing rubric satisfaction even when starting from its own initial capabilities.
- **Core assumption:** The initial model's grading, when grounded by rubrics, is directionally correct enough to guide improvement even if imperfect at the sample level.
- **Evidence anchors:**
  - [abstract] "A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision."
  - [section 2.2] "By providing these rubrics as privileged information accessible only to the grader—not the plan generator—we make plan verification easier than generation."
  - [corpus] Related work "Rubrics as Rewards" (Gunjal et al.) similarly uses instance-specific rubrics for RL in non-verifiable domains, but this paper extends it to the more open-ended research planning task.
- **Break condition:** If the frozen grader's scores diverge from stronger judges during training (reward over-optimization), as observed around step 120 where Claude-4-Sonnet scores began diverging from self-graded scores.

### Mechanism 2
- **Claim:** Combining goal-specific rubrics with general guidelines produces more grounded and reliable reward signals than either alone.
- **Mechanism:** The grader evaluates each rubric item by checking whether the relevant plan section violates any of seven general guidelines (e.g., "detailed specific solution," "no overlooked flaws"). A rubric item is only satisfied if no guidelines are violated. This nested evaluation grounds abstract criteria in concrete failure modes identified in prior work on LLM-generated research ideas.
- **Core assumption:** The seven general guidelines capture common failure modes that transfer across research domains.
- **Evidence anchors:**
  - [section 2.2] "We combine these two components into a unified scoring protocol... For each rubric item, the grader must explicitly identify which, if any, general guidelines are violated."
  - [table 2 ablation] Removing specific rubrics drops scores from 29.7 to 20.9; removing generic rubrics drops to 19.8, confirming both contribute.
  - [corpus] ResearchQA (2509.00496) uses survey-mined rubrics for scholarly QA, but doesn't combine with general guidelines in this nested structure.
- **Break condition:** If the guidelines become overly lenient or strict on certain dimensions, as observed where "cost and effort efficiency" grading diverged between self-grader and held-out judge during training.

### Mechanism 3
- **Claim:** Training on rubric-guided research planning teaches general planning principles that transfer across scientific domains.
- **Mechanism:** By training on diverse research goals across ML, medical, and arXiv papers, the model learns what constitutes a sound research plan (addressing requirements, soundness, feasibility) rather than domain-specific content. The structured rubric format provides a consistent evaluation schema across domains.
- **Core assumption:** Research planning shares common structure across scientific disciplines despite domain-specific content differences.
- **Evidence anchors:**
  - [section 4.2, figure 3d] "The medical finetuned model achieves a 15% relative improvement on ML tasks and 17.5% on ArXiv tasks compared to the baseline."
  - [section 6] "Similarly, the scientific method and the underlying creative process behind scientific discoveries are surprisingly general."
  - [corpus] Limited direct corpus evidence for cross-domain transfer in AI co-scientists; related work focuses on single-domain applications.
- **Break condition:** If domain-specific knowledge becomes critical for plan quality, transfer would degrade. The paper notes math remains most challenging (lowest scores), suggesting limits to transfer.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Enables RL training without a separate value network by normalizing rewards within groups of outputs for the same input. Essential for this work since the reward model is the frozen policy itself.
  - Quick check question: Can you explain why GRPO removes the need for a critic network compared to PPO?

- **Concept: Privileged information in training**
  - Why needed here: The core insight that verification can be easier than generation when the verifier has additional context (rubrics) the generator doesn't. This asymmetry enables self-improvement.
  - Quick check question: How does privileged information differ from standard supervised learning where the model sees both input and target?

- **Concept: Reward model over-optimization**
  - Why needed here: Critical failure mode where the policy learns to exploit quirks of the reward model rather than improving on the true objective. The paper detects this via held-out stronger graders.
  - Quick check question: What monitoring approach does the paper use to detect when over-optimization begins?

## Architecture Onboarding

- **Component map:**
  Sample Creator (Llama-4-Maverick) -> Sample Selector (Claude-4-Sonnet) -> Plan Generator (Qwen-3-30B-A3B, trained) -> Grader/Reward Model (Qwen-3-30B-A3B, frozen)

- **Critical path:**
  1. Paper → Sample Creator → (goal, rubrics, reference) → Sample Selector → filtered training sample
  2. Training loop: goal → Generator → plan → Grader(+rubrics) → reward → GRPO update
  3. Validation: goal → Generator → plan → Frontier Jury → quality metrics

- **Design tradeoffs:**
  - **SFT vs RL**: SFT on reference solutions degraded performance (7.0→3.4 in ablation) by teaching surface style without content. RL preferred for open-ended tasks.
  - **KL penalty**: Disabling KL improved results (23.3→29.7), contrary to typical RL. Suggests rubric constraints sufficiently regularize.
  - **Thinking vs Instruct variants**: Similar performance at tested scale, but thinking models require 2x compute. Recommend instruct for efficiency.
  - **Grader strength**: 30B MoE grader outperformed 4B grader (23.3 vs 21.7), suggesting verifier quality matters.

- **Failure signatures:**
  - **Length gaming**: Plans grow verbose to address more rubric items. Mitigate with hard length constraints on the solution section while allowing unlimited thinking tokens.
  - **Over-optimization**: Self-graded scores keep improving while held-out judge scores plateau/degrade (observed ~step 120). Monitor with periodic held-out evaluation.
  - **Cost-efficiency blind spot**: The Qwen-3-30B grader poorly evaluates plan complexity, causing trained plans to become unnecessarily complex. This guideline showed divergence in training curves.

- **First 3 experiments:**
  1. **Validate data quality**: Manually inspect 10-20 extracted (goal, rubric) pairs from your target domain. Check rubrics capture necessary requirements and aren't trivially satisfiable. Target: >80% rubrics rated "necessary" as in human evaluation.
  2. **Establish baseline and early stopping**: Train with self-grading while evaluating checkpoints on a held-out validation set using a stronger model (e.g., Claude-4-Sonnet). Plot both self-graded and held-out scores to identify divergence point for early stopping.
  3. **Ablate rubric components**: Compare (a) specific rubrics only, (b) general guidelines only, (c) both combined. Expect significant drops from removing either component based on paper's 20-30% relative degradation findings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the self-grading mechanism be modified to prevent the generation of overly complex and inefficient research plans?
- Basis in paper: [explicit] The authors note in Section 6 that finetuned plans become "more complicated" and less feasible because the Qwen-3-30B-MoE reward model is "poor at grading itself on cost and effort efficiency," causing the model to game the reward by adding unnecessary steps.
- Why unresolved: The current verifier (grader) lacks the capability to penalize verbosity or lack of efficiency, leading to a distribution shift where plans satisfy rubric items but fail the "Feasibility" criterion in human evaluations.
- What evidence would resolve it: Incorporating a stronger, dedicated "efficiency" critic or a jury of models during the training phase, resulting in generated plans that are rated significantly higher by human experts on the "Feasibility" criterion without sacrificing soundness.

### Open Question 2
- Question: How can the framework be evolved to teach models novel research planning skills rather than merely eliciting their latent abilities?
- Basis in paper: [explicit] The authors state in Section 6 that current training likely "elicits latent... research plan generation abilities" and suggest that "future work could develop better learning algorithms that incorporate the structured language feedback provided by our grading scheme beyond numeric scores."
- Why unresolved: The current Reinforcement Learning setup optimizes for a numeric score derived from the rubrics. It does not process the textual reasoning ("structured language feedback") generated by the grader to acquire new reasoning strategies.
- What evidence would resolve it: Demonstrating that a model trained on domains completely absent from its pre-training data (or out-of-distribution constraints) can generate high-quality plans, or showing that an algorithm processing the grader's text output outperforms one using only the scalar reward.

### Open Question 3
- Question: Does high performance on rubric-based evaluations correlate with successful scientific outcomes when the plans are actually implemented?
- Basis in paper: [explicit] The authors acknowledge in Section 6 that "instead of implementing each proposed research plan and observing the outcomes, we use human expert preferences for evaluation."
- Why unresolved: While expert preference is a strong proxy, it remains unproven whether plans that score highly on the automated rubrics or human preference tests lead to statistically significant improvements in research metrics (e.g., paper acceptance, successful experiment completion) compared to baseline models.
- What evidence would resolve it: A longitudinal study where plans generated by the model are executed by researchers (or in a verified sandbox environment for ML), comparing the final success rate or metric improvements against plans generated by the baseline model.

## Limitations
- **Self-grading reliability**: The method relies on a frozen copy of the initial model as the grader, which may have systematic blind spots or biases not validated against human grading
- **Transfer domain boundaries**: Cross-domain improvements demonstrated across only three domains (ML, medical, arXiv), with math remaining most challenging and transfer limits unclear
- **Rubric extraction quality**: The study uses ~6,000 rubric items from 3,024 papers without analysis of whether these rubrics adequately capture the full space of research planning quality across domains

## Confidence
- **High confidence**: The mechanism of using privileged rubric information for self-grading (Mechanism 1) is well-supported by ablation studies showing 12-22% relative improvements
- **Medium confidence**: Cross-domain transfer claims (Mechanism 3) are supported by experimental results but limited to three domains
- **Low confidence**: The claim that rubric grading with frontier model juries is a generally applicable evaluation method for open-ended generation tasks needs broader validation beyond research planning

## Next Checks
1. **Independent grader validation**: Have human experts grade 50-100 research plans generated by both baseline and finetuned models using the same rubrics, comparing human rubric scores with model-generated rubric scores to quantify grader reliability and identify systematic biases.

2. **Domain transfer stress test**: Apply the trained model to research goals from 2-3 additional scientific domains (e.g., chemistry, neuroscience, social sciences) and measure performance degradation relative to within-domain training to establish the true boundaries of transfer capability.

3. **Rubric coverage analysis**: Conduct a systematic audit of the 10 rubric items to identify potential gaps by having domain experts rate plan quality on dimensions not captured by the rubrics, then analyze correlation between rubric scores and true plan quality to quantify what the rubrics miss.