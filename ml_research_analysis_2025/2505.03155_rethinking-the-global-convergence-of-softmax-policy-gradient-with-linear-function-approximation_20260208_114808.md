---
ver: rpa2
title: Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function
  Approximation
arxiv_id: '2505.03155'
source_url: https://arxiv.org/abs/2505.03155
tags:
- have
- convergence
- lemma
- policy
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-examines the convergence properties of Softmax Policy
  Gradient (SPG) with linear function approximation in stochastic bandits. The authors
  challenge the conventional use of approximation error as a measure for global convergence,
  demonstrating through concrete examples that two problems with similar approximation
  errors can lead to vastly different convergence behaviors.
---

# Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation

## Quick Facts
- **arXiv ID:** 2505.03155
- **Source URL:** https://arxiv.org/abs/2505.03155
- **Reference count:** 40
- **Primary result:** Convergence of Softmax Policy Gradient (SPG) with linear function approximation depends on feature matrix's ability to preserve reward ordering, not approximation error.

## Executive Summary
This paper challenges the conventional wisdom that approximation error determines convergence of Softmax Policy Gradient (SPG) with linear function approximation in stochastic bandits. The authors demonstrate through concrete examples that two problems with similar approximation errors can exhibit vastly different convergence behaviors. They identify necessary and sufficient conditions on the feature representation that guarantee asymptotic global convergence to the optimal policy. Under these conditions, they prove that SPG with a problem-specific constant learning rate achieves O(1/T) convergence rate, while the same algorithm with any arbitrary constant learning rate still ensures asymptotic convergence at a slower O(ln(T)/T) rate.

## Method Summary
The paper analyzes Softmax Policy Gradient (SPG) with linear function approximation in K-armed stochastic bandits where d < K. Two algorithms are studied: an exact version using true gradients and a stochastic version using on-policy importance sampling. The convergence analysis focuses on feature conditions that preserve the ordering of true rewards when transformed through linear features, rather than minimizing approximation error. The key insight is that convergence depends on whether the feature matrix can maintain the relative ordering of rewards through linear transformations.

## Key Results
- Convergence depends on feature matrix's ability to preserve reward ordering, not approximation error
- Necessary and sufficient conditions on features (Assumptions 2 and 4) guarantee global convergence
- Problem-specific constant learning rate achieves O(1/T) convergence rate
- Arbitrary constant learning rate ensures asymptotic convergence at O(ln(T)/T) rate
- Experiments validate convergence across various feature dimensions and learning rates

## Why This Works (Mechanism)

### Mechanism 1: Reward Ordering Preservation via Feature Geometry
Global convergence is determined by the feature matrix's ability to preserve relative ordering of rewards through linear transformations. If there exists a weight vector w such that transformed rewards Xw preserve the order of true rewards r, then a monotonic path to the optimal policy exists. The core assumptions are Assumption 2 (Reward Ordering Preservation) and Assumption 4 (Feature Conditions), specifically the inner product condition ⟨xᵢ - xⱼ, xₐ⋆ - xₖ⟩ > 0. The break condition occurs when the feature matrix X cannot realize a vector preserving the reward order, trapping gradient ascent in suboptimal plateaus.

### Mechanism 2: Bounded Learning Rate for Monotonic Improvement
In the exact setting, a sufficiently small constant learning rate guarantees monotonic improvement in expected reward, forcing the policy to eventually become one-hot. The expected reward function ⟨πθ, r⟩ is L-smooth, and by selecting a learning rate η < 1/L, the gradient update acts as a contraction relative to the suboptimality gap. This ensures no stationary points exist in the finite parameter space, pushing ‖θ‖ → ∞ and forcing the policy into a corner of the probability simplex. The break condition occurs if the learning rate exceeds the smoothness threshold, potentially causing oscillation or divergence.

### Mechanism 3: Progress-Dominance over Stochastic Noise
In the stochastic setting, the algorithm converges almost surely because the cumulative "progress" term (deterministic drift) asymptotically dominates the cumulative "noise" term (variance of the gradient estimator). The stochastic process of the logits zₜ(a) is decomposed into a drift term Pₜ and a martingale difference noise term Wₜ. While noise grows at O(√T ln T), the drift term grows linearly O(T) provided the optimal action is sampled sufficiently often. The break condition occurs if the feature conditions are violated, allowing noise to dominate and preventing convergence to the optimal action.

## Foundational Learning

- **Concept:** Softmax Policy Gradient (SPG)
  - **Why needed here:** This is the core algorithm class. You must understand how the gradient of the expected reward ∇⟨πθ, r⟩ updates the parameters θ and why the resulting landscape is non-convex.
  - **Quick check question:** Can you derive the gradient update rule used in Algorithm 1: θₜ₊₁ = θₜ + ηXᵀ(diag(πθₜ) - πθₜπθₜᵀ)r?

- **Concept:** Linear Function Approximation Limits (Rank deficiency)
  - **Why needed here:** The paper focuses on the "under-parameterized" regime (d < K). Understanding that πθ is restricted to a low-dimensional manifold is critical to seeing why global convergence is non-trivial and why approximation error is not the correct metric.
  - **Quick check question:** If feature matrix X has rank d < K, why can't the policy represent every possible probability distribution?

- **Concept:** Non-uniform Smoothness and Łojasiewicz Inequality
  - **Why needed here:** The convergence rate proofs rely on the objective function being "smooth" and satisfying specific gradient-growth conditions. Standard smoothness is insufficient; the "non-uniform" aspect (smoothness depending on πθ(a⋆)) is key to the O(1/T) rate.
  - **Quick check question:** How does the gradient norm relate to the suboptimality gap r(a⋆) - ⟨πθ, r⟩ in the Łojasiewicz inequality (Lemma 35)?

## Architecture Onboarding

- **Component map:** Environment (K-armed bandit) -> Feature Matrix X -> Policy πθ -> Gradient Estimator -> Optimizer
- **Critical path:** 1. Initialization: Set θ₁ 2. Update Loop: Compute logits Xθ, apply softmax, sample action (stochastic only), compute gradient estimator, update θ 3. Convergence: Verify if policy concentrates on a⋆ (lim πθₜ(a⋆) = 1)
- **Design tradeoffs:** Approximation Error vs. Feature Ordering: Standard practice minimizes approximation error, but this paper argues you should verify ordering preservation even if approximation error is large. Specific vs. Arbitrary Learning Rate: A problem-specific η (Eq. 10) guarantees O(1/T) rate, while arbitrary constant η guarantees asymptotic convergence but only O(ln(T)/T) rate for average suboptimality.
- **Failure signatures:** Plateauing: Policy converges to a suboptimal action (Example 2 behavior) - check if Assumption 3/4 holds. Divergence: Logits ‖Xθ‖ → ∞ without policy convergence (unlikely under given assumptions). Oscillation: In stochastic settings with large η, value may oscillate before eventually settling (Section 6 behavior).
- **First 3 experiments:** 1. Ordering Validation: Generate random feature matrices X with d < K, calculate approximation error εapprox, run Algorithm 1, verify if convergence correlates better with "Ordering Preservation" than εapprox. 2. Learning Rate Sweep: Run Algorithm 2 in stochastic bandit environment (K=4, d=2), vary η from very small (Eq. 10) to very large, compare convergence trajectory speed and stability against Theorem 9 and Theorem 12. 3. 3-Armed Counterfactual: Construct Example 4 (K=3) where Assumption 3 is violated, run Algorithm 1, visualize trajectory getting stuck on suboptimal plateau as described in Proposition 13.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the identified feature conditions and convergence guarantees be extended to general Markov decision processes (MDPs)?
- **Basis in paper:** [explicit] The conclusion states that "extending the results and techniques to general Markov decision processes is an important and challenging next step."
- **Why unresolved:** The current analysis is restricted to stochastic bandits; MDPs introduce temporal credit assignment and value function approximation complexities not addressed by the current proofs.
- **What evidence would resolve it:** A theoretical proof establishing convergence rates for Lin-SPG in tabular or linear MDPs under the specified feature conditions (Assumptions 2 and 4).

### Open Question 2
- **Question:** Can the feature conditions be utilized to design algorithms for better representation learning?
- **Basis in paper:** [explicit] The conclusion poses the question: "investigating whether our feature conditions can be used for better representation learning is an interesting question."
- **Why unresolved:** The paper characterizes convergence given fixed features but does not propose a method to learn or construct features that satisfy Assumption 4.
- **What evidence would resolve it:** An algorithm that dynamically learns feature representations to satisfy the ordering preservation conditions, coupled with empirical validation of improved convergence speed.

### Open Question 3
- **Question:** Do the convergence results hold if Assumption 1 (Unique True Mean Reward) is removed?
- **Basis in paper:** [explicit] The paper states regarding Assumption 1: "We believe that our results would continue to hold without Assumption 1."
- **Why unresolved:** The proofs utilize strict reward ordering (r(i) ≠ r(j)) to define a unique optimal action; the authors provide this as a belief but do not prove it for cases with tied optimal rewards.
- **What evidence would resolve it:** A theoretical proof showing convergence to the optimal submanifold when multiple actions share the maximum reward, or a counterexample showing divergence.

### Open Question 4
- **Question:** Can the proof techniques be generalized to handle non-linear complex function approximation?
- **Basis in paper:** [explicit] The conclusion lists "generalize the proof techniques to handle non-linear complex function approximation" as an "ambitious goal."
- **Why unresolved:** The theoretical analysis relies on linear algebra properties of the feature matrix X (e.g., Assumption 4), which do not directly translate to non-linear approximators like neural networks.
- **What evidence would resolve it:** Deriving necessary and sufficient conditions for convergence in the non-linear setting, potentially using Neural Tangent Kernel (NTK) approximations.

## Limitations
- The theoretical analysis relies heavily on complex feature ordering conditions (Assumptions 2 and 4) that may be difficult to verify in practice
- The stochastic convergence proof assumes bounded variance in the gradient estimator, but this assumption isn't verified for common reward distributions beyond Gaussian
- The convergence rate of O(ln(T)/T) for arbitrary learning rates represents a worst-case bound

## Confidence

- **High Confidence:** The mechanism showing that approximation error is insufficient for convergence (supported by Example 2 and Proposition 13 with explicit counterexamples)
- **Medium Confidence:** The necessity of feature ordering conditions for convergence (the "if" direction of Theorem 9 appears more speculative than the "only if" direction)
- **Low Confidence:** The specific learning rate bound in Eq. (10) achieving O(1/T) convergence - this depends on precise knowledge of problem parameters that may not be available in practice

## Next Checks

1. **Empirical validation of feature ordering:** Generate a synthetic 4-armed bandit where features satisfy Assumption 4. Measure convergence behavior of Lin-SPG with varying approximation errors. Confirm that ordering preservation predicts convergence better than approximation error magnitude.

2. **Learning rate sensitivity analysis:** Run the stochastic algorithm with both the problem-specific learning rate from Eq. (10) and several arbitrary constant rates. Quantify the actual convergence speed in practice versus the theoretical O(ln(T)/T) bound for arbitrary rates.

3. **Violation impact study:** Construct feature matrices that nearly satisfy Assumption 4 (small violation). Measure how this affects convergence probability and speed, testing the robustness of the theoretical guarantees to approximate satisfaction of the conditions.