---
ver: rpa2
title: 'Not All Data are Good Labels: On the Self-supervised Labeling for Time Series
  Forecasting'
arxiv_id: '2502.14704'
source_url: https://arxiv.org/abs/2502.14704
tags:
- series
- datasets
- time
- data
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-supervised approach to improve time
  series forecasting by re-labeling datasets with pseudo labels generated from intermediate
  reconstructions. The method, Self-Correction with Adaptive Mask (SCAM), adaptively
  replaces overfitted components in raw labels with these pseudo labels, enhancing
  model generalization.
---

# Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2502.14704
- **Source URL**: https://arxiv.org/abs/2502.14704
- **Reference count**: 40
- **Primary result**: Self-supervised pseudo-labeling via reconstruction improves time series forecasting generalization across 11 datasets

## Executive Summary
This paper introduces SCAM (Self-Correction with Adaptive Mask), a self-supervised method that improves time series forecasting by replacing noisy raw labels with pseudo labels generated from intermediate reconstructions. The method identifies overfitted components using a novel adaptive mask and applies Spectral Norm Regularization to further suppress overfitting. Experiments show consistent improvements across various backbone models including MLP, CycleNet, PatchTST, and iTransformer on eleven real-world datasets.

## Method Summary
SCAM co-trains a lightweight reconstruction network with any time series forecasting backbone. The reconstruction network learns to approximate raw labels, producing smoothed pseudo labels used as training targets. An adaptive mask identifies components where predictions overfit to noisy raw data by analyzing the geometric relationship between predictions, reconstructions, and raw labels. Spectral Norm Regularization is applied to the first and last linear layers of the backbone to reduce overfitting. The reconstruction network is discarded at inference, leaving only the enhanced predictor.

## Key Results
- Pseudo labels from intermediate reconstructions consistently outperform raw labels in predictor training
- Adaptive masking successfully identifies overfitted components, reducing loss landscape sharpness
- Spectral Norm Regularization on linear layers improves generalization without causing entropy collapse
- Method shows consistent improvements across MLP, Transformer, and Conv-based backbones on 11 benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
Intermediate reconstructions provide better training targets than raw labels by filtering noise and anomalies. A lightweight reconstruction network learns an identity mapping of raw labels, producing smoother pseudo labels during early training that help the predictor avoid overfitting to noisy components.

### Mechanism 2
An adaptive mask identifies overfitted time series components by analyzing geometric relationships. When the reconstruction lies between prediction and raw label, this indicates overfitting to that component, allowing the model to focus supervision on well-generalized parts.

### Mechanism 3
Spectral Norm Regularization on embedding and projection layers reduces overfitting in Transformers by bounding the Lipschitz constant of the network. This acts as a gradient constraint surrogate that flattens the loss landscape without destabilizing self-attention.

## Foundational Learning

- **Loss Landscape Sharpness (λmax)**: Used to diagnose overfitting and justify mask design. Understanding why flat minima generalize better is essential.
  - Quick check: If λmax = 0.88 vs. 0.02 at convergence, which generalizes better and why?

- **Co-objective Optimization with Gradient Constraints**: The core formulation jointly optimizes reconstruction and prediction under gradient norm constraints. Understanding Lagrangian duality explains why SNR serves as a practical surrogate.
  - Quick check: Why is ||∇θ,ϕ ỹᵢ|| ≤ δ approximated by ||∇θ f|| ≤ δ?

- **Pseudo-labeling in Self-supervised Learning**: The method generates labels from data itself via reconstruction, differing from contrastive or generative SSL. Understanding when pseudo-labels help vs. hurt is critical.
  - Quick check: If reconstruction loss ℓ_rec is very low (ỹ ≈ y), would pseudo-labels help? What about very high ℓ_rec?

## Architecture Onboarding

- **Component map**: Raw labels y → Reconstruction Network g → Pseudo labels ỹ; Input x → Predictor f → Predictions ŷ
- **Critical path**: 1) Forward: y → g → ỹ; x → f → ŷ 2) Compute mask: M = (ỹ - ŷ)(ỹ - y) > 0 3) Compute losses with masking 4) Update ϕ with reconstruction loss, θ with prediction loss 5) Inference: use f(x;θ) only
- **Design tradeoffs**: Reconstruction network capacity (minimal 4-layer design vs. larger), mask threshold (binary vs. soft), SNR placement (pre+post vs. post only)
- **Failure signatures**: Mask collapse (all 0s/1s), reconstruction dominates (ỹ ≈ y), SNR over-regularization (high training loss)
- **First 3 experiments**: 1) Plot mask evolution on ETTh1 to verify dynamic behavior 2) Ablation: +SCAM only vs. +SNR only vs. +both on different backbones 3) Apply to held-out dataset and measure sharpness reduction

## Open Questions the Paper Calls Out

- Can SCAM be adapted for time series outlier detection and error correction tasks?
- Do alternative architectures for the reconstruction network (e.g., attention-based) outperform the current Conv-FFN prototype?
- Does replacing the binary adaptive mask with a soft masking strategy improve training stability?
- Is the co-training approach computationally efficient enough to scale to LLM-based time series architectures?

## Limitations

- The geometric masking heuristic lacks rigorous theoretical justification
- Reconstruction network is discarded at inference, creating training-inference mismatch
- Method not verified on LLM-scale models due to computational costs
- Binary masking may be too sensitive to noise or rapid loss landscape fluctuations

## Confidence

- Mechanism 1 (pseudo-labeling via reconstruction): Medium - empirical grid search provides evidence but noise filtering assumption needs broader validation
- Mechanism 2 (adaptive masking): Low - novel geometric criterion lacks direct theoretical grounding or isolated ablation studies
- Mechanism 3 (spectral norm regularization): High - sharpness analysis and empirical studies provide strong evidence for linear layer effectiveness

## Next Checks

1. Systematically vary mask threshold (binary vs. soft) and measure performance impact across datasets with different noise levels
2. Test whether increasing reconstruction network capacity beyond minimal 4-layer design improves performance on high-noise datasets
3. Apply SCAM+SNR to a held-out dataset not in original benchmark and compare performance with baseline models, explicitly measuring sharpness reduction at convergence