---
ver: rpa2
title: Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked
  Claim Dataset
arxiv_id: '2601.07985'
source_url: https://arxiv.org/abs/2601.07985
tags:
- evidence
- fact-checking
- multimodal
- claim
- french
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multilingual, multimodal pipeline for creating
  structured, fact-checked claim datasets in French and German. The approach aggregates
  ClaimReview feeds, scrapes full debunking articles, normalizes heterogeneous verdicts,
  and enriches data with aligned visual content and structured metadata.
---

# Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset

## Quick Facts
- **arXiv ID**: 2601.07985
- **Source URL**: https://arxiv.org/abs/2601.07985
- **Reference count**: 0
- **Primary result**: Creates structured, fact-checked claim datasets in French and German with LLM-extracted evidence and multimodal integration

## Executive Summary
This paper introduces a pipeline for creating structured, fact-checked claim datasets in French and German that combines textual and visual evidence. The approach aggregates ClaimReview feeds, scrapes debunking articles, normalizes verdicts, and enriches data with aligned visual content and structured metadata. Using LLMs, the pipeline extracts evidence from predefined categories and generates justifications linking evidence to verdicts. The resulting datasets enable fine-grained analysis of fact-checking practices and support more interpretable, evidence-grounded fact-checking models.

## Method Summary
The pipeline processes claim-review articles from fact-checking organizations in French and German, extracting structured information about claims, verdicts, evidence, and visual content. It employs LLM-based evidence extraction using predefined categories (e.g., authority statements, scientific evidence) and generates justification explanations linking evidence to verdicts. The system normalizes heterogeneous verdict labels and incorporates visual content from debunking articles. The pipeline is evaluated using G-Eval metrics and human assessment, with Gemini 2.5-pro achieving the highest scores for both evidence extraction and justification generation tasks.

## Key Results
- Gemini 2.5-pro achieves highest scores across correctness, coherence, and completeness for both evidence extraction and justification generation
- Incorporating visual evidence improves multimodal reasoning performance
- The resulting datasets enable fine-grained analysis of fact-checking practices across organizations and media markets

## Why This Works (Mechanism)
The pipeline leverages LLMs' natural language understanding capabilities to extract structured evidence from unstructured fact-checking articles, normalizing diverse verdict labels into standardized categories. The multimodal approach combines textual claims with visual evidence from debunking articles, enabling richer context for verification. By generating explicit justifications linking evidence to verdicts, the system creates interpretable outputs that support both automated fact-checking and human oversight.

## Foundational Learning
- **ClaimReview feed aggregation**: Essential for collecting diverse fact-checking data across organizations; quick check: verify feed endpoints and parsing accuracy
- **Verdict normalization**: Needed to handle heterogeneous labeling schemes across fact-checking organizations; quick check: validate mapping rules against ground truth
- **Multimodal evidence integration**: Combines textual and visual information for comprehensive verification; quick check: assess visual content relevance to claims
- **LLM-based evidence extraction**: Leverages large language models to identify relevant evidence categories; quick check: evaluate extraction accuracy against human annotations
- **Justification generation**: Creates explicit reasoning chains linking evidence to verdicts; quick check: verify coherence and completeness of generated justifications
- **G-Eval metric application**: Provides automated evaluation of generated outputs; quick check: validate metric reliability through human comparison

## Architecture Onboarding

**Component Map:**
Data Sources -> Scraper -> Normalizer -> LLM Evidence Extractor -> LLM Justification Generator -> Dataset

**Critical Path:**
ClaimReview Feed → Article Scraping → Evidence Extraction → Justification Generation → Dataset Output

**Design Tradeoffs:**
- LLM-based extraction provides flexibility but introduces potential model bias
- Multimodal integration improves reasoning but increases complexity
- Predefined evidence categories ensure structure but may limit coverage

**Failure Signatures:**
- Inconsistent verdict normalization across different fact-checking organizations
- LLM hallucinations in evidence extraction or justification generation
- Misalignment between visual content and textual claims
- Incomplete coverage of evidence categories for complex claims

**3 First Experiments:**
1. Validate verdict normalization accuracy by comparing automated mappings against human-annotated ground truth
2. Test LLM evidence extraction performance on a stratified sample of claims across different complexity levels
3. Evaluate multimodal reasoning improvements by comparing performance with and without visual evidence integration

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework relies on automated metrics and human assessment without detailed validation protocols or inter-annotator agreement measures
- LLM-based components may introduce biases from training data and reasoning patterns
- Limited evaluation of handling complex or ambiguous claims requiring deeper contextual understanding
- Performance generalizability to languages beyond French and German not established

## Confidence
- **High confidence**: Pipeline architecture and data collection methodology are well-described and technically sound
- **Medium confidence**: Reported performance improvements from visual evidence integration, given limited ablation studies
- **Medium confidence**: Generalizability of approach to other languages beyond French and German

## Next Checks
1. Conduct cross-validation studies using multiple LLM models to assess consistency and potential model-specific biases in evidence extraction
2. Perform detailed error analysis on a stratified sample of claims to identify systematic failure modes in both monolingual and multimodal reasoning
3. Test the pipeline's performance on claims requiring temporal or causal reasoning to evaluate its handling of complex misinformation patterns