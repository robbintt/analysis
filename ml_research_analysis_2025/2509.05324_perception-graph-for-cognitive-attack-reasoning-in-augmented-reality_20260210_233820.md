---
ver: rpa2
title: Perception Graph for Cognitive Attack Reasoning in Augmented Reality
arxiv_id: '2509.05324'
source_url: https://arxiv.org/abs/2509.05324
tags:
- control
- perception
- attack
- arxiv
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses cognitive attacks in augmented reality (AR)
  systems, where attackers manipulate human perception by altering or inserting objects,
  severely compromising user decision-making. The proposed Perception Graph model
  uses pre-trained vision-language models to mimic human interpretation of AR scenes
  and converts them into semantically rich, structured representations.
---

# Perception Graph for Cognitive Attack Reasoning in Augmented Reality

## Quick Facts
- arXiv ID: 2509.05324
- Source URL: https://arxiv.org/abs/2509.05324
- Authors: Rongqian Chen; Shu Hong; Rifatul Islam; Mahdi Imani; G. Gary Tan; Tian Lan
- Reference count: 29
- Key outcome: Perception Graph model achieves distance scores of 0.72 (3.6σ), 0.64 (2.9σ), and 1.00 (6.2σ) for route modification, fake control panel insertion, and map deletion attacks respectively in agricultural drone AR scenario, all well above normal threshold (μ=0.32, σ=0.11).

## Executive Summary
This paper addresses cognitive attacks in augmented reality systems where attackers manipulate human perception by altering or inserting objects, severely compromising user decision-making. The proposed Perception Graph model uses pre-trained vision-language models to mimic human interpretation of AR scenes and converts them into semantically rich, structured representations. It computes a quantitative distortion score by comparing semantic embeddings of objects across frames. Tested on an agricultural drone AR scenario, the model successfully flags perception distortions as attacks with interpretable Z-score metrics.

## Method Summary
The Perception Graph model operates in two phases: Graph Construction uses VLMs to generate natural language descriptions of AR scene objects, which are encoded into embeddings and organized into graphs with importance weights; Detection aligns new frame graphs with reference graphs and computes node-wise distance scores using √(1 - cosine_similarity). The system establishes a baseline normal distribution (μ=0.32, σ=0.11) and flags attacks when Z-scores exceed threshold values.

## Key Results
- Route modification attack detected with distance score 0.72 (3.6σ above baseline)
- Fake control panel insertion detected with distance score 0.64 (2.9σ above baseline)
- Map deletion attack detected with distance score 1.00 (6.2σ above baseline)
- All attack types achieved scores well outside normal variation range
- Model successfully distinguishes between benign variations and perception-manipulating attacks

## Why This Works (Mechanism)

### Mechanism 1
Vision-Language Models approximate human semantic interpretation of AR scenes by generating natural language descriptions that are projected into embedding vectors where semantic similarity maps to vector direction. By comparing embeddings across frames, the system quantifies perceived meaning changes rather than pixel-level differences. Core assumption: VLM outputs correlate sufficiently with human perception. Evidence: Model operates by mimicking human interpretation process. Break condition: If VLMs miss task-critical semantics, detection produces false negatives.

### Mechanism 2
Distance function based on embedding similarity provides quantitative, interpretable perception distortion measure. For each detected object, embeddings from reference frames are compared to current frames using cosine similarity. Distance = √(1 - Sim(E₁, E₂)). Distance of 1.0 indicates complete absence. Core assumption: Embedding distance correlates with human-perceived semantic deviation. Evidence: Map deletion achieving Distance = 1.00 (6.2σ) consistent with complete node absence. Break condition: Small embedding shifts with large operational impact may be understated.

### Mechanism 3
Statistical thresholding using Z-scores enables robust attack detection without attack-specific training data. Reference frames establish baseline distribution; new frames are scored and flagged when Z = (d - μ)/σ exceeds threshold. Core assumption: Normal frame variation is approximately Gaussian and stationary. Evidence: Normal conditions follow distribution with μ = 0.32 and σ = 0.11; attacks yield 3.6σ, 2.9σ, and 6.2σ scores. Break condition: Baseline distribution shifts over time without recalibration causing false positive drift.

## Foundational Learning

- **Vision-Language Models (VLMs)**
  - Why needed: Architecture relies on VLMs to convert visual AR content into human-like semantic descriptions
  - Quick check: Can you explain why a VLM might describe the same scene differently across two frames, and how the system handles this variability?

- **Semantic Embeddings and Cosine Similarity**
  - Why needed: Detection operates in embedding space; understanding meaning encoding as vector direction is critical
  - Quick check: Given two embeddings with cosine similarity of 0.64, what is the resulting distance score in this system?

- **Statistical Outlier Detection (Z-scores)**
  - Why needed: System flags attacks based on deviation from learned normal distribution; understanding threshold selection is critical
  - Quick check: If μ = 0.32 and σ = 0.11, what Z-score corresponds to a distance of 0.54, and would it be flagged with threshold Z > 2?

## Architecture Onboarding

- **Component map:** AR video frames -> VLM (semantic descriptions) -> Text Encoder (embeddings) -> Graph Constructor (perception graph) -> Distance Calculator (per-node scores) -> Statistical Classifier (Z-score threshold) -> Alert/Normal status

- **Critical path:**
  1. Establish reference graphs from trusted non-attack frames (offline)
  2. For each incoming frame, run VLM → text encoder → graph construction
  3. Align current graph to reference; compute distance scores
  4. Apply Z-score threshold; emit alert or normal status

- **Design tradeoffs:**
  - VLM selection: Larger models capture more nuance but increase latency; smaller models may miss subtle semantic shifts
  - Threshold setting: Lower thresholds catch more attacks but increase false positives; higher thresholds reduce noise but risk missing subtle attacks
  - Importance weighting: Over-weighting minor UI elements may amplify noise; under-weighting critical elements may miss high-impact attacks
  - Assumption: Paper does not specify latency requirements or real-time constraints for agricultural drone scenario

- **Failure signatures:**
  - High false positive rate: Baseline distribution may be too narrow; recalibrate with more reference frames or widen threshold
  - Missed subtle attacks: VLM may not capture fine-grained changes (e.g., numeric value changes in HUD); consider specialized encoders for text/numbers
  - Drift over time: Environmental changes shift baseline; implement periodic re-baselining
  - Node alignment failures: Natural object appearance/disappearance causes distance spikes; need better temporal smoothing

- **First 3 experiments:**
  1. Baseline characterization: Process 50+ non-attack frames across varying conditions to confirm distribution stability and refine μ/σ estimates
  2. Controlled attack injection: Systematically inject three attack types at varying intensities to map distance-score sensitivity and validate detection thresholds
  3. Threshold sweep analysis: Run detection with thresholds from Z = 1.5 to Z = 3.0; plot ROC-style curves to select operational threshold

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization uncertainty: Single agricultural drone scenario limits confidence in cross-domain applicability
- Temporal alignment unspecified: No detail on handling node alignment across frames with object appearance/disappearance
- False positive calibration lacking: Model's performance on benign-but-noisy frames not quantified

## Confidence

- **High Confidence**: Core mechanism of using embedding distance to quantify semantic distortion is well-supported by results (distance scores 0.72, 0.64, 1.00 for known attacks)
- **Medium Confidence**: Statistical thresholding approach is sound but depends on baseline distribution stability not extensively validated
- **Low Confidence**: Single AR scenario and lack of external validation limit real-world applicability and generalizability

## Next Checks

1. Cross-Scenario Testing: Evaluate Perception Graph model on at least two additional AR scenarios (medical surgery interface, industrial maintenance) to assess generalizability of VLM-based semantic interpretation

2. Baseline Stability Analysis: Process 100+ non-attack frames across varying environmental conditions to confirm baseline distribution stability and refine μ/σ estimates

3. Adversarial Robustness: Design and test attacks that subtly alter semantic meaning (swapping "north" for "south" in text, changing numeric values in HUD) to evaluate detection of high-impact but low-embedding-distance attacks