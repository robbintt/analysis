---
ver: rpa2
title: 'Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality
  Inversion'
arxiv_id: '2502.04263'
source_url: https://arxiv.org/abs/2502.04263
tags:
- image
- intra-modal
- clip
- features
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the intra-modal misalignment problem in
  CLIP-style vision-language models, where inter-modal contrastive training does not
  enforce intra-modal constraints, leading to suboptimal similarity measurements in
  tasks like image-to-image and text-to-text retrieval. The authors propose transforming
  intra-modal tasks into inter-modal ones using two optimization-based modality inversion
  techniques: Optimization-based Textual Inversion (OTI) and Optimization-based Visual
  Inversion (OVI).'
---

# Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion

## Quick Facts
- **arXiv ID:** 2502.04263
- **Source URL:** https://arxiv.org/abs/2502.04263
- **Reference count:** 40
- **Primary result:** Intra-modal misalignment in CLIP can be mitigated via Optimization-based Textual Inversion (OTI) and Optimization-based Visual Inversion (OVI), achieving 2-3% average mAP improvements on 15+ datasets

## Executive Summary
This paper identifies a fundamental problem in CLIP-style vision-language models: while the inter-modal contrastive loss creates well-calibrated distances between image and text embeddings, it leaves intra-modal distances uncalibrated. This intra-modal misalignment manifests as poor performance on image-to-image and text-to-text retrieval tasks despite strong zero-shot classification capabilities. The authors propose transforming these intra-modal tasks into inter-modal ones using two optimization-based modality inversion techniques that map representations from their native modality to the complementary one without requiring auxiliary data or additional adapters. Experiments demonstrate significant performance improvements across multiple datasets and model architectures.

## Method Summary
The authors propose two optimization-based modality inversion techniques to transform intra-modal tasks into inter-modal ones. Optimization-based Textual Inversion (OTI) maps image features to the text embedding space by optimizing a pseudo-token to minimize cosine distance with the frozen image encoder output, while Optimization-based Visual Inversion (OVI) maps text features to the image embedding space by optimizing pseudo-patches to match the frozen text encoder output. Both methods use gradient-based optimization with frozen CLIP encoders, optimizing either a single pseudo-token or multiple pseudo-patches. The key insight is that by stopping optimization at the right step (before features drift to their native manifold), the inverted features capture informative content while retaining the inter-modal alignment pattern that CLIP naturally produces.

## Key Results
- Intra-modal misalignment is real: On a filtered dataset with perfect inter-modal alignment, image-to-image retrieval achieves only 83% mAP
- OTI and OVI significantly outperform intra-modal baselines: 2-3% average mAP improvements across 15+ datasets
- Modality gap is necessary: When temperature increases to close the gap, inter-modal and intra-modal approaches converge in performance
- Over-optimization degrades performance: Inverted features drift to native manifold if optimization runs too long
- SLIP mitigates misalignment: Adding intra-modal loss during pre-training reduces the gap and OTI improvement

## Why This Works (Mechanism)

### Mechanism 1: Inter-modal Contrastive Loss Leaves Intra-modal Distances Unconstrained
CLIP-style training creates calibrated inter-modal distances but leaves intra-modal distances uncalibrated. The contrastive loss enforces that paired text embeddings lie at distance r from their image anchor, placing them on a hypersphere. Without intra-modal constraints, two texts expressing the same concept can be 0 to 2r apart from each other—completely uncalibrated relative to their shared image anchor.

### Mechanism 2: Modality Gap Enables Inter-modal vs Intra-modal Performance Differential
The separation between image and text embedding manifolds (modality gap) is necessary for modality inversion to provide benefits. The gap arises from random initialization and is preserved by contrastive training. When temperature τ is increased to close the gap, inter-modal and intra-modal approaches converge in performance, eliminating inversion benefits.

### Mechanism 3: Modality Inversion Captures Information While Preserving Inter-modal Alignment Pattern
OTI/OVI optimize input embeddings to match target features, and at the right optimization step, the inverted features exhibit the same alignment pattern as native inter-modal pairs. Inverted features are optimized to minimize cosine distance to the native modality feature. At the performance peak (early optimization), OTI-image similarity distribution matches text-image distribution.

## Foundational Learning

- **Concept: Contrastive N-pair Loss**
  - Why needed here: Understanding why CLIP's loss creates modality gap and leaves intra-modal distances free
  - Quick check question: Can you explain why minimizing negative pair similarity in a batch doesn't guarantee two positive text embeddings for the same image will be similar to each other?

- **Concept: Modality Gap (Liang et al., 2022)**
  - Why needed here: Core phenomenon this paper builds on; explains why image and text features occupy distinct cones
  - Quick check question: If you initialized both encoders with identical weights and trained with τ→∞, would you expect a modality gap?

- **Concept: Gradient-based Input Optimization (Adversarial Examples, Textual Inversion)**
  - Why needed here: OTI/OVI optimize input embeddings (pseudo-tokens/patches) while keeping model weights frozen
  - Quick check question: Why optimize the input rather than train an adapter network? What tradeoff does this introduce?

## Architecture Onboarding

- **Component map:**
  - **OTI Pipeline:** Image → Image encoder fθ → Optimize pseudo-token v* → Text encoder gϕ
  - **OVI Pipeline:** Text → Text encoder gϕ → Optimize pseudo-patches w* → Image encoder fθ

- **Critical path:**
  1. For OTI: Initialize v* randomly → concatenate with "a photo of" template → pass through text encoder → compute cosine loss with image features → update v*
  2. For OVI: Initialize w* → interpolate to match ViT patch count U → prepend CLS token → pass through image encoder → compute cosine loss with text features → update w*

- **Design tradeoffs:**
  - Pseudo-token count: R=1 is robust but suboptimal; higher R achieves better performance but risks drift
  - Optimization steps: 150 for OTI, 1000 for OVI in experiments; peak performance occurs early
  - Computation: ~0.2s/image for OTI, ~0.5s/text for OVI on A100 with batch size 2048

- **Failure signatures:**
  - Over-optimization: Inverted features drift to native manifold, losing inter-modal benefits
  - Insufficient pseudo-tokens: OVI with P=1 may not capture text semantic content
  - Wrong task application: Applying inversion to natively inter-modal tasks degrades performance by 2-6% accuracy

- **First 3 experiments:**
  1. Reproduce the dog vs. cat toy experiment (Appendix B): Filter for perfect inter-modal alignment, then measure image-to-image retrieval. Expect ~83% mAP despite perfect text-image alignment
  2. Ablation on optimization steps: Track OTI-image similarity distribution at steps 1, 17, 100, 1000 on Cars dataset. Confirm distribution shift from text-image-like to image-image-like
  3. Temperature ablation: Fine-tune CLIP with τ=0.01 vs τ=1.0 on COCO, measure modality gap magnitude and OTI improvement gap

## Open Questions the Paper Calls Out

- **Open Question 1:** Can efficient, non-iterative methods be developed to mitigate intra-modal misalignment that avoid the computational overhead of OTI and OVI? The authors state in the Limitations section that "The modality inversion techniques we propose are computationally expensive... This limits their practical applicability and future work should concentrate on efficient methods to mitigate the intra-modal misalignment."

- **Open Question 2:** What is the optimal strategy for fusing native image features and OTI-inverted features to maximize performance on intra-modal tasks? In Appendix G, regarding the combination of features, the authors state: "We leave further investigation of the combination of native and OTI-inverted features to future work."

- **Open Question 3:** Can intra-modal misalignment be resolved during the pre-training of Vision-Language Models without relying on external self-supervised losses like those in SLIP? The paper demonstrates that SLIP mitigates misalignment, whereas standard CLIP does not, but does not propose a novel pre-training objective optimized specifically to address this gap.

## Limitations
- **Computational efficiency:** OTI and OVI require iterative optimization (150-1000 steps) per feature, making them unsuitable for real-time applications
- **Optimization step sensitivity:** Performance critically depends on stopping optimization at the right step, requiring hyperparameter tuning that scales with dataset size
- **Model architecture dependence:** All experiments use CLIP or CLIP-like models; generalization to non-CLIP vision-language models is unclear

## Confidence
- **High Confidence:** The existence of intra-modal misalignment and the basic mechanism of modality inversion working via early optimization stopping
- **Medium Confidence:** The causal relationship between modality gap and inversion benefits (supported by temperature ablation but potentially confounded by overall performance degradation)
- **Low Confidence:** The claim that intra-modal loss terms "mitigate" misalignment during pre-training (based on SLIP comparisons, but SLIP has other architectural differences)

## Next Checks
1. **Optimization Step Robustness:** Run OTI on Cars dataset with optimization steps ranging from 10 to 300. Plot mAP against step count to verify the existence of a clear performance peak and measure how sensitive the peak location is to dataset characteristics.

2. **Cross-Model Generalization:** Apply OTI to OpenCLIP and SigLIP models on CUB dataset. Compare improvement magnitudes to original CLIP results to test whether the phenomenon and solution generalize across different vision-language model implementations.

3. **Ablation on Pseudo-token Count:** For OVI on the Flowers dataset, systematically vary P from 1 to 8 and measure T2T retrieval performance. Verify that P=1 is indeed insufficient for capturing semantic content while higher P values improve performance without excessive drift.