---
ver: rpa2
title: Learning to Skip the Middle Layers of Transformers
arxiv_id: '2506.21103'
source_url: https://arxiv.org/abs/2506.21103
tags:
- layers
- gate
- transformer
- attention
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Transformer architecture that dynamically
  skips a variable number of middle layers based on learned gating mechanisms. Guided
  by interpretability research showing middle layers are redundant and early layers
  perform token-to-semantic conversion, the method skips central blocks from the middle
  outward while preventing subsequent tokens from attending to skipped positions.
---

# Learning to Skip the Middle Layers of Transformers

## Quick Facts
- **arXiv ID:** 2506.21103
- **Source URL:** https://arxiv.org/abs/2506.21103
- **Reference count:** 13
- **Primary result:** Method skips middle Transformer layers via learned gating but shows no improvement in validation loss vs. FLOPs trade-off at small scales compared to dense baselines.

## Executive Summary
This paper proposes a Transformer architecture that dynamically skips middle layers based on learned gating mechanisms. The method is motivated by interpretability research suggesting middle layers exhibit higher redundancy than early or late layers, which handle token-to-semantic conversion and semantic-to-output conversion respectively. The architecture uses a soft mask accumulation mechanism to determine which tokens can skip central blocks, with gated attention preventing subsequent tokens from attending to skipped positions. Despite the theoretical appeal of reducing computation for simpler tokens, experiments at small scales showed no improvement in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers.

## Method Summary
The method introduces conditional computation to Transformers by allowing tokens to skip a variable number of middle layers. A gating mechanism accumulates a soft mask value for each token in the first half of the network, and if this value exceeds a threshold, the token bypasses the corresponding middle blocks via a long-range residual connection. Gated attention modifies attention logits to prevent subsequent tokens from attending to skipped token positions, while a "sandwich" or "peri-Layernorm" scheme stabilizes training by controlling residual norms. The architecture is trained with adaptive regularization to encourage sparsity in the gating decisions while maintaining performance.

## Key Results
- The gated Transformer architecture successfully learns to skip middle layers, achieving varying levels of sparsity during inference
- At small scales (12 layers on FineWeb), the method shows no improvement in the validation loss vs. FLOPs trade-off compared to dense baselines
- The authors explicitly note that benefits may only emerge at larger scales, though this remains untested
- Training stability requires the specific sandwich-LN normalization scheme; standard Pre-LN causes instability

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Redundancy
- **Claim:** Skipping central blocks reduces computational cost while preserving performance because middle layers exhibit higher redundancy than early or late layers
- **Mechanism:** A gating mechanism accumulates soft mask values $S(i, \ell)$ in the first half of the network, and if this exceeds 1.0, the token skips Transformer blocks in range $[\ell, L-\ell)$
- **Core assumption:** Early layers perform sufficient token-to-semantic aggregation, and late layers can effectively convert these intermediate features into output predictions without requiring refinement by central layers
- **Evidence anchors:** Abstract states "skipping a variable number of layers from the middle outward" guided by interpretability insights showing smaller performance impact when central layers are removed
- **Break condition:** If tasks require complex reasoning relying on middle layers' processing role, the long skip connection drops essential intermediate states, causing validation loss to spike

### Mechanism 2: Gated Attention for Context Integrity
- **Claim:** Preventing subsequent tokens from attending to skipped token positions maintains attention distribution integrity
- **Mechanism:** Gated attention adds $\ln g_j$ to pre-softmax attention logits, where $g_j$ is the gate value; if $g_j=0$, the logit becomes $-\infty$, masking the skipped token
- **Core assumption:** Skipped tokens have stable representations compatible with the current time step of non-skipped tokens, or act as static memory slots
- **Evidence anchors:** Abstract mentions "gated attention mechanism prevents subsequent tokens from attending to skipped token positions" with implementation detail in section 2.2
- **Break condition:** If critical tokens (e.g., rare entities) are skipped and subsequent tokens are blocked from attending to their updated representation, the model may fail to resolve dependencies, leading to incoherent generation

### Mechanism 3: Sandwich-LN for Training Stability
- **Claim:** The "sandwich" or "peri-LayerNorm" scheme stabilizes training when long-range skip connections are introduced
- **Mechanism:** Normalization is applied to both residual input and output of sub-layers ($y = x + \text{Norm}(\text{Module}(\text{Norm}(x)))$), controlling residual stream norms
- **Core assumption:** Standard Pre-LN causes residual norms to grow with depth, making direct connections from early to late layers numerically unstable
- **Evidence anchors:** Abstract mentions "residual norms are controlled with a 'sandwich' or 'peri-Layernorm' scheme" and section 2.3 discusses normalization for accepting outputs across skip connections
- **Break condition:** If dual normalization suppresses signal-to-noise ratio too aggressively, the model may converge to trivial solutions

## Foundational Learning

### Concept: Conditional Computation
- **Why needed here:** The architecture relies on dynamic routing; understanding that "capacity" (total parameters) is decoupled from "compute" (active parameters) is essential to interpret the FLOPs vs. Cross-entropy trade-off
- **Quick check question:** Does the model have fewer parameters than the baseline, or does it simply activate fewer of them per token?

### Concept: Layer-wise Redundancy
- **Why needed here:** The "middle-out" strategy is theoretically grounded in specific interpretability findings regarding which layers are safe to drop
- **Quick check question:** According to the paper's motivation, why is dropping the *middle* layers theoretically safer than dropping the first or last layers?

### Concept: Adaptive Regularization
- **Why needed here:** The gating mechanism requires specific loss components to encourage sparsity rather than defaulting to dense computation
- **Quick check question:** What happens to the sparsity of the gates if you train this architecture using only standard cross-entropy loss without the adaptive regularization term?

## Architecture Onboarding

### Component map:
Token Embeddings -> First Half Layers (0 to L/2) with Linear projections and soft mask accumulation -> Gate Logic (threshold check) -> Gated Attention (masked logits) and Gated FFN (conditional skip) -> Second Half Layers (L/2 to L) with mirrored gating -> Output Language Modeling Head

### Critical path:
Calculating the accumulated soft mask $S(i, \ell)$ is the critical path. If this calculation is not efficiently fused with the attention kernel, the overhead of the "router" may negate the FLOPs savings from skipping layers.

### Design tradeoffs:
The authors explicitly note that at small scales, the method fails to improve the trade-off compared to dense baselines. The gating overhead and difficulty of learning to skip effectively may outweigh benefits at small widths/depths.

### Failure signatures:
- **Collapse to Dense:** Sparsity remains $\approx 0$ (gates always 1) because the model prefers accuracy over efficiency without strong regularization
- **Performance Collapse:** Sparsity increases but validation cross-entropy degrades significantly below the dense baseline
- **Instability:** Training loss diverges if "Sandwich-LN" is replaced with standard Pre-LN due to residual norm mismatch

### First 3 experiments:
1. **Overfit Sanity Check:** Train a 12-layer Gated Transformer and 12-layer Dense Transformer on a tiny subset of data (e.g., 1M tokens) with zero regularization. Verify they can both overfit to similar loss values (checking architectural integrity).
2. **Sparsity Sweep:** Train the Gated Transformer with varying target sparsity levels ($\mu^*_{L/2}$) and plot the Pareto frontier of Validation Loss vs. Estimated FLOPs against the Dense baselines (reproducing Figure 2).
3. **Ablation on Normalization:** Train two models with the gating mechanism: one with Sandwich-LN and one with Pre-LN. Compare training stability and final loss to confirm the necessity of the specific normalization scheme.

## Open Questions the Paper Calls Out
None

## Limitations
- **Scale-dependent effectiveness:** The method shows no improvement at small scales, and claims about larger-scale benefits remain untested
- **Computational overhead uncertainty:** The paper claims FLOPs savings but doesn't report wall-clock time measurements or detailed overhead analysis
- **Mechanism validation gaps:** The empirical evidence supporting the specific mechanisms (particularly gated attention) is indirect rather than directly validating necessity

## Confidence

**High Confidence:** The architectural description is precise and reproducible with clear implementation details for gating, attention masking, and sandwich-LN normalization.

**Medium Confidence:** The theoretical motivation from interpretability research is sound and well-cited, but the leap to effective learnability at small scales involves untested assumptions.

**Low Confidence:** Claims about scalability benefits and the assertion that "benefits may only emerge at larger scales" are speculative without experiments at larger scales.

## Next Checks
1. **Scale-Scaling Experiment:** Train the same architecture at 24 and 36 layers on FineWeb and plot the Pareto frontier of validation loss vs. FLOPs across dense and gated variants at each scale to test whether effectiveness increases with model size.

2. **Ablation on Gated Attention:** Train two variants: (a) full gated attention with masking, and (b) gated attention without masking. Compare sparsity levels achieved and validation performance to isolate whether attention masking is critical for stable training.

3. **Runtime Benchmarking:** Measure wall-clock inference time for dense vs. gated models at equivalent validation loss levels using realistic batching with varying sequence lengths to validate whether theoretical FLOPs reduction translates to practical speed improvements.