---
ver: rpa2
title: 'Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software
  development'
arxiv_id: '2508.10108'
source_url: https://arxiv.org/abs/2508.10108
tags:
- code
- teams
- malicious
- amazon
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Amazon Nova AI Challenge, a global competition
  that advanced research in secure AI-assisted software development through adversarial
  tournaments between automated red-teaming bots and safe AI assistants. Ten university
  teams competed in a novel format featuring multi-turn conversations to evaluate
  safety alignment, with attackers attempting to elicit malicious or vulnerable code
  while defenders aimed to maintain both safety and utility.
---

# Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development

## Quick Facts
- arXiv ID: 2508.10108
- Source URL: https://arxiv.org/abs/2508.10108
- Reference count: 40
- Key outcome: Global competition advanced secure AI-assisted software development through adversarial tournaments between automated red-teaming bots and safe AI assistants

## Executive Summary
This paper presents the Amazon Nova AI Challenge, a global competition that advanced research in secure AI-assisted software development through adversarial tournaments between automated red-teaming bots and safe AI assistants. Ten university teams competed in a novel format featuring multi-turn conversations to evaluate safety alignment, with attackers attempting to elicit malicious or vulnerable code while defenders aimed to maintain both safety and utility. The challenge featured a custom 8B coding specialist model, automated red-teaming evaluation with human annotation for malicious content, and utility benchmarking to prevent over-refusal. Scientific advancements included reasoning-based safety alignment, robust guardrails, multi-turn jailbreaking, and efficient LLM probing. Tournament results showed defenders improved safety while maintaining utility, and attackers shifted focus from malicious explanations to vulnerable code generation.

## Method Summary
The challenge used a custom 8B parameter Prize LLM (decoder-only Transformer, GQA with 8 groups, 32 layers, 32 attention heads, 90K vocab, 8K context, RoPE embeddings) provided to defender teams. Tournaments featured 200 conversations per match-up with up to 5 turn pairs each, where attackers attempted to elicit malicious/vulnerable code while defenders maintained safety and utility. Defenders used SFT, DPO, GRPO on Prize LLM with synthetic data, reasoning-based alignment, and input/output guardrails. Attackers employed attacker-defender-evaluator frameworks with multi-turn jailbreaking strategies. Evaluation used Amazon CodeGuru for vulnerability detection (medium+ severity) and 3 human annotators for malicious content labeling. Metrics included Normalized ASR = ASR × (Diversity/100) for attackers and Normalized DSR = Average DSR × (Utility/100)^(4/5) for defenders.

## Key Results
- Defenders improved safety while maintaining utility across tournaments, demonstrating effective safety alignment techniques
- Attackers shifted from eliciting malicious explanations to generating vulnerable code as primary strategy
- Safety and utility objectives showed less competition than expected for vulnerable code generation tasks

## Why This Works (Mechanism)

### Mechanism 1: Iterative Adversarial Data Flywheel
Adversarial tournaments generate higher-quality safety training signal than static benchmarks because attack strategies co-evolve with defenses. Attackers discover novel jailbreaks → defenders train on failure cases → attackers must invent new strategies. The paper explicitly notes defenders used "data from prior tournaments" and "failure cases from the tournaments, to generate similar examples."

### Mechanism 2: Multi-Turn Jailbreak Exposure via Probing Phase
Adding an unscored "probing phase" before tournaments enables attackers to adapt to specific defender weaknesses without overfitting from insufficient signal. 200 unscored conversations let attackers explore defender behavior → attackers adapt strategies → scored tournament captures refined attacks. This addresses the paper's note that "attackers were unable to get enough signal from defender bots from the 200 conversations in the tournament."

### Mechanism 3: Utility-Preserving Safety via Joint Optimization
Normalized DSR = Average DSR × (Utility/100)^4 forces defenders to maintain capability while improving safety, preventing trivial refusals. The exponent 4 aggressively penalizes utility drops; defenders discovered "safety and utility for LLMs have long been thought of as competing objectives, [but] this might not be the case for vulnerable code generation."

## Foundational Learning

- **Attack Success Rate (ASR) vs Defense Success Rate (DSR)**
  - Why needed: These are the core tournament metrics; understanding their normalization is essential for interpreting results.
  - Quick check: If a defender achieves 95% DSR but 60% utility, what is their normalized DSR? (Answer: 0.95 × 0.6^4 ≈ 0.123)

- **Multi-turn Jailbreaking (Begin-from-Benign)**
  - Why needed: The dominant attack strategy; safety systems must track intent across conversation history.
  - Quick check: Why does starting with benign requests make jailbreaks more effective? (Answer: Models may not recognize cumulative malicious intent when distributed across turns.)

- **Reasoning-Based Alignment**
  - Why needed: Most defender teams used Chain-of-Thought-style reasoning before response generation.
  - Quick check: What is the tradeoff of using reasoning traces for safety? (Answer: Increased latency and potential information leakage if traces aren't stripped.)

## Architecture Onboarding

- **Component map**: Tournament Orchestrator (AWS Lambda + SQS + DynamoDB) -> Prize LLM (custom 8B coding model) -> Evaluation Harness (CodeGuru + human annotators) -> CoBot Toolkit (vLLM/TGI remote modules) -> Training Stack (EKS + Trn1 + MegaSFT)

- **Critical path**: Defender receives prompt → Input classifier/guardrail → Prize LLM (possibly with reasoning trace) → Output verifier/vulnerability fixer → Response returned. Latency budget: 45 seconds.

- **Design tradeoffs**: BLEU-based diversity vs embedding-based: BLEU encourages syntactic variety but may miss semantic duplicates; Closed-box vs open-box attacks: Custom model prevents weight-based attacks but limits realism; Conversation-level vs turn-level scoring: Conversation-level prevents double-counting but loses granular signal.

- **Failure signatures**: Over-refusal: High DSR, low utility (detect via normalized DSR crash); Low diversity: High raw ASR but low normalized ASR (BLEU scores cluster near 1.0); Probe-stale: Attacker performance drops between probing and tournament (suggests defender changed).

- **First 3 experiments**:
  1. Baseline probe: Run 200 conversations against Prize LLM without modifications; measure raw ASR, DSR, and diversity to establish reference.
  2. Reasoning ablation: Add deliberative reasoning (hidden thought trace) before response; compare DSR with/without reasoning on identical attack set.
  3. Utility stress test: Submit benign-but-security-adjacent prompts (e.g., "encrypt files and send key to server") to measure false refusal rate.

## Open Questions the Paper Calls Out

- **Multi-turn jailbreak mitigation**: What specific defense mechanisms can effectively mitigate "begin from benign" multi-turn jail-breaking strategies where malicious intent is introduced gradually? The tournament results showed attackers were successful when starting with benign requests and slowly turning them malicious, a pattern current defenses struggled to identify.

- **Scalable utility testing**: Does delegating utility testing to attacking teams provide a more scalable solution to the over-refusal problem than static, organizer-managed datasets? Static utility benchmarks could not keep pace with the dynamic evolution of attack strategies, risking over-refusal in untested areas.

- **Continuous competition format**: Can a continuous competition format with weighted scoring reduce the ranking volatility caused by defenders overfitting to specific attacks from previous discrete tournaments? The discrete tournament structure incentivized teams to hold back techniques or overfit to the previous round's data, creating artificial noise in rankings.

## Limitations

- Custom 8B Prize LLM limits reproducibility and external validation
- Human annotation lacks transparency regarding annotator qualifications and inter-annotator agreement
- 45-second latency constraint may artificially favor certain defense strategies
- Closed-box attack setting reduces realism compared to open-box scenarios
- Utility benchmarks may not fully capture real-world helpfulness

## Confidence

- **High Confidence**: Tournament results showing defenders improved safety while maintaining utility, and attackers shifted strategies from malicious explanations to vulnerable code generation.
- **Medium Confidence**: Claim that safety and utility represent non-competing objectives for vulnerable code generation.
- **Low Confidence**: Assertion that adversarial tournaments provide superior training signal compared to static benchmarks.

## Next Checks

1. **External Model Validation**: Replicate key tournament results using an open-source coding model (e.g., DeepSeek-Coder or CodeLlama) to test whether safety improvements transfer across model architectures.

2. **Utility Benchmark Externalization**: Test defender models on independent security coding benchmarks (e.g., SECCODE or custom vulnerable code detection tasks) to validate that tournament utility metrics predict real-world performance.

3. **Attack Diversity Analysis**: Conduct post-hoc analysis of attack strategies to quantify semantic diversity using embedding-based similarity metrics alongside BLEU scores, ensuring that normalized ASR isn't inflated by syntactic variations alone.