---
ver: rpa2
title: 'KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and
  Accelerated LLM Fine-Tuning'
arxiv_id: '2505.18886'
source_url: https://arxiv.org/abs/2505.18886
tags:
- kerzoo
- optimization
- gradient
- fine-tuning
- mezo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KerZOO, a kernel-function-informed zeroth-order
  optimization method for large language model (LLM) fine-tuning that addresses gradient
  estimation bias in existing zeroth-order approaches. By analytically characterizing
  the lower-order bias in gradient estimation and incorporating a carefully designed
  kernel function, KerZOO significantly reduces the number of iterations required
  for convergence while maintaining or improving accuracy.
---

# KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning

## Quick Facts
- arXiv ID: 2505.18886
- Source URL: https://arxiv.org/abs/2505.18886
- Reference count: 40
- Achieves up to 74% reduction in GPU training hours on WSC dataset while improving accuracy by 2.9% over MeZO baseline

## Executive Summary
This paper introduces KerZOO, a kernel-function-informed zeroth-order optimization method for large language model fine-tuning that addresses gradient estimation bias in existing zeroth-order approaches. By analytically characterizing the lower-order bias in gradient estimation and incorporating a carefully designed kernel function, KerZOO significantly reduces the number of iterations required for convergence while maintaining or improving accuracy. The method achieves substantial GPU training hour savings while improving task performance across multiple model scales and task types.

## Method Summary
KerZOO builds on standard zeroth-order gradient estimation but introduces a kernel function K₃(r) = C·(15/4)r(5-7r²) that systematically eliminates O(ε²) bias from third-order derivative terms. The method uses n=3 perturbation directions with Gaussian random vectors uᵢ normalized to unit sphere and scalar rᵢ sampled uniformly from [-1,1]. Each gradient estimate requires 6 forward passes (2 per perturbation pair) but achieves 70%+ iteration reduction through reduced bias. The approach is compatible with LoRA for parameter-efficient fine-tuning and maintains memory efficiency while providing significant training acceleration.

## Key Results
- 74% reduction in GPU training hours on WSC dataset with 2.9% accuracy improvement over MeZO baseline
- 44% reduction in GPU training hours on MultiRC dataset with 2.6% accuracy improvement
- Achieves 40%+ overall GPU-hour savings by trading 6× forward passes for 70%+ fewer iterations
- Maintains strong performance across model scales from OPT-2.7B to LLaMA-3-8B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Kernel functions systematically eliminate O(ε²) bias from zeroth-order gradient estimates
- **Mechanism:** Standard ZO gradient estimation introduces bias term E[(ε²/6)D³∇L(θ)[u,u,u]·u]. By introducing kernel K(r) satisfying E[r³K(r)]=0, this third-order term cancels under expectation, leaving only O(ε⁴) higher-order bias. The kernel K₃(r)=C·(15/4)r(5-7r²) achieves this via Legendre polynomial construction.
- **Core assumption:** The loss function L is at least third-order differentiable, enabling valid Taylor expansion through third-order terms.
- **Evidence anchors:**
  - [abstract]: "analytically identify and characterize the lower-order bias... kernel-function-based ZO framework aimed at mitigating this bias"
  - [section 3.3-3.4, equations 16-23]: Complete derivation showing E[ĝₖ] = (C/d)∇L(θ) + O(ε⁴) after kernel application, with explicit moment conditions
  - [corpus]: Limited direct corroboration; arXiv:2510.19953 addresses ZO bias but uses different approach
- **Break condition:** If loss landscape has discontinuous third derivatives or non-smooth regions, Taylor expansion invalidates and bias elimination fails.

### Mechanism 2
- **Claim:** Using n=3 perturbations with kernel weighting provides optimal accuracy-speed tradeoff
- **Mechanism:** Multiple perturbations reduce estimation variance through averaging, while kernel weights maintain unbiasedness. The paper empirically finds n=3 balances the 6× forward-pass overhead against convergence acceleration (70%+ iteration reduction).
- **Core assumption:** Random directions uᵢ are independently sampled from unit Gaussian sphere, and rᵢ uniformly from [-1,1], ensuring statistical validity of moment conditions.
- **Evidence anchors:**
  - [section 4.1]: "we set the number of perturbation directions to n = 3"
  - [table 1]: Consistent 1.7%-7.4% accuracy improvements across SST-2, MNLI, RTE with n=3
  - [figure 4]: Shows MeZO-3 (3 perturbations without kernel) provides marginal improvement vs. KerZOO's significant acceleration
  - [corpus]: arXiv:2510.18228 mentions variance reduction in ZO but via gradient-aligned perturbations, not kernels
- **Break condition:** If perturbations become correlated or batch size too small, variance reduction insufficient and convergence slows.

### Mechanism 3
- **Claim:** Shrinking perturbation magnitude r over iterations balances bias-variance tradeoff dynamically
- **Mechanism:** Early training uses larger r for exploration; as optimization progresses, shrinking r reduces variance when near optima. This adapts to changing optimization needs without sacrificing the kernel's bias elimination.
- **Core assumption:** Optimal perturbation scale varies during training—larger initially for exploration, smaller near convergence for precision.
- **Evidence anchors:**
  - [section 3.3]: "we can limit the r in a smaller range as the iteration step increases"
  - [appendix 9.1]: "gradually shrink the range of r as the number of iterations increases... trade-off between variance and bias"
  - [corpus]: No direct corpus evidence for this specific strategy
- **Break condition:** If shrinking schedule conflicts with learning rate decay or converges too aggressively, optimization may stall in suboptimal regions.

## Foundational Learning

- **Concept: Zeroth-Order Optimization via Finite Differences**
  - **Why needed here:** KerZOO builds on standard ZO gradient estimation ĝ = [f(x+εu)-f(x-εu)]/(2ε)·u. Understanding why this approximates gradients (and its limitations) is prerequisite.
  - **Quick check question:** Why does the symmetric difference [f(x+εu)-f(x-εu)]/(2ε) provide better gradient estimates than one-sided [f(x+εu)-f(x)]/ε?

- **Concept: Taylor Series Bias Analysis**
  - **Why needed here:** The paper's core contribution requires understanding how Taylor expansion reveals O(ε²) bias from third-order derivatives D³L(θ)[u,u,u].
  - **Quick check question:** If a loss function has L'''(x) = 0 everywhere, would standard ZO still have O(ε²) bias? Why or why not?

- **Concept: Moment Conditions and Orthogonal Polynomials**
  - **Why needed here:** Kernel design uses Legendre polynomials to satisfy E[r³K(r)]=0. Understanding moment-matching connects kernel theory to bias elimination.
  - **Quick check question:** Why does the condition E[r³K(r)]=0 specifically target O(ε²) bias rather than O(ε) or O(ε³)?

## Architecture Onboarding

- **Component map:**
  Perturbation Sampler -> Kernel Evaluator -> Forward Pass Executor -> Gradient Aggregator -> Parameter Updater

- **Critical path:**
  1. Sample 3 perturbation pairs (rᵢ,uᵢ) with current r-range
  2. Execute 6 forward passes through LLM (no backward pass)
  3. Weight each difference by K₃(rᵢ)
  4. Aggregate weighted gradient estimate
  5. Update θ with learning rate η and clipping
  6. Shrink r-range according to iteration schedule

- **Design tradeoffs:**
  - **n=3 perturbations:** 6× forward passes vs. MeZO's 2×, but 70%+ iteration reduction yields net 40%+ GPU-hour savings
  - **K₃ vs. K₅ kernel:** K₃ eliminates O(ε²) bias; K₅ would also eliminate O(ε⁴) but increases complexity with unclear empirical benefit
  - **Memory overhead:** 16.9G vs. MeZO's 12.8G (32% increase) traded for 42% fewer iterations—acceptable for GPU-constrained scenarios

- **Failure signatures:**
  - **Convergence slower than MeZO:** Verify kernel constant C=4 is applied correctly; check if r-range shrinking too aggressive
  - **Loss oscillation:** Perturbation scale ε may be too large; reduce from 1e-3 toward 1e-4
  - **Memory exceeds expected:** Ensure using seed-based perturbation regeneration (MeZO-style) rather than storing all uᵢ vectors
  - **Accuracy degrades mid-training:** R-range shrinking schedule may be incompatible with learning rate; synchronize schedules

- **First 3 experiments:**
  1. **Validation on RoBERTa-large/SST-2:** Implement KerZOO with K₃, n=3, ε=1e-3; compare training loss curves against MeZO baseline. Expect ~5× faster convergence (Figure 1a shows ~4.9× at 5000 iterations).
  2. **Ablation on perturbation count:** Test n∈{1,2,3,5} on RTE dataset with RoBERTa-large. Validate n=3 as sweet spot where additional perturbations yield diminishing returns.
  3. **Scale test on OPT-2.7B/WSC:** Full pipeline with LoRA integration. Target: <10G memory, >65% accuracy (Table 2 shows 65.4%), <30% of MeZO GPU hours. Monitor for numerical stability with larger model.

## Open Questions the Paper Calls Out
The paper explicitly identifies future extensions to vision-language model adaptation and LLM pruning/quantization as promising directions, though these remain unexplored in the current work.

## Limitations
- Experimental scope limited to standard NLP fine-tuning tasks without validation on vision-language or pruning applications
- No ablation studies on kernel order (K₃ vs K₅) to quantify benefits of higher-order bias elimination
- Shrinking schedule for perturbation range r is heuristic without theoretical analysis of optimal decay rates

## Confidence
- Method validity: High - Analytical derivation of bias elimination is mathematically rigorous
- Empirical results: Medium - Strong results but limited to specific benchmark tasks
- Reproducibility: Medium - Key hyperparameters specified but some implementation details missing

## Next Checks
1. Verify K₃ kernel implementation satisfies E[r³K(r)]=0 by numerical integration over [-1,1]
2. Reproduce convergence comparison on RoBERTa-large/SST-2 with k=16 to validate 5× speedup claim
3. Test memory usage with LoRA integration on OPT-2.7B to confirm <10G memory target is achievable