---
ver: rpa2
title: BERT-based model for Vietnamese Fact Verification Dataset
arxiv_id: '2503.00356'
source_url: https://arxiv.org/abs/2503.00356
tags:
- dataset
- verification
- claim
- evidence
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a BERT-based approach for fact verification
  on a Vietnamese dataset. The method integrates sentence selection and classification
  modules using pre-trained PhoBERT and XLM-RoBERTa models.
---

# BERT-based model for Vietnamese Fact Verification Dataset

## Quick Facts
- arXiv ID: 2503.00356
- Source URL: https://arxiv.org/abs/2503.00356
- Reference count: 0
- Primary result: BERT-based approach achieves 75.11% Strict Accuracy on Vietnamese fact verification

## Executive Summary
This paper presents a BERT-based approach for fact verification on a Vietnamese dataset. The method integrates sentence selection and classification modules using pre-trained PhoBERT and XLM-RoBERTa models. The approach demonstrates significant improvements over the baseline, achieving a Strict Accuracy of 75.11%, which represents a 28.83% improvement. The model effectively handles Vietnamese fact verification tasks by leveraging the power of large language models and a unified network architecture.

## Method Summary
The approach uses a two-stage pipeline: (1) Rationale Selection using PhoBERT to identify relevant evidence sentences through binary classification, and (2) Label Classification using xlm-roberta-large-xnli to predict SUPPORTED, REFUTED, or NEI verdicts through 3-way classification. The model concatenates three [CLS] token embeddings from different hidden states to capture richer semantic information. Training involves optimizing both modules separately with cross-entropy loss, and inference retrieves the top-1 evidence sentence before classification. The approach significantly improves upon the baseline BERT-FEVER model.

## Key Results
- Achieves 75.11% Strict Accuracy, representing a 28.83% improvement over baseline
- Rationale Selection module achieves 82.30% accuracy in evidence retrieval
- Single-phase classification outperforms two-phase cascade approach
- PhoBERT selected for rationale selection, XLM-RoBERTa-xnli for label classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concatenating CLS token embeddings from multiple BERT layers captures richer semantic information for Vietnamese sentence selection than using only the final layer.
- **Mechanism:** The authors extract 3 CLS token embeddings from different hidden states and concatenate them before feeding to the MLP classifier. This likely captures both surface-level and abstract representations of the claim-evidence pair.
- **Core assumption:** Intermediate BERT layers encode complementary information relevant to evidence relevance that the final layer alone may lose.
- **Evidence anchors:**
  - [section] "not only do we take one 1 embedding [CLS] token in the last hidden state, but we also take 3 embedding of [CLS] tokens and then concatenate them to capture more information" (Section 3.2)
  - [corpus] Weak/missing—no related papers specifically validate multi-layer CLS concatenation for Vietnamese fact verification.

### Mechanism 2
- **Claim:** Pre-training on XNLI (a multilingual natural language inference dataset) transfers effectively to Vietnamese fact verification classification.
- **Mechanism:** The xlm-roberta-large-xnli checkpoint was already fine-tuned on XNLI, which includes Vietnamese and involves 3-way classification (entailment/contradiction/neutral). This aligns structurally with the fact verification labels (SUPPORTED/REFUTED/NEI).
- **Core assumption:** The NLI task structure is sufficiently similar to fact verification that representations transfer without catastrophic interference.
- **Evidence anchors:**
  - [section] "the model xlm-robert-large-xnli performs better than the two other models on the evidence retrieval task (82.30% for xlm-robert-large-xnli compared to 79.41% for XLM-RoBERTa-Large and 74.22% for PhoBERT-Large)" (Section 4.2, Table 5)
  - [section] "Because the XNLI dataset is a multilingual dataset for the Natural language inference task (classify 3 labels like our task, it has also been trained on a Vietnamese dataset)" (Section 4.2)
  - [corpus] SemViQA (arXiv:2503.00955) addresses Vietnamese fact-checking but does not specifically validate XNLI pre-training transfer.

### Mechanism 3
- **Claim:** A single-stage 3-way classifier outperforms a 2-stage binary cascade for this Vietnamese fact verification task.
- **Mechanism:** The 1-phase approach directly predicts {SUPPORTED, REFUTED, NEI} in one model, while the 2-phase approach first classifies {RELEVANT, NOT-RELEVANT} then {SUPPORTED, REFUTED}. Single-stage avoids error propagation between stages.
- **Core assumption:** The model has sufficient capacity to learn the 3-way decision boundary without intermediate supervision.
- **Evidence anchors:**
  - [section] "the result of 2-phase label classification is not as good as 1-phase on all three metrics" (Section 4.2)
  - [section] "the outcome of the 1-phase approach is generally better than that of the 2-phase approach (75.11% compared to 72%)" (Section 4.2, Table 3)
  - [corpus] Trification (arXiv:2512.00267) proposes tree-based multi-stage verification but for English; no direct corpus evidence on Vietnamese single vs. multi-stage comparison.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here:** Fact verification is framed as an NLI-style task where claims are evaluated against evidence as SUPPORTED (entailment), REFUTED (contradiction), or NEI (neutral).
  - **Quick check question:** Given the premise "Vietnam Airlines received hundreds of applications" and hypothesis "No applications were received," what is the NLI label?

- **Concept: Transformer [CLS] token representation**
  - **Why needed here:** The architecture uses the [CLS] token embedding as the pooled representation for classification; understanding what it encodes is essential for debugging.
  - **Quick check question:** What does the [CLS] token represent in BERT, and why is it positioned at the start of the sequence?

- **Concept: Fine-tuning vs. feature extraction**
  - **Why needed here:** The approach fine-tunes PhoBERT and XLM-RoBERTa rather than using frozen embeddings; this updates all weights based on the Vietnamese dataset.
  - **Quick check question:** What is the difference between extracting fixed BERT embeddings and fine-tuning BERT on a downstream task?

## Architecture Onboarding

- **Component map:** Input: [CLS] claim [SEP] sentence [SEP] → Rationale Selection (PhoBERT backbone) → 3-layer CLS concatenation → MLP + Sigmoid → evidence score → Select top-1 sentence → Label Classification (XLM-RoBERTa-xnli backbone) → CLS embedding → MLP + Softmax → {SUPPORTED, REFUTED, NEI}

- **Critical path:**
  1. Preprocess: Split corpus into sentences using spaCy; lowercase; remove meaningless punctuation
  2. Rationale selection training: Binary cross-entropy on (claim, evidence) pairs; label=1 if evidence supports/refutes, else 0
  3. Label classification training: Cross-entropy on (claim, evidence, verdict) triples; NEI examples use top-2 similar sentences

- **Design tradeoffs:**
  - PhoBERT for rationale selection vs. XLM-RoBERTa: PhoBERT is monolingual Vietnamese, better for NLU; XLM-RoBERTa-xnli has NLI pre-training, better for classification
  - 1-phase vs. 2-phase: Single-stage avoids error propagation but may be harder to debug; two-stage offers intermediate interpretability

- **Failure signatures:**
  - Confusion between REFUTED and NEI when claim and evidence differ by one word (e.g., "Khang" vs. "He"): model predicts REFUTED when human labels NEI
  - Evidence retrieval accuracy (59.43%) significantly lower than classification accuracy (82.30%)—bottleneck is in rationale selection

- **First 3 experiments:**
  1. **Baseline replication:** Train mBERT-based BERT-FEVER on ISE-DSC01; verify ~46% strict accuracy matches reported baseline
  2. **Ablation on CLS concatenation:** Compare 1-layer vs. 3-layer CLS concatenation in rationale selection module; expect 1-3% difference
  3. **Backbone swap:** Test PhoBERT for both modules vs. current setup; isolate whether XNLI pre-training or language-specific pre-training matters more for classification

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively does the unified network architecture adapt to fact verification tasks in other low-resource languages or different datasets?
- **Basis in paper:** [explicit] The Introduction states, "Future research may explore the adaptation of this approach to other datasets and languages."
- **Why unresolved:** The study focuses exclusively on the Vietnamese ISE-DSC01 dataset, and the authors note that direct applicability to other languages remains uncertain due to linguistic disparities.
- **What evidence would resolve it:** Performance benchmarks of the same pipeline on standard datasets for other languages (e.g., FEVER for English, DanFEVER for Danish) without architectural changes.

### Open Question 2
- **Question:** Can the model be refined to better distinguish between "REFUTED" and "NEI" (Not Enough Information) labels when evidence contains subtle entity mismatches?
- **Basis in paper:** [inferred] Table 7 highlights a specific failure mode where the model incorrectly predicts "REFUTED" instead of "NEI" when a sentence differs by only a single entity ("He" vs. "Khang"), suggesting a sensitivity to partial overlaps rather than strict contradictions.
- **Why unresolved:** The current classification module appears to rely heavily on surface-level semantic matching, failing to recognize when a mismatch constitutes a lack of information versus a refutation.
- **What evidence would resolve it:** Improved Strict Accuracy scores on a specifically curated test set of "near-miss" NEI examples where the evidence is semantically similar but refers to a different entity.

### Open Question 3
- **Question:** Why does the hierarchical 2-phase classification approach underperform compared to the single-phase approach, and can this error propagation be mitigated?
- **Basis in paper:** [inferred] Section 4.2 reports that the 2-phase model achieved 72.00% Strict Accuracy, notably lower than the 1-phase model's 75.11%, despite the theoretical advantage of separating relevance detection from verdict classification.
- **Why unresolved:** The paper observes the performance gap but does not provide an ablation study or analysis explaining why the separate binary classification steps degrade performance.
- **What evidence would resolve it:** An error analysis quantifying how often the first stage (Relevant/Not-Relevant) filters out correct evidence, causing errors in the second stage.

## Limitations

- Vietnamese-specific preprocessing challenges (diacritics, compound words, informal vs. formal text) are not addressed
- No error analysis provided for evidence retrieval failures, which show lower accuracy than classification
- Limited generalizability to other Vietnamese datasets or claim types beyond news articles
- Assumption of independent claims ignores potential multi-hop reasoning requirements

## Confidence

- **High confidence** in the overall framework architecture and reported metrics (75.11% Strict Accuracy is a substantial improvement over the 46.38% baseline)
- **Medium confidence** in the specific implementation details such as hidden layer sizes, exact hyperparameter combinations, and Vietnamese preprocessing steps, as these are either underspecified or may vary based on SpaCy model versions
- **Low confidence** in the generalizability of results to other Vietnamese fact verification datasets or claim types beyond news articles, since no cross-dataset validation is performed

## Next Checks

1. **Ablation study on CLS concatenation:** Systematically compare rationale selection performance using 1-layer, 2-layer, and 3-layer [CLS] concatenation against max-pooling across all layers to isolate the actual contribution of multi-layer embeddings for Vietnamese sentence selection.

2. **Cross-dataset validation:** Evaluate the trained model on SemViQA (Vietnamese fact-checking) or other Vietnamese news verification datasets to assess whether the 75.11% accuracy generalizes beyond ISE-DSC01, and identify domain-specific failure patterns.

3. **Error analysis pipeline:** Implement a detailed failure analysis focusing on: (a) retrieval errors where relevant evidence exists but isn't selected, (b) classification errors where REFUTED vs. NEI are confused, and (c) cases where single evidence sentence is insufficient for verdict determination.