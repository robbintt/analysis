---
ver: rpa2
title: 'Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation'
arxiv_id: '2506.18158'
source_url: https://arxiv.org/abs/2506.18158
tags:
- memory
- information
- agent
- arxiv
- long-term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Chain-of-Memory (CoM), a novel method that
  enhances GUI agents' ability to handle complex, cross-application tasks by incorporating
  both short-term and long-term memory mechanisms. CoM addresses the challenge of
  agents struggling to retain and utilize critical historical information when navigating
  lengthy, multi-app workflows.
---

# Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation

## Quick Facts
- arXiv ID: 2506.18158
- Source URL: https://arxiv.org/abs/2506.18158
- Reference count: 10
- Key outcome: Introduces Chain-of-Memory (CoM) to enhance GUI agents' cross-application navigation by incorporating short-term and long-term memory mechanisms

## Executive Summary
This paper addresses the challenge of GUI agents struggling to retain and utilize critical historical information when navigating lengthy, multi-app workflows. The authors propose Chain-of-Memory (CoM), a method that captures action descriptions for short-term memory, extracts task-relevant screen information, and maintains a long-term memory module. To evaluate CoM, the authors created GUI Odyssey-CoM, a large dataset (111k screen-action pairs) annotated with memory information. Experimental results show that CoM significantly improves GUI agents' operational accuracy, task success rates, and task-switching success rates in cross-application tasks. Notably, fine-tuning on the GUI Odyssey-CoM dataset enabled a 7B model to achieve memory management capabilities comparable to a 72B model, validating the effectiveness of CoM and the necessity of the dataset for training smaller models.

## Method Summary
CoM consists of four stages: (1) Information Perception, where the agent compares current and previous screens to generate Action Results and Screen Information; (2) STM Update, which appends action results to a FIFO buffer with capacity N=4; (3) LTM Storage, where a utility evaluation function determines if screen information should be saved to persistent memory; and (4) Action Decision, where the agent predicts the next action using the task query, current screen, STM, and LTM. The method was evaluated using the GUI Odyssey-CoM dataset, which was created by annotating the existing GUI Odyssey dataset with explicit memory chains using a 72B model.

## Key Results
- CoM significantly improves GUI agents' operational accuracy, task success rates, and task-switching success rates in cross-application tasks
- Fine-tuning on GUI Odyssey-CoM enabled a 7B model to achieve memory management capabilities comparable to a 72B model
- LTM notably improves the accuracy of text input actions by enabling the agent to retrieve and use previously stored information
- STM capacity of N=4 shows diminishing returns beyond this threshold, with performance plateauing due to utilization limitations or increased computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Short-Term Memory (STM) as Semantic State Tracking
- Replaces raw action logs with semantic descriptions of action results to improve task state tracking
- Stores textual representations of action results in a FIFO buffer (capacity N=4) instead of relying on historical screenshots or coordinate logs
- Assumes the model can reliably summarize visual deltas into accurate text, making this representation more efficient for attention mechanisms than raw visual tokens

### Mechanism 2: Long-Term Memory (LTM) for Cross-App Information Transfer
- Enables agents to retain critical data (e.g., search results, phone numbers) when switching application contexts
- Uses an explicit memory storage module that extracts "Screen Info," evaluates its future utility, and stores core information in a persistent LTM list
- Assumes the evaluation function can accurately predict which current details will be relevant for future, unseen subtasks

### Mechanism 3: Supervised Fine-Tuning (SFT) for Cognitive Alignment
- Allows smaller models (7B) to mimic the memory management capabilities of much larger models (72B)
- Trains smaller models to predict not just the action, but the intermediate memory steps based on high-quality annotations from a 72B teacher model
- Assumes the quality of the 72B teacher model's annotations is sufficient to serve as ground truth for the student model

## Foundational Learning

**Concept: Context Window Limits & Sliding Attention**
- Why needed here: Standard MLLMs lose critical information in long cross-app tasks due to fixed context windows
- Quick check question: Can you explain why raw screenshots from Step 1 are often unavailable to the model at Step 15 in a standard transformer architecture?

**Concept: Multimodal Grounding (Vision-to-Action)**
- Why needed here: The agent must map textual decisions ("Click Home") to specific screen coordinates or UI elements
- Quick check question: How does the model determine the (x,y) coordinates for a "CLICK" action based on a visual screen input?

**Concept: Chain-of-Thought (CoT) Reasoning**
- Why needed here: CoM is structurally similar to CoT, but focuses on memory storage rather than just reasoning steps
- Quick check question: How does generating intermediate text (reasoning/memory) before the final action potentially improve output accuracy?

## Architecture Onboarding

**Component map:** Current Screen (St) -> Previous Screen (St-1) -> Perception Module -> Action Result (AR) -> STM Buffer (Mt) -> Memory Manager -> LTM Buffer (MLt) -> Agent Decision -> Action (At)

**Critical path:** The LTM Evaluation step (Section 3.2, Eq 3). If the model incorrectly tags critical info as "not worth saving" (Lt = false), the downstream decision will lack necessary context (e.g., forgetting a verification code).

**Design tradeoffs:**
- STM Capacity (N): The paper sets N=4. While higher N improves AMS, returns diminish; excessive history adds noise (Section 5.3)
- Screen Info vs. LTM: The paper notes that providing raw "Screen Info" actually hurts performance compared to using filtered LTM (Table 3), suggesting that summarization is strictly better than raw extraction

**Failure signatures:**
- High Type Error Rate: Suggests LTM is failing to save text strings (e.g., addresses) needed later
- High Stop/Home Error Rate: Suggests STM is failing to track that the current sub-task is actually complete

**First 3 experiments:**
1. Ablate STM Capacity: Run inference with N=0, 2, 4, 8 to verify the "diminishing returns" plateau on Task Success Rate
2. LTM Noise Injection: Manually insert irrelevant facts into the LTM buffer to test the model's robustness to "memory pollution"
3. Cross-Domain Transfer: Test the fine-tuned 7B model on a dataset with apps not present in GUI-Odyssey to verify generalization vs. memorization

## Open Questions the Paper Calls Out
- How can the Screen Information extraction mechanism be refined to prevent the decision-making impedance observed when it is used as a standalone input?
- Can advanced retrieval or compression mechanisms overcome the "diminishing returns" observed in Short-Term Memory (STM) capacity beyond a length of N=4?
- How susceptible is the Chain-of-Memory pipeline to error propagation, where inaccuracies in early "Action Descriptions" or "Screen Information" corrupt subsequent Long-Term Memory updates?

## Limitations
- STM generation mechanism relies on text-summary of visual differences, which may introduce hallucination risk if the model mischaracterizes screen state changes
- LTM evaluation function `f` is not fully specified, creating uncertainty about whether the model truly learns what information to retain versus simply memorizing patterns from training data
- The "comparable to 72B" claim depends heavily on the quality of the teacher model's annotations, but the paper doesn't validate whether these annotations are error-free

## Confidence
- High confidence: STM improves task state tracking by reducing redundant visual data in the context window
- Medium confidence: LTM effectively transfers critical information across application boundaries, though the evaluation mechanism's reliability is uncertain
- Medium confidence: Fine-tuning on GUI Odyssey-CoM enables 7B models to match 72B performance, though this depends on annotation quality and may not generalize to different domains

## Next Checks
1. Conduct a human evaluation of the 72B-generated annotations to measure hallucination rates in STM descriptions and LTM evaluations
2. Test the fine-tuned 7B model on a held-out dataset containing apps not present in GUI Odyssey to verify generalization versus memorization
3. Perform ablation studies on the LTM evaluation function `f` by comparing performance when LTM is populated with ground-truth critical information versus the model's predicted selections