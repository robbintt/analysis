---
ver: rpa2
title: 'CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal
  Fusion Method for Timestamped QA'
arxiv_id: '2512.00360'
source_url: https://arxiv.org/abs/2512.00360
tags:
- retrieval
- reranker
- learned
- clip
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CourseTimeQA is a lecture-video benchmark (52.3 h, 902 queries)
  for timestamped QA under single-GPU constraints. CrossFusion-RAG improves nDCG@10
  by 0.10 and MRR by 0.08 over BLIP-2 retrieval while maintaining ~1.55 s median latency
  on an A100.
---

# CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA

## Quick Facts
- arXiv ID: 2512.00360
- Source URL: https://arxiv.org/abs/2512.00360
- Authors: Vsevolod Kovalev; Parteek Kumar
- Reference count: 25
- CourseTimeQA is a lecture-video benchmark (52.3 h, 902 queries) for timestamped QA under single-GPU constraints. CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over BLIP-2 retrieval while maintaining ~1.55 s median latency on an A100.

## Executive Summary
CourseTimeQA introduces a new benchmark for timestamped QA on lecture videos with 52.3 hours of content and 902 queries. The paper presents CrossFusion-RAG, a latency-constrained method that achieves significant improvements in retrieval quality through query-agnostic cross-modal fusion. The method maintains sub-2 second latency on a single A100 GPU while demonstrating strong performance across STEM courses, with particular benefits for visual-content-heavy queries.

## Method Summary
CrossFusion-RAG uses frozen mpnet and OpenCLIP encoders with a learned 512→768 projection for vision, followed by query-agnostic cross-attention fusion over ASR tokens and frames. The method employs a 2-layer cross-attentive reranker with temporal consistency regularization and MMR diversification. Retrieval operates on 20s windows with 10s stride, producing 18,828 indexed segments via FAISS. The system achieves nDCG@10 of 0.84 versus 0.74 for BLIP-2 baseline under identical hardware constraints.

## Key Results
- nDCG@10: 0.84 (CrossFusion) vs. 0.74 (BLIP-2) on CourseTimeQA test
- MRR: 0.69 (CrossFusion) vs. 0.61 (BLIP-2) on CourseTimeQA test
- Median latency: ~1.55 s (retrieval 0.09s, reranking 0.14s, diversification 0.03s, generation 1.27s) on A100

## Why This Works (Mechanism)

### Mechanism 1
Query-agnostic cross-attention fusion enables offline indexable cross-modal representations. ASR tokens act as queries in a shallow 2-layer Transformer cross-attention over N=4 frame embeddings; outputs are pooled to a 768-d segment vector indexed via FAISS. Because fusion does not depend on the user query, retrieval reduces to a bi-encoder cosine similarity lookup. Core assumption: ASR tokens provide sufficient signal to contextualize visual frames; important visual concepts co-occur with spoken descriptions. Break condition: If lectures have significant visual content (diagrams, equations) without corresponding verbal description, ASR-query fusion may underweight purely visual evidence.

### Mechanism 2
A learned 512→768 linear projection aligns visual embeddings to the text embedding space with minimal parameters. OpenCLIP ViT-B/16 produces 512-d frame embeddings; a learned linear layer projects to 768-d to match mpnet's text space, enabling direct cosine similarity computation after fusion. Core assumption: A linear mapping suffices for cross-modal alignment in this domain; frozen encoders already provide useful representations. Break condition: If visual and text semantic spaces have highly non-linear misalignment, a linear projection may degrade retrieval quality versus joint fine-tuning.

### Mechanism 3
Temporal consistency regularization (λ=0.1) reduces over-segmentation drift across overlapping windows. Overlapping 20s windows with 10s stride produce redundant segments; the temporal loss encourages consistent representations for overlapping content, reducing spurious rank discontinuities. Core assumption: Adjacent segments should have similar relevance for overlapping queries; over-segmentation artifacts hurt ranking. Break condition: If lectures have rapid topic transitions within 10s, smoothing may conflate distinct segments and hurt precision.

## Foundational Learning

- **InfoNCE contrastive learning**
  - Why needed here: All learned models (CrossFusion, late-fusion gating, BLIP-2 baseline) are trained with InfoNCE loss over in-batch negatives.
  - Quick check question: Can you explain why InfoNCE uses temperature scaling (τ=0.07) and how it affects gradient magnitude for hard vs. easy negatives?

- **Bi-encoder vs. cross-encoder retrieval**
  - Why needed here: CrossFusion uses bi-encoder indexing for latency (0.09s retrieval) with a small cross-attentive reranker (0.14s) over top-50; understanding this tradeoff is critical.
  - Quick check question: Why does a cross-encoder provide higher accuracy than a bi-encoder, and why is it impractical for first-stage retrieval over 18,828 segments?

- **MMR diversification**
  - Why needed here: MMR (α=0.6) trades relevance for diversity, improving nDCG@10 with negligible MRR change.
  - Quick check question: How does MMR's α parameter control the balance between relevance and redundancy, and why might α=0.6 be preferred over α=0.9?

## Architecture Onboarding

- **Component map:** Frozen encoders (mpnet text 768-d, OpenCLIP ViT-B/16 vision 512-d) -> Learned projection (512→768) -> Fusion module (2-layer Transformer cross-attention ASR→frames) -> Indexing (FAISS IndexFlatIP over L2-normalized 768-d segment vectors) -> Reranker (2-layer cross-attention hidden 256, 4 heads) -> Diversification (MMR α=0.6) -> Generator (Mistral-7B-Instruct frozen int8)

- **Critical path:** Latency breakdown (median): Retrieval 0.09s → Reranking 0.14s → Diversification 0.03s → Generation 1.27s (total ~1.55s). Generation dominates; optimize retrieval/reranking only if generation is offloaded or streamed.

- **Design tradeoffs:** Window/stride (20s/10s): Smaller stride increases IoU@0.5 but grows index size. Frames/segment (N=4): More frames improve diagram coverage but increase reranker cost. Reranker depth: 2 layers recover most cross-encoder benefits at 0.14s; 1 layer is faster with small accuracy drop. MMR α: α=0.6 improves nDCG@10 with negligible MRR change; higher α may hurt diversity.

- **Failure signatures:** High ASR WER (Q4 quartile): nDCG@10 drops from 0.88 (Q1) to 0.79 (Q4). Expect degraded retrieval for poorly transcribed lectures. Diagram-heavy courses: Visual features help (relative nDCG gain 1.1–1.3× vs. text-only), but fusion may underperform if ASR lacks visual terminology. Hallucination: 8% rate with CrossFusion + Mistral-7B; monitor unsupported propositions in generated answers.

- **First 3 experiments:**
  1. Reproduce BLIP-2 baseline vs. CrossFusion on a single held-out course: Confirm reported nDCG@10 gap (0.74 vs. 0.84) under identical FAISS indexing and hardware.
  2. Ablate temporal consistency loss (λ=0 vs. λ=0.1): Measure R@1@0.5 and R@5@0.5 to verify Table VI localization improvements.
  3. Stress-test ASR robustness: Bin test queries by WER quartiles and plot nDCG@10 degradation curves for CrossFusion vs. text-only hybrid to quantify multimodal resilience.

## Open Questions the Paper Calls Out

### Open Question 1
Does CrossFusion-RAG's relative advantage generalize to disciplines and production styles beyond the six courses evaluated? Basis in paper: "External validity is limited by six courses from a small set of providers; results may differ across disciplines and production styles." Why unresolved: Leave-one-course cross-validation addresses generalization across included providers, but the benchmark does not span diverse lecture formats (e.g., labs, seminars, non-English instruction). What evidence would resolve it: Evaluation on a broader corpus spanning multiple institutions, languages, and lecture formats with similar latency constraints.

### Open Question 2
Can CrossFusion-RAG meet the <2.5 s latency target on consumer-grade or shared campus GPUs with lower memory? Basis in paper: "A single A100 meets latency targets; deployment to shared campus GPUs is feasible." Feasibility is claimed but not demonstrated. Why unresolved: All experiments use a single A100 80 GB; memory footprint and latency on lower-end hardware are unreported. What evidence would resolve it: Benchmarking latency and accuracy degradation on GPUs with ≤24 GB memory (e.g., RTX 3090/4090) under identical indexing.

### Open Question 3
What is the WER threshold beyond which CrossFusion-RAG's visual fusion no longer compensates for ASR degradation? Basis in paper: Table V shows performance drops across WER quartiles (nDCG@10: 0.88→0.79), but the highest quartile may not represent the failure point. Why unresolved: ASR robustness is analyzed within observed WER distribution; no controlled WER ceiling or synthetic noise study is reported. What evidence would resolve it: Controlled experiments with progressively degraded ASR (synthetic noise or subsampling) to identify the WER crossover where visual fusion fails to help.

### Open Question 4
Would query-aware cross-attention fusion improve retrieval accuracy at acceptable latency cost? Basis in paper: The design uses query-agnostic fusion for offline indexability; this trades potential accuracy for efficiency, but the trade-off curve is unexplored. Why unresolved: No ablation compares offline query-agnostic fusion against online query-aware fusion under matched latency budgets. What evidence would resolve it: Ablation with online cross-attention using the user query, measuring nDCG@10 vs. latency, with early-exit or caching to bound latency.

## Limitations

- Domain generalization limited to six STEM courses; effectiveness on non-STEM domains remains untested
- Significant performance degradation with poor ASR quality (8% gap between Q1 and Q4 WER quartiles)
- Real-world deployment performance on consumer-grade GPUs not demonstrated

## Confidence

**High Confidence Claims:**
- CrossFusion-RAG architecture and implementation details
- Benchmark creation and basic statistics (902 queries, 52.3 hours)
- Latency measurements under specified hardware constraints
- Relative performance improvements over BLIP-2 baseline

**Medium Confidence Claims:**
- Generalization of results to non-STEM lecture domains
- Real-world deployment performance with varying ASR quality
- Long-term stability of learned cross-modal representations

**Low Confidence Claims:**
- Applicability to non-lecture video domains
- Performance with languages other than English
- Scalability beyond single-GPU constraints

## Next Checks

1. **Domain Transfer Experiment** - Apply CrossFusion-RAG to a non-STEM lecture course (e.g., history or literature) and measure performance degradation relative to STEM courses, particularly for queries requiring visual reasoning without corresponding ASR descriptions.

2. **ASR Robustness Stress Test** - Systematically degrade ASR quality using controlled noise injection and measure the performance curve across the full WER spectrum, comparing against both text-only and vision-only retrieval baselines.

3. **Cross-Encoder Baseline Comparison** - Implement a high-quality cross-encoder retriever (e.g., ColPali) with aggressive pruning or caching strategies to achieve comparable latency, then measure the actual accuracy gap versus CrossFusion-RAG's bi-encoder + reranker approach.