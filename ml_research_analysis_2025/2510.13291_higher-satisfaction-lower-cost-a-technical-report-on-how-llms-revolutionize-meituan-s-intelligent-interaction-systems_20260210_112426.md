---
ver: rpa2
title: 'Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize
  Meituan''s Intelligent Interaction Systems'
arxiv_id: '2510.13291'
source_url: https://arxiv.org/abs/2510.13291
tags:
- data
- interaction
- intelligent
- systems
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WOWService, an intelligent interaction system
  that addresses key challenges in customer service by leveraging Large Language Models
  (LLMs) and multi-agent architectures. The system tackles issues like constructing
  high-quality training data, improving multi-turn dialogue performance, adapting
  to evolving business rules, and enabling effective multi-agent collaboration.
---

# Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems

## Quick Facts
- arXiv ID: 2510.13291
- Source URL: https://arxiv.org/abs/2510.13291
- Reference count: 6
- Primary result: Intelligent interaction system achieving 27.53% reduction in USM 1 and 25.51% increase in USM 2

## Executive Summary
WOWService is an intelligent interaction system developed by Meituan that leverages Large Language Models and multi-agent architectures to address key challenges in customer service. The system tackles issues including constructing high-quality training data, improving multi-turn dialogue performance, adapting to evolving business rules, and enabling effective multi-agent collaboration. Through a four-stage training pipeline and a unified dialogue-action model, WOWService achieves significant improvements in user satisfaction metrics. Deployed on the Meituan App, it demonstrates notable gains in understanding user needs and delivering personalized service while maintaining low operational costs.

## Method Summary
The method employs a four-stage training pipeline: Continual Pre-Training (CPT) with adaptive data mixture optimization, Supervised Fine-Tuning (SFT) with unified dialogue-action modeling, Direct Preference Optimization (DPO) for preference learning, and Reinforcement Learning (RL) for reasoning enhancement. The system uses a data-knowledge dual-driven architecture that fuses human-human conversational data with knowledge-operational data through prompt alignment, enabling minute-level business rule adaptation via retrieval-augmented generation. Self-Refinement Training (SRT) enables continuous improvement from production data without heavy manual annotation by classifying interactions into "Good Cases" and "Bad Cases" for iterative refinement. A multi-agent architecture with dynamic invocation improves complex task handling, where a master dialogue agent maintains conversational control while invoking specialized sub-agents as executable tools.

## Key Results
- USM 1 reduced by 27.53% (lower is better), indicating improved problem resolution
- USM 2 increased by 25.51% (higher is better), showing enhanced user satisfaction
- RL-CoT achieved 71.88% accuracy, +1.05pp over baseline
- Outbound-call agent improved average score from 57 to 80 (+23 points)
- Proactive collaboration reduced USM 1 from 18.2 to 12.5 and increased USM 2 from 48.0 to 58.8

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A data-knowledge dual-driven architecture enables rapid business rule adaptation while maintaining response quality.
- Mechanism: The system fuses human-human conversational data with knowledge-operational data through prompt alignment. At inference, a retrieval module fetches relevant knowledge; when available, the model applies it via CoT reasoning. When no knowledge is retrieved, the model falls back to internalized capabilities, reducing retraining frequency.
- Core assumption: Business rules change more frequently than underlying conversational patterns.
- Evidence anchors:
  - [section 2.2.2]: "This design reduces the operational overhead of purely data-driven systems while enabling minute-level adaptation via RAG-based knowledge updates."
  - [section 2.4.1]: Table 7 shows RL-CoT achieving 71.88% accuracy, +1.05pp over baseline.
  - [corpus]: Weak match—corpus lacks direct evidence on hybrid knowledge-data approaches.
- Break condition: If knowledge retrieval latency exceeds dialogue response budgets, or if knowledge bases become so fragmented that retrieval precision degrades below usable thresholds.

### Mechanism 2
- Claim: Self-Refinement Training (SRT) enables continuous model improvement from production data without heavy manual annotation.
- Mechanism: The system classifies production samples into "Good Cases" (correct solution + high satisfaction + quality conversation) and "Bad Cases" (correct solution but low satisfaction). Good cases are added to SFT data; bad cases are rewritten and used to construct preference pairs for DPO/RL training.
- Core assumption: User satisfaction signals can be reliably extracted from production interactions.
- Evidence anchors:
  - [section 2.2.4, Table 3]: SRT with Good Cases improves USM 1 from 52.91 to 28.18 and USM 2 from 27.21 to 48.15.
  - [section 2.2.4, Table 4]: Adding Bad Cases further improves USM 1 to 25.38 and USM 2 to 52.72, while reducing repetition rate from 57.43% to 20.27%.
  - [corpus]: No direct corpus support for self-refinement training loops.
- Break condition: If production data distribution shifts faster than the refinement cycle, or if bad case classification accuracy drops below the threshold needed for stable DPO learning.

### Mechanism 3
- Claim: Multi-agent specialization with dynamic invocation improves complex task handling over single-model approaches.
- Mechanism: A master dialogue agent maintains conversational control while dynamically invoking specialized sub-agents (outbound-call, proactive collaboration, multi-modal understanding) as executable tools. Sub-agents return results to the master agent, which integrates them into its response. This follows an Agents-as-Tools pattern with Handoff elements for reliability.
- Core assumption: Complex scenarios can be decomposed into subtasks with well-defined interfaces.
- Evidence anchors:
  - [section 3, Table 10]: Outbound-call agent improves average score from 57 to 80 (+23 points).
  - [section 3.2, Table 11]: Proactive collaboration reduces USM 1 from 18.2 to 12.5 and increases USM 2 from 48.0 to 58.8.
  - [corpus]: Weak match—corpus mentions multi-agent collaboration broadly but without empirical validation.
- Break condition: If sub-agent invocation overhead creates unacceptable latency, or if master agent fails to correctly determine when sub-agent results should be incorporated versus rejected.

## Foundational Learning

- Concept: **Continual Pre-Training (CPT) with Catastrophic Forgetting Mitigation**
  - Why needed here: Domain-specific training often degrades general capabilities; the paper reports a 2.93% decline on Yi-34B when naively following official data proportions.
  - Quick check question: Can you explain why mixing general and domain data at the wrong ratio would hurt instruction-following ability?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The paper uses DPO extensively to align model outputs with human preferences across 8 domain-specific issue types (unclarified demands, hallucinations, script repetition, etc.).
  - Quick check question: How does DPO differ from traditional RLHF in terms of reward model requirements?

- Concept: **Chain-of-Thought (CoT) with Knowledge Grounding**
  - Why needed here: The hybrid approach requires models to reason over retrieved knowledge before responding; CoT data is generated and validated through forward/reverse checks.
  - Quick check question: Why must CoT content be verifiable in input data rather than model-generated?

## Architecture Onboarding

- Component map: CPT → SFT → DPO → RL (four sequential stages) → Core Model (LongCat with unified dialogue-action JSON output) → Knowledge Layer (atomized knowledge units + batch retrieval + dialogue-based internalization) → Multi-Agent Layer (master agent + outbound-call agent + proactive collaboration agent + multi-modal agent) → Evaluation Layer (model-based automated evaluation + end-to-end agent evaluation) → Data Loopback (policy recognition → inspection execution → online rule-based + offline meticulous collection)

- Critical path: Start with SFT data construction (human-human + knowledge-operational fusion) → train unified dialogue-action model → deploy with knowledge retrieval → collect self-sampled data → filter good/bad cases → iterate via SRT and DPO. For complex scenarios, identify sub-agent invocation points.

- Design tradeoffs:
  - **General vs. domain capability**: ~80% general data ratio balances domain gains with general skill preservation.
  - **Data-driven vs. knowledge-driven**: Hybrid approach trades some response fluency for minute-level rule adaptability.
  - **Online vs. offline inspection**: Online rules guarantee low latency but have lower precision; offline multi-agent inspection is thorough but slower.

- Failure signatures:
  - High repetition rate (RR) indicates insufficient bad case processing in SRT.
  - Low knowledge reference correctness (R_kn) suggests retrieval-internalization imbalance.
  - USM 1 increases or USM 2 decreases after DPO iteration signals preference data quality issues.

- First 3 experiments:
  1. **Data mixture validation**: Train small proxy models on 3-5 random mixture ratios; fit regression model to predict optimal ratio; verify the predicted ratio improves validation loss by >5% over manual tuning on a held-out domain test set.
  2. **Knowledge retrieval ablation**: Deploy model with knowledge retrieval disabled for 1% of traffic; compare USM 1/USM 2 against baseline to quantify knowledge contribution. Expect degradation in rule-heavy scenarios.
  3. **Single bad case category DPO**: Isolate one issue type (e.g., "Hallucinations"); construct preference pairs from its bad cases; run DPO and measure repair rate on held-out test set. Target: >10pp improvement per Table 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agentic reinforcement learning be designed to enable LLMs to autonomously discover, construct, and invoke tools for complex, unscripted life service scenarios?
- Basis in paper: [explicit] The Conclusion states a future direction is "Empowering Tool Use through Agentic Reinforcement Learning" to allow the model to address a wider spectrum of tasks not currently defined by static business rules.
- Why unresolved: Current tool invocation relies on pre-defined APIs and workflows (Section 2.2.2), limiting the system to known scenarios rather than dynamic, autonomous tool creation.
- What evidence would resolve it: Benchmarks showing success rates in novel environments where agents must generate new code or API calls to complete tasks outside their initial configuration.

### Open Question 2
- Question: How can multi-agent architectures effectively integrate video understanding and speech modalities into the collaborative reasoning process without degrading real-time responsiveness?
- Basis in paper: [explicit] The Conclusion identifies "Advancing Multi-Agent Collaboration and Multi-Modal Integration" as a key goal, specifically mentioning the need to incorporate text, speech, image, and video.
- Why unresolved: While the paper details text-based multi-agent systems (Section 3.2) and basic multi-modal understanding (Section 3.3), it does not demonstrate a unified framework where video input directly influences the high-level planning of the master agent.
- What evidence would resolve it: End-to-end performance metrics (e.g., USM 1/2) for scenarios requiring video input (e.g., verifying physical damage) where the multi-agent system maintains low latency.

### Open Question 3
- Question: How can intelligent interaction systems transition from reactive intent fulfillment to proactive, user-centric reasoning that anticipates needs before they are explicitly stated?
- Basis in paper: [explicit] The Conclusion lists "Building Truly Personalized, User-Centric Assistants" as the "ultimate vision," specifically the ability to "proactively think ahead" rather than just extending platform services.
- Why unresolved: The current system improves reactive metrics (solving stated problems) via SRT and DPO (Section 2.2.4), but lacks a defined mechanism for predicting latent user needs based on long-term context.
- What evidence would resolve it: A new evaluation metric quantifying "proactive resolution rates," measuring instances where the system successfully addresses a user's unstated subsequent requirement.

## Limitations

- Proprietary LongCat model architecture remains opaque, limiting direct comparison with open alternatives
- Exact calculation formulas for USM1 and USM2 metrics are not disclosed, making precise benchmarking challenging
- Knowledge base schema and specific business rules for the Meituan domain are not detailed, creating barriers to faithful reproduction
- Report lacks sufficient ablation studies to isolate individual mechanism contributions

## Confidence

**High Confidence**: The four-stage training pipeline and its documented improvements in user satisfaction metrics are well-supported by empirical data. The Self-Refinement Training mechanism shows strong evidence with clear before/after comparisons. The multi-agent architecture's effectiveness is substantiated by specific score improvements.

**Medium Confidence**: The data-knowledge dual-driven architecture's minute-level adaptation capability is supported by mechanism descriptions and Table 7, but lacks comprehensive latency and precision degradation studies under high-load conditions. The unified dialogue-action model's JSON output format is specified, but real-world parsing reliability remains partially unverified.

**Low Confidence**: The catastrophic forgetting mitigation through adaptive data mixture optimization lacks detailed hyperparameter guidance and validation protocols. The knowledge retrieval-internalization balance is stated but not rigorously justified through extensive ablation studies.

## Next Checks

1. **Latency-Precision Tradeoff Validation**: Deploy the knowledge retrieval system under simulated high-load conditions (10K+ concurrent sessions) and measure knowledge reference correctness (R_kn) degradation over 60-minute intervals. Target: maintain R_kn >85% while keeping average retrieval latency <200ms.

2. **Cross-Domain General Capability Preservation**: Train a proxy model on the optimal data mixture ratio (predicted via the regression model) and evaluate on both domain-specific and general benchmarks (e.g., MMLU, HumanEval). Target: domain improvement >10% while general capability degradation <3%.

3. **Sub-Agent Invocation Overhead Analysis**: Instrument the multi-agent system to measure master agent decision latency and total response time when invoking specialized sub-agents. Target: sub-agent invocation overhead <150ms and total response time <2 seconds for 95% of interactions.