---
ver: rpa2
title: Science Consultant Agent
arxiv_id: '2512.16171'
source_url: https://arxiv.org/abs/2512.16171
tags:
- data
- agent
- science
- questionnaire
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Science Consultant Agent is a web-based AI tool designed to
  help practitioners select and implement effective modeling strategies for AI-based
  solutions. It addresses the challenge of navigating complex modeling decisions in
  a rapidly evolving AI landscape by combining structured questionnaires, literature-backed
  recommendations, and prototype generation.
---

# Science Consultant Agent

## Quick Facts
- arXiv ID: 2512.16171
- Source URL: https://arxiv.org/abs/2512.16171
- Reference count: 10
- Primary result: Web-based AI tool combining structured questionnaires, literature-backed recommendations, and prototype generation to guide practitioners in selecting effective modeling strategies

## Executive Summary
The Science Consultant Agent is a web-based AI tool designed to help practitioners select and implement effective modeling strategies for AI-based solutions. It addresses the challenge of navigating complex modeling decisions in a rapidly evolving AI landscape by combining structured questionnaires, literature-backed recommendations, and prototype generation. The system consists of four components: a Questionnaire that captures task requirements, Smart Fill that auto-completes fields using project descriptions, Research-Guided Recommendation that retrieves and synthesizes relevant arXiv literature, and Prototype Builder that generates baseline implementations. Evaluation through user feedback shows that recommendations often align with user expectations and provide convincing justifications, though some terminology clarity and implementation guidance need improvement.

## Method Summary
The Science Consultant Agent implements a four-component architecture: (1) Questionnaire captures task requirements through a 6-part structured form covering introduction, data understanding, evaluation, task mechanism, constraints, and miscellaneous details; (2) Smart Fill auto-completes questionnaire fields using LLM analysis of project descriptions and metadata; (3) Research-Guided Recommendation retrieves arXiv papers (K=50 queries, N=50 papers filtered), constructs context via Abstract/Full Paper/Summaries strategies, and LLM synthesizes evidence-backed recommendations; (4) Prototype Builder uses tool-based SageMaker functions to generate baseline implementations with standardized evaluation reports. The system emphasizes safety through predefined tools rather than arbitrary code generation, and aims to reduce example-induced bias by forcing structured consideration of modeling trade-offs.

## Key Results
- 82% of users rated their experience as excellent or good in Round 1 evaluation
- 100% of users found recommendation justifications convincing, with 73% rating them moderately to very convincing
- 100% of users observed alignment with their expectations, with 63% reporting moderate to perfect alignment

## Why This Works (Mechanism)

### Mechanism 1
Structured questionnaires reduce example-induced bias and prevent defaulting to familiar but suboptimal solutions. Pre-defined questions systematically capture task requirements, data characteristics, and constraints—forcing users to consider trade-offs (latency, cost, performance) early rather than designing around narrow familiar examples. Core assumption: Users lack awareness of which questions matter for modeling decisions; explicit structure compensates for this gap. Evidence anchors: User feedback confirms questionnaire's educational role, though unclear terminology caused some issues. Break condition: If questionnaire terminology is unclear or questions are left unanswered, the mechanism degrades.

### Mechanism 2
Literature-grounded recommendations improve decision quality over internal LLM knowledge alone. arXiv retrieval generates task-specific queries, filters relevant papers by abstract, and constructs context for the LLM to synthesize evidence-backed recommendations. Core assumption: Recent peer-reviewed research contains superior modeling strategies that practitioners cannot easily discover or recall. Evidence anchors: User testing showed recommendations aligned with expectations and justifications were convincing. Break condition: If arXiv search returns irrelevant papers or token limits restrict context to few papers, recommendation quality may degrade.

### Mechanism 3
Tool-based prototype generation provides safe, reproducible baselines without risks of arbitrary code execution. LLM selects from predefined tools (each implementing a standard modeling strategy), provides parameters, and the tool executes via SageMaker with validation and standardized data templates. Core assumption: A limited set of standard baselines suffices to demonstrate feasibility for most tasks. Evidence anchors: Approach is safe and avoids arbitrary code risks. Break condition: For tasks outside supported modalities or generative text evaluation, tools cannot produce evaluation reports.

## Foundational Learning

- **Concept: Modeling strategy decision space** - Why needed here: Users must understand what options exist (prompting, RAG, fine-tuning, distillation, specialized models) to interpret recommendations meaningfully. Quick check question: Can you name three distinct approaches for building an AI solution and their key trade-offs?

- **Concept: Evaluation metrics and task framing** - Why needed here: The questionnaire asks about metrics, task type (regression/classification/generation), and evaluation constraints; incorrect answers propagate to recommendations. Quick check question: For a binary classification task with imbalanced classes, which metric—accuracy or AUC-ROC—would you prioritize and why?

- **Concept: Data characteristics and availability assessment** - Why needed here: Smart Fill cannot reliably determine data availability; users must assess whether training/validation/test data exists and is sufficient. Quick check question: Do you know what data is required for your task, and can you assess whether existing datasets are relevant?

## Architecture Onboarding

- **Component map:** User Input → Questionnaire (6 parts) → Smart Fill (auto-completes from project description + metadata) → Evidence Retrieval (arXiv MCP: query generation → search → deduplication → filtering) → Context Construction (abstract/full paper/summaries) → Recommendation Synthesis (LLM generates Best Solution + Strong Baseline) → Prototype Builder (tool selection → SageMaker execution → evaluation report)

- **Critical path:** Questionnaire completeness → arXiv query quality → paper relevance filtering → context construction choice (abstract vs. summary vs. full paper) → recommendation accuracy. If questionnaire responses are incomplete or incorrect, all downstream components degrade.

- **Design tradeoffs:** Abstract-only context: fast, many papers, shallow understanding; Full paper context: deep understanding, 1-2 papers only, biased toward those papers; Summaries: balanced depth and breadth, but slow (requires per-paper summarization); Tool-based prototyping: safe and reproducible, but limited to predefined baselines; code-generation would be flexible but risky.

- **Failure signatures:** Users leave questionnaire fields blank or answer incorrectly → Smart Fill propagates errors; arXiv search returns irrelevant papers → LLM generates generic or mismatched recommendations; LaTeX concatenation loses document order → context may be incoherent; Text generation evaluation fails due to surface-form mismatches → no evaluation report generated; Non-scientist users unsure how to translate recommendations into implementation.

- **First 3 experiments:** 1) Test Smart Fill accuracy by comparing auto-completed fields against ground truth for 10 historical projects; measure precision/recall per field type. 2) Compare recommendation quality across context strategies (abstract vs. summary vs. full paper) using expert ranking on 5 diverse tasks. 3) Run Prototype Builder on 3 tabular classification tasks; verify that evaluation reports match manually executed baselines within acceptable variance.

## Open Questions the Paper Calls Out

### Open Question 1
How can the effectiveness of the Science Consultant Agent be evaluated objectively without implementing every potential modeling strategy? Basis in paper: Section 3 states that "evaluating recommendation quality is challenging" because determining the true best strategy would require implementing and tuning many alternatives, which is not feasible. Why unresolved: The paper relies on user feedback surveys rather than performance benchmarks, as stable benchmarks for "best modeling strategy" do not exist. What evidence would resolve it: Development of a benchmark dataset where the "optimal" strategy is pre-determined, allowing for quantitative comparison of the agent's recommendation against the verified optimum.

### Open Question 2
Can an autonomous agent reliably replicate research implementations from papers and GitHub repositories without introducing security risks or data corruption? Basis in paper: Section 2.4.2 proposes a "more ambitious improvement" where an agent reads papers and GitHub repos to replicate code, but notes this is currently too "risky" and "dangerous." Why unresolved: LLM-generated code carries a non-negligible risk of producing misleading results or corrupting data, preventing the adoption of fully autonomous coding agents. What evidence would resolve it: A safety framework or verification mechanism that allows an agent to generate and execute code from research papers with guarantees against data corruption.

### Open Question 3
How can the Prototype Builder accurately evaluate generative text tasks where outputs differ semantically but not syntactically from ground truth? Basis in paper: Section 2.4.1 highlights that text evaluation is difficult because "model outputs and ground truth answers often differ in surface form," causing string matching to fail. Why unresolved: While LLM-as-judge is suggested as an alternative in Section 2.4.2, it is noted to be computationally costly and prone to error. What evidence would resolve it: An evaluation framework that integrates semantic similarity or LLM-based judging into the prototype loop with verified accuracy and acceptable latency.

## Limitations
- Smart Fill relies on proprietary Amazon internal tables unavailable externally, restricting practical deployment
- Tool-based prototyping supports only predefined baselines, limiting applicability to novel or complex modeling tasks
- Small user study sample size (11 participants Round 1, 3 participants Round 2) may not represent broader practitioner populations

## Confidence

- **High confidence:** The four-component architecture design and its educational value through structured questionnaires
- **Medium confidence:** Literature-grounded recommendations improve over internal LLM knowledge alone (based on limited user feedback)
- **Low confidence:** Prototype Builder's practical utility across diverse tasks (only anecdotal user interest reported)

## Next Checks

1. Conduct a larger-scale user study (n≥50) across different practitioner roles to validate questionnaire effectiveness and recommendation quality
2. Compare arXiv-grounded recommendations against LLM-only recommendations on standardized tasks using expert evaluation
3. Test Smart Fill accuracy on diverse project descriptions using publicly available project datasets to assess auto-completion reliability