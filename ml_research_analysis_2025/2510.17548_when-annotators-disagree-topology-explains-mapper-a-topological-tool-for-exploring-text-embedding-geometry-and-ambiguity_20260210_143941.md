---
ver: rpa2
title: 'When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for
  Exploring Text Embedding Geometry and Ambiguity'
arxiv_id: '2510.17548'
source_url: https://arxiv.org/abs/2510.17548
tags:
- mapper
- topological
- lens
- across
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mapper, a topological data analysis tool, was applied to RoBERTa-Large
  embeddings on the MD-Offense dataset to study how fine-tuning reshapes internal
  representation geometry for ambiguous text. Unlike PCA or UMAP, Mapper captures
  non-convex, modular regions in embedding space aligned with model predictions.
---

# When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity

## Quick Facts
- **arXiv ID**: 2510.17548
- **Source URL**: https://arxiv.org/abs/2510.17548
- **Reference count**: 20
- **Primary result**: Mapper reveals that fine-tuned models impose confident, region-level decisions on ambiguous text, even when ground-truth labels disagree

## Executive Summary
This paper introduces a topological approach to understanding how language models handle ambiguous text. Using Mapper, a topological data analysis tool, the authors analyze RoBERTa-Large embeddings on the MD-Offense dataset, revealing that fine-tuning restructures embedding geometry to create modular, non-convex regions aligned with model predictions. Unlike traditional dimensionality reduction techniques (PCA, UMAP), Mapper captures the true geometric complexity of embedding space, showing that models impose confident predictions even on highly ambiguous inputs where annotators disagree. The analysis exposes a tension between structural confidence and label uncertainty, demonstrating that models often "resolve" ambiguity by forcing decisions rather than reflecting true uncertainty.

## Method Summary
The study analyzes MD-Offense, a binary offensive language detection dataset with 10,753 tweets labeled by 5 annotators each. The authors fine-tune RoBERTa-Large for 3 epochs with batch size 8 and learning rate 2×10⁻⁵, then extract [CLS] embeddings from the final layer. Using KeplerMapper, they construct topological graphs with r=40 intervals and 30% overlap, clustering with HDBSCAN in the original 1024-dimensional space. They compute six 1D lens functions (centroid, PCA-1, eccentricity, L2-norm, and two random projections) and evaluate Component Purity (CP), Edge Agreement (EA), and Majority Match (MM) metrics across ambiguity levels (A++, A+, A0). The analysis compares base vs. fine-tuned models to understand how training reshapes embedding geometry for ambiguous instances.

## Key Results
- Fine-tuning increases component purity (>98% in prediction labels) and edge agreement, creating coherent topological structures even for highly ambiguous inputs
- Despite strong prediction purity, alignment with ground-truth labels drops in ambiguous cases (A0), indicating models impose confident decisions rather than reflecting true uncertainty
- Mapper captures non-convex, modular regions in embedding space aligned with model predictions, which traditional tools like PCA and UMAP flatten or distort
- The topological behavior is consistent across multiple models and lens types, revealing systematic model overconfidence on subjective NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapper preserves non-convex, modular structures that dimensionality reduction techniques flatten
- **Mechanism:** Mapper projects data via a lens into overlapping intervals but clusters points within the original high-dimensional space, maintaining local connectivity without forcing global Euclidean assumptions
- **Core assumption:** High-dimensional text embeddings exhibit non-linear, potentially hyperbolic structures
- **Evidence anchors:** Abstract states Mapper "captures this geometry directly uncovering decision regions" unlike PCA/UMAP; Section 1 notes linguistic embedding space exhibits natural hyperbolic geometry
- **Break condition:** Poor lens function choice or low resolution makes topological summary arbitrary or oversimplified

### Mechanism 2
- **Claim:** Fine-tuning imposes topological regularity by reorganizing ambiguous instances into coherent, class-aligned regions
- **Mechanism:** Optimization encourages mapping inputs to smooth regions in embedding space to minimize loss, forcing even high-disagreement instances into prediction-pure connected components
- **Core assumption:** Cross-entropy loss encourages region-level separation manifesting as increased component purity
- **Evidence anchors:** Abstract shows fine-tuning increased component purity (>98%); Section 5 states fine-tuning "restructures the embedding space... by organizing them into modular, non-convex regions"
- **Break condition:** Under-trained models or purely random noise data would show low prediction purity and high fragmentation

### Mechanism 3
- **Claim:** Topological metrics expose tension between structural confidence and label uncertainty
- **Mechanism:** Mapper graphs where nodes are clusters and edges represent shared data; measuring homogeneity within components (Purity) and across edges (Agreement) detects smooth decision boundaries even where ground truth is mixed
- **Core assumption:** Edges in Mapper graph correspond to meaningful semantic transitions or decision boundaries
- **Evidence anchors:** Section 4.4 defines Edge Agreement as proportion of edges whose endpoints share same majority class label; Section 5 notes alignment with ground-truth labels drops in ambiguous data
- **Break condition:** Too low overlap artificially removes edges destroying agreement signals; too high creates spurious edges inflating connectivity

## Foundational Learning

- **Concept:** Mapper Algorithm (TDA)
  - **Why needed here:** Core analytical tool to bypass PCA/UMAP limitations; understand "lenses," "coverings," and "nerve graphs" to interpret results
  - **Quick check question:** If you change the lens function from PCA to Eccentricity, are you changing the data or the perspective through which you view the data's structure?

- **Concept:** Representation Geometry (Convexity vs. Modularity)
  - **Why needed here:** Fine-tuning creates "modular, non-convex" regions; understanding that decision boundaries aren't simple hyperplanes is key to grasping why linear probes fail
  - **Quick check question:** Why would a linear classifier fail to capture a "modular" decision region where two disjoint clusters belong to the same class?

- **Concept:** Ambiguity & Disagreement (A0/A++)
  - **Why needed here:** Central thesis that models behave differently on ambiguous data (A0) vs. consensus data (A++); "A0" represents semantic uncertainty, not just "hard" data
  - **Quick check question:** Does the paper claim models fail to predict A0 data, or that they predict it with *unjustified confidence*?

## Architecture Onboarding

- **Component map:** MD-Offense Dataset (Text + Annotator Agreement) -> RoBERTa-Large Encoder ([CLS] embeddings) -> Mapper Pipeline (Lens -> Cover -> Cluster) -> NetworkX Graph Analysis (Purity, Edge Agreement)

- **Critical path:** Selection of Lens Function and HDBSCAN parameters; class-aligned lenses (Centroid, PCA) yield different noise rates than geometric ones (Eccentricity); properly tuning cover resolution (r) is critical to avoid over-fragmentation

- **Design tradeoffs:**
  - Resolution (r): High r (>50) leads to fragmentation/singletons; Low r (<30) oversimplifies (Paper uses 40)
  - Overlap (ε): Low overlap disconnects regions; High overlap creates "fan-like" artifacts (Paper uses 0.3)
  - Lens Dimension: 1D lenses simplify comparison; 2D lenses offer richer structure but higher complexity

- **Failure signatures:**
  - Over-fragmentation: Graph looks like "cloud" of disconnected dots; Fix: Decrease resolution r or increase overlap ε
  - Fan Artifacts: Graph has dense, spiky connections; Fix: Decrease overlap ε
  - Noise Exclusion: HDBSCAN labels >20% of data as noise; Fix: Adjust HDBSCAN min_cluster_size or switch lens

- **First 3 experiments:**
  1. Replicate Topological Shift: Train classifier on MD-Offense; generate Mapper graphs for Base vs. Fine-tuned embeddings using PCA-1D lens; verify "mixed" nodes decrease and component purity increases
  2. Ambiguity Stress Test: Isolate A0 (high disagreement) test subset; run Mapper and calculate Component Purity vs. Majority Match; confirm prediction purity (>90%) coexists with lower ground-truth alignment
  3. Lens Sensitivity Check: Apply "Random" lens vs. "Centroid" lens on fine-tuned embeddings; check if structural confidence persists regardless of lens

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do modular, non-convex prediction regions emerge in tasks beyond offensive language detection, such as natural language inference or sentiment analysis?
- **Basis in paper:** [explicit] Limitations section states it "remains an open question whether similar topological phenomena... would emerge in other tasks"
- **Why unresolved:** Study focuses exclusively on MD-Offense dataset (binary classification) to ensure controlled analysis, leaving cross-task validity unverified
- **What evidence would resolve it:** Replicating Mapper analysis on diverse NLP tasks with high annotator disagreement to observe if high component purity and region-level overconfidence persist

### Open Question 2
- **Question:** Does the model's confident resolution of ambiguous inputs reflect true generalization or overfitting to misleading surface patterns?
- **Basis in paper:** [explicit] Conclusion explicitly asks: "Whether this resolution reflects true generalization or an overfit to misleading surface patterns remains an open question"
- **Why unresolved:** Mapper reveals high structural confidence (prediction purity) even when ground-truth alignment is low, but paper doesn't isolate specific linguistic features driving this divergence
- **What evidence would resolve it:** Probing specific topological clusters for spurious correlations or artifacts (e.g., specific token usage) to determine if model's structural certainty is semantically justified

### Open Question 3
- **Question:** How can topological metrics be operationalized to guide proactive modeling strategies, such as data subset selection or robustness evaluations?
- **Basis in paper:** [inferred] Abstract and Conclusion position Mapper as tool that "may inform proactive modeling strategies," but paper only demonstrates utility as post-hoc diagnostic
- **Why unresolved:** Work establishes link between topology and ambiguity but doesn't implement feedback loop where metrics actively influence training
- **What evidence would resolve it:** Experiment where training data is sampled or loss functions weighted based on topological metrics (e.g., prioritizing low-purity regions), resulting in improved calibration or accuracy

## Limitations
- Fine-tuning procedure uses averaged results over 3 runs without specifying random seeds, creating potential reproducibility variance
- Topological analysis relies heavily on 1D lens functions which may oversimplify embedding geometry - higher-dimensional lenses could reveal additional structure
- Claims about "ambiguity resolution" are limited to MD-Offense dataset and binary classification, leaving open questions about multi-class or non-text domains

## Confidence

**High Confidence**: Topological methodology (Mapper + HDBSCAN) is sound and structural findings about increased purity and edge agreement post-fine-tuning are reproducible; core claim that Mapper captures non-convex structures missed by PCA/UMAP is well-supported

**Medium Confidence**: Interpretation that fine-tuning "resolves" ambiguity by imposing confident predictions on A0 instances is mechanistically plausible but relies on correlation rather than direct causal evidence; paper doesn't test whether removing these instances affects downstream performance

**Low Confidence**: Generalizability claims about topological analysis exposing systematic model overconfidence across "subjective NLP tasks" lack empirical support beyond single MD-Offense dataset; specific numerical thresholds (98% purity) may be sensitive to hyperparameter choices

## Next Checks

1. **Seed Sensitivity Analysis**: Re-run fine-tuning procedure with documented random seeds (seed=42, 123, 999) to establish variance bounds on topological metrics and confirm robustness of >98% purity findings

2. **Lens Dimensionality Expansion**: Replicate analysis using 2D lens functions (e.g., PCA-2D, UMAP-2D) to test whether modular structure and ambiguity resolution patterns persist or reveal additional geometric features missed by 1D projections

3. **Cross-Dataset Generalization**: Apply same topological pipeline to different subjective NLP dataset (e.g., emotion classification with crowd-sourced labels) to test whether pattern of high prediction purity with low ground-truth alignment on ambiguous instances generalizes beyond offensive language detection