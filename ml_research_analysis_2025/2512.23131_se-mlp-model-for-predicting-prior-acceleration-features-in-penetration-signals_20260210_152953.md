---
ver: rpa2
title: SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals
arxiv_id: '2512.23131'
source_url: https://arxiv.org/abs/2512.23131
tags:
- feature
- acceleration
- prediction
- penetration
- se-mlp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a lightweight SE-MLP model to rapidly predict
  penetration acceleration prior features, addressing the challenge of long simulation
  cycles and high computational costs in traditional methods. By integrating a squeeze-and-excitation
  channel attention mechanism with residual connections into a multi-layer perceptron,
  the model achieves efficient nonlinear mapping between physical parameters and acceleration
  features.
---

# SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals

## Quick Facts
- **arXiv ID**: 2512.23131
- **Source URL**: https://arxiv.org/abs/2512.23131
- **Reference count**: 27
- **Primary result**: Lightweight SE-MLP model predicts penetration acceleration prior features with R² 0.9383 (peak) and 0.9862 (pulse width), outperforming Transformer, XGBoost, Random Forest, and SVR.

## Executive Summary
This paper proposes a lightweight SE-MLP model to rapidly predict penetration acceleration prior features, addressing the challenge of long simulation cycles and high computational costs in traditional methods. By integrating a squeeze-and-excitation channel attention mechanism with residual connections into a multi-layer perceptron, the model achieves efficient nonlinear mapping between physical parameters and acceleration features. Comparative experiments with Transformer, XGBoost, Random Forest, and SVR demonstrate that SE-MLP achieves superior prediction accuracy (average R² of 0.9383 for acceleration peak, 0.9862 for pulse width) and stability. Ablation studies confirm that both SE attention and residual structures contribute significantly to performance gains. Validation using numerical simulations and range recovery tests shows prediction errors within 15% for acceleration peaks and less than 4% for pulse widths, confirming the model's feasibility and engineering applicability for penetration fuze control.

## Method Summary
The SE-MLP model uses a 3-layer MLP with GELU activation, batch normalization, and dropout, enhanced by squeeze-and-excitation (SE) channel attention after each block and a final residual connection. Input features (mass, velocity, material, layers) are normalized to [0,1], while acceleration peak is log-transformed and pulse width is max-normalized. The model is trained using 4-fold cross-validation with AdamW optimizer, weighted MSE loss (0.7 for peak, 0.3 for width), and ReduceLROnPlateau scheduler. The approach achieves efficient nonlinear mapping between physical parameters and acceleration features while maintaining lightweight architecture suitable for embedded deployment.

## Key Results
- SE-MLP achieves average R² of 0.9383 for acceleration peak prediction and 0.9862 for pulse width prediction
- Outperforms Transformer, XGBoost, Random Forest, and SVR in both accuracy and stability
- Ablation studies show SE module reduces peak RMSE by 7.8% and residual connections reduce it by 12.9%
- Validation errors remain within 15% for acceleration peaks and less than 4% for pulse widths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Squeeze-and-Excitation (SE) module improves prediction accuracy by adaptively reweighting input feature channels based on their global importance to the output.
- **Mechanism:** Global average pooling compresses spatial information into a channel descriptor vector; two fully-connected layers with GELU and Sigmoid activations learn inter-channel dependencies, producing scale factors that amplify informative channels (e.g., penetration velocity, target material) and suppress redundant ones. The reweighted features are then passed to subsequent layers.
- **Core assumption:** Input physical parameters have unequal predictive importance for acceleration features, and this importance can be learned from data rather than manually specified.
- **Evidence anchors:** [abstract] "integrates a channel attention mechanism with residual connections to enable rapid prediction"; [section 2.2] "The SE module mainly includes two phases: Squeeze (compression) and Excitation (excitation)"; [section 4.4.2] "After introducing the SE module... peak prediction RMSE decreases to 3835.33 (reduced by about 7.8% compared with basic MLP)"; [corpus] No directly comparable SE-MLP architectures found; neighbor papers use MLP in different domains (T-MLP for signal representation, MLP for recognition tasks) but do not validate SE attention specifically.
- **Break condition:** If input features are already orthogonal or equally important, SE reweighting provides negligible benefit and adds computational overhead.

### Mechanism 2
- **Claim:** Residual connections stabilize training and improve generalization by enabling direct gradient flow across layers.
- **Mechanism:** A shortcut path adds the input `x` to the transformed output `F(x)`, allowing gradients to bypass intermediate layers during backpropagation. This mitigates gradient vanishing in deeper networks and preserves low-level physical features for final prediction.
- **Core assumption:** The optimal mapping from physical parameters to acceleration features can be expressed as a perturbation (residual) of the identity function, making it easier to learn than the full mapping.
- **Evidence anchors:** [abstract] "residual structures contribute significantly to performance gains" (confirmed by ablation); [section 2.1] "The residual structure can be expressed as: H(x) = F(x) + x"; [section 4.4.2] "After further adding the residual connection structure... RMSE further decreases to 3339.22 (reduced by 12.9% compared with MLP+SE)"; [corpus] ResNet-style skip connections are standard (He et al. 2016 cited); corpus papers do not contradict this mechanism but lack penetration-specific validation.
- **Break condition:** If network depth is shallow (1-2 layers), residual connections provide minimal benefit and may introduce noise.

### Mechanism 3
- **Claim:** Logarithmic normalization of acceleration peak targets prevents gradient instability caused by cross-scale output magnitudes.
- **Mechanism:** Acceleration peaks span ~30,000–70,000 g across conditions, while pulse widths span only 0.7–0.9 ms. Logarithmic compression (Eq. 7) reduces peak dynamic range; max normalization (Eq. 8) standardizes pulse width. Weighted MSE (0.7 peak, 0.3 width) prioritizes the physically critical feature.
- **Core assumption:** The model can learn the inverse transformation (denormalization) during inference without accumulating systematic bias.
- **Evidence anchors:** [section 4.1.2] "the acceleration peak has a large span under different working conditions... which easily leads to gradient explosion"; [section 4.3] "loss weights of 0.7 and 0.3 are assigned to the acceleration peak and pulse width respectively"; [section 5, Table 6] Validation shows peak errors within 15% and pulse width errors <4%, suggesting normalization did not introduce systematic bias; [corpus] Normalization sensitivity is acknowledged in [6] (De et al. 2023, cited); no corpus papers contradict the log-transform approach for high-dynamic-range targets.
- **Break condition:** If prediction requires absolute scale precision (e.g., sub-1% peak accuracy), log compression may introduce reconstruction error at extreme values.

## Foundational Learning

- **Concept: Multi-Layer Perceptron (MLP) with non-linear activation**
  - **Why needed here:** The SE-MLP backbone is a 3-layer MLP; understanding forward propagation (`h = f(Wx + b)`) and how GELU activation enables non-linear mapping from physical parameters to acceleration features is essential.
  - **Quick check question:** Given input `[60 kg, 1200 m/s, C60, 8 layers]`, can you trace the dimensions through three hidden layers (assuming hidden size 128) to output `[peak, width]` for each of 8 layers?

- **Concept: Channel Attention (Squeeze-and-Excitation)**
  - **Why needed here:** The SE block is inserted after each MLP layer; you must understand how global pooling + FC layers produce channel-wise scale factors that modulate feature importance.
  - **Quick check question:** For a feature tensor of shape `[batch, 128]`, what are the shapes of the squeeze output, excitation hidden layer (with reduction ratio r=16), and final scale vector?

- **Concept: K-Fold Cross-Validation with Strict Separation**
  - **Why needed here:** The paper uses 4-fold CV (75%/25% split) to report metrics; understanding why this matters for generalization claims is critical for interpreting results.
  - **Quick check question:** If the same penetration condition appears in both training and validation folds, what type of data leakage occurs, and how would it inflate reported R²?

## Architecture Onboarding

- **Component map:** Input (4D: mass, velocity, material, layers) → Linear → BatchNorm → GELU → Dropout → SE Block → Linear → BatchNorm → GELU → Dropout → SE Block → Linear → BatchNorm → GELU → Dropout → Residual Add (input features) → Linear → Output (2×num_layers: peak + width per layer)

- **Critical path:** The SE block's scale multiplication (`X̃_c = s_c · X_c`) directly modulates feature contributions. If this fails (e.g., all weights collapse to near-zero), the model degenerates to a standard MLP with no attention benefit.

- **Design tradeoffs:**
  - **Lightweight vs. expressiveness:** 3 layers keep parameter count low for embedded deployment, but may limit capacity for highly heterogeneous target materials. The paper does not test beyond C40–C80 concrete.
  - **Weighted loss (0.7/0.3):** Prioritizes peak accuracy over width accuracy; appropriate for layer-counting applications but may need retuning for other control strategies.
  - **Log-normalization:** Stabilizes training but adds denormalization step during inference—verify no precision loss in deployment.

- **Failure signatures:**
  1. **R² for peak drops below 0.85:** Likely cause is insufficient training diversity across velocity/material combinations; check for distribution shift between train and test conditions.
  2. **Large fold-to-fold variance in MAPE:** Indicates overfitting to specific conditions; reduce model capacity or increase dropout.
  3. **Systematic under/over-prediction at high velocities (>1500 m/s):** Log-normalization may not fully compress extreme values; consider alternative scaling or separate models per velocity regime.

- **First 3 experiments:**
  1. **Reproduce 4-fold CV baseline:** Use the provided hyperparameters (batch=32, lr=1e-3, AdamW, 200 epochs, WMSE with 0.7/0.3 weights). Verify average R² ~0.938 (peak) and ~0.986 (width) match Table 3.
  2. **Ablate SE and residual independently:** Train three variants (MLP-only, MLP+SE, SE-MLP) and confirm RMSE reductions follow Table 5 (4161→3835→3339 for peak).
  3. **Out-of-distribution test:** Hold out one warhead class (e.g., 600 kg) entirely from training; evaluate generalization error to assess whether the model learns physics or memorizes condition clusters.

## Open Questions the Paper Calls Out

- **Question:** Does incorporating multidimensional parameters such as attitude angle, length-to-diameter ratio, and frequency domain response improve the model's prediction accuracy for oblique or complex penetrations?
- **Basis:** [explicit] The conclusion states that future research will focus on "expanding the physical feature space by incorporating multidimensional parameters—such as attitude angle, length-to-diameter ratio, and frequency domain response."
- **Why unresolved:** The current model inputs are limited to mass, velocity, layer count, and material type, restricting applicability to specific impact scenarios.
- **Evidence:** Improved R² and reduced MAPE when testing the updated model on datasets containing oblique impact angles and varying warhead geometries.

## Limitations
- Dataset confidentiality prevents independent verification of feature distributions and model generalization across conditions.
- Hidden layer widths and SE reduction ratio are unspecified, forcing assumption-based reproduction.
- Log-normalization may introduce reconstruction error at extreme acceleration peaks, though validation shows <15% peak error.

## Confidence
- **High**: MLP with residual connections stabilizes training (supported by ablation and ResNet literature).
- **High**: Weighted MSE (0.7 peak, 0.3 width) prioritizes critical feature without bias (validated by <4% width error).
- **Medium**: SE channel attention improves accuracy (7.8% peak RMSE reduction in ablation, but no direct SE-MLP literature).
- **Medium**: Model generalizes to unseen warhead classes (holdout test suggested but not reported).

## Next Checks
1. **Holdout Warhead Class Test**: Exclude one warhead type (e.g., 600 kg) from training; evaluate out-of-distribution error to confirm physics learning vs. memorization.
2. **SE Ablation on Extreme Conditions**: Test SE-MLP on high-velocity (>1500 m/s) cases to check if attention weights collapse or amplify noise.
3. **Dataset Release Request**: Request anonymized version of the 108-condition dataset to verify fold separation and reproduce exact R² scores.