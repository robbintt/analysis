---
ver: rpa2
title: 'Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning'
arxiv_id: '2508.05681'
source_url: https://arxiv.org/abs/2508.05681
tags:
- iter
- acquisition
- epoch
- samples
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first framework for clean-label backdoor
  attacks in active learning by exploiting acquisition functions as an attack surface.
  The proposed ALA method optimizes poisoned samples to exhibit high uncertainty scores,
  increasing their selection probability during active learning iterations.
---

# Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning

## Quick Facts
- arXiv ID: 2508.05681
- Source URL: https://arxiv.org/abs/2508.05681
- Reference count: 40
- Primary result: First framework for clean-label backdoor attacks in active learning achieving 94% attack success rates under 0.5%-1.0% poisoning budgets

## Executive Summary
This work introduces ALA (Active Learning Attack), the first framework for clean-label backdoor attacks in active learning systems. The attack exploits acquisition functions by optimizing poisoned samples to exhibit high uncertainty scores, increasing their selection probability during active learning iterations. The method achieves high attack success rates while maintaining model utility on clean data and remaining undetectable to human annotators. Extensive experiments across multiple datasets and acquisition functions demonstrate the effectiveness of this approach, with the SIG trigger method consistently outperforming the CL trigger.

## Method Summary
ALA is a clean-label backdoor attack framework that targets active learning systems by exploiting acquisition functions. The method optimizes poisoned samples to have high uncertainty scores (using Entropy, Margin, or Least Confidence metrics), making them more likely to be selected during active learning iterations. The attack maintains clean-label appearance, making poisoned samples indistinguishable from legitimate data to human annotators. The optimization process uses gradient-based methods to generate poisoned samples that maximize uncertainty while preserving their original labels, effectively poisoning the training process without detection.

## Key Results
- Achieves attack success rates up to 94% under low poisoning budgets (0.5%-1.0%)
- Maintains model utility on clean data while successfully executing backdoor attacks
- SIG trigger method consistently outperforms CL trigger across all datasets and acquisition functions
- Attack remains undetectable to human annotators while being effective against multiple acquisition functions

## Why This Works (Mechanism)
The attack exploits the fundamental vulnerability in active learning systems where acquisition functions prioritize uncertain samples for labeling. By optimizing poisoned samples to exhibit high uncertainty scores, ALA increases their selection probability during the active learning process. This allows the attack to influence the model's training trajectory without requiring a large number of poisoned samples. The clean-label nature of the attack makes detection difficult both by automated systems and human annotators, as the poisoned samples maintain their original labels and appear legitimate.

## Foundational Learning
- Active Learning: Iterative ML process where model selects most informative samples for annotation; needed to understand the attack surface being exploited
- Acquisition Functions: Metrics (Entropy, Margin, Least Confidence) that score sample informativeness; quick check: verify how each function calculates uncertainty
- Clean-Label Backdoors: Backdoor attacks that don't modify input labels; needed to understand attack stealth properties
- Gradient-Based Optimization: Method for generating poisoned samples; quick check: verify optimization converges and maintains label consistency
- Model Uncertainty: Measure of model confidence in predictions; needed to understand how ALA manipulates acquisition functions

## Architecture Onboarding

Component Map: Data Pool -> Acquisition Function -> Sample Selection -> Model Training -> ALA Optimization

Critical Path: The attack flow follows Data Pool → Acquisition Function evaluation → High-uncertainty sample selection → Model training with poisoned samples → ALA optimization to maintain high uncertainty

Design Tradeoffs: ALA trades off between maintaining clean-label appearance (undetectability) and achieving high uncertainty scores (selection probability). The SIG trigger method provides better balance than CL trigger across all settings.

Failure Signatures: Attack fails when poisoned samples are rejected by acquisition functions, when optimization cannot maintain high uncertainty while preserving clean labels, or when model detects poisoned samples during training.

First Experiments:
1. Test ALA with a single poisoned sample against Entropy acquisition on Fashion-MNIST to verify basic mechanism
2. Compare SIG vs CL trigger effectiveness on CIFAR-10 with 1% poisoning budget
3. Validate human annotator detection rates with 10 poisoned samples from each trigger type

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted to image classification tasks and simple trigger types (SIG and CL methods)
- Effectiveness depends on assumption that acquisition functions prioritize high-uncertainty samples
- Focus on low poisoning budgets (0.5%-1.0%) without analysis of scaling behavior
- Human-in-the-loop annotation assumption may not cover all real-world scenarios

## Confidence

High: Experimental methodology is sound, results are reproducible across multiple datasets and acquisition functions, and the core attack mechanism (exploiting uncertainty scores) is technically valid.

Medium: Claim about human annotator detectability based on visual inspection rather than systematic human studies; relative effectiveness of SIG vs CL triggers may depend on specific dataset characteristics.

Medium: Assertion that ALA is "the first" framework for clean-label backdoor attacks in active learning requires verification against all related literature.

## Next Checks

1. Test ALA against adaptive acquisition functions that explicitly filter out poisoned samples through anomaly detection or outlier rejection mechanisms.

2. Evaluate the attack's effectiveness on larger-scale datasets (ImageNet-level) and with more sophisticated, naturalistic backdoor triggers.

3. Conduct a systematic human study with multiple annotators to validate the claim that poisoned samples remain undetectable to human inspection across different annotator expertise levels.