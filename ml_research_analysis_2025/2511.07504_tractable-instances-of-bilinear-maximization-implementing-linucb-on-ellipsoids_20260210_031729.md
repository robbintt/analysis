---
ver: rpa2
title: 'Tractable Instances of Bilinear Maximization: Implementing LinUCB on Ellipsoids'
arxiv_id: '2511.07504'
source_url: https://arxiv.org/abs/2511.07504
tags:
- newton
- algorithm
- which
- convex
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the computational challenge of solving bilinear\
  \ maximization problems arising in linear bandit algorithms, particularly when the\
  \ action set is an ellipsoid. The authors show that for certain convex sets like\
  \ \u2113p balls with p 2, these problems are NP-hard to approximate, making optimistic\
  \ bandit algorithms impractical in high dimensions."
---

# Tractable Instances of Bilinear Maximization: Implementing LinUCB on Ellipsoids

## Quick Facts
- arXiv ID: 2511.07504
- Source URL: https://arxiv.org/abs/2511.07504
- Authors: Raymond Zhang; Hédi Hadiji; Richard Combes
- Reference count: 40
- Primary result: MaxNorm and Newton algorithms enable efficient implementation of optimistic bandit algorithms on ellipsoids in high dimensions

## Executive Summary
This paper addresses the computational challenge of solving bilinear maximization problems in linear bandit algorithms, particularly when action sets are ellipsoids. The authors demonstrate that for certain convex sets like ℓp balls with p > 2, these problems are NP-hard to approximate, making optimistic bandit algorithms impractical in high dimensions. They provide two efficient algorithms—MaxNorm (non-iterative) and Newton (iterative interior-point)—that solve the ellipsoid case efficiently. Both methods enable practical implementation of optimistic bandit algorithms in high-dimensional settings, with MaxNorm showing approximately 10x speedup over Newton in numerical experiments.

## Method Summary
The paper presents two algorithms for solving bilinear maximization problems when the action set is an ellipsoid. MaxNorm is a non-iterative method with time complexity O(d³ + d log(1/ε)) and space complexity O(d²), leveraging the geometric properties of ellipsoids to avoid iterative optimization. Newton is an iterative interior-point method with time complexity O(d³ + d⁵/² log² log²(1/ε) + d⁵/² log(1/ε)), using second-order optimization techniques. Both algorithms are specifically designed to handle the bilinear structure that arises in linear bandit algorithms like LinUCB when computing optimistic estimates.

## Key Results
- NP-hardness of bilinear maximization on ℓp balls with p > 2, explaining why optimistic bandits fail in these settings
- MaxNorm algorithm achieves O(d³ + d log(1/ε)) time complexity for ellipsoid action sets
- Newton algorithm achieves O(d³ + d⁵/² log² log²(1/ε) + d⁵/² log(1/ε)) time complexity
- Both algorithms efficiently solve high-dimensional instances (d ≥ 10³) in practice
- MaxNorm is approximately 10x faster than Newton in numerical experiments

## Why This Works (Mechanism)
The algorithms exploit the specific geometric structure of ellipsoids to transform the bilinear maximization problem into more tractable forms. MaxNorm leverages closed-form solutions that arise from the ellipsoidal geometry, while Newton's method uses the smooth, strongly convex nature of the problem within the ellipsoid to apply efficient interior-point techniques. The key insight is that while general convex sets lead to computationally intractable problems, the additional structure of ellipsoids enables polynomial-time solutions.

## Foundational Learning
- **Bilinear maximization**: Understanding the structure of max_x∈X ⟨Ax, b⟩ problems is crucial as this forms the core computational bottleneck in optimistic bandit algorithms
  - Why needed: These problems arise when computing optimistic estimates in linear bandits
  - Quick check: Verify that the bilinear form captures the essential difficulty of the optimization problem

- **NP-hardness reductions**: The hardness results for ℓp balls rely on known reductions from combinatorial optimization problems
  - Why needed: Establishes fundamental computational barriers for general convex sets
  - Quick check: Confirm that the reduction preserves approximation ratios

- **Interior-point methods**: Newton's method uses second-order optimization techniques that exploit problem smoothness
  - Why needed: Enables efficient iterative solution with strong convergence guarantees
  - Quick check: Verify that the Hessian structure is well-behaved throughout the optimization

- **Ellipsoid geometry**: The closed-form solutions in MaxNorm depend on specific properties of ellipsoidal sets
  - Why needed: Enables non-iterative solution that avoids the complexity of general convex optimization
  - Quick check: Confirm that the geometric constraints are correctly applied

## Architecture Onboarding

**Component Map**: Input parameters → MaxNorm/Newton algorithms → Solution vector → Bandit algorithm update

**Critical Path**: Problem formulation → Algorithm selection → Solution computation → Bandit decision making

**Design Tradeoffs**: MaxNorm favors speed over flexibility (non-iterative but ellipsoid-specific), while Newton offers broader applicability at computational cost

**Failure Signatures**: 
- NP-hardness for non-ellipsoidal sets indicates fundamental limitations
- Numerical instability in high dimensions suggests need for careful implementation
- Poor approximation quality indicates epsilon parameter tuning issues

**First Experiments**:
1. Verify MaxNorm solution quality on synthetic ellipsoid instances with known optima
2. Benchmark Newton method convergence rate across varying dimensionality
3. Compare both algorithms against naive grid search on small-dimensional instances

## Open Questions the Paper Calls Out
None

## Limitations
- Hardness results for ℓp balls with p > 2 suggest fundamental computational barriers for general convex sets
- Complexity analysis assumes efficient linear algebra routines that may not hold for extremely high-dimensional instances
- Limited validation to synthetic datasets without testing on real-world bandit problems
- Does not explore alternative action sets like simplices or product sets commonly used in practice

## Confidence
- High confidence in NP-hardness results for ℓp balls (p > 2)
- Medium confidence in MaxNorm algorithm's O(d³ + d log(1/ε)) complexity
- Medium confidence in Newton method's O(d³ + d⁵/² log² log²(1/ε) + d⁵/² log(1/ε)) complexity
- Low confidence in practical scalability claims without independent verification

## Next Checks
1. Verify empirical performance on non-synthetic datasets with varying noise distributions and correlation structures
2. Implement and benchmark against alternative approaches (first-order methods or approximate solvers)
3. Extend complexity analysis to alternative action sets (simplices, product sets) commonly used in bandit applications