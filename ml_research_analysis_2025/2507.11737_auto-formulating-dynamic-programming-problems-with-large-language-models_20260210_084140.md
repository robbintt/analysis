---
ver: rpa2
title: Auto-Formulating Dynamic Programming Problems with Large Language Models
arxiv_id: '2507.11737'
source_url: https://arxiv.org/abs/2507.11737
tags:
- problems
- problem
- generation
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPLM, a 7B-parameter language model fine-tuned
  for auto-formulating dynamic programming (DP) problems from natural language descriptions.
  To address the challenge of limited training data, the authors propose DualReflect,
  a synthetic data generation framework that combines forward generation (for diversity)
  and backward generation (for reliability).
---

# Auto-Formulating Dynamic Programming Problems with Large Language Models

## Quick Facts
- arXiv ID: 2507.11737
- Source URL: https://arxiv.org/abs/2507.11737
- Reference count: 38
- DPLM-7B achieves 65.6% accuracy on easy DP problems and 38.1% on hard problems

## Executive Summary
This paper introduces DPLM, a 7B-parameter language model fine-tuned for auto-formulating dynamic programming (DP) problems from natural language descriptions. To address the challenge of limited training data, the authors propose DualReflect, a synthetic data generation framework that combines forward generation (for diversity) and backward generation (for reliability). The resulting model, trained on 113K synthetic examples distilled from GPT-4o, achieves 65.6% accuracy on easy DP problems and 38.1% on hard problems, outperforming its teacher model and rivaling much larger models like DeepSeek-R1. The study introduces DP-Bench, the first benchmark for DP auto-formulation, and demonstrates that domain-specific small models can effectively tackle specialized tasks in operations research.

## Method Summary
DPLM is trained through a two-stage pipeline: SFT followed by GRPO alignment. The synthetic training data is generated using DualReflect, which combines forward generation (problem→solution) for diversity and backward generation (solution→problem) for reliability. The model starts from Qwen-2.5-7B-Instruct and is fine-tuned on 113K synthetic examples distilled from GPT-4o. The forward generation creates novel problem formulations while backward generation provides correctness guarantees by starting from verified code solutions. Reflected CoT allows iterative error recovery, recovering 19.1% of otherwise-discarded samples. The model is evaluated on DP-Bench, a new benchmark with 132 problems (90 easy, 42 hard).

## Key Results
- DPLM-7B achieves 65.6% accuracy on easy DP problems and 38.1% on hard problems
- Outperforms GPT-4o teacher (34.6% vs 38.1% on hard problems) and rivals DeepSeek-R1-32B
- Two-stage training (SFT→GRPO) significantly outperforms either stage alone (56.8% vs 23.5% for RL-only)
- DualReflect hybrid approach outperforms pure forward or backward generation at scale

## Why This Works (Mechanism)

### Mechanism 1: Complementary Data Generation via Forward-Backward Synthesis
Combining forward generation (problem→solution) with backward generation (solution→problem) produces higher-quality training data than either method alone. Backward generation provides correctness guarantees by starting from verified code solutions, recovering 19.1% of otherwise-discarded samples via Reflected CoT. Forward generation introduces novel problem formulations beyond seed structures, increasing diversity at scale. The hybrid approach outperforms pure methods at larger dataset sizes.

### Mechanism 2: Two-Stage Training (SFT→RL) for Format Acquisition and Correctness Refinement
Sequential SFT followed by RL alignment yields substantially better performance than either stage alone. SFT "cold-starts" the model by pulling the initial policy toward demonstration distributions, narrowing the search space for RL. RL then directly optimizes the reward signal for verifiable correctness. SFT alone achieves 33.3% micro-average; SFT→GRPO achieves 56.8%.

### Mechanism 3: Reflected Chain-of-Thought for Error Recovery
Iterative reflection-and-revision loops recover challenging problems by allowing models to compare failed solutions against ground-truth reference implementations. Given failed code C₀ and correct reference Ĉ, the model generates revised CoT₁, produces new solution (M₁, C₁), and iterates until outputs match (max 6 attempts). This produces full reasoning trajectories capturing both final solutions and iterative correction processes.

## Foundational Learning

- **Concept: Dynamic Programming Formulation** (Bellman equations, state/action spaces, transition probabilities)
  - Why needed here: DPLM must translate natural language into formal DP models M = {T, S, A, p_t, r_t, γ}; understanding these components is prerequisite for evaluating model outputs
  - Quick check question: Given an inventory problem with stochastic demand, can you identify the state variable(s), decision variable(s), and transition probability structure?

- **Concept: Synthetic Data Distillation from Teacher Models**
  - Why needed here: The entire training pipeline relies on GPT-4o-generated synthetic data; understanding distillation trade-offs (teacher model limitations, distribution shift) is essential for data quality assessment
  - Quick check question: What failure modes might arise when training a 7B student model exclusively on 113K examples from a 200B teacher?

- **Concept: RLHF Alternatives (DPO vs. GRPO)**
  - Why needed here: The paper compares DPO (offline, preference pairs) and GRPO (online, group-normalized advantages); selecting between them involves computational cost vs. accuracy trade-offs
  - Quick check question: Why does GRPO require ~8x more wall time than DPO on identical hardware, and when might this overhead be justified?

## Architecture Onboarding

- **Component map**: Seed Data (91 problems) → Scenario Expansion (400+ templates) → DualReflect (Forward + Backward generators) → RAG-based Solution Generator → SFT Dataset (113K samples) → SFT Stage (2 epochs) → RL Stage (GRPO or DPO) → DPLM-7B
- **Critical path**: Seed curation quality determines backward-generation diversity ceiling; RAG retrieval with structural labels improves solution generator accuracy; GRPO reward shaping provides intermediate learning signal for multi-stage DP tasks
- **Design tradeoffs**: Model size: 7B chosen for computational feasibility; Generation mix: 70K forward / 34K backward / 8K reflected; RL algorithm: GRPO achieves 56.8% vs DPO's 40.2% but requires 8x more compute
- **Failure signatures**: Forward-generated problems may be ill-posed; backward-generated problems share model formulations with seeds; RL-only training achieves only 23.5% micro-average
- **First 3 experiments**: Validate RAG retrieval; ablate generation mix at 8K/16K/32K scales; compare RL algorithms (GRPO vs DPO) from same SFT checkpoint

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can expanding the initial seed dataset beyond the current 91 examples effectively close the performance gap between DPLM and larger models like DeepSeek-R1 on "easy" DP problems?
- **Basis in paper:** The paper notes in Section 6.2 and Section 7 that DPLM lags behind DeepSeek-R1 on easy problems due to "limited coverage of specialized domain knowledge," suggesting that "expanding the initial seed data and scaling up the synthetic data generation process seem promising to overcome this limitation."
- **Why unresolved:** The current study uses a fixed seed set, and the authors identify this specific deficiency in performance on simpler problems without testing the proposed solution.
- **What evidence would resolve it:** Experiments showing the accuracy delta on easy problems decreasing as the diversity and quantity of the initial seed corpus are systematically increased.

### Open Question 2
- **Question:** How should model size and synthetic data volume be co-scaled in the 7B to 32B parameter range to overcome the observed shifting bottlenecks in training efficiency?
- **Basis in paper:** Appendix EC.9 states that while model capacity is the bottleneck below 7B parameters, "between 7B and 32B, the limiting factor shifts to the volume of training data," leading the authors to propose that "a more effective path to further accuracy gains is to co-scale model size and training data in tandem."
- **Why unresolved:** The paper empirically tests varying model sizes but does not define the optimal ratio or scaling law for simultaneously increasing parameter count and synthetic data magnitude.
- **What evidence would resolve it:** A series of ablations varying both model dimensions and dataset sizes simultaneously to identify the interaction effects and optimal scaling coefficients.

### Open Question 3
- **Question:** To what extent does the DPLM model, trained on synthetic "textbook-level" data, transfer to industrial-scale DP problems characterized by ambiguous constraints?
- **Basis in paper:** The introduction distinguishes between formal CS-style DP problems and "text-rich, business-oriented scenarios" common in OR, yet the proposed DP-Bench consists of "textbook-level" problems with specific numerical answers.
- **Why unresolved:** The paper establishes performance on standardized benchmarks but does not evaluate the model's robustness against the "implicit assumptions" and "contextual nuances" found in real-world industrial descriptions that lack the clarity of textbook exercises.
- **What evidence would resolve it:** Evaluation of the fine-tuned model on a hold-out set of real-world, industry-sourced OR problems where problem formulations are not standardized.

## Limitations

- The model's performance on hard problems (38.1%) remains substantially below human-level performance
- Synthetic data generation introduces noise through occasional ill-posed problems and incorrect solutions
- Backward generation constrains diversity by sharing model formulations with seed problems
- The study's focus on a narrow problem domain (dynamic programming) limits broader claims about LLM capabilities

## Confidence

**High Confidence (95%+)**: The two-stage training approach (SFT→GRPO) demonstrably outperforms either stage alone, with SFT providing essential format acquisition and GRPO refining for correctness.

**Medium Confidence (75-95%)**: The reported performance improvements over teacher models and competitive results against much larger models are convincing but should be interpreted cautiously given the synthetic nature of the training data.

**Low Confidence (<75%)**: Claims about the fundamental correctness-diversity trade-off in DP formulation tasks and the optimal generation mix require more extensive ablation studies across different problem domains.

## Next Checks

1. **Generalization Test**: Evaluate DPLM on naturally-occurring DP problems from real-world applications (not synthetically generated) to assess whether the model can handle genuine domain diversity beyond the 400+ scenarios in the synthetic corpus.

2. **Data Quality Audit**: Conduct a systematic analysis of synthetic training samples to quantify the rate of ill-posed problems and incorrect solutions in the forward-generated subset, measuring the effectiveness of the majority voting filter and identifying failure patterns.

3. **Cross-Domain Transfer**: Fine-tune DPLM on a small set of problems from a related formal reasoning domain (e.g., constraint satisfaction or planning) to test whether the learned DP formulation skills transfer to novel problem structures without extensive retraining.