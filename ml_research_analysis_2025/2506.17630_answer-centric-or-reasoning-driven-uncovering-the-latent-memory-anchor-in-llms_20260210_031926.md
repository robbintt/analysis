---
ver: rpa2
title: Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in
  LLMs
arxiv_id: '2506.17630'
source_url: https://arxiv.org/abs/2506.17630
tags:
- reasoning
- answer
- llms
- arxiv
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models (LLMs) rely
  more on final answers or on reasoning chains during problem-solving. To test this,
  the authors designed a five-level prompt framework that systematically varies the
  visibility of final answers within prompts, ranging from explicit answers to fully
  answer-free conditions.
---

# Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs

## Quick Facts
- arXiv ID: 2506.17630
- Source URL: https://arxiv.org/abs/2506.17630
- Reference count: 8
- Key outcome: LLMs show a strong reliance on final answers over reasoning chains, with a 26.90% performance drop when answer cues are masked.

## Executive Summary
This paper investigates whether large language models solve problems by reasoning through logic or by anchoring to final answers. Using a five-level prompt framework on a Chinese mathematical reasoning benchmark, the authors find that models perform significantly worse when answer cues are masked, even when full reasoning chains are provided. This suggests that observed reasoning often reflects post-hoc rationalization around memorized answers rather than genuine inference. The findings challenge assumptions about LLM reasoning capabilities and highlight a critical limitation in current evaluation methods.

## Method Summary
The authors designed a five-level prompt framework to systematically vary the visibility of final answers within prompts: Answer-Explicit (AE), Answer-Embedded-Reasoning (AER), Answer-Masked-Reasoning (AMR), Answer-Removed-Reasoning (ARR), and Answer-Free (AF). Experiments were conducted on state-of-the-art LLMs using a Chinese mathematical reasoning benchmark (RoR-Bench), with zero-shot Chain-of-Thought prompting and temperature=0. Accuracy was measured using GPT-4o-1120 as an automated verifier. Additional experiments tested conflict resolution between correct answers with flawed reasoning versus correct reasoning with incorrect answers, and the effect of explicit warnings about incorrect answers.

## Key Results
- Performance drops by 26.90% when answer cues are masked, even with complete reasoning chains provided.
- Models prioritize correct answers over correct reasoning, performing better with right answers and wrong reasoning than with wrong answers and right reasoning.
- Explicit warnings about incorrect answers only partially mitigate the answer-anchoring effect.

## Why This Works (Mechanism)

### Mechanism 1: Answer-Anchored Memory Retrieval
The explicit answer acts as a strong prior, triggering retrieval of associated reasoning templates from pre-training data. The model "recites" the reasoning path bound to that answer rather than executing computation. Performance degrades sharply to baseline levels when the answer cue is masked (AMR) or removed (ARR), revealing the model cannot sustain performance via reasoning alone.

### Mechanism 2: Post-Hoc Rationalization
Once an answer is anchored, the generation process fills in plausible intermediate steps to satisfy the prompt format, creating an illusion of inference. This coherence allows for plausible-sounding but logically non-causal bridges between a problem and a memorized answer.

### Mechanism 3: Conflict Resolution via Answer Dominance
When provided with conflicting signals—an explicit answer versus a reasoning chain—the model prioritizes the answer cue even if the reasoning is flawed. The attention mechanism or decision boundary weighs the explicit answer token more heavily than the structural logic of the reasoning chain.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) vs. Genuine Inference**
  - Why needed: The paper challenges the assumption that CoT generation equals reasoning. Distinguishing "reciting a template" from "deriving a result" is crucial.
  - Quick check: If a model produces a correct reasoning chain but fails when the answer token is masked, is it reasoning or reciting?

- **Concept: Behavioral Probing (Black-box Analysis)**
  - Why needed: The authors use input manipulation to infer internal states because they cannot inspect weights. This indirect methodology is key to the paper's logic.
  - Quick check: Why is varying "answer visibility" (AE vs AMR) a valid way to test memory anchoring without looking at neurons?

- **Concept: Data Contamination / Memorization**
  - Why needed: The "answer anchoring" effect relies on the model having seen problem-answer pairs before. Understanding training data contamination explains why these anchors exist.
  - Quick check: How does the "training-testing gap" influence the brittleness of LLM reasoning on perturbed problems?

## Architecture Onboarding

- **Component map:** Problem Statement + Answer Cue Manipulation (AE/AER/AMR/ARR/AF) -> Zero-shot CoT with specific answer visibility constraints -> GPT-4o-1120 based binary scoring of exact answer matches

- **Critical path:** The transition from Answer-Explicit (AE) to Answer-Masked-Reasoning (AMR) is the most sensitive diagnostic step; a sharp drop here confirms the "Answer Anchoring" diagnosis.

- **Design tradeoffs:** The framework uses artificial prompts (providing answers to the model) to diagnose behavior. While diagnostically powerful, this diverges from standard user interactions where answers are rarely provided.

- **Failure signatures:**
  - High AE / Low AF gap: >50% difference indicates high reliance on memory/memorization over reasoning.
  - RA/WR > WA/RR: If accuracy with "Right Answer/Wrong Reasoning" exceeds "Wrong Answer/Right Reasoning," the model is answer-centric, not logic-centric.

- **First 3 experiments:**
  1. Baseline Visibility Test: Run the specific model (e.g., DeepSeek-R1) on the 5-level framework (AE to AF) to establish the "Anchoring Profile."
  2. Conflict Stress Test: Feed the model Right Answer + Wrong Reasoning inputs to see if it blindly trusts the answer or corrects based on logic.
  3. Warning Override Test: Apply "Hard Warning" prompts (telling the model the answer is wrong) to measure the "tenacity" of the anchor—does accuracy drop below the baseline?

## Open Questions the Paper Calls Out
- Does the answer-anchoring phenomenon persist in multi-modal models, particularly those performing visual reasoning tasks? (The authors explicitly state this investigation is left for future work.)
- To what extent do these findings generalize to non-mathematical domains or languages other than Chinese? (The paper acknowledges this generalizability warrants further exploration.)
- Can training interventions prioritizing process supervision over outcome supervision successfully mitigate answer-anchoring? (The study diagnoses the dependency but does not propose or test training methodologies to correct it.)

## Limitations
- Findings are based on a Chinese mathematical reasoning benchmark (RoR-Bench) with 158 questions, limiting generalization to other domains or languages.
- The study uses zero-shot CoT with artificially provided answers, a scenario rarely encountered in real-world usage.
- Accuracy is measured using GPT-4o-1120 as a binary verifier, which may introduce model-specific biases despite 92.15% agreement with human annotators.

## Confidence
- **High Confidence:** The answer-anchoring effect (26.90% drop when answers are masked) is robust and clearly demonstrated.
- **Medium Confidence:** The post-hoc rationalization hypothesis is well-supported but remains inferential; the study does not directly prove reasoning chains are constructed after answer selection.
- **Low Confidence:** Generalization to non-mathematical tasks or real-world usage scenarios is speculative and not tested.

## Next Checks
1. Replicate the five-level framework on English-language benchmarks like GSM8K or MATH to test whether the answer-anchoring effect persists across languages and problem types.
2. Conduct a small-scale human evaluation (e.g., 50 samples) to measure the actual accuracy gap between model outputs and ground truth, independent of the automated verifier.
3. Modify the experiment to use standard prompting (no provided answers) and compare performance to the zero-shot CoT with answer cues to reveal whether observed effects are artifacts of the artificial setup or reflect inherent model behavior.