---
ver: rpa2
title: Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based
  Language Models
arxiv_id: '2508.17734'
source_url: https://arxiv.org/abs/2508.17734
tags:
- layer
- layers
- baseline
- ffns
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the layerwise importance of feed-forward
  networks (FFNs) in Transformer-based language models during pretraining. The authors
  develop an experimental approach that, while maintaining the total parameter count,
  increases FFN dimensions in some layers and completely removes FFNs from other layers.
---

# Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models

## Quick Facts
- arXiv ID: 2508.17734
- Source URL: https://arxiv.org/abs/2508.17734
- Reference count: 28
- Authors: Wataru Ikeda; Kazuki Yano; Ryosuke Takahashi; Jaesung Lee; Keigo Shibata; Jun Suzuki
- Key outcome: Concentrating FFNs in 70% of consecutive middle layers consistently outperforms standard uniform FFN placement across multiple downstream tasks, with edge-layer FFNs potentially redundant.

## Executive Summary
This study investigates the layerwise importance of feed-forward networks (FFNs) in Transformer-based language models during pretraining. The authors develop an experimental approach that, while maintaining the total parameter count, increases FFN dimensions in some layers and completely removes FFNs from other layers. Instead of using pretrained models, they train models from scratch with varying sizes (285M, 570M, and 1.2B parameters) and layer counts (12, 24, and 40 layers) to examine how FFN importance varies by layer position. The results demonstrate that concentrating FFNs in 70% of consecutive middle layers consistently outperforms standard uniform FFN placement across multiple downstream tasks. Additionally, the study finds that FFNs in the first and last few layers may be redundant, as their functionality can be replaced by FFNs in middle layers.

## Method Summary
The study trains LLaMA-style models from scratch with modified FFN placement patterns. The experimental design removes FFNs from some layers and expands FFN dimensions in others while maintaining total parameter count. Three placement modes (first, middle, final) and six concentration ratios (10-100%) are tested across model scales (285M/12L, 570M/24L, 570M/40L, 1.2B/40L). Models are trained on FineWeb-Edu for 20× Chinchilla tokens and evaluated on lm-evaluation-harness tasks plus zsRE for knowledge capacity. The layerwise importance metric measures performance impact when deactivating FFNs layer-by-layer.

## Key Results
- Middle-70 configuration (70% FFN concentration in middle layers) achieves +1-4% Relative Improvement over baseline across scales
- FFNs in first and last few layers show negative importance scores, suggesting functional redundancy
- Deeper 40-layer models show importance shift toward earlier layers, possibly due to overcontextualization from excessive self-attention
- 70% middle concentration particularly excels on knowledge-intensive tasks like zsRE (+3.06% improvement)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Storage Concentration in Middle Layers
Concentrating FFN capacity in middle layers improves downstream task performance because middle layers handle the most knowledge-intensive processing. When FFNs are removed from edge layers and their parameters redistributed to expanded FFNs in middle layers, the model retains total capacity while positioning it where prior work suggests knowledge storage primarily occurs. The expanded intermediate dimensions in middle layers provide richer key-value memory for factual associations.

### Mechanism 2: Functional Redundancy of Edge-Layer FFNs
First and last layer FFNs contribute less to downstream performance and their removal can be compensated by middle-layer FFNs during training from scratch. The layerwise importance metric quantifies that deactivating edge-layer FFNs causes smaller performance drops than deactivating middle-layer FFNs. During pretraining, expanded middle FFNs learn to handle representations that would otherwise require edge FFNs.

### Mechanism 3: Depth-Dependent Importance Shift
As models deepen, the optimal FFN concentration region shifts earlier in the network. In deeper networks (40 layers vs. 12 layers), hidden states undergo more self-attention operations before reaching middle layers, potentially creating over-contextualized representations that FFNs process less effectively. Earlier FFN placement mitigates this.

## Foundational Learning

- **Concept: Pre-LN Transformer Layer Structure (Self-Attention → FFN → Residual)**
  - Why needed here: The paper modifies FFN placement within this structure; understanding the baseline helps interpret what's being changed.
  - Quick check question: Can you sketch the data flow through a single pre-LN transformer layer?

- **Concept: FFN as Key-Value Memory (Knowledge Storage Hypothesis)**
  - Why needed here: The entire experimental design assumes FFNs store knowledge; without this, the importance analysis lacks theoretical motivation.
  - Quick check question: What is the proposed functional role of FFNs vs. self-attention in transformer LMs?

- **Concept: Parameter Budget Constraint Under Architecture Modification**
  - Why needed here: The paper removes FFNs from some layers but expands others to maintain total parameters; understanding this parity is essential for fair comparison.
  - Quick check question: If you remove FFNs from 30% of layers, how do you calculate the new intermediate dimension for remaining FFNs to preserve total parameters?

## Architecture Onboarding

- **Component map:**
  - Standard Layer: Self-Attention → FFN (dimension df) → Residual
  - FFN-Expanded Layer: Self-Attention → FFN (dimension d′f > df) → Residual
  - FFN-Deactivated Layer: Self-Attention only → Residual (no FFN)
  - Three placement modes: `first` (input-side concentration), `middle` (symmetric around L/2), `final` (output-side concentration)

- **Critical path:**
  1. Choose baseline model config (layers L, hidden dim d, intermediate dim df)
  2. Select ratio r% of FFN-expanded layers (paper tests 10, 30, 50, 70, 90)
  3. Select placement position (first/middle/final)
  4. Calculate expanded dimension d′f to maintain total parameters
  5. Train from scratch (not applicable to pretrained models)

- **Design tradeoffs:**
  - Higher concentration (lower r%) → larger individual FFNs but fewer nonlinear transformation points → risk of underfitting
  - Middle placement → best average RI across scales but may not be optimal for very deep (>40 layer) models
  - Training from scratch required; post-hoc modification of pretrained models not addressed

- **Failure signatures:**
  - r% < 30: Consistent negative RI across tasks (Figure 4 shows -1% to -19% degradation)
  - `first` placement in 12-24 layer models: Underperforms `middle`/`final` by 1-4% RI
  - Very deep models with `final` placement: Inconsistent results, sometimes underperforms `first`

- **First 3 experiments:**
  1. **Reproduce middle-70 on small scale**: Train 285M/12-layer model with middle-70 configuration; expect +1-4% average RI over baseline on Wikitext, LAMBADA, HellaSwag.
  2. **Ablate placement position**: Compare first-70, middle-70, final-70 on same base model; verify middle-70 outperforms others on knowledge tasks (zsRE).
  3. **Test edge case**: Try r=10% to confirm performance collapse; this validates the importance metric methodology and establishes the lower bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the middle-layer FFN concentration advantage persist through instruction fine-tuning, RLHF, or other post-pretraining adaptation methods?
- Basis in paper: [explicit] The study focuses exclusively on pretraining ("our focus is on the importance of FFNs during pretraining") and evaluates downstream tasks without examining how fine-tuning interacts with the proposed architecture.
- Why unresolved: Modern LLMs undergo extensive post-training, and architectural advantages at pretraining may not transfer through alignment procedures that could redistribute learned representations.
- What evidence would resolve it: Train models with concentrated middle-layer FFNs, apply identical instruction fine-tuning and RLHF, then compare downstream task performance against uniformly distributed FFN baselines.

### Open Question 2
- Question: Why does the optimal FFN placement shift toward earlier layers in deeper networks (observed in 40-layer models), and does this trend continue for models beyond 1.2B parameters?
- Basis in paper: [explicit] The authors note "the 40-layer model shifts importance toward the earlier portion of the network" and speculate this "might occur because, in deeper networks, hidden states execute more self-attention functions before reaching the middle layers, potentially resulting in overcontextualized representations that FFNs may struggle to process effectively."
- Why unresolved: The explanation is post-hoc speculation without experimental verification, and no data exists for 7B+ parameter models where this shift could have significant architectural implications.
- What evidence would resolve it: Train larger models (e.g., 7B, 13B parameters) with varying FFN placements and analyze whether the forward shift continues; conduct probing experiments to measure "overcontextualization" in middle layers of deep networks.

### Open Question 3
- Question: Does the 70% middle-layer concentration finding generalize to non-LLaMA architectures such as encoder-only (BERT-style) or encoder-decoder (T5-style) Transformers?
- Basis in paper: [inferred] The study explicitly limits investigation to "the LLaMA architecture (Touvron et al., 2023), which has become the de facto standard model for Transformer LMs," leaving other architectural families unexplored.
- Why unresolved: Different architectures use FFNs differently (e.g., bidirectional attention in BERT, cross-attention in T5), which may alter the layerwise importance distribution of FFN components.
- What evidence would resolve it: Replicate the experimental methodology on BERT-base and T5-base architectures, comparing middle-concentrated vs. uniform FFN placement on architecture-appropriate benchmarks.

## Limitations
- The layerwise importance metric's stability across different datasets, model scales, and training durations remains unverified.
- The claim that FFNs function as key-value memories relies heavily on citations rather than direct empirical validation within this study.
- The finding that 70% middle-layer concentration is optimal may be specific to the LLaMA architecture and SwiGLU activation.

## Confidence
- **High confidence**: The experimental design showing 70% middle-layer FFN concentration outperforms uniform distribution across multiple scales and tasks.
- **Medium confidence**: The layerwise importance metric's validity for identifying redundant edge-layer FFNs, as this depends on training from scratch rather than post-hoc modification of pretrained models.
- **Low confidence**: The depth-dependent importance shift claim (40+ layers requiring earlier FFN placement), as this extrapolates from only one data point.

## Next Checks
1. **Cross-architectural replication**: Test the 70% middle-layer configuration on BERT-style transformers with GELU activation to verify the finding extends beyond LLaMA/SwiGLU.

2. **Pretraining vs. finetuning importance**: Compare layerwise importance scores when models are finetuned on downstream tasks versus during pretraining to determine if the metric reflects pretraining-specific dynamics.

3. **Edge case stress test**: Systematically evaluate r=10% and r=90% configurations across all three scales to confirm the performance collapse at low concentrations and diminishing returns at high concentrations.