---
ver: rpa2
title: Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users
arxiv_id: '2510.17173'
source_url: https://arxiv.org/abs/2510.17173
tags:
- tool
- health
- policy
- arxiv
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Offline policy evaluation (OPE) on real-world health coaching\
  \ logs and a lightweight simulator reveal that while uniform heavy-tool usage maximizes\
  \ average reward, it systematically harms specific user subgroups\u2014particularly\
  \ low-literacy/high-efficacy users\u2014demonstrating the need for archetype-aware\
  \ personalization. Adding a small, bounded early information-gain bonus consistently\
  \ improves task success, reliability (pass@3), and shortens trait-identification\
  \ time in simulation."
---

# Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users

## Quick Facts
- **arXiv ID**: 2510.17173
- **Source URL**: https://arxiv.org/abs/2510.17173
- **Reference count**: 25
- **Primary result**: Offline policy evaluation reveals uniform heavy-tool usage maximizes average reward but systematically harms low-literacy/high-efficacy users, with early information-gain bonus improving success and trait identification in simulation.

## Executive Summary
This paper introduces a framework for offline policy evaluation (OPE) of multi-turn LLM health coaching with real users. Using real-world logs and a calibrated simulator, the authors demonstrate that factorized decision heads (TOOL × STYLE) enable personalized routing without modifying the frozen LLM generator. The analysis reveals that while heavy tool usage maximizes average reward, it harms specific subgroups—particularly low-literacy/high-efficacy users—highlighting the need for archetype-aware personalization. Adding a bounded early-turn information-gain bonus consistently improves task success, reliability, and shortens trait identification time in simulation.

## Method Summary
The method employs offline policy evaluation over real user logs (7 users, 280 rated turns) with a frozen Qwen3-235B-A22B generator. The policy factorizes into discrete TOOL and STYLE decision heads learned from user features and belief states. OPE uses SNIPS and AIPW estimators with importance-ratio clipping and calibration diagnostics. A lightweight simulator models latent user archetypes (health literacy × self-efficacy) to test counterfactual policies. Reward composition includes tool outcomes, satisfaction ratings, and engagement signals with literacy-stratified weights.

## Key Results
- Uniform heavy-tool usage maximizes average reward but systematically harms low-literacy/high-efficacy users
- Per-archetype OPE reveals heterogeneous treatment effects that aggregate metrics obscure
- Adding a small early information-gain bonus (λ=0.1-0.2, K=2 turns) improves task success, pass@3, and shortens trait identification time in simulation

## Why This Works (Mechanism)

### Mechanism 1
- Factorizing the policy into discrete TOOL and STYLE decision heads enables targeted personalization without modifying the frozen generator. The system learns separate lightweight classifiers for when to invoke tools and response style, reducing the action space and allowing archetype-aware routing based on observable user features.
- Core assumption: Optimal TOOL and STYLE decisions are conditionally independent given context state and user features.
- Evidence: Abstract shows uniform heavy-tool usage maximizes average reward but harms specific subgroups; section 3.3 describes the factorization approach.
- Break condition: If tool and style choices exhibit strong conditional dependence, the factorization will underperform joint policy learning.

### Mechanism 2
- A small, bounded early-turn information-gain bonus accelerates user archetype identification and improves downstream task success. The curiosity reward encourages actions that reduce entropy over latent user archetypes in the first K turns.
- Core assumption: Early exploration actions that reduce archetype uncertainty lead to better personalization decisions later.
- Evidence: Abstract states the bonus "reliably shortens trait identification and improves goal success"; section 3.4 defines the curiosity bonus applied to first K turns.
- Break condition: If early probing damages user trust or causes disengagement before personalization can help.

### Mechanism 3
- Per-archetype OPE analysis reveals heterogeneous treatment effects that aggregate metrics obscure. By stratifying OPE estimates by user archetype, the analysis shows policies with identical average value have opposite effects across subgroups.
- Core assumption: Reconstructed behavior policy adequately approximates true logging propensities with well-calibrated importance weights.
- Evidence: Abstract highlights harms to specific subgroups; Table 6 shows large signed differences by archetype in ΔObjective and ΔSatisfaction.
- Break condition: If archetype labels are mis-specified or inferred with high error, stratified OPE will be noisy and potentially misleading.

## Foundational Learning

- **Offline Policy Evaluation (OPE) with Importance Sampling**: Needed to compare counterfactual policies without running new user trials. Quick check: Given propensity 0.3 under behavior policy and 0.9 under target policy, what is the importance weight? (Answer: 3.0)

- **Partial Observability and Belief States (POMDP)**: User archetypes are latent, so the system maintains belief state summarizing dialogue history and inferred user features. Quick check: Why can't the policy directly condition on the true user archetype? (Answer: Archetype is latent; only observable through survey proxies and dialogue behavior)

- **Typed Rewards and Reward Composition**: The paper distinguishes tool outcomes, satisfaction ratings, and engagement signals with personalized weights varying by literacy stratum. Quick check: If a low-literacy user experiences tool failure but rates response 5/5, which reward component dominates? (Answer: Satisfaction weighted at 0.6 vs. tool outcomes at 0.2)

## Architecture Onboarding

- **Component map**: User uploads health data → pipeline generates features → user sends message → system encodes belief state → decision heads sample TOOL/STYLE → if tool invoked, execute tool → generator produces response → user rates response → log rating and update user model

- **Critical path**: 1) User uploads health data → pipeline generates features and predictions; 2) User sends message → system encodes belief state; 3) Decision heads sample TOOL/STYLE; 4) If tool invoked → execute tool, observe outcome, compute R_tool; 5) Generator produces response in chosen style; 6) User rates response → log rating, update user model, store turn for OPE

- **Design tradeoffs**: Frozen generator vs. end-to-end RL (freezing reduces complexity and risk but limits adaptation); early curiosity vs. immediate task focus (exploration improves trait identification but may frustrate users); stratified vs. aggregate OPE (per-archetype analysis reveals harms but requires sufficient samples)

- **Failure signatures**: High clipping rate (>5%) indicates importance weights exploding; low rating-propensity AUC (<0.6) suggests selection bias not adequately modeled; large ECE on TOOL head (>0.2) indicates mis-calibrated behavior policy probabilities

- **First 3 experiments**: 1) Validate behavior policy reconstruction by fitting calibrated classifiers and reporting per-head ECE and propensity AUC; 2) Re-run per-archetype OPE with bootstrap CIs to verify consistency; 3) Run simulator ablation on curiosity weight (λ ∈ {0, 0.1, 0.2, 0.3}, K ∈ {1, 2, 3}) to plot trait-identification turn vs. final return

## Open Questions the Paper Calls Out

- **Will incorporating self-efficacy into reward weighting and decision-head policies improve personalization beyond literacy-only routing?** The paper states this as a planned extension, noting that the Llow×Ehigh subgroup showed the largest harm under heavy-tool policies.

- **Can end-to-end RL policy learning outperform the frozen-generator approach with learned decision heads?** Authors recommend freezing the generator and learning subgroup-aware decision heads, explicitly leaving end-to-end policy learning to future work.

- **Do the OPE subgroup findings and curiosity-bonus benefits generalize to larger, demographically diverse populations?** The paper notes its deployment sample is small and demographically narrow, limiting generalizability.

## Limitations

- Small user cohort (N=7) limits statistical power for subgroup analysis and makes confidence intervals sensitive to individual users
- Simulator calibration introduces simulation bias not quantified against real-world performance
- Policy definitions for HEURISTICGATED and PERSONALIZEDWEIGHTS are loosely specified with exact thresholds unavailable
- Long-term impact of early curiosity-driven probing on user trust and retention is not measured

## Confidence

- **High Confidence**: Offline policy evaluation framework and simulator design; factorized decision heads enable personalization; early curiosity bonus improves trait identification in simulation
- **Medium Confidence**: Per-archetype OPE results and heterogeneous treatment effects; simulator calibration and transferability to real users; policy effectiveness for specific archetypes
- **Low Confidence**: Real-world impact of curiosity bonus on user trust and retention; generalizability beyond studied cohort; simulation bias and fidelity to real user behavior

## Next Checks

1. **Behavior Policy Reconstruction Validation**: Fit calibrated classifiers on held-out logs, report per-head ECE and propensity AUC. If ECE > 0.2, apply temperature or Platt scaling before OPE.

2. **Archetype-Stratified OPE Replication**: Re-run Tables 5–6 with 1000+ bootstrap resamples; verify consistency of AlwaysTool–NoTool differences and absence of single-user-driven effects.

3. **Simulator Curiosity Ablation**: Vary λ ∈ {0, 0.1, 0.2, 0.3} and K ∈ {1, 2, 3}; plot trait-identification turn vs. final return to confirm the Pareto frontier supports λ=0.1–0.2, K=2.