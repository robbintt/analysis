---
ver: rpa2
title: 'ESM: A Framework for Building Effective Surrogate Models for Hardware-Aware
  Neural Architecture Search'
arxiv_id: '2508.01505'
source_url: https://arxiv.org/abs/2508.01505
tags:
- latency
- encoding
- architecture
- accuracy
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ESM, a framework for building effective latency
  prediction models for hardware-aware neural architecture search (HW-NAS). The work
  addresses the challenge of accurately predicting DNN inference latency on resource-constrained
  devices, which is critical for efficient HW-NAS.
---

# ESM: A Framework for Building Effective Surrogate Models for Hardware-Aware Neural Architecture Search

## Quick Facts
- arXiv ID: 2508.01505
- Source URL: https://arxiv.org/abs/2508.01505
- Reference count: 19
- ESM achieves up to 99% accuracy in latency prediction across diverse hardware devices using Feature Combination Count (FCC) encoding

## Executive Summary
This paper presents ESM, a framework for building accurate latency prediction models for hardware-aware neural architecture search (HW-NAS). The work addresses the challenge of predicting DNN inference latency on resource-constrained devices, which is critical for efficient HW-NAS. ESM introduces the Feature Combination Count (FCC) encoding scheme that captures feature interactions while maintaining manageable model complexity, and employs an iterative train-evaluate-extend approach to minimize dataset collection costs. Extensive experiments on ResNet, MobileNetV3, and DenseNet architectures across multiple devices (RTX 4090, AMD CPU, RTX 3080Ti, Raspberry Pi 4) demonstrate superior accuracy compared to state-of-the-art methods, with average accuracies reaching up to 99% on certain architectures and devices.

## Method Summary
ESM is a framework for building latency prediction models for HW-NAS that uses an iterative train-evaluate-extend approach. The framework supports two sampling strategies (random and balanced) and three encoding schemes (one-hot, feature, and statistical). The key innovation is the Feature Combination Count (FCC) encoding, which creates all possible combinations of block-level features and counts occurrences of each combination across the model. The iterative process starts with N_I initial samples, trains an MLP regressor (3 layers × 64 hidden units), evaluates bin-wise accuracy, and extends the dataset by N_Step samples for bins below the accuracy threshold (AccTH). The process repeats until all bins meet the threshold or maximum iterations are reached.

## Key Results
- ESM with FCC encoding achieves up to 99% accuracy in latency prediction on resource-constrained devices like Raspberry Pi 4
- Balanced sampling strategy converges to accuracy thresholds with ~8x fewer samples than random sampling by mitigating depth-distribution bias
- The train-evaluate-extend approach minimizes data collection costs while guaranteeing accuracy across the full depth range of architecture spaces
- FCC encoding preserves feature interactions that statistical encodings compress away, leading to superior prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Feature Combination Count (FCC) encoding achieves higher prediction accuracy than statistical encoding by preserving feature interactions while avoiding excessive vector sparsity.
- **Mechanism:** FCC creates all possible combinations of block-level features (e.g., kernel size + expansion ratio pairs), then counts occurrences of each combination across the model. This captures joint feature effects that statistical encodings (mean/std) compress away.
- **Core assumption:** Latency depends on feature combinations (e.g., kernel=5 with expansion=2/3) rather than individual feature statistics aggregated independently.
- **Evidence anchors:**
  - [abstract] "authors propose novel architecture encoding schemes, particularly the Feature Combination Count (FCC) encoding"
  - [Section II.C, Page 3-4] "FCC first makes combinations of all possible features of a block, and then counts the number of instances each combination is used"
  - [corpus] Weak corpus support; related papers focus on NAS frameworks rather than encoding schemes specifically.
- **Break condition:** If your architecture space has no block-level feature interactions (independent features), FCC provides no advantage over simpler encodings.

### Mechanism 2
- **Claim:** Balanced sampling converges to accuracy thresholds with ~8x fewer samples than random sampling by mitigating depth-distribution bias.
- **Mechanism:** Random sampling produces Gaussian-like depth distributions (central limit theorem), underrepresenting shallow/deep architectures. Balanced sampling enforces equal samples per depth bin, ensuring corner-case coverage.
- **Core assumption:** Prediction accuracy is bottlenecked by underrepresented regions of the architecture space, not total sample count.
- **Evidence anchors:**
  - [Section II.C, Page 3] "This bias in the sampling distribution forces the predictor to perform poorly for models present in the corner cases"
  - [Section III.D, Page 6, Fig. 11] "Balanced sampling strategy converged after 3 iterations with 500 samples, while random sampling strategy took 37 iteration to converge with 4000 samples"
  - [corpus] Not directly addressed in neighbor papers.
- **Break condition:** If your architecture space is small or already uniform in depth distribution, balanced sampling adds complexity without benefit.

### Mechanism 3
- **Claim:** Iterative train-evaluate-extend with bin-wise accuracy thresholds minimizes data collection cost while guaranteeing accuracy across the full depth range.
- **Mechanism:** Instead of collecting a fixed large dataset upfront, ESM starts small, evaluates per-bin accuracy, and extends only where accuracy falls below threshold. This avoids over-sampling well-predicted regions.
- **Core assumption:** Latency measurement is expensive (multiple inferences for noise reduction), so minimizing measurements while meeting accuracy targets is valuable.
- **Evidence anchors:**
  - [Section I.A, Page 2, Fig. 4] "average latency measurement time per model is comparable to the time required to train a latency predictor with over 8,000 samples"
  - [Section I.B, Page 2] "train-evaluate-extend approach, allowing it to efficiently converge... while minimizing dataset collection costs"
  - [corpus] SNAC-Pack (arXiv:2512.15998) similarly addresses cost-efficient surrogate modeling but uses different approaches.
- **Break condition:** If latency measurement is cheap (fast hardware, single inference sufficient), iterative extension overhead may outweigh savings.

## Foundational Learning

- **Concept: Supernet search spaces (OFA-style)**
  - Why needed here: ESM operates on layer/block-wise spaces derived from a fixed macro-architecture (ResNet, MobileNetV3, DenseNet supernets). Understanding that architectures are configurations of a supernet—not independently designed networks—is essential.
  - Quick check question: Can you explain why the same supernet produces 8.38×10²⁶ ResNet configurations?

- **Concept: Surrogate models in NAS**
  - Why needed here: The entire paper frames latency prediction as a regression problem. You need to understand why NAS uses predictors (surrogates) instead of direct hardware profiling during search.
  - Quick check question: Why can't NAS just run each candidate on hardware during search?

- **Concept: Encoding schemes for architectures**
  - Why needed here: The core innovation is FCC encoding. You need baseline knowledge of one-hot, feature, and statistical encodings to appreciate what FCC improves.
  - Quick check question: What information is lost when using mean/std statistical encoding of block features?

## Architecture Onboarding

- **Component map:** User Inputs (sampling strategy, N_I, N_Step, encoding type, AccTH, N_Bins) -> Dataset Generation -> Sampling (random/balanced) -> Encoding (FCC/FC/statistical) -> Hardware execution -> Quality Assurance (reference models, <3% variance check) -> Predictor Training (MLP with 3 layers × 64 hidden units) -> Evaluation (per-bin accuracy vs AccTH) -> Extension Loop (weighted resampling for below-threshold bins)

- **Critical path:** Encoding choice (FCC) -> Sampling strategy (balanced for large spaces) -> Bin-wise evaluation. Errors in encoding cascade to all downstream accuracy.

- **Design tradeoffs:**
  - FCC vs Statistical: FCC is more accurate but requires defining feature combinations upfront; statistical is simpler but loses interaction information.
  - Random vs Balanced: Balanced converges faster but requires depth-binning logic; random is simpler but needs more samples.
  - N_I size: Larger N_I reduces extension iterations but increases upfront measurement cost.

- **Failure signatures:**
  - Accuracy plateaus despite increasing training data -> Likely encoding issue (overlapping representations in feature space)
  - High variance across repeated measurements (>3%) -> Hardware instability; need more inference runs or outlier removal
  - Specific bins consistently below threshold -> Under-sampling those depth ranges; switch to balanced sampling or increase w_1 weight

- **First 3 experiments:**
  1. **Validate encoding on small subset:** Train MLP with FCC vs statistical encoding on 1,000 samples from a small architecture space (e.g., DenseNet). Verify FCC achieves higher accuracy.
  2. **Compare sampling strategies:** Run ESM with random vs balanced sampling on ResNet supernet with N_I=300, N_Step=100. Measure iterations to convergence.
  3. **Cross-device validation:** Train predictor on RTX 4090 data, test on Raspberry Pi 4. Assess whether FCC encoding generalizes across hardware (paper shows ~99% accuracy on Pi 4, but verify with your own setup).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the ESM framework and FCC encoding effectively predict non-latency hardware metrics, such as energy consumption and memory footprint?
- **Basis in paper:** [inferred] The abstract explicitly lists "energy consumption" as a key performance characteristic for HW-NAS, yet the experimental validation (Section III) and results focus exclusively on inference latency.
- **Why unresolved:** The paper does not demonstrate if the correlation between architecture features and energy/memory mirrors that of latency, or if the "train-evaluate-extend" loop converges efficiently for these different metrics.
- **What evidence would resolve it:** Experimental results showing prediction accuracy (e.g., percentage error) and convergence speed of the ESM framework when trained to predict energy consumption or peak memory usage on the target devices.

### Open Question 2
- **Question:** Is the Feature Combination Count (FCC) encoding effective for search spaces with variable graph topologies, such as cell-based search spaces?
- **Basis in paper:** [inferred] The authors explicitly state in Section I.B that the framework targets "layer/block-wise architecture spaces built on a fixed macro-architecture."
- **Why unresolved:** The FCC encoding relies on counting feature combinations within a defined unit structure; it is unclear if this representation can capture the complex topological connectivity (e.g., skip connections, graph structures) inherent in non-fixed search spaces.
- **What evidence would resolve it:** A study applying the FCC encoding to a cell-based search space (e.g., NASBench-101 or 201) and comparing its accuracy against graph-based encoding methods like GCNs.

### Open Question 3
- **Question:** Does the FCC encoding maintain high prediction accuracy on specialized hardware accelerators (e.g., TPUs, NPUs) where operator fusion and tiling strategies differ significantly from general-purpose CPUs/GPUs?
- **Basis in paper:** [inferred] The evaluation covers CPUs, GPUs, and Raspberry Pi, but the introduction notes the diverse set of "specialized hardware accelerators" used for DNN deployment.
- **Why unresolved:** Specialized accelerators often exhibit latency behaviors that are highly dependent on layer fusion and memory layout, which might not be fully captured by feature combination counts alone.
- **What evidence would resolve it:** Benchmark results of the ESM framework on a specialized accelerator (e.g., Google TPU or Edge TPU) to verify if the accuracy remains superior to lookup-table or statistical approaches.

## Limitations

- FCC encoding specification lacks precise feature combination definitions, making exact reproduction challenging
- Bin boundaries and N_Bins configuration details are not explicitly provided for each architecture type
- Cross-device generalization claims (especially for resource-constrained devices like Raspberry Pi 4) would benefit from broader hardware validation

## Confidence

- **High Confidence:** FCC encoding improves accuracy over statistical methods (supported by extensive experimental data across 5 device configurations)
- **Medium Confidence:** Balanced sampling strategy provides 8x efficiency gain (robust for large spaces like ResNet, but untested on smaller architecture spaces)
- **Medium Confidence:** Train-evaluate-extend approach reliably converges to accuracy thresholds (demonstrated across multiple architectures, but extension dynamics not fully characterized)

## Next Checks

1. **Encoding Validation:** Implement FCC encoding and compare against statistical encoding on a small DenseNet subset (1,000 samples) to verify accuracy improvement
2. **Sampling Strategy Comparison:** Run ESM with random vs balanced sampling on ResNet supernet (N_I=300, N_Step=100) to measure iteration count to convergence
3. **Cross-Device Generalization:** Train predictor on RTX 4090, test on Raspberry Pi 4 (or other resource-constrained device) to validate hardware generalization claims