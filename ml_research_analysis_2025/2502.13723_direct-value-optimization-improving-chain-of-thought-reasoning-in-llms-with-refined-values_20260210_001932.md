---
ver: rpa2
title: 'Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with
  Refined Values'
arxiv_id: '2502.13723'
source_url: https://arxiv.org/abs/2502.13723
tags:
- arxiv
- reasoning
- value
- preprint
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Direct Value Optimization (DVO), a reinforcement
  learning framework that enhances large language models' reasoning abilities by optimizing
  at the individual step level using value signals rather than preference labels.
  DVO estimates stepwise values using Monte Carlo Tree Search or an outcome value
  model, then aligns the policy model with these values using mean squared error loss,
  eliminating the need for labor-intensive human annotations.
---

# Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values

## Quick Facts
- arXiv ID: 2502.13723
- Source URL: https://arxiv.org/abs/2502.13723
- Reference count: 24
- Key outcome: DVO improves Llama3-8B-Instruct accuracy from 74.6% to 80.6% on GSM8K, 22.5% to 26.5% on MATH, and 23.5% to 27.9% on AGIEval-Math using stepwise value optimization instead of preference labels.

## Executive Summary
Direct Value Optimization (DVO) is a reinforcement learning framework that enhances large language models' reasoning abilities by optimizing individual reasoning steps using value signals rather than preference labels. The method treats the policy model as a soft Q-function and aligns it with estimated step values using mean squared error loss, eliminating the need for labor-intensive human annotations. Experiments demonstrate consistent improvements over existing offline preference optimization techniques on mathematical and commonsense reasoning tasks.

## Method Summary
DVO optimizes LLMs by treating log-probabilities of reasoning steps as Q-values and minimizing MSE between these predictions and target values estimated via Monte Carlo Tree Search or an outcome value model. The method operates at the step level (blocks of text separated by "\n\n") rather than the response level, providing granular credit assignment. Target values are computed from the expected future reward of reasoning paths, and the model is trained to match these values while maintaining a KL-regularization term against the reference model.

## Key Results
- DVO consistently outperforms existing offline preference optimization techniques on mathematical reasoning benchmarks
- Three rounds of DVO training improve Llama3-8B-Instruct accuracy on GSM8K from 74.6% to 80.6%
- MCTS-based value estimation achieves better performance than outcome value models while avoiding computational overhead of training separate networks
- Unlike DPO, DVO maintains increasing implicit rewards for correct solutions throughout training

## Why This Works (Mechanism)

### Mechanism 1: Soft Q-Learning via MSE Regression
The method treats the policy as an optimal soft Q-function where log-probabilities correspond to accumulated rewards, then minimizes MSE between predicted and target values. This provides denser supervision than binary classification and directly optimizes the absolute value of responses rather than margins.

### Mechanism 2: Preservation of Positive Probability Mass
By optimizing absolute values rather than preference margins, DVO encourages maintaining or increasing the probability of correct reasoning paths. This mitigates the "probability degradation" issue common in DPO where correct answer likelihoods drop as models learn to separate good from bad.

### Mechanism 3: Monte Carlo Tree Search Value Bootstrapping
MCTS estimates intermediate step values by rolling out multiple reasoning paths and assigning values based on eventual success or failure. This enables learning from outcome-only rewards without per-step human labels, providing accurate credit assignment for chain-of-thought reasoning.

## Foundational Learning

- **Concept:** Maximum Entropy Reinforcement Learning (Soft Q-Learning)
  - **Why needed here:** DVO derives from maximizing both reward and entropy, making the temperature parameter β critical for balancing exploitation and exploration
  - **Quick check question:** How does the β parameter in the DVO loss function affect the sensitivity of the model to the value target?

- **Concept:** Step-wise Markov Decision Process (MDP)
  - **Why needed here:** DVO operates on "steps" rather than tokens or full responses, requiring definition of states and actions
  - **Quick check question:** In this paper, is the "action" defined as a single token or a block of text ending in `\n\n`?

- **Concept:** Credit Assignment in Chain-of-Thought
  - **Why needed here:** DVO's core value is distinguishing utility of intermediate steps versus final outcomes
  - **Quick check question:** How does DVO handle a correct reasoning step that leads to a wrong final answer due to a later error?

## Architecture Onboarding

- **Component map:** Policy Model ($\pi_\theta$) -> MCTS Engine -> Value Targets -> MSE Loss -> Policy Update
- **Critical path:** 1) Format backbone with step-by-step reasoning via few-shot fine-tuning 2) Generate MCTS rollouts to collect trajectories and step-values 3) Compute target values and optimize MSE loss with β=0.1
- **Design tradeoffs:** MCTS provides superior value estimation but is computationally expensive at data generation time; value models are faster but less accurate
- **Failure signatures:** Degraded performance on simple tasks with insufficient MCTS iterations; significant accuracy drops with β values too high or low
- **First 3 experiments:** 1) Beta scan with values [0.05, 0.1, 0.5] to find stable region 2) MCTS iteration ablation with [20, 40, 80] to verify scaling with accuracy 3) Compare step-level vs response-level DVO to confirm fine-grained credit assignment utility

## Open Questions the Paper Calls Out

- **Open Question 1:** How does DVO performance change when transitioned from offline to online settings where MCTS search and policy updates are integrated?
- **Open Question 2:** What gains in reasoning accuracy are achieved when MCTS search width increases beyond the experimentally constrained limit of 5?
- **Open Question 3:** Can an outcome value model be refined to match MCTS accuracy without incurring high computational overhead?

## Limitations

- Current implementation confined to offline settings, limiting integration with real-time reasoning applications
- Computational constraints restricted MCTS search width to 5, potentially limiting performance gains
- Performance gains on simpler tasks (GSM8K) require higher precision in value estimation, making them more sensitive to noise

## Confidence

**High Confidence:** Core mechanism of MSE loss for step-level value optimization is clearly described and mathematically sound; experimental setup and benchmark selection are transparent
**Medium Confidence:** Comparative advantages over DPO demonstrated, but exact mechanisms preventing probability degradation need further validation; MCTS vs value model performance depends on computational resources
**Low Confidence:** Step-level credit assignment in failure cases not explicitly addressed; scalability to larger reasoning tasks unverified; general applicability beyond mathematical reasoning suggested but not demonstrated

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary β across [0.05, 0.1, 0.5] and MCTS iterations across [20, 40, 80] on held-out validation set to confirm optimal values and identify failure thresholds

2. **Failure Case Attribution:** Design controlled experiments where correct intermediate steps lead to incorrect final answers due to later errors; analyze whether DVO correctly assigns credit to early steps

3. **Computational Cost-Benefit Analysis:** Compare wall-clock training time and inference latency between MCTS-based value estimation and learned value model approaches across different task complexities