---
ver: rpa2
title: Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection
  Improvement
arxiv_id: '2504.09058'
source_url: https://arxiv.org/abs/2504.09058
tags:
- reasoning
- answer
- reflection
- mcts
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SKROP, a framework that uses Monte Carlo Tree
  Search (MCTS) to generate step-level supervision for knowledge-intensive reasoning
  tasks, especially in the legal domain. To address challenges in applying MCTS to
  domain-specific problems, it introduces XML-structured thought representations,
  a random proposal mechanism to enhance node diversity, and a two-stage warmup process
  for better XML parsing.
---

# Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement

## Quick Facts
- arXiv ID: 2504.09058
- Source URL: https://arxiv.org/abs/2504.09058
- Reference count: 23
- Primary result: SKROP uses MCTS with step-level supervision to improve legal-domain reasoning; PORP further improves reflection quality.

## Executive Summary
This paper introduces SKROP, a framework that uses Monte Carlo Tree Search (MCTS) to generate step-level supervision for knowledge-intensive reasoning tasks, especially in the legal domain. To address challenges in applying MCTS to domain-specific problems, it introduces XML-structured thought representations, a random proposal mechanism to enhance node diversity, and a two-stage warmup process for better XML parsing. Additionally, PORP is introduced to improve self-reflection in reasoning by guiding the model to generate better reflections when it recognizes mistakes. Experiments on legal-domain datasets show that SKROP outperforms baseline methods, with PORP further enhancing reflection quality and reasoning accuracy. Step-level supervision proves more effective than solution-level supervision, and the approach benefits tasks requiring deep legal reasoning and analysis.

## Method Summary
SKROP combines Monte Carlo Tree Search with preference optimization to generate step-level supervision for legal-domain reasoning. It uses XML-structured thought representations to maintain reasoning traces, a two-stage warmup process to stabilize XML parsing, and a random proposal mechanism to enhance node diversity. MCTS explores multiple reasoning paths, assigns Q-values to each node, and generates preference pairs for training. PORP builds on this by learning preferences over reflection paths, guiding the model to produce better reflections when it recognizes mistakes. The framework is evaluated on multiple-choice legal QA datasets, showing improved reasoning accuracy over baselines.

## Key Results
- SKROP outperforms baseline methods on legal-domain datasets.
- Step-level supervision is more effective than solution-level supervision.
- PORP improves reflection quality and reasoning accuracy.
- Two-stage XML warmup significantly improves parse success and accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-level supervision via MCTS improves domain-specific reasoning more than solution-level supervision.
- Mechanism: MCTS explores multiple reasoning paths, assigns Q-values to each node based on terminal rewards, and generates fine-grained preference pairs between chosen and rejected steps. This creates denser training signal than outcome-only labels.
- Core assumption: The value head can approximate reliable step-level credit assignment before search is exhaustive; Q-value margins reflect true quality differences.
- Evidence anchors:
  - [abstract]: "Empirical results demonstrate the effectiveness on various legal-domain problems... Step-level supervision proves more effective than solution-level supervision."
  - [Section 5.2, Figure 4]: Step-level supervision outperforms solution-level on most datasets, with margins up to 9.32 on NJE.
  - [corpus]: Related work (SVPO, CPL) similarly shows step-level value guidance benefits reasoning, though mostly in math/coding domains.
- Break condition: If the retriever returns irrelevant articles or the policy model collapses to short outputs, Q-value estimates become unreliable and training signal degrades.

### Mechanism 2
- Claim: Two-stage warmup enables stable XML parsing and reduces early search failures.
- Mechanism: Stage 1 trains only `<STEP>`, `<PROPOSAL>`, `<FINAL_ANSWER>` (answer prediction). Stage 2 adds `<THOUGHT>`, `<ACTION>`, `<OBSERVATION>`. This curriculum lets the model first learn structure before complex reasoning patterns.
- Core assumption: The model can transfer structural knowledge from Stage 1 to Stage 2 without catastrophic forgetting.
- Evidence anchors:
  - [Section 3.3]: "Our experiments illustrate that this warmup approach significantly outperforms the one-way initialization gathering all tags."
  - [Appendix B, Table 5]: Warmup achieves 45.79 average accuracy vs. 42.02–44.10 without warmup across rounds.
  - [corpus]: No direct corpus evidence for two-stage XML warmup; this appears novel to this work.
- Break condition: If training data lacks sufficient XML-tagged examples, or if Stage 1 overfits to simple answer prediction, Stage 2 may fail to generalize.

### Mechanism 3
- Claim: PORP improves self-reflection by learning preferences over reflection paths.
- Mechanism: After MCTS generates correct/incorrect trajectories, wrong path prefixes are merged with precedent steps to simulate error states. A general LLM generates reflection text bridging wrong to correct paths. Preference optimization then increases likelihood of preferred reflections over less appropriate ones.
- Core assumption: The reflection text quality is sufficient to guide correction; the policy model's distribution is close enough to benefit from preference tuning.
- Evidence anchors:
  - [Section 4]: "PORP aims to guide the policy model in mastering the skill of optimal reflection upon encountering missteps."
  - [Table 1]: PORP achieves 65.84 avg accuracy on Qwen, outperforming Journey-Learning (64.80) and other baselines.
  - [corpus]: Related work on self-reflection (Instruct-of-Reflection) notes intrinsic self-correction can fail without external guidance, supporting PORP's preference-based approach.
- Break condition: If reflections degenerate to simple reasoning without corrective insight, or if the model over-fits to specific error patterns, generalization suffers.

## Foundational Learning

- Concept: **Monte Carlo Tree Search (PUCT variant)**
  - Why needed here: SKROP uses MCTS to build reasoning trees and assign step-level values. Understanding selection (PUCT), expansion, and backpropagation is essential.
  - Quick check question: Can you explain how PUCT balances exploration (visit count) vs. exploitation (Q-value and policy probability)?

- Concept: **Preference Optimization (DPO-style)**
  - Why needed here: Both SKROP and PORP use DPO-like losses to increase likelihood of chosen over rejected paths.
  - Quick check question: Why does DPO require a reference model, and what happens if the policy diverges too far from it?

- Concept: **Value Heads on LLMs**
  - Why needed here: A scalar value head predicts Q-values per step; training uses MSE with margin to prevent overfitting to noisy estimates.
  - Quick check question: How would you regularize a value head to avoid overconfidence on sparsely explored nodes?

## Architecture Onboarding

- Component map:
  - Policy Model π -> XML-tagged reasoning steps
  - Value Head φ -> Q-value predictions
  - Retriever -> Legal articles (top-K=3)
  - MCTS Engine -> Tree search with PUCT
  - Data Sampler -> Preference pairs
  - PORP Module -> Reflection pairs

- Critical path:
  1. Two-stage warmup on XML tags (no reasoning initially).
  2. Run MCTS on training questions to build trees.
  3. Sample preference pairs (Algorithm 1).
  4. Train policy + value head with combined loss (Eq. 15).
  5. Optionally, run PORP for reflection pairs and iterate.

- Design tradeoffs:
  - **Step-level vs. solution-level supervision**: Step-level is more effective but requires more careful sampling and value estimation.
  - **Simulation depth vs. compute**: Deeper trees improve Q-value accuracy but increase cost; paper uses 40–80 simulations.
  - **Reflection length weighting**: PORP includes length in sampling to avoid short, uninformative reflections (Appendix G).

- Failure signatures:
  - **Non-thought answers**: Model skips `<THOUGHT>` after retrieval (Appendix F). Mitigate by filtering in sampling (Line 5, Algorithm 1).
  - **BLEU collapse**: Similar actions merged if BLEU-4 > 0.7 (Eq. 5); if too aggressive, diversity drops.
  - **Warmup bypass**: Skipping warmup causes parse errors; Table 5 shows 3+ point accuracy drop.

- First 3 experiments:
  1. **Warmup ablation**: Train with and without two-stage warmup; compare XML parse success rate and early-round accuracy.
  2. **Supervision granularity**: Compare step-level vs. solution-level preference pairs on a held-out legal QA set; measure accuracy gap.
  3. **PORP vs. Journey-Learning**: Run both reflection methods on the same MCTS-generated trees; compare reflection text quality and downstream accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can SKROP and PORP be effectively adapted to other knowledge-intensive domains (e.g., medicine or finance) that lack the structural consistency of legal reasoning?
  - Basis in paper: [explicit] The authors state in the Limitations section that they "omit empirical studies for other domains due to the absence of a comprehensive knowledge base," identifying this as an "intriguing avenue for future research."
  - Why unresolved: The current experiments are confined to the legal domain, and the framework relies on domain-specific retrievers and XML structures that may not transfer directly without domain expertise.
  - What evidence would resolve it: Successful application and evaluation of SKROP on non-legal benchmarks (e.g., medical licensing exams) using corresponding domain corpora.

- **Open Question 2**: Under what specific conditions does CoT supervision degrade performance compared to solution-level supervision?
  - Basis in paper: [inferred] Appendix D reports that "CoTs are not always beneficial," noting that "CoT + ANS" methods performed suboptimally compared to answer-only baselines, likely due to noise in the generated thoughts.
  - Why unresolved: The paper demonstrates the success of *stepwise* MCTS supervision but does not fully analyze the failure modes of standard CoT fine-tuning on domain knowledge tasks.
  - What evidence would resolve it: An ablation study analyzing the noise sensitivity of CoT supervision versus answer-only supervision across varying degrees of data quality.

- **Open Question 3**: How can the framework prevent the policy model from defaulting to "non-thought answers" without requiring computationally expensive increases in simulation depth?
  - Basis in paper: [inferred] Appendix F identifies that models tend to output final answers immediately after retrieval because these "non-thought" paths are shorter and explored earlier within "limited simulation times."
  - Why unresolved: The current fix relies on a heuristic restriction (removing non-thought answers from training data), which addresses the symptom rather than the underlying search efficiency bias.
  - What evidence would resolve it: A modified search strategy or reward function that inherently prioritizes complete reasoning chains over short, direct answers during the simulation phase.

## Limitations
- The exact XML parsing accuracy of the base LLM after Stage 1 warmup is not reported; poor parse rates would undermine later MCTS quality.
- The reflection generation mechanism depends on a "general LLM" not described in detail; reflection quality may vary significantly with the chosen model.
- The value head's Q-value estimation reliability for early/mid-depth nodes is assumed but not empirically validated; incorrect values could propagate poor step-level supervision.

## Confidence

- **High**: SKROP's step-level supervision outperforms solution-level on legal QA tasks; two-stage XML warmup improves accuracy; PORP improves reflection quality over Journey-Learning baseline.
- **Medium**: The proposed XML-structured thought format enables stable MCTS exploration in domain-specific tasks; random proposal effectively increases node diversity.
- **Low**: The specific reflection generation process (LLM-in-the-loop) reliably produces corrective guidance; PORP's reflection weighting scheme generalizes beyond the legal domain.

## Next Checks
1. **Warmup Parse Rate**: Measure the percentage of questions successfully parsed after Stage 1 vs. Stage 2 to confirm the necessity of two-stage training.
2. **Value Estimation Calibration**: Plot Q-value distributions across MCTS depths; verify that estimated values align with final accuracy outcomes.
3. **Reflection Quality Audit**: Sample generated reflections from PORP and Journey-Learning; annotate for corrective insight and compare manually to ground-truth reasoning paths.