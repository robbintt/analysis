---
ver: rpa2
title: Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics
  Competition Problems
arxiv_id: '2512.24505'
source_url: https://arxiv.org/abs/2512.24505
tags:
- step
- reasoning
- line
- equation
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the reasoning abilities of three large language\
  \ models (LLMs)\u2014GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3\u2014on underrepresented\
  \ mathematics competition problems from the Missouri Collegiate Mathematics Competition.\
  \ Using a dataset of 30 problems across Calculus, Analytic Geometry, and Discrete\
  \ Mathematics, the models were assessed for both correct final answers and correct\
  \ reasoning."
---

# Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems

## Quick Facts
- **arXiv ID:** 2512.24505
- **Source URL:** https://arxiv.org/abs/2512.24505
- **Reference count:** 40
- **Primary result:** DeepSeek-V3 achieved the highest reasoning accuracy (90% in Discrete Math) on underrepresented mathematics competition problems, while all models struggled with Geometry.

## Executive Summary
This study evaluates three large language models (LLMs)—GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3—on 30 mathematics competition problems from the Missouri Collegiate Mathematics Competition across Calculus, Analytic Geometry, and Discrete Mathematics. Using an underrepresented dataset claimed to minimize training contamination, the models were assessed for both final answer correctness and reasoning quality. DeepSeek-V3 demonstrated superior performance, particularly in Discrete Mathematics (90% correct reasoning), while all models showed notably weak performance in Geometry. The study reveals distinct error patterns: DeepSeek-V3 primarily makes computational/logical errors, GPT-4o-mini frequently exhibits logical and approach-related errors, and Gemini-2.0-Flash tends toward incomplete reasoning and rushed conclusions.

## Method Summary
The study evaluated three LLMs on 30 Missouri Collegiate Mathematics Competition problems (10 each in Calculus, Analytic Geometry, and Discrete Mathematics) using identical text prompts with instructions to solve analytically. Responses were classified into four outcome types based on final answer and reasoning correctness, with incorrect reasoning further categorized into 11 specific error types. The evaluation compared model outputs against known correct solutions, focusing on both accuracy and reasoning quality while using a dataset claimed to be underrepresented in LLM training data.

## Key Results
- DeepSeek-V3 achieved the highest reasoning accuracy across all categories, particularly excelling in Discrete Mathematics (90% correct reasoning)
- All three LLMs exhibited notably weak performance in Geometry problems (0-80% correct reasoning)
- Distinct error patterns emerged: DeepSeek-V3 primarily makes computational/logical errors, GPT-4o-mini frequently exhibits logical and approach-related errors, and Gemini-2.0-Flash tends toward incomplete reasoning and rushed conclusions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluating LLMs on underrepresented datasets isolates reasoning capability from training data memorization
- Mechanism: Standard benchmarks suffer from "dataset contamination" where test examples appear in training corpora, inflating scores. Using the Missouri Collegiate Mathematics Competition (MCMC), which is claimed to be "relatively unknown," forces models to generalize rather than retrieve specific solutions
- Core assumption: The MCMC dataset has not been widely included in the pre-training or fine-tuning data of the evaluated models
- Evidence anchors:
  - [abstract] "The majority of these studies use the same datasets for benchmarking... limits the generalizability... may not fully capture the diverse challenges"
  - [materials_and_methods] "MCMC datasets are relatively unknown and, to the best of our knowledge, have not been previously used to evaluate LLMs capabilities... significantly reduces the likelihood of dataset contamination"
  - [corpus] FrontierMath (Neighbor 2210) supports the need for original, challenging problems to avoid contamination

### Mechanism 2
- Claim: Distinct fine-tuning objectives and architectural inductive biases cause model-specific error patterns in mathematical reasoning
- Mechanism: The study reveals that models optimized for efficiency (Gemini-2.0-Flash) tend to truncate reasoning ("rushed conclusions"), while models tuned for technical tasks (DeepSeek-V3) may err in calculation but maintain logical structure. GPT-4o-mini shows higher rates of "approach-related errors," suggesting a struggle in strategy selection
- Core assumption: The generation parameters were consistent across models, and the error classification taxonomy was applied consistently
- Evidence anchors:
  - [results] "DeepSeek-V3's errors were mostly computational or logical... GPT-4o-mini frequently exhibited logical and approach-related errors... Gemini... struggled with incomplete reasoning"
  - [table_3] Shows detailed error type percentages (e.g., Gemini-2.0-Flash had 33% "Rushed to Conclusion" errors in Geometry)
  - [corpus] CipherBank (Neighbor 28822) and other reasoning benchmarks highlight that reasoning capabilities vary significantly by domain

### Mechanism 3
- Claim: Text-only LLMs face a structural disadvantage in Analytic Geometry due to the necessity of implicit spatial visualization
- Mechanism: Geometry problems often require parsing text descriptions into spatial relationships or interpreting diagrams (omitted in this text-only study). The universal drop in Geometry performance suggests a lack of "visual grounding" or a module to map algebraic constraints to geometric shapes
- Core assumption: The models were not provided with images or diagrams, forcing them to rely solely on linguistic representations of spatial problems
- Evidence anchors:
  - [abstract] "All three LLMs exhibited notably weak performance in Geometry"
  - [results] "All three LLMs exhibited notably weak performance in Geometry... highlighting the limitations... in visualizing abstract geometric figures"
  - [corpus] GeoLaux (Neighbor 48289) explicitly evaluates Multimodal LLMs (MLLMs) on geometry, implying that text-only processing is insufficient

## Foundational Learning

- **Concept: Data Contamination / Memorization**
  - Why needed here: To understand why a model might score 90% on a benchmark without possessing actual reasoning skills
  - Quick check question: Can the model solve the problem if the variable names and constants are shifted in a way that breaks exact string matching with training data?

- **Concept: Reasoning vs. Computation**
  - Why needed here: The paper distinguishes between getting the logic right but the math wrong (Computational Error) vs. applying the wrong theorem (Logical/Approach Error)
  - Quick check question: Did the model set up the integral correctly but integrate incorrectly, or did it try to take a derivative when the problem asked for an integral?

- **Concept: Chain-of-Thought (CoT) Fidelity**
  - Why needed here: To diagnose "Rushed to Conclusion" errors, where the model outputs a final answer without sufficient intermediate steps
  - Quick check question: Does the output trace every step required to bridge the givens to the conclusion, or does it jump from premise to result?

## Architecture Onboarding

- **Component map:**
  - MCMC Problems -> Prompt Engineering -> LLM Generation -> Human/Automated Classification -> Error Analysis

- **Critical path:**
  1. Prompt engineering (ensuring "solve analytically" instruction is clear)
  2. Execution of generation
  3. Evaluation Bottleneck: Human or automated classification of "Logical" vs "Computational" errors is the hardest step to automate

- **Design tradeoffs:**
  - **Sample Size vs. Depth:** The study uses only 30 problems (10 per category). This allows for deep qualitative error analysis but lowers statistical confidence
  - **Model Selection:** Testing freely accessible models rather than flagship versions provides utility for the general public but yields lower overall performance ceilings

- **Failure signatures:**
  - **Gemini-2.0-Flash:** "Rushed Conclusion" — terminates generation early without full derivation
  - **GPT-4o-mini:** "Approach Error" — selects an invalid method or theorem early in the step sequence
  - **DeepSeek-V3:** "Computational Error" — correct logic path, but arithmetic failure

- **First 3 experiments:**
  1. **Visual Ablation Test:** Run the same Geometry problems on Multimodal versions with diagrams included to quantify the "visual deficit" gap
  2. **CoT Injection:** Force Gemini-2.0-Flash to use a specific "step-by-step" prompt template to see if "Rushed to Conclusion" rates drop
  3. **Contamination Probe:** Synthesize "mutated" versions of the MCMC problems (changing constants/contexts) and re-test DeepSeek-V3 to verify if its high performance holds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specific prompt engineering or architectural interventions mitigate the distinct error patterns (e.g., Gemini's incomplete reasoning vs. GPT's logical errors) identified in underrepresented mathematical tasks?
- **Basis in paper:** [explicit] The Conclusion states, "Future research should focus on refining reasoning processes, reducing errors, and expanding the models' understanding of the process of solving multi-step problems"
- **Why unresolved:** The study quantifies the errors but does not propose or test methods to correct these specific reasoning failures in different model architectures
- **What evidence would resolve it:** A follow-up study applying targeted strategies, such as chain-of-thought prompting for logical errors or constraint checking for incomplete reasoning, showing a statistically significant reduction in these specific error types

### Open Question 2
- **Question:** Do proprietary or "flagship" models (e.g., GPT-4o, Gemini Ultra) overcome the specific "weak performance in Geometry" observed in the cost-efficient models tested?
- **Basis in paper:** [explicit] The Introduction notes previous studies often focus on flagship models, while this study focused on accessible ones; the Conclusion suggests "more advanced variants" should be tested to see if they resolve the geometry struggles
- **Why unresolved:** The paper only evaluated GPT-4o-mini and Gemini-2.0-flash, leaving the performance of larger models on this specific underrepresented dataset unknown
- **What evidence would resolve it:** Comparative benchmarks of flagship models on the same MCMC Geometry subset, analyzed using the paper's defined error taxonomy

### Open Question 3
- **Question:** How does the distribution of reasoning errors shift when expanding the evaluation to include mathematical proof problems?
- **Basis in paper:** [inferred] The "Materials and Methods" section states, "problems requiring mathematical proof were not included in this study," leaving the models' ability to handle the strict logical structure of proofs untested
- **Why unresolved:** The current dataset relies on problems with definitive final answers, potentially masking reasoning deficiencies that would be evident in proof construction
- **What evidence would resolve it:** An extension of the current dataset to include MCMC proof problems, evaluated for logical validity rather than just final answer correctness

## Limitations
- Small sample size (30 problems total, 10 per category) constrains statistical power and generalizability
- Claims about MCMC dataset avoiding contamination rely on author assertion without systematic verification
- Error classification depends on human judgment of reasoning quality, introducing potential subjectivity
- Exclusively uses text-based problems without diagrams, potentially underestimating model capabilities on visual geometry tasks

## Confidence
- **High Confidence:** DeepSeek-V3 outperforming other models on reasoning accuracy (supported by specific numerical results across multiple categories)
- **Medium Confidence:** Model-specific error patterns (Logical vs Computational errors), though classification methodology could influence results
- **Medium Confidence:** Geometry being the most challenging category for all models, but text-only format may artificially depress scores
- **Low Confidence:** Claims about MCMC dataset avoiding contamination without systematic verification

## Next Checks
1. **Contamination Verification:** Systematically search model training corpora (CommonCrawl dumps, arXiv dumps) for MCMC problem text to verify the "unrepresented" dataset claim and quantify contamination risk
2. **Prompt Engineering Sensitivity:** Test how different prompt formulations (e.g., "show all work," "explain reasoning step-by-step") affect each model's error patterns, particularly addressing Gemini's "rushed conclusions" issue
3. **Multimodal Geometry Extension:** Re-run Geometry problems with multimodal models (GPT-4V, Gemini Pro Vision) using the original diagrams to quantify the performance gap between text-only and multimodal processing in spatial reasoning tasks