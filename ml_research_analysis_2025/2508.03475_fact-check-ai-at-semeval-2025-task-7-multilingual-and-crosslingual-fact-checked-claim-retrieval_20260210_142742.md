---
ver: rpa2
title: 'fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked
  Claim Retrieval'
arxiv_id: '2508.03475'
source_url: https://arxiv.org/abs/2508.03475
tags:
- training
- retrieval
- languages
- multilingual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multilingual and cross-lingual fact-checked
  claim retrieval for combating online disinformation. The method employs a bi-encoder
  model fine-tuned from pre-trained transformer encoders optimized for sentence similarity,
  using both source languages and English translations for multilingual retrieval
  and only English translations for cross-lingual retrieval.
---

# fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval

## Quick Facts
- arXiv ID: 2508.03475
- Source URL: https://arxiv.org/abs/2508.03475
- Reference count: 2
- Primary result: Achieved 92% Success@10 in multilingual and 80% Success@10 in cross-lingual fact-checked claim retrieval using lightweight models under 500M parameters

## Executive Summary
This paper addresses multilingual and cross-lingual fact-checked claim retrieval for combating online disinformation by employing bi-encoder models fine-tuned from pre-trained transformer encoders optimized for sentence similarity. The approach uses both source languages and English translations for multilingual retrieval and only English translations for cross-lingual retrieval, achieving competitive performance while using lightweight models under 500M parameters trained on constrained hardware. The system demonstrates that high performance is attainable even with limited computational resources through techniques like layer freezing and gradient checkpointing, ranking 10th and 5th respectively in the two tracks.

## Method Summary
The method approaches fact-checked claim retrieval as a learning-to-rank task using bi-encoder models fine-tuned from pre-trained transformer encoders optimized for sentence similarity. The system employs separate encoders for posts and fact-checks that generate dense vector representations, with relevance computed via temperature-scaled cosine similarity and optimized using MNR Loss combined with Cross-Entropy Loss. Training uses both source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval, with 5-fold ensembling further amplifying performance. All models maintain fewer than 500M parameters and are trained on Kaggle T4 GPUs using memory-efficient techniques including gradient checkpointing and selective layer freezing.

## Key Results
- Achieved 92% Success@10 in multilingual retrieval track
- Achieved 80% Success@10 in cross-lingual retrieval track
- Ranked 10th and 5th respectively in the two tracks
- Demonstrated competitive performance using models under 500M parameters trained on constrained hardware

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-encoder architectures with contrastive learning enable efficient retrieval of semantically similar claim-post pairs.
- Mechanism: Separate encoders for posts and fact-checks generate dense vector representations; relevance is computed via temperature-scaled cosine similarity and optimized with MNR Loss combined with Cross-Entropy Loss to maximize similarity for positive pairs.
- Core assumption: Semantic similarity in embedding space correlates with factual relevance between a social media post and a fact-checked claim.
- Evidence anchors: [abstract] "approached as a Learning-to-Rank task using a bi-encoder model fine-tuned from a pre-trained transformer optimized for sentence similarity" [section] "We use a Bi-encoder design... We then find the similarity between their embeddings... For training, we apply MNR Loss and Vanilla Cross-Entropy Loss"

### Mechanism 2
- Claim: Training on English translations of source languages improves cross-lingual retrieval through alignment in a shared semantic space.
- Mechanism: By translating all inputs to English, the model operates in a single language space, reducing cross-lingual transfer complexity. Ensemble methods across 5 folds trained on different data distributions further improve generalization.
- Core assumption: Translation quality preserves the semantic features necessary for claim matching, and the English embedding space captures cross-lingual semantics adequately.
- Evidence anchors: [abstract] "Training used both the source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval" [section] "For cross-lingual training, we re-used the same bi-encoder models but trained them on English-translated text... This process was further amplified using 5-fold ensembling"

### Mechanism 3
- Claim: Lightweight models (<500M parameters) with gradient checkpointing and layer freezing can achieve competitive retrieval performance on constrained hardware.
- Mechanism: Pre-trained sentence embedding models are fine-tuned with memory-efficient techniques—gradient checkpointing reduces memory during backprop, selective layer freezing limits trainable parameters.
- Core assumption: Pre-trained multilingual embeddings already encode sufficient semantic structure; fine-tuning primarily adapts the task-specific alignment rather than learning representations from scratch.
- Evidence anchors: [abstract] "Using lightweight models with fewer than 500M parameters and training on Kaggle T4 GPUs, the method achieved 92% Success@10" [section] "All models were less than 500M parameters, and training was done using only two Kaggle T4 GPUs showing that high performance is attainable even with constrained computational resources"

## Foundational Learning

- Concept: Bi-encoder vs Cross-encoder architectures
  - Why needed here: The system uses bi-encoders for scalability—both encoders process inputs independently, enabling pre-computation of fact-check embeddings and fast retrieval via similarity search.
  - Quick check question: Can you explain why bi-encoders are preferred over cross-encoders for large-scale retrieval tasks, and what accuracy-efficiency trade-off this introduces?

- Concept: Contrastive learning (MNR Loss / Multiple Negatives Ranking)
  - Why needed here: The training objective maximizes similarity for positive post-claim pairs while pushing apart negative pairs within each batch.
  - Quick check question: Given a batch of 24 post-claim pairs, how many negative samples does each positive pair implicitly compare against under MNR Loss?

- Concept: Temperature-scaled cosine similarity
  - Why needed here: The temperature parameter (T=0.05) sharpens the similarity distribution, making the model more discriminative in ranking.
  - Quick check question: What happens to the similarity score distribution when temperature is decreased from 1.0 to 0.05, and how does this affect gradient flow during training?

## Architecture Onboarding

- Component map:
  Input Pipeline: Preprocessing (URL/emoji removal, OCR noise filtering) → Tokenization
  Encoders: Shared-weight transformer backbone (multilingual-e5-large-instruct / stella-en-400M-v5)
  Pooling: Mean Pooling or Attention Pooling with BiLSTM → Fixed-size sentence embeddings
  Similarity: Temperature-scaled cosine similarity (T=0.05)
  Training: MNR Loss + symmetric Cross-Entropy, AdamW optimizer (LR: 5×10⁻⁶ transformer, 1×10⁻⁴ custom layers)
  Inference: Embedding generation → Semantic search → Top-10 retrieval

- Critical path:
  1. Pre-trained model selection (must support target languages; multilingual-e5 for source languages, stella/mxbai for English)
  2. Contrastive fine-tuning on task-specific post-claim pairs
  3. Pooling strategy selection (Mean vs Attention—paper shows both viable)
  4. Ensemble aggregation for cross-lingual track

- Design tradeoffs:
  - Mean Pooling vs Attention Pooling: Attention adds complexity (BiLSTM + learnable weights) but may capture token importance; paper doesn't report clear winner.
  - English-only vs Multilingual training: English translations simplify cross-lingual alignment but risk translation-induced errors; multilingual training preserves source semantics but requires more data per language.
  - Single model vs 5-fold ensemble: Ensemble improves robustness (+2.9% S@10 for cross-lingual) but increases inference cost 5×.

- Failure signatures:
  - Low S@10 on morphologically rich languages (Polish, Turkish): Pre-trained embeddings may lack sufficient representation quality.
  - High variance across languages (0.864–0.994 S@10 range): Indicates dataset imbalance or language-specific overfitting.
  - False positives on shared named entities: Surface similarity dominates when semantic differentiation is weak.
  - Assumption: Cross-lingual retrieval errors on idiomatic expressions suggest translation-based approach loses cultural nuance.

- First 3 experiments:
  1. Baseline reproduction: Fine-tune multilingual-e5-large-instruct on a single language (e.g., French) with Mean Pooling, MNR Loss, and the paper's hyperparameters (batch size 24, 20 epochs, LR 5×10⁻⁶). Validate S@10 against paper's reported 0.932.
  2. Pooling ablation: Compare Mean Pooling vs Attention Pooling (BiLSTM + attention) on the same language split. Measure both S@10 and training time/memory to quantify the efficiency-effectiveness trade-off.
  3. Cross-lingual translation quality test: Train on English translations vs source language for a high-resource language pair (e.g., Spanish-English). Analyze error cases where translation changes semantic content relevant to fact-checking.

## Open Questions the Paper Calls Out
None

## Limitations
- Translation-based approach may introduce semantic drift for idiomatic expressions and culturally specific references, potentially degrading cross-lingual retrieval quality.
- Lightweight model constraint (<500M parameters) may limit performance on morphologically rich languages where pre-trained embeddings have weaker representations.
- The paper lacks error analysis to quantify whether retrieval failures stem from translation quality versus model capacity or training data issues.

## Confidence
**High Confidence** (mechanisms with direct textual evidence):
- Bi-encoder architecture with MNR Loss and temperature-scaled cosine similarity for training and inference
- Model parameter constraints (<500M) and training on Kaggle T4 GPUs with memory optimization techniques
- Ensemble approach using 5-fold cross-validation for cross-lingual track

**Medium Confidence** (mechanisms with reasonable but indirect evidence):
- Translation quality preserving semantic features necessary for claim matching
- Pre-trained multilingual embeddings providing sufficient semantic structure for fine-tuning
- Attention Pooling strategy effectiveness compared to Mean Pooling

**Low Confidence** (mechanisms with minimal or no supporting evidence):
- Specific reasons for performance differences across languages (beyond parameter counts)
- Error rate distribution between translation-induced errors vs. model limitations
- Optimal temperature value selection (T=0.05) justification

## Next Checks
1. **Translation Quality Impact Analysis**: Systematically compare cross-lingual retrieval performance when using original source language text versus English translations for high-resource language pairs (e.g., Spanish-English). Measure the semantic drift introduced by translation by analyzing retrieval errors where the post and fact-check discuss the same topic but differ in factual claims.

2. **Language-Specific Model Capacity Test**: Fine-tune a larger multilingual model (>500M parameters) on morphologically rich languages (Polish, Turkish) while maintaining the same optimization constraints. Compare S@10 improvements against the lightweight models to quantify the capacity limitations for complex morphology.

3. **Error Type Classification**: Manually annotate a sample of retrieval failures to classify errors into categories: (a) translation-induced semantic drift, (b) insufficient model capacity for language-specific features, (c) false positives from shared named entities, and (d) other semantic mismatches. This analysis would reveal whether the translation approach or model architecture is the primary bottleneck.