---
ver: rpa2
title: 'Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation'
arxiv_id: '2511.17844'
source_url: https://arxiv.org/abs/2511.17844
tags:
- data
- backbone
- synthetic
- training
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data-efficient method for fine-tuning text-to-video
  diffusion models to enable precise control over physical camera parameters like
  shutter speed, aperture, and color temperature. The key innovation is a joint training
  strategy that uses simple synthetic data to avoid backbone corruption, employing
  a low-rank adapter for domain adaptation and a cross-attention adapter for control
  disentanglement.
---

# Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation

## Quick Facts
- arXiv ID: 2511.17844
- Source URL: https://arxiv.org/abs/2511.17844
- Reference count: 40
- Primary result: Simple synthetic data enables better controllable video generation than complex real data

## Executive Summary
This paper addresses the challenge of fine-tuning text-to-video diffusion models for precise control over physical camera parameters while minimizing data requirements and backbone degradation. The key insight is that synthetic data, despite being low-fidelity, can induce cleaner adaptation by avoiding entanglement with real-world scene complexity. The method employs a joint training strategy using a low-rank adapter for domain adaptation and a cross-attention adapter for control disentanglement, achieving high-quality controllable video generation with minimal synthetic data.

## Method Summary
The approach fine-tunes Wan 2.1 (14B DiT) using 150 synthetic samples per control type. It employs joint training of a backbone LoRA (rank-32, all blocks) and a cross-attention adapter (4 tokens, 256-dim, deepest 1/3 blocks). The conditional signal (scalar c ∈ [-1,1]) is projected through an MLP to generate cross-attention keys/values. At inference, shallow LoRA weights are pruned to restore backbone fidelity while retaining control capability in deeper blocks. Training uses AdamW with 2×10⁻⁵ learning rate and up to 1000 epochs.

## Key Results
- Synthetic data training achieves comparable or better semantic fidelity and video quality than photorealistic real data
- Joint training prevents "Bulldozer Effect" (adapter-only training) and maintains backbone fidelity
- Clean inference (pruning shallow LoRA) restores baseline performance while preserving learned controls
- Low-rank conditional signals (effective rank ≈ 1) indicate clean disentanglement of physical effects

## Why This Works (Mechanism)

### Mechanism 1
Low-fidelity synthetic data preserves backbone priors better than photorealistic data. Simple geometric scenes induce low spectral drift in backbone weights, producing fewer high-ranking singular vectors dissimilar to pre-trained weights.

### Mechanism 2
Factorized adaptation separates domain shift from control learning. Backbone LoRA absorbs the synthetic-to-natural domain gap, freeing the cross-attention adapter to learn only the physical effect in a low-dimensional subspace.

### Mechanism 3
Pruning shallow LoRA weights at inference restores backbone fidelity while preserving learned control. Deeper DiT blocks retain the conditional adapter where semantic abstraction is higher.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed - Core mechanism for parameter-efficient domain adaptation; enables joint training without modifying full backbone weights. Quick check - Explain why low-rank decomposition reduces catastrophic forgetting compared to full weight fine-tuning.

- **Cross-attention conditioning**: Why needed - Mechanism for injecting external signals into the denoising process. Quick check - How does cross-attention differ from self-attention, and why is it standard for controllable generation?

- **Catastrophic forgetting and spectral drift**: Why needed - Central problem this paper diagnoses and solves. Quick check - What does a high "intruder dimension" count in SVD analysis indicate about training data complexity?

## Architecture Onboarding

- **Component map**: T2V Backbone (Wan 2.1) -> Backbone LoRA (rank-32, all blocks) -> Conditional Cross-Attention Adapter (4 tokens, 256-dim, deepest 1/3 blocks) -> Condition MLP -> Gate parameter (fixed 0.5)

- **Critical path**: Scalar condition c → MLP_cond → 256-dim embedding → Adapter generates K_cond, V_cond → Cross-attention produces y_cond → Combined with y_text via gate → Inference: Prune LoRA from shallow 2/3 blocks

- **Design tradeoffs**: Synthetic vs. photorealistic data (realism vs. disentanglement); Clean vs. dirty inference (fidelity vs. simplicity); Adapter depth placement (spectral separation vs. convergence)

- **Failure signatures**: Bulldozer Effect (adapter-only training produces high-rank signal); Content collapse (model copies training textures); Semantic entanglement (control alters unintended attributes)

- **First 3 experiments**:
  1. Data complexity ablation: Train on single synthetic vs. photorealistic scene (7 conditions each). Monitor FEP drift rate and final SVP scores.
  2. Training strategy ablation: Joint training vs. adapter-only. Compute effective rank of conditional signal via SVD.
  3. Inference mode ablation: Clean vs. dirty inference across full control range. Measure SSF/SS-FD during training and X-CLIP/VQA at convergence.

## Open Questions the Paper Calls Out

- Can a unified model successfully disentangle multiple physical parameters from a single joint control vector?
- Does the "Less is More" hypothesis hold for structural or spatial controls, such as depth or pose estimation?
- Would catastrophic forgetting persist if real-world dataset were scaled to match pre-training data size?
- Is the pruning strategy effective across different architectural backbones, such as U-Nets?

## Limitations

- Evaluates only three physical controls on a single T2V backbone (Wan 2.1)
- Method's effectiveness for other controls or different architectures remains unverified
- Clean inference strategy assumes control signals localize to deeper blocks
- Paper doesn't investigate scalability with backbone size or control dimensionality

## Confidence

- **Data-Efficient Adaptation**: High - Multiple ablations consistently show synthetic data achieves comparable or better results than real data with fewer samples
- **Joint Training Strategy**: High - Spectral analysis and qualitative results demonstrate joint training prevents "Bulldozer Effect" and maintains backbone fidelity
- **"Less is More" Hypothesis**: High - Controlled experiments directly compare synthetic vs. real data training
- **Clean Inference Restoration**: Medium - Quantitative metrics show clean inference maintains baseline performance, but qualitative ablation studies are limited

## Next Checks

1. Train the same method on a different T2V backbone (e.g., SVD or Gen-2) with identical synthetic data. Compare FEP drift rates and SVP scores.

2. Extend the method to control two physical parameters simultaneously (e.g., shutter speed + aperture). Measure control disentanglement quality and whether low-rank adapter assumption holds.

3. Create hybrid training datasets combining synthetic and real data (e.g., 75% synthetic + 25% real). Compare adaptation quality to pure synthetic training to determine optimal data mixing ratios.