---
ver: rpa2
title: 'Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?'
arxiv_id: '2505.23749'
source_url: https://arxiv.org/abs/2505.23749
tags:
- distortion
- avgutil
- rlhf
- bound
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the distortion of AI alignment methods when
  optimizing for heterogeneous user preferences. It introduces a formal framework
  to quantify the loss in average utility when aligning models based on pairwise comparisons
  from diverse users.
---

# Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?

## Quick Facts
- arXiv ID: 2505.23749
- Source URL: https://arxiv.org/abs/2505.23749
- Reference count: 40
- Primary result: RLHF/DPO suffer at least (1-o(1))·β distortion without KL constraints and e^Ω(β) or unbounded distortion with KL constraints; NLHF achieves minimax optimal distortion of (1/2 + o(1))·β robustly across settings

## Executive Summary
This paper introduces a formal framework for quantifying alignment distortion—the worst-case ratio between optimal average utility and achieved utility—when optimizing AI models based on heterogeneous user preferences. The key insight is that methods like RLHF and DPO can suffer severe distortion because their reward-learning phases misrepresent preferences on rarely-observed pairs, while NLHF's Nash equilibrium formulation naturally hedges against this by considering worst-case opponent policies. The analysis shows NLHF achieves optimal distortion of β/2 under KL constraints, while RLHF and DPO can experience exponential or even unbounded distortion depending on sampling distributions.

## Method Summary
The paper formalizes AI alignment as a social choice problem where n users provide pairwise comparisons following individual Bradley-Terry models with temperature β. Methods are evaluated by their distortion—the worst-case ratio between the maximum achievable average utility and the utility of the learned policy within a KL constraint ball around a reference policy. RLHF/DPO use MLE to fit rewards from win-rates then optimize, while NLHF computes a Nash equilibrium in the margin game. The key technical contribution is showing how sigmoid linearization bounds enable tight distortion analysis for NLHF, while RLHF's susceptibility to sampling bias causes exponential distortion when the comparison pair distribution differs from the reference policy.

## Key Results
- NLHF achieves minimax optimal distortion of (β/2 + o(1))·β under KL constraints
- RLHF/DPO suffer at least (1-o(1))·β distortion without KL constraints
- RLHF can experience e^Ω(β) distortion or unbounded distortion under KL constraints when sampling distribution differs from reference policy
- The distortion lower bound of β/2 is tight and cannot be overcome with ordinal comparisons alone

## Why This Works (Mechanism)

### Mechanism 1: Distortion as Alignment Quality Metric
The paper introduces distortion as the worst-case ratio between optimal average utility and achieved utility. This adapts social choice theory to the alignment setting with KL constraints, quantifying utility loss when methods rely on ordinal comparisons rather than true cardinal utilities. Users provide pairwise comparisons following individual Bradley-Terry models, and the goal is maximizing average utility across the population.

### Mechanism 2: Sigmoid Linearization Enables Tight Bounds
The key technical insight is sandwiching the sigmoid function with affine bounds, converting nonlinear preference probabilities into tractable linear expressions in average utility. Lemma 1 shows expected win-rates can be bounded between β·(ℓ_β·AvgUtil(x) - L·AvgUtil(y)) + 1/2 and β·(L·AvgUtil(x) - ℓ_β·AvgUtil(y)) + 1/2, where L=1/4 and ℓ_β=(σ(β)-1/2)/β.

### Mechanism 3: NLHF's Nash Equilibrium Provides Automatic Hedging
NLHF achieves optimal distortion because computing a Nash equilibrium in the margin-matrix game implicitly hedges against all possible opponent policies, including the utility-maximizing benchmark. Since NLHF solves max_{π₁∈B_τ} min_{π₂∈B_τ} E[M_{x₁,x₂}], the resulting policy must achieve non-negative expected margin against any policy in the KL ball.

### Mechanism 4: RLHF's Exponential Distortion from MLE Misrepresentation
RLHF can suffer e^Ω(β) or unbounded distortion because the MLE reward-fitting phase sacrifices accuracy on rarely-observed pairs to fit frequently-observed ones. When the sampling distribution μ differs from the reference policy π_ref, this misrepresentation compounds, causing RLHF to select wrong alternatives when forced to choose between misrepresented options.

## Foundational Learning

- **Concept: Bradley-Terry Model for Pairwise Comparisons**
  - Why needed here: Core generative assumption for how users make probabilistic comparisons based on latent utilities
  - Quick check question: Can you explain why P(x≻y) = σ(β(u(x)-u(y))) and what the temperature β controls?

- **Concept: Nash Equilibrium in Zero-Sum Games**
  - Why needed here: NLHF's optimality stems from its game-theoretic formulation; understanding why Nash strategies hedge is essential
  - Quick check question: In a symmetric zero-sum game, why must the equilibrium value be zero?

- **Concept: KL Divergence Constraints vs. Regularization**
  - Why needed here: The paper shows equivalence between constrained and regularized formulations; practical implementations use regularization
  - Quick check question: Why does the equivalence in Appendix E.4 matter for interpreting the distortion bounds?

## Architecture Onboarding

- **Component map:** Preference data collection (users → d comparisons each) → Win-rate estimation (empirical win-rates) → Policy optimization (RLHF MLE+optimization or NLHF Nash game) → KL constraint/regularization (B_τ(π_ref))

- **Critical path:** Understanding why NLHF's Nash equilibrium naturally competes with the optimal policy π* while RLHF's MLE phase can misrepresent relative preferences

- **Design tradeoffs:**
  - Borda/RLHF: Simpler, but distortion O(β²) unconstrained, e^Ω(β) constrained
  - NLHF/Maximal Lotteries: Minimax optimal distortion β/2, but requires solving a game
  - Sampling distribution: μ matching π_ref may help RLHF; mismatch is catastrophic

- **Failure signatures:**
  1. RLHF selects low-utility alternative when μ≠π_ref and MLE misrepresents rarely-observed pairs
  2. Unbounded distortion with correlated pair sampling (Theorem 9) — relevant for adaptive leaderboards
  3. High variance in finite samples — check finite-sample bounds for required n

- **First 3 experiments:**
  1. Replicate the 3-alternative lower bound (Theorem 6): Verify MLE gives r(b)>r(a) despite AvgUtil(a)>>AvgUtil(b)
  2. Compare RLHF vs NLHF distortion on synthetic heterogeneous preferences: Vary β and KL budget, measure empirical distortion
  3. Test sensitivity to μ vs π_ref mismatch: Fix heterogeneous population, vary sampling distribution, plot distortion vs divergence

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the distortion of RLHF be bounded when the sampling distribution µ coincides with the reference policy πref? The paper's exponential lower bound exploits mismatch between µ and πref; this special case remains uncharacterized.

- **Open Question 2:** Do alternative aggregation rules (Maximal Lotteries, Copeland) provide more robust model rankings for AI leaderboards like Chatbot Arena? Current leaderboard methodology uses MLE-based Bradley-Terry, equivalent to Borda scoring with known distortion issues.

- **Open Question 3:** Can the distortion lower bound of (1/2 + o(1))·β be overcome with additional information beyond ordinal comparisons? The lower bound stems from fundamental non-identifiability of heterogeneous user utilities from pairwise comparisons alone.

- **Open Question 4:** How does distortion analysis extend to settings with generalization across states? The paper analytically treats each state independently, but real alignment requires generalizing preferences to unseen contexts.

## Limitations

- Analysis is purely theoretical without empirical validation on real-world datasets
- Framework assumes Bradley-Terry comparison models with bounded utilities and known KL constraints
- Distortion bounds depend critically on temperature parameter β being well-behaved and utilities normalized to [0,1]

## Confidence

- **High confidence:** Distortion as rigorous metric; NLHF's minimax optimality under KL constraints; exponential distortion lower bound for RLHF when μ ≠ π_ref
- **Medium confidence:** Practical significance of β/2 distortion bound for NLHF (depends on realistic β values); applicability of correlated sampling results to adaptive leaderboards
- **Low confidence:** Exact scaling behavior of DPO under KL constraints (stated as "at least β · Ω(1)" with open questions about tightness)

## Next Checks

1. Implement the 3-alternative construction from Theorem 6 to verify that MLE-based methods can achieve exponential distortion when comparison sampling differs from the reference policy

2. Run finite-sample experiments varying β, KL budget τ, and sampling distribution μ to empirically verify the O(√(log(m/δ)/(n·min{1,d·μ²_min}))) error bounds from Theorems 2 and 7

3. Test the correlated sampling scenario from Theorem 9 by simulating adaptive pair selection in a leaderboard setting, measuring whether distortion becomes unbounded as claimed