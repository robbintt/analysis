---
ver: rpa2
title: 'Forget and Explain: Transparent Verification of GNN Unlearning'
arxiv_id: '2512.07450'
source_url: https://arxiv.org/abs/2512.07450
tags:
- unlearning
- graph
- attribution
- post
- gnndelete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes an explainability-driven verifier for GNN\
  \ unlearning to provide transparent evidence of forgetting. The method uses five\
  \ metrics\u2014Residual Attribution, Heatmap Shift, Explainability Score Deviation,\
  \ Graph Edit Distance, and Graph Rule Shift\u2014to quantify attribution and structural\
  \ changes before and after deletion."
---

# Forget and Explain: Transparent Verification of GNN Unlearning

## Quick Facts
- arXiv ID: 2512.07450
- Source URL: https://arxiv.org/abs/2512.07450
- Reference count: 31
- Primary result: Proposes five explainability metrics (RA, HS, ESD, GED, GRS) to verify GNN unlearning; shows Retrain/GNNDelete achieve near-complete forgetting while GraphEditor leaves partial signals

## Executive Summary
This paper addresses the challenge of verifying GNN unlearning by proposing an explainability-driven framework. The method uses five metrics—Residual Attribution, Heatmap Shift, Explainability Score Deviation, Graph Edit Distance, and Graph Rule Shift—to quantify attribution and structural changes before and after deletion. Evaluated on five benchmarks using two backbones and four unlearning strategies, results show the framework can distinguish between complete and partial forgetting, providing human-readable evidence that complements privacy metrics.

## Method Summary
The verification framework captures pre-unlearning attribution heatmaps (vanilla saliency), k-hop proxy graphs, and symbolic rules (GraphChef), then compares them to post-unlearning snapshots. Five metrics quantify the difference: RA measures direct influence through forgotten nodes, HS/ESD capture attribution redistribution, GED quantifies structural changes in proxy graphs, and GRS counts disappeared rules. The approach is evaluated across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics) using GCN/GAT backbones and four unlearning methods (Retrain, GraphEditor, GNNDelete, IDEA).

## Key Results
- Retrain and GNNDelete achieve near-complete forgetting (RA≈0%, large GEDΔ)
- GraphEditor provides partial erasure (RA>0%, moderate GEDΔ)
- IDEA leaves residual signals (RApost>0%, GEDΔ≈0)
- Explanation deltas offer human-readable evidence of forgetting, complementing MI ROC-AUC privacy metrics

## Why This Works (Mechanism)

### Mechanism 1
Attribution deltas between pre- and post-unlearning snapshots provide localized, human-readable evidence of whether forgotten nodes still influence predictions. Gradient-based saliency maps are computed for all nodes before deletion and after unlearning. Residual Attribution quantifies the proportion of total attribution still flowing through forgotten nodes; Heatmap Shift and Explainability Score Deviation capture broader attribution redistribution. When RA drops to ~0% and HS/ESD show substantive pre→post shifts, this indicates the model's dependency on forgotten data has been disrupted.

### Mechanism 2
Structural proxy graphs (k-hop ego networks around forgotten nodes) provide a stable, localized fingerprint for detecting whether influence pathways through the neighborhood have been altered. For each node in the forget set, extract its k=2-hop ego-network; the union forms the local proxy graph. After unlearning, extract the same region to form the post-proxy graph. Graph Edit Distance computes the symmetric difference in edges, quantifying structural change. Large GEDΔ indicates influence routes through the neighborhood were disrupted; GEDΔ≈0 suggests minimal structural impact.

### Mechanism 3
Symbolic rule sets extracted from surrogate decision trees offer auxiliary, qualitative evidence of forgetting at the decision-logic level. GraphChef distills the GNN into a depth-3 decision tree surrogate; root-to-leaf paths form the rule set. After unlearning, extract the new rule set and count disappeared rules. Larger rule count reduction suggests the unlearning altered the model's decision logic, though surrogate variability limits interpretability.

## Foundational Learning

- **Message passing in GNNs (k-hop neighborhoods)**: Unlearning in GNNs is challenging because removing one node affects its k-hop neighborhood via message passing; understanding this propagation is essential to interpret why residual signals may persist. Quick check: If you delete node v from a 2-layer GCN, which other nodes' representations might change?

- **Gradient-based attribution/saliency maps**: The verifier relies on computing gradients w.r.t. cross-entropy loss to quantify node importance; understanding what saliency captures (and its limitations) is critical for interpreting verification metrics. Quick check: What does a high gradient magnitude at node v indicate about the model's dependence on v's features?

- **Machine unlearning paradigms (exact vs. approximate)**: The paper evaluates four unlearning strategies with different guarantees—Retrain (exact), GNNDelete/GraphEditor/IDEA (approximate); knowing the distinction helps interpret why verification outcomes differ. Quick check: Why does retraining from scratch guarantee complete forgetting, while approximate methods may leave residual signals?

## Architecture Onboarding

- **Component map**: Pre-unlearning snapshot module -> Unlearning module -> Post-unlearning verification module -> Complementary privacy check -> Output (human-readable evidence profile)

- **Critical path**: Pre-snapshot → Unlearning → Post-snapshot → Metric computation → Interpretation (RA≈0 + large GEDΔ + substantive HS/ESD → "complete forgetting"; non-zero RA + small GEDΔ → "partial/ineffective")

- **Design tradeoffs**: Vanilla saliency chosen for reproducibility but may miss nuanced attributions; k=2-hop proxy fixed for consistency but larger k captures more influence; GRS is diagnostic-only due to surrogate variability; MI ROC-AUC is a weak signal in these experiments

- **Failure signatures**: RApost > 0 for deletion-based methods → unlearning implementation bug; GEDΔ≈0 but RApost≈0 → node removed but neighborhood influence routes intact; HS/ESD show large shifts but MI AUC unchanged → verification detects residual memorization; GRS changes dramatically while other metrics are stable → surrogate instability

- **First 3 experiments**: 1) Run Retrain on Cora with 5% forget set; verify RApost=0%, GEDΔ is large; 2) Run GNNDelete on GCN vs. GAT; compare RA, HS, ESD, and GEDΔ; 3) Run GraphEditor on Pubmed; expect RApost>0 and moderate GEDΔ

## Open Questions the Paper Calls Out

- **Question**: Does the verification framework remain effective when applied to alternative, potentially more complex explanation methods beyond vanilla saliency maps? The authors limit their implementation to vanilla saliency for reproducibility but explicitly list "consider[ing] alternative explainers" as a direction for future work.

- **Question**: How does the ratio of forgotten nodes impact the reliability and magnitude of the proposed verification metrics? The methodology fixes the forget set at 5%, while the conclusion states that "future work will vary the forget-set ratio."

- **Question**: Can the attribution-based metrics (HS, ESD) be normalized to allow for robust comparisons of unlearning efficacy across different datasets? The authors explicitly state they "avoid cross-dataset magnitude comparisons" because HS and ESD inherit gradient scale.

## Limitations

- Implementation unknowns: Key hyperparameters for GraphEditor and IDEA are not specified, and MI attack details are missing
- Attribution reliability: Gradient-based saliency can be noisy or saturated, especially for deep or residual architectures
- Surrogate variability: GraphChef rule sets are acknowledged as unstable; GRS is only diagnostic

## Confidence

- **High**: Core mechanism (RA+HS+ESD+GED) effectively distinguishes complete vs. partial forgetting for standard GNN backbones
- **Medium**: Claims about detecting residual signals missed by MI ROC-AUC are plausible but require deeper analysis
- **Low**: GRS as corroborative evidence—too sensitive to surrogate instability for actionable conclusions

## Next Checks

1. **Hyperparameter isolation**: Run ablation on GraphEditor/IDEA with varying unlearning rates/strengths; observe how RA/GED trade off

2. **Attribution robustness**: Repeat RA/HS/ESD with GNNExplainer or Integrated Gradients; check if deltas remain consistent

3. **MI attack detail**: Implement MI ROC-AUC with standardized protocols (e.g., shadow model training, loss thresholding) and compare results to paper-reported values