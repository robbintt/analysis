---
ver: rpa2
title: 'Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in
  Multi-Stage Visual Grounding'
arxiv_id: '2509.25794'
source_url: https://arxiv.org/abs/2509.25794
tags:
- arxiv
- visual
- object
- grounding
- embodied
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Point-It-Out (PIO), a benchmark for evaluating
  vision-language models (VLMs) on embodied reasoning through precise visual grounding.
  Unlike prior work relying on multiple-choice or language-only planning, PIO uses
  a three-stage hierarchy: S1 for object localization, S2 for task-driven pointing,
  and S3 for visual trace prediction.'
---

# Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding

## Quick Facts
- **arXiv ID**: 2509.25794
- **Source URL**: https://arxiv.org/abs/2509.25794
- **Reference count**: 27
- **Primary result**: PIO benchmark reveals grounding-specialized models excel in pixel-level localization but struggle in trajectory prediction, while general VLMs achieve higher S3 scores through reasoning and world knowledge.

## Executive Summary
This paper introduces Point-It-Out (PIO), a benchmark for evaluating vision-language models (VLMs) on embodied reasoning through precise visual grounding. Unlike prior work relying on multiple-choice or language-only planning, PIO uses a three-stage hierarchy: S1 for object localization, S2 for task-driven pointing, and S3 for visual trace prediction. The benchmark spans four domains—household, kitchen, driving, and robotics—with 600+ human-annotated examples. Testing 10+ VLMs reveals that models with explicit grounding supervision (e.g., RoboRefer, MoLMO-7B-D) excel in S1/S2, but struggle in S3, while strong general models like GPT-o3 and Gemini-2.5-Pro achieve higher scores in complex reasoning and planning tasks. Fine-grained visual grounding remains challenging, especially for affordance, contact, and part localization, indicating the need for improved grounding-aware training.

## Method Summary
PIO evaluates VLMs on a three-stage embodied reasoning task: S1 (object localization), S2 (task-driven pointing), and S3 (visual trace prediction). The benchmark uses 241 images from BDD-100K, Where2Place, EPIC-Kitchens, RT-1, and AgiBot datasets. S1 and S2 are evaluated using normalized IoU between predicted and ground-truth masks, while S3 uses human or GPT-o4-mini scoring. Models receive prompt templates specifying output format (bounding boxes, points, or trajectories). The benchmark isolates failure modes by separating spatial perception from temporal planning, revealing that grounding-supervised models excel at localization but underperform in trajectory generation compared to general-purpose VLMs.

## Key Results
- Grounding-specialized models (RoboRefer, MoLMO) outperform general VLMs on S1/S2 object localization and affordance pointing.
- General-purpose VLMs (GPT-o3, Gemini-2.5-Pro) achieve higher S3 scores due to superior reasoning and world knowledge.
- Performance drops significantly for fine-grained tasks: part localization, affordance, and contact reasoning.
- S3 remains challenging for all models, with trajectory prediction requiring integration of spatial and temporal reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical task decomposition improves diagnostic granularity for embodied reasoning evaluation.
- **Mechanism:** PIO separates embodied reasoning into S1→S2→S3 progression: object localization (spatial perception), task-driven pointing (affordance reasoning), and visual trace prediction (temporal planning). This isolates failure modes—e.g., a model may localize well but fail affordance grounding.
- **Core assumption:** Embodied reasoning decomposes into learnable sub-skills with explicit dependencies; S2 depends on S1, S3 integrates both.
- **Evidence anchors:**
  - [abstract]: "three-stage hierarchy: S1 for object localization, S2 for task-driven pointing, and S3 for visual trace prediction"
  - [section 3]: "This structure mirrors the natural complexity of embodied tasks progressively from simple object detection to more challenging tasks such as affordance prediction"
  - [corpus]: Weak direct corpus support for this specific hierarchy; related work (EmbRACE-3K) explores embodied reasoning but without staged grounding.
- **Break condition:** If S1/S2 performance does not predict S3 outcomes (models ace S1/S2 but randomly fail S3), the hierarchy may capture independent rather than cumulative skills.

### Mechanism 2
- **Claim:** Explicit grounding supervision improves pixel-level spatial precision but does not guarantee temporal coherence.
- **Mechanism:** Models trained with bounding box or point annotations (RoboRefer, MoLMO, Qwen) outperform general-purpose VLMs (GPT-4o, Claude-3.7) on S1/S2. However, this training emphasizes "where" not "how," leaving S3 underdeveloped.
- **Core assumption:** Spatial grounding and temporal planning are partially separable; grounding-specific fine-tuning may not transfer to trajectory generation.
- **Evidence anchors:**
  - [section 5]: "models such as MoLMO perform well in S1 and S2 but struggle in S3"
  - [figure 8]: MoLMO and Qwen underperform in visual trace prediction despite strong S1/S2
  - [corpus]: MedGround (arXiv 2601.06847) similarly finds that grounding data improves localization but not multi-step reasoning.
- **Break condition:** If grounding-supervised models match general models on S3 after trajectory-specific fine-tuning, the gap is data/coverage, not architectural.

### Mechanism 3
- **Claim:** Strong general-purpose VLMs compensate for weaker grounding precision through enhanced reasoning and world knowledge.
- **Mechanism:** GPT-o3 and Gemini-2.5-Pro leverage broader pretraining to infer plausible trajectories, achieving higher S3 scores despite lower S1/S2 precision. They generalize across domains without domain-specific grounding data.
- **Core assumption:** World knowledge and abstract planning partially substitute for fine-grained spatial precision when tasks tolerate coarse trajectories.
- **Evidence anchors:**
  - [section 5]: "Gemini-2.5-Pro achieves almost 4 out of 5... attributed to inclusion of embodied data and grounding data"
  - [corpus]: S-Chain (arXiv 2510.22728) shows structured chain-of-thought improves medical VLM reasoning without pixel-perfect grounding.
- **Break condition:** If general VLMs fail when tasks require sub-centimeter precision (e.g., surgical manipulation), reasoning alone is insufficient.

## Foundational Learning

- **Concept: Visual Grounding**
  - **Why needed here:** PIO's core innovation is pixel-level grounding (points, bboxes, traces) as the evaluation modality; without understanding grounding, the benchmark's contribution is unclear.
  - **Quick check question:** Given an image and the phrase "the leftmost cup's handle," can you define what a successful grounding output looks like?

- **Concept: Affordance Reasoning**
  - **Why needed here:** S2 requires models to infer interaction points not explicitly mentioned (e.g., "open the drawer" → localize handle); this goes beyond perception to functional understanding.
  - **Quick check question:** For a door and the command "close it," where should a robot apply force? How does this differ from localizing "the door"?

- **Concept: Embodied Reasoning vs. Passive Vision**
  - **Why needed here:** PIO critiques existing benchmarks (multiple-choice, language-only) for missing action-grounding; understanding this distinction clarifies why pixel-level outputs matter.
  - **Quick check question:** Why might a VLM correctly answer "Which trajectory reaches the mug?" in multiple-choice yet fail to draw that trajectory on the image?

## Architecture Onboarding

- **Component map:** Image + language query → grounding model → spatial output (S1/S2) → trajectory generator (S3) → normalized IoU or scoring evaluation

- **Critical path:** 1. Image + language query → grounding model → spatial output; 2. For S3: spatial output + task context → trajectory generation; 3. Output compared to human-annotated mask/trace

- **Design tradeoffs:**
  - **Point vs. bounding box:** Points suit fine-grained tasks (affordance); boxes better for object extent. PIO normalizes IoU to handle this.
  - **Specialized vs. general VLMs:** Grounding-specialized models win S1/S2; general models win S3. Assumption: hybrid architectures may balance both.
  - **2D vs. 3D grounding:** PIO uses 2D for scalability; 3D would increase realism but complicate annotation.

- **Failure signatures:**
  - S1 failure: Incorrect object/part localized (e.g., points to wrong cup)
  - S2 failure: Object localized correctly but wrong interaction point (e.g., points to drawer body, not handle)
  - S3 failure: Trajectory directionally wrong or misses key waypoints

- **First 3 experiments:**
  1. **Baseline S1/S2 evaluation:** Run RoboRefer, MoLMO, GPT-4o on PIO's 501 QA pairs; confirm normalized IoU metric correlates with human judgment.
  2. **Ablate grounding data:** Fine-tune a general VLM (e.g., Qwen) on increasing amounts of grounding data; plot S1/S2 vs. S3 performance curves.
  3. **Cross-domain transfer:** Train on household+kitchen data, test on driving+robotics; measure domain gap and identify which subclasses (affordance, contact) transfer poorly.

## Open Questions the Paper Calls Out

- **Question:** What specific architectural or data augmentation strategies are required to improve VLM performance on fine-grained "part" localization and "contact/affordance" reasoning, where all tested models showed the steepest performance drops?
- **Question:** How can the gap between strong spatial grounding (S1/S2) and temporal visual trace prediction (S3) be bridged, given that top-performing grounding models like MoLMO fail to translate localization skills into coherent plans?
- **Question:** Can the integration of explicit grounding supervision into general-purpose reasoning models (e.g., GPT-o3, Gemini) improve spatial precision in S1/S2 without compromising their superior performance in complex visual trace planning (S3)?

## Limitations

- **Data access bottleneck:** Exact image IDs and S3 ground-truth marker coordinates not fully specified; replication requires manual curation.
- **Evaluation metric granularity:** S3 depends on subjective human or GPT-o4-mini scoring, introducing potential evaluator bias.
- **Generalization claims:** Strong S3 performance by general VLMs not tested on fine-grained sub-centimeter precision tasks.

## Confidence

- **High confidence:** S1/S2 performance differences between grounding-specialized and general VLMs (clear metric, direct comparison).
- **Medium confidence:** S3 scoring methodology and cross-model rankings (depends on GPT/human evaluation, less reproducible).
- **Low confidence:** Transfer claims for high-precision tasks (not experimentally tested).

## Next Checks

1. **Metric robustness test:** Replicate S1/S2 scoring on a held-out subset with two independent annotators; measure inter-rater reliability for IoU and GPT-based S3 scoring.

2. **Prompt sensitivity analysis:** Vary GPT-o4-mini S3 prompts (e.g., emphasize trajectory smoothness vs. waypoint accuracy) and measure score variance across models.

3. **Domain transfer experiment:** Train a general VLM on household+kitchen PIO data, test on driving+robotics; quantify performance drop specifically for affordance and contact localization sub-classes.