---
ver: rpa2
title: 'MV-S2V: Multi-View Subject-Consistent Video Generation'
arxiv_id: '2601.17756'
source_url: https://arxiv.org/abs/2601.17756
tags:
- video
- reference
- subject
- generation
- multi-view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of single-view subject-to-video
  generation by proposing a multi-view subject-to-video generation (MV-S2V) task that
  enforces 3D-level subject consistency. The key challenge is the lack of training
  data with multi-view subject displays and the need to distinguish between cross-subject
  and cross-view references in conditioning.
---

# MV-S2V: Multi-View Subject-Consistent Video Generation

## Quick Facts
- **arXiv ID**: 2601.17756
- **Source URL**: https://arxiv.org/abs/2601.17756
- **Reference count**: 13
- **Key outcome**: Introduces multi-view subject-to-video generation (MV-S2V) with 3D-level subject consistency using Temporally Shifted RoPE and a synthetic data pipeline.

## Executive Summary
This paper addresses the challenge of generating videos from multi-view reference images while maintaining 3D subject consistency. The key insight is that single-view references are insufficient for 3D-consistent synthesis, and existing methods fail when distinguishing between cross-subject and cross-view conditioning. The authors propose a novel framework that uses a synthetic data pipeline to generate training data with multi-view subject displays, complemented by a small real-world dataset. A key innovation is Temporally Shifted RoPE (TS-RoPE), which enables the model to clearly distinguish between different subjects and distinct views of the same subject via rotary position encoding. Extensive experiments demonstrate that their framework achieves superior 3D subject consistency with multi-view references and high-quality visual outputs, outperforming state-of-the-art methods on multi-view and 3D subject consistency metrics.

## Method Summary
The MV-S2V framework fine-tunes from Phantom-Wan, a DiT-based video generation model. It takes text prompts and multi-view reference images as input, encodes them through a 3D VAE, and merges the tokens using TS-RoPE to distinguish between different subjects and views. The model is trained with Rectified Flow on a synthetic dataset generated through a 5-stage pipeline: Image Synthesis → Video Synthesis → Video Captioning → Reference Extraction → Data Filtering, complemented by a small real-world dataset. Evaluation uses bidirectional DINO/CLIP similarity, MEt3R scores, and 3D point cloud alignment metrics to assess multi-view and 3D subject consistency.

## Key Results
- MV-S2V outperforms Phantom and MAGREF baselines on multi-view and 3D subject consistency metrics (DINO `S_v→r` of 0.816 vs. 0.795 for Vanilla RoPE).
- Ablation study shows TS-RoPE significantly reduces subject deformation and abrupt changes compared to Vanilla RoPE and SS-RoPE.
- The synthetic data pipeline generates 11,804 OC + 10,130 HOI samples, with a small real-world dataset (1,724 OC + 1,514 HOI) improving robustness.
- MV-S2V achieves superior 3D consistency on the NAVI benchmark, particularly on 3D point cloud alignment metrics.

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Curation Pipeline
The paper develops a synthetic data curation pipeline to generate highly customized training data with multi-view subject displays. Existing I2V models (Uni3C for camera control, Wan2.2 for prompt following) generate videos where objects are systematically shown from multiple views. Reference images are extracted from these generated videos, creating (video, multi-view references, caption) triplets for training. A small real-world dataset is added to improve robustness to natural image variations. This compensates for the scarcity of real-world multi-view video data.

### Mechanism 2: Temporally Shifted RoPE (TS-RoPE)
TS-RoPE enables the model's self-attention to disambiguate between different subjects and between different views of the same subject within the reference set. Instead of naively concatenating all reference latents along the temporal dimension, TS-RoPE inserts a fixed temporal shift δ between the video tokens and reference tokens, and between reference tokens of different subjects. Views of the same subject are placed in adjacent temporal positions. This structural separation in the positional encoding space allows the DiT's self-attention to treat "same subject, different view" and "different subject" relationships distinctly.

### Mechanism 3: Comprehensive Evaluation Metrics
The evaluation framework uses a combination of feature similarity (DINO/CLIP) and geometry-aware metrics (MEt3R, 3D point cloud alignment) to effectively measure multi-view and 3D subject consistency in generated videos. The framework uses bidirectional metrics: `S_v→r` checks if each generated video frame matches a reference view, while `S_r→v` checks if all reference views are represented in the video. MEt3R compares features after aligning viewpoints, and π3-estimated point clouds are compared via nearest-neighbor distance. This provides a more comprehensive assessment than single-view similarity.

## Foundational Learning

- **Concept: Diffusion Transformers (DiT) and Rectified Flow**
  - **Why needed here**: The MV-S2V framework is built on the Wan 2.1 DiT model and trained with Rectified Flow. Understanding how tokens, attention, and flow matching work is essential to grasp the conditioning mechanism.
  - **Quick check question**: Can you explain how a DiT processes a sequence of tokens differently from a U-Net, and how Rectified Flow defines the denoising objective?

- **Concept: Rotary Positional Embedding (RoPE)**
  - **Why needed here**: TS-RoPE is a core innovation. You must understand how RoPE encodes position information in attention mechanisms to see why temporal shifting is a meaningful intervention.
  - **Quick check question**: How does RoPE allow a model to understand the relative position between two tokens in a sequence?

- **Concept: Subject-to-Video (S2V) vs. Image-to-Video (I2V) Paradigm**
  - **Why needed here**: The paper explicitly positions MV-S2V as going beyond the "S2I + I2V" pipeline. Understanding the limitation of single-view references is key to appreciating the problem formulation.
  - **Quick check question**: What is the fundamental limitation of using only a single-view reference image for subject-consistent video generation?

## Architecture Onboarding

- **Component map**: Text prompt (T5 Encoder) → Multi-view reference images → 3D VAE encoder → Token merger with TS-RoPE → DiT denoising backbone → Video output
- **Critical path**: The most critical step for a new engineer is implementing and debugging the TS-RoPE token merging logic. This requires correctly implementing the temporal shifts for video vs. references and for different subjects, and ensuring it integrates cleanly with the pre-trained Wan/Phantom DiT.
- **Design tradeoffs**:
  - **Synthetic vs. Real Data**: The paper uses ~87% synthetic data. This trades off perfect realism for scale and control. Risk: The model may learn synthetic artifacts.
  - **TS-RoPE vs. SS-RoPE**: TS-RoPE operates in the more familiar temporal dimension, closer to the base model's pre-training. SS-RoPE requires learning new spatial-shift patterns from scratch.
  - **Metric Complexity**: The proposed evaluation is thorough but computationally expensive (requires π3 point cloud estimation). A simpler proxy may be needed for rapid iteration.
- **Failure signatures**:
  - **Object Deformation / Fragmentation**: Likely a failure in the conditioning mechanism (e.g., using Vanilla RoPE or an incorrect TS-RoPE implementation), causing the model to blend incompatible views.
  - **"Copy-Paste" Behavior**: The model memorizes and directly inserts reference frames into the video. This suggests overfitting to the synthetic data where references were extracted from the video itself. Mitigation: Ensure real-world data is used, and apply strong augmentation to references.
  - **Poor View Coverage**: Some reference views never appear in the generated video. This may indicate the `S_r→v` optimization signal is not being learned, or the model is biased towards certain viewpoints.
- **First 3 experiments**:
  1. **Baseline Reproduction**: Train the model with Vanilla RoPE on a small subset of the synthetic data. Measure `S_dino` and visually inspect for deformation. This establishes a failing baseline.
  2. **TS-RoPE Ablation**: Implement TS-RoPE and train on the same subset. Compare metrics and visual quality against the Vanilla baseline. The goal is to confirm the directional improvement shown in Table 2.
  3. **Data Sensitivity Test**: Train two models: one with synthetic data only, and one with the mixed synthetic + real data. Evaluate on the NAVI benchmark to measure the impact of real data on robustness and metric scores.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the MV-S2V framework be extended to scenarios involving multiple distinct subjects where each subject is conditioned on a set of multi-view reference images without causing feature entanglement? The current work deals with one central subject or with an additional human subject, and future works may extend to cases where multiple subjects all have multi-view references.
- **Open Question 2**: How can multi-view conditioning be adapted to control the appearance of deformable or articulating subjects (e.g., a refrigerator door opening) where reference views represent distinct functional states rather than just rigid spatial orientations? The work focuses on rigid subjects and explicitly identifies controlling a deformable subject with multi-state references as a direction for future work.
- **Open Question 3**: Does the reliance on synthetic training data—generated by existing I2V models which may hallucinate details or lack physical plausibility—impose an upper bound on the "3D Subject Consistency" and visual fidelity of the MV-S2V model? The paper relies entirely on a synthetic data curation pipeline because suitable real-world data is scarce, and if teacher models generate videos with subtle Janus problems or inconsistent geometry, the student MV-S2V model may learn to replicate these artifacts.
- **Open Question 4**: How robust are the proposed 3D consistency metrics (specifically π3 point cloud alignment and MEt3R) against "billboard" effects where the model generates high-fidelity 2D textures that project well but lack true volumetric density? The evaluation section introduces these metrics, but the paper does not deeply validate their correlation with human judgment of "3D-ness" versus simply "view-consistent texture."

## Limitations

- The synthetic data generation assumes that I2V models can produce videos with reliable multi-view subject consistency, which may not hold across all object categories or camera motion patterns.
- The fixed temporal shift δ in TS-RoPE is not specified, leaving implementation details to engineering judgment.
- The small real-world dataset may not be sufficient to fully prevent synthetic bias or the "copy-paste" effect in the model's output.

## Confidence

- **High Confidence**: The paper's core claims about TS-RoPE's effectiveness are well-supported by the ablation study (Table 2, Figure 5), showing consistent improvements in subject consistency metrics over both vanilla RoPE and SS-RoPE. The synthetic data pipeline design is clearly specified and builds on established I2V methods.
- **Medium Confidence**: The overall framework's superiority on the NAVI benchmark is convincing, but some metrics (particularly VBench aesthetic scores) show more modest gains. The ablation on data composition is missing, leaving questions about the real-data contribution.
- **Low Confidence**: The exact value of the temporal shift δ in TS-RoPE is unspecified, and the impact of this hyperparameter on performance is not explored. The robustness of the synthetic pipeline to different object types or interaction styles is not demonstrated.

## Next Checks

1. **TS-RoPE Ablation with Varying δ**: Implement the TS-RoPE scheme with multiple temporal shift values (e.g., δ ∈ {8, 16, 32}) and measure subject consistency metrics. This validates whether the shift magnitude is critical to the method's success.

2. **Synthetic vs. Real Data Contribution**: Train two models: one using only synthetic data, and one using the mixed dataset (as described in the paper). Evaluate both on NAVI to quantify the real data's contribution to robustness and multi-view consistency.

3. **Failure Mode Analysis on NAVI**: Generate videos for NAVI objects and conduct a detailed failure analysis, categorizing errors as: (a) reference view omission, (b) subject deformation/fragmentation, (c) identity inconsistency across frames. Correlate failures with specific object classes or reference view combinations to identify limitations of the approach.