---
ver: rpa2
title: Surprise Calibration for Better In-Context Learning
arxiv_id: '2506.12796'
source_url: https://arxiv.org/abs/2506.12796
tags:
- uni00000013
- uni00000011
- qwen2
- uni00000003
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Surprise Calibration (SC), a novel approach
  to address bias calibration in in-context learning (ICL) by leveraging the concept
  of "surprise" as a signal for detecting and adjusting class prior shifts. SC dynamically
  updates class priors based on the temporal dynamics of surprise signals derived
  from contextual demonstrations, offering a more adaptive and computationally efficient
  solution compared to existing methods.
---

# Surprise Calibration for Better In-Context Learning

## Quick Facts
- arXiv ID: 2506.12796
- Source URL: https://arxiv.org/abs/2506.12796
- Reference count: 40
- Primary result: Surprise Calibration (SC) improves ICL accuracy by +4.59% (3B) and +3.54% (7B) over vanilla ICL

## Executive Summary
This paper introduces Surprise Calibration (SC), a novel approach to address bias calibration in in-context learning (ICL) by leveraging the concept of "surprise" as a signal for detecting and adjusting class prior shifts. SC dynamically updates class priors based on the temporal dynamics of surprise signals derived from contextual demonstrations, offering a more adaptive and computationally efficient solution compared to existing methods. Evaluated across eight datasets from six natural language processing tasks using Qwen2.5-3B and Qwen2.5-7B models, SC consistently outperforms state-of-the-art calibration baselines, achieving average accuracy improvements of +4.59% and +3.54% over vanilla ICL for the respective model sizes. SC demonstrates superior robustness and stability across varying demonstration selection and ordering strategies, while maintaining computational efficiency by avoiding additional inference iterations per query.

## Method Summary
Surprise Calibration computes "surprise" as the negative log-likelihood of ground-truth labels given demonstration contexts, then uses a GRU to model the temporal dynamics of these surprise signals across demonstration sequences. The method constructs signed surprise vectors where matching class labels receive negative signs, processes them through the GRU to produce an adjustment vector, and applies this vector to calibrate the model's log-probabilities before final prediction. Unlike baseline methods requiring repeated inference passes, SC performs calibration in a single forward pass with a lightweight trained GRU, achieving both accuracy improvements and computational efficiency. The approach is trained end-to-end using cross-entropy loss on labeled training samples.

## Key Results
- SC achieves average accuracy improvements of +4.59% (3B) and +3.54% (7B) over vanilla ICL across eight datasets
- SC maintains computational efficiency by avoiding repeated inference iterations, requiring only M+T passes vs. n×T for baselines
- SC demonstrates superior robustness and stability across different demonstration selection (Random, BM25, Top-k) and ordering (Increase, Decrease, U-curve) strategies
- Ablation studies show surprise magnitude contributes significantly to performance (performance drops when magnitude is clamped to 1)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A "surprise" signal, derived from prediction error on demonstrations, correlates with the degree of class prior shift required.
- **Mechanism:** The method computes surprise as the negative log-likelihood of the ground-truth label given the preceding context ($-\log p(y_j | e_j, D_{j-1})$). The paper argues that high surprise indicates a mismatch between the model's current implicit belief and the demonstration, signaling a need for larger prior adjustments.
- **Core assumption:** ICL functions as implicit sequential Bayesian inference, where the model maintains a latent belief state $p(z|D)$ that updates upon observing new tokens.
- **Evidence anchors:**
  - [abstract] Identifies "surprise" as an informative signal for class prior shift.
  - [section 3.1] Equation 4 defines surprise; Equation 6 links low likelihood (high surprise) to amplified belief shifts.
  - [corpus] Related work (e.g., *ICL CIPHERS*) discusses dual modes of ICL (retrieval vs. learning), supporting the idea that models maintain dynamic internal states during inference.
- **Break condition:** If ICL operates purely as a retrieval mechanism without updating latent task beliefs based on local error signals, the surprise-prior shift correlation would likely collapse.

### Mechanism 2
- **Claim:** Calibration can be achieved by modeling the temporal dynamics of surprise signals across the demonstration sequence.
- **Mechanism:** A Gated Recurrent Unit (GRU) processes a sequence of "surprise vectors"—encoding both the magnitude and direction (sign) of surprise for each class—to output a cumulative prior adjustment vector. This vector is subtracted from the model's original log-probabilities.
- **Core assumption:** The sequential order of demonstrations matters (temporal dynamics), and a simple recurrent model can approximate the complex Bayesian update rule of the LLM.
- **Evidence anchors:**
  - [section 4] Describes the SC architecture: Surprise vector construction (Eq. 8) and the GRU-based aggregation into an adjustment vector (Eq. 9).
  - [table 3] Ablation study showing performance drops when surprise magnitude is removed, validating the importance of the signal's strength.
  - [corpus] Weak/No direct corpus evidence for the GRU-based calibration specific to surprise; related papers focus on demonstration selection rather than probability calibration.
- **Break condition:** If the LLM's bias is static (e.g., purely frequency-based in pre-training) and insensitive to the sequential presentation of demonstrations, the temporal modeling approach would fail to capture the necessary dynamics.

### Mechanism 3
- **Claim:** Surprise Calibration allows for query-specific calibration without the computational cost of repeated inference iterations.
- **Mechanism:** Unlike Contextual Calibration (CC+) or Batch Calibration (BC+), which require $n \times$ inference passes to estimate priors per query, SC performs a single forward pass to extract surprise signals, then uses the lightweight trained GRU to compute the adjustment.
- **Core assumption:** The surprise signals extracted during the initial forward pass are sufficient statistics for the bias, removing the need for "content-free" or batched sampling approaches.
- **Evidence anchors:**
  - [table 1] Compares inference counts, showing SC requires only $M+T$ (training + test) vs. $n \times T$ for baselines.
  - [section 5.2] Reports SC outperforms baselines while maintaining efficiency.
  - [abstract] Claims the method avoids repeated inference iterations.
- **Break condition:** If the demonstration context is extremely short (e.g., 1-shot) or noisy, the extracted surprise signals may be insufficiently stable for the GRU to generalize, potentially requiring the robustness of repeated sampling.

## Foundational Learning

- **Concept: In-Context Learning (ICL) Bias**
  - **Why needed here:** The entire method is designed to mitigate specific biases (majority label bias, recency bias) that distort LLM predictions in few-shot settings. Understanding that LLMs favor specific tokens regardless of the input is crucial.
  - **Quick check question:** Can you explain why a model might predict "Positive" for a negative review just because the last demonstration was "Positive"?

- **Concept: Bayesian Inference & Priors**
  - **Why needed here:** The paper frames ICL as implicit Bayesian inference. To understand the mechanism, one must grasp the idea of a "prior" (initial belief) and a "posterior" (updated belief), and how "surprise" relates to the likelihood term in Bayes' theorem.
  - **Quick check question:** In a Bayesian context, does observing a "surprising" event (low likelihood) typically result in a small or large update to the prior belief?

- **Concept: Log-Probability Space Calibration**
  - **Why needed here:** The method operates in log-space (Eq. 9: $-\log p_{calib} = -\log p_{orig} - \alpha$). Engineers must understand that adding/subtracting in log-space equates to multiplying/dividing in probability space, which is how the class priors are effectively re-weighted.
  - **Quick check question:** If the adjustment vector $\alpha$ for class A is 0.5, does the probability of class A increase or decrease relative to the uncalibrated output?

## Architecture Onboarding

- **Component map:** LLM (Frozen) -> Surprise Extractor -> Surprise Vector Builder -> Calibration Model (GRU) -> Calibrator -> Final Prediction

- **Critical path:** The accurate extraction of the "pre-label" probability. If the extraction happens *after* the label is appended (teacher forcing), the "surprise" signal is lost because the model already sees the answer. The extraction must happen at the specific separator token position.

- **Design tradeoffs:**
  - **Backbone Selection:** The paper uses GRU for simplicity (Table 5 shows RNN/LSTM/Transformer all perform similarly), but Transformer backbones might capture longer-range dependencies in the demonstration order.
  - **Efficiency vs. Robustness:** SC is faster than BC+ but requires a labeled training set ($M$) to train the GRU. If no labeled data exists, SC cannot be initialized.

- **Failure signatures:**
  - **Sign confusion:** If the surprise vector sign logic (Eq. 8: $1 - 2\delta_{c,y}$) is implemented incorrectly, the calibration will reinforce bias rather than reduce it.
  - **Training instability:** The paper notes the model is sensitive to data noise; if the training set for the GRU is small ($<125$ samples in Figure 7), variance is high.
  - **Tokenizer misalignment:** If the label is tokenized into multiple tokens (e.g., "truthful" -> "truth", "ful"), the extraction must use only the probability of the first token.

- **First 3 experiments:**
  1. **Vanilla vs. SC:** Run vanilla ICL and SC on a binary classification task (e.g., SST-2) with a deliberately biased prompt (e.g., all negative examples followed by one positive query) to verify bias mitigation.
  2. **Ablation on Magnitude:** Run SC with surprise magnitude clamped to 1 (sign only) vs. full magnitude to verify the paper's finding that magnitude contributes to performance (Table 3).
  3. **Efficiency Benchmarks:** Measure wall-clock time for SC vs. CC+ (Contextual Calibration) on a batch of 100 queries to validate the "no repeated inference" claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical framework of implicit sequential Bayesian inference be refined to explain the rare cases where Surprise Calibration underperforms vanilla ICL?
- **Basis in paper:** [explicit] The limitations section notes that SC fails in rare cases, likely because "implicit sequential Bayesian inference does not fully capture the behavior of ICL."
- **Why unresolved:** The paper identifies the existence of these failure cases but does not characterize the specific data distributions or prompt structures that cause the theoretical assumption to break down.
- **What evidence would resolve it:** An analysis of the "exceptional instances" identifying common features where the surprise signal fails to correlate with the required prior shift.

### Open Question 2
- **Question:** Can more complex sequence modeling architectures improve the robustness of Surprise Calibration against data noise?
- **Basis in paper:** [explicit] The authors admit the calibration model structure is "relatively simple and can be easily affected by data noise caused by factors such as model inference accuracy."
- **Why unresolved:** The paper tested GRU, LSTM, and a vanilla Transformer (Table 5) and found similar performance, but did not explore if deeper or more specialized architectures could specifically mitigate the identified noise sensitivity.
- **What evidence would resolve it:** Comparative experiments evaluating the variance of SC using deeper architectures under conditions of simulated inference noise or low-precision quantization.

### Open Question 3
- **Question:** How can Surprise Calibration be adapted for environments with extremely limited labeled resources?
- **Basis in paper:** [explicit] The limitations section states that the method "requires a certain amount of labeled data, making it difficult to apply in environments with extremely limited resources."
- **Why unresolved:** The current methodology relies on training a calibration model on labeled training sets, creating a barrier for zero-shot tasks or domains where annotation is impossible.
- **What evidence would resolve it:** A study demonstrating a self-supervised or unsupervised adaptation of the SC model that operates without task-specific labeled training data.

## Limitations
- Requires labeled training data to fit the GRU calibration model, limiting applicability in truly few-shot scenarios
- Core mechanism relies on assumption that surprise signals reliably predict class prior shifts, lacking theoretical grounding
- Tested only on two model sizes (3B and 7B parameters) and limited task domains, raising questions about generalizability
- Performance gains are modest in absolute terms (+3-5% accuracy), suggesting it may not be a universal solution

## Confidence

**High Confidence Claims:**
- The SC method outperforms vanilla ICL on the tested datasets (accuracy improvements of +4.59% and +3.54% are clearly demonstrated)
- SC is computationally more efficient than Contextual Calibration (CC+) and Batch Calibration (BC+) by avoiding repeated inference iterations
- The GRU-based calibration model can learn to map surprise sequences to effective prior adjustments, as shown by ablation studies

**Medium Confidence Claims:**
- Surprise signals reliably correlate with class prior shifts across different datasets and model sizes (supported by experiments but not theoretically proven)
- The method's performance is stable across different demonstration selection and ordering strategies (based on limited ablation experiments)
- The efficiency gains translate to meaningful practical improvements (wall-clock timing data is limited)

**Low Confidence Claims:**
- The method will generalize to much larger models (the paper only tests up to 7B parameters)
- The surprise-prior shift correlation is robust to extreme class imbalance or noisy demonstrations (not extensively tested)
- The GRU architecture is optimal; other architectures might perform better (the paper shows RNN/LSTM/Transformer perform similarly but doesn't explore this fully)

## Next Checks
1. **Theoretical Validation:** Develop a theoretical framework explaining why surprise signals correlate with class prior shifts. This could involve analyzing the gradient flow through demonstrations or studying how LLMs update internal representations during ICL.

2. **Robustness Testing:** Evaluate SC on datasets with extreme class imbalance (>90/10 ratios) and varying levels of demonstration noise. Measure performance degradation compared to vanilla ICL and test whether the surprise signal remains informative under these conditions.

3. **Scaling Analysis:** Test SC on models ranging from 1B to 70B parameters across multiple task families (not just text classification). This would validate whether the surprise calibration mechanism scales with model capacity and whether larger models exhibit different bias patterns that SC can address.