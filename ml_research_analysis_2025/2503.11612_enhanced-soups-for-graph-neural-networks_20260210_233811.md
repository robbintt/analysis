---
ver: rpa2
title: Enhanced Soups for Graph Neural Networks
arxiv_id: '2503.11612'
source_url: https://arxiv.org/abs/2503.11612
tags:
- souping
- graph
- memory
- training
- ingredients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Learned Souping for Graph Neural Networks
  (LS), a gradient-descent-based model aggregation approach that outperforms existing
  GNN souping algorithms. LS optimizes interpolation parameters across model layers
  using gradient descent, achieving up to 1.2% accuracy improvement and 2.1X speedup
  on the Reddit dataset with GAT architecture.
---

# Enhanced Soups for Graph Neural Networks

## Quick Facts
- arXiv ID: 2503.11612
- Source URL: https://arxiv.org/abs/2503.11612
- Authors: Joseph Zuber; Aishwarya Sarkar; Joseph Jennings; Ali Jannesari
- Reference count: 40
- Key outcome: Introduces LS and PLS methods achieving 1.2% accuracy improvement and 2.1X speedup on Reddit dataset

## Executive Summary
This paper introduces Learned Souping for Graph Neural Networks (LS), a gradient-descent-based model aggregation approach that outperforms existing GNN souping algorithms. The method optimizes interpolation parameters across model layers using gradient descent, achieving significant performance improvements on benchmark datasets. To address memory constraints, the authors also propose Partition Learned Souping (PLS), which reduces memory usage by up to 76% while maintaining accuracy through subgraph partitioning strategies.

## Method Summary
The paper presents two novel approaches for model aggregation in GNNs. Learned Souping (LS) uses gradient descent to optimize interpolation parameters across multiple model layers, replacing traditional greedy interpolation methods. Partition Learned Souping (PLS) extends this concept by partitioning large graphs into subgraphs, training on these partitions, and then aggregating the results. Both methods demonstrate superior performance compared to traditional approaches across multiple datasets and GNN architectures, with PLS specifically addressing scalability challenges for large-scale graphs.

## Key Results
- LS achieves up to 1.2% accuracy improvement and 2.1X speedup on Reddit dataset with GAT architecture
- PLS reduces memory usage by up to 76% and achieves 24.5X speedup on ogbn-products dataset with GraphSAGE architecture
- Both methods outperform traditional greedy interpolation approaches across multiple datasets

## Why This Works (Mechanism)
LS works by replacing the traditional greedy interpolation approach with gradient descent optimization of interpolation parameters. This allows the model to learn optimal weight combinations across different layers rather than relying on fixed interpolation strategies. The gradient-based approach can capture more complex relationships between layers and adaptively adjust weights based on the specific characteristics of the dataset and architecture.

PLS addresses memory constraints by partitioning large graphs into smaller, more manageable subgraphs. Each subgraph is processed independently, significantly reducing the memory footprint during training. The partitioned approach maintains accuracy by ensuring that the aggregation process effectively combines information from all partitions while avoiding the memory bottlenecks associated with processing entire large graphs simultaneously.

## Foundational Learning

**Gradient Descent Optimization**: Why needed - To optimize interpolation parameters across model layers. Quick check - Verify convergence behavior on small datasets before scaling up.

**Graph Partitioning**: Why needed - To enable processing of large graphs with limited memory resources. Quick check - Ensure partition quality metrics maintain graph connectivity.

**Model Aggregation**: Why needed - To combine multiple model layers effectively. Quick check - Validate aggregation preserves important feature relationships.

## Architecture Onboarding

Component Map: Input Graph -> Partitioner -> Subgraph Trainer -> Aggregator -> Output Model

Critical Path: Graph input → Partitioning (for PLS) → Model training → Parameter optimization → Model aggregation → Final output

Design Tradeoffs: LS vs PLS - Memory efficiency vs computational overhead; Single vs multiple training passes; Global vs local optimization

Failure Signatures: Poor partitioning leading to disconnected subgraphs; Gradient optimization getting stuck in local minima; Aggregation losing important inter-partition relationships

First Experiments:
1. Test LS on small graph with known optimal interpolation to verify gradient optimization
2. Validate PLS partitioning maintains graph connectivity and feature distribution
3. Compare memory usage between LS and PLS on progressively larger graphs

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Generalizability to GNN architectures beyond GAT and GraphSAGE remains unproven
- Memory reduction claims for PLS may vary significantly with different graph structures
- Computational overhead of gradient descent optimization in LS not thoroughly quantified for very large graphs

## Confidence
High: LS outperforming greedy interpolation on tested datasets and architectures
Medium: Memory reduction and speedup claims for PLS, particularly on ogbn-products
Low: Generalizability to other GNN architectures and graph types not tested

## Next Checks
1. Test LS and PLS across at least five additional GNN architectures (e.g., GCN, GIN, GATv2) on diverse graph datasets to verify architectural robustness
2. Conduct ablation studies comparing different partitioning strategies in PLS to understand optimal graph division methods
3. Measure the computational overhead of gradient descent optimization in LS across varying graph sizes to quantify scalability limits