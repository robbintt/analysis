---
ver: rpa2
title: Self-Supervised Learning by Curvature Alignment
arxiv_id: '2511.17426'
source_url: https://arxiv.org/abs/2511.17426
tags:
- curvature
- learning
- kernel
- curvssl
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CurvSSL and kernel CurvSSL, self-supervised
  learning methods that incorporate curvature regularization into the learning objective.
  The approach extends standard non-contrastive SSL frameworks by computing discrete
  curvature scores for each embedding using k-nearest neighbors on the unit hypersphere,
  and aligns these curvature scores across augmentations using a Barlow Twins-style
  loss.
---

# Self-Supervised Learning by Curvature Alignment

## Quick Facts
- arXiv ID: 2511.17426
- Source URL: https://arxiv.org/abs/2511.17426
- Reference count: 8
- Primary result: Curvature-regularized SSL methods (CurvSSL and kernel CurvSSL) achieve competitive or improved performance on MNIST and CIFAR-10 compared to Barlow Twins and VICReg

## Executive Summary
This paper introduces CurvSSL and kernel CurvSSL, self-supervised learning methods that incorporate curvature regularization into the learning objective. The approach extends standard non-contrastive SSL frameworks by computing discrete curvature scores for each embedding using k-nearest neighbors on the unit hypersphere, and aligns these curvature scores across augmentations using a Barlow Twins-style loss. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone demonstrate that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg.

## Method Summary
The paper presents two curvature-based self-supervised learning methods. The first, CurvSSL, computes discrete curvature scores for each embedding using k-nearest neighbors on the unit hypersphere and aligns these curvature scores across augmentations using a Barlow Twins-style loss. The second, kernel CurvSSL, extends this approach using kernel methods. Both methods build upon non-contrastive SSL frameworks and aim to preserve local manifold geometry while maintaining discriminative features. The curvature alignment is integrated into the learning objective, with the goal of improving representation quality and downstream task performance.

## Key Results
- CurvSSL achieves 97.9% accuracy on MNIST and 75.1% on CIFAR-10
- Kernel CurvSSL reaches 98.4% accuracy on MNIST and 76.5% on CIFAR-10
- UMAP visualizations show well-separated clusters, indicating preserved local manifold geometry
- Both methods outperform or match Barlow Twins and VICReg on the tested datasets

## Why This Works (Mechanism)
The mechanism behind CurvSSL's effectiveness lies in its ability to preserve local manifold geometry through curvature regularization. By computing and aligning discrete curvature scores across augmentations, the method ensures that the learned representations maintain important geometric properties of the data manifold. This geometric awareness, combined with the discriminative power of non-contrastive SSL frameworks, leads to improved downstream task performance. The curvature alignment acts as a regularizer that encourages the model to learn representations that are both geometrically faithful and task-relevant.

## Foundational Learning
- **Self-supervised learning (SSL)**: Learning useful representations without labels; needed because it enables training on large unlabeled datasets
- **Non-contrastive SSL**: SSL methods without negative pairs; needed for stable training and scalability
- **Curvature in manifold learning**: Measures how data bends in high-dimensional space; needed to capture local geometric structure
- **Barlow Twins loss**: Contrastive loss that aligns representations across views; needed as the base framework for curvature alignment
- **k-nearest neighbors (k-NN)**: Algorithm for finding nearest neighbors; needed for discrete curvature estimation
- **Unit hypersphere**: Normalized space where embeddings lie; needed for consistent curvature computation

## Architecture Onboarding

**Component map:** Input -> Augmentation -> Encoder (ResNet-18) -> Embedding -> k-NN Curvature Computation -> Curvature Alignment Loss -> Barlow Twins Loss -> Total Loss

**Critical path:** Input augmentation → Embedding → Curvature computation → Curvature alignment → Total loss optimization

**Design tradeoffs:** Uses k-NN for curvature estimation (computationally tractable but may miss complex geometry) vs. more sophisticated but expensive curvature measures; standard ResNet-18 vs. more complex architectures

**Failure signatures:** Poor curvature estimation with small k or noisy data; sensitivity to augmentation strength; suboptimal performance on datasets with different manifold structures

**First experiments to run:**
1. Evaluate CurvSSL on a more complex dataset (e.g., CIFAR-100 or a domain-specific benchmark)
2. Perform ablation studies varying k in k-NN curvature estimation
3. Compare against other geometric regularization methods in SSL

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to MNIST and CIFAR-10 datasets, raising questions about scalability to more complex, real-world data
- k-NN curvature estimation may not capture full geometric complexity of high-dimensional embedding manifolds
- UMAP visualizations provide only qualitative evidence of manifold preservation, lacking quantitative metrics

## Confidence
- Performance claims on MNIST/CIFAR-10: Medium
- Claims about manifold geometry preservation: Low
- Generalizability to complex datasets: Low
- Effectiveness of kernel variant: Low

## Next Checks
1. Evaluate CurvSSL and kernel CurvSSL on more challenging, high-dimensional datasets (e.g., ImageNet, CIFAR-100, or domain-specific benchmarks) to test scalability and robustness.
2. Conduct quantitative analysis of manifold preservation using metrics such as trustworthiness, continuity, or reconstruction error, rather than relying solely on UMAP visualizations.
3. Perform ablation studies to assess the sensitivity of performance to key hyperparameters (e.g., k in k-NN curvature estimation, kernel bandwidth) and compare against alternative curvature or geometric regularization methods.