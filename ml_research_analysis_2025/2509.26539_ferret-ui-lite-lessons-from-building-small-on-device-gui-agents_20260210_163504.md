---
ver: rpa2
title: 'Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents'
arxiv_id: '2509.26539'
source_url: https://arxiv.org/abs/2509.26539
tags:
- grounding
- wang
- agents
- data
- navigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Ferret-UI Lite, a 3B multimodal language model
  designed for efficient on-device GUI interaction across mobile, web, and desktop
  platforms. The authors address the challenge of building small-scale, end-to-end
  GUI agents capable of both grounding (identifying UI elements) and navigation (performing
  multi-step actions).
---

# Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents

## Quick Facts
- arXiv ID: 2509.26539
- Source URL: https://arxiv.org/abs/2509.26539
- Reference count: 20
- Primary result: 3B model achieving 91.6% grounding on ScreenSpot-V2, competitive with 7B models

## Executive Summary
This paper presents Ferret-UI Lite, a 3B multimodal language model designed for efficient on-device GUI interaction across mobile, web, and desktop platforms. The authors address the challenge of building small-scale, end-to-end GUI agents capable of both grounding (identifying UI elements) and navigation (performing multi-step actions). To do so, they combine supervised fine-tuning on diverse real and synthetic datasets with reinforcement learning using verifiable rewards. The RL stage includes a zoom-in mechanism for improved grounding precision and uses reward functions tailored for action type and parameter matching. Ferret-UI Lite achieves strong grounding performance—91.6% on ScreenSpot-V2, 53.3% on ScreenSpot-Pro, and 55.3% on OSWorld-G—outperforming other 3B models. For navigation, it reaches 28.0% success on AndroidWorld and 19.8% on OSWorld, competitive with 7B models. The work highlights that small models can benefit from balanced training mixtures and RL but remain challenged in long-horizon multi-step tasks.

## Method Summary
Ferret-UI Lite uses a two-stage training pipeline: supervised fine-tuning (SFT) on mixed grounding and navigation data followed by reinforcement learning with verifiable rewards (RLVR). The model employs a VitDet image encoder with AnyRes for dynamic resolution handling and a 3B dense decoder-only transformer backbone. The unified action space supports 11 actions across mobile, desktop, and web platforms. RLVR uses Group Relative Policy Optimization (GRPO) with zoom-in refinement for grounding tasks and combined action-type and dense-grounding rewards for navigation. Synthetic data is generated via a multi-agent curriculum that produces online rollouts, with a VLM-based critic filtering low-quality trajectories.

## Key Results
- Achieves 91.6% grounding accuracy on ScreenSpot-V2, 53.3% on ScreenSpot-Pro, and 55.3% on OSWorld-G
- Navigation success rates: 28.0% on AndroidWorld, 19.8% on OSWorld
- Outperforms other 3B models on grounding benchmarks
- Balanced 50:50 grounding:navigation data mixture yields best overall performance
- Long-CoT reasoning improves navigation success by +4.1% over baseline

## Why This Works (Mechanism)

### Mechanism 1: Reinforcement Learning with Verifiable Rewards (RLVR) for Grounding
- Claim: RLVR improves grounding precision by allowing flexible reward functions that reward any prediction within the ground-truth bounding box, rather than forcing exact center-point imitation as in SFT.
- Mechanism: The model samples multiple predictions, receives containment-based rewards, and optimizes via Group Relative Policy Optimization (GRPO). A zoom-in mechanism further refines predictions by cropping around the initial guess and re-predicting on a smaller region.
- Core assumption: Small models struggle with complex spatial reasoning across large images; zooming in reduces token complexity and focuses capacity on local decisions.
- Evidence anchors:
  - [abstract] "reinforcement learning with designed rewards" contributes to strong grounding (91.6% ScreenSpot-V2, 53.3% ScreenSpot-Pro)
  - [Section 3, Figure 6a] RL improves grounding over SFT; zoom-in provides additional gains
  - [corpus] AgentCPM-GUI similarly uses reinforcement fine-tuning for mobile GUI agents, suggesting cross-validation of RL approaches, though direct comparison data is limited
- Break condition: If the bounding-box annotations are noisy or inconsistent, containment rewards may reinforce incorrect behaviors.

### Mechanism 2: Balanced Grounding-Navigation Data Mixing
- Claim: Training on a mixture of grounding and navigation data yields better overall performance than training on either alone, with balanced ratios (e.g., 50:50) performing best.
- Mechanism: Navigation data provides contextual supervision that strengthens grounding (learning where elements are in task context), while grounding data does not degrade navigation. Diversity prevents overfitting to single-task patterns.
- Core assumption: Small models require diverse training tokens to generalize; grounding and navigation share underlying visual-semantic representations.
- Evidence anchors:
  - [Section 4.1, Figure 6b] "navigation and grounding data can mutually benefit each other, with balanced ratios achieving the best overall results"
  - [Section 2.1] Diverse datasets from GroundUI, OSAtlas, UGround, Aria-UI, etc., spanning platforms
  - [corpus] AndroidControl-Curated notes benchmark purification reveals GUI agent potential, implying data quality matters, but direct mixture-ratio evidence is not provided in neighbors
- Break condition: If navigation trajectories contain systematic errors (e.g., incorrect actions labeled as correct), they may corrupt grounding representations.

### Mechanism 3: Synthetic Online Rollouts via Multi-Agent Curriculum
- Claim: Online synthetic trajectories from a multi-agent system—with task generator, planner, grounding agent, and critic—improve navigation by exposing the model to error recovery and environmental stochasticity absent from static datasets.
- Mechanism: The curriculum generates increasingly difficult goals; the critic filters low-quality trajectories via VLM-as-a-judge. This yields diverse replanning scenarios (e.g., recovering from stuck states).
- Core assumption: Human-annotated data lacks coverage of failure modes and edge cases; synthetic rollouts fill this gap.
- Evidence anchors:
  - [Section 2.3, Figure 4] Multi-agent pipeline with curriculum task generator and VLM filtering
  - [Section 4.2, Table 4] Synthetic data scaling from 5K to 17K trajectories improves AndroidWorld success from 20.3% to 25.2%
  - [corpus] LightAgent notes similar on-device constraints and data challenges for mobile agents, but does not provide direct evidence on multi-agent rollout effectiveness
- Break condition: If the critic model is miscalibrated, it may pass low-quality trajectories or filter informative edge cases.

## Foundational Learning

- Concept: **GUI Grounding vs. Navigation**
  - Why needed here: Grounding maps instructions to screen coordinates (single-step); navigation requires multi-step planning and action execution. The paper treats these as related but distinct capabilities.
  - Quick check question: Given a screenshot and instruction "Click the Submit button," can you identify the (x, y) coordinate? Now, given "Fill out the form and submit," can you generate the action sequence?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL stage uses GRPO, which normalizes rewards within sampled groups to stabilize training for small models sensitive to reward variance.
  - Quick check question: If you sample 8 predictions with rewards [0, 0, 1, 1, 1, 1, 1, 1], how does GRPO compute advantage for the first sample vs. the third?

- Concept: **Chain-of-Thought (CoT) Reasoning for GUI Agents**
  - Why needed here: The paper uses short-CoT (plan) and long-CoT (plan + action think + reflect) to improve multi-step navigation.
  - Quick check question: For a task "Delete the email from John," what would a short-CoT trace look like vs. a long-CoT trace?

## Architecture Onboarding

- Component map:
  Image Encoder (VitDet with AnyRes) -> LLM Backbone (3B dense decoder) -> Unified Action Space (11 actions) -> Zoom-in Module (optional) -> Training Pipeline (SFT -> RLVR)

- Critical path:
  1. Pretrain on text + vision-language data (internal 3B model)
  2. SFT on unified grounding + navigation data mixture
  3. RLVR with action-type + dense-grounding rewards
  4. Inference with optional zoom-in for grounding tasks

- Design tradeoffs:
  - Sparse vs. dense grounding rewards: Dense rewards (Eq. 3.2) outperform sparse (Section 4.2, Figure 7b)
  - Short vs. long CoT: Long-CoT adds +4.1% over baseline vs. +2.1% for short-CoT (Table 4)
  - SFT steps before RL: Gains are larger when SFT is limited (Figure 7a), suggesting RL compensates for under-trained SFT

- Failure signatures:
  - Grounding accuracy drops on high-resolution desktop screens without zoom-in (ScreenSpot-Pro: 52.3% → 53.3% with zoom-in)
  - Navigation success degrades on long-horizon tasks (OSWorld 19.8% vs. Claude-4-Sonnet 43.9%)
  - RL with action-type reward only causes substantial performance drop (Figure 7b)

- First 3 experiments:
  1. **Ablate zoom-in**: Run RL grounding with and without zoom-in on ScreenSpot-Pro; expect ~1% gap per Figure 6a
  2. **Vary data mixture**: Train with grd:nav ratios of 30:70, 50:50, 70:30; verify balanced ratio yields best grounding + navigation tradeoff per Figure 6b
  3. **Test reward design**: Compare action-type-only, sparse-grounding-only, and action-type + dense-grounding on AndroidWorld; confirm combined dense rewards outperform per Figure 7b

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can small on-device GUI agents achieve parity with larger models on long-horizon multi-step navigation tasks, and through what techniques?
- Basis in paper: [explicit] The authors explicitly state that Ferret-UI Lite's "overall navigation performance remains constrained by the model scale, falling short of the SOTA result on the OSWorld leaderboard" (19.8% vs 43.9% for Claude-4-Sonnet), and highlight "the inherent challenges of developing lightweight, on-device agents capable of robust long-horizon reasoning."
- Why unresolved: The paper demonstrates grounding performance can match larger models but navigation remains a fundamental gap; neither synthetic data scaling nor RL closes this fully.
- What evidence would resolve it: A small model matching 7B+ navigation success rates on OSWorld/AndroidWorld, or theoretical analysis explaining why long-horizon planning inherently requires larger capacity.

### Open Question 2
- Question: What reward design principles make RL robust for small GUI agents across heterogeneous tasks?
- Basis in paper: [explicit] The authors state that "small models can benefit from reinforcement learning, they are sensitive to RL reward designs, underscoring the difficulty of designing robust rewards across heterogeneous UI agentic tasks." Figure 7b shows performance varies significantly with different reward configurations.
- Why unresolved: The paper tests sparse vs dense grounding rewards but doesn't establish generalizable principles for why certain designs work better or how to automate robust reward selection.
- What evidence would resolve it: Systematic study varying reward formulations across task types with clear performance patterns, or automated reward tuning methods that stabilize small-model RL.

### Open Question 3
- Question: Why does grounding and navigation data provide mutual benefit, and does this transfer across all platform domains?
- Basis in paper: [explicit] The authors note that "navigation and grounding data can mutually benefit each other, with balanced ratios achieving the best overall results," but provide no mechanism or theoretical explanation.
- Why unresolved: The ablation (Figure 6b) shows the effect empirically, but the underlying cause—whether shared representations, regularization, or other factors—remains unclear.
- What evidence would resolve it: Probing experiments examining representation overlap between grounding and navigation, or testing whether the effect holds across mobile/desktop/web independently.

### Open Question 4
- Question: How far can synthetic data scaling push small GUI agent performance before saturation or negative transfer?
- Basis in paper: [inferred] Table 4 shows progressive gains from 5K to 17K synthetic trajectories (20.3% to 25.2%), but the paper doesn't explore the upper bound or whether returns diminish/invert.
- Why unresolved: Understanding scaling limits is critical for practical training budgets and knowing when synthetic data generation investment stops paying off.
- What evidence would resolve it: Extended scaling experiments to 50K-100K+ trajectories with performance curves, analyzed for saturation points and cross-benchmark generalization.

## Limitations
- Base model architecture details unspecified (only described as "internal 3B dense model")
- Synthetic data generation pipeline not fully detailed (multi-agent system specifics missing)
- Navigation performance still lags behind larger models on long-horizon tasks
- RL reward design sensitivity creates reproducibility challenges

## Confidence
- **High confidence** in core mechanisms: RLVR with verifiable rewards, balanced data mixing, and zoom-in grounding refinement are all empirically supported with ablation studies and benchmark results
- **Medium confidence** in synthetic data contribution: while scaling from 5K to 17K trajectories shows measurable improvement, exact quality depends on multi-agent system implementation
- **Low confidence** in exact reproduction without base model and full synthetic data pipeline details

## Next Checks
1. **Reproduce the zoom-in grounding ablation**: Train a 3B model with RLVR on ScreenSpot-Pro with and without the zoom-in mechanism, measuring the expected ~1% improvement gap to validate the spatial reasoning benefit

2. **Validate data mixture effects**: Systematically train models with grd:nvg ratios of 30:70, 50:50, and 70:30 on the unified datasets, confirming the 50:50 mixture yields optimal balance between grounding and navigation performance as claimed

3. **Test reward design sensitivity**: Compare action-type-only, sparse-grounding-only, and combined dense rewards (action-type + f_param with λ=0.5) on AndroidWorld navigation tasks to verify the combined reward formulation prevents the substantial degradation observed with action-type rewards alone