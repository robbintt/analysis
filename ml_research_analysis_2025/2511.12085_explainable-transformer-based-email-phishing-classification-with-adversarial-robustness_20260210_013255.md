---
ver: rpa2
title: Explainable Transformer-Based Email Phishing Classification with Adversarial
  Robustness
arxiv_id: '2511.12085'
source_url: https://arxiv.org/abs/2511.12085
tags:
- phishing
- email
- adversarial
- robustness
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a hybrid transformer-based phishing detection
  framework combining DistilBERT with adversarial training and Explainable AI. It
  addresses vulnerabilities in phishing classifiers to both adversarial perturbations
  and character-level noise by employing FGM-based embedding-level adversarial training
  and stochastic character-level perturbations during training.
---

# Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness

## Quick Facts
- **arXiv ID:** 2511.12085
- **Source URL:** https://arxiv.org/abs/2511.12085
- **Reference count:** 40
- **Primary result:** Hybrid transformer-based phishing detection framework combining DistilBERT with adversarial training and Explainable AI, achieving superior robustness to both adversarial perturbations and character-level noise.

## Executive Summary
This paper introduces a hybrid transformer-based framework for phishing email detection that integrates adversarial training and Explainable AI. The approach combines DistilBERT with FGM-based adversarial training and character-level noise augmentation to improve robustness against both adversarial and noisy inputs. The framework also employs LIME, SHAP, and IG for feature attribution, and uses Flan-T5-Small to generate plain-language explanations from these attributions. Evaluation demonstrates superior robustness compared to single-method defenses, maintaining accuracy above 98% on clean data while reducing accuracy degradation from 43.5% to 4.9% under severe character-level noise.

## Method Summary
The framework uses DistilBERT-base-uncased with DistilBertTokenizerFast, fine-tuned on the zefang-liu/phishing-email-dataset (~18,650 samples) with a 70/15/15 train/val/test split. Hybrid robustness is achieved through (1) FGM perturbations on embeddings with ε=0.01 and λ=0.5 during training, and (2) character-level noise applied to 30% of training samples (~10% characters corrupted). LIME, SHAP, and IG provide token-level feature attributions, while Flan-T5-Small generates user-friendly explanations from top LIME tokens. The model is trained with class-weighted cross-entropy and evaluated on test sets with varying levels of character-level noise (0%-20%).

## Key Results
- The hybrid approach achieves 98.1% accuracy on clean data and 93.2% accuracy at 20% character-level noise
- Accuracy degradation is reduced from 43.5% (baseline) to 4.9% (hybrid approach) under severe noise
- The framework maintains superior performance on both in-domain and cross-domain datasets
- User feedback indicates high clarity and information focus of generated explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining gradient-based embedding perturbations (FGM) with stochastic character-level noise provides superior resilience to input manipulation than either method alone.
- **Mechanism:** FGM perturbs the continuous embedding space to smooth decision boundaries, while character-level noise forces the model to rely on robust contextual patterns rather than brittle surface tokens.
- **Core assumption:** The model learns to map noisy surface forms to stable internal representations, preventing overfitting to specific token spellings.
- **Evidence anchors:**
  - [abstract]: "combines DistilBERT with FGM-based adversarial training and character-level noise augmentation."
  - [section]: "By operating at different stages of the text processing pipeline, this hybrid strategy provides complementary protection." (Page 6)
  - [corpus]: "Every Character Counts: From Vulnerability to Defense..." (arXiv:2509.20589) supports the efficacy of character-level analysis in phishing defense.
- **Break condition:** If the character-level corruption rate exceeds 20% and destroys semantic structure, accuracy may collapse.

### Mechanism 2
- **Claim:** Adversarial training stabilizes the feature attributions generated by Explainable AI (XAI) methods, making explanations more reliable under attack.
- **Mechanism:** Smoothing the loss landscape via FGM reduces variance of gradients and local approximations, preventing XAI tools from hallucinating importance on spurious tokens when input is perturbed.
- **Core assumption:** Stable decision boundaries imply stable internal feature importance rankings, even when input surface form changes.
- **Evidence anchors:**
  - [abstract]: "framework also integrates LIME, SHAP, and IG for feature attribution."
  - [section]: "The proposed hybrid framework... encourages reliance on more generalizable contextual representations rather than brittle surface forms." (Page 15, Discussion)
  - [corpus]: Corpus evidence specifically linking adversarial training to XAI stability is weak; related papers focus on detection accuracy.
- **Break condition:** If XAI method relies on sampling that conflicts with specific noise distribution, explanations might remain inconsistent regardless of model robustness.

### Mechanism 3
- **Claim:** Instruction-tuned small language models (Flan-T5-Small) can generate coherent security narratives when conditioned on extracted feature tokens.
- **Mechanism:** The framework converts structured data (top LIME tokens, confidence scores) into a natural language prompt, leveraging the LLM's pre-trained instruction-following capabilities to map keywords to phishing archetypes.
- **Core assumption:** The instruction-tuned model possesses sufficient parametric knowledge to correctly interpret isolated tokens as phishing cues without additional fine-tuning.
- **Evidence anchors:**
  - [abstract]: "uses Flan-T5-Small to generate plain-language explanations."
  - [section]: "LIME-based token attributions are selected... mapped into a structured, tactic-specific prompt." (Page 8)
  - [corpus]: "Phishing Email Detection Using Large Language Models" (arXiv:2512.10104) explores LLM usage in this domain, though primarily for detection rather than explanation.
- **Break condition:** If extracted features are incorrect, the LLM will confidently generate a persuasive but misleading justification.

## Foundational Learning

- **Concept:** **Gradient-based Adversarial Training (FGM/FGSM)**
  - **Why needed here:** Standard transformers overfit to training distribution; understanding how to perturb embeddings via $\nabla_e L$ is required to implement the robustness layer.
  - **Quick check question:** How does adding normalized gradient noise $\delta = \epsilon \cdot \nabla_e L / \|\nabla_e L\|_2$ to the embedding differ from standard data augmentation?

- **Concept:** **Local Surrogate Models (LIME)**
  - **Why needed here:** The framework relies on LIME to identify "evidence tokens"; you must understand that LIME approximates the complex model locally with a linear interpretable model.
  - **Quick check question:** Why might LIME attributions change significantly if the model's decision boundary is not locally linear?

- **Concept:** **Prompt Engineering for Generation**
  - **Why needed here:** The quality of the natural language explanation depends entirely on how prediction results are serialized into the Flan-T5 prompt.
  - **Quick check question:** If the model outputs "PHISHING (0.99)", what specific tokens must be included in the prompt to generate a grounded explanation?

## Architecture Onboarding

- **Component map:** Raw Email Text -> Tokenizer (DistilBertTokenizerFast) -> DistilBERT (Transformer Encoder) -> Linear Classifier -> Softmax -> FGM Perturbation Module (Training only) + Character Noise Injector -> LIME Explainer -> Prompt Template -> Flan-T5-Small

- **Critical path:**
  1. **Training:** Clean Forward -> Compute Gradient -> Add FGM Perturbation -> Adversarial Forward -> Joint Loss ($L_{clean} + \lambda L_{adv}$)
  2. **Inference:** Tokenize -> Classify -> Extract Top-N LIME tokens -> Format Prompt -> Generate Explanation

- **Design tradeoffs:**
  - **DistilBERT vs. BERT:** 40% smaller/faster but loses some depth in attention layers (97% performance retention claimed)
  - **FGM vs. DeepFool/PGD:** FGM is a "single-step" attack (fast), whereas multi-step attacks might offer stronger robustness but cost 3-5x training time
  - **LIME vs. SHAP for Prompts:** The paper chooses LIME for prompts because SHAP attributions were more "distributed" and harder to fit into a concise prompt context

- **Failure signatures:**
  - **High False Positives:** Legitimate urgent alerts trigger high attribution on words like "account" and "security"
  - **Tokenization Artifacts:** Character noise creates subword fragments (e.g., "termmination" -> "term", "##mina", "##tion"), which may confuse XAI attributions or dilute the explanation prompt

- **First 3 experiments:**
  1. **Baseline vs. Robust Accuracy:** Train standard DistilBERT vs. proposed framework and evaluate accuracy on test set injected with 20% character-level noise
  2. **Explanation Stability Check:** Take a single email, apply 5 different random character perturbations, and measure variance in LIME token attribution rankings
  3. **Prompt Ablation:** Generate explanations using only top-3 tokens vs. top-10 tokens to see if Flan-T5-Small hallucinates less context with fewer (but more precise) features

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be extended to effectively detect multilingual and multimodal phishing attacks, such as those utilizing images or QR codes?
- **Basis in paper:** Section 6.1 proposes "extensions to multilingual and multimodal phishing detection, including image- and QR-code-based attacks."
- **Why unresolved:** Current study explicitly limits dataset to English text and excludes image-based scenarios, creating gap in handling diverse attack vectors like malicious QR codes.
- **What evidence would resolve it:** Experimental results demonstrating model's performance on datasets containing non-English emails and image-embedded phishing payloads.

### Open Question 2
- **Question:** Does incorporating continuous adaptive learning via an adversarial training loop maintain long-term robustness against evolving linguistic strategies?
- **Basis in paper:** Section 6.1 suggests future work on "incorporating continuous adaptive learning via an adversarial training loop to maintain long-term robustness."
- **Why unresolved:** Section 5.1 notes current robustness mechanisms "do not fully capture... socially engineered linguistic strategies," and static model may struggle with continuously evolving adversaries.
- **What evidence would resolve it:** A longitudinal study measuring performance drift and robustness retention when model is dynamically updated against new attack campaigns.

### Open Question 3
- **Question:** Can the framework's robustness against semantic manipulation be enhanced using paraphrasing or back-translation-based adversarial example generation?
- **Basis in paper:** Section 6.1 recommends "exploring paraphrasing and back-translation-based adversarial example generation to enhance robustness against linguistic manipulation."
- **Why unresolved:** Current hybrid approach relies on character-level noise and embedding perturbations, which may not fully defend against attacks that preserve surface form but alter semantic meaning.
- **What evidence would resolve it:** Comparative analysis of model's accuracy and stability when attacked by semantic paraphrasing techniques versus character-level perturbations used in current study.

## Limitations

- **Character-level corruption granularity** remains underspecified - exact operationalization of 10% corruption rate applied to 30% of samples is unclear
- **XAI attribution reliability under attack** is asserted but not rigorously validated - lacks quantitative stability metrics under different noise levels
- **Prompt engineering brittleness** poses fundamental limitation - no analysis of failure cases where selected tokens are semantically ambiguous or explanation trustworthiness

## Confidence

- **High Confidence:** Claims about hybrid training methodology (combining FGM with character-level noise) are well-supported by implementation description and evaluation results
- **Medium Confidence:** Claims about superior robustness compared to single-method defenses are supported by accuracy metrics but lack ablation studies proving specific contribution of each component
- **Low Confidence:** Claims about explanation stability and trustworthiness under adversarial conditions are weakly supported, relying primarily on qualitative examples without systematic error analysis or user validation

## Next Checks

1. **Reproduce the robustness gap:** Train a standard DistilBERT baseline and the proposed hybrid model on identical data splits, then evaluate both on test sets with 0%, 5%, 10%, and 20% character-level noise to verify the claimed accuracy degradation difference (43.5% → 4.9%).

2. **Quantify explanation stability:** For a fixed set of test emails, apply 10 different random character perturbations (5% corruption each) and measure the Jaccard similarity of LIME top-5 token sets across perturbations. Compare this stability metric between the robust and non-robust models.

3. **Analyze false positive cases:** Manually examine the top 50 false positive predictions from both models under 20% noise conditions. Document whether the explanations (LIME tokens → Flan-T5 narrative) provide meaningful insight into the misclassification or simply generate plausible-sounding but incorrect justifications.