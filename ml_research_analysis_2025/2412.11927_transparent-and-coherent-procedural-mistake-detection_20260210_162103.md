---
ver: rpa2
title: Transparent and Coherent Procedural Mistake Detection
arxiv_id: '2412.11927'
source_url: https://arxiv.org/abs/2412.11927
tags:
- questions
- question
- vlms
- wang
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reformulates procedural mistake detection (PMD) to require
  transparent, justified decisions via visual self-dialog rationales. Using vision-language
  models (VLMs), the approach generates iterative yes-no questions about egocentric
  video frames to support binary success/mistake classification.
---

# Transparent and Coherent Procedural Mistake Detection

## Quick Facts
- arXiv ID: 2412.11927
- Source URL: https://arxiv.org/abs/2412.11927
- Reference count: 40
- Primary result: VLM accuracy, coherence, and efficiency improve with coherence-based question selection and fine-tuning, but trade-offs remain

## Executive Summary
This paper reformulates procedural mistake detection (PMD) to require transparent, justified decisions via visual self-dialog rationales. Using vision-language models (VLMs), the approach generates iterative yes-no questions about egocentric video frames to support binary success/mistake classification. To evaluate rationale coherence, two automated metrics—relevance and informativeness—are defined using a fine-tuned natural language inference model. Experiments with InstructBLIP, LLaVA, Llama 3, and GPT-4o show that VLMs struggle off-the-shelf but improve in accuracy, coherence, and efficiency when coherence-based question selection and fine-tuning are applied, albeit with trade-offs. Multi-faceted coherence metrics also enable detailed diagnosis of VLM behaviors, revealing failures in visual perception, reasoning, and interpretation.

## Method Summary
The method introduces a visual self-dialog rationale framework for transparent procedural mistake detection. VLMs iteratively generate and answer yes-no questions about egocentric video frames, using the resulting rationales to classify success or mistake. Two automated metrics—relevance and informativeness—are defined using a fine-tuned NLI model to assess the coherence of generated rationales. The approach is evaluated across multiple VLMs, with fine-tuning and coherence-based question selection applied to improve performance. Experimental results highlight both improvements and persistent trade-offs in accuracy, coherence, and efficiency.

## Key Results
- VLMs struggle off-the-shelf for procedural mistake detection with visual self-dialog rationales
- Accuracy, coherence, and efficiency improve with coherence-based question selection and fine-tuning, but trade-offs remain
- Coherence metrics enable diagnosis of VLM failures in visual perception, reasoning, and interpretation

## Why This Works (Mechanism)
The visual self-dialog rationale framework forces VLMs to explicitly reason about procedural steps by generating and answering yes-no questions about video frames. This iterative questioning process surfaces the model's reasoning, enabling transparent justification of success/mistake classifications. The coherence metrics (relevance and informativeness) provide automated, multi-faceted assessment of rationale quality, guiding model improvements and revealing failure modes. Fine-tuning and coherence-based question selection further enhance model performance by focusing on high-quality, relevant rationales.

## Foundational Learning
- **Procedural Mistake Detection (PMD)**: Identifying errors in task execution via video; needed to enable reliable, automated quality control in real-world applications; quick check: can the model detect a missing step or wrong action in a procedural video?
- **Vision-Language Models (VLMs)**: Models that process both visual and textual inputs; needed to interpret egocentric video frames and generate natural language rationales; quick check: does the VLM correctly describe objects and actions in a frame?
- **Natural Language Inference (NLI)**: Determining if one statement logically follows from another; needed to assess coherence (relevance and informativeness) of rationales; quick check: does the NLI model correctly judge entailment between rationale and task description?
- **Egocentric Video**: First-person video capturing task execution; needed to ground PMD in realistic, task-specific contexts; quick check: is the video clear enough to discern task steps and mistakes?

## Architecture Onboarding
- **Component Map**: VLM -> Question Generator -> NLI-based Coherence Evaluator -> Rationale Assembler -> Binary Classifier
- **Critical Path**: Visual input → Iterative question generation → Coherence evaluation → Rationale assembly → Mistake classification
- **Design Tradeoffs**: Coherence-based question selection improves rationale quality but increases computational cost; fine-tuning boosts accuracy but may overfit to training data; no single configuration dominates across all metrics
- **Failure Signatures**: Identified failures in visual perception (missing objects/actions), reasoning (incorrect causal links), and interpretation (misunderstanding task goals)
- **First Experiments**: (1) Compare off-the-shelf VLMs with fine-tuned versions on procedural mistake detection accuracy; (2) Evaluate coherence metrics (relevance, informativeness) on generated rationales; (3) Test the impact of coherence-based question selection on efficiency and quality

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on synthetic data and off-the-shelf VLMs without full domain adaptation
- Coherence metrics are based on a fine-tuned NLI model, introducing potential bias and limiting external validity
- Trade-offs between accuracy, coherence, and efficiency persist; no single configuration dominates
- Scalability to real-world egocentric video streams remains untested

## Confidence
- Primary result: Medium (performance gains depend on fine-tuning and coherence-based selection, may not generalize)
- Coherence metrics: Medium (themselves based on fine-tuned NLI model, potential bias)
- Scalability and real-world robustness: Low (not tested beyond initial experiments)

## Next Checks
1. Test the visual self-dialog rationale approach on diverse, real-world egocentric video datasets beyond the initial experimental setup to assess robustness.
2. Validate the coherence metrics (relevance and informativeness) against human expert judgments to confirm their reliability and minimize model-induced bias.
3. Conduct ablation studies isolating the impact of each component (question generation, coherence-based selection, fine-tuning) to quantify their individual contributions and identify potential bottlenecks.