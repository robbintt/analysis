---
ver: rpa2
title: Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social
  Media
arxiv_id: '2510.08365'
source_url: https://arxiv.org/abs/2510.08365
tags:
- suicide
- bert
- arxiv
- preprint
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses suicide risk detection on social media, particularly
  handling implicit suicidal ideation expressed indirectly through metaphor or sarcasm.
  It introduces a two-stage voting architecture that routes high-confidence explicit
  cases through a fine-tuned BERT classifier while escalating ambiguous inputs to
  either a multi-perspective LLM ensemble or a feature-based ML ensemble guided by
  psychologically grounded indicators.
---

# Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media

## Quick Facts
- **arXiv ID:** 2510.08365
- **Source URL:** https://arxiv.org/abs/2510.08365
- **Reference count:** 40
- **Primary result:** Achieves 98.0% F1 on explicit cases, 99.7% on implicit ones, with <2% cross-domain gap while reducing computational costs.

## Executive Summary
This work introduces a two-stage voting architecture for detecting suicide risk on social media, specifically designed to handle both explicit and implicit suicidal ideation. The approach uses a lightweight BERT classifier in Stage 1 to resolve high-confidence explicit cases, while escalating ambiguous or long texts to Stage 2 for more sophisticated analysis. Stage 2 employs either a multi-perspective LLM ensemble or a feature-based ML ensemble using psychologically grounded indicators. Evaluated on Reddit and DeepSuiMind datasets, the method achieves strong performance across both explicit and implicit domains while significantly reducing computational costs compared to end-to-end LLM approaches.

## Method Summary
The framework employs a cascaded architecture where a fine-tuned BERT classifier first filters high-confidence explicit cases based on confidence scores and text length thresholds. Ambiguous or long texts are escalated to Stage 2, which offers two paths: a multi-perspective LLM ensemble (Bearish/Bullish/Expert agents) that maximizes recall on implicit ideation, or a feature-based ML ensemble using structured vectors of psychological indicators (e.g., suicide intent, metaphor detection, distress level) extracted by an LLM. The system uses convex optimization to weight ensemble components, constraining BERT's influence to prevent bias toward explicit patterns. The approach operationalizes clinical constructs as structured features to improve cross-domain generalization.

## Key Results
- Achieves 98.0% F1 score on explicit cases and 99.7% F1 on implicit cases
- Reduces cross-domain performance gap below 2% between Reddit and DeepSuiMind datasets
- Significantly lowers computational costs by resolving ~67.6% of cases in Stage 1
- LLM voting achieves 99.97% F1 on implicit-only DeepSuiMind dataset
- Feature-based ML ensemble (99.72% F1) approaches LLM performance with lower latency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Routing inputs based on classifier confidence and text length preserves efficiency for explicit cases while reserving computational resources for ambiguous ones.
- **Mechanism:** A Stage 1 BERT classifier filters high-confidence, short texts (typically explicit ideation), resolving them immediately. Ambiguous or long texts are escalated to Stage 2. This prevents wasting expensive LLM inference on simple cases that lightweight models already handle well.
- **Core assumption:** Confidence scores from the fine-tuned BERT model correlate strongly with case explicitness and classification difficulty.
- **Evidence anchors:** [abstract] "In Stage 1, a lightweight BERT classifier rapidly resolves high-confidence explicit cases." [section 3.3.1] "...short posts with high-confidence predictions are accepted directly, while... long posts or those with ambiguous probabilities are forwarded to Stage 2."
- **Break condition:** If the BERT confidence threshold is set too aggressively, implicit cases with high lexical overlap may be prematurely classified (false negatives), or the router may fail to defer cases where BERT is confidently wrong.

### Mechanism 2
- **Claim:** Operationalizing unstructured text as psychologically grounded feature vectors improves generalization to implicit domains.
- **Mechanism:** Instead of relying on raw embeddings, a prompt-engineered LLM extracts structured indicators (e.g., *suicide_intent*, *is_metaphor*, *distress_level*) into JSON. These vectors are fed into classical ML models (Random Forest, XGBoost). This forces the model to reason via clinical constructs rather than superficial keyword patterns, reducing overfitting to the source domain.
- **Core assumption:** The prompt-engineered LLM can reliably extract these psychological states from nuanced text, and these states are predictive of risk even when implicit.
- **Evidence anchors:** [abstract] "...operationalizes LLM-extracted psychological features as structured vectors... reducing the cross-domain performance gap below 2%." [section 4.3] "On DeepSuiMind... fundamental features excel cross-domain: LinearSVC/XGBoost... exceed 99%, outperforming BERT (93.9%)."
- **Break condition:** If the extraction LLM hallucinates features or fails to capture specific implicit markers, the downstream ML models will receive noisy input, degrading performance.

### Mechanism 3
- **Claim:** Multi-perspective prompt ensembling (Bearish/Bullish/Expert) stabilizes recall for implicit ideation by covering diverse risk interpretations.
- **Mechanism:** Three LLM agents with distinct "personas" analyze the text. A "Bullish" agent prioritizes safety (high recall), a "Bearish" agent prioritizes evidence (high precision), and an "Expert" balances both. Voting aggregates these biases, mitigating the instability typically seen when a single LLM prompt encounters ambiguous inputs.
- **Core assumption:** The errors of the three prompting strategies are uncorrelated or complementary, allowing voting to smooth out extreme outputs.
- **Evidence anchors:** [abstract] "...multi-perspective LLM voting framework to maximize recall on implicit ideation." [section 4.4] "LLM voting... achieves the best generalization to implicit cases (99.97% [F1 on DeepSuiMind])."
- **Break condition:** If the prompts are too highly correlated, voting provides no benefit over a single agent. If the "Bullish" agent dominates the vote, precision may drop unacceptably in deployment.

## Foundational Learning

- **Concept:** **Cascade Inference / Selective Routing**
  - **Why needed here:** The architecture relies on the premise that not all inputs require the same computational depth. Understanding the trade-off between the latency of Stage 2 and the accuracy of Stage 1 is critical.
  - **Quick check question:** If Stage 1 confidence thresholds are lowered, what happens to the system's latency and the Stage 2 load?

- **Concept:** **Feature Engineering vs. Representation Learning**
  - **Why needed here:** The paper explicitly contrasts BERT's dense embeddings with hand-crafted, LLM-extracted "fundamental features." You must understand why structured vectors (JSON features) might generalize better to out-of-domain (implicit) data than implicit embeddings.
  - **Quick check question:** Why would a Random Forest model trained on 6 psychological features outperform a BERT model trained on millions of parameters when tested on the DeepSuiMind dataset?

- **Concept:** **Prompt Engineering for Personas**
  - **Why needed here:** The Stage 2 LLM pathway depends on specific instructions (e.g., "Be ultra-sensitive" vs. "Be conservative") to manipulate the model's prior probability of positive classification.
  - **Quick check question:** How does the "Bullish" prompt change the LLM's behavior regarding false positives vs. false negatives?

## Architecture Onboarding

- **Component map:** Input -> Stage 1 Router (BERT + Length/Confidence Thresholds) -> Stage 2 (High Accuracy Path: LLM Ensemble + Voting OR Efficiency Path: Feature Extractor -> Structured Vector -> ML Ensemble + Convex Optimization Voting)
- **Critical path:** The flow from Input -> Stage 1 Router -> Stage 2 ML Voting is the critical path for a balanced production system (Efficiency + Robustness). The LLM Ensemble path is reserved for maximizing recall on the most difficult cases.
- **Design tradeoffs:**
  - **Stage 2 LLM vs. ML:** The LLM pathway yields higher recall on implicit data (99.94%) but is computationally expensive. The ML pathway (using LLM-extracted features) offers nearly equivalent F1 (99.72%) with lower latency after feature extraction.
  - **Convex Optimization:** The weights are capped (w_BERT <= 0.5) to prevent the system from defaulting to BERT's bias toward explicit signals.
- **Failure signatures:**
  - **Router Drift:** If implicit expressions drift to look more like explicit training data, Stage 1 may misclassify them as "Easy/Explicit" and bypass Stage 2, causing recall to drop.
  - **Extraction Failure:** If the Feature Extractor LLM outputs invalid JSON, the ML Ensemble path fails.
  - **Voting Gridlock:** In the LLM pathway, if the three agents disagree completely (1 vs 1 vs 1), the tie-breaker (BERT) relies on the exact component the system was trying to augment.
- **First 3 experiments:**
  1. **Threshold Sensitivity Analysis:** Vary the Stage 1 confidence thresholds (tau) to plot the curve of "Percent of data routed to Stage 2" vs. "Overall Recall."
  2. **Feature Ablation:** Retrain the Stage 2 ML models removing one psychological feature at a time (e.g., removing is_metaphor) to measure the marginal contribution of each clinical indicator.
  3. **Cross-Domain Stress Test:** Train the BERT router on Reddit data and test the routing behavior on the DeepSuiMind (implicit) set to verify that implicit cases are indeed being escalated to Stage 2 and not falsely resolved in Stage 1.

## Open Questions the Paper Calls Out
- Can adaptive, uncertainty-based routing allocation outperform the current static, handcrafted thresholds?
- Does replacing the character-length proxy with richer semantic embeddings for the "reasoning" feature improve detection accuracy?
- How does the model perform when validated on clinician-annotated datasets in a human-in-the-loop setting?

## Limitations
- The routing mechanism relies on unspecified confidence threshold values, creating uncertainty about optimal configuration
- The psychological feature extraction LLM model is not identified, limiting reproducibility and validation
- Cross-domain generalization claims are based on only two datasets, limiting external validity

## Confidence
- **High Confidence:** The two-stage architecture design and its efficiency benefits are well-supported by results
- **Medium Confidence:** The multi-perspective prompt ensembling approach for implicit ideation shows strong performance but has limited domain-specific validation
- **Low Confidence:** The exact psychological feature extraction process and its reliability across different implicit expression styles remain unclear

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary the BERT confidence thresholds to determine the optimal balance between Stage 1 efficiency and Stage 2 escalation, measuring the impact on overall recall and false negative rates.
2. **Cross-Domain Routing Validation:** Test the Stage 1 router trained on Reddit data with the DeepSuiMind implicit dataset to verify that implicit cases are properly escalated to Stage 2 rather than being prematurely resolved.
3. **Feature Extraction Robustness:** Evaluate the LLM feature extraction component's performance when processing texts with different implicit expression styles (sarcasm, metaphor, cultural references) to ensure the structured vectors remain predictive across diverse implicit ideation patterns.