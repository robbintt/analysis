---
ver: rpa2
title: 'Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation
  Tasks: A Case Study Using Banks Public Disclosures'
arxiv_id: '2507.21360'
source_url: https://arxiv.org/abs/2507.21360
tags:
- condition
- task
- time
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of AI RAG tools in assisting
  analysts with information extraction and data annotation from complex financial
  documents. Using a within-subjects design with three professional financial analysts,
  the research compares human-only annotation against two AI-assisted conditions (naive
  and interactive) on a real-world task involving thousands of pages of public disclosure
  documents from global systemically important banks.
---

# Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures

## Quick Facts
- **arXiv ID:** 2507.21360
- **Source URL:** https://arxiv.org/abs/2507.21360
- **Reference count:** 7
- **Primary result:** AI RAG tools reduced task completion time by up to 90% and improved accuracy to 65.9% agreement vs 52.9% baseline

## Executive Summary
This study evaluates AI RAG tools for information extraction from complex financial documents, specifically bank public disclosures. Using three professional financial analysts in a within-subjects design, the research compares human-only annotation against two AI-assisted conditions (naive and interactive) across thousands of pages of disclosure documents.

The findings demonstrate substantial productivity gains with up to 90% time reduction in the naive condition and 70% in the interactive condition. Accuracy improved significantly in the interactive AI+ condition, reaching 65.9% agreement compared to 52.9% in the human-only baseline. Analyst proficiency with AI tools emerged as a critical factor, with the most experienced analyst achieving a 41.7 percentage point accuracy improvement.

## Method Summary
The study employed a within-subjects experimental design with three professional financial analysts who completed information extraction and data annotation tasks using three different conditions: human-only annotation, naive AI assistance, and interactive AI assistance. The AI tool was implemented using OpenWebUI with Claude 3.5 Sonnet. Analysts worked with thousands of pages of public disclosure documents from global systemically important banks, performing the same annotation tasks under each condition to enable direct comparison of time efficiency and accuracy.

## Key Results
- Time reduction of up to 90% in naive AI condition and 70% in interactive AI condition
- Accuracy improvement to 65.9% agreement in interactive AI+ vs 52.9% baseline
- Individual analyst proficiency with AI tools significantly impacted outcomes (41.7 percentage point improvement for most experienced user)

## Why This Works (Mechanism)
The AI RAG tool's effectiveness stems from its ability to rapidly process and extract information from large volumes of complex financial documents, reducing the manual search and extraction burden on analysts. The interactive condition allows analysts to refine AI outputs through follow-up prompts and document verification, creating a human-in-the-loop system that leverages both AI speed and human judgment. The naive condition demonstrates the raw productivity gains from AI assistance without analyst intervention, while the interactive condition shows how human expertise can enhance AI outputs for improved accuracy.

## Foundational Learning
**Information Extraction** - The process of automatically extracting structured information from unstructured text; needed to understand how AI tools can identify and categorize relevant data points from financial disclosures; quick check: verify the tool can accurately identify key financial metrics from sample documents.

**RAG (Retrieval-Augmented Generation)** - Combines information retrieval with language model generation to provide context-aware responses; needed to understand how the AI tool accesses and utilizes document content; quick check: test the tool's ability to retrieve specific passages from large document sets.

**Inter-annotator Agreement** - A metric measuring consistency between different annotators; needed to evaluate annotation quality and reliability; quick check: calculate agreement rates between human-only and AI-assisted conditions.

## Architecture Onboarding

**Component Map:** Documents -> Retrieval System -> Language Model -> Output -> Analyst Review

**Critical Path:** Document ingestion → semantic search/retrieval → context-aware generation → analyst verification/refinement → final annotation

**Design Tradeoffs:** The system balances automation (speed) against accuracy (human verification), with the interactive condition adding time for refinement but improving quality. The naive condition prioritizes speed while the interactive condition emphasizes accuracy through human oversight.

**Failure Signatures:** Potential failures include hallucination of non-existent information, retrieval of irrelevant passages, over-reliance on AI outputs without verification, and analyst fatigue from reviewing AI suggestions. The interactive condition may also introduce cognitive overhead if analysts must frequently correct AI errors.

**First Experiments:** 1) Time trials comparing naive vs interactive conditions on identical document sets; 2) Accuracy benchmarking using known ground truth data; 3) User experience assessment measuring analyst satisfaction and perceived workload across conditions.

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of three analysts limits generalizability
- Task focused specifically on financial disclosures from global systemically important banks
- Evaluation relied on inter-annotator agreement rather than external ground truth validation

## Confidence
- **High confidence:** The reported time savings (90% naive, 70% interactive) are robust given the controlled experimental conditions and clear measurement methodology
- **Medium confidence:** The accuracy improvements (65.9% vs 52.9% agreement) are suggestive but require larger-scale validation due to small sample size and limited statistical significance testing
- **Medium confidence:** The individual variation in AI tool proficiency is well-documented within the study but needs broader testing across different user populations

## Next Checks
1. Conduct a larger-scale study with 20+ analysts across multiple industries to validate the generalizability of time and accuracy improvements
2. Implement external ground truth validation using domain experts unfamiliar with the annotation task to verify accuracy claims
3. Test the interactive AI+ condition with varying levels of analyst guidance to determine optimal prompt refinement strategies and quantify the marginal benefit of follow-up prompts