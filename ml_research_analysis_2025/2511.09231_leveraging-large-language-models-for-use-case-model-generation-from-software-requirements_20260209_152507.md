---
ver: rpa2
title: Leveraging Large Language Models for Use Case Model Generation from Software
  Requirements
arxiv_id: '2511.09231'
source_url: https://arxiv.org/abs/2511.09231
tags:
- case
- modeling
- requirements
- participants
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how Large Language Models (LLMs) can assist
  in generating use case models from software requirements, addressing the time-consuming
  and expertise-demanding manual process. A semi-automated method was developed using
  prompt engineering and the Llama 3.1 70B model to extract actors and use cases from
  textual requirements.
---

# Leveraging Large Language Models for Use Case Model Generation from Software Requirements

## Quick Facts
- arXiv ID: 2511.09231
- Source URL: https://arxiv.org/abs/2511.09231
- Authors: Tobias Eisenreich; Nicholas Friedlaender; Stefan Wagner
- Reference count: 32
- Primary result: LLM-based semi-automated method reduced use case modeling time by 60% while maintaining comparable quality to manual approaches

## Executive Summary
This study investigates how Large Language Models can automate the extraction of use case models from textual software requirements. The researchers developed a semi-automated method using prompt engineering with the Llama 3.1 70B model to extract actors and use cases, producing PlantUML diagrams. An exploratory study with five professional software engineers demonstrated that the LLM-based approach significantly reduced modeling time while maintaining quality comparable to manual methods. Participants valued the tool for accessibility and guidance, though they noted limitations in handling iterative refinement and domain adaptability.

## Method Summary
The method employs a three-step semi-automated workflow: (1) extract all actors from requirements text, (2) extract use cases that relate to validated actors, and (3) generate the complete use case model in PlantUML syntax. At each step, users validate and refine the LLM-generated output before proceeding to the next stage. The approach uses three prompt engineering patterns: role prompting (instructing the LLM to act as a software engineering expert), knowledge injection (providing PlantUML syntax rules), and negative prompting (avoiding common errors). The workflow requires access to the Llama 3.1 70B model and can be deployed locally or via API, with intermediate validation points to catch hallucinations and syntax errors.

## Key Results
- 60% reduction in modeling time compared to manual approaches
- Comparable model quality (precision, recall, F1) to manual methods
- Participants found the tool valuable for accessibility and guidance in modeling tasks

## Why This Works (Mechanism)

### Mechanism 1
Sequential extraction of actors → use cases → model with intermediate validation points reduces error propagation and improves final quality. The workflow decomposes a complex modeling task into discrete stages with user validation at each point, preventing hallucinated entities from contaminating downstream steps. Core assumption: Users can accurately identify incorrect extractions even if they couldn't generate correct outputs from scratch. Evidence: Authors opted for semi-automated workflow because automated processes frequently resulted in hallucinations and inaccurate PlantUML syntax. Break condition: If validation burden causes user fatigue or if sequential dependencies prevent parallel optimization without quality loss.

### Mechanism 2
Composing multiple prompt engineering patterns (role prompting + knowledge injection + negative prompting) yields more accurate PlantUML output than unstructured prompting. Role prompting establishes domain expertise context, knowledge injection provides specific syntax rules, and negative prompting suppresses known error patterns, together constraining the output space toward valid models. Core assumption: The three patterns interact positively and their combined effect exceeds using any single pattern. Evidence: Prompt patterns were explicitly implemented with role prompting instructing LLM to act as software engineering expert, knowledge injection providing PlantUML syntax, and negative prompting helping to avoid common errors. Break condition: If patterns over-constrain outputs, preventing valid domain-specific variations, or if pattern effectiveness doesn't generalize across requirement styles.

### Mechanism 3
Semi-automated human-in-the-loop architecture produces higher-quality outputs than fully automated generation because LLMs lack reasoning to verify semantic alignment with requirements. The tool generates candidate outputs but delegates verification to humans, catching hallucinations and domain mismatches that the LLM cannot self-detect. Core assumption: Humans provide semantic grounding that LLMs lack; the cost of validation is lower than the cost of correcting fully automated errors. Evidence: Participants made recurrent adjustments to intermediate or final outputs, deleting unnecessary actors, editing use case titles, or refining the final output. Break condition: If users develop over-reliance and reduce critical scrutiny, or if validation interfaces don't surface the most likely error modes.

## Foundational Learning

- **Use Case Model Components**
  - Why needed here: The system extracts four specific elements—actors, use cases, system boundaries, and associations. Without understanding what constitutes a valid actor (external entities) vs. use case (user-centric goals), you cannot validate intermediate outputs.
  - Quick check question: Given a requirements paragraph about a banking app, can you distinguish between "Customer" (actor), "Transfer Money" (use case), and "Transaction Database" (internal component, not an actor)?

- **LLM Non-Determinism and Hallucination**
  - Why needed here: Participants reported "inconsistency across iterations" and the authors explicitly cite hallucination as the reason for choosing semi-automation. Understanding that identical prompts can yield different outputs is critical for designing validation workflows.
  - Quick check question: If you run the same extraction prompt twice on identical requirements and get different actor lists, is this a bug or expected behavior?

- **Prompt Engineering Patterns (Role, Knowledge Injection, Negative)**
  - Why needed here: The method's effectiveness is attributed to three specific patterns. To adapt or improve the system, you need to understand what each pattern contributes and how to modify them.
  - Quick check question: What type of error would "Negative Prompting" address that "Knowledge Injection" would not?

## Architecture Onboarding

- **Component map**: Requirements text → Actor extraction (LLM call 1) → User validation → Use case extraction (LLM call 2) → User validation → Model generation (LLM call 3) → User validation → PlantUML output

- **Critical path**: Requirements text → Actor extraction (LLM call 1) → User validation → Use case extraction (LLM call 2) → User validation → Model generation (LLM call 3) → User validation → PlantUML output

- **Design tradeoffs**:
  - Semi-automated vs. fully automated: Authors chose semi-automated because full automation caused hallucinations and syntax errors. Tradeoff: slower throughput but higher accuracy.
  - Sequential vs. parallel extraction: Sequential (actors first, then use cases) enforces dependency constraints but prevents parallel LLM calls. Assumption: use cases must reference validated actors.
  - Open-weight model (Llama 3.1 70B): Enables local deployment and customization but requires infrastructure. Tradeoff not explicitly justified vs. proprietary APIs.

- **Failure signatures**:
  - Hallucinated entities: Actors or use cases not grounded in the requirements text (caught at validation stages)
  - Invalid PlantUML syntax: Diagram fails to render (addressed by knowledge injection, but still possible)
  - Missing relationships: Use cases not linked to any actor (possible if use case extraction ignores actor constraints)
  - Inconsistent outputs: Same input yields different results across runs (inherent LLM non-determinism)
  - Domain mismatch: Generic use cases that ignore domain-specific terminology (reported by participants)

- **First 3 experiments**:
  1. **Prompt pattern ablation**: Run extraction with (a) no patterns, (b) only role prompting, (c) all three patterns on identical requirements. Measure precision/recall against ground truth to isolate each pattern's contribution.
  2. **Scalability test**: Apply the method to requirement documents of increasing length (1 page, 5 pages, 20 pages). Monitor for performance degradation and new failure modes, as the paper explicitly notes results may not generalize to larger texts.
  3. **Validation accuracy assessment**: Track what percentage of intermediate outputs users modify. If modification rates are very low, test whether validation can be selectively applied only to low-confidence extractions without quality loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LLM-based method maintain efficiency and quality when applied to significantly larger and more complex industrial requirement specifications?
- Basis in paper: [explicit] The authors state in the "Threats to Validity" that results cannot be generalized to extensive requirements because the study used crafted texts and LLM performance across varying input lengths is "not well understood."
- Why unresolved: The study only tested on specific, managed texts, leaving the tool's ability to handle the volume and ambiguity of real-world industrial documents unknown.
- What evidence would resolve it: An evaluation of the tool using unprocessed, large-scale requirements documents from industrial software projects.

### Open Question 2
- Question: How can the method be extended to support the iterative and collaborative evolution of requirements?
- Basis in paper: [explicit] The authors note in the Conclusion and Discussion that the current method lacks support for the "iterative dialogue" typical of professional engineering, as models usually evolve alongside requirements rather than in a single session.
- Why unresolved: The current workflow assumes a static set of requirements and does not support the dynamic refinement process identified as critical by participants.
- What evidence would resolve it: A longitudinal study or new tool features that allow for incremental updates to the use case model as the underlying requirements change.

### Open Question 3
- Question: Are the observed efficiency gains consistent across a larger and more diverse population of software engineers?
- Basis in paper: [explicit] The authors identify the "limited sample size" of five participants as a threat to validity and explicitly call for "future research" to use a larger participant pool to enhance generalizability.
- Why unresolved: The small, homogeneous sample (avg. 7 years experience) may not capture the variability in modeling skills or the challenges faced by novices.
- What evidence would resolve it: A large-scale experiment involving a statistically significant number of participants with varying levels of expertise.

## Limitations
- Results cannot be generalized to extensive requirements documents, as the study used only two crafted requirement texts
- The evaluation focused solely on modeling time reduction and intermediate entity quality, not on overall system quality of final use case models
- Small sample size (five participants) limits generalizability across different populations of software engineers

## Confidence

- **High confidence**: The reported 60% time reduction and precision/recall metrics for actors and use cases, as these are direct measurements from controlled experiments with defined ground truth
- **Medium confidence**: The claimed necessity of semi-automation, as this is supported by qualitative observations and participant feedback but lacks quantitative comparison with fully automated approaches
- **Low confidence**: The generalizability of findings to different domains, requirement styles, or larger document sets, given the small sample size and limited diversity of test cases

## Next Checks

1. **Scalability boundary test**: Systematically evaluate the method on requirement documents of increasing length (e.g., 1, 5, 10, 20+ pages) to identify when performance degrades and by what magnitude

2. **Fully automated baseline comparison**: Implement and evaluate a fully automated version using alternative prompt strategies (e.g., chain-of-thought prompting, iterative refinement) to quantify the actual cost/benefit tradeoff of human validation

3. **Long-term usability study**: Conduct a multi-week field study with the same participants to assess whether the validation burden becomes prohibitive over time and whether quality improvements persist with repeated use