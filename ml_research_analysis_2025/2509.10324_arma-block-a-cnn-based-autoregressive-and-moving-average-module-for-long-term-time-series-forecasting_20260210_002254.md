---
ver: rpa2
title: 'ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term
  Time Series Forecasting'
arxiv_id: '2509.10324'
source_url: https://arxiv.org/abs/2509.10324
tags:
- block
- forecasting
- arma
- series
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The ARMA block is a CNN-based module for long-term time series\
  \ forecasting, inspired by the ARIMA model. It uses two convolutional components\u2014\
  one for trend prediction (autoregression) and one for local variation refinement\
  \ (moving average)\u2014enabling direct multi-step forecasting."
---

# ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting

## Quick Facts
- arXiv ID: 2509.10324
- Source URL: https://arxiv.org/abs/2509.10324
- Authors: Myung Jin Kim; YeongHyeon Park; Il Dong Yun
- Reference count: 0
- Key outcome: A lightweight CNN module for long-term time series forecasting that decomposes trend and residual components, showing competitive accuracy especially on non-stationary data.

## Executive Summary
The ARMA block is a novel CNN-based module for long-term time series forecasting, inspired by the ARIMA model. It uses two convolutional components—one for trend prediction (autoregression) and one for local variation refinement (moving average)—enabling direct multi-step forecasting without iterative recursion. Experiments on nine benchmark datasets show competitive accuracy, particularly on trend-varying data, while maintaining simplicity. The block inherently encodes positional information without explicit embeddings, making it a lightweight alternative to positional encoding modules. Ablation studies confirm the importance of both AR and MA components. Overall, the ARMA block provides an efficient, robust, and adaptable solution for real-world time series forecasting.

## Method Summary
The ARMA block uses two convolutional layers: an autoregressive (AR) component that captures the overall trend and a moving average (MA) component that refines local variations. The AR layer processes the raw input, while the MA layer processes the residual (input minus AR output). Their outputs are summed to produce the final forecast. This design enables direct multi-step forecasting, avoiding iterative error accumulation. The block also inherently encodes positional information through padding, potentially eliminating the need for explicit positional embeddings. RevIN normalization can be applied to handle non-stationarity.

## Key Results
- Competitive forecasting accuracy on nine benchmark datasets, especially for trend-varying data
- Superior performance to simple CNN baselines (e.g., MSE 1.353 vs 1.699 on ILI dataset)
- Effective direct multi-step forecasting without iterative recursion
- Inherent positional encoding capability demonstrated through probe experiments
- Lightweight architecture with fewer parameters than transformer-based alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing forecasting into trend capture (AR) and residual refinement (MA) improves accuracy on non-stationary data.
- Mechanism: The AR component receives the raw input and learns the overall trend due to optimizer pressure toward larger gradients. The MA component receives the residual (input − AR output), which tends toward higher-frequency content, and corrects fine-grained errors. Final output is the sum: `y_out = y_ar + y_ma`.
- Core assumption: The input signal admits a meaningful separation into low-frequency trend and high-frequency residual components that can be modeled independently.
- Evidence anchors:
  - [abstract] "consists of two convolutional components: one for capturing the trend (autoregression) and the other for refining local variations (moving average)"
  - [section 2.1] "Due to the optimiser's nature, learning proceeds in the direction of the larger gradient, meaning the AR component learns the overall trend."
  - [section 3.4] Ablation shows ARMA (MSE 1.353, MAE 0.776) outperforms simple CNN (MSE 1.699, MAE 0.914) on ILI.
  - [corpus] Weak direct corpus support for this specific AR/MA decomposition in neural modules; neighbor papers mostly address classical ARIMA or unrelated hybrid models.
- Break condition: If input lacks discernible trend structure (e.g., near-white noise), the AR/MA split may not yield complementary signals and benefits can collapse.

### Mechanism 2
- Claim: Direct multi-step forecasting with high-dimensional filters avoids iterative error accumulation present in classical ARIMA.
- Mechanism: Instead of recursively feeding predictions back as inputs for subsequent steps, the ARMA block learns filters that simultaneously map the input sequence to the full output horizon, improving efficiency and multivariate extensibility.
- Core assumption: The learned filters can capture step-specific response characteristics without requiring explicit error-correction recursion.
- Evidence anchors:
  - [abstract] "Unlike conventional ARIMA, which requires iterative multi-step forecasting, the block directly performs multi-step forecasting, making it easily extendable to multivariate settings."
  - [section 2] "the ARMA block enhances efficiency by directly forecasting multiple steps, using a high-dimensional filter to predict the characteristics of each step."
  - [corpus] No direct corpus neighbor validates this direct multi-step filter design; related works focus on classical ARIMA or hybrid LSTM/ARIMA pipelines.
- Break condition: If target horizons exhibit strong conditional dependencies across steps that cannot be captured by the filter's receptive field, direct forecasting may underfit.

### Mechanism 3
- Claim: CNNs in the ARMA block encode absolute positional information through padding, reducing the need for explicit positional embeddings.
- Mechanism: Convolutions process features with boundary padding that introduces absolute position cues; a probe experiment with a frozen CNN and a trained linear Position Encoding Module (PEM) shows the CNN outputs can predict linear indices, gradations, and sinusoidal signals with low MAE (~0.0017–0.0019).
- Core assumption: Positional information is preserved through the convolution stack and can be decoded via a lightweight readout.
- Evidence anchors:
  - [abstract] "the block inherently encodes absolute positional information, suggesting its potential as a lightweight replacement for positional embeddings in sequential models."
  - [section 3.2] PEM trained on frozen CNN outputs successfully predicts positional encodings (Table 2; Fig. 3).
  - [section 1] "CNNs learn information about absolute positions from padding when learning each feature."
  - [corpus] No neighbor paper directly corroborates positional encoding via padding for time-series CNNs; the claim is anchored within the paper's own probe experiment.
- Break condition: If deeper stacks or pooling significantly dilute positional cues, explicit positional embeddings may become necessary again.

## Foundational Learning

- Concept: ARIMA decomposition (AR vs. MA terms)
  - Why needed here: The ARMA block is explicitly inspired by ARIMA's separation of autoregressive trend and moving-average error correction; understanding this clarifies why two CNNs are used.
  - Quick check question: Given a simple time series, can you hand-compute one AR step and one MA correction?

- Concept: Direct vs. recursive multi-step forecasting
  - Why needed here: The paper claims efficiency gains from direct multi-step prediction; practitioners must recognize when recursive error accumulation is a real risk in their problem.
  - Quick check question: For a 96-step horizon, what is the potential drawback of feeding each prediction back as input for the next step?

- Concept: Positional encoding in sequence models
  - Why needed here: The paper argues that CNNs inherently encode position, which is relevant when deciding whether to add explicit positional embeddings to sequential architectures.
  - Quick check question: What would happen to positional information in a CNN with aggressive pooling and large strides?

## Architecture Onboarding

- Component map: Input → RevIN → AR Conv → y_ar → Residual (x_in - y_ar) → MA Conv → y_ma → y_out = y_ar + y_ma → Denormalization

- Critical path:
  1. Input normalization (RevIN) → AR Conv → intermediate trend.
  2. Compute residual → MA Conv → residual correction.
  3. Sum outputs → denormalization → final forecast.

- Design tradeoffs:
  - Filter size: 5×5 used; larger filters increase receptive field but add parameters and may over-smooth.
  - Depth: The paper keeps it shallow (two convs); deeper stacks could capture longer dependencies but may obscure positional cues.
  - Horizons: Direct forecasting requires output-dimension-matched filters; very long horizons may need architectural scaling.

- Failure signatures:
  - Flat predictions on high-frequency data: MA component may be under-capacity; check filter count/size.
  - Lagged trend shifts: AR receptive field may be insufficient; increase filter size or input window.
  - Positional drift in long sequences: Positional cues may weaken; consider auxiliary positional signals.

- First 3 experiments:
  1. Replicate single-dataset ablation (ILI) comparing ARMA vs. simple CNN on MSE/MAE to confirm decomposition benefit.
  2. Probe positional encoding by freezing ARMA CNNs, training a linear PEM to predict index/gradation/sinusoid signals, and reproducing Table 2 metrics.
  3. Horizon sensitivity test: vary forecast horizons (96, 192, 336, 720) on one trend-shift-prone dataset (e.g., Exchange) and one stationary dataset to characterize where direct multi-step forecasting excels or degrades.

## Open Questions the Paper Calls Out
None

## Limitations
- The positional encoding claim lacks external validation beyond the authors' own probe experiment.
- The AR/MA decomposition's effectiveness assumes signals cleanly separate into trend and residual components, which may not hold for highly stochastic data.
- Direct multi-step forecasting efficiency gains are empirically demonstrated but lack theoretical guarantees about filter capacity across varying dependency structures.

## Confidence
- **High**: ARMA block architecture and basic ablation showing AR+MA > CNN alone (ILI dataset results)
- **Medium**: Claims about inherent positional encoding and direct multi-step forecasting efficiency
- **Low**: Generalization of positional encoding claims to other CNN architectures or deeper stacks

## Next Checks
1. Replicate positional encoding probe across different CNN depths (1-4 layers) and pooling configurations to test robustness of the padding-based position claim.
2. Test ARMA block on chaotic/brown noise datasets where trend/residual separation should fail, to establish break conditions for the AR/MA decomposition.
3. Compare error accumulation in recursive vs. direct forecasting across horizons where step dependencies are artificially manipulated (e.g., adding known lag correlations).