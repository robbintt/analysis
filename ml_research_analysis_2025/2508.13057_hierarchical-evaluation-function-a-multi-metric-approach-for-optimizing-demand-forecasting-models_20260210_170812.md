---
ver: rpa2
title: 'Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing Demand
  Forecasting Models'
arxiv_id: '2508.13057'
source_url: https://arxiv.org/abs/2508.13057
tags:
- maef
- improves
- evaluation
- optimized
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study proposes a Hierarchical Evaluation Function (HEF) for\
  \ optimizing demand forecasting models by integrating multiple metrics\u2014R\xB2\
  , MAE, and RMSE\u2014through a hierarchical and penalty-based structure. HEF addresses\
  \ the limitations of single-metric evaluation functions by balancing explanatory\
  \ power, average accuracy, and robustness against extreme errors."
---

# Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing Demand Forecasting Models

## Quick Facts
- arXiv ID: 2508.13057
- Source URL: https://arxiv.org/abs/2508.13057
- Reference count: 0
- Primary result: HEF consistently improves global performance metrics (R², GRA, RMSE, RMSSE) compared to MAE-based reference function across four benchmark datasets and three optimizers.

## Executive Summary
This study introduces the Hierarchical Evaluation Function (HEF), a multi-metric evaluation approach for optimizing demand forecasting models by integrating R², MAE, and RMSE through a hierarchical and penalty-based structure. HEF addresses single-metric evaluation limitations by balancing explanatory power, average accuracy, and robustness against extreme errors. Experiments across Walmart, M3, M4, and M5 datasets using Grid Search, PSO, and Optuna demonstrate that HEF consistently improves global performance metrics while maintaining stability across different training/test partitions.

## Method Summary
HEF combines R², MAE, and RMSE into a single hierarchical evaluation function with fixed weights (ωR²=1.0, ωMAE=1.0, ωRMSE=0.5) and adaptive penalty thresholds based on coefficient of variation. The function normalizes MAE and RMSE by training series mean to enable cross-series comparison. HEF employs progressive penalties (×1.2 to ×1.8) when error thresholds are exceeded, with separate tolerances for MAE and RMSE that adapt to series volatility. The approach was evaluated against a MAE-based reference function (MAEF) using 19 forecasting models across four benchmark datasets with three different optimizers.

## Key Results
- HEF consistently improves global performance metrics (R², GRA, RMSE, RMSSE) compared to MAEF across all optimizers and datasets
- MAEF remains superior in mean error reduction and runtime efficiency
- HEF demonstrates stability across different training/test partitions (91:9, 80:20, 70:30 splits)
- The choice between HEF and MAEF should align with operational objectives: HEF for long-term planning requiring robustness, MAEF for short-term, resource-constrained tasks

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Multi-Metric Integration Reduces Single-Metric Bias
Combining R², MAE, and RMSE with differentiated weights produces more balanced model selection than optimizing any single metric alone. HEF computes a weighted score: ωR² × (1-R²) + ωMAE × (MAE/mean) + ωRMSE × (RMSE/mean) with fixed weights (1.0, 1.0, 0.5). RMSE receives lower weight because it is more sensitive to outliers, preventing optimization from overfitting to any single error dimension.

### Mechanism 2: Adaptive Penalty Thresholds Based on Coefficient of Variation
Tolerance thresholds that adapt to series volatility prevent disproportionate penalties in high-variance contexts while maintaining strictness in stable series. HEF computes CV = std/mean and assigns tolerance thresholds dynamically. For MAE: CV < 0.2 → threshold 0.1; CV ≥ 1.0 → threshold 0.4. Similar logic for RMSE with slightly relaxed values. Progressive penalties (×1.2 to ×1.8) apply when thresholds are exceeded.

### Mechanism 3: Scale Normalization via Training Mean Enables Cross-Series Comparison
Normalizing MAE and RMSE by the training series mean removes scale dependency while preserving interpretability. MAE_normalized = MAE / mean(y_train) and RMSE_normalized = RMSE / mean(y_train). This expresses errors as proportions of average demand level, conceptually similar to MASE/RMSSE.

## Foundational Learning

- Concept: R² (Coefficient of Determination)
  - Why needed here: HEF prioritizes explanatory power alongside error metrics. R² measures variance explained but can overestimate fit in high-dispersion scenarios.
  - Quick check question: Can you explain why a model with high R² might still have operationally unacceptable errors?

- Concept: MAE vs RMSE Sensitivity Profiles
  - Why needed here: HEF weights RMSE lower (0.5) than MAE (1.0) specifically because RMSE over-penalizes outliers. Understanding this tradeoff is essential for interpreting HEF's design.
  - Quick check question: For a demand series with occasional spikes, which metric would drive optimization toward more conservative forecasts?

- Concept: Hyperparameter Optimization Methods (Grid Search, PSO, Optuna)
  - Why needed here: HEF is evaluated as the objective function across three optimizers. Understanding their search behaviors helps diagnose whether improvements stem from HEF or the optimizer.
  - Quick check question: Why did the paper use multiple optimizers rather than relying on a single method?

## Architecture Onboarding

- Component map: Data Preparation -> Train/Test Split -> Model Pool -> Optimizer -> HEF Evaluation -> Validation Metrics
- Critical path: The evaluation function (HEF or MAEF) is the only component that changes in controlled comparisons. All models, optimizers, and datasets remain constant, isolating the evaluation function's effect.
- Design tradeoffs:
  - HEF vs MAEF: HEF improves R², GRA, RMSE, RMSSE; MAEF improves MAE, MASE, runtime. Choose HEF for long-term planning (robustness matters); MAEF for short-term operations (mean error and speed matter).
  - Fixed vs Adaptive Weights: Paper uses fixed weights (R²=1.0, MAE=1.0, RMSE=0.5) for comparability. Adaptive weighting is identified as future work.
  - Hierarchical vs Pareto: HEF is NOT Pareto-based multi-objective optimization. It uses sequential priority to reduce computational complexity.
- Failure signatures:
  - Negative predictions → Trigger penalize_level_4 (×1.8 multiplier)
  - Both MAE and RMSE exceed thresholds → penalize_level_3 (×1.5)
  - Mean ≈ 0 → Fallback to small constant (1e-6) may produce unreliable normalized scores
  - High CV with legitimate spikes → May receive overly lenient thresholds, masking real problems
- First 3 experiments:
  1. Replicate the MAEF vs HEF comparison on a single dataset using Optuna with 91:9 split. Count cases where each function wins on R², MAE, and RMSE. Verify statistical significance with difference-of-proportions test.
  2. Test threshold sensitivity: Modify CV thresholds (e.g., tighten by 0.05) and observe whether HEF's advantage on global metrics changes. This probes whether results depend heavily on threshold calibration.
  3. Apply HEF to a domain outside the paper (e.g., energy demand from corpus neighbor papers). Compare HEF vs MAEF performance to assess generalization beyond retail/supply chain contexts.

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is HEF's performance to variations in the hierarchical weights (ωR², ωMAE, ωRMSE) and tolerance thresholds across different forecasting contexts? The study used fixed weights derived from literature, intentionally avoiding exploration of alternative parameterizations to isolate the hierarchical design effect.

### Open Question 2
How does HEF's hierarchical integration compare to formal Pareto-based multi-objective optimization frameworks in balancing accuracy, robustness, and explanatory power? The authors deliberately adopted a hierarchical structure rather than Pareto optimization to reduce computational complexity.

### Open Question 3
Can metaheuristic approaches dynamically adapt hierarchical weights based on statistical properties of each time series to enhance HEF's generalization capacity? Current tolerance thresholds adapt to coefficient of variation, but weights remain globally fixed regardless of individual series characteristics.

### Open Question 4
How well does HEF generalize to domains with distinct temporal dynamics, such as energy demand forecasting, healthcare resource planning, and logistics? The current validation covers retail demand with specific non-stationarity patterns, but other domains exhibit different characteristics.

## Limitations

- No Pareto-based multi-objective validation: HEF explicitly avoids strict Pareto optimization for computational efficiency, potentially missing dominance relations between models.
- Threshold calibration dependency: HEF's adaptive penalty thresholds rely on CV-based heuristics calibrated on the paper's datasets, untested in domains with different volatility patterns.
- Single-weight configuration: Fixed weights were chosen for comparability but may not be optimal across all domains; adaptive weighting is noted as future work but not evaluated.

## Confidence

- **High confidence**: HEF consistently improves global metrics (R², GRA, RMSE, RMSSE) vs MAEF across all optimizers and datasets tested.
- **Medium confidence**: The claim that HEF is superior for long-term planning while MAEF suits short-term operations is logically sound but based on correlation rather than causal validation in operational contexts.
- **Low confidence**: HEF's generalization to non-retail domains (energy, finance, healthcare) is asserted but not empirically demonstrated beyond the paper's four benchmark datasets.

## Next Checks

1. **Cross-domain transfer test**: Apply HEF to energy demand forecasting datasets from corpus neighbor papers and compare performance gains vs MAEF.
2. **Pareto frontier comparison**: For a subset of models, compute Pareto-optimal solutions using strict multi-objective optimization and compare against HEF's single-score rankings.
3. **Threshold sensitivity analysis**: Systematically vary CV-based thresholds (±0.1 increments) and measure impact on HEF's global metric improvements to quantify calibration sensitivity.