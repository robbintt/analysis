---
ver: rpa2
title: Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners
  with Large Language Models
arxiv_id: '2508.11524'
source_url: https://arxiv.org/abs/2508.11524
tags:
- state
- planning
- domain
- llms
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes two paradigms for leveraging large language
  models (LLMs) to assist classical planners in handling large-scale planning problems:
  LLM4Inspire and LLM4Predict. LLM4Inspire uses LLMs to select the most promising
  action based on general knowledge, while LLM4Predict predicts intermediate states
  constrained by domain-specific knowledge to partition the search space.'
---

# Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models

## Quick Facts
- **arXiv ID:** 2508.11524
- **Source URL:** https://arxiv.org/abs/2508.11524
- **Reference count:** 8
- **Primary result:** LLM4Predict achieves over 95% success rate in three domains by predicting intermediate states with domain-specific knowledge, outperforming LLM4Inspire's action selection approach.

## Executive Summary
This paper explores two paradigms for leveraging large language models (LLMs) to assist classical planners with large-scale planning problems: LLM4Inspire and LLM4Predict. The authors propose decomposing complex planning problems into simpler sub-tasks using Directed Acyclic Dependency Graphs (DADGs) and then using LLMs to either select promising actions (Inspire) or predict intermediate states (Predict). Experiments across four domains demonstrate that LLM4Predict significantly outperforms LLM4Inspire, achieving higher success rates while requiring fewer LLM calls and less solver time, validating the effectiveness of domain-specific knowledge injection over general knowledge approaches.

## Method Summary
The framework decomposes large planning problems into multiple simpler sub-tasks using Directed Acyclic Dependency Graphs (DADGs) constructed from goal states. It employs Fast Downward as the base solver and integrates DeepSeek-R1 LLM through two paradigms: LLM4Inspire selects actions from applicable lists when the solver fails, while LLM4Predict generates intermediate states as JSON constraints. The system allows up to 10 retries per sub-goal, switching between solver and LLM assistance as needed. The key innovation is the LLM4Predict approach that constrains the LLM with domain-specific knowledge to predict valid intermediate states, transforming the search complexity from O(b^k) to O(2b^(k/2)).

## Key Results
- LLM4Predict achieves over 95% success rate in Blocks, Logistics, and Depot domains, significantly outperforming LLM4Inspire
- LLM4Predict reduces solver time and LLM calls compared to LLM4Inspire, demonstrating greater effectiveness in simplifying search spaces
- In the Mystery domain with obfuscated object names, both LLM approaches fail completely, proving the necessity of domain-specific knowledge over general semantic understanding

## Why This Works (Mechanism)

### Mechanism 1: Search Space Partitioning via Goal Ordering
The system constructs Directed Acyclic Dependency Graphs (DADGs) from goal states to identify sequential constraints between objects, then topologically sorts these dependencies to solve shallow search trees sequentially rather than one deep monolithic tree, mitigating state-space explosion.

### Mechanism 2: Exponential Complexity Reduction via State Landmarking
LLM4Predict identifies intermediate landmark states between initial and goal states, transforming search complexity from O(b^k) to O(2b^(k/2)), while LLM4Inspire's action selection risks drifting from optimal paths requiring more frequent correction.

### Mechanism 3: Constraint Injection via Domain-Specific Prompting
LLM4Predict explicitly injects domain-specific constraints and PDDL predicates into LLM prompts, forcing logical inference rather than probabilistic generation, which proves essential when general semantic knowledge fails (as demonstrated in the Mystery domain).

## Foundational Learning

- **Concept: Classical Planning (PDDL)**
  - **Why needed here:** The architecture relies on Fast Downward and PDDL predicates; understanding initial states, goal states, and domain definitions is essential for debugging sub-problem generation.
  - **Quick check question:** Given goal `on(A,B)` and `on(B,C)`, why must the planner achieve `on(B,C)` before `on(A,B)` in Blocks World?

- **Concept: State Space Search & Complexity**
  - **Why needed here:** The paper's core value proposition is mitigating state-space explosion; understanding branching factor and depth explains why dividing depth d into two problems of size d/2 yields exponential savings.
  - **Quick check question:** Why does branching factor 10 and depth 10 result in 10^10 states, while two sequential searches of depth 5 result in only 2 × 10^5 states?

- **Concept: In-Context Learning & Prompt Engineering**
  - **Why needed here:** The difference between LLM4Inspire and LLM4Predict lies in prompt structure; distinguishing between "probability-based selection" vs. "logical deduction under constraints" is crucial.
  - **Quick check question:** If an LLM is asked to "predict an intermediate state," what specific output format (JSON vs. natural language) minimizes parsing errors for the planner?

## Architecture Onboarding

- **Component map:** Domain File + Instance File → Model Parser → Instance Disassembler (builds DADGs) → Instance Factory (creates sub-PDDL files) → Solver (Fast Downward) → Successor Generator + LLM Module (Inspire/Predict) → Action or Intermediate State → Solver retry loop

- **Critical path:** The LLM4Predict module is critical; valid intermediate state prediction drastically drops solver time, while invalid predictions hit the 10-try limit and fail.

- **Design tradeoffs:** LLM4Inspire is safer but slower with more calls; LLM4Predict is faster and higher success but fails catastrophically without domain understanding. Decomposition granularity must balance overhead vs. search space size.

- **Failure signatures:** High LLM calls/timeouts indicate LLM4Inspire selecting non-productive actions; zero success rate in Mystery domain shows LLM reliance on semantic meaning; plan invalidity occurs when LLM4Predict violates PDDL constraints.

- **First 3 experiments:** 1) Baseline comparison: Run standard Fast Downward vs. Decomposition-only on Blocks instance with 10+ blocks. 2) Paradigm validation: Compare LLM4Inspire vs. LLM4Predict on Logistics problem. 3) Ablation on Prompting: Run LLM4Predict on Mystery domain to confirm break condition.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's reliance on LLM's ability to infer valid intermediate states under strict constraints remains unproven beyond tested domains.
- The decomposition strategy assumes goals can be linearized through DADGs but doesn't address scenarios with cyclic dependencies.
- The comparative analysis focuses on success rates and efficiency but doesn't evaluate plan quality or optimality relative to traditional planners.

## Confidence

- **High Confidence:** Exponential complexity reduction claim when partitioning search space is well-supported by formal analysis and experimental results.
- **Medium Confidence:** Superiority of domain-specific knowledge over general knowledge is demonstrated through Mystery domain ablation but needs broader testing.
- **Low Confidence:** Claim that LLM4Predict universally reduces solver time and LLM calls compared to LLM4Inspire needs validation, as Mystery domain shows complete failure when domain knowledge is obscured.

## Next Checks

1. **Stress Test Decomposition Limits:** Create PDDL instances with intentionally circular dependencies or non-sequential goals to verify framework behavior when DADG construction fails.

2. **Domain Transferability Test:** Apply LLM4Predict to domains with significantly different semantics (e.g., temporal planning, numeric fluents) to assess LLM adaptation to new constraint types.

3. **Plan Optimality Evaluation:** Compare plan lengths and quality metrics between LLM-assisted approaches and optimal classical planners on smaller instances where optimal solutions are computable.