---
ver: rpa2
title: 'PG-Agent: An Agent Powered by Page Graph'
arxiv_id: '2509.03536'
source_url: https://arxiv.org/abs/2509.03536
tags:
- page
- graph
- agent
- arxiv
- episodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving generalization
  in GUI agents by modeling complex page transition relationships. It proposes an
  automated pipeline to convert sequential GUI episodes into page graphs, which explicitly
  capture the interconnected structure of pages.
---

# PG-Agent: An Agent Powered by Page Graph

## Quick Facts
- **arXiv ID**: 2509.03536
- **Source URL**: https://arxiv.org/abs/2509.03536
- **Reference count**: 40
- **Primary result**: Outperforms existing methods in navigation accuracy on AITW, Mind2Web, and GUI Odyssey benchmarks

## Executive Summary
This paper addresses the challenge of improving generalization in GUI agents by modeling complex page transition relationships. It proposes an automated pipeline to convert sequential GUI episodes into page graphs, which explicitly capture the interconnected structure of pages. A tailored multi-agent framework called PG-Agent is then introduced, enhanced with Retrieval-Augmented Generation (RAG) to retrieve reliable navigation guidelines from the page graphs. This allows the agent to better perceive the GUI environment and generalize to unseen scenarios. Experimental results on three benchmarks—AITW, Mind2Web, and GUI Odyssey—show that PG-Agent outperforms existing methods in navigation accuracy, even when constructed from limited episodes.

## Method Summary
PG-Agent transforms sequential GUI episodes into page graphs through an automated pipeline that merges semantically similar pages into nodes and consolidates consecutive actions into edges. The multi-agent system uses RAG to retrieve navigation guidelines from these graphs, with specialized agents for global planning, observation, sub-task planning, and decision-making. The framework leverages Qwen2.5-VL-72B as the base model with BGE-M3 embeddings for retrieval, FAISS for vector similarity search, and BFS exploration up to 3 layers for guideline retrieval.

## Key Results
- Achieves 59.5% overall accuracy on AITW benchmark vs. 48.1% for Qwen2.5-VL-72B baseline
- Demonstrates superior performance on Cross-Task and Cross-Domain splits, showing effective generalization
- Page graph construction from just 1/10 of training episodes yields competitive results, with diminishing returns from full dataset usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting sequential episodes into page graphs enables the agent to access recombined navigation paths that were never explicitly demonstrated in any single training episode.
- Mechanism: The page graph construction pipeline merges semantically similar pages into single nodes (via dual-level similarity check) and consolidates consecutive in-page actions onto single edges. When the agent retrieves from this graph, it can traverse paths that recombine action sequences across originally independent episodes.
- Core assumption: Pages that appear visually and semantically similar represent the same logical state in the application, and action sequences that led to different outcomes in different episodes can be validly recombined.
- Evidence anchors:
  - [abstract] "design an automated pipeline to transform the sequential episodes into page graphs, which explicitly model the graph structure of the pages that are naturally connected by actions"
  - [section 3.1] Shows that nodes in page graphs are fewer than original images (e.g., AITW General: 341 images → 132 nodes), demonstrating successful deduplication
  - [corpus] Weak corpus evidence—related work (GraphPilot) uses knowledge graphs but does not validate the specific recombination claim
- Break condition: If the MLLM-based page similarity check produces false positives (merging genuinely different pages), the graph will conflate distinct application states and provide misleading guidance.

### Mechanism 2
- Claim: Injecting retrieved guidelines at the sub-task planning stage improves generalization to unseen tasks more effectively than providing them only at the final decision stage.
- Mechanism: The RAG pipeline retrieves action queues and associated task descriptions from the page graph via BFS exploration (up to 3 layers). These guidelines are fed to the Sub-Task Planning Agent, which generates candidate actions aligned with both the global plan and the retrieved navigation patterns.
- Core assumption: The base MLLM has sufficient grounding capability to map abstract sub-goals onto concrete screen elements when provided with similar action patterns from the page graph.
- Evidence anchors:
  - [section 3.2] "Sub-Task Planning Agent will conduct in-depth analysis of the context information, including... guidelines GIt and complete the fine-grained plan Rs"
  - [section 4.3] Ablation shows guidelines in Sub-Task Planning Agent help more in Cross-Task and Cross-Domain splits, while Decision Agent injection helps more in Cross-Website
  - [corpus] SimpleDoc demonstrates dual-cue retrieval for document understanding, but does not address multi-agent injection points
- Break condition: If retrieved guidelines are contextually irrelevant (e.g., from a different app domain), they may bias the sub-task planner toward inapplicable actions.

### Mechanism 3
- Claim: Task decomposition distributes reasoning load across specialized agents, reducing "lost-in-the-middle" failures common in single-prompt approaches.
- Mechanism: Global Planning Agent decomposes the user goal into sub-tasks. Observation Agent handles visual parsing. Sub-Task Planning Agent generates candidate actions. Decision Agent selects the final action. Each agent receives targeted context rather than full history.
- Core assumption: The decomposition is stable—sub-tasks remain valid as the agent executes actions, and re-planning is not frequently needed.
- Evidence anchors:
  - [section 3.2] "each agent receives different input content and only completes specific tasks, which alleviates the context processing pressure of the model"
  - [section 4.2] PG-Agent achieves 59.5% overall accuracy on AITW vs. 48.1% for Qwen2.5-VL-72B baseline
  - [corpus] ReachAgent identifies "local optimal solutions" in stepwise agents, supporting decomposition benefits
- Break condition: If the global plan is incorrect or obsolete (e.g., environment changes mid-task), sub-task planning will cascade errors without correction.

## Foundational Learning

- Concept: **Markov Decision Processes for GUI Navigation**
  - Why needed here: The multi-agent workflow is formalized as an MDP, where state = current screenshot, action = UI operation, and the policy is approximated by the agent chain. Understanding MDPs clarifies why history (τ<t) is injected selectively.
  - Quick check question: Given a sequence of 5 screenshots and actions, can you identify which components constitute the state, transition, and policy in this framework?

- Concept: **Breadth-First Search on Directed Graphs**
  - Why needed here: Guidelines retrieval uses BFS from matched nodes to explore achievable tasks within 3 layers. This enables discovery of multi-hop navigation paths beyond immediate neighbors.
  - Quick check question: If the page graph has 200 nodes with average out-degree 3, what is the worst-case number of nodes explored in a 3-layer BFS?

- Concept: **Retrieval-Augmented Generation (RAG) for Structured Knowledge**
  - Why needed here: Unlike document RAG, PG-Agent retrieves from a graph where nodes are page summaries and edges are action sequences with task labels. This requires understanding graph traversal combined with semantic similarity.
  - Quick check question: How does retrieving (action queue, associated tasks) pairs differ from retrieving free-text documentation?

## Architecture Onboarding

- Component map:
  - Page Graph Construction Pipeline: MLLM-based jump detection → similarity check (semantic + pixel) → graph update
  - Page Graph Store: Nodes = (page summary, image location); Edges = (action queue, task description)
  - RAG Pipeline: Screen summarization → vector retrieval → BFS exploration → guideline formatting
  - Multi-Agent System: Global Planning → Observation → Sub-Task Planning → Decision

- Critical path:
  1. Build page graph from sampled episodes (offline, one-time per app/scenario)
  2. At inference: summarize current screen → retrieve top-4 similar nodes → BFS 3 layers → format up to 20 guidelines
  3. Generate global plan → observe screen → plan sub-task with guidelines → decide action
  4. Execute action, append to trajectory, repeat until COMPLETE or max steps

- Design tradeoffs:
  - Sampling fraction: Authors used 1/10 of AITW training data. Full data did not consistently improve results (Table 7), suggesting diminishing returns.
  - BFS depth: Set to 3. Deeper exploration retrieves more distant tasks but increases noise.
  - Guideline count: Capped at 20 for AITW, 10 for Mind2Web. Too many guidelines may overwhelm the Sub-Task Planning Agent.
  - Agent separation: More agents reduce per-agent context but add coordination overhead.

- Failure signatures:
  - Over-execution: Agent continues past ground-truth completion (Figure 10)—the agent infers a more thorough completion criteria than the dataset label.
  - SELECT vs. CLICK confusion: Dataset inconsistency in action labeling causes F1 drops even when agent behavior is reasonable (Table 5).
  - Missing page graph: Cross-domain scenarios (Service, Info in Mind2Web) have no constructed graph; fallback to other scenario graphs provides partial benefit.

- First 3 experiments:
  1. Replicate page graph construction on a single AITW scenario (e.g., Install). Verify node count reduction matches Table 8 proportions. Manually inspect 5 merged nodes for similarity validity.
  2. Ablate BFS depth (1, 2, 3, 4 layers) on a held-out subset. Measure guideline relevance (human rating) and action accuracy. Identify where noise overtakes signal.
  3. Swap guideline injection point: provide guidelines only to Decision Agent vs. only to Sub-Task Planning Agent. Compare step success rates on Cross-Task vs. Cross-Website splits to validate Table 4 patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the agent's termination strategy be refined to prevent "over-execution" in scenarios where the task goal is technically met at an intermediate step rather than upon full completion of the logical workflow?
- Basis in paper: [explicit] The authors state in Section 4.2 and the Case Study that in the "Single" scenario, the agent often fails because it "tends to continue executing some actions to completely finish the task," whereas the ground truth marks the task as complete earlier (e.g., after searching but before clicking a result).
- Why unresolved: The current multi-agent framework prioritizes completing the logical chain of actions retrieved from the page graph, lacking a mechanism to recognize "sufficient" completion states defined by specific benchmark constraints.
- What evidence would resolve it: A modified stopping criterion or reward signal that aligns step-wise completion with benchmark-specific success definitions, demonstrated by improved accuracy in the "Single" scenario of AITW.

### Open Question 2
- Question: Can the page graph construction pipeline be adapted to normalize semantic equivalence between different action granularities (e.g., treating a "SELECT" action as equivalent to two consecutive "CLICK" actions) to mitigate label inconsistency errors?
- Basis in paper: [explicit] Section 4.3 notes a decrease in performance for 'CLICK' actions (Op.F1 score) because the agent often predicts 'SELECT' actions which are semantically correct but labeled as failures due to dataset inconsistency.
- Why unresolved: The current construction pipeline stores action summaries directly from episodes without a mechanism to unify distinct action types that result in the same state transition, causing a mismatch between retrieved guidelines and ground-truth evaluations.
- What evidence would resolve it: An ablation study where the page graph edges are canonicalized to handle both action types, resulting in recovered F1 scores for click-based interactions without loss of performance in other metrics.

### Open Question 3
- Question: How does the reliance on purely visual clues for node similarity checking impact the stability of the page graph when processing screens with high-frequency dynamic content (e.g., advertisements or loading animations)?
- Basis in paper: [inferred] The method constructs graphs "purely based on visual clues without additional modal inputs" (Section 3.1). While effective for the datasets used, visual-only hashing or comparison is theoretically susceptible to interpreting non-functional visual changes (like an ad banner rotating) as distinct page nodes.
- Why unresolved: The paper does not analyze the noise ratio in the constructed graphs or the frequency of false positives in "page jump" determination caused by dynamic visual elements common in real-world web and app interfaces.
- What evidence would resolve it: An evaluation of graph compactness and retrieval precision when the pipeline is applied to environments with injected dynamic visual noise, measuring the rate of redundant node creation.

## Limitations
- Page graph construction assumes MLLM-based similarity detection reliably identifies logically equivalent pages, but false positive/negative rates are not reported
- The claim that retrieved guidelines improve generalization assumes the base MLLM can effectively map abstract navigation patterns to concrete UI actions, without direct measurement of this capability
- Task decomposition across four specialized agents is assumed superior without comparing to alternative multi-agent configurations or ablation studies on agent count

## Confidence
- **High Confidence**: Claims about the general framework architecture (page graph construction pipeline, multi-agent system structure) and baseline performance improvements on benchmark datasets. The experimental results show consistent improvements across multiple splits and datasets.
- **Medium Confidence**: Claims about specific mechanism effectiveness (RAG injection points, BFS depth selection). While ablation studies support these choices, the paper does not explore the full parameter space or compare against alternative retrieval strategies.
- **Low Confidence**: Claims about the precise effectiveness of MLLM-based page similarity detection and the assumption that merged nodes in the page graph truly represent equivalent logical states. The paper provides node reduction statistics but lacks validation of semantic equivalence.

## Next Checks
1. **Similarity Detection Validation**: Manually audit 50 randomly selected page merges from the AITW General scenario to verify that merged nodes are semantically equivalent. Calculate precision, recall, and F1 score for the MLLM-based similarity detection pipeline.
2. **Guideline Relevance Assessment**: Implement a human evaluation protocol where annotators rate retrieved guideline relevance on a 3-point scale for 100 randomly selected inference steps. Correlate relevance scores with step success rates to quantify the actual impact of retrieved guidelines.
3. **Cross-Domain Generalization Test**: Construct page graphs for the "Service" and "Info" scenarios in Mind2Web (which lack graphs in the paper) and measure whether PG-Agent performance on these domains improves relative to the baseline when using fallback graphs from other scenarios, validating the paper's claim about partial benefits from cross-domain knowledge transfer.