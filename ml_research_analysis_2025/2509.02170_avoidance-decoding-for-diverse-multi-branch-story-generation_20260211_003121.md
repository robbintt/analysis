---
ver: rpa2
title: Avoidance Decoding for Diverse Multi-Branch Story Generation
arxiv_id: '2509.02170'
source_url: https://arxiv.org/abs/2509.02170
tags:
- arxiv
- story
- diversity
- degeneration
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Avoidance Decoding, a novel decoding strategy\
  \ that enhances diversity in multi-branch story generation by contrastively penalizing\
  \ similarity to previously generated outputs. The method combines two similarity\
  \ penalties\u2014concept-level (CSP) for early-stage diversity and narrative-level\
  \ (NSP) for later-stage coherence\u2014into a hybrid formulation that adaptively\
  \ shifts emphasis as generation progresses."
---

# Avoidance Decoding for Diverse Multi-Branch Story Generation

## Quick Facts
- **arXiv ID:** 2509.02170
- **Source URL:** https://arxiv.org/abs/2509.02170
- **Reference count:** 17
- **Primary result:** Up to 2.6× higher diversity and 30% lower repetition than strong baselines while maintaining robustness against text degeneration.

## Executive Summary
This paper introduces Avoidance Decoding, a novel decoding strategy that enhances diversity in multi-branch story generation by contrastively penalizing similarity to previously generated outputs. The method combines two similarity penalties—concept-level (CSP) for early-stage diversity and narrative-level (NSP) for later-stage coherence—into a hybrid formulation that adaptively shifts emphasis as generation progresses. Automatic and human evaluations show that Avoidance Decoding achieves superior diversity metrics while effectively mitigating text degeneration, with dormant neuron analysis suggesting the approach activates broader neuron ranges to leverage the model's intrinsic creativity.

## Method Summary
Avoidance Decoding modifies token logits during generation by subtracting a scaled similarity penalty computed against previously generated outputs (negative samples). The method uses a hybrid penalty combining concept-level similarity (based on hidden states for early conceptual diversity) and narrative-level similarity (based on sentence embeddings for later coherence), with a progressive shift from CSP to NSP controlled by an inflection point hyperparameter. At each decoding step, the algorithm computes CSP and NSP for each candidate token, combines them with a mixing ratio that shifts from CSP-dominant to NSP-dominant, scales by a penalty strength, and subtracts from the original logits before greedy token selection.

## Key Results
- Achieves up to 2.6× higher diversity (measured by diversity metrics) compared to strong baselines
- Reduces repetition by 30% while maintaining narrative quality
- Demonstrates robustness against text degeneration, only exceeding degeneration threshold at 2000+ token lengths
- Dormant neuron analysis shows activation of broader neuron ranges suggesting deeper model creativity utilization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastively penalizing token similarity to previously generated outputs enhances multi-branch story diversity.
- **Mechanism:** At each decoding step, the algorithm modifies the logit of each candidate token by subtracting a scaled similarity score, computed against a set of negative samples (i.e., prior branches), steering the model away from tokens that would replicate existing narratives.
- **Core assumption:** The similarity functions used correlate with the semantic or narrative overlap the method intends to minimize.
- **Evidence anchors:**
  - [abstract] "Avoidance Decoding ... modifies token logits by penalizing similarity to previously generated outputs, thereby encouraging more diverse multi-branch stories."
  - [section] "We then have an approximated formula: log P(xt | x1:t−1, ¬N) ≈ log Pθ(xt | x1:t−1) − Σ αi Ps(ni | x1:t). ... ℓ* t = ℓt − max_{i=1..N} αi Ps(ni | x1:t)."
  - [corpus] Related corpus work (e.g., G2, GUARD) explores contrastive decoding for diversity, but evidence for this specific logit-penalty formulation is confined to this paper.
- **Break condition:** If similarity metrics do not capture meaningful semantic overlap, the penalty will misguide token selection without improving diversity.

### Mechanism 2
- **Claim:** A hybrid penalty combining concept-level (CSP) and narrative-level (NSP) similarity, with a progressive shift from CSP to NSP, improves both early conceptual diversity and later narrative coherence.
- **Mechanism:** Early in generation, CSP (hidden-state cosine similarity between candidate tokens and negative samples) dominates to diversify story premises; after an inflection point T0, NSP (sentence-embedding similarity between the growing output and negative samples) is progressively upweighted via a mixing ratio γ.
- **Core assumption:** Early hidden-state similarity reflects concept similarity, while later sentence-embedding similarity captures narrative-level coherence; a sigmoidal shift appropriately balances the two.
- **Evidence anchors:**
  - [abstract] "This penalty adaptively balances two similarity measures: (1) Concept-level Similarity Penalty ... (2) Narrative-level Similarity Penalty ... increasingly emphasized later."
  - [section] "Therefore, we rely primarily on the CSP in the early stage, until the decoding step reaches the inflection point T0, ensuring diverse story planning. Then we progressively increase the weight of the NSP ..."
  - [corpus] No direct corpus evidence on this specific hybrid formulation.
- **Break condition:** If the inflection point or weighting function is misconfigured for a given task, the method may either sacrifice coherence (excessive CSP) or diversity (excessive NSP).

### Mechanism 3
- **Claim:** Avoidance Decoding mitigates text degeneration more robustly than standard diversity-focused sampling methods.
- **Mechanism:** The deterministic, similarity-based logit adjustment avoids the stochasticity that can lead to degeneration in sampling methods, while NSP's sentence-level semantic constraint helps maintain narrative coherence.
- **Core assumption:** Deterministic logit adjustment with semantic constraints provides a safer path to diversity than stochastic sampling alone.
- **Evidence anchors:**
  - [abstract] "...while effectively mitigating text degeneration."
  - [section] "As shown in Figure 3, our model demonstrates greater robustness to degeneration than other sampling methods, only exceeding a degeneration score of 0.1 when the maximum token length reaches 2000..."
  - [corpus] Corpus evidence on this specific robustness claim is limited; related work (GUARD) targets robustness via different mechanisms.
- **Break condition:** If penalty strength (α, β) is too high, the model may be over-penalized, leading to incoherence or degeneration despite the method's intent.

## Foundational Learning

- **Concept: Contrastive Decoding / Contrastive Search**
  - **Why needed here:** This method is a direct extension of contrastive-style decoding. Understanding the base concept (penalizing undesirable tokens via a contrastive term) is essential.
  - **Quick check question:** How does Contrastive Search define and apply its degeneration penalty, and how does Avoidance Decoding extend it?

- **Concept: Sentence Embeddings (e.g., Sentence-BERT)**
  - **Why needed here:** Used to compute the Narrative-level Similarity Penalty. The core idea is that embeddings capture semantic meaning beyond surface n-gram overlap.
  - **Quick check question:** Why would a sentence embedding be more suitable for measuring narrative similarity than a metric like BLEU?

- **Concept: Hidden States in Language Models**
  - **Why needed here:** Used to compute the Concept-level Similarity Penalty. The key assumption is that early hidden states correlate with conceptual content.
  - **Quick check question:** From which part of the model is the "hidden state" for a candidate token derived, and what does it theoretically represent?

## Architecture Onboarding

- **Component map:** LLM decoder -> Negative sample cache (previous branches' text and hidden states) -> Penalty computation unit (CSP, NSP, hybrid weighting) -> Modified argmax selection -> Output
- **Critical path:** At each decoding step: (1) Generate top-k candidate tokens and their hidden states from the LLM. (2) Compute CSP for each candidate against cached negative sample hidden states. (3) Form the partial sentence for each candidate and compute its embedding, then NSP against cached negative sample embeddings. (4) Combine penalties via the hybrid weighting γ. (5) Apply the final scaled penalty to original logits and select the highest-scoring token via greedy decoding.
- **Design tradeoffs:**
  - **Diversity vs. Degeneration:** CSP strongly promotes diversity but increases degeneration risk. NSP promotes narrative distinction with less risk. The hybrid approach and hyperparameters (T0, β, δ) manage this tradeoff.
  - **Quality vs. Speed:** Computing penalties against multiple negative samples and using sentence embeddings introduces significant computational overhead compared to standard sampling or greedy decoding.
- **Failure signatures:**
  - **Excessive Repetition:** Penalty strength is too low or the negative sample set is too small.
  - **Incoherent Output (Degeneration):** Penalty strength is too high, or the T0 inflection point is too late, forcing the model into implausible tokens.
  - **Slow Generation:** Latency increases linearly with the number of negative samples if not managed with a fixed-size window.
- **First 3 experiments:**
  1. **Baseline Comparison:** Reproduce core results by running Avoidance Decoding, Ours_CSP, Ours_NSP, and Top-p/Top-k on a small prompt set. Compare diversity (BLEU/ROUGE-L/Sent-Sim) and degeneration scores to validate the hybrid method's balance.
  2. **Hyperparameter Ablation:** Systematically vary the inflection point T0 and penalty strength β to map the diversity-degeneration tradeoff curve. This reveals parameter sensitivity and optimal operating regions.
  3. **Scaling Behavior:** Measure generation latency and degeneration scores as the number of negative samples (branches) increases (e.g., 5, 15, 30). This tests robustness and computational practicality for real-world multi-branch use cases.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a fixed-size sliding window of negative samples mitigate the computational overhead of Avoidance Decoding without sacrificing narrative diversity?
- **Basis in paper:** [explicit] Section 7 (Limitations) states, "To mitigate the computational overhead, one potential solution is to store only a fixed-size window of recent outputs rather than maintaining the entire output history."
- **Why unresolved:** The authors identify the computational cost of accumulating negative samples as a drawback but do not implement or validate the proposed windowing solution.
- **What evidence would resolve it:** Experiments comparing diversity metrics (e.g., LLMScore) and latency using sliding windows of varying sizes (e.g., last 3, 5, 10 branches) versus the full history.

### Open Question 2
- **Question:** Is the observed reduction in dormant neuron ratios causally linked to "intrinsic creativity," or is it a byproduct of forced representation divergence?
- **Basis in paper:** [inferred] Section 5.2.6 concludes that the method "leverages the model's intrinsic creativity" based on dormant neuron analysis, but this relies on correlation rather than causal tracing.
- **Why unresolved:** While the paper shows lower dormant neuron ratios correlate with diverse outputs, it does not prove that specific newly activated neurons are responsible for high-level creative concepts rather than just surface-level variance.
- **What evidence would resolve it:** Causal mediation analysis linking specific reactivated neuron clusters to the generation of novel plot elements or semantic shifts.

### Open Question 3
- **Question:** How robust is the inflection point hyperparameter ($T_0$) when scaling generation length from short stories (200 tokens) to long-form narratives?
- **Basis in paper:** [inferred] Section 5.2.4 optimizes $T_0$ (inflection point) on 200-token generations. Section 5.2.5 shows degeneration robustness up to 2000 tokens, but the optimal scheduling of the Concept-to-Narrative penalty shift for these longer lengths is unstated.
- **Why unresolved:** The optimal balance between Concept-level (CSP) and Narrative-level (NSP) penalties likely depends on the narrative arc, which changes with story length.
- **What evidence would resolve it:** A hyperparameter sweep of $T_0$ (and the mixing ratio $\gamma$) on generation tasks requiring 1000+ tokens to see if the "early stage" assumption holds.

## Limitations

- **Generalizability Across Domains:** The method is evaluated exclusively on short story generation tasks. Its effectiveness for other multi-branch generation tasks (e.g., code synthesis, dialogue, or long-form content) remains unverified.
- **Sensitivity to Penalty Hyperparameters:** The method's performance is highly sensitive to the penalty strength (β=2.0) and the inflection point (T0=25). The paper tests a limited range of values, and the optimal settings may vary significantly across different model sizes, domains, or prompt characteristics.
- **Computational Overhead:** Computing similarity penalties against multiple negative samples (15 per prompt in the paper) and using Sentence-BERT embeddings introduces substantial computational overhead compared to standard sampling or greedy decoding.

## Confidence

- **High Confidence:** The core technical contribution of the method—modifying logits with a weighted combination of CSP and NSP penalties—is well-defined and its implementation is clear. The results showing increased diversity (up to 2.6×) and reduced repetition (30%) compared to strong baselines are supported by the presented experiments.
- **Medium Confidence:** The claim that the hybrid penalty structure (CSP early, NSP late) is optimal for balancing diversity and coherence is supported by the results, but the evidence is based on a single inflection point (T0=25) and mixing parameter (δ=0.5). The robustness against degeneration is demonstrated, but the specific threshold of 0.1 is somewhat arbitrary.
- **Low Confidence:** The broader claim about the method's general applicability to any multi-branch generation task is not supported by evidence beyond the story domain. The assertion that the method "activates broader neuron ranges" to leverage intrinsic creativity is an interpretation of dormant neuron analysis that is not conclusively proven by the data.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Conduct a grid search over the penalty strength (β) and inflection point (T0) parameters to map the full diversity-degeneration tradeoff curve. This will quantify the method's sensitivity and identify robust operating regions for different model sizes and domains.

2. **Generalization to Non-Narrative Tasks:** Apply the method to a fundamentally different multi-branch generation task, such as code synthesis or dialogue generation. Evaluate its performance on task-specific diversity and quality metrics to test the domain-specificity of the current results.

3. **Objective-Subjective Alignment Study:** Design a controlled human evaluation to directly compare the automatic similarity metrics (CSP, NSP) with human judgments of narrative diversity and coherence. This will validate whether the mathematical penalties are optimizing for the right aspects of output quality.