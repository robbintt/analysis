---
ver: rpa2
title: Complete Chess Games Enable LLM Become A Chess Master
arxiv_id: '2501.17186'
source_url: https://arxiv.org/abs/2501.17186
tags:
- chess
- data
- move
- language
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ChessLLM, a large language model capable of
  playing complete chess games by transforming the game into a textual format with
  Forsyth-Edwards Notation (FEN) representing board states. The model is trained using
  supervised fine-tuning on a large dataset of chess games collected from open-source
  platforms, with best moves generated using Stockfish evaluations.
---

# Complete Chess Games Enable LLM Become A Chess Master

## Quick Facts
- arXiv ID: 2501.17186
- Source URL: https://arxiv.org/abs/2501.17186
- Reference count: 9
- Primary result: LLMs can play complete chess games at professional level (1788 Elo) using FEN-based state representation and supervised fine-tuning

## Executive Summary
This paper presents ChessLLM, a transformer-based language model fine-tuned to play complete chess games by transforming the game into a textual format. The model takes Forsyth-Edwards Notation (FEN) strings representing board states as input and outputs the best move in standard algebraic notation. By training on a large dataset of chess games with moves generated by Stockfish at various search depths, ChessLLM achieves a professional-level Elo rating of 1788 when permitted to sample multiple times. The key innovation is demonstrating that LLMs can effectively learn chess policies through imitation learning on state-to-move pairs, achieving competitive performance against established chess engines.

## Method Summary
ChessLLM transforms chess into a textual sequence modeling problem by converting board states into FEN strings and moves into algebraic notation. The model is fine-tuned on 20 billion tokens of chess game data using Open-LLaMA-3B as the base architecture. Training data consists of FEN-Best Move pairs generated by Stockfish at two different search depths: short-round (12-50) and long-round (50-200). During inference, the model uses a sampling strategy where it generates multiple move candidates and selects the first legal move found, significantly improving both legality and playing strength.

## Key Results
- ChessLLM achieves a professional-level Elo rating of 1788 against Stockfish when permitted to sample 10 times
- Long-round data supervision provides a 350 Elo rating improvement over short-round data
- Wins 61% of games at Stockfish skill level 0, 56% at level 1, and 30% at level 2 with 10-sample protocol
- Achieves 89.8% win rate against Stockfish skill level 0, compared to 61.3% for ChessGPT-Base and 59.8% for ChessGPT-Chat

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting board states into sequential text (FEN) allows an autoregressive model to approximate a chess policy via imitation learning
- Mechanism: The model maps a Forsyth-Edwards Notation (FEN) string—a text serialization of piece positions—directly to a move token. By training on "FEN-Best Move" pairs derived from Stockfish, the LLM learns the conditional probability P(move | state) through standard next-token prediction, effectively functioning as a behavioral cloning agent
- Core assumption: The FEN string provides a sufficient and distinct state representation that does not require the temporal history of moves (PGN) to predict the optimal next action
- Evidence anchors:
  - [abstract] "We transform the game into a textual format with the best move represented in the Forsyth-Edwards Notation."
  - [page 3] "We constructed our dataset as FEN-Best move pairs... allowing imitation learning for policy through casual language modeling."
  - [corpus] "LLM CHESS" and related papers confirm that framing chess as a sequence modeling problem is a prevailing research direction, though often limited by legality constraints
- Break condition: If the FEN string length exceeds the model's context window during complex endgames, or if the model fails to capture spatial dependencies from the flattened text structure

### Mechanism 2
- Claim: Training on "long-round" data (deeper search depths) significantly improves playing strength over "short-round" data
- Mechanism: The paper distinguishes between short-round data (search depth 12-50) and long-round data (depth 50-200). Long-round supervision forces the model to learn strategies derived from deeper calculation (likely better endgame tactics), resulting in a +350 Elo improvement
- Core assumption: The "Best Move" label generated by Stockfish at higher depths represents a ground-truth distribution that is learnable and distinct from the heuristics found at lower search depths
- Evidence anchors:
  - [abstract] "Long-round data supervision enjoys a 350 Elo rating improvement over short-round data."
  - [page 3] "Search depths of 12-50 for short rounds and 50-200 for long rounds... The highest win-rate moves were selected."
  - [corpus] Related work "Search-contempt" emphasizes the computational cost of search; this paper suggests distilling that search depth into the model weights is highly effective
- Break condition: If the improvement is merely an artifact of overfitting to specific endgame patterns not representative of general play, or if the computational cost of generating deep-search data outweighs the training gains

### Mechanism 3
- Claim: Permitting multiple sampling attempts at inference effectively boosts legality and rating
- Mechanism: The model acts as a stochastic generator. By allowing up to 10 sampling iterations (pass@k) and selecting the first legal move, the system compensates for the LLM's tendency to hallucinate illegal moves. This filtering mechanism is essential for achieving the reported 1788 Elo
- Core assumption: The model's probability mass is sufficiently concentrated on legal moves such that a legal move appears within the top k samples (k=10) in critical positions
- Evidence anchors:
  - [abstract] "Achieved a professional-level Elo rating of 1788... when permitted to sample 10 times."
  - [page 5] "Should the model fail to produce a valid move even after 50 sampling efforts... 50% chance of favoring either the best move [or random]."
  - [page 4] Figure 1 shows Pass@1 legality hovers around 90-95%, implying single-sample reliability is insufficient for high-level play without rejection sampling
- Break condition: If the temperature/top-k settings are too conservative (limiting diversity) or too high (increasing illegality), breaking the balance required to find a valid move within the sampling budget

## Foundational Learning
- **Forsyth-Edwards Notation (FEN)**
  - Why needed here: This is the input modality. Unlike PGN (game history), FEN is a snapshot of the board. Understanding that the LLM reads `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1` as the starting position is critical for data preparation
  - Quick check question: Can you identify the active player and castling rights from a standard FEN string?

- **Behavioral Cloning (Imitation Learning)**
  - Why needed here: The model is not using Reinforcement Learning (RL) to discover moves via self-play rewards; it is strictly mimicking the output of a stronger engine (Stockfish). The "learning" is purely supervised pattern matching
  - Quick check question: What is the primary failure mode of behavioral cloning when the agent encounters a state not well-represented in the expert dataset? (Distributional shift)

- **Pass@k (Sampling Strategy)**
  - Why needed here: The distinction between the model's "raw" ability (Pass@1) and its "effective" rating (Pass@10) is massive. This metric measures the probability of finding a correct solution in $k$ tries
  - Quick check question: If a model has a 60% chance of generating a legal move per sample (Pass@1), what is the approximate probability of success if you allow 3 independent samples?

## Architecture Onboarding
- Component map: Data Engine -> Input Processor -> Base Model (Open-LLaMA-3B) -> Inference Wrapper
- Critical path:
  1. Generate "Long-round" dataset using deep Stockfish evaluation (most expensive step)
  2. Format data as "USER: [FEN] ASSISTANT: [Move]" prompt structure
  3. Fine-tune LLaMA-3B on 20B tokens
  4. Evaluate via "Actual Games" against Stockfish using the 10-sample rejection protocol
- Design tradeoffs:
  - **FEN vs. PGN**: The paper chooses FEN (state-only) over PGN (history). This reduces token count (efficiency) but theoretically discards historical context (e.g., 3-fold repetition logic), though the paper claims this is sufficient for move generation
  - **Data Quality vs. Quantity**: The 350 Elo gain suggests that tokens from deep-search games are worth significantly more than tokens from quick games. Training compute is better spent on higher-quality search depth than sheer volume of low-quality games
- Failure signatures:
  - **Hallucinated Moves**: Model outputs piece movements that are geometrically impossible (e.g., Bishop moving diagonally but "jumping" like a Knight)
  - **Legality Drift**: As the game progresses beyond training distribution (move 40+), Pass@1 accuracy typically drops, requiring more samples
  - **Forgetting**: Catastrophic forgetting of general language capabilities if the fine-tuning dataset is purely chess and lacks general language instruction data
- First 3 experiments:
  1. **Legality Baseline**: Fine-tune on 0.5B tokens of short-round data and measure Pass@1 legal move accuracy on a hold-out set of FEN positions
  2. **Depth Ablation**: Train two identical models—one on short-round data (depth 20) and one on long-round data (depth 100)—and compare their Elo rating against a fixed Stockfish level (e.g., Skill Level 0)
  3. **Sampling Sensitivity**: Plot the Elo rating of the best model as a function of `k` (number of allowed samples) to find the inflection point where additional sampling yields diminishing returns

## Open Questions the Paper Calls Out
None

## Limitations
- Elo calculation methodology lacks transparency, making direct comparison to traditional chess engines difficult
- Data quality attribution doesn't control for confounding variables like dataset size and game diversity
- Performance characterization on endgame positions and positions beyond training distribution is incomplete

## Confidence
- **High Confidence**: The core finding that FEN-to-move transformation enables LLMs to play chess at a professional level is well-supported by experimental results
- **Medium Confidence**: The claim about long-round data providing 350 Elo improvement is supported but requires careful interpretation due to lack of variable control
- **Low Confidence**: The extrapolation of these results to general chess expertise or claims about fundamental AI reasoning advances

## Next Checks
1. **Independent Elo Verification**: Have an independent chess engine evaluation framework (like cutechess-cli) run a statistically significant number of games (at least 1000) between ChessLLM and Stockfish at various skill levels, using both single-sample and 10-sample protocols, to verify the reported Elo ratings and their dependency on sampling strategy

2. **Distribution Shift Analysis**: Systematically test the model's performance on positions from the endgame phase (moves 40+) and on positions not present in the training dataset. Measure Pass@1 legality and move quality degradation as a function of game progression to characterize the model's limitations

3. **Architecture Ablation Study**: Compare ChessLLM's performance against variants that use different state representations (PGN vs FEN) and different sampling strategies (temperature, top-k values). This would help isolate the contribution of the FEN representation and the rejection sampling mechanism to the overall performance