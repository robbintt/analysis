---
ver: rpa2
title: 'WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization'
arxiv_id: '2507.15061'
source_url: https://arxiv.org/abs/2507.15061
tags:
- data
- arxiv
- formalization
- webshaper
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WebShaper, a formalization-driven approach
  for synthesizing high-quality training data for information-seeking agents. The
  key innovation is formalizing information-seeking tasks using set theory and Knowledge
  Projections, enabling precise control over reasoning structures.
---

# WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization

## Quick Facts
- arXiv ID: 2507.15061
- Source URL: https://arxiv.org/abs/2507.15061
- Reference count: 7
- Key outcome: Achieves 60.1 points on GAIA and 52.2 points on WebWalkerQA, state-of-the-art among open-sourced IS agents.

## Executive Summary
WebShaper introduces a formalization-driven approach for synthesizing high-quality training data for information-seeking agents. The method uses set-theoretic Knowledge Projections to precisely control reasoning structures, enabling the generation of complex multi-hop web reasoning tasks. An agentic Expander iteratively generates and validates questions through a layer-wise expansion strategy, avoiding redundancy and reasoning shortcuts. Experiments demonstrate significant improvements over baseline datasets and reinforcement learning further boosts performance by 7.8-13.5 points on Pass@1 metrics.

## Method Summary
WebShaper constructs training data through a three-phase pipeline: seed question generation from Wikipedia random walks filtered by WebDancer/QwQ, layer-wise expansion using an agentic Expander with Search/Summarize/Validate tools, and trajectory construction with a ReAct agent. The core innovation is Knowledge Projection formalization that represents information-seeking tasks as set-theoretic operations (R-Union and Intersection) over entity-relation pairs. The Expander iteratively deepens task complexity while validation ensures question-answer consistency. The resulting 5,000 trajectories train IS agents via SFT followed by GRPO-based reinforcement learning.

## Key Results
- Achieves 60.1 points on GAIA (Levels 1-3, Avg) and 52.2 points on WebWalkerQA benchmarks
- Outperforms open-sourced IS agents on both GAIA and WebWalkerQA
- RL training improves Pass@1 by 7.8-13.5 points across different backbone sizes
- Layer-wise expansion significantly outperforms Sequential and Random expansion strategies

## Why This Works (Mechanism)

### Mechanism 1
Formalization via Knowledge Projections enables controllable, consistent reasoning structure synthesis. IS tasks are decomposed into set-theoretic operations: R-Union (∪) for broader conditions and Intersection (∩) for multiple simultaneous constraints. A Knowledge Projection R(V) = {u | ∃v ∈ V, (u,v) ∈ R} defines entity sets under relations, allowing recursive composition into complex task structures. This explicit representation lets the Expander generate questions with verifiable reasoning paths.

### Mechanism 2
Layer-wise expansion mitigates redundancy and reasoning shortcuts that plague random or sequential expansion. The KP representation is visualized as a graph (variables, constants, relations). Layer-wise expansion traverses leaf constants and replaces each with a variable connected to new sub-structures, ensuring: (1) no constant-to-constant edges (redundancy), (2) no constant-to-target direct edges (shortcuts). Each expansion preserves the answer while increasing depth.

### Mechanism 3
Agentic Expander with validation tools ensures question-answer consistency during synthesis. The Expander (ReAct-based) uses three tools: Search (Google queries), Summarize (integrates multiple URLs for R-Union), and Validate (checks consistency via QwQ). Validation verifies: (1) sub-question type matches the constant, (2) sub-question is not trivially solvable. Invalid generations trigger re-expansion.

## Foundational Learning

- Concept: Set-theoretic Knowledge Projections
  - Why needed here: Core formalization language; without understanding R(V), ∪, ∩ operations, you cannot interpret task structures or debug expansion logic.
  - Quick check question: Given R_playAt({2004}) ∩ R_bornIn({90s}), what does the resulting set represent?

- Concept: ReAct agent framework
  - Why needed here: Both Expander and inference agents follow Thought-Action-Observation loops; understanding this pattern is essential for modifying tool interfaces or trajectory construction.
  - Quick check question: In a ReAct loop, what triggers termination?

- Concept: GRPO (Group Relative Policy Optimization)
  - Why needed here: RL training uses GRPO for on-policy optimization; understanding advantage estimation and clipping is necessary for tuning hyperparameters or debugging training instability.
  - Quick check question: What does the clipping parameter ε control in GRPO?

## Architecture Onboarding

- Component map:
  - Seed Question Construction → Wikipedia random walks + LLM generation + WebDancer filtering (18k seeds)
  - KP Representation → Triplet format [X, r, S] with variables (V@) and constants (C@)
  - Layer-wise Expander → Search → Summarize → Validate loop, graph traversal
  - Trajectory Construction → ReAct agent with Search/Visit tools, 5 rollouts per question
  - Training Pipeline → SFT (masked observation loss, Eq. 12) → RL (GRPO, Eq. 13)

- Critical path:
  1. Seed quality → directly affects expansion diversity (Section 3.1)
  2. KP representation correctness → determines whether Expander interprets tasks correctly
  3. Validation rigor → prevents invalid questions from propagating
  4. Layer count (hyperparameter l) → controls task difficulty/coverage tradeoff

- Design tradeoffs:
  - Higher layer count → more complex tasks but higher synthesis cost and potential error propagation
  - Stricter validation → higher quality but lower data yield
  - R-Union merging via induction ({90s}) vs. explicit union ({1990_1991}) → readability vs. precision

- Failure signatures:
  - Redundant sentences in generated questions (constant-to-constant edges)
  - Trivial sub-questions passing validation (validator blind spots)
  - Disproportionate domain clustering (seed bias not corrected by expansion)
  - RL instability with high variance advantage estimates

- First 3 experiments:
  1. Validate KP representation parsing: Manually inspect 50 expanded questions—verify triplet lists correctly encode intended reasoning structure.
  2. Layer-wise vs. Sequential ablation: Replicate Figure 7b on a smaller backbone (7B) to confirm pattern holds at lower scale.
  3. Validation strictness sweep: Vary "too simple" threshold in Validate tool; measure tradeoff between data yield and GAIA Pass@1.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Knowledge Projection formalization be extended to support negation and universal quantification operations while maintaining tractable validation?
- Basis in paper: [explicit] The formalization handles only R-Union (Eq. 3) and Intersection (Eq. 4) operations. Negation (¬R(S)) and universal quantification (∀), common in complex information-seeking tasks, are not addressed.
- Why unresolved: Proposition 1 proves distributivity for R-Union, but negation and universal quantification require different mathematical treatment that may compromise the validation guarantees enabling structural consistency (Section 3.2.3).
- What evidence would resolve it: Formal extension of KP to include negation and universal quantification, theoretical analysis of validation tractability, and empirical comparison on tasks requiring these operations.

### Open Question 2
What is the optimal distribution of task complexity levels in training data for maximizing reinforcement learning gains?
- Basis in paper: [inferred] Section 4.3.3 shows RL improves Pass@1 by 7.8-13.5 points, attributing gains to "breadth and complexity of tasks," but does not characterize how complexity distribution affects RL effectiveness.
- Why unresolved: The 5,000 trajectories have an implicit complexity distribution, but the relationship between this distribution and RL stimulation remains unexplored.
- What evidence would resolve it: Controlled experiments varying proportions of low/medium/high complexity tasks (measured by KP depth or tool call counts) and measuring RL improvement curves.

### Open Question 3
Does filtering seed questions using QwQ introduce systematic biases in task type diversity?
- Basis in paper: [inferred] Section 3.1 filters seed questions by completing them with WebDancer/QwQ, keeping only those with ≥1 correct rollout. QwQ's limitations may preferentially exclude valid questions requiring reasoning patterns it struggles with.
- Why unresolved: While Figure 5 shows balanced domain distribution, task type diversity at the reasoning-structure level may be narrowed by filter model biases.
- What evidence would resolve it: Comparing task type distributions before/after filtering, plus training on datasets filtered by different models (e.g., DeepSeek-R1, GPT-4o) to measure sensitivity.

### Open Question 4
Can the layer-wise expansion strategy be improved through adaptive depth selection based on task characteristics?
- Basis in paper: [inferred] Section 3.2.2 fixes expansion depth l as a hyperparameter applied uniformly, but does not explore whether different task types benefit from different expansion depths.
- Why unresolved: Uniform depth may over-simplify some seed tasks while over-complicating others, potentially affecting data quality.
- What evidence would resolve it: Experiments with adaptive depth policies (e.g., based on seed question complexity, domain, or retrieval difficulty) compared against fixed-depth baselines on final agent performance.

## Limitations
- KP formalization expressiveness may not capture complex temporal or procedural reasoning that cannot be mapped to entity-relation pairs
- Validation reliability concerns due to QwQ serving as both agent and validator, potentially introducing confirmation bias
- Layer-wise expansion scalability is unclear beyond the reported 5,000 trajectories due to computational costs

## Confidence
- **High confidence**: Pass@1 improvements on GAIA and WebWalkerQA benchmarks (60.1 and 52.2 points respectively)
- **Medium confidence**: Layer-wise expansion superiority over Sequential/Random methods
- **Low confidence**: Real-world capability translation without human evaluation of task complexity

## Next Checks
1. **KP representation validation**: Manually inspect 100 expanded questions to verify that triplet lists correctly encode the intended reasoning structure and that no constant-to-target edges exist.
2. **Cross-dataset generalization**: Evaluate the trained agent on held-out domains not represented in the WebShaper corpus to test whether formalization enables genuine reasoning transfer or mere pattern matching.
3. **Validator independence test**: Run a subset of questions through an independent LLM-as-Judge to measure agreement rates with QwQ validation, quantifying potential bias in the synthesis pipeline.