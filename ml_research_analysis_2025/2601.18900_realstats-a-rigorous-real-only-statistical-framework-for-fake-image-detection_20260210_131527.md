---
ver: rpa2
title: 'RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection'
arxiv_id: '2601.18900'
source_url: https://arxiv.org/abs/2601.18900
tags:
- real
- images
- image
- statistics
- fake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free statistical framework for
  detecting AI-generated images by testing whether a candidate image is drawn from
  the real-image distribution. The method computes scalar statistics over real images,
  maps them to empirical p-values, and aggregates independent p-values into a single
  interpretable score.
---

# RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection

## Quick Facts
- arXiv ID: 2601.18900
- Source URL: https://arxiv.org/abs/2601.18900
- Authors: Haim Zisman; Uri Shaham
- Reference count: 35
- Primary result: Training-free fake image detection achieving Avg AUC=0.775 and Avg AP=0.756 on large-scale benchmarks

## Executive Summary
This paper introduces a training-free statistical framework for detecting AI-generated images by testing whether a candidate image is drawn from the real-image distribution. The method computes scalar statistics over real images, maps them to empirical p-values, and aggregates independent p-values into a single interpretable score. It avoids reliance on fake data, supports adaptability to evolving generative models, and allows integration of new statistics. The framework was evaluated on large-scale benchmarks including CNNSpot, Universal Fake Detect, Stable-Diffusion-Faces, Synthbuster, and GenImage. Results show competitive average AUC (0.775) and AP (0.756) compared to state-of-the-art baselines, with improved stability and interpretability. Incorporating additional statistics further improved performance on challenging generators. The approach is scalable, efficient, and robust to image corruptions.

## Method Summary
RealStats uses a two-phase approach: null distribution modeling and inference. During modeling, it computes perturbation-stability statistics across multiple frozen encoders (CLIP, DINOv2, DINOv3, ConvNeXt, BEiT) at λ∈{0.05,0.10}, builds ECDFs for each statistic, performs pairwise χ² independence tests, constructs an independence graph using Cramér's V threshold (V_χ²=0.07), and extracts a maximal clique passing KS uniformity test. During inference, it computes only selected statistics, maps them to two-sided p-values via stored ECDFs, and aggregates using Stouffer or min-p methods. The framework requires only a reference set of real images and avoids any fake data during training.

## Key Results
- Achieves competitive Avg AUC of 0.775 and Avg AP of 0.756 across multiple benchmarks
- Outperforms many training-based baselines while maintaining training-free approach
- Shows improved stability and interpretability compared to single-statistic methods
- Robust to JPEG compression with ~5-6% AUC drop
- Adaptable through integration of additional statistics (e.g., ManifoldBias improved performance on CycleGAN)

## Why This Works (Mechanism)

### Mechanism 1: Empirical P-Value Calibration Against Real Distribution
The method computes scalar statistics s(x) over real images and constructs empirical cumulative distribution functions (ECDFs). For a test image, it computes a two-sided empirical p-value: p(x) = 2·min(F̂_N(s(x)), 1 - F̂_N(s(x))). This captures extremeness in either direction, avoiding assumptions about whether fakes score higher or lower than reals. The core assumption is that reference samples are i.i.d. draws from the true real-image distribution.

### Mechanism 2: Independence Enforcement via Graph-Based Subset Selection
The method tests pairwise independence among all candidate statistics using χ² tests on p-value distributions over real images. It quantifies association via Cramér's V (threshold V_χ² = 0.07), constructs an independence graph where edges indicate weak association, and extracts a maximal clique. A Kolmogorov–Smirnov test verifies that aggregated p-values remain uniform under the null.

### Mechanism 3: Multi-Statistic Perturbation Stability as Deviation Signal
For each encoder fⱼ and perturbation strength λₖ, compute s_{j,k}(x) = sim(fⱼ(x), fⱼ(x + λₖδ)). Real images typically yield more stable embeddings (higher similarity). The framework uses encoders including CLIP ViT-L/14, DINOv2, DINOv3, ConvNeXT, and BEiT with λ ∈ {0.05, 0.10}, maps each to a p-value, and aggregates via Stouffer's test or the minimum-p method.

## Foundational Learning

- **Concept: Empirical Cumulative Distribution Function (ECDF)**
  - Why needed here: The core detection logic depends on estimating the distribution of each statistic over real images and computing tail probabilities (p-values).
  - Quick check question: Given samples {0.2, 0.5, 0.8}, what is the ECDF value at 0.6? (Answer: 2/3)

- **Concept: P-Value Interpretation and Two-Sided Tests**
  - Why needed here: RealStats outputs unified p-values, not binary labels. Interpreting results requires understanding that a p-value of 0.01 means ~1% probability of observing such extremeness if the image were real.
  - Quick check question: Under H₀, what distribution should p-values follow if the test is valid? (Answer: Uniform[0,1])

- **Concept: Pairwise Independence Testing via χ² and Cramér's V**
  - Why needed here: The independence selection stage uses χ² tests to detect dependence between statistics and Cramér's V to quantify effect size.
  - Quick check question: Why use Cramér's V instead of χ² p-values for large samples? (Answer: χ² p-values become unstable/uninformative with large N; Cramér's V normalizes effect size.)

## Architecture Onboarding

- **Component map:** Statistic extraction module -> ECDF estimation per statistic -> Independence graph construction (χ² + Cramér's V) -> Maximal clique enumeration with KS uniformity check -> Compute selected statistics -> Map to p-values via ECDFs -> Aggregate via Stouffer or min-p method

- **Critical path:** Acquire representative real-image reference set (30% held out for modeling) -> Run statistic extraction over all encoder-λ configurations (parallelizable across GPU workers) -> Compute pairwise Cramér's V, construct graph, enumerate cliques (negligible runtime) -> Validate uniformity via KS test; select clique containing preferred encoders -> At inference: compute only selected statistics → lookup ECDF → aggregate p-value

- **Design tradeoffs:** Stouffer vs. Min-p aggregation (Stouffer amplifies moderate collective deviation; Min-p emphasizes strongest individual signal). Cramér's V threshold (0.07) balances independence strictness vs. clique size. Statistic diversity vs. independence (excluding correlated but informative detectors may hurt separability).

- **Failure signatures:** Reference-test mismatch (real p-values deviate from uniformity; discriminative signal may persist but formal interpretation compromised). Generator-specific collapse (single-statistic methods show high variance; RealStats reduces but doesn't eliminate this). JPEG compression (~5-6% AUC drop; p-value validity preserved but separability reduced).

- **First 3 experiments:** Reproduce null distribution modeling on a small subset (1K real images, 3 encoder-λ pairs, verify p-values approximate uniformity via histogram/KS test). Ablate independence selection (compare AUC using all statistics vs. independent subset; expect latter to maintain p-value uniformity under H₀). Domain shift stress test (construct ECDFs from FFHQ, evaluate on CelebA-HQ; replicate finding that real p-values deviate from uniformity but AUC may remain competitive).

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the framework perform under sophisticated adversarial attacks designed to fool multiple statistics simultaneously? The preliminary analysis tested gradient-based perturbations against individual statistics, not coordinated attacks exploiting dependencies across the aggregated ensemble.

- **Open Question 2:** Can the pairwise independence criterion be relaxed to include more informative correlated statistics while maintaining statistical validity? The current framework enforces pairwise independence via Cramér's V thresholding, but this may discard statistics that provide complementary detection signals despite correlation.

- **Open Question 3:** What is the minimum reference dataset size required to achieve reliable ECDF estimates across diverse image domains? The paper uses 30% of real images but doesn't characterize how detection performance or p-value validity degrades as reference size decreases.

## Limitations

- Reference distribution mismatch can compromise p-value validity even when discriminative signal persists
- Pairwise independence criterion may exclude informative correlated statistics, reducing separability
- Minimum reference dataset size for reliable performance across diverse domains is not characterized

## Confidence

High confidence in core statistical framework validity and empirical results. Medium confidence in independence enforcement methodology and its impact on detection performance. Low confidence in robustness to sophisticated adversarial attacks.

## Next Checks

1. Verify null distribution modeling produces uniform p-values on held-out real images by histogram analysis and KS test
2. Compare detection AUC with and without independence selection to assess statistical validity tradeoff
3. Test domain shift by constructing ECDFs from one dataset and evaluating on a different dataset to observe p-value uniformity and AUC behavior