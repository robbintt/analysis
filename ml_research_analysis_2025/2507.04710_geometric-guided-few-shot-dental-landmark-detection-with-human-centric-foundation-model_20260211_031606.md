---
ver: rpa2
title: Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation
  Model
arxiv_id: '2507.04710'
source_url: https://arxiv.org/abs/2507.04710
tags:
- detection
- landmark
- geosapiens
- teeth
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot dental landmark
  detection in CBCT images, which is crucial for orthodontic, periodontic, and implant
  dentistry but limited by scarce training data and high annotation costs. The authors
  propose GeoSapiens, a framework combining a human-centric foundation model (Sapiens)
  with LoRA fine-tuning for efficiency and a novel geometric loss function to capture
  anatomical relationships.
---

# Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model

## Quick Facts
- **arXiv ID**: 2507.04710
- **Source URL**: https://arxiv.org/abs/2507.04710
- **Reference count**: 20
- **Primary result**: 8.18% higher success detection rate at 0.5 mm threshold compared to best existing method

## Executive Summary
This paper addresses the challenge of few-shot dental landmark detection in CBCT images, which is crucial for orthodontic, periodontic, and implant dentistry but limited by scarce training data and high annotation costs. The authors propose GeoSapiens, a framework combining a human-centric foundation model (Sapiens) with LoRA fine-tuning for efficiency and a novel geometric loss function to capture anatomical relationships. Experiments on a newly collected anterior teeth dataset (LDTeeth) show GeoSapiens achieves an 8.18% higher success detection rate at the clinically relevant 0.5 mm threshold compared to the best existing method, with an SDRaverage of 80.23% and MRE of 0.747 mm. The approach reduces trainable parameters by 92.73% while maintaining strong performance.

## Method Summary
The framework uses Sapiens-0.3B Vision Transformer backbone pre-trained on 300M human-centric images, fine-tuned with LoRA (r=4, α=4 for qkv; r=8, α=8 for proj) to reduce trainable parameters from 330M to 24M. A top-down heatmap decoder generates 16 heatmaps for dental landmarks. The geometric loss enforces perpendicularity and parallelism constraints between anatomical lines defined by landmark coordinates, computed via soft-argmax. The model is trained on a strict patient-separated dataset (3 patients for training, 3 for validation, 16 for testing) with 16 landmarks per image.

## Key Results
- GeoSapiens achieves SDRaverage of 80.23% and MRE of 0.747 mm on anterior teeth CBCT landmark detection
- 8.18% higher success detection rate at 0.5 mm threshold compared to best existing method
- Reduces trainable parameters by 92.73% while maintaining comparable performance to full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Symmetry-Aware Foundation Model Transfer
The Sapiens foundation model, pre-trained on 300M human-centric images, transfers effectively to dental landmark detection in CBCT images when fine-tuned with limited data. The shared symmetrical and morphological features between human body structures (from pre-training) and anterior teeth CBCT images create favorable feature distribution alignment. The Vision Transformer backbone learns robust visual representations through Masked Autoencoder pre-training that generalize across domains with similar structural patterns.

### Mechanism 2: Geometric Loss for Anatomical Constraint Encoding
Explicitly encoding anatomical geometric relationships (perpendicularity and parallelism between landmark-defined lines) through a differentiable loss function improves few-shot generalization. The geometric loss function L_geo computes dot products between unit direction vectors derived from landmark coordinates extracted via soft-argmax. Perpendicularity between the AP-CP axis and horizontal anatomical lines, plus parallelism among the three horizontal lines (Root Apex Level, Apical Third Level, Mid-root Level), are enforced as soft constraints during training, guiding the model toward anatomically coherent predictions.

### Mechanism 3: Low-Rank Adaptation for Efficient Few-Shot Fine-Tuning
LoRA applied to attention layers enables effective fine-tuning of a 0.3B parameter Vision Transformer using only 24M trainable parameters (92.73% reduction) without significant performance degradation. LoRA injects trainable low-rank decomposition matrices into the query-key-value projections (α=4, r=4) and output projections (α=8, r=8) of transformer attention layers while freezing pre-trained weights. This constrains weight updates to a low-dimensional subspace, preventing overfitting to the small training set (3 patients, 36 images) while preserving the foundation model's learned representations.

## Foundational Learning

- **Soft-Argmax for Differentiable Coordinate Extraction**
  - Why needed here: Standard argmax is non-differentiable, preventing gradient-based optimization of coordinate-level losses. The geometric loss requires landmark coordinates as continuous values during training.
  - Quick check question: Can you explain why taking the expected value over a heat-map (weighted sum of coordinates) enables backpropagation through landmark positions, and how temperature T controls the sharpness of this approximation?

- **Heatmap-Based Landmark Detection with MSE Loss**
  - Why needed here: The baseline model outputs Gaussian heatmaps centered at landmark locations rather than direct coordinate regression, requiring understanding of how heatmap generation, MSE loss over all pixels, and coordinate extraction compose during training versus inference.
  - Quick check question: Why might the MSE loss magnitude be naturally small compared to geometric loss, and what does this imply for loss weighting (λ = 10^-5)?

- **LoRA (Low-Rank Adaptation) Mechanics**
  - Why needed here: Understanding how LoRA modifies attention layers is critical for implementation—specifically, how the low-rank matrices A and B are initialized, how they compose with frozen weights (W + BA), and why this constrains the hypothesis space.
  - Quick check question: If a pre-trained weight matrix W has dimension d×d, and LoRA uses rank r, how many new trainable parameters are introduced per layer, and why does α (scaling factor) matter for initialization?

## Architecture Onboarding

- **Component map**: Input image → ViT encoder (LoRA-modified attention) → feature maps → heatmap decoder → 16 heatmaps → soft-argmax → coordinates → geometric loss (during training only)
- **Critical path**: Input image → ViT encoder (LoRA-modified attention) → feature maps → heatmap decoder → 16 heatmaps → soft-argmax → coordinates → geometric loss (during training only). During inference, argmax replaces soft-argmax.
- **Design tradeoffs**:
  - Full fine-tuning vs. LoRA: Full fine-tuning (330M params) achieves slightly better MRE (0.740mm vs 0.747mm) but is impractical for clinical deployment; LoRA sacrifices ~1% performance for 13× parameter reduction.
  - Geometric loss weight (λ): Set to 10^-5 to balance against MSE loss magnitude; too high may conflict with pixel-level accuracy, too low provides insufficient structural guidance.
  - Temperature T=0.1: Balances coordinate sharpness (approaching argmax) versus utilizing neighboring probability mass for gradient signal.
- **Failure signatures**:
  - Geometric loss not decreasing: Check soft-argmax temperature and coordinate normalization; verify line-fitting from coordinates produces valid unit vectors.
  - Performance degrades with LoRA: Increase rank r or scaling α; ensure LoRA is applied to both Q/K/V and projection layers.
  - Good SDR@2mm but poor SDR@0.5mm: Model captures coarse structure but lacks precision—consider increasing geometric loss weight or adding landmark-specific uncertainty weighting.
- **First 3 experiments**:
  1. Reproduce baseline comparison: Train GU2Net, FM-OSD, NFDP, and GeoSapiens on LDTeeth with 3-patient training split; verify SDR@0.5mm matches reported values (±2% tolerance).
  2. Ablate geometric loss: Train GeoSapiens with L_geo disabled (λ=0); confirm performance drop at tight thresholds (0.5mm, 1.0mm) per Table 3 patterns.
  3. LoRA rank sensitivity: Vary r ∈ {2, 4, 8, 16} while holding other hyperparameters fixed; plot trainable parameters vs. SDR_avg to characterize efficiency-accuracy frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the geometric loss function hinder detection accuracy in cases of severe pathological deformity where standard anatomical angular relationships (perpendicularity/parallelism) do not hold?
- **Basis in paper:** [Inferred] The geometric loss enforces strict constraints (v_⊥ · v_j ≈ 0) based on the standard annotation protocol (Section 2.3), but the dataset description does not explicitly filter for or analyze pathological cases where bone loss might distort these geometric priors.
- **Why unresolved:** The paper evaluates overall performance on a general dataset but does not isolate performance on anatomically irregular samples where the "structural priors" enforced by the loss might conflict with ground truth.
- **What evidence would resolve it:** A subgroup analysis of detection error rates specifically on teeth with severe dehiscence, fenestration, or curvature compared to normal tooth structures.

### Open Question 2
- **Question:** How does GeoSapiens performance scale when expanding the training data beyond the extreme few-shot (3-patient) regime?
- **Basis in paper:** [Explicit] The authors state: "To represent the few-shot setting, we utilized a training set containing images from only 3 patients" (Section 3, Implementation details).
- **Why unresolved:** It is unclear if the 8.18% improvement over baselines is sustained as data increases, or if the foundation model's prior simply provides a steeper learning curve that plateaus early compared to methods designed for larger datasets.
- **What evidence would resolve it:** A scaling analysis showing the SDR/MRE trend for GeoSapiens vs. baselines when trained on 10, 50, and 100+ patients.

### Open Question 3
- **Question:** Can the framework generalize to posterior teeth which lack the distinct symmetrical features hypothesized to facilitate the transfer from the human-centric Sapiens model?
- **Basis in paper:** [Explicit] The paper notes the method is designed for "anterior teeth" and states: "anterior teeth exhibit significant morphological differences... based on their position... making it highly challenging... to capture these features" (Section 2.1).
- **Why unresolved:** The theoretical motivation relies on the similarity between "symmetrical characteristics" of anterior teeth and human bodies; posterior teeth (molars) possess different morphologies and root complexities that may not benefit from the Sapiens initialization.
- **What evidence would resolve it:** Experimental results applying the same GeoSapiens architecture to a newly collected dataset of posterior teeth without altering the pre-trained backbone.

## Limitations

- **Dataset dependency**: The approach relies on a custom LDTeeth dataset that may not be publicly available, blocking reproduction and generalizability testing.
- **Limited pathological validation**: The geometric loss constraints may not hold for teeth with severe deformities, but the paper doesn't test robustness to pathological variations.
- **Transfer mechanism uncertainty**: The claimed similarity between human-centric and dental features lacks quantitative validation and may not generalize to other medical domains.

## Confidence

- **High Confidence**: LoRA fine-tuning mechanism and parameter reduction (92.73% with 7% performance trade-off) are well-supported by ablation study and established techniques.
- **Medium Confidence**: Overall performance improvement (8.18% SDR@0.5mm over best baseline) is directly measured but tempered by dataset access uncertainty.
- **Low Confidence**: Mechanism of Sapiens foundation model transfer via shared "structural patterns" and "symmetrical characteristics" is primarily intuitive without quantitative validation.

## Next Checks

1. **Dataset Accessibility Verification**: Confirm whether the LDTeeth dataset is publicly available via the GitHub repository or if access requires a separate request. If unavailable, create a synthetic dataset following the 16-landmark protocol to test the framework's implementation and baseline performance.

2. **Geometric Loss Robustness Test**: Design an experiment introducing controlled anatomical variations (simulated malocclusions, root asymmetries) into the test set. Measure whether SDR@0.5mm degrades significantly, indicating the geometric constraints may be overfitting to idealized anatomy.

3. **Foundation Model Transfer Analysis**: Perform an ablation where the Sapiens backbone is replaced with a randomly initialized ViT of the same size. Train with full fine-tuning (no LoRA) on the 3-patient split. Compare SDR@0.5mm to quantify the actual contribution of pre-trained human-centric features versus the architecture and fine-tuning strategy alone.