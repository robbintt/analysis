---
ver: rpa2
title: Mitigating Forgetting Between Supervised and Reinforcement Learning Yields
  Stronger Reasoners
arxiv_id: '2510.04454'
source_url: https://arxiv.org/abs/2510.04454
tags:
- reasoning
- data
- updates
- arxiv
- mifo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of combining supervised fine-tuning
  (SFT) and reinforcement learning (RL) for mathematical reasoning in large language
  models. While RL excels at refining reasoning through self-generated trajectories,
  it struggles to acquire new knowledge and risks overwriting learned reasoning skills
  when combined with SFT, which introduces out-of-distribution data.
---

# Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners

## Quick Facts
- arXiv ID: 2510.04454
- Source URL: https://arxiv.org/abs/2510.04454
- Reference count: 40
- Primary result: State-of-the-art mathematical reasoning using only 1.5% of SFT data and 20.4% of RL data compared to prior SoTA

## Executive Summary
This paper addresses the challenge of combining supervised fine-tuning (SFT) and reinforcement learning (RL) for mathematical reasoning in large language models. While RL excels at refining reasoning through self-generated trajectories, it struggles to acquire new knowledge and risks overwriting learned reasoning skills when combined with SFT, which introduces out-of-distribution data. The proposed MIFO framework dynamically interleaves SFT within RL, selecting only challenging examples and high-entropy tokens for SFT updates, while freezing parameters identified as important for RL during SFT to prevent catastrophic forgetting. This approach achieves state-of-the-art reasoning performance with dramatically reduced data requirements.

## Method Summary
MIFO combines SFT and RL through an interleaved training schedule where SFT phases are dynamically triggered based on RL performance. During RL rollouts, questions with accuracy below a threshold (1/8) are buffered along with verified solutions. When the buffer fills, SFT training is triggered using only high-entropy tokens (top 20% by token-level entropy) to reduce redundant updates. Critically, MIFO tracks parameter importance during RL by maintaining an exponential moving average of squared parameter updates, then freezes the top 50% of RL-important parameters during subsequent SFT phases to protect reasoning capabilities from being overwritten. This creates a cyclical process of RL exploration, SFT knowledge injection, and forgetting prevention.

## Key Results
- Achieves state-of-the-art mathematical reasoning performance across six benchmarks (AIME 2024/2025, AMC, OlympiadBench, MATH500, MMLU-Pro)
- Uses only 1.5% of SFT data and 20.4% of RL data compared to prior SoTA (SRFT)
- Produces more concise responses while maintaining or improving accuracy
- Demonstrates significant improvements in data efficiency without sacrificing reasoning quality

## Why This Works (Mechanism)

### Mechanism 1: High-Entropy Token Selection Reduces Redundant SFT Updates
- Claim: Computing SFT loss only on high-entropy tokens reduces parameter update magnitude while maintaining learning effectiveness.
- Mechanism: By filtering tokens where the model is already confident (low entropy), gradients concentrate on genuinely uncertain positions. This avoids spreading updates across redundant tokens that would increase update magnitude without proportional learning benefit.
- Core assumption: High-entropy tokens encode the informative learning signal; low-entropy tokens contribute primarily to redundant updates and potential overfitting.
- Evidence anchors:
  - [abstract]: "we select high-entropy tokens for loss calculation and freeze parameters identified as critical for RL"
  - [section 4.1]: "Concentrating learning on these tokens encourages acquisition of missing knowledge while avoiding unnecessary fitting of low-entropy (confident) tokens, which can exacerbate overfitting or lead to entropy collapse"
  - [section 5.4, Figure 6]: High-entropy-only loss yields smaller weight updates than full-token loss for most layers
  - [corpus]: Related work (Wang et al., 2025a) on high-entropy minority tokens driving RL effectiveness supports this premise
- Break condition: If entropy-based selection does not reduce update magnitude, or if it significantly degrades reasoning performance compared to full-token SFT.

### Mechanism 2: Freezing RL-Critical Parameters Prevents Catastrophic Overwriting
- Claim: Freezing parameters that received large updates during RL protects RL-acquired reasoning skills from being overwritten during SFT.
- Mechanism: The method maintains an importance map tracking cumulative RL parameter updates. Top-k parameters by magnitude are frozen (gradient zeroed) during subsequent SFT, then unfrozen for the next RL phase.
- Core assumption: RL induces more "valuable" updates per parameter than SFT; losing RL updates harms reasoning more than losing equivalent SFT updates.
- Evidence anchors:
  - [section 3.1, Figure 1-2]: Dropping 50% of gradients causes 4.23 pass@1 drop for RL vs only 0.7 for SFT; RL shows consistent degradation with pruning while SFT can tolerate or benefit from small pruning
  - [section 3.2, Figure 3]: SFT produces substantially larger parameter changes than RL across all layers
  - [corpus]: "Retaining by Doing" (corpus neighbor) discusses on-policy data's role in mitigating forgetting—related but addresses data rather than parameter-level protection
- Break condition: If freezing harms overall convergence, or if importance map doesn't correlate with actual parameter criticality for reasoning.

### Mechanism 3: Selective Buffering Based on Rollout Difficulty
- Claim: Selecting SFT examples based on low RL rollout accuracy (challenging cases) enables knowledge injection only where needed.
- Mechanism: During RL rollouts, questions with accuracy ≤ p are buffered along with verified solutions. SFT trains on this buffer when full, then returns to RL. This targets cases where self-exploration fails and external knowledge is required.
- Core assumption: Questions the model cannot solve via RL rollouts indicate genuine knowledge gaps; already-solved questions need no SFT reinforcement.
- Evidence anchors:
  - [abstract]: "dynamically integrates SFT into RL by selecting challenging examples for SFT... reduces SFT data requirements"
  - [section 4.1]: Buffer collection threshold generalized from ReLIFT's acc(q)=0 to acc(q)≤p "which admits more informative yet solvable cases"
  - [section 5.2, Figure 5]: MIFO uses only 1.5% of SFT data and 20.4% of RL data vs prior SoTA (SRFT)
  - [corpus]: No direct corpus evidence for this specific thresholding mechanism
- Break condition: If threshold is too restrictive (missing useful supervision) or too permissive (including unnecessary data that triggers forgetting).

## Foundational Learning

- **Catastrophic forgetting in sequential training**
  - Why needed here: The paper's central thesis is that SFT overwrites RL-learned reasoning patterns. Understanding why sequential optimization on different objectives causes forgetting is essential to grasp why MIFO's interventions help.
  - Quick check question: Why does training on a new objective (SFT) cause a neural network to degrade on previously learned capabilities (RL), and what parameters would you expect to be most vulnerable?

- **Policy gradient methods and advantage estimation (GRPO/PPO)**
  - Why needed here: The RL component uses GRPO with group-based advantage normalization. Understanding policy gradients, advantage functions, and clipping is necessary to implement, debug, and potentially substitute alternative RL algorithms.
  - Quick check question: In GRPO, how is the advantage computed per-group, and why does clipping the probability ratio prevent excessive policy degradation?

- **Gradient sparsification and parameter importance**
  - Why needed here: The method relies on identifying "important" parameters via update magnitude. Understanding how gradient masking affects learning dynamics helps interpret why selective freezing works.
  - Quick check question: If you randomly zero 50% of gradients during training, what would you expect to happen to convergence speed and final performance, and why might some training objectives tolerate this better than others?

## Architecture Onboarding

- **Component map:**
  - RL rollout → compute per-question accuracy → add challenging examples to buffer
  - RL update → record parameter deltas → update importance map
  - When buffer ≥ S: switch to SFT
  - SFT: compute entropy per token → mask low-entropy tokens → apply parameter freeze mask → update
  - Unfreeze all → return to step 1

- **Critical path:**
  1. RL rollout → compute per-question accuracy → add challenging examples to buffer
  2. RL update → record parameter deltas → update importance map
  3. When buffer ≥ S: switch to SFT
  4. SFT: compute entropy per token → mask low-entropy tokens → apply parameter freeze mask → update
  5. Unfreeze all → return to step 1

- **Design tradeoffs:**
  - **Buffer threshold p**: Lower = less SFT data, but may miss edge cases. Paper: p=1/8
  - **Entropy ratio ρ**: Higher = more tokens in loss. Paper: ρ=0.2
  - **Freeze fraction k**: Higher = more protection, less SFT flexibility. Paper: k=0.5
  - **Assumption**: These hyperparameters are claimed insensitive (Appendix E.1), but this should be validated on your target domain.

- **Failure signatures:**
  - **Oscillating validation performance**: RL-SFT switching may be too frequent; try increasing buffer size S
  - **SFT buffer never fills**: Threshold p too strict for current model capability; increase p
  - **Performance degrades after SFT phases**: Freezing may be insufficient; try increasing k or verify importance map is being updated correctly
  - **Response length explodes**: High-entropy selection without PF can encourage excessive exploration; verify PF is active

- **First 3 experiments:**
  1. **Validate core assumption**: Reproduce Figure 1/2 gradient-dropping experiment on your base model to confirm SFT redundancy vs RL parsimony before implementing full MIFO.
  2. **Ablation path**: Run Interleave-only baseline, then add entropy selection (ES) only—monitor parameter update magnitudes to verify ES reduces updates (as in Figure 6).
  3. **Hyperparameter sensitivity**: Full MIFO with sweep on buffer threshold p ∈ {1/8, 2/8, 3/8} to validate claimed insensitivity before committing to long training runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will MIFO's parameter freezing strategy remain effective when scaled to much larger language models (e.g., 70B+ parameters)?
- Basis in paper: [explicit] The authors state: "Our study focuses on small (1.5B) and medium (7B) models due to limited computational resources. Scaling the method to larger models is a promising direction for future work."
- Why unresolved: The redundancy and parsimony patterns observed between SFT and RL may change with model scale; freezing strategies optimized for smaller models may not transfer directly.
- What evidence would resolve it: Empirical evaluation of MIFO on models ranging from 13B to 70B+ parameters, analyzing whether the same TopK freezing fraction and entropy thresholds remain optimal.

### Open Question 2
- Question: How should the freezing strategy be adapted for parameter-efficient fine-tuning (PEFT) methods like LoRA?
- Basis in paper: [explicit] The authors note: "Our approach primarily targets full-parameter SFT; when applying parameter-efficient fine-tuning (PEFT), the freezing strategy may need to be adjusted to prevent catastrophic forgetting between SFT and RL."
- Why unresolved: PEFT methods modify only a subset of parameters by design; the importance mapping and freezing mechanism developed for full-parameter tuning may not directly apply.
- What evidence would resolve it: Experiments applying MIFO with LoRA, Prefix-tuning, or Adapter-based methods, comparing different strategies for tracking and protecting RL-critical parameters within PEFT constraints.

### Open Question 3
- Question: What is the nature of the partial overlap between parameters important for SFT versus RL, and can this overlap be exploited?
- Basis in paper: [inferred] The authors observe: "RL top-K selection is slightly outperformed by random selection, suggesting partial overlap between parameters important for SFT and those important for RL" (Appendix E.4, p.19).
- Why unresolved: The paper documents this phenomenon but does not characterize which parameters are shared vs. task-specific, nor whether selective freezing could jointly preserve both.
- What evidence would resolve it: Layer-wise and head-wise analysis comparing parameter importance rankings from SFT and RL trajectories; experiments with overlap-aware freezing strategies.

### Open Question 4
- Question: Can MIFO generalize effectively to non-mathematical reasoning domains and tasks with non-binary reward structures?
- Basis in paper: [inferred] The evaluation is limited to mathematical reasoning benchmarks (AIME, AMC, MATH-500, OlympiadBench, MMLU-Pro), and the reward is defined as binary: "R = 1 if the answer is correct, 0 otherwise" (Appendix D, p.16).
- Why unresolved: Mathematical reasoning may have distinct entropy and accuracy patterns; tasks with partial credit or continuous rewards may require different entropy thresholds or buffer selection criteria.
- What evidence would resolve it: Evaluation on code generation, logical reasoning, or multi-step planning tasks with graded reward functions, analyzing whether the entropy-based token selection and accuracy-based buffering remain effective.

## Limitations
- **Solution quality source**: The paper assumes access to verified high-quality solutions D(q) for buffered questions, but doesn't specify whether these come from the original dataset, a separate stronger model, or human annotation. This is critical because poor-quality D(q) would directly poison SFT updates and negate MIFO's benefits.
- **Hyperparameter specifics**: While k=0.5, ρ=0.2, and p=1/8 are specified, the buffer size threshold S that triggers SFT phases is not numerically stated, leaving a gap in reproducing the exact switching behavior.
- **Evaluation scope**: Results are benchmarked only on mathematical reasoning tasks (AIME, AMC, OlympiadBench, MATH500, MMLU-Pro). The method's generalizability to other domains (e.g., code, reasoning in other languages) remains untested.

## Confidence
- **High confidence**: The core mechanism of interleaving SFT within RL with parameter freezing is well-supported by ablation results and the foundational observation that RL parameters are more "fragile" than SFT parameters.
- **Medium confidence**: The high-entropy token selection reduces redundant updates and overfitting is plausible given the gradient magnitude analysis in Figure 6, but the causal link between entropy reduction and reasoning improvement needs more direct validation.
- **Medium confidence**: The claim that MIFO achieves SoTA with 1.5% of SFT data and 20.4% of RL data is strong, but the comparison to SRFT assumes SRFT used the full OpenR1-Math-46k dataset, which isn't explicitly stated in the cited work.

## Next Checks
1. **Verify the foundational assumption**: Reproduce the gradient-dropping experiment (Figure 1-2) on your base model to confirm that RL is indeed more sensitive to gradient pruning than SFT before implementing the full MIFO framework.
2. **Test solution quality sensitivity**: Run MIFO with two different sources of D(q)—one using high-quality dataset solutions and one using lower-quality generated solutions—to confirm that solution quality is a critical determinant of MIFO's success.
3. **Validate hyperparameter insensitivity**: Conduct a formal sweep of the buffer threshold p ∈ {1/8, 2/8, 3/8} and report performance variance to substantiate the claim that p is insensitive and to identify the optimal setting for your target domain.