---
ver: rpa2
title: A Review on Large Language Models for Visual Analytics
arxiv_id: '2503.15176'
source_url: https://arxiv.org/abs/2503.15176
tags:
- data
- visual
- language
- analytics
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive review of the integration of
  Large Language Models (LLMs) with visual analytics, addressing their foundational
  concepts, capabilities, and wide-ranging applications. It begins by outlining the
  theoretical underpinnings of visual analytics and the transformative potential of
  LLMs, specifically focusing on their roles in natural language understanding, natural
  language generation, dialogue systems, and text-to-media transformations.
---

# A Review on Large Language Models for Visual Analytics

## Quick Facts
- arXiv ID: 2503.15176
- Source URL: https://arxiv.org/abs/2503.15176
- Authors: Navya Sonal Agarwal; Sanjay Kumar Sonbhadra
- Reference count: 40
- One-line primary result: Comprehensive review of LLM integration with visual analytics, evaluating tools, taxonomies, and addressing ethical considerations.

## Executive Summary
This paper provides a comprehensive review of the integration of Large Language Models (LLMs) with visual analytics, addressing their foundational concepts, capabilities, and wide-ranging applications. It begins by outlining the theoretical underpinnings of visual analytics and the transformative potential of LLMs, specifically focusing on their roles in natural language understanding, natural language generation, dialogue systems, and text-to-media transformations. The review further investigates how the synergy between LLMs and visual analytics enhances data interpretation, visualization techniques, and interactive exploration capabilities.

## Method Summary
The paper is a review/survey that evaluates existing LLM-based visual analytics tools (LIDA, Chat2VIS, Julius AI) via case studies rather than proposing a new trainable model architecture. The evaluation methodology relies on querying off-the-shelf models (GPT-3.5, GPT-4) via specific prompt interfaces within the reviewed tools. A dataset named `movies.csv` is used across multiple tool comparisons. The review provides a qualitative assessment of text-to-visualization capabilities, specifically the ability to translate natural language queries into accurate charts.

## Key Results
- Integration of LLMs enhances accessibility and flexibility in visual analytics through natural language interfaces.
- Specialized multimodal models like ChartLlama and CharXIV enable complex chart reasoning and insight extraction.
- The review identifies key strengths (accessibility, flexibility) and weaknesses (computational demands, biases) of LLM-visual analytics integration.

## Why This Works (Mechanism)

### Mechanism 1: Natural Language to Visualization Code Synthesis
- Claim: LLMs can translate natural language queries into executable visualization code, automating the creation of charts and dashboards from user intent.
- Mechanism: The model interprets a user's prompt, infers data requirements and visual encodings, and generates code (e.g., Python, Vega-Lite) to render the chart. This relies on its pre-trained knowledge of both programming syntax and data visualization principles.
- Core assumption: The LLM's training data contains sufficient examples of code, data structures, and visualization logic to map a user's natural language request to a syntactically correct and semantically appropriate visualization specification.
- Evidence anchors:
  - [abstract] The paper mentions "text-to-media transformations" as a key LLM role.
  - [section] The review of tools like LIDA and Chat2VIS explicitly demonstrates this mechanism, where a prompt like "Give top 10 movies with the highest gross revenue" results in generated code and a corresponding executed graph.
  - [corpus] Related research titles like "Text2Chart" and "Chart-GPT" (from corpus summary) reinforce the focus on this translation mechanism.
- Break condition: The mechanism fails when a user's query is ambiguous, contains domain-specific terms not present in the model's training, or when the required data transformations are too complex for the model to infer from context alone.

### Mechanism 2: Multimodal Chart Reasoning
- Claim: Specialized multimodal LLMs can interpret a chart's visual elements to perform complex reasoning and answer questions beyond simple data extraction.
- Mechanism: The model processes the chart image and a text query jointly, aligning visual patterns (e.g., bar heights, trend lines) with their semantic meaning. It then uses this aligned representation to synthesize information from multiple visual components and generate a reasoned textual answer.
- Core assumption: The model's multimodal training has created a robust internal representation that links visual marks to their underlying data values and analytical significance.
- Evidence anchors:
  - [abstract] The paper highlights specialized multimodal models like ChartLlama and CharXIV for chart understanding.
  - [section] The paper describes CharXIV as a benchmark for "reasoning questions, which require models to synthesize insights from multiple visual components," noting a performance gap between even advanced models and human accuracy.
  - [corpus] Corpus signal is weak for specific multimodal chart reasoning papers beyond the ones reviewed; this is an area of active development.
- Break condition: Performance degrades with highly complex or non-standard chart types, poor image quality, or when reasoning requires external knowledge not contained within the chart or prompt.

### Mechanism 3: Insight Extraction via Semantic Pattern Recognition
- Claim: LLMs can analyze data and visualizations to automatically generate natural language summaries, captions, and key insights.
- Mechanism: The model processes structured data or a description of a visualization, identifying statistically significant patterns, trends, and outliers. It then maps these quantitative findings into coherent, human-readable natural language, effectively narrating the "story" in the data.
- Core assumption: The LLM can recognize which statistical patterns are analytically significant and possesses the rhetorical capability to describe them clearly and accurately.
- Evidence anchors:
  - [abstract] The paper identifies "natural language generation" (NLG) as a core task in its taxonomy, supporting data interpretation and insight extraction.
  - [section] The paper discusses research where LLMs generate "engaging captions for data visualizations" and facilitate "natural language-based interpretation," with user studies ranking LLM-generated captions higher for engagement.
  - [corpus] The paper "LangLasso" (from corpus) focuses on generating interactive cluster descriptions, which aligns with this semantic summarization mechanism.
- Break condition: The mechanism can produce superficial or misleading insights if the model lacks domain context, or if it "hallucinates" patterns that are statistically insignificant but linguistically plausible.

## Foundational Learning

- Concept: **Visualization Grammars (e.g., Vega-Lite, Matplotlib)**
  - Why needed here: This is the target language for Mechanism 1. Understanding how a visualization is specified declaratively (data, marks, encodings) is essential for diagnosing why an LLM might generate incorrect code.
  - Quick check question: How would you declaratively specify a bar chart showing average sales per region using a grammar like Vega-Lite?

- Concept: **Multimodal Model Alignment**
  - Why needed here: This underpins Mechanism 2. It is important to understand that these models are not just "reading" text from an image but are aligning visual features with textual concepts in a shared embedding space.
  - Quick check question: What is the fundamental difference between an LLM performing OCR on a chart image versus a multimodal LLM reasoning about the same image?

- Concept: **Visual Analytics Pipeline (Task → Data → Viz → Insight)**
  - Why needed here: This provides the framework for Mechanism 3 and onboarding. Knowing where the LLM intervenes in the pipeline (e.g., at the data-to-viz or viz-to-insight stage) clarifies its role in the system.
  - Quick check question: At which two primary stages of the visual analytics pipeline does an LLM provide the most value in the reviewed tools?

## Architecture Onboarding

- Component map:
  1.  **User Interface (UI) / Interaction Layer**: A conversational chatbot or prompt box for user input. May also display the rendered visualizations.
  2.  **Prompt Engineering & Context Module**: Constructs the prompt sent to the LLM, incorporating user query, dataset schema, and conversation history.
  3.  **Core LLM Engine**: The generative model (e.g., GPT-4, LLaMA) responsible for code synthesis, reasoning, and text generation.
  4.  **Execution & Rendering Engine**: A secure sandbox that runs the LLM-generated code (e.g., Python/Matplotlib) and renders the visual output.
  5.  **Validation/Verification Layer (Optional but Recommended)**: A component to check generated code for errors or run static analysis to mitigate security risks.

- Critical path: The primary path for automation is **User Prompt → Context Augmentation → LLM Code Synthesis → Secure Code Execution → Visualization Rendering**. A secondary path for reasoning is **Chart Image + Text Query → Multimodal LLM → Textual Insight**.

- Design tradeoffs:
  - **Model Choice**: Proprietary models (e.g., GPT-4) offer high performance and ease of use but raise privacy and cost concerns. Open-source models (e.g., LLaMA) allow for on-premise deployment and control but require more engineering and may have lower reasoning performance.
  - **Flexibility vs. Guardrails**: A system that allows free-form code generation is powerful but risky and error-prone. Constraining the LLM to a specific library or a predefined set of templates improves reliability but limits user flexibility.
  - **Latency vs. Intelligence**: More complex reasoning or larger models introduce latency. Real-time interaction may require smaller, faster models or aggressive caching, trading off insight quality for responsiveness.

- Failure signatures:
  - **Hallucinated Data or Code**: The LLM references non-existent columns in the dataset or uses a function from a library that is not imported or does not exist.
  - **Context Drift**: In a long conversation, the model loses track of the initial query or dataset, leading to visualizations that are inconsistent with earlier steps in the analysis.
  - **Visualization Mismatch**: The generated chart is syntactically correct but semantically wrong (e.g., using a line chart for categorical data), indicating a failure in the model's visual analytics knowledge.

- First 3 experiments:
  1.  **End-to-End NL2VIS Test**: Use a tool like Chat2VIS or a custom script with a simple CSV and GPT-4. Test the prompt "Visualize the distribution of sales by region" and verify if the generated code and chart are correct.
  2.  **Multimodal Reasoning Probe**: Provide a multimodal LLM (e.g., GPT-4o) with an image of a complex chart and ask a reasoning question (e.g., "What is the trend in X compared to Y over the last quarter?"). Compare its answer to the ground truth.
  3.  **Insight Generation Evaluation**: Feed a summary of a dataset to an LLM and ask for three key insights. Manually verify these insights against the raw data to check for hallucinations or superficial observations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can interactive feedback loops be architected to unify fragmented LLM modules into cohesive visual analytics workflows?
- Basis in paper: [explicit] The authors state that "fragmented implementation" in disconnected modules necessitates "interactive feedback loops which can unify different components" to enhance accuracy.
- Why unresolved: Current tools often separate data cleaning, code generation, and visualization into siloed steps, lacking a unified architecture for iterative refinement.
- What evidence would resolve it: A framework where automated data curation and visualization generation iteratively inform and correct one another without manual intervention.

### Open Question 2
- Question: How can LLM interfaces be designed to prevent analyst skill degradation and "blind trust" in automated outputs?
- Basis in paper: [explicit] The SWOT analysis identifies "Over-reliance and skill degradation" as a critical threat, warning that excessive dependence may erode human expertise.
- Why unresolved: There is a design tension between providing automated, accessible insights and ensuring users remain critical, active participants in the analysis.
- What evidence would resolve it: User studies demonstrating that new interaction models maintain or improve analytical proficiency compared to fully automated "black box" assistants.

### Open Question 3
- Question: What methodologies can effectively integrate privacy-preserving technologies into LLM visual analytics without compromising performance?
- Basis in paper: [explicit] The paper highlights "Data privacy and security" as a major threat and explicitly suggests incorporating "differential privacy, federated learning, and blockchain technology."
- Why unresolved: The computational overhead and latency introduced by these privacy techniques often conflict with the real-time requirements of interactive visual analytics.
- What evidence would resolve it: A system capable of real-time visual analysis on sensitive data that mathematically guarantees privacy preservation without significant latency.

## Limitations

- **Dataset Reproducibility**: The paper relies on a `movies.csv` dataset for multiple tool comparisons but does not provide the exact data source or schema.
- **Model Version Specificity**: The review uses GPT-3.5 and GPT-4 without specifying API versions or dates, affecting result comparability.
- **Tool Configuration Gaps**: For proprietary platforms like Julius AI and Zoho Analytics, the specific settings or templates used are not detailed.

## Confidence

- **High Confidence**: The paper's systematic taxonomy of LLM tasks (NLU, NLG, dialogue systems, text-to-media) and its structured review of tools and multimodal models are well-supported by citations and direct tool evaluations.
- **Medium Confidence**: The qualitative SWOT analysis and claims about the transformative potential of LLMs in visual analytics are reasonable but based on the current state of the art, which is rapidly evolving.
- **Low Confidence**: Specific performance comparisons between tools (e.g., LIDA vs. Chat2VIS) are difficult to verify due to dataset and configuration unknowns.

## Next Checks

1. **End-to-End NL2VIS Reproduction**: Use the exact prompt "Give top 10 movies with the highest gross revenue" in LIDA and Chat2VIS with a standard movie dataset. Verify if the generated code and visualizations match the paper's figures.
2. **Multimodal Reasoning Benchmark**: Test ChartLlama or CharXIV with a complex chart image and a reasoning query (e.g., "What is the trend in X compared to Y over the last quarter?"). Compare the model's answer to the ground truth.
3. **Insight Generation Hallucination Check**: Use an LLM to generate three key insights from a dataset summary. Manually verify each insight against the raw data to quantify hallucination rates.