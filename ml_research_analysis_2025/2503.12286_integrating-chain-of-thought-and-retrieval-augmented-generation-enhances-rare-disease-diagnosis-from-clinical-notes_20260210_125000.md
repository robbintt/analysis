---
ver: rpa2
title: Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare
  Disease Diagnosis from Clinical Notes
arxiv_id: '2503.12286'
source_url: https://arxiv.org/abs/2503.12286
tags:
- clinical
- gene
- notes
- disease
- prioritization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study integrates Chain-of-Thought (CoT) and Retrieval-Augmented
  Generation (RAG) to improve rare disease diagnosis from clinical notes using large
  language models (LLMs). The RAG-driven CoT approach retrieves biomedical knowledge
  before structured reasoning, while CoT-driven RAG applies reasoning before retrieval.
---

# Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes

## Quick Facts
- arXiv ID: 2503.12286
- Source URL: https://arxiv.org/abs/2503.12286
- Reference count: 0
- Integration of CoT and RAG improves rare disease diagnosis from clinical notes using LLMs.

## Executive Summary
This study integrates Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG) to enhance rare disease diagnosis from clinical notes. By combining structured reasoning with external knowledge retrieval, the RAG-driven CoT approach (retrieval-first) and CoT-driven RAG approach (reasoning-first) significantly outperform baseline LLMs in gene prioritization and disease diagnosis. Evaluated on 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes, these methods demonstrate that retrieval and reasoning synergy improves LLMs' clinical inference capabilities, with DeepSeek-R1-distill-Llama70B achieving over 40% top-10 gene accuracy on Phenopacket data.

## Method Summary
The study combines CoT and RAG for rare disease diagnosis from unstructured clinical notes without intermediate HPO extraction. It uses two backbone LLMs (Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B) with five inference strategies: base prompt, CoT only, RAG only, RAG-driven CoT (retrieve first, then reason), and CoT-driven RAG (reason first, use structured outputs to query). The RAG pipeline uses pubmedbert-base-embeddings + FAISS (top-3) with ColBERT reranker (top-1). CoT uses a 5-step protocol: extract/classify HPO terms, assess demographics, map gene-disease associations, refine by inheritance, prioritize top-10. The study evaluates on three datasets: 5,980 Phenopacket-derived notes (synthetic), 255 PubMed free-text narratives, and 220 in-house CHOP clinical notes.

## Key Results
- RAG-driven CoT excels with high-quality notes, achieving 42.13% top-10 gene accuracy on Phenopacket data with DeepSeek-R1-distill-Llama70B
- CoT-driven RAG better handles noisy, lengthy clinical records, improving Llama3.3 disease diagnosis accuracy from 29.55% to 35.00%
- Structured 5-step CoT prompting alone substantially improves gene prioritization, increasing accuracy from 11.72% to 41.18% on Phenopacket data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-driven CoT (retrieval-first) improves diagnostic accuracy on high-quality, structured clinical notes by grounding reasoning in external knowledge before inference begins.
- Mechanism: The system queries HPO/OMIM databases using the raw clinical note, retrieves top-k relevant documents via embedding similarity + reranking, then feeds retrieved context into a structured 5-step CoT prompt. This anchors the reasoning process in domain-specific evidence, reducing hallucination and improving phenotype-to-gene mapping.
- Core assumption: The input note contains sufficiently clear phenotypic signal that retrieval can exploit; the knowledge base covers relevant disease/gene associations.
- Evidence anchors: [abstract] "RAG-driven CoT excels with high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence." [results] "For Phenopacket-derived tasks, especially gene prioritization task, such early context can yield the highest Top-10 accuracies: DeepSeek-distill-Llama3 ascends from 11.72% to 42.13%." [corpus] Limited direct corpus support for RAG-first sequencing; most related work (e.g., CLI-RAG, HyFedRAG) focuses on retrieval quality or privacy, not retrieval-reasoning ordering.
- Break condition: If input notes are noisy, lengthy, or contain irrelevant content, early retrieval may fetch off-target documents, degrading performance (observed drop in in-house data: 35.00% → 27.73% Top-10 for Llama3.3 disease diagnosis).

### Mechanism 2
- Claim: CoT-driven RAG (reasoning-first) outperforms retrieval-first on noisy, lengthy clinical records by filtering and structuring phenotypic information before database queries.
- Mechanism: The model first executes the 5-step CoT protocol (extract/classify HPO terms → assess demographics → map to gene-disease associations → refine by inheritance → prioritize top-10). The structured outputs (e.g., extracted HPO terms, refined hypotheses) become the retrieval query, reducing semantic drift and ensuring retrieved documents match clinically salient features.
- Core assumption: The LLM can reliably extract and classify phenotypic features from unstructured text; reasoning quality determines retrieval relevance.
- Evidence anchors: [abstract] "CoT-driven RAG has advantage when processing lengthy and noisy notes." [results] "DeepSeek-distill-Llama3's disease diagnosis accuracy climbs from 29.55% to 35.00% [with CoT-driven RAG], substantially higher than the 28.18% achieved by RAG-driven CoT." [corpus] CoT-RAG (arXiv:2504.13534) similarly integrates CoT with RAG but focuses on code prompts; confirms synergy but not sequencing effects.
- Break condition: If CoT extraction produces incomplete or incorrect HPO mappings, the downstream retrieval will be misdirected; performance gains depend on reasoning fidelity.

### Mechanism 3
- Claim: Structured 5-step CoT prompting alone substantially improves gene prioritization even without retrieval, by enforcing explicit phenotypic reasoning.
- Mechanism: The prompt forces the model to: (1) extract/classify HPO terms by organ system, (2) assess demographic factors, (3) map to gene-disease associations, (4) refine by inheritance patterns, (5) output ranked top-10 genes. This decomposition reduces reasoning shortcuts and exposes intermediate logic.
- Core assumption: The base LLM has sufficient parametric medical knowledge; the bottleneck is reasoning structure, not knowledge gaps.
- Evidence anchors: [results] "CoT prompting increased gene prioritization accuracy from 11.72% to 41.18% [for DeepSeek-R1-Distill-Llama3 on Phenopacket data]." [discussion] "Prompted CoT reasoning helps refine predictions by guiding the model toward more structured decision-making processes." [corpus] Prior work (Kim et al., cited in paper) found CoT alone can reduce accuracy to 23% on ChatGPT-3.5; improvement is model-dependent.
- Break condition: On models with weak internal medical knowledge (e.g., GPT-3.5, early Llama), CoT may amplify reasoning biases without retrieval support.

## Foundational Learning

- Concept: Human Phenotype Ontology (HPO) — standardized vocabulary for phenotypic abnormalities, structured as a DAG.
  - Why needed here: All retrieval targets and CoT extraction steps reference HPO terms; diagnosis depends on mapping clinical features to HPO IDs.
  - Quick check question: Given "hypotonia, areflexia, facial weakness," which HPO terms apply and which organ systems do they span?

- Concept: Retrieval-Augmented Generation (RAG) — retrieve external documents via embedding similarity, inject as context before generation.
  - Why needed here: The pipeline uses RAG to access HPO/OMIM knowledge; chunk size, embedding model, and reranking directly affect retrieval relevance.
  - Quick check question: What is the tradeoff between retrieving 3 documents vs. 10 for a complex rare disease query?

- Concept: Chain-of-Thought (CoT) prompting — prompt the model to emit intermediate reasoning steps before final output.
  - Why needed here: The 5-step CoT protocol structures clinical reasoning; understanding prompt design is critical for debugging and iteration.
  - Quick check question: Why might CoT degrade performance if the model lacks relevant parametric knowledge?

## Architecture Onboarding

- Component map: Clinical note -> (Optional preprocessing) -> Branch A (RAG-driven CoT): Embedding retrieval (PubMedBERT) -> FAISS search -> ColBERT reranking -> CoT prompt with retrieved context -> LLM inference; Branch B (CoT-driven RAG): CoT prompt -> extract HPO terms/hypotheses -> use as query -> retrieval -> final LLM inference -> Output: Top-10 ranked genes or diseases with reasoning trace

- Critical path:
  1. Clinical note quality determines which pipeline to use (high-quality -> RAG-driven CoT; noisy/long -> CoT-driven RAG)
  2. Retrieval quality (embedding + reranking) caps maximum achievable accuracy
  3. CoT prompt fidelity determines how well extracted features map to retrieval queries and final rankings

- Design tradeoffs:
  - RAG-driven CoT vs. CoT-driven RAG: Retrieval-first anchors reasoning but risks noise contamination; reasoning-first filters queries but depends on extraction accuracy
  - Chunk size (512 tokens): Smaller chunks improve precision; larger chunks provide context but may dilute relevance
  - Context window (2048 vs. 5120 tokens): Longer windows handle in-house notes but increase latency and cost

- Failure signatures:
  - Top-10 accuracy < 20% on Phenopacket data: Likely retrieval failure (wrong documents) or base model unsuitable
  - RAG-driven CoT underperforms CoT alone on noisy notes: Retrieval is contaminating reasoning; switch to CoT-driven RAG
  - CoT extraction produces malformed HPO terms: Prompt needs refinement; check output parsing

- First 3 experiments:
  1. Reproduce baseline vs. CoT vs. RAG vs. RAG-driven CoT vs. CoT-driven RAG on a 100-sample Phenopacket subset to validate pipeline behavior
  2. Ablate retrieval: Run CoT-driven RAG with ground-truth HPO terms (manually provided) vs. model-extracted terms to quantify extraction error impact
  3. Test retrieval quality: Manually inspect top-3 retrieved documents for 20 cases; correlate relevance score with final accuracy to calibrate retrieval thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of multimodal clinical data (biochemical tests, imaging, genomic sequencing) into RAG-driven CoT frameworks significantly improve diagnostic accuracy compared to text-only analysis?
- Basis in paper: [explicit] The authors state that "integrating multimodal clinical data—including biochemical blood tests, electroencephalogram results, imaging scans, or even genomic sequencing —may significantly improve diagnostic accuracy."
- Why unresolved: The current study is limited to unstructured text (clinical notes); it does not assess quantitative lab results or image-based data.
- What evidence would resolve it: A comparative study evaluating model performance on datasets containing both text and non-text modalities versus text-only baselines.

### Open Question 2
- Question: To what extent does expanding the retrieval knowledge base beyond HPO and OMIM to include ClinVar, Orphanet, and biomedical literature improve the precision and recall of gene prioritization?
- Basis in paper: [explicit] The Discussion notes that "Expanding retrieval sources beyond OMIM and HPO to other clinical phenotype databases such as ClinVar, Orphanet and biomedical literature, will likely improve knowledge precision and recall."
- Why unresolved: The current retrieval mechanism is limited to the HPO and OMIM databases, potentially missing variant-level evidence found in ClinVar or Orphanet.
- What evidence would resolve it: Ablation studies measuring gene prioritization accuracy using RAG systems populated with these additional databases.

### Open Question 3
- Question: Can advanced reasoning algorithms (graph-based or neuro-symbolic reasoning) combined with reinforcement learning bridge the gap between current LLM performance and clinically acceptable diagnostic accuracy thresholds?
- Basis in paper: [explicit] The authors suggest that "more sophisticated reasoning algorithms, such as graph-based reasoning and neuro-symbolic reasoning... [and] reinforcement learning techniques such as Group relative Policy Optimization (GRPO) may further refine LLM decision-making."
- Why unresolved: Current accuracy remains "far from clinically acceptable thresholds," and the study relied on standard CoT prompting rather than complex reasoning architectures.
- What evidence would resolve it: Benchmarking current CoT methods against graph-based or neuro-symbolic models on the same rare disease datasets.

## Limitations
- The synthetic Phenopacket dataset may not fully represent real-world clinical complexity and variability
- The in-house dataset is relatively small (220 notes) and may not capture the full spectrum of rare disease presentations
- The study relies on OMIM knowledge base, which has licensing constraints and may have completeness limitations

## Confidence
- High Confidence: RAG-driven CoT outperforms baselines on high-quality Phenopacket data (DeepSeek-R1-distill-Llama70B achieving 42.13% Top-10 gene accuracy)
- Medium Confidence: Structured 5-step CoT prompting alone substantially improves performance without retrieval (41.18% Top-10 accuracy on Phenopacket data)
- Low Confidence: Sequencing retrieval before or after reasoning is the primary driver of performance differences, as confounding factors were not fully controlled

## Next Checks
1. Apply the same pipeline to an independent rare disease dataset (e.g., MIMIC-IV clinical notes) to verify generalizability
2. Manually annotate relevance of top-3 retrieved documents for 50 random cases and correlate with prediction accuracy to quantify retrieval quality impact
3. Repeat experiments using weaker base models (GPT-3.5, Llama-8B) to determine whether gains persist when underlying LLM has less medical knowledge