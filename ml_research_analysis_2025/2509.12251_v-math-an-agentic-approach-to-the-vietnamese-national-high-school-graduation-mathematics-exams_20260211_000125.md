---
ver: rpa2
title: 'V-Math: An Agentic Approach to the Vietnamese National High School Graduation
  Mathematics Exams'
arxiv_id: '2509.12251'
source_url: https://arxiv.org/abs/2509.12251
tags:
- trong
- exam
- question
- agent
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'V-MATH is an autonomous multi-agent framework that assists Vietnamese
  students preparing for the National High School Graduation Mathematics Exams (NHSGMEs).
  It integrates three specialized agents: a specification-matrix-conditioned question
  generator, a high-accuracy solver/explainer, and a personalized tutor.'
---

# V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams

## Quick Facts
- arXiv ID: 2509.12251
- Source URL: https://arxiv.org/abs/2509.12251
- Reference count: 40
- Key outcome: V-MATH achieves perfect average accuracy (100%) across 2019-2023 NHSGME exams, outperforming leading LLMs

## Executive Summary
V-MATH is an autonomous multi-agent framework designed to assist Vietnamese students preparing for the National High School Graduation Mathematics Exams (NHSGMEs). The framework integrates three specialized agents: a specification-matrix-conditioned question generator, a high-accuracy solver/explainer, and a personalized tutor. It employs Memento-style memory-based reinforcement learning to continually adapt from past interactions without costly fine-tuning. Experiments on a newly curated NHSGME dataset (500 exams) and the VNHSGE benchmark show V-MATH achieving perfect average accuracy across 2019-2023, outperforming leading LLMs like o1-preview and GPT-4 Omni.

## Method Summary
V-MATH uses a multi-agent architecture with three specialized components working in concert. The question generator creates exam-style problems conditioned on a specification matrix, the solver provides step-by-step solutions with explanations, and the personalized tutor adapts to individual student needs. The system employs a Memento-style memory mechanism that stores past interactions and uses them to improve future performance through reinforcement learning, eliminating the need for expensive model fine-tuning. The framework was evaluated on a newly curated dataset of 500 NHSGME exams and the VNHSGE benchmark, demonstrating superior performance across all exam sections.

## Key Results
- V-MATH achieves perfect average accuracy (100%) across 2019-2023 NHSGME exams
- Section-wise accuracies reach 98.1%, 93.8%, and 88.4% on Recognition, Comprehension, and Application parts respectively
- Memory augmentation improves overall accuracy from 88.1% to 90.4% and reduces recurring skill errors by 43%

## Why This Works (Mechanism)
V-MATH's effectiveness stems from its multi-agent architecture that divides complex problem-solving into specialized subtasks. The specification-matrix-conditioned generator ensures questions match exam patterns and difficulty levels, while the solver/explainer provides accurate, step-by-step solutions. The personalized tutor adapts to individual learning patterns, and the Memento memory mechanism enables continuous improvement without retraining. This division of labor allows each agent to focus on its strengths, resulting in higher overall accuracy and better pedagogical outcomes than monolithic approaches.

## Foundational Learning
- **Multi-agent systems**: Multiple specialized agents collaborate to solve complex problems, each handling a specific aspect of the task
- **Specification matrices**: Structured representations that encode problem requirements, constraints, and desired difficulty levels for question generation
- **Memory-based reinforcement learning**: Using past interactions as a knowledge base to improve future performance without model updates
- **Pedagogical scaffolding**: Educational techniques that provide structured support tailored to individual student needs and learning progression
- **Mathematical reasoning decomposition**: Breaking down complex math problems into logical steps that can be solved sequentially
- **Benchmark construction methodology**: Creating standardized evaluation datasets that represent real-world exam conditions

## Architecture Onboarding

**Component Map**
Question Generator -> Solver/Explainer -> Personalized Tutor -> Memento Memory

**Critical Path**
1. Student query enters Personalized Tutor
2. Tutor determines learning needs and passes requirements to Question Generator
3. Generator creates specification-matrix-conditioned questions
4. Questions solved by Solver/Explainer with step-by-step explanations
5. Memory system stores interaction and updates learning patterns

**Design Tradeoffs**
- Memory-based learning vs. fine-tuning: avoids computational costs but may have limited capacity
- Specialized agents vs. monolithic approach: higher accuracy but increased system complexity
- Question generation vs. problem selection: creates custom problems but requires more processing time

**Failure Signatures**
- Memory saturation: diminishing returns from repeated interactions
- Specification matrix drift: generated questions becoming too easy or too hard
- Solver hallucination: providing incorrect solutions with confident explanations
- Tutor over-adaptation: becoming too personalized and losing general applicability

**3 First Experiments**
1. Test question generation accuracy on held-out exam specifications
2. Evaluate solver correctness on known problem types
3. Measure memory system retention over multiple interaction cycles

## Open Questions the Paper Calls Out
None

## Limitations
- Perfect accuracy claims raise concerns about potential data leakage from training to evaluation sets
- Memento memory mechanism's long-term stability under adversarial question distributions unverified
- VNHSGE benchmark construction methodology lacks transparency for independent assessment

## Confidence
- **High confidence**: Multi-agent architecture design and component integration
- **Medium confidence**: Internal ablation study results showing memory improvement
- **Low confidence**: State-of-the-art performance claims relative to commercial models

## Next Checks
1. Conduct cross-validation experiments using time-stratified splits with 2019-2022 training data and 2023 testing
2. Implement blind evaluation with human experts verifying generated questions and solutions
3. Test Memento memory robustness by introducing controlled perturbations in question distributions and measuring performance degradation over multiple cycles