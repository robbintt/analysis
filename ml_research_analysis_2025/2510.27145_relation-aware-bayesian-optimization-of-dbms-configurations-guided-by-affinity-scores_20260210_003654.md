---
ver: rpa2
title: Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity
  Scores
arxiv_id: '2510.27145'
source_url: https://arxiv.org/abs/2510.27145
tags:
- performance
- parameters
- optimization
- latent
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RelTune is a relation-aware DBMS configuration tuning framework
  that models inter-parameter dependencies using a relational graph and encodes them
  into a Graph Neural Network (GNN)-based latent space. To improve Bayesian Optimization
  (BO) exploration efficiency, it introduces a Hybrid-Score-Guided BO (HBO) that combines
  surrogate predictions with an Affinity Score measuring proximity to previously high-performing
  configurations.
---

# Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores

## Quick Facts
- arXiv ID: 2510.27145
- Source URL: https://arxiv.org/abs/2510.27145
- Authors: Sein Kwon; Seulgi Baek; Hyunseo Yang; Youngwan Jo; Sanghyun Park
- Reference count: 40
- Primary result: Achieves up to 56.5% throughput improvement and 60.9% latency reduction over default settings

## Executive Summary
RelTune is a relation-aware DBMS configuration tuning framework that models inter-parameter dependencies using a relational graph and encodes them into a Graph Neural Network (GNN)-based latent space. To improve Bayesian Optimization (BO) exploration efficiency, it introduces a Hybrid-Score-Guided BO (HBO) that combines surrogate predictions with an Affinity Score measuring proximity to previously high-performing configurations. Experiments on MySQL and PostgreSQL workloads show RelTune achieves up to 56.5% throughput improvement and 60.9% latency reduction over default settings, outperforming state-of-the-art methods like OtterTune, RGPE, CSAT, and GPTuner.

## Method Summary
The framework extracts parameter descriptions from DBMS manuals using GPT-4, constructs a relational graph based on cosine similarity of these descriptions, and learns a GAT-based encoder to map configurations to a 32-dimensional latent space. It trains an autoencoder with a performance prediction head on 5,000 samples, then uses HBO (combining GP surrogate with affinity guidance) to optimize configurations over 300 iterations, achieving faster convergence and superior performance compared to baselines.

## Key Results
- Up to 56.5% throughput improvement and 60.9% latency reduction over default configurations
- Outperforms OtterTune, RGPE, CSAT, and GPTuner on MySQL and PostgreSQL workloads
- Affinity Score demonstrates strong correlation with actual performance (AUROC up to 0.98, AUPRC up to 0.99)
- Faster convergence than Vanilla BO, with up to 60.9% reduction in tuning time for PostgreSQL

## Why This Works (Mechanism)

### Mechanism 1: Semantic Dependency Encoding via Relational Graphs
The framework uses LLM-extracted descriptions and cosine similarity to build a relational graph that captures functional dependencies between parameters. This mitigates information loss from independent feature selection methods and allows the GAT to learn embeddings where dependent parameters influence each other.

### Mechanism 2: Hybrid-Score-Guided Exploration (HBO)
HBO combines surrogate predictions with an Affinity Score that measures proximity to previously high-performing configurations in latent space. This accelerates convergence by biasing exploration toward regions historically associated with good performance rather than purely uncertainty-driven exploration.

### Mechanism 3: Surrogate-Assisted Evaluation Decoupling
By training a metric prediction head alongside the autoencoder, RelTune decouples performance evaluation from the optimization loop. This surrogate-based approach reduces wall-clock tuning time by avoiding expensive benchmark execution for every iteration.

## Foundational Learning

- **Graph Attention Networks (GAT)**
  - Why needed: To aggregate parameter features while weighting the influence of specific dependencies differently
  - Quick check: How does the attention coefficient α_ij change a parameter's embedding if its neighbor is semantically similar but functionally irrelevant?

- **Bayesian Optimization Acquisition Functions**
  - Why needed: To understand how the Hybrid Score modifies the utility landscape that the acquisition function explores
  - Quick check: In standard UCB, does higher standard deviation σ(z) encourage or discourage selection, and how does the Affinity Score alter this?

- **Latent Space Constraints (Autoencoders)**
  - Why needed: To ensure the compressed latent space preserves performance-relevant information
  - Quick check: If reconstruction loss is low but metric prediction loss is high, what does this imply about the latent space geometry?

## Architecture Onboarding

- **Component map:** LLM Extractor → Sentence-Transformer → Relational Graph → GAT Encoder → Latent Vector → Decoder & Metric Head → HBO Loop (GP Surrogate + Affinity Calculator → Acquisition Function → New z)
- **Critical path:** The similarity threshold (τ = 0.75) is the most sensitive static parameter that determines graph connectivity and subsequent GAT performance
- **Design tradeoffs:** 32-dimensional latent space balances compression benefits against loss of nuanced interactions; surrogate-based evaluation trades accuracy for speed
- **Failure signatures:** Graph fragmentation (modularity spikes but ARI drops near 0), affinity collapse (AUROC drops to ~0.5), or poor reconstruction-prediction correlation
- **First 3 experiments:**
  1. Run Louvain algorithm on constructed graph and verify communities align with known DBMS subsystems
  2. Train autoencoder and plot reconstruction error vs. prediction accuracy to check latent space quality
  3. Vary Affinity Score weight γ and measure AUROC to validate affinity heuristic reliability

## Open Questions the Paper Calls Out
The paper states that RelTune "lays the foundation for future extensions toward real-time adaptive tuning" where workload characteristics shift dynamically during operation.

## Limitations
- Performance heavily depends on quality of LLM-generated descriptions and the fixed 0.75 similarity threshold
- The assumption that high-performing configurations cluster in latent space may not hold for highly multimodal landscapes
- Surrogate-based evaluation introduces prediction errors that could mislead the optimizer

## Confidence
- **High Confidence**: Throughput improvements (56.5%) and latency reductions (60.9%) vs. default configurations
- **Medium Confidence**: Faster convergence than Vanilla BO, though requires careful tuning of affinity parameters
- **Low Confidence**: Generalization of the 0.75 threshold across different DBMS versions and robustness to LLM output variations

## Next Checks
1. Apply Louvain community detection to verify relational graph communities align with known DBMS subsystems
2. Train autoencoder and plot reconstruction vs. prediction accuracy to validate latent space quality
3. Systematically vary affinity score weight γ and measure AUROC to test heuristic reliability across workloads