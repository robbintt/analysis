---
ver: rpa2
title: On-Sensor Convolutional Neural Networks with Early-Exits
arxiv_id: '2503.16939'
source_url: https://arxiv.org/abs/2503.16939
tags:
- ispu
- memory
- time
- when
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first implementation of Depth-First Convolutional
  Neural Networks (CNNs) on Intelligent Sensor Processing Units (ISPUs), addressing
  the challenge of efficient machine learning at the extreme edge. The proposed methodology
  partitions CNN computation between the ISPU and microcontroller (MCU), employing
  an incremental Depth-First approach for convolutions and an Early-Exit mechanism
  to minimize MCU activation.
---

# On-Sensor Convolutional Neural Networks with Early-Exits

## Quick Facts
- arXiv ID: 2503.16939
- Source URL: https://arxiv.org/abs/2503.16939
- Reference count: 17
- This paper introduces the first implementation of Depth-First CNNs on Intelligent Sensor Processing Units (ISPUs)

## Executive Summary
This paper presents a novel approach for deploying convolutional neural networks on Intelligent Sensor Processing Units (ISPUs) by combining Depth-First incremental convolutions, partitioned ISPU-MCU computation, and an Early-Exit (EE) mechanism. The methodology addresses the challenge of efficient machine learning at the extreme edge, where memory and computational constraints are severe. By processing CNN layers incrementally on the ISPU and using EE to conditionally suppress microcontroller (MCU) activation, the solution achieves 11% power reduction compared to traditional MCU-only pipelines while maintaining equal accuracy. The Depth-First implementation ensures memory usage remains independent of input window size, overcoming the linear scaling limitations of conventional approaches and enabling operation within ISPU constraints (10 MHz clock, <32 kB memory).

## Method Summary
The methodology partitions CNN computation between the ISPU and microcontroller, employing an incremental Depth-First approach for convolutions and an Early-Exit mechanism to minimize MCU activation. The CNN is split at a boundary where the ISPU performs feature extraction (g) and the MCU handles classification (h). Depth-First convolutions update outputs incrementally per sample rather than buffering entire windows, ensuring memory independence from input size. After feature extraction, an EE module (binary classifier) predicts whether MCU activation is necessary, allowing the MCU to remain in sleep mode when features are sufficiently confident. The approach was evaluated on a NUCLEO-F411RE board with an LSM6DSO16IS IMU, achieving 4.8 mA average current consumption with 96-99% EE accuracy.

## Key Results
- Average current consumption of 4.8 mA, representing an 11% reduction compared to traditional MCU-only inference pipelines
- Memory occupation and inference time independent of input window size, overcoming linear scaling limitations
- Proof-of-concept smart eyewear application extended battery life from 37 to 42 hours of continuous use
- EE module achieved 96-99% accuracy in suppressing unnecessary MCU activations

## Why This Works (Mechanism)

### Mechanism 1: Depth-First Incremental Convolutions
The incremental update formula y(j,−)t−i = y(j,−)t−i−1 + c(j)i × xt−i eliminates the need to store all T samples simultaneously, making memory usage constant regardless of window size. This mathematical decomposition of convolution into per-sample contributions works for 1D convolutions with additive properties.

### Mechanism 2: Partitioned ISPU-MCU Computation
By splitting the CNN at a boundary where g outputs compact 16-dimensional features rather than raw windows, the ISPU handles computationally intensive feature extraction while the MCU performs lightweight classification. This partitioning respects ISPU memory limits while maintaining task accuracy.

### Mechanism 3: Early-Exit Confidence-Based MCU Gating
The EE module acts as a binary classifier that predicts {activate, not-activate} based on extracted features. When "not-activate" is predicted, the MCU remains in sleep mode, reducing average current consumption by 15% in those instances. The learnable decision boundary correlates with downstream MCU utility.

## Foundational Learning

- **Concept: Depth-First vs. Width-First Layer Execution**
  - Why needed here: Understanding incremental computation is essential to grasp how memory becomes window-independent
  - Quick check question: If a 1D convolution with kernel size T=100 receives one new sample per timestep, how many samples must a Width-First approach buffer before computing any output? (Answer: 100; Depth-First needs only 1 at a time for incremental update)

- **Concept: ISPU Hardware Constraints**
  - Why needed here: The entire methodology is shaped by 10MHz clock, 8kB data RAM, 32kB program memory limits
  - Quick check question: Why does Width-First convolution fail on ISPU for windows >6s at 26Hz? (Answer: Memory scales linearly with window size; 4kB already consumed by input buffer alone)

- **Concept: Confidence-Based Early Exit Design**
  - Why needed here: The EE module is trained separately after g and h are fixed, requiring understanding of two-stage training
  - Quick check question: What labels does the EE module require during training? (Answer: {activate, not-activate} paired with input sequences—task-specific, not the original classification labels)

## Architecture Onboarding

- **Component map:**
  LSM6DSO16IS IMU (6-axis, 26Hz ODR) -> ISPU core (10MHz, 8kB data RAM, 32kB program) -> g(incr): 2× 1D Conv (16 filters) + MaxPool -> EE Module: FC layer -> binary {wake MCU, skip} -> NUCLEO-F411RE MCU (sleep mode default) -> h(·): task-specific classifier on 16-dim features

- **Critical path:**
  1. Sample arrives at ODR frequency (26Hz)
  2. ISPU updates g(incr) incrementally (≤6.3ms per inference, max 158Hz ODR capacity)
  3. After window T completes, EE evaluates confidence
  4. If wake: interrupt → MCU reads 16-dim features → h(·) executes
  5. If skip: MCU stays asleep, ISPU awaits next window

- **Design tradeoffs:**
  | Tradeoff | Option A | Option B |
  |----------|----------|----------|
  | Partition point | Earlier (simpler g) | Later (richer features, more ISPU load) |
  | EE threshold | Conservative (few skips, less savings) | Aggressive (more skips, risk of missed activations) |
  | Window size T | Small (faster response) | Large (better accuracy, no memory penalty with Depth-First) |

- **Failure signatures:**
  - Sample loss: ISPU computation exceeds 1/ODR time → new sample overwrites previous; symptom: dropped timestamps
  - Memory overflow: g weights + intermediate activations >8kB; symptom: linker errors or silent corruption
  - EE false negatives: MCU never wakes for valid events; symptom: task accuracy drops disproportionately to EE accuracy
  - Width-First accidental use: Memory scales with window; symptom: works at T=1s, fails at T=6s

- **First 3 experiments:**
  1. Baseline power measurement: Run MCU-only inference pipeline (Width-First) at 26Hz, 1s window; record current consumption (expected: ~5.4mA total including IMU)
  2. Memory profiling with varying T: Implement Depth-First g on ISPU; measure RAM usage for T∈{1, 3, 6, 10}s; confirm stability at ~3.8kB
  3. EE accuracy vs. power tradeoff curve: Train EE with varying confidence thresholds; plot (EE accuracy, %MCU wakeups, total current); identify operating point with equal task accuracy to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How will quantized weights affect the accuracy and energy efficiency of Depth-First CNNs on ISPUs compared to the current floating-point implementation?
- Basis in paper: [explicit] "Additionally, quantized weights will be integrated into the ISPU to enable support for larger neural network architectures."
- Why unresolved: The current implementation does not use quantized weights, limiting the complexity of neural networks that can be deployed within the ISPU's 32 kB memory constraint.
- What evidence would resolve it: Experimental results comparing accuracy, memory usage, and power consumption between floating-point and quantized implementations across multiple CNN architectures.

### Open Question 2
- Question: Can the proposed methodology maintain its energy efficiency advantages when applied to more complex tasks and deeper h(·) classification functions?
- Basis in paper: [explicit] "Future work will focus on more complex tasks, including the development of a function h(·) able to further process the features computed by the ISPU."
- Why unresolved: The current evaluation uses a simple binary classification task (glasses worn/not worn), and it's unclear how the energy savings scale with task complexity and MCU activation frequency.
- What evidence would resolve it: Benchmarks on diverse, computationally intensive tasks (e.g., multi-class activity recognition, gesture detection) showing power consumption metrics as h(·) complexity increases.

### Open Question 3
- Question: How does the Depth-First approach perform when extended beyond 1D convolutions to other layer types (e.g., 2D convolutions, attention mechanisms)?
- Basis in paper: [inferred] "In this paper, we will focus on tailoring the incremental approach to 1D convolutions. Nevertheless, it's worth noting that the incremental approach can be extended to all major neural network layers."
- Why unresolved: Only 1D convolutions were implemented and tested, leaving the feasibility and efficiency of extending this approach to other layer types unverified.
- What evidence would resolve it: Implementation details and performance metrics (memory usage, inference time, accuracy) for Depth-First 2D convolutions and other layer types on the ISPU.

## Limitations

- The methodology's effectiveness depends heavily on ISPU-specific hardware constraints that may not generalize to other edge processors
- The 11% power reduction claim is based on a relatively simple binary classification task, with uncertain scalability to more complex applications
- The EE module's binary classification accuracy (96-99%) is critical to overall power savings, but the paper doesn't explore failure modes or robustness across different application domains

## Confidence

- **High confidence**: Depth-First convolution implementation correctly reduces memory from O(T) to O(1), ISPU-MCU partitioning structure, EE module design for conditional MCU wake-up
- **Medium confidence**: 11% power reduction claim (dependent on specific hardware measurements and EE accuracy), task accuracy parity (assumes EE false negatives are rare/acceptable)
- **Low confidence**: Generalizability to other ISPU platforms, performance on complex classification tasks, robustness of EE module across different application domains

## Next Checks

1. Measure actual power consumption on target hardware with varied EE thresholds to map the full efficiency-accuracy tradeoff curve beyond the single operating point reported
2. Implement the same architecture on a different edge processor (e.g., Cortex-M4 with CMSIS-NN) to verify memory independence from window size and assess portability
3. Test the methodology on a multi-class classification task (e.g., human activity recognition with 6+ classes) to evaluate whether EE module accuracy and power savings scale with task complexity