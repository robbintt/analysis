---
ver: rpa2
title: 'The Biased Samaritan: LLM biases in Perceived Kindness'
arxiv_id: '2506.11361'
source_url: https://arxiv.org/abs/2506.11361
tags:
- bias
- control
- demographic
- biases
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates demographic biases in popular Large Language
  Models (LLMs) by using human-generated prompts to evaluate willingness-to-help ratings
  across gender, race, and age categories. The authors designed 412 prompts with varied
  syntax and semantics, covering 103 moral scenarios, and tested them against ten
  major LLMs.
---

# The Biased Samaritan: LLM biases in Perceived Kindness

## Quick Facts
- arXiv ID: 2506.11361
- Source URL: https://arxiv.org/abs/2506.11361
- Reference count: 4
- LLMs show statistically significant demographic bias in perceived kindness across all tested models

## Executive Summary
This study reveals systematic demographic biases in popular Large Language Models when evaluating willingness-to-help across gender, race, and age categories. Using 412 carefully designed prompts based on 103 moral scenarios, researchers tested ten major LLMs and found consistent patterns of bias. All models showed statistically significant bias in at least six demographic categories, with women, non-binary individuals, and most racial minorities rated as more likely to help compared to control groups. Teenage respondents were consistently rated as less likely to help, while seniors received the highest ratings.

## Method Summary
The researchers created 412 prompts with varied syntax and semantics, covering 103 moral scenarios designed to evaluate willingness-to-help ratings. These prompts were tested against ten major LLMs using a 1-100 scale. The study employed human-generated prompts to evaluate demographic biases across gender, race, and age categories. Statistical analysis included repeated trials showing 96.95% agreement, indicating high reliability of the bias patterns observed.

## Key Results
- All models displayed statistically significant bias in at least six demographic categories (p < 0.001)
- Women rated 1.77% higher in willingness-to-help than control group; Native Hawaiians rated 12.53% higher
- Teenage respondents consistently rated as least likely to help across all models
- Brittleness averaged 4-8 points across models, with OpenAI showing least variability and Anthropic showing most

## Why This Works (Mechanism)
Assumption: The biases likely stem from training data patterns that associate certain demographic groups with specific behavioral expectations. Models may have learned stereotypical associations between demographic characteristics and prosocial behavior from their training corpora, which often contain societal biases and cultural stereotypes. The consistent patterns across all models suggest these biases are deeply embedded in how LLMs process demographic information and generate behavioral predictions.

## Foundational Learning
The study demonstrates that LLMs have internalized societal biases about demographic groups' willingness to help others. This learning appears to be consistent across different model architectures and training approaches, suggesting that demographic bias in perceived kindness is a fundamental characteristic of current LLM systems rather than an artifact of specific implementations. The systematic nature of the bias across gender, race, and age categories indicates that models are applying learned patterns about demographic groups to predict behavior in moral scenarios.

## Architecture Onboarding
Component map: Prompts -> LLM models -> Output scoring -> Statistical analysis
Critical path: Scenario design → Prompt generation → Model evaluation → Bias measurement
Design tradeoffs: Standardized prompts vs. real-world complexity; single-prompt approach vs. comprehensive evaluation
Failure signatures: Brittleness scores (4-8 points average); inconsistent ratings across demographic categories
First experiments: 1) Test with real-world interaction scenarios, 2) Cross-cultural validation, 3) Mixed demographic information evaluation

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but implicit questions include how these biases might affect real-world LLM applications in hiring, lending, or healthcare settings where demographic-based judgments could have serious consequences. The paper also raises questions about how to mitigate these biases during model training or through post-training interventions.

## Limitations
- Study uses simulated scenarios rather than real-world interactions
- 1-100 scale may not fully capture nuanced human behavior
- Cultural context and training data geography not explicitly controlled
- Brittleness metric indicates significant response variability affecting real-world applications

## Confidence
- High confidence: Statistical significance of bias across all tested models
- Medium confidence: Magnitude of bias differences between demographic groups
- Medium confidence: Brittleness measurements

## Next Checks
1. Test models with real-world interaction scenarios to validate simulation-based findings
2. Conduct cross-cultural validation using prompts and contexts from different geographical regions
3. Evaluate model behavior when presented with mixed demographic information in single prompts