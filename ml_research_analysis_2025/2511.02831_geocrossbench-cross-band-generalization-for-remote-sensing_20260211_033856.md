---
ver: rpa2
title: 'GeoCrossBench: Cross-Band Generalization for Remote Sensing'
arxiv_id: '2511.02831'
source_url: https://arxiv.org/abs/2511.02831
tags:
- bands
- remote
- sensing
- data
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces GeoCrossBench, a benchmark designed to evaluate
  cross-band generalization in remote sensing models. It extends GeoBench by incorporating
  Sentinel-1 SAR data and creating three evaluation protocols: in-distribution (same
  bands for training and testing), no-overlap (transferring between non-overlapping
  bands), and superset (testing with more bands than seen during training).'
---

# GeoCrossBench: Cross-Band Generalization for Remote Sensing

## Quick Facts
- **arXiv ID:** 2511.02831
- **Source URL:** https://arxiv.org/abs/2511.02831
- **Reference count:** 27
- **Primary result:** GeoCrossBench benchmark reveals 2-4x performance drops in cross-band transfer for RS models; χViT achieves best generalization.

## Executive Summary
This work introduces GeoCrossBench, a benchmark designed to evaluate cross-band generalization in remote sensing models. It extends GeoBench by incorporating Sentinel-1 SAR data and creating three evaluation protocols: in-distribution (same bands for training and testing), no-overlap (transferring between non-overlapping bands), and superset (testing with more bands than seen during training). The authors develop χViT, a self-supervised ChannelViT extension, and compare it against various foundation models. Results show that general-purpose models like DINOv3 outperform specialized RS models in-distribution, all models suffer 2-4x performance drops when generalizing to non-overlapping bands, and χViT achieves the best cross-band transfer. Fine-tuning only the last layer with oracle labels can achieve consistent performance across satellites, indicating the benchmark is not saturated. The code and datasets are publicly released.

## Method Summary
GeoCrossBench evaluates cross-band generalization across three protocols using 12-band RS imagery (10 Sentinel-2 optical + 2 Sentinel-1 SAR). Models are trained on subsets of bands and tested on either the same bands (In-Distribution), different non-overlapping bands (No-Overlap), or with additional bands (Superset). χViT extends ChannelViT with iBOT-style self-supervised pretraining and hierarchical channel sampling. Models are evaluated on scene classification, segmentation, and change detection tasks using accuracy, mIoU, and F1 metrics. The benchmark includes x-bigearthnet, x-so2sat, x-eurosat, x-oscd, and other datasets.

## Key Results
- General-purpose models (DINOv3) outperform specialized RS models (DOFA, TerraFM) in-distribution.
- All models experience 2-4× performance drops when generalizing to non-overlapping bands (e.g., RGB→SAR).
- χViT achieves the best cross-band transfer, reducing the No-Overlap gap by 2-3 percentage points.
- Linear probing with oracle labels achieves consistent performance across satellites, showing the benchmark is not saturated.

## Why This Works (Mechanism)

### Mechanism 1: Channel-wise Tokenization with Shared Projections
- Claim: Tokenizing each spectral band independently with shared projection weights enables cross-band generalization.
- Mechanism: Unlike standard ViTs that create one token from a multi-channel patch, ChannelViT generates one token per single-channel patch. Shared projection weights across bands promote learning of transferable low-level features, while learnable channel embeddings retain spectral identity.
- Core assumption: Assumes that sharing weights across spectrally distinct bands (e.g., optical vs. SAR) still yields meaningful features—a non-obvious design choice.
- Evidence anchors:
  - [section 3.2] "Crucially, these projection weights W (image filters) are shared across all channels, promoting the learning of shared low-level features and enhancing robustness."
  - [corpus] No direct corpus support for this specific mechanism.
- Break condition: If bands share no statistical structure (e.g., thermal vs. visible light), shared projections may learn trivial or harmful features.

### Mechanism 2: Hierarchical Channel Sampling During Pretraining
- Claim: Randomly sampling channel subsets during self-supervised pretraining forces models to learn band-agnostic representations.
- Mechanism: During iBOT-style pretraining, student channels are sampled as subsets of teacher channels. This prevents over-reliance on any specific band combination and encourages learning patterns that transfer across sensor types.
- Core assumption: The authors hypothesize this explains χViT's No-Overlap performance but do not isolate it causally in ablation.
- Evidence anchors:
  - [section 4] "One of the reasons for the relatively strong performance of χViT compared to other multispectral models might be the trick of sampling of the bands during pretraining."
  - [section D, Table 4] Ablation shows subset sampling contributes ~3.6 mAP gain (51.10 → 54.72) on BigEarthNet 1%.
  - [corpus] No direct corpus support.
- Break condition: If pretraining data lacks diverse band combinations or "parallel" imagery (same location, different sensors), sampling may not expose useful cross-band structure.

### Mechanism 3: Linear Probe Sufficiency Indicates Representation Quality
- Claim: Frozen backbone representations already encode cross-band information accessible via linear probing.
- Mechanism: Fine-tuning only the final linear layer with oracle labels from all bands achieves relatively consistent performance across satellites, suggesting the bottleneck is not representation quality but adaptation strategy.
- Core assumption: Oracle labels represent an upper bound; real-world scenarios lack such labels.
- Evidence anchors:
  - [abstract] "fine-tuning just the last linear layer of these models using oracle labels from all bands can get relatively consistent performance across all satellites, highlighting that the benchmark is far from being saturated."
  - [section 4, Figure 3b] Shows linear probing on mixed band combinations improves S1 performance for several backbones.
  - [corpus] No direct corpus support.
- Break condition: If linear probing fails on truly novel band combinations, representations may not transfer as assumed.

## Foundational Learning

- **Vision Transformer (ViT) basics**
  - Why needed here: χViT extends ViT with channel-wise tokenization. Understanding patch embedding, positional embeddings, and the CLS token is essential.
  - Quick check question: Can you explain how ViT processes a 224×224×3 image into a sequence of tokens?

- **Self-supervised learning (iBOT/DINO paradigm)**
  - Why needed here: χViT uses iBOT-style self-distillation with masked image modeling. Understanding student-teacher networks, EMA updates, and patch-level losses is critical.
  - Quick check question: What role does the teacher network play in self-distillation, and how are its weights updated?

- **Remote sensing spectral bands**
  - Why needed here: The benchmark tests generalization across Sentinel-2 optical (10 bands) and Sentinel-1 SAR (2 polarizations). Understanding what different bands measure (e.g., NIR, SWIR, VV/VH) clarifies the difficulty of cross-band transfer.
  - Quick check question: Why might RGB→SAR transfer be fundamentally harder than RGB→NIR transfer?

## Architecture Onboarding

- **Component map:**
  - Input: 12-band images (10 S2 optical + 2 S1 SAR)
  - Tokenization: Per-band patch embedding with shared projection weights + learnable channel embeddings
  - Encoder: Standard ViT-B backbone
  - Heads: Task-specific (linear classifier, UPerNet decoder for segmentation/change detection)
  - Pretraining: iBOT-style self-distillation with hierarchical channel sampling

- **Critical path:**
  1. Prepare 12-band GeoCrossBench datasets (S1+S2 fusion)
  2. Load pretrained χViT or baseline (DINOv3, DOFA, etc.)
  3. Fine-tune on training bands (RGB or S2)
  4. Evaluate on all three protocols (In-Distribution, No-Overlap, Superset)

- **Design tradeoffs:**
  - Full fine-tuning vs. frozen backbone: Full fine-tuning averages ~10-20 points higher but loses some generalization.
  - Shared vs. band-specific projections: Shared weights improve cross-band transfer but may underfit band-specific structure.
  - Pretraining data mix: "Parallel" imagery (S1+S2 pairs) helps cross-modal transfer but is scarce.

- **Failure signatures:**
  - 2-4× performance drop on No-Overlap: Model overfit to training bands.
  - 5-25% drop on Superset: Model cannot integrate additional bands at test time.
  - RS-specific models underperforming general-purpose models (e.g., DOFA < DINOv3 in-distribution): Check pretraining scale and data quality.

- **First 3 experiments:**
  1. Replicate In-Distribution baseline (RGB→RGB, S2→S2) with DINOv3 to validate setup.
  2. Test No-Overlap transfer (RGB→S1) with χViT vs. DINOv3 to confirm reported 2-3 point gap.
  3. Run linear probing with mixed-band oracle labels (Figure 3b setup) to assess representation quality independently of fine-tuning strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel architectural mechanisms be developed to allow models to robustly leverage spectral bands at test time that were not present during training?
- Basis in paper: [explicit] The authors state that the poor performance in the "Superset" setting "implies that new ideas are necessary for the models to leverage the additional signal coming from unseen bands at test time."
- Why unresolved: Current models suffer a 5–25% performance drop when given additional bands (e.g., testing on RGBN when trained on RGB), treating the extra information as noise rather than a signal.
- What evidence would resolve it: A model architecture or attention mechanism that maintains or increases performance in the "Superset" evaluation protocol compared to the in-distribution baseline.

### Open Question 2
- Question: What specific pretraining strategies or data modalities are required for remote sensing foundation models to consistently outperform general-purpose vision models?
- Basis in paper: [explicit] The authors ask, "what additional knowledge can RS-specific foundation models learn that will help them beat general-purpose models" like DINOv3.
- Why unresolved: Specialized RS models (e.g., DOFA, TerraFM) currently fail to outperform generic models even in in-distribution settings, suggesting current domain-specific pretraining is insufficient.
- What evidence would resolve it: A remote sensing-specific training regime that yields statistically significant improvements over DINOv3 across the In-Distribution and No-Overlap protocols.

### Open Question 3
- Question: To what extent does high-quality, co-registered "parallel" imagery (e.g., aligned optical-SAR pairs) drive cross-band generalization compared to simply scaling model size?
- Basis in paper: [explicit] The paper suggests future work should focus on "parallel" imagery datasets, hypothesizing they might "improve cross-band generalization abilities" similarly to translation data in LLMs.
- Why unresolved: While χViT uses some parallel data, the specific causal link between the volume of these aligned pairs and the reduction of the 2–4x performance drop in the No-Overlap setting remains underexplored.
- What evidence would resolve it: Ablation studies isolating the variable of "parallel data" volume during pretraining and measuring the resulting delta in transfer learning performance.

## Limitations

- The benchmark assumes co-registered S1+S2 imagery is available, which limits real-world applicability.
- χViT's superiority is not fully mechanistically isolated—hierarchical channel sampling's specific contribution lacks ablation.
- Oracle label experiments are unrealistic since true cross-band labels rarely exist in practice.

## Confidence

- **High confidence**: In-distribution performance rankings (DINOv3 > DOFA > RS-specific models) and the general magnitude of cross-band performance drops (2-4×) are reproducible findings.
- **Medium confidence**: χViT's cross-band transfer advantage is supported but not fully isolated mechanistically; the channel sampling hypothesis lacks ablation.
- **Low confidence**: The oracle label experiment's practical relevance and the assumption that shared projections work across optical-SAR domains without degradation.

## Next Checks

1. **Ablation study**: Remove hierarchical channel sampling from χViT pretraining and measure No-Overlap performance drop to isolate this mechanism's contribution.
2. **Realism check**: Replace oracle labels in linear probing with pseudo-labels from source bands and measure performance decay to test practical transferability.
3. **Distribution shift test**: Evaluate models on temporally misaligned S1+S2 pairs (e.g., S1 from day 1, S2 from day 15) to assess robustness beyond co-registration assumptions.