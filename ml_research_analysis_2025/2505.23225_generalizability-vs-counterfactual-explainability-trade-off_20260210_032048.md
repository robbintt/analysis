---
ver: rpa2
title: Generalizability vs. Counterfactual Explainability Trade-Off
arxiv_id: '2505.23225'
source_url: https://arxiv.org/abs/2505.23225
tags:
- counterfactual
- training
- decision
- boundary
- valid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a novel theoretical and empirical connection\
  \ between model generalization and counterfactual explainability. The authors introduce\
  \ \u03B5-valid counterfactual probability (\u03B5-VCP) - the probability of finding\
  \ perturbations within an \u03B5-neighborhood that change a model's prediction."
---

# Generalizability vs. Counterfactual Explainability Trade-Off

## Quick Facts
- **arXiv ID**: 2505.23225
- **Source URL**: https://arxiv.org/abs/2505.23225
- **Reference count**: 40
- **One-line primary result**: Model overfitting creates a fundamental trade-off between generalization and counterfactual explainability, as overfitting reduces decision boundary margins, making valid counterfactuals more probable.

## Executive Summary
This paper establishes a theoretical and empirical connection between model generalization and counterfactual explainability through the novel concept of ε-valid counterfactual probability (ε-VCP). The authors prove that as models overfit and decision boundaries tighten around training data, the geometric margin between data points and boundaries shrinks, increasing the probability of finding valid counterfactuals within an ε-neighborhood. This creates a trade-off: more complex models that fit training data tightly become easier to explain via counterfactuals but generalize worse. Empirical results on Water Potability and Air Quality datasets validate this theory, showing increasing ε-VCP as training accuracy improves for both linear and non-linear models. Regularization techniques like dropout partially mitigate this effect by smoothing decision boundaries.

## Method Summary
The method estimates ε-VCP by uniformly sampling perturbations within an ε-radius of each training point and measuring the fraction that change the model's prediction. For linear models, exact geometric margins are calculated analytically, while for non-linear MLPs, Monte Carlo integration with 1000 samples per point approximates the probability. The study trains logistic regression with polynomial expansion and 5-layer MLPs on two binary classification datasets, tracking ε-VCP across training epochs alongside accuracy metrics. Regularization is implemented via dropout (0.5 rate) to assess its impact on the generalization-explainability trade-off.

## Key Results
- ε-VCP increases non-linearly as geometric margins between data points and decision boundaries decrease during overfitting
- Linear models with polynomial expansion and MLPs both exhibit rising ε-VCP as training accuracy approaches 100%
- Dropout regularization partially mitigates the trade-off by maintaining larger geometric margins despite high training accuracy
- The average ε-VCP metric provides a quantitative measure of model overfitting through counterfactual explainability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The probability of finding a valid counterfactual (ε-VCP) increases non-linearly as the geometric margin between a data point and the decision boundary decreases.
- **Mechanism:** Valid counterfactuals must lie within a specific "shell" (distance γ to ε). As the margin γ shrinks (the boundary moves closer to the point), the volume of the valid counterfactual region (a spherical cap) occupies a larger proportion of this shell, making random perturbations more likely to cross the boundary.
- **Core assumption:** Perturbations are sampled uniformly or densely enough within the ε-neighborhood to approximate the geometric volume ratio.
- **Evidence anchors:**
  - [Page 5, Theorem 4.2] Formalizes ε-VCP as a function of margin γᵢ and dimensionality n.
  - [Page 3, Definition 3.4] Defines the geometry of the valid counterfactual shell.
  - [corpus] "Countering Overfitting with Counterfactual Examples" supports the general link between overfitting and counterfactual accessibility.
- **Break condition:** If data lies strictly on a low-dimensional manifold where uniform sampling misses the boundary directions, the volume-based probability estimation fails.

### Mechanism 2
- **Claim:** Model overfitting acts as a primary driver for increasing ε-VCP by compressing average geometric margins.
- **Mechanism:** Overfitting forces the model to contort the decision boundary to correctly classify noisy training samples. This "tight wrapping" reduces the distance (margin) of training points to the boundary, which, via Mechanism 1, statistically guarantees higher counterfactual probability.
- **Core assumption:** High training accuracy in complex models implies convoluted decision boundaries rather than robust feature learning.
- **Evidence anchors:**
  - [Page 1, Figure 1] Visual intuition of distance reduction during overfitting.
  - [Page 7, Section 4.4] Jensen's inequality applied to show average ε-VCP increases as average margin decreases.
  - [corpus] "Harmful Overfitting in Sobolev Spaces" discusses generalization degradation in overparameterized models, consistent with the margin compression premise.
- **Break condition:** If a model is "benignly overfitting" (interpolating noise without shrinking margins), the correlation between accuracy and ε-VCP may decouple.

### Mechanism 3
- **Claim:** Regularization techniques (specifically dropout) partially decouple generalization from explainability by smoothing decision boundaries.
- **Mechanism:** Dropout prevents the network from relying on specific sparse features, forcing a "smoother" decision surface. This maintains larger geometric margins compared to unregularized models, thereby suppressing the spike in ε-VCP even as training accuracy rises.
- **Core assumption:** Smoother decision boundaries correlate with larger geometric margins for training data.
- **Evidence anchors:**
  - [Page 8, Figure 4] Empirical results showing regularized MLPs have lower ε-VCP than plain MLPs at similar accuracy levels.
  - [Page 9, Section 5.2] Notes that dropout "smooths the decision boundary" but may only mitigate, not eliminate, the trade-off.
  - [corpus] Corpus provides limited direct evidence on dropout's specific effect on counterfactual probability; mechanism relies primarily on the paper's empirical findings.
- **Break condition:** If regularization is too weak or the model capacity is excessive, the smoothing effect is insufficient to prevent margin collapse.

## Foundational Learning

- **Concept**: **Geometric Margin (γᵢ)**
  - **Why needed here**: This is the central variable in the paper's theoretical proof. Understanding that γᵢ is the minimum distance to the decision boundary is required to grasp why small perturbations succeed or fail.
  - **Quick check question**: If a point lies exactly on the decision boundary, what is its geometric margin?

- **Concept**: **Monte Carlo Integration**
  - **Why needed here**: The paper estimates the theoretical probability (ε-VCP) practically by sampling random perturbations. You need to understand that this is a numerical method to measure the "volume" of valid counterfactuals.
  - **Quick check question**: Why does the paper sample 1,000 random perturbations per point rather than calculating the probability analytically for the MLP?

- **Concept**: **Regularization (Dropout)**
  - **Why needed here**: The paper positions regularization as the counter-force to the generalization-explainability trade-off.
  - **Quick check question**: According to the paper, does dropout lower ε-VCP by making the model less accurate, or by changing the geometry of the decision boundary?

## Architecture Onboarding

- **Component map**: Perturbation Engine -> Margin Estimator -> Probability Estimator
- **Critical path**:
  1. Select a trained model checkpoint and a dataset sample xᵢ.
  2. Determine the perturbation budget ε (must be calibrated to feature scale; see Appendix on polynomial expansion).
  3. Generate N perturbed samples inside the shell S(xᵢ, γᵢ, ε).
  4. Query the model for label changes to estimate pᵢε.

- **Design tradeoffs**:
  - **Uniform vs. Manifold Sampling**: The paper uses uniform sampling for theoretical simplicity. In production, this may generate unrealistic/invalid off-manifold counterfactuals, potentially inflating the "validity" metric semantically.
  - **Exact vs. Approximate Margin**: Calculating exact margins for non-linear models (MLPs) is computationally expensive; the paper suggests local linear approximations or Monte Carlo methods.

- **Failure signatures**:
  - **ε-VCP ≈ 0.5 immediately**: The decision boundary might be passing directly through the data cluster (random guessing or extreme overfitting).
  - **ε-VCP ≈ 0 always**: The chosen ε is likely too small relative to the geometric margin of the data (the shell is empty).
  - **Dimensionality Mismatch**: If features are expanded (e.g., polynomial degree 6 in the paper), ε must be rescaled (ε_aug = ε_ori √(n'/n)), otherwise the search radius becomes meaningless.

- **First 3 experiments**:
  1. **Linear Baseline Validation**: Train a Logistic Regression model. Compute exact margins analytically and compare them against the Monte Carlo ε-VCP estimate to verify Theorem 4.2.
  2. **Overfitting Stress Test**: Train an MLP to 100% training accuracy. Plot ε-VCP per epoch. Confirm that the metric rises as test accuracy diverges from training accuracy.
  3. **Regularization Ablation**: Train two identical MLPs (with and without dropout). Compare the slope of the ε-VCP curve; the regularized model should show a flatter trajectory.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a regularization term explicitly based on ε-valid counterfactual probability (ε-VCP) be designed to effectively mitigate overfitting while preserving model explainability?
- **Basis in paper:** [explicit] The authors state in Section 6: "In future work, we plan to design a regularization term based on ε-VCP and incorporate it into standard training objectives to mitigate overfitting while preserving explainability."
- **Why unresolved:** The current work only establishes the theoretical trade-off and observes that standard regularization (dropout) only partially mitigates the increase in ε-VCP. The implementation of a specific counterfactual-based regularizer is left for future investigation.
- **What evidence would resolve it:** A modified training objective that penalizes high ε-VCP values, demonstrated empirically to maintain or improve test accuracy (generalization) without excessively increasing the difficulty of finding valid counterfactuals.

### Open Question 2
- **Question:** How does the relationship between generalization and counterfactual explainability change when using manifold-based or data-driven perturbations rather than uniform distributions?
- **Basis in paper:** [explicit] Section 6 notes: "We assume uniform perturbations to compute ε-VCP, which may not reflect real-world data geometry... Future work could incorporate manifold-based or data-driven perturbations... to obtain sharper insights into the generalization–explainability trade-off."
- **Why unresolved:** The theoretical analysis and Monte Carlo estimation rely on uniform sampling within an ε-ball, which ignores the underlying data manifold and may produce unrealistic perturbations.
- **What evidence would resolve it:** A theoretical extension of the ε-VCP metric using on-manifold sampling techniques (e.g., via generative models) and empirical validation showing if the trade-off persists or shifts under these constraints.

### Open Question 3
- **Question:** Can the theoretical bounds linking ε-VCP to the geometric margin be extended to complex non-linear models using K-Lipschitz assumptions?
- **Basis in paper:** [explicit] Section 6 identifies a limitation regarding the geometric margin: "For more complex models, a potential direction is to estimate the margin using a K-Lipschitz assumption... similar to those proposed by Hein and Andriushchenko [2017] and Tsuzuku et al. [2018]."
- **Why unresolved:** The current theoretical results for non-linear models (Theorem 4.3) rely on local linear approximations (tangent hyperplanes), lacking global certification or bounds for highly curved decision boundaries common in deep learning.
- **What evidence would resolve it:** A formal proof deriving the relationship between ε-VCP and the decision boundary geometry for general Lipschitz continuous classifiers, rather than just locally linearized ones.

## Limitations

- Theoretical proof relies heavily on geometric assumptions about uniform sampling and convex decision regions that may not hold for complex real-world data
- Empirical validation uses only two datasets, limiting generalizability of the findings across different domains and problem types
- The choice of ε values appears empirically determined rather than systematically derived, with unclear methodology for selecting appropriate values for new datasets

## Confidence

- **High Confidence**: The theoretical relationship between margin compression and ε-VCP in linear models (Theorem 4.2); the empirical observation that overfitting correlates with increased ε-VCP in both datasets tested
- **Medium Confidence**: The extension of the geometric argument to non-linear models via local approximations; the effectiveness of dropout in mitigating the trade-off (based on single regularization technique)
- **Low Confidence**: The practical utility of ε-VCP as a diagnostic metric in real-world scenarios with complex feature interactions; the claim that this represents a "fundamental trade-off" rather than a training artifact

## Next Checks

1. **Multi-class Extension**: Test the ε-VCP trade-off on multi-class classification tasks (e.g., CIFAR-10) to verify the theory extends beyond binary classification
2. **Alternative Regularization**: Compare dropout against other regularization methods (weight decay, early stopping) to determine if the mitigation effect is specific to dropout or represents a broader phenomenon
3. **Decision Boundary Geometry**: Use activation maximization or adversarial example generation to visualize how decision boundaries actually evolve during overfitting, providing qualitative validation of the margin compression hypothesis