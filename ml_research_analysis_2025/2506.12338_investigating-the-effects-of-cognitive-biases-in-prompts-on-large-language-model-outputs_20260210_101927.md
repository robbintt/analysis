---
ver: rpa2
title: Investigating the Effects of Cognitive Biases in Prompts on Large Language
  Model Outputs
arxiv_id: '2506.12338'
source_url: https://arxiv.org/abs/2506.12338
tags:
- biases
- cognitive
- bias
- prompts
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study examines how cognitive biases in user prompts affect\
  \ LLM outputs, introducing a framework that systematically injects confirmation\
  \ and availability biases into prompts and measures their impact on model accuracy.\
  \ Using datasets like BIG-Bench Hard and FinQA, experiments with GPT-3.5, GPT-4,\
  \ Vicuna, and Mistral show that biased prompts significantly reduce LLM accuracy\u2014\
  sometimes by over 20%\u2014compared to unbiased baselines."
---

# Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs

## Quick Facts
- arXiv ID: 2506.12338
- Source URL: https://arxiv.org/abs/2506.12338
- Reference count: 0
- Primary result: Biased prompts reduce LLM accuracy by over 20% compared to unbiased baselines

## Executive Summary
This study introduces a framework for systematically injecting cognitive biases into prompts and measuring their impact on LLM outputs. Using datasets like BIG-Bench Hard and FinQA, experiments with GPT-3.5, GPT-4, Vicuna, and Mistral demonstrate that biased prompts significantly degrade model accuracy. Attention weight analysis reveals that biases shift model focus toward incorrect answer options, providing a mechanistic explanation for unreliable outputs.

## Method Summary
The research systematically injects confirmation and availability biases into prompts using a controlled framework. Binary-choice questions from BIG-Bench Hard (3 tasks, 760 total samples) and FinQA (144 samples) are presented with CoT instructions. Biases are appended at the end of prompts and include suggested answers, many wrong answers, and recall/reflection biases. Models are evaluated using zero-shot inference with temperature=0, and attention weights are extracted from the last layer focusing on answer tokens to measure bias effects.

## Key Results
- Biased prompts reduce LLM accuracy by over 20% compared to unbiased baselines
- Attention weight analysis shows biases shift model focus toward incorrect answer options
- Different bias types (suggested answers, many wrong answers, recall biases) produce varying degrees of accuracy degradation

## Why This Works (Mechanism)
The framework works by exploiting how LLMs process contextual information in prompts. When biases are injected, they create misleading contextual cues that influence the model's attention distribution. The attention weight analysis demonstrates that biased prompts cause the model to allocate more attention to incorrect answer tokens, leading to higher probability of selecting wrong answers. This mechanism explains why even sophisticated models like GPT-4 and Vicuna are susceptible to cognitive biases in user prompts.

## Foundational Learning
- **Attention mechanisms**: Why needed - Understanding how models weight different tokens helps explain bias effects. Quick check - Verify attention weights shift toward incorrect answers when biases are present.
- **Zero-shot inference**: Why needed - The study uses zero-shot prompting to test baseline model capabilities. Quick check - Confirm temperature=0 and no additional training is used.
- **Binary-choice evaluation**: Why needed - The study focuses on binary tasks for clear accuracy measurement. Quick check - Verify all tasks have exactly two answer options.

## Architecture Onboarding
- **Component map**: Datasets (BBH, FinQA) -> Prompt Construction -> LLM Inference -> Accuracy Calculation -> Attention Analysis
- **Critical path**: Prompt generation → Model inference → Output parsing → Accuracy computation → Attention weight extraction
- **Design tradeoffs**: Binary tasks provide clear metrics but limit generalizability; bias injection at prompt end tests recency effects but may not reflect natural usage patterns
- **Failure signatures**: Non-deterministic outputs from incorrect temperature settings; attention extraction errors from wrong layer/head selection
- **First experiments**: 1) Verify unbiased baseline accuracy on sample prompts; 2) Test single bias type on small dataset; 3) Compare attention weights between unbiased and biased conditions

## Open Questions the Paper Calls Out
- How do implicit cognitive biases in prompts affect LLM outputs compared to explicit biases studied in this framework?
- Does the placement of cognitive bias within a prompt (beginning vs. end) alter the degree of performance degradation?
- Do cognitive biases in prompts affect LLM performance in non-binary tasks to the same extent observed in binary Q&A?
- Can domain-specific fine-tuning mitigate the susceptibility of LLMs to cognitive biases in prompts?

## Limitations
- Exact prompt templates for BBH and FinQA datasets are not fully specified
- "Many Wrong Answers" bias construction is underspecified beyond basic format
- Selection criteria for biased prompts directed toward incorrect answers are unclear
- Results may not generalize beyond binary-choice tasks

## Confidence
- High confidence: Accuracy degradation from biased prompts is well-supported with statistical analysis
- Medium confidence: Framework generalizability to other bias types and architectures is suggested but not demonstrated
- Low confidence: Specific attention weight patterns are difficult to verify without complete prompt specifications

## Next Checks
1. Reconstruct exact prompt templates for BBH and FinQA binary-choice tasks using available information
2. Implement sensitivity analysis by varying "Many Wrong Answers" sequence length and formatting
3. Conduct ablation study testing attention extraction at different layers and head combinations