---
ver: rpa2
title: Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based
  Auto-Labeling
arxiv_id: '2511.10866'
source_url: https://arxiv.org/abs/2511.10866
tags:
- video
- detection
- violence
- short
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Short-Window Sliding Learning framework for
  real-time violence detection in CCTV footages. The method divides videos into 1-2
  second clips and applies Large Language Model (LLM)-based auto-caption labeling
  to construct fine-grained datasets.
---

# Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling

## Quick Facts
- arXiv ID: 2511.10866
- Source URL: https://arxiv.org/abs/2511.10866
- Authors: Seoik Jung; Taekyung Song; Yangro Lee; Sungjun Lee
- Reference count: 17
- Primary result: 95.25% accuracy on RWF-2000 using short-window sliding learning with LLM-based auto-labeling

## Executive Summary
This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footage. The method processes 1-2 second clips using all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. The framework employs LLM-based auto-captioning (via Google Gemini) to generate fine-grained labels, which are then validated by human reviewers to create high-quality training data. The approach achieves 95.25% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25%), demonstrating strong generalization and real-time applicability in intelligent surveillance systems.

## Method Summary
The method segments videos into 1-2 second clips using sliding window segmentation, then generates captions for each clip using Google Gemini LLM. These captions are organized into coarse-to-fine label structures (Violence/Non-violence â†’ Punching, Kicking, Pushing, Chasing) and validated by three human reviewers over 120 hours. The InternVL3 vision-language model is trained on these clip-caption pairs, processing 12-15 frames per clip. During inference, the system applies 15-frame sliding windows to real-time video streams, enabling continuous violence detection while maintaining temporal precision through the short-window approach.

## Key Results
- Achieves 95.25% accuracy on RWF-2000 dataset
- Improves UCF-Crime long video performance from 55.75% to 83.25% accuracy
- Demonstrates strong generalization from short-clip training to long-video inference
- Reduces temporal information loss compared to existing sampling-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Processing 1-2 second clips with all frames preserves temporal continuity for detecting rapid violent events.
- Mechanism: By constraining clips to 1-2 seconds and utilizing every frame rather than sampling at intervals, the model receives complete motion information within the violence-relevant timescale.
- Core assumption: Violent acts in CCTV footage typically manifest within 1-2 second windows.
- Evidence anchors: Abstract states "Each short clip fully utilizes all frames to preserve temporal continuity," and Section III.B explains how all frames minimize temporal information loss.

### Mechanism 2
- Claim: LLM-based auto-captioning with human review enables fine-grained label hierarchies that improve semantic learning.
- Mechanism: Gemini generates sentence-form action descriptions, organized into coarse-to-fine label structures, with human reviewers validating labels over ~120 hours.
- Core assumption: The LLM can reliably distinguish violent from non-violent actions in CCTV footage after human correction.
- Evidence anchors: Abstract mentions "Large Language Model (LLM)-based auto-caption labeling," and Section III.A describes the human review process.

### Mechanism 3
- Claim: Short-window training generalizes to long-video inference when combined with multi-domain short-clip aggregation.
- Mechanism: Training exclusively on short clips from multiple datasets produces representations that transfer to longer videos at inference time.
- Core assumption: Temporal patterns learned at 1-2 second scales compose into longer-duration understanding.
- Evidence anchors: Table I shows UCF-Crime/Long training: 55.75% accuracy vs. UCF-Crime/Short training: 83.25% accuracy.

## Foundational Learning

- Concept: **Sliding Window Segmentation with Stride**
  - Why needed here: Understanding how N = (L-W)/S + 1 derives clip count from video length L, window size W, and stride S is essential for data construction.
  - Quick check question: Given a 10-second video, 2-second windows, and 1-second stride, how many clips are generated? (Answer: 9)

- Concept: **Vision-Language Model (VLM) Architecture**
  - Why needed here: InternVL3 jointly encodes visual frames and text captions, requiring understanding of feature alignment in shared representation space.
  - Quick check question: What is the difference between a VLM and a pure vision classifier for this task? (Answer: VLM learns semantic consistency between visual patterns and action meanings via text; classifier maps directly to label indices.)

- Concept: **Frame Sampling vs. Dense Frame Utilization**
  - Why needed here: The paper explicitly rejects sampling-based approaches; understanding why dense frames matter for short-duration actions is critical.
  - Quick check question: Why does sampling at 1 FPS from a 30-second video risk missing a 1-second punch? (Answer: The sampled frame may fall outside the action window entirely.)

## Architecture Onboarding

- Component map: Raw CCTV video -> Sliding window segmentation (1-2s clips) -> Gemini auto-captioning -> Human review -> Coarse/fine label assignment -> InternVL3 model

- Critical path: Video segmentation quality -> Caption accuracy -> Human review thoroughness -> Frame count per clip

- Design tradeoffs: Shorter windows increase temporal precision but reduce context; longer windows add context but dilute short-event signal. Higher human review time improves labels but scales poorly.

- Failure signatures: Low accuracy on rapid strikes suggests window size too large; high false positive rate suggests caption over-interpretation; poor generalization suggests insufficient spatial diversity.

- First 3 experiments: 1) Baseline window ablation: Train with 1s vs 2s windows on RWF-2000 subset; 2) Caption quality audit: Sample 100 auto-generated captions for agreement rate; 3) Long-video inference test: Run sliding-window inference on full UCF-Crime videos.

## Open Questions the Paper Calls Out

- Question: Does integrating audio and subtitle modalities significantly improve detection accuracy for complex, non-physical anomalies (e.g., theft, suicide attempts) compared to vision-only short-window approach?
  - Basis in paper: The conclusion states future research plans to "integrate multiple modalities (voice, subtitles, action features) to cover more complex abnormal situations."
  - Why unresolved: The current study validates the method using only visual data for physical violence; it has not been tested on anomalies requiring auditory or sequential context.
  - What evidence would resolve it: Experimental results comparing the proposed short-window model against a multimodal version on datasets containing complex audio-visual anomalies.

- Question: To what extent does the model's quality depend on the 120 hours of manual human review required to correct LLM hallucinations?
  - Basis in paper: The methodology notes that 3 reviewers spent 40 hours each (120 hours total) manually checking auto-generated captions.
  - Why unresolved: It is unclear if the high accuracy (95.25%) is due to the LLM's semantic understanding or the subsequent heavy human filtration; the scalability remains unquantified.
  - What evidence would resolve it: An ablation study measuring performance degradation when training on raw, unreviewed LLM labels versus human-refined labels.

## Limitations

- The framework relies heavily on LLM-based auto-labeling, introducing uncertainty regarding caption quality and domain generalization.
- The claim that short-window training generalizes to long-video inference lacks empirical validation on real-world continuous surveillance footage.
- The paper does not provide error analysis of Gemini's hallucination rates or discuss performance degradation with varying CCTV quality.

## Confidence

- High Confidence: 95.25% accuracy on RWF-2000 and 83.25% on UCF-Crime/Short are well-documented with clear comparisons to baseline methods.
- Medium Confidence: LLM-based auto-labeling efficacy - while the framework describes human review, it lacks quantitative analysis of label noise rates and impact on downstream performance.
- Low Confidence: Long-video inference generalization - based on limited evidence from UCF-Crime, which may not represent real-world deployment scenarios.

## Next Checks

1. **Caption Quality Audit**: Conduct systematic evaluation of Gemini's auto-generated captions by sampling 200 clips and computing: (a) inter-annotator agreement between Gemini outputs and human reviewers, (b) hallucination rate, and (c) caption consistency across similar actions.

2. **Real-World Deployment Simulation**: Test the framework on continuous 2+ hour CCTV streams from public sources to evaluate: (a) computational efficiency under sustained inference, (b) temporal coherence of detections across sliding windows, and (c) performance degradation due to real-world factors.

3. **Temporal Context Ablation Study**: Systematically vary window sizes (0.5s, 1s, 2s, 3s) and stride parameters on a subset of RWF-2000 to quantify the tradeoff between temporal precision and contextual awareness.