---
ver: rpa2
title: Scaling Spatial Intelligence with Multimodal Foundation Models
arxiv_id: '2511.13719'
source_url: https://arxiv.org/abs/2511.13719
tags:
- spatial
- image
- object
- sensenova-si
- degrees
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of current multimodal foundation
  models in spatial intelligence, which is the ability to understand, reason about,
  and act within three-dimensional space. To address this, the authors propose a data-centric
  approach to scale up spatial intelligence in multimodal models.
---

# Scaling Spatial Intelligence with Multimodal Foundation Models

## Quick Facts
- arXiv ID: 2511.13719
- Source URL: https://arxiv.org/abs/2511.13719
- Authors: Zhongang Cai; Ruisi Wang; Chenyang Gu; Fanyi Pu; Junxiang Xu; Yubo Wang; Wanqi Yin; Zhitao Yang; Chen Wei; Qingping Sun; Tongxi Zhou; Jiaqi Li; Hui En Pang; Oscar Qian; Yukun Wei; Zhiqian Lin; Xuanke Shi; Kewang Deng; Xiaoyang Han; Zukai Chen; Xiangyu Fan; Hanming Deng; Lewei Lu; Liang Pan; Bo Li; Ziwei Liu; Quan Wang; Dahua Lin; Lei Yang
- Reference count: 40
- One-line primary result: Data-centric scaling with 8M diverse samples achieves state-of-the-art on 5 spatial intelligence benchmarks while maintaining general multimodal competence.

## Executive Summary
This paper addresses the limitations of current multimodal foundation models in spatial intelligence—the ability to understand, reason about, and act within three-dimensional space. To address this, the authors propose a data-centric approach to scale up spatial intelligence in multimodal models. They curate a large dataset, SenseNova-SI-8M, containing 8 million diverse data samples across five key spatial capabilities: Metric Measurement, Spatial Relations, Mental Reconstruction, Perspective-taking, and Comprehensive Reasoning. Using this dataset, they train and evaluate models based on InternVL3, Qwen3-VL, and Bagel, achieving state-of-the-art performance on five recent spatial intelligence benchmarks while maintaining strong general multimodal understanding. The study also analyzes data scaling laws, emergent generalization, robustness against overfitting, and potential downstream applications.

## Method Summary
The authors curate SenseNova-SI-8M, an 8.5M-sample dataset organized around a taxonomy of five spatial capabilities. They fine-tune base models (InternVL3, Qwen3-VL, Bagel) for one epoch using AdamW with learning rate 5e-6 and batch size 2048 on 128 GPUs. Training includes both spatial and general QA data to prevent catastrophic forgetting. The models are evaluated on five spatial benchmarks (VSI-Bench, MMSI, MindCube, ViewSpatial, SITE) and general multimodal benchmarks to verify retention.

## Key Results
- State-of-the-art performance on five spatial intelligence benchmarks while maintaining general multimodal competence
- Clear evidence of emergent generalization and transfer across spatial capabilities
- Demonstrated robustness against shortcut learning and language priors via circular tests and vision-ablated evaluations
- Performance gains primarily driven by diverse, taxonomy-guided data mixing rather than architectural changes

## Why This Works (Mechanism)

### Mechanism 1: Taxonomy-Guided Data Mixing for Spatial Capability Coverage
Performance gains are primarily driven by diverse, taxonomy-guided data mixing, not architectural changes. A principled taxonomy of five spatial capabilities ensures the model receives training signal across all core spatial competencies rather than overfitting to a subset. This addresses known gaps in existing open-source datasets—particularly perspective-taking tasks.

### Mechanism 2: Cross-Task Spill-Over and Emergent Generalization
Training on one spatial task family produces nontrivial transfer to seemingly unrelated spatial tasks, suggesting shared underlying representations. Models trained on perspective-taking data improve on downstream tasks requiring sequential viewpoint simulation and cross-view aggregation. The paper hypothesizes "meta-tasks" in perspective-taking that unify related capabilities.

### Mechanism 3: Robustness via Data Diversity Against Shortcuts and Overfitting
Large-scale diverse training reduces reliance on language priors and annotation biases compared to smaller, single-source training. The paper uses "circular tests" and vision-ablated evaluations to detect shortcut learning, showing SenseNova-SI has smaller performance drops under these stress tests than baselines.

## Foundational Learning

- **Multimodal Foundation Models (MLLMs)**
  - Why needed here: The paper builds directly on InternVL3, Qwen3-VL, and Bagel—architectures that already provide strong visual-linguistic alignment. Understanding their pretraining paradigms informs why data-centric fine-tuning is sufficient.
  - Quick check question: Can you explain why a natively multimodal model like InternVL3 might require different fine-tuning assumptions than a language-extended model like Qwen3-VL?

- **Spatial Intelligence Taxonomy**
  - Why needed here: The SenseNova-SI-8M dataset is explicitly organized around five capability categories. Reproducing or extending this work requires understanding what each capability entails and why certain tasks are underrepresented in prior datasets.
  - Quick check question: Distinguish between "spatial relations" (egocentric vs. allocentric) and "perspective-taking" (view correspondence vs. allocentric transformation). Which is more data-scarce according to the paper?

- **Supervised Fine-Tuning (SFT) on Mixed Domains**
  - Why needed here: The training scheme involves one epoch of SFT on mixed spatial and general QA data. Understanding learning rate schedules, batch sizes, and catastrophic forgetting mitigation is critical for stable training.
  - Quick check question: Why does the paper include "General QA" data alongside spatial data, and what failure mode does this aim to prevent?

## Architecture Onboarding

- **Component map:** Base MLLM (InternVL3/Qwen3-VL/Bagel) -> Data Pipeline (unified annotation of 3D datasets) -> Training Loop (1 epoch, batch 2048, AdamW) -> Evaluation Suite (spatial benchmarks + general retention checks)

- **Critical path:** Dataset curation via taxonomy-driven sampling and synthetic QA generation → Filtering for visibility, ambiguity, and cross-view difficulty → Mixed-domain SFT on combined spatial + general QA → Evaluation on spatial benchmarks + general multimodal benchmarks → Stress testing via circular tests and vision-ablated baselines

- **Design tradeoffs:**
  - Data volume vs. saturation: Performance gains diminish beyond 7–8M samples
  - CoT vs. direct QA: Text-based chain-of-thought yields marginal gains (~2%) but adds token overhead
  - Model capacity vs. task complexity: 2B models underperform on perspective-taking tasks compared to 8B
  - Specialization vs. generalization: Including general QA data trades some spatial peak performance for retained multimodal competence

- **Failure signatures:**
  - Shortcut reliance: Near-identical scores with/without vision indicate language prior exploitation
  - Task imbalance: Over-representation of MM/SR tasks and under-representation of PT/MR leads to skewed capability profiles
  - Catastrophic forgetting: Significant drops on general benchmarks after spatial-only training
  - Frame-context mismatch: Models trained on 16 frames may degrade on longer sequences

- **First 3 experiments:**
  1. Single-dataset ablation: Train InternVL3-8B on each source dataset independently and evaluate across all five benchmarks to identify contribution profiles
  2. Scaling curve analysis: Train with incremental data volumes (0M → 8M) and plot per-capability and per-benchmark performance to identify saturation points
  3. Shortcut stress test: Evaluate trained models on circular variants of benchmarks and with vision ablated to quantify reliance on language priors vs. visual grounding

## Open Questions the Paper Calls Out

### Open Question 1
What algorithmic innovations beyond data scaling are required to overcome performance saturation in spatial intelligence? While the paper validates the effectiveness of data scaling, it does not propose or evaluate specific paradigm shifts (e.g., world models, explicit 3D representations) that could break through the identified performance plateau.

### Open Question 2
What reasoning mechanisms are necessary for effective spatial chain-of-thought, given that text-based CoT yields only marginal improvements? The paper's hypotheses about train-test mismatch and bias toward human-preferred explanation paths are suggestive but not empirically validated; no alternative CoT paradigm is proposed.

### Open Question 3
What is the minimum model capacity required to robustly learn perspective-taking transformations? The paper observes that 2B models "diverge sharply" from 8B models on perspective-taking tasks, hypothesizing that smaller models "lack sufficient capacity to robustly learn viewpoint transformations," but this hypothesis is not tested systematically.

### Open Question 4
Under what conditions does capability transfer occur between distinct spatial skills (e.g., from correspondence tasks to route planning)? The paper demonstrates transfer effects but does not identify the mechanism or predict the conditions under which such spill-over will occur.

## Limitations
- Exact data mixing ratios and QA generation pipeline from 3D datasets are not fully specified
- No systematic ablation of individual data sources to quantify their contributions
- Cross-task transfer mechanism remains unexplained despite observed effects
- Performance saturation point and diminishing returns are observed but not precisely characterized

## Confidence
- **High Confidence:** Core experimental results (state-of-the-art performance, retention of general competence) are well-supported
- **Medium Confidence:** Mechanisms of cross-task spill-over, emergent generalization, and robustness against shortcuts are plausible but lack full ablation
- **Low Confidence:** Exact contribution of each data source, precise data mixing strategy, and QA generation pipeline are not fully specified

## Next Checks
1. **Ablation of Data Sources and Mixing Ratios:** Train models using only subsets of the SenseNova-SI-8M dataset and systematically vary the mixing ratios of the five capabilities to quantify their individual contributions.

2. **Independent Replication of Cross-Task Transfer:** Replicate the single-dataset ablation experiments with a different base model to confirm that the observed cross-task spill-over is not model-specific.

3. **Benchmark Debiasing and Robustness Stress Tests:** Apply the latest debiasing techniques to the spatial benchmarks and re-evaluate the trained models to ensure that performance gains are not due to exploitable biases.