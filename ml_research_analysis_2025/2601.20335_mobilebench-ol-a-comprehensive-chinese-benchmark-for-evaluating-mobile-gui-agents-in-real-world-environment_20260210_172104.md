---
ver: rpa2
title: 'MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI
  Agents in Real-World Environment'
arxiv_id: '2601.20335'
source_url: https://arxiv.org/abs/2601.20335
tags:
- task
- tasks
- action
- page
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MobileBench-OL, an online benchmark for
  evaluating mobile GUI agents in real-world environments. The benchmark includes
  1080 tasks from 80 Chinese apps and evaluates agents across five subsets: Base,
  Long-Tail, Long-Horizon, GUI-Reasoning, and Noise-Robust.'
---

# MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment

## Quick Facts
- **arXiv ID:** 2601.20335
- **Source URL:** https://arxiv.org/abs/2601.20335
- **Reference count:** 40
- **Primary result:** MobileBench-OL evaluates GUI agents across 1080 tasks from 80 Chinese apps, revealing significant performance gaps in real-world environments with best models achieving only 60.97% success on basic tasks and 15.00% on long-horizon tasks.

## Executive Summary
MobileBench-OL introduces a comprehensive benchmark for evaluating mobile GUI agents in real-world Chinese app environments. The benchmark consists of 1080 tasks across 80 popular Chinese apps, organized into five difficulty subsets that test various aspects of agent performance including basic task execution, handling long-tail scenarios, complex reasoning, long-horizon planning, and robustness to environmental noise. The benchmark employs an auto-evaluation framework with a reset mechanism to ensure stable and repeatable real-world testing, addressing the challenge of maintaining consistent testing conditions for GUI agents.

The evaluation of 12 leading GUI agents reveals substantial room for improvement in real-world GUI agent performance. The best-performing model, UI-TARS-1.5-7B, achieved a 60.97% success rate on the Base subset but only 15.00% on the Long-Horizon subset, demonstrating the significant challenges agents face with complex, multi-step tasks. Human evaluation confirms the benchmark's reliability in measuring agent performance, validating its effectiveness as a tool for advancing GUI agent development.

## Method Summary
The MobileBench-OL benchmark evaluates mobile GUI agents through 1080 tasks across 80 Chinese apps, organized into five difficulty subsets: Base (basic task execution), Long-Tail (rare scenarios), Long-Horizon (complex multi-step tasks), GUI-Reasoning (complex reasoning), and Noise-Robust (environmental variations). The benchmark employs an auto-evaluation framework with a reset mechanism to ensure stable and repeatable real-world testing conditions. This framework addresses the challenge of maintaining consistent testing environments for GUI agents, which is critical for reliable benchmarking. The evaluation was conducted primarily on emulators, though the benchmark is designed to assess real-world performance across various conditions.

## Key Results
- Best model (UI-TARS-1.5-7B) achieved 60.97% success rate on Base subset tasks
- Long-Horizon subset showed significant difficulty with only 15.00% success rate
- Human evaluation confirmed benchmark reliability in measuring agent performance
- All 12 evaluated GUI agents showed substantial room for improvement in real-world environments

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of real-world app scenarios and the systematic difficulty progression across five subsets. The auto-evaluation framework with reset mechanism ensures stable and repeatable testing conditions, which is critical for reliable benchmarking of GUI agents. The focus on Chinese apps and language interfaces allows for targeted evaluation of agents in specific cultural and linguistic contexts, though this also limits generalizability.

## Foundational Learning
- **GUI agent evaluation**: Understanding how to systematically assess GUI agent performance is crucial for advancing the field. Quick check: Can the benchmark differentiate between basic and complex task performance?
- **Real-world testing environments**: Creating stable and repeatable testing conditions is essential for reliable benchmarking. Quick check: Does the reset mechanism effectively isolate test cases?
- **Multilingual GUI evaluation**: Evaluating agents in specific language contexts reveals cultural and linguistic challenges. Quick check: How well do results generalize to non-Chinese environments?
- **Task difficulty scaling**: Organizing tasks by complexity helps identify specific areas where agents struggle. Quick check: Does the five-subset structure effectively capture performance differences?

## Architecture Onboarding

**Component map:**
MobileBench-OL -> 80 Chinese apps -> 1080 tasks -> 5 subsets (Base, Long-Tail, Long-Horizon, GUI-Reasoning, Noise-Robust) -> Auto-evaluation framework with reset mechanism

**Critical path:**
Task definition → App selection → Subset categorization → Auto-evaluation setup → Agent execution → Performance measurement → Human validation

**Design tradeoffs:**
- Language specificity (Chinese apps) vs. global applicability
- Emulator testing vs. physical device variability
- Automated evaluation vs. human validation requirements
- Task complexity vs. practical evaluation feasibility

**Failure signatures:**
- Low success rates on Long-Horizon tasks indicating planning limitations
- Performance drops in Noise-Robust subset suggesting environmental sensitivity
- Language-specific failures in Long-Tail subset revealing cultural adaptation issues

**3 first experiments:**
1. Run Base subset evaluation to establish baseline performance metrics
2. Test Long-Horizon subset to identify planning and reasoning limitations
3. Evaluate Noise-Robust subset to assess environmental adaptation capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on Chinese apps and language interfaces limits generalizability to global app ecosystems
- Primary evaluation on emulators may not fully capture real-world hardware variability and network conditions
- Uncertainty about the extent and statistical significance of human evaluation coverage

## Confidence
- **High:** Basic task execution and Base subset evaluations
- **Medium:** Long-Tail and GUI-Reasoning subsets due to potential language and cultural specificity
- **Low:** Long-Horizon tasks due to combination of task complexity and evaluation methodology uncertainty

## Next Checks
1. Validate benchmark results on physical devices across different hardware specifications and network conditions
2. Conduct extensive cross-cultural validation by testing the benchmark with non-Chinese speaking evaluators
3. Perform ablation studies to quantify the impact of the reset mechanism and auto-evaluation framework on measured agent performance