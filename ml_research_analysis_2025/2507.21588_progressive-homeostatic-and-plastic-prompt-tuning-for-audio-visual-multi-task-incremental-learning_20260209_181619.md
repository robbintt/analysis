---
ver: rpa2
title: Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task
  Incremental Learning
arxiv_id: '2507.21588'
source_url: https://arxiv.org/abs/2507.21588
tags:
- learning
- tasks
- task
- audio-visual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a progressive three-stage prompting framework
  (PHP) for audio-visual multi-task incremental learning that addresses catastrophic
  forgetting while maintaining cross-task knowledge transfer. The method introduces
  task-shared modality aggregating adapters in shallow layers for universal audio-visual
  representations, task-specific modality-shared dynamic generating adapters in middle
  layers for task-aware cross-modal integration, and task-specific modality-independent
  prompts in deep layers for preserving task and modality specificity.
---

# Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning

## Quick Facts
- arXiv ID: 2507.21588
- Source URL: https://arxiv.org/abs/2507.21588
- Reference count: 40
- Primary result: State-of-the-art audio-visual multi-task incremental learning with 62.36% single-task accuracy and 63.47% multi-task accuracy while maintaining only 3.32% forgetting rate

## Executive Summary
This paper addresses the challenging problem of audio-visual multi-task incremental learning, where a model must learn multiple audio-visual tasks sequentially without catastrophic forgetting. The proposed Progressive Homeostatic and Plastic Prompt Tuning (PHP) framework introduces a three-stage prompting approach that balances plasticity (learning new tasks) with homeostasis (preserving old knowledge). By strategically ordering task-shared and task-specific adapters across different depth levels of frozen CLIP and CLAP backbones, the method achieves positive knowledge transfer (+7.79%) while maintaining remarkably low forgetting rates (3.32%).

## Method Summary
PHP is a three-stage framework for audio-visual multi-task incremental learning that operates on frozen CLIP and CLAP backbones. The method progressively introduces: (1) a task-shared Modality Aggregating adapter in shallow layers for universal audio-visual representation learning, (2) a task-specific Modality-shared Dynamic Generating adapter in middle layers for task-aware cross-modal integration, and (3) task-specific Modality-Independent prompts in deep layers for preserving task and modality specificity. The framework is trained sequentially on tasks, with each new task updating only the relevant prompt and adapter parameters while keeping the backbone frozen.

## Key Results
- Achieves 62.36% single-task accuracy and 63.47% multi-task accuracy across four audio-visual tasks
- Maintains remarkably low forgetting rate of 3.32% compared to 34.84% for fine-tuning baseline
- Demonstrates positive knowledge transfer (+7.79%) versus negative transfer in comparison methods
- Shows superior performance to all baseline methods across all evaluation metrics (A_mean, A_final, F_mean, Diff)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Progressive Prompting
Routing task and modality information through a shallow-middle-deep progression facilitates better knowledge transfer and reduced forgetting compared to uniform or reversed orderings. The model separates shared cross-modal learning (shallow) from task-specific cross-modal adaptation (middle) and finally modality-specific preservation (deep), mirroring how representations in large pre-trained models evolve from abstract to specific.

### Mechanism 2: Task-Shared Modality Aggregating Adapter (TMA)
Early cross-modal integration via a shared adapter improves knowledge transfer between tasks by establishing a common audio-visual feature space. The TMA adapter uses channel, spatial, and temporal attention mechanisms to fuse audio and video features at shallow layers, creating a universal representation that subsequent stages can build upon.

### Mechanism 3: Task-Specific Modality-Independent Prompts (TMI)
Isolating task and modality-specific information in deep layers via independent prompts is the primary mechanism for preventing catastrophic forgetting. The TMI prompts are small, learnable parameters appended to deep backbone layers, with each task learning its own set of prompts for each modality, stored and selected based on task identity at inference.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: This is the central problem the paper addresses - the tendency of neural networks to lose previously learned knowledge when trained on new tasks.
  - Quick check question: If you fine-tune a model on Task A, then Task B, and finally test it on Task A, would you expect its performance to remain stable? If no, this is the problem.

- **Concept: Prompt Tuning**
  - Why needed here: This is the core technical tool - adding small learnable "prompt" vectors to adapt a frozen pre-trained model to a new task without modifying the original weights.
  - Quick check question: Instead of updating all the weights of a huge pre-trained ViT, what if you could only train a few extra input vectors to make it recognize a new category? That is prompt tuning.

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / Adapter**
  - Why needed here: The method uses "adapters" which are small, trainable modules inserted into the model - a form of PEFT crucial for understanding how the method updates the model without retraining the entire backbone.
  - Quick check question: How can you modify a frozen model's behavior efficiently? By inserting small, trainable bottleneck layers (adapters) into its architecture.

## Architecture Onboarding

- **Component map:** Frozen CLIP ViT (video) / CLAP HTS-AT (audio) -> Shallow TMA Adapter -> Middle TMDG Adapter -> Deep TMI Prompts -> Output Head

- **Critical path:**
  1. Input: Raw Audio-Video pair for task `t`
  2. Encoding: Pass through frozen CLIP/CLAP layers up to injection point 1 (Shallow)
  3. Stage 1 (TMA): Fuse audio/video features using the shared TMA adapter
  4. Mid-Processing: Pass through more frozen layers to injection point 2 (Middle)
  5. Stage 2 (TMDG): Use task `t`'s TMDG adapter to generate and inject instance-specific prompts
  6. Deep-Processing: Pass through more frozen layers to injection point 3 (Deep)
  7. Stage 3 (TMI): Concatenate task `t`'s stored TMI prompts for audio and video
  8. Output: Final prediction head for task `t`

- **Design tradeoffs:**
  - The TMA adapter is shared across all tasks, promoting transfer but risking interference; TMDG and TMI components are task-specific, isolating knowledge but increasing parameter count
  - S-M-D ordering is crucial; reversing it drastically reduces performance (47.29% vs 58.85% accuracy)
  - Parameter cost scales linearly with task count due to TMI prompts; storage becomes a bottleneck with many tasks

- **Failure signatures:**
  1. Negative transfer on new task: Check if TMA adapter is being updated too aggressively, causing interference
  2. High forgetting on old tasks: Verify that task identity is correctly used to select prompts at inference
  3. Poor cross-modal performance: The TMA adapter's fusion mechanism may need tuning or the fusion weights are not optimal

- **First 3 experiments:**
  1. Ablation on component necessity: Train with full model, then only TMA, only TMDG, and only TMI prompts to isolate contributions
  2. Ordering sensitivity test: Swap injection order of components (D-M-S, M-D-S, S-D-M) to confirm S-M-D design choice
  3. Incremental sequence stress test: Train on 3-4 tasks and evaluate performance on all previous tasks to verify low forgetting and check for positive transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PHP framework perform when scaled to task streams significantly longer than the four-task sequences evaluated?
- Basis in paper: Experiments are constrained to three-task and four-task sequences, leaving long-term task accumulation unexplored
- Why unresolved: Unclear if progressive accumulation of task-specific prompts leads to capacity saturation or increased interference in lifelong scenarios
- What evidence would resolve it: Evaluation on a benchmark with 10+ audio-visual tasks to analyze asymptotic behavior of forgetting rates and parameter storage

### Open Question 2
- Question: Is the hierarchical S-M-D architecture dependent on the specific semantic layering of CLIP and CLAP backbones?
- Basis in paper: Method description explicitly ties design to frozen CLIP and CLAP models, and ablation study shows performance drops when order is changed
- Why unresolved: Paper does not demonstrate if "Shallow=Universal, Deep=Specific" mapping holds for other architectures like CNNs or non-pretrained transformers
- What evidence would resolve it: Experiments applying framework to alternative audio-visual backbones without architecture-specific hyperparameter tuning

### Open Question 3
- Question: Can the trade-off between prompt length and knowledge transfer be resolved through dynamic prompt allocation?
- Basis in paper: Figure 3(a) explicitly shows trade-off where increasing prompt length improves resistance to forgetting but reduces knowledge transfer efficiency
- Why unresolved: Current implementation uses fixed prompt lengths and layer depths, requiring manual tuning to balance stability and plasticity
- What evidence would resolve it: Mechanism that adapts prompt capacity per task based on its complexity or similarity to previous tasks' feature space

## Limitations

- Scalability concerns with task-specific prompts as parameter count grows linearly with number of tasks
- Limited experimental breadth with only 4 tasks and one random sequence per comparison
- Missing architectural hyperparameters (prompt pool sizes, injection layer indices, attention mechanism dimensions) that limit reproducibility

## Confidence

- **High confidence**: The mechanism of progressive depth-based prompting (S-M-D ordering) and its effectiveness in reducing forgetting while enabling transfer
- **Medium confidence**: The specific implementation details of the TMDG adapter's prompt pool and selection mechanism
- **Medium confidence**: The generalization of results to larger task sets or different task sequences

## Next Checks

1. Implement the full model with different task sequence orders to test robustness of the S-M-D ordering advantage
2. Measure parameter efficiency scaling by testing the model on 8-10 tasks instead of 4, tracking both performance and total parameter count
3. Perform an ablation where the TMA adapter is moved to middle or deep layers to empirically verify that shallow placement is optimal for knowledge transfer