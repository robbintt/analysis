---
ver: rpa2
title: '"There Is No Such Thing as a Dumb Question," But There Are Good Ones'
arxiv_id: '2505.09923'
source_url: https://arxiv.org/abs/2505.09923
tags:
- questions
- evaluation
- question
- context
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of systematically evaluating
  question quality in human-AI interactions. The researchers propose a two-dimensional
  framework assessing appropriateness (sociolinguistic competence) and effectiveness
  (strategic competence).
---

# "There Is No Such Thing as a Dumb Question," But There Are Good Ones

## Quick Facts
- arXiv ID: 2505.09923
- Source URL: https://arxiv.org/abs/2505.09923
- Authors: Minjung Shin; Donghyun Kim; Jeh-Kwang Ryu
- Reference count: 15
- Primary result: A two-dimensional rubric-based framework evaluates question quality in human-AI interactions through appropriateness and effectiveness dimensions.

## Executive Summary
This paper addresses the challenge of systematically evaluating question quality in human-AI interactions. The researchers propose a two-dimensional framework assessing appropriateness (sociolinguistic competence) and effectiveness (strategic competence). A rubric-based scoring system was developed with six sub-components across these dimensions, incorporating dynamic contextual variables. The evaluation was validated using the CAUS and SQUARE datasets, demonstrating the framework's ability to discriminate between well-formed and problematic questions while adapting to different contexts. Results showed consistent scoring patterns across question types, with the metric effectively identifying context-sensitive variations in question quality.

## Method Summary
The framework evaluates questions using a two-dimensional rubric-based approach implemented through LLM-as-a-judge methodology. The method employs six sub-components (cohesion, answerability, respectfulness, clarity, coherence, informativeness) scored on 5-point scales. Contextual variables (${answerer}, ${goal}) are parameterized to enable semi-adaptive evaluation. The system uses Claude-3-5-Sonnet with temperature=0, max_tokens=1500, applying the rubric to questions from CAUS (150 questions across sequential positions) and SQUARE (150 questions across contentious/ethical/predictive categories) datasets.

## Key Results
- The framework successfully discriminates between appropriate and inappropriate questions across different contexts
- Questions showed context-sensitive scoring variations when contextual variables were adjusted
- The rubric-based approach produced consistent scoring patterns across both CAUS and SQUARE datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing question quality into two orthogonal dimensions enables structured evaluation of pragmatically complex questioning behavior.
- Mechanism: The framework separates appropriateness (context-fit, sociolinguistic) from effectiveness (goal-achievement, strategic). This allows a question to score high on one dimension while low on the other—capturing cases where a well-formed question fails its purpose, or a goal-aligned question violates conversational norms.
- Core assumption: Appropriateness and effectiveness are partially independent; they are not subsumed under a single quality metric.
- Evidence anchors:
  - [abstract] "We propose two key evaluation dimensions: appropriateness (sociolinguistic competence in context) and effectiveness (strategic competence in goal achievement)."
  - [Page 2, Table 1] Explicitly contrasts the two: Appropriateness asks "Is the question aligned with the flow and context?" while Effectiveness asks "Does the question achieve its intended purpose?"
  - [corpus] Weak direct corpus support; related work (Task Matters, Doctor Agents) examines questioning behavior but does not validate this specific dimensional split.
- Break condition: If future work shows strong correlation between appropriateness and effectiveness scores across diverse contexts, the dimensional separation adds little discriminative value.

### Mechanism 2
- Claim: Parameterizing context variables (${answerer}, ${goal}) enables semi-adaptive evaluation without rebuilding rubrics per scenario.
- Mechanism: The rubric embeds placeholders that shift score expectations based on who is answering and what the discourse goal is. The same question ("Are you satisfied with your new home?") scores low on informativeness when the goal is information acquisition, but higher when the goal is icebreaking for social interaction.
- Core assumption: Contextual variation in question quality can be captured by a small number of explicit variables; implicit context is secondary or can be folded into these.
- Evidence anchors:
  - [Page 4, Figure 2] Demonstrates FQ#2 receiving favorable scores in social interaction context but unfavorable scores in information acquisition context.
  - [Page 3] "Configuring contextual variables allows semi-adaptive evaluations to be applied to different criteria depending on the situation."
  - [corpus] No direct corpus validation of this specific variable-parameterization approach.
- Break condition: If runtime context requires more than two variables (e.g., power dynamics, cultural norms, temporal urgency), the current parameterization underfits real-world complexity.

### Mechanism 3
- Claim: Rubric-based LLM evaluation transforms unstructured question assessment into quantitative, explainable scores.
- Mechanism: Detailed 5-point rubric descriptions are injected into LLM prompts. The LLM applies pattern recognition to map question-context pairs onto score levels. This decomposes evaluation into six explicit criteria rather than a holistic judgment.
- Core assumption: LLMs can reliably apply human-defined rubrics with consistency comparable to or exceeding human annotators.
- Evidence anchors:
  - [Page 3] "Applying rubrics to LLMs, as a variant of the LLM-as-a-judge paradigm... enables objective assessment based on human-defined standards."
  - [Page 5] Claude 3.5 Sonnet selected after comparative testing with GPT-4, GPT-4-turbo, and Claude Opus, showing highest agreement with human evaluators.
  - [corpus] Related work (Socratic Questioning, FLASK) uses LLM-as-judge patterns but does not directly validate rubric-based question evaluation.
- Break condition: If LLM rubric application exhibits systematic bias (e.g., inflated clarity scores for grammatically fluent but semantically vacuous questions), the mechanism produces misleading evaluations.

## Foundational Learning

- Concept: **Communicative Competence (Formal vs. Functional)**
  - Why needed here: The paper explicitly grounds its evaluation scope in functional linguistic competence—pragmatics and social linguistics—rather than formal grammar. Understanding this distinction explains why the rubric ignores grammatical correctness.
  - Quick check question: Can you explain why a grammatically perfect question might score low on effectiveness?

- Concept: **Speech Act Theory**
  - Why needed here: Questions are framed as speech acts aimed at obtaining information, not propositions with truth values. This explains why "right/wrong" evaluation is rejected in favor of "appropriate/effective."
  - Quick check question: What makes evaluating a question fundamentally different from evaluating a factual statement?

- Concept: **Rubric-Based Assessment Design**
  - Why needed here: The scoring system relies on 5-point progressive descriptors (deficiency → full achievement). Understanding rubric construction is necessary to modify or extend criteria.
  - Quick check question: If you needed to add a seventh sub-component, what structural elements would it require?

## Architecture Onboarding

- Component map: Rubric definitions (6 sub-components) → Context variable config (${answerer}, ${goal}) → Evaluation prompt template → LLM judge (Claude 3.5 Sonnet) → Dataset processors (CAUS, SQUARE) → Aggregation/visualization (pandas, matplotlib)
- Critical path: Context + FQ → Prompt construction (with variable substitution) → LLM scoring → Parse 6 scores → Aggregate by dataset/category → Visualize
- Design tradeoffs:
  - **Structure vs. flexibility**: Fixed 6-component rubric ensures consistency but may miss domain-specific criteria (e.g., pedagogical scaffolding in education)
  - **Automation vs. interpretability**: LLM-as-judge enables scale but introduces opacity; rubric descriptors partially mitigate this
  - **Context parameterization**: Two variables cover common cases but cannot encode all pragmatic nuance
- Failure signatures:
  - **Flat scoring**: All questions receiving 4-5 on clarity suggests rubric is not discriminating
  - **Context insensitivity**: Same scores across different ${goal} values indicates variable substitution failure
  - **Category collapse**: Ethical questions showing low cohesion + low respectfulness but high coherence suggests internal inconsistency in rubric application
- First 3 experiments:
  1. **Boundary validation**: Replicate Figure 2 with new synthetic invalid questions to confirm the rubric discriminates known failure modes
  2. **Inter-annotator comparison**: Run human evaluators on a sample; compute Cohen's kappa between human and LLM scores per sub-component
  3. **Context variable stress test**: Systematically vary ${answerer} and ${goal} on the same question set; verify score shifts align with theoretical expectations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be expanded and adapted for educational contexts where questioning serves pedagogical purposes beyond information acquisition?
- Basis in paper: [explicit] The discussion states: "Future research should focus on expanding this framework to tackle emerging challenges in educational and ethical contexts, where appropriate and effective questioning enhances productive and safe interactions for meaningful communication."
- Why unresolved: The current framework was validated primarily on information-seeking (CAUS) and safety-focused (SQUARE) datasets, not educational dialogue where questions must facilitate learning and reflection.
- What evidence would resolve it: Validation studies applying the framework to educational dialogue datasets, with potential rubric modifications to capture pedagogical effectiveness.

### Open Question 2
- Question: What is the correlation between the rubric-based evaluation scores and actual human-perceived question quality in ecologically valid, multi-turn conversations?
- Basis in paper: [inferred] The validity test used only three artificially constructed questions for boundary case detection, and the framework was evaluated on isolated context-question pairs rather than extended dialogues.
- Why unresolved: The paper demonstrates discriminative validity between extreme cases but does not establish whether the scoring system aligns with human judgments in naturalistic, extended interactions.
- What evidence would resolve it: Correlation studies comparing rubric scores with human ratings across larger, more diverse multi-turn conversation datasets.

### Open Question 3
- Question: How robust is the evaluation framework across different LLM judge architectures, and to what extent do evaluation outcomes depend on the specific model used?
- Basis in paper: [inferred] The evaluation relied on Claude-3-5-Sonnet with only comparative testing against GPT-4 variants mentioned without detailed results, raising questions about cross-model consistency.
- Why unresolved: LLM-as-a-judge approaches may exhibit model-specific biases, and the generalizability of the rubric-based approach across different evaluator architectures remains untested.
- What evidence would resolve it: Systematic comparison of evaluation outcomes across multiple LLM architectures using the same rubric and test questions.

## Limitations

- The framework may not capture domain-specific questioning requirements (e.g., medical history-taking, educational scaffolding) without significant adaptation
- LLM-as-judge approach introduces potential for systematic bias in rubric application, particularly for nuanced sociolinguistic judgments
- Reliance on just two context variables (${answerer}, ${goal}) may oversimplify complex real-world interaction dynamics

## Confidence

- **High confidence**: The dimensional framework's conceptual validity (appropriateness vs. effectiveness as orthogonal axes) is well-supported by linguistic theory and demonstrated through consistent scoring patterns across datasets
- **Medium confidence**: The LLM rubric application shows promise but requires more rigorous inter-annotator validation to establish reliability comparable to human evaluators
- **Medium confidence**: The context parameterization mechanism works in principle but may not generalize to scenarios requiring more complex contextual modeling

## Next Checks

1. **Inter-annotator reliability test**: Conduct a human evaluation study where 3-5 independent raters score 50-100 questions from both datasets using the same rubric. Calculate Cohen's kappa between human and LLM scores per sub-component to quantify agreement levels and identify systematic biases.

2. **Cross-domain generalization**: Apply the framework to a third dataset from a different domain (e.g., medical interview questions or educational questioning sequences). Compare scoring patterns and rubric effectiveness across domains to identify which sub-components are domain-specific versus universally applicable.

3. **Context variable expansion**: Systematically test the framework with additional context variables (e.g., power dynamics, urgency level, cultural norms) on a subset of questions. Analyze whether the two-variable parameterization captures sufficient variation or if additional dimensions improve discriminative power.