---
ver: rpa2
title: Pretraining Finnish ModernBERTs
arxiv_id: '2511.09213'
source_url: https://arxiv.org/abs/2511.09213
tags:
- data
- language
- context
- table
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents ModernBERT encoder models trained on 362 to\
  \ 448 billion tokens in languages relevant to Finland, including Finnish, Swedish,\
  \ Northern S\xE1mi, English, and Latin. The models range from 51M to 475M parameters\
  \ and demonstrate competitive performance with existing multilingual models, particularly\
  \ excelling in long-context retrieval tasks."
---

# Pretraining Finnish ModernBERTs

## Quick Facts
- **arXiv ID:** 2511.09213
- **Source URL:** https://arxiv.org/abs/2511.09213
- **Reference count:** 39
- **Primary result:** Finnish ModernBERTs achieve competitive performance with existing multilingual models, excelling in long-context retrieval tasks and outperforming other multilingual models on the MLDR benchmark by more than 14 points.

## Executive Summary
This paper presents ModernBERT encoder models trained on 362 to 448 billion tokens in languages relevant to Finland, including Finnish, Swedish, Northern Sámi, English, and Latin. The models range from 51M to 475M parameters and demonstrate competitive performance with existing multilingual models, particularly excelling in long-context retrieval tasks. They outperform other multilingual models in out-of-domain retrieval by more than 14 points on the MLDR benchmark, while maintaining strong performance in standard NLU tasks. The study also explores the impact of using different data types during the final training stages, showing that models trained with educational data perform slightly worse on retrieval tasks despite more balanced language distribution.

## Method Summary
The authors pretrain ModernBERT encoder models using a three-phase training schedule: stable phase (1024 tokens) optimizes token-level representations, context extension incrementally increases sequence length to 16K tokens in six steps, and annealing reinforces high-quality domain data during learning rate decay. Models use rotary position embeddings with base=1,000,000 to enable long-context extrapolation, and are trained with 30% masking rate MLM objective. Six model variants are trained with BPE tokenizers (vocabulary sizes 27K-128K) on 362-448B tokens across multiple languages, using AdamW optimizer with trapezoidal WSD scheduler on 8 nodes (64 GPUs) AMD MI250x on LUMI.

## Key Results
- Finnish ModernBERTs outperform other multilingual models in out-of-domain retrieval by more than 14 points on the MLDR benchmark
- Models achieve strong performance in standard NLU tasks while excelling at long-context retrieval (up to 16K tokens)
- The three-phase training schedule enables long-context retrieval without catastrophic forgetting of short-context representations
- Increasing RoPE rotary base to 1M improves long-context extrapolation but may degrade performance on token-level prediction tasks like question answering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A three-phase training schedule enables long-context retrieval without catastrophic forgetting of short-context representations.
- **Mechanism:** The stable phase (1024 tokens) first optimizes token-level representations. Context extension then incrementally increases sequence length in six steps up to 16K tokens, allowing attention patterns to adapt. Annealing reinforces high-quality domain data. This sequential specialization prevents the gradient conflict that would arise from jointly training on all lengths.
- **Core assumption:** Representations learned at short contexts transfer to longer contexts when extended gradually.
- **Evidence anchors:**
  - [section 2.5]: "We describe the training as a three-step process where we assume that models' parameters are first optimized for short context representations of the tokens (stable phase), then refine the token representations for longer dependencies (context extension phase)..."
  - [section 3.2]: "the context extension was successful, as the models can retrieve beyond the stable phase sequence length of 1024"
  - [corpus]: llm-jp-modernbert (arXiv:2504.15544) applies similar three-phase training for Japanese long-context, supporting transferability across languages.
- **Break condition:** If your target domain has fundamentally different token statistics (e.g., primarily code with different granularity), the stable-phase representations may not transfer.

### Mechanism 2
- **Claim:** Increasing RoPE rotary base from 160K to 1M improves long-context extrapolation at the cost of fine-grained local position discrimination.
- **Mechanism:** RoPE encodes position via rotation angles θ_i = base^(-2i/d). A larger base makes the angle change per position increment smaller, extending the period before positional aliasing. However, this flattens the angle differences between nearby positions, reducing sensitivity to local token ordering.
- **Core assumption:** The target task relies more on global document-level coherence than precise local token ordering.
- **Evidence anchors:**
  - [section 2.4]: "We chose this rotary base because it can, in theory, extrapolate to a longer context than the 160K rotary base"
  - [section 3.1]: "Our models seemed to perform disadvantageously in tasks where token-level predictions are important, such as question answering. This may be due to the large rotary base, which levels out the angle functions, causing less differentiation between close positions."
  - [corpus]: BioClinical ModernBERT (arXiv:2506.10896) and TabiBERT (arXiv:2512.23065) adopt ModernBERT's RoPE configuration, but neither ablates rotary base specifically—corpus evidence for the tradeoff is weak.
- **Break condition:** If your task requires precise span extraction (e.g., extractive QA with exact boundaries), the local position degradation may outweigh long-context benefits.

### Mechanism 3
- **Claim:** Annealing on education-classified data (Finnish/Swedish-focused, balanced) does *not* improve retrieval over English-dominant baseline annealing, likely because retrieval benchmarks are English-aligned.
- **Mechanism:** Annealing exposes the model to high-quality domain data during learning rate decay. The "edu" data (54.9% Finnish, 16% Swedish) provides better multilingual coverage, but retrieval benchmarks like MLDR may inherently favor representations learned from English-heavy corpora due to evaluation design or query/document distribution.
- **Core assumption:** Retrieval benchmark performance reflects the language distribution of annealing data, not just its quality.
- **Evidence anchors:**
  - [section 3.2]: "The evaluation trend was that models trained on the edu data during the annealing phase performed a bit worse on both out-of-domain retrieval tasks. This may simply reflect the fact that the baseline annealing data contained more English overall."
  - [Table 8]: Finnish-ModernBERT-large-short (baseline annealing) achieves 0.311 nDCG@10 on MLDR vs. 0.302 for large-short-edu.
  - [corpus]: No corpus papers directly address language-specific annealing effects on retrieval; this mechanism lacks external validation.
- **Break condition:** If you are evaluating on Finnish/Swedish-native retrieval benchmarks rather than MLDR (which may have English-query bias), this pattern may reverse.

## Foundational Learning

- **Concept: Encoder-only vs. Decoder-only Transformers**
  - **Why needed here:** Finnish ModernBERTs are encoder-only models optimized for discriminative tasks (classification, retrieval, ranking). Understanding why encoders differ from decoders clarifies when to use each.
  - **Quick check question:** If your task requires generating fluent Finnish text, should you use Finnish ModernBERT? (Answer: No—use a decoder or encoder-decoder.)

- **Concept: Rotary Position Embeddings (RoPE)**
  - **Why needed here:** RoPE is the positional encoding mechanism. The paper modifies its hyperparameter (rotary base), directly affecting context length capacity.
  - **Quick check question:** Why does increasing the rotary base from 160K to 1M theoretically extend maximum context length?

- **Concept: Masked Language Modeling (MLM) with 30% masking rate**
  - **Why needed here:** This is the pretraining objective. The 30% rate (higher than BERT's 15%) follows ModernBERT's design for denser supervision.
  - **Quick check question:** How does masking 30% of tokens differ from next-sentence prediction, and why did the authors exclude NSP?

## Architecture Onboarding

- **Component map:** BPE tokenizer (27K-128K vocab) -> ModernBERT encoder (6-28 layers, 768-1024 hidden dim, 12-16 attention heads) -> RoPE with rotary_base=1,000,000 -> MLM objective with 30% masking

- **Critical path:**
  1. **Tokenizer training:** Train BPE on 48GB corpus sample → vocabulary divisible by 64
  2. **Stable phase:** 1024-token context, ~370B tokens, LR warmup → constant
  3. **Context extension:** 6-step ramp to 16K tokens, dedicated long-document data
  4. **Annealing:** LR decay on high-quality data (baseline or edu mix)
  5. **Fine-tuning:** Task-specific (e.g., MS-MARCO for retrieval)

- **Design tradeoffs:**
  - **RoPE base (1M vs. 160K):** Longer context vs. weaker local position discrimination (hurts QA).
  - **Vocabulary size (27K-128K):** Smaller vocabularies reduce parameters but increase fertility (tokens per word). The paper finds 27K-56K optimal given compute budget.
  - **Annealing data:** Baseline (English-heavy) improves retrieval; edu (Finnish-balanced) does not clearly improve NLU in Finnish/Swedish.

- **Failure signatures:**
  - **Poor QA performance:** High rotary base flattens local position differences → consider reducing base or adding local attention.
  - **Retrieval degradation with edu annealing:** Likely English-alignment in benchmarks → evaluate on native-language retrieval sets.
  - **Inefficient Finnish tokenization:** If using generic multilingual tokenizer → retrain BPE on Finnish-dominant corpus.

- **First 3 experiments:**
  1. **Establish baseline on your task:** Load Finnish-ModernBERT-large-short, fine-tune on your target dataset, compare against XLM-R-large and monolingual Finnish BERT.
  2. **Ablate context length:** Evaluate performance with 512 vs. 4096 vs. 16384 token inputs to confirm long-context benefit for your data.
  3. **Test tokenizer efficiency:** Measure fertility (tokens/word) on your corpus; if >1.5 for Finnish, consider training a domain-specific BPE tokenizer from the released checkpoints.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the inclusion of high-quality non-English educational data during the annealing phase reliably improve model performance on Finnish and Swedish benchmarks?
- **Basis in paper:** [explicit] The authors state that the usefulness of this specific data mix "remains unconfirmed" because performance fluctuated across evaluation datasets compared to the baseline.
- **Why unresolved:** The "edu" models showed inconsistent results (e.g., slight drops in retrieval vs. gains in some NLU tasks), preventing a definitive conclusion on whether the balanced language distribution or the educational quality score drove the changes.
- **What evidence would resolve it:** A controlled ablation study comparing models annealed on educational data against baseline models while strictly controlling for language distribution differences.

### Open Question 2
- **Question:** To what extent does the choice of a large rotary base (1,000,000) negatively impact performance on token-level prediction tasks like question answering?
- **Basis in paper:** [explicit] The paper hypothesizes that the models' disadvantage in QA tasks "may be due to the large rotary base," which might reduce differentiation between close positions, but notes they lacked resources for ablation trials.
- **Why unresolved:** The authors modified the rotary base to theoretically allow longer context extrapolation, but did not compare this setting against the standard ModernBERT base (160K) to isolate its effect on short-context QA accuracy.
- **What evidence would resolve it:** A comparison of identical model architectures trained with different rotary base values (160K vs. 1M) evaluated specifically on token-level QA benchmarks.

### Open Question 3
- **Question:** How do Finnish ModernBERTs perform on multilingual long-context Natural Language Understanding (NLU) tasks beyond document retrieval?
- **Basis in paper:** [explicit] The authors acknowledge that while the models excel at long-context retrieval (MLDR), their "performance in multilingual long context NLU tasks is unknown."
- **Why unresolved:** The evaluation suite used (EuroEval) focused on short-context tasks, while the long-context evaluation was restricted to the MLDR retrieval benchmark.
- **What evidence would resolve it:** Evaluation results from long-context NLU benchmarks (e.g., long-document classification or summarization in Finnish and Swedish) for the 16K context models.

## Limitations

- **Limited evaluation scope:** The models are primarily evaluated on English-centric tasks, with Finnish/Swedish NLU performance lacking extensive task coverage
- **RoPE rotary base tradeoff uncertainty:** Claims about the rotary base tradeoff lack strong external validation from corpus evidence
- **Ablation limitations:** The study does not systematically vary critical hyperparameters independently, making it difficult to isolate which factors drive observed performance differences

## Confidence

- **High confidence:** The three-phase training methodology and its implementation details are well-documented and reproducible; models' strong performance on long-context retrieval tasks is clearly demonstrated
- **Medium confidence:** Claims about the rotary base tradeoff are supported by internal observations but lack extensive external validation
- **Low confidence:** The assertion that edu annealing underperforms due to English-aligned retrieval benchmarks is speculative and lacks direct evidence

## Next Checks

1. **Evaluate on Finnish/Swedish retrieval benchmarks:** Test the edu-annealed models on native-language retrieval tasks to determine if the English-alignment hypothesis explains the MLDR performance gap.

2. **Ablate rotary base systematically:** Train models with rotary bases of 160K, 500K, and 1M while keeping all other factors constant to quantify the exact tradeoff between context length and local position discrimination.

3. **Test tokenization efficiency:** Measure token fertility (tokens/word) on Finnish and Swedish corpora using both the released tokenizers and generic multilingual tokenizers to quantify the efficiency gains from language-specific BPE training.