---
ver: rpa2
title: 'ACE: Exploring Activation Cosine Similarity and Variance for Accurate and
  Calibration-Efficient LLM Pruning'
arxiv_id: '2505.21987'
source_url: https://arxiv.org/abs/2505.21987
tags:
- pruning
- activation
- weight
- input
- cosp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of efficient and effective pruning
  of large language models (LLMs) to reduce memory and computational requirements.
  It proposes ACE, a pruning method that combines two innovations: an activation cosine
  similarity loss-guided pruning metric (CosP) and an activation variance-guided pruning
  metric (VarP).'
---

# ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning

## Quick Facts
- **arXiv ID**: 2505.21987
- **Source URL**: https://arxiv.org/abs/2505.21987
- **Reference count**: 40
- **Primary result**: Achieves up to 18% perplexity reduction and 63% pruning time decrease on LLaMA-30B with 2:4 semi-structured pruning

## Executive Summary
ACE addresses the challenge of efficient and effective pruning of large language models by introducing two innovations: an activation cosine similarity loss-guided pruning metric (CosP) and an activation variance-guided pruning metric (VarP). The method combines angular deviation minimization to preserve semantic integrity with variance-aware importance scoring to maintain semantic distinctions between tokens. Experiments demonstrate that ACE achieves significant improvements in both accuracy (measured by perplexity) and efficiency (measured by pruning time) compared to state-of-the-art baselines like Wanda and RIA, particularly when using shorter calibration sequences.

## Method Summary
ACE introduces two pruning metrics that operate on input activations collected during a calibration forward pass. CosP incorporates angular deviation of output activations to preserve semantic direction, while VarP uses input activation variance to maintain token-level semantic distinctions. The combined metric multiplies both importance scores element-wise. The method is theoretically shown to be calibration-efficient, requiring shorter input sequences than magnitude-only methods. ACE is evaluated on LLaMA, LLaMA-2, and OPT models using both unstructured and N:M semi-structured sparsity patterns.

## Key Results
- ACE achieves up to 18% reduction in perplexity compared to baseline methods
- Pruning time decreased by up to 63% compared to Wanda and RIA baselines
- On LLaMA-30B with 2:4 semi-structured pruning, ACE achieves 0.15-0.2 lower perplexity while using only 40%-50% of the pruning time
- VarP enables effective pruning with only 16-token calibration sequences versus 2048+ tokens for baselines

## Why This Works (Mechanism)

### Mechanism 1: Cosine Similarity Loss Preserves Semantic Direction
CosP approximates the cosine similarity loss when weight $W_{ij}$ is pruned: $L_{cos_{ij}} \approx \frac{a_i \cdot W_{ij} \cdot x_j}{\|a\|^2}$. The importance score $S_{cos_{ij}} = |W_{ij}| \cdot \|X_j\|_2 \cdot L_{cos_{ij}}$ combines weight magnitude, input activation norm, and angular perturbation. This penalizes weights whose removal would rotate output embeddings significantly, better preserving semantic similarity in vector space.

### Mechanism 2: Input Variance Prevents Semantic Collapse
VarP adds a variance term: $S_{var_{ij}} = |W_{ij}| \cdot E\left[\frac{1}{1 - \|X_j\|_2^2}\right]$. When two weights have equal $|W_{ij}| \cdot \|X_j\|_2$, the one with higher input variance receives lower importance because it produces more uniform outputs across tokens, diminishing semantic differentiation. This maintains token-level semantic distinctions during pruning.

### Mechanism 3: Calibration Efficiency via Sequence Length Sensitivity
Theoretical analysis shows $diff = \frac{1}{L+1} E[\|X_j\|_2^2]$, where $L$ is sequence length. As $L$ decreases, the advantage of VarP over Wanda's diagonal approximation increases monotonically. This enables shorter calibration sequences without proportional accuracy loss, making pruning more efficient while maintaining quality.

## Foundational Learning

- **Cosine Similarity in Embedding Space**
  - Why needed: Understanding why angular deviation matters for semantics; tokens close in angle are semantically similar
  - Quick check: Given two embedding vectors [1,0] and [0.9, 0.436], compute their cosine similarity. (Answer: ~0.9)

- **Hessian-Based Pruning Sensitivity**
  - Why needed: SparseGPT's OBS foundation; VarP derives from approximating $(X^TX + \lambda I)^{-1}$ differently than Wanda
  - Quick check: Why does the diagonal of the inverse Hessian matter for pruning importance? (Answer: It captures output sensitivity to weight perturbation)

- **N:M Semi-Structured Sparsity**
  - Why needed: 2:4 sparsity (2 weights kept per 4) is hardware-accelerated on NVIDIA Ampere+; experiments show ACE excels here
  - Quick check: For a 1D weight vector [0.5, 0.1, 0.3, 0.9] with 2:4 sparsity, which weights would be pruned by magnitude? (Answer: 0.1 and 0.3)

## Architecture Onboarding

- **Component map**:
Input Calibration Data (C4 samples) -> Forward Pass -> Collect Input Activations X per layer -> Per-Layer Importance Computation -> Mask Generation -> Apply Mask -> Evaluation

- **Critical path**: The VarP computation (expectation over calibration samples) must happen before mask generation. CosP can be computed independently but requires output activation statistics. Both need the forward pass first.

- **Design tradeoffs**:
  - Sequence length vs. accuracy: Shorter L reduces pruning time but typically hurts accuracy; VarP mitigates this tradeoff
  - CosP-only vs. VarP-only vs. Combined: CosP better for semantic preservation; VarP better for calibration efficiency; Combined yields best overall but adds computation
  - Unstructured vs. N:M: N:M sparsity provides hardware speedup but constrains mask flexibility; ACE shows larger gains on N:M

- **Failure signatures**:
  - Exploded importance scores: If $\|X_j\|_2^2 \approx 1$, VarP denominator approaches zero; add epsilon stabilization
  - Negative cosine loss: If output activation norm is near-zero, angular term becomes unstable; skip or clamp
  - No time savings: If still using full sequence length, VarP provides no efficiency gain; explicitly set L=16

- **First 3 experiments**:
  1. Sanity check: Run ACE on LLaMA-7B with 50% unstructured sparsity, L=2048. Verify PPL matches paper (~7.18). Compare against Wanda baseline.
  2. Calibration efficiency sweep: Fix LLaMA-7B, 50% sparsity. Test L ∈ {16, 32, 128, 512, 2048}. Plot PPL vs. pruning time. Expect ACE to degrade less at low L.
  3. N:M sparsity validation: Run 2:4 semi-structured pruning on LLaMA-30B with L=16. Compare ACE vs. Wanda vs. RIA. Verify ~40-50% time reduction and ~0.15-0.2 PPL improvement reported in paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the ACE pruning metrics be effectively adapted for or integrated with post-training quantization (PTQ) techniques?
- **Basis in paper**: Section 5 explicitly states, "Whether it can be extended to the quantization domain, or integrated with existing quantization techniques, remains an open question and a promising direction for future research."
- **Why unresolved**: The current method is designed specifically for pruning (setting weights to zero). Quantization involves mapping weights to discrete levels rather than removing them, which changes the impact on angular deviation and variance in ways not explored in this work.

### Open Question 2
- **Question**: Does the ACE method generalize to non-text architectures, specifically vision models (ViTs) or vision-language models (VLMs)?
- **Basis in paper**: Section 5 notes, "The current scope of our study is limited to large language models. In future work, we aim to explore the applicability of our approach to a broader range of architectures, such as vision models and vision-language models."
- **Why unresolved**: The theoretical motivation relies on semantic relationships in text embedding spaces (e.g., "King-Queen" analogies). Visual activation distributions differ statistically, and it is unclear if minimizing angular deviation in vision activations preserves visual semantics as effectively.

### Open Question 3
- **Question**: How robust is the VarP metric when the input activation normalization assumption ($||X_j||_2 < 1$) is violated?
- **Basis in paper**: The derivation in Section 3.2 relies on the assumption that "input activations are normalized... less than 1" to approximate the power series expansion. The paper does not test scenarios where activations exceed this bound.
- **Why unresolved**: If inputs are not normalized or exhibit large outliers, the approximation $E[\frac{1}{1 - \|X_j\|^2}]$ may become unstable or invalid, potentially degrading pruning performance relative to simpler magnitude-based metrics.

## Limitations
- The VarP metric's reliability depends on stable input activation normalization, which may not hold across all layers or models
- The method's effectiveness on non-text architectures remains unproven, limiting generalizability beyond LLMs
- The calibration efficiency advantage is theoretically derived but requires empirical validation across diverse model families and tasks

## Confidence
- **High confidence**: ACE's theoretical foundation combining angular deviation (CosP) and variance-aware metrics (VarP) is sound and well-articulated
- **Medium confidence**: The empirical results showing 18% perplexity reduction and 63% pruning time decrease are well-documented, but replication on different model families would strengthen confidence
- **Low confidence**: The claim that ACE maintains semantic integrity better than magnitude-only methods needs qualitative validation beyond perplexity metrics

## Next Checks
1. **Activation distribution analysis**: Run ACE pruning on LLaMA-7B with L=16, then visualize input activation norms across layers to verify they remain within stable ranges for VarP computation. Identify any layers where ||X_j||²₂ approaches 1.

2. **Calibration length sensitivity test**: Systematically vary calibration sequence length from 16 to 2048 tokens while keeping sparsity constant. Measure both perplexity and pruning time to empirically validate the theoretical calibration efficiency advantage claimed for VarP.

3. **Semantic preservation qualitative check**: Generate text samples from dense, ACE-pruned, and baseline-pruned LLaMA-7B models. Compare semantic coherence and factual consistency to assess whether ACE truly preserves semantic integrity better than magnitude-only methods.