---
ver: rpa2
title: 'DMRL: Data- and Model-aware Reward Learning for Data Extraction'
arxiv_id: '2505.06284'
source_url: https://arxiv.org/abs/2505.06284
tags:
- data
- reward
- arxiv
- number
- security
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of large language models
  (LLMs) to privacy breaches through data extraction. The authors propose DMRL, a
  novel Data- and Model-aware Reward Learning approach that leverages inverse reinforcement
  learning to extract sensitive information from LLMs.
---

# DMRL: Data- and Model-aware Reward Learning for Data Extraction

## Quick Facts
- arXiv ID: 2505.06284
- Source URL: https://arxiv.org/abs/2505.06284
- Authors: Zhiqiang Wang; Ruoxi Cheng
- Reference count: 24
- Key outcome: DMRL achieves PII inference accuracy exceeding 50% across most LLM configurations, with Baichuan2-7B reaching up to 55.1% on the Enron dataset.

## Executive Summary
This paper introduces DMRL, a novel approach for extracting sensitive information from large language models through inverse reinforcement learning. The method constructs an introspective reasoning dataset capturing leakage mindsets, then trains reward models with Group Relative Policy Optimization (GRPO) that dynamically tune optimization based on task difficulty at both data and model levels. Comprehensive experiments demonstrate that DMRL significantly outperforms baseline methods across various LLMs and datasets, achieving high PII inference accuracy while requiring no explicit preference labels.

## Method Summary
DMRL employs a two-stage approach: first, it constructs a demonstration dataset of privacy-leakage Q&A pairs using structured prompts to elicit reasoning chains from target models. Second, it trains N category-specific shadow reward models via ML-IRL optimization, computes dual-level hardness coefficients (data-level via CLIP similarity and model-level via reward gaps), and fine-tunes the extraction policy using GRPO-S with hardness-weighted advantages. The method operates on 7B-scale LLMs across ECHR, Enron, and Yelp-Health datasets, measuring performance through PII extraction precision/recall, reconstruction accuracy, and inference accuracy.

## Key Results
- Achieves PII inference accuracy exceeding 50% across most LLM configurations
- Baichuan2-7B reaches up to 55.1% accuracy on Enron dataset
- Outperforms all baseline methods in data extraction performance
- Demonstrates effectiveness across multiple datasets and model architectures

## Why This Works (Mechanism)

### Mechanism 1: Inverse Reinforcement Learning from Demonstration Data
Learning reward models from demonstration data via IRL guides policy optimization more effectively than supervised fine-tuning alone. The ML-IRL formulation creates a bilevel optimization where the upper level maximizes likelihood on demonstration data while the lower level optimizes policy based on learned rewards. The minimax reformulation contrasts rewards on demonstrated vs. generated responses, extracting implicit preferences without explicit labels. Demonstration data capturing "leakage mindsets" encodes sufficient signal to learn reward functions that generalize to new extraction scenarios.

### Mechanism 2: Dual-Level Hardness Measurement for Adaptive Optimization
Combining data-level and model-level hardness measurements enables more effective policy updates by adapting optimization effort to both input difficulty and model responsiveness. Data hardness uses CLIP similarity between demonstration and generated responses; model hardness uses filtered reward gaps from shadow reward models. The product scales the GRPO advantage function, concentrating updates where they matter most. Harder samples combined with high model responsiveness indicate greater potential for effective extraction learning.

### Mechanism 3: Category-Specific Shadow Reward Model Specialization
Training specialized reward models per privacy-leakage category enables more targeted extraction than a single generalist model. The hierarchical structure allows each category to have M harmful instructions, with each specialist trained on its corresponding category. During GRPO-S optimization, each category uses its corresponding specialist, allowing fine-grained reward signals. Privacy leakage patterns are sufficiently category-distinct that specialist models outperform generalist approaches.

## Foundational Learning

- **Inverse Reinforcement Learning (IRL)**
  - Why needed here: The method's core learning paradigm extracts reward functions from demonstrations rather than explicit preferences.
  - Quick check question: Can you explain why Eq. 2's minimax formulation enables learning from demonstrations without preference labels?

- **CLIP-based Semantic Similarity**
  - Why needed here: Data hardness measurement depends on CLIP's text-text similarity for comparing demonstration vs. generated responses.
  - Quick check question: How does CLIP compute cross-modal similarity, and what are its limitations for comparing privacy-related text fragments?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Policy optimization uses GRPO, which eliminates the critic by using group-level rewards as baselines.
  - Quick check question: How does GRPO's group-relative advantage computation differ from PPO's value-function-based advantage?

## Architecture Onboarding

- **Component map:** Privacy-leakage Q&A pairs → N category-specific shadow reward models → Dual-level hardness coefficients → GRPO-S policy optimization

- **Critical path:** Demonstration dataset quality → Reward model foundation → Shadow reward convergence → Hardness measurement accuracy → GRPO-S stability

- **Design tradeoffs:**
  - Category count N: More specialists vs. computational cost
  - Outlier threshold τ: Filtering noise vs. discarding signal
  - Group size G in GRPO: Baseline stability vs. sampling cost
  - KL penalty β: Conservative updates vs. convergence speed

- **Failure signatures:**
  - Near-zero reward gaps: Reward model cannot distinguish demonstrated/generated responses
  - High hardness variance: Noisy CLIP similarities or inconsistent model outputs
  - KL divergence explosion: Updates too aggressive; increase β
  - Category collapse: Specialists converging to identical behavior

- **First 3 experiments:**
  1. Ablate hardness components: Run with αD only, αM only, both combined on Enron dataset to validate dual-level contribution.
  2. Category sensitivity: Vary N ∈ {3, 5, 10} and measure PII inference accuracy to identify optimal granularity.
  3. Reward convergence analysis: Track reward gap distributions across SRL iterations to determine adequate training duration before policy optimization.

## Open Questions the Paper Calls Out
- The approach currently focuses on textual leakage and does not address multimodal privacy threats involving images or audio.
- How robust is the DMRL attack against models specifically hardened with state-of-the-art defense mechanisms like differential privacy or machine unlearning?
- To what extent does the synthetic nature of the "introspective reasoning" demonstration data limit the extraction of rare or complex real-world PII not captured in the training set?

## Limitations
- Exact hyperparameter settings for Algorithms 1 and 2 are not provided, limiting reproducibility
- The optimal number of privacy-leakage categories remains unclear and may impact performance
- Absolute PII inference accuracy figures cannot be fully verified without access to complete demonstration datasets

## Confidence
- **High Confidence:** The core IRL formulation (Eq. 2) and its equivalence to ML-IRL is well-established in the literature
- **Medium Confidence:** The dual-level hardness measurement approach is theoretically motivated but lacks direct empirical validation
- **Low Confidence:** The absolute PII inference accuracy figures (>50% across most configurations) cannot be fully verified without exact demonstration datasets and complete hyperparameter specifications

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary T, K, and τ across Algorithms 1-2 to determine the robustness of performance to these critical but unspecified settings
2. **Category Granularity Experiment:** Test N ∈ {3, 5, 10, 15} category configurations on the same datasets to empirically identify the optimal tradeoff between specialization and generalization
3. **Hardness Component Ablation:** Run controlled experiments isolating αD and αM contributions on Enron dataset to quantify the marginal benefit of each component in the combined hardness measurement