---
ver: rpa2
title: Large Language Models are Good Relational Learners
arxiv_id: '2506.05725'
source_url: https://arxiv.org/abs/2506.05725
tags:
- arxiv
- relational
- data
- graph
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Rel-LLM, a framework that combines graph
  neural networks (GNNs) and large language models (LLMs) to process relational databases.
  Unlike existing approaches that flatten relational data into text, Rel-LLM preserves
  the relational structure by using a GNN to encode local subgraphs and generate structured
  prompts.
---

# Large Language Models are Good Relational Learners

## Quick Facts
- arXiv ID: 2506.05725
- Source URL: https://arxiv.org/abs/2506.05725
- Reference count: 33
- Primary result: Rel-LLM preserves relational structure using GNN-generated subgraph embeddings and achieves strong performance on RelBench benchmark

## Executive Summary
This paper introduces Rel-LLM, a framework that combines graph neural networks (GNNs) and large language models (LLMs) to process relational databases while preserving their structural integrity. Unlike existing approaches that flatten relational data into text, Rel-LLM maintains the relational structure by using a GNN to encode local subgraphs and generate structured prompts for the LLM. Experiments on the RelBench benchmark demonstrate that Rel-LLM outperforms traditional methods, achieving strong results in both entity classification (AUROC up to 83.74) and regression (MAE down to 11.65), showcasing the effectiveness of integrating structured graph representations with LLMs for relational reasoning.

## Method Summary
Rel-LLM operates by first using a GNN to encode local subgraphs from relational databases, generating structured embeddings that capture the relational context. These embeddings are then converted into structured prompts that preserve the original relational information. The LLM processes these structured prompts to perform reasoning tasks, maintaining the integrity of relationships between entities. This approach contrasts with traditional methods that flatten relational data into text, potentially losing important structural information. The framework leverages the strengths of both GNNs in capturing local graph structures and LLMs in performing complex reasoning over structured inputs.

## Key Results
- Rel-LLM achieves an AUROC of up to 83.74 on entity classification tasks in the RelBench benchmark
- The framework reaches a mean absolute error (MAE) as low as 11.65 on regression tasks
- Rel-LLM outperforms traditional flattened-text approaches while preserving relational structure

## Why This Works (Mechanism)
The framework works by preserving relational structure through GNN-generated subgraph embeddings rather than flattening data into text. GNNs excel at capturing local graph structures and relationships between entities, encoding this information into dense representations. By maintaining these structural relationships in the LLM's input through structured prompts, Rel-LLM enables the model to perform reasoning tasks while preserving the relational context that would be lost in text-based approaches. This structural preservation allows the LLM to leverage its reasoning capabilities more effectively on relational data.

## Foundational Learning
- Graph Neural Networks (GNNs): Needed to encode local graph structures and capture relational information between entities. Quick check: Verify GNN can effectively represent subgraph structures relevant to the task.
- Structured Prompt Engineering: Required to convert GNN embeddings into formats that preserve relational information for LLM processing. Quick check: Ensure prompts maintain structural relationships without information loss.
- Relational Database Schema Understanding: Essential for correctly interpreting and processing the underlying relational structure. Quick check: Confirm proper handling of entity-relationship mappings.

## Architecture Onboarding

Component Map:
Database -> GNN Encoder -> Structured Embeddings -> Prompt Generator -> LLM -> Output

Critical Path:
The critical path flows from the database through the GNN encoder to generate subgraph embeddings, which are then transformed into structured prompts for the LLM. Performance bottlenecks could occur at either the GNN encoding stage for complex graphs or at the LLM processing stage for large prompts.

Design Tradeoffs:
The framework trades computational complexity (GNN + LLM pipeline) for accuracy by preserving relational structure. This introduces additional processing overhead compared to flattened-text approaches but maintains information that would otherwise be lost. The design prioritizes accuracy over efficiency, which may limit scalability for very large databases.

Failure Signatures:
- Poor performance on multi-hop queries may indicate GNN limitations in capturing long-range dependencies
- Degradation with extremely large databases could signal computational bottlenecks in the GNN encoding stage
- Inconsistent results across different relational schemas might suggest insufficient generalization of the prompt generation process

First Experiments:
1. Benchmark Rel-LLM against traditional flattened-text approaches on a simple relational dataset to establish baseline performance improvements
2. Test the framework on multi-hop relational queries to evaluate its ability to capture long-range dependencies
3. Evaluate computational efficiency and memory usage as database size scales to identify practical limitations

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness on diverse real-world relational databases beyond the RelBench benchmark remains unproven
- Scalability to enterprise-scale databases with millions of entities has not been thoroughly validated
- The additional GNN processing layer may introduce computational bottlenecks for complex multi-hop queries

## Confidence

High confidence in:
- The core technical contribution of preserving relational structure through GNN-generated subgraph embeddings
- Demonstrated performance improvements on the RelBench benchmark for both classification and regression tasks

Medium confidence in:
- Generalizability to diverse real-world relational databases beyond the controlled benchmark environment
- Ability to handle complex multi-hop queries and extensive relational traversal

Low confidence in:
- Scalability to enterprise-scale databases with millions of entities
- Computational efficiency trade-offs compared to existing flattened-text approaches

## Next Checks
1. Test Rel-LLM on large-scale, real-world relational databases from business intelligence or scientific domains to evaluate performance and scalability beyond the controlled RelBench benchmark environment

2. Conduct comprehensive ablation studies to quantify the individual contributions of the GNN encoding step versus the LLM reasoning step, and to identify potential bottlenecks in the pipeline

3. Compare computational efficiency and inference time of Rel-LLM against traditional flattened-text approaches and pure GNN methods on datasets of increasing size and complexity to assess practical deployment viability