---
ver: rpa2
title: Adversarial ML Problems Are Getting Harder to Solve and to Evaluate
arxiv_id: '2502.02260'
source_url: https://arxiv.org/abs/2502.02260
tags:
- arxiv
- attacks
- adversarial
- preprint
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that adversarial machine learning (ML) problems
  have become harder to define, solve, and evaluate, especially with the rise of large
  language models (LLMs). Traditional ML focused on well-defined tasks like image
  classification, where success and failure were clearly measurable.
---

# Adversarial ML Problems Are Getting Harder to Solve and to Evaluate

## Quick Facts
- arXiv ID: 2502.02260
- Source URL: https://arxiv.org/abs/2502.02260
- Reference count: 15
- This paper argues that adversarial machine learning problems have become harder to define, solve, and evaluate with the rise of large language models.

## Executive Summary
This paper argues that adversarial machine learning (ML) problems have become harder to define, solve, and evaluate, especially with the rise of large language models (LLMs). Traditional ML focused on well-defined tasks like image classification, where success and failure were clearly measurable. However, LLMs introduce challenges such as unbounded attack spaces, subjective safety definitions, and difficulty in measuring harm or utility. Evaluating defenses is complicated by closed-source models and frequent updates, making reproducibility and fair benchmarking difficult. The authors use case studies like jailbreaks, un-finetunable models, poisoning, and prompt injections to illustrate these challenges, concluding that the field faces significant obstacles in making meaningful progress.

## Method Summary
This is a position paper analyzing the conceptual difficulty of adversarial ML for LLMs through literature synthesis and case studies. The authors examine four key adversarial ML problems - jailbreaks, un-finetuning, poisoning, and prompt injections - to demonstrate how LLMs have introduced fundamental challenges in problem definition, solution methods, and evaluation infrastructure. They draw comparisons with traditional ML adversarial examples to highlight the increased complexity in LLM security research.

## Key Results
- LLM safety problems lack formal definitions and bounded inputs, making optimization extremely inefficient
- Closed-source models and silent updates prevent reproducible evaluation of defenses
- LLM-as-a-judge evaluation creates circular measurement artifacts that overestimate defense effectiveness
- Parameter-space attacks expand the threat model beyond traditional input-space perturbations

## Why This Works (Mechanism)

### Mechanism 1: Definition-Optimization Cascade
The shift from narrow classification tasks to general-purpose LLMs creates a cascade where ill-defined safety properties prevent formal optimization, which in turn blocks principled defense construction. Traditional adversarial ML had bounded input spaces (ℓp-norm constraints) and clear success criteria (misclassification). LLM safety lacks both: "harmful content has no formal definition" and attack spaces are "virtually infinite." Without formal objectives, gradient-based optimization becomes "extremely inefficient and close to random search."

### Mechanism 2: Attack Surface Expansion Through Unbounded Threat Models
LLM threat models permit strictly stronger adversaries than traditional ML, making defense evaluation intractable. Traditional adversaries could perturb inputs but not alter model functionality. LLM adversaries can "not only change the input arbitrarily, but also modify the model itself" through fine-tuning or pruning. This expands the search space from input-space gradients to "parameter-space gradients... often 1000× higher dimensional."

### Mechanism 3: Evaluation Validity Erosion Through Circular Measurement
Using LLMs-as-judges to evaluate LLM safety creates measurement artifacts that overestimate defense effectiveness. When a defense uses similar output filtering as the judge, it achieves "near-perfect scores without necessarily being effective." Additionally, judges themselves are "vulnerable to attacks that make them misclassify model outputs," creating false negatives.

## Foundational Learning

- **Adversarial Examples in Classification**: Traditional ML had bounded input spaces and clear success criteria. Understanding this baseline is prerequisite for seeing why LLM safety is harder.
  - Quick check: Can you explain why constraining perturbations to an ℓp-ball was useful even though it didn't capture all real-world attacks?

- **Threat Models and Adversarial Capabilities**: The paper's core argument hinges on how threat models have expanded. You need to understand what white-box vs. black-box access means.
  - Quick check: What capabilities does a white-box attacker have that a black-box attacker lacks, and why does fine-tuning access change the game?

- **Discrete vs. Continuous Optimization**: The paper emphasizes that text's discrete nature makes gradient-based attacks ineffective. Understanding why gradients work for images but not text is essential.
  - Quick check: Why can't you directly backpropagate gradients through token embeddings to find adversarial text prefixes?

## Architecture Onboarding

- **Component map**: The paper organizes challenges into three layers: (1) Problem Definition → what constitutes attack success/failure and valid inputs; (2) Solution Methods → attack search algorithms and defense construction; (3) Evaluation Infrastructure → harm measurement, utility preservation, reproducibility mechanisms. Each case study (jailbreaks, unlearning, poisoning, etc.) maps to specific cells in this 3×N matrix (Table 1).

- **Critical path**: For any new adversarial ML project on LLMs, follow this sequence: (1) explicitly define whether the goal is real-world security or scientific understanding; (2) if scientific, identify a formalizable sub-problem with bounded inputs/outputs; (3) establish evaluation methodology independent of proposed defenses; (4) only then attempt solution development.

- **Design tradeoffs**: The paper highlights a central tension: studying "real" unbounded attacks vs. tractable formalized problems. Unbounded attacks better reflect deployment but prevent progress measurement; bounded problems enable rigor but may not transfer. Additionally, closed-source model evaluation trades realism for reproducibility.

- **Failure signatures**: The paper identifies several red flags in prior work: (1) defenses evaluated only against known attacks without adaptive adversaries; (2) LLM-judge evaluation where the judge resembles the defense mechanism; (3) claims of "unfinetunability" without parameter-space gradient verification; (4) unlearning evaluations that only check generation refusal without probing latent knowledge.

- **First 3 experiments**:
  1. For any proposed LLM safety defense, test whether an LLM-judge that shares architectural similarities with the defense produces artificially inflated scores by comparing against a structurally different judge.
  2. Attempt to reproduce published attack success rates on a closed-source API by testing at multiple time points to detect silent patching.
  3. For a claimed "unlearned" model, probe whether the knowledge is truly removed by testing relearning speed: if the model relearns removed content faster than learning genuinely new content, knowledge likely persisted in weights.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can specific, formal "toy" problems for LLM security (analogous to ℓp-bounded robustness in computer vision) be defined to enable rigorous scientific progress?
- **Basis in paper**: Section 4.2 suggests that to advance scientific understanding, the field must identify "formal, well-defined sub-problems that can be rigorously studied," rather than solely tackling fuzzy, real-world scenarios.
- **Why unresolved**: Current LLM research prioritizes complex, real-world threats with unbounded inputs and subjective safety definitions, which resist formal modeling and reproducible measurement.
- **What evidence would resolve it**: The establishment of a standardized benchmark based on a narrow, formal constraint (e.g., restricted perturbations or fixed-length prefixes) that allows for precise quantitative comparison of defenses.

### Open Question 2
- **Question**: How can the field evaluate attack success and defense utility without relying on "LLM-as-a-judge" methods that introduce circularity, bias, or vulnerabilities?
- **Basis in paper**: Section 2.3.1 details how LLM judges often fail to match human judgment, can be attacked to misclassify outputs, and may create artificial correlations that overestimate defense robustness.
- **Why unresolved**: Safety properties for generative models are qualitative and context-dependent, making automated, objective measurement of "harm" in free-form text fundamentally difficult.
- **What evidence would resolve it**: The development of an evaluation framework or metric that robustly quantifies harm and utility in open-ended text without being susceptible to the biases or attacks that plague current LLM-based judges.

### Open Question 3
- **Question**: Can automated optimization techniques be developed that effectively navigate the discrete, unbounded input space of LLMs to find worst-case attacks?
- **Basis in paper**: Section 2.2.1 and Section 3.1 note that manual, human-driven attacks currently outperform automated methods because the optimization landscape is vast and lacks a differentiable, well-defined objective function.
- **Why unresolved**: Text inputs are discrete, preventing the direct application of gradient-based attacks used in vision, and proxy objectives (like maximizing compliance prefixes) often result in suboptimal or "gibberish" inputs.
- **What evidence would resolve it**: An automated attack algorithm that consistently identifies adversarial prompts with higher success rates or efficiency than human red-teamers across diverse models.

## Limitations

- The argument relies heavily on qualitative observations rather than systematic quantification of difficulty
- Limited empirical evidence showing that formalizing LLM safety problems would necessarily enable principled defenses
- The circular evaluation critique, while conceptually valid, is demonstrated through specific examples but lacks broader systematic validation
- Paper doesn't adequately address whether some LLM-specific challenges might be unavoidable rather than merely difficult to formalize

## Confidence

**High Confidence**: The claim that closed-source models and frequent updates hinder reproducibility is well-supported by the literature and practical experience. The observation that unbounded attack spaces make optimization difficult has clear theoretical grounding.

**Medium Confidence**: The argument that LLMs face fundamentally harder adversarial problems than traditional ML is compelling but relies on several contested premises, particularly the necessity of formal definitions for progress. The circular evaluation critique is sound but may overestimate the practical impact on current research.

**Low Confidence**: The assertion that parameter-space attacks are strictly harder to defend against than input-space attacks lacks systematic empirical validation across different model architectures and attack methodologies.

## Next Checks

1. **Formalization Experiment**: Attempt to formalize a specific LLM safety sub-problem (e.g., refusal evasion under ℓp-bounded perturbations) and develop a principled defense. Measure whether this approach yields more robust solutions than current heuristic methods.

2. **Evaluation Independence Test**: For a set of published LLM safety defenses, systematically test whether using structurally different judges (non-LLM-based, different architectures, or human evaluation) produces significantly different effectiveness rankings compared to standard LLM-judge evaluations.

3. **Transferability Study**: Compare the transferability of defenses developed for formalized sub-problems (like ℓp-bounded attacks) against real-world unbounded attacks. Measure whether formalization actually helps or if the simplification is too severe to capture practical vulnerabilities.