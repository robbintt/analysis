---
ver: rpa2
title: 'CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data
  Augmentation'
arxiv_id: '2508.08386'
source_url: https://arxiv.org/abs/2508.08386
tags:
- guidance
- reasoning
- answer
- assistant
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CoDAE, a Chain-of-Thought data augmentation
  framework that adapts large language models for educational tutoring. It collects
  real-world student-tutor dialogues, enhances them using CoT prompting to promote
  step-by-step reasoning, and creates specialized dataset variants to address over-compliance,
  low response adaptivity, and threat vulnerability.
---

# CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation
## Quick Facts
- arXiv ID: 2508.08386
- Source URL: https://arxiv.org/abs/2508.08386
- Reference count: 40
- Primary result: Chain-of-Thought data augmentation framework improves educational alignment of LLMs through pedagogical fine-tuning and adversarial training

## Executive Summary
This paper introduces CoDAE, a Chain-of-Thought data augmentation framework that adapts large language models for educational tutoring. The approach collects real-world student-tutor dialogues, enhances them using CoT prompting to promote step-by-step reasoning, and creates specialized dataset variants to address over-compliance, low response adaptivity, and threat vulnerability. Fine-tuning four open-source LLMs on these augmented datasets results in models that deliver more pedagogically appropriate guidance, better support reasoning processes, and effectively resist premature answer disclosure.

## Method Summary
The method involves collecting real student-tutor dialogues, preprocessing to retain complete exchanges, and augmenting with structured CoT using Qwen2.5-72B-Instruct to generate Socratic-style guidance. Three specialized dataset variants (CoDAE I, A, I+A) add adversarial cases for distress handling and threat resistance. Fine-tuning uses LoRA adapters with token masking that isolates `<guidance>` spans, preventing user imitation while preserving context. Four model families (Llama-3.1-8B, Qwen2.5-7B, InternLM3-8B, Gemma2-9B) are trained on each variant and evaluated using LLM-as-a-Judge metrics plus jailbreak robustness tests.

## Key Results
- Fine-tuned models show improved pedagogical helpfulness (4.05 vs 3.89 base for Qwen2.5) and scaffolding effectiveness
- Accuracy (answer disclosure) significantly reduced, with Gemma2 FT I+A achieving lowest rate at 0.06
- Most models achieve perfect jailbreak resistance (1.0) except InternLM3 which degrades to 0.77
- All variants show improved clarity and reasoning progression scores across multiple LLM families

## Why This Works (Mechanism)
### Mechanism 1
Enriching low-quality dialogues with Socratic-style Chain-of-Thought prompting improves pedagogical alignment in fine-tuned models. Raw student-tutor dialogues are structured with contextual metadata and fed to Qwen2.5-72B-Instruct using few-shot prompts that demonstrate multi-turn Socratic tutoring, generating enriched guidance in `<guidance>` tags.

### Mechanism 2
Targeted adversarial dialogue cases reduce over-compliance, improve response adaptivity, and maintain robustness against manipulative prompts. Three specialized dataset variants (CoDAE I, A, I+A) teach models to handle edge cases pedagogically through exposure to distress messages and emotionally coercive attack prompts.

### Mechanism 3
Masking non-guidance tokens during fine-tuning focuses learning on pedagogical response generation while preventing user imitation. All tokens outside `<guidance>...</guidance>` spans are assigned loss weight -100, ensuring the model learns assistant behavior without reproducing student input patterns.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables efficient parameter updates for 7-9B models using only projection layers, reducing memory requirements for educational fine-tuning
  - Quick check question: Can you explain how LoRA's low-rank decomposition (rank=64, alpha=128 in this paper) reduces trainable parameters while preserving model capacity?

- **Chain-of-Thought Prompting (Pedagogical Variant)**
  - Why needed here: Unlike standard CoT that targets correct answers, this variant generates reasoning chains that withhold answers and guide student thinking
  - Quick check question: How does the pedagogical CoT objective differ from standard mathematical reasoning CoT, and what prompt constraints enforce this?

- **Token Masking in Supervised Fine-Tuning**
  - Why needed here: Critical for multi-turn dialogue training where only specific utterances should contribute to loss
  - Quick check question: What is the effect of assigning loss weight -100 to tokens, and how does this differ from attention masking?

## Architecture Onboarding
- **Component map:** Raw dialogues → Preprocessing → CoT augmentation (Qwen2.5-72B-Instruct) → Adversarial case injection → 4 dataset variants (base, I, A, I+A)
- **Training:** LoRA adapters on Llama-3.1-8B, Qwen2.5-7B, InternLM3-8B, Gemma-2-9B with guidance-token masking
- **Evaluation:** LLM-as-a-Judge (LLaMA-3.3-70B) for pedagogical metrics + JailbreakBench for robustness

- **Critical path:**
  1. Preprocess raw dialogues (retain complete exchanges, extract metadata)
  2. Augment with structured CoT using few-shot prompts (see Appendix prompt template)
  3. Inject adversarial cases for each targeted limitation
  4. Fine-tune with LoRA (rank=64) using guidance-only loss
  5. Evaluate on held-out 1000-query test set across 4 configurations

- **Design tradeoffs:**
  - Dataset size (2,901 samples) vs. overfitting risk: LoRA reduces parameters but small data may still cause specialization
  - Adversarial case density vs. natural dialogue distribution: Too many adversarial cases may degrade fluency on normal queries
  - Masking strictness vs. context utilization: Full masking prevents user imitation but may limit learning from dialogue structure

- **Failure signatures:**
  - Model outputs user-like messages: Masking not applied correctly to labels
  - High answer disclosure on adversarial prompts: Adversarial cases underweighted or insufficient variety
  - Low Self-BLEU but incoherent responses: Over-augmentation producing unnatural guidance
  - InternLM jailbreak resistance drop (0.77 vs. 1.00 base): Fine-tuning degraded safety alignment in this model family

- **First 3 experiments:**
  1. **Baseline vs. CODAE comparison:** Measure accuracy (answer disclosure), pedagogical helpfulness, and scaffolding scores on held-out queries across all four base models.
  2. **Ablation on adversarial variants:** Compare CODAE vs. CODAE I vs. CODAE A vs. CODAE I+A to isolate contribution of each augmentation type.
  3. **Masking validation:** Train with full-sequence loss vs. guidance-only masking and measure user imitation rate in generated outputs.

## Open Questions the Paper Calls Out
- **Real-world deployment validation:** Do the pedagogical improvements observed in simulated scenarios transfer to live classroom settings with actual students? The conclusion explicitly lists this as a future direction to validate results.

- **Reinforcement learning exploration:** Can reinforcement learning-based fine-tuning improve pedagogical alignment beyond the capabilities of the supervised CoDAE approach? The conclusion identifies this as a specific future research direction.

- **LLM judge protocol robustness:** How robust is the LLM-as-a-Judge evaluation protocol for assessing nuanced pedagogical skills like scaffolding and reasoning progression? While the paper claims empirical reliability, it relies entirely on an LLM judge without independent validation against human experts.

## Limitations
- Small dataset size (2,901 dialogues) may limit diversity of educational scenarios and cause model specialization
- LLM-as-a-Judge evaluation introduces potential bias without independent validation against human expert ratings
- InternLM3 model shows concerning safety alignment degradation (0.77 vs 1.00 base jailbreak resistance) during fine-tuning

## Confidence
- **High Confidence**: LoRA fine-tuning effectiveness, masking technique for isolating pedagogical responses, and overall improvement in pedagogical helpfulness metrics
- **Medium Confidence**: Generalizability of adversarial case effectiveness across model families and robustness of Chain-of-Thought augmentation mechanism
- **Low Confidence**: Complete absence of human evaluation data and reliance on synthetic adversarial cases for threat modeling

## Next Checks
1. **Human Evaluation Validation**: Conduct small-scale human expert study comparing base vs. CoDAE fine-tuned models on actual student queries, focusing on pedagogical alignment and safety compliance.

2. **Adversarial Case Diversity Testing**: Systematically generate and test 100+ novel adversarial prompts not present in training data, measuring model robustness across all fine-tuned variants.

3. **Longitudinal Student Interaction Study**: Deploy fine-tuned models in controlled tutoring environment with actual students over multiple sessions, tracking learning outcomes and model behavior consistency.