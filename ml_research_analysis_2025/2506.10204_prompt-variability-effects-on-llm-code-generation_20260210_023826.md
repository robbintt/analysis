---
ver: rpa2
title: Prompt Variability Effects On LLM Code Generation
arxiv_id: '2506.10204'
source_url: https://arxiv.org/abs/2506.10204
tags:
- code
- llms
- prompt
- prompts
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how sensitive LLM-generated code is to
  variations in input prompts. The authors propose two complementary evaluation methods:
  a synthetic pipeline that systematically alters prompts using typos, synonyms, and
  paraphrasing, and a persona-based approach that simulates how users with different
  backgrounds would describe the same coding task.'
---

# Prompt Variability Effects On LLM Code Generation

## Quick Facts
- **arXiv ID**: 2506.10204
- **Source URL**: https://arxiv.org/abs/2506.10204
- **Reference count**: 40
- **Key result**: LLM code generation shows high sensitivity to typos but greater robustness to synonyms and paraphrasing in prompts

## Executive Summary
This paper investigates how variations in input prompts affect the quality and consistency of LLM-generated code. The authors develop two complementary evaluation frameworks: a synthetic pipeline that systematically perturbs prompts with typos, synonyms, and paraphrasing, and a persona-based approach that simulates how users with different technical backgrounds would describe the same coding task. Through experiments on multiple popular LLMs using both LeetCode problems and open-ended coding tasks, they demonstrate that code similarity degrades rapidly with typos but remains relatively stable under synonym substitution and paraphrasing. The persona-based analysis reveals that technical background influences both prompt formulation and the resulting code characteristics. These findings highlight the importance of understanding LLM sensitivity for building reliable code generation systems and suggest opportunities for improving robustness through user guidance or prompt regularization techniques.

## Method Summary
The authors employ two complementary evaluation methods to assess LLM sensitivity to prompt variations. The synthetic pipeline systematically alters prompts through three perturbation types: introducing typos (using a confusion matrix-based error model), substituting words with synonyms (using WordNet and embedding-based methods), and paraphrasing (using T5-based models). For each perturbation type, they generate multiple variants of the same prompt and compare the resulting code outputs using similarity metrics (BLEU, ROUGE, Jaccard). The persona-based approach complements this by having authors with different technical backgrounds (software engineering, machine learning, mathematics) independently describe the same coding tasks, creating a natural variation dataset. Both methods are applied to evaluate multiple LLMs on LeetCode problems and a curated dataset of open-ended coding tasks, allowing systematic comparison of code generation consistency across different perturbation types and user personas.

## Key Results
- Code similarity drops rapidly with typos but shows greater robustness to synonyms and paraphrasing
- Different technical personas produce systematically different prompts and code characteristics
- LLMs exhibit varying sensitivity profiles across perturbation types and model architectures
- The persona-based approach reveals qualitative differences in how technical background influences code generation

## Why This Works (Mechanism)
The paper doesn't explicitly detail the underlying mechanisms, but we can infer that LLMs rely heavily on exact token matching for code generation, making them vulnerable to typos that break semantic coherence. Synonyms and paraphrasing preserve semantic meaning through contextual embeddings, allowing the model to recover intended functionality despite lexical variations. The persona-based differences likely stem from how technical backgrounds shape vocabulary choices and problem-solving approaches, which the LLM interprets differently based on its training data distribution.

## Foundational Learning

**Code similarity metrics (BLEU, ROUGE, Jaccard)**: These metrics quantify the overlap between generated code and reference implementations, enabling systematic comparison of code variants across prompt perturbations. Quick check: Verify metric sensitivity by testing on known code variants with expected similarity levels.

**Prompt perturbation techniques**: Systematic introduction of typos, synonyms, and paraphrasing creates controlled variations that isolate specific factors affecting LLM code generation. Quick check: Validate perturbation coverage by testing on diverse prompt types and measuring perturbation effectiveness.

**Persona-based evaluation**: Simulating different user backgrounds provides ecological validity by reflecting real-world variation in how users express coding requirements. Quick check: Compare persona-generated prompts with actual user data to assess representativeness.

## Architecture Onboarding

**Component map**: Synthetic pipeline (typo generator -> synonym replacer -> paraphraser) -> Code generator (LLM) -> Similarity metrics
**Critical path**: Prompt perturbation → Code generation → Similarity comparison
**Design tradeoffs**: The synthetic approach offers controlled, reproducible perturbations but may miss real-world complexity, while the persona approach captures authentic variation but lacks systematic control.
**Failure signatures**: Rapid similarity degradation indicates prompt sensitivity; consistent outputs across perturbations suggest robustness.
**First experiments**: 1) Test typo sensitivity on simple functions, 2) Evaluate synonym robustness on standard library usage, 3) Assess paraphrasing effects on algorithm implementation

## Open Questions the Paper Calls Out
None explicitly stated in the original paper, but the research raises several implicit questions about the generalizability of findings across different LLM architectures, the relationship between prompt sensitivity and code execution correctness, and whether robustness to prompt variations can be improved through model fine-tuning or prompt engineering techniques.

## Limitations

- Prompt variability coverage is limited to three perturbation types, missing real-world complexities like domain terminology and multi-turn conversations
- Code quality assessment relies on similarity metrics rather than functional correctness or execution-based validation
- Results may not generalize beyond the specific LLMs, tasks, and datasets used in the experiments
- Persona construction depends on subjective assumptions about how different technical backgrounds express requirements

## Confidence

- **High confidence**: Code similarity drops significantly with typos in synthetic prompts
- **Medium confidence**: Differential robustness to synonyms versus paraphrasing
- **Medium confidence**: Qualitative observations about persona-based prompt differences

## Next Checks

1. Execute and validate functional correctness of generated code variants across prompt perturbations to assess whether similarity metrics align with practical usability differences
2. Conduct user studies with actual developers from different technical backgrounds to validate the persona-based prompt generation approach and assess real-world sensitivity to prompt variations
3. Expand synthetic perturbation space to include multi-turn conversation contexts, incomplete specifications, and domain-specific terminology to better model real-world prompt variability