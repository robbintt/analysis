---
ver: rpa2
title: 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities'
arxiv_id: '2505.13195'
source_url: https://arxiv.org/abs/2505.13195
tags:
- llms
- adversary
- action
- adversarial
- learner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an adversarial evaluation framework to systematically
  probe the decision-making vulnerabilities of large language models (LLMs) under
  interactive and adversarial conditions. The framework, inspired by cognitive psychology
  and game theory, tests LLMs in two canonical tasks: the two-armed bandit task and
  the Multi-Round Trust Task.'
---

# Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities

## Quick Facts
- arXiv ID: 2505.13195
- Source URL: https://arxiv.org/abs/2505.13195
- Authors: Lili Zhang; Haomiaomiao Wang; Long Cheng; Libao Deng; Tomas Ward
- Reference count: 29
- Primary result: LLMs exhibit varying degrees of susceptibility to adversarial manipulation, with GPT-4, Gemini-1.5, and DeepSeek-V3 being highly predictable once preferences are established, while GPT-3.5 shows more exploratory but exploitable behavior.

## Executive Summary
This paper introduces an adversarial evaluation framework to systematically probe the decision-making vulnerabilities of large language models under interactive and adversarial conditions. The framework, inspired by cognitive psychology and game theory, tests LLMs in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task. Applied to GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, the framework reveals that while some LLMs like GPT-4, Gemini-1.5, and DeepSeek-V3 exhibit high predictability and susceptibility to manipulation once preferences are established, others like GPT-3.5 show more exploratory but exploitable behavior. The findings highlight the importance of adaptability and fairness recognition for robust, trustworthy AI decision-making, offering a diagnostic methodology to identify model-specific vulnerabilities rather than a performance benchmark.

## Method Summary
The framework employs a three-phase approach: First, it collects behavioral data from target LLMs by running them through decision-making tasks. Second, it trains an RNN-based learner model to predict LLM actions based on previous actions, rewards, and observations. Third, it trains a Deep Q-learning adversary to manipulate rewards and observations delivered to the learner model, learning to maximize target behavior. Finally, the trained adversary is deployed against the actual LLMs to validate the attack. The framework was tested on two tasks: a two-armed bandit task (100 trials, binary choice with reward feedback) and a Multi-Round Trust Task (10 rounds, investment allocation and returns).

## Key Results
- GPT-4, Gemini-1.5, and DeepSeek-V3 showed high predictability and susceptibility to manipulation once preferences were established, with adversaries achieving 93-95% target selection rates
- GPT-3.5 exhibited more exploratory behavior but was still exploitable, particularly in the bandit task where it showed lower reward efficiency
- Models demonstrated varying ability to recognize fairness, with Gemini-1.5 and DeepSeek-V3 showing stronger fairness awareness compared to GPT-3.5
- The framework successfully identified model-specific vulnerabilities, revealing that strategic flexibility and adaptability are critical for robust AI decision-making

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Modeling for Black-Box Adversaries
The framework trains an RNN learner model on LLM behavioral data, creating a differentiable surrogate that approximates the LLM's decision policy. An RL adversary then interacts with this surrogate, learning to generate reward sequences that drive the Learner's internal state toward target outcomes. Because the Learner mimics the LLM, this adversarial policy transfers effectively to the actual LLM, with success rates reaching 93-95% in target selection.

### Mechanism 2: Exploitation of Exploration-Exploitation Rigidity
LLMs with high decision rigidity (low switching rates after negative feedback) become highly susceptible to manipulation once an initial preference is established. The adversary identifies this rigidity and front-loads rewards to establish a target preference early. Once the model locks onto this preference, the adversary can "burn" non-target rewards without altering behavior, effectively trapping the model in the target policy.

### Mechanism 3: State-Dependent Feedback Engineering
Unlike static attacks, this framework's adversary observes the RNN Learner's internal state, which encodes the cumulative history of the LLM. The adversary learns to withhold or provide rewards based on this history (e.g., rewarding the target only when the model is wavering) to minimize resource expenditure while maintaining control, demonstrating sophisticated state-aware manipulation strategies.

## Foundational Learning

- **Concept: Reinforcement Learning (RL) Basics**
  - **Why needed here:** The core adversary is a Deep Q-Learning agent. You must understand how an agent learns a policy to maximize cumulative reward by interacting with an environment (the Learner Model).
  - **Quick check question:** How does the "adversarial reward" differ from the "learner reward" in the context of training the adversary?

- **Concept: Recurrent Neural Networks (RNNs) & Sequential Modeling**
  - **Why needed here:** The "Learner Model" uses an RNN to approximate the LLM. You need to understand how RNNs maintain an "internal state" that evolves over time steps to capture history.
  - **Quick check question:** Why is an RNN chosen over a standard feed-forward network to model the LLM's behavior in a multi-round task?

- **Concept: Exploration vs. Exploitation Trade-off**
  - **Why needed here:** The paper diagnoses vulnerabilities based on how models balance trying new options vs. sticking to known rewards.
  - **Quick check question:** In the context of the Two-Armed Bandit task, what does a low "no-reward-switch rate" indicate about a model's strategy?

## Architecture Onboarding

- **Component map:** Target LLM -> Data Collection Interface -> Learner Model (RNN) -> Adversary Agent (DQN) -> Task Environment
- **Critical path:** 1) Behavioral Cloning: Train RNN Learner on LLM logs until it accurately predicts LLM actions. 2) Adversary Training: Freeze the Learner; train the DQN Adversary against the Learner to maximize target behavior. 3) Live Deployment: Connect the trained Adversary to the live LLM to validate the attack.
- **Design tradeoffs:** Learner Model Complexity (larger RNN better approximates LLM but slows adversary training); Task Constraints (strict constraints make problem realistic but harder for adversary to solve); Assumption that surrogate is faithful (highly stochastic LLMs may cause surrogate drift).
- **Failure signatures:** Surrogate Drift (Learner fails to predict LLM actions during live testing); Adversary Oscillation (repeats switching without driving target probability up); LLM Refusal (exits game persona, breaking environment loop).
- **First 3 experiments:** 1) Sanity Check: Verify Learner Model significantly outperforms random baseline in predicting LLM's next action on held-out test set. 2) Overfitting Test: Train adversary against Learner Model alone; achieve >95% target selection in simulation. 3) Transfer Attack: Deploy trained adversary against live LLM; compare "percentage of target selection" vs control condition (random rewards).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the adversarial framework effectively diagnose decision-making vulnerabilities in task paradigms beyond the two-armed bandit and Multi-Round Trust Task?
- **Basis in paper:** The authors state the framework is "designed to be adaptable, this framework is applicable to a wider range of decision-making scenarios" but only validate it on two canonical tasks.
- **Why unresolved:** No additional task paradigms have been tested with this framework to demonstrate its broader diagnostic utility.
- **What evidence would resolve it:** Application of the framework to other cognitive tasks (e.g., prisoner's dilemma, ultimatum game, Iowa gambling task) showing similar success in identifying model-specific vulnerabilities.

### Open Question 2
- **Question:** What training interventions or architectural modifications can reduce the identified susceptibility to adversarial manipulation while preserving model capabilities?
- **Basis in paper:** The paper identifies vulnerabilities but only states "the need for continuous refinement of LLMs to enhance their strategic flexibility and robustness" without proposing specific remediation strategies.
- **Why unresolved:** The framework is purely diagnostic and no intervention studies were conducted.
- **What evidence would resolve it:** Studies demonstrating that specific fine-tuning approaches, constitutional AI methods, or prompt engineering reduce vulnerability scores while maintaining task performance.

### Open Question 3
- **Question:** Do the observed vulnerability profiles generalize to a broader range of LLMs beyond GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3?
- **Basis in paper:** Only four models were tested, and the authors note "model-specific susceptibilities," leaving unclear whether these patterns reflect broader architectural or training-related factors.
- **Why unresolved:** Limited model sample prevents generalization about whether vulnerability patterns cluster by model family, size, or training methodology.
- **What evidence would resolve it:** Testing additional models (Claude, LLaMA variants, Mistral) with analysis of whether exploration-exploitation biases and fairness recognition cluster systematically.

### Open Question 4
- **Question:** How do these laboratory-identified vulnerabilities manifest in real-world decision-making deployments?
- **Basis in paper:** The paper emphasizes relevance to "healthcare, finance, and autonomous systems" but all experiments use simplified simulated scenarios with explicit numerical feedback.
- **Why unresolved:** No validation connecting laboratory vulnerability scores to outcomes in actual deployment contexts.
- **What evidence would resolve it:** Correlation studies between framework-derived vulnerability metrics and performance degradation in deployed LLM-based decision systems under adversarial conditions.

## Limitations
- The framework's efficacy depends heavily on the RNN learner's ability to faithfully approximate the target LLM's decision policy, which may struggle with highly stochastic or context-dependent models
- The approach focuses on short-term manipulation within defined task boundaries and may miss longer-term strategic vulnerabilities or emergent behaviors in open-ended scenarios
- The surrogate modeling approach may not generalize across diverse LLM architectures and behaviors, with generalizability across different models remaining uncertain

## Confidence

- **High Confidence:** The core mechanism of using surrogate modeling for adversarial testing is well-supported by experimental results, particularly the successful transfer of adversarial policies from learner model to actual LLMs
- **Medium Confidence:** The diagnostic insights about LLM decision-making vulnerabilities (exploitation rigidity, state-dependent feedback engineering) are compelling but may be specific to tested models and tasks
- **Medium Confidence:** The framework's potential as a diagnostic methodology for identifying model-specific vulnerabilities is promising but requires further validation across broader range of models and tasks

## Next Checks

1. **Cross-Model Generalization:** Apply the framework to additional LLM families (e.g., Claude, LLaMA) and tasks to assess whether identified vulnerabilities are model-specific or generalizable
2. **Temporal Robustness:** Test the framework's effectiveness over extended interaction periods and across sessions to evaluate persistence of learned adversarial policies
3. **Real-World Applicability:** Adapt the framework to simulate real-world adversarial scenarios (e.g., social engineering, misinformation campaigns) to assess practical security implications