---
ver: rpa2
title: A State-of-the-Art SQL Reasoning Model using RLVR
arxiv_id: '2509.21459'
source_url: https://arxiv.org/abs/2509.21459
tags:
- rlvr
- data
- bird
- arxiv
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work demonstrates how reinforcement learning with verifiable
  rewards (RLVR) can produce state-of-the-art reasoning models for text-to-SQL tasks
  without proprietary models or extra data. The method uses a two-stage fine-tuning
  pipeline: an offline warm-up with TAO followed by online RLVR, both using simple
  execution-based rewards.'
---

# A State-of-the-Art SQL Reasoning Model using RLVR

## Quick Facts
- **arXiv ID**: 2509.21459
- **Source URL**: https://arxiv.org/abs/2509.21459
- **Reference count**: 40
- **Primary result**: Achieved 73.56% exact-match accuracy on BIRD benchmark without proprietary models or extra data

## Executive Summary
This work demonstrates that reinforcement learning with verifiable rewards (RLVR) can produce state-of-the-art reasoning models for text-to-SQL tasks. The method uses a two-stage fine-tuning pipeline: an offline warm-up with TAO followed by online RLVR, both using simple execution-based rewards. Applied to the BIRD benchmark, the resulting model achieved 73.56% exact-match accuracy without self-consistency and 75.68% with self-consistency, outperforming existing methods while requiring fewer generations.

## Method Summary
The approach uses Qwen 2.5 32B Coder Instruct as the base model with a modified OmniSQL prompt. It employs a two-stage fine-tuning pipeline: first, offline RL with TAO generates multiple responses per training example and performs offline optimization; second, online RLVR fine-tunes the TAO model using verifiable rewards from SQL execution. The reward function is binary (0 or 1) based on execution match, with a -1 penalty for syntactically incorrect SQL. Inference uses self-consistency with 7 generations and weighted majority voting.

## Key Results
- Achieved 73.56% exact-match accuracy on BIRD test set without self-consistency
- Reached 75.68% accuracy with self-consistency using only 7 generations
- Outperformed existing methods while requiring fewer LLM calls (7 vs. 8-32)

## Why This Works (Mechanism)

### Mechanism 1
Offline RL warm-start provides better initialization for online RLVR than supervised fine-tuning alone. TAO generates multiple responses per training example, computes rewards, and performs offline optimization, creating a stronger inductive bias before online RL, improving sample efficiency and final performance.

### Mechanism 2
Simple binary execution reward with syntax penalty is sufficient for text-to-SQL; reward shaping provides negligible gains. The verifiable reward directly measures task success (execution match: 0 or 1) plus a -1 penalty for syntax errors, incentivizing correct reasoning without complex reward engineering.

### Mechanism 3
RLVR-trained models require fewer self-consistency samples than baseline models to achieve comparable or better performance. RLVR concentrates probability mass on correct outputs during training, producing higher-quality candidate distributions that reduce the number of samples needed for weighted majority voting to converge on the correct answer.

## Foundational Learning

- **Concept: Verifiable Rewards vs. Reward Models**
  - Why needed here: The paper distinguishes RLVR from RLHF—verifiable rewards come from objective execution (SQL runs and matches ground truth), not learned human preferences.
  - Quick check question: Given a code generation task, can you identify what makes a reward "verifiable" versus "learned"?

- **Concept: Offline vs. Online RL for LLMs**
  - Why needed here: The two-stage pipeline (TAO offline → RLVR online) is central to the method. Offline RL learns from a fixed dataset of model outputs; online RL generates new samples during training.
  - Quick check question: What data does offline RL require that online RL does not, and vice versa?

- **Concept: Self-Consistency / Test-Time Compute**
  - Why needed here: The paper combines training-time improvements (RLVR) with inference-time improvements (self-consistency). Understanding how majority voting works over multiple generations is essential.
  - Quick check question: Why would a model with higher accuracy also need fewer samples for self-consistency to work well?

## Architecture Onboarding

- **Component map**: Prompt/Model Selector → TAO (Offline RL) → RLVR Trainer (Online RL) → Inference Self-Consistency

- **Critical path**: Prompt design → Model selection (dev set evaluation) → TAO warm-up (offline) → RLVR fine-tuning (online) → Self-consistency at inference. Errors in early stages compound; bad prompt selection cannot be recovered by RL alone.

- **Design tradeoffs**:
  - TAO intensity: "limited TAO training" prevented over-training that "led to less overall gains in the later online RL stage"
  - KL divergence: "completely removing the KL divergence term in RLVR did not significantly hurt performance" but this may not generalize to other tasks
  - Self-consistency samples: 7 chosen; more samples increase compute cost with diminishing returns

- **Failure signatures**:
  - Model doesn't generate reasoning traces → prompt modification needed (add reasoning instructions)
  - Syntactically incorrect SQL → -1 penalty signal should reduce over time
  - Poor dev→test generalization → check for overfitting, reduce KL coefficient or learning rate

- **First 3 experiments**:
  1. **Baseline establishment**: Run Qwen 2.5 32B Coder Instruct with OmniSQL prompt on BIRD dev set; compare to Figure 2 baselines (expect ~64.8%)
  2. **TAO-only ablation**: Apply 1-2 iterations of TAO; measure dev set improvement (expect ~67% based on paper); stop before over-training
  3. **Reward signal validation**: Verify execution environment correctly returns 0/1 + syntax penalty; test on 100 examples with known ground truth before full RLVR training

## Open Questions the Paper Calls Out

### Open Question 1
Why do open-source models (Qwen, Llama) outperform proprietary models (GPT-4o, O3, Claude Sonnet) on the BIRD text-to-SQL benchmark? The authors observe this phenomenon but do not investigate causes such as training data composition, code-focused pre-training, or benchmark contamination.

### Open Question 2
What is the optimal balance between TAO (offline RL) warm-up duration and subsequent online RLVR training? The paper reports using "limited" TAO but does not systematically characterize the trade-off or identify optimal stopping criteria.

### Open Question 3
Can more sophisticated reward shaping beyond the simple 0-1 execution metric with syntax penalty improve RLVR performance? The claim that "preliminary investigation using reward shaping did not yield any significant improvements" lacks details on what shaping was tried.

### Open Question 4
How well does the RLVR approach generalize to real-world enterprise text-to-SQL tasks beyond the BIRD benchmark? BIRD may not capture enterprise-specific challenges like proprietary schemas, domain jargon, or complex access patterns; no enterprise evaluation is presented.

## Limitations

- Limited ablation studies on TAO initialization benefits—only indirect evidence from dev set improvement
- No comprehensive exploration of alternative reward shaping strategies beyond binary execution rewards
- Lack of head-to-head comparisons validating claimed self-consistency sample efficiency advantage

## Confidence

- **High confidence**: The overall pipeline (TAO → RLVR → self-consistency) achieves state-of-the-art BIRD performance of 73.56% exact-match accuracy
- **Medium confidence**: The two-stage fine-tuning approach improves over supervised fine-tuning alone (dev set improvement of 2.6% observed)
- **Medium confidence**: Binary execution rewards with syntax penalty are sufficient for text-to-SQL (supported by "preliminary investigation" but not comprehensive ablation)
- **Low confidence**: RLVR-trained models require significantly fewer self-consistency samples (7 vs. 8-32) than baseline approaches (no direct comparative evidence provided)

## Next Checks

1. **Ablation of TAO initialization**: Train a baseline model using only supervised fine-tuning on BIRD data, then train another using only online RLVR without TAO warm-up. Compare final test performance to isolate the offline RL contribution.

2. **Reward shaping exploration**: Systematically test alternative reward functions beyond binary execution + syntax penalty, including partial credit for semantically correct SQL, execution time penalties, or learned reward models. Compare to baseline to validate the "simple rewards, strong reasoning" claim.

3. **Sample efficiency validation**: Run head-to-head comparisons where RLVR and non-RLVR models (both with identical self-consistency settings) are evaluated across different sample counts (1, 3, 7, 15). Measure accuracy improvement per additional sample to quantify the claimed efficiency gains.