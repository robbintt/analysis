---
ver: rpa2
title: Explainable Probabilistic Machine Learning for Predicting Drilling Fluid Loss
  of Circulation in Marun Oil Field
arxiv_id: '2511.06607'
source_url: https://arxiv.org/abs/2511.06607
tags:
- drilling
- loss
- circulation
- lost
- predictive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops an explainable probabilistic machine learning
  framework using Gaussian Process Regression (GPR) to predict drilling fluid loss
  of circulation in the Marun oil field. The GPR model captures complex nonlinear
  relationships between drilling parameters while quantifying predictive uncertainty,
  providing enhanced reliability for high-risk decision-making.
---

# Explainable Probabilistic Machine Learning for Predicting Drilling Fluid Loss of Circulation in Marun Oil Field

## Quick Facts
- arXiv ID: 2511.06607
- Source URL: https://arxiv.org/abs/2511.06607
- Reference count: 22
- Predicts drilling fluid loss of circulation in Marun oil field using Gaussian Process Regression

## Executive Summary
This study presents an explainable probabilistic machine learning framework using Gaussian Process Regression (GPR) to predict drilling fluid loss of circulation in the Marun oil field. The approach captures complex nonlinear relationships between drilling parameters while quantifying predictive uncertainty, providing enhanced reliability for high-risk decision-making. The integration of probabilistic learning and interpretable AI delivers accurate predictions with quantified confidence and actionable insights into feature influence, supporting proactive well control and data-driven drilling optimization.

## Method Summary
The framework employs Gaussian Process Regression with an exponential covariance function to model the relationship between drilling parameters and mud loss severity. The model is trained on 2,820 samples with 18 features including drilling parameters, formation properties, and fluid characteristics. Hyperparameters are optimized using the L-BFGS algorithm to maximize log-likelihood. Local Interpretable Model-agnostic Explanations (LIME) are applied to identify key influencing features, with global importance calculated via mean absolute contribution and weighted average methods. The Savitzky–Golay filter is used for data denoising, though specific parameters are not provided.

## Key Results
- GPR model demonstrates strong agreement between predictions and actual mud loss severity
- Higher uncertainty is observed for unfamiliar operating conditions
- LIME analysis identifies flow rate, pore pressure, pump pressure, and mud rheology indices as the most influential features
- Some features act as causative factors while others serve as mitigating factors

## Why This Works (Mechanism)
The GPR framework captures complex nonlinear relationships between drilling parameters and fluid loss severity through probabilistic modeling. The exponential covariance function encodes similarity between data points, while L-BFGS optimization ensures optimal hyperparameter selection. LIME provides local interpretability by approximating the model's behavior around specific instances, revealing which features drive predictions.

## Foundational Learning
- **Gaussian Process Regression**: Probabilistic regression method that provides uncertainty estimates alongside predictions; needed for quantifying confidence in high-stakes drilling decisions
- **Exponential Covariance Function**: Encodes similarity between data points based on distance; quick check: verify positive definiteness of kernel matrix
- **L-BFGS Optimization**: Quasi-Newton method for maximizing log-likelihood; needed to find optimal hyperparameters efficiently
- **LIME Explainability**: Local approximation technique for interpreting black-box models; quick check: validate that feature weights sum to 1 for each instance
- **Savitzky–Golay Filtering**: Polynomial smoothing technique for noise reduction; needed to improve data quality before modeling

## Architecture Onboarding

**Component Map:**
Data Preparation -> GPR Training -> LIME Interpretation -> Feature Analysis

**Critical Path:**
The core workflow follows: data preprocessing (filtering and splitting) → GPR training with hyperparameter optimization → model evaluation with uncertainty quantification → LIME-based feature interpretation → aggregation of global feature importance scores.

**Design Tradeoffs:**
The probabilistic approach sacrifices some predictive accuracy compared to deterministic models but gains crucial uncertainty quantification. The choice of exponential covariance balances expressiveness with computational efficiency. LIME provides local explanations but requires careful sampling parameter selection.

**Failure Signatures:**
- Matrix inversion failures during GPR training (mitigated by adding jitter to covariance matrix)
- Local minima convergence in L-BFGS optimization (mitigated by careful initialization)
- Unstable LIME explanations due to insufficient perturbation samples

**3 First Experiments:**
1. Test GPR with diagonal jitter addition (10^-6 to 10^-4) when Cholesky decomposition fails
2. Verify L-BFGS optimization by initializing with Table II values and checking log-likelihood improvement
3. Assess Savitzky–Golay filter impact by testing different window sizes (11, 15, 21) and polynomial orders (2, 3)

## Open Questions the Paper Calls Out
None

## Limitations
- Missing Savitzky–Golay filter parameters (window size and polynomial order) affect exact reproducibility
- Train/test split ratio not specified, impacting model validation consistency
- LIME sampling parameters (perturbation count and kernel width) not detailed, affecting explainability results

## Confidence
- **High Confidence**: GPR model architecture, covariance function formulation, L-BFGS optimization approach, and LIME explainability methodology are all precisely specified and verifiable
- **Medium Confidence**: Hyperparameter values provided in Table II should yield a functional model, though exact numerical match depends on data preprocessing specifics
- **Low Confidence**: Absolute numerical reproduction requires the missing preprocessing parameters and data split details

## Next Checks
1. Verify matrix inversion stability by testing GPR with diagonal jitter addition (10^-6 to 10^-4) when Cholesky decomposition fails
2. Confirm L-BFGS optimization convergence by initializing with Table II values and checking if log-likelihood improves or matches reported optimum
3. Test Savitzky–Golay filter sensitivity by running the pipeline with different window sizes (11, 15, 21) and polynomial orders (2, 3) to assess impact on final results