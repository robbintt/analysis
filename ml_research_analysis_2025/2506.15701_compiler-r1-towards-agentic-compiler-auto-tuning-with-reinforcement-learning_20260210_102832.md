---
ver: rpa2
title: 'Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning'
arxiv_id: '2506.15701'
source_url: https://arxiv.org/abs/2506.15701
tags:
- pass
- compiler
- tool
- optimization
- count
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Compiler-R1 is the first reinforcement learning framework for LLM-based
  compiler auto-tuning, addressing the lack of high-quality reasoning datasets and
  limited environment interaction. It introduces a curated dataset of 19,603 samples
  and a two-stage training pipeline combining supervised fine-tuning and reinforcement
  learning.
---

# Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.15701
- Source URL: https://arxiv.org/abs/2506.15701
- Reference count: 40
- Key outcome: First RL framework for LLM-based compiler auto-tuning, achieving 8.46% IR instruction reduction vs opt -Oz baseline with 96.71% success rate

## Executive Summary
Compiler-R1 introduces the first reinforcement learning framework for LLM-based compiler auto-tuning, addressing the challenge of optimizing LLVM pass sequences to reduce IR instruction count. The framework combines a curated dataset of 19,603 samples with a two-stage training pipeline (SFT + RL) that enables efficient environment exploration and learning through outcome-based rewards. Experiments across seven benchmarks demonstrate that Compiler-R1 outperforms traditional autotuners and non-interactive SFT models, achieving an average 8.46% reduction in IR instruction count compared to the opt -Oz baseline while maintaining a 96.71% task success rate.

## Method Summary
Compiler-R1 employs a two-stage training pipeline: Stage 1 uses supervised fine-tuning on 800 curated samples to teach the LLM the tool interaction protocol (thought-tool-answer format), while Stage 2 applies reinforcement learning (GRPO/PPO/RPP) to optimize the policy through 19,000+ episodes of environment interaction. The agent uses AutoPhase's 56 statistical features as input representation and interacts with the environment through two tools: `instrcount` for evaluating candidate sequences and `find_best_pass_sequence` for guided search when initial candidates fail. The reward function combines format compliance (1.5 points if valid) with optimization outcome (α × OverOrig), where OverOrig measures IR reduction relative to unoptimized code.

## Key Results
- Achieves 8.46% average IR instruction reduction versus opt -Oz baseline
- Maintains 96.71% task success rate across seven benchmarks
- Outperforms SFT-only models requiring N=40 sampling attempts (55s) with single-shot optimization in 26s
- AutoPhase features yield performance comparable to raw IR inputs (OverOrig: 0.4837 vs 0.4795)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage training (SFT → RL) outperforms either stage alone for compiler auto-tuning agents
- Mechanism: SFT provides cold-start initialization by teaching the LLM the interaction protocol (thought-tool-answer trajectory format, tool invocation syntax). RL then optimizes the policy beyond imitation by receiving outcome-based rewards from actual compilation results, enabling discovery of strategies not present in training data
- Core assumption: The interaction protocol learned via SFT transfers to RL exploration, and the reward signal is sufficiently dense to guide policy improvement
- Evidence anchors:
  - [abstract]: "two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward"
  - [section 4.3]: "Ablation results also confirm that neither SFT-only nor RL-only models are sufficient alone: only the two-stage SFT+RL pipeline consistently achieves high interaction reliability"
  - [corpus]: Limited direct corpus evidence on this specific two-stage mechanism for compiler tuning; related work (AutoPhase) uses RL-only with LSTM, showing lower success rates

### Mechanism 2
- Claim: Tool-augmented interaction enables adaptive refinement that non-interactive models cannot achieve
- Mechanism: The LLM agent invokes `instrcount` to evaluate candidate sequences and `find_best_pass_sequence` when initial candidates fail. This feedback loop allows the agent to correct poor predictions within a single inference episode, rather than requiring N independent sampling attempts
- Core assumption: The agent correctly follows the protocol (valid format, proper tool calls); otherwise, fallback to `-Oz` degrades performance
- Evidence anchors:
  - [section 3.2.1]: "Two key tools are integrated into the SFT process: instrcount [and] find_best_pass_sequence"
  - [section 4.3]: "SFT-only models, lacking interaction feedback, require extensive sampling to perform well... demonstrating their reliance on stochastic inference rather than adaptive refinement"
  - [section 4.2]: GRPO-7B achieves 8.46% OverOz in 26s vs SFT-Qwen-7B at 4.66% requiring N=40 attempts (55s)
  - [corpus]: Related compiler auto-tuning work (GRACE, Synergy-Guided) uses evolutionary/search methods without explicit tool-augmented LLM interaction

### Mechanism 3
- Claim: AutoPhase features provide a compact representation sufficient for pass sequence prediction, matching raw IR performance
- Mechanism: AutoPhase's 56 statistical features capture structural properties (BB counts, instruction type distributions, control-flow patterns) that abstract away syntactic noise (variable names, ordering). This enables fixed-length input regardless of program size, avoiding context window limits
- Core assumption: The 56 features preserve sufficient information about optimization-relevant program structure; no critical information is lost in compression
- Evidence anchors:
  - [section 3.1]: "AutoPhase's 56 statistical features... capturing structural properties such as type distributions and control-flow properties"
  - [section 4.4]: "AutoPhase features yield performance closely comparable to raw IR inputs... OverOrig score of 0.4837 [AutoPhase] vs 0.4795 [raw IR] at N=40"
  - [corpus]: AutoPhase (Haj-Ali et al., 2020) is cited as the source; corpus neighbor "Synergy-Guided Compiler Auto-Tuning" also references AutoPhase features for representation

## Foundational Learning

- Concept: **Reinforcement Learning with LLMs (RLHF/PPO/GRPO)**
  - Why needed here: The Stage 2 training uses PPO, GRPO, or REINFORCE++ to update the LLM policy based on reward signals. Understanding on-policy RL, advantage estimation, and policy gradients is essential for debugging training instability
  - Quick check question: Can you explain why PPO's clipped objective prevents excessive policy updates compared to vanilla REINFORCE?

- Concept: **LLM Tool-Use / Function Calling**
  - Why needed here: The agent must generate structured JSON tool calls (`instrcount`, `find_best_pass_sequence`) and parse responses. Format adherence is explicitly rewarded (R_format = 1.5 if correct, 0 otherwise)
  - Quick check question: How would you handle a case where the LLM generates syntactically invalid JSON for a tool call?

- Concept: **Compiler Intermediate Representation (IR) and Optimization Passes**
  - Why needed here: The task involves selecting sequences from 124 LLVM passes to reduce IR instruction count. Understanding what passes do (e.g., `mem2reg`, `dse`, `licm`) helps interpret model decisions and debug failures
  - Quick check question: Why might applying `-Oz` as a fallback be safer than an arbitrary pass sequence?

## Architecture Onboarding

- Component map:
Input: AutoPhase features (56-dim) + initial instruction count → LLM Backbone (Qwen2.5-Instruct: 1.5B/3B/7B) → generates Thought blocks (reasoning) + Tool calls (JSON) → Environment (CompilerGym/LLVM) → returns Tool response (status, improvement metrics) → Reward computation: R_final = w_f × R_format + w_a × R_answer → Policy update (GRPO/PPO/RPP) → Output: Final pass sequence in <answer> tags

- Critical path:
  1. **SFT data construction** (Section 3.1): Synergy pass pairs → Global Synergy Graph → optimal sequences → simulated CoT trajectories. Errors here propagate to RL stage
  2. **Format reward enforcement** (Section 3.2.2): Without proper R_format, the agent may generate unparseable outputs, triggering fallback and wasting exploration budget
  3. **Reward scaling (α)**: OverOrig must provide distinguishable signal; if most programs yield near-zero improvement, RL gradients become noisy

- Design tradeoffs:
  - **AutoPhase vs. Raw IR**: AutoPhase enables fixed-length input and faster inference but may lose semantic detail. Use raw IR for debugging specific failure cases
  - **Model scale vs. latency**: GRPO-7B achieves best performance (8.46%) but requires 26s/program; GRPO-1.5B is faster (20s) with lower performance (3.87%)
  - **Repetition penalty tuning**: Table 2 shows success rate sensitivity (GRPO-7B: 51.92% → 96.71% with penalty adjustment)

- Failure signatures:
  - **Low success rate + high OverOz**: Agent bypassing protocol (e.g., GRPO-3B directly calling `find_best_pass_sequence` without `instrcount` validation). Check for format violations
  - **High success rate + low OverOz**: Agent over-conservative, defaulting to `-Oz`. Check if reward signal is too weak or exploration insufficient
  - **RL training divergence**: Loss spikes, reward collapse. Check learning rate, advantage normalization, and reward scaling factor α

- First 3 experiments:
  1. **Reproduce SFT-only baseline**: Train Qwen-1.5B on the 800-sample protocol initialization, evaluate with N=1 and N=40 sampling. Confirm reported OverOz (~3.15% at N=40)
  2. **Ablate RL-only (no SFT)**: Train GRPO-1.5B from scratch without SFT initialization. Expect low success rate (~8.92% per Table 2), confirming SFT necessity
  3. **AutoPhase vs. Raw IR comparison**: Train two SFT models (Qwen-1.5B) with identical data but different input representations. Evaluate OverOrig at N=1, N=40. Expect near-equivalent performance (~0.48)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a multi-turn dialogue approach, where the agent applies passes incrementally based on intermediate feedback, outperform the current single-trajectory generation method?
- Basis in paper: [explicit] The conclusion states the intention to "explore the efficacy of interactive, turn-by-turn pass application dialogues that could simulate traditional RL auto-tuning paradigms"
- Why unresolved: The current Compiler-R1 framework generates a sequence in a single trajectory or via a tool-invoked search, rather than a multi-step, stateful dialogue
- What evidence would resolve it: A comparative study measuring the convergence rate and final OverOz% of a dialogue-based agent against the current Compiler-R1 implementation

### Open Question 2
- Question: How does the AutoPhase feature representation impact the ceiling of RL policy optimization compared to using raw LLVM IR context?
- Basis in paper: [inferred] Experiment 3 compares AutoPhase vs. Raw IR for *SFT only*, finding comparable results, but the *RL* training pipeline exclusively uses AutoPhase features
- Why unresolved: It is unknown if the RL agent's performance is bottlenecked by the information loss in the 56-dimensional AutoPhase abstraction during policy updates
- What evidence would resolve it: Training the RL stage with raw IR inputs (utilizing larger context windows) and comparing the optimization performance against the feature-based RL agent

### Open Question 3
- Question: To what extent do pass sequences optimized by Compiler-R1 for IR instruction count transfer to improvements in actual runtime execution speed?
- Basis in paper: [inferred] The paper focuses exclusively on "Intermediate Representation (IR) instruction count" as the optimization objective, acknowledging "performance metrics" generally but validating only this one
- Why unresolved: Instruction count reduction is a proxy for optimization; it does not always correlate with runtime speed due to factors like cache locality or instruction pipelining
- What evidence would resolve it: Benchmarking the execution time of binaries optimized by Compiler-R1 against the `opt -Oz` baseline on standard hardware

## Limitations
- Unknown hyperparameters (learning rates, batch sizes, clip ratios) for GRPO/PPO/RPP algorithms
- Reward weights and scaling factor α not fully specified
- Implementation details of find_best_pass_sequence tool (search budget, random walk parameters) unclear
- GRPO-3B model bypassed protocol checks yet achieved high OverOz, suggesting potential shortcut learning

## Confidence
- **High**: Compiler-R1 framework architecture, SFT+RL two-stage pipeline necessity (confirmed by ablation), AutoPhase feature viability
- **Medium**: Absolute performance numbers (OverOz, success rates), optimal hyperparameter settings, generalization across program domains
- **Low**: Long-term stability of RL training, robustness to different LLVM versions, scalability beyond 10k-instruction programs

## Next Checks
1. **Protocol Compliance Audit**: Run Compiler-R1 with detailed logging of tool call formats and fallback triggers to quantify how often the agent bypasses validation steps versus following proper interaction protocol
2. **Cross-Domain Generalization**: Evaluate Compiler-R1 on programs from domains not represented in the training set (e.g., cryptographic code, numerical kernels) to test robustness beyond the curated benchmarks
3. **Reward Signal Sensitivity**: Systematically vary the scaling factor α in the reward function to determine the threshold below which RL training fails to converge, validating the claim about reward density requirements