---
ver: rpa2
title: Organ-Aware Attention Improves CT Triage and Classification
arxiv_id: '2601.13385'
source_url: https://arxiv.org/abs/2601.13385
tags:
- organ
- masked
- auroc
- attention
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the need for accurate and interpretable triage
  and classification of high-volume medical imaging, specifically computed tomography
  (CT). The core method, ORACLE-CT, is an encoder-agnostic, organ-aware head that
  combines organ-masked attention with organ-scalar fusion.
---

# Organ-Aware Attention Improves CT Triage and Classification

## Quick Facts
- **arXiv ID**: 2601.13385
- **Source URL**: https://arxiv.org/abs/2601.13385
- **Reference count**: 40
- **Key outcome**: ORACLE-CT achieves state-of-the-art supervised AUROC of 0.86 on chest CT (CT-RATE) and 0.85 on abdomen CT (MERLIN) using organ-masked attention with scalar fusion.

## Executive Summary
This work addresses the need for accurate and interpretable triage and classification of high-volume medical imaging, specifically computed tomography (CT). The core method, ORACLE-CT, is an encoder-agnostic, organ-aware head that combines organ-masked attention with organ-scalar fusion. Organ-masked attention uses segmentation masks to restrict softmax pooling to clinically relevant organ regions, while organ-scalar fusion appends simple, mask-derived scalars (e.g., normalized volume, mean HU, border flag) to improve classification. This design yields spatially localized, interpretable weight maps and calibrated predictions.

## Method Summary
ORACLE-CT is an encoder-agnostic, organ-aware classification head that operates on CT feature maps. It uses segmentation masks to constrain attention pooling to specific organs, then fuses spatial embeddings with simple mask-derived scalars (volume, mean HU, border flag) for improved classification. The method supports multiple backbones (2.5D ViT or 3D CNN) and is trained with a multi-label, uncertain-label loss. Organ-specific classifier heads produce spatially localized, interpretable attention maps.

## Key Results
- On chest CT (CT-RATE), ORACLE-CT’s masked attention model achieves an AUROC of 0.86.
- On abdomen CT (MERLIN), the supervised baseline with masked attention plus scalar fusion reaches an AUROC of 0.85.
- The approach establishes state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Restricting attention pooling to organ masks improves classification by reducing off-organ noise and localizing evidence.
- Mechanism: A unary scorer produces per-location attention scores, then softmax normalization is constrained to voxels/tokens inside a dilated organ mask. This yields organ-specific embeddings where attention weights sum to 1 within the organ, not globally.
- Core assumption: Segmentation masks correctly delineate clinically relevant regions, and disease evidence is spatially concentrated within those regions.
- Evidence anchors: [abstract] "Organ-masked attention uses segmentation masks to restrict softmax pooling to clinically relevant organ regions"; [section 3.3.2] "wo,i = exp(˜αo,i/τo)mo,i / Σj∈Ωo exp(˜αo,j/τo)" — attention normalized within organ support Ωo.

### Mechanism 2
- Claim: Appending simple mask-derived scalars (volume, mean HU, border flag) improves findings sensitive to size or density.
- Mechanism: After masked pooling, a compact vector of z-scored volume and mean HU is concatenated to the organ embedding. A learned gate down-weights spatial features when the organ is truncated at the scan boundary, shifting reliance toward scalars.
- Core assumption: Volume and mean HU are informative proxies for pathology; the fusion MLP can learn non-linear interactions between spatial and scalar features.
- Evidence anchors: [abstract] "organ-scalar fusion appends simple, mask-derived scalars (e.g., normalized volume, mean HU, border flag) to improve classification"; [section 4.5/B] "Volume tends to help size/morphology findings (e.g. cardiomegaly, hepatomegaly, splenomegaly), while HU favors intensity/density patterns (e.g. calcifications, thrombosis)".

### Mechanism 3
- Claim: Per-organ classifier heads with fixed label-to-organ anchoring outperform global classifiers by isolating label-specific evidence.
- Mechanism: Each disease label is pre-assigned to exactly one organ group. A bank of lightweight classifier heads processes only the corresponding organ embedding, preventing cross-organ interference and enabling interpretable organ-level attention maps.
- Core assumption: Labels can be unambiguously anchored to organ groups; multi-organ diseases or systemic conditions are handled by a separate "other" head.
- Evidence anchors: [section 3.3.2] "Each study-level label ℓ ∈ L is anchored to exactly one group via a fixed map κ: L → O ∪ {other}, yielding disjoint label sets {Lo}o∈O"; [figure 3] Visualization shows lung findings produce lung-confined attention, renal findings produce kidney-confined attention.

## Foundational Learning

- **Softmax pooling (attention over sets)**:
  - Why needed here: Organ-masked attention uses softmax over spatial locations within an organ mask. Understanding that softmax normalizes over a set (not globally) is essential for grasping why mask-restriction changes the pooling distribution.
  - Quick check question: If softmax is computed over 1000 voxels in a lung mask vs. 10,000 voxels in the full volume, how does the attention distribution differ?

- **Multiple Instance Learning (MIL) formulation**:
  - Why needed here: ORACLE-CT treats each organ as a "bag" of instances (voxels/tokens) and learns to pool them into a single embedding. No per-voxel labels are needed—only study-level disease labels.
  - Quick check question: Why does MIL allow training with only study-level labels while still localizing evidence spatially?

- **Encoder-agnostic feature lattices**:
  - Why needed here: The head operates on {ui}i∈Ω—a flattened lattice of tokens or voxels—regardless of whether the backbone is a 2.5D ViT (DINOv3) or 3D CNN (ResNet, MedNeXt). Understanding this abstraction is key to plugging in new encoders.
  - Quick check question: Given a new encoder that outputs [B, D, H, W, d] features, what preprocessing is needed before the ORACLE-CT head?

## Architecture Onboarding

- **Component map**: Encoder backbone → Token lattice (2.5D ViT) or voxel lattice (3D CNN) → outputs {ui} → Mask projector → Resamples 3D organ masks to encoder resolution → produces mo,i indicators → Organ-masked attention head (per organ group): Unary scorer → masked softmax → weighted pooling → organ embedding ho → Scalar extractor: Computes volume, mean HU, border flag from masks → uo → Fusion classifier: [ho; uo] → linear/MLP → logits for labels in that organ group → "Other" head: Global attention (mask-free) for non-localizable labels.

- **Critical path**: CT volume → preprocessing (HU clip, normalize) → encoder features → Organ masks → dilation → resampling to feature grid → For each organ group: masked softmax pooling → scalar extraction → fusion → classification → Assemble per-organ logits into final L-dimensional output.

- **Design tradeoffs**:
  - **Mask vs. Bounding Box**: Mask is better for diffuse findings; BBox better for small/tubular or peri-organ context (e.g., appendicitis, biliary ducts). Choice is class-dependent—validate per label.
  - **Frozen vs. unfrozen backbone**: Unfreezing helps multi-organ and morphology-driven findings but adds compute and risk of overfitting. Paper uses unfrozen for main results.
  - **Dilation radius**: Larger dilation absorbs boundary uncertainty but may include irrelevant tissue. Paper uses organ-specific radii (e.g., 2mm lungs, 5mm kidneys).

- **Failure signatures**:
  - Empty mask at pooling resolution → falls back to uniform global pooling (silent failure mode)
  - High FP rate on organs with strong attention but incorrect prediction → suggests scalar or contextual features are misleading, not just pooling
  - Large gap between GAP and masked attention on a class → check if mask quality is poor or label-to-organ mapping is wrong

- **First 3 experiments**:
  1. **Reproduce GAP baseline**: Run GAP head on MERLIN/CT-RATE with provided preprocessing and splits. Verify AUROC matches paper (e.g., DINOv3 GAP on MERLIN ≈ 0.83).
  2. **Ablate masked vs. global attention**: On a single organ group (e.g., liver), compare GAP → Global Attention → Masked Attention. Inspect attention heatmaps for anatomical localization.
  3. **Validate scalar fusion on size-driven findings**: For cardiomegaly/hepatomegaly/splenomegaly, compare Masked vs. Masked+Scalar. Expect AUROC lift; if not, check if volume scalars are correctly z-scored and organ masks are accurate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic estimation of mask reliability and adaptive region selection (switching between segmentation masks, bounding boxes, or expanded regions) mitigate performance degradation caused by segmentation errors in small or tubular structures?
- Basis in paper: [explicit] The "Future Work" section explicitly proposes investigating "Mask reliability & fallbacks" to "estimate mask quality and marginalize/switch across mask, padded box, or expanded regions."
- Why unresolved: The current method relies on fixed pre-processing masks which propagate errors, and while ablations show bounding boxes help specific classes (e.g., appendicitis), a unified mechanism to switch regions based on estimated mask quality remains undeveloped.
- What evidence would resolve it: Implementation of a mask quality estimator that triggers region fallbacks, resulting in recovered AUROC performance on classes previously identified as sensitive to segmentation fidelity.

### Open Question 2
- Question: Does replacing uniform scalar fusion with "disease-aware, gated scalars" (e.g., robust intensity summaries or sparse per-label gates) improve the detection of focal lesions compared to the current mean-HU and volume approach?
- Basis in paper: [explicit] The "Future Work" section lists "Disease-aware, gated scalars" as a priority, and the "Limitations" section notes that the current uniform fusion of mean HU "can dilute focal signal."
- Why unresolved: The paper demonstrates that simple scalars help size/morphology-driven findings, but the authors explicitly hypothesize that learned, sparse gates are needed to prevent diluting signals from focal targets.
- What evidence would resolve it: An ablation study comparing the existing Organ-Scalar Fusion (OSF) against a gated scalar mechanism, showing statistically significant improvements in AUPRC for focal findings like renal cysts or nodules.

### Open Question 3
- Question: Do the organ-masked attention maps provide faithful causal evidence for model predictions, or do they merely offer correlated post-hoc localization despite the organ constraints?
- Basis in paper: [explicit] The "Limitations" section states that "Attention maps aid auditability but are not guaranteed faithful," and "Future Work" proposes "deletion–insertion curves" and "counterfactual mask edits" to test this.
- Why unresolved: While qualitative figures show spatially localized heatmaps, the paper acknowledges these are "supportive evidence rather than causal attributions," leaving their robustness as an audit tool unverified.
- What evidence would resolve it: Quantitative faithfulness metrics (e.g., sufficiency/comprehensiveness scores) derived from deletion-insertion experiments on the held-out test sets.

## Limitations

- **Mask quality dependency**: Performance degrades when organ segmentation masks are inaccurate, especially for small or truncated organs, leading to silent fallback to global pooling.
- **Limited multi-organ handling**: The fixed label-to-organ mapping struggles with multi-organ or systemic diseases, which are relegated to a less anatomically-informed "other" head.
- **Unverified model details**: Exact pretrained model versions and some training details (e.g., batch size, GPU count) are not fully specified, which could hinder exact reproduction.

## Confidence

- **High Confidence**: The core mechanism of organ-masked attention (Mechanism 1) is well-supported by the described equations and experimental results, particularly the AUROC scores on CT-RATE and MERLIN datasets. The claim that restricting attention to organ masks reduces off-organ noise and localizes evidence is consistent with the mathematical formulation and ablation studies.
- **Medium Confidence**: The effectiveness of organ-scalar fusion (Mechanism 2) is supported by the observed AUROC improvements on specific size-driven findings (e.g., cardiomegaly, hepatomegaly). However, the general applicability of this mechanism across all pathology types is less certain, as the evidence is primarily focused on a subset of findings.
- **Medium Confidence**: The design choice of per-organ classifier heads (Mechanism 3) is logically sound and aligns with the goal of interpretability. The fixed label-to-organ mapping is a practical design decision, but its limitations for multi-organ diseases are acknowledged and represent a known constraint.

## Next Checks

1. **Segmentation Quality Impact**: Systematically evaluate the performance of ORACLE-CT as a function of organ segmentation quality. Introduce varying levels of segmentation noise or use alternative segmentation models to quantify the degradation in AUROC and identify the threshold at which masked attention fails to outperform global pooling.

2. **Multi-Organ Disease Handling**: Develop and validate an extension to ORACLE-CT that can handle multi-organ diseases more effectively. This could involve a multi-label organ assignment strategy or a dedicated head that learns spatial relationships between organs, and test its performance on datasets with known multi-organ pathologies.

3. **Scalar Fusion Ablation Across Pathology Types**: Conduct a detailed ablation study of the organ-scalar fusion component across a broader range of pathology types, including focal lesions, calcifications, and thrombosis. Compare the performance of Masked+OSF against Masked alone for each finding to identify which classes benefit most and which may be harmed by the inclusion of global organ statistics.