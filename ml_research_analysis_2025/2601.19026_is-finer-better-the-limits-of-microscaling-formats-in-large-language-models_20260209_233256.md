---
ver: rpa2
title: Is Finer Better? The Limits of Microscaling Formats in Large Language Models
arxiv_id: '2601.19026'
source_url: https://arxiv.org/abs/2601.19026
tags:
- block
- quantization
- scales
- scale
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Microscaling quantization formats enable aggressive model compression
  by grouping elements into blocks with shared scales, but this study reveals a counterintuitive
  limitation: reducing block size can paradoxically increase quantization error for
  narrow tensor distributions. This phenomenon, driven by the interaction between
  narrow distributions and limited scale dynamic range, was experimentally observed
  across multiple large language models and theoretically modeled using a framework
  based on normal distribution approximations.'
---

# Is Finer Better? The Limits of Microscaling Formats in Large Language Models

## Quick Facts
- arXiv ID: 2601.19026
- Source URL: https://arxiv.org/abs/2601.19026
- Reference count: 40
- Finer microscaling block sizes can paradoxically increase quantization error for narrow tensor distributions due to scale quantization effects.

## Executive Summary
This study reveals a counterintuitive limitation in microscaling quantization: reducing block size from 16 to 8 elements can increase quantization error for narrow tensor distributions (σ < 2×10⁻²) when using FP8 UE4M3 scales. The phenomenon arises because smaller blocks amplify the impact of quantized scale factors, particularly for block maximum elements and zero-rounding cases. The authors develop a theoretical framework decomposing quantization error into three sources and validate it across multiple LLM architectures. To address this, they propose FP8 UE5M3 scales that extend dynamic range without requiring per-tensor scaling operations, achieving comparable accuracy to conventional approaches with minimal hardware overhead.

## Method Summary
The paper develops a theoretical framework for microscaling quantization error by approximating weight and activation distributions as Normal distributions. They analyze three error contributions: quantization of non-maximum elements, quantization of maximum elements, and zero-scale rounding. Experimental validation uses granite-3.3-8b, llama-2-7b, and other models with FP4 element formats and FP8 scale formats (UE4M3 and UE5M3). The UE5M3 format extends exponent range from 4 to 5 bits while maintaining 3-bit mantissa, enabling finer scale quantization without per-tensor scaling overhead.

## Key Results
- Perplexity inversion occurs when block size decreases from 16 to 8 for narrow distributions (σ < 2×10⁻²)
- The crossover threshold at σ≈2×10⁻² is consistent across multiple LLM architectures
- UE5M3 scales achieve comparable performance to UE4M3-S without per-tensor scaling overhead
- Zero-rounding becomes significant for distributions with σ < 0.5×10⁻²

## Why This Works (Mechanism)

### Mechanism 1
Decreasing microscaling block size can increase quantization error for narrow tensor distributions when using FP8 UE4M3 scales. Three coupled error sources emerge from scale quantization: (1) non-maximum elements incur quantization error scaled by the discretized scale; (2) the block maximum element now has non-zero error because the scale itself is quantized; (3) entire blocks round to zero when their maximum falls below s_min/2. As block size decreases, contributions (2) and (3) increase in relative weight, overwhelming the benefit of finer granularity.

### Mechanism 2
Narrow distributions (σ < 2×10⁻²) are disproportionately affected by limited scale dynamic range. When tensor standard deviation is low, block maxima cluster near small values. FP8 UE4M3's minimum non-zero scale is 2⁻⁹. Blocks with maxima below this threshold cannot be accurately scaled, causing either zero-rounding or large relative errors on the maximum element itself.

### Mechanism 3
Repurposing FP8's unused sign bit as an additional exponent bit (UE5M3) extends dynamic range sufficiently to eliminate perplexity inversion without per-tensor scaling. UE5M3 shifts minimum representable scale from 2⁻⁹ to 2⁻¹⁷, an 8× dynamic range extension. This allows accurate representation of low-magnitude block scales without global scaling operations. Hardware impact is minimal because mantissa processing remains at 3 bits.

## Foundational Learning

- **Block-wise microscaling quantization**: Why needed here: The entire paper's anomaly stems from how scale factors are derived per-block and then quantized. Understanding that scales = Q_scale(x_max/C) is essential.
  - Quick check question: For a block of 16 elements with maximum 0.001, what happens when quantizing to FP8 UE4M3 scale?

- **Floating-point format structure (E/M notation)**: Why needed here: The proposal hinges on understanding what E4M3 vs E5M3 means—4 vs 5 exponent bits, 3 mantissa bits—and how this affects dynamic range.
  - Quick check question: Why does adding one exponent bit extend minimum representable value from 2⁻⁹ to 2⁻¹⁷?

- **Quantization error decomposition**: Why needed here: The theoretical framework separates error into three distinct sources. Without this decomposition, the non-monotonic behavior is inexplicable.
  - Quick check question: Why does the block maximum element have zero error when scales are infinite precision, but non-zero error when scales are quantized?

## Architecture Onboarding

- **Component map**: Element quantizer -> Scale quantizer -> Block partitioner -> Dequantizer -> Per-tensor scaler (optional)
- **Critical path**: 1) Measure x_max per block, 2) Compute and quantize scale: s = Q_FP8(x_max / C), 3) Normalize and quantize elements: q = Q_FP4(x / s), 4) Store/transfer compressed {q, s} pairs, 5) Dequantize for compute: ŷ = s·q
- **Design tradeoffs**: Block size vs. scale storage: Smaller blocks reduce element quantization error but increase scale overhead (4/(N+4) storage increase per halving); UE4M3 vs UE5M3: UE5M3 adds 0.5% area, 4ps timing; eliminates need for per-tensor scaling and dynamic absmax computation; Per-tensor scaling: Effective but requires on-the-fly absmax (activations) or calibration (weights), plus outlier sensitivity
- **Failure signatures**: Perplexity inversion: Upward curve at small block sizes in perplexity-gap plots; Per-block MSE inversion: >25% of blocks show higher error at bs=8 vs bs=16; Zero-rounded blocks: High MSE spikes for tensors with σ < 0.5×10⁻²; Model-dependent crossover: granite-3.3-8b inverts at bs=16, llama-2-7b doesn't invert until bs<8
- **First 3 experiments**: 1) Reproduce the anomaly: Quantize granite-3.3-8b weights with FP4/UE4M3 at block sizes 256→8; plot perplexity gap. Expected: non-monotonic with upswing at bs≤16. 2) Isolate scale quantization impact: Same experiment with BF16 scales. Expected: monotonic decrease, confirming scale quantization as root cause. 3) Validate UE5M3 fix: Implement UE5M3 scale quantization (5-bit exponent, 3-bit mantissa) and compare perplexity at bs=8 against UE4M3-S. Expected: comparable performance without per-tensor scaling overhead.

## Open Questions the Paper Calls Out

### Open Question 1
Does the perplexity inversion anomaly manifest during quantization-aware training, and can UE5M3 scales mitigate training instability? The paper focuses exclusively on inference quantization; training dynamics with microscaling formats remain unexplored despite the authors noting the need for "efficient training and inference." Experiments quantizing weights and activations during training with block sizes 8-16, comparing convergence curves between UE4M3 and UE5M3 scales would resolve this.

### Open Question 2
How does the theoretical framework predict MSE behavior for sub-4-bit element formats (e.g., FP2, INT2) combined with sub-8-bit scales? The authors state the framework "can play a role in analyzing the impact on the quantization error of scaling down precision, to sub-4-bit element formats, sub-8-bit scales, and smaller block sizes." Only FP4, INT4, and FP8/FP6 scales were validated; no sub-4-bit element experiments were conducted.

### Open Question 3
Can adaptive or hybrid block size strategies eliminate perplexity inversion by assigning larger blocks to narrow-distribution tensors? The paper identifies that narrow distributions (σ < 2×10⁻²) trigger inversion at block size 8, while wider distributions benefit from smaller blocks. The paper uses uniform block sizes; tensor-specific or layer-adaptive block sizing was not investigated.

## Limitations

- Theoretical framework relies on Normal distribution approximations that may not capture long-tail behaviors in some transformer architectures
- UE5M3 hardware efficiency claims depend on specific implementation details not fully explored across different manufacturing processes
- Study focuses primarily on perplexity-based evaluation; downstream task performance and robustness to input variations remain unexplored

## Confidence

**High confidence**: The quantization error decomposition framework and its three-component model are well-supported by both theoretical derivation and experimental validation across multiple models.

**Medium confidence**: The UE5M3 format extension's hardware efficiency claims depend on specific implementation details not fully explored in the paper.

**Medium confidence**: The generalization of Normal distribution assumptions to diverse LLM architectures is supported by empirical validation but lacks formal statistical testing across the full distribution of model weights and activations.

## Next Checks

1. **Distribution sensitivity analysis**: Systematically vary weight and activation distributions (uniform, Laplacian, power-law) to determine if the σ≈2×10⁻² crossover threshold holds or shifts for non-Normal distributions.

2. **Hardware implementation validation**: Prototype UE5M3 in a realistic hardware accelerator to measure actual area, power, and timing impacts across different manufacturing processes and design constraints.

3. **Robustness benchmarking**: Evaluate quantized models on adversarial inputs, domain-shifted data, and diverse downstream tasks to assess whether UE5M3's perplexity improvements translate to consistent performance gains across the full model capability spectrum.