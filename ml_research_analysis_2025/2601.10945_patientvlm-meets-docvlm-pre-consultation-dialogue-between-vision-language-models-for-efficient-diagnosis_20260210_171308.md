---
ver: rpa2
title: 'PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language
  Models for Efficient Diagnosis'
arxiv_id: '2601.10945'
source_url: https://arxiv.org/abs/2601.10945
tags:
- patient
- doctor
- dialogue
- diagnosis
- docvlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PCDF, a training framework that simulates
  doctor-patient dialogues between two vision-language models (DocVLM and PatientVLM)
  to enhance medical image diagnosis. DocVLM asks follow-up questions based on the
  image and dialogue history, while PatientVLM responds using symptoms from the ground-truth
  diagnosis.
---

# PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis

## Quick Facts
- arXiv ID: 2601.10945
- Source URL: https://arxiv.org/abs/2601.10945
- Authors: K Lokesh; Abhirama Subramanyam Penamakuri; Uday Agarwal; Apoorva Challa; Shreya K Gowda; Somesh Gupta; Anand Mishra
- Reference count: 33
- Primary result: 37.2% F1 improvement over image-only fine-tuning on medical image diagnosis

## Executive Summary
The paper introduces PCDF, a training framework that simulates doctor-patient dialogues between two vision-language models (DocVLM and PatientVLM) to enhance medical image diagnosis. DocVLM asks follow-up questions based on the image and dialogue history, while PatientVLM responds using symptoms from the ground-truth diagnosis. The resulting image-dialogue-diagnosis triplets are used to fine-tune DocVLM, improving its ability to reason from both visual and conversational cues. PCDF consistently improves F1 scores across four medical imaging benchmarks, with gains of up to 37.2% over image-only fine-tuning.

## Method Summary
PCDF operates in two phases: Dialogue Simulation and Dialogue-conditioned Fine-tuning. During simulation, DocVLM (frozen) generates follow-up questions conditioned on images and dialogue history, while PatientVLM (frozen) responds using ground-truth diagnosis symptoms without revealing the condition. This produces image-dialogue-diagnosis triplets. In the fine-tuning phase, DocVLM is trained with LoRA (rank=16, alpha=32) on these triplets to predict diagnoses from combined visual and conversational inputs. The framework uses MedMNIST v2 datasets and requires 3× A6000 GPUs with 48GB memory for training.

## Key Results
- PCDF improves F1 scores across four medical imaging benchmarks with gains up to 37.2% over image-only fine-tuning
- Dialogue length positively correlates with performance; extending from 2 to 8 turns yields +18.4% absolute F1 gain on DermaMNIST
- Small-scale clinical validation shows 96.9% of generated question-answer pairs rated as clinically relevant by licensed clinicians

## Why This Works (Mechanism)

### Mechanism 1
Simulating doctor-patient dialogues between two distinct VLMs generates training data that improves diagnostic accuracy over image-only approaches. DocVLM generates clinically relevant follow-up questions while PatientVLM responds using symptom profiles from ground-truth diagnosis, producing image-dialogue-diagnosis triplets that capture iterative clinical reasoning. The asymmetry ensures realistic exchanges where PatientVLM doesn't reveal the diagnosis. This works because the synthetic dialogues transfer to real clinical scenarios where iterative questioning reveals discriminative symptoms.

### Mechanism 2
Dialogue-conditioned fine-tuning enables DocVLM to learn P(diagnosis | image, dialogue_history), improving performance over P(diagnosis | image) models. The fine-tuning phase trains DocVLM to predict diagnoses from combined visual and conversational inputs using auto-regressive generation loss. This conditions the model on multi-turn symptom elicitation patterns. The relationship is approximately monotonic within the tested range (2-8 turns), with longer dialogues providing more diagnostic signal.

### Mechanism 3
Clinical realism of generated dialogues correlates with downstream diagnostic utility. Licensed clinicians rated 96.9% of generated question-answer pairs as clinically relevant, with symptom coverage scores of 4.5/5 and dialogue realism scores of 3.9/5. This suggests the synthetic data approximates real consultations. Expert-rated clinical relevance serves as a proxy for training utility, with higher realism scores corresponding to greater diagnostic gains.

## Foundational Learning

- Concept: Instruction-tuned VLMs
  - Why needed here: PCDF relies on VLMs that can follow structured prompts (P_doc, P_pat) to generate domain-appropriate outputs. Without instruction-following capability, the framework cannot enforce role separation or output formatting.
  - Quick check question: Can you explain how instruction tuning differs from pretraining, and why it enables controlled text generation?

- Concept: LoRA fine-tuning
  - Why needed here: The paper uses LoRA (rank=16, alpha=32) to efficiently fine-tune DocVLM on simulated dialogues. Understanding parameter-efficient fine-tuning is necessary to reproduce results with limited compute.
  - Quick check question: What are the trade-offs between full fine-tuning and LoRA in terms of memory, training time, and task performance?

- Concept: MedMNIST benchmarks
  - Why needed here: All experiments use MedMNIST v2 datasets (DermaMNIST, PneumoniaMNIST, etc.). Familiarity with these benchmarks is required to interpret F1 scores and compare against baselines.
  - Quick check question: What modalities and class imbalances characterize the four MedMNIST datasets used in this paper?

## Architecture Onboarding

- Component map: DocVLM -> Dialogue Simulator -> PatientVLM -> Dialogue Quality Validator -> DocVLM (fine-tuned)
- Critical path: (1) Load pre-trained VLMs → (2) Generate dialogues for training split → (3) Validate dialogue quality → (4) Fine-tune DocVLM → (5) Evaluate on test split with real or simulated patient inputs
- Design tradeoffs:
  - Dialogue length (T): Longer dialogues improve performance but increase generation cost. Paper uses T=8
  - PatientVLM choice: mPLUG-Owl3 achieved highest F1 (73.3 avg), but any VLM outperforms image-only baseline
  - DocVLM architecture: Generic VLMs (InternVL3) showed larger absolute gains than medical-domain VLMs (MedGemma)
- Failure signatures:
  - Label leakage: PatientVLM revealing diagnosis in responses (monitored via expert validation; 0 instances reported)
  - Technical jargon: DocVLM generating questions too complex for layperson understanding
  - Low symptom coverage: Short dialogues (T<4) may not capture sufficient discriminative information
- First 3 experiments:
  1. Reproduce baseline comparison: Run InternVL3-2B with image-only SFT vs. PCDF on DermaMNIST to verify the reported 37.2% F1 gain
  2. Ablate dialogue length: Test T ∈ {2, 4, 6, 8} to confirm monotonic improvement and identify diminishing returns point
  3. Swap PatientVLM: Compare mPLUG-Owl3 vs. Qwen2.5-VL as PatientVLM to assess sensitivity to this component choice

## Open Questions the Paper Calls Out

### Open Question 1
How does the framework perform in large-scale, rigorous clinical trials involving real-world deployment with human patients? Current evaluation relies on simulated dialogues and small-scale clinical validation of text quality, rather than live deployment outcomes with actual patients.

### Open Question 2
Can the dialogue generation process be refined to reduce overly technical follow-up questions and improve comprehension for layperson patients? Some follow-up questions generated by the DocVLM tend to be overly technical, which may be challenging for layperson patients to understand.

### Open Question 3
How can the PCDF framework be extended to support multilingual healthcare settings? The current system supports only English, limiting its usability in multilingual healthcare settings.

## Limitations
- Framework assumes ground-truth diagnoses are available for simulation, which may not be practical in real deployment scenarios where diagnosis is the goal
- Evaluation focuses exclusively on MedMNIST benchmarks, which represent relatively constrained classification tasks with uncertain translation to complex medical imaging
- Computational overhead of dialogue simulation versus performance gains is not explored, which could be significant for large-scale deployment

## Confidence

- **High confidence**: The core mechanism of dialogue simulation improving diagnostic performance is well-supported by quantitative results across four benchmarks and expert validation of dialogue quality
- **Medium confidence**: The claim that instruction-tuned VLMs are essential for controlled role separation is plausible but not rigorously tested against non-instruction-tuned alternatives
- **Low confidence**: The assertion that clinical realism directly correlates with diagnostic utility relies on indirect evidence without direct causal link between specific dialogue quality metrics and model performance gains

## Next Checks
1. **Ablation on dialogue quality**: Systematically vary the clinical relevance scores of generated dialogues and measure the corresponding impact on DocVLM performance to establish whether expert-rated quality directly drives diagnostic improvement
2. **Generalization to unseen conditions**: Test DocVLM fine-tuned with PCDF on medical images from conditions not present in the training dialogues to evaluate whether the framework learns transferable diagnostic reasoning
3. **Real-world deployment simulation**: Implement a controlled experiment where human clinicians interact with DocVLM in a simulated consultation setting, comparing diagnostic accuracy and consultation efficiency against traditional image-only approaches