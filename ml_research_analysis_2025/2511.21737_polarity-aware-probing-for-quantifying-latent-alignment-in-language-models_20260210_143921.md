---
ver: rpa2
title: Polarity-Aware Probing for Quantifying Latent Alignment in Language Models
arxiv_id: '2511.21737'
source_url: https://arxiv.org/abs/2511.21737
tags:
- polarity
- alignment
- across
- language
- pa-ccs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Polarity-Aware Contrast-Consistent Search (PA-CCS) introduces\
  \ two metrics\u2014Polar Consistency and Contradiction Index\u2014to evaluate whether\
  \ language models' internal representations remain consistent when processing harmful\
  \ vs. safe statements under polarity inversion."
---

# Polarity-Aware Probing for Quantifying Latent Alignment in Language Models

## Quick Facts
- **arXiv ID**: 2511.21737
- **Source URL**: https://arxiv.org/abs/2511.21737
- **Reference count**: 40
- **Key outcome**: PA-CCS introduces Polar Consistency and Contradiction Index metrics to evaluate latent alignment by measuring internal consistency under polarity inversion across 16 models (110M–9B parameters)

## Executive Summary
PA-CCS introduces two metrics—Polar Consistency and Contradiction Index—to evaluate whether language models' internal representations remain consistent when processing harmful vs. safe statements under polarity inversion. The method probes hidden states across model layers using linear probes trained on contrastive completions, measuring semantic robustness without requiring labeled data. Experiments on 16 models (110M–9B parameters) across encoder, decoder, and encoder-decoder architectures show that PA-CCS captures meaningful alignment signals, with larger and instruction-tuned models demonstrating more stable polarity-aware representations. Results validate that PA-CCS detects genuine polarity structure rather than surface cues, offering a scalable tool for auditing internal alignment in LLMs.

## Method Summary
PA-CCS trains linear probes on contrastive Yes/No completions of the same proposition to extract belief-relevant directions from hidden states. For each statement, four variants are created (safe/harmful × Yes/No). The probe direction captures latent belief polarity independent of surface tokens. PC measures alignment between p(x+) and p(x̄−), while CI quantifies simultaneous agreement with both polarities. Control validation via token substitution tests whether probes rely on genuine semantic understanding versus lexical shortcuts.

## Key Results
- PA-CCS successfully detects polarity structure in models, with larger and instruction-tuned models showing more stable representations
- Control experiments confirm PA-CCS identifies semantic negation understanding rather than surface lexical cues
- Decoder models exhibit distinct geometric separation behaviors compared to encoder models, revealing architectural asymmetries

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Completion Encoding
PA-CCS extracts belief-relevant directions by training linear probes on contrastive Yes/No completions of the same proposition. For each statement, four variants are created (safe/harmful × Yes/No). A linear probe maps hidden states to scalar belief scores [0,1] via consistency and confidence losses. The probe direction captures latent belief polarity independent of surface tokens. Core assumption: Models encode belief information linearly in their hidden states, and contrastive pairs reveal this structure.

### Mechanism 2: Polarity Consistency Scoring
The Polar Consistency (PC) and Contradiction Index (CI) metrics detect whether models treat semantically opposite statements coherently. PC measures alignment between p(x+) and p(x̄−) (and vice versa)—a well-aligned model should affirm safe statements and reject harmful ones symmetrically. CI quantifies simultaneous agreement with both polarities, capturing internal contradictions. Low PC near 0 with moderate CI (~0.4) indicates clean separation. Core assumption: Consistent internal beliefs should show symmetric probability patterns across polarity-inverted pairs.

### Mechanism 3: Control Validation via Token Substitution
Replacing negation tokens with meaningless placeholders (e.g., "ttt") tests whether probes rely on genuine semantic understanding versus lexical shortcuts. The "Not Random Check" dataset substitutes "not" with arbitrary tokens. If PA-CCS scores degrade significantly for well-aligned models but not for poorly-calibrated ones, this confirms sensitivity to semantic negation rather than surface patterns. Core assumption: Genuine polarity understanding requires processing the negation's semantic role, not just its lexical presence.

## Foundational Learning

- **Contrast-Consistent Search (CCS)**
  - Why needed here: PA-CCS extends CCS; understanding the base method's consistency/confidence losses is prerequisite
  - Quick check question: Can you explain why CCS requires both consistency (p(x+) + p(x−) ≈ 1) and confidence (at least one probability should be high) constraints?

- **Linear Probing of Hidden States**
  - Why needed here: The method extracts activations from specific layers and trains lightweight classifiers; layer selection affects results
  - Quick check question: Given a 24-layer decoder model, which layers would you probe first for semantic belief information, and why?

- **Polarity and Negation in Language Models**
  - Why needed here: The entire framework depends on how models represent semantic opposition; negation handling varies by architecture
  - Quick check question: How might encoder (bidirectional attention) vs. decoder (causal attention) architectures differ in representing "X is harmful" vs. "X is not harmful"?

## Architecture Onboarding

- **Component map**: Dataset pairs → Hidden state extraction → CCS probe training → Probability extraction → PC/CI computation → Control validation

- **Critical path**: Dataset pairs → Hidden state extraction → CCS probe training → Probability extraction → PC/CI computation → Control validation

- **Design tradeoffs**:
  - Layer selection: Earlier layers capture syntax; later layers capture semantics—but optimal layer varies by architecture and model size
  - Probe complexity: Linear probes are interpretable but may miss non-linear belief structure
  - Dataset balance: Mixed dataset (rephrasing) vs. Not dataset (pure negation) test different aspects of polarity understanding

- **Failure signatures**:
  - High CI (>0.7) with low PC: Model simultaneously affirms contradictory statements
  - No degradation on control (ttt) dataset: Probe relies on lexical "not" cue rather than semantic polarity
  - Large variance across runs: Undertrained probe or unstable representations
  - ESA < 0.625: Model lacks meaningful polarity separation; metrics become unreliable

- **First 3 experiments**:
  1. Baseline CCS probe on a small decoder (GPT-2): Extract hidden states from middle layers (6-10), train CCS probe on mixed dataset, report accuracy and PC/CI across layers to establish expected performance floor
  2. Architecture comparison (encoder vs. decoder): Run PA-CCS on DeBERTa-base and GPT-2-large side-by-side on the Not dataset; compare variance and median PC/CI to validate architectural differences claimed in Section 4.2
  3. Control validation on instruction-tuned model: Test Gemma-2B-it on Not vs. ttt datasets; confirm that ESA drops and PC degrades when negation token is replaced, validating genuine semantic sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
How can alignment probes be designed to be architecture-invariant given the observed asymmetries between decoder and encoder models? The conclusion explicitly states that PA-CCS "reveals asymmetries between decoder and encoder models, highlighting an open challenge in designing architecture-invariant alignment probes." This remains unresolved because the study demonstrates that decoder models exhibit distinct geometric separation behaviors compared to the more stable but less discriminative encoder models, yet offers no unified solution.

### Open Question 2
How does the semantic origin of substitution tokens specifically influence the stability of PA-CCS metrics across different model scales? Appendix A.5 lists "RQ: How does the robustness of the metrics depend on the semantics of the token?" and notes that for small models, the variance of metrics fluctuates unpredictably based on token choice. This remains unresolved because the authors found that semantic, symbolic, and numeric tokens affect small models differently, with symbolic tokens sometimes rendering metrics ineffective, but a clear pattern or causal mechanism remains unidentified.

### Open Question 3
Can the latent directions identified by PA-CCS be utilized for causal interventions, such as activation steering, to correct harmful beliefs? The introduction cites "activation steering" as a related technique, and the conclusion claims the tool is "critical for... debiasing," yet the experiments only cover observational probing. This remains unresolved because the study validates PA-CCS as a diagnostic measurement tool (quantifying the Contradiction Index) but does not test if manipulating the identified belief directions forces the model to change its output or internal consistency.

## Limitations
- Method assumes linear separability of polarity-related features in hidden states, which may not hold for all model architectures
- Probe training details (learning rate, optimizer, batch size) are unspecified, potentially affecting reproducibility
- Control validation assumes negation is encoded lexically rather than through distributed representations

## Confidence
- **High Confidence**: PA-CCS framework design and metric definitions are clearly specified. Results showing larger and instruction-tuned models having better polarity consistency align with existing findings about alignment and calibration in LLMs
- **Medium Confidence**: Control validation findings depend on implementation details not fully specified. Architectural comparisons require careful layer selection to interpret correctly
- **Low Confidence**: Relationship between PA-CCS scores and real-world model behavior remains unclear. Method's sensitivity to different types of harmful content requires further testing

## Next Checks
1. **Probe Stability Analysis**: Run PA-CCS across 20+ random seeds for the same model/layer combination to quantify variance and establish whether observed differences between architectures are statistically significant
2. **Negation Type Generalization**: Test PA-CCS on datasets containing multiple negation strategies (lexical negation, antonym replacement, semantic negation) to verify the method's sensitivity to genuine polarity understanding rather than specific syntactic patterns
3. **Downstream Behavior Correlation**: Compare PA-CCS scores with models' actual refusal rates or safety responses on identical harmful-safe pairs to establish whether internal consistency predicts behavioral alignment