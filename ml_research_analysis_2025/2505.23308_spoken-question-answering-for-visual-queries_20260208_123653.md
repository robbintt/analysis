---
ver: rpa2
title: Spoken question answering for visual queries
arxiv_id: '2505.23308'
source_url: https://arxiv.org/abs/2505.23308
tags:
- speech
- spoken
- question
- visual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling spoken visual question
  answering (SVQA), where a model must answer questions about images using both spoken
  and visual input. The authors extend a visual question answering (VQA) model to
  support speech by adding a speech encoder and projector, creating a multimodal system
  capable of processing text, speech, and images.
---

# Spoken question answering for visual queries

## Quick Facts
- arXiv ID: 2505.23308
- Source URL: https://arxiv.org/abs/2505.23308
- Reference count: 0
- Key outcome: Achieves 61.4% accuracy on SEED-Bench using synthesized speech data, nearly matching text-based model's 68.2% performance

## Executive Summary
This paper introduces spoken visual question answering (SVQA), enabling models to answer questions about images using both spoken and visual input. The authors extend a visual question answering (VQA) model to support speech by adding a speech encoder and projector, creating a multimodal system capable of processing text, speech, and images. To train and evaluate this model, they synthesize VQA datasets into speech using two high-quality zero-shot TTS systems (StyleTTS2 and F5-TTS), generating datasets with 3.4M audio samples and 5135 hours of speech. The model is pre-trained in two phases: first training the speech projector on speech-only tasks, then fine-tuning the full system on SVQA tasks. Experiments show that the SVQA model achieves 61.4% accuracy on SEED-Bench, nearly matching the upper-bound text-based model at 68.2%, with minimal performance differences between the two TTS systems. These results demonstrate the effectiveness of synthesized speech data and highlight SVQA's potential for more natural human-machine interaction, making this an exceptional contribution to multimodal AI research.

## Method Summary
The method extends LLaVA-1.5 to support spoken question answering by adding a speech encoder (Whisper-large-v3) and projector module that aligns speech representations to the LLM's embedding space. The system uses two-phase training: first pre-training the speech projector on speech-only tasks using Multilingual LibriSpeech data, then fine-tuning the full model with LoRA on synthesized speech data from the LLaVA-Instruct dataset. The authors synthesize 3.4M audio samples using two zero-shot TTS systems (StyleTTS2 and F5-TTS) with diverse speaker prompts from MLS to prevent overfitting to specific voices. The architecture maintains frozen encoders (Whisper, CLIP) while training only the projector modules and LoRA adapters, enabling efficient multimodal reasoning without modifying pretrained backbones.

## Key Results
- SVQA model achieves 61.4% accuracy on SEED-Bench, nearly matching the text upper bound of 68.2%
- Minimal performance difference between StyleTTS2 (61.4%) and F5-TTS (59.4%) synthesis systems
- Speech-only pre-training phase improves performance from baseline 27.4% to 61.4% after fine-tuning
- Cross-TTS validation shows only 1-2% accuracy gap when training on one TTS and testing on another

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projector modules align speech and vision encoders to the LLM's embedding space, enabling cross-modal reasoning without modifying pretrained backbones.
- Mechanism: Frozen encoders (Whisper, CLIP) extract modality-specific representations. Trainable projectors (2-layer MLPs) map these to the LLM token space. The LLM then performs joint reasoning over concatenated embeddings (text + visual + audio). This preserves encoder quality while learning alignment.
- Core assumption: Encoder representations are sufficiently rich that linear-ish projections can align them to semantic space.
- Evidence anchors:
  - [abstract] "...fusion of text, speech, and image modalities... achieved through... additional projector module"
  - [Section 3] "The visual data from the encoder's output space is then aligned to the LLM input space using a projector module... We extend this model to support spoken question answering... with additional projector module"
  - [corpus] Weak direct evidence; related VQA work (EverydayMMQA) also uses projector-based alignment but for different modalities.
- Break condition: If encoder representations are misaligned with semantic reasoning (e.g., Whisper captures acoustic detail irrelevant to QA), projectors may fail to bridge the gap—evidenced by SVQA-baseline scoring only 27.4% vs 68.2% text upper bound before fine-tuning.

### Mechanism 2
- Claim: Zero-shot multi-speaker TTS with diverse voice prompts prevents overfitting to speaker-specific acoustic patterns.
- Mechanism: StyleTTS2/F5-TTS receive random MLS audio prompts + text, generating speech with varied voices/styles. Training uses disjoint speaker sets from testing (MLS train vs. test splits), forcing the model to learn speaker-invariant speech-to-semantic mappings rather than memorizing TTS artifacts.
- Core assumption: Synthetic speech acoustics are sufficiently similar to natural speech that learned representations transfer.
- Evidence anchors:
  - [abstract] "...choice of the TTS model has a minor impact on accuracy"
  - [Section 4.2] "We use random samples from the MLS dataset as audio prompts... large variety of voices and styles helps us avoid over-fitting"
  - [corpus] No direct corpus evidence on TTS diversity effects; EverydayMMQA addresses cultural grounding but not speaker variability.
- Break condition: If TTS introduces systematic artifacts (e.g., consistent prosodic patterns), the model may learn these instead of true speech understanding—cross-TTS validation (train on StyleTTS2, test on F5) helps detect this (results show only 1-2% gap).

### Mechanism 3
- Claim: Two-phase training (speech-only pre-training → joint SVQA fine-tuning with LoRA) stabilizes multimodal learning.
- Mechanism: Phase 1 trains only the speech projector on ASR + audio description tasks, establishing basic speech-to-text alignment. Phase 2 unfreezes projectors and applies LoRA (r=64, α=16) to the LLM for joint vision-audio training, preventing catastrophic forgetting while adapting to SVQA.
- Core assumption: Speech projector gains from Phase 1 transfer to multimodal reasoning in Phase 2.
- Evidence anchors:
  - [abstract] "...pre-trained in two phases: first training the speech projector on speech-only tasks, then fine-tuning the full system"
  - [Section 5.2] SVQA-baseline (no joint training) achieves 27.4%; SVQA-STTS2 (with LoRA fine-tuning) achieves 61.4%
  - [corpus] Weak evidence; no comparable two-phase analysis in neighbors.
- Break condition: If speech-only pre-training teaches task-averse behaviors (e.g., only transcribing, not reasoning), Phase 2 may struggle—authors added "audio context description" task to mitigate this (Section 4.1).

## Foundational Learning

- **Modality Alignment via Projectors**
  - Why needed here: Core architectural pattern for extending frozen LLMs to new modalities.
  - Quick check question: Can you explain why projector modules are trained while encoders remain frozen?

- **Speech Representations (Whisper encoder outputs)**
  - Why needed here: Understanding what acoustic/linguistic features the speech encoder provides to the projector.
  - Quick check question: What is the frame rate and dimensionality of Whisper-large-v3 encoder outputs?

- **LoRA Fine-tuning**
  - Why needed here: Understanding how to adapt large models efficiently without full parameter updates.
  - Quick check question: What do the rank (r=64) and alpha (α=16) parameters control in LoRA?

## Architecture Onboarding

- **Component map**:
  Input: [Speech audio] → Whisper encoder → Speech projector → LLM embeddings
         [Image] → CLIP encoder → Image projector → LLM embeddings
         [Text tokens] → Tokenizer → Token embeddings
                                         ↓
                              Vicuna-13B LLM (LoRA-adapted)
                                         ↓
                              Text response (generated)

- **Critical path**: Speech projector quality → Phase 1 pre-training on MLS (ASR + description tasks) → Phase 2 joint fine-tuning on LLaVA-Instruct synthesized speech → SEED-Bench evaluation. Without Phase 1, baseline drops to 27.4% accuracy.

- **Design tradeoffs**:
  - LoRA fine-tuning vs. full fine-tuning: LoRA (r=64) achieves 61.4% vs. text upper bound 68.2%; authors note full fine-tuning of original LLaVA may explain part of this gap.
  - TTS diversity vs. compute: Synthesizing 3.4M samples with varied voices requires ~2200 GPU hours (V100).
  - Speech-only transcription task vs. description task: Pure ASR pre-training caused models to ignore question content; adding description task improved semantic grounding (Section 4.1).

- **Failure signatures**:
  - Model transcribes instead of answering: WER analysis shows fine-tuned models often ignore transcription prompts and answer questions instead (Table 4: 87-115% WER on synthesized speech).
  - TTS overfitting: Cross-validate by training on StyleTTS2, testing on F5 (Table 1 shows <3% gap).
  - Long spoken prompts degrade performance: Full speech synthesis (question + post-prompt) drops accuracy 7-13% vs. question-only speech (Table 3).

- **First 3 experiments**:
  1. **Reproduce SVQA-baseline**: Load LLaVA-1.5 + pretrained Whisper encoder + randomly initialized speech projector. Evaluate on SEED-Bench with synthesized speech. Expected: ~27-29% accuracy (Table 1).
  2. **Phase 1 speech projector pre-training**: Train only the speech projector on MLS for 1 epoch (ASR + description tasks, frozen backbone), then fine-tune full model with LoRA on synthesized SVQA data for 1 epoch on 4x A100 GPUs.
  3. **Full SVQA fine-tuning (LoRA)**: Fine-tune with LoRA (r=64, α=16) on StyleTTS2-synthesized LLaVA-Instruct for 1 epoch. Evaluate on SEED-Bench with both TTS systems. Expected: 59-62% accuracy (Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will SVQA models trained exclusively on synthetic speech generalize effectively to natural human speech inputs?
- Basis in paper: [explicit] The authors state "we anticipate that similar results would be obtained when testing with natural human speech" but acknowledge this is untested. No natural speech-image QA dataset exists.
- Why unresolved: The entire training and evaluation pipeline uses TTS-generated speech. Cross-validation between two TTS systems reduces overfitting concerns but does not validate natural speech performance.
- What evidence would resolve it: Evaluation on a newly collected dataset with real human speech queries about images, compared against synthetic speech baselines.

### Open Question 2
- Question: What causes the performance gap between textual VQA (68.2%) and SVQA (~61%), and how can it be closed?
- Basis in paper: [explicit] The authors identify two potential causes: "(1) the original LLaVA model was fully fine-tuned for VQA as opposed to our LoRA fine-tuning" and "(2) errors introduced during speech synthesis and during the conversion of speech back to text during inference."
- Why unresolved: These hypotheses are not isolated or tested experimentally. The relative contribution of each factor remains unknown.
- What evidence would resolve it: Ablation studies comparing LoRA vs. full fine-tuning for SVQA, and experiments measuring error propagation from synthesis through inference.

### Open Question 3
- Question: Why does SVQA fine-tuning degrade speech transcription ability while improving question answering?
- Basis in paper: [inferred] Table 4 shows WER increases from 7.4% (baseline) to 84.6–87.1% (fine-tuned models) on MLS test. The authors note "the language models specialize in question answering and therefore try to do this instead of transcription."
- Why unresolved: The trade-off between transcription and QA capabilities during multimodal fine-tuning is not characterized. It is unclear whether this is inherent to the training approach or remediable.
- What evidence would resolve it: Experiments with multi-task training combining transcription and QA objectives, measuring both capabilities on held-out sets.

## Limitations
- The paper relies entirely on synthesized speech data without validating performance on natural human speech, which limits generalizability to real-world applications.
- The two-phase training approach, while effective, adds complexity and computational cost compared to potential single-phase alternatives.
- The performance gap between text-based (68.2%) and speech-based (61.4%) models suggests inherent challenges in processing spoken input, though the exact causes remain unclear.

## Confidence
- **High Confidence**: The core architectural design (projector-based modality alignment) and the overall performance improvement through two-phase training. The experimental results are reproducible and the methodology is sound.
- **Medium Confidence**: The claim that synthesized speech data is sufficient for training effective SVQA models. While results are strong, the lack of natural speech validation introduces uncertainty.
- **Low Confidence**: The assertion that TTS choice has minimal impact on accuracy. This conclusion is based on limited comparison between two TTS systems and may not generalize to other synthesis approaches.

## Next Checks
1. **Natural Speech Validation**: Test the trained SVQA model on a dataset of naturally spoken questions (rather than synthesized) to verify that the model generalizes beyond the synthetic speech domain. This would validate whether the alignment learned through TTS synthesis transfers to real-world speech patterns.

2. **Cross-TTS Generalization Study**: Conduct systematic experiments training on multiple TTS systems and testing across all combinations to better understand the impact of TTS choice. This would provide more robust evidence for the claim that TTS selection has minimal effect on accuracy.

3. **Alternative Training Strategy Comparison**: Compare the two-phase training approach against a single-phase strategy that incorporates speech tasks directly into the joint fine-tuning process. This would help determine whether the two-phase approach is necessary or if simpler training methods could achieve similar results.