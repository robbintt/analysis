---
ver: rpa2
title: Evaluating the Performance of RAG Methods for Conversational AI in the Airport
  Domain
arxiv_id: '2505.13006'
source_url: https://arxiv.org/abs/2505.13006
tags:
- flight
- questions
- information
- graph
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates three RAG methods\u2014Traditional RAG, SQL\
  \ RAG, and Graph RAG\u2014for handling airport queries with specialized terminology\
  \ and dynamic reasoning needs. Traditional RAG (BM25 + GPT-4) achieved 84.84% accuracy\
  \ but occasionally hallucinated, posing safety risks."
---

# Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain

## Quick Facts
- arXiv ID: 2505.13006
- Source URL: https://arxiv.org/abs/2505.13006
- Reference count: 7
- Primary result: Graph RAG achieved 91.49% accuracy with significantly fewer hallucinations compared to Traditional RAG (84.84%) and SQL RAG (80.85%) on airport queries

## Executive Summary
This paper evaluates three RAG methods—Traditional RAG, SQL RAG, and Graph RAG—for handling airport queries with specialized terminology and dynamic reasoning needs. Traditional RAG (BM25 + GPT-4) achieved 84.84% accuracy but occasionally hallucinated, posing safety risks. SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations. Graph RAG excelled at reasoning tasks and dynamic queries by leveraging flight relationships in a knowledge graph. Question classification and prompt engineering effectively managed jargon and abbreviations. SQL RAG and Graph RAG are recommended for high-safety environments like airports due to reduced hallucination risk and better handling of complex queries.

## Method Summary
The study compares three RAG pipelines for conversational AI in airport domain: Traditional RAG using BM25 retrieval and LLM generation, SQL RAG using Text-to-SQL generation with schema-aware prompts, and Graph RAG using Text-to-Cypher generation over a flight knowledge graph. A question classifier routes queries to specialized prompts for handling jargon, abbreviations, and incomplete queries. Evaluation used 220 questions across straightforward and complicated/ambiguous datasets, measuring accuracy, Exact Match (EM), Execution Match (EX), and hallucination rates.

## Key Results
- Graph RAG achieved 91.49% accuracy, outperforming Traditional RAG (84.84%) and SQL RAG (80.85%)
- Graph RAG achieved 68.75% accuracy on reasoning queries vs. 6.25% for SQL RAG and 9.38% for Traditional RAG
- SQL RAG and Graph RAG significantly reduced hallucinations compared to Traditional RAG by grounding LLM output in query-executed results
- Question classification achieved 90.45% accuracy for handling airport jargon and abbreviations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph RAG outperforms Traditional RAG and SQL RAG on reasoning-heavy queries involving multi-hop relationships (e.g., connecting flights, temporal sequencing).
- Mechanism: Flight entities are nodes linked by edges (e.g., `connecting_flight`, `same_ramp`, `expected_on_ramp_time`). The LLM generates Cypher queries that traverse these edges, enabling structured reasoning over relationships rather than relying on keyword proximity or flat table lookups.
- Core assumption: The knowledge graph schema accurately captures the temporal and relational structure of flight operations, and the LLM can reliably translate natural language into valid Cypher queries.
- Evidence anchors:
  - [abstract] "Graph RAG was especially effective for questions that involved reasoning."
  - [Section 5.1.3, Table 5] Graph RAG achieved 68.75% accuracy on the reasoning question dataset vs. 6.25% for SQL RAG and 9.38% for Traditional RAG.
  - [corpus] Neighbor paper "Beyond Chunks and Graphs" supports triplet-driven reasoning for RAG, but no direct corpus replication of the airport-domain graph reasoning result.

### Mechanism 2
- Claim: SQL RAG and Graph RAG reduce hallucinations compared to Traditional RAG by constraining LLM output to query-executed results rather than retrieved text chunks.
- Mechanism: In SQL/Graph RAG, the LLM generates a structured query (SQL/Cypher), which is executed against the database. The returned data is deterministic and grounded in the schema, limiting the LLM's generative freedom to factually accurate records. Traditional RAG passes retrieved text chunks directly to the LLM, increasing the risk of confabulation.
- Core assumption: The generated SQL/Cypher query is syntactically correct and semantically aligned with the user's intent; schema context is sufficient for disambiguation.
- Evidence anchors:
  - [abstract] "SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations."
  - [Section 5.2] "SQL RAG and Graph RAG reduce hallucinations by converting natural language questions into SQL or Cypher queries. Thereby, the input to the LLM is accurate data, which significantly reduces hallucinations."
  - [corpus] Neighbor paper "Balancing Content Size in RAG-Text2SQL System" notes Text-to-SQL systems inherit LLM hallucination risks, but query-grounded retrieval mitigates them.

### Mechanism 3
- Claim: Question classification + specialized prompts enable handling of airport jargon, abbreviations, and incomplete queries.
- Mechanism: A classifier (LLM-based with few-shot examples) assigns each query to one of six categories (TAQ, BGQ, NFQ, TWAQ, BQA, AFQ). Each category routes to a tailored prompt that normalizes input (e.g., expands "123" to flight number pattern, resolves "Delta" to airline code) before RAG retrieval.
- Core assumption: The classification prompt covers the linguistic variation encountered in operational use, and few-shot examples generalize to unseen phrasings.
- Evidence anchors:
  - [Section 5.1.2] Classification accuracy averaged 90.45% across five runs on 220 questions with 60 few-shot examples.
  - [Appendix A.1.1] Categories defined based on airport operations specialist communication patterns (abbreviations, short sentences, assumed context).
  - [corpus] Neighbor paper "PRACTIQ" addresses ambiguous/unanswerable queries in conversational Text-to-SQL but does not replicate the airport-specific jargon classification approach.

## Foundational Learning

- **Concept: Knowledge Graph Schema Design (Property Graph Model)**
  - Why needed here: Graph RAG's reasoning advantage depends on how flights, ramps, times, and connecting flights are modeled as nodes and edges. Understanding labeled property graphs (Neo4j) is prerequisite to interpreting the Cypher generation pipeline.
  - Quick check question: Given a flight table with `flight_number`, `ramp`, `expected_on_ramp_time`, and `connecting_flight_number`, sketch a minimal property graph schema that supports "find the next flight from the same ramp."

- **Concept: Text-to-SQL Prompt Engineering (CRP vs. ODP)**
  - Why needed here: SQL RAG performance hinges on prompt design that conveys schema structure. The Code Representation Prompt (CRP) outperformed OpenAI Demonstration Prompt (ODP) by including column types and foreign keys.
  - Quick check question: Compare two prompt styles for converting "What is the gate for flight KL1000?" to SQL—one with just table/column names, one with `CREATE TABLE` statements. Which is more robust for ambiguous column names?

- **Concept: Hallucination Sources in RAG Systems**
  - Why needed here: Traditional RAG retrieves text chunks; the LLM may confabulate when chunks are noisy or incomplete. Understanding retrieval-to-generation handoff clarifies why query-based RAG reduces this risk.
  - Quick check question: A Traditional RAG system retrieves 10 flight records for "Which flights are at B18?" but the LLM outputs a flight number not in the records. Identify three possible causes in the retrieval or generation pipeline.

## Architecture Onboarding

- **Component map:**
  - Question Classifier → routes to one of 6 category-specific prompts
  - Prompt Router → selects normalization/generation prompt based on category
  - RAG Pipeline Selector → Traditional RAG (BM25 retrieval), SQL RAG (Text-to-SQL), or Graph RAG (Text-to-Cypher)
  - Query Executor → runs SQL/Cypher against flight database or graph
  - Answer Generator → LLM synthesizes final response from query results
  - Fallback Handler → prompts user for clarification on ambiguous/underspecified queries

- **Critical path:**
  1. User query → Question Classifier (few-shot LLM)
  2. Classified query → Prompt Router → Normalized query
  3. Normalized query → RAG Pipeline Selector
  4. If Graph RAG: LLM generates Cypher → Query Executor → Graph DB → Answer Generator
  5. If SQL RAG: LLM generates SQL → Query Executor → SQL DB → Answer Generator
  6. If Traditional RAG: BM25 retrieves chunks → Answer Generator

- **Design tradeoffs:**
  - Traditional RAG: Faster setup, no schema engineering, but higher hallucination risk (~10% observed) and weaker on reasoning queries.
  - SQL RAG: Lower hallucination, requires schema-accurate prompts (CRP), struggles with multi-hop reasoning (6.25% on reasoning dataset).
  - Graph RAG: Best reasoning and accuracy (91.49%), requires upfront graph modeling and Cypher expertise; query generation more flexible but harder to validate (low EM, high EX).

- **Failure signatures:**
  - `Classification Misroute`: Question labeled BGQ when it's TWAQ → wrong prompt → extraction returns `['0']` → system prompts for more info unnecessarily.
  - `Query Generation Failure`: Complex temporal query produces syntactically invalid Cypher/SQL → executor returns error → fallback to generic response.
  - `Schema Drift`: New flight attribute added to DB but not to prompt schema → queries referencing new attribute fail silently or return null.

- **First 3 experiments:**
  1. **Reproduce retrieval accuracy comparison**: Implement BM25, TF-IDF, and FAISS retrieval on a sample of 100 straightforward questions; measure top-1 and top-10 accuracy to validate Table 1.
  2. **Test Graph RAG on reasoning queries**: Build a minimal Neo4j graph with 50 flights including `connecting_flight` edges; run 10 Next Flight Questions and compare Cypher-generated answers to ground truth.
  3. **Hallucination rate baseline**: Run 20 ambiguous queries through Traditional RAG (BM25+GPT-4) and SQL RAG (CRP+GPT-4); manually label outputs for factual accuracy and count hallucinations to replicate the direction of the paper's finding (do not assume exact 10% rate).

## Open Questions the Paper Calls Out

- **Question 1**: How does the integration of live, real-time APIs for flight status and gate information impact the performance and latency of Graph RAG compared to static datasets?
  - Basis in paper: [explicit] The authors state in the Future Work section that they plan to "connect the system with live APIs" to handle real-time changes like delays and gate changes.
  - Why unresolved: The current experiments were conducted exclusively in a controlled environment using static flight data.
  - What evidence would resolve it: Performance benchmarks (accuracy and response time) collected from a prototype connected to live air traffic data streams.

- **Question 2**: Can the question classification and RAG pipelines generalize effectively to other airports with different terminology and data schemas?
  - Basis in paper: [explicit] The Limitations section notes the evaluation is specific to Schiphol airport and that "adapting the model to other airports or domains may present new challenges."
  - Why unresolved: The prompt engineering and classification categories were tailored specifically to the jargon and data structure of a single airport.
  - What evidence would resolve it: Successful deployment and accuracy evaluation of the system using datasets from diverse international airports without extensive re-engineering.

- **Question 3**: Does the superiority of Graph RAG over SQL RAG persist as the dataset scales to include significantly more flights and complex relationships?
  - Basis in paper: [explicit] The authors identify the "relatively small size of our current dataset" as a limitation and aim to "significantly expand" it to validate their conclusions.
  - Why unresolved: It is unclear if the retrieval efficiency and reasoning capabilities of Graph RAG scale effectively with massive data volumes.
  - What evidence would resolve it: Comparative accuracy and latency results from experiments run on a substantially larger, more diverse flight dataset.

## Limitations

- The study lacks complete transparency in prompt engineering templates and evaluation datasets, with exact prompt structures and full question templates not disclosed.
- The knowledge graph schema details and relationship definitions are not fully specified, limiting reproducibility.
- Hallucination rate reduction in SQL and Graph RAG is qualitative rather than quantified, and the comparison is against a single Traditional RAG configuration.
- The reported reasoning accuracy advantage for Graph RAG (68.75%) needs validation on larger datasets to confirm scalability.

## Confidence

- **High Confidence**: The mechanism by which Graph RAG achieves superior reasoning performance through structured relationship traversal (supported by 68.75% accuracy on reasoning queries vs. 6.25% for SQL RAG and 9.38% for Traditional RAG).
- **Medium Confidence**: The claim that SQL and Graph RAG reduce hallucinations by constraining LLM output to query-executed results, based on qualitative observation rather than quantified hallucination rates.
- **Medium Confidence**: The question classification accuracy of 90.45% for handling airport jargon, though the robustness of this classifier to evolving operational language remains untested.

## Next Checks

1. **Reproduce retrieval accuracy comparison**: Implement BM25, TF-IDF, and FAISS retrieval on a sample of 100 straightforward questions; measure top-1 and top-10 accuracy to validate Table 1 findings.

2. **Test Graph RAG on reasoning queries**: Build a minimal Neo4j graph with 50 flights including connecting flight edges; run 10 Next Flight Questions and compare Cypher-generated answers to ground truth to verify the 68.75% reasoning accuracy claim.

3. **Hallucination rate baseline**: Run 20 ambiguous queries through Traditional RAG (BM25+GPT-4) and SQL RAG (CRP+GPT-4); manually label outputs for factual accuracy and count hallucinations to replicate the qualitative finding that SQL/Graph RAG reduces hallucinations.