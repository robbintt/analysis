---
ver: rpa2
title: 'Probability Consistency in Large Language Models: Theoretical Foundations
  Meet Empirical Discrepancies'
arxiv_id: '2505.08739'
source_url: https://arxiv.org/abs/2505.08739
tags:
- layer
- attention
- context
- distance
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study formally proves that sequence perplexity is theoretically
  invariant under any factorization order, including forward, backward, or arbitrary
  permutations, establishing a mathematical benchmark for evaluating LLM consistency.
  Despite this theoretical expectation, empirical results show systematic deviations:
  forward and backward models achieve similar but not identical perplexities, with
  forward models consistently outperforming backward models; permuted training yields
  significantly higher losses.'
---

# Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies

## Quick Facts
- arXiv ID: 2505.08739
- Source URL: https://arxiv.org/abs/2505.08739
- Reference count: 40
- Primary result: Forward and backward LLMs achieve similar but not identical perplexities, contradicting theoretical invariance expectations

## Executive Summary
This study establishes that sequence perplexity is theoretically invariant under any factorization order (forward, backward, or arbitrary permutations) based on the chain rule of probability. However, empirical results reveal systematic deviations: forward and backward models achieve similar but not identical perplexities, with forward models consistently outperforming backward models; permuted training yields significantly higher losses. Attention analysis shows forward/backward models develop strong positional biases favoring adjacent and long-range tokens, while permuted models exhibit distinct patterns. Representational alignment is higher between forward and backward models but degrades with depth, diverging sharply for permuted models. On the BrainBench benchmark, both forward and backward models match human expert performance but align poorly with human judgment patterns, contradicting prior claims about backward model inferiority.

## Method Summary
The study trains GPT-2 models (124M, 355M, 774M) on neuroscience publications (1.3B tokens, 2002-2022) using three factorization orders: forward (standard), backward (reversed tokens), and permuted (fixed arbitrary order). All sequences are 1,024 tokens with a prepended BOS token. A single BPE tokenizer (50,257 vocab) trained on forward text is applied to all factorizations. Models are trained with AdamW (lr=2e-5, warm-up 0.03, weight decay 0.001) for 5 epochs. Evaluation includes perplexity comparison, attention entropy analysis, representational similarity analysis (RSA) between layers, and BrainBench accuracy testing.

## Key Results
- Forward and backward models achieve similar but not identical perplexities, with forward models consistently outperforming backward models
- Permuted training yields significantly higher losses than either forward or backward approaches
- Attention analysis reveals strong positional biases in forward/backward models favoring both adjacent and long-range tokens
- Representational alignment between forward and backward models degrades with depth, diverging sharply for permuted models
- Both forward and backward models match human expert performance on BrainBench but align poorly with human judgment patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity invariance across factorization orders emerges from the chain rule of probability
- Mechanism: Any factorization P(X₁,...,Xₙ) = ∏P(Xᵢ|context) telescopes to the same joint probability when summed in log space—the conditional terms cancel sequentially, leaving only ln P(X₀, X₁,...,Xₙ) regardless of permutation order
- Core assumption: The probability distribution is well-defined and normalized; BOS token ensures first conditional is well-defined
- Evidence anchors:
  - [abstract] "We prove formally that for any well-defined probability distribution, sequence perplexity is invariant under any factorization, including forward, backward, or arbitrary permutations"
  - [section 2] "This sum telescopes... yielding PPσ = exp(-1/n ln P(X₀, X₁, ..., Xₙ))"
  - [corpus] Related work on internal probability and self-consistency (arXiv:2510.15444) explores similar theoretical foundations
- Break condition: If BOS token is omitted or tokenizer is retrained on reversed text, factorizations become incomparable

### Mechanism 2
- Claim: Self-attention positional biases cause systematic perplexity deviations from theoretical equivalence
- Mechanism: Causal self-attention develops dual biases—locality (nearby tokens) and long-range preference (initial-final pairs)—which interact differently with forward/backward/permuted training orders, disrupting the theoretical symmetry
- Core assumption: Biases stem from both architectural constraints and training data structure
- Evidence anchors:
  - [abstract] "Attention analysis reveals that forward/backward models develop strong positional biases favoring adjacent and long-range tokens, while permuted models exhibit distinct patterns"
  - [section 3.2] "Forward and backward trained models show strong biases toward both adjacent tokens and those at the maximum context length"
  - [corpus] Limited direct corpus evidence on positional bias mechanisms; related work on attention backward passes (arXiv:2410.09397) addresses I/O complexity but not bias emergence
- Break condition: If attention patterns were uniform across token distances, perplexities would converge regardless of training order

### Mechanism 3
- Claim: Representational divergence in deeper layers underlies persistent ordering effects
- Mechanism: While forward and backward models share similar early-layer representations (higher RSA correlation), representations progressively diverge through depth; permuted models diverge sharply from both, indicating fundamentally different learning dynamics
- Core assumption: Layer-wise representational similarity reflects functional equivalence
- Evidence anchors:
  - [abstract] "Representational alignment is higher between forward and backward models but degrades with depth, diverging sharply for permuted models"
  - [section 3.3] "As layers advance, representational alignment decreases across all direction pairs"
  - [corpus] No direct corpus evidence on layer-wise representational alignment in ordering contexts
- Break condition: If representations remained aligned across all layers, perplexity equivalence would hold empirically

## Foundational Learning

- Concept: **Chain Rule of Probability**
  - Why needed here: Underlies the theoretical proof that perplexity is factorization-invariant
  - Quick check question: Can you explain why P(A,B,C) = P(A)P(B|A)P(C|A,B) equals P(C)P(B|C)P(A|B,C)?

- Concept: **Self-Attention Mechanism in Transformers**
  - Why needed here: Required to understand how positional biases emerge and affect perplexity
  - Quick check question: What constraints does causal masking impose on which tokens can attend to each other?

- Concept: **Representational Similarity Analysis (RSA)**
  - Why needed here: Method used to quantify how differently models process identical inputs
  - Quick check question: Why use Spearman correlation on RDM upper triangles rather than direct hidden state comparison?

## Architecture Onboarding

- Component map:
  GPT-2 decoder-only transformer (124M/355M/774M variants) -> Causal self-attention (12-36 layers) -> BPE tokenizer (50,257 vocab) -> Neuroscience corpus (1.3B tokens) -> Factorizations (forward/backward/permuted)

- Critical path:
  1. Tokenize all data with single forward-trained tokenizer
  2. Prepend BOS token to every sequence; ensure sequences span full 1024-token context
  3. Apply factorization (forward/backward/permute) within context window only
  4. Train with identical data ordering, differing only in token arrangement
  5. Evaluate perplexity, attention entropy, RSA, and BrainBench accuracy

- Design tradeoffs:
  - **Same tokenizer vs. retrain**: Using one tokenizer enables valid perplexity comparison but may disadvantage backward/permuted models
  - **Full context window vs. natural boundaries**: Full window ensures theoretical validity but removes document structure
  - **Permutation strategy**: Arbitrary fixed permutation tests theoretical limit but disrupts all linguistic structure

- Failure signatures:
  - Missing BOS token → forward and backward factorizations incomparable
  - Retrained tokenizer on reversed text → different vocabularies, invalid comparison
  - Logical reversal instead of strict token reversal → violates proof conditions
  - Cohen's d > 2.0 for perplexity differences indicates strong deviation from equivalence

- First 3 experiments:
  1. Replicate perplexity comparison across factorizations with proper BOS and single tokenizer; expect forward ≈ backward (d < 0.7) but permuted >> both
  2. Visualize attention entropy by layer and context size for each factorization; expect forward/backward similar, permuted higher in early-middle layers
  3. Compute layer-wise RSA between model pairs; expect forward-backward correlation declining with depth, permuted approaching zero

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do deviations from theoretical probability consistency directly contribute to hallucinations and unpredictable out-of-distribution behaviors in LLMs?
- Basis in paper: [explicit] "We suspect that deviations from proper probability distributions may contribute to thorny problems like hallucination and unpredictable out-of-distribution behaviors in LLMs... While this connection requires further exploration..."
- Why unresolved: The paper establishes that practical models deviate from theoretical probability consistency but does not empirically link these deviations to downstream failure modes like hallucination.
- What evidence would resolve it: Correlational or causal studies measuring the relationship between perplexity inconsistency metrics across factorizations and hallucination rates on standard benchmarks.

### Open Question 2
- Question: Does bidirectional learning's theoretical advantage in capturing joint probabilities translate to improved downstream performance compared to unidirectional LMs?
- Basis in paper: [explicit] "A promising future direction could compare modernized bidirectional models (e.g., ModernBERT) with unidirectional LMs to determine if bidirectional learning's theoretical advantage in capturing joint probabilities translates to downstream performance."
- Why unresolved: The paper focuses on autoregressive models and only briefly references bidirectional approaches without empirical comparison.
- What evidence would resolve it: Controlled experiments comparing ModernBERT-style architectures against autoregressive baselines on identical training data, evaluating both perplexity consistency and downstream task accuracy.

### Open Question 3
- Question: Why does the forward-backward perplexity gap widen as model size increases?
- Basis in paper: [explicit] "forward-trained models consistently exhibit lower perplexity than backward-trained models, with the gap widening as model size increases (Table 2)."
- Why unresolved: The paper documents this scaling phenomenon but does not identify the mechanistic cause—whether it stems from architectural capacity, optimization dynamics, or data-driven biases.
- What evidence would resolve it: Layer-wise and head-wise attention analyses across model scales, combined with controlled experiments varying model depth and width independently.

### Open Question 4
- Question: Can perplexity inconsistency across factorizations serve as a diagnostic metric for detecting untrustworthy or unreliable model outputs?
- Basis in paper: [explicit] "Our theoretical and empirical results... suggest methods for detecting when LLMs' probability distributions are inconsistent and therefore untrustworthy."
- Why unresolved: The paper proposes this application but does not validate whether inconsistency metrics predict real-world reliability failures.
- What evidence would resolve it: Evaluation on datasets containing adversarial examples or distribution shifts, testing whether factorization-based inconsistency scores correlate with prediction errors or calibration failures.

## Limitations

- **Permutation Strategy Ambiguity**: The specific random permutation σ used for permuted training is not specified, preventing exact replication.
- **Tokenizer Asymmetry Effects**: Using a single forward-trained tokenizer may systematically disadvantage backward and permuted models through suboptimal segmentation.
- **BrainBench Methodology Gap**: The binary classification threshold for selecting the "better" abstract is unspecified, potentially introducing noise into performance comparisons.

## Confidence

**High Confidence**: The mathematical proof of perplexity invariance under the chain rule, assuming proper BOS token usage and single tokenizer.

**Medium Confidence**: The claim that forward and backward models achieve similar but not identical perplexities, supported by effect sizes but requiring exact replication conditions.

**Low Confidence**: The assertion that permuted models "exhibit distinct patterns" requires verification with the exact permutation used.

## Next Checks

1. **Replication with specified permutation**: Train forward, backward, and permuted models using the exact same random permutation seed reported in the original study. Verify that perplexity differences between forward and backward remain small (Cohen's d < 0.7) while permuted shows substantially higher loss.

2. **Tokenizer retraining experiment**: Train separate BPE tokenizers on forward, backward, and permuted corpora. Compare perplexity distributions across factorization orders to quantify how much tokenizer asymmetry contributes to observed differences.

3. **Layer-wise attention bias analysis**: Extract attention maps from middle layers (e.g., layers 6-18 in 24-layer models) and compute average normalized rank by token distance for each factorization. Verify the dual bias pattern (adjacent + long-range) in forward/backward models and distinct patterns in permuted models.