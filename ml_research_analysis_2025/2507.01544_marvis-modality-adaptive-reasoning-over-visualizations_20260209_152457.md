---
ver: rpa2
title: 'MARVIS: Modality Adaptive Reasoning over VISualizations'
arxiv_id: '2507.01544'
source_url: https://arxiv.org/abs/2507.01544
tags:
- class
- visualization
- data
- point
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MARVIS transforms vision-language models into universal predictors
  by converting embedding spaces into visual representations, enabling training-free
  predictions across vision, audio, biological, and tabular domains. Using a single
  3B parameter model, it achieves competitive performance with specialized methods
  while preserving privacy, reaching 84.5% accuracy on tabular classification and
  approaching state-of-the-art vision results without domain-specific training.
---

# MARVIS: Modality Adaptive Reasoning over VISualizations

## Quick Facts
- arXiv ID: 2507.01544
- Source URL: https://arxiv.org/abs/2507.01544
- Reference count: 40
- Primary result: Single 3B parameter VLM achieves competitive performance across vision, audio, biological, and tabular domains without domain-specific training

## Executive Summary
MARVIS transforms embedding spaces into visual representations and leverages Vision-Language Models (VLMs) for training-free predictions across diverse data modalities. The system converts domain-specific embeddings into 2D scatter plots using t-SNE, then queries a VLM with these visualizations to perform classification and regression tasks. Using a single 3B parameter model, MARVIS achieves competitive results with specialized methods while preserving privacy and requiring no fine-tuning.

## Method Summary
MARVIS converts raw data into embeddings using modality-specific models, reduces dimensions to 2D with t-SNE, and renders scatter plots highlighting query points. The VLM analyzes these visualizations with structured prompts containing class legends and neighbor information. Classification predictions are extracted from VLM responses using regex parsing. The approach uses consistent visualization strategies (tsne_knn) across all benchmarks, with zoom factors calibrated per domain to fit within the VLM's attention window.

## Key Results
- Achieves 98.0% accuracy on CIFAR-10 using DINOv2 embeddings
- Reaches 91.3% accuracy on ESC-50 audio classification with CLAP embeddings
- Scores 84.5% accuracy on OpenML CC18 tabular classification using TabPFN embeddings
- Matches or exceeds Gemini zero-shot performance across vision, audio, and biological domains

## Why This Works (Mechanism)

### Mechanism 1
Visual representations serve as a universal interface for cross-modal reasoning in VLMs. Domain-specific embedding models compress raw data into latent vectors; t-SNE projects these to 2D visualizations; the VLM applies pre-trained spatial reasoning capabilities to interpret cluster structure and neighbor relationships for classification. Core assumption: VLMs pre-trained on web-scraped scientific imagery can generalize to novel embedding visualizations without domain-specific training.

### Mechanism 2
The information environment design (visualization type, context provided) causally affects VLM reasoning quality and prediction accuracy. Different visualization configurations provide varying information density; richer visualizations elicit longer, more sophisticated reasoning traces with quantitative distance analysis rather than proximity heuristics. Core assumption: VLMs condition their reasoning on available visual information rather than applying fixed heuristics.

### Mechanism 3
Specialist embedding models preserve domain structure that VLMs cannot natively extract from raw data. Modality-specific encoders transform raw inputs into semantically meaningful embeddings where similar instances cluster; this structure survives t-SNE projection sufficiently for VLM spatial reasoning. Core assumption: The embedding model captures predictive structure that would be lost in direct serialization or naive encoding approaches.

## Foundational Learning

- **Embedding spaces and latent representations**
  - Why needed here: MARVIS relies entirely on embedding models to capture domain structure before visualization; understanding that embeddings encode semantic similarity is prerequisite for debugging why certain clusters form.
  - Quick check question: Given two audio clips of the same sound, would you expect their CLAP embeddings to be closer in cosine distance than embeddings of different sounds?

- **Dimensionality reduction (t-SNE)**
  - Why needed here: The core transformation is high-dimensional embeddings → 2D scatter plot; t-SNE's stochastic nature and perplexity parameter directly affect cluster visibility and VLM interpretability.
  - Quick check question: If t-SNE shows overlapping clusters for clearly separable classes in embedding space, what parameter might you adjust, and what tradeoff does this introduce?

- **Vision-Language Model architecture**
  - Why needed here: MARVIS exploits specific VLM capabilities (spatial reasoning, arbitrary aspect ratio processing, scientific imagery understanding); understanding patch-based processing helps explain why "zoom" matters.
  - Quick check question: Why does the paper emphasize Qwen 2.5 VL's 14×14 patch structure and sliding window attention for distance-based visualizations?

## Architecture Onboarding

- **Component map**: Embedding generators (modality-specific) → Dimensionality reducer (t-SNE) → Visualization renderer → VLM backbone → Response parser
- **Critical path**: 1) Fit t-SNE on training embeddings (per-benchmark, fixed random state) → 2) Generate visualization with query point highlighted as red star → 3) Apply "zoom" factor so query point and neighbors fit within sliding attention window → 4) Construct prompt with legend, optional metadata, task instruction → 5) Parse VLM response for class label using known class-color mapping
- **Design tradeoffs**: Visualization richness vs. speed (k-NN overlays improve accuracy but require computation); VLM size vs. deployment (3B chosen for efficiency); t-SNE vs. UMAP vs. PCA (t-SNE chosen for cluster preservation)
- **Failure signatures**: Legend parsing errors (VLM outputs correct reasoning but wrong format); resolution mismatch (query point falls outside attention window); cluster overlap in visualization (t-SNE distorts global structure)
- **First 3 experiments**: 1) Reproduce CIFAR-10 baseline with DINOv2 embeddings, vary t-SNE perplexity to characterize sensitivity → 2) Ablate visualization strategy comparing basic t-SNE vs. tsne_knn vs. tsne_perturbation on tabular data → 3) Cross-domain sanity check applying audio-trained CLAP embeddings to held-out audio benchmark

## Open Questions the Paper Calls Out

- Can fine-tuning strategies specifically targeting scientific imagery reasoning improve MARVIS performance?
- What is the optimal combination of visualization methods and embedding models for diverse data modalities?
- How can the visualization scaling factor be automated to optimize local attention windows?

## Limitations

- Cross-modal generalization gap: Fundamental assumption that VLMs can reason about arbitrary embedding visualizations remains empirically untested beyond specific benchmarks
- Stochasticity control: Fixed t-SNE random states don't report variance across runs or sensitivity to initialization
- Information completeness: Ablation shows baseline visualizations contain insufficient information, but minimal information requirements remain unstudied

## Confidence

- **High confidence**: MARVIS successfully leverages VLMs for training-free prediction when given appropriate embedding visualizations
- **Medium confidence**: The information environment design meaningfully affects VLM reasoning quality
- **Low confidence**: VLMs can serve as "universal predictors" for arbitrary embedding spaces

## Next Checks

1. **t-SNE sensitivity analysis**: Run MARVIS with 10 different random seeds for each benchmark's t-SNE initialization. Report variance in accuracy and analyze whether reasoning trace quality correlates with embedding visualization stability.

2. **Minimal information threshold**: Systematically ablate visualization components (legend, neighbor connections, pie charts) and measure the point where VLM performance drops to chance levels. This quantifies the actual information requirements for reliable reasoning.

3. **Novel domain stress test**: Apply MARVIS to a held-out domain with known challenging embedding properties (e.g., highly overlapping classes, non-convex manifolds) and compare performance against specialized embedding models. This tests the universal prediction claim beyond benchmark optimization.