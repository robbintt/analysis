---
ver: rpa2
title: Investigating Neurons and Heads in Transformer-based LLMs for Typographical
  Errors
arxiv_id: '2502.19669'
source_url: https://arxiv.org/abs/2502.19669
tags:
- typo
- neurons
- heads
- typos
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer-based LLMs handle inputs
  with typographical errors by identifying specific neurons and attention heads that
  respond to typos. The authors propose a method to measure neuron and head activation
  differences between clean and typo-containing inputs, then use ablation studies
  to assess their impact.
---

# Investigating Neurons and Heads in Transformer-based LLMs for Typographical Errors

## Quick Facts
- arXiv ID: 2502.19669
- Source URL: https://arxiv.org/abs/2502.19669
- Reference count: 24
- This paper investigates how transformer-based LLMs handle inputs with typographical errors by identifying specific neurons and attention heads that respond to typos.

## Executive Summary
This paper investigates how transformer-based LLMs handle inputs with typographical errors by identifying specific neurons and attention heads that respond to typos. The authors propose a method to measure neuron and head activation differences between clean and typo-containing inputs, then use ablation studies to assess their impact. Results show that typo neurons exist in both early/late layers (handling local context) and middle layers (handling global context), while typo heads fix errors by broadly considering context rather than focusing on specific tokens. Both typo neurons and heads also contribute to general grammatical and morphological understanding. The study reveals that LLMs fix typos through both local and global context processing, suggesting robustness improvements should emphasize both context types.

## Method Summary
The paper proposes a method to identify "typo neurons" and "typo heads" in LLMs by comparing activation patterns between clean, typo, and split-tokenized inputs. For neurons, they calculate a typo neuron score Δn as the activation difference between typo and the maximum of clean/split inputs. For heads, they measure KL divergence from uniform attention distribution, with negative Δh scores indicating heads that broadly attend to context. The authors then perform ablation studies by zeroing out identified components and measuring performance degradation on word identification tasks.

## Key Results
- Typo neurons are concentrated in early/late layers (local context) and middle layers (global context) depending on the model
- Typo heads fix errors by adopting broad attention distributions rather than focusing on specific tokens
- Ablating typo neurons and heads degrades performance on both clean and typo inputs, suggesting functional entanglement with general grammatical processing
- Performance degradation varies by model size, with smaller models showing more severe drops from typo head ablation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Typo correction in LLMs appears to be processed by distinct "typo neurons" located in specific layer regions depending on whether local or global context is required.
- **Mechanism:** Neurons in early and late layers handle typo-fixing via local contexts (detokenization/retokenization), while middle-layer neurons handle fixing via global contexts (feature engineering). The model can often correct typos using local processing if either early or late layers are active, but middle layers provide core global correction.
- **Core assumption:** The functional division of transformer layers (early/late = local, middle = global) proposed in prior work holds true for typo-specific processing.
- **Evidence anchors:**
  - [abstract] "LLMs can fix typos with local contexts when the typo neurons in either the early or late layers are activated... Typo neurons in the middle layers are responsible for the core of typo-fixing with global contexts."
  - [Section 4.2] "Gemma 2 fixes typos as Detokenization, while LLaMA 3 family and Qwen 2.5 fix typos as Retokenization... In the middle layers (i.e., 0.2-0.8), all models have many typo neurons."
  - [corpus] *Even Heads Fix Odd Errors* supports the concept of localized error correction in attention layers, though focused on heads rather than neurons.
- **Break condition:** If a model utilizes a non-standard layer architecture (e.g., depth-wise processing without distinct global/local stages), this spatial distribution of typo neurons may not hold.

### Mechanism 2
- **Claim:** "Typo heads" (attention heads) correct errors by distributing attention broadly across the context rather than focusing on specific error tokens.
- **Mechanism:** These heads exhibit a shift toward a uniform attention distribution when typos are present (indicated by negative $\Delta_h$ scores), effectively "averaging" the context to infer the correct token from the surrounding sentence structure rather than isolating the typo.
- **Core assumption:** A wider attention distribution (closer to uniform) allows the model to bypass the corrupted token representation and rely on aggregate semantic context.
- **Evidence anchors:**
  - [abstract] "Typo heads fix typos by broadly considering the context rather than focusing on specific tokens."
  - [Section 5.1] "A large negative $\Delta_h$ indicates the head that widely attends contexts for typo-fixing."
  - [Section 5.2] "Heads recognize and fix typos by observing the wider context... few heads near the minimum $\Delta_h$ are distinctive."
  - [corpus] *Focus Directions Make Your Language Models Pay More Attention* discusses "contextual heads" that manage overall attention, tangentially supporting the role of specific heads in context management.
- **Break condition:** If the typo is in a highly ambiguous short context where global averaging dilutes the limited semantic clues, this broad-focus mechanism might fail.

### Mechanism 3
- **Claim:** Neurons responsible for typo correction are functionally entangled with general grammatical and morphological processing, not solely dedicated to error correction.
- **Mechanism:** When "typo neurons" are ablated (deactivated), performance drops on clean inputs as well as typo inputs. This suggests these neurons implement general linguistic rules (e.g., morphology) that incidentally enable the model to "repair" corrupted inputs by mapping them to valid linguistic structures.
- **Core assumption:** Performance degradation on clean inputs implies the ablated neurons were performing a general function necessary for both clean and noisy processing.
- **Evidence anchors:**
  - [abstract] "Typo neurons and typo heads work not only for typo-fixing but also for understanding general contexts."
  - [Section 4.3.1] "The ablation of typo neurons also resulted in a larger performance decrease... This indicates that typo neurons may not exclusively act on typos but could also play a crucial role in processing general grammatical or morphological features."
  - [corpus] Corpus evidence for this specific "shared mechanism" is weak; *Understanding and Controlling Repetition Neurons* discusses skill neurons but does not explicitly link them to shared grammatical/morphological functions in the provided text.
- **Break condition:** If distinct "error-correction only" neurons exist in significantly larger models (>32B parameters) or models trained with explicit denoising objectives, this entanglement might be reduced.

## Foundational Learning

- **Concept:** **Feed-Forward Network (FFN) Neurons as Knowledge/Skill Stores**
  - **Why needed here:** The paper identifies "typo neurons" within FFN layers. Understanding that individual neurons can store specific skills or knowledge (e.g., "merging subwords") is required to grasp how deactivating them affects typo correction.
  - **Quick check question:** Can you explain why setting a specific neuron's output to zero (ablation) would specifically degrade typo correction without destroying the entire model?

- **Concept:** **Attention Head Distribution (KL Divergence)**
  - **Why needed here:** The paper quantifies "typo heads" using KL divergence against a uniform distribution. You need to understand that high KL = focused attention and low/negative KL = broad/distributed attention.
  - **Quick check question:** If an attention head shifts from a peaked distribution to a uniform distribution when a typo appears, what does that imply about how it is trying to solve the problem?

- **Concept:** **Local vs. Global Context in Transformers**
  - **Why needed here:** The paper maps typo processing to specific layer depths based on context scope (Early/Late = Local, Middle = Global).
  - **Quick check question:** Why would a "middle" layer be better suited for fixing a typo that requires understanding the whole sentence, compared to an "early" layer?

## Architecture Onboarding

- **Component map:**
  - Input: Tokenized text (Clean vs. Typo vs. Split)
  - Processing:
    - Early/Late Layers: FFN Neurons (Local typo handling)
    - Middle Layers: FFN Neurons (Global typo handling)
    - All Layers: Attention Heads (Broad context aggregation)
  - Metrics: Activation Score ($\Delta_n$) for neurons; KL Divergence difference ($\Delta_h$) for heads

- **Critical path:**
  1. Identify important tokens (gradient-based importance)
  2. Inject typos → Tokenizer splits tokens differently
  3. Model processes input; "Typo Neurons" activate in FFNs
  4. "Typo Heads" broaden attention to encompass context
  5. Output: Corrected word prediction

- **Design tradeoffs:**
  - Redundancy vs. Efficiency: Small models rely heavily on specific typo heads (ablation causes high damage), while large models seem to have redundancy (ablation causes less damage)
  - Generalization: Improving typo robustness likely requires training on general grammatical tasks, not just typo-specific data, due to the shared neuron mechanism

- **Failure signatures:**
  - Local Failure: If early/late typo neurons are ablated or fail, the model may struggle to merge split subwords (e.g., "youneg" → "young")
  - Global Failure: If middle typo neurons fail, the model may fix the word locally but choose a word that doesn't fit the sentence's global meaning

- **First 3 experiments:**
  1. **Neuron Activation Deltas:** Run clean vs. typo pairs through the model and calculate $\Delta_n$ scores for FFN neurons to identify the top 0.5% "typo neurons" in your specific architecture
  2. **Targeted Ablation:** Zero out the identified top typo neurons and measure the performance drop on both clean and noisy datasets to confirm functional entanglement
  3. **Attention Visualization:** Visualize the attention maps of heads with the most negative $\Delta_h$ scores to verify they are adopting "broad context" patterns when typos are present

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can analysis methods distinguish between neurons/heads that actively repair typos and those that merely respond to or are "damaged" by the noise?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that their "method mostly detected neurons and heads that respond to inputs with typos," but "it cannot distinguish between those that contribute to typo-fixing and those that are damaged by typos."
- **Why unresolved:** The current methodology identifies components based on activation differences ($\Delta_n$), which indicates a correlation with typo presence but does not isolate the causal mechanism of error correction versus error detection.
- **What evidence would resolve it:** A causal tracing or intervention study that measures the direct effect of specific neuron ablation on the probability of the corrected output token versus the corrupted token.

### Open Question 2
- **Question:** What specific architectural or training data factors cause typo neurons to concentrate in early layers for some models (Gemma 2) but late layers for others (Llama 3, Qwen 2.5)?
- **Basis in paper:** [explicit] The authors note a distinct variance where Gemma 2 fixes typos in early layers while others do so in late layers, but state in the Limitations: "This may be due to differences in training methods or datasets. However, the true reason remains unclear."
- **Why unresolved:** The study is observational across pre-trained models; it lacks controlled experiments to determine if the distribution shift is caused by model architecture, tokenization strategies, or the nature of the pre-training corpus.
- **What evidence would resolve it:** A comparative training analysis where identical architectures are trained on different data regimes, or an analysis of intermediate checkpoints to observe when these layer-specific neuron distributions emerge.

### Open Question 3
- **Question:** Does fine-tuning on grammatical error correction (GEC) tasks enhance the functional density or efficacy of identified typo neurons and heads?
- **Basis in paper:** [explicit] In the Future Work section, the authors propose "investigating how additional training on tasks such as grammatical error correction or determining whether a given subword is part of a specific word affects robustness against typos."
- **Why unresolved:** The current work focuses solely on interpreting the internal mechanics of existing pre-trained models without exploring interventions to improve these specific mechanisms.
- **What evidence would resolve it:** Experiments measuring the change in $\Delta_n$ scores and ablation impacts for typo neurons before and after fine-tuning on GEC datasets.

## Limitations
- The method identifies components that respond to typos but cannot distinguish between those that actively repair typos versus those damaged by the noise
- The reason for layer-specific concentration of typo neurons (early vs. late) varies by model and remains unclear
- The study is limited to single runs, which may introduce variability in the identified components

## Confidence
- **High Confidence:** The existence of typo neurons and heads that respond differently to clean vs. typo inputs (measured by Δn and Δh scores) is well-supported by the ablation results showing clear performance degradation
- **Medium Confidence:** The claim that typo neurons are functionally entangled with general grammatical processing is supported by the ablation results but lacks strong independent evidence in the corpus
- **Medium Confidence:** The mechanism that typo heads fix errors through broad context distribution (negative Δh) is plausible given the data, though the exact attention patterns would benefit from direct visualization

## Next Checks
1. **Cross-Model Architecture Test:** Verify the local/global typo neuron distribution pattern holds across different transformer architectures (e.g., decoder-only, encoder-decoder, different depth patterns) to confirm the layer-region mapping isn't architecture-specific
2. **Attention Pattern Visualization:** Generate and analyze attention heatmaps for the most negative Δh heads when processing typos to confirm they are adopting the proposed "broad context" distribution rather than other patterns
3. **Clean Input Specificity Test:** Design an experiment where typo neurons are ablated on inputs with grammatical errors (but no typos) to test whether the performance drop is specifically due to loss of typo-correction capability or general grammatical processing ability