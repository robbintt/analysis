---
ver: rpa2
title: Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold
  Networks
arxiv_id: '2602.02056'
source_url: https://arxiv.org/abs/2602.02056
tags:
- online
- learning
- on-chip
- updates
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of ultrafast, model-free online
  learning on sub-microsecond timescales for applications like quantum control and
  nuclear fusion, where conventional MLPs are inefficient and numerically unstable
  under strict latency, memory, and fixed-point precision constraints. The core insight
  is that Kolmogorov-Arnold Networks (KANs) with B-spline activations enable sparse,
  localized gradient updates, making them far more efficient and stable than MLPs
  for on-chip training.
---

# Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2602.02056
- Source URL: https://arxiv.org/abs/2602.02056
- Reference count: 38
- This paper tackles ultrafast, model-free online learning on sub-microsecond timescales for applications like quantum control and nuclear fusion, where conventional MLPs are inefficient and numerically unstable under strict latency, memory, and fixed-point precision constraints.

## Executive Summary
This paper demonstrates that Kolmogorov-Arnold Networks (KANs) with B-spline activations enable sparse, localized gradient updates that make them far more efficient and stable than MLPs for on-chip training. By implementing KANs on FPGAs with deterministic, fixed-point arithmetic, the authors achieve sub-100ns forward/backward passes and fully on-chip parameter updates—three to four orders of magnitude faster than host-accelerator training loops. Across experiments on drifting regression, adaptive qubit readout, and non-stationary control, KANs consistently outperform parameter-matched MLPs, converging faster, adapting to non-stationary dynamics more robustly, and remaining stable under aggressive quantization.

## Method Summary
The method implements KANs with B-spline activations on FPGAs using fixed-point arithmetic for ultrafast online learning. The core innovation is exploiting B-spline locality: each input activates only s=p+1 coefficients per edge, enabling sparse gradient updates. The implementation includes grid indexers to compute active cells, LUT-based B-spline basis evaluations, and on-chip parameter storage. Training uses single-sample SGD with fixed-point updates, achieving sub-100ns latency. The approach is validated on three tasks: drifting regression with regime changes, adaptive qubit readout with rotating XOR constellation, and non-stationary Acrobot control.

## Key Results
- KANs achieve sub-100ns forward/backward passes on FPGAs versus milliseconds for host-accelerator training
- KANs remain stable under aggressive quantization (6-8 bit total width) while MLPs often diverge
- Across all three benchmark tasks, KANs converge faster and adapt more robustly to non-stationary dynamics than parameter-matched MLPs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KANs achieve lower per-sample update cost than MLPs under equal parameter budgets due to B-spline locality.
- **Mechanism:** B-spline basis functions have local support—at any input coordinate, only s = p+1 coefficients (where p is spline order) are non-zero. Gradient updates touch only these active coefficients, not the full parameter tensor.
- **Core assumption:** Inputs fall within the spline grid domain; grid resolution G >> s (typical: G=10, s=4).
- **Evidence anchors:**
  - [abstract] "KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling"
  - [section 3.2] Theorem 3.3: Cupdate(KAN) = s/(G+s) × Cupdate(MLP)
  - [corpus] No direct corroboration; corpus papers focus on KAN applications, not hardware-efficient training mechanics
- **Break condition:** If grid is too coarse relative to input distribution, multiple samples may activate identical coefficients, increasing interference.

### Mechanism 2
- **Claim:** KAN capacity can scale via grid size G without proportionally increasing per-sample compute.
- **Mechanism:** Increasing G adds stored coefficients (memory depth) but active computation per sample remains O(s) per edge. Approximation error improves as O(G^{-k-1+m}) for smooth targets.
- **Core assumption:** Target function is sufficiently smooth; on-chip memory (BRAM/LUTRAM) available for coefficient storage.
- **Evidence anchors:**
  - [section 3.3] "increasing G adds stored coefficients but not active computation"
  - [figure 1] KAN latency remains sub-100ns across parameter counts while MLP grows linearly
  - [corpus] MatrixKAN paper notes slow training but does not address locality-based acceleration
- **Break condition:** Memory exhaustion before compute limits; quantization floor limits returns from larger G (Figure 5 shows bitwidth-dependent ceiling).

### Mechanism 3
- **Claim:** KANs are inherently more stable than MLPs under fixed-point quantization.
- **Mechanism:** (1) Forward pass: KAN activations are convex combinations of coefficients, bounded by [min W_g, max W_g] independent of input magnitude. (2) Backward pass: gradients multiply by B_g(x) ∈ [0,1], not by input values.
- **Core assumption:** Coefficients initialized within representable range; B-spline LUT values precomputed accurately.
- **Evidence anchors:**
  - [section 3.4] Theorem 3.4: min_i W_i ≤ φ(x) ≤ max_i W_i
  - [section 3.4] Theorem 3.5: Var[ϵ_KAN] = O(Δ²), independent of input statistics; Var[ϵ_MLP] = O(Δ² Var[x])
  - [corpus] QuantKAN paper addresses KAN quantization but targets inference, not online training stability
- **Break condition:** Extremely low bitwidth (< 6 bits total) may still cause divergence; coefficient dynamic range must fit integer precision budget.

## Foundational Learning

- **Concept: B-spline local support**
  - Why needed here: Understanding why only s=p+1 basis functions are active per sample is essential to grasping update sparsity.
  - Quick check question: For a cubic B-spline (p=3) with G=10 grid cells, how many coefficients does a single input activate per edge? (Answer: 4)

- **Concept: Fixed-point arithmetic (⟨W, I⟩ notation)**
  - Why needed here: All hardware results use fixed-point; understanding overflow/clipping behavior is critical for reproducing stability claims.
  - Quick check question: In ⟨8, 3⟩ format with 2 integer bits (sign + magnitude), what's the representable range? (Answer: approximately [-2, 2] with 6 fractional bits)

- **Concept: Online learning protocol**
  - Why needed here: Experiments use single-sample SGD updates, not batch training; this differs from typical PyTorch workflows.
  - Quick check question: Why does the paper avoid replay buffers for the Acrobot task? (Answer: sub-microsecond latency constraint requires streaming updates)

## Architecture Onboarding

- **Component map:**
  - KAN layer: Grid indexer (computes cell k, fractional index u) → LUT lookup (p+1 basis values) → coefficient fetch (active subset) → accumulate → output
  - Context buffer: Stores (k, u) per edge during forward pass for reuse in backward
  - Parameter storage: 3D array Ws[o][i][c], partitioned across o/i dimensions
  - ROM LUTs: Precomputed B_r[u] and dB_r[u] values, bound to LUTRAM

- **Critical path:**
  1. Forward: Input x → compute (k, u) → fetch p+1 coefficients + LUT values → multiply-accumulate
  2. Backward: Upstream gradient × LUT values → update only active coefficients → compute downstream gradient using derivative LUT
  3. Latency bottleneck: Coefficient fetch and reduction across input dimension (unrolled where resources permit)

- **Design tradeoffs:**
  - Grid size G vs. bitwidth: Larger G improves approximation until quantization floor
  - Spline order p: Higher p increases per-edge compute (s=p+1) but improves smoothness
  - Parallelism: Unrolling across output dimension increases DSP usage; partial unrolling trades latency for resources (Figure 10)
  - Assumption: LUT resolution F controls interpolation quality; higher F increases ROM size logarithmically

- **Failure signatures:**
  - MLP divergence under low bitwidth: Regret grows steadily, high variance across seeds (Figure 4)
  - KAN quantization floor: Accuracy plateaus despite increasing G when bitwidth is insufficient (Figure 5)
  - MLP resource explosion: DSP/FF/LUT scale linearly with parameters; may exceed FPGA capacity for larger models

- **First 3 experiments:**
  1. **Replicate drifting regression (Section 5.2):** Implement KAN [1,1] with G=10, p=3, ⟨6,2⟩ fixed-point. Verify cumulative regret stays below 50 across 1500 steps with regime changes at t=500, 1000.
  2. **Quantization sweep:** Run adaptive function approximation with total bitwidths W∈{5,6,7,8} and fixed 2 integer bits. Plot final regret; expect KAN stable, MLP-L showing sharp degradation below W=7.
  3. **Latency measurement:** Synthesize KAN and parameter-matched MLP for single-shot qubit readout task. Target sub-200ns forward+backward on Virtex UltraScale+ at 200 MHz. Verify KAN achieves ≥2× higher update frequency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do adaptive optimizers (e.g., Adam) perform under fixed-point constraints for KANs compared to the currently used SGD?
- **Basis in paper:** [explicit] The authors state in Section 7, "More work also needs to be done in understanding how other optimizers (e.g. Adam) work under fixed-point learning dynamics for KAN and MLP architectures."
- **Why unresolved:** The current implementation relies on SGD for simplicity; adaptive methods require storing momentums (state variables), which may introduce instability or resource overhead under aggressive quantization.
- **What evidence would resolve it:** FPGA synthesis results and convergence plots showing KAN stability and resource usage when using fixed-point Adam or RMSprop updates.

### Open Question 2
- **Question:** Can accumulating gradients over short temporal windows improve training stability without breaking the sub-microsecond latency budget?
- **Basis in paper:** [explicit] Section 7 notes that "kernels could be extended to accumulate gradients over short windows, trading latency and resources for stability while remaining streaming-friendly."
- **Why unresolved:** The current study uses single-batch updates to achieve ultra-low latency; the specific trade-off curve between window size, FPGA resource usage, and convergence stability is unexplored.
- **What evidence would resolve it:** Profiling the latency and accuracy of a windowed-gradient KAN kernel on the target FPGA to find the maximum stable window size under 1μs.

### Open Question 3
- **Question:** Do auxiliary training mechanisms like layer normalization or dropout alter the relative performance gap between KANs and MLPs in on-chip scenarios?
- **Basis in paper:** [explicit] The authors mention, "Future work can incorporate these enhancements [layer normalization/dropout] and quantify the resulting accuracy–resource–latency trade-offs."
- **Why unresolved:** The paper uses a streamlined MLP baseline to isolate architectural effects, leaving open whether standard software regularization techniques would rescue MLP stability or simply add prohibitive hardware overhead.
- **What evidence would resolve it:** A comparative hardware study measuring the DSP/LUT cost and convergence rate of KANs vs. MLPs when these auxiliary layers are implemented in fixed-point logic.

## Limitations

- The paper doesn't specify exact B-spline grid range [x_min, x_max] per layer and LUT resolution parameter F, which impacts numerical accuracy
- The stability analysis focuses on specific bitwidths and spline orders, with unclear behavior at extremely low bitwidths (< 6 bits) or different spline orders
- The comparison baseline uses parameter-matched MLPs but doesn't explore other potential alternatives like quantized neural networks or specialized hardware architectures

## Confidence

- **High Confidence (4-5/5):** The core claims about KANs having lower per-sample update cost due to B-spline locality and achieving sub-100ns latency on FPGAs are well-supported by theoretical analysis and experimental results
- **Medium Confidence (3-4/5):** The claims about KANs being inherently more stable under fixed-point quantization are supported by theoretical analysis and some experimental evidence, but the stability analysis could be more comprehensive
- **Low Confidence (1-2/5):** Claims about KANs scaling capacity via grid size G without proportionally increasing per-sample compute are theoretically sound but lack extensive experimental validation across different problem domains

## Next Checks

1. **Grid resolution sensitivity analysis:** Systematically vary the B-spline grid size G from 5 to 50 and measure the impact on approximation error, FPGA resource utilization, and latency to validate whether the claimed O(s) per-sample complexity holds across a wider range of grid sizes

2. **Cross-domain generalization test:** Apply the KAN architecture to a different ultrafast learning domain (e.g., high-frequency trading signals or real-time video analytics) to verify whether the advantages observed in quantum control and regression tasks generalize to other domains with strict latency requirements

3. **Extended quantization analysis:** Test KAN and MLP performance across a broader range of bitwidths (from 4 to 16 bits) and different quantization schemes (including non-uniform quantization) to establish the precise conditions under which KAN stability advantages manifest and whether there are fundamental limits to this advantage