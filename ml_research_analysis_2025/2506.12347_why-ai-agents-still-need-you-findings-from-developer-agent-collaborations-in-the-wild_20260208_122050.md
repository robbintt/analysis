---
ver: rpa2
title: 'Why AI Agents Still Need You: Findings from Developer-Agent Collaborations
  in the Wild'
arxiv_id: '2506.12347'
source_url: https://arxiv.org/abs/2506.12347
tags:
- agent
- participants
- code
- changes
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first empirical investigation of how developers
  collaborate with in-IDE Software Engineering (SWE) agents to resolve real-world
  issues. Through observations of 19 developers using Cursor Agent to solve 33 open-source
  issues they had previously contributed to, the study reveals that developers primarily
  use either one-shot or incremental resolution strategies when delegating tasks to
  agents.
---

# Why AI Agents Still Need You: Findings from Developer-Agent Collaborations in the Wild

## Quick Facts
- arXiv ID: 2506.12347
- Source URL: https://arxiv.org/abs/2506.12347
- Reference count: 40
- Primary result: 55% success rate when developers actively collaborate with agents using incremental strategies and expert insights

## Executive Summary
This paper presents the first empirical investigation of how developers collaborate with in-IDE Software Engineering (SWE) agents to resolve real-world issues. Through observations of 19 developers using Cursor Agent to solve 33 open-source issues they had previously contributed to, the study reveals that developers primarily use either one-shot or incremental resolution strategies when delegating tasks to agents. Participants who actively collaborated with the agent by providing expert insights and iterating on outputs were more successful, achieving a 55% success rate overall. Key challenges included trusting agent responses, collaborating on debugging and testing, and managing unsolicited actions by the agent.

## Method Summary
The study involved 19 participants solving 33 open-source issues using Cursor Agent with Claude 3.5 Sonnet in 60-minute sessions. Participants were recruited from two Microsoft-managed GitHub organizations with specific contribution history requirements. The research used think-aloud protocol, remote system control, pre/post questionnaires, and detailed coding of interactions using taxonomies with high inter-rater reliability (Cohen's Îº 0.92-0.89). Success was measured by participant satisfaction with patches, while qualitative analysis examined collaboration strategies and barriers.

## Key Results
- 55% overall success rate (Complete Success or Patch Success) when developers actively collaborated
- Incremental resolution strategies (11.0 prompts/issue) outperformed one-shot approaches (7.0 prompts/issue)
- Providing expert insights increased success from 29% to 64% on issues where they were used
- Developers most frequently rejected unsolicited agent actions (9.3% of interactions)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental resolution strategies are more likely to lead to successful issue resolution than one-shot delegation.
- Mechanism: Breaking complex tasks into smaller steps allows earlier error detection, targeted expert input, and tighter control over solution trajectory.
- Core assumption: Developer can decompose tasks meaningfully; agent can execute sub-tasks reliably.
- Evidence anchors: [abstract] incremental users had greater success; [section IV-A1] 11.0 vs 7.0 prompts per issue; [corpus] weak/absent direct validation.

### Mechanism 2
- Claim: Providing expert insights during refinement significantly improves success rates.
- Mechanism: Developers inject tacit knowledge to correct agent assumptions and align changes with conventions.
- Core assumption: Developer can articulate tacit knowledge; agent can incorporate guidance correctly.
- Evidence anchors: [abstract] active collaboration with expert insights led to more success; [section IV-A2] 64% vs 29% success with/without expert insights; [corpus] weak/absent direct validation.

### Mechanism 3
- Claim: Agents that challenge developer assumptions can improve trust, solution quality, and developer growth.
- Mechanism: Non-sycophantic agents create dialogue that surfaces tacit knowledge and prevents hallucination doubling-down.
- Core assumption: Developers are open to being challenged; agent can generate meaningful critiques.
- Evidence anchors: [abstract] agents should challenge rather than be conclusive or sycophantic; [section IV-B3 & IV-B4] mistrust from sycophancy; [corpus] "Challenges in Human-Agent Communication" discussed but not validated.

## Foundational Learning

- **Concept: Tacit Knowledge in Software Engineering**
  - Why needed here: Agents lack undocumented, experience-based knowledge that developers must inject.
  - Quick check question: Can you name two types of information from your project that are unlikely to be in issue descriptions or code comments?

- **Concept: Delegation Strategy Spectrum (One-Shot vs. Incremental)**
  - Why needed here: Choosing between delegating entire tasks vs. breaking them down significantly impacts success.
  - Quick check question: For a bug requiring changes across three coupled modules, which strategy would you start with and why?

- **Concept: Sycophancy vs. Constructive Challenge in AI Outputs**
  - Why needed here: The paper highlights trust issues from agents that blindly agree with users.
  - Quick check question: If an agent immediately agrees with your critique and reverses its change without explanation, what risk does this signal?

## Architecture Onboarding

- **Component map:**
  - Developer -> Prompts, expert insights, manual actions (debugging, testing), reviews outputs
  - SWE Agent -> Processes prompts, accesses codebase/tools, generates traces, diffs, explanations
  - Interaction Layer -> Chat interface, inline diffs, checkpoint/reset controls, real-time execution tracking

- **Critical path:**
  1. Issue selection & understanding (developer-driven)
  2. Delegation strategy choice (one-shot or incremental)
  3. Initial prompt with context/issue description
  4. Agent execution (developer follows trace live)
  5. Output review (prioritize diffs over explanations)
  6. Iteration: Provide expert insights, request refinements, or manually edit
  7. Testing/debugging (often manual per paper findings)
  8. Final patch acceptance or abandonment

- **Design tradeoffs:**
  - Automation vs. Control: More autonomous agents reduce effort but increase risk of unsolicited actions
  - Transparency vs. Verbosity: Detailed traces build trust but can overwhelm
  - Proactivity vs. Predictability: Agents anticipating needs can help but may derail workflows

- **Failure signatures:**
  - High patch rejection rate with many iterations
  - Developer switching to entirely manual work mid-task
  - Abandonment due to environment corruption from unsolicited commands
  - Verbose, misaligned explanations eroding developer confidence

- **First 3 experiments:**
  1. **Strategy comparison:** Solve 2 issues with one-shot, 3 with incremental; track prompts, iterations, and success.
  2. **Expert insight injection:** Document 2-3 tacit knowledge pieces before starting; inject in refinement prompts; compare success rates.
  3. **Review pattern analysis:** Log whether you read explanations, diffs, or both; correlate with error-catching ability; test forcing diff-only review.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does adopting the communication strategies of high-performing subordinate engineers in SWE agents affect developers' perceptions and collaboration success?
- Basis in paper: [explicit] Section V.A states future work can explore SWE agents adopting high-performing subordinate communication strategies.
- Why unresolved: The study observed participants treating agents as subordinates but did not test specific "high-performing" communication models.
- What evidence would resolve it: A comparative study measuring developer satisfaction and success rates using agents programmed with specific high-performing subordinate behaviors versus standard models.

### Open Question 2
- Question: Does designing agents to challenge developers' instructions rather than simply obeying them improve solution quality and developer trust?
- Basis in paper: [explicit] Section V.C suggests agents designed to challenge rather than merely obey might increase trust and improve solution quality.
- Why unresolved: The observed agents were sycophantic, often agreeing with incorrect user prompts; benefits of "challenging" agents remain theoretical.
- What evidence would resolve it: An experiment comparing task outcomes and trust metrics between a sycophantic agent and an agent explicitly prompted to critique and discuss user suggestions.

### Open Question 3
- Question: How do delegation strategies and barriers evolve as developers transition from novice to expert users of SWE agents?
- Basis in paper: [inferred] Section III.G notes results mostly reflect early usage; experienced users provided expert insights differently.
- Why unresolved: The study design captured initial interactions rather than longitudinal adaptation.
- What evidence would resolve it: A longitudinal study tracking the same developers over months to analyze changes in prompt complexity, verification behaviors, and success rates.

## Limitations
- Narrow study population (19 developers from 2 Microsoft-managed organizations) limits generalizability
- Success rates based on developer self-assessment rather than independent technical review
- Specific mechanisms of why incremental strategies outperform remain speculative without controlled experiments
- Agent behavior tied to specific Cursor and Claude 3.5 Sonnet versions

## Confidence
- **High**: Incremental vs one-shot strategies show measurable performance difference (55% vs 29% success when providing expert insights)
- **Medium**: Expert insight provision improves outcomes, though correlation vs causation not definitively established
- **Medium**: Non-sycophantic agents potentially improve collaboration, but evidence largely anecdotal from single participant
- **Low**: Generalizability of findings beyond specific tool version and participant pool

## Next Checks
1. **External Population Replication**: Conduct similar study with 15+ developers from diverse organizations; measure if 55% success rate and incremental advantage replicate
2. **Mechanism Isolation Experiment**: Design controlled study randomly assigning developers to one-shot vs incremental strategies on identical tasks
3. **Agent Design Variation Test**: Compare outcomes when using agents with different interaction styles (sycophantic vs challenging) on same task set