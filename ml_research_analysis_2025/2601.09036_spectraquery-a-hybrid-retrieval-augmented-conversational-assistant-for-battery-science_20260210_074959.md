---
ver: rpa2
title: 'SpectraQuery: A Hybrid Retrieval-Augmented Conversational Assistant for Battery
  Science'
arxiv_id: '2601.09036'
source_url: https://arxiv.org/abs/2601.09036
tags:
- raman
- literature
- battery
- data
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpectraQuery is a hybrid QA system that integrates structured Raman
  spectroscopy data with unstructured battery literature, enabling researchers to
  pose natural-language questions combining both sources. It uses a SUQL planner to
  generate coordinated SQL queries and literature search terms, executes them in parallel,
  and synthesizes grounded, cited answers via retrieval-augmented generation.
---

# SpectraQuery: A Hybrid Retrieval-Augmented Conversational Assistant for Battery Science

## Quick Facts
- **arXiv ID**: 2601.09036
- **Source URL**: https://arxiv.org/abs/2601.09036
- **Reference count**: 40
- **Primary result**: Hybrid system achieves ~80% SQL correctness and 93-97% answer groundedness by integrating structured spectroscopy data with unstructured battery literature.

## Executive Summary
SpectraQuery is a hybrid QA system that integrates structured Raman spectroscopy data with unstructured battery literature, enabling researchers to pose natural-language questions combining both sources. It uses a SUQL planner to generate coordinated SQL queries and literature search terms, executes them in parallel, and synthesizes grounded, cited answers via retrieval-augmented generation. Evaluations show ~80% SQL correctness, 93-97% answer groundedness with 10-15 retrieved passages, and expert ratings of 4.1-4.6/5 across accuracy, relevance, grounding, and clarity. The system demonstrates that hybrid retrieval architectures can meaningfully support scientific workflows by bridging data and discourse for high-volume experimental datasets.

## Method Summary
SpectraQuery implements a two-stage pipeline where a SUQL planner (GPT-4 via LiteLLM) translates natural language questions into SQL queries for operando Raman spectroscopy data and keyword queries for literature search. These execute in parallel against a SQLite database of peak parameters and a ChromaDB vector store of embedded literature chunks. The synthesis LLM then fuses results into cited answers. The system was validated on LTMO cathode Raman data (114 timesteps, 30×30 spatial grid) and 50 battery/Raman PDFs, with performance measured on a 30-question benchmark and expert review panel.

## Key Results
- SQL correctness reaches ~80% for generated queries on benchmark questions
- Answer groundedness scales from 83% to 97% as retrieval depth increases from 5 to 15 passages
- Expert ratings: accuracy 4.1/5, relevance 4.6/5, grounding 4.2/5, clarity 4.6/5, citation interpretability 3.27/5
- Literature retrieval: Precision@5 = 0.58, Recall@5 = 0.60, UniqueDocs@3 ≈ 1.8

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing open-ended scientific questions into parallel structured and unstructured retrieval operations enables multi-modal reasoning that neither modality alone can achieve.
- Mechanism: The SUQL planner uses an LLM with schema-aware prompting and few-shot examples to parse natural language into: (1) a SQL SELECT query over spectroscopy peak parameters and (2) a keyword query for vector-based literature search. These execute in parallel, returning numerical evidence and textual passages that are then fused by a synthesizer LLM.
- Core assumption: The LLM planner can reliably map scientific intent to correct SQL joins/filters and meaningful literature keywords; failures in either branch directly degrade answer quality.
- Evidence anchors:
  - [abstract] "translates open-ended questions into coordinated SQL and literature retrieval operations"
  - [Section 3.4] "The SUQL planner uses an LLM... to transform the user's natural language question into two outputs: 1) a SQL SELECT query... and 2) a concise keyword query for the literature search"
  - [corpus] HySemRAG paper demonstrates related hybrid ETL+RAG synthesis; limited direct evidence for SUQL-style dual-path decomposition outside this work.
- Break condition: If SQL correctness drops (e.g., complex multi-join queries) OR retrieval misses key mechanisms (low recall), synthesized answers become incomplete or over-generalized regardless of planner quality.

### Mechanism 2
- Claim: Groundedness of synthesized answers scales with retrieval depth up to ~10-15 passages, after which returns diminish.
- Mechanism: The answer synthesizer conditions on concatenated SQL results plus top-k retrieved passages. With more passages, coverage of relevant mechanisms increases, reducing unsupported claims. Gains plateau when additional passages become redundant or noise-dominant.
- Core assumption: Retrieved passages are relevant and diverse enough to cover mechanistic explanations; the LLM accurately attributes claims to provided context.
- Evidence anchors:
  - [abstract] "synthesized answers reach 93-97% groundedness when conditioned on 10-15 retrieved passages"
  - [Section 4.1.2] "83.3% of answers are fully grounded with top-5 passages, rising to 93.3% with top-10 and 96.7% with top-15"
  - [corpus] PaperHelper demonstrates RAG-based grounded QA for scientific literature; ScienceDB AI shows agentic retrieval for datasets but limited hybrid grounding evidence.
- Break condition: Low retrieval diversity (UniqueDocs@3 ≈ 1.8) means passages cluster on single sources; missing alternative mechanisms causes incomplete synthesis even with high k.

### Mechanism 3
- Claim: SQL correctness depends critically on schema-aware prompting with example-driven patterns for complex scientific operations (ratios, cross-timestep comparisons).
- Mechanism: The planner prompt explicitly describes allowed tables/fields and includes few-shot examples for derived metrics (e.g., D/G ratio via self-join of peaks) and spatiotemporal comparisons (joining samples on x,y coordinates across timesteps). This guides the LLM to generate executable, semantically correct queries.
- Core assumption: The LLM generalizes from examples to novel query structures; errors occur when queries require patterns not covered in examples or involve multiple nested conditions.
- Evidence anchors:
  - [abstract] "approximately 80% of generated SQL queries are fully correct"
  - [Section 3.4.1] "Example-driven patterns: We included in the prompt an example of computing the D/G ratio via self-join of peaks... Another example shows how to compare across timesteps"
  - [corpus] FloodSQL-Bench addresses geospatial Text-to-SQL complexity; limited evidence for scientific domain-specific schema-aware prompting patterns.
- Break condition: Queries requiring >2 table joins, derived metrics not in examples, or ambiguous natural language phrasing ("early cycles") lead to partial/incorrect SQL (~20% failure rate concentrated in complex queries).

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core synthesis mechanism; understanding retrieval-k dependence and groundedness tradeoffs is essential for debugging answer quality.
  - Quick check question: If groundedness is 83% at k=5 and 97% at k=15, what does this imply about the marginal value of additional passages?

- Concept: **Text-to-SQL Semantic Parsing**
  - Why needed here: The SUQL planner converts natural language to executable SQL; understanding schema constraints, join patterns, and failure modes is critical.
  - Quick check question: What SQL pattern would compute a D/G ratio from a peaks table where each row is one peak with a family field?

- Concept: **Vector Search / Embedding Retrieval**
  - Why needed here: Literature retrieval uses embedding similarity; understanding precision/recall tradeoffs and diversity metrics explains retrieval effectiveness gaps.
  - Quick check question: If Precision@5=0.58 but UniqueDocs@5=2.3, what does this reveal about the retrieval distribution?

## Architecture Onboarding

- Component map:
  SUQL Planner (LLM) -> SQLite Database + ChromaDB Vector Store -> Answer Synthesizer (LLM) -> Web Interface (Streamlit)

- Critical path: User question -> SUQL planner prompt (schema + examples) -> parallel: (SQL execution on SQLite, vector search on ChromaDB) -> concatenate results + top-k passages -> synthesizer LLM -> cited natural language answer

- Design tradeoffs:
  - **Parallel vs iterative retrieval**: Current design executes SQL and literature search in parallel for speed; tradeoff is no cross-modal query refinement (paper proposes two-pass iterative strategy as future work)
  - **k=10 vs k=15 passages**: Higher k improves groundedness marginally (93%→97%) but increases token costs and redundancy
  - **Closed vs open model**: GPT-4/GPT-5 used for planning/synthesis; paper suggests LLaMA-2 fine-tuning as cost-reduction path but not implemented

- Failure signatures:
  - **SQL partial correctness (20%)**: Missing conditions, flipped inequalities, incorrect joins for cross-timestep comparisons
  - **Low retrieval diversity**: UniqueDocs@3 ≈ 1.8 indicates passages cluster on single papers, missing alternative mechanisms
  - **Citation interpretability gaps (3.27/5)**: Experts struggle to trace claims to specific sources/sections
  - **Recall failures at 60%**: Relevant papers not in top-k for broad/abstract queries

- First 3 experiments:
  1. **SQL error analysis**: Run planner on the 30 benchmark questions with verbose logging; categorize failures by type (missing condition, join error, filter error) to identify prompt improvement targets.
  2. **Retrieval diversity intervention**: Implement a reranking step or multi-query expansion to increase UniqueDocs@k; measure impact on groundedness and expert completeness ratings.
  3. **Iterative two-pass prototype**: Build a branch where preliminary SQL results trigger a refined literature query (e.g., unexpected peak shifts → targeted mechanism search); compare answer completeness vs parallel baseline on hard queries from benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implementing a reranking or multi-stage retrieval mechanism significantly improve retrieval recall and document diversity?
- Basis in paper: [explicit] The authors identify "Improving recall and diversity via reranking or multi-stage retrieval is therefore the most direct lever for improving scientific reliability," noting that UniqueDocs@3 is often limited to 1.8.
- Why unresolved: Current retrieval shows limited diversity (retrieving multiple passages from the same paper), which deprives the LLM of alternative mechanistic explanations and contributes to groundedness errors.
- What evidence would resolve it: Benchmark tests showing increased UniqueDocs@k and Recall@k scores without sacrificing precision, alongside improved groundedness ratings.

### Open Question 2
- Question: Does an iterative, closed-loop query strategy improve answer completeness compared to the current parallel execution model?
- Basis in paper: [explicit] The authors propose an "iterative two-pass strategy" where "preliminary SQL findings... trigger a refined literature query" to reduce missed mechanisms.
- Why unresolved: The current parallel execution cannot dynamically adjust literature retrieval based on intermediate numerical findings (e.g., unexpected spatial hotspots), potentially limiting the depth of mechanistic insight.
- What evidence would resolve it: Ablation studies comparing the parallel baseline against an iterative approach, measuring improvements in expert-rated completeness scores.

### Open Question 3
- Question: Can smaller, domain-fine-tuned models (e.g., LLaMA-2) achieve parity with GPT-4 in semantic parsing and answer synthesis for this specific domain?
- Basis in paper: [explicit] The authors suggest "smaller open models such as LLaMA-2 could be fine-tuned on domain-specific QA pairs to improve accuracy and reduce hallucinations."
- Why unresolved: The system currently relies on large proprietary models (GPT-4/5), which may be cost-prohibitive or suffer from reproducibility issues for widespread lab deployment.
- What evidence would resolve it: Performance metrics (SQL correctness, groundedness) of a fine-tuned 7B or 13B parameter model matching the GPT-4 baseline on the 30-question benchmark.

## Limitations
- Retrieval diversity is limited (UniqueDocs@3 ≈ 1.8), often clustering passages on single papers and missing alternative mechanisms
- Citation interpretability scores are moderate (3.27/5), with experts struggling to trace claims to specific sources/sections
- SQL correctness is ~80%, with failures concentrated in complex multi-join queries and derived metrics not covered in examples

## Confidence
- **High**: SQL correctness (~80%), groundedness scaling with k (93-97% at 10-15 passages), Precision@5 (0.58) and Recall@5 (0.60) for literature retrieval
- **Medium**: Expert ratings for accuracy/relevance/clarity (4.1-4.6/5) and citation interpretability (3.27/5), given small expert panel (n=5) and potential subjectivity
- **Medium-Low**: Generalization to new domains/larger datasets, as the system is validated only on LTMO Raman data and 50 PDFs without ablation on dataset size or domain shifts

## Next Checks
1. **Retrieval Diversity Intervention**: Implement a reranking step or multi-query expansion to increase UniqueDocs@k; measure impact on groundedness and expert completeness ratings. Target: UniqueDocs@3 > 2.5 without reducing Precision@5.

2. **Iterative Two-Pass Prototype**: Build a branch where preliminary SQL results trigger a refined literature query (e.g., unexpected peak shifts → targeted mechanism search); compare answer completeness vs parallel baseline on hard queries from benchmark.

3. **SQL Error Analysis**: Run planner on the 30 benchmark questions with verbose logging; categorize failures by type (missing condition, join error, filter error) to identify prompt improvement targets. Target: Reduce incorrect SQL rate from 20% to <10% via targeted prompt refinement.