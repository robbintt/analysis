---
ver: rpa2
title: 'VisualPRM: An Effective Process Reward Model for Multimodal Reasoning'
arxiv_id: '2503.10291'
source_url: https://arxiv.org/abs/2503.10291
tags:
- arxiv
- step
- correct
- reasoning
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisualPRM, an 8B parameter Process Reward
  Model (PRM) designed to enhance multimodal reasoning in Multimodal Large Language
  Models (MLLMs) through Best-of-N evaluation strategies. The model improves reasoning
  performance across three types of MLLMs and four model scales, achieving a 5.9-point
  improvement on InternVL2.5-78B across seven benchmarks.
---

# VisualPRM: An Effective Process Reward Model for Multimodal Reasoning

## Quick Facts
- **arXiv ID**: 2503.10291
- **Source URL**: https://arxiv.org/abs/2503.10291
- **Reference count**: 40
- **Primary result**: An 8B parameter Process Reward Model (PRM) improves multimodal reasoning performance by 5.9 points on InternVL2.5-78B across seven benchmarks via Best-of-N evaluation.

## Executive Summary
VisualPRM introduces a Process Reward Model (PRM) for multimodal reasoning that evaluates step-by-step correctness rather than just final outcomes. The 8B parameter model outperforms Outcome Reward Models and Self-Consistency in Best-of-N evaluation across three MLLM families and four model scales. To enable training, the authors construct VisualPRM400K, a dataset of 400K samples with Monte Carlo-generated process supervision labels, and propose VisualProcessBench, a benchmark with 2,866 samples and 26,950 human-annotated step-wise correctness labels. Experimental results show PRMs are particularly effective as N increases, highlighting their potential for test-time scaling of MLLMs.

## Method Summary
VisualPRM is trained as a multi-turn chat task where the model predicts step-wise correctness (+/−) for each reasoning step in a multimodal context. The training data (VisualPRM400K) is automatically generated using Monte Carlo sampling: for each partial solution, multiple completions are sampled and the expected accuracy is calculated to determine step labels. The model is fine-tuned from an 8B MLLM using value-based supervision with average score aggregation. At inference, VisualPRM scores N candidates generated by a policy MLLM (temperature 0.7) and selects the highest-scoring response based on averaged step scores. The approach is evaluated on seven multimodal reasoning benchmarks and a new step-wise correctness detection benchmark.

## Key Results
- VisualPRM achieves 5.9-point improvement on InternVL2.5-78B across seven benchmarks in Best-of-N evaluation
- PRMs outperform Outcome Reward Models and Self-Consistency, with performance gaps widening as N increases (2.4-4.3 points)
- VisualPRM400K dataset contains 400K samples with 2M steps, where ~10% of steps are labeled incorrect
- Existing open-source MLLMs struggle to accurately assess step-wise correctness, highlighting the need for specialized critic models

## Why This Works (Mechanism)

### Mechanism 1: Step-wise Process Supervision Enables Finer-grained Selection
- Claim: PRMs outperform ORMs because they assess reasoning quality at each step rather than only the final answer
- Mechanism: The PRM assigns a correctness probability to each step (+ or − tokens), then aggregates step scores (via averaging) into a response-level score for candidate selection
- Core assumption: Most solution errors occur in intermediate steps, so per-step scoring provides a more reliable signal than outcome-only scoring
- Evidence: PRM outperforms ORM and Self-Consistency in experiments; performance gap widens as N increases

### Mechanism 2: Monte Carlo Estimation Automates Process Labeling
- Claim: Automatic data pipeline can generate step-level correctness labels at scale without human annotation
- Mechanism: For each partial solution prefix, sample multiple completions and define expected accuracy mc_i = (# correct completions)/(# sampled completions)
- Core assumption: The expected accuracy of completions from a prefix correlates well with the true correctness of the step leading to that prefix
- Evidence: VisualPRM400K comprises ~400K samples with 2M steps; ~10% labeled incorrect

### Mechanism 3: Multi-turn Formulation Leverages MLLM Generative Capabilities
- Claim: Formulating PRM training as a multi-turn chat task enables effective use of MLLM generation abilities for step-wise prediction
- Mechanism: Each step is presented in a separate turn; the model predicts correctness (+/−) conditioned on image, question, and prior steps
- Core assumption: The MLLM can learn to discriminate step quality via this turn-by-turn supervision without explicit dense reward heads
- Evidence: Supervising all steps (not just up to first error) performs slightly better than early stopping

## Foundational Learning

- **Process Reward Models (PRMs) vs Outcome Reward Models (ORMs)**
  - Why needed: VisualPRM's core contribution is demonstrating PRM superiority over ORM for multimodal test-time scaling
  - Quick check: Given a solution with a correct final answer but a flawed intermediate step, would an ORM or PRM be more likely to downweight it?

- **Best-of-N (BoN) Evaluation and Test-Time Scaling**
  - Why needed: BoN is the deployment setting for VisualPRM; the paper quantifies gains via BoN across scales and families
  - Quick check: If N=8 candidates are generated with temperature 0.7, how does increasing N to 64 affect the theoretical upper bound of BoN performance?

- **Monte Carlo Estimation for Process Labels**
  - Why needed: The VisualPRM400K dataset uses automated Monte Carlo labeling; understanding its assumptions is critical for assessing data quality
  - Quick check: If a step has mc_i = 0.5 after 16 sampled completions, what does that imply about the policy model's ability to recover from that step?

## Architecture Onboarding

- **Component map**: Image-Questions (MMPR v1.1) -> Policy Model (MLLM) -> N Solutions -> Critic Model (VisualPRM-8B) -> Step Scores -> Best Response
- **Critical path**:
  1. Prepare image-question pairs (MMPR v1.1)
  2. Sample solutions and split into ≤12 steps
  3. Run Monte Carlo labeling (16 completions/step) to generate VisualPRM400K
  4. Fine-tune an 8B MLLM on multi-turn PRM task
  5. Deploy PRM in BoN: generate N responses, score with PRM, select highest
- **Design tradeoffs**:
  - Value-based PRM outperforms advantage-based due to label noise
  - Averaging outperforms max/min pooling for score aggregation
  - Supervising all steps yields slightly better results than early stopping
- **Failure signatures**:
  - Weak policy model: Base model generates mostly incorrect responses
  - Temperature issues: Too low limits diversity; too high reduces individual quality
  - PRM overfitting: May misjudge out-of-distribution domains
  - Label noise: Incorrect labels can teach PRM to reward bad steps
- **First 3 experiments**:
  1. Run BoN-8 evaluation comparing VisualPRM vs ORM vs Self-Consistency on 2-3 benchmarks
  2. Measure BoN performance as N increases (8, 16, 32, 64, 128) to verify scaling trends
  3. Train two small PRM variants (value-based vs advantage-based) and compare on VisualProcessBench F1

## Open Questions the Paper Calls Out

- **Can PRM be used for RL?** Can the process supervision provided by VisualPRM be effectively utilized for reinforcement learning to update the weights of the policy model, rather than solely for test-time Best-of-N selection?

- **Can Advantage-based PRMs work?** Can the inherent noise in automatic Monte Carlo data pipelines be reduced sufficiently to make Advantage-based PRMs superior to Value-based PRMs for multimodal reasoning?

- **What causes critic failures?** Does the failure of existing MLLMs to act as critics stem primarily from an inability to detect visual hallucinations versus logical fallacies in the reasoning steps?

- **Compute-optimal scaling?** What are the compute-optimal trade-offs when scaling the VisualPRM Best-of-N strategy compared to simply scaling up the policy model parameters?

## Limitations

- Monte Carlo labeling relies on assumptions about policy model error recovery that may not hold across all reasoning domains
- PRM performance depends on having sufficient diversity among N candidates, which may not scale well with extremely large N
- The 90% correct step distribution in training data may bias the PRM toward positive predictions
- Human evaluation is limited, relying primarily on automated metrics for performance assessment

## Confidence

- **High confidence**: PRM outperforms ORM in BoN evaluation (consistent results across 7 benchmarks)
- **Medium confidence**: Monte Carlo labeling produces sufficiently accurate step labels (lacks ablation studies on sampling budgets)
- **Medium confidence**: Multi-turn chat formulation is optimal (limited comparative evidence against alternatives)
- **Low confidence**: VisualPRM generalizes to all multimodal reasoning domains (only tested on mathematical/visual reasoning benchmarks)

## Next Checks

1. **Label noise analysis**: Run ablation studies varying Monte Carlo sampling budgets (4, 8, 16, 32 completions) and measure impact on PRM BoN performance to quantify sensitivity to label quality.

2. **Cross-domain transfer**: Test VisualPRM on non-mathematical multimodal reasoning tasks (e.g., visual question answering, image captioning) to validate domain generalization beyond MathVista/MMMU.

3. **Human preference study**: Conduct pairwise human evaluation comparing PRM-selected vs ORM-selected responses across 100+ samples to verify that automatic metrics align with human judgment of reasoning quality.