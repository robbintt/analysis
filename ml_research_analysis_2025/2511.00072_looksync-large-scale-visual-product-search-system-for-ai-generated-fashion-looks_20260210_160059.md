---
ver: rpa2
title: 'LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion
  Looks'
arxiv_id: '2511.00072'
source_url: https://arxiv.org/abs/2511.00072
tags:
- search
- product
- products
- system
- looks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LookSync, a large-scale visual product search
  system designed to match AI-generated fashion looks with real-world products from
  e-commerce catalogs. The system processes over 350,000 AI-generated looks daily
  across 12 million products globally.
---

# LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks

## Quick Facts
- arXiv ID: 2511.00072
- Source URL: https://arxiv.org/abs/2511.00072
- Authors: Pradeep M; Ritesh Pallod; Satyen Abrol; Muthu Raman; Ian Anderson
- Reference count: 7
- Primary result: LookSync processes 350,000 AI-generated looks daily across 12M products with CLIP achieving 3-7% higher MOS scores than alternatives

## Executive Summary
LookSync is a production system that bridges AI-generated fashion looks with real-world e-commerce products. The system handles over 350,000 queries daily across a 12 million product catalog, maintaining sub-second latency while achieving human-evaluated relevance scores (MOS) that outperform alternative models by 3-7%. The architecture uses a four-stage pipeline: LLM-based query generation, CLIP embedding, vector database retrieval, and LLM reranking with segmentation fallback. Human evaluation demonstrates CLIP's superiority for this task, though the paper notes domain-specific models excel at fine-grained attributes like color and pattern.

## Method Summary
The system ingests product catalog images by embedding them with CLIP-ViT-H/14-laion2B-s32B-b79K and storing in a vector database. For each AI-generated look, an LLM extracts structured product descriptions by layer (outerwear, bottomwear, accessories). These text descriptions are embedded using the same CLIP model and used for vector retrieval. The top-k candidates undergo LLM reranking based on visual similarity, with a fallback to SAM v2 + Florence segmentation if reranking fails. The pipeline maintains <1 second end-to-end latency while achieving human-evaluated MOS scores of 3.0-4.5.

## Key Results
- CLIP-ViT-H/14-laion2B-s32B-b79K outperforms Fashion SigLIP and DINOv2 by 3-7% in human-judged MOS scores
- System processes 350,000 AI-generated looks daily across 12 million products
- Maintains sub-second (<1s) end-to-end latency at production scale
- Human annotators rate retrieval quality on 1-5 scale with scores ranging 3.0-4.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP embeddings enable cross-modal matching between text descriptions and product images in a unified vector space.
- Mechanism: The system uses CLIP (specifically ViT-H/14 trained on LAION-2B) to encode both product catalog images during ingestion and LLM-generated text queries during search. Since CLIP is trained on image-text pairs, these embeddings occupy the same high-dimensional space, allowing cosine similarity comparison between text queries and product images.
- Core assumption: CLIP's pre-training on general image-text data transfers sufficiently to fashion-specific visual and semantic features without domain-specific fine-tuning.
- Evidence anchors:
  - [abstract] "CLIP outperformed alternative models by a small relative margin of 3–7% in mean opinion scores"
  - [section 4.2.2] "we employ the ViT-H/14 variant trained on the LAION-2B dataset... this allows us to directly compare the query embeddings to the product embeddings for similarity"
  - [corpus] Weak direct corpus support for CLIP specifically; related papers focus on LLM-based product attribution and retrieval, not CLIP comparisons.

### Mechanism 2
- Claim: LLM-based query generation extracts structured product attributes from AI-generated images, improving retrieval precision over direct image-to-image matching.
- Mechanism: An LLM receives the AI-generated look image and outputs a structured dictionary mapping product layers (outermost_topwear, bottomwear, accessories) to detailed text descriptions. These descriptions capture attributes like color, pattern, sleeve type, and style. The structured output enables layer-specific searches rather than treating the entire outfit as one query.
- Core assumption: The LLM can accurately identify and describe fashion attributes from AI-generated images, and text-based descriptions capture the visual features users care about.
- Evidence anchors:
  - [abstract] "query generation via LLM" is listed as the first pipeline stage
  - [section 4.2.1] "The output of this stage is a structured dictionary mapping each detected product layer to its corresponding descriptive query"
  - [corpus] "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes?" explores similar LLM-based attribute extraction, supporting feasibility.

### Mechanism 3
- Claim: Two-stage reranking (LLM primary, segmentation fallback) improves final relevance by applying fine-grained similarity judgment beyond embedding similarity.
- Mechanism: After candidate retrieval, the top-k products are reranked by an LLM that compares the AI-generated look to candidate products. If this fails, the system falls back to SAM v2 + Florence for product segmentation from the AI image, then embeds segments with CLIP for cosine similarity reranking. This provides robustness against single-point-of-failure in the LLM reranker.
- Core assumption: LLMs can perform visual comparison tasks, and segmentation-based fallback adequately approximates product boundaries in AI-generated images.
- Evidence anchors:
  - [section 4.2.4-4.2.5] "We then pass the top-k candidates for each product group to an LLM... If the LLM-based reranker fails, we fall back to a segmentation-based approach"
  - [abstract] "reranking for relevance" listed as the final pipeline stage
  - [corpus] No direct corpus evidence for this specific reranking architecture; related papers don't address LLM-based visual reranking.

## Foundational Learning

- Concept: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: The entire retrieval system depends on understanding how CLIP creates a shared embedding space for images and text. Without this, you can't debug similarity scores or understand why certain products match.
  - Quick check question: Given a text query "red silk evening gown" and three product images (red cotton dress, blue silk gown, red silk blouse), which would CLIP embeddings rank highest, and why might this differ from pixel-level image similarity?

- Concept: Vector databases and approximate nearest neighbor (ANN) search
  - Why needed here: The system indexes 12 million products and must retrieve results in under 1 second. Understanding ANN algorithms (HNSW, IVF) is essential for tuning the precision-latency trade-off.
  - Quick check question: If your vector DB recall drops from 95% to 80% after increasing the query speed requirement, which pipeline component would you adjust first, and what metric would you monitor?

- Concept: Mean Opinion Score (MOS) evaluation methodology
  - Why needed here: The paper's primary success metric is human-judged MOS. You need to understand how subjective evaluation works to interpret the 3-7% improvement claim and design your own evaluation pipelines.
  - Quick check question: If Annotator 1 gives consistently lower scores than Annotator 5 (see Table 1), what normalization or aggregation approach would you use to fairly compare models across all annotators?

## Architecture Onboarding

- Component map:
[Vendor Catalogs] → Message Queue → [Ingestion Pipeline] → CLIP Embedding + Metadata Enrichment → [Vector Database] + Redis Cache
[AI-Generated Look] → [LLM Query Generator] → [CLIP Text Encoder] → [Vector DB Retrieval] → [Hard Filters + Dedup] → [LLM Reranker] --(on failure)--> [SAM v2 + Florence Segmentation → CLIP → Cosine Sim] → [Ranked Products]

- Critical path: Query generation → CLIP text embedding → Vector DB retrieval → LLM reranking. Any latency spike in this path directly impacts the <1 second end-to-end requirement.

- Design tradeoffs:
  1. **CLIP vs. domain-specific models**: CLIP provides 3-7% MOS improvement over Fashion SigLIP and DINOv2, but the paper notes these alternatives excel at fine-grained features (color, pattern). If your use case prioritizes exact color matching over overall style, this trade-off may differ.
  2. **LLM reranking latency vs. quality**: LLM reranking improves relevance but adds latency. The fallback mechanism adds complexity but ensures robustness.
  3. **Redis cache vs. freshness**: Caching standardized metadata reduces vector DB load but may serve stale data if not invalidated properly during catalog updates.

- Failure signatures:
  1. **Low MOS scores (<3.0)**: Indicates embedding or query generation failure—check if LLM is producing meaningful descriptions.
  2. **High latency (>1s)**: Likely bottleneck in LLM reranking or vector DB query—check top-k size and network latency.
  3. **Fallback triggered frequently**: LLM reranker failing consistently—check prompt design or API availability.
  4. **No products returned**: Hard filters (brand, size, price) too restrictive for the retrieved candidates.

- First 3 experiments:
  1. **Embedding model ablation**: Deploy FashionCLIP and DINOv2 alongside CLIP on a 10% traffic split. Measure MOS per category (topwear, bottomwear, accessories) to validate or refute the paper's 3-7% claim in your catalog.
  2. **Top-k sensitivity analysis**: Vary the number of candidates passed to the LLM reranker (k=10, 20, 50). Plot MOS vs. latency to find the optimal operating point for your latency budget.
  3. **Query generation prompt iteration**: Test 3 prompt variants for the LLM query generator (minimal attributes, detailed attributes, category-focused). Measure impact on retrieval recall@10 and final MOS to determine if attribute verbosity helps or hurts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid ensemble architecture combining CLIP with domain-specific models (e.g., DINOv2) outperform the current single-model baseline for fine-grained attribute retrieval?
- Basis in paper: [explicit] Section 5.2 notes that while CLIP was the "most consistent performer," alternative models like DINOv2 and Fashion SigLIP "showed great results in fine-grained aspects such as color and pattern detection."
- Why unresolved: The paper selected CLIP as the sole backbone for production to balance general performance, leaving the potential synergy of combining these specialized models unexplored.
- What evidence would resolve it: Ablation studies showing MOS scores for a pipeline that utilizes CLIP for semantic retrieval and DINOv2/SigLIP for re-ranking or filtering based on specific attributes like pattern and color.

### Open Question 2
- Question: What is the marginal performance gain of the LLM-based reranking stage compared to the initial vector retrieval, and is it justified by the latency cost?
- Basis in paper: [inferred] The system prioritizes maintaining an average end-to-end latency of under 1 second (Introduction) while employing a multi-stage pipeline including a complex LLM reranker (Section 4.2.4).
- Why unresolved: The paper evaluates the system holistically via Mean Opinion Scores (MOS) but does not isolate the contribution of the LLM reranker, making it unclear if this computationally expensive step is strictly necessary for all queries.
- What evidence would resolve it: A comparative evaluation of MOS scores and latency between the full pipeline and a truncated pipeline that relies solely on the vector database candidate retrieval.

### Open Question 3
- Question: Does the intermediate step of converting AI-generated images into textual descriptions (Query Generation) result in a loss of visual fidelity compared to direct image-to-image retrieval?
- Basis in paper: [inferred] The method relies on an LLM to generate text queries from images (Section 4.2.1), which are then vectorized, rather than embedding the image pixels directly.
- Why unresolved: While the text-based approach handles non-existent products well, the paper does not compare this "image-to-text-to-vector" approach against a direct "image-to-image" vector search, leaving the information loss from the text conversion unquantified.
- What evidence would resolve it: An experiment comparing the retrieval accuracy of the current text-intermediate pipeline against a baseline using direct visual embeddings of the AI-generated look.

## Limitations

- **Model Architecture Gaps**: The paper doesn't specify which LLM is used for query generation and reranking, only that it's an "LLM." This creates uncertainty about whether the system would perform similarly with different models (GPT-4o, Claude, or proprietary alternatives).
- **Evaluation Scope**: MOS evaluation was conducted on a subset of looks (10% of daily traffic) using 5 annotators. The 3-7% improvement margin, while statistically meaningful, represents a relatively small absolute gain that may not generalize across different fashion categories or cultural contexts.
- **Fallbacks and Edge Cases**: The paper reports fallback to segmentation-based reranking when LLM reranking fails, but provides no metrics on fallback frequency or quality degradation. This mechanism's effectiveness in production remains uncertain.

## Confidence

**High Confidence**: The core claim that CLIP embeddings enable effective cross-modal search between AI-generated looks and product images is well-supported by the 3-7% MOS improvement over alternatives and the established success of CLIP in similar applications.

**Medium Confidence**: The two-stage reranking architecture (LLM primary + segmentation fallback) appears sound, but the paper lacks detailed performance metrics for the fallback mechanism, making it difficult to assess its real-world reliability.

**Low Confidence**: The specific choice of LLM for query generation and reranking significantly impacts system performance, but this component is underspecified in the paper. Different LLMs could yield substantially different results.

## Next Checks

1. **Ablation Study on Reranking Components**: Deploy a production A/B test that removes the LLM reranking stage to quantify its actual contribution to MOS and latency. Measure fallback frequency and MOS degradation when reranking fails.

2. **Category-Specific Performance Analysis**: Segment MOS evaluation by product category (topwear, bottomwear, accessories) to identify which categories benefit most from CLIP versus where alternative models like FashionCLIP might be superior.

3. **Catalog Scale Stress Test**: Validate the <1 second latency requirement at full 12M product scale by testing with progressively larger catalog sizes (100K → 1M → 12M). Monitor recall@10 degradation and identify the point where latency exceeds requirements.