---
ver: rpa2
title: Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index
arxiv_id: '2508.11959'
source_url: https://arxiv.org/abs/2508.11959
tags:
- feature
- scores
- shap
- importance
- axfi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AxFi, a new feature importance score based
  on game theory, specifically leveraging Shapley value and Banzhaf index to quantify
  how effectively each feature excludes adversarial examples (AExs). Unlike existing
  rigorous feature attribution methods, AxFi considers non-weak abductive explanation
  (WAXp) sets, providing a finer-grained quantification of feature importance.
---

# Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index

## Quick Facts
- arXiv ID: 2508.11959
- Source URL: https://arxiv.org/abs/2508.11959
- Reference count: 40
- Primary result: Introduces AxFi, a rigorous feature importance score based on Shapley value and Banzhaf index that quantifies how effectively each feature excludes adversarial examples

## Executive Summary
This paper introduces AxFi, a new feature importance score based on game theory that quantifies how effectively each feature excludes adversarial examples. Unlike existing rigorous feature attribution methods, AxFi considers non-weak abductive explanation (WAXp) sets, providing a finer-grained quantification of feature importance. The scores are computed using a novel characteristic function called contrastive explanation forests (CXp-Forest), which efficiently represents the relationship between features and adversarial examples.

The method leverages Shapley value and Banzhaf index to assign feature importance scores that satisfy desirable properties such as Efficiency, Symmetry, and CXp-minimal monotonicity. Experimental results show that AxFi scores are more efficient to compute than existing rigorous feature importance measures for decision trees, and provide a different and informative perspective on feature importance compared to SHAP scores.

## Method Summary
The method computes feature importance scores by first extracting Contrastive Explanations (CXps) from the model, then constructing a CXp-Forest where each tree represents a CXp. For each CXp, the method counts the number of Adversarial Examples (AExs) it covers, using exact counting for decision trees and sampling-based estimation for other model types. The characteristic function is defined as the weighted ratio of CXps intersected by feature sets, and Shapley or Banzhaf indices are computed over this forest structure. The resulting scores quantify how effectively each feature excludes adversarial examples while maintaining formal guarantees about logical relevancy.

## Key Results
- AxFi scores satisfy Efficiency, Symmetry, and CXp-minimal monotonicity properties
- For decision trees, AxFi scores can be computed in polynomial time relative to the number of tree paths
- Experimental results on multiple datasets show AxFi provides a different and informative perspective compared to SHAP scores
- Runtime comparison demonstrates AxFi is more efficient than existing rigorous feature importance measures for decision trees

## Why This Works (Mechanism)

### Mechanism 1: Quantification of Adversarial Exclusion
The method replaces binary characteristic functions with a continuous function that measures the weighted ratio of CXps intersected by feature sets. This allows fine-grained differentiation between non-WAXp sets by quantifying how many adversarial examples each feature set excludes.

### Mechanism 2: Efficient Computation via CXp-Forests
For Decision Trees, the method constructs a CXp-Forest where each tree represents a CXp. Because the number of CXps is polynomial in the number of paths for Decision Trees, the entire score calculation remains tractable.

### Mechanism 3: Consistency with Formal Relevancy
The characteristic function is strictly defined by the intersection of feature sets with CXps. If a feature appears in no CXps, its marginal contribution to excluding adversarial examples is zero, ensuring scores are consistent with logical relevancy.

## Foundational Learning

- **Abductive (AXp) vs. Contrastive (CXp) Explanations**: AXps are minimal sets sufficient to predict a class, while CXps are minimal sets necessary to change the prediction. Needed to understand the basis of the characteristic function. Quick check: If I fix the features in a CXp, do I guarantee the prediction stays the same, or do I identify a way to change it?

- **Characteristic Function ($\upsilon$) in Cooperative Game Theory**: Maps a coalition of features to a real number representing their worth. In AxFi, $\upsilon$ is based on logical exclusion of adversarial examples rather than prediction probability. Quick check: In standard SHAP, $\upsilon$ is often the expected value. In AxFi, is $\upsilon$ based on prediction probability or logical exclusion?

- **$l_0$ Norm and Adversarial Examples (AExs)**: AExs are defined using the $l_0$ norm (count of changed features). The weights in AxFi scores correspond to the count of these minimal AExs. Quick check: Does an $l_0$-minimal AEx imply a small Euclidean distance or a small number of flipped feature values?

## Architecture Onboarding

- **Component map**: Input (Trained Model + Target Instance) -> Explanation Extractor (CXps) -> CXp-Forest Builder -> Weight Calculator (AEx counts) -> Power Index Engine (Shapley/Banzhaf)

- **Critical path**: The Explanation Extractor and Weight Calculator. If these are slow or intractable for your specific model class, the entire pipeline stalls. The paper proves this is polynomial for Decision Trees but may be slower for Boosted Trees.

- **Design tradeoffs**: Rigor vs. Speed (AxFi is slower than standard SHAP but offers logical guarantees); Exactness vs. Sampling (sampling introduces variance for non-tree models)

- **Failure signatures**: Runtime Explosion if the model has an exponential number of CXps; Zero-Score Paradox if the definition of "relevancy" is too strict for the data

- **First 3 experiments**:
  1. Validation on Decision Trees: Measure AxFi computation time vs. number of tree paths on Adult or COMPAS datasets
  2. Relevancy Check: Compare AxFi vs. SHAP on synthetic datasets with known relevant features
  3. Ranking Similarity (RBO): Compute Rank-Biased Overlap between AxFi and WFFA/SHAP to confirm different perspective

## Open Questions the Paper Calls Out

### Open Question 1
How can feature dependencies be formally integrated into the CXp-Forest characteristic function without compromising the rigorous guarantees of AxFi scores? The paper identifies this limitation and suggests applying constraint outputs as penalties but provides no implementation or testing.

### Open Question 2
Can computing formal explanations relative to a subset of the feature space effectively approximate AxFi scores when the number of CXps is exponential? The paper suggests this as a solution but provides no algorithm or experimental validation.

### Open Question 3
Are there model families other than decision trees for which the exact computation of AxFi weights is tractable? The current results imply intractability for general cases, leaving the existence of other tractable model subclasses an open area.

## Limitations
- Computational scalability for non-tree models relies on sampling-based AEx estimation, which may introduce significant variance
- The assumption of CXp-AEx disjointness may not hold in high-dimensional continuous feature spaces
- The relationship between formal logical relevancy and domain-specific importance is not empirically validated

## Confidence
- High confidence: Polynomial-time computation for decision trees (proven in Section 4)
- Medium confidence: Runtime comparisons with WFFA and SHAP (empirical results may depend on implementation details)
- Medium confidence: Properties (Efficiency, Symmetry, CXp-minimal monotonicity) for decision trees (theoretical proofs provided)

## Next Checks
1. Replicate polynomial runtime scaling by measuring AxFi computation time on decision trees with varying numbers of paths (10, 50, 100, 200 paths) on Adult dataset
2. Test Consistency with relevancy property by constructing synthetic datasets where ground-truth relevant features are known and comparing AxFi vs SHAP score distributions
3. Evaluate variance in AxFi scores for boosted trees by running the sampling procedure with different random seeds and measuring coefficient of variation across 10 runs