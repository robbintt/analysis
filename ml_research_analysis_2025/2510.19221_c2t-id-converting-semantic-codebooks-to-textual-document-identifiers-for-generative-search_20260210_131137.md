---
ver: rpa2
title: 'C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for
  Generative Search'
arxiv_id: '2510.19221'
source_url: https://arxiv.org/abs/2510.19221
tags:
- docid
- retrieval
- semantic
- c2t-id
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: C2T-ID addresses the trade-off between semantic expressiveness
  and constrained search space in generative retrieval by converting numeric codebook
  identifiers into structured textual document identifiers. The method constructs
  a hierarchical clustering tree over the corpus, then replaces each numeric label
  with high-frequency keywords extracted from document metadata, joined by hyphens
  to preserve hierarchical structure.
---

# C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search

## Quick Facts
- arXiv ID: 2510.19221
- Source URL: https://arxiv.org/abs/2510.19221
- Reference count: 39
- C2T-ID significantly outperforms atomic numeric identifiers, semantic codebook identifiers, and pure textual identifiers using titles on both Natural Questions and Taobao datasets.

## Executive Summary
C2T-ID addresses the trade-off between semantic expressiveness and constrained search space in generative retrieval by converting numeric codebook identifiers into structured textual document identifiers. The method constructs a hierarchical clustering tree over the corpus, then replaces each numeric label with high-frequency keywords extracted from document metadata, joined by hyphens to preserve hierarchical structure. An optional two-level semantic smoothing further enhances fluency. Experiments on Natural Questions and Taobao's product search show substantial improvements over baselines, demonstrating that injecting rich textual priors while maintaining search space constraints can significantly improve generative retrieval performance.

## Method Summary
C2T-ID constructs a hierarchical clustering tree using k-means, where each document receives a unique routing path through the tree. For each cluster node, high-frequency metadata keywords are extracted and used to replace numeric labels. The resulting textual identifiers preserve the hierarchical structure through hyphen-joined keywords. During inference, a trie-constrained decoder ensures generation follows valid paths through the clustering tree. An optional semantic smoothing step uses LLMs to enhance fluency. The method is evaluated on Natural Questions (Wikipedia) and Taobao product search, using BART-large and LLAMA-3.1-8B models respectively.

## Key Results
- On Natural Questions, C2T-ID achieves Hits@5 of 34.3 and MRR@20 of 31.2, compared to 28.3 Hits@5 and 26.3 MRR@20 for semantic codebook identifiers
- On Taobao, C2T-ID reaches Hits@5 of 64.0 and Hits@20 of 77.5, substantially exceeding all baselines
- Zero-shot C2T-ID achieves 33.5 Hits@5 vs 22.0 for numeric codebooks, demonstrating semantic priors transfer without query-level training

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic Codebook Construction
Hierarchical clustering creates a semantically organized document space that enables structured generation with bounded search complexity. Recursive k-means partitioning with parameters k=30 and c=30 builds a tree where documents receive unique routing paths. This structured search space prevents early-step error propagation while maintaining semantic coherence. The approach assumes documents can be meaningfully grouped into semantic clusters.

### Mechanism 2: Textual Label Replacement via Metadata Priors
Replacing numeric cluster labels with high-frequency keywords from document metadata injects semantic priors that LLMs can leverage during generation. For each tree node, keywords are aggregated from member documents, counted for frequency, and top-K (typically 3) are selected and hyphen-joined. This assumes metadata contains high-quality semantic signals predictive of query-document relevance.

### Mechanism 3: Trie-Constrained Decoding for Search Space Control
Constraining generation to follow the clustering tree structure limits vocabulary expansion at each step, reducing early-step error propagation while preserving semantic navigation. At each tree level, the model predicts from at most k candidate nodes rather than the full vocabulary. This assumes the hierarchical structure provides meaningful constraints that guide generation toward relevant documents.

## Foundational Learning

- **Autoregressive Generative Retrieval**: GR treats retrieval as seq2seq generation of docids. Early mistakes cascade, motivating constrained decoding approaches.
  - Quick check: Given query "What is the capital of France?", if a GR model incorrectly generates the first token of a docid, can it still recover the correct document? Why or why not?

- **Hierarchical Clustering for Document Organization**: C2T-ID builds semantic trees via recursive k-means. Understanding cluster granularity trade-offs (k and c parameters) is essential for tuning.
  - Quick check: If you increase c (max documents per leaf) from 30 to 100, what happens to tree depth, and how might this affect retrieval latency vs. accuracy?

- **Trie-Constrained Decoding**: The core innovation uses the clustering tree as a decoding constraint. This differs from both unconstrained text generation and pure numeric codebook decoding.
  - Quick check: At a tree node with 3 children labeled "phone-accessories", "phone-cases", and "phone-repair", what tokens can the model generate next if the current prefix is "phone-"?

## Architecture Onboarding

- **Component map**: Document Encoder -> Hierarchical Clustering Module -> Metadata Extraction Pipeline -> Label Replacement Engine -> GR Model -> Constrained Decoder
- **Critical path**: Encode all documents → Build clustering tree → Extract metadata keywords per document → Aggregate keywords per tree node → Replace numeric labels with top-K keyword strings → Train GR model on (document, C2T-ID) and (query, C2T-ID) pairs → Inference with trie-constrained decoding
- **Design tradeoffs**: 
  - K (keywords per level): Higher K = more semantic signal but longer docids and higher inference cost. Paper uses K=3.
  - k and c (clustering parameters): Larger k = broader tree, shorter paths; smaller c = deeper tree, more specific leaves. Paper uses k=30, c=30.
  - Semantic smoothing: Improves fluency modestly (+1-2 points on NQ) but requires additional LLM calls during docid construction.
  - Model scale: Taobao experiments use LLAMA-3.1-8B vs BART-large for NQ. Larger models show stronger gains with C2T-ID.
- **Failure signatures**:
  1. Low Hits@5 but reasonable Hits@20: Likely over-specific clustering or weak keyword quality causing early-step errors
  2. Zero-shot >> supervised performance: Indicates overfitting to query-docid pairs; semantic priors not effectively integrated
  3. High variance across queries: Metadata keywords may be domain-appropriate for some query types but not others
- **First 3 experiments**:
  1. Baseline reproduction: Implement semantic codebook docid (numeric only) on NQ with k=30, c=30. Verify ~28.3 Hits@5.
  2. Ablation on K: Test K∈{1,2,3,5,10} with fixed clustering. Plot Hits@5 vs. docid length to find efficiency frontier.
  3. Zero-shot validation: Train only on document-docid pairs. Compare C2T-ID vs. semantic codebook vs. title-based docid. Expect C2T-ID to show smallest degradation from supervised baseline.

## Open Questions the Paper Calls Out

### Open Question 1
How can the inference latency introduced by longer C2T-ID token sequences be effectively mitigated for real-time applications?
- Basis: [explicit] Section 5 states C2T-ID "typically uses more tokens... which substantially increases inference overhead and may be unsuitable for latency-sensitive IR systems."
- Why unresolved: The paper identifies increased length as a necessary trade-off but does not propose or test compression techniques or inference optimizations.
- What evidence would resolve it: A study measuring latency vs. accuracy when applying methods like token pruning, quantization, or speculative decoding to the C2T-ID decoding process.

### Open Question 2
Can C2T-ID be adapted to general domains where high-quality, structured metadata is unavailable?
- Basis: [explicit] Section 5 notes the method "relies on dataset-specific metadata sources, requiring customized prior designs for different domains."
- Why unresolved: Experiments rely on Wikipedia categories and e-commerce attributes; it's unclear if the method works when such explicit priors are missing.
- What evidence would resolve it: Experiments on datasets lacking explicit metadata where keywords are extracted solely via unsupervised methods or LLM-generation.

### Open Question 3
How does C2T-ID perform when integrated with state-of-the-art generative retrieval training objectives?
- Basis: [inferred] Section 4.2 mentions "basic GR implementation" used, suggesting potential relies on scaling to "advanced GR architectures" not tested here.
- Why unresolved: The paper uses standard DSI-style training; interaction between structured textual docids and advanced techniques remains unexplored.
- What evidence would resolve it: Benchmark results combining C2T-ID with advanced training pipelines to see if gains are additive.

## Limitations
- The method relies heavily on the availability and quality of structured metadata, which may not be available in all domains.
- Increased inference latency due to longer textual docids compared to numeric identifiers, potentially unsuitable for real-time applications.
- The paper uses a basic GR implementation without exploring integration with advanced training objectives or architectures.

## Confidence

**High confidence**: The empirical results showing C2T-ID outperforming semantic codebook and textual docid baselines on both Natural Questions and Taobao datasets. The controlled experiments with consistent metrics across datasets provide strong evidence for the core claim.

**Medium confidence**: The mechanism explanations for why hierarchical clustering and keyword replacement improve retrieval. While logically coherent, these mechanisms rely on assumptions about semantic coherence in clustering and metadata quality that aren't fully validated across diverse domains.

**Low confidence**: The claim that C2T-ID provides optimal balance between semantic expressiveness and search space constraints. This requires comparison against alternative approaches for constructing textual identifiers that aren't explored in depth.

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary k (branching factor) and c (leaf capacity) across a range of values on Natural Questions to identify optimal configurations and understand how corpus characteristics affect parameter selection.

2. **Metadata quality ablation**: Test C2T-ID performance when using progressively noisier metadata sources: (a) original categories/attributes, (b) randomly sampled keywords from documents, (c) synthetic metadata with controlled noise levels. This would quantify the method's robustness to metadata quality.

3. **Cross-domain transferability**: Apply C2T-ID to a third dataset with fundamentally different metadata characteristics (e.g., news articles with minimal categorization, scientific papers with inconsistent keyword extraction). Compare performance degradation against the Wikipedia and Taobao results to assess generalizability.