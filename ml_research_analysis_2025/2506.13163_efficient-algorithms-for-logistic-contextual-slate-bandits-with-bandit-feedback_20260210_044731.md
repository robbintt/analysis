---
ver: rpa2
title: Efficient Algorithms for Logistic Contextual Slate Bandits with Bandit Feedback
arxiv_id: '2506.13163'
source_url: https://arxiv.org/abs/2506.13163
tags:
- slate
- algorithms
- follows
- lemma
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the logistic contextual slate bandit problem
  with bandit feedback, where an agent selects slates of N items from exponentially
  large candidate sets to maximize cumulative reward over T rounds. The authors propose
  two algorithms, Slate-GLM-OFU and Slate-GLM-TS, that achieve N^O(1) per-round time
  complexity through local planning (independent slot selections) while maintaining
  low regret through global learning (joint parameter estimation).
---

# Efficient Algorithms for Logistic Contextual Slate Bandits with Bandit Feedback

## Quick Facts
- arXiv ID: 2506.13163
- Source URL: https://arxiv.org/abs/2506.13163
- Reference count: 40
- Authors: Tanmay Goyal; Gaurav Sinha
- One-line primary result: Achieves N^O(1) per-round time complexity through slot-level decomposition while maintaining Õ(√T) regret via joint parameter estimation

## Executive Summary
This paper addresses the logistic contextual slate bandit problem with bandit feedback, where an agent selects slates of N items from exponentially large candidate sets to maximize cumulative reward over T rounds. The authors propose two algorithms, Slate-GLM-OFU and Slate-GLM-TS, that achieve N^O(1) per-round time complexity through local planning (independent slot selections) while maintaining low regret through global learning (joint parameter estimation). Under a diversity assumption, Slate-GLM-OFU achieves Õ(dN√T) regret. Extensive experiments across synthetic and real-world settings demonstrate that these algorithms consistently outperform state-of-the-art baselines, achieving both the lowest regret and fastest runtime.

## Method Summary
The paper proposes two algorithms for logistic contextual slate bandits: Slate-GLM-OFU (Optimism in the Face of Uncertainty) and Slate-GLM-TS (Thompson Sampling). Both algorithms decompose slate selection into N independent slot-level optimizations, achieving polynomial per-round complexity. Slate-GLM-OFU maintains a single global parameter vector θ across all slots and uses adaptive two-path updates (fast gradient updates or slow confidence set refinements) based on an inequality criterion. The algorithms exploit a multiplicative equivalence between slot-level and slate-level design matrices under a diversity assumption, allowing efficient exploration bonuses. Slate-GLM-TS-Fixed (non-contextual variant) has proven Õ(√T) regret, while Slate-GLM-OFU has proven Õ(dN√T) regret. The methods are validated on synthetic data and applied to in-context example selection for Language Model prompts, achieving ~80% test accuracy on sentiment analysis tasks.

## Key Results
- Slate-GLM-OFU achieves Õ(dN√T) regret under diversity assumption
- Algorithms run in N^O(1) per-round time complexity, exponentially faster than baselines
- Slate-GLM-OFU achieves competitive ~80% test accuracy for in-context example selection
- Both algorithms consistently outperform state-of-the-art baselines in regret and runtime across synthetic and real-world experiments

## Why This Works (Mechanism)

### Mechanism 1: Slot-Level Decomposition with Multiplicative Equivalence
The algorithm decomposes slate selection into N independent slot-level optimizations. Under the diversity assumption, the block-diagonal slot-level design matrix U_t = diag(W¹_t, ..., W^N_t) is multiplicatively equivalent to the full slate-level design matrix W_t (¾U_t ≼ W_t ≼ 5⁄4U_t). This allows using slot-level exploration bonuses ||x||_{W^i_t^{-1}} instead of slate-level ||x||_{W_t^{-1}}, enabling polynomial-time selection.

### Mechanism 2: Adaptive Two-Path Parameter Updates
The update subroutine evaluates an inequality criterion based on local logistic curvature. When satisfied, it performs fast gradient updates to θ_{t+1}. When violated, it accumulates observations into set H_t and tightens the confidence set Θ_{t+1}. This ensures expensive updates occur only for a small fraction of rounds (Õ(κdN S⁶)).

### Mechanism 3: Global Learning with Joint Parameter Estimation
A single shared parameter vector θ ∈ R^(dN) across all slots enables sample-efficient learning despite receiving only slate-level bandit feedback. After receiving binary reward y_t for slate x_t, it updates θ using the full slate context, not credit assignment to individual items. The design matrices W^i_t share the common weighting factor ˙μ(x_s^T θ_{s+1}), propagating global reward information across all slots.

## Foundational Learning

- **Optimism in the Face of Uncertainty (OFU)**
  - Why needed: Core principle for balancing exploration-exploitation; constructs confidence sets containing θ* and selects actions maximizing optimistic reward estimates
  - Quick check: Can you explain why selecting actions that maximize an optimistic upper bound on expected reward naturally encourages exploration in uncertain regions?

- **Self-Concordance of the Logistic Function**
  - Why needed: Critical for achieving κ-independent regret bounds; self-concordance bounds how much the logistic function's curvature can deviate, enabling analysis that doesn't depend on worst-case nonlinearity parameter κ
  - Quick check: What does the self-concordance property state about the relationship between ˙μ(x) at different points x, and how does Lemma B.14 exploit this?

- **Thompson Sampling Distribution Properties**
  - Why needed: For Slate-GLM-TS-Fixed, the noise distribution D_TS must satisfy concentration (bounded norm with high probability) and anti-concentration (sufficient probability of exploring in any direction) for provable regret guarantees
  - Quick check: What are the two properties in Definition E.2 that a multivariate distribution must satisfy for Thompson Sampling, and why is each necessary?

## Architecture Onboarding

- **Component map:**
```
Main Loop (Slate-GLM-OFU/TS)
├── Slot Selection Module
│   ├── Per-slot optimization: x^i_t = argmax_{x∈X^i_t} [⟨x, θ^i_t⟩ + bonus_term]
│   └── Parallel execution across N slots (embarrassingly parallel)
├── Slate Assembly: Concatenate x_t = (x¹_t, ..., x^N_t)
├── Environment Interface
│   ├── Receive candidate sets X^i_t from environment
│   └── Submit slate x_t, receive binary reward y_t
└── Parameter Update Module (Algorithm 2)
    ├── Fast Path (gradient update):
    │   ├── Solve: θ_{t+1} = argmin_{Θ_t} [η||θ-θ_t||²_{W_t} + ℓ(x_t^T θ, y_t)]
    │   └── Update W_{t+1} = W_t + ˙μ(x_t^T θ_{t+1}) x_t x_t^T
    └── Slow Path (confidence refinement):
        ├── Append (x_t, y_t) to history H_t
        ├── Recompute θ^H_{t+1} via regularized MLE on H_t
        └── Tighten Θ_{t+1} = {θ: ||θ - θ^H_{t+1}||²_{V^H_t} ≤ β_t(δ)} ∩ Θ_1
```

- **Critical path:**
  1. **Diversity initialization**: Verify feature distributions satisfy covariance lower bound during warm-up (τ rounds for TS-Fixed variant)
  2. **Per-slot optimization**: N independent argmax operations (dominates per-round cost, but polynomial in N)
  3. **Update path selection**: Evaluate inequality criterion to determine fast vs. slow update path
  4. **Matrix maintenance**: Incremental rank-1 updates to W_t and W^i_t (must maintain positive definiteness)

- **Design tradeoffs:**
  - **OFU vs TS variants**: OFU (Algorithm 1) has proven Õ(√T) regret for contextual setting but requires constrained optimization; TS (Algorithm 3) is computationally simpler (just sampling + linear argmax) but only has proven guarantees for fixed-arm setting (Algorithm 4)
  - **Update frequency**: The threshold 2 min{˙μ(x_t^T θ⁰_t), ˙μ(x_t^T θ¹_t)} trades off between more frequent gradient updates (faster but potentially unstable) and confidence set updates (stable but O(Ndt) complexity)
  - **Single vs multiple parameter models**: Unlike prior work (MPS algorithm) that attributes reward to individual slots via credit assignment, this approach maintains statistical efficiency through joint estimation but cannot disambiguate per-slot contributions

- **Failure signatures:**
  - **Regret not decreasing**: Check if min eigenvalues of W^i_t are growing linearly; if plateauing, diversity assumption may be violated
  - **Per-round time growing exponentially with N**: Indicates slot decomposition failing; verify that per-slot optimization is actually independent
  - **Numerical overflow in ˙μ**: Occurs when |x_t^T θ| is large; ensure ||x^i_t||₂ ≤ 1/√N and ||θ*||₂ ≤ S bounds are enforced
  - **Confidence set never shrinking**: Inequality criterion triggering too rarely; check that θ̄_t, θ⁰_t, θ¹_t are computed correctly

- **First 3 experiments:**
  1. **Diversity assumption validation** (replicate Figure 3): Generate random instances with N=3, d=5, K=5 items per slot; run for T=10,000 rounds; plot λ_min(W^i_t) vs t for each slot; verify approximately linear growth
  2. **Regret comparison across algorithms** (replicate Figure 1a/1b): Compare Slate-GLM-OFU, Slate-GLM-TS against baselines (ada-OFU-ECOLog, TS-ECOLog, MPS) for both finite context (C=5 contexts) and infinite contexts; measure regret at T∈{1000, 5000, 10000, 15000, 20000}
  3. **Runtime scaling analysis** (replicate Figure 1d/1e): Vary N∈{3,4,5,6} with K=7 items per slot; measure average and maximum per-round time for all algorithms over T=1000 rounds; verify Slate-GLM variants show polynomial growth while baselines show exponential growth

## Open Questions the Paper Calls Out

- **Can the Õ(√T) regret guarantee be theoretically established for Slate-GLM-TS in the contextual setting?**
  The paper provides such a guarantee only for Slate-GLM-TS-Fixed (non-contextual). The proof for the fixed-arm version relies on a specific warm-up phase and fixed-context eigenvalue growth that may not directly transfer to the adaptive, time-varying context distributions of the general setting.

- **Can the "local planning" efficiency and low regret be maintained if the diversity assumption is violated?**
  The entire efficiency argument relies on Lemma B.9, which requires this eigenvalue growth to ensure U_t ≈ W_t. If features in a slot are not sufficiently diverse, the minimum eigenvalue of the design matrix may not grow linearly, potentially breaking the slot-level optimization guarantee.

- **Can the dependency on the non-linearity parameter κ be removed from the constant term of the Slate-GLM-OFU regret bound?**
  While the abstract claims "κ-free" regret, Theorem 3.1 retains a term O(S^6 d^2 N^2 κ). This arises from bounding the "hard" rounds T where the self-concordance condition fails.

## Limitations

- The analysis assumes the diversity assumption holds, which may be restrictive in practice
- The single parameter model cannot disambiguate individual slot contributions, limiting interpretability
- Thompson Sampling variants lack proven regret guarantees for the contextual setting
- Real-world performance depends on the correctness of the logistic model assumption and feature representation quality

## Confidence

- **High Confidence**: N^O(1) per-round complexity and Õ(√T) regret claims for Slate-GLM-OFU are supported by formal theorems with clear proof structure
- **Medium Confidence**: Practical advantages demonstrated in experiments are compelling but limited to specific synthetic and prompt-tuning settings
- **Low Confidence**: Thompson Sampling variants have weaker theoretical guarantees for the contextual setting, and D_TS distribution properties required for analysis are not fully specified

## Next Checks

1. **Diversity Assumption Robustness**: Generate synthetic datasets with varying degrees of feature correlation across slots (from independent to highly correlated). Run Slate-GLM-OFU and measure how λ_min(W^i_t) growth rate and regret scale with correlation strength.

2. **Model Misspecification Stress Test**: Replace the logistic reward model with a non-logistic function (e.g., step function or polynomial). Compare regret and parameter estimation quality between Slate-GLM-OFU and baselines to quantify impact of logistic model assumption violations.

3. **Thompson Sampling Contextual Guarantees**: Implement a simpler contextual bandit setting (e.g., finite contexts with K=2 items per slot) and run extensive experiments to empirically evaluate Slate-GLM-TS regret behavior. Compare against theoretical predictions for the fixed-arm variant to assess the gap between theory and practice.