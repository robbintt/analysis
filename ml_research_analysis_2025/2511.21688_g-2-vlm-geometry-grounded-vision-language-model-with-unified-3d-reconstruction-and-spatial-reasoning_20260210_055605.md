---
ver: rpa2
title: 'G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction
  and Spatial Reasoning'
arxiv_id: '2511.21688'
source_url: https://arxiv.org/abs/2511.21688
tags:
- spatial
- geometry
- reasoning
- wang
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: G2VLM is a unified vision-language model that bridges spatial 3D
  reconstruction and high-level spatial reasoning by integrating a geometric perception
  expert with a semantic perception expert in a Mixture-of-Experts architecture. The
  model learns 3D geometry from 2D images and enhances spatial reasoning through shared
  self-attention, interleaved reasoning, and in-context learning.
---

# G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning

## Quick Facts
- **arXiv ID:** 2511.21688
- **Source URL:** https://arxiv.org/abs/2511.21688
- **Reference count:** 40
- **Primary result:** Achieves 18.5-point improvement over GPT-4o on SPAR-Bench while matching specialized 3D reconstruction models on geometry tasks

## Executive Summary
G$^2$VLM is a unified vision-language model that bridges spatial 3D reconstruction and high-level spatial reasoning by integrating a geometric perception expert with a semantic perception expert in a Mixture-of-Experts architecture. The model learns 3D geometry from 2D images and enhances spatial reasoning through shared self-attention, interleaved reasoning, and in-context learning. Experimental results show G$^2$VLM achieves comparable performance to state-of-the-art 3D reconstruction models on depth, point, and camera pose estimation tasks, while outperforming existing spatial reasoning models on comprehensive benchmarks like SPAR-Bench, MindCube, and OmniSpatial. Specifically, G$^2$VLM-SR surpasses GPT-4o by 18.5 points on SPAR-Bench, demonstrating superior spatial understanding despite its small 2B parameter size. The unified design enables mutual improvement between geometric and semantic representations, offering a strong baseline for future 3D vision and reasoning applications.

## Method Summary
G$^2$VLM employs a Mixture-of-Transformers-Experts architecture with dual experts: a geometric perception expert for 3D reconstruction (depth, point maps, camera poses) and a semantic perception expert for spatial reasoning. The geometric expert uses a DINOv2 encoder with 28 transformer layers and lightweight decoder heads, while the semantic expert uses Qwen2-VL-2B weights. During joint training, shared self-attention enables bidirectional feature exchange between experts. The model undergoes two-stage training: geometry pretraining on 3D-annotated datasets (ScanNet, Co3Dv2, DL3DV, etc.) followed by joint optimization with spatial reasoning data. The unified design enables mutual improvement between geometric and semantic representations through explicit 3D geometry supervision and interleaved reasoning.

## Key Results
- G$^2$VLM-SR outperforms GPT-4o by 18.5 points on SPAR-Bench spatial reasoning benchmark
- Achieves depth estimation performance (Abs Rel ~0.12) comparable to specialized models like DVOG and BEVStereo
- Matches state-of-the-art point reconstruction accuracy on ETH3D and 7-Scenes datasets
- Demonstrates superior performance on MindCube and OmniSpatial benchmarks compared to baseline VLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating geometric and semantic processing into specialized experts with shared attention enables mutual improvement between 3D reconstruction and spatial reasoning.
- **Mechanism:** The Mixture-of-Transformers-Experts architecture dedicates one expert ("where pathway") to low-level geometry learning (depth, points, camera poses) and another ("what pathway") to semantic understanding. Shared self-attention allows geometry features to inform reasoning while semantic context refines spatial understanding—a bidirectional flow absent in standard VLMs that flatten multi-view inputs.
- **Core assumption:** Spatial reasoning requires explicit 3D representations, not just implicit 2D-to-3D inference from language priors.
- **Evidence anchors:**
  - [abstract]: "integrating a geometric perception expert with a semantic perception expert in a Mixture-of-Experts architecture... shared self-attention, interleaved reasoning"
  - [section 3.1]: "These experts interact via shared self-attention, enabling the interplay between these two fundamental aspects to mutually improve one another"
  - [corpus]: VLM-3R paper (VLM-3R) also augments VLMs with 3D reconstruction, but uses frozen external geometry encoder—G²VLM integrates geometry natively, suggesting architectural integration matters.

### Mechanism 2
- **Claim:** Explicit 3D geometry supervision (depth, point maps, camera poses) provides grounding that improves spatial reasoning beyond what 2D image-text training can achieve.
- **Mechanism:** The geometric expert is trained with multi-view reconstruction losses (point reconstruction, camera pose, surface normals) on large-scale 3D-annotated datasets. This forces the model to learn metric spatial relationships. During joint training, frozen geometry features provide in-context 3D priors to the semantic expert, which learns to query and reason over them via cross-entropy loss on spatial QA tasks.
- **Core assumption:** 3D geometry learning from multi-view images transfers to single-image spatial reasoning tasks.
- **Evidence anchors:**
  - [abstract]: "learns 3D geometry from 2D images and enhances spatial reasoning through shared self-attention"
  - [section 3.2]: "LVG = Lpoints + λcamLcam + λnormalLnormal" defines explicit geometry supervision
  - [corpus]: Chain-of-Visual-Thought (CVA) paper notes VLMs struggle with "dense visual perception" for spatial reasoning—G²VLM addresses this via explicit geometry heads rather than pure token reasoning.

### Mechanism 3
- **Claim:** Interleaved reasoning—explicitly predicting geometry before answering spatial questions—provides a reasoning scaffold that decomposes complex spatial tasks.
- **Mechanism:** For spatial questions, G²VLM can generate intermediate geometry predictions (depth maps, camera poses) before producing the final text answer. This forces explicit spatial computation rather than relying on memorized 2D patterns. The paper calls this "in-context learning" via geometry features.
- **Core assumption:** Decomposing spatial reasoning into explicit geometry prediction steps improves answer accuracy, similar to chain-of-thought in language.
- **Evidence anchors:**
  - [abstract]: "enhances spatial reasoning through... interleaved reasoning, and in-context learning"
  - [section 3.3]: "optimizing the semantic perception expert to effectively utilize these geometric features via in-context learning and interleaved reasoning"
  - [corpus]: VisualSphinx paper shows structured reasoning on vision puzzles improves VLM performance—parallel to G²VLM's geometry-as-reasoning-step approach.

## Foundational Learning

- **Concept: Feed-forward 3D reconstruction (DUSt3R, VGGT, π3 family)**
  - **Why needed here:** G²VLM's geometric expert architecture inherits from this line of work—predicting pixel-aligned 3D points, depth, and camera poses via transformer decoders rather than optimization-based SfM.
  - **Quick check question:** Can you explain how DUSt3R predicts dense 3D point maps from image pairs without known camera parameters?

- **Concept: Mixture-of-Experts / Mixture-of-Transformers-Experts**
  - **Why needed here:** The core architecture uses MoT to route tokens to geometric vs. semantic experts while sharing self-attention—understanding sparse routing vs. shared attention is critical for implementation.
  - **Quick check question:** How does MoT differ from standard MoE? What is shared vs. specialized across experts?

- **Concept: 3D reconstruction loss functions (scale-invariant depth, geodesic rotation distance, surface normals)**
  - **Why needed here:** Training the geometric expert requires understanding the specific losses: optimal scale for point reconstruction, geodesic distance for rotations, angular normal loss for smooth surfaces.
  - **Quick check question:** Why does point reconstruction loss require solving for optimal scale factor s* rather than using fixed depth normalization?

## Architecture Onboarding

**Component map:**
Input: N RGB images
  ├─→ DINOv2 encoder → linear projection → Geometric Perception Expert (28 layers, global attention)
  │                                         ├─→ Local point head (MLP + pixel shuffle)
  │                                         ├─→ Camera head (MLP + SVD orthogonalization)
  │                                         └─→ Global point head (training stabilization)
  └─→ Qwen2-VL encoder → Semantic Perception Expert (28 layers, global attention)
                                          ↓
                              Shared Self-Attention (all tokens)
                                          ↓
                              Text de-tokenizer → spatial reasoning output

**Critical path:**
1. **Dual-encoder input:** Images must flow through BOTH DINOv2 (geometry) and Qwen2-VL encoder (semantic)—single encoder ablation shows degraded performance on both tasks.
2. **Geometry pretraining stage:** Geometric expert trained from scratch on 3D-annotated datasets while semantic expert is frozen (Qwen2-VL weights).
3. **Joint training stage:** Semantic expert unfrozen, trained with CE loss on spatial reasoning data; geometric expert either frozen (G²VLM) or jointly optimized with CE+VG loss (G²VLM-SR).
4. **Inference:** Geometry heads can be active (interleaved reasoning) or bypassed (pure text output) depending on task.

**Design tradeoffs:**
- **Global attention vs. alternating attention:** Paper ablates global attention against frame-wise and mixed attention—global wins for LLM compatibility but may sacrifice local geometry precision.
- **CE-only vs. VG+CE joint training:** VG+CE yields best spatial reasoning but requires scarce 3D annotations; CE-only scales better with abundant video data.
- **Single vs. dual encoder:** DINOv2 provides low-level geometry features that CLIP lacks; dual-encoder design outperforms single-encoder on both geometry and reasoning.

**Failure signatures:**
- **Training instability with large-scale models:** Paper explicitly notes this limitation—requires gradient clipping (threshold 1.0), bfloat16, and careful data curation.
- **Loss spikes during geometry pretraining:** Noisy 3D annotations cause outlier losses; paper clips losses >10 to 0.
- **Degraded geometry performance if jointly trained without VG loss:** CE+CE variant improves reasoning but sacrifices reconstruction accuracy.

**First 3 experiments:**
1. **Reproduce encoder ablation:** Train geometric expert with only CLIP encoder vs. DINOv2—confirm dual-encoder advantage on depth estimation (Abs Rel) and SPAR-Bench scores.
2. **Attention mechanism comparison:** Within geometric expert, compare global vs. frame-wise vs. mixed attention on point map completion (ETH3D/7-Scenes metrics) and training loss curves.
3. **Joint training strategy validation:** On ScanNet, compare CE-only vs. CE+CE vs. VG+CE on both geometry (point accuracy) and reasoning (SPAR-Bench subset) to reproduce Figure 4 tradeoff curves.

## Open Questions the Paper Calls Out

- **Question:** What specific optimization techniques or architectural modifications are required to mitigate training instability when scaling the G$^2$VLM architecture to sizes significantly larger than the current 2B parameters?
  - **Basis in paper:** [explicit] The Conclusion explicitly lists "training instability with large-scale models" as a potential limitation, noting that this challenge "requires advanced optimization techniques."
  - **Why unresolved:** The authors limited their experiments to a 2B parameter model (Qwen2-VL-2B) and did not demonstrate successful training stability at larger scales (e.g., 7B or 70B).
  - **What evidence would resolve it:** A study demonstrating stable convergence and improved performance when training G$^2$VLM on larger backbones (e.g., 7B+), utilizing specific techniques like distinct learning rate schedules or gradient management to handle the instability.

- **Question:** Can the superior joint-training strategy ("VG + CE Loss") be scaled efficiently without relying on difficult-to-collect 3D annotations, perhaps through synthetic data or self-supervision?
  - **Basis in paper:** [explicit] Section 3.3 states that the best-performing strategy ("VG + CE Loss") "requires a large-scale 3D annotated dataset... which limits its scalability." Consequently, the authors used "CE Loss Only" for the main model to leverage abundant video data.
  - **Why unresolved:** There is a trade-off between the superior mutual improvement found in "VG + CE" training and the data scalability of the "CE Loss Only" approach. It is unclear if the performance gap can be closed while maintaining scalability.
  - **What evidence would resolve it:** An experiment showing that incorporating synthetic 3D data or improved self-supervised geometric objectives allows the "VG + CE" strategy to scale to internet-sized video datasets without manual 3D annotations.

- **Question:** Does scaling the unified model to larger parameter sizes (e.g., 72B) specifically bridge the performance gap observed in "online spatio-temporal scene understanding" tasks like OST-Bench?
  - **Basis in paper:** [explicit] Section 4.2 notes that on the OST-Bench, the much larger Qwen2.5-VL-72B outperforms G$^2$VLM-SR, suggesting that this specific task "requires models to store significant knowledge, favoring larger architectures."
  - **Why unresolved:** It is unclear if the G$^2$VLM architecture retains its advantages over standard VLMs at larger scales, or if the "positive interplay" between geometry and semantics scales linearly with parameter count to compete with massive proprietary models.
  - **What evidence would resolve it:** Evaluation of a larger-scale G$^2$VLM variant on OST-Bench to see if the unified geometry learning closes the gap with large language-only or standard VLM models that rely on massive parametric memory.

## Limitations
- Training instability with large-scale models requires advanced optimization techniques and careful hyperparameter tuning
- Dual-encoder architecture increases computational cost compared to single-encoder VLMs
- Reliance on curated 3D-annotated datasets limits scalability to domains with scarce geometry annotations

## Confidence

**High:** Core geometry-reasoning synergy mechanism (consistent benchmark improvements across multiple tasks)
**Medium:** Architectural design choices (validated through ablation but may not generalize to all dataset regimes)
**Low:** Real-world deployment readiness (noted training instability and computational overhead)

## Next Checks

1. **Scaling experiment:** Train a 7B-parameter variant of G$^2$VLM on a subset of the geometric datasets to test whether the architecture maintains stability and performance gains at larger scales, or if the training instability becomes prohibitive.

2. **Single-encoder ablation:** Replace the dual DINOv2+Qwen2-VL encoder with a single frozen CLIP encoder in the geometric expert to quantify the contribution of low-level geometry features versus semantic priors in the reconstruction and reasoning performance.

3. **Cross-domain generalization:** Evaluate G$^2$VLM on outdoor/indoor scene understanding tasks (e.g., KITTI depth, ScanNet-MLT) to test whether the geometry-reasoning transfer observed on curated benchmarks holds in less controlled environments with varying camera intrinsics and scene complexity.