---
ver: rpa2
title: 'SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable
  Big Data in Circuits'
arxiv_id: '2509.00071'
source_url: https://arxiv.org/abs/2509.00071
tags:
- graph
- circuit
- design
- circuits
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of open-source circuit design
  data by proposing SynCircuit, a framework for automatically generating synthetic
  RTL circuits. The method employs a diffusion-based generative model to create directed
  cyclic graphs representing circuits, followed by probability-guided post-processing
  to ensure circuit validity and Monte Carlo tree search to optimize logic redundancy.
---

# SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits

## Quick Facts
- **arXiv ID:** 2509.00071
- **Source URL:** https://arxiv.org/abs/2509.00071
- **Reference count:** 35
- **Primary result:** Generated synthetic RTL circuits with structural properties more similar to real designs, improving ML model performance in PPA prediction tasks.

## Executive Summary
This paper addresses the scarcity of open-source circuit design data by proposing SynCircuit, a framework for automatically generating synthetic RTL circuits. The method employs a diffusion-based generative model to create directed cyclic graphs representing circuits, followed by probability-guided post-processing to ensure circuit validity and Monte Carlo tree search to optimize logic redundancy. Experiments demonstrate that SynCircuit generates circuits with structural properties more similar to real designs, as measured by Wasserstein distance metrics, and enhance ML model performance in RTL-level PPA prediction tasks.

## Method Summary
SynCircuit generates synthetic RTL circuits through a three-phase process: first, a diffusion-based model generates directed cyclic graphs representing circuit topologies; second, probability-guided post-processing refines these graphs to ensure functional validity by enforcing circuit constraints; finally, Monte Carlo tree search optimizes the circuits to reduce logic redundancy, maximizing the preservation of sequential elements after synthesis. The generated circuits are validated through structural similarity metrics and downstream ML task performance.

## Key Results
- Wasserstein distance metrics show SynCircuit generates circuits more similar to real designs (W1=0.236) compared to baselines (0.598-1.31).
- Sequential Cell Preservation Ratio (SCPR) improves through MCTS optimization, indicating reduced logic redundancy.
- ML models trained on SynCircuit data show up to 10% reduction in MAPE for Register Slack prediction compared to models trained only on real designs.

## Why This Works (Mechanism)

### Mechanism 1: Diffusion-based Directed Cyclic Graph (DCG) Generation
- Claim: Generating synthetic RTL circuits as graphs is feasible using a diffusion-based model adapted for directed cyclic graphs.
- Mechanism: The model uses a diffusion process (forward corruption of adjacency matrices) and a reverse denoising process. A Message Passing Neural Network (MPNN) encoder captures structural information, and an asymmetric edge decoder using learnable relation embeddings reconstructs directed edge probabilities.
- Core assumption: The structural properties of valid circuits can be learned and sampled from a latent distribution represented by the diffusion model.

### Mechanism 2: Probability-Guided Graph Post-Processing
- Claim: Raw graph generation often produces invalid circuits; a post-processing step is required to enforce functional validity.
- Mechanism: An autoregressive method iteratively refines the initial synthetic graph ($G_{ini}$) using the edge probability matrix ($P_E^{(t=0)}$) from the diffusion model. It sequentially assigns parent nodes based on probability while checking for circuit constraints like fanin limitations and the prevention of combinational loops.

### Mechanism 3: Monte Carlo Tree Search (MCTS) for Logic Redundancy Optimization
- Claim: Synthetic circuits often contain excess logic redundancy; search-based optimization can reduce it to better match real designs.
- Mechanism: MCTS is used to fine-tune the valid graph ($G_{val}$) into an optimized graph ($G_{opt}$). The search uses a new metric, Post-Synthesis Circuit Size (PCS), as a reward. An "atomic swapping operation" on the adjacency matrix is the action space, which preserves in/out degrees while seeking to maximize the Sequential Cell Preservation Ratio (SCPR).

## Foundational Learning

- **Concept: Directed Cyclic Graphs (DCGs) in Circuit Design**
  - Why needed here: Understanding that digital circuits, especially at the RTL level with feedback loops (registers), are best modeled as DCGs, not DAGs. This distinction is the core problem the paper addresses.
  - Quick check question: Why does a topological sort, used for DAG generation, fail for a circuit with registers in a feedback loop?

- **Concept: Diffusion Models for Graphs**
  - Why needed here: The core generative engine is a diffusion model. You need to understand the high-level concept of forward (adding noise) and reverse (denoising) processes applied to graph adjacency matrices.
  - Quick check question: In the context of graph generation, what does the denoising network predict at each time step?

- **Concept: Monte Carlo Tree Search (MCTS)**
  - Why needed here: The final optimization phase relies on MCTS. Understanding its four steps (selection, expansion, simulation, backpropagation) is crucial to grasp how the circuit is refined.
  - Quick check question: What is the role of the reward signal (PCS) in the MCTS-based circuit optimization process?

## Architecture Onboarding

- **Component map:**
  Input Parser -> Diffusion Model (Forward Process -> Denoising Network) -> Validity Refiner -> MCTS Optimizer -> Output Parser

- **Critical path:**
  1. Training: Train the denoising network $\phi_{\theta,t}$ on real circuit graphs.
  2. Generation: Sample noise -> Diffusion Denoising -> Get $G_{ini}$ and $P_E$.
  3. Refinement: Apply post-processing to $G_{ini}$ using $P_E$ -> Get $G_{val}$.
  4. Optimization: Run MCTS on $G_{val}$ -> Get $G_{opt}$.
  5. Deployment: Convert $G_{opt}$ to HDL and use for downstream ML training.

- **Design tradeoffs:**
  - **Realism vs. Diversity:** The diffusion model learns from a small set of real designs. It may struggle to generate highly novel topologies unseen in training.
  - **Validity vs. Fidelity:** The probability-guided post-processing ensures 100% validity but may alter the generated structure, potentially reducing fidelity to the diffusion model's raw output.
  - **Quality vs. Compute Cost:** The MCTS optimization is computationally expensive (500 simulations per cone) but is necessary to reduce logic redundancy and improve data quality.

- **Failure signatures:**
  - **Invalid Circuits:** If post-processing fails, $G_{val}$ will have combinational loops or invalid fanins, leading to synthesis errors.
  - **High Redundancy:** If MCTS is skipped or misconfigured, the SCPR of $G_{opt}$ will be low (<20%), meaning most of the design is optimized away, making it useless for training.
  - **Poor Graph Statistics:** If the diffusion model is not trained well, $G_{ini}$ will have degree distributions far from real circuits (high Wasserstein distance).

- **First 3 experiments:**
  1. **Structural Similarity Test:** Generate a batch of synthetic circuits. Calculate the Wasserstein distance for degree distributions and orbit counts. Compare against baselines to ensure $G_{ini}$ is high-quality.
  2. **Validity & Redundancy Check:** Synthesize both $G_{val}$ (post-processed) and $G_{opt}$ (MCTS-optimized). Check for synthesis errors and measure the SCPR for both to quantify the impact of MCTS.
  3. **Downstream Task Ablation:** Train a simple ML model for a known task (e.g., PPA prediction from the paper) on three datasets: only real data, real + $G_{val}$, and real + $G_{opt}$. Compare prediction error (MAPE) to prove the utility of the synthetic data and the importance of the optimization step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the diffusion model be trained to inherently minimize logic redundancy, removing the need for the computationally intensive Monte Carlo Tree Search (MCTS) post-processing step?
- Basis in paper: [inferred] Section VI highlights that initial generated designs often have a Sequential Cell Preservation Ratio (SCPR) under 10%, requiring a separate MCTS phase to correct this "serious logic redundancy."
- Why unresolved: The paper treats redundancy as a post-hoc error correction task rather than a learned constraint within the generative model itself.
- What evidence would resolve it: A demonstration that modifying the diffusion loss function or training objective yields high SCPR scores (e.g., >70%) without any MCTS refinement.

### Open Question 2
- Question: How can the framework's memory efficiency be improved to handle industrial-scale circuit graphs that significantly exceed the current 10,000-node limitation?
- Basis in paper: [explicit] Section IV.C states: "Handling large-scale graphs with more than 10K nodes poses computational challenges," specifically noting the linear complexity of MPNNs is beneficial but the full adjacency matrix handling remains a bottleneck.
- Why unresolved: The method relies on full adjacency matrix corruption and reconstruction, which scales quadratically with node count, making industrial designs (often >100k nodes) infeasible.
- What evidence would resolve it: Generation of valid circuits with >50k nodes using a modified sparse representation or latent diffusion approach that avoids full matrix storage.

### Open Question 3
- Question: Do the generated circuits provide utility for generative downstream tasks (e.g., logic synthesis or placement optimization) or are they limited to discriminative tasks like PPA prediction?
- Basis in paper: [inferred] Section VII validates the designs only on PPA prediction, while the Introduction lists "automated chip design generation" as a key potential application that remains unverified in the results.
- Why unresolved: It is unclear if the synthetic designs are "realistic" enough to train models that act upon the netlist structure to improve it, rather than just predict its properties.
- What evidence would resolve it: Using SynCircuit data to train a reinforcement learning agent for logic optimization that transfers successfully to real designs.

## Limitations

- The specific HDL-to-Graph parser implementation and rules for mapping complex Verilog constructs to the 5-node type system are not provided, which is critical for reproducing the pipeline.
- The MCTS optimization relies on a "trained discriminator" to approximate the Post-Synthesis Size (PCS) reward without requiring full synthesis, but the architecture, training data, and performance of this proxy model are not disclosed.
- The diffusion model is trained on only 22 real RTL designs, raising questions about the generalizability and diversity of the generated circuits.

## Confidence

- **High Confidence:** The core methodology of using a diffusion model for DCG generation and the structural similarity results (Wasserstein distance) are well-supported by the paper's experiments and mathematical formulation.
- **Medium Confidence:** The effectiveness of the probability-guided post-processing for ensuring circuit validity is plausible but lacks strong, explicit evidence in the related work corpus.
- **Low Confidence:** The specific implementation and impact of the MCTS optimization for logic redundancy reduction are the least transparent, particularly due to the reliance on a black-box "trained discriminator" for the reward signal.

## Next Checks

1. **Parser Validation:** Implement or obtain an open-source HDL parser and validate its ability to correctly convert a set of Verilog testbenches into the specified 5-node type DCG representation. Check for edge cases in bit-slicing and complex logic expressions.
2. **Diffusion Model Ablation:** Train the diffusion model (Phase 1) and generate a batch of initial graphs ($G_{ini}$). Analyze the degree distributions and orbit counts. Compare the Wasserstein distance to the claimed ~0.236 without running the full pipeline to isolate the impact of the generative model itself.
3. **MCTS Reward Proxy Evaluation:** If possible, obtain or train a proxy model to predict PCS from a graph. Evaluate its accuracy against a small set of ground-truth synthesis results. This will quantify the potential error introduced by using a proxy reward in the MCTS search.