---
ver: rpa2
title: 'From Logits to Hierarchies: Hierarchical Clustering made Simple'
arxiv_id: '2410.07858'
source_url: https://arxiv.org/abs/2410.07858
tags:
- clustering
- hierarchical
- clusters
- hierarchy
- logits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Recent deep models for hierarchical clustering struggle with scalability
  and underperform at the leaf level compared to flat clustering. This paper proposes
  a lightweight method, Logits to Hierarchies (L2H), which derives a hierarchical
  structure from the logits of a pre-trained flat clustering model without fine-tuning.
---

# From Logits to Hierarchies: Hierarchical Clustering made Simple

## Quick Facts
- arXiv ID: 2410.07858
- Source URL: https://arxiv.org/abs/2410.07858
- Reference count: 24
- Primary result: Achieves state-of-the-art hierarchical clustering performance on CIFAR-10, CIFAR-100, and Food-101 while being significantly more efficient than deep specialized approaches

## Executive Summary
This paper introduces Logits to Hierarchies (L2H), a lightweight method that derives hierarchical clustering structures from the logits of pre-trained flat clustering models without requiring fine-tuning. Unlike existing deep learning approaches that struggle with scalability and underperform at leaf levels, L2H achieves superior hierarchical clustering performance while maintaining computational efficiency. The method works by extracting a flat partition from model logits, computing a similarity matrix based on prediction probabilities, and applying hierarchical clustering to create a tree structure. L2H demonstrates state-of-the-art results on standard image benchmarks and can generalize to supervised models to reveal meaningful class hierarchies and potential biases.

## Method Summary
L2H operates by leveraging the internal representations of pre-trained flat clustering models to construct hierarchical structures. The method extracts a flat partition from the model's logits, computes a similarity matrix using prediction probabilities across samples, and applies standard hierarchical clustering algorithms to generate the tree. This approach avoids the computational overhead of end-to-end deep learning methods while achieving superior performance. The method is particularly effective because logits from flat clustering models contain rich hierarchical information that can be extracted through appropriate similarity measures. L2H can also be applied to supervised models, where it recovers semantically meaningful hierarchies from ImageNet classifiers and reveals potential biases in class relationships.

## Key Results
- Achieves state-of-the-art hierarchical clustering performance on CIFAR-10, CIFAR-100, and Food-101 datasets
- Significantly more efficient than deep specialized approaches, avoiding expensive fine-tuning requirements
- Successfully recovers meaningful class hierarchies from ImageNet classifiers when applied to supervised models
- Reveals potential biases in class relationships through hierarchical structure analysis

## Why This Works (Mechanism)
The method works by exploiting the rich hierarchical information encoded in the logits of pre-trained flat clustering models. When a model makes predictions, the logits contain not just the most likely class but also the relative confidence across all classes. This confidence distribution captures semantic relationships between classes - similar classes tend to have similar probability distributions across samples. By computing pairwise similarities based on these probability distributions, L2H can identify natural groupings and sub-groupings that form a hierarchy. The approach is particularly effective because it doesn't require learning new parameters or fine-tuning, instead leveraging existing model knowledge through careful similarity computation and standard hierarchical clustering algorithms.

## Foundational Learning
1. **Flat vs Hierarchical Clustering**: Flat clustering assigns each sample to exactly one cluster without explicit relationships, while hierarchical clustering creates tree structures showing nested relationships between clusters. This distinction is crucial because L2H bridges these paradigms by deriving hierarchies from flat model outputs.
   - Why needed: Understanding this relationship helps explain why pre-trained flat models contain sufficient information for hierarchical structures
   - Quick check: Can you identify examples where flat clustering fails to capture natural groupings that hierarchical clustering would reveal?

2. **Logits and Probability Distributions**: Logits are the raw output scores from neural networks before softmax normalization. These scores, when converted to probabilities, reveal the model's confidence across all classes for each sample.
   - Why needed: The method relies on extracting meaningful similarity measures from these probability distributions
   - Quick check: What information is lost when only considering the highest-probability class versus the full distribution?

3. **Similarity Matrix Construction**: L2H creates a matrix where each entry represents the similarity between two samples based on their predicted probability distributions across classes.
   - Why needed: This matrix forms the foundation for hierarchical clustering algorithms to identify natural groupings
   - Quick check: How would different similarity metrics (cosine, Euclidean, etc.) affect the resulting hierarchy?

## Architecture Onboarding

**Component Map**: Pre-trained Model -> Logits Extraction -> Probability Conversion -> Similarity Matrix Computation -> Hierarchical Clustering Algorithm -> Tree Structure

**Critical Path**: The most time-consuming steps are the similarity matrix computation (O(n²)) and the hierarchical clustering algorithm itself. The method's efficiency advantage comes from avoiding fine-tuning and working directly with pre-computed logits.

**Design Tradeoffs**: 
- Accuracy vs Efficiency: L2H trades the potential gains from end-to-end training for significant computational savings
- Model Dependency: Performance depends on the quality of the pre-trained model, but avoids the need for specialized hierarchical training
- Interpretability: The resulting hierarchies are more interpretable than black-box deep methods, as they reflect the original model's reasoning

**Failure Signatures**: 
- Poor hierarchies when pre-trained models have low accuracy or biased training data
- Computational bottlenecks with very large datasets due to O(n²) similarity matrix
- Flattened hierarchies when classes are too distinct or model confidences are too extreme

**First Experiments**:
1. Apply L2H to a simple pre-trained model on a small dataset (e.g., MNIST) to verify basic functionality
2. Compare similarity matrices using different distance metrics to understand their impact on hierarchy structure
3. Test L2H on a supervised ImageNet model to observe recovered class hierarchies and identify potential biases

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Method relies on availability of pre-trained flat clustering models, which may not always be accessible
- Assumes logits contain sufficient hierarchical information, which may not hold for all datasets or model architectures
- Experiments focus primarily on image datasets, limiting understanding of cross-modal applicability

## Confidence
- **High confidence**: The efficiency gains over deep specialized methods are well-supported by runtime comparisons
- **Medium confidence**: State-of-the-art performance claims are supported by benchmark results but need validation across more diverse datasets
- **Medium confidence**: Bias discovery claims in supervised models are interesting but require more systematic investigation

## Next Checks
1. Test L2H on non-image datasets (text, audio, or multimodal) to verify cross-modal applicability
2. Conduct ablation studies removing the pre-trained model assumption to understand robustness to model quality variations
3. Perform bias analysis on additional ImageNet classes and compare with known human-annotated hierarchies to quantify bias detection accuracy