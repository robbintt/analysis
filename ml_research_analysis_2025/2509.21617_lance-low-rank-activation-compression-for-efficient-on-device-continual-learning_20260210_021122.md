---
ver: rpa2
title: 'LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning'
arxiv_id: '2509.21617'
source_url: https://arxiv.org/abs/2509.21617
tags:
- learning
- lance
- memory
- continual
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LANCE, a low-rank activation compression framework
  for efficient on-device learning. LANCE addresses the high memory cost of storing
  intermediate activations during backpropagation by performing a one-shot higher-order
  singular value decomposition (HOSVD) to obtain a reusable low-rank subspace for
  activation projection.
---

# LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning

## Quick Facts
- **arXiv ID:** 2509.21617
- **Source URL:** https://arxiv.org/abs/2509.21617
- **Reference count:** 27
- **Primary result:** Achieves up to 250× memory reduction in activation storage while maintaining accuracy comparable to full backpropagation for continual learning

## Executive Summary
This paper introduces LANCE, a low-rank activation compression framework designed to address the high memory costs associated with storing intermediate activations during backpropagation in on-device continual learning scenarios. LANCE employs a one-shot higher-order singular value decomposition (HOSVD) to obtain a reusable low-rank subspace for activation projection, eliminating the need for repeated decompositions and reducing both memory and computational overhead. The fixed low-rank subspaces enable efficient fine-tuning and continual learning by allocating tasks to orthogonal subspaces without storing large task-specific matrices. Experiments demonstrate that LANCE maintains accuracy comparable to full backpropagation while achieving up to 250× reduction in activation storage across various datasets.

## Method Summary
LANCE operates by performing a one-shot HOSVD decomposition to create a fixed low-rank subspace that can be reused across multiple training iterations. This approach eliminates the computational overhead of repeated decompositions while maintaining the ability to project activations into lower-dimensional spaces. For continual learning, LANCE allocates different tasks to orthogonal subspaces, preventing catastrophic forgetting without requiring storage of large task-specific matrices. The framework is designed to be compatible with both convolutional neural networks and transformer architectures, providing a general solution for memory-efficient on-device learning. The low-rank projection is applied during both forward and backward passes, ensuring that memory savings are realized throughout the training process.

## Key Results
- Achieves up to 250× reduction in activation storage compared to standard backpropagation
- Maintains accuracy comparable to full backpropagation across various datasets
- Demonstrates competitive performance with orthogonal gradient projection methods for continual learning at significantly lower memory cost

## Why This Works (Mechanism)
LANCE leverages the inherent redundancy in neural network activations by projecting them onto low-rank subspaces. The one-shot HOSVD decomposition creates a reusable basis that captures the most salient features of the activation space while discarding less important components. This fixed subspace approach avoids the computational overhead of repeated decompositions while maintaining sufficient representational capacity for effective learning. For continual learning, the orthogonal allocation of tasks to different subspaces prevents interference between tasks, addressing the catastrophic forgetting problem without requiring additional memory for task-specific parameters.

## Foundational Learning

**Singular Value Decomposition (SVD):** A matrix factorization technique that decomposes a matrix into three components representing its most important features. Why needed: Forms the mathematical foundation for low-rank approximation. Quick check: Can be computed using standard linear algebra libraries.

**Higher-Order SVD (HOSVD):** An extension of SVD to multi-dimensional tensors. Why needed: Enables decomposition of activation tensors that have spatial, channel, and batch dimensions. Quick check: Available in tensor computation libraries like PyTorch and TensorFlow.

**Orthogonal Subspace Projection:** A technique where different tasks are allocated to mutually orthogonal subspaces. Why needed: Prevents interference between tasks in continual learning scenarios. Quick check: Can be verified by checking dot products between subspace bases.

**Catastrophic Forgetting:** The phenomenon where neural networks forget previously learned tasks when trained on new ones. Why needed: The primary problem LANCE addresses in continual learning. Quick check: Measured by comparing performance on old vs. new tasks.

**Memory-Accuracy Tradeoff:** The balance between model size/memory usage and predictive performance. Why needed: Central consideration in on-device learning applications. Quick check: Evaluated through ablation studies varying rank dimensions.

## Architecture Onboarding

**Component Map:** Input data → Forward pass through compressed activations → Loss computation → Backward pass with low-rank gradients → Parameter update

**Critical Path:** Data flow follows standard neural network training with LANCE inserting low-rank projection layers after each activation tensor. The critical computational path includes the HOSVD decomposition (one-time cost) and subsequent projection operations during training.

**Design Tradeoffs:** Fixed rank selection provides computational efficiency but may limit adaptability to varying activation distributions across layers. Orthogonal subspace allocation prevents forgetting but constrains the total number of learnable tasks based on available subspaces.

**Failure Signatures:** Performance degradation may occur if rank selection is too aggressive (losing critical information) or if task sequences exceed available orthogonal subspaces (causing interference). Memory savings diminish if rank selection approaches full dimensionality.

**First Experiments:** 1) Verify rank selection impact on accuracy using CIFAR-10 with varying compression ratios. 2) Test continual learning performance on split MNIST with different task sequence lengths. 3) Benchmark memory usage against standard backpropagation on a small transformer model.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to extremely large-scale models (billion parameters) remains unverified
- Fixed rank approach may not adapt well to varying activation distributions across different layer types
- Long-term continual learning with many sequential tasks may exceed available orthogonal subspaces

## Confidence
- **Memory Efficiency Claims:** High confidence - comprehensive experimental validation across multiple architectures
- **Accuracy Preservation Claims:** High confidence - extensive comparisons with full backpropagation baselines
- **Continual Learning Performance:** Medium confidence - evaluated on limited task sequences, long-term performance uncertain
- **Scalability Claims:** Low confidence - insufficient evidence from large-scale or diverse model architectures

## Next Checks
1. Evaluate LANCE on state-of-the-art large language models (e.g., LLaMA, GPT variants) to assess scalability and identify any architecture-specific limitations
2. Conduct long-horizon continual learning experiments with 20+ sequential tasks to test the durability of orthogonal subspace allocation over extended periods
3. Perform ablation studies varying the fixed rank across different layer types (attention vs. feed-forward) to determine whether a universal rank selection strategy is optimal or if adaptive per-layer approaches would yield better performance-memory tradeoffs