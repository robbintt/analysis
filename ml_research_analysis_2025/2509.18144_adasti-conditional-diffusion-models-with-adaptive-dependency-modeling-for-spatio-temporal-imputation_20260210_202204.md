---
ver: rpa2
title: 'AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for
  Spatio-Temporal Imputation'
arxiv_id: '2509.18144'
source_url: https://arxiv.org/abs/2509.18144
tags:
- data
- imputation
- spatio-temporal
- missing
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaSTI is a novel spatio-temporal imputation approach based on
  conditional diffusion models. It addresses the limitations of previous methods by
  introducing a BiS4PI network for pre-imputation using bidirectional S4 layers, and
  a Noise-Aware Spatio-Temporal (NAST) network with gated attention to capture step-variant
  dependencies across diffusion steps.
---

# AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation

## Quick Facts
- arXiv ID: 2509.18144
- Source URL: https://arxiv.org/abs/2509.18144
- Authors: Yubo Yang; Yichen Zhu; Bo Jiang
- Reference count: 40
- Primary result: AdaSTI achieves up to 46.4% reduction in imputation error on spatio-temporal datasets

## Executive Summary
AdaSTI is a novel spatio-temporal imputation approach based on conditional diffusion models that addresses limitations of previous methods. It introduces a BiS4PI network for pre-imputation using bidirectional S4 layers and a Noise-Aware Spatio-Temporal (NAST) network with gated attention to capture step-variant dependencies across diffusion steps. The model extracts global dependencies using a Spatial-Temporal Conditionalizer (STC) network and combines them with local dependencies. Extensive experiments on three real-world datasets show that AdaSTI outperforms existing methods in all settings, achieving up to 46.4% reduction in imputation error.

## Method Summary
AdaSTI implements a conditional diffusion model for spatio-temporal data imputation that consists of three main components: a BiS4PI network using bidirectional S4 layers for initial pre-imputation, an STC network to extract global spatio-temporal conditional information, and a NAST network with gated attention for step-variant denoising. The model processes incomplete spatio-temporal data through these components in sequence, with the STC providing global context to the NAST network during the denoising process. Training involves a combined loss function including diffusion MSE and consistency losses between forward and backward pre-imputations.

## Key Results
- Achieves 16.99 MAE and 27.75 RMSE on Metro dataset (25% random missing), significantly outperforming state-of-the-art methods
- Performance advantage increases with higher missing rates, demonstrating strong generalization ability
- Outperforms existing methods like NETS-ImpGAN and PriSTI across all tested datasets and missing rates

## Why This Works (Mechanism)

### Mechanism 1: Robust Pre-Imputation via Bidirectional State Space Models
Replacing linear interpolation with a trainable bidirectional S4 model (BiS4PI) minimizes initial imputation errors and prevents their propagation into the conditional diffusion stage. The Structured State Space (S4) layer initializes state matrices using HiPPO theory, assigning uniform weights to historical data rather than prioritizing recent inputs. This convolutional approach makes the hidden state robust to missing entries in the input sequence. By running this bidirectionally, the model captures both past and future contexts before generating a coarse imputation $X_c$.

### Mechanism 2: Adaptive Dependency Gating
A gated attention mechanism allows the model to dynamically balance global spatio-temporal context with local (step-specific) noisy features during denoising. The Noise-Aware Spatio-Temporal (NAST) network computes a gated output $R = G \odot R_{self} + (1-G) \odot R_{cross}$. Here, $R_{cross}$ represents global dependencies extracted by the STC network, while $R_{self}$ represents dependencies inherent in the current noisy data $X_t$. This allows the model to adjust the influence of the global prior vs. the current noisy state at every diffusion step.

### Mechanism 3: Hybrid Loss Consistency
Enforcing consistency between forward and backward pre-imputations stabilizes training when ground truth is missing. The BiS4PI network is trained with a consistency loss $\ell_{cons} = MAE(X^f_c \odot (1-M), X^b_c \odot (1-M))$. This penalizes divergence in the imputed missing values generated by the forward and backward passes, acting as a regularizer alongside the standard reconstruction loss on observed data.

## Foundational Learning

- **Structured State Space Models (S4)**: Understanding how S4 layers handle missing data via HiPPO initialization is critical to understanding why BiS4PI outperforms RNNs or interpolation. *Quick check: How does the S4 layer's treatment of history differ from a standard LSTM, and why does that help with missing values?*

- **Conditional Diffusion Models (DDPM)**: The core generative engine. You must understand the forward (noise adding) and reverse (denoising) processes and how conditions $U$ are injected. *Quick check: In the reverse process, how does the model use the estimated noise $\epsilon_\theta$ to recover $X_{t-1}$?*

- **Graph Convolutional Networks (GCN)**: The STC and NAST networks utilize GCNs to capture spatial dependencies defined by the adjacency matrix $A$. *Quick check: How does a GCN propagate information across the graph structure to update a node's feature representation?*

## Architecture Onboarding

- **Component map**: Input $(X^M, M)$ -> BiS4PI -> STC -> $U$. Concurrent path: Target $X^{ta}_0$ -> noise -> $X^{ta}_t$. Merge: NAST takes ($X^{ta}_t$, $U$) -> Predicted Noise $\epsilon_\theta$.

- **Critical path**: The input passes through BiS4PI for pre-imputation, then STC extracts global conditional information $U$, which is combined with the noisy target in NAST for denoising.

- **Design tradeoffs**: BiS4PI vs. Interpolation: Higher computational cost (trainable network) for lower error accumulation. NAST Gating: Increased parameter count (self + cross attention) vs. standard cross-attention, justified by ablation study showing MAE reduction.

- **Failure signatures**: High RMSE on high missing rates suggests BiS4PI is failing to capture temporal patterns. Training instability may indicate consistency loss weight issues. Blurred imputations suggest gating mechanism is ignoring local self-attention features.

- **First 3 experiments**:
  1. BiS4PI Validation: Run BiS4PI alone (forward/backward) on a validation set with held-out observed values to measure pre-imputation MAE vs. Linear Interpolation.
  2. Ablation on NAST: Compare full AdaSTI against a variant with standard cross-attention (removing gated self-attention) to verify the contribution of step-variant dependencies.
  3. Scaling Test: Evaluate performance on the Metro dataset with increasing missing rates (2% -> 75%) to confirm the claimed generalization ability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does AdaSTI maintain its performance advantage under Missing Not At Random (MNAR) mechanisms?
- Basis in paper: Page 2 states, "We focus on the case of Missing Completely At Random (MCAR)," explicitly excluding non-random missing patterns from the analysis.
- Why unresolved: Real-world sensor failures (e.g., malfunctions caused by extreme weather) often result in MNAR data, where the missingness depends on the unobserved values, challenging the model's conditional distribution assumptions.
- Evidence: Evaluation results on datasets specifically constructed with MNAR patterns (e.g., missing peaks in traffic flow) compared against the MCAR baselines.

### Open Question 2
- Question: What is the computational cost of the inference process relative to real-time application requirements?
- Basis in paper: Page 5 notes that to obtain the final imputation, "We repeat this process for k times and take the median," and Page 6 specifies "randomly sample... 100 times."
- Why unresolved: The iterative denoising process of diffusion models is inherently slow. While accuracy is improved, the latency of sampling 100 times per imputation may prohibit use in real-time monitoring systems.
- Evidence: Wall-clock time benchmarks comparing AdaSTI's inference latency against single-shot baselines (e.g., GRIN, BRITS) on the Metro dataset.

### Open Question 3
- Question: How does the reliance on a static adjacency matrix affect performance on datasets with dynamic spatial correlations?
- Basis in paper: Page 6 states, "The adjacency matrices of the datasets are calculate by thresholded Gaussian kernel," and the model inputs a fixed graph $G$.
- Why unresolved: Traffic and environmental patterns often exhibit dynamic spatial dependencies (e.g., changing traffic flow directions) that a fixed distance-based graph cannot capture, potentially limiting the STC network's ability to extract relevant conditional information.
- Evidence: Ablation studies replacing the static adjacency matrix with a dynamic or adaptive graph learning module within the Spatio-Temporal Conditionalizer (STC).

## Limitations
- The claim that bidirectional S4 pre-imputation prevents error propagation lacks direct quantitative validation against simpler baselines like linear interpolation.
- The gated attention mechanism's adaptive benefit is inferred from overall performance rather than step-by-step analysis of gating behavior.
- The computational cost of the proposed architecture relative to simpler methods is not discussed, which is important for practical deployment considerations.

## Confidence

- **High Confidence**: Empirical results showing AdaSTI outperforms existing methods across all tested datasets and missing rates. The architecture description and loss formulation are clearly specified.
- **Medium Confidence**: The three proposed mechanisms (BiS4PI pre-imputation, gated attention for adaptive dependencies, and consistency loss) are theoretically sound and their general contribution is supported by the results, but specific quantitative contributions from each mechanism are not clearly isolated.
- **Low Confidence**: The claim about error propagation prevention through BiS4PI is not directly validated, and the specific scenarios where the gated attention mechanism provides the most benefit are not characterized.

## Next Checks

1. **BiS4PI Contribution Validation**: Replace BiS4PI with linear interpolation in AdaSTI and measure the performance drop specifically on the Metro dataset at 25% missing rate to quantify the pre-imputation contribution.

2. **Gating Mechanism Analysis**: Track the distribution of gate values G during training and evaluate a variant where G is fixed at 0.5 (no gating) to determine if the adaptive behavior is actually being utilized.

3. **Consistency Loss Impact**: Train AdaSTI without the consistency loss (Î»=0) and compare performance to the full model to assess whether this training stabilizer actually improves final imputation quality or merely helps convergence.