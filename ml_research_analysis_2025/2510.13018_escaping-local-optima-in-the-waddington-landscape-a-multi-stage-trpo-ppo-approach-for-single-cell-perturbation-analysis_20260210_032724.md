---
ver: rpa2
title: 'Escaping Local Optima in the Waddington Landscape: A Multi-Stage TRPO-PPO
  Approach for Single-Cell Perturbation Analysis'
arxiv_id: '2510.13018'
source_url: https://arxiv.org/abs/2510.13018
tags:
- perturbation
- cell
- gene
- dataset
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of modeling cellular responses
  to perturbations in single-cell biology, where existing methods are prone to local
  optima in the nonconvex Waddington landscape. The authors propose a two-stage TRPO-PPO
  reinforcement learning algorithm.
---

# Escaping Local Optima in the Waddington Landscape: A Multi-Stage TRPO-PPO Approach for Single-Cell Perturbation Analysis

## Quick Facts
- **arXiv ID**: 2510.13018
- **Source URL**: https://arxiv.org/abs/2510.13018
- **Reference count**: 36
- **Primary result**: Two-stage TRPO-PPO reinforcement learning outperforms PPO alone on single-cell perturbation modeling with significantly lower MSE and higher Pearson correlations

## Executive Summary
This work addresses the challenge of modeling cellular responses to perturbations in single-cell biology, where existing methods are prone to local optima in the nonconvex Waddington landscape. The authors propose a two-stage TRPO-PPO reinforcement learning algorithm. First, they use TRPO to compute a curvature-aware initialization via natural gradient updates, scaled by a KL trust-region constraint. Then, PPO fine-tunes the policy using minibatch efficiency. This approach is evaluated on single-cell RNA-seq and ATAC-seq perturbation data, showing improved generalization compared to PPO alone. On the Replogle genome-wide dataset, TRPO-PPO achieved MSE of 0.0327 vs. 1.3798 for PPO, and higher Pearson correlations (0.9906 vs. 0.3379). Similar gains were observed on cytokine perturbation data. The method also demonstrated trajectory similarity between simulated and experimental perturbations using dynamic time warping and Wasserstein distance.

## Method Summary
The authors propose a two-stage reinforcement learning approach combining Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) to model cellular state transitions under perturbations. The method first uses TRPO to compute a curvature-aware initialization through natural gradient updates constrained by KL divergence, which helps escape local optima in the nonconvex Waddington landscape. This initialization is then fine-tuned using PPO with minibatch updates for improved sample efficiency. The approach is applied to single-cell RNA-seq and ATAC-seq perturbation datasets, including the Replogle genome-wide CRISPR perturbation data and cytokine perturbation experiments. Trajectory similarity between simulated and experimental perturbations is assessed using dynamic time warping and Wasserstein distance metrics.

## Key Results
- TRPO-PPO achieved MSE of 0.0327 vs. 1.3798 for PPO on the Replogle genome-wide dataset
- Pearson correlations improved from 0.3379 (PPO) to 0.9906 (TRPO-PPO)
- Similar performance gains observed on cytokine perturbation data
- Simulated trajectories showed high similarity to experimental perturbations using DTW and Wasserstein distance metrics

## Why This Works (Mechanism)
The two-stage approach works by first using TRPO's curvature-aware natural gradient updates to find a better initialization that escapes local optima in the nonconvex Waddington landscape. The KL trust-region constraint prevents large, destabilizing updates during this initialization phase. The subsequent PPO fine-tuning leverages minibatch efficiency to optimize the policy while maintaining proximity to the good initialization found by TRPO. This combination addresses the fundamental challenge of nonconvex optimization in cellular state space modeling, where poor initialization can lead to suboptimal solutions that fail to capture the true biological dynamics of cellular responses to perturbations.

## Foundational Learning
- **Waddington landscape**: Conceptual framework representing cellular differentiation as movement through a potential energy landscape; needed to understand the optimization challenge of navigating complex cellular state spaces
- **Natural gradients**: Optimization method that accounts for parameter space geometry using Fisher information matrix; quick check: verify gradient updates are scaled by inverse Fisher matrix
- **KL trust-region constraint**: Limits policy updates to prevent large deviations; quick check: monitor KL divergence between consecutive policy updates
- **Dynamic Time Warping (DTW)**: Algorithm for measuring similarity between temporal sequences that may vary in speed; quick check: compute DTW distance between simulated and experimental trajectories
- **Wasserstein distance**: Metric for comparing probability distributions based on optimal transport; quick check: calculate Wasserstein distance between simulated and experimental state distributions

## Architecture Onboarding

**Component Map**: Data preprocessing -> TRPO initialization -> PPO fine-tuning -> Trajectory evaluation

**Critical Path**: The TRPO initialization phase is critical as it determines the starting point for PPO optimization. Poor initialization leads to suboptimal local optima regardless of PPO performance.

**Design Tradeoffs**: TRPO provides better initialization through natural gradients but is computationally expensive and uses full batch updates. PPO is more sample-efficient with minibatches but can get stuck in local optima without good initialization.

**Failure Signatures**: If TRPO-PPO shows no improvement over PPO alone, likely causes include: insufficient TRPO training steps, overly restrictive KL constraints preventing effective initialization, or incompatible hyperparameters between the two stages.

**3 First Experiments**:
1. Train TRPO-PPO on synthetic Waddington landscape data with known ground truth to verify convergence properties
2. Compare TRPO initialization against random initialization and pretraining-based initialization on a small perturbation dataset
3. Perform ablation study removing the KL constraint from TRPO to assess its necessity

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Only compared against PPO baseline without benchmarking against established single-cell modeling approaches like scGen or dyngen
- Trajectory validation relies on similarity metrics (DTW, Wasserstein) without functional or biological interpretation of perturbation effects
- Potential overfitting to specific perturbation types or batch effects common in single-cell data not addressed
- The practical necessity of the two-stage approach versus alternative initialization strategies remains unclear

## Confidence
- TRPO-PPO outperforms PPO alone: Medium
- Biological trajectory similarity: Medium
- Curvature-aware initialization necessity: Low
- Generalization across perturbation types: Low

## Next Checks
1. Benchmark against established single-cell perturbation models (scGen, dyngen) on identical datasets
2. Perform ablation studies testing TRPO initialization versus random or pretraining-based initialization
3. Validate biological relevance through gene set enrichment analysis of simulated perturbation trajectories