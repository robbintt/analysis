---
ver: rpa2
title: A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand
  Prediction
arxiv_id: '2509.06976'
source_url: https://arxiv.org/abs/2509.06976
tags:
- traffic
- prediction
- data
- feature
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of traffic demand prediction in
  intelligent transportation systems by proposing a knowledge-guided cross-modal feature
  fusion model (KGCM) that integrates structured temporal traffic data with textual
  descriptions of human knowledge and experience. The core method idea involves constructing
  a prior knowledge dataset using large language models combined with manual revision,
  then learning multimodal features through local and global adaptive graph networks
  and cross-modal feature fusion mechanisms.
---

# A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction

## Quick Facts
- **arXiv ID:** 2509.06976
- **Source URL:** https://arxiv.org/abs/2509.06976
- **Reference count:** 40
- **Primary result:** KGCM outperforms state-of-the-art models with relative improvements of 0.58-12.66% in MAPE, 3.30-20.52% in MAE, and 3.72-23.7% in RMSE

## Executive Summary
This paper addresses traffic demand prediction in intelligent transportation systems by proposing a knowledge-guided cross-modal feature fusion model (KGCM) that integrates structured temporal traffic data with textual descriptions of human knowledge and experience. The core innovation involves constructing a prior knowledge dataset using large language models combined with manual revision, then learning multimodal features through local and global adaptive graph networks and cross-modal feature fusion mechanisms. A reasoning-based dynamic update strategy optimizes the graph model's parameters. Experiments on multiple traffic datasets demonstrate KGCM's superiority over state-of-the-art models, particularly in predicting peak-period demand and maintaining stability under abnormal road network conditions.

## Method Summary
KGCM employs a two-stage training pipeline to integrate structured traffic data with textual knowledge. Stage 1 uses Local Prompt Optimization (LPO) with BERT text encoding, guided cross-attention, and gated fusion to combine traffic features with local textual knowledge. Stage 2 incorporates Dynamic Graph Structure Optimization (DGSO) that learns adaptive feature relationships through a dynamic adjacency matrix and graph convolution. The model then applies Global Regional Common Prompt Guidance (RCPG) for conditional gated fusion with global semantic cues, followed by Adaptive Correlation Matrix-based Feature Weighting (ACMFW) and Structure-Aware Self-Attention (SSA). The model is trained end-to-end with a joint loss combining MSE and prompt regularization.

## Key Results
- KGCM achieves 0.58-12.66% relative improvement in MAPE over state-of-the-art models
- MAE improvements of 3.30-20.52% demonstrate enhanced prediction accuracy
- RMSE gains of 3.72-23.7% indicate better handling of prediction variance
- Model excels at peak-period demand prediction and shows robustness under abnormal road network conditions

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Knowledge Fusion
The model integrates textual knowledge with numerical traffic data using guided cross-attention and gated fusion mechanisms. The cross-attention aligns structured traffic data with textual descriptions of human experience, while the gated fusion dynamically balances their contribution. This works because textual knowledge contains meaningful signals about traffic patterns (peak hours, holidays) that are not explicitly present in raw numerical data. The mechanism could fail if textual knowledge is generic, noisy, or poorly aligned with numerical data, potentially degrading performance.

### Mechanism 2: Dynamic Graph Structure Optimization
KGCM uses adaptive graph structure learning to capture evolving feature dependencies over time. The model dynamically builds an adaptive feature relationship graph at each time step and layer, representing dependency strength between features (e.g., holiday's impact on order volume). This graph is used in graph convolution to update feature representations. The approach assumes traffic feature relationships are not static but change dynamically over time. If dynamic relationships are purely noise or the model lacks sufficient data to learn them, the dynamic graph may overfit to short-term fluctuations.

### Mechanism 3: Two-Stage Feature Learning with Global Priors
The two-stage training process first learns local multimodal features, then integrates global semantic cues. Stage 1 focuses on local characteristics through LPO, while Stage 2 uses RCPG to fuse local features with regional common prompt information and applies adaptive correlation matrix for feature weighting. This works because traffic patterns have both local characteristics and global/common semantic cues that, when combined, provide a more complete signal. The approach could fail if global cues are irrelevant to specific local regions or if information is lost during the first stage.

## Foundational Learning

- **Concept:** Cross-Attention Mechanism
  - **Why needed here:** Core of the Local Prior Knowledge Guidance module (Section 3.2) for aligning structured traffic data with textual knowledge
  - **Quick check question:** Can you explain how the query, key, and value are derived in the guided cross-attention mechanism described in Eq. 4?

- **Concept:** Graph Convolutional Networks (GCNs) and Adaptive Adjacency Matrices
  - **Why needed here:** Used in dynamic graph structure (Section 3.3) where adaptive adjacency matrix is learned and used in graph convolution (Eq. 12) to model feature dependencies
  - **Quick check question:** How does the relationship matrix $M_t^l$ in Eq. 12 differ from a standard, pre-defined adjacency matrix in a typical GCN?

- **Concept:** Informer Architecture (for long-sequence prediction)
  - **Why needed here:** Used as the final prediction backbone, a Transformer-based model designed for efficiency on long sequences
  - **Quick check question:** What is the main innovation of the Informer model compared to a standard Transformer for time-series forecasting?

## Architecture Onboarding

- **Component map:** Input Processing -> Local Cross-Modal Fusion -> Dynamic Graph Learning -> Global Knowledge Injection -> Final Sequence Prediction
- **Critical path:** Structured traffic data and textual knowledge enter LPO module for local fusion, pass through DGSO for dynamic graph learning, receive global knowledge injection via RCPG, undergo feature weighting with ACMFW, and finally predict using Structure-Aware Informer
- **Design tradeoffs:** The complex multi-stage architecture with multiple fusion and graph learning mechanisms increases accuracy but also risk of overfitting and training difficulty compared to simpler baselines. The model's success heavily depends on the quality and relevance of the LLM-generated knowledge corpus.
- **Failure signatures:** Generic LLM-generated text may not provide useful signals for cross-attention fusion; dynamic graph mechanism failing to learn meaningful relationships could cause the model to behave like a less effective static model; poor Stage 1 performance could cascade and affect global knowledge injection in Stage 2
- **First 3 experiments:** 1) Ablation by Component: Remove each of the five key components (LPO, DGSO, ACMFW, RCPG, SSA) to quantify individual contributions; 2) Knowledge Quality Test: Train with different knowledge sources (no text, LLM-only, manual-only, hybrid) to isolate knowledge source impact; 3) Baseline Comparison: Compare against cited baselines (STID, STAEformer, etc.) using specified metrics (MAE, RMSE, MAPE)

## Open Questions the Paper Calls Out
- **Generalization across diverse cities:** The authors explicitly state they will prioritize enhancing the model's generalization across diverse cities and scenarios, implying current performance may be localized to specific training contexts
- **Efficient inference mechanisms:** The paper notes intention to explore efficient inference mechanisms for real-time demand prediction and decision-making, acknowledging current offline accuracy improvements need computational efficiency investigation
- **Robustness to automated knowledge generation:** The reliance on manual revision in the knowledge generation process introduces scalability bottlenecks; it's unclear if performance depends heavily on manual refinement or if fully automated LLM pipelines would suffice

## Limitations
- Heavy reliance on manually curated textual knowledge creates reproducibility and scalability bottlenecks
- Performance improvements cannot be independently verified without access to exact hyperparameters and knowledge corpus
- Model complexity creates significant risk of overfitting given relatively small evaluation periods
- Paper does not adequately address performance with noisy or incomplete textual knowledge

## Confidence
- **High confidence:** The core architectural framework (BERT + cross-attention + gated fusion + dynamic graph convolution) is technically sound with standard experimental methodology
- **Medium confidence:** Performance improvements are plausible given innovative fusion approach, but lack of hyperparameter details prevents full verification
- **Low confidence:** Scalability and robustness claims regarding knowledge quality sensitivity are not empirically supported

## Next Checks
1. **Hyperparameter sensitivity analysis:** Systematically vary key hyperparameters (embedding dimensions, GCN layers, attention heads, learning rates) to identify whether reported performance is stable across reasonable ranges
2. **Knowledge source ablation:** Train and evaluate with four knowledge conditions: no textual knowledge, fully automated LLM generation without manual correction, manual knowledge only, and hybrid approach
3. **Cross-dataset generalization:** Evaluate on temporally distinct data (different month/year of NYC Taxi TLC) to assess whether learned knowledge representations generalize beyond specific training period or simply memorize temporal patterns