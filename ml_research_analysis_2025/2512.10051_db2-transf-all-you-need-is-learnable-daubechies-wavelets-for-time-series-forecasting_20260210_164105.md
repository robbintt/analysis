---
ver: rpa2
title: 'DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series
  Forecasting'
arxiv_id: '2512.10051'
source_url: https://arxiv.org/abs/2512.10051
tags:
- time
- series
- forecasting
- db2-transf
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DB2-TransF, a time series forecasting model
  that replaces the quadratic-complexity self-attention mechanism in Transformers
  with a learnable Daubechies wavelet coefficient layer. This wavelet-based approach
  efficiently captures multi-scale local and global temporal patterns, significantly
  reducing memory usage while maintaining or improving predictive accuracy.
---

# DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting

## Quick Facts
- **arXiv ID:** 2512.10051
- **Source URL:** https://arxiv.org/abs/2512.10051
- **Reference count:** 40
- **Primary result:** DB2-TransF replaces self-attention with learnable Daubechies wavelets, achieving comparable or superior performance to Transformers on 13 time series forecasting benchmarks while significantly reducing computational overhead.

## Executive Summary
DB2-TransF introduces a novel time series forecasting architecture that replaces the quadratic-complexity self-attention mechanism in Transformers with a learnable Daubechies wavelet coefficient layer. The model uses hierarchical wavelet decomposition to capture multi-scale temporal patterns while maintaining linear computational complexity. Experiments on 13 diverse time series benchmarks show DB2-TransF achieves state-of-the-art performance with substantial reductions in memory usage compared to conventional attention-based models.

## Method Summary
DB2-TransF employs a learnable Daubechies wavelet transform layer where the filter coefficients (α_k, β_k) are initialized near classical DB2 values but optimized via backpropagation. The architecture performs multi-scale decomposition across L levels, cascading approximation coefficients while retaining detail coefficients at each level. The final representation concatenates approximation and detail coefficients across all levels. This wavelet-based approach replaces self-attention, reducing computational complexity from O(T²) to O(T) while preserving representational capacity for capturing both local and global temporal patterns.

## Key Results
- DB2-TransF achieves comparable or superior performance to conventional Transformers on 13 time series forecasting benchmarks
- The model demonstrates significant reductions in GPU memory usage compared to iTransformer, Informer, and FEDformer across all tested datasets
- Optimal decomposition levels vary by dataset (L=1 for Electricity/PEMS04, L=6 for ETTm1), indicating the need for dataset-specific hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Learnable Wavelet Coefficient Adaptation
Making Daubechies wavelet filter coefficients learnable allows the model to adapt frequency decomposition to dataset-specific temporal patterns. The low-pass (α_k) and high-pass (β_k) filter coefficients are initialized near classical DB2 values but optimized via backpropagation, enabling the model to learn task-optimal frequency separations rather than relying on mathematically generic wavelets. The optimal frequency decomposition for forecasting differs from classical signal processing definitions; data-driven adaptation captures domain-specific patterns better.

### Mechanism 2: Multi-Scale Hierarchical Decomposition
Cascading wavelet decompositions across L levels captures both fine-grained local variations and coarse global trends simultaneously. At each level l, approximation coefficients A_l become input for level l+1, while detail coefficients D_l are retained. The final representation concatenates {A_L, D_1, ..., D_L}, providing a multi-resolution view of temporal dynamics. Time series forecasting benefits from explicit separation of trend (approximation) and noise/transient (detail) components at multiple resolutions.

### Mechanism 3: Linear Complexity via Wavelet Convolution Substitution
Replacing self-attention (O(T²) complexity) with wavelet convolution operations (O(T) per level) reduces memory/compute while preserving representational capacity. Self-attention computes pairwise token interactions, requiring T×T attention matrices. Wavelet convolution uses fixed-width filters applied across the sequence, avoiding quadratic scaling. Temporal dependencies in forecasting tasks can be captured through hierarchical frequency decomposition without explicit pairwise attention.

## Foundational Learning

- **Wavelet Transform Fundamentals**
  - Why needed here: Understanding how low-pass (approximation) and high-pass (detail) filters decompose signals into smooth trends vs. rapid variations is essential for interpreting MLDB block behavior
  - Quick check question: Given a signal [10, 12, 8, 6], what would a simple moving average (low-pass) vs. difference operation (high-pass) extract?

- **Multi-Head Mechanism Design**
  - Why needed here: DB2-TransF splits input across H heads, each potentially learning different wavelet decompositions—understanding why parallel heads help is critical for hyperparameter tuning
  - Quick check question: Why might different heads benefit from learning different wavelet filter coefficients rather than sharing them?

- **Residual Connections in Deep Architectures**
  - Why needed here: The architecture uses residual connections after MLDB blocks; understanding gradient flow through residuals is necessary for debugging training instability
  - Quick check question: If you removed residual connections and training diverged after epoch 5, what would that suggest about the MLDB block's gradient properties?

## Architecture Onboarding

- **Component map**: Input → Linear Projection → LayerNorm → [Multi-Head MLDB Block]×N → FFN → ForecastHead
- **Critical path**: 
  1. Correct initialization of α_k, β_k near classical DB2 values (Eq. 6-7)
  2. Proper handling of sequence length padding for even-length wavelet convolution
  3. Dimension alignment between MLDB output and residual connection (trimming if needed)

- **Design tradeoffs**:
  - L (decomposition levels): Higher L captures more scales but increases memory/time (Level 6 uses ~30-40% more GPU than Level 1)
  - H (heads): More heads enable diverse filter learning but increase parameters linearly
  - Dataset-dependent L selection required (no universal optimal L)

- **Failure signatures**:
  - Coefficients diverging far from DB2 initialization → may indicate unstable gradients or inappropriate learning rate
  - Performance degrading with longer lookback → check if padding/truncation is corrupting temporal alignment
  - Flat loss curves → may indicate filter initialization too rigid; consider small perturbation range

- **First 3 experiments**:
  1. Reproduce Table 10 ablation on Electricity varying L ∈ {1, 4, 6} to validate scale-level sensitivity
  2. Compare fixed vs. learnable wavelet coefficients to quantify the contribution of learnability
  3. Stress-test long-sequence scaling: increase lookback to 512+ and compare GPU/memory scaling against iTransformer baseline

## Open Questions the Paper Calls Out
- Can DB2-TransF generalize to non-time-series domains such as computer vision, natural language processing, and audio classification?
- Is there a theoretical or automated method to determine the optimal number of wavelet decomposition levels (L) for a given dataset?
- Do learnable Daubechies-2 (DB2) wavelets outperform other wavelet families (e.g., Haar, Coiflets) or generic learnable filters in this architecture?

## Limitations
- The paper lacks full architectural specification (H, N, d, dropout rates) and exact training hyperparameters, creating reproducibility barriers
- Linear-complexity claims are supported by GPU memory comparisons but lack rigorous asymptotic analysis
- Comparative analysis only benchmarks against Transformers and attention variants, not against closest wavelet-based alternatives (WaLRUS, PRISM)

## Confidence
- **High confidence**: The learnable wavelet coefficient mechanism is well-specified and mathematically sound; ablation results for decomposition level L are clearly presented
- **Medium confidence**: The multi-scale decomposition benefits are demonstrated empirically but could benefit from more rigorous ablation isolating approximation vs. detail coefficients
- **Low confidence**: Claims about "all you need" and superiority over all alternatives are overreaching given limited baseline comparisons and missing architectural specifications

## Next Checks
1. Replicate the fixed-vs-learnable wavelet ablation study to quantify the contribution of coefficient adaptivity beyond the stated initialization approach
2. Conduct head-sharing ablation (shared vs. independent filters across H heads) to determine whether multi-head diversity provides measurable benefit over single-head with more levels
3. Benchmark against WaLRUS and PRISM on identical datasets to establish whether the Transformer replacement or the wavelet decomposition itself drives performance gains