---
ver: rpa2
title: 'Following Route Instructions using Large Vision-Language Models: A Comparison
  between Low-level and Panoramic Action Spaces'
arxiv_id: '2508.02917'
source_url: https://arxiv.org/abs/2508.02917
tags:
- action
- panoramic
- low-level
- navigation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of off-the-shelf Large Vision-Language
  Models (LVLMs) for Vision-and-Language Navigation (VLN) tasks. Specifically, it
  fine-tunes Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset without architectural
  changes or simulator-based training.
---

# Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces

## Quick Facts
- **arXiv ID**: 2508.02917
- **Source URL**: https://arxiv.org/abs/2508.02917
- **Reference count**: 20
- **Primary result**: Off-the-shelf LVLMs can perform VLN via behavior cloning, achieving 41% SR on R2R test set, with panoramic action space outperforming low-level by 16% SR.

## Executive Summary
This paper explores using off-the-shelf Large Vision-Language Models (LVLMs) for Vision-and-Language Navigation (VLN) tasks, specifically fine-tuning Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset without architectural changes or simulator-based training. The study compares performance across low-level (atomic Move/Left/Right/Stop actions) and panoramic action spaces (selecting from candidate viewpoints). The panoramic setting achieves a 16% higher success rate, likely due to shorter action sequences and richer visual information. While the model demonstrates that general-purpose LVLMs can navigate following instructions, it still lags behind specialized VLN models, suggesting room for architectural improvements.

## Method Summary
The approach uses Qwen2.5-VL-3B-Instruct with frozen vision encoder and cross-modal projector, fine-tuning only the LLM via behavior cloning on the R2R dataset. The model receives structured multimodal prompts incorporating navigation history, current visual observation, and auxiliary information like step number and cumulative distance. Two action spaces are compared: low-level (Move/Left/Right/Stop requiring multiple atomic steps) and panoramic (direct candidate selection). Training uses negative log-likelihood loss on expert actions, with evaluation in the Matterport3D simulator using greedy decoding. Images are resized to half resolution and batched at size 1 on A100 80GB GPU.

## Key Results
- Panoramic action space achieves 16% higher success rate than low-level (41% vs 25% on val-unseen)
- Model reaches 41% success rate on R2R test set, demonstrating off-the-shelf LVLMs can perform VLN
- Offline accuracy and macro F1 scores reach 78% and 77% respectively on val-unseen
- Conservative Success Rate (CSR) shows panoramic setting maintains better path adherence (15% vs 3%)

## Why This Works (Mechanism)

### Mechanism 1: Panoramic Action Space Reduces Sequence Complexity
Panoramic action space outperforms low-level action space, likely because shorter action sequences reduce opportunities for error accumulation. Panoramic selection directly chooses navigable viewpoints in a single step, whereas low-level requires multiple atomic rotations and movements. Average path length drops from ~13 steps to ~6 steps, halving decision points where errors can compound.

### Mechanism 2: Frozen Vision Encoder with LLM-Only Fine-tuning
Freezing the vision encoder and cross-modal projector while fine-tuning only the LLM can yield functional navigation performance without architectural modification. Pre-trained visual features are projected into LLM token space; gradient updates optimize the LLM's ability to map structured multimodal prompts to discrete action tokens via behavior cloning.

### Mechanism 3: History-Aware Multimodal Prompting
Structured prompts incorporating navigation history enable the LVLM to maintain temporal context across multi-step decisions. Each prompt concatenates route instruction, step number, cumulative distance, previous step images, previous actions, and current visual observation into a unified multimodal context for next-action prediction.

## Foundational Learning

- **Concept**: Behavior Cloning vs. Reinforcement Learning
  - Why needed here: This paper uses pure behavior cloning (expert imitation) without RL or student forcing, which may explain the performance gap vs. state-of-the-art
  - Quick check question: Why might an agent trained via behavior cloning fail to recover from errors that an RL-trained agent could handle?

- **Concept**: Vision-Language Alignment Architectures
  - Why needed here: Qwen2.5-VL uses a fully autoregressive design where visual tokens are projected into LLM input space—understanding this explains what gets frozen vs. fine-tuned
  - Quick check question: What is the difference between cross-attention injection (Flamingo-style) and autoregressive concatenation for multimodal fusion?

- **Concept**: Conservative Success Rate (CSR)
  - Why needed here: This metric reveals whether the model stays on the expert path from start to finish—not just whether it eventually reaches the goal
  - Quick check question: Why might a model have reasonable SR but very low CSR, and what does that suggest about its error recovery?

## Architecture Onboarding

- **Component map**: Vision Encoder (frozen ViT) -> Cross-modal Projector (frozen) -> LLM Backbone (fine-tuned) -> Prompt Constructor -> Matterport3D Simulator (evaluation only)

- **Critical path**: 1) Preprocess R2R trajectories into action sequences (low-level: Move/Left/Right/Stop; panoramic: candidate indices) 2) Construct multimodal prompts per Figure 2 schema 3) Encode images through frozen vision encoder → frozen projector → concatenate with text tokens 4) Compute negative log-likelihood loss on expert action token 5) Accumulate gradients across all steps in episode, update LLM weights end-of-episode

- **Design tradeoffs**: Low-level action space is more hardware-agnostic and requires no pre-computed navigability, but has 2× longer sequences; panoramic action space has shorter sequences and richer context but assumes graph navigability info and panoramic camera hardware; behavior cloning requires no simulator dependency but may underperform RL approaches

- **Failure signatures**: High path length, low SR indicates agent failing to predict Stop appropriately; large gap between val-seen and val-unseen SR suggests overfitting to training environments; low CSR with moderate SR indicates agent deviates from expert path but occasionally recovers; disproportionate failure on longer trajectories suggests context window or error accumulation issues

- **First 3 experiments**: 1) Reproduce offline metrics on val-seen: Expect ~73% accuracy, CSR of 3% (low-level) vs 15% (panoramic) 2) Ablate "Automatically Turn Towards Node" in low-level setting: Compare default (25% SR) vs No-Adjust (29% SR on val-unseen) 3) Compare Qwen2-VL-2B vs Qwen2.5-VL-3B: Validate that larger model with more pre-training tokens improves navigation performance

## Open Questions the Paper Calls Out

### Open Question 1
How does architectural variation in off-the-shelf LVLMs beyond Qwen2.5-VL impact Vision-and-Language Navigation performance? The authors explicitly call for evaluating a "broader range of models" to identify which specific architectural choices lead to better results. This study limited its scope to the Qwen family of models.

### Open Question 2
To what extent does the field of view (FoV) and the inclusion of panoramic visual information—versus shorter action sequences—drive the performance gap between action spaces? The conclusion suggests "ablation studies that isolate the effect of including a panoramic view" and "systematically vary the field of view." The performance gap may stem from visual richness or simply the shorter decision horizon of panoramic settings.

### Open Question 3
Can state-of-the-art VLN models optimized for panoramic spaces (e.g., NaviLLM, NavGPT-2) be effectively adapted to low-level action spaces? The authors encourage "future work to further explore the low-level action space for more recent approaches" like NaviLLM and NavGPT-2. It is currently unknown if modern specialized architectures can operate effectively without the graph-based navigation used in panoramic setups.

## Limitations
- Relies on frozen general-purpose vision encoders that may lack navigation-specific spatial representations compared to specialized VLN architectures
- Performance gap (16% SR) between panoramic and low-level action spaces indicates off-the-shelf LVLMs still lag behind specialized models
- Only evaluates on R2R dataset in simulation, leaving real-world deployment and generalization untested

## Confidence

**High confidence**: The core finding that panoramic action space outperforms low-level action space by 16% SR is well-supported by ablation results (Table 3) and quantitative analysis of sequence lengths. The mechanism linking shorter sequences to reduced error accumulation is plausible given the average path length difference (13 vs 6 steps).

**Medium confidence**: The claim that frozen vision encoders with LLM-only fine-tuning can yield functional navigation performance is supported by the achieved 41% test SR, but this falls short of state-of-the-art (63.2% by NaviLLM). The paper acknowledges this gap but doesn't fully explore whether it stems from architectural limitations or training methodology.

**Low confidence**: The assertion that panoramic panoramas provide "richer visual information" contributing to performance gains is stated but not empirically validated. While shorter sequences are clearly quantified, the visual information quality difference remains speculative without direct comparison of feature representations.

## Next Checks

1. **Ablation on sequence length vs. visual information**: Create a controlled experiment comparing panoramic views (short sequences) against low-level sequences with panoramic input at each step, isolating whether the 16% gap stems from sequence length or visual richness.

2. **Error trajectory analysis**: For failed episodes, analyze the relationship between path deviation and sequence length by measuring how quickly agents diverge from the ground-truth path in low-level vs panoramic settings.

3. **Feature representation comparison**: Extract and compare frozen vision encoder features from successful vs failed navigation steps to determine if the model struggles with specific spatial concepts (room boundaries, doorways, landmarks) that general-purpose pre-training may not capture adequately.