---
ver: rpa2
title: 'Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in
  Vision-Language Models'
arxiv_id: '2509.12724'
source_url: https://arxiv.org/abs/2509.12724
tags:
- attack
- jailbreak
- visual
- vlms
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Defense2Attack is a jailbreak attack method for Vision-Language
  Models (VLMs) that reveals incorporating weak defenses into the attack pipeline
  significantly enhances jailbreak effectiveness. The method consists of three components:
  a visual optimizer embedding adversarial perturbations with affirmative semantics,
  a textual optimizer refining input using defensive-styled prompts, and an LLM-based
  red-team suffix generator.'
---

# Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2509.12724
- **Source URL:** https://arxiv.org/abs/2509.12724
- **Reference count:** 6
- **Key outcome:** Defense2Attack achieves ~80% ASR on open-source VLMs and ~50% on commercial ones in single-shot jailbreaks

## Executive Summary
Defense2Attack is a single-shot jailbreak attack method for Vision-Language Models that leverages weak defenses to enhance attack effectiveness. The approach combines a visual optimizer that embeds affirmative semantics into adversarial perturbations, a textual optimizer that frames harmful queries within defensive-styled prompts, and an LLM-based suffix generator that exploits generation weaknesses. Experiments show the method significantly outperforms state-of-the-art approaches on four VLMs and four safety benchmarks, with ablation studies confirming each component's contribution to overall effectiveness.

## Method Summary
Defense2Attack operates through three sequential components: (1) a visual optimizer using PGD to embed universal adversarial perturbations carrying affirmative semantic meaning into input images, (2) a textual optimizer that rewrites harmful prompts using defensive-styled templates to mask true intent, and (3) a suffix generator fine-tuned via PPO to append token sequences that maximize harmful response likelihood. The method is designed as a single-shot attack, requiring no iterative refinement based on target responses. Training involves generating perturbations on open-source models and transferring them to commercial targets, with the suffix generator trained on specific safety benchmarks using GPT-4o as a harmfulness judge.

## Key Results
- Achieves ~80% Attack Success Rate (ASR) on open-source VLMs (LLaVA, MiniGPT-4, InstructionBLIP)
- Achieves ~50% ASR on commercial VLMs (Gemini) through transferability
- Outperforms state-of-the-art methods that require multiple attempts while maintaining single-shot efficiency
- Ablation studies show significant performance drops when removing visual optimizer or textual defense components

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Positive Visual Bias
Injecting universal adversarial perturbations guided by affirmative semantics lowers refusal probability by biasing the visual encoder toward compliance. The visual optimizer maximizes log-likelihood of affirmative sentences within visual embedding space, shifting the model's prior from refusal to assistance before textual query processing.

### Mechanism 2: Defense-Styled Intent Obfuscation
Framing malicious queries within "defense-styled" contexts (e.g., instructions on how to reject harm) deceives safety classifiers by masking true intent. The textual optimizer wraps harmful requests in language mimicking safety prompts, creating a deceptive context that causes models to categorize inputs as meta-tasks about safety rather than harmful requests.

### Mechanism 3: Reinforced Suffix Exploitation
A reinforcement-learning agent discovers token suffixes that exploit local weaknesses in the VLM's generation policy to force harmful completions. The suffix generator (fine-tuned LLM) appends short token sequences trained via PPO to maximize harmfulness reward, effectively searching for "magic words" that disrupt refusal flow.

## Foundational Learning

- **Concept: Projected Gradient Descent (PGD) in Multimodal Space**
  - **Why needed here:** This is the engine of the visual optimizer for generating semantic-positive perturbations
  - **Quick check:** How does the $\ell_\infty$ constraint ($\epsilon$) in Equation (2) balance attack potency against visual imperceptibility?

- **Concept: Safety Alignment vs. Contextual Compliance**
  - **Why needed here:** The core vulnerability exploited is the conflict between being "helpful" and "harmless"
  - **Quick check:** Why does adding "defense-styled" text trick a model trained to be helpful into being harmful?

- **Concept: Reinforcement Learning from AI Feedback (RLAIF)**
  - **Why needed here:** The suffix generator is learned via an automated feedback loop (Judge -> Reward -> Update)
  - **Quick check:** In Algorithm 1, what serves as the "Environment" and what serves as the "Agent"?

## Architecture Onboarding

- **Component map:** Input image + harmful query → Visual optimizer (PGD with affirmative semantics) → Textual optimizer (defensive template wrapping) → Suffix generator (PPO fine-tuned) → Target VLM
- **Critical path:** Visual $x^*_v$ biases model toward "Yes" → Textual $x^*_t$ masks "Why" → Suffix $x_{suffix}$ forces generation. Removing visual optimizer causes significant ASR drop, indicating it's the primary driver.
- **Design tradeoffs:**
  - Visual optimizer requires white-box gradient access; transferability to black-box models causes ~30% ASR loss
  - Single-shot design trades adaptability for speed compared to iterative methods
- **Failure signatures:**
  - Image purification defenses destroy visual optimizer's semantic signal
  - Hard-coded refusal prefixes or low temperature settings prevent suffix exploitation
- **First 3 experiments:**
  1. Visual Transferability Test: Generate perturbations on LLaVA, apply to Gemini to measure ASR decay
  2. Ablation on Textual Defense: Remove "defense-styled" text to quantify intent obfuscation contribution
  3. Cross-Dataset Suffix Generalization: Test suffix generator from MM-SafetyBench on RedTeam-2K for universality

## Open Questions the Paper Calls Out

**Open Question 1:** Can the "defense-to-attack" principle extend to bypass active visual preprocessing defenses (e.g., diffusion purification) beyond textual safety cues? The paper evaluates transferability to commercial models but not against inference-time image filters that might destroy subtle adversarial perturbations.

**Open Question 2:** Is susceptibility to "defense-styled" prompts a fundamental artifact of current safety alignment techniques, or can it be eliminated via adversarial training without reducing helpfulness? The paper demonstrates attack success but doesn't explore defensive training specifically designed to recognize "trojan" safety cues.

**Open Question 3:** Does the universal nature of semantic-positive visual perturbation depend on specific vision-language connector architecture (Q-Former vs. Linear Projection) used in the target VLM? The paper describes "universal" perturbation but primarily tests transferability across similar architectures.

## Limitations

- **Limited generalization to unseen domains:** Suffix generator trained on specific safety benchmarks may overfit to their harm categories
- **Transferability assumptions under stress:** Visual optimizer relies on perturbation transferability that degrades by ~30% when moving from open-source to commercial models
- **Defense evasion vs. fundamental capability:** "Defense-styled" textual optimizer masks intent through superficial framing rather than addressing underlying safety mechanisms

## Confidence

- **High confidence (8/10):** Core claim that combining weak defenses enhances jailbreak effectiveness is well-supported by quantitative results across four VLMs and four benchmarks
- **Medium confidence (6/10):** Specific mechanisms are theoretically sound but rely on assumptions about VLM architectures not fully validated
- **Low confidence (4/10):** Claim of "superior performance in single shot" compared to state-of-the-art is difficult to verify without access to specific baselines and identical evaluation protocols

## Next Checks

**Check 1: Cross-Domain Generalization Test.** Evaluate Defense2Attack on held-out dataset with harm categories not in training benchmarks to measure generalization capability.

**Check 2: Defense Mechanism Dissection.** Implement target VLM with incremental safety layers and systematically remove each while measuring ASR changes to quantify which defensive component most effectively neutralizes each attack mechanism.

**Check 3: Visual Perturbation Transferability Analysis.** Generate perturbations on multiple source VLMs and measure ASR on various targets to analyze systematic transferability patterns across different model pairs.