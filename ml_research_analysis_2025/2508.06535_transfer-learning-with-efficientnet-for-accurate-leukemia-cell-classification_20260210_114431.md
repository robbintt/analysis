---
ver: rpa2
title: Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification
arxiv_id: '2508.06535'
source_url: https://arxiv.org/abs/2508.06535
tags:
- learning
- transfer
- classification
- data
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a transfer learning approach using EfficientNet
  variants for automated classification of Acute Lymphoblastic Leukemia (ALL) from
  peripheral blood smear images. To address class imbalance in the dataset (3,631
  Hematologic vs 7,644 ALL images), extensive data augmentation techniques were applied
  to create a balanced training set of 10,000 images per class.
---

# Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification

## Quick Facts
- **arXiv ID:** 2508.06535
- **Source URL:** https://arxiv.org/abs/2508.06535
- **Reference count:** 32
- **Primary result:** EfficientNet-B3 achieves 94.30% F1-score, 92.02% accuracy, and 94.79% AUC on C-NMC ALL classification

## Executive Summary
This study presents a transfer learning approach using EfficientNet variants for automated classification of Acute Lymphoblastic Leukemia (ALL) from peripheral blood smear images. To address class imbalance in the C-NMC dataset (3,631 Hematologic vs 7,644 ALL images), extensive data augmentation techniques were applied to create a balanced training set of 10,000 images per class. Multiple pretrained CNN architectures were evaluated, with EfficientNet-B3 achieving the best performance at 94.30% F1-score, 92.02% accuracy, and 94.79% AUC, outperforming previously reported methods in the C-NMC Challenge.

## Method Summary
The approach employs EfficientNet-B3 with ImageNet pretraining, fine-tuned for binary classification of ALL versus Hematologic cells. A 90/10 stratified split is created from the merged official training and validation sets of the C-NMC 2019 dataset. Data augmentation creates balanced training data (10,000 samples per class) using a nine-step pipeline including flips, rotations, color jitter, affine transforms, blur, and perspective distortion. The model is trained with Adam optimizer (learning rate 1e-4), cross-entropy loss, and early stopping based on validation macro F1-score with a patience of 15 epochs.

## Key Results
- EfficientNet-B3 achieves 94.30% F1-score, outperforming ResNet50 (93.27%), ResNet101 (92.74%), EfficientNet-B0 (93.68%), and EfficientNet-B1 (93.38%)
- Model achieves 92.02% accuracy and 94.79% AUC on the test set
- Data augmentation strategy successfully balances classes and prevents overfitting
- Results significantly improve upon previously reported methods in the C-NMC Challenge

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning from ImageNet Pretraining
Pretrained ImageNet weights provide useful feature initialization that accelerates convergence and improves final performance on blood smear classification. The backbone has already learned hierarchical visual features (edges → textures → shapes) from 1.2M+ natural images. Fine-tuning adapts these general representations to cell morphology patterns, requiring far fewer domain-specific samples than training from scratch. Core assumption: Low-level and mid-level visual features transfer meaningfully from natural images to histopathological blood cell images despite domain shift.

### Mechanism 2: Data Augmentation Balances Classes and Regularizes Learning
Extensive stochastic augmentation creates a balanced training set and forces the model to learn perturbation-invariant features, reducing overfitting. Sequential transforms (rotation, flipping, color jitter, affine, blur, perspective) generate synthetic diversity. Balancing both classes to 10,000 samples prevents majority-class bias, while the variability prevents memorization of dataset-specific artifacts. Core assumption: Applied transformations reflect realistic variations that could occur in clinical practice without introducing misleading artifacts.

### Mechanism 3: EfficientNet Compound Scaling Matches Model Capacity to Data Scale
EfficientNet-B3's balanced scaling of depth, width, and input resolution provides sufficient representational capacity for this dataset size without overfitting. EfficientNet scales all three dimensions jointly using optimized coefficients. B3 offers more capacity than B0/B1 but remains tractable for ~20K augmented samples. The architecture's efficiency means more parameters contribute meaningfully to feature extraction rather than redundancy. Core assumption: The optimal scaling coefficients discovered for ImageNet generalize to medical imaging; B3's resolution suits blood cell morphology.

## Foundational Learning

- **Concept: Transfer Learning and Fine-tuning Strategy**
  - Why needed here: The entire methodology depends on correctly leveraging pretrained weights—understanding what to freeze, what to fine-tune, and appropriate learning rates.
  - Quick check question: Why does the paper use lr=10⁻⁴ rather than a larger learning rate for fine-tuning?

- **Concept: Class Imbalance Effects on Training Dynamics**
  - Why needed here: Original dataset had ~2:1 imbalance (7,644 ALL vs. 3,631 Hem); understanding why this biases predictions is critical.
  - Quick check question: If you trained on the original imbalanced data and only reported accuracy, why might results appear better than they actually are?

- **Concept: Neural Architecture Scaling (Depth vs. Width vs. Resolution)**
  - Why needed here: Paper compares B0, B1, B3—understanding how EfficientNet scales differently than ResNet explains performance gaps.
  - Quick check question: What does "compound scaling" mean in EfficientNet, and how does it differ from simply making a network deeper?

## Architecture Onboarding

- **Component map:**
  RGB blood smear image → resize to 224×224 → augmentation pipeline → EfficientNet-B3 backbone → modified classifier head → 2 logits → softmax

- **Critical path:**
  Data loading → augmentation (training only) → backbone forward → classification head → loss → backprop through head and optionally backbone

- **Design tradeoffs:**
  - Augmentation strength: More diversity vs. risk of unrealistic samples
  - Model variant: B3 capacity vs. computational cost vs. overfitting risk
  - Early stopping patience: Preventing overfitting vs. potentially undertrained model

- **Failure signatures:**
  - High training F1, low validation F1: Overfitting—reduce augmentation intensity or model size
  - Poor Hem (minority) recall: Class imbalance still affecting learning—increase Hem augmentation or use class-weighted loss
  - ResNet outperforms EfficientNet: Check preprocessing mismatch (ImageNet normalization missing)
  - Loss diverges: Learning rate too high for fine-tuning regime

- **First 3 experiments:**
  1. Reproduce paper results with EfficientNet-B3 and full augmentation pipeline; target F1 ~94% on test split.
  2. Ablation: Train without augmentation on original imbalanced data to quantify augmentation contribution.
  3. Architecture sweep: Compare B0, B1, B3, B5 to verify B3 is optimal and identify overfitting in larger variants.

## Open Questions the Paper Calls Out
- **Open Question 1:** Does the EfficientNet-B3 model maintain high diagnostic performance when applied to multi-center datasets with diverse imaging protocols and staining variations?
- **Open Question 2:** Can integrating multimodal patient data with EfficientNet image features improve classification accuracy beyond the reported 94.30% F1-score?
- **Open Question 3:** What specific interpretability methods are required to transition the EfficientNet-B3 model from a research tool to a trusted real-time clinical decision support system?

## Limitations
- Study limited by relatively homogeneous dataset which may affect generalizability to diverse clinical settings
- Current methodology is strictly vision-based, utilizing only peripheral blood smear images while ignoring potential complementary clinical or genomic features
- Model's decision logic and interpretability for clinical adoption not assessed

## Confidence
- **High Confidence:** Transfer learning efficacy (well-established mechanism with strong evidence from ImageNet pretraining and medical imaging literature)
- **Medium Confidence:** EfficientNet-B3 superiority (performance ranking clear but comparison set limited; B3 optimal scaling not proven definitively)
- **Medium Confidence:** Data augmentation contribution (mechanistic rationale solid, but class imbalance remains a challenge and augmentation impact not fully isolated)

## Next Checks
1. Reproduce core results: Train EfficientNet-B3 with full augmentation pipeline; target macro F1 ~94% on test split.
2. Ablation study: Train without augmentation on original imbalanced data to quantify augmentation contribution to performance gains.
3. Architecture sweep: Compare EfficientNet-B0, B1, B3, B5 to verify B3 is optimal and identify potential overfitting in larger variants.