---
ver: rpa2
title: 'Learning to Look: Cognitive Attention Alignment with Vision-Language Models'
arxiv_id: '2509.21247'
source_url: https://arxiv.org/abs/2509.21247
tags:
- attention
- decoymnist
- maps
- saliency
- colormnist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of shortcut learning in CNNs,
  where models rely on superficial correlations instead of robust, generalizable features.
  The proposed solution uses vision-language models to automatically generate semantic
  attention maps via natural language prompts, guiding CNN attention during training
  through an auxiliary loss.
---

# Learning to Look: Cognitive Attention Alignment with Vision-Language Models

## Quick Facts
- arXiv ID: 2509.21247
- Source URL: https://arxiv.org/abs/2509.21247
- Authors: Ryan L. Yang; Dipkamal Bhusal; Nidhi Rastogi
- Reference count: 15
- Primary result: VLM-guided attention alignment reduces shortcut learning, achieving 64.88% accuracy on ColoredMNIST and 96.19% on DecoyMNIST without manual annotations.

## Executive Summary
This paper addresses the problem of shortcut learning in CNNs, where models rely on superficial correlations instead of robust, generalizable features. The proposed solution uses vision-language models to automatically generate semantic attention maps via natural language prompts, guiding CNN attention during training through an auxiliary loss. This approach eliminates the need for manual annotations and expert supervision. Experiments on ColoredMNIST and DecoyMNIST datasets show state-of-the-art performance on ColoredMNIST (64.88% accuracy) and competitive results on DecoyMNIST (96.19% accuracy) compared to annotation-heavy baselines. The method reduces shortcut reliance and produces attention maps that better align with human intuition.

## Method Summary
The method aligns CNN attention with semantic pseudo-maps generated by a Vision-Language Model (VLM) using an auxiliary Kullback-Leibler (KL) divergence loss. The framework employs a two-phase training schedule: first optimizing only attention alignment to establish a stable prior, then jointly optimizing classification and attention with ramped-up alignment weight. VLM-generated attention maps are post-processed with morphological operations to inject inductive biases, such as prioritizing shape contours over color. This approach eliminates manual annotation requirements while effectively reducing shortcut learning on biased datasets.

## Key Results
- Achieves 64.88% accuracy on ColoredMNIST, surpassing annotation-heavy baselines
- Competitive 96.19% accuracy on DecoyMNIST dataset
- Demonstrates reduced reliance on spurious correlations through attention alignment
- Produces attention maps that better align with human intuition compared to standard training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning CNN attention with VLM-generated semantic maps reduces shortcut reliance by shifting focus to task-relevant features.
- **Core assumption:** VLM attention accurately identifies task-relevant regions and ignores spurious features based on text prompts.
- **Evidence anchors:** KL divergence loss between CAM and WeCLIP+ attention maps; similar approaches in AutoFocus-IL.
- **Break condition:** If VLM attention is biased, incorrect supervision propagates to degrade performance.

### Mechanism 2
- **Claim:** Two-phase training establishes attention prior before joint optimization.
- **Core assumption:** Attention prior learned in Phase 1 persists through Phase 2 optimization.
- **Evidence anchors:** Phase 1 optimizes only attention alignment; Phase 2 resets optimizer and ramps up alignment weight.
- **Break condition:** Insufficient Phase 1 duration or excessive duration causing overfitting.

### Mechanism 3
- **Claim:** Morphological preprocessing injects specific inductive biases into attention maps.
- **Core assumption:** Task-relevant information is more strongly encoded in object boundaries than interior values.
- **Evidence anchors:** Edge detection and dilation applied to ColorMNIST maps to focus on digit contours.
- **Break condition:** Aggressive preprocessing erodes essential signal and harms performance.

## Foundational Learning

- **Concept: Class Activation Mapping (CAM)**
  - **Why needed here:** CAM extracts CNN's internal attention map for alignment with VLM maps.
  - **Quick check question:** Given feature map and fully-connected layer weights, can you compute spatial heatmap showing regions influencing class prediction?

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** Measures difference between model's attention distribution and VLM's pseudo-mask distribution.
  - **Quick check question:** Why is KL divergence asymmetric and what behavior does minimizing KL(S_model || M_VLM) encourage?

- **Concept: Shortcut Learning in CNNs**
  - **Why needed here:** Core problem being solved is models exploiting spurious correlations instead of robust features.
  - **Quick check question:** On ColoredMNIST, why does 100% training accuracy using color alone fail completely on test set with reversed color-to-class mapping?

## Architecture Onboarding

- **Component map:** Data Loader -> CNN Backbone (LeNet) -> Saliency Extractor (CAM) -> Pre-computed Pseudo-Masks (WeCLIP+) -> Loss Aggregator (L_CE + λL_attn) -> Training Controller (two-phase schedule)

- **Critical path:**
  1. Pre-compute WeCLIP+ attention maps using class prompts
  2. (Optional) Apply morphological preprocessing based on task needs
  3. Phase 1: Train only with L_attn for E_attn epochs
  4. Reset optimizer at E_attn
  5. Phase 2: Train with L = L_CE + λL_attn, ramping up λ

- **Design tradeoffs:**
  - Memory vs. Computation: Pre-computing saves VLM inference but increases storage
  - Annotation Cost vs. Teacher Bias: Avoids human annotation but introduces VLM biases
  - Preprocessing Complexity: Adds hyperparameter but can significantly improve performance

- **Failure signatures:**
  - Model doesn't converge in Phase 1: Check pseudo-mask quality and CAM normalization
  - No improvement over baseline: VLM may attend to shortcut features; inspect M_VL qualitatively
  - Phase 2 performance collapse: Initial learning rate after reset too high or λ ramp too aggressive

- **First 3 experiments:**
  1. Baseline Establishment: Train CNN on ColoredMNIST/DecoyMNIST using only L_CE; visualize CAM to confirm shortcut learning
  2. Hyperparameter Search: Grid search over λ and E_attn using validation set; replicate Fig. 2 heatmaps
  3. Ablation on Preprocessing: Compare performance using raw VLM maps, dilated maps, and edge-detected maps on ColoredMNIST

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the framework maintain effectiveness on complex, high-dimensional benchmarks beyond simple datasets like MNIST?
  - Basis: Current evaluation restricted to simple datasets; future work will investigate complex benchmarks
  - Why unresolved: Experiments conducted solely on ColoredMNIST and DecoyMNIST
  - What evidence would resolve it: Empirical results on high-dimensional datasets (ImageNet, CIFAR) with spurious correlations

- **Open Question 2:** Can strategies like multi-teacher supervision or debiasing mitigate biases inherited from vision-language teacher model?
  - Basis: Reliance on single VLM may introduce its own biases; suggests investigating mitigation strategies
  - Why unresolved: Current study uses single VLM (WeCLIP+) assuming negligible biases compared to target task shortcuts
  - What evidence would resolve it: Experiments comparing single vs. ensemble VLM teachers or debiased VLMs

- **Open Question 3:** Can on-the-fly generation of attention maps replace precomputed storage without compromising efficiency or performance?
  - Basis: Precomputed maps are memory-intensive; proposes on-the-fly generation techniques as future direction
  - Why unresolved: Current implementation requires storing pseudo-masks for entire training set
  - What evidence would resolve it: Comparison of memory footprint and wall-clock time between precomputed and dynamic generation

## Limitations

- The method relies heavily on quality of VLM-generated attention maps as pseudo-supervision
- Two-phase training schedule introduces sensitive hyperparameters requiring careful tuning
- Memory overhead of pre-computing and storing pseudo-masks for large datasets is significant
- Limited analysis of hyperparameter sensitivity across diverse datasets and model architectures

## Confidence

- **High confidence** in core claim that VLM-guided attention alignment reduces shortcut reliance
- **Medium confidence** in specific implementation details due to incomplete specification
- **Low confidence** in generalizability of two-phase schedule without extensive hyperparameter analysis

## Next Checks

1. **Qualitative Inspection of VLM Maps:** Inspect WeCLIP+ attention maps for foreground/background prompts on ColoredMNIST to verify focus on digit shape rather than color patches

2. **Hyperparameter Ablation Study:** Systematically vary λ (alignment weight) and E_attn (attention-only epochs) to identify sensitivity of performance to critical hyperparameters

3. **Cross-Dataset Generalization:** Apply method to new shortcut dataset using ColoredMNIST-tuned hyperparameters to test approach generalizability or need for per-dataset tuning