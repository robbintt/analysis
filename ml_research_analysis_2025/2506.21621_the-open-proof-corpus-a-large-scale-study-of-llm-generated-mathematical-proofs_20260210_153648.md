---
ver: rpa2
title: 'The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs'
arxiv_id: '2506.21621'
source_url: https://arxiv.org/abs/2506.21621
tags:
- proof
- should
- problem
- proofs
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Open Proof Corpus (OPC) is a large-scale, human-evaluated dataset
  of over 5,000 LLM-generated mathematical proofs from high-level competitions like
  IMO and USAMO. Using this dataset, the authors analyze the gap between natural language
  and formal proof generation, showing that natural language methods (e.g., Gemini-2.5-Pro)
  outperform formal approaches by a factor of four on the PutnamBench.
---

# The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs

## Quick Facts
- **arXiv ID**: 2506.21621
- **Source URL**: https://arxiv.org/abs/2506.21621
- **Reference count**: 40
- **Primary result**: Open Proof Corpus (OPC) contains 5,000+ human-evaluated LLM-generated proofs; natural language methods outperform formal approaches by 4x on PutnamBench

## Executive Summary
The Open Proof Corpus (OPC) is a large-scale dataset of over 5,000 LLM-generated mathematical proofs from high-level competitions like IMO and USAMO, rigorously human-evaluated for correctness. The study reveals a significant performance gap between natural language and formal proof generation methods, with natural language approaches like Gemini-2.5-Pro outperforming formal methods by a factor of four on the PutnamBench benchmark. Additionally, the research demonstrates that final-answer accuracy does not reliably predict proof correctness, as models like O3 can lose up to 30% in correctness despite strong final answers. The study also validates best-of-n sampling strategies, particularly ranking methods, as effective approaches for improving proof quality, and showcases an 8B-parameter model fine-tuned on OPC achieving 88.1% accuracy in proof evaluation.

## Method Summary
The authors constructed the Open Proof Corpus by generating mathematical proofs using various LLM approaches for competition-level problems from IMO, USAMO, and Putnam contests. These proofs were then evaluated by human experts for correctness, creating a gold-standard dataset. The study compared natural language proof generation methods against formal approaches, analyzed the relationship between final answer accuracy and proof correctness, and evaluated best-of-n sampling strategies for proof generation. Additionally, they fine-tuned an 8B-parameter model on the OPC dataset and assessed its proof evaluation capabilities against established benchmarks.

## Key Results
- Natural language methods (e.g., Gemini-2.5-Pro) outperform formal approaches by a factor of four on the PutnamBench
- Final-answer accuracy does not reliably predict proof correctness, with models like O3 losing up to 30% in correctness despite strong final answers
- Best-of-n sampling strategies significantly improve proof quality, with ranking methods achieving the highest gains
- An 8B-parameter model fine-tuned on OPC achieves 88.1% accuracy in proof evaluation, close to the best-performing model, GPT-5

## Why This Works (Mechanism)
The success of natural language methods over formal approaches stems from the inherent flexibility and expressiveness of natural mathematical language, which better captures the intuitive reasoning processes used in human proofs. The disconnect between final answer accuracy and proof correctness reveals that models can arrive at correct conclusions through flawed reasoning, highlighting the importance of evaluating the entire proof structure rather than just the conclusion. Best-of-n sampling strategies work by leveraging the diversity of generated proofs, allowing selection of higher-quality outputs through ranking mechanisms that capture subtle correctness indicators.

## Foundational Learning
**Mathematical Proof Structure** - Understanding the logical flow and dependencies in mathematical arguments is essential for evaluating proof quality. *Why needed*: Forms the basis for assessing whether a proof is mathematically valid. *Quick check*: Can identify the main claim, supporting lemmas, and logical progression in a sample proof.

**Formal vs. Natural Language Mathematics** - Knowledge of both formal proof systems (like Lean or Isabelle) and natural mathematical language. *Why needed*: Critical for understanding the performance gap between different proof generation approaches. *Quick check*: Can translate a simple proof between formal and natural language representations.

**Evaluation Metrics for Proofs** - Understanding various metrics beyond simple accuracy, including logical completeness, rigor, and pedagogical clarity. *Why needed*: Enables proper assessment of proof quality beyond just final answers. *Quick check*: Can distinguish between a correct but incomplete proof and a fully rigorous one.

## Architecture Onboarding

**Component Map**: Problem Generator -> Proof Generator -> Human Evaluator -> Quality Assessment -> Fine-tuning Pipeline

**Critical Path**: Competition Problem → LLM-based Proof Generation → Human Expert Evaluation → OPC Dataset → Model Fine-tuning → Proof Evaluation

**Design Tradeoffs**: Natural language approaches prioritize expressiveness and accessibility over formal rigor, while formal methods ensure mathematical correctness but may sacrifice readability. The choice between them depends on whether the goal is mathematical verification or pedagogical communication.

**Failure Signatures**: 
- Incorrect proofs with correct final answers (logical gaps in reasoning)
- Over-reliance on pattern matching without genuine understanding
- Inability to handle novel problem structures despite strong performance on standard problems
- Degradation in proof quality when scaling to more complex mathematical domains

**3 First Experiments**:
1. Generate proofs for a small set of competition problems using both natural language and formal methods, then compare human evaluation scores
2. Test the correlation between final answer accuracy and full proof correctness across different model families
3. Implement and evaluate simple best-of-n sampling strategies (random selection, quality ranking) on proof generation outputs

## Open Questions the Paper Calls Out
The paper identifies several key open questions: whether the observed performance gap between natural language and formal methods persists across broader mathematical domains beyond competition problems; how to develop reliable automated evaluation metrics that can scale beyond human expert review; and what optimal training protocols and formalization strategies might improve formal method performance to close the current gap.

## Limitations
- Evaluation methodology may contain subjectivity despite human expert review
- Dataset constrained to high-level competition problems, limiting generalizability to broader mathematical domains
- Performance gap between natural language and formal methods may be influenced by specific evaluation metrics and prompt engineering rather than inherent limitations
- Best-of-n sampling improvements may not scale effectively to more complex proofs or different problem domains

## Confidence
**Natural Language vs. Formal Methods**: Medium confidence - The 4x performance gap is well-documented within the dataset, but the underlying causes and generalizability remain uncertain.

**Proof Correctness vs. Final Answer Accuracy**: High confidence - The systematic observation that models like O3 lose 30% correctness despite strong final answers is robust across multiple evaluations.

**Best-of-n Sampling Effectiveness**: Medium confidence - While the ranking methods show strong gains, the optimal sampling strategy may depend heavily on problem type and model architecture.

## Next Checks
1. Cross-domain validation: Test the OPC-trained 8B model on proofs from undergraduate-level mathematics and research-level conjectures to assess generalizability beyond competition problems.

2. Automated evaluation reliability: Compare human expert evaluations against automated metrics (BLEU, ROUGE, and proof-specific metrics) to determine if reliable automated assessment is feasible for large-scale deployment.

3. Formal method optimization: Systematically vary proof formalization strategies and training protocols for Lean/Isabelle-based approaches to determine whether the natural language advantage persists with optimized formal methods.