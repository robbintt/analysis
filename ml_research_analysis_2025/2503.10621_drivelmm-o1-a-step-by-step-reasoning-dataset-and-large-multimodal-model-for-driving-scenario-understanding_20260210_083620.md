---
ver: rpa2
title: 'DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for
  Driving Scenario Understanding'
arxiv_id: '2503.10621'
source_url: https://arxiv.org/abs/2503.10621
tags:
- reasoning
- driving
- autonomous
- vehicle
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DriveLMM-o1 introduces a novel dataset and benchmark for step-by-step
  reasoning in autonomous driving, featuring over 18k VQA examples across perception,
  prediction, and planning tasks, enriched with multiview images and LiDAR point clouds.
  The proposed large multimodal model, fine-tuned on this reasoning dataset, demonstrates
  improved performance by generating structured, interpretable reasoning chains before
  arriving at final decisions.
---

# DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding

## Quick Facts
- **arXiv ID**: 2503.10621
- **Source URL**: https://arxiv.org/abs/2503.10621
- **Reference count**: 32
- **Primary result**: 7.49% gain in final answer accuracy and 3.62% improvement in reasoning score over prior best open-source model on autonomous driving VQA tasks.

## Executive Summary
DriveLMM-o1 introduces a novel dataset and benchmark for step-by-step reasoning in autonomous driving visual question answering. The dataset contains over 18,000 Q&A examples spanning perception, prediction, and planning tasks, enriched with multiview images and LiDAR point clouds from NuScenes. The authors propose a large multimodal model fine-tuned on this reasoning dataset, demonstrating improved performance by generating structured, interpretable reasoning chains before final decisions. Evaluated against multiple open and closed-source models, DriveLMM-o1 achieves significant improvements in both final answer accuracy and reasoning quality.

## Method Summary
The approach centers on creating a step-by-step visual reasoning dataset for autonomous driving VQA tasks. The dataset comprises 18,507 Q&A pairs (train) from 1,962 scenes and 4,633 QAs (test) from 539 frames, built on NuScenes with 6-view images plus LiDAR. Each question-answer pair includes manually verified reasoning chains with approximately 55,000 reasoning steps total. The proposed model fine-tunes InternVL2.5-8B using LoRA (rank 16) on attention layers while freezing vision encoder and LLM layers, resulting in only 0.49% trainable parameters. Training runs for one epoch on 4× NVIDIA A6000 GPUs with dynamic image patching for stitched multiview images. Evaluation uses GPT-4o to score reasoning quality across 12 metrics plus final answer accuracy for MCQs.

## Key Results
- 7.49% gain in final answer accuracy compared to previous best open-source model
- 3.62% improvement in reasoning score (average of 12 metrics) over prior work
- Demonstrates enhanced reasoning capabilities and robustness in complex driving scenarios through structured reasoning chains

## Why This Works (Mechanism)
DriveLMM-o1 works by explicitly training models to generate interpretable reasoning chains before making final decisions, mimicking human reasoning processes. The step-by-step approach breaks down complex driving scenario understanding into manageable sub-tasks, allowing the model to build contextual understanding progressively. By fine-tuning on a dataset specifically constructed with reasoning chains, the model learns to decompose problems systematically rather than jumping to conclusions. The use of LoRA enables efficient adaptation of large models while preserving pretrained capabilities. The multiview and multimodal input integration provides comprehensive scene understanding necessary for reliable autonomous driving decisions.

## Foundational Learning
- **Visual Question Answering in Autonomous Driving**: Understanding how to answer questions about driving scenes using visual and sensor data is fundamental for building intelligent vehicles. Quick check: Verify the model can correctly answer basic perception questions about object locations and road conditions.
- **Step-by-Step Reasoning**: Breaking complex reasoning into sequential steps improves interpretability and accuracy. Quick check: Ensure reasoning chains follow logical progression from observation to conclusion.
- **Multiview Image Stitching**: Combining multiple camera views into a unified representation provides complete scene coverage. Quick check: Validate that stitched images preserve spatial relationships and don't introduce distortions.
- **LoRA Fine-tuning**: Parameter-efficient adaptation of large models through low-rank matrix decomposition. Quick check: Confirm that only attention layer parameters are being updated during training.
- **Reasoning Score Evaluation**: Using GPT-4o to evaluate reasoning quality across multiple dimensions provides comprehensive assessment beyond simple accuracy. Quick check: Test evaluation pipeline on sample outputs to verify scoring consistency.
- **Multimodal Fusion**: Integrating image and LiDAR data provides complementary information for scene understanding. Quick check: Verify that both modalities contribute meaningfully to final predictions.

## Architecture Onboarding
- **Component Map**: NuScenes dataset -> Data preprocessing (stitching + patching) -> InternVL2.5-8B with LoRA -> GPT-4o evaluation
- **Critical Path**: Data preparation → Model fine-tuning → Evaluation pipeline
- **Design Tradeoffs**: Fine-tuning only attention layers with LoRA trades parameter efficiency for potentially limited adaptation of vision encoder capabilities. The choice of 1 epoch balances training efficiency against potential underfitting.
- **Failure Signatures**: Poor reasoning scores indicate issues with prompt format or evaluation setup; low accuracy gains suggest reasoning format misalignment with training data; GPU OOM points to stitching configuration problems.
- **First Experiments**:
  1. Test GPT-4o evaluation pipeline on 10 sample questions to verify reasoning score calculation matches reported metrics
  2. Implement multiview stitching with different layouts and evaluate impact on model performance
  3. Perform ablation study comparing full LoRA fine-tuning vs. freezing vision encoder only

## Open Questions the Paper Calls Out
None

## Limitations
- The paper provides limited implementation details for key components, particularly the exact GPT-4o evaluation prompt structure and scoring rubric, which could lead to inconsistencies in reproducing reported metrics.
- Multiview stitching configuration is underspecified, leaving uncertainty about the exact layout and preprocessing pipeline that could affect model performance.
- The dataset construction process lacks detailed documentation on annotation consistency and inter-rater reliability measures.

## Confidence
- **High Confidence**: Core contribution of creating a step-by-step reasoning dataset with 18k+ examples and general architecture of fine-tuning InternVL2.5-8B with LoRA
- **Medium Confidence**: Claimed performance improvements based on evaluation protocol described, but exact implementation of GPT-4o scoring introduces moderate uncertainty
- **Low Confidence**: Precise multiview image stitching configuration and complete hyperparameter settings remain unclear

## Next Checks
1. Test GPT-4o evaluation pipeline on 10 sample questions to verify that the reasoning score calculation matches reported metrics, adjusting prompt format if needed.
2. Implement multiview stitching with different layouts (e.g., 2×3 grid vs. 3×2 grid) and evaluate impact on downstream model performance to determine optimal configuration.
3. Perform ablation study comparing full LoRA fine-tuning vs. freezing only vision encoder (as stated) vs. freezing both vision encoder and LLM layers to validate the claimed 0.49% trainable parameter count.