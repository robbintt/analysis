---
ver: rpa2
title: 'Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging'
arxiv_id: '2505.22934'
source_url: https://arxiv.org/abs/2505.22934
tags:
- merging
- performance
- task
- arxiv
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of performance degradation when
  merging multiple LoRA-based fine-tuned language models into a single multi-task
  model. The core issue stems from parameter-data interference where learned LoRA
  subspaces for different tasks adversely affect each other during merging.
---

# Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging

## Quick Facts
- arXiv ID: 2505.22934
- Source URL: https://arxiv.org/abs/2505.22934
- Authors: Haobo Zhang; Jiayu Zhou
- Reference count: 17
- Primary result: OSRM achieves 3.8-7.9% average improvements across GLUE tasks while preserving single-task accuracy

## Executive Summary
This paper addresses performance degradation when merging multiple LoRA-based fine-tuned language models into a single multi-task model. The core issue stems from parameter-data interference where learned LoRA subspaces for different tasks adversely affect each other during merging. The proposed method, OSRM (Orthogonal Subspaces for Robust model Merging), constrains the LoRA subspace prior to fine-tuning by finding an analytical solution that minimizes interference between tasks. Extensive experiments on eight GLUE datasets using three widely used language models demonstrate that OSRM consistently outperforms existing merging baselines while preserving single-task accuracy.

## Method Summary
OSRM addresses LoRA interference by constraining the A matrix in LoRA to have orthonormal rows corresponding to directions with minimal variance in out-of-task data distributions. The method finds an analytical solution via eigendecomposition of the sample covariance matrix, extracting eigenvectors corresponding to smallest eigenvalues. Rather than freezing this initialization, OSRM allows A to update during fine-tuning while preserving the initialization's interference-reducing properties. The approach integrates with existing merging algorithms and shows robustness to hyperparameters like scaling coefficients and sample sizes.

## Key Results
- OSRM achieves 3.8% to 7.9% average improvements across different merging techniques on GLUE benchmark
- Single-task accuracy preserved with <1% performance gap on average
- Greater robustness to merging hyperparameters (scaling coefficients, sample sizes) compared to baselines
- Performance advantages increase with number of merged tasks (2-8 tasks tested)
- Changes to initialized A matrix remain marginal (≤14%) after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining the LoRA A matrix to lie in subspaces orthogonal to out-of-task data distributions reduces harmful output shifts when models are merged.
- **Mechanism:** OSRM initializes A₂ to minimize ‖A₂H₁ᵀ‖_F, where H₁ contains latent features from task T₁. This places A₂ in directions where H₁ has minimal variance, reducing the interference term B₂A₂H₁ᵀ that would otherwise perturb task T₁ outputs when models merge.
- **Core assumption:** The row space of A can be constrained independently of B without severely degrading task performance; B can learn appropriate scaling to compensate.
- **Evidence anchors:**
  - [abstract] "constrains the LoRA subspace before fine-tuning by minimizing the interaction between model parameters and out-of-task data distributions"
  - [section 4.1] "B₂A₂h₁ can be viewed as a 'perturbation' induced by the knowledge learned for task T₂"
  - [corpus] Related work on LoRA orthogonality (LoRI, OPLoRA) supports interference reduction through orthogonal constraints, though corpus papers focus on different mechanisms (reduced interference training, orthogonal projection).
- **Break condition:** If in-task and out-of-task distributions heavily overlap in feature space, forcing orthogonality may discard useful shared knowledge, potentially degrading single-task performance.

### Mechanism 2
- **Claim:** An analytical solution exists for the constrained optimization via eigendecomposition of the sample covariance matrix.
- **Mechanism:** Given covariance matrix S = (1/k-1)H₁ᵀH₁ = VΛVᵀ, the solution to minimizing ‖AH₁ᵀ‖_F subject to AAᵀ = I is Ã = Vᵀ_{:,n-r:n}—the last r eigenvectors corresponding to the smallest eigenvalues. This selects directions with minimal variance in out-of-task data.
- **Core assumption:** H₁ is full-rank (verified empirically in Figure 2), preventing trivial solution A = 0; the orthogonal constraint AAᵀ = I removes scale ambiguity between A and B.
- **Evidence anchors:**
  - [section 4.3] "the analytical solution to Eq. (2) is: Ã₂ = Vᵀ_{:,n-r:n}"
  - [section 4.2] "Fig. 2 shows that H₁ is typically full-rank in real scenarios due to intrinsic data variability"
  - [corpus] Corpus papers lack this specific eigendecomposition approach; most use data-free methods or different orthogonalization techniques.
- **Break condition:** With very few samples (k→1), the covariance matrix becomes ill-conditioned, making eigendecomposition unstable.

### Mechanism 3
- **Claim:** Relaxing the constraint by using Ã only as initialization (allowing A to update during fine-tuning) preserves single-task accuracy while maintaining merging benefits.
- **Mechanism:** Rather than freezing Ã after initialization, OSRM allows gradient updates. Figure 3 shows Ã changes by ≤14% after fine-tuning (measured via orthogonal Procrustes distance), indicating the initialization remains approximately valid while recovering representation capacity.
- **Core assumption:** The optimization landscape has local minima near the initialized orthogonal subspace that satisfy both task performance and interference reduction objectives.
- **Evidence anchors:**
  - [section 4.4] "Results in Fig. 3 show that the change of Ã is up to 14% approximately, which is marginal"
  - [section 5.2, Tables 1-5] Single-task performance gap <1% on average; OSRM improves individual performance on some tasks
  - [corpus] LoRI paper similarly allows constrained updates during multi-task training, though using fixed masks rather than initialization.
- **Break condition:** If fine-tuning optimization significantly drifts from the initialized subspace (large learning rates, many epochs), interference benefits may diminish.

## Foundational Learning

- **Concept: LoRA decomposition (ΔW = BA)**
  - Why needed here: OSRM operates specifically on the A matrix; understanding that B and A serve different roles (B handles scaling, A defines transformation directions) is essential for grasping why constraining A alone is viable.
  - Quick check question: Why does constraining A while leaving B free not collapse the model's expressiveness?

- **Concept: Sample covariance eigendecomposition**
  - Why needed here: The core algorithm requires computing covariance S = (1/k-1)HᵀH and extracting eigenvectors; practitioners must understand why smallest eigenvalues correspond to low-variance (interference-minimizing) directions.
  - Quick check question: If all eigenvalues of S are nearly equal, what does this imply about the feasibility of OSRM?

- **Concept: Model merging paradigms (Task Arithmetic, TIES, RegMean)**
  - Why needed here: OSRM is a pre-processing step that integrates with existing merging algorithms; understanding baseline methods clarifies where OSRM fits in the pipeline.
  - Quick check question: At which stage—before fine-tuning, during fine-tuning, or during merging—does OSRM modify the workflow?

## Architecture Onboarding

- **Component map:** Feature extraction -> Covariance computation -> Eigendecomposition solver -> LoRA fine-tuning wrapper -> Merge interface
- **Critical path:**
  1. Collect out-of-task features H_¬t for each task T_t (concatenate features from all other tasks)
  2. Compute per-layer covariance and eigendecomposition
  3. Initialize A^{(l)}_t = Vᵀ_{:,n-r:n} for each layer l and task t
  4. Fine-tune {B, A} on respective tasks (A updated, not frozen)
  5. Merge using chosen algorithm

- **Design tradeoffs:**
  - **Sample size k:** Too few → ill-conditioned covariance; too many → knowledge overlap between tasks may hurt orthogonality benefit. Paper finds k=10-100 optimal (Table 6).
  - **Freezing vs. updating A:** Freezing guarantees orthogonality but degrades task performance; updating preserves performance with ~14% drift (Figure 3).
  - **Memory vs. privacy:** Storing full H_¬t requires more memory; averaging features per Eq. (4) reduces memory and addresses privacy concerns.
  - **LoRA rank r:** Higher r provides more capacity but may increase interference; paper uses r=8 throughout.

- **Failure signatures:**
  - Single-task performance drops >2%: Likely A initialization too restrictive; check if k is too large causing over-orthogonalization to shared knowledge
  - Merging performance unchanged from baseline: Verify A initialization is actually being applied; check that H_¬t contains correct out-of-task features
  - Numerical instability in eigendecomposition: k too small causing rank-deficient covariance matrix
  - Scaling coefficient sensitivity: OSRM should reduce sensitivity; if still sensitive, check initialization correctness

- **First 3 experiments:**
  1. **Validation of A initialization effect:** Fine-tune two models on different tasks—one with OSRM initialization, one with standard random initialization—merge with Task Arithmetic (λ=0.3). Compare merged accuracy on both tasks. Expected: OSRM shows higher average merged performance, similar single-task performance.
  2. **Sample size sensitivity:** Repeat experiment 1 with k ∈ {10, 100, 1000} samples per task. Expected: Performance plateaus or slightly degrades beyond k=100 (per Table 6).
  3. **Multi-task scaling:** Merge N ∈ {2, 4, 6, 8} tasks using OSRM+RegMean. Expected: OSRM advantage increases with N (per Figure 5), showing scalability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the orthogonality constraints of OSRM be effectively adapted for merging fully fine-tuned models or models with heterogeneous architectures?
- Basis in paper: [explicit] The authors state in Section 7 (Limitations) that the method currently relies on identical architectures and LoRA-specific structures, limiting applicability to fully fine-tuned models or different model types.
- Why unresolved: The current method depends on the low-rank structure of LoRA ($A$ and $B$ matrices) to apply orthogonality constraints; full fine-tuning involves dense updates without this inherent low-rank decomposition.
- What evidence would resolve it: A modification of OSRM that operates on dense weight differences or architectural permutations, demonstrating improved merging performance on non-LoRA or cross-architecture baselines.

### Open Question 2
- Question: How can the post-hoc decomposition of existing LoRA modules be modified to preserve single-task performance?
- Basis in paper: [explicit] Section 5.4 notes that applying OSRM to already-trained LoRA modules causes a "significant drop in the individual performance" and identifies preserving this performance as a "promising direction for future work."
- Why unresolved: The current recovery method ($\hat{B}_t = \min_B \|B\tilde{A}_t - \Delta W_t\|_F$) projects the original weights onto a new orthogonal basis, resulting in irreversible information loss regarding the original task's specific features.
- What evidence would resolve it: A decomposition technique that minimizes the reconstruction error of the original weights $\Delta W_t$ while maintaining the orthogonal properties required for robust merging.

### Open Question 3
- Question: Why does increasing the number of calibration samples ($k$) lead to suboptimal merging performance?
- Basis in paper: [inferred] Section 5.3 (Impact of Sample Size) discusses counter-intuitive results where increasing samples increases knowledge overlap, hurting performance. The authors state, "it is challenging to analytically determine the cause... We believe the exploration of this situation is interesting."
- Why unresolved: While the paper hypothesizes that large $k$ increases "knowledge overlap" between tasks (making orthogonality constraints too strict), the exact theoretical boundary between useful statistical estimation and harmful feature overlap remains undefined.
- What evidence would resolve it: A theoretical analysis or empirical ablation isolating the trade-off between estimation accuracy and task distinctiveness as sample size grows.

## Limitations
- Assumes task-specific subspaces can be meaningfully orthogonalized without losing task-relevant information
- Requires storing or processing out-of-task samples for covariance computation, raising privacy concerns
- Current method limited to identical architectures and LoRA-specific structures

## Confidence
- **High Confidence:** The analytical solution for subspace initialization and the core mathematical framework are sound
- **Medium Confidence:** The empirical results demonstrate consistent improvements across multiple benchmarks, though the absolute magnitude of gains varies by model size and task combinations
- **Medium Confidence:** The claim about robustness to hyperparameters is supported by ablation studies, but the exploration space was not exhaustive

## Next Checks
1. **Distribution Overlap Analysis:** Measure feature distribution overlap between tasks to determine when OSRM's orthogonality assumption breaks down
2. **Single-Task Degradation Study:** Systematically evaluate the trade-off between merging performance and single-task accuracy across a wider range of task pairs
3. **Privacy-Aware Variant:** Develop and benchmark a variant that uses synthetic data or differential privacy for covariance computation to address privacy concerns