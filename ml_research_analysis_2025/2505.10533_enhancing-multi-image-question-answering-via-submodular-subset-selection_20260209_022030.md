---
ver: rpa2
title: Enhancing Multi-Image Question Answering via Submodular Subset Selection
arxiv_id: '2505.10533'
source_url: https://arxiv.org/abs/2505.10533
tags:
- subset
- images
- submodular
- image
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving retrieval accuracy
  in multi-image question answering (MIQA) scenarios, particularly for large collections
  of images (haystacks). The authors propose enhancing the MIRAGE model's retriever
  by incorporating submodular subset selection techniques to pre-filter relevant images
  before the main retrieval step.
---

# Enhancing Multi-Image Question Answering via Submodular Subset Selection

## Quick Facts
- arXiv ID: 2505.10533
- Source URL: https://arxiv.org/abs/2505.10533
- Authors: Aaryan Sharma; Shivansh Gupta; Samar Agarwal; Vishak Prasad C.; Ganesh Ramakrishnan
- Reference count: 8
- Key outcome: The paper addresses the challenge of improving retrieval accuracy in multi-image question answering (MIQA) scenarios, particularly for large collections of images (haystacks). The authors propose enhancing the MIRAGE model's retriever by incorporating submodular subset selection techniques to pre-filter relevant images before the main retrieval step. Their method uses query-aware submodular functions like GraphCut, Facility Location, and Log Determinant to select semantically relevant image subsets, with experiments showing that using anchor-based queries and data augmentation significantly improves the submodular-retriever pipeline's effectiveness, especially for large haystack sizes.

## Executive Summary
This paper addresses the challenge of retrieving relevant images from large image collections (haystacks) for multi-image question answering tasks. The authors enhance the MIRAGE model's retriever by incorporating submodular subset selection techniques that pre-filter semantically relevant images before the main retrieval step. Their approach uses query-aware submodular functions (GraphCut, Facility Location, LogDet) to select image subsets, with experiments showing that anchor-based queries and data augmentation significantly improve retrieval accuracy, particularly for large haystack sizes (500-1000+ images).

## Method Summary
The method enhances MIRAGE's retriever by adding a pre-filtering stage using submodular subset selection. It extracts VGG features from images, computes pairwise cosine similarities between query and haystack images, and applies query-aware submodular functions (GCMI, FLVMI, LogDet) with a NaiveGreedy optimizer to select semantically relevant image subsets. The approach uses anchor-based queries (extracted from "For the image with an {Anchor}, is there a {Target}?" templates) rather than target-based queries, and expands the query set with reference images and SimCLR-style augmentations. The filtered subset is then passed to the main MIRAGE retriever for final needle identification.

## Key Results
- GCMI submodular function outperforms mixed-function (70:20:10) approaches in pre-filtering image haystacks
- Anchor-based queries significantly outperform Target-based queries for subset selection
- Data augmentation (2-4 augmented images per class) improves retriever accuracy by capturing richer semantic context
- The submodular-retriever pipeline shows substantial improvements in retriever success fraction, especially for large haystack sizes (500-1000+ images)

## Why This Works (Mechanism)

### Mechanism 1
Pre-filtering large image haystacks using submodular subset selection improves retriever accuracy by reducing the search space while preserving semantically relevant images. Submodular functions compute pairwise cosine similarity between query image features and haystack images, with NaiveGreedy optimizer selecting top-ranked images that balance query relevance, diversity, and redundancy. This filtered subset is passed to the main retriever.

### Mechanism 2
Anchor-based query construction outperforms Target-based queries for subset selection in the Visual Haystacks benchmark. Anchor images (e.g., "Truck") provide stronger contextual cues than Target images (e.g., "Dog") because COCO images contain multiple objects, and anchor objects tend to be more distinctive for similarity matching.

### Mechanism 3
Expanding the query set with multiple reference images and data augmentation improves retriever success by capturing richer semantic context. Instead of a single reference image, the query set is expanded to 2-5 images per class with SimCLR-style augmentations (random crop, flip, color distortion, gaussian blur), increasing robustness since COCO images contain multiple objects.

## Foundational Learning

- **Submodular Functions and Mutual Information**: Core to understanding how GraphCut, Facility Location, and LogDet balance relevance, diversity, and redundancy in subset selection. *Quick check: Can you explain why a submodular function exhibits diminishing returns property and how this enables efficient greedy optimization?*

- **Vision-Language Retrieval and Embedding Spaces**: The method relies on cosine similarity in VGG feature space; understanding embedding quality is critical for debugging retrieval failures. *Quick check: Why might cosine similarity in a general VGG feature space fail to capture fine-grained distinctions (e.g., skateboard vs. snowboard)?*

- **Retrieval-Augmented Generation (RAG) for Vision**: The method enhances the MIRAGE retriever component within a RAG pipeline; understanding the full pipeline clarifies where failures propagate. *Quick check: In a vision RAG pipeline, how does retriever recall@k affect downstream LMM generation quality?*

## Architecture Onboarding

- **Component map**: Query Processor -> Feature Extractor -> Submodular Selector -> MIRAGE Retriever -> LMM
- **Critical path**: 1. Query parsing â†’ Anchor identification, 2. Query set construction (reference images + augmentation), 3. VGG feature extraction for query set and haystack, 4. Submodular subset selection (GCMI recommended), 5. Pass filtered subset to MIRAGE retriever, 6. Retriever outputs needle image to LMM
- **Design tradeoffs**: Single-function (GCMI) vs. mixed-function (70:20:10) - paper found single-function GCMI outperformed mixed; Query set size - more reference images help but with diminishing returns; Subset size as % of haystack - larger subsets preserve recall but reduce efficiency gains
- **Failure signatures**: Anchor Confusion - retriever selects images with visually similar objects (snowboard vs. skateboard); Target Salience Bias - distractor images with prominent targets override correct anchors; Anchor Ambiguity - multiple images contain the anchor; retriever cannot disambiguate
- **First 3 experiments**: 1. Baseline replication - Run MIRAGE retriever on COCO haystacks of sizes 100, 500, 1000 without submodular pre-filtering; measure success fraction, 2. Anchor vs. Target query comparison - Implement GCMI subset selection (10-30% of haystack) using Anchor images as query; compare against Target-based selection on haystack size 1000, 3. Query set expansion with augmentation - Use 2 reference images + 2 augmentations per class; measure retriever success fraction against single-reference baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can training a deep submodular function using gradients from a QA loss outperform general-purpose submodular functions (like GraphCut) in the MIQA retrieval pipeline? The authors state in Sections 2 and 3.1 that in future work, they aim to "adapt this strategy to select contextually important and diverse image subsets by using gradients from a QA loss to train a deep submodular function." The current work relies on pre-defined submodular functions (GCMI, FLVMI, LogDet). The proposed deep submodular approach is mentioned as a theoretical extension but has not been implemented or tested.

### Open Question 2
Can the proposed submodular pre-filtering pipeline be effectively adapted for "Document Haystacks" requiring OCR and layout analysis? Section 5 explicitly identifies "Document Haystacks" as a future work direction, noting the need for "structured document processing, requiring integration of OCR, visual layout analysis and caption generation." The current system is validated on natural images (COCO) using VGG features. It is untested whether similarity-based submodular selection works for text-heavy visual documents where layout and text are primary features.

### Open Question 3
How does the reliance on distinct "Anchor" and "Target" phrases in the query affect performance on open-ended or complex natural language questions? The paper notes in Section 4 that the VHS benchmark uses specific query templates (e.g., "For the image with a Truck..."). It acknowledges that "a more general setting MIQA task could involve open-ended or more complex queries," but the method currently depends heavily on extracting anchor cues.

## Limitations

- The observed superiority of Anchor-based queries may be dataset-specific to COCO and may not generalize to other datasets where target objects are more distinctive
- The evaluation relies entirely on the VHS benchmark, which may have dataset-specific biases in query construction and object co-occurrence patterns
- The method requires explicit anchor-target templates in queries, limiting applicability to open-ended or complex natural language questions

## Confidence

- **High**: The core mechanism of using submodular functions for pre-filtering (GCMI outperforming mixed functions, augmentation benefits) is well-supported by ablation studies
- **Medium**: The Anchor vs. Target distinction and its impact on retrieval success is supported by internal analysis but lacks external corpus validation
- **Medium**: The claim that this approach is "effective" for large haystacks is demonstrated on COCO but requires validation on other benchmarks

## Next Checks

1. **Dataset Transferability Test**: Evaluate the Anchor vs. Target query distinction on at least two additional multi-image QA benchmarks (e.g., VCR, GQA) to determine if the observed pattern generalizes beyond COCO.

2. **Feature Space Sensitivity Analysis**: Test retrieval performance using different embedding spaces (CLIP, ResNet, EfficientNet) to assess whether VGG feature space limitations (snowboard vs. skateboard confusion) constrain the method's effectiveness.

3. **Ground-Truth Coverage Validation**: Systematically measure the percentage of ground-truth images excluded by submodular subset selection across different subset sizes and query strategies to quantify the risk of false negatives in the pre-filtering step.