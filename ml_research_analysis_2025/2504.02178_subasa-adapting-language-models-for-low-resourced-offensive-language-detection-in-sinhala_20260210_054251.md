---
ver: rpa2
title: Subasa - Adapting Language Models for Low-resourced Offensive Language Detection
  in Sinhala
arxiv_id: '2504.02178'
source_url: https://arxiv.org/abs/2504.02178
tags:
- language
- offensive
- sinhala
- detection
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of offensive language detection
  in Sinhala, a low-resource language, by introducing four novel models: Subasa-XLM-R,
  Subasa-Llama (two variants), and Subasa-Mistral. The core method involves adapting
  fine-tuning strategies, including an intermediate Pre-Finetuning step using Masked
  Rationale Prediction for Subasa-XLM-R and task-specific fine-tuning strategies for
  the LLM variants.'
---

# Subasa - Adapting Language Models for Low-resourced Offensive Language Detection in Sinhala

## Quick Facts
- arXiv ID: 2504.02178
- Source URL: https://arxiv.org/abs/2504.02178
- Reference count: 16
- Primary result: Four novel models (Subasa-XLM-R, Subasa-Llama, Subasa-Mistral) achieve state-of-the-art Macro F1 of 0.84 on SOLD dataset, surpassing GPT-4o zero-shot performance

## Executive Summary
This paper addresses offensive language detection in Sinhala, a low-resource language, by introducing four novel models: Subasa-XLM-R, Subasa-Llama (two variants), and Subasa-Mistral. The core method involves adapting fine-tuning strategies, including an intermediate Pre-Finetuning step using Masked Rationale Prediction for Subasa-XLM-R and task-specific fine-tuning strategies for the LLM variants. Subasa-XLM-R achieves the highest Macro F1 score of 0.84 on the SOLD benchmark dataset, surpassing state-of-the-art large language models like GPT-4o in zero-shot settings. All models outperform existing baselines.

## Method Summary
The Subasa framework introduces two adaptation strategies for low-resource offensive language detection. For Subasa-XLM-R, a two-stage fine-tuning approach is used: first, an intermediate Pre-Finetuning stage with Masked Rationale Prediction (MRP) where token embeddings are fused with rationale embeddings and 75% of non-special tokens are masked for prediction; second, a classification fine-tuning stage using the MRP-weights-initialized model. For Subasa-Llama and Subasa-Mistral, a task-specific instruction fine-tuning strategy with QLoRA is employed, using 4-bit quantization and low-rank adapters to adapt base LLMs for binary classification plus offensive phrase extraction.

## Key Results
- Subasa-XLM-R achieves state-of-the-art Macro F1 of 0.84 on SOLD dataset
- Subasa-Llama-3.1-8B achieves 0.826 Macro F1, outperforming GPT-4o zero-shot (0.733)
- MRP intermediate task provides focused learning signal, though MLM with 50% masking performs nearly as well (0.83 vs 0.84)
- All Subasa models significantly outperform existing baselines on SOLD dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intermediate pre-finetuning with Masked Rationale Prediction (MRP) improves downstream offensive language detection for Sinhala by reinforcing token-level context understanding before task-specific training.
- **Mechanism:** MRP operates by fusing token embeddings (X_S) with rationale embeddings (X_R), then masking 75% of non-special rationale tokens and training the model to predict masked rationale labels. This creates a learning signal that forces the model to reason about *which tokens justify the offensive label*, not just the label itself. The model transfers this rationale-awareness to the downstream classification task.
- **Core assumption:** The paper assumes that rationale annotations (token-level indicators of offensiveness) provide a meaningful intermediate learning signal that transfers to sentence-level classification. This assumes rationale quality is consistent and annotators agree on what constitutes offensive tokens.
- **Evidence anchors:**
  - [abstract] "Subasa-XLM-R... incorporates an intermediate Pre-Finetuning step using Masked Rationale Prediction"
  - [Section 3.1] Describes the two-stage strategy with equations (1-6) showing embedding fusion H^{(0)}_{MRP} = X_S + \tilde{X}_R
  - [Section 4, Table 6] Ablation shows MRP (0.75 mask ratio) achieves 0.84 Macro-F1 vs. no intermediate task (0.82)
  - [corpus] Related work on multi-task learning for Arabic offensive speech detection (FMR=0.54) supports intermediate task benefits, but corpus lacks direct MRP comparisons for Sinhala.

### Mechanism 2
- **Claim:** Task-specific instruction fine-tuning with QLoRA enables smaller open-source LLMs (3B-8B parameters) to surpass zero-shot GPT-4o performance on Sinhala offensive language detection.
- **Mechanism:** QLoRA (4-bit quantization + Low-Rank Adaptation) freezes base model weights and trains only small adapter matrices (rank=16). The instruction template explicitly prompts for OFF/NOT classification and offensive phrase extraction, creating a dual-task signal (classification + localization) that concentrates model capacity on task-relevant patterns.
- **Core assumption:** Assumes the base LLM has sufficient multilingual pre-training exposure to Sinhala that fine-tuning can activate, even if implicitly. Also assumes the prompt template design doesn't introduce task confusion.
- **Evidence anchors:**
  - [abstract] "Two variants of Subasa-Llama and Subasa-Mistral are fine-tuned versions... with a task-specific strategy"
  - [Section 3.2] Details QLoRA hyperparameters (rank=16, α=16, targeting all linear projections)
  - [Section 4, Table 4] Subasa-Llama-3.1-8B achieves 0.826 Macro-F1 vs. GPT-4o zero-shot (0.733)
  - [corpus] SinLlama paper demonstrates continued pre-training on Sinhala improves LLM performance, supporting the assumption that base models have latent Sinhala capacity.

### Mechanism 3
- **Claim:** XLM-R's smaller parameter count (270M) can outperform larger LLMs (8B) for Sinhala offensive detection when combined with MRP, due to stronger multilingual pre-training foundation.
- **Mechanism:** XLM-R is pre-trained on 100 languages including Sinhala, providing stronger cross-lingual representations than Llama/Mistral which are English-dominant. MRP then specializes these representations for the offensive detection task. The smaller model avoids overfitting risks that larger models face with limited low-resource data.
- **Core assumption:** Assumes the multilingual pre-training quality for Sinhala in XLM-R exceeds the latent Sinhala knowledge in English-centric LLMs, even after fine-tuning. Assumes dataset size (6,750 training samples) is insufficient for larger models to overcome their pre-training bias.
- **Evidence anchors:**
  - [Section 4] "Subasa-XLM-R (0.84 macro-F1) slightly trails the smaller Subasa-XLM-R model... suggests MRP's intermediate task provides a focused learning signal"
  - [Section 4] Explicitly notes Llama variants "inherit base models with minimal Sinhala pre-training data compared to XLM-R's multilingual foundation"
  - [Section 5, Limitations] Notes hardware constraints prevented xlm-roberta-large comparison

## Foundational Learning

- **Concept: Transfer Learning with Intermediate Tasks**
  - **Why needed here:** MRP is an intermediate task between pre-training and fine-tuning. Understanding why intermediate tasks help (knowledge transfer, task rehearsal) is essential to interpret ablation results showing MLM performs similarly to MRP.
  - **Quick check question:** If you replaced MRP with a generic masked language modeling task, would you expect similar gains? What does the paper's ablation reveal?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA/QLoRA)**
  - **Why needed here:** The LLM variants use 4-bit quantization and LoRA adapters rather than full fine-tuning. Understanding low-rank adaptation (freezing base weights, training small matrices) explains why 3B-8B models can be adapted affordably.
  - **Quick check question:** Why does LoRA target all linear projections (q_proj, k_proj, v_proj, etc.) rather than just attention layers? What tradeoff does this represent?

- **Concept: Rationale-based Explainability**
  - **Why needed here:** DSOLD provides token-level rationale annotations (which tokens justify the offensive label). Understanding rationales explains why MRP is possible and why the prompt template includes phrase extraction.
  - **Quick check question:** If rationale annotations were unavailable, could you synthesize them? What quality thresholds would be acceptable?

## Architecture Onboarding

- **Component map:**
```
Stage 1 (MRP Pre-Finetuning):
  Input Text → Tokenizer (+ special tokens @USER, <URL>)
            → X_S (token embeddings) + X_R (rationale embeddings)
            → Mask 75% of rationale tokens → Transformer layers
            → MLP → Predict masked rationale labels

Stage 2 (Classification Fine-Tuning):
  Load Stage 1 weights → XLM-R base → Classification head
                       → Binary output (OFF/NOT)

Alternative Path (LLM Fine-Tuning):
  Input Text → Instruction Template (system + user + assistant)
            → 4-bit Quantized LLM (Llama/Mistral)
            → LoRA Adapters (rank-16) → OFF/NOT + phrase extraction
```

- **Critical path:**
  1. Data preparation: Ensure rationale labels align with tokenized sequence length (rationale processing step in Stage 1)
  2. MRP training: Mask ratio selection (0.75 optimal per ablation) and embedding fusion
  3. Classification fine-tuning: Initialize from MRP weights, train on full DSOLD
  4. For LLM path: Prompt template construction, QLoRA configuration, instruction fine-tuning

- **Design tradeoffs:**
  - **MRP vs. MLM intermediate task:** Paper shows MLM (50% masking) nearly matches MRP (0.83 vs. 0.84). MRP provides rationale-awareness but requires token-level annotations; MLM is annotation-free but less targeted.
  - **XLM-R vs. LLM choice:** XLM-R is smaller, faster, and has stronger multilingual pre-training; LLMs offer instruction-following and generative capabilities but require more compute and may have weaker Sinhala foundation.
  - **Mask ratio (0.75):** Paper empirically validates 0.75 as optimal; higher ratios (1.0) or lower (0.25) both underperform slightly.

- **Failure signatures:**
  - Low recall on OFFENSIVE class with high precision: Model is conservative, possibly due to class imbalance or insufficient offensive signal in rationales
  - LLM zero-shot predicts majority class (NOT OFFENSIVE): Indicates base model has no Sinhala offensive language understanding (Table 4 shows Llama-3.2-3B-Instruct has 0.0 OFFENSIVE recall)
  - MRP training loss plateaus early: May indicate rationale annotations are too sparse or inconsistent

- **First 3 experiments:**
  1. **Reproduce ablation:** Train XLM-R with MLM (0.15 and 0.50 masking) vs. MRP (0.75 masking) on DSOLD. Verify if MLM matches MRP performance, confirming paper's finding that any token-level intermediate task helps.
  2. **Prompt sensitivity test:** Vary the instruction template for LLM fine-tuning (e.g., remove phrase extraction requirement, change system prompt). Measure impact on Macro-F1 to assess template sensitivity (paper notes this as a limitation).
  3. **Cross-dataset validation:** If another Sinhala offensive language dataset exists (paper notes <5 available), evaluate Subasa-XLM-R and Subasa-Llama on it to assess generalization beyond DSOLD. If unavailable, create a small held-out set from DSOLD training data and test domain shift.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does Masked Rationale Prediction (MRP) provide specific benefits over standard Masked Language Modeling (MLM) with high masking ratios for low-resource offensive language detection?
- **Basis in paper:** [explicit] Page 5 states that while MRP improved performance, standard MLM with 50% masking nearly matched it (0.83 vs 0.84 Macro F1). The authors conclude this "warrant[s] further study into task-specific intermediate objectives for low-resource languages."
- **Why unresolved:** The empirical results suggest that the improvements may stem from reinforcing local context generally (via any token-level task) rather than the specific rationale mechanism of MRP.
- **What evidence would resolve it:** A comparative study evaluating MRP against MLM across multiple low-resource languages to determine if the equivalence holds or if MRP offers a consistent advantage.

### Open Question 2
- **Question:** How would larger model variants (e.g., XLM-R-Large or Llama-70B) perform when adapted using the proposed Subasa strategies?
- **Basis in paper:** [explicit] Page 6 (Limitations) notes that hardware constraints restricted experiments to `xlm-roberta-base` and smaller LLM variants (3B/7B). The authors state this "precludes direct comparisons with larger variants... which might exhibit different behaviors."
- **Why unresolved:** It is currently unknown if the success of the intermediate pre-finetuning scales with model size or if the counter-intuitive result (smaller XLM-R outperforming larger Llama) persists with larger backbones.
- **What evidence would resolve it:** Running the Subasa-XLM-R and Subasa-Llama pipelines on larger parameter versions of the base models using the SOLD benchmark.

### Open Question 3
- **Question:** To what extent does the specific prompt template design influence the performance of the instruction-finetuned LLMs?
- **Basis in paper:** [explicit] Page 6 (Limitations) explains that the task-specific fine-tuning utilized a "single prompt template in a zero-shot prompting setting," which "limited insights into the sensitivity of results against alternative prompting strategies."
- **Why unresolved:** The reported high performance of Subasa-Llama and Subasa-Mistral may be contingent on the specific phrasing of the system prompt (Appendix A) and might not generalize to different instruction formats.
- **What evidence would resolve it:** An ablation study varying the prompt templates (e.g., changing system instructions or output formatting) during the fine-tuning and evaluation phases to measure performance variance.

## Limitations
- Hardware constraints prevented comparison with larger models (xlm-roberta-large, Llama-70B), leaving uncertainty about scalability
- Only single prompt template used for LLM fine-tuning, limiting understanding of prompt sensitivity and generalizability
- All evaluation conducted on single dataset (DSOLD), raising questions about generalizability to other Sinhala offensive language detection tasks
- MRP performance advantage over simpler MLM intermediate tasks is marginal (0.84 vs 0.83), suggesting benefits may come from any token-level task rather than MRP specifically

## Confidence

- **High confidence**: Subasa-XLM-R's state-of-the-art performance on SOLD dataset (Macro F1: 0.84). The methodology is clearly specified, ablation results support the MRP approach, and the dataset is well-defined.
- **Medium confidence**: Claims about XLM-R's multilingual pre-training superiority over Llama/Mistral for Sinhala. While logically consistent with model architecture differences, lacks direct comparison against larger multilingual models or comprehensive cross-lingual benchmarks.
- **Medium confidence**: Claims about QLoRA's effectiveness for Sinhala offensive detection. The mechanism is sound and results are strong, but sensitivity to prompt design and base model Sinhala exposure remain unquantified.
- **Low confidence**: Generalizability claims to other Sinhala offensive language detection tasks or datasets. No external validation or cross-dataset testing is presented.

## Next Checks

1. **Prompt template ablation study**: Systematically vary the instruction template components (remove phrase extraction requirement, change system prompt, modify formatting) across multiple runs. Measure the variance in Macro-F1 to quantify template sensitivity and identify which components are critical for performance.

2. **Cross-dataset generalization test**: If another Sinhala offensive language dataset becomes available, evaluate Subasa-XLM-R and Subasa-Llama models on it. If no external dataset exists, create a held-out subset from the SOLD training data by time-splitting (e.g., last month's tweets) and test domain shift performance.

3. **MRP vs. MLM replication**: Reproduce the intermediate task ablation by training XLM-R with MLM (0.15 and 0.50 masking) and MRP (0.75 masking) on DSOLD. Verify whether the paper's finding that MLM nearly matches MRP performance holds, which would suggest that any token-level intermediate task provides benefits rather than MRP specifically.