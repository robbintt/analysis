---
ver: rpa2
title: Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration
arxiv_id: '2509.15882'
source_url: https://arxiv.org/abs/2509.15882
tags:
- feature
- registration
- point
- cloud
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CrossI2P is a self-supervised framework for image-to-point cloud
  registration that bridges the semantic-geometric gap between 2D images and 3D point
  clouds. It combines dual-path contrastive learning for cross-modal feature alignment
  with a coarse-to-fine registration pipeline that establishes superpoint-superpixel
  correspondences before refining them geometrically.
---

# Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration

## Quick Facts
- **arXiv ID:** 2509.15882
- **Source URL:** https://arxiv.org/abs/2509.15882
- **Reference count:** 8
- **Primary result:** Outperforms state-of-the-art by 23.7% on KITTI and 37.9% on nuScenes

## Executive Summary
CrossI2P introduces a self-supervised framework for image-to-point cloud registration that bridges the semantic-geometric gap between 2D images and 3D point clouds. The method combines dual-path contrastive learning for cross-modal feature alignment with a coarse-to-fine registration pipeline that establishes superpoint-superpixel correspondences before refining them geometrically. Using a differentiable PnP solver and dynamic gradient normalization, CrossI2P achieves state-of-the-art performance on autonomous driving datasets, demonstrating superior accuracy and robustness across diverse scenarios.

## Method Summary
CrossI2P addresses the challenging task of estimating 6-DoF pose between images and point clouds without requiring labeled correspondences. The framework employs a dual-path architecture with ResNet-50 for images and DGCNN for point clouds, projecting features into a shared 256-dimensional space. Cross-modal and intra-modal contrastive learning align features across modalities, while a two-stage transformer matching module performs coarse block-to-block matching followed by fine point-to-pixel refinement. A differentiable PnP solver enables end-to-end optimization, with GradNorm dynamically balancing the three loss components. The method is trained self-supervised on KITTI Odometry and nuScenes, achieving significant improvements over existing approaches.

## Key Results
- Achieves 23.7% improvement on KITTI Odometry benchmark
- Demonstrates 37.9% improvement on nuScenes dataset
- Shows superior performance across diverse autonomous driving scenarios
- Effectively bridges semantic-geometric gap between 2D and 3D modalities

## Why This Works (Mechanism)
The method's effectiveness stems from its ability to align semantic features from images with geometric features from point clouds through contrastive learning, while the coarse-to-fine matching pipeline ensures robust correspondence establishment. The differentiable PnP solver enables end-to-end training, and GradNorm prevents the pose loss from dominating feature learning. This combination allows the model to learn meaningful cross-modal embeddings that capture both appearance and geometry, leading to accurate 6-DoF pose estimation without requiring labeled correspondences.

## Foundational Learning

**Point Cloud Voxelization**
- *Why needed:* Reduces computational complexity and provides structured representation for deep learning
- *Quick check:* Verify voxel grid resolution preserves fine geometric details while maintaining efficient processing

**Dual-Path Contrastive Learning**
- *Why needed:* Aligns semantic features from images with geometric features from point clouds in a shared embedding space
- *Quick check:* Monitor contrastive loss convergence and feature space visualization for cross-modal alignment

**Differentiable PnP Solver**
- *Why needed:* Enables end-to-end optimization by making the pose estimation process differentiable
- *Quick check:* Validate gradient flow through the PnP module during backpropagation

## Architecture Onboarding

**Component Map**
Image Encoder (ResNet-50) -> Feature Projector -> Cross-Modal Contrastive Loss
Point Cloud Encoder (DGCNN) -> Feature Projector -> Cross-Modal Contrastive Loss
Coarse Transformer Matching -> Fine Transformer Matching -> Differentiable PnP -> Pose Loss
GradNorm -> Loss Weight Balancing

**Critical Path**
Image/Point Cloud -> Encoders -> Projector -> Contrastive Learning -> Matching (Coarse/Fine) -> Differentiable PnP -> Pose Estimation

**Design Tradeoffs**
- Heavy backbones (ResNet50, DGCNN) provide strong feature extraction but limit batch size to 1
- Two-stage transformer matching ensures robust correspondences but increases computational complexity
- GradNorm prevents loss imbalance but requires careful hyperparameter tuning

**Failure Signatures**
- High coarse matching loss with low fine matching loss indicates failure to capture global context
- Vanishing gradient norms in contrastive loss suggest GradNorm misconfiguration
- Poor registration recall despite low pose loss indicates suboptimal feature alignment

**First Experiments**
1. Validate feature projector output dimension consistency (256-dim)
2. Test GradNorm weight initialization and dynamic adjustment
3. Verify differentiable PnP gradient computation correctness

## Open Questions the Paper Calls Out

**Real-time Deployment Adaptation**
The authors plan to enhance CrossI2P for real-time deployment via lightweight model distillation. Current architecture uses heavy backbones and Transformers, limiting batch size to 1 on high-end GPUs, suggesting high computational overhead for practical deployment.

**Multi-sensor Fusion Generalization**
Future work aims to extend applicability to broader multi-sensor fusion scenarios beyond RGB and LiDAR. Current contrastive learning modules are specifically designed for image-to-point cloud gap; scalability to modalities with different physical properties (e.g., Radar, Event cameras) remains unclear.

**True Self-supervision Capability**
The current implementation relies on pre-calibrated ground-truth pose files to establish correspondences, creating dependency on calibrated datasets despite the "self-supervised" label. True self-supervision would require learning from uncalibrated data using only raw sensor data and relative geometric constraints.

## Limitations

- Heavy computational requirements limit batch size to 1, restricting practical deployment
- Performance metrics combine multiple error measures in undocumented ways, obscuring true improvement magnitude
- Method's effectiveness may be partially dataset-specific due to heavy preprocessing requirements
- True self-supervision is not achieved due to reliance on pre-calibrated ground-truth poses

## Confidence

- **High confidence:** Core technical approach (dual-path contrastive learning + coarse-to-fine matching + differentiable PnP) is clearly specified and methodologically sound
- **Medium confidence:** Ablation study results showing GradNorm's importance are convincing but require independent verification
- **Low confidence:** Absolute performance numbers (especially 37.9% improvement on nuScenes) need independent verification due to complex evaluation protocol

## Next Checks

1. Implement GradNorm weight dynamics monitoring to verify they prevent pose loss from dominating during training
2. Validate coarse stage matching quality by logging coarse matching loss and recall metrics
3. Test cross-dataset generalization by training on KITTI and evaluating on nuScenes with minimal adaptation