---
ver: rpa2
title: 'DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning'
arxiv_id: '2504.07080'
source_url: https://arxiv.org/abs/2504.07080
tags:
- reasoning
- qwen-2
- deductive
- consistency
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a deductive consistency metric to evaluate
  language models' reasoning capabilities beyond final accuracy. The authors develop
  a framework that assesses how well models understand input premises and infer conclusions
  over multiple reasoning steps by generating novel, perturbed versions of benchmark
  problems.
---

# DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning

## Quick Facts
- arXiv ID: 2504.07080
- Source URL: https://arxiv.org/abs/2504.07080
- Reference count: 40
- Primary result: LLM reasoning accuracy masks 15-30% decay in deductive consistency across 1-5 reasoning hops, revealed by perturbation analysis

## Executive Summary
This paper introduces a deductive consistency metric to evaluate language model reasoning capabilities beyond final answer accuracy. The framework isolates multi-step reasoning failures by providing partial solution prefixes and measuring consistency at each inference hop. On GSM-8K math problems, the authors reveal that models maintain strong consistency with increasing input premises but suffer significant accuracy decay as reasoning hops increase - an effect masked by benchmark memorization. The method uses templatization and code generation to create perturbed problem variants, enabling systematic evaluation of reasoning robustness. The analysis provides a new perspective for characterizing LM reasoning as computations over input premises and reasoning hops.

## Method Summary
The DeduCE framework evaluates deductive reasoning by decomposing problems into premises and inference hops. For each benchmark problem, the pipeline generates templatized representations with executable Python code capturing the reasoning DAG. By sampling novel variable values, the method creates perturbed problems that preserve structure while changing content. The framework then measures deductive consistency - the percentage of correctly inferred intermediate variables at each hop - given varying numbers of prefix steps. This isolates premise comprehension from multi-step inference capability, revealing reasoning limitations masked by perfect benchmark accuracy.

## Key Results
- Models show 15-30% deductive consistency decay from 1 to 5 reasoning hops on perturbed GSM-8K problems
- Original benchmark accuracy remains near 100% while perturbed variants reveal reasoning failures
- Larger models (72B parameters) demonstrate greater resilience to increasing hops compared to smaller models (8B parameters)
- Post-training methods like RL improve task-specific patterns but don't generalize to synthetic deductive reasoning datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial chain-of-thought traces can isolate reasoning failures to specific inference steps.
- Mechanism: Given a valid reference solution, the framework provides k correct reasoning steps as "prefix" and evaluates whether the model correctly generates the next l "hops." By varying k (input premises) and l (remaining hops), the method separates premise comprehension from multi-step inference capability.
- Core assumption: A single correct solution provides sufficient grounding to identify valid intermediate predicates for verification.
- Evidence anchors:
  - [abstract] "The proposed metric studies LMs' performance on these subtasks... how well do LMs understand input premises with increasing context lengths, and how well can they infer conclusions over multiple reasoning hops?"
  - [section 3.1] "We extend the above metric to include longer input premises than in an original problem... by sampling a (correct) proof from the reference proof system and adding the first k steps of the proof to the input premises."
  - [corpus] Related work on backward logical reasoning (arxiv:2512.03360) supports decomposing reasoning into premise-to-conclusion chains, but does not validate the specific hop-based decay finding.
- Break condition: If reference solutions contain alternative valid paths not captured by the template, the consistency metric may under-estimate model capability (acknowledged in coverage metric).

### Mechanism 2
- Claim: Benchmark memorization masks reasoning limitations that perturbation reveals.
- Mechanism: The pipeline templatizes benchmark problems, generates executable Python code representing the reasoning graph, then samples novel variable values to create perturbed problems. Near-perfect accuracy on original problems paired with 15-30% consistency decay on perturbed versions indicates memorization rather than genuine reasoning.
- Core assumption: Changing numeric values preserves problem difficulty and reasoning structure.
- Evidence anchors:
  - [abstract] "Interestingly, these errors are masked in the original benchmark as all models achieve near 100% accuracy."
  - [section 5.3] "We find that the deductive consistency as a function of hops on the original benchmark achieves a constant value of 1 across all models. When deductive consistency is computed on perturbed problem, we find it to be significantly lower."
  - [corpus] GSM-Symbolic (Mirzadeh et al., 2024) and related perturbation studies corroborate accuracy drops on novel variants, consistent with this finding.
- Break condition: If perturbation inadvertently changes problem structure or introduces unnatural values, observed decay may conflate reasoning failure with comprehension failure.

### Mechanism 3
- Claim: Reasoning hop capacity scales with model size; post-training improves task patterns rather than general deduction.
- Mechanism: Larger models (72B parameters) maintain higher base consistency and lower decay rates across hops. RL fine-tuning reduces decay on in-distribution data but generalizes poorly to synthetic datasets; SFT distillation often increases decay.
- Core assumption: Synthetic dataset (SynDeduct) with controlled DAG structures provides valid out-of-distribution test of generalization.
- Evidence anchors:
  - [section 6.1] "Larger models demonstrate greater resilience to increases in the number of hops, while smaller models—such as Llama-3-8B-Instruct—experience a substantial drop in performance."
  - [section 7.1, Table 1-4] RL reduces decay (0.0602→0.0273 for Qwen-72B on SynDeduct), but SFT distillation increases decay (0.0211→0.0381 for Qwen-7B).
  - [corpus] Related work on outcome reward models (arxiv:2508.19903) suggests test-time scaling may complement training improvements, but does not directly validate the SFT/RL comparison.
- Break condition: If synthetic datasets lack linguistic diversity or problem structures present in real benchmarks, generalization conclusions may not transfer.

## Foundational Learning

- Concept: **Deductive closure and proof systems**
  - Why needed here: The framework formalizes reasoning as deriving predicates within a logical closure given inference rules. Understanding this abstraction is necessary to interpret the consistency metric.
  - Quick check question: Given premises {A→B, B→C} and transitivity, what predicates are in the deductive closure?

- Concept: **Right censoring in sequential evaluation**
  - Why needed here: The paper identifies that simply measuring accuracy at each step creates bias—shorter remaining chains are easier. The hops parameter corrects for this.
  - Quick check question: If you provide 4 of 5 reasoning steps, why might high accuracy not indicate strong reasoning?

- Concept: **Template-based data augmentation**
  - Why needed here: The pipeline relies on templatizing questions and code generation to create novel variants. Understanding this process is essential for reproducing or extending the framework.
  - Quick check question: What verification step ensures the generated code matches the templatized chain-of-thought?

## Architecture Onboarding

- Component map:
  - Templatization LM (Llama-3-70B-Instruct) -> Code Generation LM -> Variable Extraction LM -> Subject LM -> Consistency Computer

- Critical path:
  1. Original question → Templatization → Template + Factual Assignment
  2. Template → Code Generation → Executable Python (sanity-checked for equivalence)
  3. Sample new values → Execute code → Generate perturbed Q' and CoT'
  4. Provide k-step prefix to Subject LM → Extract values from completion → Compare at each hop l

- Design tradeoffs:
  - Single reference solution enables scalability but may miss alternative valid reasoning paths (addressed partially by coverage metric)
  - GSM8K limits maximum premise length to ~7 due to problem simplicity; SynDeduct provides controlled longer chains but sacrifices real-world linguistic complexity
  - Templatization failure rate ~30% requires filtering; stricter sanity checks improve quality but reduce dataset size (165 problems from 1000-sample subset)

- Failure signatures:
  - Low coverage (<0.9) indicates Subject LM using different reasoning path than reference; consistency metric becomes unreliable
  - High base consistency with high decay suggests strong single-step computation but weak multi-hop chaining
  - Near-zero decay on original benchmark with high decay on perturbed indicates memorization

- First 3 experiments:
  1. Reproduce GSM8K evaluation on 165-problem filtered set using provided prompts; verify coverage >0.95 before interpreting consistency
  2. Generate SynDeduct-style synthetic problems with 6-12 hops; compare decay rates between base and instruct-tuned variants of same model family
  3. Ablate prefix length (k=1,3,5) at fixed hop count (l=3) to confirm premise robustness claim; expect low variance across k values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do post-training methods like RL and SFT generalize deductive reasoning improvements to unseen problem distributions?
- Basis in paper: [explicit] The authors state: "More work is required to study the extent of generalization that such post-training methods provide" after finding that RL-tuned models show improved decay on SynDeduct but SFT models show worse performance.
- Why unresolved: The paper shows contrasting results—RL reduces decay on synthetic data but not consistently across benchmarks—without determining if this reflects task-specific pattern learning versus genuine reasoning improvement.
- What evidence would resolve it: Evaluate the same post-trained models across diverse reasoning domains (logic puzzles, code, scientific reasoning) with controlled distribution shifts to measure cross-domain transfer.

### Open Question 2
- Question: Is robustness to input premise length a general property of LLM reasoning, or an artifact of GSM8K's simplicity?
- Basis in paper: [explicit] The authors note: "A caveat is that due to the simplicity of the GSM8K problems, the maximum premise length we could evaluate on is 7."
- Why unresolved: GSM8K problems have short premise chains by design; real-world reasoning may involve substantially longer premise sequences where attention or memory limitations could degrade performance.
- What evidence would resolve it: Apply DeduCE to problems requiring longer premise chains (e.g., legal case analysis, multi-document reasoning) to establish whether premise-length robustness persists.

### Open Question 3
- Question: What training interventions specifically target multi-hop deductive reasoning decay rather than task-specific accuracy?
- Basis in paper: [inferred] The paper finds models suffer 15-30% decay from 1 to 5 hops, and post-training methods "enhance task-specific patterns rather than general deductive reasoning."
- Why unresolved: The paper identifies the problem but does not propose or test interventions designed to maintain consistency across increasing reasoning hops.
- What evidence would resolve it: Develop and evaluate training objectives that explicitly penalize hop-dependent decay (e.g., curriculum learning over hop depth, consistency regularization across partial solutions).

### Open Question 4
- Question: How do different perturbation types (variable names, irrelevant information, structural reordering) affect deductive consistency independently?
- Basis in paper: [explicit] The paper uses only value perturbations and notes: "Other perturbations, such as changing variable names and adding irrelevant info can be easily added."
- Why unresolved: It is unclear whether the observed reasoning gaps stem from numerical novelty, linguistic disruption, or attention distraction—each requiring different mitigations.
- What evidence would resolve it: Conduct ablation studies isolating each perturbation type while controlling for others, measuring their individual contributions to consistency decay.

## Limitations
- Single-reference-path dependency may under-value alternative valid reasoning approaches
- Numeric value perturbation assumes difficulty preservation, but special values may create confounds
- GSM-8K's simple problems limit premise length evaluation; synthetic dataset lacks real-world linguistic complexity

## Confidence
- **High confidence**: Framework's ability to detect memorization and general trend of larger models showing better hop resilience
- **Medium confidence**: Specific decay rates (15-30% from 1 to 5 hops) and comparative effectiveness of RL vs SFT, depending on synthetic dataset validity
- **Low confidence**: Applicability to non-arithmetic reasoning tasks due to templatization pipeline limitations

## Next Checks
1. **Alternative-path robustness test**: Generate 3-5 reference solutions per problem using different valid reasoning approaches. Measure consistency against each reference and compare coverage metrics to assess whether the single-reference limitation significantly impacts results.

2. **Perturbation structure sensitivity**: Create perturbed variants that intentionally modify problem structure (e.g., changing from addition to multiplication) alongside numeric changes. Compare consistency decay rates to isolate reasoning failure from structural comprehension issues.

3. **Multi-hop capacity threshold**: Systematically increase prefix length (k) while holding hop count (l) constant across different model sizes. Measure the point at which consistency plateaus to identify each model's effective reasoning capacity limit, validating the claimed relationship between model size and hop resilience.