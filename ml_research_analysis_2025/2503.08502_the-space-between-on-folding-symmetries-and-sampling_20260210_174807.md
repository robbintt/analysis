---
ver: rpa2
title: 'The Space Between: On Folding, Symmetries and Sampling'
arxiv_id: '2503.08502'
source_url: https://arxiv.org/abs/2503.08502
tags:
- folding
- space
- activation
- measure
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper extends a space folding measure beyond ReLU activation
  to a broader class of activation functions through equivalence classes defined by
  pre-images. It proves several theoretical properties including stability under redundant
  steps, flatness characterization, and asymmetry of folding with respect to path
  direction while showing flatness invariance.
---

# The Space Between: On Folding, Symmetries and Sampling

## Quick Facts
- arXiv ID: 2503.08502
- Source URL: https://arxiv.org/abs/2503.08502
- Reference count: 3
- Primary result: Generalizes space folding measure beyond ReLU to monotonic activations using equivalence classes; introduces parameter-free sampling and folding-based regularization

## Executive Summary
This paper extends the concept of space folding in neural networks from ReLU activations to a broader class of monotonic activation functions through equivalence classes defined by pre-images. The authors prove several theoretical properties including stability under redundant steps, flatness characterization, and asymmetry of folding with respect to path direction. They introduce a parameter-free sampling strategy that exploits the structure of equivalence classes to reduce redundant computations, and propose a novel regularization scheme that encourages higher folding values early in training. The work provides both theoretical foundations and practical methods for quantifying and utilizing space folding as a feature of neural network architecture.

## Method Summary
The method generalizes space folding by defining equivalence classes via thresholded pre-images: inputs x₁ and x₂ belong to the same class if their activation patterns have zero Hamming distance. The folding measure χ(Γ) quantifies deviation from convexity along straight-line paths by comparing Hamming distances to the start point versus cumulative distances. An adaptive step-size sampling algorithm reduces redundant computations by using Hamming distance feedback to skip samples within the same equivalence class. For regularization, the paper proposes adding λ/(Φ_N + 1)² to the loss, where Φ_N is the global folding measure computed as the average inter-class folding values.

## Key Results
- Extends space folding measure from ReLU to monotonic activations through equivalence classes
- Proves folding measure is stable under redundant steps and invariant to flatness
- Introduces parameter-free adaptive sampling that reduces redundant computations
- Shows folding is asymmetric with respect to path direction while maintaining flatness invariance
- Proposes regularization scheme encouraging higher folding values early in training

## Why This Works (Mechanism)

### Mechanism 1: Equivalence Class Partitioning for Activation Generalization
The folding measure generalizes from ReLU to any monotonic activation by reframing linear regions as equivalence classes defined by thresholded pre-images. For ReLU networks, linear regions correspond to inputs producing identical binary activation patterns. This extends by defining equivalence classes via `x₁ ~_N x₂ ⟺ d_H(π(x₁), π(x₂)) = 0`, then thresholding activations at zero to create binary patterns regardless of the underlying activation function. The partition `{(-∞, 0], (0, ∞)}` yields disjoint connected sets for monotonic activation functions.

### Mechanism 2: Hamming Distance Non-Monotonicity as Folding Detection
Space folding manifests as decreases in Hamming distance between activation patterns along straight-line input paths, violating the monotonic increase expected in Euclidean space. The measure χ(Γ) = 1 - max_i d_H(π₁, πᵢ) / Σ d_H(πᵢ, πᵢ₊₁) captures this deviation from convexity. When intermediate points are farther from π₁ than the endpoint πₙ, folding has occurred (χ > 0).

### Mechanism 3: Adaptive Step-Size Sampling via Hamming Feedback
Computational efficiency improves by using Hamming distance feedback to skip redundant samples within the same equivalence class. Algorithm 1 starts with step size Δ_init, computes π_next, and checks d_H(π_prev, π_next). If 0, skip storing; if 1, store; if >1, halve Δ until either distance becomes 1 or Δ reaches Δ_min. This ensures each stored sample represents a distinct equivalence class.

## Foundational Learning

- **Concept: Hamming distance on binary hypercubes**
  - Why needed here: The entire folding measure relies on computing d_H between activation patterns in H^N = {0,1}^N
  - Quick check question: For π₁ = (0,1,1,0) and π₂ = (1,1,0,0), what is d_H(π₁, π₂)?

- **Concept: Equivalence classes and pre-images**
  - Why needed here: The extension beyond ReLU depends on understanding how f⁻¹ partitions the input domain
  - Quick check question: For σ(x) = ReLU, describe the pre-images f⁻¹({0}) and f⁻¹((0,∞))

- **Concept: Linear regions in piecewise linear networks**
  - Why needed here: For ReLU networks, these regions are the geometric interpretation of equivalence classes; understanding them motivates the generalization
  - Quick check question: Why does a 2-layer ReLU network with n neurons per layer have more linear regions than a single layer with 2n neurons?

## Architecture Onboarding

- **Component map:** Input pair (x₁, x₂) → Straight-line interpolation → Network forward passes → Binary activation patterns (π₁, ..., πₙ) → Hamming distances → Folding measure χ(Γ) → Global measure Φ_N (median of inter-class χ values)

- **Critical path:**
  1. Implement `GetActivationPattern(x)` that returns binary vector by thresholding all hidden layer activations
  2. Implement Algorithm 1's adaptive step-size loop with Δ_init ≈ 0.1, Δ_min ≈ 10⁻⁶
  3. Compute χ(Γ) via Equation 3 using stored activation patterns
  4. For global measure Φ_N: sample pairs across classes, compute χ, take median of non-zero values per class pair, average

- **Design tradeoffs:**
  - **Δ_init vs. Δ_min**: Larger Δ_init speeds up sampling but risks missing small equivalence regions; smaller Δ_min catches more boundaries but increases compute
  - **Exact vs. smooth maximum**: For regularization, replacing max with log-sum-exp enables differentiability but introduces temperature parameter β
  - **Pairwise sampling vs. class-based**: Computing all O(n²) pairs is prohibitive; class-based aggregation provides tractable approximation

- **Failure signatures:**
  - χ stuck at 0 despite varied input pairs → check if equivalence classes are actually changing (possible if network is essentially linear)
  - Excessive refinement (M → ∞) → Δ_min too small or pathological network with dense boundary regions
  - Asymmetric χ(Γ) ≠ χ(-Γ) with large discrepancy → expected behavior, but extreme asymmetry may indicate unstable regions

- **First 3 experiments:**
  1. **Validation on known architecture**: Train small ReLU MLP on MNIST, compute Φ_N at initialization, early training, and convergence. Verify that Φ_N increases as generalization error decreases.
  2. **Activation function comparison**: Train identical architectures with ReLU, GELU, and Swish on same task. Compare Φ_N values to test whether folding patterns are architecture-invariant.
  3. **Regularization pilot**: Implement the proposed loss modification `Loss + λ/(Φ_N + 1)²` with λ ∈ {0.01, 0.1, 1.0}. Monitor both generalization error and Φ_N over training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed folding-based regularization scheme effectively improve generalization performance when integrated into gradient-based training?
- Basis in paper: Section 7 states the regularization procedure "remains untested" and currently requires optimization regarding the frequency of calculation (every $n$ epochs).
- Why unresolved: The paper outlines the theoretical formulation using a log-sum-exp approximation but provides no experimental results validating its impact on convergence or accuracy.
- What evidence would resolve it: Empirical training curves comparing validation accuracy and loss landscapes of models trained with and without the folding regularization term.

### Open Question 2
- Question: Can the interaction effect (deviation from additivity) serve as a quantitative metric for detecting or analyzing adversarial attacks?
- Basis in paper: Section 7 notes that the interaction effect $I(\Gamma_1, \Gamma_2)$ was defined but not used, and the authors intend to evaluate it specifically in the "context of adversarial attacks."
- Why unresolved: While the measure is mathematically defined, its sensitivity to adversarial perturbations versus standard data variations is unknown.
- What evidence would resolve it: Experiments measuring $I(\Gamma_1, \Gamma_2)$ on paths involving adversarial examples to see if they exhibit distinct deviation profiles compared to benign samples.

### Open Question 3
- Question: Does the global folding measure $\Phi_N$ converge to a constant determined solely by the network architecture as dataset size and width approach infinity?
- Basis in paper: Section 4.4 posits that $\Phi_N$ is a feature of the architecture rather than dataset size, hypothesizing convergence to a constant $const(N)$ as dimensions grow.
- Why unresolved: This is stated as a posit based on prior observations in smaller networks, but rigorous theoretical proof or large-scale empirical verification is absent.
- What evidence would resolve it: Scaling laws showing the variance of $\Phi_N$ decreasing as the number of samples and neurons increases, stabilizing at a value characteristic of the depth.

## Limitations

- **Activation generalization gap**: The paper assumes monotonic activation functions yield disjoint partitions `{(-∞, 0], (0, ∞)}` that behave like ReLU's binary regions. For non-monotonic activations, this assumption breaks, potentially invalidating the entire extension mechanism.
- **Computational scalability**: While Algorithm 1 reduces redundant samples, the pairwise comparison complexity O(n²) for global folding Φ_N remains prohibitive for large architectures.
- **Regularization efficacy uncertainty**: The proposed loss modification `λ/(Φ_N + 1)²` lacks rigorous theoretical grounding—the paper states it "encourages higher folding values" but doesn't prove this improves generalization beyond empirical observation.

## Confidence

- **Extension to general activations**: Medium confidence. The mathematical framework is sound, but relies on unverified assumptions about monotonic activation behavior.
- **Adaptive sampling efficiency**: High confidence. Algorithm 1's complexity analysis and comparison to Gamba et al.'s method provide strong empirical justification.
- **Regularization mechanism**: Low confidence. The connection between early folding induction and improved generalization is stated but not rigorously established.

## Next Checks

1. **Activation generalization test**: Train identical architectures (e.g., 3-layer MLPs) with ReLU, GELU, Swish, and a non-monotonic activation (e.g., sin) on MNIST. Compare Φ_N values and verify monotonic activations maintain disjoint partitions while non-monotonic activations fail to produce valid equivalence classes.

2. **Sampling efficiency benchmark**: Implement both Algorithm 1 and Gamba et al.'s direction-based sampling on a 5-layer ReLU network. Measure computational time and boundary detection accuracy across varying network widths and input path complexities.

3. **Regularization ablation study**: Train ResNet-18 variants on CIFAR-10 with the proposed loss modification. Vary λ ∈ {0.01, 0.1, 1.0} and measure generalization error, Φ_N progression, and final accuracy. Include control conditions: fixed Φ_N target, no regularization, and L2 regularization for baseline comparison.