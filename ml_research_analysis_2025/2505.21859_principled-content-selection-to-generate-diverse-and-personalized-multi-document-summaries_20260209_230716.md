---
ver: rpa2
title: Principled Content Selection to Generate Diverse and Personalized Multi-Document
  Summaries
arxiv_id: '2505.21859'
source_url: https://arxiv.org/abs/2505.21859
tags:
- points
- coverage
- source
- user
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating diverse and personalized
  multi-document summaries, particularly when large language models exhibit attention
  biases that lead to uneven coverage of source material. The proposed solution involves
  a three-step pipeline: extracting atomic key points from each document, selecting
  key points using determinantal point processes (DPPs) to prioritize diversity and
  relevance, and rewriting the selected points into a coherent summary.'
---

# Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries

## Quick Facts
- arXiv ID: 2505.21859
- Source URL: https://arxiv.org/abs/2505.21859
- Authors: Vishakh Padmakumar; Zichao Wang; David Arbour; Jennifer Healey
- Reference count: 25
- Primary result: LLM-based pipeline using DPP selection improves source coverage across multiple models on DIVERSESUM benchmark

## Executive Summary
This paper addresses the challenge of generating diverse and personalized multi-document summaries when large language models exhibit attention biases that lead to uneven coverage of source material. The proposed solution involves a three-step pipeline: extracting atomic key points from each document, selecting key points using determinantal point processes (DPPs) to prioritize diversity and relevance, and rewriting the selected points into a coherent summary. This principled content selection approach consistently improves source coverage on the DIVERSESUMM benchmark across multiple LLMs (GPT-3.5, GPT-4o, Claude-3-Sonnet, and Llama 3.1), outperforming both naive and LLM-only baselines. By incorporating user intent into the DPP kernel, the method also generates personalized summaries with better relevance while maintaining high coverage.

## Method Summary
The method decomposes into three components: (1) atomic key point extraction using LLM prompts to convert context-dependent sentences into context-independent claims, (2) DPP-based selection using DeBERTa-V3 embeddings and BertScore similarity with greedy MAP inference to maximize diversity, and (3) rewriting selected key points into a coherent summary using LLM prompts. For personalization, the DPP kernel incorporates relevance scores from an instruction-tuned retrieval model. The approach addresses LLM attention biases by explicitly optimizing for coverage rather than relying on end-to-end summarization.

## Key Results
- LLM+DPP consistently improves source coverage across GPT-3.5, GPT-4o, Claude-3-Sonnet, and Llama 3.1 on DiverseSumm benchmark
- The method mitigates positional attention biases, achieving more uniform coverage across source documents
- Personalization via relevance-weighted DPP kernels improves relevance while maintaining coverage
- Key points outperform extracted sentences for selection (0.5805 vs 0.5090 coverage on GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing documents into atomic key points before selection improves source coverage in multi-document summarization.
- Mechanism: The extraction step converts context-dependent sentences into context-independent atomic claims, which serve as more reliable units for selection. This transforms the problem from selecting coherent text spans to selecting discrete information units, making the optimization space more tractable for DPP-based diversity maximization. The rewrite step then handles coherence generation separately.
- Core assumption: LLMs can reliably extract atomic, context-independent key points that preserve the essential information from source documents (assumption based on prior work citations: Kim et al., 2024; Krishna et al., 2023).
- Evidence anchors:
  - [abstract]: "first extracts atomic key points from each document using LLMs"
  - [section 3.1]: "Prior work has demonstrated that LLMs reliably break down individual documents into atomic claims or key points via a zero-shot prompt"
  - [corpus]: Limited direct corpus support for key point extraction efficacy; related work on medical MDS mentions hierarchical organization but not atomic decomposition specifically.

### Mechanism 2
- Claim: DPP-based selection mitigates LLM positional attention biases and improves uniform coverage across source documents.
- Mechanism: DPPs model subset selection probability to maximize determinants of similarity kernels, explicitly favoring diverse subsets. By constructing the kernel matrix L from embedding similarity (BertScore with DeBERTa-V3), the method selects key points that span the semantic space rather than clustering around prominent themes. This counters LLMs' tendency to overweight start/end context positions.
- Core assumption: Embedding similarity correlates with information redundancy—similar embeddings indicate overlapping content worth diversifying away from.
- Evidence anchors:
  - [abstract]: "selects diverse and relevant key points using determinantal point processes (DPPs)"
  - [section 5.1, Figure 2]: "While Naive LLM exhibits biases to better cover the articles at the start (GPT-4o, GPT-3.5) or end (Llama) of the context, LLM + DPP has higher and more uniform coverage"

### Mechanism 3
- Claim: Incorporating relevance scoring into the DPP kernel enables personalized summaries that balance diversity with user intent alignment.
- Mechanism: The modified kernel L' = RLRT multiplies the similarity matrix by diagonal relevance matrix R (computed via instruction-tuned retrieval model e5-mistral-7b-instruct). This transforms DPP optimization to maximize both diversity (through L) and relevance (through R weighting), treating personalization as a joint objective rather than post-hoc filtering.
- Core assumption: Relevance scores from the retrieval model accurately reflect user intent alignment, and the multiplicative combination appropriately trades off diversity vs. relevance.
- Evidence anchors:
  - [abstract]: "incorporating relevance to a provided user intent into the DPP kernel, we can generate personalized summaries"
  - [section 3.2.3]: "L'(i1,j1),(i2,j2) = frel(vi1j1|quser) × fk(vi1j1, vi2j2) × frel(vi2j2|quser)"

## Foundational Learning

- Concept: **Determinantal Point Processes (DPPs)**
  - Why needed here: Core mathematical machinery for diversity-aware subset selection; understanding how determinants of similarity kernels relate to subset diversity is essential for implementing and tuning the selection step.
  - Quick check question: Given a 3×3 kernel matrix with high diagonal (quality) and low off-diagonal (dissimilarity), would a DPP prefer selecting all 3 items or a subset? Why?

- Concept: **"Lost in the Middle" Phenomenon**
  - Why needed here: Motivates the entire approach—understanding why naive LLM prompting fails on multi-document tasks explains why explicit content selection is necessary.
  - Quick check question: If you concatenate 10 documents and ask an LLM to summarize, which documents' information is most likely to be underrepresented in the output?

- Concept: **Query-Focused vs. Generic Summarization**
  - Why needed here: The personalization extension requires understanding how user intent modifies the selection objective; distinguishes between coverage-maximizing and relevance-maximizing goals.
  - Quick check question: How does the evaluation metric change between MDDS (Section 2) and query-focused MDDS?

## Architecture Onboarding

- Component map: Key Point Extraction -> Embedding Layer -> DPP Selector -> Relevance Scorer (optional) -> Rewriter

- Critical path: Extraction quality → embedding fidelity → kernel construction (σ parameter in Gaussian kernel) → DPP inference → rewrite coherence. Errors propagate; extraction failures cannot be recovered downstream.

- Design tradeoffs:
  - **Kernel choice**: Gaussian (σ-tuned) vs. Linear—Table 2 shows σ=1 performs best for most models, but optimal σ varies
  - **Selection granularity**: Key points vs. extracted sentences—Appendix C.2 shows key points outperform sentences (0.5805 vs 0.5090 coverage on GPT-4o)
  - **Exact vs. k-DPP sampling**: Exact sampling adapts subset size to data; k-DPP fixes it—Figure 6 shows exact outperforms fixed-k

- Failure signatures:
  - **Low coverage despite DPP**: Check embedding quality; if key points from different documents cluster tightly, DPP treats them as redundant
  - **Over-long summaries**: Claude produces longer outputs than other LLMs (Table 3)—may indicate rewrite prompt needs model-specific tuning
  - **Poor personalization**: If LLM+DPP-Relevance underperforms LLM+DPP, relevance scores may be noisy; check e5-mistral calibration

- First 3 experiments:
  1. **Ablate extraction quality**: Use a weaker LLM for key point extraction but stronger LLM for rewriting; quantify extraction vs. selection vs. rewriting contribution
  2. **Stress test DPP on redundant sources**: Create synthetic document sets with controlled overlap ratios; verify DPP selection correlates with ground-truth unique information
  3. **Calibrate relevance weighting**: Vary the threshold for relevance scoring (currently 0.6 in Appendix B.2) and measure diversity-relevance tradeoff curves; identify domain-specific optimal points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM attention biases (e.g., "lost in the middle") interact with information relevance to user intents during summary generation?
- Basis in paper: [explicit] The authors state: "There exists an open question to investigate how the attention biases of LLMs interact with information relevance to user intents when generating summaries."
- Why unresolved: This work focuses on diversity-driven selection and only begins exploring relevance via a simple DPP kernel modification; the interplay between positional attention bias and relevance scoring remains uncharacterized.
- What evidence would resolve it: A controlled study comparing coverage of query-relevant content placed at different context positions, with and without principled content selection.

### Open Question 2
- Question: Does principled content selection with DPPs generalize to domains beyond news and to non-English languages?
- Basis in paper: [explicit] The limitations section states: "It is yet unclear if our findings would generalize beyond the news domain, and to other languages."
- Why unresolved: Experiments were conducted only on DiverseSumm, a news-domain benchmark in English; key point extraction quality and embedding-based similarity may behave differently in other domains/languages.
- What evidence would resolve it: Evaluation on multi-document summarization benchmarks from other domains (e.g., scientific papers, legal documents) and multilingual datasets.

### Open Question 3
- Question: Can reliability or source trustworthiness be incorporated into the DPP kernel without sacrificing coverage?
- Basis in paper: [explicit] The authors note: "We select key points purely based on diversity—we do not incorporate any information about the reliability of the particular news articles."
- Why unresolved: Current selection optimizes for diversity and relevance but treats all sources equally; real-world deployment requires filtering or down-weighting unreliable information.
- What evidence would resolve it: Extending the DPP kernel with a quality/reliability term and measuring trade-offs between coverage and factuality/factual consistency.

### Open Question 4
- Question: Why do different LLMs exhibit divergent summary length behaviors in the rewriting step, and can this be controlled?
- Basis in paper: [explicit] In Section 5.1, the authors observe Claude produces significantly different summary lengths than other LLMs and note: "this differing behavior... presents a direction for future exploration."
- Why unresolved: The cause (model training, instruction tuning, or context handling) is unknown, and its impact on downstream coverage metrics is unquantified.
- What evidence would resolve it: Systematic analysis of rewriting behavior across LLMs with controlled key point inputs and explicit length constraints.

## Limitations

- Key point extraction quality critically depends on LLM instruction-following ability and may introduce hallucinations that propagate through selection.
- The approach requires significant computational overhead (multiple LLM calls per document) compared to naive prompting.
- The DeBERTa-V3 embeddings may not capture all semantic nuances, particularly for nuanced personalization tasks where the relevance model's calibration becomes crucial.

## Confidence

- **High Confidence**: The improvement in source coverage when comparing LLM+DPP to naive LLM baselines across multiple models and datasets (Table 1, Figure 2)
- **Medium Confidence**: The personalization mechanism's effectiveness (smaller sample size, synthetic data in DiverseSumm Relevance)
- **Medium Confidence**: The superiority of key points over extracted sentences for selection (Appendix C.2, limited comparison)

## Next Checks

1. **Extraction Quality Audit**: Manually evaluate 50 randomly sampled key points from different LLMs for atomicity, completeness, and hallucination presence. Correlate extraction quality scores with downstream coverage improvements.

2. **Generalization Test**: Apply the LLM+DPP pipeline to a different multi-document summarization domain (e.g., scientific literature or legal documents) to assess robustness beyond news summarization.

3. **Component Ablation Study**: Systematically ablate each pipeline component (extraction, DPP selection, rewriting) using a smaller dataset with ground-truth key points to isolate individual contributions to the final performance.