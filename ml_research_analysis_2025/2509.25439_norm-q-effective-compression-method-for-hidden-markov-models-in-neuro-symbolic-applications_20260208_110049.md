---
ver: rpa2
title: 'Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic
  Applications'
arxiv_id: '2509.25439'
source_url: https://arxiv.org/abs/2509.25439
tags:
- quantization
- norm-q
- rate
- hidden
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Norm-Q proposes a normalized quantization method for compressing
  hidden Markov models (HMMs) in neuro-symbolic applications. Traditional quantization
  and pruning methods fail for probabilistic models due to semantic weight preservation
  requirements.
---

# Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications

## Quick Facts
- arXiv ID: 2509.25439
- Source URL: https://arxiv.org/abs/2509.25439
- Authors: Hanyuan Gao; Xiaoxuan Yang
- Reference count: 27
- One-line primary result: 4096-hidden-state HMM compressed to 8 bits with no success rate loss and 3 bits with minimal degradation (<1% rate loss, <2.9% score loss)

## Executive Summary
Norm-Q introduces a normalized quantization method for compressing hidden Markov models in neuro-symbolic applications where traditional quantization fails due to probabilistic semantics. The method applies fixed-point linear quantization with row-wise normalization to maintain probability distribution integrity while achieving over 99% compression. Unlike standard pruning or quantization that work for neural networks, Norm-Q addresses the unique challenge that HMM weight values are semantically meaningful as probabilities, requiring preservation of distribution properties rather than just numerical precision.

## Method Summary
Norm-Q applies fixed-point linear quantization followed by row-wise normalization to HMM transition (α), emission (β), and initial distribution (γ) matrices. The quantization uses Q(p) = clip[round(p × (2^b - 1))] / 2^b, then applies α_ij ← (α_ij + ε) / Σ_j(α_ij + ε) with ε = 1e-12 to ensure each row sums to 1. Norm-Q aware EM extends this by quantizing periodically during training (every 20 steps) rather than only post-training. The method targets neuro-symbolic systems where HMMs provide constrained generation guidance to large language models, addressing memory bottlenecks while preserving generation quality and logical constraints.

## Key Results
- 4096-hidden-state HMM compressed to 8 bits with no success rate loss
- 3-bit quantization achieves <1% success rate loss and <2.9% average score loss
- Overall compression exceeds 99% while maintaining generation quality
- Scalability demonstrated for larger HMMs (8192, 16384 states) with <4% degradation
- Row normalization prevents empty-row generation failures that cause garbled output

## Why This Works (Mechanism)

### Mechanism 1
Row-wise normalization after quantization prevents probability distribution collapse and empty-row generation failures. After fixed-point linear quantization rounds values (potentially to zero), row-wise normalization redistributes probability mass so each row sums to 1. The operation is α_ij ⇐ (α_ij + ε_j) / Σ_j(α_ij + ε_j), where ε prevents division by zero. Core assumption: Probability distributions in HMMs require row-wise integrity (sum-to-one property) more than absolute value precision. Evidence: 8-bit quantization shows <1% success rate loss with row normalization; 70% rate loss without it.

### Mechanism 2
Fixed-point linear quantization combined with row normalization implicitly extends the effective codebook size without additional storage overhead. Linear quantization maps values to discrete fixed points. Row-wise normalization shifts these points differently per row, creating row-specific quantization grids. The effective codebook far exceeds 2^b fixed points while storing only b-bit indices. Core assumption: The combined quantize-then-normalize operation preserves sufficient information for downstream tasks even though it is not mathematically invertible. Evidence: 8-bit quantization maintains generation quality despite 256-fold reduction in storage.

### Mechanism 3
Integrating quantization into EM training (Norm-Q aware EM) improves final model likelihood compared to post-training quantization alone. Quantization is applied periodically after the M-step of EM training, constraining weights to the quantized cookbook during optimization. The quantization interval controls the trade-off between training stability and compression alignment. Core assumption: The quantization interval acts as a regularizer and convergence stabilizer; too frequent quantization prevents convergence, too infrequent leaves large quantization gaps. Evidence: Interval=20 balances convergence stability and quantization alignment; likelihood increases with interval up to threshold.

## Foundational Learning

- **Hidden Markov Model Components (transition matrix α, emission matrix β, initial distribution γ)**: Why needed here: Norm-Q quantizes these three matrices differently based on their dimensions and semantic roles. Understanding that each row represents a probability distribution is essential for grasping why row-wise normalization is necessary. Quick check question: If you quantize a transition matrix row to [0, 0, 0, 0], what happens to the HMM's ability to transition from that state?

- **Linear vs. Non-linear Quantization**: Why needed here: The paper rejects K-means clustering (non-linear) and selects fixed-point linear quantization. Understanding the difference clarifies why linear quantization is reversible-compatible with layer-wise operations and more hardware-friendly. Quick check question: Why can't K-means centroids be used directly in a layer-wise quantization scheme that requires dequantification?

- **Expectation-Maximization (EM) for HMM Training**: Why needed here: Norm-Q aware EM modifies standard EM by quantizing after the M-step. Understanding the E-step (computing expected sufficient statistics) and M-step (maximizing likelihood) clarifies where quantization fits. Quick check question: In Norm-Q aware EM, quantization happens after the M-step. What would go wrong if you quantized after the E-step instead?

## Architecture Onboarding

- **Component map**: Neural part (GPT-2 large, 774M params) -> Symbolic part (HMM, 223M params at 4096 hidden size + DFA for constraints) -> Interface (HMM provides logits adjustment for LLM token generation; DFA enforces keyword constraints)
- **Critical path**: Memory bandwidth for HMM weight matrices during constrained generation. The forward algorithm requires accessing transition and emission matrices per token. Latency scales 2x when HMM hidden size doubles (vs. 1.45x for LLM parameter doubling).
- **Design tradeoffs**: Bit-width vs. success rate: 8-bit = no loss; 3-bit = ~3% loss; 2-bit = ~8% loss. Quantization interval vs. training time: Interval=20 balances convergence stability and quantization alignment. Pruning ratio vs. stability: 85% pruning tolerable; 86% causes catastrophic failure (empty rows).
- **Failure signatures**: Garbled output: Typically indicates empty rows in emission matrix (state cannot generate tokens). Success rate drops to 0: Pruning exceeded threshold; entire rows zeroed out. >70% success rate degradation at 8-bit: Using integer quantization without row normalization.
- **First 3 experiments**: 1. Profile baseline latency: Run the neuro-symbolic pipeline (GPT-2 + HMM + DFA) with FP32 HMM weights. Measure per-token latency and operator breakdown using PyTorch profiler. Target: Confirm memory operations dominate symbolic part (>90%). 2. Ablate quantization methods: Compare (a) integer quantization, (b) K-means clustering, (c) fixed-point linear quantization, (d) Norm-Q at 8-bit and 4-bit. Measure success rate and generation scores (ROUGE, BLEU4, CIDER, SPICE). Target: Replicate that Norm-Q achieves <1% loss at 8-bit while integer quantization fails. 3. Test scalability: Train HMMs with hidden sizes 4096, 8192, 16384. Apply Norm-Q at 8-bit and 3-bit. Verify that success rate degradation remains <4% even at largest scale.

## Open Questions the Paper Calls Out

### Open Question 1
How can the neural and symbolic components be jointly optimized to enhance overall system performance? Basis: The conclusion identifies "co-optimization of the neural and symbolic parts" as necessary to investigate the interactions and synergy between the two components. Unresolved because the current work treats compression of the symbolic HMM as an optimization step orthogonal to the neural network (LLM) component. Evidence needed: A training framework that updates both the LLM and HMM simultaneously, demonstrating improved convergence rates or task accuracy compared to the independent optimization approach.

### Open Question 2
What specific hardware architectures are required to maximize the efficiency of Norm-Q compressed models? Basis: The conclusion states that "compression and quantization like Norm-Q require dedicated hardware support." Unresolved because experimental results are based on software simulations using PyTorch on standard high-performance computing clusters rather than custom hardware. Evidence needed: An FPGA or ASIC implementation that validates the actual latency reduction and energy efficiency gains of the row-normalized quantization format.

### Open Question 3
Can the Norm-Q aware Expectation Maximization process be stabilized to consistently outperform post-training quantization? Basis: The authors note that Norm-Q aware EM suffers from "unstable convergence" because linear quantization is a "discontinuous and aggressive operation," resulting in performance levels similar to standard post-training Norm-Q. Unresolved because quantization-aware training is generally expected to yield better accuracy than post-training methods, but the aggressive rounding in Norm-Q currently negates this benefit. Evidence needed: A modified training algorithm that smooths the quantization step, achieving statistically higher test likelihoods and generation scores than the post-training baseline.

## Limitations
- The claim that row-wise normalization implicitly extends the quantization codebook size lacks empirical validation through ablation studies
- The neuro-symbolic application appears highly specialized (keyword-constrained generation), limiting generalizability to other HMM use cases
- The paper does not provide error bounds or variance measures for the reported success rates and generation scores

## Confidence

1. **Effectiveness of Row-Wise Normalization**: Medium - Supported by theoretical reasoning and ablation showing 70% success rate drop without normalization, but lacks comparative analysis with alternative normalization approaches.
2. **Quantization Quality vs. Bit-Depth**: High - Multiple quantization levels (8-bit, 3-bit, 2-bit) show clear trade-offs between compression and performance degradation.
3. **Scalability to Larger HMMs**: Medium - Demonstrates 4096→16384 state scaling but only reports final performance without intermediate validation points or runtime analysis.
4. **Norm-Q Aware EM Benefits**: Low - Single experiment with one hyperparameter sweep; no comparison to alternative quantization-aware training methods.

## Next Checks

1. **Statistical Validation of Success Rate Claims**: Re-run the 8-bit quantization experiment across 5 different random seeds for HMM training and report mean ± standard deviation for success rate and generation scores. This addresses whether the <1% loss claim is statistically significant.

2. **Comparative Normalization Analysis**: Implement and compare three variants: (a) no normalization, (b) row-wise normalization (Norm-Q), (c) column-wise normalization. Evaluate all three on 8-bit quantization with the same HMM model to isolate the impact of normalization direction on generation quality.

3. **Cross-Domain HMM Application Test**: Apply Norm-Q to a standard HMM benchmark (e.g., UCI speech recognition dataset) rather than the neuro-symbolic application. Measure recognition accuracy vs. quantization bit-depth to assess generalizability beyond constrained text generation.