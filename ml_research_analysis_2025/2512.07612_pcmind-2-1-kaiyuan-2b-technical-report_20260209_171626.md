---
ver: rpa2
title: PCMind-2.1-Kaiyuan-2B Technical Report
arxiv_id: '2512.07612'
source_url: https://arxiv.org/abs/2512.07612
tags:
- data
- training
- score
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of training high-performance
  large language models under limited computational resources and heterogeneous open-source
  data. The authors introduce Kaiyuan-2B, a 2-billion-parameter fully open-source
  model, employing three key innovations: Quantile Data Benchmarking for systematic
  comparison of heterogeneous datasets, Strategic Selective Repetition to leverage
  sparse high-quality data across multiple training phases, and Multi-Domain Curriculum
  Training that orders samples by quality within each phase.'
---

# PCMind-2.1-Kaiyuan-2B Technical Report

## Quick Facts
- arXiv ID: 2512.07612
- Source URL: https://arxiv.org/abs/2512.07612
- Reference count: 40
- Kaiyuan-2B is a 2B-parameter fully open-source LLM trained on heterogeneous data using three key innovations: Quantile Data Benchmarking, Strategic Selective Repetition, and Multi-Domain Curriculum Training

## Executive Summary
This paper presents Kaiyuan-2B, a 2-billion-parameter fully open-source language model designed to address the challenge of training high-performance LLMs under limited computational resources and heterogeneous open-source data. The authors introduce three key innovations: Quantile Data Benchmarking for systematic dataset comparison, Strategic Selective Repetition to leverage sparse high-quality data across multiple training phases, and Multi-Domain Curriculum Training that orders samples by quality within each phase. Trained on a heterogeneous corpus of ~2.2T tokens across five phases, Kaiyuan-2B achieves competitive performance with state-of-the-art fully open-source models, surpassing the frontier in Chinese, math, and coding capabilities while approaching open-weight models like Qwen2-1.5B.

## Method Summary
The authors train Kaiyuan-2B using a five-phase curriculum approach on heterogeneous open-source data. They employ Quantile Data Benchmarking to compare dataset quality, Strategic Selective Repetition to leverage high-quality data across multiple phases, and Multi-Domain Curriculum Training with quality-ascending order within phases 3-5. The model uses a Qwen3-based architecture with modifications including sandwich normalization, logits soft-capping, and QK-norm for FP16 stability. Training occurs on Huawei Ascend 910A with 1.4B non-embedding parameters, using AdamW optimizer and dynamic loss scaling. The final model averages the last 8 checkpoints.

## Key Results
- Achieves competitive performance with state-of-the-art fully open-source models
- Surpasses frontier in Chinese, math, and coding capabilities
- Approaches open-weight models like Qwen2-1.5B
- Demonstrates superior architectural efficiency with 1.4B non-embedding parameters

## Why This Works (Mechanism)

### Mechanism 1: Quantile Data Benchmarking
Evaluating data partitions by quality-score quantiles, rather than just top-k filtering, provides granular insight into dataset characteristics, enabling more informed data mixing. The method selects data subsets around target quality score quantiles and trains small reference models on them. Performance differences across these probes reveal task-dependent superiority and non-monotonic quality-performance relationships.

### Mechanism 2: Strategic Selective Repetition
Mild repetition of high-quality data subsets across multi-phase training improves efficiency over single-pass regimes without degradation. High-quality samples are repeated in multiple training phases while low-quality samples appear once, selectively amplifying scarce, high-value data.

### Mechanism 3: Sandwich Normalization and Logits Soft-Capping
Architectural modifications to bound activations enable stable FP16-only training on hardware like Ascend 910A. Logits Soft-Capping squashes logits via tanh into a fixed range, and Sandwich Normalization adds normalization to sub-layer outputs before residual addition, preventing unbounded activation growth.

## Foundational Learning

- **Concept: FP16 Dynamic Range Limits**
  - **Why needed here:** The Ascend 910A accelerator only supports FP16, which has a limited numerical range. Unchecked activation growth leads to overflow, underflow, and training instability.
  - **Quick check question:** Can you explain why a dynamic loss scaler might fail to compensate if activations grow unboundedly?

- **Concept: Curriculum Learning in Pretraining**
  - **Why needed here:** The paper employs a multi-dataset, quality-ordered curriculum within phases 3–5. Understanding curriculum learning is essential to implement this correctly.
  - **Quick check question:** What is the expected behavior of training loss when higher-quality data is presented later in training?

- **Concept: Data Deduplication for LLM Pretraining**
  - **Why needed here:** Deduplication is performed before quantile benchmarking and is a core part of the data pipeline. Understanding its role helps prevent over-representation of repetitive content.
  - **Quick check question:** Why might deduplication increase the value of repeating high-quality data samples?

## Architecture Onboarding

- **Component map:**
  Kaiyuan-Spark (Spark-based framework) -> Chukonu (C++ acceleration) -> Deduplication -> Quantile Benchmarking -> Phase-wise Mixture Construction -> Multi-Dataset Curriculum Construction -> Qwen3-based architecture (modified) -> RMSNorm -> RoPE -> SwiGLU -> Sandwich Normalization -> Logits Soft-Capping -> QK-Norm -> Ascend 910A cluster -> FP16 mixed precision -> Dynamic loss scaling -> 5-phase training schedule

- **Critical path:**
  1. Data selection and deduplication using Kaiyuan-Spark
  2. Quantile benchmarking to inform mixing ratios
  3. Constructing multi-phase, multi-dataset curriculum with selective repetition
  4. Training with stability modifications on FP16-only hardware
  5. Final model averaging over last checkpoints

- **Design tradeoffs:**
  - Efficiency vs. Stability: FP16-only hardware necessitates stability modifications which may introduce minor compute overhead
  - Quality vs. Generalization: Aggressive filtering and repetition of high-quality data can boost specific benchmarks but risks overfitting
  - Compute Cost vs. Insight: Quantile benchmarking adds ~0.6% overhead but provides critical data insights

- **Failure signatures:**
  - Training loss spikes or NaNs: Check FP16 overflow; ensure soft-capping and sandwich normalization are correctly implemented
  - Sudden validation loss increases during curriculum phases: Likely due to distribution shift from curriculum ordering
  - Poor performance despite high-quality data: Verify quality scores are correctly aligned with task objectives
  - Curriculum implementation failures causing unstable mixtures: Verify Algorithm 1 produces proportional interleaving

- **First 3 experiments:**
  1. Small-scale quantile benchmarking: Select a target dataset, create 5 quantile-based probing datasets, train 0.5B-parameter reference models on each, and evaluate on core benchmarks
  2. Repetition ablation: Train 1.5B-parameter models on a 30B-token subset with varying top-k retention ratios and epochs
  3. Curriculum vs. Uniform comparison: Train two 1.5B-parameter models—one with multi-dataset curriculum and another with uniform shuffling—using similar token budgets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-grained, quantitative frameworks be developed to optimize dataset comparison and mixing ratios beyond heuristic methods?
- **Basis in paper:** Section 3.1.3 states the current analysis "remains primarily qualitative and coarse-grained" and that "More fine-grained, quantitative frameworks for dataset comparison and mixing ratio optimization represent promising directions for future research."
- **Why unresolved:** The paper successfully demonstrates heuristic strategies but does not propose or validate an automated, mathematical optimization framework for determining these ratios.
- **What evidence would resolve it:** A comparative study where a quantitative, optimization-based mixing strategy outperforms the heuristic ratios used in Kaiyuan-2B on similar benchmarks.

### Open Question 2
- **Question:** What is the optimal composition of validation sets for monitoring multi-domain curriculum training?
- **Basis in paper:** Section 4.3 notes "anomalous increases" in validation loss during phases 3–4, which "likely reflect domain misalignment between the validation set (primarily English text) and training data."
- **Why unresolved:** The authors used a standard DCLM-Baseline subset for validation, which failed to accurately track progress when the training distribution shifted toward code and math.
- **What evidence would resolve it:** Training runs monitored by multi-domain validation sets showing consistent loss decay, contrasting with the instability observed with single-domain validation sets.

### Open Question 3
- **Question:** Why do standard quality metrics exhibit non-monotonic relationships with specific downstream capabilities?
- **Basis in paper:** Section 3.1.2 observes that increasing quality scores can "paradoxically lead to decreased performance on HellaSwag and PIQA," calling into question the "universal applicability of quality metrics."
- **Why unresolved:** The paper identifies the phenomenon but does not isolate the specific semantic features or distributional biases in the high-score quantiles that cause performance degradation in reasoning tasks.
- **What evidence would resolve it:** A feature attribution analysis linking high classifier scores to the suppression of specific reasoning patterns required for benchmarks like PIQA.

## Limitations
- Critical implementation specifics missing including MinHash deduplication parameters and cross-dataset deduplication scope
- Quality score normalization/alignment across heterogeneous datasets with different metric sources is only partially described
- Custom Kaiyuan-Spark and Chukonu frameworks are not publicly documented, making exact pipeline replication challenging
- Complete μP (maximal update parameterization) configuration from the base model is not fully specified

## Confidence

- **High Confidence:** The core architectural modifications (sandwich normalization, logits soft-capping, QK-norm) are well-described with implementation details and their stability benefits are clearly demonstrated through activation norm monitoring
- **Medium Confidence:** The three key innovations are conceptually sound and supported by ablation studies, but the exact implementation details for quality score normalization and curriculum ordering have some gaps
- **Medium Confidence:** The evaluation results showing Kaiyuan-2B's competitive performance against baselines are reproducible given the correct datasets and training setup, though exact validation set composition and generation parameters for math/code evaluation are not fully specified

## Next Checks

1. **Implementation Validation:** Replicate the small-scale quantile benchmarking experiment (0.5B parameter models on 5 quantile datasets) to verify the quality-performance relationship patterns described in the paper, particularly the non-monotonic behavior

2. **Stability Verification:** Implement and test the sandwich normalization and logits soft-capping modifications on a smaller model (1.5B) to confirm FP16 training stability, monitoring activation L1 norms as specified to ensure they remain within the safe ranges described

3. **Curriculum Effectiveness:** Conduct the curriculum vs. uniform comparison experiment with 1.5B parameter models to validate whether the quality-ordered training within phases 3-5 provides measurable benefits over random shuffling for reasoning and knowledge benchmarks