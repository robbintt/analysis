---
ver: rpa2
title: 'CafeQ: Calibration-free Quantization via Learned Transformations and Adaptive
  Rounding'
arxiv_id: '2511.19705'
source_url: https://arxiv.org/abs/2511.19705
tags:
- quantization
- matrices
- learning
- matrix
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CafeQ, a calibration-free post-training quantization
  method for large language models that learns optimal linear transformations to mitigate
  quantization errors without requiring calibration data. The core idea is to use
  structured matrix transformations and adaptive rounding to minimize quantization
  loss, particularly for handling outliers and coupled matrices.
---

# CafeQ: Calibration-free Quantization via Learned Transformations and Adaptive Rounding

## Quick Facts
- arXiv ID: 2511.19705
- Source URL: https://arxiv.org/abs/2511.19705
- Reference count: 37
- Key outcome: 4-bit quantization of Gemma 2 9B achieves 62.4 average benchmark score (vs 61.9 baseline), with <3% computation overhead

## Executive Summary
CafeQ introduces a calibration-free post-training quantization method for large language models that learns optimal linear transformations to mitigate quantization errors without requiring calibration data. The method uses structured matrix transformations and adaptive rounding to minimize quantization loss, particularly for handling outliers and coupled matrices. Experiments on Gemma 2 models show consistent improvements over baselines, achieving 62.4 average benchmark score for 4-bit quantization (vs 61.9 baseline) and 60.6 for 3-bit quantization (vs 52.0 baseline) on Gemma 2 9B, while adding less than 3% computation overhead. The method also performs comparably to calibration-based GPTQ while avoiding privacy concerns associated with calibration data.

## Method Summary
CafeQ addresses calibration-free quantization by learning block-diagonal transformations for single matrices and coupled transformations for matrix pairs (Wv, Wo). The method optimizes these transformations using a Frobenius norm proxy loss that correlates with downstream performance, then applies quantization with adaptive rounding to minimize reconstruction error. For coupled matrices, the technique exploits computational invariance to absorb transformations into the weights, achieving zero inference overhead. The approach is validated on Gemma 2 2B and 9B models across 16 benchmarks.

## Key Results
- 4-bit quantization of Gemma 2 9B achieves 62.4 average benchmark score (vs 61.9 baseline)
- 3-bit quantization of Gemma 2 9B achieves 60.6 average benchmark score (vs 52.0 baseline)
- Learned block-diagonal transformations reduce ℓ2 loss by 43% compared to random rotations
- Adaptive rounding for coupled matrices reduces PQE by 21% compared to independent rounding
- Less than 3% computation overhead for single-matrix transformations

## Why This Works (Mechanism)

### Mechanism 1
Frobenius norm of weight reconstruction error serves as a valid proxy for downstream task performance when calibration data is unavailable. By assuming input norm changes little under quantization, minimizing ‖M⁻¹[MW−W‖_F approximates minimizing expected output error. Empirical validation shows Spearman correlation of -0.640 to -0.679 between ℓ2 proxy loss and benchmark scores on Gemma 2 2B.

### Mechanism 2
Learned block-diagonal transformations reduce outlier-driven quantization error more effectively than random rotations while maintaining low inference overhead. The method optimizes block-diagonal M by minimizing surrogate loss, limiting FLOPs to O(d·k) where k is block size. Table 1 shows learned block-diagonal (size 128) achieves 0.085 avg relative ℓ2 loss vs. 0.155 for random Hadamard.

### Mechanism 3
For coupled weight matrices (W₁, W₂), joint transformation and adaptive rounding minimize product quantization error (PQE) with zero inference overhead. The technique exploits that W₁W₂ = (W₁M⁻¹)(MW₂) for any invertible M, then applies iterative adaptive rounding to compensate first-matrix error via pseudoinverse-weighted adjustment in second matrix.

## Foundational Learning

- Concept: **Uniform quantization error bounds**
  - Why needed here: Understanding Eq. (2)—error scales with ‖w‖_∞/(2^N−1)—explains why outliers dominate and motivate transformation-based mitigation.
  - Quick check question: For 4-bit quantization of weights with max=10, min=-2, what is the worst-case per-weight error bound?

- Concept: **Computational invariance in transformers**
  - Why needed here: Core insight that (W₁M⁻¹)(MW₂) = W₁W₂ enables zero-overhead paired quantization; requires understanding of where matrix pairs appear in attention.
  - Quick check question: Why does this technique apply to (Wv, Wo) but not (Wq, Wk) in models with RoPE?

- Concept: **Stochastic rounding and variance bounds**
  - Why needed here: CafeQ uses stochastic quantization's variance properties to construct differentiable surrogate loss for block-diagonal M optimization.
  - Quick check question: How does stochastic rounding enable gradient-based optimization despite the non-differentiability of the quantization operator?

## Architecture Onboarding

- Component map: Single-matrix quantizer (Block-diagonal M → uniform quantization → online M⁻¹ application) → Coupled-matrix quantizer (Learned full-rank M → iterative adaptive rounding)

- Critical path: 1) Identify matrix type (single vs. coupled) 2) Initialize M 3) Optimize M via gradient descent on surrogate loss 4) Apply learned transformation + quantization 5) For coupled: run Algorithm 1 adaptive rounding iterations

- Design tradeoffs: Block size k (larger → lower error but higher overhead), M orthonormality constraint (unconstrained → lower PQE but potential stability issues), adaptive rounding iterations I (more → lower PQE but higher quantization-time cost)

- Failure signatures: Random rotation baseline outperforms learned M (insufficient iterations or high learning rate), catastrophic PQE (>0.5) (check M condition number), RoPE models (Q-K paired quantization corrupts positional information)

- First 3 experiments: 1) Replicate Table 1 on Gemma 2 2B down-projection layer: sweep block sizes {32, 64, 128, 256} and verify ℓ2 loss vs. FLOPs tradeoff curve matches reported values. 2) Validate proxy loss correlation: Quantize FF weights with various hyperparameters, plot relative ℓ2 error vs. downstream benchmark scores; confirm Spearman ρ < -0.6. 3) Implement Algorithm 1 adaptive rounding on V-O pairs: Compare independent vs. adaptive rounding PQE; target >15% relative reduction as shown in Table 2.

## Open Questions the Paper Calls Out
- How can the paired quantization technique be effectively applied to the query-key matrix pair (Wq, Wk) in architectures that utilize Rotary Positional Embeddings (RoPE)?
- Can CafeQ be combined with calibration-based methods (such as GPTQ) to achieve performance superior to either method individually?
- Does the Frobenius norm proxy loss and learned transformation approach generalize to diverse architectures (e.g., Llama 3, Mistral) and scales beyond 9B parameters?
- Can the learned transformation strategy effectively reduce quantization error in non-uniform quantization formats, such as floating-point (MXFP) or lookup-table (LUT) quantization?

## Limitations
- Critical hyperparameters (learning rate, batch size, optimizer settings) for single-matrix transformation learning are underspecified
- Number of adaptive rounding iterations I is only demonstrated on toy examples without production guidance
- Embedding layer handling is mentioned as applicable but lacks transformation details
- Claims about numerical stability with unconstrained M (eigenvalues >900) require practical validation

## Confidence
- **High confidence**: Core mathematical framework and benchmark improvements
- **Medium confidence**: Effectiveness of learned block-diagonal transformations vs. random rotations
- **Medium confidence**: Adaptive rounding mechanism's contribution to PQE reduction
- **Low confidence**: Numerical stability with unconstrained M having large eigenvalues

## Next Checks
1. **Reproduce Table 1 trade-off curve**: Implement block-diagonal transformation learning with varying block sizes (32, 64, 128, 256) on Gemma 2 2B down-projection layer, measuring ℓ2 loss vs. FLOPs overhead to verify the reported 0.085 avg relative ℓ2 loss at 3% overhead.

2. **Validate proxy loss correlation**: Systematically quantize FF weights across multiple hyperparameter settings, computing relative ℓ2 error and downstream benchmark scores to confirm Spearman correlation remains below -0.6, ensuring the proxy loss remains a reliable optimization target.

3. **Test adaptive rounding iteration sensitivity**: Implement Algorithm 1 for V-O paired quantization with different iteration counts (I=1, 3, 5, 10), measuring PQE reduction at each step to determine optimal iteration count and verify the >15% relative reduction claim.