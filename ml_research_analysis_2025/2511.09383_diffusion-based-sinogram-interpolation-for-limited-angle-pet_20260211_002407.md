---
ver: rpa2
title: Diffusion-based Sinogram Interpolation for Limited Angle PET
arxiv_id: '2511.09383'
source_url: https://arxiv.org/abs/2511.09383
tags:
- sinogram
- diffusion
- sinograms
- data
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the problem of reconstructing high-quality\
  \ images from limited-angle PET sinograms caused by non-cylindrical detector geometries.\
  \ The core method involves using a conditional diffusion model\u2014based on a fine-tuned\
  \ Stable Diffusion v1.5 architecture\u2014to interpolate missing sinogram regions\
  \ by leveraging a learned prior from fully sampled sinograms."
---

# Diffusion-based Sinogram Interpolation for Limited Angle PET

## Quick Facts
- arXiv ID: 2511.09383
- Source URL: https://arxiv.org/abs/2511.09383
- Reference count: 6
- Primary result: Diffusion-based sinogram interpolation improves PSNR from 47.93 dB to 55.08 dB for limited-angle PET reconstruction

## Executive Summary
This work addresses the challenge of reconstructing high-quality PET images from limited-angle sinograms caused by non-cylindrical detector geometries. The authors propose a conditional diffusion model that learns to interpolate missing sinogram regions by leveraging a learned prior from fully sampled data. By fine-tuning Stable Diffusion v1.5 with multi-scale conditional adapters, the model generates physically plausible sinogram completions that significantly reduce reconstruction artifacts compared to direct limited-angle MLEM reconstruction.

## Method Summary
The method involves fine-tuning Stable Diffusion v1.5 with convolutional control adapters inserted at multiple scales to condition on limited-angle sinograms. The model is trained on 2D sinograms derived from whole-body FDG-PET/CT data, with angular gaps simulated by omitting consecutive bins. During inference, the limited-angle sinogram guides the diffusion denoising process to generate plausible completions, which are merged with the original measurements before MLEM reconstruction. The approach effectively transfers the generative prior from natural images to the sinogram domain through full UNet fine-tuning.

## Key Results
- PSNR improves from 47.93 dB (limited-angle reconstruction) to 55.08 dB (diffusion-interpolated reconstruction)
- The method effectively reduces streak artifacts in reconstructed images
- Multi-scale conditional integration proves more effective than single-scale approaches

## Why This Works (Mechanism)

### Mechanism 1: Conditional Diffusion for Sinogram Completion
The model learns p(x) from complete sinograms during training, then uses the limited-angle sinogram as conditional input during inference to guide denoising toward physically consistent completions. This leverages the learned prior to constrain the solution space for missing regions while respecting observed measurements.

### Mechanism 2: Multi-Scale Conditional Integration via Control Adapters
Convolutional blocks are inserted at multiple resolutions both before and after the original SD UNet. These adapters learn to transform the conditional LA sinogram into features that guide generation while preserving the backbone's generative capacity, enabling effective integration without disrupting pretrained representations.

### Mechanism 3: Full UNet Fine-Tuning for Domain Transfer
Fine-tuning all UNet blocks (rather than freezing encoder/decoder) allows the network to recalibrate from natural images to PET sinograms, which have fundamentally different statistics including sinusoidal patterns and distinct intensity distributions. This domain adaptation is crucial for effective sinogram interpolation.

## Foundational Learning

- **Diffusion Models and Denoising Process**
  - Why needed: Understanding how diffusion models iteratively denoise from pure noise to structured outputs is essential for grasping why they can synthesize plausible sinogram completions
  - Quick check: Can you explain why the reverse diffusion process requires learning to predict the added noise at each timestep?

- **PET Sinograms and Radon Transform**
  - Why needed: Sinograms are the native acquisition domain for PET; understanding their structure (sinusoidal traces) is necessary to interpret why sinogram-domain processing may outperform image-domain approaches
  - Quick check: What does each row of a sinogram represent, and why do point sources trace sinusoidal curves?

- **Conditional Control in Diffusion (ControlNet paradigm)**
  - Why needed: This work builds directly on Zhang et al.'s ControlNet approach; understanding how auxiliary conditioning modules integrate with frozen/fine-tuned backbones is critical
  - Quick check: How do control adapters differ from simply concatenating the conditional input with the noisy sample?

## Architecture Onboarding

- **Component map:**
  - LA sinogram -> ControlNet adapters (multi-scale) -> Fine-tuned SD v1.5 UNet -> Predicted full sinogram -> Merge with LA bins -> MLEM reconstruction

- **Critical path:**
  1. Load pretrained SD v1.5 weights
  2. Initialize conditioning convolutional blocks (zero-initialized recommended per ControlNet)
  3. Forward pass: LA sinogram → conditioning blocks → injected at each UNet scale → diffusion denoising (50 steps at inference)
  4. Post-process: Preserve original LA bins, replace only missing regions with predictions
  5. MLEM reconstruction on completed sinogram

- **Design tradeoffs:**
  - Full fine-tuning vs. frozen backbone: Full fine-tuning enables better domain adaptation but requires more data and risks overfitting
  - 2D vs. 3D processing: Current 2D approach ignores inter-slice correlations; 3D would improve accuracy but increase computational cost substantially
  - Sinogram-domain vs. image-domain: Sinogram completion addresses the root cause of artifacts but requires physics understanding; image-domain methods may be simpler but less principled

- **Failure signatures:**
  - Inconsistent sinusoidal patterns in predicted regions → conditioning integration failed or insufficient training
  - Blurry or over-smoothed predictions → diffusion steps too few or model undertrained
  - Artifacts at boundaries between observed and predicted regions → naive blending; consider soft masking or Poisson blending
  - Mode collapse (all predictions similar regardless of input) → conditioning signal not reaching generation pathway

- **First 3 experiments:**
  1. Baseline validation: Train with frozen SD backbone (no fine-tuning) and compare PSNR to full fine-tuning—quantifies domain adaptation benefit
  2. Angular coverage sweep: Systematically vary missing angular range (30°, 60°, 90°, 120° gaps) to identify the break-even point where predictions become unreliable
  3. Ablation on conditioning injection scale: Test single-scale vs. multi-scale conditioning adapter injection to validate the multi-scale design choice

## Open Questions the Paper Calls Out

- **Open Question 1**: Can conditional diffusion models effectively interpolate 3D sinograms while preserving inter-slice spatial correlations, or does the dimensionality increase introduce new challenges? [Basis: Authors state future work includes extending to 3D sinograms using GATE simulations]
- **Open Question 2**: Does the diffusion model generate physically plausible sinogram content, or does it introduce hallucinated structures that merely appear consistent? [Basis: Paper reports improved PSNR but does not evaluate whether predicted regions contain anatomically or physically accurate information]
- **Open Question 3**: How robust is the pretrained Stable Diffusion fine-tuning strategy when applied to radically different scanner geometries not represented in the training distribution? [Basis: Authors note plans for evaluating across diverse datasets and scanner geometries]

## Limitations
- Performance on angular gaps >120° remains unknown; systematic coverage ablation studies are not reported
- No validation on truly non-cylindrical detector geometries - LA simulation uses cylindrical scanner data with artificially removed angular bins
- Limited generalization evidence: training on one FDG-PET/CT dataset with specific scanner parameters; unknown performance on different tracers, protocols, or scanner types

## Confidence
- **High confidence**: The PSNR improvement from 47.93 dB to 55.08 dB is well-documented and demonstrates clear artifact reduction for the tested configuration
- **Medium confidence**: The diffusion-based sinogram completion approach is theoretically sound and validated on relevant data, but lacks systematic ablation studies and cross-dataset validation
- **Medium confidence**: The ControlNet-style conditioning architecture should work as described, but implementation details are sparse and domain-specific adaptation needs more rigorous testing

## Next Checks
1. Conduct systematic angular coverage ablation: test the model with missing angular ranges of 30°, 60°, 90°, 120°, 150°, and 180° to quantify the relationship between gap size and reconstruction quality, identifying the practical limit of the approach
2. Validate on non-cylindrical geometries: apply the method to breast or brain PET data with inherently limited angular coverage to test real-world applicability beyond simulated gaps
3. Implement uncertainty quantification: add Monte Carlo dropout or ensemble prediction methods to estimate confidence in the completed sinogram regions, preventing MLEM from over-relying on uncertain predictions