---
ver: rpa2
title: Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided
  Alignment
arxiv_id: '2511.18766'
source_url: https://arxiv.org/abs/2511.18766
tags:
- anomaly
- detection
- multi-view
- conference
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of unsupervised visual anomaly
  detection in multi-view imaging systems, where the main difficulty is distinguishing
  true defects from benign appearance variations caused by viewpoint changes. Existing
  methods often treat multiple views independently, leading to inconsistent feature
  representations and high false-positive rates.
---

# Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment

## Quick Facts
- **arXiv ID**: 2511.18766
- **Source URL**: https://arxiv.org/abs/2511.18766
- **Reference count**: 17
- **Primary result**: Proposes VSAD, achieving state-of-the-art unsupervised multi-view anomaly detection by learning viewpoint-invariant representations through homography-guided alignment

## Executive Summary
This paper addresses unsupervised visual anomaly detection in multi-view imaging systems, where the main challenge is distinguishing true defects from benign appearance variations caused by viewpoint changes. The authors propose VSAD (ViewSense-AD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. The core innovation is the Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views, integrated into a View-Align Latent Diffusion Model (VALDM) for progressive multi-stage alignment during the denoising process.

## Method Summary
VSAD builds on Stable Diffusion v2 with a VAE encoder and U-Net decoder backbone. The Multi-View Alignment Module (MVAM) is inserted after each decoder ResBlock to perform homography-guided cross-view feature alignment using attention-based aggregation with positional embeddings. A Fusion Refiner Module (FRM) refines aligned features for global consistency. Training uses 80 epochs with AdamW optimizer, storing normal samples' features in a memory bank. At inference, DDIM inversion extracts multi-level refined features from decoder blocks 3 and 4, comparing them against the memory bank to compute pixel-level anomaly scores, aggregated to view and sample levels.

## Key Results
- Achieves state-of-the-art performance on RealIAD and MANTA datasets for pixel, view, and sample-level anomaly detection
- Outperforms existing methods by significant margins in handling large viewpoint shifts and complex textures
- Extensive ablation studies confirm MVAM and FRM contributions, with optimal patch sampling radius R=3 and decoder levels 4+3

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Homography-guided cross-view alignment reduces false positives caused by viewpoint variations.
- **Mechanism**: Pre-computed homography matrices project patch locations from a query view into neighboring views. A local search window (R×R) samples candidate patches around the projected location. Relative displacement from the projected point is encoded via 2D frequency positional embeddings (γ(Δp_j)). Attention-based aggregation (q, k, v projections) fuses aligned features, producing viewpoint-invariant representations.
- **Core assumption**: The imaging setup (fixed cameras or turntables) produces images with significant spatial overlap where homography is a valid geometric model for inter-view correspondence.
- **Evidence anchors**:
  - [abstract] "MVAM leverages homography to project and align corresponding feature regions between neighboring views"
  - [Section 3.3] Equations 1-5 detail the attention-based aggregation with positional encoding
  - [corpus] SCoNE (arXiv:2512.05540) similarly targets consistent neighborhood representations across views, supporting the premise that cross-view consistency is central to multi-view anomaly detection
- **Break condition**: If camera intrinsics/extrinsics are unknown or highly dynamic, pre-computed homography fails. Paper explicitly notes this limitation: "removes the reliance on pre-calibrated cameras" as future work.

### Mechanism 2
- **Claim**: Multi-stage progressive alignment during diffusion denoising builds coherent representations from coarse to fine scales.
- **Mechanism**: MVAM is embedded into each U-Net decoder layer (l). At each denoising timestep t, the feature map X^(l)_t is processed by ResNet/attention blocks, then aligned by MVAM before passing to the next layer. This recursive alignment across semantic levels (Equation 7) enforces consistency at progressively finer resolutions throughout the reverse diffusion process.
- **Core assumption**: Diffusion models' hierarchical feature decomposition is amenable to alignment at each scale; anomalies persist across scales while viewpoint artifacts can be normalized.
- **Evidence anchors**:
  - [abstract] "progressive and multi-stage alignment during the denoising process... from coarse to fine scales"
  - [Section 3.4] Equation 7: Z̃^(l)_t = MVAM^(l)(U-NetBlock^(l)(Z^(l)_t))
  - [corpus] Limited direct corpus evidence on progressive diffusion alignment for multi-view AD; assumption extrapolated from diffusion's scale hierarchy property
- **Break condition**: If alignment noise accumulates across layers (over-correction), representations may collapse. Ablation shows features from layers 1-2 degrade performance (Table 4), suggesting lower-level features add noise.

### Mechanism 3
- **Claim**: Memory bank comparison with multi-level refined features enables fine-grained anomaly localization.
- **Mechanism**: During inference, DDIM inversion extracts refined features {F^(l)} from decoder layers 3 and 4. A memory bank M stores normal prototypes from training. Anomaly scores (Equation 12) compute weighted minimum L2 distance between test features and memory entries, aggregated across levels. View/sample scores aggregate pixel scores via max-pooling.
- **Core assumption**: Normal features form compact clusters in the learned space; anomalies produce out-of-distribution distances. FRM's consistency loss (Lr) tightens these clusters.
- **Evidence anchors**:
  - [abstract] "comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes"
  - [Section 3.6] Equation 12 defines pixel-level scoring
  - [Figure 5] t-SNE visualization confirms tighter clustering post-MVAM and FRM
  - [corpus] Multi-Flow (arXiv:2504.03306) uses normalizing flows for multi-view AD; memory bank approaches are common but this specific diffusion + multi-level extraction combination is less established
- **Break condition**: If memory bank under-represents normal variation (insufficient coverage), inliers may be misclassified. Paper uses all training features (no sampling sparsity noted).

## Foundational Learning

- **Concept**: Homography and planar projection
  - **Why needed here**: MVAM relies on pre-computed homography matrices H_i→j to project coordinates across views. Understanding that homography maps points between planes under perspective transform is essential.
  - **Quick check question**: Given a 3×3 homography matrix and a point (x, y), can you compute its projection in the target view?

- **Concept**: Latent diffusion models (LDMs) and DDIM inversion
  - **Why needed here**: VALDM operates in latent space with a VAE encoder; DDIM inversion is used for feature extraction at inference. Understanding forward/reverse processes and how features emerge in U-Net decoder layers is critical.
  - **Quick check question**: What does DDIM inversion produce given a clean image, and which U-Net components contain semantic vs. spatial information?

- **Concept**: Attention mechanisms and positional encodings
  - **Why needed here**: MVAM uses learnable q/k/v projections and sinusoidal positional embeddings to encode displacement offsets. The attention weights (Equation 4) aggregate aligned values.
  - **Quick check question**: Why encode relative displacement (Δp_j) rather than absolute position in the attention mechanism?

## Architecture Onboarding

- **Component map**: Multi-view RGB images → VAE encoder → latent Z_0 → U-Net backbone with MVAM after each decoder ResBlock → FRM → denoising output → DDIM inversion → extract F^(3), F^(4) → L2 distance to memory bank M → anomaly map

- **Critical path**: Homography accuracy → MVAM alignment quality → FRM consistency → memory bank discriminability. Homography errors propagate through all stages.

- **Design tradeoffs**:
  - Patch sampling radius R: R=3 optimal (Table 5); smaller R insufficient alignment, larger R introduces noise from excessive sampling
  - Decoder layers: Levels 4+3 best (Table 4); adding levels 2+1 degrades performance due to low-level noise
  - Homography vs. learnable deformation: Rigid homography assumes planar surfaces; future work suggests learnable fields for non-rigid objects

- **Failure signatures**:
  - High false positives on categories with large viewpoint variation → likely homography misalignment or insufficient R
  - Low true positive activation on subtle texture defects → check if using only high-level features (try adding layer 3)
  - Scattered feature clusters in t-SNE → MVAM or FRM not converging; verify L_r contribution

- **First 3 experiments**:
  1. **Reproduce ablation**: Train with MVAM only (no FRM) and FRM only (no MVAM) on RealIAD subset; confirm performance drops match Table 3 (-8% to -1%).
  2. **Hyperparameter sweep**: Test R ∈ {1, 2, 3, 4} on one category (e.g., USB); verify R=3 peak per Table 5.
  3. **Visualization check**: Run inference on 5 test samples; plot anomaly maps alongside PatchCore baseline. Confirm reduced false positives per Figure 3 qualitative results.

## Open Questions the Paper Calls Out

- **Question**: Can learnable deformation fields effectively replace rigid homography to handle non-rigid objects like textiles?
- **Basis in paper**: [explicit] The authors state in the "Limitation and future work" section that "future work could advance our geometric approach by replacing rigid homography with learnable deformation fields to model non-rigid objects (e.g., textiles)."
- **Why unresolved**: The current VSAD framework relies on rigid homography assumptions ($H_{i \to j}$), which cannot accurately model the geometric variations of flexible or deformable surfaces.
- **What evidence would resolve it**: Demonstration of high detection performance on multi-view anomaly datasets containing deformable objects using a non-rigid alignment method.

- **Question**: Can the alignment transformation be learned end-to-end to remove the reliance on pre-calibrated cameras?
- **Basis in paper**: [explicit] The conclusion suggests that "learning the alignment transformation end-to-end would create a more flexible system that removes the reliance on pre-calibrated cameras, increasing its adaptability in dynamic settings."
- **Why unresolved**: The current Multi-View Alignment Module (MVAM) requires pre-computed homography matrices, restricting the system to fixed camera setups or turntables.
- **What evidence would resolve it**: An adaptation of VSAD that infers cross-view correspondences dynamically without external calibration input while maintaining SOTA accuracy.

- **Question**: Is the computational cost of the diffusion-based inference compatible with real-time industrial inspection speeds?
- **Basis in paper**: [inferred] The paper targets "industrial anomaly detection" and uses a Latent Diffusion Model (VALDM) with DDIM inversion. While effective, diffusion models typically have significantly higher latency than standard feed-forward networks (e.g., PatchCore), yet no inference time metrics are provided.
- **Why unresolved**: The computational overhead of multi-stage progressive denoising and the FRM module may render the method too slow for high-throughput manufacturing lines despite its accuracy.
- **What evidence would resolve it**: Reporting frames-per-second (FPS) or latency metrics on standard hardware, comparing the method against real-time capable baselines.

## Limitations

- **Rigid homography assumption**: The framework requires pre-computed homography matrices between views, limiting applicability to fixed camera setups and planar surfaces
- **No dynamic inference metrics**: The paper does not report computational latency or frames-per-second, leaving industrial real-time applicability unclear
- **Memory bank scalability**: Using all training features without subsampling could lead to excessive memory usage and slow inference on larger datasets

## Confidence

- **High confidence**: MVAM reduces false positives via homography-guided alignment (extensive ablation and qualitative evidence in Table 3 and Figure 3)
- **Medium confidence**: Progressive multi-stage alignment builds coherent representations (diffusion scale hierarchy assumed but limited direct corpus evidence)
- **Medium confidence**: Memory bank comparison enables fine-grained localization (standard approach but specific diffusion + multi-level combination less established)

## Next Checks

1. **Homography robustness test**: Replace pre-computed homographies with learned spatial transformers; measure performance degradation to quantify sensitivity.
2. **Memory bank scalability**: Train on 20% of RealIAD normal samples with and without coresets; compare P-AUROC and inference speed to establish subsampling necessity.
3. **Cross-dataset generalization**: Fine-tune VSAD on MANTA (different object types) from RealIAD checkpoint; evaluate transfer learning capability and domain adaptation limits.