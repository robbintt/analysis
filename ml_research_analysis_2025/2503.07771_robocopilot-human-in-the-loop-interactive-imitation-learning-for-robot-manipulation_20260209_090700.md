---
ver: rpa2
title: 'RoboCopilot: Human-in-the-loop Interactive Imitation Learning for Robot Manipulation'
arxiv_id: '2503.07771'
source_url: https://arxiv.org/abs/2503.07771
tags:
- robot
- learning
- human
- policy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RoboCopilot, a system enabling seamless human
  intervention during robot execution for interactive imitation learning. The system
  combines a compliant bimanual manipulator with a bilateral teleoperation device,
  allowing operators to take over robot control when autonomous policies fail.
---

# RoboCopilot: Human-in-the-loop Interactive Imitation Learning for Robot Manipulation

## Quick Facts
- **arXiv ID:** 2503.07771
- **Source URL:** https://arxiv.org/abs/2503.07771
- **Reference count:** 40
- **Primary result:** Interactive imitation learning system combining compliant bimanual robot with bilateral teleoperation enables seamless human intervention during execution, achieving higher success rates than offline methods

## Executive Summary
RoboCopilot presents an interactive imitation learning framework for robot manipulation that enables seamless human intervention during execution. The system combines a compliant bimanual manipulator with a bilateral teleoperation device, allowing operators to take over robot control when autonomous policies fail. Following a Human-Gated DAgger framework, the approach collects corrective demonstrations in failure states, addressing covariate shift that plagues offline behavior cloning. Simulation experiments on Robomimic benchmark tasks demonstrate that interactive data collection outperforms offline methods, with Batched DAgger achieving the best performance by retraining from scratch on collected data.

## Method Summary
The method implements a Human-Gated DAgger algorithm using a Diffusion Policy to parameterize the robot's behavior. The system follows a simple loop: collect warmup demonstrations via teleoperation, train initial policy via behavior cloning, deploy policy for execution, and allow human operators to intervene when failures occur. During intervention, the human provides corrective demonstrations which are logged to the dataset. The policy is then retrained either continually (fine-tuning) or batched (from scratch on aggregated data). The hardware consists of a mobile base with bimanual 7-DOF arms equipped with QDD actuators and bilateral teleoperation via a custom leader device with force feedback.

## Key Results
- Simulation experiments on Robomimic benchmark tasks show interactive data collection outperforms offline behavior cloning
- Batched DAgger (training from scratch on collected data) achieves the best performance across all tasks
- Real-world experiments demonstrate higher success rates with fewer required demonstrations compared to offline methods
- System successfully learns complex bimanual manipulation skills including mobile manipulation tasks requiring base movement coordination

## Why This Works (Mechanism)

### Mechanism 1: Interactive Intervention Mitigates Covariate Shift
- **Claim:** Interactive intervention mitigates covariate shift by training on the policy's induced state distribution rather than the expert's distribution.
- **Mechanism:** Offline Behavior Cloning trains on expert states, but when the policy drifts, it encounters out-of-distribution states it hasn't learned to correct. Human-Gated DAgger collects corrective data in the exact failure states the policy visits.
- **Core assumption:** The human operator can accurately diagnose failure modes in real-time and provide a recoverable corrective action.
- **Evidence anchors:** Interactive teaching has "appealing theoretical and practical properties" for learning complex skills; explicitly links policy failure to "errors accumulated during online execution" (covariate shift).

### Mechanism 2: Batched Retraining Avoids Catastrophic Forgetting
- **Claim:** Batched retraining on aggregated data outperforms continual fine-tuning by avoiding catastrophic forgetting.
- **Mechanism:** While Continual DAgger updates the model online, it suffers from non-stationary data distributions. Batched DAgger trains a fresh policy from scratch on the full interactive dataset.
- **Core assumption:** The final dataset size remains manageable for training from scratch without prohibitive computational cost.
- **Evidence anchors:** Batched DAgger "achieved the best performance" in simulation experiments; consistently outperforms Continual DAgger particularly in the "Transport" task.

### Mechanism 3: Bilateral Haptic Feedback Reduces Cognitive Load
- **Claim:** Bilateral haptic feedback and kinematic matching reduce the cognitive load and latency of human intervention.
- **Mechanism:** The system uses a leader-follower architecture with scaled force feedback. This allows the operator to physically feel contact forces and maintain kinematic synchronization, enabling "seamless" take-over without needing to re-align hands visually.
- **Core assumption:** The operator's reflexes and the hardware latency are sufficient to stabilize the system during the switch from autonomy to teleoperation.
- **Evidence anchors:** System enables "seamless control switching" for bimanual tasks; bilateral control law and force feedback scaling described in detail.

## Foundational Learning

- **Concept: Covariate Shift & Distribution Mismatch**
  - **Why needed here:** This is the theoretical motivation for the entire system. Without understanding that BC fails because it never sees its own mistakes, the value of interactive correction is lost.
  - **Quick check question:** Why would a policy that perfectly clones expert actions fail when deployed?

- **Concept: Diffusion Policies**
  - **Why needed here:** The paper uses a Diffusion Network to parameterize the policy π_i. Understanding that the policy predicts action sequences conditioned on observations is necessary for the implementation.
  - **Quick check question:** How does a diffusion policy handle multimodal action distributions compared to a standard Gaussian policy?

- **Concept: Bilateral Teleoperation**
  - **Why needed here:** The hardware interface is not a simple remote control; it is a dynamic system where force/torque information flows back to the user.
  - **Quick check question:** What is the difference in user cognitive load between position-control teleoperation and bilateral force-feedback teleoperation?

## Architecture Onboarding

- **Component map:**
  - Robot (Follower): Mobile base + Bimanual Arms (7-DOF each) with QDD actuators + Grippers
  - Interface (Leader): Scaled kinematic replica (GELLO-style) + Quest 2 controller for buttons
  - Compute: Onboard computer handling perception (Realsense D405s), policy inference (Diffusion Policy), and control loops (ROS)
  - Data Pipeline: ROS bags -> Dataset D -> Diffusion Training Loop

- **Critical path:**
  1. Collect Warmup Demos (Teleop) -> Train π_1 (BC)
  2. Deploy π_1 -> Human watches execution
  3. On failure -> Human grasps leader -> System switches control -> Human corrects -> Data logs to D
  4. End of episode -> Retrain/Batch Train π_{i+1} on expanded D

- **Design tradeoffs:**
  - Compliance vs. Precision: QDD actuators provide compliance/safety but introduce backlash, potentially limiting ultra-fine precision
  - Continual vs. Batched: Continual allows immediate use of data but risks forgetting; Batched requires full retraining time but yields higher success rates

- **Failure signatures:**
  - Oscillation during switch: Leader and follower fight each other; check bilateral gains (α, β) and gravity compensation
  - Policy regression: Success rate drops after an update; likely catastrophic forgetting (solution: switch to Batched DAgger or lower learning rate)
  - Drift in mobile base: Base movement interferes with arm manipulation; check if base velocity is correctly modeled in the observation space

- **First 3 experiments:**
  1. **Latency & Compliance Test:** Verify the "seamless switch." Execute a policy, force an intervention, and measure time-to-control and stability.
  2. **Simulation Ablation (Robomimic):** Compare BC vs. Continual DAgger vs. Batched DAgger on the "Transport" task to validate data efficiency.
  3. **Real-World "Can Picking":** Demonstrate a full loop: 10 demos -> Train -> 5 interventions -> Retrain -> Evaluate success rate improvement.

## Open Questions the Paper Calls Out

- **Question 1:** How does real human intervention variability affect interactive learning performance compared to the deterministic expert policy used in simulation experiments?
  - **Basis in paper:** Section 5.1 states "Experiments involving human interaction can be difficult to reproduce due to the volatile nature of human interaction. To improve reproducibility, we use the pre-trained expert diffusion policy π∗... as a human substitute."
  - **Why unresolved:** Simulation substitutes humans with a pre-trained policy to improve reproducibility, but this may not capture real human intervention patterns, timing variability, and suboptimal corrections.
  - **What evidence would resolve it:** A controlled study comparing learning outcomes using real human operators versus the expert policy substitute, measuring policy performance, data efficiency, and intervention patterns.

- **Question 2:** How can catastrophic forgetting be effectively mitigated during continual fine-tuning in the interactive learning loop?
  - **Basis in paper:** Section 3 acknowledges: "Prior work has shown that continually fine-tuning neural networks on data from non-stationary distributions causes catastrophic forgetting."
  - **Why unresolved:** The paper uses fine-tuning despite this known issue and finds Batched DAgger outperforms Continual DAgger, suggesting the forgetting problem remains unsolved.
  - **What evidence would resolve it:** Experiments comparing continual fine-tuning with replay buffers, elastic weight consolidation, or other anti-forgetting techniques in the interactive learning setting.

- **Question 3:** How does system performance scale to deployment contexts where skilled operators are unavailable or tasks are too difficult to teleoperate?
  - **Basis in paper:** Section 6 states: "This dependency might limit the scalability of our system in environments where such expertise is not readily available or the task is too difficult to even teleoperate."
  - **Why unresolved:** The current system requires skilled operators throughout training; no experiments test novice users or evaluate minimum skill requirements.
  - **What evidence would resolve it:** User studies with operators of varying skill levels, plus evaluation on tasks at the limits of teleoperation feasibility.

- **Question 4:** To what extent does planetary gearbox backlash degrade performance on precision manipulation tasks?
  - **Basis in paper:** Section 6 notes: "the use of planetary gearboxes... introduces some degree of backlash, which may affect the precision of fine manipulation tasks."
  - **Why unresolved:** No systematic evaluation isolates backlash effects; tested tasks may not require sufficient precision to reveal limitations.
  - **What evidence would resolve it:** Comparative experiments using higher-precision transmissions on tasks specifically designed to require fine positional accuracy.

## Limitations

- Reliance on simulated expert intervention in Robomimic experiments may not capture real human-in-the-loop interaction complexities
- Evaluation limited to specific tasks (industrial part picking and kitchen manipulation) without broader generalization testing
- Hardware-specific details for the bilateral teleoperation device are not fully specified, limiting reproducibility of the exact implementation

## Confidence

- **High Confidence:** The theoretical framework of Human-Gated DAgger addressing covariate shift through interactive correction is well-established in the literature and supported by the experimental results
- **Medium Confidence:** The superiority of Batched DAgger over Continual DAgger is demonstrated in simulation but requires further validation across diverse task families and longer learning horizons
- **Medium Confidence:** The claims about seamless control switching and reduced cognitive load are supported by system design but lack quantitative user studies or latency measurements

## Next Checks

1. Conduct real-world user studies comparing cognitive load and intervention accuracy between bilateral force-feedback teleoperation and position-control alternatives
2. Extend evaluation to tasks requiring longer time horizons and more complex state-action spaces to validate generalization of the interactive learning approach
3. Perform ablation studies on intervention frequency and timing to determine optimal human-autonomy handoff policies for different failure modes