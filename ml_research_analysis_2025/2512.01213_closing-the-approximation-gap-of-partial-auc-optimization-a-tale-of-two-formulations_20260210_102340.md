---
ver: rpa2
title: 'Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations'
arxiv_id: '2512.01213'
source_url: https://arxiv.org/abs/2512.01213
tags:
- optimization
- uni00000013
- have
- ieee
- pattern
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing partial AUC (PAUC)
  for imbalanced classification with decision constraints. The key difficulty lies
  in the NP-hard sample selection within constrained ROC regions, which leads to uncontrollable
  approximation errors and limited scalability in existing methods.
---

# Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations

## Quick Facts
- **arXiv ID:** 2512.01213
- **Source URL:** https://arxiv.org/abs/2512.01213
- **Reference count:** 40
- **Primary result:** Proposed instance-wise minimax reformulations achieve O(ϵ⁻³) convergence rate and tight generalization bounds for partial AUC optimization.

## Executive Summary
This paper addresses the challenge of optimizing partial AUC (PAUC) for imbalanced classification with decision constraints. The key difficulty lies in the NP-hard sample selection within constrained ROC regions, which leads to uncontrollable approximation errors and limited scalability in existing methods. The authors propose two novel instance-wise minimax reformulations: one with asymptotically vanishing approximation gap, and one with exact unbiasedness. Their core idea transforms the pairwise problem into an instance-wise formulation by introducing global variables and converting top-k selection into differentiable threshold learning. This eliminates explicit pair enumeration and achieves linear per-iteration complexity. The proposed algorithms achieve O(ϵ⁻³) convergence rate for both one-way and two-way PAUC, significantly faster than existing methods. A tight generalization bound is established showing sharp order of Õ(α⁻¹n₊⁻¹ + β⁻¹n₋⁻¹), explicitly demonstrating the impact of TPR/FPR constraints. Extensive experiments on multiple imbalanced datasets validate the effectiveness of the proposed methods, which consistently outperform existing PAUC optimization algorithms in both OPAUC and TPAUC metrics.

## Method Summary
The paper proposes two instance-wise minimax reformulations for PAUC optimization that transform the traditional pairwise problem into a more scalable formulation. The key innovation is introducing global reference variables (a, b) that decouple sample dependencies, converting the O(n₊ × n₋) pairwise enumeration into linear complexity operations. For top-β quantile selection, the method replaces explicit sorting with smooth threshold learning via convex conjugate formulation. The first formulation (PAUCI) uses softplus smoothing with controlled bias, while the second (UPAUCI) maintains unbiasedness through auxiliary weights c(x) ∈ [0,1]. Both are optimized via Accelerated Stochastic Gradient Descent Ascent (ASGDA) with momentum variance reduction, achieving O(ϵ⁻³) convergence rate compared to O(ϵ⁻⁶) for existing methods.

## Key Results
- Proposed instance-wise reformulations achieve O(ϵ⁻³) convergence rate versus O(ϵ⁻⁶) for existing methods
- Generalization bound shows sharp scaling of Õ(α⁻¹n₊⁻¹ + β⁻¹n₋⁻¹), explicitly demonstrating TPR/FPR constraint impact
- Extensive experiments show consistent outperformance over SOPA and SOPA-S across multiple imbalanced datasets
- Achieves linear per-iteration complexity by eliminating pairwise enumeration through global reference variables

## Why This Works (Mechanism)

### Mechanism 1: Instance-wise reformulation eliminates pairwise enumeration
- Claim: Instance-wise reformulation eliminates O(n₊ × n₋) pairwise enumeration by introducing global reference variables that decouple sample dependencies.
- Mechanism: The pairwise loss ℓ(f(xᵢ) - f(x'ⱼ)) is equivalently expressed as minimization over reference points (a, b) and maximization over γ. Each sample interacts only with these global statistics: P(f,a,γ,x) = (f(x)-a)² - 2(1+γ)f(x) for positives and N(f,b,γ,x) = (f(x)-b)² + 2(1+γ)f(x) for negatives.
- Core assumption: The squared surrogate loss ℓ(x) = (1-x)² admits this decomposition; other surrogates require Bernstein-polynomial approximation per Remark 4.
- Evidence anchors:
  - [abstract]: "Our key idea is to first establish an equivalent instance-wise problem to lower the time complexity"
  - [section 4.2]: Theorem 1 shows min_f R̂β(f,S) ⇔ min_{f,(a,b)} max_γ E_z[F_op(...)] with linear cost
  - [corpus]: Weak direct support; related work on scaling ROC-SVM mentions similar complexity challenges but different solutions
- Break condition: If surrogate loss is non-convex or discontinuous, the closed-form optimum for a* = E[f(x)] may not hold.

### Mechanism 2: Threshold learning replaces sorting for top-k selection
- Claim: Top-β quantile selection is reformulated as smooth threshold learning via the identity E[1_{f(x')≥ηβ} · ℓ⁻(x')] = min_s' E[βs' + [ℓ⁻(x') - s']⁺]/β.
- Mechanism: Rather than sorting negatives to find the β-quantile threshold ηβ, learn threshold s' directly. The convex conjugate formulation ensures [ℓ⁻(x') - s']⁺ activates only for losses above the threshold, implicitly selecting high-scoring negatives.
- Core assumption: Loss ℓ⁻(x') is increasing in f(x'), which holds when γ ∈ [b-1, 1] per Proposition 1.
- Evidence anchors:
  - [abstract]: "simplify the complicated sample selection procedure by threshold learning"
  - [section 4.2]: Equation (10) and Theorem 2 derive the sorting-free reformulation
  - [corpus]: No direct corpus support for this specific technique
- Break condition: If γ violates the constraint [b-1, 1], monotonicity fails and the equivalence breaks.

### Mechanism 3: Unbiased formulation via auxiliary weights
- Claim: Unbiased reformulation via auxiliary weights c(x) ∈ [0,1] preserves exactness: [x]⁺ = max_{c∈[0,1]} c·x.
- Mechanism: The softplus surrogate r_κ(x) = log(1+exp(κx))/κ introduces O(1/κ) bias. Instead, treat each sample's contribution to the max as a learnable weight c, maintaining convexity w.r.t. the threshold and enabling min-max swapping.
- Core assumption: The inner domain is convex and compact; the objective is concave in c (or made strongly concave via ω·||c||² regularization).
- Evidence anchors:
  - [abstract]: "one with an asymptotically vanishing gap, the other with the unbiasedness at the cost of more variables"
  - [section 4.3.2]: Equation (16)-(20) derives the unbiased form via dual variable c
  - [corpus]: Prior PAUC methods (SOPA, AUC-poly) use weighting but remain pairwise
- Break condition: Without sufficient regularization ω > 0, convergence degrades from O(ϵ⁻³) to O(ϵ⁻⁶) per Remark 6 and Figure 10 sensitivity analysis.

## Foundational Learning

### Concept: Minimax optimization (non-convex strongly-concave)
- **Why needed here:** Both formulations reduce to min_τ max_γ structures solvable via ASGDA with momentum variance reduction.
- **Quick check question:** Can you explain why strong concavity in γ enables faster O(ϵ⁻³) vs O(ϵ⁻⁶) convergence?

### Concept: Quantile functions and partial order statistics
- **Why needed here:** PAUC operates on constrained FPR/TPR regions, requiring selection of top-β or bottom-α samples.
- **Quick check question:** How does ηβ(f) relate to the empirical quantile ̂ηβ(f) in finite samples?

### Concept: Local Rademacher complexity
- **Why needed here:** The generalization bound ̃O(α⁻¹n₊⁻¹ + β⁻¹n₋⁻¹) uses local complexity to capture dependency on TPR/FPR constraints.
- **Quick check question:** Why does the bound scale inversely with α and β but not with total n?

## Architecture Onboarding

### Component map
- **Scoring network f_θ:** ResNet-18 backbone with sigmoid output ∈ [0,1]
- **Reference variables (a, b):** Learnable calibration points for positive/negative score distributions
- **Selection thresholds (s, s'):** Control which samples contribute to TPAUC (s for bottom-α positives, s' for top-β negatives)
- **Dual variable γ:** Balances positive-negative score gap; domain Ωγ = [max{-a, b-1}, 1]
- **Sample weights c:** Per-instance weights ∈ [0,1] for unbiased version only
- **Lagrange multipliers (θ_a, θ_b):** Decouple coupled constraints; M₁ = M₂ = 10⁹ in experiments

### Critical path
1. Forward pass computes f(x) for batch
2. Compute P(f,a,γ,x) and N(f,b,γ,x) losses
3. Apply threshold selection via softplus (PAUCI) or max-c formulation (UPAUCI)
4. Update τ = {θ, a, b, s, s', θ_a, θ_b} with momentum descent
5. Update γ (and c for UPAUCI) with momentum ascent
6. Project to feasible domains: (a,b) ∈ [0,1]², γ ∈ Ωγ

### Design tradeoffs
- **PAUCI vs UPAUCI:** PAUCI has fewer variables (no c), faster per-iteration; UPAUCI is unbiased but requires O(n⁻) weight storage
- **κ selection:** Larger κ → smaller O(1/κ) bias but gradients approach non-smooth; paper uses κ ∈ [2,6]
- **ω regularization:** Required for strong concavity; ω = 0 degrades to O(ϵ⁻⁶) convergence (Figure 10 shows ω ≈ 0.3-0.4 optimal)

### Failure signatures
- Score collapse: If a ≈ b, all samples get similar scores; check γ → 0
- Threshold divergence: s' → 5 (upper bound) suggests no negatives selected; increase learning rate for s'
- Quantile deviation: With small κ, softplus selects top-̃β ≠ top-β; Figure 3 illustrates the shift

### First 3 experiments
1. **Sanity check on toy data:** Generate 100 positive (mean=0.3) and 900 negative (mean=0.7) samples; verify OPAUC(FPR≤0.3) ≈ 0.85 after 50 epochs with PAUCI
2. **Ablation of κ:** On CIFAR-10-LT-1, sweep κ ∈ {2,4,6,8,10} holding ω=0.3; expect performance plateau around κ=6 with increasing instability beyond
3. **Convergence comparison:** Plot training PAUC vs epoch for SOPA, SOPA-S, PAUCI, UPAUCI without warmup; expect UPAUCI to stabilize by epoch 20 while SOPA-S continues drifting (replicate Figure 6)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the quantile deviation caused by finite κ in the surrogate approximation (PAUCI) theoretically impact the estimation of model parameters, beyond the established asymptotic gap?
- **Basis in paper:** [Explicit] Section 4.3.1 states that for a limited κ, "the quantile deviation could have a potential impact on the variable estimation to some extent," but the paper only characterizes the objective gap.
- **Why unresolved:** While Theorem 3 establishes that the objective gap vanishes as κ → ∞, the convergence properties of the model parameters θ under a finite, practical κ remain unquantified.
- **What evidence would resolve it:** A theoretical bound relating the parameter estimation error to the finite value of κ.

### Open Question 2
- **Question:** Can the tight generalization bounds of order Õ(·) be explicitly derived for the unbiased formulation using non-smooth surrogates (e.g., Hinge loss) without relying on smoothing approximations?
- **Basis in paper:** [Inferred] Remark 4 claims the framework extends to broad convex surrogates, but the primary theoretical analysis (Theorem 5) is derived specifically for the squared loss.
- **Why unresolved:** The proof technique for the generalization bound relies on the specific properties of the squared loss and instance-wise reformulation; extending this to non-smooth functions may alter the complexity terms or require different covering number assumptions.
- **What evidence would resolve it:** A derivation of the Rademacher complexity or generalization bound specifically for the Hinge loss formulation.

### Open Question 3
- **Question:** Is there an adaptive strategy or theoretical justification for selecting the regularization parameter ω that balances the trade-off between convergence speed and the introduction of approximation bias?
- **Basis in paper:** [Inferred] Section 7.7 analyzes the "Necessity of Hyperparameter ω" for UPAUCI, noting that while necessary for the O(ϵ⁻³) rate, it introduces bias and requires tuning.
- **Why unresolved:** The paper empirically determines that "a slight ω is sufficient" but does not provide a theoretical rule or adaptive mechanism to set ω optimally relative to the data distribution or optimization stage.
- **What evidence would resolve it:** A theoretical analysis of the bias-variance trade-off regarding ω or an algorithm that dynamically adjusts ω during training.

## Limitations
- Strong dependence on convex surrogate loss functions, particularly the squared margin loss ℓ(x) = (1-x)²
- O(ϵ⁻³) convergence rate assumes strong concavity in dual variables which may not hold under constraint violations
- Theoretical extension to non-smooth surrogates (e.g., Hinge loss) remains incomplete

## Confidence
**High Confidence:**
- The instance-wise reformulation correctly eliminates pairwise enumeration complexity
- ASGDA with momentum variance reduction achieves O(ϵ⁻³) convergence under stated conditions
- Generalization bound scaling O(α⁻¹n₊⁻¹ + β⁻¹n₋⁻¹) is mathematically sound

**Medium Confidence:**
- The equivalence between pairwise and instance-wise formulations holds for all valid parameter ranges
- Unbiased formulation (UPAUCI) consistently outperforms biased (PAUCI) across all dataset configurations
- The proposed method's superiority over SOPA/SOPA-S generalizes beyond the tested datasets

**Low Confidence:**
- Exact hyperparameter sensitivity analysis covers all relevant operating regimes
- The initialization strategies are robust across different model architectures
- The computational complexity advantage scales linearly with dataset size in all scenarios

## Next Checks
1. **Surrogate Robustness Test:** Implement and evaluate the Bernstein-polynomial extension for non-quadratic surrogates (e.g., logistic loss) to verify approximation gap bounds hold beyond the squared margin case.

2. **Constraint Violation Analysis:** Systematically study algorithm behavior when γ violates the [b-1, 1] constraint or when Lagrange multipliers exceed M=10⁹, quantifying performance degradation and identifying early warning signals.

3. **Architecture Generalization:** Test the framework on different backbone architectures (e.g., EfficientNet, Vision Transformer) to determine if the performance gains and complexity benefits are architecture-dependent or universal.