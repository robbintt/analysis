---
ver: rpa2
title: 'DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models''
  Understanding on Indian Culture'
arxiv_id: '2509.19274'
source_url: https://arxiv.org/abs/2509.19274
tags:
- cultural
- arxiv
- across
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces DRISHTIKON, a first-of-its-kind multimodal,
  multilingual benchmark designed to evaluate the cultural understanding of language
  models in Indian contexts. It includes over 64,000 aligned text-image pairs across
  15 languages, covering all 28 states and 8 union territories, and addressing diverse
  cultural themes such as festivals, attire, cuisines, and art forms.
---

# DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture

## Quick Facts
- arXiv ID: 2509.19274
- Source URL: https://arxiv.org/abs/2509.19274
- Reference count: 37
- Over 64,000 aligned text-image pairs across 15 languages evaluating VLMs on Indian cultural understanding

## Executive Summary
DRISHTIKON introduces the first multimodal, multilingual benchmark designed to evaluate cultural understanding of language models in Indian contexts. The benchmark contains over 64,000 aligned text-image pairs covering 15 languages, 28 states, and 8 union territories, with questions spanning festivals, attire, cuisines, and art forms. The study evaluates diverse vision-language models including open-source small and large models, proprietary systems, reasoning-specialized VLMs, and Indic-focused models under zero-shot and chain-of-thought settings. Results reveal significant gaps in models' ability to reason over culturally grounded, multimodal inputs, particularly for low-resource languages and less-documented traditions.

## Method Summary
DRISHTIKON evaluates VLMs using 64,288 instances of image-question pairs with four-option MCQs across 15 Indic languages. The benchmark covers all 28 states and 8 union territories of India, with questions categorized by type (General, Common Sense Cultural, Multi-hop Reasoning, Analogy) and cultural attributes. Evaluation uses zero-shot and chain-of-thought prompting strategies, with images referenced by URL and questions translated from English using Gemini Pro with human verification. The dataset is provided in tabular CSV/Excel format with metadata including question type, language tag, state/UT tag, and cultural attribute labels.

## Key Results
- High-resource languages (English, Hindi, Bengali, Marathi) significantly outperform low-resource languages (Sindhi, Konkani, Kannada), with accuracy drops exceeding 40% for low-resource languages
- Compact models like SmolVLM and InternVL3-1B sometimes match or outperform larger models on specific cultural reasoning tasks, challenging the scale-equals-performance assumption
- Chain-of-thought prompting improves reasoning-intensive questions by 10-15% but shows inconsistent gains across languages and model families, with limited benefit for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal grounding improves cultural question-answering by forcing vision-language alignment on region-specific artifacts.
- Mechanism: Image-question pairs require the model to ground textual descriptions in visual features (attire, architecture, rituals). Models with strong cross-modal alignment (e.g., GPT-4o mini, Maya) leverage visual cues to disambiguate semantically close distractors, while models with weaker alignment rely on lexical heuristics and fail.
- Core assumption: Visual features provide disambiguating signal for culturally specific concepts that text alone cannot resolve.
- Evidence anchors:
  - [abstract] "evaluate the cultural understanding of generative AI systems... over 64,000 aligned text-image pairs"
  - [section A.9] Error cases show models confuse visually similar traditions (Paika of Odisha vs. Paika Akhara of Jharkhand) when visual grounding is weak.
  - [corpus] SANSKRITI (text-only benchmark) complements this; IndicVisionBench independently validates multimodal grounding hypothesis.
- Break condition: If visual cues are redundant with text (e.g., questions answerable without the image), multimodal advantage collapses to unimodal baseline.

### Mechanism 2
- Claim: Performance degrades predictably with language resource level due to pretraining data imbalance.
- Mechanism: High-resource languages (English, Hindi, Bengali, Marathi) have more pretraining tokens and better multilingual alignment, yielding higher accuracy. Low-resource languages (Sindhi, Konkani, Kannada) lack sufficient representation, causing >40% accuracy drops even for capable models.
- Core assumption: Training data distribution, not architectural capacity, is the primary bottleneck for low-resource language performance.
- Evidence anchors:
  - [section 5.3] "English remains the most reliably understood language... languages like Sindhi, Konkani, and Kannada consistently pose the greatest challenges, with accuracy dropping by over 40%"
  - [section 5.3] "Assamese and Odia, despite their wide speaker base, do not exhibit uniformly high performance, hinting at underrepresentation"
  - [corpus] IndicMMLU-Pro and HinTel-AlignBench confirm resource-level correlates with performance across Indic benchmarks.
- Break condition: If model is explicitly fine-tuned on low-resource Indic languages with curated data, this correlation may weaken.

### Mechanism 3
- Claim: Chain-of-thought prompting provides modest gains for complex reasoning but does not close cultural knowledge gaps.
- Mechanism: CoT decomposes multi-hop and analogy questions into intermediate steps, improving accuracy by 10–15% for reasoning-intensive categories. However, gains are inconsistent across languages and model families, with low-resource languages benefiting less due to weaker linguistic scaffolding in pretraining.
- Core assumption: Models possess latent cultural knowledge that CoT elicits but cannot retrieve in zero-shot settings.
- Evidence anchors:
  - [section 5.6] "CoT proved most beneficial for reasoning-intensive categories such as multi-hop and analogy questions, yielding accuracy gains of up to 10–15%"
  - [section 5.6] "reasoning-specialized (e.g., Kimi-VL-A3B-Thinking) and Indic-focused models (e.g., Chitrarth) exhibited limited or inconsistent improvements"
  - [corpus] MMA-ASIA framework similarly finds CoT effects are context-dependent; no corpus paper claims CoT as a general solution.
- Break condition: If the model lacks underlying cultural knowledge, CoT cannot generate correct reasoning chains and may amplify hallucinations.

## Foundational Learning

- Concept: Vision-Language Model (VLM) alignment
  - Why needed here: DRISHTIKON evaluates VLMs, which require understanding how visual encoders (CLIP, ViT) map to language decoders. Without this, you cannot interpret why Maya (7B) outperforms larger general-purpose VLMs on cultural tasks.
  - Quick check question: Can you explain why a 256M-parameter model (SmolVLM) sometimes matches a 7B model on specific cultural attributes?

- Concept: Resource-level taxonomy in multilingual NLP
  - Why needed here: The paper categorizes languages by resource level; you need to distinguish high-resource (Hindi, Bengali) from low-resource (Sindhi, Konkani) to interpret performance disparities correctly.
  - Quick check question: Why might a widely spoken language like Assamese still perform poorly if it is underrepresented in pretraining corpora?

- Concept: MCQ evaluation limitations
  - Why needed here: The benchmark uses 4-option MCQs; understanding the 25% chance baseline and distractor design (semantically close vs. thematically plausible) is essential for interpreting accuracy scores meaningfully.
  - Quick check question: What is the risk of a model achieving high accuracy via "test-taking strategies" rather than genuine cultural understanding?

## Architecture Onboarding

- Component map:
  Dataset -> CSV/Excel table with 64,288 rows containing question, 4 options, correct label, image URL, question type, language tag, state/UT tag, cultural attribute

- Critical path:
  1. Download images from URLs (respect source licensing)
  2. Load CSV/Excel into evaluation pipeline
  3. Standardize inputs: image resolution ≥224×224, model-specific prompt templates
  4. Run inference under zero-shot and CoT settings
  5. Compute accuracy by language, state/UT, question type, cultural attribute

- Design tradeoffs:
  - MCQ vs. open-ended: MCQs enable reproducible accuracy scoring but may underestimate generative capabilities; authors plan open-ended expansion
  - 4-option format: Lowers chance guessing (25%) but limits distractor complexity compared to 5+ options
  - Translation via LLM: Scalable but introduces potential semantic drift; mitigated by two-stage human verification

- Failure signatures:
  - Fine-grained semantic confusion: Model selects semantically close distractor (e.g., Paika dance forms across states)
  - Geographic bias: Low accuracy on Lakshadweep, Mizoram, Dadra and Nagar Haveli (underrepresented in training data)
  - Low-resource language collapse: Accuracy drops >40% for Sindhi, Konkani regardless of model scale

- First 3 experiments:
  1. Baseline evaluation: Run GPT-4o mini and Maya on English subset; compare accuracy across question types to establish upper bounds.
  2. Language ablation: Evaluate SmolVLM-256M-Instruct on high-resource (Hindi) vs. low-resource (Sindhi) subsets; quantify resource-level gap.
  3. CoT vs. zero-shot: Test InternVL3-1B on multi-hop reasoning questions under both prompting strategies; measure gain variance across languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can culturally grounded Chain-of-Thought (CoT) prompting be optimized to close the performance gap between high-resource and low-resource Indic languages?
- Basis in paper: [explicit] Section 5.6 states that while CoT improves reasoning, benefits are inconsistent, with high-resource languages gaining significantly more than low-resource languages like Konkani and Sindhi.
- Why unresolved: The paper identifies the disparity but does not propose specific mechanisms to stabilize or improve reasoning scaffolds for languages with limited training data.
- What evidence would resolve it: A comparative study showing uniform accuracy improvements across all 15 languages using a modified, language-specific CoT strategy.

### Open Question 2
- Question: What specific alignment techniques allow Small Language Models (SLMs) to outperform larger Vision-Language Models in fine-grained cultural reasoning?
- Basis in paper: [explicit] Section 5.2 notes that compact models like SmolVLM and InternVL3-1B "punch above their parameter scale," challenging the assumption that scale alone drives multimodal performance.
- Why unresolved: The paper observes the result but does not isolate whether this stems from better instruction tuning, data efficiency, or architectural inductive biases.
- What evidence would resolve it: An ablation study analyzing the training corpora and alignment objectives of high-performing SLMs versus underperforming large models.

### Open Question 3
- Question: How does model performance vary when evaluating cultural understanding via open-ended generation rather than multiple-choice questions?
- Basis in paper: [inferred] Section 3.1 acknowledges the "merit of open-ended formats" but utilizes multiple-choice questions (MCQs) for reproducibility, while Section 6 lists MCQs as a limitation regarding real-world complexity.
- Why unresolved: It remains unclear if models possess deep generative knowledge or if current results are influenced by test-taking strategies and distractor design.
- What evidence would resolve it: A new evaluation suite using semantic similarity metrics (e.g., BERTScore) to grade free-text descriptions of cultural artifacts against ground truth.

## Limitations

- MCQ format may underestimate generative cultural understanding capabilities by favoring test-taking strategies over genuine knowledge
- LLM-based translation (Gemini Pro) introduces potential semantic drift despite human verification, affecting cross-language comparisons
- Benchmark covers only 15 languages and 36 administrative regions, leaving significant gaps in India's linguistic diversity

## Confidence

- High confidence: Performance disparities between high-resource and low-resource languages are well-established and consistent across multiple model families
- Medium confidence: The mechanism by which CoT improves reasoning-intensive question types is demonstrated but shows variability across languages and models
- Low confidence: Claims about specific architectural advantages lack detailed ablation studies to isolate contributing factors

## Next Checks

1. **Ablation on MCQ format**: Test a subset of DRISHTIKON questions in open-ended format with the same models to quantify the performance gap between MCQ selection and free-form generation for cultural knowledge assessment.

2. **Translation verification study**: Select 50 questions from low-resource languages and back-translate them to English via independent human translators to measure semantic drift from the original English prompts.

3. **Data quality audit**: For the three lowest-performing states/UTs (Lakshadweep, Mizoram, Dadra and Nagar Haveli), manually verify that image-question pairs are culturally accurate and not introducing systematic noise that could explain the performance drops.