---
ver: rpa2
title: 'PL-Guard: Benchmarking Language Model Safety for Polish'
arxiv_id: '2506.16322'
source_url: https://arxiv.org/abs/2506.16322
tags:
- safety
- dataset
- polish
- pl-guard
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PL-Guard, a manually annotated benchmark\
  \ dataset for evaluating language model safety in Polish, along with an adversarial\
  \ variant (PL-Guard-adv) to test robustness under noise. The authors fine-tune three\
  \ models\u2014Llama-Guard-3-8B, a HerBERT-based classifier, and PLLuM\u2014using\
  \ combinations of annotated Polish data, machine-translated WildGuard, and PolyGuard\
  \ datasets."
---

# PL-Guard: Benchmarking Language Model Safety for Polish

## Quick Facts
- arXiv ID: 2506.16322
- Source URL: https://arxiv.org/abs/2506.16322
- Authors: Aleksandra Krasnodębska; Karolina Seweryn; Szymon Łukasik; Wojciech Kusa
- Reference count: 10
- Primary result: HerBERT-based classifier achieves highest safety F1 (0.913) on adversarial Polish safety benchmark

## Executive Summary
PL-Guard introduces the first manually annotated Polish language model safety benchmark, comprising 6,487 training samples and 900 test samples across 15 hazard categories. The dataset includes an adversarial variant (PL-Guard-adv) with character-level perturbations to test model robustness. Three models were fine-tuned: Llama-Guard-3-8B, HerBERT-based classifier, and PLLuM. The HerBERT-based classifier achieved the highest overall performance, particularly under adversarial conditions, demonstrating that smaller, language-specific models can outperform larger multilingual alternatives when fine-tuned on high-quality native-language data.

## Method Summary
The study created PL-Guard through manual annotation of Polish language model outputs, mapping them to the 14-category Llama Guard hazard taxonomy plus a safe class. Three models were fine-tuned on combinations of PL-Guard, machine-translated WildGuard, and PolyGuard datasets using AdamW optimization with varying hyperparameters. Models were evaluated on both clean and adversarially perturbed test sets using macro F1-score for binary safety classification and multi-class hazard categorization. The HerBERT-based classifier used standard fine-tuning with lr=1e-5, batch_size=32, while Llama-Guard-3-8B employed FSDP with lr=1e-7, batch_size=4.

## Key Results
- HerBERT-based classifier achieved 0.913 F1 on PL-Guard-adv (adversarial) vs 0.927 on clean data
- Llama-Guard-3-8B showed ~12% performance drop under adversarial conditions (0.889→0.782 F1)
- HerBERT outperforms Llama in 10 of 14 hazard categories on clean data
- Models trained on Polish data show severe degradation when evaluated on English (HerBERT: 0.663→0.315 category F1)

## Why This Works (Mechanism)

### Mechanism 1: Language-specific pre-training creates stronger representations for low-resource safety tasks
- Claim: Smaller models pre-trained on Polish corpora can outperform larger multilingual models on Polish safety classification when both are fine-tuned on similar data
- Mechanism: HerBERT's Polish-specific pre-training captures morphological and syntactic patterns that multilingual LLMs dilute across languages, enabling better feature extraction for downstream safety classification
- Core assumption: Safety classification relies more on surface linguistic patterns than complex reasoning that would benefit from larger model capacity
- Evidence anchors: HerBERT achieves highest overall performance particularly under adversarial conditions; HerBERT outperforms Llama in majority of categories

### Mechanism 2: Native-language annotation reduces cascading translation errors
- Claim: Manual annotation in the target language produces higher-quality training signal than machine-translated datasets
- Mechanism: Native Polish annotations preserve cultural context and subtle threat phrasing that machine translation flattens; training on clean native data avoids error propagation from translation artifacts
- Core assumption: Machine translation introduces systematic distortions in safety-critical contexts (e.g., idiomatic threats, culturally-specific harmful content)
- Evidence anchors: Manual review with Krippendorff's alpha 0.92; PolyGuard (LLM-translated) showed 90.66% high fluency vs WildGuard (transformer-translated) at 69.09%

### Mechanism 3: Encoder-only architectures exhibit superior robustness to character-level perturbations
- Claim: BERT-style classifiers degrade less under adversarial perturbations than autoregressive decoder models
- Mechanism: Bidirectional encoders produce fixed representations from full context, reducing sensitivity to localized token noise; autoregressive decoders propagate perturbation errors through sequential generation
- Core assumption: The perturbation types tested (typos, OCR errors, diacritic removal) primarily affect tokenization and local context
- Evidence anchors: HerBERT shows ~1.5% performance drop on adversarial vs ~12% drop for Llama-Guard; perturbations included altered diacritics, keyboard typos, OCR errors

## Foundational Learning

- **Concept: Llama Guard 14-category hazard taxonomy**
  - Why needed here: All experiments map to this classification scheme; understanding S1-S14 categories is prerequisite for interpreting results and reproducing the benchmark
  - Quick check question: Which hazard category would "Creating fake reviews to damage a competitor's business" map to? (Answer: S5 - Defamation)

- **Concept: Macro F1-score for imbalanced multi-class evaluation**
  - Why needed here: Paper uses macro F1 (not weighted) to evaluate across 14 hazard categories plus safe class; this choice affects interpretation since rare categories have equal weight
  - Quick check question: Why would macro F1 penalize a model that achieves high accuracy but ignores rare categories? (Answer: Macro F1 averages per-class F1 equally, so poor recall on any single category significantly impacts the score)

- **Concept: Text perturbation robustness testing**
  - Why needed here: PL-Guard-adv is a core contribution; understanding the five perturbation types is needed to replicate adversarial evaluation
  - Quick check question: What perturbation type caused the highest average Levenshtein distance in PL-Guard-adv? (Answer: Replacing all Polish diacritic characters with plain Latin equivalents)

## Architecture Onboarding

- **Component map:** PL-Guard-train (6,487 manually annotated) -> PL-Guard-test (900 balanced: 50/hazard + 200 safe) -> PL-Guard-adv (900 perturbed) -> HerBERT-base-cased fine-tuning -> Evaluation with macro F1

- **Critical path:** Load HerBERT-base-cased from HuggingFace, prepare single-label classification data, fine-tune with AdamW optimizer (lr=1e-5, batch_size=32, 5 epochs), evaluate on both clean and adversarial sets, compute macro F1 for binary safety and 15-class categorization

- **Design tradeoffs:** Encoder vs decoder (HerBERT is ~80x smaller but cannot generate explanations); training data scale (adding WG+PG improves Llama models but HerBERT shows inverse trend); single-label vs multi-label simplification (loses multi-hazard detection but aligns with data distribution)

- **Failure signatures:** Llama-PLLuM shows inconsistent outputs (0.181 category F1 with PL-Guard only); GPT-4.1-nano 1-shot collapsed to always predicting "unsafe" (F1=0.438); HerBERT on English shows severe degradation (0.315 vs 0.663 category F1)

- **First 3 experiments:**
  1. Replicate HerBERT fine-tuning on PL-Guard-train only; verify target >0.92 binary F1 on clean, >0.90 on adversarial
  2. Ablate training data composition (PLG vs PLG+WG vs PLG+WG+PG); confirm HerBERT's inverse scaling trend on adversarial while Llama models improve
  3. Test cross-lingual transfer by evaluating on English-translated PL-Guard-test; confirm expected degradation pattern

## Open Questions the Paper Calls Out

- **Open Question 1:** Can BERT-style architectures effectively classify LLM refusal and compliance behaviors in Polish safety contexts?
  - Basis: Authors aim to develop additional model to assess refusal or compliance with user queries
  - Why unresolved: Current PL-Guard model exclusively classifies inputs as safe or unsafe
  - What evidence would resolve it: Fine-tuning HerBERT-based model on dataset annotated for refusal and compliance labels

- **Open Question 2:** To what extent does the distribution gap between synthetically generated prompts and real user queries affect safety classifier generalization?
  - Basis: Authors note prompts were generated automatically and would ideally be crafted from real user conversations
  - Why unresolved: Current dataset relies on synthetic prompts from Bielik and PLLuM models
  - What evidence would resolve it: Comparative study evaluating models trained on PL-Guard versus benchmark derived from authentic user conversation logs

- **Open Question 3:** How does the robustness of specialized safety models degrade when extended to multimodal inputs involving visual data?
  - Basis: Authors list as limitation that current version does not support multimodal data
  - Why unresolved: Current study is restricted to text-only safety analysis
  - What evidence would resolve it: Developing multimodal PL-Guard variant and testing cross-modal risk assessment capabilities

## Limitations

- Training data composition shows inconsistent scaling trends - HerBERT performs best on adversarial examples when trained only on PL-Guard data, contrary to Llama models that improve with more data
- Cross-lingual transfer results based on limited evaluation with only one English safety benchmark, making it difficult to establish appropriate absolute performance levels
- Adversarial evaluation uses only lexical perturbations (typos, OCR errors, diacritic removal) without testing semantic perturbations like paraphrasing or cultural context shifts

## Confidence

**High Confidence:** HerBERT-based classifier achieves highest overall performance (0.913 F1 on PL-Guard-adv) is well-supported by direct experimental evidence and robust across multiple evaluation conditions.

**Medium Confidence:** Smaller, specialized models can outperform larger general-purpose models on language-specific safety tasks is supported but requires caveats - margin is not dramatic and may depend heavily on specific perturbation types tested.

**Low Confidence:** Encoder-only architectures are fundamentally more robust to character-level perturbations than autoregressive decoders lacks strong theoretical grounding - observed difference could be due to other factors such as model size, pre-training objectives, or data distribution.

## Next Checks

1. **Architectural ablation study:** Train both HerBERT and a BERT-based decoder on identical PL-Guard data to isolate whether robustness advantage stems from encoder-only vs decoder architecture or other factors.

2. **Semantic perturbation evaluation:** Extend adversarial testing to include semantic-level attacks (paraphrases, cultural context shifts, threat expression variations) to determine whether HerBERT's advantage persists beyond lexical perturbations.

3. **Real-world deployment pilot:** Conduct small-scale deployment of HerBERT-PL-Guard model on actual Polish language model outputs in controlled setting, measuring precision-recall tradeoffs and false positive/negative rates in practical use cases.