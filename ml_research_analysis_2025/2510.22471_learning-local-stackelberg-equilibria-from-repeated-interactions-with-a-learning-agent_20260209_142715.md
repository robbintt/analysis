---
ver: rpa2
title: Learning Local Stackelberg Equilibria from Repeated Interactions with a Learning
  Agent
arxiv_id: '2510.22471'
source_url: https://arxiv.org/abs/2510.22471
tags:
- utility
- principal
- stackelberg
- agent
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the problem of finding local Stackelberg equilibria
  in repeated games between a principal and a learning agent. The agent employs a
  mean-based learning algorithm, which makes it computationally intractable to find
  the global Stackelberg value.
---

# Learning Local Stackelberg Equilibria from Repeated Interactions with a Learning Agent
## Quick Facts
- arXiv ID: 2510.22471
- Source URL: https://arxiv.org/abs/2510.22471
- Reference count: 40
- One-line primary result: A polynomial-time algorithm finds an approximate local Stackelberg equilibrium in games with mean-based learning agents, with runtime exponential in 1/ε but polynomial in game size.

## Executive Summary
This paper addresses the challenge of computing Stackelberg equilibria in repeated games when the follower uses a mean-based learning algorithm. The authors show that finding the global Stackelberg value is computationally intractable under these conditions, so they focus on finding approximate local Stackelberg equilibria instead. They present an algorithm that iteratively optimizes within best-response polytopes and searches for neighboring polytopes, achieving a Polynomial Time Approximation Scheme (PTAS) with runtime polynomial in game size but exponential in 1/ε.

## Method Summary
The algorithm finds approximate local Stackelberg equilibria by iteratively optimizing within best-response polytopes and searching for neighboring polytopes. It works by maintaining an estimate of the current polytope, optimizing utility within this estimate, and detecting when the strategy crosses into a neighboring polytope. The principal uses a best-response oracle to approximate the agent's response and employs binary search to find polytope boundaries. The method is designed specifically for mean-based learning agents and proves that the exponential dependence on 1/ε is unavoidable for achieving polynomial runtime in game size.

## Key Results
- The algorithm provides a PTAS for local Stackelberg equilibria with runtime polynomial in game size but exponential in 1/ε
- The exponential dependence on 1/ε is proven to be unavoidable for polynomial-time algorithms
- The algorithm works under smoothed analysis when the agent's utility matrix is perturbed by Gaussian noise
- The method successfully handles the computational intractability of finding global Stackelberg equilibria against mean-based learners

## Why This Works (Mechanism)
### Mechanism 1
- Claim: A principal can efficiently find an approximate local Stackelberg equilibrium against a mean-based learning agent by making only local adjustments to the historical average strategy.
- Mechanism: The algorithm performs a local search by incrementally modifying the principal's average strategy and observing the agent's responses. It optimizes utility within current best-response polytopes and searches for neighboring polytopes upon reaching a local optimum.
- Core assumption: The agent employs a mean-based learning algorithm, making it computationally intractable to find the global Stackelberg value. Assumptions ensure polytopes are sufficiently separated.
- Break condition: If the agent deviates significantly from mean-based behavior, the algorithm's learning ability breaks down.

### Mechanism 2
- Claim: The optimization loop within a best-response polytope can be performed without knowing the exact boundaries.
- Mechanism: `OptimizeWithinPolytope` maintains an estimate of the current best-response polytope, initially the entire strategy simplex. It alternates between moving toward the utility-maximizing strategy and refining the estimate by detecting boundary crossings.
- Core assumption: The principal can detect polytope boundary crossings via a `BROracle` which repeatedly queries the agent.
- Break condition: If `BROracle` is unreliable or cannot efficiently approach a boundary, polytope estimation becomes inaccurate.

### Mechanism 3
- Claim: All neighboring polytopes within a certain radius can be discovered efficiently using an iterative dimension-reduction approach.
- Mechanism: `SearchForPolytopes` sequentially discovers separating hyperplanes. After each hyperplane is found, the search space is restricted to the intersection of previously discovered hyperplanes.
- Core assumption: Assumption ensures the number of surrounding polytopes is bounded and hyperplanes are well-separated.
- Break condition: If surrounding polytopes are not well-separated, random search may fail to discover all relevant neighbors.

## Foundational Learning
- Concept: **Stackelberg Games and Stackelberg Equilibrium**. A two-player game where a leader commits to a strategy first, and a follower best responds. The leader maximizes utility, anticipating the follower's response.
  - Why needed here: This is the fundamental game-theoretic model. The paper defines the "global Stackelberg value" as an intractable benchmark, motivating the focus on "local Stackelberg equilibria."
  - Quick check question: In a Stackelberg game, does the leader choose their action before or after observing the follower's action?

- Concept: **Mean-Based Learning Algorithms**. A class of online learning algorithms where the learner's strategy is an approximate best response to the empirical average of the opponent's past strategies (e.g., Fictitious Play).
  - Why needed here: The agent's use of this algorithm is the central constraint. Its long memory and slow adaptation make it difficult for the principal to query best responses to arbitrary strategies.
  - Quick check question: If an opponent plays actions A, A, B in rounds 1-3, what strategy would a Fictitious Play agent best-respond to in round 4?

- Concept: **Best-Response Polytopes**. For each follower action, the set of all leader mixed strategies for which that action is a best response. These polytopes partition the leader's strategy space.
  - Why needed here: The principal's utility is linear within each polytope but discontinuous across boundaries. The algorithm's search is structured around optimizing inside these polytopes and searching for neighbors at the boundaries.
  - Quick check question: Is the leader's utility function continuous across the entire strategy simplex? What geometric structures define its discontinuities?

## Architecture Onboarding
- Component map: `BROracle` -> `MoveOneStep` -> `OptimizeWithinPolytope` -> `SearchForPolytopes` -> `LocalStackelbergEquilibrium`
- Critical path:
  1. **Initialization**: Play arbitrary strategies for a "burn-in" period.
  2. **Polytope Optimization**: Call `OptimizeWithinPolytope` to find a locally optimal strategy within the current polytope.
  3. **Local Equilibrium Check**: Call `SearchForPolytopes` from the local optimum to find all neighboring polytopes and their utility values.
  4. **Decision**: If a better neighbor exists, step into it and repeat. If not, declare the current strategy an approximate local Stackelberg equilibrium.

- Design tradeoffs:
  - **Local vs. Global Optimality**: The algorithm is a PTAS for *local* equilibria, a tradeoff for computational tractability. It cannot guarantee finding the global Stackelberg value.
  - **Exponential vs. Polynomial Dependence**: Runtime is polynomial in game size but exponential in `1/ε`. High-precision requirements will be slow.
  - **Complexity**: The subroutines introduce significant complexity to handle discontinuous utilities and high-dimensional polytope boundaries efficiently.

- Failure signatures:
  - **Agent is not mean-based**: The assumption that the principal can only query local points is invalid.
  - **Boundary detection is unreliable**: If `BROracle` cannot reliably detect best-response changes, optimization is corrupted.
  - **Assumptions on utility structure are violated**: Degenerate utility matrices can cause `SearchForPolytopes` to fail.
  - **Runtime explosion**: For very small `ε`, the exponential term dominates.

- First 3 experiments:
  1. **Agent Algorithm Ablation**: Test performance against different agent algorithms (Fictitious Play, MWU, myopic best-responder). Validate the mean-based assumption.
  2. **Scalability and Precision Test**: Vary game size and `ε` to confirm polynomial dependence on size and exponential dependence on `1/ε`.
  3. **Polytope Structure Analysis**: For a small game, record and visualize the sequence of visited polytopes and discovered boundaries. Compare to the true polytope structure.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the equivalence between Local and Global Stackelberg Equilibria be extended to game classes beyond Stackelberg Security Games?
- Basis: Section 2.1 notes that while local is generally weaker than global, they are equivalent in the special case of security games.
- Why unresolved: The paper focuses on finding local equilibria in general games but does not characterize other instances where this utility gap disappears.
- Evidence: Identification of structural conditions on utility matrices that guarantee local optima are global.

### Open Question 2
- Question: Can the algorithm be adapted for agents using non-mean-based learning algorithms, such as Optimistic Mirror Descent?
- Basis: The paper explicitly restricts the agent class to "mean-based" learners, relying on the specific property that they best-respond to the average history.
- Why unresolved: Non-mean-based learners may react to recent history or specific patterns rather than the cumulative average, potentially invalidating the polytope search strategy.
- Evidence: Convergence guarantees for local equilibria when the principal interacts with agents exhibiting last-iterate convergence properties.

### Open Question 3
- Question: Can the assumption regarding prior knowledge of regularity parameters (`R_min` and `Δ`) be relaxed or estimated online?
- Basis: Assumptions 2.3 and 2.5 require known bounds on polytope margins and utility differences to set step sizes and oracle tolerances.
- Why unresolved: The current algorithm requires these parameters as inputs to ensure the search points remain robustly within polytopes; their absence may break the theoretical guarantees.
- Evidence: An adaptive algorithm that dynamically estimates the smoothness of the agent's best-response regions without explicit parameter bounds.

## Limitations
- The algorithm is specifically designed for mean-based learning agents and may not work effectively against other types of learning algorithms
- The exponential dependence on 1/ε means that achieving high precision (small ε) can be computationally expensive
- The method requires prior knowledge of regularity parameters (`R_min` and `Δ`) which may not always be available in practice

## Confidence
- **High confidence** in the core structural claim: The algorithm provides a PTAS for local Stackelberg equilibria with the claimed runtime complexity, and the exponential dependence on 1/ε is rigorously established as unavoidable.
- **Medium confidence** in the practical effectiveness of the subroutines, particularly `SearchForPolytopes` and `BinarySearch`, as empirical performance depends heavily on parameter choices and floating-point precision.
- **Low confidence** in the smoothed analysis guarantees for general utility matrices, as the relationship between matrix conditioning and polytope geometry in high dimensions is not fully explored.

## Next Checks
1. **Agent Algorithm Ablation**: Implement the algorithm against multiple agent types (Fictitious Play, Multiplicative Weights Update, and a myopic best-responder). Measure performance degradation when the agent deviates from mean-based behavior to validate the core assumption.

2. **Exponential Scaling Test**: For a fixed game size, measure runtime as ε decreases (e.g., ε ∈ {0.1, 0.05, 0.025, 0.0125}). Plot runtime vs. 1/ε on a log scale to empirically verify the exponential dependence predicted by Theorem 3.1.

3. **Polytope Discovery Reliability**: In a small game (m=3, n=3), instrument `SearchForPolytopes` to record all discovered hyperplanes and compare against the ground-truth best-response polytope structure. Measure the probability of missing a neighboring polytope as a function of the number of random samples d.