---
ver: rpa2
title: 'ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering'
arxiv_id: '2509.11663'
source_url: https://arxiv.org/abs/2509.11663
tags:
- questions
- question
- embodied
- exploration
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Embodied Questions Answering
  (EQsA), where an agent must handle multiple, asynchronous questions with varying
  urgencies in a 3D environment, extending beyond the single-question focus of classical
  EQA. To tackle this, the authors propose ParaEQsA, a parallel framework that leverages
  a shared group memory to reduce redundant exploration and a priority-planning module
  to dynamically schedule questions based on urgency, scope, reward, and dependencies.
---

# ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering

## Quick Facts
- **arXiv ID**: 2509.11663
- **Source URL**: https://arxiv.org/abs/2509.11663
- **Reference count**: 33
- **Primary result**: Proposed framework ParaEQsA significantly outperforms sequential baselines, reducing Normalized Steps to 0.321 and Normalized Urgency-Weighted Latency to 0.204, while achieving a Direct Answer Rate of 9.0% on the PAEQs dataset.

## Executive Summary
This paper addresses the problem of Embodied Questions Answering (EQsA) with multiple, asynchronous questions of varying urgencies in 3D environments. The authors propose ParaEQsA, a parallel framework that leverages shared group memory to reduce redundant exploration and a priority-planning module to dynamically schedule questions based on urgency, scope, reward, and dependencies. They introduce the Parallel Asynchronous Embodied Questions (PAEQs) dataset and novel metrics including Direct Answer Rate (DAR) and Normalized Urgency-Weighted Latency (NUWL). ParaEQsA demonstrates significant improvements over sequential baselines in efficiency, responsiveness, and knowledge reuse in multi-question scenarios.

## Method Summary
ParaEQsA employs a priority-based scheduling system where questions are scored using a weighted combination of urgency, scope, reward, and dependency factors. The framework uses a shared group memory that persists across questions within a scenario, enabling direct answers without additional exploration when sufficient information is already available. A VLM-guided targeted exploration mechanism focuses navigation on regions likely relevant to the current question. The system maintains a Question Pool with a Directed Acyclic Graph (DAG) to track dependencies and prevent deadlocks. Questions are processed in parallel rather than sequentially, with the agent able to explore for one question while other questions wait in the pool based on their priority scores.

## Key Results
- ParaEQsA achieves a Direct Answer Rate (DAR) of 9.0%, significantly higher than sequential baselines which score 0%
- Normalized Steps (NS) reduced to 0.321, compared to 0.472 for Explore-EQA and 0.410 for Memory-EQA baselines
- Normalized Urgency-Weighted Latency (NUWL) decreased to 0.204, demonstrating superior responsiveness to urgent questions
- Overall accuracy reaches 0.65, with ablation studies confirming the importance of priority-based scheduling

## Why This Works (Mechanism)

### Mechanism 1: Shared Group Memory for Knowledge Reuse
Maintaining a persistent, shared memory across multiple questions enables direct answering of some questions without additional exploration, reducing redundant navigation. As the agent explores for one question, it populates a group memory with visual and textual observations. When new questions arrive, a Finishing Module first queries this memory; if sufficient information exists, the question is answered immediately (zero exploration steps). This contrasts with sequential baselines that treat each question in isolation, forcing re-exploration of previously visited regions. The core assumption is that questions within a group share spatial or object-level relevance, such that observations gathered for one query are likely informative for others.

### Mechanism 2: Priority-Based Dynamic Question Scheduling
A weighted priority score combining urgency, scope, reward, and dependency reduces urgency-weighted latency by processing critical and low-effort questions before exploratory or dependent ones. A Question Pool maintains a buffer with a Directed Acyclic Graph (DAG) of dependencies. An Updater re-scores all questions whenever the pool changes using: `P(q_i) = w_u·Urgency + w_s·Scope + w_r·Reward + w_d·Dependency`. The Planner selects the highest-score question for targeted exploration. High-urgency questions use a non-linear transformation (`-ln(1-u_i)`) to sharply elevate their priority. Local-scope questions are prioritized over global ones to achieve quick wins. The core assumption is that the weighting coefficients approximate the true relative importance of each factor for latency reduction.

### Mechanism 3: VLM-Guided Targeted Exploration
Using a Vision-Language Model to assign semantic values to exploration frontiers focuses navigation on regions likely relevant to the current question, reducing steps needed to gather sufficient information. During exploration, a semantic map overlays spatial information with object labels. A VLM analyzes each observation to identify regions/objects relevant to the active question and assigns them a "semantic value." These values weight frontier-based exploration, biasing the agent toward high-relevance regions. A Stopping Module periodically evaluates if enough information has been gathered. The core assumption is that the VLM can accurately ground question-relevant semantics in visual observations.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) for Dependency Management**: The Question Pool uses a DAG to track dependencies between questions (e.g., "Is there dirt on my shirt?" depends on "Where is my shirt?"). Understanding DAGs is essential to reason about task ordering and deadlock prevention. Quick check: If question A depends on B, and B depends on C, which question must be answered first? Can a DAG have cycles?

- **Weighted Multi-Objective Scoring**: The priority score combines four normalized factors (urgency, scope, reward, dependency) with learnable or tuned weights. Engineers must understand how to balance competing objectives and interpret sensitivity. Quick check: If w_u is increased relative to w_s, what happens to the priority of a high-urgency global-scope question vs. a low-urgency local-scope question?

- **Vision-Language Models (VLMs) for Semantic Grounding**: VLMs (GPT-5-mini, Qwen2.5-VL) are used for question parsing, semantic annotation, and stopping decisions. Understanding their capabilities and failure modes is critical for diagnosing exploration and answering errors. Quick check: How might a VLM fail when asked to identify "the red mug on the counter" in a cluttered image? What safety checks could mitigate this?

## Architecture Onboarding

- **Component map**: Generator -> Parser -> Finishing Module -> Question Pool -> Updater -> Planner -> Stopping Module -> Answering Module, with Group Memory shared across questions
- **Critical path**: Question arrives → Parser → Finishing Module checks Group Memory → if answerable: Answering Module → if not: Question Pool → Updater scores all questions → Planner selects highest-priority → Targeted Exploration loop → when stopping triggered: Answering Module
- **Design tradeoffs**: Parallel scheduling + shared memory reduces redundancy but adds coordination complexity; LLM/VLM choice balances performance and cost; exploration continuity assumes spatial locality; urgency inference introduces noise if prompts are ambiguous
- **Failure signatures**: DAR near 0% indicates Group Memory not persisting or retrieval failing; high NUWL despite priority suggests urgency mis-inferred or w_u too low; high NS with low accuracy indicates VLM semantic grounding failing; deadlock in Question Pool suggests circular dependencies
- **First 3 experiments**: 1) Run ParaEQsA vs. Explore-EQA and Memory-EQA on PAEQs, logging Acc, DAR, NS, NUWL to verify improvements; 2) Disable each priority factor (urgency, scope, reward, dependency) one at a time to quantify impact on NS and NUWL; 3) Inject a question whose answer was fully observed during exploration for a previous question to confirm DAR path (zero steps) and high confidence

## Open Questions the Paper Calls Out
The paper explicitly identifies extending the framework to multi-agent coordination as a major future direction, noting that adapting ParaEQsA for decentralized collaborative solving of EQsA tasks presents new challenges in task allocation and information sharing that require empirical validation.

## Limitations
- The exact weighting coefficients (w_u, w_s, w_r, w_d) for priority calculation are not specified, making it difficult to reproduce the precise scheduling behavior
- The method for inferring dependencies between questions from text is not detailed, which could lead to circular dependencies or missed prioritization opportunities
- The system's performance on open-vocabulary or generative answers rather than multiple-choice format remains unverified
- Real-world VLM inference latency and computational overhead are not accounted for in the efficiency metrics

## Confidence
- **High**: Core architectural claims about shared memory reducing redundant exploration and priority-based scheduling improving urgency-weighted latency are well-supported by ablation results
- **Medium**: Effectiveness of VLM-guided targeted exploration is inferred from step reduction metrics but relies on unstated VLM prompt quality and stopping criteria
- **Low**: Exact quantitative impact of each priority component is not isolated in ablation; observed improvements could be due to combined effects

## Next Checks
1. **Weight sensitivity sweep**: Systematically vary w_u from 0.1 to 0.9 while holding other weights constant. Plot NUWL and DAR vs. w_u to identify optimal range and confirm urgency weighting is the dominant factor.
2. **Dependency DAG robustness**: Manually inject synthetic dependencies and verify the DAG prevents cycles and correctly reorders priorities. Test with missing or incorrect dependency labels.
3. **VLM failure stress test**: Replace GPT-5-mini with a weaker VLM and measure degradation in DAR, NS, and NUWL. Introduce adversarial questions with rare objects or ambiguous descriptions to quantify grounding robustness.