---
ver: rpa2
title: 'TheMCPCompany: Creating General-purpose Agents with Task-specific Tools'
arxiv_id: '2510.19286'
source_url: https://arxiv.org/abs/2510.19286
tags:
- gid00001
- tool
- tools
- azure
- gid00032
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TheMCPCompany, a benchmark for evaluating
  tool-calling agents in complex enterprise environments. It provides over 18,000
  task-specific tools across real-world services including Microsoft Azure, exposing
  full functionality through MCP servers.
---

# TheMCPCompany: Creating General-purpose Agents with Task-specific Tools

## Quick Facts
- arXiv ID: 2510.19286
- Source URL: https://arxiv.org/abs/2510.19286
- Reference count: 40
- Primary result: Task-specific MCP tools improve performance by 13.79 points and reduce costs by $2.29 per task compared to browser-based agents

## Executive Summary
This paper introduces TheMCPCompany, a benchmark for evaluating tool-calling agents in complex enterprise environments with over 18,000 task-specific tools across real-world services including Microsoft Azure. The authors implement MCPAgent, which uses a tool finder function to dynamically discover and invoke required tools from a gateway MCP server. Experiments show that task-specific tools significantly outperform browser-based approaches in both performance and cost, but agents struggle with complex multi-service orchestration, revealing challenges in navigating thousands of tools and combining them for non-trivial problem solving.

## Method Summary
TheMCPCompany evaluates tool-calling agents using 18,505 MCP tools across Azure, GitLab, RocketChat, Plane, and ownCloud services. The MCPAgent uses a gateway MCP server with two functions: `find_tools` (dense retrieval via embedding similarity) and `call_tool` (execution). Tasks include 175 from TheAgentCompany benchmark plus 10 primitive and 7 composite Azure tasks. The system uses OpenHands 0.48.0 CodeAct agent as base, modified to remove browser and add MCP integration. Models tested include GPT-4.1, o3, GPT-5-mini, GPT-5, Sonnet-4, and Opus-4.1 with ground-truth tools and tool retrieval conditions.

## Key Results
- Task-specific tools improve performance by 13.79 points and reduce costs by $2.29 per task compared to browser-based agents
- MCPAgent with tool retrieval outperforms browser-based approaches by 5.39 points even without oracle tools
- Advanced models (GPT-5) nearly close the gap between retrieval and oracle tool access in simpler environments
- Agents struggle significantly with complex Azure tasks, achieving only 1/7 success on composite tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific MCP tools outperform browser-based interaction when tools are correctly retrieved.
- Mechanism: Direct tool invocation eliminates GUI navigation overhead and reduces context tokens—agents process only relevant outputs rather than full webpage content. This lowers inference cost and focuses model capacity on reasoning rather than parsing.
- Core assumption: The semantic gap between task descriptions and tool specifications can be bridged by the agent's ability to decompose tasks and generate appropriate search queries.
- Evidence anchors: [abstract] "task-specific tools improve performance by 13.79 points and reduce costs by $2.29 per task compared to browser-based agents"; [section 5.2] "using task-specific tools increases performance by 13.79 points on average... reduces inference costs by $2.29 on average per task".
- Break condition: When task descriptions share minimal semantic overlap with tool specifications and the model cannot decompose tasks into searchable sub-goals, retrieval recall drops.

### Mechanism 2
- Claim: Agent-driven dynamic tool discovery via a gateway MCP server enables scalable operation across thousands of tools.
- Mechanism: A two-function gateway (`find_tools` + `call_tool`) treats retrieval itself as an agentic action. The model constructs queries, receives top-k tool specifications via embedding similarity, infers dependencies from arguments or error messages, and iteratively searches. This allows exploration of alternative solution paths without pre-loading all tools.
- Core assumption: The model can infer inter-tool dependencies from argument schemas or execution failures and generate follow-up queries accordingly.
- Evidence anchors: [abstract] "MCPAgent with tool retrieval outperforms browser-based approaches by 5.39 points"; [section 4] "the tool finder uses a text embedding model to encode the JSON specification of the tools and also the agent's query... returns the specification for the top-k tools".
- Break condition: When models cannot proactively identify dependencies or generate sufficiently specific queries, performance degrades.

### Mechanism 3
- Claim: Advanced reasoning models can nearly close the gap between retrieval-based and oracle tool access in simpler environments, but struggle with complex multi-service orchestration.
- Mechanism: Stronger models (GPT-5, Opus-4.1) generate longer, more comprehensive queries (52.9 chars avg for GPT-5 vs 31.6 for GPT-4.1), persist through failed attempts, and systematically explore alternatives. However, in complex environments (Azure composite tasks), systematic problem-solving diminishes—models fixate on common causes without verification, implement partial solutions, and fail to validate outcomes.
- Core assumption: Reasoning capability generalizes from tool discovery to multi-step problem-solving, but this generalization is environment-dependent.
- Evidence anchors: [abstract] "GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools"; [section 5.3] "all models consistently fail on almost all these tasks [Azure composite]... agents mainly struggle with the diversity and complexity of Azure services".
- Break condition: When task complexity requires coordinating across many services with non-obvious dependencies, even advanced models fail to explore systematically.

## Foundational Learning

- Concept: **Model Context Protocol (MCP) tool schema structure**
  - Why needed here: Understanding JSON specifications (name, description, inputSchema) is essential for interpreting `find_tools` output and constructing valid `call_tool` invocations. The benchmark exposes 18,505 tools via standardized schemas.
  - Quick check question: Given a tool with `inputSchema` requiring `subscriptionId` and `resourceGroupName`, can you identify which arguments must be sourced from prior tool outputs vs. task instructions?

- Concept: **Dense retrieval with embedding-based similarity**
  - Why needed here: MCPAgent's tool finder uses cosine similarity between query embeddings and tool specification embeddings. Understanding this helps debug why certain queries return irrelevant tools and how to reformulate.
  - Quick check question: If a query for "list virtual machines" returns storage-related tools, what does this indicate about the embedding space, and how might you adjust the query?

- Concept: **Inter-tool dependency inference**
  - Why needed here: Complex tasks require chaining tools where output from one provides input to another. The paper shows models infer dependencies from argument requirements (proactive) or error messages (reactive).
  - Quick check question: A retrieved tool requires `principalId` but you only have a username. Do you: (a) fail and report error, (b) search for a user-to-principalId mapping tool, or (c) guess a value?

## Architecture Onboarding

- Component map: Task → decompose into sub-tasks → `find_tools` per sub-task → reason about dependencies → `call_tool` → handle errors/iterate
- Critical path:
  1. Task instruction received
  2. Agent generates initial `find_tools` query based on task decomposition
  3. Gateway returns top-k tool specifications (default k=5)
  4. Agent inspects arguments, infers missing dependencies
  5. If dependencies missing: issue new `find_tools` query OR call dependency-resolution tools first
  6. Execute `call_tool` with constructed arguments
  7. On error: parse error message, search for alternative tools or fix arguments
  8. Repeat until task complete or budget exhausted
- Design tradeoffs:
  - **k value (tools retrieved)**: Higher k increases recall but consumes context window; paper shows ~15-25 retrieved tools on average is sufficient for TheAgentCompany tasks
  - **Query formulation strategy**: Longer, specific queries (GPT-5: 52.9 chars) improve recall but require stronger models; shorter queries (o3: 19.2 chars) may miss relevant tools
  - **Proactive vs. reactive dependency resolution**: Proactive (infer from arguments) is faster but requires stronger reasoning; reactive (infer from errors) adds steps but works with weaker models
- Failure signatures:
  - **Instruction drift with wrong tools** (33% of GPT-4.1 failures): Agent finds tangentially related tool and completes wrong task variant
  - **Instruction drift with correct tools** (50% of GPT-4.1 failures): Agent has correct tools but ignores formatting/completeness requirements—speculated cause: increased cognitive load from retrieval context
  - **Fixation without verification** (Azure composite): Agent assumes common cause (e.g., IAM issues) without checking logs or testing hypothesis
  - **Partial solution implementation**: Agent changes configuration but not application code, or vice versa, breaking functionality
- First 3 experiments:
  1. **Reproduce oracle vs. retrieval gap**: Run MCPAgent with GPT-4.1 on 10 TheAgentCompany tasks with oracle tools vs. `find_tools`. Measure score difference, retrieval recall, and instruction drift rate. Compare against Table 2 baseline.
  2. **Ablate query length**: Modify system prompt to encourage/disourage verbose queries. Test if forcing longer queries on GPT-4.1 improves retrieval recall (hypothesis: partial improvement, but reasoning limitations remain).
  3. **Stress-test dependency inference**: Create synthetic tasks where Tool A output is required for Tool B input but schema doesn't explicitly state this. Measure how often models proactively search for bridging tools vs. fail and require error-triggered recovery.

## Open Questions the Paper Calls Out

- What mechanisms can effectively mitigate the risks of unintended actions in production environments without restricting the action space of LLM agents? The Limitations section states, "We encourage future work to also investigate potential approaches for mitigating the side effects of LLM actions without limiting the available actions."

- Does the cognitive load induced by tool retrieval negatively impact instruction-following capabilities in general-purpose agents? The MCPAgent Error Analysis speculates that "increased cognitive load and context length from tool retrieval reduces the model’s instruction following capabilities."

- How do current tool-calling agents perform on complex enterprise scenarios such as multi-subscription governance or disaster recovery? The Limitations section explicitly invites future work to "investigate LLMs’ behavior on other tasks and types of problems, such as multi-subscription governance."

## Limitations

- Model access limitations: The paper relies on proprietary models (GPT-5, Opus-4.1) not publicly available, making exact reproduction impossible. The performance claims for these models may not be achievable with currently accessible alternatives.

- Tool retrieval scalability gaps: While the benchmark demonstrates retrieval challenges with 18,505 tools, the paper doesn't address how retrieval performance scales beyond this or how performance degrades with millions of tools. The 16.2% Recall@20 for standard retrieval suggests fundamental limitations in current approaches.

- Enterprise complexity boundary: Azure composite tasks represent the most complex environment tested, but even these are simplified compared to real enterprise scenarios. The 1/7 success rate on composite tasks indicates current agents cannot handle production-level complexity.

## Confidence

**High confidence**: Task-specific tools improve performance (13.79 points) and reduce costs ($2.29) compared to browser-based agents. This is supported by controlled experiments with clear metrics and reproducible methodology.

**Medium confidence**: Advanced reasoning models (GPT-5) nearly match oracle tool performance in simpler environments. While the gap is quantified, the proprietary model access limits independent verification.

**Low confidence**: The claim that current agents can handle "general-purpose" enterprise environments. The Azure composite task results (1/7 success) directly contradict this, suggesting the agents are far from general-purpose.

## Next Checks

1. **Independent retrieval benchmark**: Reproduce the retrieval performance using open-source models (Llama-3.1-70B, DeepSeek-Coder-V2) on the same 18,505 tool corpus. Measure Recall@20 for various query lengths and compare against the paper's proprietary model results.

2. **Dependency resolution stress test**: Create a synthetic tool corpus where Tool A requires Tool B output but schema doesn't explicitly state this dependency. Measure how often models proactively search for bridging tools vs. fail and require error-triggered recovery across different model families.

3. **Cost-performance tradeoff analysis**: Run MCPAgent with increasing k values (1, 5, 10, 20) on a subset of tasks to quantify the retrieval-recall relationship and identify the optimal k that maximizes performance while minimizing context window costs.