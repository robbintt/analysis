---
ver: rpa2
title: 'TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering
  Agents'
arxiv_id: '2505.13291'
source_url: https://arxiv.org/abs/2505.13291
tags:
- challenges
- time
- timeseriesgym
- agents
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TimeSeriesGym is a scalable benchmark framework for evaluating
  AI agents on time series machine learning engineering tasks. It features 34 challenges
  spanning 8 unique problems across 15+ domains, including Kaggle competitions and
  original tasks.
---

# TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents

## Quick Facts
- arXiv ID: 2505.13291
- Source URL: https://arxiv.org/abs/2505.13291
- Authors: Yifu Cai; Xinyu Li; Mononito Goswami; Michał Wiliński; Gus Welter; Artur Dubrawski
- Reference count: 40
- Primary result: Evaluates AI agents on 34 time series ML engineering challenges; best model (o3) achieves 94.4% valid submissions but only 33.3% reasonable submissions

## Executive Summary
TimeSeriesGym introduces a comprehensive benchmark framework for evaluating AI agents on time series machine learning engineering tasks. The framework features 34 challenges across 8 problem types spanning 15+ domains, with challenges derived from Kaggle competitions, existing ML repositories, and original tasks. The benchmark employs a multimodal evaluation approach combining quantitative metrics with LLM-based qualitative assessment to evaluate diverse artifacts including prediction files, code, and trained models. Experiments reveal that while current AI agents can produce valid submissions, they struggle with more complex ML engineering tasks, particularly original challenges requiring multi-file code analysis and integration.

## Method Summary
The benchmark provides a Docker-based environment with standardized datasets, code repositories, and challenge descriptions. Agents receive access to `/home/data/` containing challenge resources and must submit artifacts to `/home/submission/` for automated grading. The evaluation combines exact grading (numeric metrics, regex matching, code inspection) with LLM-as-judge assessment using the G-Eval framework for contextual quality evaluation. The framework includes specialized tools for scalable challenge generation and artifact grading, with TimeSeriesGym-Lite offering a reduced 6-challenge subset for efficient evaluation. Experiments compare different agent scaffolds (AIDE vs. OpenHands) and models (GPT-4.1, o3) across 34 challenges with varying difficulty levels and skill requirements.

## Key Results
- GPT-4.1 with AIDE scaffolding produces more valid submissions than OpenHands on Kaggle challenges
- The best model (o3) achieves 94.4% valid submissions but only 33.3% reasonable submissions
- Agents struggle particularly with TimeSeriesGym Originals challenges requiring multi-file code analysis
- TimeSeriesGym-Lite enables efficient evaluation with 6 challenges covering critical ML engineering skills

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining quantitative metrics with LLM-based qualitative assessment enables more comprehensive evaluation of multimodal ML artifacts than either approach alone.
- Mechanism: Exact grading (numeric metrics, regex matching, code inspection) validates technical compliance, while LLM-as-judge evaluates open-ended qualities like code architecture and implementation elegance through chain-of-thought reasoning.
- Core assumption: LLM judges can reliably assess code quality and research implementation quality when given structured evaluation criteria.
- Evidence anchors: [abstract] "using both precise numeric measures and more flexible LLM-based evaluation approaches. This dual strategy balances objective assessment with contextual judgment." [Section 3, Multimodal evaluation] "This hybrid approach balances the reliability of objective metrics with the flexibility of subjective assessment."

### Mechanism 2
- Claim: Isolating specific ML engineering skills through targeted challenge design provides actionable diagnostic feedback rather than opaque aggregate scores.
- Mechanism: Challenges are categorized by required skills (data handling, hyperparameter tuning, code migration, research code utilization), enabling identification of specific capability gaps when agents fail particular challenge types.
- Core assumption: Skill deficiencies identified on isolated challenges generalize to real-world ML engineering workflows that combine multiple skills.
- Evidence anchors: [Section 1] "evaluations are typically outcome-based... while combining and obfuscating the impact of multiple component skills" [Table 3] Shows 34 challenges mapped to specific skills with corresponding metrics.

### Mechanism 3
- Claim: Agent-agnostic scaffold design enables fair comparison across fundamentally different agent architectures while maintaining consistent evaluation conditions.
- Mechanism: The benchmark provides standardized resources and Docker environments, allowing any agent scaffold to interact through the same interface regardless of internal planning or execution strategies.
- Core assumption: Different scaffolds (AIDE's tree search vs. OpenHands' CodeAct) can meaningfully be compared using the same success metrics.
- Evidence anchors: [Section 3.1, Agent-agnostic design] "TimeSeriesGym is agnostic to specific agent implementations... we include latest implementations of 3 different scaffolds, AIDE, ResearchAgent, and OpenHands with fundamentally different designs." [Table 2] Shows direct comparison of OpenHands vs. AIDE with same model, enabling scaffold-level analysis.

## Foundational Learning

- Concept: Agent scaffolds (AIDE vs. OpenHands architecture)
  - Why needed here: The paper compares AIDE (tree-based solution search) and OpenHands (CodeAct agent with greedy exploitation), and understanding their architectural differences explains performance gaps on repository-level vs. single-file tasks.
  - Quick check question: Can you explain why AIDE's single-step solution strategy fails on multi-file codebase challenges?

- Concept: LLM-as-judge evaluation methodology
  - Why needed here: The benchmark relies on G-Eval framework for qualitative code assessment; understanding chain-of-thought judging and score normalization is necessary to interpret judge-based results.
  - Quick check question: What are the trade-offs between deterministic exact grading and LLM-based judge grading for code migration tasks?

- Concept: Time-series ML task taxonomy (forecasting, classification, anomaly detection, imputation)
  - Why needed here: The 34 challenges span 8 problem types with domain-specific metrics (SMAPE, RMSE, F1); understanding these task categories is prerequisite to selecting appropriate challenges for agent evaluation.
  - Quick check question: Which evaluation metric would be appropriate for an imputation challenge on PM2.5 weather data?

## Architecture Onboarding

- Component map: Challenge resources → `/home/data/` (datasets, repositories), `/home/data/description.md` (task specification) → Submission interface → `/home/submission/` (output artifacts), validation server at `localhost:5000/validate` → Grading pipeline → Exact grading (regex, AST parsing, linting) + Judge grading (G-Eval with structured criteria) → Agent scaffold adapters → Modified AIDE and OpenHands implementations with unified environment setup

- Critical path: 1. Load challenge configuration (description.md, dataset paths, grading function) 2. Initialize Docker container with resources and base Python packages 3. Execute agent scaffold within resource limits (4h/50 steps default) 4. Collect submission artifacts (prediction files, code, models) 5. Apply challenge-specific grading (exact + judge-based where applicable) 6. Compute aggregate metrics (valid submission rate, reasonable submission rate)

- Design tradeoffs: TimeSeriesGym-Lite (6 challenges, ~$8/run) vs. Full benchmark (34 challenges, ~$62/run): Lite enables rapid iteration but reduces skill coverage; Structured output requirements (specific file formats, model checkpoints) vs. agent autonomy: More structure enables precise evaluation but constrains creative solutions; LLM judge scoring provides nuanced feedback but introduces variability; exact grading is reproducible but may miss implementation quality

- Failure signatures: OpenHands: Greedy exploitation bias—commits to single approach without backtracking, wastes steps on linear file scanning (see Appendix D.1); AIDE: Single-file encapsulation with subprocess calls obscures tracebacks; `if __name__ == "__main__"` blocks skipped due to exec() in non-main scope (see Appendix D.2, D.3); Both scaffolds: Rush toward solutions without strategic time allocation; step-wise reminders don't improve performance

- First 3 experiments: 1. Run GPT-4.1 with AIDE on TimeSeriesGym-Lite to establish baseline valid/reasonable submission rates 2. Compare OpenHands vs. AIDE scaffolds with identical model to isolate architectural effects on repository-level challenges 3. Test o3 reasoning model to validate claim that reasoning models produce higher valid submission rates (94.4% vs. 66.7% reported in Table 2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can plagiarism and appropriate code reuse be clearly defined and evaluated for LLM agents in open-ended ML engineering tasks?
- Basis in paper: [explicit] The authors state it is "crucial to develop clear, legally correct definitions and evaluation criteria for data contamination and plagiarism" when agents leverage existing code repositories.
- Why unresolved: It is difficult to distinguish between a model hallucinating a solution and appropriately reusing a public resource with citation, similar to human practitioners.
- What evidence would resolve it: A standardized rubric or framework that successfully differentiates valid code citation from plagiarism in agent-generated artifacts.

### Open Question 2
- Question: What robust, diagnostic success metrics can evaluate agents holistically while preserving autonomy and providing actionable feedback?
- Basis in paper: [explicit] The paper notes that "the development of robust, holistic and diagnostic success metrics remains an important research direction" beyond simple accuracy or leaderboard rankings.
- Why unresolved: Current metrics like accuracy or binary completion rates fail to capture nuances like code quality or partial progress, and direct comparisons to human leaderboards are often misleading due to data splits.
- What evidence would resolve it: New evaluation protocols that correlate with real-world utility and successfully guide specific improvements in agent design.

### Open Question 3
- Question: How can agent scaffolds be redesigned to balance exploration and exploitation for complex, repository-level tasks?
- Basis in paper: [explicit] The authors highlight the "need for more adaptive scaffolds that dynamically expand their action repertoire [and] balance exploration and exploitation" to handle large codebases.
- Why unresolved: Current designs like AIDE (single-file focus) and OpenHands (greedy exploitation bias) fail to identify critical information across multiple files in original research challenges.
- What evidence would resolve it: A new scaffold architecture that demonstrably improves valid submission rates on multi-file "Original" challenges compared to existing baselines.

## Limitations
- Reproducibility barriers due to missing instructions for accessing Kaggle datasets and TimeSeriesGym Originals grading functions
- LLM-as-judge evaluation methodology introduces significant variability without established inter-annotator agreement measures
- Benchmark scope limitations—34 curated challenges may not generalize to broader ML engineering tasks
- Agent scaffold comparison assumes architectural differences can be fairly evaluated using same metrics

## Confidence
- High Confidence: Technical implementation (Docker setup, resource allocation, basic evaluation pipeline) appears well-specified and reproducible in principle
- Medium Confidence: Core finding that current AI agents struggle with ML engineering tasks is supported by quantitative results, though evaluation methodology introduces uncertainty
- Low Confidence: Claim that TimeSeriesGym provides actionable diagnostic feedback through skill-specific challenge isolation requires further validation

## Next Checks
1. Reproduce TimeSeriesGym-Lite: Implement the minimal benchmark with 6 challenges using provided Docker configurations and scaffold modifications to verify baseline valid/reasonable submission rates match paper claims

2. Cross-Annotator Reliability Test: Run the LLM-as-judge evaluation on identical submissions with multiple prompts to quantify score variance and establish inter-annotator agreement for qualitative assessments

3. Skill Transfer Validation: Design a follow-up experiment testing whether agents that perform well on isolated skill challenges (data handling, hyperparameter tuning) demonstrate improved performance on multi-skill TimeSeriesGym Originals challenges