---
ver: rpa2
title: 'T$^\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL'
arxiv_id: '2601.11214'
source_url: https://arxiv.org/abs/2601.11214
tags:
- block
- equation
- tracerl
- size
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of scaling masked diffusion\
  \ language models (MDMs) to larger block sizes without significant performance degradation\
  \ on mathematical reasoning tasks. The core method, T\u22C6, employs trajectory-aware\
  \ reinforcement learning (TRACERL) in a progressive block scaling strategy, starting\
  \ from an autoregressive-initialized small-block MDM and gradually increasing block\
  \ size through alternating RL updates and block boundary shifts."
---

# T$^\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL

## Quick Facts
- **arXiv ID:** 2601.11214
- **Source URL:** https://arxiv.org/abs/2601.11214
- **Reference count:** 32
- **Primary result:** Progressive block scaling with trajectory-aware RL enables larger block sizes in MDMs while maintaining math reasoning performance

## Executive Summary
This work addresses the challenge of scaling masked diffusion language models (MDMs) to larger block sizes without significant performance degradation on mathematical reasoning tasks. The core method, T⋆, employs trajectory-aware reinforcement learning (TRACERL) in a progressive block scaling strategy, starting from an autoregressive-initialized small-block MDM and gradually increasing block size through alternating RL updates and block boundary shifts. This approach enables higher-parallelism decoding while preserving strong reasoning capability. Experiments on SDAR models (1.7B and 4B) show that T⋆ consistently matches or exceeds the performance of both base models and direct TRACERL training at the same block size on benchmarks like MATH500, GSM8K, and AIME24. Analysis further reveals that T⋆ induces an alternative denoising schedule that achieves comparable performance to canonical left-to-right decoding, without requiring external search procedures.

## Method Summary
T⋆ introduces a progressive block scaling approach for masked diffusion models that starts with a small block size (B=4) and gradually increases it through a curriculum of stages. At each stage, the model undergoes TRACERL training with a 50/50 split between normal and shifted block boundaries, then doubles the block size. The TRACERL component uses trajectory-aware reinforcement learning to optimize denoising trajectories based on sequence-level correctness rewards, computed via generalized advantage estimation. The method is evaluated on SDAR-1.7B-Chat and SDAR-4B-Chat models initialized at B=4, trained on 8K math problems from Openr1math (levels 3-5), and tested on MATH500, GSM8K, and AIME24 benchmarks. Training uses AdamW optimizer with learning rate 1e-6 and KL penalty β=0.01, with 16 responses per problem and batch size 128 across 8×H200 GPUs.

## Key Results
- T⋆ achieves 81.4% pass@3 on MATH500 at B=16, matching or exceeding both base models and direct TRACERL at the same block size
- Progressive scaling prevents the sharp accuracy drops seen in direct TRACERL when increasing block size from B=4 to B=8 and B=16
- The method induces an alternative denoising schedule that achieves comparable performance to canonical left-to-right decoding without external search procedures
- Performance gains are consistent across multiple benchmarks including GSM8K and AIME24, with T⋆ matching or exceeding base model performance at equivalent block sizes

## Why This Works (Mechanism)
The progressive block scaling approach works by gradually adapting the model to larger block sizes while maintaining denoising capability. By starting with a small, well-trained block size and using trajectory-aware RL with alternating boundary shifts, the model can learn to handle increased context without catastrophic forgetting. The boundary shift mechanism specifically addresses the challenge of maintaining consistency across block boundaries when the block size changes. The trajectory-aware component allows the model to optimize its denoising decisions based on the full sequence-level reward, rather than just local token-level objectives. This combination enables the model to maintain reasoning capability even as block size increases, effectively learning an alternative decoding schedule that can match canonical left-to-right performance.

## Foundational Learning
- **Masked Diffusion Models (MDMs):** A denoising diffusion framework where tokens are unmasked in blocks rather than sequentially. Needed because traditional autoregressive models have limited parallelism. Quick check: Verify that the model can denoise tokens given partial context from neighboring blocks.
- **Trajectory-Aware Reinforcement Learning:** RL method that optimizes denoising trajectories based on sequence-level rewards using generalized advantage estimation. Needed to align local denoising decisions with global reasoning performance. Quick check: Confirm that trajectory advantages correlate with sequence correctness.
- **Block Boundary Shifting:** Training technique where 50% of batches use shifted block boundaries during progressive scaling. Needed to ensure the model generalizes across different boundary positions. Quick check: Compare accuracy on shifted vs. unshifted samples after boundary adaptation.
- **Generalized Advantage Estimation (GAE):** Method for computing step-level advantages from sequence-level rewards in RL. Needed to provide stable learning signals for trajectory optimization. Quick check: Verify that GAE produces reasonable advantage estimates that improve over random initialization.

## Architecture Onboarding

**Component Map:**
SDAR model -> TRACERL training -> Progressive scaling (B=4→8→16→32) -> Boundary shift mechanism -> Final MDM with large blocks

**Critical Path:**
Training flow: Initialize small-block MDM → TRACERL with 50/50 normal/shifted blocks → Double block size → Repeat until target size. Inference flow: Start with all-masked sequence → Denoise in blocks using learned schedule → Output completed sequence.

**Design Tradeoffs:**
- Progressive scaling vs. direct scaling: T⋆ sacrifices some training efficiency for significant performance gains at larger blocks
- Boundary shift mechanism: Adds complexity but prevents boundary artifacts during block expansion
- Trajectory-aware RL vs. token-level training: More computationally expensive but aligns with sequence-level reasoning tasks

**Failure Signatures:**
- Sharp accuracy drops when block size increases (indicates insufficient boundary adaptation)
- Inconsistent performance between shifted and unshifted blocks (suggests boundary artifacts)
- Slow convergence during TRACERL training (may indicate poor advantage estimation or learning rate issues)

**3 First Experiments:**
1. Train SDAR-1.7B-Chat-b4 on Openr1math with TRACERL at B=4, verify basic denoising capability and trajectory advantages
2. Implement boundary shift mechanism and test accuracy difference on shifted vs. unshifted samples at B=4
3. Run first progressive scaling stage (B=4→8) and monitor validation accuracy to confirm absence of sharp degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap remains between T⋆ and autoregressive baselines at large block sizes (B=32), with approximately 5% difference on MATH500
- Progressive scaling shows diminishing returns beyond B=16, suggesting a practical limit to the approach
- Several methodological details are underspecified, including exact GAE hyperparameters, inference parameters, and training epochs per stage

## Confidence
- **High confidence:** Progressive block scaling with T⋆ outperforms direct scaling on math reasoning benchmarks
- **Medium confidence:** T⋆ induces an alternative denoising schedule achieving comparable performance to left-to-right decoding
- **Medium confidence:** T⋆ matches or exceeds base model performance at equivalent block sizes, though margins are sometimes narrow

## Next Checks
1. Replicate progressive scaling results by training SDAR-1.7B-Chat-b4 using T⋆ through B=4→8→16→32 on Openr1math, evaluating pass@3 accuracy on MATH500 at each stage to confirm absence of sharp degradation
2. Validate boundary shift mechanism by implementing the 50/50 split between normal and shifted blocks during TRACERL updates, then analyzing accuracy differences on shifted vs. unshifted samples
3. Analyze LOCALSTRICT metric computation by replicating the decoding schedule analysis from Figure 4, computing LOCALSTRICT scores for T⋆ versus canonical left-to-right decoding to verify alternative schedule performance