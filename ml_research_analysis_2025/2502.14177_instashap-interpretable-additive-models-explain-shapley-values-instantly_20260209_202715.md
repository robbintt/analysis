---
ver: rpa2
title: 'InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly'
arxiv_id: '2502.14177'
source_url: https://arxiv.org/abs/2502.14177
tags:
- shap
- instashap
- error
- which
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a deep connection between Shapley values
  (SHAP) and generalized additive models (GAMs), showing they are fundamentally equivalent
  under certain conditions. The authors demonstrate that SHAP's limitations in handling
  feature interactions are shared with GAMs, and propose a new method called InstaSHAP
  that automatically purifies GAM models to compute Shapley values in a single forward
  pass.
---

# InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly

## Quick Facts
- **arXiv ID:** 2502.14177
- **Source URL:** https://arxiv.org/abs/2502.14177
- **Reference count:** 40
- **Primary result:** InstaSHAP computes Shapley values in a single forward pass by training a surrogate GAM, offering faster and more accurate explanations than FastSHAP while providing a trustworthiness check based on accuracy gaps.

## Executive Summary
This paper establishes that Shapley values (SHAP) and Generalized Additive Models (GAMs) are fundamentally equivalent under certain conditions, reframing SHAP as an optimization problem rather than a weighted average. The authors introduce InstaSHAP, a method that trains a GAM surrogate to explain black-box models, automatically purifying additive components to handle feature correlations without post-hoc adjustments. A key contribution is the trustworthiness audit: if a GAM cannot match a DNN's accuracy, SHAP explanations are likely insufficient because they cannot capture necessary feature interactions.

## Method Summary
InstaSHAP trains a neural network constrained to a GAM architecture that outputs Shapley values directly in a single forward pass. The method samples feature subsets from the SHAP kernel distribution and uses a masking objective to enforce additive structure during training. For each input, the model predicts the masked output of the black-box function, with the loss function ensuring that only the shape functions corresponding to the present features contribute to the prediction. This approach automatically handles feature correlations and provides interpretable explanations while being model-agnostic.

## Key Results
- InstaSHAP computes Shapley values in a single forward pass, eliminating the need for expensive post-hoc computation
- Experimental results show InstaSHAP provides more accurate and interpretable explanations than FastSHAP across synthetic, tabular, and vision datasets
- The accuracy gap between DNNs and GAMs serves as a reliable proxy for when SHAP explanations are trustworthy
- On CUB dataset, large accuracy gap (81.8% vs 53.7%) indicates SHAP is oversimplifying model behavior

## Why This Works (Mechanism)

### Mechanism 1: Variational Equivalence of SHAP and GAMs
The paper reframes Shapley value calculation from a weighted sum over feature subsets into a least-squares minimization problem. By solving this optimization using a neural network constrained to a GAM architecture, the model learns to output Shapley values directly. This equivalence holds under the assumption that the least-squares characterization of Shapley values accurately captures the cooperative game theory axioms for the target function.

### Mechanism 2: Automatic Purification via Masked Training
Enforcing a specific masking pattern during training automatically purifies additive components, resolving feature correlation issues without post-hoc statistical adjustments. The method samples feature subsets and trains the model to predict the masked output of the original function, with a loss function that only activates specific additive components if all their constituent features are present.

### Mechanism 3: Fidelity-Based Trustworthiness Check
The accuracy gap between a standard DNN and a constrained GAM serves as a proxy for the trustworthiness of SHAP explanations. Because SHAP is mathematically limited to first-order additive representations, it inherently fails to capture complex feature interactions. If a GAM cannot achieve accuracy comparable to a DNN, it implies the data requires complex interactions, making SHAP explanations insufficient.

## Foundational Learning

- **Concept: Functional ANOVA (FANOVA) Decomposition**
  - **Why needed here:** The paper builds its theoretical argument by mapping Shapley values to the Sobol-Hoeffding decomposition of a function. Understanding that any function can be split into mean, main effects, and interaction effects is critical to understand what SHAP discards.
  - **Quick check question:** Can you explain why variance decomposition fails when input variables are correlated, and how "purification" attempts to fix this?

- **Concept: Removal-Based Explanations (Conditional vs. Marginal)**
  - **Why needed here:** InstaSHAP relies on the "conditional expectation" method of feature removal, contrasting with "marginal" or "baseline" methods. Understanding how to simulate a feature's absence by integrating over its conditional distribution is critical for implementing the training objective.
  - **Quick check question:** What is the "off-the-manifold" problem mentioned in the paper, and why does it discourage the use of marginal or baseline removal methods?

- **Concept: Surrogate Modeling**
  - **Why needed here:** InstaSHAP trains a surrogate model that mimics the DNN's behavior but is structurally constrained to be interpretable (a GAM). It does not explain the original weights of a DNN directly.
  - **Quick check question:** In the InstaSHAP loss function, what represents the "ground truth" target that the GAM is trying to match?

## Architecture Onboarding

- **Component map:** Raw data x and randomly sampled binary mask S ~ p_SHAP -> GAM architecture (sum of separate embedding networks for features and interactions) -> Masking layer (zeros out contribution of any shape function φ_T(x_T) if any feature in T is masked in S) -> Objective (MSE between masked GAM output and masked target function output)

- **Critical path:** The data pipeline (sampling S) -> The forward pass (computing masked sum) -> The loss calculation (comparing against the black-box oracle)

- **Design tradeoffs:**
  - **Speed vs. Accuracy:** Increasing the order k of the GAM allows for better approximation of interactions but exponentially increases parameter count and reduces interpretability.
  - **Surrogate vs. Direct:** Training a surrogate explainer requires access to the black-box model's inference API but not its gradients. It is model-agnostic but adds training overhead.

- **Failure signatures:**
  - **Oversmoothing:** If the GAM accuracy is significantly lower than the DNN accuracy, the resulting "explanations" will be confidently wrong, highlighting irrelevant features.
  - **High Variance:** If the learned shape functions fluctuate wildly between training runs, the model is likely overfitting to specific mask samples rather than learning the underlying conditional expectations.

- **First 3 experiments:**
  1. **Synthetic Sanity Check:** Generate data with a known additive structure (e.g., y = x_1 + x_2*x_3). Train InstaSHAP and verify if the method correctly assigns 0 importance to x_1 in the interaction term and correctly isolates the x_2*x_3 synergy.
  2. **Ablation on Correlation:** Use the synthetic setup in Appendix D.1 (correlated Gaussian inputs). Compare InstaSHAP vs. Vanilla GAM to confirm that InstaSHAP reduces the "variance" of the shape functions, indicating successful purification of redundant interactions.
  3. **Trustworthiness Audit:** Train a ResNet and an InstaSHAP-GAM on a tabular dataset (e.g., Bike Sharing). Report the accuracy gap. If the gap is small, report the fidelity of the explanations; if large, flag the dataset as "unsafe for SHAP."

## Open Questions the Paper Calls Out

- **Can novel GAM architectures be developed to close the large performance gap with black-box models like ResNets in high-dimensional domains such as Computer Vision?**
  - Basis in paper: The authors note that even higher-order GAMs fail to match ResNet performance on the CUB dataset, but speculate that an alternate GAM architecture would be able to further improve upon these accuracy results.
  - Why unresolved: The paper restricts its CV experiments to localized patch-based GAM architectures, leaving the potential for global or hybrid architectures unexplored.

- **Can the variational framework of InstaSHAP be adapted to efficiently compute the Stone-Hooker decomposition for correlated inputs?**
  - Basis in paper: Appendix A.1 identifies the Stone-Hooker decomposition as theoretically superior but computationally intractable, noting that the authors' variational perspective potentially allows to unlock the same advantages for Hooker-type purified models.
  - Why unresolved: The paper focuses exclusively on the Sobol-Hoeffding decomposition and does not derive the necessary loss function modifications or masking schemes required to enforce the hierarchical orthogonality constraints of the Stone-Hooker decomposition.

- **Is there a general, computationally efficient metric for selecting the optimal "frontier" of feature interactions in the presence of correlated input variables?**
  - Basis in paper: Appendix E.3 states that as of yet, there is seemingly no known measurements for the correlated input case paralleling the Sobol indices in the independent input case to solve the meta-optimization of selecting interaction subsets.
  - Why unresolved: While the paper provides a theoretical method to solve the weights for a given frontier, it does not provide a method to identify which frontier is optimal without training the model.

## Limitations
- The equivalence between SHAP and GAMs may break down for functions with essential high-order interactions that cannot be approximated by GAM's restricted hypothesis space
- The trustworthiness audit conflates optimization failure with representation limits—a GAM might fail to match DNN accuracy due to suboptimal training rather than fundamental interaction requirements
- Practical implementation details for vision tasks are sparse, particularly the exact GAM-ResNet architecture and how patch-based masking interacts with the SHAP kernel distribution

## Confidence

- **High Confidence:** The mechanistic equivalence between SHAP's weighted average formulation and its variational (optimization) formulation. The masking-based training objective and its theoretical justification via Euler-Lagrange equations.
- **Medium Confidence:** The claim that accuracy gaps between DNNs and GAMs serve as reliable trustworthiness proxies. This assumes all performance differences stem from interaction capacity rather than optimization or architectural choices.
- **Low Confidence:** The practical implementation details for vision tasks, particularly the exact GAM-ResNet architecture and how patch-based masking interacts with the SHAP kernel distribution.

## Next Checks

1. **Synthetic Interaction Test:** Generate a function with known high-order interactions (e.g., y = x_1*x_2*x_3 + x_4) and verify whether InstaSHAP's explanations systematically fail to capture these interactions when constrained to order k=1, confirming the theoretical limitations.

2. **Optimization Ablation:** Train GAMs with identical architectures but different optimization strategies (SGD vs. Adam, different learning rates) on the same dataset. Compare accuracy gaps to disentangle representation limits from training artifacts in the trustworthiness assessment.

3. **Correlation Sensitivity:** Systematically vary input correlation ρ in synthetic data (Appendix D.1) and measure how the purification effect degrades. Identify the correlation threshold beyond which InstaSHAP's automatic purification fails, validating the conditional expectation assumptions.