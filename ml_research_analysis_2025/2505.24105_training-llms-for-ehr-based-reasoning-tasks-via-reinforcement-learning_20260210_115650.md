---
ver: rpa2
title: Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning
arxiv_id: '2505.24105'
source_url: https://arxiv.org/abs/2505.24105
tags:
- reasoning
- clinical
- training
- arxiv
- rlvr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces EHRMIND, a reinforcement learning framework\
  \ with verifiable rewards (RLVR) to adapt large language models (LLMs) to EHR-based\
  \ clinical reasoning tasks. It identifies two failure modes in LLMs\u2014misapplied\
  \ knowledge and missing knowledge\u2014and addresses them with a two-stage approach:\
  \ lightweight supervised fine-tuning (SFT) to inject domain knowledge and guide\
  \ interpretable outputs, followed by RLVR to reinforce outcome correctness."
---

# Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.24105
- Source URL: https://arxiv.org/abs/2505.24105
- Reference count: 40
- Introduces EHRMIND, a reinforcement learning framework with verifiable rewards (RLVR) to adapt large language models (LLMs) to EHR-based clinical reasoning tasks

## Executive Summary
The paper introduces EHRMIND, a reinforcement learning framework with verifiable rewards (RLVR) to adapt large language models (LLMs) to EHR-based clinical reasoning tasks. It identifies two failure modes in LLMs—misapplied knowledge and missing knowledge—and addresses them with a two-stage approach: lightweight supervised fine-tuning (SFT) to inject domain knowledge and guide interpretable outputs, followed by RLVR to reinforce outcome correctness. Experiments on MEDCALC, TREC Clinical Trials, and EHRSHOT show EHRMIND achieves state-of-the-art accuracy, improves interpretability, and generalizes better across tasks. A diagnostic tool, Pass@k, is introduced to determine when SFT is necessary, enabling efficient training in healthcare settings.

## Method Summary
EHRMIND employs a two-stage training pipeline to address LLM failures in EHR reasoning tasks. First, lightweight supervised fine-tuning (SFT) injects domain knowledge and guides models toward producing interpretable outputs by demonstrating correct reasoning patterns. Second, reinforcement learning with verifiable rewards (RLVR) fine-tunes the model using outcome-based rewards, reinforcing correct answers while maintaining the interpretability established during SFT. The framework includes a diagnostic tool, Pass@k, which evaluates whether a given LLM requires SFT before RLVR, optimizing training efficiency. This approach specifically targets two LLM failure modes: misapplied knowledge (using irrelevant or incorrect domain knowledge) and missing knowledge (failing to apply necessary domain knowledge).

## Key Results
- Achieves state-of-the-art accuracy on MEDCALC, TREC Clinical Trials, and EHRSHOT datasets
- Improves interpretability of model outputs through structured reasoning patterns
- Demonstrates better generalization across tasks compared to baseline approaches
- Pass@k diagnostic tool effectively identifies when SFT is necessary, reducing training overhead

## Why This Works (Mechanism)
The two-stage approach works because it addresses distinct LLM weaknesses through targeted interventions. SFT provides the model with explicit domain knowledge and reasoning patterns, preventing the "missing knowledge" failure mode where models fail to apply necessary medical concepts. RLVR then reinforces correct outcomes while preserving the interpretability learned during SFT, addressing the "misapplied knowledge" failure mode where models use irrelevant or incorrect information. The verifiable rewards ensure that the reinforcement learning process focuses on objective correctness rather than subjective preferences. The Pass@k diagnostic tool optimizes the pipeline by identifying when domain knowledge injection via SFT is necessary, avoiding unnecessary training steps for models that already possess sufficient domain knowledge.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: A training approach where models learn from objective, verifiable outcomes rather than human feedback. Why needed: Traditional RLHF requires subjective human evaluation, which is costly and inconsistent in medical domains. Quick check: Can the task produce a clear "correct/incorrect" signal for each output?
- **Two-stage Fine-tuning**: Sequential application of SFT followed by RLVR. Why needed: SFT provides necessary domain knowledge and reasoning patterns, while RLVR reinforces outcome correctness without disrupting learned interpretability. Quick check: Does the model show improvement after SFT that is then further enhanced by RLVR?
- **Pass@k Diagnostic Tool**: A metric for determining whether a model requires SFT before RLVR. Why needed: Avoids unnecessary SFT training for models that already possess sufficient domain knowledge, reducing computational costs. Quick check: Does Pass@k accurately predict which models benefit from SFT?
- **Interpretable Output Generation**: Training models to produce structured, explainable reasoning rather than just correct answers. Why needed: Critical for clinical applications where understanding the reasoning process is as important as the final answer. Quick check: Can domain experts trace and validate the model's reasoning path?

## Architecture Onboarding

Component map: Raw EHR Data -> Preprocessing -> SFT Stage -> RLVR Stage -> Fine-tuned Model -> Evaluation

Critical path: The sequence from data preprocessing through SFT to RLVR represents the core training pipeline. The Pass@k diagnostic sits at the decision point between preprocessing and SFT, determining whether to proceed with knowledge injection.

Design tradeoffs: The framework balances accuracy gains from RLVR against the interpretability preserved from SFT. Using verifiable rewards ensures objective optimization but limits applicability to tasks with clear correct answers. The two-stage approach adds training complexity but enables better control over model behavior.

Failure signatures: Models may overfit to training data during SFT if not properly regularized. RLVR may optimize for reward without maintaining interpretability if the reward function doesn't capture reasoning quality. The Pass@k tool may incorrectly predict SFT necessity if the evaluation dataset doesn't represent the target task distribution.

First experiments: 1) Test Pass@k on diverse LLMs to validate its predictive accuracy for SFT necessity across model sizes and initial capabilities. 2) Compare single-stage RLVR versus two-stage SFT+RLVR to quantify the contribution of each stage. 3) Evaluate model performance on out-of-distribution EHR tasks to assess generalization beyond the three tested datasets.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation scope limited to medical question-answering datasets, unclear generalizability to other EHR reasoning tasks like summarization or clinical coding
- Two-stage training pipeline requires significant domain expertise for creating fine-tuning data and reward functions, with no quantification of human effort required
- RLVR approach relies on verifiable rewards, making it unsuitable for tasks requiring subjective judgment or involving multiple valid approaches

## Confidence
- High confidence in technical implementation of RLVR framework and its accuracy improvements on evaluated tasks
- Medium confidence in generalizability of two-stage approach to non-question-answering EHR tasks
- Medium confidence in interpretability improvements, though quantitative metrics are limited
- Low confidence in scalability assessment and practical deployment guidance for resource-constrained settings

## Next Checks
1. Cross-task evaluation: Test EHRMIND on diverse EHR reasoning tasks including summarization, clinical coding, and risk stratification to assess generalizability beyond question-answering
2. Resource requirement quantification: Measure human effort and domain expertise needed for SFT stage, including time to create fine-tuning data and design reward functions
3. Subjective task adaptation: Develop and validate reward functions for tasks with non-verifiable answers (e.g., clinical note summarization) to test framework's broader applicability