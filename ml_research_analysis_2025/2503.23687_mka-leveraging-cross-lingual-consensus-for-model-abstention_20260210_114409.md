---
ver: rpa2
title: 'MKA: Leveraging Cross-Lingual Consensus for Model Abstention'
arxiv_id: '2503.23687'
source_url: https://arxiv.org/abs/2503.23687
tags:
- language
- confidence
- pipeline
- accuracy
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multilingual Knowledge Abstention (MKA)
  pipeline, which leverages a large language model's multilingual capabilities to
  decide whether to answer or abstain from answering questions. The method translates
  a question into multiple auxiliary languages, prompts the model in each language,
  translates the answers back, and uses cosine similarity to calibrate confidence
  and make abstention decisions.
---

# MKA: Leveraging Cross-Lingual Consensus for Model Abstention

## Quick Facts
- **arXiv ID:** 2503.23687
- **Source URL:** https://arxiv.org/abs/2503.23687
- **Reference count:** 4
- **Primary result:** MKA improves multilingual MCQA abstention accuracy, with Bengali accuracy increasing by 71.2% and English by 15.5% compared to baseline.

## Executive Summary
This paper introduces the Multilingual Knowledge Abstention (MKA) pipeline, which leverages a large language model's multilingual capabilities to decide whether to answer or abstain from answering questions. The method translates a question into multiple auxiliary languages, prompts the model in each language, translates the answers back, and uses cosine similarity to calibrate confidence and make abstention decisions. Evaluated across six target languages (Bengali, English, Swahili, Yoruba, Japanese, Indonesian) and three auxiliary language sets (low-, mid-, high-resource), the MKA pipeline shows significant improvements over baseline performance. Bengali accuracy improves by 71.2% and English by 15.5% compared to the baseline. Performance varies by model and language but generally benefits from the pipeline, particularly when good translation systems are available.

## Method Summary
The MKA pipeline addresses inference-time confidence calibration for LLM abstention on multilingual MCQA tasks using cross-lingual consensus. The method translates questions into multiple auxiliary languages using NLLB-200 1.3B Distilled (int8), prompts the model in each auxiliary language, translates responses back to the target language, and performs centroid polling using character n-gram cosine similarity to select a consensus answer. Confidence is calibrated using paraphrase-multilingual-mpnet-base-v2 embeddings with a weight of 1.5 for similarity >0.8, and abstention occurs when confidence falls below model-specific cutoffs (0.58–0.70). The pipeline is evaluated on the multilingual MMLU benchmark across six target languages with three auxiliary language sets (low/mid/high-resource), using effective accuracy (AC_eff = AC_comp × coverage) as the primary metric with a cosine similarity threshold of 0.85 for correctness.

## Key Results
- Bengali accuracy improves by 71.2% and English by 15.5% compared to baseline
- Performance varies by model and language but generally benefits from the pipeline
- Yoruba accuracy decreased by 24.6% due to poor translation performance (NLLB sp-BLEU: 26.6/13.8)

## Why This Works (Mechanism)
The MKA pipeline works by exploiting cross-lingual consensus through multiple translation steps. When a question is translated into multiple auxiliary languages and answered in each, consistent answers across languages indicate higher confidence in correctness. The centroid polling mechanism aggregates these multilingual responses by computing character n-gram cosine similarity, selecting the answer with the highest consensus score. This cross-lingual verification helps filter out low-confidence responses that might occur when answering directly in the target language, especially for low-resource languages where the model has less training data. The translation-encode-poll framework effectively creates a multilingual ensemble that leverages the model's knowledge across languages to improve both accuracy and coverage.

## Foundational Learning
- **Cross-lingual consensus:** Using multiple language representations to validate answer consistency - needed because single-language responses can be unreliable, especially for low-resource languages; quick check: compare answer consistency across auxiliary languages
- **Centroid polling:** Selecting the answer with highest average similarity to other responses - needed to aggregate multilingual answers into a single confident prediction; quick check: verify centroid answer matches majority vote across auxiliary languages
- **Character n-gram cosine similarity:** Measuring textual similarity using n-gram vectors - needed for language-agnostic comparison of answers; quick check: confirm similarity scores between identical answers approach 1.0
- **Effective accuracy:** Coverage-weighted accuracy metric - needed to evaluate abstention systems fairly; quick check: verify AC_eff decreases when coverage drops despite high accuracy
- **Confidence calibration:** Mapping similarity scores to confidence thresholds - needed to determine when to abstain; quick check: plot calibration curves for different threshold values

## Architecture Onboarding

**Component Map:** Translation system (NLLB-200) -> Prompt generator -> LLM -> Answer translator -> Embedding model (paraphrase-multilingual-mpnet-base-v2) -> Confidence calibrator -> Abstention decision

**Critical Path:** Question → NLLB-200 translation → LLM prompting → Answer translation → n-gram vectorization → Centroid polling → Confidence scoring → Abstention decision

**Design Tradeoffs:** Uses external translation systems (introduces artifacts, dependency on translation quality) vs. internal multilingual knowledge (not implemented); character n-grams (language-agnostic, but may miss semantic nuances) vs. semantic embeddings (more meaningful but language-dependent); fixed thresholds (simple, but may not adapt to model/language differences) vs. dynamic calibration (more complex, potentially more accurate).

**Failure Signatures:** Poor translation quality for low-resource languages causes accuracy degradation; prompt instruction volatility affects answer consistency; high abstention rates indicate overly conservative confidence thresholds; low coverage suggests the model is abstaining too frequently.

**First Experiments:**
1. Test translation quality by translating sample questions to auxiliary languages and back, measuring BLEU scores
2. Verify centroid polling by manually checking consensus answers against individual auxiliary language responses
3. Validate confidence calibration by plotting similarity scores vs. actual accuracy to confirm threshold effectiveness

## Open Questions the Paper Calls Out
- Can multilingual knowledge be leveraged internally within the model without explicit translation between target and auxiliary languages? The current pipeline relies on external translation systems, introducing artifacts and dependencies that internal access to multilingual representations could eliminate.
- Would using LLMs as evaluators of semantic equivalence improve the pipeline's accuracy compared to cosine similarity? Current reliance on cosine similarity between sentence embeddings may not capture nuanced semantic equivalence, especially across languages with different structures.
- How can the pipeline's performance be stabilized for languages with poor translation system support? Performance varies dramatically based on translation quality, limiting applicability to languages where machine translation is unreliable.
- What causes the pipeline's volatility to prompting instructions and how can it be mitigated? The root cause remains speculative, and no solution is proposed despite this affecting reproducibility and reliability.

## Limitations
- Performance heavily depends on translation quality, with significant degradation for low-resource languages like Yoruba (sp-BLEU 13.8)
- Large variation in effectiveness across languages and models suggests context-dependent limitations
- Underspecified prompt templates may affect reproducibility and consistency across implementations
- Fixed sample size of n=200 per target language may not provide sufficient statistical power for low-resource language evaluation

## Confidence
- **High:** The MKA pipeline's core methodology is clearly described and reproducible, including the translation-encode-poll framework and centroid polling mechanism using character n-gram cosine similarity
- **Medium:** The reported accuracy improvements are plausible given the method's design, though the large variation across languages and models suggests context-dependent effectiveness
- **Low:** The specific prompt templates and exact implementation details for the MCQA prompting are underspecified, potentially affecting reproducibility

## Next Checks
1. Conduct ablation studies on the cosine similarity threshold (currently fixed at 0.85) and confidence cutoffs to determine their impact on effective accuracy across different language-resource settings
2. Test the MKA pipeline with alternative translation systems (beyond NLLB-200) for low-resource languages to quantify the impact of translation quality on overall performance, particularly for languages like Yoruba with sp-BLEU scores below 20
3. Evaluate the method's sensitivity to prompt template variations, especially for models known to be prompt-sensitive like Qwen-2.5 7B, by testing multiple instruction formats and measuring consistency of abstention decisions across runs