---
ver: rpa2
title: Unreliable Uncertainty Estimates with Monte Carlo Dropout
arxiv_id: '2512.14851'
source_url: https://arxiv.org/abs/2512.14851
tags:
- data
- uncertainty
- training
- bayesian
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Monte Carlo dropout (MCD) was proposed as a computationally efficient
  alternative to Bayesian inference in deep neural networks by applying dropout at
  inference time to generate multiple sub-models. We empirically investigated MCD's
  ability to capture true uncertainty and compared it with Gaussian Processes and
  Bayesian Neural Networks.
---

# Unreliable Uncertainty Estimates with Monte Carlo Dropout

## Quick Facts
- arXiv ID: 2512.14851
- Source URL: https://arxiv.org/abs/2512.14851
- Authors: Aslak Djupskås; Alexander Johannes Stasik; Signe Riemer-Sørensen
- Reference count: 36
- Primary result: MCD consistently failed to accurately reflect true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions where Bayesian models naturally showed higher confidence intervals.

## Executive Summary
Monte Carlo dropout (MCD) was proposed as a computationally efficient alternative to Bayesian inference in deep neural networks by applying dropout at inference time to generate multiple sub-models. We empirically investigated MCD's ability to capture true uncertainty and compared it with Gaussian Processes and Bayesian Neural Networks. Across controlled regression experiments with varying data amounts and noise levels, MCD consistently failed to accurately reflect underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions where Bayesian models naturally showed higher confidence intervals.

## Method Summary
The study compared Monte Carlo dropout uncertainty estimates against Gaussian Processes and Bayesian Neural Networks on synthetic 1D/2D regression tasks. The experimental setup used a synthetic function f(x) = sin(4x) + r₀ + r₁x + (r₂ + 1/2)x² with r₀,r₁,r₂ ~ N(0,1), training data excluding the gap [-0.5, 0.5], and two noise levels (N(0, 0.05) and N(0, 0.2)). Training sizes varied (n=15, 50, 150). MCD used 2 hidden layers with ReLU activation and dropout rate 0.15, GP used GPJax with RBF kernel, and BNN used NumPyro with NUTS sampler. Visual comparison of 95% confidence intervals and MSE between predictions and true function served as primary metrics.

## Key Results
- MCD produced more uniform uncertainty across the input space compared to GP and BNN models which showed characteristic uncertainty increases in data-scarce regions
- MCD's uncertainty estimates showed strong dependency on random seed initialization, with MSE std=0.156 across 100 seeds
- Ensembling multiple MCD models trained with different seeds produced uncertainty profiles closer to Bayesian models but at significantly higher computational cost

## Why This Works (Mechanism)
None

## Foundational Learning
- **Monte Carlo Dropout**: Using dropout at inference time to approximate Bayesian inference by generating multiple stochastic forward passes. Why needed: Provides uncertainty estimates without expensive posterior sampling. Quick check: Verify dropout is active during inference, not just training.
- **Gaussian Processes**: Non-parametric Bayesian models that provide natural uncertainty estimates through posterior variance. Why needed: Serves as gold standard for uncertainty quantification. Quick check: Confirm GP shows increased variance in data-scarce regions.
- **Bayesian Neural Networks**: Neural networks with weight distributions that capture uncertainty through posterior sampling. Why needed: Represents full Bayesian treatment for comparison. Quick check: Verify NUTS sampling produces reasonable posterior samples.
- **Extrapolation vs Interpolation**: Extrapolation refers to prediction outside training data range, interpolation within but with gaps. Why needed: Tests model's ability to quantify uncertainty in data-scarce regions. Quick check: Plot confidence intervals specifically in gap region [-0.5, 0.5].
- **Ensemble Methods**: Combining multiple model predictions to improve robustness and uncertainty estimates. Why needed: Tests whether aggregating MCD models improves uncertainty calibration. Quick check: Compare single MCD vs ensemble variance profiles.
- **Synthetic Regression Tasks**: Controlled experiments using generated data with known ground truth. Why needed: Allows precise evaluation of uncertainty quantification. Quick check: Verify synthetic function generation matches specified form exactly.

## Architecture Onboarding
- **Component Map**: Data Generation -> Model Training (MCD/GP/BNN) -> Inference (Multiple Forward Passes) -> Uncertainty Estimation -> Comparison
- **Critical Path**: Synthetic data generation → Model training with specified hyperparameters → Multiple stochastic forward passes → Compute mean and variance → Plot confidence intervals
- **Design Tradeoffs**: MCD offers computational efficiency but sacrifices accurate uncertainty calibration; GPs provide accurate uncertainty but scale poorly with data size; BNNs offer full Bayesian treatment but require expensive sampling
- **Failure Signatures**: Flat/uniform variance across input space instead of increasing in data-scarce regions; high seed-to-seed variance in predictions; failure to capture increased uncertainty in extrapolation/interpolation regions
- **First Experiments**:
  1. Generate 1D synthetic data with specified function, noise levels, and gap exclusion
  2. Implement MCD model with custom dropout layer active during inference and perform multiple forward passes
  3. Implement GP and BNN baselines and plot 95% confidence intervals for comparison

## Open Questions the Paper Calls Out
- **Open Question 1**: Does ensembling MCD models trained with different random seeds offer a reliable and efficient solution for approximating Bayesian uncertainty? The authors observe that combining predictions from different seeds mimics GP/BNN uncertainty profiles but requires significantly higher computational load.
- **Open Question 2**: Do the observed failures of MCD in interpolation and extrapolation generalize to high-dimensional, real-world data? The authors note "further experiments are needed" as their study was limited to controlled 1D and 2D regression tasks.
- **Open Question 3**: Is the constant variance profile in MCD an intrinsic property of the dropout approximation or a consequence of insufficient model complexity? The paper questions the theoretical equivalence of MCD to Bayesian inference.

## Limitations
- Synthetic toy problems may not capture the complexity of natural data distributions where MCD might behave differently
- Limited to 1D and 2D regression tasks, not testing high-dimensional real-world scenarios
- Computational cost of ensembling MCD models may negate the efficiency advantage over Bayesian methods

## Confidence
- **Medium Confidence**: The core experimental design (synthetic function generation, training data configuration with gap, and three-model comparison framework) is clearly specified and reproducible
- **Low Confidence**: The MCD implementation details and hyperparameter settings (particularly the number of forward passes and training convergence criteria) introduce significant uncertainty in reproducing the specific uncertainty profile patterns shown in the figures
- **Medium Confidence**: The GP and BNN baseline implementations are sufficiently specified through GPJax and NumPyro libraries, though some parameter choices remain unknown

## Next Checks
1. Run MCD with varying numbers of forward passes (T=10, 50, 100) and verify whether the uncertainty profile converges to the pattern shown in Figure 1, particularly checking for flat variance in extrapolation regions
2. Perform seed sensitivity analysis by training MCD models with 10-20 different random seeds and quantifying the variance in predictions (targeting MSE std ~0.156 as reported)
3. Test whether an ensemble of 5-10 MCD models with different seeds produces uncertainty estimates that better capture interpolation/extrapolation behavior, comparing against single-model performance to validate the paper's ensemble recommendation