---
ver: rpa2
title: Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through
  Clinical Cognitive Chain Reasoning
arxiv_id: '2507.17539'
source_url: https://arxiv.org/abs/2507.17539
tags:
- data
- fundus
- retinal
- image
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multimodal large language
  models (MLLMs) in specialized domains like ophthalmology, where fragmented annotation
  granularity and inconsistent clinical reasoning logic hinder precise cross-modal
  understanding. To overcome this, the authors introduce FundusExpert, an ophthalmology-specific
  MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen,
  a dataset constructed using the intelligent Fundus-Engine system.
---

# Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning

## Quick Facts
- arXiv ID: 2507.17539
- Source URL: https://arxiv.org/abs/2507.17539
- Authors: Xinyao Liu; Diping Song
- Reference count: 40
- FundusExpert achieves 26.6% higher accuracy than 40B MedRegA on ophthalmic question-answering tasks

## Executive Summary
This work addresses fundamental limitations in multimodal large language models for ophthalmology, where fragmented annotation granularity and inconsistent clinical reasoning hinder precise cross-modal understanding. The authors introduce FundusExpert, an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen, a dataset constructed using the intelligent Fundus-Engine system. Fundus-Engine automates localization and leverages MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis within single fundus images. The system also constructs a clinically aligned cognitive chain to guide the model in generating interpretable reasoning paths.

## Method Summary
The authors develop FundusExpert by fine-tuning InternVL2.5-8B on FundusGen, a dataset created through Fundus-Engine's automated pipeline. Fundus-Engine integrates multiple stages: fine-grained label collection from 8+ sources, nnU-Net segmentation for pseudo-label generation, DBSCAN clustering for bounding box creation, and GPT-4o semantic expansion with clinical constraints. The resulting dataset contains 300K curated samples across 5 instruction types, including multi-turn diagnostic reasoning that follows a "region localization → feature analysis → diagnostic reasoning" cognitive pathway.

## Key Results
- FundusExpert achieves 26.6% higher accuracy than 40B MedRegA on ophthalmic question-answering tasks
- Outperforms GPT-4o with 77.0% clinical consistency in zero-shot report generation vs 47.6%
- Reveals scaling law between data quality and model capability (L ∝ N^0.068)
- Achieves IoU of 0.738 for optic disc localization compared to 0.543 for MedRegA

## Why This Works (Mechanism)

### Mechanism 1: Multi-Granularity Collaborative Annotation
Integrating global disease classification, local object detection, and fine-grained feature analysis within single fundus images enables cross-scale semantic associations that fragmented "single-image-single-dimension" datasets cannot provide. By co-locating disease labels, bounding box coordinates, and feature descriptors on the same image, the training data creates explicit grounding between micro-level features (e.g., microaneurysm distribution) and macro-level diagnoses (e.g., diabetic retinopathy staging).

### Mechanism 2: Clinical Cognitive Chain Structuring
Structuring training data as multi-turn dialogues that follow the "region localization → feature analysis → diagnostic reasoning" pathway improves both diagnostic accuracy and interpretability compared to equivalent single-turn formulations. The cognitive chain enforces progressive evidence accumulation—early dialogue turns establish spatial facts, later turns integrate these into diagnostic conclusions—mimicking how clinicians analyze the spatial distribution of fundus structures and local features to infer disease progression.

### Mechanism 3: Positioning-Diagnosis Representation Sharing
Joint training on localization (bounding box prediction) and diagnostic text generation within a single model creates "region-semantic self-reference" capabilities that outperform decoupled detection-then-description pipelines. By sharing visual representations between spatial prediction and language generation tasks, the model learns to ground diagnostic statements (e.g., "microaneurysms present") in specific image coordinates, reducing spatial perception inaccuracies.

## Foundational Learning

- **Concept: Multi-turn Instruction Tuning**
  - Why needed here: The cognitive chain mechanism depends on correctly structuring progressive dialogue turns that enforce "local→feature→diagnosis" reasoning
  - Quick check question: Based on the ablation results, why does splitting multi-turn diagnostic dialogues into independent single-turn tasks cause disproportionate accuracy loss on rare/complex diseases (4.8%) versus average diseases (3.5%)?

- **Concept: Bounding Box–Text Grounding Formats**
  - Why needed here: Grounding Report tasks require explicit association between spatial coordinates and textual descriptions (e.g., `<ref>optic disc</ref><box>[[136, 452, 277, 571]]</box>`)
  - Quick check question: How would you modify a prompt to force a model to reference bounding box coordinates when describing a lesion's diagnostic significance?

- **Concept: Semi-Supervised Pseudo-Label Quality Control**
  - Why needed here: The automatic bounding box annotation system expands training data from <1,000 true labels to 5,000–16,000 pseudo-labels per category; quality affects downstream model reliability
  - Quick check question: The paper validates pseudo-labels on Messidor (OOD test set). What additional cross-domain validation would you add before using pseudo-labels for production training?

## Architecture Onboarding

- **Component map**: FundusExpert (8B) -> InternViT-300M (visual encoder) -> MLP projection layer -> InternLM-7B (language decoder); Fundus-Engine (data pipeline) -> Stage 1: Fine-grained label collection -> Stage 2: nnU-Net segmentation -> DBSCAN clustering -> bounding boxes -> Stage 3: GPT-4o semantic expansion

- **Critical path**: 1) Aggregate fine-grained labels from 8+ data sources; 2) Train per-category nnU-Net segmentation models (<1,000 samples each); 3) Generate pseudo-labels via iterative self-training; 4) Convert segmentations to bounding boxes (DBSCAN: epsilon=160, min_samples=10); 5) Run MLLM semantic expansion with clinical constraint prompts; 6) Expert double-blind review; 7) Full fine-tune InternVL2.5-8B on 300K curated samples

- **Design tradeoffs**: Quality over scale (10% FundusGen ≈ 100% Classification Annotation-Guided Data); Startup data cost (General Report data reduces training epochs by ~0.5); Cognitive chain complexity (adds annotation overhead but yields 4.8% gain on rare diseases)

- **Failure signatures**: IoU <0.3 on optic disc/cup → Check nnU-Net training data quality; Strong classification but poor report grounding → Verify Grounding Report task inclusion; High hallucination rate → Review Stage 3 constraint prompts; Slow convergence → Confirm startup data present

- **First 3 experiments**: 1) Region ablation: Train with vs. without Grounding Report + Regional QA data; 2) Cognitive chain degradation: Convert all multi-turn dialogues to single-turn; 3) Scaling law validation: Train on 10%/25%/50%/100% FundusGen subsets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reinforcement learning (RL) be effectively integrated with FundusGen’s semantic hierarchy to enhance model adaptability in low-annotation medical settings?
- Basis in paper: [explicit] The conclusion states, "Future research will integrate reinforcement learning with FundusGen’s semantic hierarchy to enhance model adaptability in low-annotation settings..."
- Why unresolved: The current work focuses on supervised fine-tuning with a static dataset; the mechanism for integrating RL into the specific semantic hierarchy defined by Fundus-Engine remains unexplored
- What evidence would resolve it: Successful training runs showing improved performance or faster convergence on downstream tasks using significantly fewer labeled examples compared to the current supervised approach

### Open Question 2
- Question: To what extent can methodologies like test-time scaling and reinforcement learning-based post-training improve FundusExpert's dynamic reasoning capabilities?
- Basis in paper: [explicit] The authors identify "broadening its adaptability for dynamic reasoning" as a next step, citing "test-time scaling and reinforcement learning-based post-training like Deepseek-r1" as promising methodologies
- Why unresolved: The current model is trained via instruction fine-tuning; the potential gains and architectural adjustments needed to support these advanced inference-time techniques in ophthalmic MLLMs are unknown
- What evidence would resolve it: Experiments demonstrating that increasing inference compute (test-time scaling) yields measurable improvements in diagnostic accuracy or logical consistency

### Open Question 3
- Question: Does the automated bounding box system maintain robustness when segmenting rare or novel lesion types that lack sufficient initial seed data?
- Basis in paper: [inferred] The paper notes the bounding box system is initialized with "<1,000 samples per category" and relies on nnU-Net. While cross-domain feasibility is shown, the generalizability to classes with extremely scarce or heterogeneous initial annotations is not fully validated
- Why unresolved: Semi-supervised expansion relies on pseudo-labels which may degrade if the initial model fails to capture the feature diversity of rare conditions
- What evidence would resolve it: Evaluation of the nnU-Net segmentation performance on rare lesion categories not well-represented in the initial training set

## Limitations

- Lack of external validation beyond Messidor for pseudo-label quality raises concerns about generalizability to truly unseen clinical populations
- The 26.6% accuracy improvement over MedRegA lacks statistical significance testing across the 21 complex/rare diseases
- Clinical consistency metric for report generation (77.0%) lacks standardized validation protocols required for clinical deployment

## Confidence

- **High Confidence**: The architectural design of FundusExpert and the multi-granularity annotation framework are well-supported by ablation studies and comparative benchmarks
- **Medium Confidence**: The clinical cognitive chain mechanism and its superiority over single-turn formulations are demonstrated but could benefit from additional qualitative case studies
- **Medium Confidence**: The scaling law relationship between data quality and model capability shows promise but requires validation across different model sizes and domains

## Next Checks

1. **Cross-Domain Pseudo-Label Validation**: Test the nnU-Net + DBSCAN pseudo-label generation pipeline on two additional independent ophthalmic datasets to verify robustness across different imaging protocols and disease prevalence
2. **Clinical Expert Review Protocol**: Implement double-blind evaluation where three independent ophthalmologists assess a stratified sample of 100 generated reports using standardized rubrics for diagnostic accuracy, completeness, and clinical reasoning quality
3. **Statistical Significance Analysis**: Conduct paired t-tests or Wilcoxon signed-rank tests comparing FundusExpert performance against baselines across all evaluation metrics, with particular focus on the 21 complex/rare diseases subset where gains were most pronounced