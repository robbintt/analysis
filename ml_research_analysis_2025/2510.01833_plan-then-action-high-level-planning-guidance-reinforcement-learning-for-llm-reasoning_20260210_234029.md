---
ver: rpa2
title: Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM
  Reasoning
arxiv_id: '2510.01833'
source_url: https://arxiv.org/abs/2510.01833
tags:
- reasoning
- plan
- arxiv
- reward
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of global planning in LLM reasoning,
  which often leads to redundant or inaccurate reasoning. To tackle this, it proposes
  Plan-Then-Action Enhanced Reasoning with Group Relative Policy Optimization (PTA-GRPO),
  a two-stage framework.
---

# Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning

## Quick Facts
- **arXiv ID:** 2510.01833
- **Source URL:** https://arxiv.org/abs/2510.01833
- **Reference count:** 39
- **Primary result:** PTA-GRPO framework improves LLM reasoning by combining high-level planning guidance with RL, achieving over 20-point score gains for weaker models on MATH, AIME, and AMC benchmarks.

## Executive Summary
This paper introduces a two-stage framework called Plan-Then-Action Enhanced Reasoning with Group Relative Policy Optimization (PTA-GRPO) to address the global planning deficit in large language model reasoning. The method injects high-level planning guidance via supervised fine-tuning, then jointly optimizes both the final answer and the quality of these plans using a guidance-aware reinforcement learning approach. Experiments demonstrate consistent improvements across multiple reasoning benchmarks and model scales, with the largest gains observed for weaker models.

## Method Summary
PTA-GRPO operates in two stages: first, a cold-start supervised fine-tuning stage where a teacher model (Qwen3-235B) generates concise high-level plans that are distilled into the student model; second, a reinforcement learning stage using a novel guidance-aware GRPO that jointly optimizes answer quality and plan quality. The RL reward function incorporates three components: an analytical plan reward based on multiple plan and CoT sampling, a format reward for adherence to structured output templates, and a chain-of-thought (CoT) reward for final answer accuracy. The framework aims to provide global planning guidance while maintaining the model's ability to adapt through RL.

## Key Results
- PTA-GRPO achieves consistent performance gains across MATH, AIME, and AMC benchmarks
- Weaker models show the largest improvements, with average score gains exceeding 20 points
- Even stronger models demonstrate steady improvements, validating the framework's broad applicability

## Why This Works (Mechanism)
The approach works by addressing the lack of global planning in LLM reasoning through explicit high-level guidance. The cold-start SFT stage injects concise, structured plans that serve as global guidance, while the guidance-aware RL stage ensures these plans are both high-quality and aligned with final answers. The analytical plan reward mechanism samples multiple plans and CoT trajectories to evaluate plan quality independently of answer correctness, encouraging genuine reasoning over superficial format compliance.

## Foundational Learning
- **Chain-of-Thought (CoT) reasoning**: Why needed - LLMs often fail on complex reasoning without intermediate steps; Quick check - Model can solve multi-step math problems with explicit reasoning traces
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed - Supervised learning alone doesn't optimize for long-term reasoning quality; Quick check - Model improves through reward-based optimization rather than just imitation
- **Group Relative Policy Optimization (GRPO)**: Why needed - Standard PPO requires value network training, which can be unstable; Quick check - GRPO achieves comparable results without separate value function
- **Analytical reward functions**: Why needed - Tree-based methods are computationally expensive; Quick check - Reward can be computed analytically without expensive search
- **Cold-start training**: Why needed - RL from scratch often fails to learn basic reasoning; Quick check - SFT stage produces coherent intermediate reasoning before RL
- **Plan distillation**: Why needed - Strong teacher models can generate better plans than weaker students; Quick check - Distilled plans improve reasoning performance when properly integrated

## Architecture Onboarding

**Component Map:** Dataset (Math reasoning problems) → Teacher model (Qwen3-235B) → Plan distillation (SFT) → Student model → RL fine-tuning (PTA-GRPO) → Final model

**Critical Path:** Problem → Plan generation → CoT reasoning → Answer → Reward computation (plan + CoT + format) → Policy update

**Design Tradeoffs:** The framework trades computational efficiency (analytical plan rewards vs. tree search) for potential approximation errors in plan quality evaluation. Structured output formats ensure plan-answer alignment but may constrain reasoning flexibility.

**Failure Signatures:** 
- Low plan reward with high CoT reward indicates plans are superficial or misaligned with actual reasoning
- High format reward with low plan/CoT rewards suggests reward hacking through template compliance
- Poor cold-start SFT performance leads to unstable RL training and convergence issues

**First 3 Experiments:**
1. Verify cold-start SFT produces coherent high-level plans on held-out reasoning problems
2. Test analytical plan reward computation with different numbers of plan samples (m) and CoT samples (z)
3. Evaluate format reward enforcement by checking model adherence to `<plan>` and `<answer>` tags

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does PTA-GRPO generalize effectively to reasoning domains beyond mathematics, such as code generation, logical deduction, or multi-step planning tasks?
- **Basis in paper:** [inferred] The experimental evaluation is restricted to mathematical reasoning benchmarks (MATH, AIME, AMC), despite the authors positioning the method as a general framework for addressing global planning deficits in LLM reasoning.
- **Why unresolved:** No experiments or discussion demonstrate domain transferability, leaving the broader applicability of the guidance-aware RL approach unverified.
- **What evidence would resolve it:** Benchmark results on diverse reasoning tasks (e.g., Big-Bench Hard, CodeContests, or planning benchmarks) showing comparable performance gains to those observed in mathematics.

### Open Question 2
- **Question:** What is the computational overhead of the analytical plan reward mechanism during RL training, particularly compared to standard GRPO?
- **Basis in paper:** [inferred] The analytical plan reward requires sampling multiple plans per question and resampling multiple CoT trajectories per plan (m plans × z CoTs), but computational costs are not analyzed despite the paper criticizing tree-based methods for high computational expense.
- **Why unresolved:** The paper does not report training time, FLOPs, or memory requirements for PTA-GRPO versus baselines.
- **What evidence would resolve it:** Direct comparison of training wall-clock time, GPU hours, and FLOPs between PTA-GRPO and standard GRPO across equivalent data scales.

### Open Question 3
- **Question:** To what extent does PTA-GRPO's cold-start SFT stage depend on the quality and capabilities of the teacher model used for plan distillation?
- **Basis in paper:** [inferred] The SFT dataset is constructed using Qwen3-235B as the teacher to distill "general analytical knowledge," but weaker models like Qwen2.5-7B-Instruct are shown to generate low-quality plans independently.
- **Why unresolved:** No ablation examines the impact of teacher model quality or size on downstream RL performance, leaving the dependency on strong teachers unclear.
- **What evidence would resolve it:** Ablation experiments varying teacher model size/quality (e.g., using 7B, 14B, and 72B teachers) and measuring final PTA-GRPO performance.

### Open Question 4
- **Question:** Does the structured output format (explicit `<plan>` and `<answer>` tags) constrain model flexibility or introduce format-level reward hacking?
- **Basis in paper:** [inferred] The format reward enforces strict adherence to the template, but the paper does not analyze whether models might optimize for format compliance over genuine reasoning quality.
- **Why unresolved:** No analysis examines whether models trained with format rewards can generalize to unstructured outputs or whether format constraints limit reasoning expression.
- **What evidence would resolve it:** Evaluation of PTA-GRPO models on prompts without format requirements, plus analysis of failure cases to detect superficial format compliance without substantive planning.

## Limitations
- Experimental scope limited to mathematical reasoning benchmarks, with unclear generalization to other domains
- Lack of detailed ablation studies to isolate the impact of planning versus RL components
- No analysis of computational overhead compared to standard GRPO approaches

## Confidence

**High:** The proposed two-stage framework and GRPO-based RL method are technically sound and well-motivated.

**Medium:** Experimental results show consistent gains, but lack of detailed ablations and training details reduces confidence in the magnitude and source of improvements.

**Low:** The claim of broad applicability across model scales is plausible but not fully substantiated due to limited analysis of dataset quality, plan generation, and robustness to different reasoning styles.

## Next Checks
1. Perform detailed ablation studies isolating the impact of high-level planning versus reinforcement learning, and compare against simpler baselines (e.g., adding plans without RL).
2. Evaluate model robustness by testing on out-of-distribution reasoning problems or adversarially constructed examples to assess plan and answer quality.
3. Provide transparency on the supervised fine-tuning dataset and plan generation process, including human evaluation of plan relevance and correctness.