---
ver: rpa2
title: 'GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography'
arxiv_id: '2509.10344'
source_url: https://arxiv.org/abs/2509.10344
tags:
- multi-view
- local
- mammography
- alignment
- view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GLAM, a novel visual language pretraining
  approach for mammography that leverages geometry-guided local alignment to learn
  multi-view correspondence. Unlike existing methods that treat mammograms as independent
  images or rely solely on global feature fusion, GLAM uses cross-attention to align
  patches across craniocaudal and mediolateral oblique views along the anterior-posterior
  axis, capturing fine-grained local features.
---

# GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography

## Quick Facts
- arXiv ID: 2509.10344
- Source URL: https://arxiv.org/abs/2509.10344
- Authors: Yuexi Du; Lihui Chen; Nicha C. Dvornek
- Reference count: 39
- Primary result: GLAM achieves up to 2.3% improvement in AUC for mammography cancer detection and density classification

## Executive Summary
This paper introduces GLAM, a novel visual language pretraining approach for mammography that leverages geometry-guided local alignment to learn multi-view correspondence. Unlike existing methods that treat mammograms as independent images or rely solely on global feature fusion, GLAM uses cross-attention to align patches across craniocaudal and mediolateral oblique views along the anterior-posterior axis, capturing fine-grained local features. Pretrained on EMBED, one of the largest mammography datasets, GLAM outperforms state-of-the-art baselines in zero-shot, linear probing, and fine-tuning settings across three datasets, achieving up to 2.3% improvement in AUC.

## Method Summary
GLAM combines DiNOv2 ViT-B vision encoder with BioClinical-BERT text encoder to learn multi-view correspondence in mammography. The method preprocesses mammograms by removing pectoral regions, aligning images to the anterior-posterior axis, and applying random affine transformations. Spatial attention aggregation creates super-patches from raw patch tokens, and cross-attention aligns query patches with corresponding AP slices in the opposite view. The model is pretrained using joint global multi-view contrastive loss, symmetric image-text contrastive loss, and geometry-guided local alignment loss, with evaluation on cancer detection and density classification tasks.

## Key Results
- Achieves up to 2.3% improvement in AUC compared to state-of-the-art baselines
- Outperforms baselines in zero-shot, linear probing (1%, 10%, 100% data), and fine-tuning settings
- Demonstrates strong generalization to out-of-domain data across three datasets
- Ablation studies confirm importance of geometry-guided alignment, same-position negatives, and spatial attention aggregation

## Why This Works (Mechanism)

### Mechanism 1
Patch-to-slice alignment along the anterior-posterior (AP) axis enables anatomically grounded cross-view correspondence. Rather than enforcing rigid patch-to-patch matching (which assumes the breast is undeformable), GLAM aligns each CC patch to all patches in the corresponding AP slice of the MLO view using cross-attention. This accounts for breast deformation during compression and the oblique angle of MLO imaging. The core assumption is that tissues at the same AP position in both views represent the same underlying 3D anatomy, and cross-attention can soft-select the correct correspondences within each slice.

### Mechanism 2
Same-position negatives from different patients force the model to learn semantic content rather than positional shortcuts. In addition to standard negatives from different spatial positions, GLAM samples patches at the same position from other patients in the batch. This prevents the model from relying purely on positional encoding to distinguish patches. The core assumption is that patches at identical spatial locations across patients carry different semantic content but similar positional patterns; contrasting them forces feature discrimination.

### Mechanism 3
Spatial attention aggregation creates semantically richer super-patches for more meaningful cross-view alignment. Raw ViT patch tokens have limited receptive fields. GLAM applies a spatial attention pooling layer to aggregate patches into M super-patches with larger receptive fields and higher-level semantics before alignment. The core assumption is that super-patch representations capture diagnostically relevant structures better than raw patches, making cross-view matching more meaningful.

## Foundational Learning

- **InfoNCE Contrastive Loss**: All GLAM objectives (global visual-visual, visual-language, local patch alignment) are instantiated as InfoNCE. Understanding the numerator/denominator structure is essential to debug alignment quality.
  - Quick check question: Can you explain why temperature τ affects hard-vs-soft negative weighting in InfoNCE?

- **Cross-Attention for Correspondence**: The patch-to-slice alignment uses cross-attention where the CC patch is the query and the MLO slice patches are keys/values. The attention weights implicitly encode correspondence confidence.
  - Quick check question: Given a query patch q^{cc}_{i,j} and slice s^{mlo}_j, what does the softmax output represent geometrically?

- **Mammography Imaging Geometry (CC/MLO Views)**: The entire method hinges on the AP-axis correspondence assumption derived from how CC and MLO projections are acquired. Without this domain knowledge, the alignment design appears arbitrary.
  - Quick check question: Why does an ROI appear at the same AP position in both views, and how does MLO angle create single-view ambiguity?

## Architecture Onboarding

- **Component map**: Image Encoder (f_V) -> Spatial Attention Aggregation -> Patch-to-Slice Cross-Attention -> Contrastive Heads; Text Encoder (f_T) -> Contrastive Heads
- **Critical path**: 1. Preprocess mammograms: pectoral removal → AP alignment → random affine 2. Encode images → extract patch tokens 3. Aggregate to super-patches via spatial attention 4. Compute patch-to-slice cross-attention for local alignment loss 5. Compute global CLIP-style alignment (visual-visual + visual-language) 6. Backpropagate L_final = L_global + L_local
- **Design tradeoffs**: M (number of super-patches): Higher M = finer granularity but noisier alignment; preprocessing aggressiveness: Pectoral removal + rotation improves AP alignment but may introduce artifacts; text synthesis quality: Synthesized reports may limit visual-language alignment richness
- **Failure signatures**: Zero-shot AUC drops significantly (check if global alignment dominates); multi-view prediction ≈ single-view (cross-attention may be collapsing); fine-tuning unstable (same-position negatives may be insufficient)
- **First 3 experiments**: 1. Ablate each component (GLA, SPN, SAA) on held-out EMBED test set using exact settings in Table 5 2. Visualize cross-view attention on validation set with annotated ROIs to confirm anatomically plausible alignment 3. Test out-of-domain generalization by linear probing on VinDr with varying training fractions (1%, 10%, 100%)

## Open Questions the Paper Calls Out

- **Dense Multi-Modal Contrastive Learning**: Can integrating dense multi-modal contrastive learning with the current global-local alignment framework improve fine-grained lesion localization and detection performance? The paper states future plans include introducing dense multi-modal contrastive learning and extending multi-view alignment to both sides of the breast.

- **Robustness to Pre-Processing Failures**: How robust is the patch-to-slice alignment when pre-processing steps (pectoral removal, rotation, affine augmentation) fail or produce misalignment in challenging cases? The paper acknowledges that extreme cases such as large pectoral regions may cause misalignment despite random affine transformation.

- **Bilateral View Extension**: Does extending multi-view alignment to bilateral views (comparing left and right breasts) improve asymmetry detection for clinical diagnosis? The paper explicitly mentions future plans to extend multi-view alignment to both sides of the breast.

## Limitations

- **Geometry Assumption Dependency**: The method's effectiveness hinges on the anterior-posterior alignment assumption, which may not hold for patients with significant breast asymmetry, prior surgeries, or extreme pectoral deformation.

- **Computational Overhead**: The cross-attention patch-to-slice alignment introduces significant computational complexity, and the paper does not report training/inference time comparisons with baseline methods.

- **Text Quality Impact**: GLAM uses synthesized radiology reports from tabular data, and the paper does not validate whether the synthesized text captures the same semantic richness as real clinical reports.

## Confidence

- **High Confidence**: Claims about GLAM outperforming baselines on EMBED dataset (verified by reported metrics and ablation studies)
- **Medium Confidence**: Claims about geometric alignment effectiveness (based on architectural design and limited ablation evidence)
- **Low Confidence**: Claims about out-of-domain generalization (evaluated on only two external datasets with limited diversity analysis)

## Next Checks

1. **Cross-Attention Visualization**: Generate attention weight heatmaps for query patches across multiple patients to verify that the model consistently attends to anatomically corresponding regions in the opposite view, not just same spatial positions.

2. **Geometric Assumption Robustness**: Systematically evaluate GLAM performance on patients with known breast asymmetry or prior surgeries to quantify how often the AP alignment assumption fails and measure the performance degradation.

3. **Text Quality Ablation**: Compare GLAM performance using synthesized reports versus real clinical reports (if available) to isolate the impact of text quality on visual-language alignment effectiveness.