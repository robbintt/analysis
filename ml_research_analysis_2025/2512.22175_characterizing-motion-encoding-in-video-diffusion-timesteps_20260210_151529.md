---
ver: rpa2
title: Characterizing Motion Encoding in Video Diffusion Timesteps
arxiv_id: '2512.22175'
source_url: https://arxiv.org/abs/2512.22175
tags:
- motion
- diffusion
- video
- timesteps
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic characterization of how motion
  is encoded across timesteps in video diffusion models, addressing the challenge
  of disentangling motion from appearance for controllable video generation. The authors
  propose a novel prompt-tampering probe that quantifies the trade-off between appearance
  editing and motion preservation by injecting new conditions over specified timestep
  ranges during the denoising trajectory.
---

# Characterizing Motion Encoding in Video Diffusion Timesteps

## Quick Facts
- arXiv ID: 2512.22175
- Source URL: https://arxiv.org/abs/2512.22175
- Reference count: 40
- Primary result: Novel systematic characterization of motion encoding across timesteps in video diffusion models, enabling simplified one-shot video motion customization

## Executive Summary
This paper introduces a systematic characterization of how motion is encoded across timesteps in video diffusion models, addressing the challenge of disentangling motion from appearance for controllable video generation. The authors propose a novel prompt-tampering probe that quantifies the trade-off between appearance editing and motion preservation by injecting new conditions over specified timestep ranges during the denoising trajectory. Their analysis reveals a consistent spatiotemporal structure: an early motion-dominant regime where re-conditioning strongly affects temporal dynamics, and a later appearance-dominant regime where re-conditioning mainly changes spatial details while largely keeping the motion. Based on this characterization, they develop a simplified one-shot video motion customization framework that restricts both training and inference to motion-dominant timesteps, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.

## Method Summary
The method introduces a prompt-tampering probe that swaps subject descriptions while keeping motion descriptions constant across different timestep ranges during denoising to identify motion-dominant vs. appearance-dominant regimes. Based on this analysis, the authors propose training temporal LoRA adapters only on early timesteps (t ∈ [1000, τ]) to learn motion without appearance leakage. The approach restricts LoRA fine-tuning to value and output projections of temporal attention layers, achieving motion customization through simple reconstruction loss without specialized debiasing modules. The framework applies to three architectures (ModelScope, Latte, CogVideoX) with architecture-specific boundaries (τ ≈ 700-950).

## Key Results
- Systematic identification of early motion-dominant and late appearance-dominant timestep regimes across three video diffusion architectures
- One-shot video motion customization framework achieving 28.16-31.96 CLIP Score, 96.42-97.19 temporal consistency, and 20.77-21.68 Pick Score
- Motion transfer without auxiliary debiasing modules or specialized objectives, outperforming previous state-of-the-art methods
- Discovery that only value and output projections of temporal attention need fine-tuning for motion customization

## Why This Works (Mechanism)

### Mechanism 1: Motion-Dominant Timestep Regime
- Claim: Motion signals are processed primarily in early denoising timesteps (t ∈ [τ, 1000], where τ ≈ 700-900), while appearance is refined in later timesteps.
- Mechanism: The denoising trajectory exhibits coarse-to-fine behavior where temporal dynamics (motion, layout) are established early, then spatial details are filled in. Re-conditioning with a new prompt at early timesteps disrupts motion; doing so at later timesteps changes appearance while preserving motion.
- Core assumption: The prompt-tampering probe (swapping subject descriptions while keeping motion descriptions constant) isolates the appearance-movement trade-off in a way that generalizes beyond this specific intervention.
- Evidence anchors:
  - [abstract] "we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space"
  - [Section 3.4] "motion signals are dominantly encoded in early denoising timesteps in video diffusion models"
  - [corpus] Related work (Time-to-Move, FlexiAct) assumes similar early/late timestep roles but without systematic quantification; this paper provides the first boundary characterization.
- Break condition: If different architectures encode motion at different timestep ranges (they do—CogVideoX requires τ≈950 vs. ~700 for others), the boundary is architecture-specific, not universal.

### Mechanism 2: Timestep-Constrained Training Blocks Appearance Leakage
- Claim: Restricting LoRA fine-tuning to motion-dominant timesteps prevents the model from overfitting to reference video appearance, eliminating the need for auxiliary debiasing modules.
- Mechanism: Since appearance information is processed in later timesteps (excluded from training), gradients from the vanilla reconstruction loss cannot propagate appearance features into the temporal attention adapters. The model only learns what is present in the training timesteps—motion.
- Core assumption: The motion information in a single reference video is sufficient for generalization without requiring appearance diversity during training.
- Evidence anchors:
  - [Section 4.2] "we propose to train the temporal LoRA with the ground truth caption in a restricted timestep range t ∈ [1000, τ]"
  - [Section 4.7, Table 5] Scaling LoRA rank to full-rank direct tuning still prevents appearance leakage, demonstrating the timestep constraint (not parameter count) is the critical factor.
  - [corpus] Prior methods (MotionDirector, VMC) required spatial debiasing modules or specialized losses; this paper shows timestep constraint alone is sufficient.
- Break condition: If training on multiple reference videos with diverse appearances, the timestep constraint may become unnecessary (the paper does not test this).

### Mechanism 3: Value/Output Projections Are Motion-Sufficient
- Claim: Within temporal attention layers, only the value (V) and output (O) projections need to be fine-tuned for motion customization; query (Q) and key (K) are unnecessary.
- Mechanism: Temporal attention cross-frame aggregation is primarily governed by how information is read out (V, O) rather than how attention patterns are computed (Q, K). Freezing Q, K preserves pre-trained attention structure while adapting motion dynamics.
- Core assumption: This finding generalizes across architectures beyond Latte (the only model tested for this ablation).
- Evidence anchors:
  - [Section 4.7, Table 4] "only training the value and output projections is necessary for motion customization... training only the query and key parameters yields no noticeable change"
  - [corpus] Weak direct evidence in corpus; this is a novel finding specific to this paper.
- Break condition: Unified spatiotemporal architectures (like CogVideoX) may not exhibit this separation since they lack dedicated temporal attention blocks.

## Foundational Learning

- Concept: **DDIM Inversion**
  - Why needed here: The probe methodology requires recovering the noise latent from a reference video, then resampling with modified conditions. Understanding inversion is prerequisite to understanding the analysis.
  - Quick check question: Given a clean video x₀, can you explain how DDIM inversion obtains x̂_T and why inversion enables trajectory manipulation?

- Concept: **Temporal Attention in Video Diffusion**
  - Why needed here: The method fine-tunes temporal attention LoRAs. Understanding how temporal attention differs from spatial attention (cross-frame vs. within-frame) is essential.
  - Quick check question: In a video transformer with separate spatial and temporal attention blocks, which block processes information across frames?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The method uses LoRA fine-tuning on temporal attention. Understanding rank, alpha, and how LoRA modifies base weights is required for implementation.
  - Quick check question: If LoRA rank=4 and you train on all timesteps vs. only early timesteps, which scenario would you expect to leak more appearance information and why?

## Architecture Onboarding

- Component map:
  - DDIM Inversion -> Prompt Tampering Probe -> Motion-Dominant Boundary Identification -> Temporal LoRA Training (V, O projections only) -> Inference with Restricted Timesteps

- Critical path:
  1. For analysis: Run prompt-tampering probe across all (τ_start, τ_end) pairs to identify architecture-specific motion-appearance boundary
  2. For customization: Train temporal LoRA on single reference video using only timesteps t ∈ [1000, τ]
  3. Inference: Apply LoRA only during t ∈ [1000, τ]; use base model for remaining timesteps

- Design tradeoffs:
  - **Stricter τ (higher value)**: Better appearance disentanglement but may lose some motion nuance
  - **Looser τ (lower value)**: More motion fidelity but risk of appearance leakage
  - **CogVideoX requires higher τ (≈950)** due to unified spatiotemporal attention entangling motion/appearance more deeply

- Failure signatures:
  - Appearance leakage in outputs: τ threshold is too low (training includes appearance-dominant timesteps)
  - Weak motion transfer: τ threshold is too high (insufficient motion signal captured)
  - No change after training: LoRA applied to Q, K instead of V, O projections

- First 3 experiments:
  1. **Boundary calibration**: On your target architecture, run the prompt-tampering probe on 10-20 videos to identify the optimal τ (where CLIP score improvement accelerates while optical flow similarity remains high).
  2. **Minimal LoRA validation**: Train V, O-only LoRA (r=α=4) on a single reference video at t ∈ [1000, τ] and verify motion transfers to a new subject prompt without appearance artifacts.
  3. **Architecture robustness check**: Compare motion transfer quality when using τ from this paper vs. your empirically-derived τ; if they differ significantly, recalibrate for your specific model checkpoint.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mechanistic explanation for why motion information becomes concentrated in early denoising timesteps across diverse architectures?
- Basis in paper: [explicit] The authors state "how motion is encoded across timesteps remains poorly understood" and characterize the phenomenon observationally but do not provide a theoretical or mechanistic account of why this spatiotemporal structure emerges.
- Why unresolved: The paper demonstrates the empirical existence of motion-dominant and appearance-dominant regimes but does not investigate the underlying network dynamics, attention patterns, or training dynamics that cause this separation.
- What evidence would resolve it: Ablation studies analyzing attention maps across timesteps, frequency-domain analysis of latent representations at each step, or training dynamics experiments tracking when motion vs. appearance information becomes irreversibly committed during the denoising process.

### Open Question 2
- Question: Does the motion-appearance boundary location generalize to longer videos (e.g., >16 frames) and higher resolutions?
- Basis in paper: [inferred] All experiments use 16-frame videos at relatively low resolutions (256×256 to 512×512). The boundary values (τ ≈ 700-950) may not scale directly to longer temporal contexts where motion complexity increases.
- Why unresolved: The relationship between video length, temporal complexity, and the optimal timestep boundary was not investigated. Longer videos may require different boundary calibration or exhibit multiple regime transitions.
- What evidence would resolve it: Systematic evaluation of the prompt-tampering probe and motion customization performance on videos of varying lengths (32, 64, 128 frames) and resolutions to identify scaling patterns.

### Open Question 3
- Question: Why does the optimal boundary τ vary significantly across architectures (τ≈700 for ModelScope/Latte vs. τ≈950 for CogVideoX)?
- Basis in paper: [explicit] The authors observe "CogVideoX... requires τ=950 over all other configurations. This value is significantly larger than other base models" but attribute this only to "unified spatiotemporal attentions" without deeper analysis.
- Why unresolved: The paper does not investigate whether the difference stems from architectural factors (attention design), training procedures, noise schedules, or latent space properties.
- What evidence would resolve it: Controlled experiments isolating individual factors (e.g., applying unified attention to U-Net backbones, varying noise schedules, analyzing latent spectral properties) to determine which architectural components drive boundary shifts.

### Open Question 4
- Question: Can the timestep-constrained approach be combined with auxiliary debiasing modules for further performance gains, or are they fundamentally redundant?
- Basis in paper: [explicit] The authors state their method achieves strong results "without auxiliary debiasing modules or specialized objectives" and suggest "timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods."
- Why unresolved: The paper positions timestep constraint as an alternative to debiasing but does not explore whether combining both approaches yields complementary benefits or whether they address the same underlying issue.
- What evidence would resolve it: Experiments combining timestep constraints with existing spatial debiasing techniques (e.g., from MotionDirector, VMC) to measure whether performance improves additively or plateaus, indicating functional equivalence.

## Limitations

- Architecture-specific boundaries: The motion-appearance boundary τ is highly architecture-dependent (τ≈700 for ModelScope/Latte vs. τ≈950 for CogVideoX), suggesting the boundary characterization may not generalize beyond tested models.
- Single-video assumption validity: The method's effectiveness with one reference video is claimed, but the paper doesn't test scenarios with multiple reference videos of diverse appearances.
- Probe methodology specificity: The conclusions rely on swapping subject descriptions while keeping motion descriptions constant, which may not capture all aspects of motion-encoding behavior.

## Confidence

- High confidence: The systematic identification of early/late timestep regimes for motion vs. appearance processing is well-supported by the prompt-tampering analysis across three architectures.
- Medium confidence: The claim that only V and O projections need fine-tuning for motion customization is demonstrated on Latte but not extensively validated across other architectures.
- Medium confidence: The simplified one-shot framework's superiority over state-of-the-art methods is demonstrated, but comparisons are limited to specific metrics.

## Next Checks

1. **Architecture boundary mapping**: Apply the prompt-tampering probe to at least 5 additional video diffusion architectures (including LDM-based and newer designs) to determine whether the motion-appearance boundary is architecture-specific or follows broader patterns.

2. **Multi-video generalization test**: Evaluate the method using 3-5 reference videos with diverse appearances and subjects to assess whether the single-video assumption holds and if the timestep constraint remains sufficient.

3. **Cross-manipulation probe validation**: Design and apply alternative probe methodologies (e.g., modifying motion prompts while keeping appearance constant, or using optical flow-based interventions) to verify that the identified boundaries are robust to different analysis approaches.