---
ver: rpa2
title: 'Lost in Literalism: How Supervised Training Shapes Translationese in LLMs'
arxiv_id: '2503.04369'
source_url: https://arxiv.org/abs/2503.04369
tags:
- translation
- translationese
- translations
- training
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) for machine translation often produce
  "translationese," overly literal and unnatural translations despite pre-training
  on natural language. This occurs due to biases introduced during supervised fine-tuning
  (SFT) when models learn to prioritize literal semantic mapping over fluent target-language
  generation.
---

# Lost in Literalism: How Supervised Training Shapes Translationese in LLMs

## Quick Facts
- arXiv ID: 2503.04369
- Source URL: https://arxiv.org/abs/2503.04369
- Reference count: 32
- Over 40% of LLM-generated translations exhibit substantial translationese patterns

## Executive Summary
Large language models trained for machine translation often produce overly literal and unnatural translations ("translationese") despite pre-training on natural language. This occurs because supervised fine-tuning on translation data containing translationese patterns teaches models to prioritize literal semantic mapping over fluent target-language generation. We systematically evaluated translationese across English-Chinese and German-English pairs, finding more than 34% of training instances contained translationese. Two mitigation strategies—polishing golden references using GPT-4 and filtering unnatural training instances—significantly reduced translationese while improving both naturalness and translation quality.

## Method Summary
We fine-tuned Llama-3.1-8B and Qwen-2.5-7B models using LoRA (rank 16) on the ALMA training set (WMT'17–WMT'21 + Flores-200, 31,621 parallel instances). Three variants were tested: baseline SFT, SFT-KD (GPT-4 direct translation of source), and SFT-Polished (GPT-4 polishing of golden references). Models were evaluated on WMT'22 test sets and document-level data from web sources using perplexity, lexical density, length variance, COMET-QE, and human Translationese Span Ratio (TSR) annotation. Perplexity-based filtering was applied to remove 20-40% of lowest-quality training instances.

## Key Results
- Over 40% of translations exhibit substantial translationese patterns across both language pairs
- More than 34% of training instances contain translationese
- GPT-4 polishing reduces translationese proportion from 43% to 25%
- Filtering 20% of lowest-perplexity training instances improves both naturalness and translation quality
- Both polishing and filtering significantly improve COMET-QE scores while reducing perplexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning introduces literalism bias that overrides pre-training's natural language priors
- Mechanism: During SFT on translation data containing translationese patterns, models learn to associate "translation task" with source-aligned literal mapping rather than target-language-fluent generation
- Core assumption: Translationese patterns in SFT data are frequent and consistent enough to establish learnable correlation
- Evidence anchors: 40.4% of En-Zh and 34.2% of De-En instances contain over 20% translationese spans; related work confirms translationese exists in MT benchmark test sets
- Break condition: Improved SFT data quality would diminish literalism bias while pre-training priors reassert

### Mechanism 2
- Claim: LLMs retain latent capacity for natural target-language generation but fail to access it under "translation" framing
- Mechanism: Pre-training establishes strong language modeling priors for natural text, but "translation" task frame conditions different behavioral mode that suppresses naturalness
- Core assumption: Perplexity-TSR correlation reflects genuine model preference rather than spurious correlation
- Evidence anchors: Positive correlation between perplexity and human-annotated TSR (0.34 for En-Zh, 0.21 for De-En); GPT-4 produces more natural outputs when refining own translations
- Break condition: Changing task framing to "polish" vs. "translate" would access latent naturalness capacity

### Mechanism 3
- Claim: Task format modulation (polishing vs. direct translation) bypasses SFT-induced translationese behavior
- Mechanism: "Polishing" prompt conditions refinement mode that leverages pre-trained natural language priors rather than SFT-learned translation patterns
- Core assumption: Behavioral difference between task formats persists across model scales and language pairs
- Evidence anchors: GPT-4 decreases translationese proportion from 43% to 25% through self-polishing; SFT-Polished achieves consistently lower perplexities than baselines
- Break condition: Polishing fails on severely mistranslated content (nothing natural to refine)

## Foundational Learning

- Concept: **Translationese vs. Translation Quality**
  - Why needed here: Distinguishes naturalness (fluency) from adequacy (semantic correctness). Standard MT metrics often fail to capture translationese because references themselves contain it
  - Quick check question: Can a translation have high COMET-QE score but still exhibit translationese? (Yes—SFT and SFT-Polished have similar COMET scores but vastly different naturalness)

- Concept: **Perplexity as Proxy for Human Judgment of Naturalness**
  - Why needed here: Uses perplexity as automatic signal for translationese detection, validated against human TSR annotations
  - Quick check question: Why might perplexity be an imperfect proxy? (Domain mismatch between pre-training and target domain, or language imbalance affecting multilingual perplexity calibration)

- Concept: **Data Quality vs. Data Quantity in SFT**
  - Why needed here: Shows filtering 20% of lowest-quality training data improves both naturalness AND quality, challenging assumption that more data is always better
  - Quick check question: Why does excessive filtering (beyond 40%) hurt performance? (Removes genuine signal along with noise, reducing model's translation capability)

## Architecture Onboarding

- Component map:
  Base LLM (Llama-3.1-8B, Qwen-2.5-7B) -> LoRA fine-tuning -> Perplexity calculator -> Polishing module (GPT-4) -> Filtered training data -> Evaluation pipeline

- Critical path:
  1. Collect parallel SFT data (WMT + Flores)
  2. Score target-side references with perplexity using base LLM
  3. Either: (a) Polish low-quality references with GPT-4, or (b) Filter instances below threshold
  4. Fine-tune base model on cleaned/augmented data
  5. Evaluate using perplexity, lexical density, length variance, and human TSR annotation

- Design tradeoffs:
  - **Polishing vs. Filtering**: Polishing retains data volume but adds API costs and may introduce GPT-4's own biases. Filtering is cheaper but reduces training data
  - **Filtering threshold**: 20% removal improves quality; 40%+ removal hurts both naturalness and translation capability
  - **Inference-time vs. training-time polishing**: Training-time polishing outperforms inference-time polishing for same model

- Failure signatures:
  - **Mistranslation propagation**: GPT-4 polishing introduces semantic errors while improving fluency
  - **Language imbalance**: Perplexity threshold calibrated on one language pair may not transfer to others
  - **Over-filtering**: Removing more than 40% of data degrades both naturalness and quality metrics

- First 3 experiments:
  1. **Baseline replication**: Fine-tune Llama-3.1-8B on unmodified ALMA training data, measure perplexity and TSR on held-out documents
  2. **Filtering sweep**: Train separate models with 0%, 20%, 40%, 60% filtering of lowest-perplexity instances; plot perplexity and COMET-QE curves
  3. **Polishing ablation**: Compare original references, GPT-4 direct translation of source, and GPT-4 polishing of original references

## Open Questions the Paper Calls Out
None

## Limitations
- Core findings may not generalize beyond 8B parameter models or English-Chinese/German-English language pairs
- Reliance on GPT-4 for polishing introduces external dependencies and potential biases from the polishing model
- Custom-collected document-level evaluation data was not publicly released, limiting reproducibility

## Confidence
- **High confidence**: Existence of translationese in SFT training data (>34% instances) and its impact on translation outputs (>40% exhibiting substantial patterns)
- **Medium confidence**: Effectiveness of perplexity-based filtering and GPT-4 polishing as mitigation strategies
- **Medium confidence**: Correlation between perplexity and human-annotated translationese (0.34 for En-Zh, 0.21 for De-En)
- **Low confidence**: Claim that supervised fine-tuning's translationese bias "overrides" pre-training's natural language priors

## Next Checks
1. Test polishing and filtering strategies on additional language pairs (e.g., English-French, English-Japanese) to assess generalizability across different linguistic distances and script systems
2. Implement perplexity-based filtering on the test set rather than training data to create post-hoc correction mechanism that doesn't require retraining
3. Conduct ablation studies removing "translation" framing entirely—have GPT-4 generate translations from scratch using both "translate" and "write naturally in target language" prompts