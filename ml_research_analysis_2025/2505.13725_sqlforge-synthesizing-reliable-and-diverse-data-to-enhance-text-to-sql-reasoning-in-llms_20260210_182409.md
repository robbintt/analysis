---
ver: rpa2
title: 'SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning
  in LLMs'
arxiv_id: '2505.13725'
source_url: https://arxiv.org/abs/2505.13725
tags:
- data
- domain
- text-to-sql
- schema
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SQLForge, a data synthesis framework that improves
  the reliability and diversity of text-to-SQL data for fine-tuning open-source large
  language models. It addresses the challenge of generating high-quality text-to-SQL
  data at scale by combining SQL syntax constraints, template enrichment, and iterative
  domain exploration.
---

# SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs

## Quick Facts
- **arXiv ID**: 2505.13725
- **Source URL**: https://arxiv.org/abs/2505.13725
- **Reference count**: 17
- **Primary result**: Achieves 85.7% exact match accuracy on Spider and 59.8% on BIRD benchmarks using open-source LLMs

## Executive Summary
SQLForge is a data synthesis framework that generates high-quality text-to-SQL data by combining SQL syntax constraints, template enrichment, and iterative domain exploration. The framework generates SQL statements in new domains, derives corresponding schemas, and translates them into natural language questions, ensuring semantic alignment. Using SQLForge, the authors fine-tune open-source models to create SQLForge-LM, achieving state-of-the-art performance among open-source methods on the Spider (85.7% exact match accuracy) and BIRD (59.8% exact match accuracy) benchmarks, significantly narrowing the gap with closed-source models.

## Method Summary
SQLForge is a four-stage pipeline that generates reliable and diverse text-to-SQL data for fine-tuning open-source LLMs. It starts with seed SQL queries from Spider and BIRD datasets, parses them into AST-based templates with syntax constraints, then uses iterative domain exploration to generate new domains and auxiliary SQL statements. The Schema Architect module creates database schemas, and the Question Reverse-Translator generates natural language questions by incorporating schema information. The augmented data is combined with seed data and used to fine-tune open-source LLMs using LoRA (rank=128, alpha=256) with 6 NVIDIA A100 (40GB) GPUs.

## Key Results
- Achieves 85.7% exact match accuracy on Spider Dev set and 59.8% on BIRD Dev set
- Generates 25K samples across 1,000+ domains from 18K seed samples
- Maintains >90% executable rate even for complex SQL (150+ tokens) compared to direct augmentation methods

## Why This Works (Mechanism)

### Mechanism 1
SQL template-based generation with syntax constraints improves data reliability compared to direct LLM prompting. The SQL Parser converts seed SQL queries into ASTs and replaces non-keyword nodes with typed placeholders, creating syntactically valid templates that act as hard constraints during generation.

### Mechanism 2
Iterative domain exploration with auxiliary SQL constraints increases data diversity while maintaining database semantic coherence. The framework generates new domain names paired with auxiliary SQL statements, using conditional entropy constraints to compress the naming space into database-compatible domains.

### Mechanism 3
Reverse translation (SQL → Question) with schema integration improves semantic alignment over direct Question → SQL generation. SQLForge generates SQL first, then derives the schema, then reverse-translates to natural language questions, ensuring questions reference actual database entities and relationships.

## Foundational Learning

- **Abstract Syntax Trees (AST) for SQL**: The SQL Parser relies on AST traversal to extract templates while preserving grammatical structure. Quick check: Given `SELECT name FROM users WHERE age > 18`, what nodes would be replaced with placeholders in the AST?

- **Conditional Entropy H(Y|X) in Text Generation**: SQL Foundry uses conditional entropy constraints to ensure generated domain names are conditioned on auxiliary SQL context. Quick check: Why does conditioning domain generation on auxiliary SQL reduce the probability of generating irrelevant domains compared to unconditional generation?

- **LoRA (Low-Rank Adaptation) Fine-Tuning**: SQLForge-LM uses LoRA fine-tuning (rank=128, α=256) rather than full parameter fine-tuning. Quick check: What are the memory and expressiveness trade-offs when using LoRA with rank=128 vs. full fine-tuning?

## Architecture Onboarding

- **Component map**: Seed Data (Spider/BIRD training sets) → [SQL Parser] → Templates (AST-based extraction + crossover) → [SQL Foundry] → Domain Exploration (K iterations) → Domain-Specific SQL Generation → [Schema Architect] → Database schemas (CREATE TABLE statements) → [Question Reverse-Translator] → Natural language questions → Augmented Data (Q, Sch, S) → Merge with seed data → Fine-tune open-source LLMs (LoRA)

- **Critical path**: SQL Parser template quality → SQL Foundry domain diversity → Schema Architect schema validity → Question Reverse-Translator semantic alignment. Errors propagate forward; invalid templates cause downstream SQL failures.

- **Design tradeoffs**: Template rigidity vs. diversity (stricter templates improve reliability but may limit SQL structural variety); exploration iterations (K) (more iterations increase domain diversity but require more compute); closed-source (GPT-4) vs. open-source generation (paper uses GPT-4 for synthesis but shows open-source models can achieve ~90% of GPT-4's executable rate).

- **Failure signatures**: High executable rate but low benchmark accuracy (likely semantic misalignment between questions and SQL); low executable rate at high token lengths (>150) (direct augmentation baseline shows this); poor generalization to BIRD (insufficient complex SQL templates or domain diversity in augmented data).

- **First 3 experiments**:
  1. Reproduce executable rate comparison: Generate 1K SQL samples each at token lengths 50, 100, 150, 200 using both SQLForge and direct augmentation with GPT-4.
  2. Ablate schema integration in reverse translation: Generate questions with and without schema context in the Question Reverse-Translator prompt.
  3. Scale analysis: Train CodeLlama-7B with 0×, 1/4×, 1/2×, 1× augmented data to verify scaling behavior.

## Open Questions the Paper Calls Out

### Open Question 1
How does full parameter fine-tuning compare to the Low-Rank Adaptation (LoRA) method used in the study? The study restricted its experiments to LoRA due to computational resource constraints, and benchmarking SQLForge-LM performance with full parameter updates versus LoRA results would resolve this.

### Open Question 2
Can the SQLForge synthesis framework maintain high data quality when deployed on fine-tuned open-source models instead of GPT-4? The primary data synthesis pipeline relied on GPT-4; adaptability tests on open-source models were limited to standard models, not fine-tuned ones.

### Open Question 3
Do the benefits of SQLForge data synthesis persist when applied to models significantly larger than 13B parameters? The reported experiments focus primarily on the 7B and 13B parameter scales, and fine-tuning a large-scale model (e.g., 70B parameters) on the augmented dataset would measure the performance implications.

## Limitations
- The synthesis pipeline's dependence on GPT-4 raises reproducibility concerns, particularly regarding unspecified few-shot examples and exploration iteration count (K)
- The paper doesn't address potential bias amplification from synthetic data—if seed data contains systematic errors, the template-based generation could perpetuate and scale these issues
- Claims about domain diversity improvements lack comparative corpus analysis and verification against alternative domain generation strategies

## Confidence

**High confidence**: The general approach of using syntax constraints and reverse translation is well-established in the text-to-SQL literature. The reported benchmark results (85.7% Spider, 59.8% BIRD) are internally consistent with the scaling experiments shown in Figure 3.

**Medium confidence**: The executable rate improvements (Figure 6) are compelling but rely on GPT-4 evaluation. The gap between SQLForge and direct augmentation widening with complexity is expected given the template-based approach, but the absolute numbers depend on evaluation methodology not fully specified.

**Low confidence**: Claims about domain diversity improvements lack comparative corpus analysis. The assertion that conditional entropy constraints reduce domain drift is theoretically sound but unverified against alternative domain generation strategies. The scalability analysis assumes linear improvements that may not hold for larger model families.

## Next Checks

1. Isolate template quality impact: Generate SQL using only the SQL Parser module (no domain exploration, no reverse translation) with the same seed data. Measure executable rates and compare against the full SQLForge pipeline to quantify the template mechanism's isolated contribution.

2. Validate domain coherence: Implement an unconstrained domain generation baseline using the same seed data but without the conditional entropy constraint. Use embedding-based clustering to measure domain semantic coherence and drift in both approaches across 1K generated samples.

3. Test reverse translation fidelity: Create a controlled test set of 100 complex SQL queries (including nested queries, subqueries, and multiple joins). Generate questions using both SQLForge's reverse translation and a direct question-to-SQL approach, then have human annotators rate semantic alignment quality without seeing the original SQL.