---
ver: rpa2
title: Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima
arxiv_id: '2505.15643'
source_url: https://arxiv.org/abs/2505.15643
tags:
- arms
- optimal
- multiple
- identification
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of best-arm identification in
  stochastic multi-armed bandits under the fixed-confidence setting, particularly
  focusing on instances with multiple optimal arms. The authors revisit the Track-and-Stop
  algorithm, which is widely conjectured to be instance-optimal, and propose a modified
  stopping rule that ensures instance-optimality even when the set of optimal arms
  is not a singleton.
---

# Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima

## Quick Facts
- arXiv ID: 2505.15643
- Source URL: https://arxiv.org/abs/2505.15643
- Authors: Lan V. Truong
- Reference count: 18
- Primary result: Instance-optimal algorithm for best-arm identification with multiple optimal arms

## Executive Summary
This paper addresses the problem of best-arm identification in stochastic multi-armed bandits under fixed-confidence settings when multiple optimal arms exist. The authors propose a modified stopping rule for the Track-and-Stop algorithm that achieves instance-optimality even when the set of optimal arms contains more than one element. The key innovation is a new information-theoretic lower bound that explicitly accounts for multiple optimal arms, which the modified algorithm matches. This work closes a significant theoretical gap in the bandit literature by providing a complete solution to the best-arm identification problem under this important and realistic scenario.

## Method Summary
The authors revisit the Track-and-Stop algorithm, which uses a track-and-stop sampling rule combined with Chernoff's stopping rule. They modify the stopping rule to handle cases where multiple arms share the optimal mean reward. The analysis introduces a refined theoretical framework that combines generalized likelihood ratio statistics with convex optimization techniques. The stopping rule modification involves computing a threshold that depends on the specific bandit instance parameters and the number of optimal arms, allowing the algorithm to correctly identify any subset of optimal arms while maintaining instance-optimal sample complexity.

## Key Results
- The modified Track-and-Stop algorithm achieves instance-optimal sample complexity when multiple optimal arms exist
- A new information-theoretic lower bound explicitly accounts for multiple optimal arms
- The proposed stopping rule matches the theoretical lower bound tightly
- The algorithm maintains the same order of sample complexity as the single-optimal-arm case

## Why This Works (Mechanism)
The mechanism works by modifying the stopping rule to incorporate the structure of the problem when multiple optimal arms exist. The key insight is that the traditional Chernoff stopping rule needs adjustment because it implicitly assumes a unique optimal arm. By introducing a stopping threshold that accounts for the gap between suboptimal arms and the set of optimal arms (rather than just a single optimal arm), the algorithm can correctly terminate when sufficient evidence has been gathered to identify any member of the optimal set. The analysis leverages the generalized likelihood ratio to create a statistic that properly handles the multiplicity of optimal solutions.

## Foundational Learning

1. **Stochastic Multi-armed Bandits**
   - Why needed: Provides the fundamental problem setting and terminology
   - Quick check: Understand the difference between regret minimization and best-arm identification

2. **Information-theoretic Lower Bounds**
   - Why needed: Establishes the theoretical limit that any algorithm should achieve
   - Quick check: Can derive the Lai-Robbins lower bound for simple bandit instances

3. **Track-and-Stop Algorithm**
   - Why needed: The base algorithm that requires modification
   - Quick check: Understand the track-and-stop sampling rule and Chernoff's original stopping rule

4. **Generalized Likelihood Ratio Statistics**
   - Why needed: Key tool for constructing the modified stopping statistic
   - Quick check: Know how to compute and interpret likelihood ratio tests

5. **Convex Optimization Techniques**
   - Why needed: Used in the analysis to establish optimality guarantees
   - Quick check: Can formulate and solve the optimization problems that arise in the analysis

## Architecture Onboarding

Component map: Sampling rule -> Track-and-Stop allocation -> Observations -> Generalized likelihood ratio statistic -> Modified stopping rule threshold -> Decision

Critical path: The algorithm iteratively collects samples according to the track-and-stop rule, computes the generalized likelihood ratio statistic, compares it against the modified threshold, and stops when the threshold is exceeded.

Design tradeoffs: The modified stopping rule adds computational overhead but achieves optimality. The threshold computation requires knowledge of instance parameters, which may not be available in practice but can be estimated.

Failure signatures: The algorithm may fail to stop in finite time if the stopping threshold is set too high, or may stop prematurely if set too low. The generalized likelihood ratio may not concentrate properly if the sampling rule doesn't explore sufficiently.

First experiments:
1. Verify the algorithm on a simple 2-armed bandit with two optimal arms
2. Test on a larger instance with k arms and verify the sample complexity scaling
3. Compare against the original Track-and-Stop algorithm on instances with multiple optimal arms

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes stochastic rewards with bounded support, which may not hold in all practical scenarios
- Implementation complexity may pose challenges in high-dimensional or large-scale bandit problems
- Limited empirical validation to synthetic settings only

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical framework is sound | High |
| Modified stopping rule achieves optimality | High |
| Instance-optimal sample complexity is achieved | High |
| Extension to non-stochastic settings | Low |

## Next Checks

1. Implement and test the modified Track-and-Stop algorithm on real-world datasets to assess practical performance and computational efficiency

2. Extend the theoretical framework to handle contextual bandit settings and validate the optimality of the proposed stopping rule

3. Analyze the robustness of the algorithm under different reward distributions, including heavy-tailed distributions, to test the bounded support assumption