---
ver: rpa2
title: 'SPEX: Scaling Feature Interaction Explanations for LLMs'
arxiv_id: '2502.13870'
source_url: https://arxiv.org/abs/2502.13870
tags:
- spex
- interaction
- interactions
- fourier
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SPEX is a model-agnostic interaction attribution method that scales\
  \ to large input lengths (n\u22481000) by leveraging sparse Fourier transforms and\
  \ channel decoding algorithms. It exploits the natural sparsity of interactions\
  \ in real-world data to efficiently identify important feature interactions without\
  \ exhaustive enumeration."
---

# SPEX: Scaling Feature Interaction Explanations for LLMs

## Quick Facts
- arXiv ID: 2502.13870
- Source URL: https://arxiv.org/abs/2502.13870
- Reference count: 40
- Primary result: SPEX scales to large input lengths (â‰ˆ1000) using sparse Fourier transforms to identify important feature interactions with O(sdn) complexity

## Executive Summary
SPEX introduces a novel approach to identifying feature interactions in large language models by leveraging sparse Fourier transforms and channel decoding algorithms. The method exploits the natural sparsity of interactions in real-world data to efficiently identify important feature interactions without exhaustive enumeration. SPEX demonstrates significant improvements over marginal attribution methods, achieving up to 20% better faithfulness metrics while scaling to large input lengths that were previously computationally infeasible.

## Method Summary
SPEX is a model-agnostic interaction attribution method that reformulates feature interaction detection as a sparse recovery problem. It uses a sparse Fast Fourier Transform (sFFT) to estimate Fourier coefficients of the model's score function, then applies channel decoding algorithms to identify the k most important interactions. The method operates by constructing a feature interaction function from the model's predictions, computing its Fourier coefficients through a finite difference approximation, and solving a sparse recovery problem to identify significant interactions. This approach exploits the natural sparsity of interactions in real-world data, reducing computational complexity from O(nd) to O(sdn) where s is the sparsity level.

## Key Results
- Outperforms marginal attribution methods by up to 20% in faithfulness metrics
- Successfully identifies key features and interactions influencing model outputs on HotpotQA
- Demonstrates reasoning errors in GPT-4o mini and compositional reasoning in vision-language models
- Scales to input lengths of approximately 1000 features while maintaining computational efficiency

## Why This Works (Mechanism)
SPEX exploits the fundamental observation that feature interactions in real-world data are typically sparse - only a small subset of all possible interactions significantly influence model outputs. By reformulating interaction detection as a sparse recovery problem, SPEX leverages the mathematical efficiency of sparse Fourier transforms to identify important interactions without enumerating all possible combinations. The channel decoding algorithms then efficiently recover the sparse set of significant interactions from the Fourier coefficient estimates, enabling scalable analysis of complex models.

## Foundational Learning

**Sparse Fourier Transform (sFFT)**: Efficient algorithm for computing Fourier coefficients when the signal is sparse in the frequency domain. Why needed: Traditional FFT requires O(n log n) operations even when most coefficients are zero, making it inefficient for sparse signals. Quick check: Verify sparsity of the Fourier spectrum for the interaction function.

**Channel Decoding Algorithms**: Methods for recovering sparse signals from noisy measurements, such as Orthogonal Matching Pursuit (OMP) and Basis Pursuit. Why needed: To identify the most significant feature interactions from the estimated Fourier coefficients. Quick check: Test recovery accuracy with different decoding algorithms under varying noise levels.

**Feature Interaction Function**: A mathematical function that maps feature subsets to their combined effect on model output. Why needed: Provides the foundation for analyzing how groups of features interact to influence predictions. Quick check: Validate the interaction function captures non-additive effects between features.

## Architecture Onboarding

**Component Map**: Input features -> Sparse Fourier Transform -> Fourier Coefficient Estimation -> Channel Decoding -> Significant Interactions

**Critical Path**: The computation proceeds through: (1) constructing the interaction function from model predictions, (2) applying sFFT to estimate Fourier coefficients, (3) using channel decoding to identify k significant interactions, (4) returning the sparse set of important feature interactions.

**Design Tradeoffs**: 
- Accuracy vs. computational efficiency: Higher precision in Fourier coefficient estimation improves accuracy but increases computational cost
- Sparsity assumption: Works best when interactions are truly sparse; performance degrades with dense interaction patterns
- Model-agnostic approach: Works with any model but may not capture model-specific interaction patterns as effectively as specialized methods

**Failure Signatures**: 
- Poor performance when interactions are dense rather than sparse
- Reduced accuracy with continuous features requiring discretization
- Computational overhead when k approaches d (number of features)

**First Experiments**:
1. Synthetic data with known sparse interactions to validate recovery accuracy
2. Text classification tasks with varying input lengths to test scalability
3. Ablation study comparing different channel decoding algorithms

## Open Questions the Paper Calls Out

None

## Limitations

- Restricted to discrete-valued features, requiring discretization for continuous inputs which may alter interaction patterns
- Relies on strong assumptions including sparsity of interactions, local sensitivity of models, and computational efficiency requirements
- Empirical validation focuses on text and multimodal tasks where ground truth interactions are difficult to establish definitively

## Confidence

**Confidence assessments:**
- High confidence in the mathematical formulation and algorithmic approach
- Medium confidence in the computational complexity claims (theoretical bounds vs practical performance)
- Medium confidence in the faithfulness improvements (dependent on evaluation metrics)
- Low confidence in generalizability to continuous features and non-sparse interaction regimes

## Next Checks

1. Empirical runtime comparison against existing methods across varying input lengths and sparsity levels on identical hardware
2. Ablation study testing SPEX performance with different discretization strategies for continuous features
3. Cross-dataset evaluation measuring faithfulness improvements across diverse domains beyond text and vision-language tasks