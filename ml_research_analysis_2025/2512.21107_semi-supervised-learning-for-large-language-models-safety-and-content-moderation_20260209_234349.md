---
ver: rpa2
title: Semi-Supervised Learning for Large Language Models Safety and Content Moderation
arxiv_id: '2512.21107'
source_url: https://arxiv.org/abs/2512.21107
tags:
- safety
- which
- data
- examples
- semi-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training safety classifiers
  for Large Language Models (LLMs) using limited labeled data, which is costly to
  acquire and prone to errors. To mitigate this, the authors propose leveraging semi-supervised
  learning (SSL) techniques that combine a small set of labeled data with a large
  set of unlabeled data to improve safety classification performance.
---

# Semi-Supervised Learning for Large Language Models Safety and Content Moderation

## Quick Facts
- arXiv ID: 2512.21107
- Source URL: https://arxiv.org/abs/2512.21107
- Authors: Eduard Stefan Dinuta; Iustin Sirbu; Traian Rebedea
- Reference count: 21
- Key outcome: SSL achieves up to 5% higher F1 scores than supervised learning with only 200 labeled examples, matching full supervision with 2000 examples

## Executive Summary
This paper addresses the challenge of training safety classifiers for Large Language Models (LLMs) using limited labeled data, which is costly to acquire and prone to errors. To mitigate this, the authors propose leveraging semi-supervised learning (SSL) techniques that combine a small set of labeled data with a large set of unlabeled data to improve safety classification performance. Specifically, they explore several SSL algorithms—FixMatch, MarginMatch, and MultiMatch—and introduce a novel task-specific augmentation method that uses LLMs to identify and rephrase harmful text fragments while preserving malicious intent. The results show that SSL significantly outperforms supervised learning, especially with small labeled datasets (e.g., 200 examples), achieving up to 5% higher F1 scores on harmful prompt classification. With only 2000 labeled examples, SSL approaches nearly match the performance of fully supervised training on the entire dataset. The task-specific LLM-generated augmentations consistently outperform standard backtranslation methods, particularly for prompt classification, highlighting the importance of nuanced, safety-aware data augmentation in this domain.

## Method Summary
The authors propose a semi-supervised learning framework for LLM safety classification that combines labeled and unlabeled data to overcome the limitations of scarce safety annotations. Their approach integrates three SSL algorithms (FixMatch, MarginMatch, MultiMatch) with a novel LLM-based augmentation technique that identifies and rephrases harmful text while preserving malicious intent. The method processes data through supervised training on labeled examples combined with pseudo-labeling of high-confidence unlabeled examples, using consistency regularization to ensure model predictions remain stable across augmented views of the same input. The task-specific augmentation leverages LLMs to generate safety-aware paraphrases that maintain the harmful characteristics necessary for effective safety training, contrasting with generic backtranslation approaches.

## Key Results
- SSL algorithms achieve up to 5% higher F1 scores than supervised learning with only 200 labeled examples
- With 2000 labeled examples, SSL approaches match the performance of fully supervised training on the entire dataset
- Task-specific LLM augmentations outperform standard backtranslation methods, especially for prompt classification
- MarginMatch consistently achieves the highest F1 scores across different dataset sizes

## Why This Works (Mechanism)
The effectiveness of this approach stems from the synergistic combination of three key mechanisms: (1) SSL algorithms leverage the statistical regularities in unlabeled data to provide additional training signals beyond what's available in limited labeled examples, (2) consistency regularization forces the model to learn robust, invariant features by maintaining prediction stability across different augmented views of the same input, and (3) task-specific LLM augmentations preserve critical safety-related characteristics that generic augmentation methods might inadvertently remove, ensuring that the model learns to recognize harmful patterns regardless of surface-level variations.

## Foundational Learning
- Semi-supervised learning (SSL): Combines small labeled datasets with large unlabeled data to improve model performance when annotations are scarce
  - Why needed: Labeled safety data is expensive and time-consuming to acquire
  - Quick check: SSL should outperform supervised learning with limited labels
- Consistency regularization: Ensures model predictions remain stable across augmented views of the same input
  - Why needed: Encourages the model to learn robust features invariant to input variations
  - Quick check: Consistency loss should decrease during training
- Pseudo-labeling: Assigns labels to unlabeled data based on model predictions with high confidence
  - Why needed: Leverages unlabeled data by creating additional training examples
  - Quick check: Pseudo-labeled data should improve validation performance
- Task-specific augmentation: Generates safety-aware paraphrases that preserve harmful intent while modifying surface form
  - Why needed: Standard augmentations may lose critical safety characteristics
  - Quick check: Augmented examples should maintain original harm labels

## Architecture Onboarding

Component map: Input data -> Data augmentation (LLM or backtranslation) -> SSL algorithm (FixMatch/MarginMatch/MultiMatch) -> Classifier -> Output predictions

Critical path: Raw text → Task-specific augmentation → SSL pseudo-labeling → Consistency regularization → Final classifier training

Design tradeoffs: The paper trades computational overhead of SSL methods against the need for high-quality safety classifiers with limited labeled data. LLM-based augmentation provides better task-specific performance but requires additional inference costs compared to backtranslation.

Failure signatures: SSL performance degrades when pseudo-labels are incorrect (low confidence samples), when augmentations lose harmful characteristics, or when the unlabeled data distribution differs significantly from labeled data.

First experiments:
1. Compare SSL performance across different labeled dataset sizes (200, 500, 1000, 2000 examples)
2. Evaluate task-specific LLM augmentation versus backtranslation on held-out test sets
3. Ablation study removing consistency regularization to measure its contribution

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, though the limitations section suggests several areas for future research including generalizability across different safety domains and systematic evaluation of LLM augmentation quality.

## Limitations
- Evaluation relies on a single, relatively small safety dataset (LaSAIDA with 10k examples), limiting generalizability
- Performance analysis assumes high-quality LLM augmentations without systematic error analysis of generated examples
- The paper does not address potential biases introduced by LLM-based augmentation
- Limited discussion of computational overhead and training time compared to simpler supervised approaches

## Confidence
- SSL algorithm performance claims: High
- Task-specific augmentation superiority: High
- Generalizability across safety domains: Medium
- Long-term robustness of LLM-generated augmentations: Low
- Computational efficiency claims: Low

## Next Checks
1. Evaluate SSL performance on additional safety datasets covering different domains (e.g., code safety, multilingual content) to assess generalizability
2. Conduct systematic error analysis of LLM-generated augmentations to identify failure modes and potential safety risks
3. Compare computational efficiency and training time against simpler supervised approaches with data augmentation baselines
4. Investigate potential biases introduced by LLM-based augmentation through targeted experiments