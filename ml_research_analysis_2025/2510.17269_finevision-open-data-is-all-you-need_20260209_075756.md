---
ver: rpa2
title: 'FineVision: Open Data Is All You Need'
arxiv_id: '2510.17269'
source_url: https://arxiv.org/abs/2510.17269
tags:
- arxiv
- https
- datasets
- http
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FineVision unifies 200+ open datasets into a single, large-scale
  corpus of 24 million samples with rigorous curation and decontamination. A semi-automated
  pipeline with human-in-the-loop ensures consistent schema, data quality, and safety
  while removing duplicates and near-leaks.
---

# FineVision: Open Data Is All You Need

## Quick Facts
- arXiv ID: 2510.17269
- Source URL: https://arxiv.org/abs/2510.17269
- Reference count: 40
- Primary result: State-of-the-art open-data VLM trained on 24M curated samples achieves 12.7 percentage point improvement over best existing open mixtures

## Executive Summary
FineVision unifies 200+ open datasets into a single, large-scale corpus of 24 million samples with rigorous curation and decontamination. A semi-automated pipeline with human-in-the-loop ensures consistent schema, data quality, and safety while removing duplicates and near-leaks. Models trained on FineVision achieve state-of-the-art results among open-data VLMs, improving by 12.7 percentage points over the best existing open mixtures on average across 11 benchmarks. The corpus also enables GUI/agentic task training via a unified action space, with strong zero-shot and fine-tuned performance. All data, tools, and precomputed embeddings are released to support reproducible, high-quality VLM research.

## Method Summary
FineVision employs a semi-automated human-in-the-loop pipeline to unify 200+ sources into 185 subsets with standardized chat schema, then removes near-duplicates and test-set overlaps via SSCD embeddings (τ = 0.95). The resulting corpus contains 24.3M samples (17.3M images, 88.9M turns, 9.5B answer tokens). Training uses 460M SmolVLM architecture with single-stage training for ~20 hours on 32 H100 GPUs, achieving state-of-the-art performance on 11 benchmarks and enabling GUI agent capabilities through unified action-space normalization.

## Key Results
- FineVision-trained models achieve state-of-the-art results among open-data VLMs, improving by 12.7 percentage points over best existing open mixtures
- Contamination rate of only 1.02% achieved through rigorous decontamination against 66 public benchmarks
- Unified action-space curation enables GUI/agentic capabilities with efficient fine-tuning, matching models 4x larger

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on a unified, decontaminated corpus improves benchmark generalization relative to fragmented alternatives
- Mechanism: Semi-automated human-in-the-loop pipeline converts 200+ sources into 185 subsets with standardized chat schema, then removes near-duplicates and test-set overlaps via SSCD embeddings (τ = 0.95), reducing train–test leakage and increasing effective diversity
- Core assumption: Contamination and inconsistent formats materially degrade VLM performance
- Evidence anchors:
  - [abstract] "We unify more than 200 sources into 185 subsets via a semi-automated, human-in-the-loop pipeline... rigorous de-duplication... and decontamination against 66 public benchmarks"
  - [section 2.4] "Images with similarity ≥τ are flagged... FineVision has a contamination rate of only 1.02%"
  - [corpus] Neighbors discuss fragmented data challenges (e.g., AutoClimDS notes heterogeneous sources hinder reproducibility), indirectly supporting the motivation but not this exact mechanism
- Break condition: If decontamination removes too many examples from a capability-critical domain (e.g., GUI data), performance on that domain may plateau or drop

### Mechanism 2
- Claim: Scaling both size and conceptual balance yields stronger generalization than scale alone
- Mechanism: FineVision's curation increases image count (17.3M vs 5.4M for Cambrian) while raising participation ratio (182.52 vs 152.70), so variance is more uniformly distributed across dimensions, reducing concept dominance and improving robustness
- Core assumption: Diversity metrics (effective rank, participation ratio) predict downstream generalization better than raw sample count
- Evidence anchors:
  - [section 3.3] "FineVision possesses a substantially higher participation ratio... conceptual coverage is not only broad but also significantly more uniform"
  - [section 4.2] "FineVision yields an average absolute score improvement of 12.7 percentage points over The Cauldron"
  - [corpus] Weak direct evidence for diversity–generalization link in neighbors; not explicitly validated externally
- Break condition: If domain rebalancing oversamples low-quality subsets (per prompt-based scores), noise may dominate signal

### Mechanism 3
- Claim: Unified action-space curation enables GUI/agentic capabilities with efficient fine-tuning
- Mechanism: Parse and normalize heterogeneous GUI action signatures into a unified schema with typed parameters and normalized coordinates; train on combined desktop/mobile/browser trajectories so small models learn coherent action patterns
- Core assumption: Action-space consistency across platforms is critical; trajectory fidelity is preserved after normalization
- Evidence anchors:
  - [section 2.2] "Our pipeline includes (i) a parser that extracts and normalizes arbitrary function signatures... (ii) an action conversion module that maps all action representations into a unified schema"
  - [section 4.2] "After fine-tuning FineVision-trained models achieve results on par with an architecturally equivalent model 4x its size"
  - [corpus] No direct neighbor validation of unified GUI action spaces; evidence is paper-internal only
- Break condition: If new GUI environments diverge semantically (e.g., new action types), the unified schema may underfit and require schema extension

## Foundational Learning

- Concept: Train–test contamination and near-duplicate leakage
  - Why needed here: Essential to understand why decontamination matters and how SSCD embedding matching is applied
  - Quick check question: What cosine similarity threshold does FineVision use to flag potential test-set leakage, and why is it conservative?

- Concept: Self-supervised copy-detection descriptors (SSCD)
  - Why needed here: Provides the technical basis for deduplication and diversity measurement
  - Quick check question: How do SSCD embeddings support both duplicate detection and diversity metrics like effective rank?

- Concept: Unified action space for GUI agents
  - Why needed here: Critical for understanding how heterogeneous GUI datasets are harmonized for cross-domain training
  - Quick check question: What normalization strategy ensures resolution-agnostic training for screen coordinates?

## Architecture Onboarding

- Component map: Data ingestion → schema mapping → cleaning → SSCD deduplication → test-set decontamination → LLM/VLM quality scoring → unified conversational schema → (optional) GUI action-space normalization
- Critical path: Decontamination and deduplication directly determine whether reported gains are reproducible; schema unification ensures multi-turn conversations are consistent across 200+ sources
- Design tradeoffs: Conservative τ = 0.95 reduces false negatives but may retain some near-duplicates; prompt-based quality scores (Formatting, Relevance, Visual Dependency, Image–Question Correspondence) are released but do not improve performance when used as filters
- Failure signatures: Models trained on aggressively filtered subsets underperform full FineVision; contamination removal causes <2 pp drop for FineVision vs >2.7 pp for baselines
- First 3 experiments:
  1. Replicate contamination analysis by recomputing SSCD embeddings for a subset of benchmarks and measuring overlap rates
  2. Train a small VLM on FineVision vs a baseline mixture and compare average scores across the 11 reported benchmarks
  3. Fine-tune the FineVision-trained model on a held-out GUI subset (e.g., aguvis-stage-1) and evaluate on ScreenSpot-V2 to validate unified action-space benefits

## Open Questions the Paper Calls Out

- Question: Why does merging single-image questions into multi-turn conversations fail to improve downstream performance?
  - Basis in paper: [explicit] Section 2.2 states that the authors "experiment with generally merging multiple individual questions for the same image into a multi-turn conversation, but this did not result in improved performance during our ablations."
  - Why unresolved: The paper reports the negative result but does not provide an analysis of whether this is due to context window limitations, noise introduction, or the specific grouping strategy used.
  - What evidence would resolve it: A detailed ablation comparing training loss curves and attention patterns between single-turn and grouped multi-turn variants, or tests using different grouping heuristics (e.g., semantic clustering vs. random).

- Question: Can the released per-turn quality scores be leveraged effectively for data selection via methods other than naive thresholding?
  - Basis in paper: [inferred] Appendix A.4 concludes that "naive prompt-score thresholding... is insufficient" for filtering, yet the authors release these scores specifically to support "reweighting" and analysis.
  - Why unresolved: The current scores act as poor filters for quality control, leaving open the question of whether they are valid metrics for curriculum learning or importance sampling.
  - What evidence would resolve it: Experiments applying curriculum learning or dynamic loss weighting based on these quality scores that demonstrate improved convergence or final accuracy over the unfiltered baseline.

- Question: How can FineVision be extended to overcome the remaining limitations in long-context and multi-document reasoning?
  - Basis in paper: [explicit] The Conclusion acknowledges that "long-context and multi-document reasoning are still challenging" and invites the community to extend the corpus to "longer-context reasoning."
  - Why unresolved: The paper does not specify if the failure in these tasks is due to the model architecture (SmolVLM) or a lack of specific long-context supervision in the current dataset mixture.
  - What evidence would resolve it: Evaluations on long-context benchmarks using a model trained on a modified FineVision subset enriched with explicit multi-document chains-of-thought.

## Limitations

- The paper's contamination removal process is conservative (τ = 0.95) but may still retain some near-duplicates, and the extent to which these affect downstream performance is unclear
- While the unified GUI action space enables cross-platform training, the paper doesn't demonstrate robustness to new GUI environments with different action signatures
- The claim that prompt-based quality scores don't improve performance when used as filters needs further validation across different model sizes

## Confidence

- High confidence in: Contamination rates and SSCD deduplication methodology (1.02% contamination rate is precisely measured)
- Medium confidence in: Performance improvements (12.7 pp gain) as the exact subset mixing ratios and optimizer settings are not fully specified
- Medium confidence in: Unified action-space benefits for GUI tasks, as evidence is primarily paper-internal

## Next Checks

1. Replicate contamination analysis by recomputing SSCD embeddings for a subset of benchmarks and measuring overlap rates against the reported 1.02% contamination rate
2. Train a small VLM on FineVision vs a baseline mixture and compare average scores across the 11 reported benchmarks to verify the 12.7 pp improvement claim
3. Fine-tune the FineVision-trained model on a held-out GUI subset (e.g., aguvis-stage-1) and evaluate on ScreenSpot-V2 to validate unified action-space benefits with a model 4x smaller than the architectural baseline