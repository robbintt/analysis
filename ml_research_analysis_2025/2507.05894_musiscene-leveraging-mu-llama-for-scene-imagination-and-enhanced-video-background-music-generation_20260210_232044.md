---
ver: rpa2
title: 'MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background
  Music Generation'
arxiv_id: '2507.05894'
source_url: https://arxiv.org/abs/2507.05894
tags:
- music
- video
- captions
- musiscene
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of existing music captioning
  models, which focus primarily on musical elements and lack scene imagination capabilities.
  The authors propose MusiScene, a model that fine-tunes the Music Understanding LLaMA
  (MU-LLaMA) to perform Music Scene Imagination (MSI), generating contextually relevant
  captions that imagine video scenes suitable for given music.
---

# MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation

## Quick Facts
- arXiv ID: 2507.05894
- Source URL: https://arxiv.org/abs/2507.05894
- Reference count: 3
- Primary result: MusiScene outperforms MU-LLaMA in scene imagination, with BLEU 0.421 vs 0.190, and improves VBMG coherence when using generated captions.

## Executive Summary
MusiScene addresses the limitation of existing music captioning models that focus on musical elements without scene imagination capabilities. The model fine-tunes the Music Understanding LLaMA (MU-LLaMA) to generate contextually relevant captions that imagine video scenes suitable for given music. Using a cross-modal dataset of 3,371 video-audio pairs, MusiScene achieves state-of-the-art performance in Music Scene Imagination (MSI) and demonstrates improved Video Background Music Generation (VBMG) when its captions are used as prompts.

## Method Summary
MusiScene fine-tunes MU-LLaMA by freezing the LLaMA backbone and training only the music understanding adapter. The model is trained on a cross-modal dataset where video captions (SwinBERT), music captions (MU-LLaMA), and fused captions (Mixtral-generated) are combined to create ground truth for MSI. During inference, MusiScene generates scene imagination captions from audio alone, which can then be used as prompts for MusicGen to create contextually appropriate background music for videos.

## Key Results
- MusiScene achieves BLEU 0.421, METEOR 0.403, and BERT-Score 0.901 on MSI task, outperforming MU-LLaMA baseline (BLEU 0.190, METEOR 0.207, BERT-Score 0.863)
- For VBMG, MusiScene-generated captions achieve subjective score of 74.2 versus 73.5 for music captions and 61.4 for video captions
- Objective metrics show MusiScene captions produce coherent music with Fréchet Audio Distance of 5.78 and KL Divergence of 2.10

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Grounding via Synthetic Caption Fusion
Fine-tuning on LLM-fused video-music captions enables scene imagination from audio alone. The pipeline generates ground-truth MSI captions by prompting Mixtral to answer "What type of scene is this music suitable for?" given paired SwinBERT video captions and MU-LLaMA music captions. This teaches the model to map audio features to scene semantics without requiring video input at inference.

### Mechanism 2: Adapter-Only Fine-Tuning Preserves Musical Knowledge
Freezing LLaMA backbone while training only the music understanding adapter allows scene imagination without catastrophic forgetting of musical understanding. The MERT encoder extracts audio features; the adapter (Conv1D + dense layers) projects these into the LLaMA embedding space. By freezing LLaMA weights, the model retains pre-trained language and musical knowledge while the adapter learns to route audio features toward scene-related token generation.

### Mechanism 3: MSI Captions as Compressed Scene Representations for VBMG
Scene imagination captions capture sufficient context for music generation, outperforming raw video or music captions as text-to-music prompts. MSI captions encode the "atmosphere, ambiance, and settings" evoked by music in natural language. When fed to MusicGen, these captions guide generation toward contextually appropriate audio without requiring the model to interpret video frames directly.

## Foundational Learning

- **Audio Feature Extraction with MERT**
  - Why needed here: The model relies on MERT to convert raw audio into meaningful embeddings before the adapter processes them. Understanding self-supervised audio representations is essential for debugging feature quality.
  - Quick check question: Can you explain how MERT differs from raw spectrogram inputs in terms of what musical attributes it captures?

- **Parameter-Efficient Fine-Tuning (Adapter Training)**
  - Why needed here: Only the adapter is trained while LLaMA remains frozen. Understanding why this prevents catastrophic forgetting and what capacity limitations exist is critical for architectural decisions.
  - Quick check question: What happens if the adapter dimension is too small to represent the audio-to-scene mapping?

- **Cross-Modal Evaluation Metrics (BLEU, METEOR, BERT-Score for Caption Quality; FAD, KL Divergence for Audio)**
  - Why needed here: The paper uses NLP metrics for caption evaluation and audio metrics for downstream music generation. Knowing what each metric captures (and misses) is essential for interpreting results.
  - Quick check question: Why might a high BLEU score not guarantee that generated captions are semantically meaningful for scene imagination?

## Architecture Onboarding

- **Component map**: Raw audio waveform -> MERT Encoder (frozen) -> Audio embeddings -> Adapter (trainable) -> Projected features -> LLaMA-7B-chat (frozen) -> Generates MSI caption -> MusicGen -> Generated background music

- **Critical path**:
  1. Verify MERT encoder outputs correct embedding dimensions for your audio inputs
  2. Ensure adapter projection matches LLaMA's expected input embedding size
  3. Validate prompt formatting matches training-time MSI prompts (see Table 1: "What type of scene the music is suitable for?")

- **Design tradeoffs**:
  - Adapter-only training: Faster, less compute, preserves base knowledge—but may limit expressivity for complex scene reasoning
  - Dataset size (3,371 pairs): Sufficient for proof-of-concept but may not cover diverse genres/scene types; scaling requires more cross-modal annotation effort
  - Reliance on Mixtral for ground truth: Efficient labeling but inherits any LLM hallucination biases

- **Failure signatures**:
  - MSI captions revert to generic music descriptions (e.g., "upbeat and energetic") -> Adapter may not have learned scene mapping; check training loss convergence
  - Generated music lacks coherence with video despite high MSI scores -> May indicate evaluation metric misalignment or MusicGen distribution shift
  - Low BLEU/METEOR scores on held-out test set -> Possible overfitting to training fusion captions or insufficient adapter capacity

- **First 3 experiments**:
  1. **Baseline replication**: Train MusiScene on VACAD subset and verify BLEU/METEOR scores approximate reported values (0.421/0.403) to validate pipeline correctness.
  2. **Ablation on adapter capacity**: Vary adapter hidden dimension (e.g., 256, 512, 1024) and measure impact on scene imagination quality and training time.
  3. **Cross-dataset generalization**: Evaluate MusiScene on out-of-distribution music (e.g., genres not well-represented in AudioSet 'Music' class) to assess robustness of scene imagination beyond training distribution.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text. However, several open questions emerge from the limitations and implications of the work:

- To what extent does the reliance on synthetic, LLM-generated ground truth (Mixtral) limit the model's ability to capture nuanced human emotional connections between music and visual scenes?
- How can objective evaluation metrics be refined to correlate more strongly with human subjective preference in cross-modal music generation tasks?
- Can the "scene imagination" capabilities of MusiScene be effectively transferred to improve performance in music information retrieval (MIR) tasks, such as emotion-based tagging or recommendation?

## Limitations
- Dataset scale: The 3,371-pair dataset may not capture sufficient diversity in music genres and scene types for robust generalization
- Synthetic ground truth reliance: MSI captions are generated by Mixtral rather than human-annotated, introducing potential LLM hallucination biases
- Adapter capacity constraints: Freezing the LLaMA backbone while only training the adapter may limit the model's ability to perform complex scene reasoning

## Confidence
- **High confidence**: BLEU/METEOR/BERT-Score improvements over MU-LLaMA baseline for scene imagination task
- **Medium confidence**: Effectiveness of adapter-only fine-tuning for preserving musical knowledge while learning scene mapping
- **Low confidence**: Real-world applicability of MSI captions for VBMG without domain-specific MusicGen fine-tuning

## Next Checks
1. **Cross-dataset generalization test**: Evaluate MusiScene on music from genres underrepresented in AudioSet (e.g., classical, traditional folk) to assess scene imagination robustness beyond training distribution.
2. **Human evaluation of MSI quality**: Conduct a human study comparing Mixtral-generated GT captions against human-written scene descriptions for the same music clips to quantify hallucination rates.
3. **Adapter capacity ablation study**: Systematically vary adapter hidden dimensions (256→512→1024) and measure impact on both scene imagination quality and downstream VBMG coherence to identify performance ceilings.