---
ver: rpa2
title: 'kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator
  Learning'
arxiv_id: '2512.21409'
source_url: https://arxiv.org/abs/2512.21409
tags:
- learning
- operator
- systems
- kostic
- dynamical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "kooplearn is a Python library implementing algorithms for learning\
  \ evolution operators in dynamical systems, including Koopman/transfer operators\
  \ and continuous-time infinitesimal generators. The library provides state-of-the-art\
  \ estimators for both linear and kernel-based models, with randomized and Nystr\xF6\
  m-based variants for scalability."
---

# kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning

## Quick Facts
- arXiv ID: 2512.21409
- Source URL: https://arxiv.org/abs/2512.21409
- Reference count: 6
- Primary result: A Python library implementing algorithms for learning evolution operators (Koopman/transfer operators and infinitesimal generators) with state-of-the-art estimators, including kernel-based reduced-rank regression that provably outperforms traditional methods in spectral approximation.

## Executive Summary
kooplearn is a Python library implementing algorithms for learning evolution operators in dynamical systems, including Koopman/transfer operators and continuous-time infinitesimal generators. The library provides state-of-the-art estimators for both linear and kernel-based models, with randomized and Nyström-based variants for scalability. It supports deep-learning approaches using encoder-only and encoder-decoder architectures with theoretically-grounded loss functions. A key innovation is the kernel-based Reduced Rank Regression estimator, which provably outperforms traditional methods in spectral approximation. The library also implements methods for learning infinitesimal generators of diffusion processes, enabling analysis from static data. Benchmark datasets with known spectral decompositions are included for reproducibility.

## Method Summary
kooplearn implements multiple families of evolution operator estimators including Ridge regression, kernel Ridge regression, and kernel-based reduced-rank regression. The library provides randomized and Nyström-based kernel variants for scalability, deep learning approaches using encoder-decoder and encoder-only architectures with VAMP and spectral contrastive losses, and methods for learning infinitesimal generators from static equilibrium data. The estimators operate on trajectory data (x_t, x_{t+1}) or static samples from equilibrium distributions. Key innovations include the reduced-rank regression estimator for kernel methods and the integration of generator learning for diffusion processes.

## Key Results
- Kernel-based Reduced Rank Regression provides more accurate spectral approximation than traditional kernel DMD methods
- Randomized and Nyström-based kernel estimators reduce fit time by orders of magnitude while maintaining approximation quality
- Infinitesimal generator learning enables dynamical modeling from static equilibrium data without lag-time observations
- Experiments demonstrate improved spectral accuracy compared to kernel DMD and significant computational speedups for large datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernel-based Reduced Rank Regression provides more accurate spectral approximation than traditional kernel DMD methods.
- Mechanism: The estimator constrains the learned operator to a reduced-rank form, which aligns with the finite-dimensional subspace spanned by the representation φ. This rank constraint acts as a regularizer that filters spurious spectral components, yielding eigenfunction approximations that better capture the true transfer operator structure.
- Core assumption: The underlying dynamics admit a good finite-dimensional approximation in the chosen kernel-induced RKHS.
- Evidence anchors: [abstract] "A key innovation is the kernel-based Reduced Rank Regression estimator, which provably outperforms traditional methods in spectral approximation." [Page 2] "This estimator provably outperforms traditional methods (Williams et al., 2015) in approximating the operator's spectrum (Kostic et al., 2023), as illustrated in Figure 2."

### Mechanism 2
- Claim: Randomized and Nyström-based kernel estimators reduce fit time by orders of magnitude while maintaining approximation quality.
- Mechanism: These methods approximate the full kernel matrix using a subset of landmark points (Nyström) or random projections, reducing the O(n³) exact solution to O(m²n) or better, where m ≪ n. This preserves the kernel's representational capacity while making computation tractable for large datasets.
- Core assumption: The kernel matrix has approximately low-rank structure, or the random projections sufficiently capture its column space.
- Evidence anchors: [abstract] "The library provides state-of-the-art estimators for both linear and kernel-based models, with randomized and Nyström-based variants for scalability." [Page 3, Figure 3] Demonstrates fit time reductions on 5000-observation Lorenz 63 dataset.

### Mechanism 3
- Claim: Infinitesimal generator learning enables dynamical modeling from static equilibrium data without lag-time observations.
- Mechanism: For continuous-time diffusion processes, the generator L relates to the evolution operator via E = exp(L). Since L's eigenfunctions are preserved under exponentiation, one can use knowledge of L (or its properties) to learn dynamical behavior without requiring lag-time data.
- Core assumption: The system satisfies time-homogeneous diffusion dynamics with accessible equilibrium samples; boundary conditions are known or learnable.
- Evidence anchors: [Page 3] "Since the exponential of an operator preserves its eigenfunctions, one can use knowledge of L (or its properties) to learn dynamical behavior without requiring lag-time data." [Page 3] References Kostic et al. (2024b) and Devergne et al. (2024) for kernel and neural implementations.

## Foundational Learning

- **Koopman/Transfer Operators**: Why needed: These are the fundamental mathematical objects kooplearn learns; understanding that they linearly evolve observables (functions of state) is essential for interpreting all library outputs. Quick check: Given a dynamical system x_{t+1} = F(x_t), can you explain why the Koopman operator K acting on functions f is linear even when F is nonlinear?

- **Spectral Decomposition of Operators**: Why needed: The eigenvalues and eigenfunctions of learned operators determine mode frequencies, growth/decay rates, and coherent structures—all primary outputs of kooplearn's analysis pipeline. Quick check: If an evolution operator has eigenvalue λ = 0.8 with eigenfunction φ, what happens to the amplitude of mode φ(x_t) over 10 time steps?

- **Reproducing Kernel Hilbert Spaces (RKHS)**: Why needed: Kernel methods in kooplearn operate in RKHS; understanding that the kernel k(x,y) implicitly defines features φ(x) helps interpret model capacity and hyperparameter choices. Quick check: Why does the "kernel trick" allow computing inner products 〈φ(x), φ(y)〉 without explicitly constructing φ?

## Architecture Onboarding

- **Component map**: kooplearn.models (estimator classes) -> kooplearn.datasets (benchmark generators) -> kooplearn.nn (neural modules and losses)

- **Critical path**: 1. Select or generate dataset via `kooplearn.datasets` 2. Choose estimator class based on data scale and representation needs 3. Fit model using `.fit(X, Y)` where X = {x_t}, Y = {x_{t+1}} 4. Extract spectral quantities via `.eig()` or predict via `.predict()`

- **Design tradeoffs**:
  - **Ridge vs. KernelRidge**: Ridge assumes linear φ (fast, limited expressivity); KernelRidge uses implicit nonlinear features (slower, more expressive)
  - **Full kernel vs. Nyström/randomized**: Full is exact O(n³); Nyström is O(m²n) with approximation error—use randomized for n > 5000
  - **Encoder-only vs. Encoder-decoder**: Encoder-only (VAMP/spectral contrastive) optimizes representation directly for operator structure; encoder-decoder adds reconstruction loss, improving interpretability at potential cost of spectral fidelity

- **Failure signatures**:
  - Eigenvalues with |λ| > 1 for stable systems: Often indicates overfitting or insufficient regularization; increase Tikhonov penalty
  - Spurious high-frequency modes: May indicate spectral pollution; try reduced-rank estimator or increase rank constraint
  - Slow fit times on moderate data: Kernel matrix operations dominating; switch to Nyström variant
  - Poor predictions on out-of-sample states: Representation φ not generalizing; consider neural encoder or different kernel

- **First 3 experiments**:
  1. Reproduce Figure 2 locally: Fit both kernel DMD and reduced-rank estimator on the Noisy Logistic Map dataset; compare eigenfunction MSE to ground truth.
  2. Benchmark scalability: On Lorenz-63 with n ∈ {1000, 5000, 10000}, measure fit time for KernelRidge vs. Nyström variant; plot scaling curve.
  3. Static-data generator learning: Use infinitesimal generator estimator on equilibrium samples from a quadruple-well Langevin dynamics; compare recovered eigenvalues to time-series-based approach.

## Open Questions the Paper Calls Out

- **Theoretical conditions for static-to-dynamic generalization**: Under what conditions can infinitesimal generators learned from static equilibrium data reliably generalize to predict non-equilibrium dynamical behavior? The paper states this approach improves sample complexity but doesn't specify the bounds or conditions under which static-to-dynamic inference remains valid.

- **Estimator selection criteria**: What principled criteria should guide practitioners in selecting between linear, kernel-based, and deep learning estimators for a given dynamical system? The library implements all three families but provides no comparative analysis, decision framework, or theoretical guidance on estimator selection.

- **Accuracy-speed tradeoffs in randomized methods**: What are the fundamental accuracy-speed tradeoffs introduced by randomized and Nyström-based approximations in spectral decomposition tasks? While the paper claims "significant computational speedups," it provides no systematic analysis of how spectral accuracy degrades as a function of approximation rank.

## Limitations
- Hyperparameter dependence: The claimed spectral accuracy gains and speedups may strongly depend on kernel bandwidth, rank selection, and regularization strength, with no sensitivity analyses provided.
- Spectral pollution and rank constraints: Reduced-rank regression may truncate physically meaningful modes if the true operator's spectrum is not well-approximated by a low-rank model.
- Static-data generator learning: This innovative method lacks external validation and its robustness to noise, non-equilibrium distributions, or time-inhomogeneous dynamics is unknown.

## Confidence
- **High confidence**: Library's compatibility with scikit-learn and modular architecture (Ridge/KernelRidge models, neural modules, benchmark datasets)
- **Medium confidence**: Improved spectral accuracy of reduced-rank regression and scalability benefits of Nyström/randomized kernels, supported by internal benchmarks
- **Low confidence**: Infinitesimal generator learning from static data is based on theory but lacks independent benchmarks or error analyses

## Next Checks
1. Reproduce spectral accuracy claims: Fit both kernel DMD and reduced-rank estimators on Noisy Logistic Map or Overdamped Langevin datasets. Compare eigenvalue and eigenfunction errors to ground truth and record fit times.
2. Test scalability empirically: On Lorenz-63, measure and compare fit times for KernelRidge vs. Nyström/randomized variants for n ∈ {1000, 5000, 10000}, plotting scaling curves and verifying reported speedups.
3. Validate generator learning from static data: Use equilibrium samples from a quadruple-well Langevin system. Compare the spectrum recovered by the infinitesimal generator estimator to that obtained from a standard time-series approach, assessing both accuracy and robustness.