---
ver: rpa2
title: Audio-Guided Visual Perception for Audio-Visual Navigation
arxiv_id: '2510.11760'
source_url: https://arxiv.org/abs/2510.11760
tags:
- sound
- visual
- navigation
- agvp
- audio-visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGVP transforms sound from policy-memorized acoustic fingerprints
  into spatial guidance for audio-visual navigation. It uses audio context to guide
  visual attention, highlighting sound-source-related regions at the feature level
  before policy decisions.
---

# Audio-Guided Visual Perception for Audio-Visual Navigation

## Quick Facts
- arXiv ID: 2510.11760
- Source URL: https://arxiv.org/abs/2510.11760
- Authors: Yi Wang; Yinfeng Yu; Fuchun Sun; Liejun Wang; Wendong Zheng
- Reference count: 25
- Primary result: AGVP achieves 66.5% success rate for unheard sounds with 34.8% SPL improvement over AV-WaN on Replica dataset

## Executive Summary
AGVP transforms sound from policy-memorized acoustic fingerprints into spatial guidance for audio-visual navigation. It uses audio context to guide visual attention, highlighting sound-source-related regions at the feature level before policy decisions. This explicit cross-modal alignment reduces dependency on specific acoustic fingerprints and improves cross-scenario generalization. On Replica dataset, AGVP achieves 66.5% success rate for unheard sounds (34.8% improvement in SPL over AV-WaN), while reducing redundant exploration compared to baseline methods.

## Method Summary
AGVP introduces an Audio-Guided Visual Perception (AGVP) framework for audio-visual navigation. The method extracts audio context via self-attention, then uses this context as queries to guide visual feature attention through cross-modal attention (GA). This produces audio-guided visual representations where sound-relevant regions are amplified before policy decisions. The approach is trained end-to-end using PPO and evaluated on Replica and Matterport3D datasets with heard and unheard sound sources.

## Key Results
- Achieves 66.5% success rate for unheard sounds on Replica dataset
- Shows 34.8% improvement in SPL over AV-WaN for unheard sounds
- Reduces redundant exploration with paths closer to shortest route while significantly reducing backtracking and wandering

## Why This Works (Mechanism)

### Mechanism 1
Using audio context as queries to guide visual feature attention enables sound-source localization before policy decisions. Audio self-attention extracts global auditory context which serves as the Query in cross-modal attention over visual features, producing audio-guided visual representation where sound-relevant regions are amplified via softmax attention mechanism.

Core assumption: Audio contains sufficient spatial/directional cues to identify relevant visual regions when the sound source is occluded or distant.

Break condition: If audio provides no spatial information (e.g., mono audio without binaural cues), the Query will lack directional semantics and guidance degrades to noise.

### Mechanism 2
Explicit feature-level audio-visual alignment reduces dependency on memorized acoustic fingerprints. Traditional methods concatenate audio-visual features at the policy level, causing the network to learn spurious "acoustic fingerprint-scenario" correlations. AGVP forces alignment at the perceptual level via GA before policy processing, making the model learn generalizable sound-to-visual-region mappings rather than scenario-specific memorization.

Core assumption: Cross-modal alignment at feature level transfers better to unheard sounds than policy-level fusion.

Break condition: If training sound sources are too homogeneous (insufficient diversity), even feature-level alignment may overfit to acoustic characteristics present in training set.

### Mechanism 3
"Sound-first, vision-follows" processing reduces redundant exploration by pre-activating goal-relevant visual regions. By computing GA before policy reasoning, the visual feature map is recalibrated to highlight source-relevant regions. The policy receives pre-filtered visual information, reducing the need for random exploration to discover relevant areas.

Core assumption: Sound reliably indicates target location direction even when the target is visually occluded.

Break condition: In environments with significant acoustic occlusion (continuous walls, thick obstacles), sound direction may be misleading, causing incorrect visual region activation.

## Foundational Learning

- **Cross-Modal Attention (Query-Key-Value):** The core GA module uses audio as Query and vision as Key/Value. Understanding QKV attention is essential to grasp how audio "queries" visual features. *Quick check:* Can you explain why audio features serve as Query rather than Key in the GA module?

- **Binaural Audio Spectrograms:** The input audio is binaural spectrograms (two channels), which encode interaural time/level differences for spatial localization. *Quick check:* How does binaural audio differ from mono audio in terms of spatial information content?

- **PPO (Proximal Policy Optimization):** AGVP uses PPO for policy optimization after temporal modeling with GRU. *Quick check:* What is the purpose of the "proximal" constraint in PPO?

## Architecture Onboarding

- **Component map:** RGB/Depth → Visual Encoder → Self-Attention (SA) → [Key/Value] ← [Query from Audio SA] ← Audio Spectrogram → Audio Encoder → Self-Attention (SA) → Guided-Attention (GA) → GRU (temporal modeling) → PPO Actor-Critic → Action

- **Critical path:** Audio context extraction (SA) → Cross-modal attention (GA) → Temporal fusion (GRU) → Policy (PPO). If GA is removed (ablation), SPL drops from 80.2% to 43.1% on Replica.

- **Design tradeoffs:**
  - Depth vs. RGB input: Depth provides better SPL (80.2% vs. 70.3% on Replica heard sounds)
  - SA overhead: Adding self-attention to both modalities increases computation but improves SNA from 49.0% to 51.3%
  - Assumption: GA order (audio→vision) is fixed; reverse direction not explored

- **Failure signatures:**
  - Low SPL with high SR: Agent reaches goal but takes inefficient paths → check GA alignment quality
  - Good heard-sound, poor unheard-sound performance → model may still be memorizing fingerprints; increase training sound diversity
  - Failure in occluded environments (e.g., apartment4 scene baseline failure) → verify audio spatial cues are preserved through SA

- **First 3 experiments:**
  1. Ablate GA module: Replace GA with simple concatenation; expect SPL drop of ~37 percentage points (per Table III w/o GA results)
  2. Visualize attention maps: Extract GA attention weights and overlay on RGB frames to verify sound-source regions are highlighted; qualitative validation of alignment
  3. Cross-dataset transfer: Train on Replica, test on Matterport3D unheard sounds; expect SR drop (66.5%→60.8% per Table I) to assess environment generalization gap

## Open Questions the Paper Calls Out

- **Multi-source and moving sound sources:** Can the AGVP framework be effectively extended to handle navigation in multi-source and moving sound source scenarios? (Basis: Conclusion explicitly states this as future work)

- **Geometric acoustic modeling:** Does incorporating geometric acoustic modeling improve navigation efficiency in environments with complex occlusion? (Basis: Conclusion identifies this as specific direction for future work)

- **Feature Self-Attention comparison:** How does AGVP compare to the Feature Self-Attention (FSA) fusion layer when applied to static navigation tasks? (Basis: Related Work section notes planned comparative validation)

- **Spatial memory integration:** Can integrating explicit spatial memory mechanisms further reduce redundant exploration in large-scale environments? (Basis: Conclusion proposes integrating "enhanced spatial memory")

## Limitations

- Encoder architectures are underspecified (assumed ResNet-18/50 for vision, standard audio CNN from SoundSpaces)
- PPO hyperparameters not provided (learning rate, batch size, rollout length)
- Transformer configuration details missing (number of heads, FFN dimension)

## Confidence

- **High confidence** in the core GA mechanism (audio→visual attention) and its role in cross-modal alignment
- **Medium confidence** in the claimed memorization reduction, as evidence is indirect
- **Low confidence** in exact architectural details needed for faithful reproduction

## Next Checks

1. Ablate GA module and measure SPL drop; expect ~37 percentage point decrease per Table III
2. Visualize GA attention weights overlaid on RGB frames to verify sound-relevant regions are highlighted
3. Train on Replica, test on Matterport3D unheard sounds; expect SR drop from 66.5% to 60.8% to assess environment generalization