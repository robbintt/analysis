---
ver: rpa2
title: 'AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu'
arxiv_id: '2510.16573'
source_url: https://arxiv.org/abs/2510.16573
tags:
- urdu
- text
- detection
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting AI-generated text
  in Urdu, a low-resource language with limited detection tools. A novel framework
  was developed using a balanced dataset of 3,600 human-written and AI-generated texts
  from models like GPT-4o-mini, Gemini, and Kimi AI.
---

# AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu

## Quick Facts
- arXiv ID: 2510.16573
- Source URL: https://arxiv.org/abs/2510.16573
- Reference count: 20
- Primary result: mDeBERTa-v3-base achieved F1-score 91.29% and accuracy 91.26% on Urdu AI-generated text detection

## Executive Summary
This study addresses the challenge of detecting AI-generated text in Urdu, a low-resource language with limited detection tools. The authors developed a novel framework using a balanced dataset of 3,600 human-written and AI-generated texts from models like GPT-4o-mini, Gemini, and Kimi AI. By expanding the dataset via sliding window chunking to 7,667 samples and fine-tuning three multilingual transformer models, they achieved state-of-the-art performance. The mDeBERTa-v3-base model demonstrated the highest accuracy at 91.26%, contributing to combating misinformation and advancing NLP capabilities for underrepresented languages.

## Method Summary
The methodology involves creating the UHAT Dataset with 3,600 original samples (1,800 human, 1,800 AI-generated), expanding it to 7,667 chunks using sliding window techniques on texts exceeding 450 characters. Three multilingual transformers (mDeBERTa-v3-base, DistilBERT-multilingual, XLM-RoBERTa-base) were fine-tuned using AdamW optimizer with early stopping on validation loss. The preprocessing pipeline included Unicode normalization, diacritic removal, and special character cleanup. The 80/10/10 train/val/test split preserved label balance, and evaluation focused on F1-score, accuracy, and precision metrics.

## Key Results
- mDeBERTa-v3-base achieved the highest performance with F1-score 91.29% and accuracy 91.26%
- Statistical analysis revealed clear differences between human and AI texts in 5 out of 6 complexity measures
- Dataset expansion from 3,600 to 7,667 samples through sliding window chunking improved model exposure

## Why This Works (Mechanism)

### Mechanism 1
Multilingual transformers pre-trained on diverse corpora can transfer linguistic knowledge to Urdu AI-text detection with limited language-specific training data. Models like mDeBERTa-v3-base encode cross-lingual representations during pre-training; fine-tuning on 7,667 Urdu samples adapts these representations to the binary classification task without requiring massive Urdu corpora. Core assumption: structural and semantic patterns distinguishing AI from human text are partially language-agnostic and transferable across scripts. Evidence: mDeBERTa-v3-base achieved F1-score 91.29%; transfer learning effectiveness affirmed in Section V; neighboring papers (UrBLiMP, GHaLIB) demonstrate similar multilingual transfer effectiveness. Break condition: if future LLMs produce Urdu text with statistical properties indistinguishable from human writing.

### Mechanism 2
Human and AI-generated Urdu texts exhibit measurable linguistic differences that provide learnable classification signals. Statistical analysis identified separating features—humans show higher Type-Token Ratio (0.7091 vs 0.6741), greater sentence length variability (13.36 vs 4.06), and slightly longer average word length. Transformers learn to weight these patterns during fine-tuning. Core assumption: AI models used for generation produce text with consistent statistical fingerprints that differ from human writing in detectable ways. Evidence: statistical tests found clear differences in 5 out of 6 complexity measures; T-tests and Mann-Whitney U tests validated significance. Break condition: if AI-generated Urdu text converges toward human-like TTR and sentence variability.

### Mechanism 3
Sliding window chunking with overlap preserves semantic context while expanding limited training data for fixed-context transformers. Texts exceeding 450 characters are split into overlapping segments, expanding 3,600 original samples to 7,667 chunks. Overlap ensures boundary words appear in multiple contexts, reducing information loss at segment edges. Core assumption: chunking does not introduce artifacts that models learn instead of genuine human/AI distinctions. Evidence: chunking expanded dataset to 7,667 total chunks; 46% of texts required chunking with average chunk length 412.76 characters; appears novel to this work. Break condition: if chunk boundaries consistently fall in regions critical to human/AI distinction.

## Foundational Learning

- Concept: **Type-Token Ratio (TTR)**
  - Why needed here: TTR measures vocabulary richness (unique words / total words). The paper uses TTR as a key differentiator—human texts show 0.7091 vs AI's 0.6741. Understanding this metric is essential for interpreting the linguistic analysis and potential feature engineering.
  - Quick check question: Given a 100-word text with 65 unique words, what is the TTR? (Answer: 0.65)

- Concept: **Transfer Learning with Multilingual Transformers**
  - Why needed here: The entire approach relies on models pre-trained on 100+ languages being fine-tuned for Urdu. Without understanding that mDeBERTa, XLM-RoBERTa, and DistilBERT share cross-lingual representations, the results seem inexplicable given the small Urdu dataset.
  - Quick check question: Why might mDeBERTa-v3-base outperform DistilBERT-multilingual despite both being multilingual? (Hint: Consider architecture differences—mDeBERTa uses disentangled attention.)

- Concept: **Sliding Window with Overlap**
  - Why needed here: The paper expands its dataset 2.13× through chunking. Understanding how overlap preserves context at boundaries prevents misinterpretation of the sample count and helps assess potential data leakage risks.
  - Quick check question: If you chunk a 900-character text with window size 450 and stride 300, how many chunks do you produce? (Answer: 3 chunks at positions 0-450, 300-750, 600-900)

## Architecture Onboarding

- Component map: Raw Urdu Text (Human sources + AI generation) → Preprocessing (Unicode normalization, diacritic removal, whitespace cleanup) → Sliding Window Chunking (texts >450 chars → overlapping segments) → Train/Val/Test Split (80/10/10, stratified by label) → Tokenizer (model-specific: mDeBERTa / DistilBERT / XLM-RoBERTa) → Fine-tuning (AdamW optimizer, early stopping on validation loss) → Binary Classification (Human vs AI-generated)

- Critical path: Preprocessing quality → Chunking consistency → Tokenization alignment. Errors in Urdu Unicode normalization cascade through the pipeline, as transformers are sensitive to tokenization fidelity. The diacritic removal trade-off is language-critical.

- Design tradeoffs:
  - Diacritic removal: Reduces vocabulary complexity and improves generalization, but obscures phonetic nuances—acceptable for classification but problematic for generation tasks.
  - Chunk size (450 chars): Balances transformer input limits against context preservation; smaller chunks increase data volume but may fragment semantic units.
  - Model selection: mDeBERTa-v3-base offers highest accuracy (91.26%) but is larger than DistilBERT-multilingual; deployment constraints may favor the smaller model with 89.57% accuracy.

- Failure signatures:
  - Overfitting to training AI models: Detector may fail on text from LLMs not in training set (only GPT-4o-mini, Gemini, Kimi AI were used).
  - Chunk boundary artifacts: If models learn chunking patterns rather than genuine signals, performance will degrade on unchunked inference texts.
  - Class imbalance in chunks: Human chunks (4,231) outnumber AI chunks (3,436)—mild imbalance but worth monitoring per-split.

- First 3 experiments:
  1. Baseline validation: Replicate the mDeBERTa-v3-base fine-tuning on the provided train/test split; confirm F1 ≈ 91.29% to validate pipeline correctness.
  2. Cross-model generalization test: Evaluate the fine-tuned detector on Urdu text from an LLM not in training (e.g., Claude, Llama). Expect performance drop—quantify the gap.
  3. Ablation on chunking: Train on original 3,600 samples (no chunking) vs. chunked 7,667 samples. Isolate whether data expansion or chunking-specific patterns drive performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
Can the fine-tuned mDeBERTa model effectively detect text generated by state-of-the-art LLMs not included in the training set (e.g., GPT-4 or Claude)? Basis: The authors highlight the "moving target" challenge, noting detectors trained on earlier models often fail against newer ones, but only evaluate against GPT-4o-mini, Gemini, and Kimi AI. Unresolved because the study does not test generalization capabilities against more powerful or architecturally distinct models. Evidence needed: Performance metrics (F1-score/Accuracy) from testing on a dataset generated exclusively by LLMs released after the training data was compiled.

### Open Question 2
Does the removal of diacritics (Harakat) in the preprocessing stage degrade the model's ability to detect AI-generated content in formal or literary Urdu contexts? Basis: The paper states diacritic removal was a "practical trade-off to simplify vocabulary" but acknowledges it may "obscure phonetic nuances." Unresolved because it's unclear if discarded phonetic information contains critical stylistic markers that could improve detection accuracy, particularly for high-literary texts. Evidence needed: Comparative ablation study evaluating model performance on a test set with preserved diacritics versus the current normalized version.

### Open Question 3
Does training on sliding window chunks create a bias that reduces detection accuracy on full-length, unsegmented documents? Basis: The methodology expands the dataset via chunking to handle transformer input limits, assuming that learning patterns from segments translates to whole-document detection. Unresolved because evaluation is performed on the chunked test set; the paper does not verify if the model maintains high accuracy when processing complete, long-form articles without segmentation. Evidence needed: Evaluation of the model's classification accuracy on a dataset of full-length documents that exceed the standard transformer context window.

## Limitations

- Dataset size remains relatively small (3,600 original samples) despite chunking expansion, limiting robust generalization
- Limited evaluation scope with only three AI models (GPT-4o-mini, Gemini, Kimi AI) may not represent broader LLM landscape
- Urdu script complexity and preprocessing choices may introduce tokenization inconsistencies across different pipelines

## Confidence

- High Confidence: The mDeBERTa-v3-base model achieving F1=91.29% and accuracy=91.26% on the test set (direct empirical result)
- Medium Confidence: Generalizability to other AI models and real-world deployment scenarios (limited evaluation scope, potential "moving target" problem)
- Medium Confidence: Assumption that sliding window chunking with overlap preserves semantic context without introducing artifacts (not empirically validated through ablation studies)

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate the fine-tuned mDeBERTa-v3-base detector on Urdu text generated by LLMs not included in the training set (e.g., Claude, Llama, or local models). Measure performance degradation to quantify the detector's robustness to model variation.

2. **Chunking Ablation Study**: Train and evaluate two versions—one on the original 3,600 samples without chunking, and one on the chunked 7,667 samples. Compare performance metrics to isolate whether data expansion or chunking-specific patterns drive the reported results.

3. **Temporal Robustness Test**: Generate a new set of Urdu AI text using the same models (GPT-4o-mini, Gemini, Kimi AI) after 6-12 months and evaluate against the original detector. This tests whether the "moving target" problem manifests as performance degradation over time.