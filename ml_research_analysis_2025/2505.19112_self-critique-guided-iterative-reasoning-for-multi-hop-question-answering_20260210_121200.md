---
ver: rpa2
title: Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering
arxiv_id: '2505.19112'
source_url: https://arxiv.org/abs/2505.19112
tags:
- reasoning
- retrieval
- question
- iterative
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-Critique Guided Iterative Reasoning (SiGIR) improves multi-hop
  question answering by integrating self-critique feedback into the iterative reasoning
  process. The method enables the model to decompose complex questions, retrieve relevant
  documents, conduct reasoning, and self-evaluate intermediate steps.
---

# Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering

## Quick Facts
- arXiv ID: 2505.19112
- Source URL: https://arxiv.org/abs/2505.19112
- Reference count: 40
- Improves multi-hop QA by 8.6% F1 over previous state-of-the-art

## Executive Summary
Self-Critique Guided Iterative Reasoning (SiGIR) is a novel framework designed to enhance multi-hop question answering by integrating self-critique mechanisms into the iterative reasoning process. The approach enables models to decompose complex questions, retrieve relevant documents, conduct reasoning, and self-evaluate intermediate steps. Through iterative exploration and reward-guided selection, SiGIR refines reasoning trajectories to select the most promising paths. Experiments demonstrate that SiGIR achieves state-of-the-art performance on three multi-hop reasoning datasets, surpassing previous methods by a significant margin while requiring fewer iterations.

## Method Summary
SiGIR introduces a self-critique guided iterative reasoning framework for multi-hop question answering. The method combines iterative exploration with reward-guided selection to refine reasoning trajectories. It begins by decomposing complex questions into sub-questions, then retrieves relevant documents through a hybrid approach of semantic and keyword-based retrieval. The reasoning process involves an iterative loop where the model explores multiple reasoning paths, generates critiques of intermediate steps, and uses these critiques to guide future reasoning attempts. A reward mechanism evaluates the quality of reasoning trajectories, allowing the model to learn from successful paths and avoid dead ends. This approach enables more efficient and accurate multi-hop reasoning compared to traditional methods that rely solely on end-to-end training.

## Key Results
- SiGIR achieves 8.6% improvement in F1 score over previous state-of-the-art on multi-hop QA datasets
- The method requires fewer iterations than baseline approaches while maintaining superior performance
- Demonstrates significant improvements in retrieval accuracy, reasoning quality, and overall efficiency across HotpotQA, 2WikiMQA, and MuSiQue datasets

## Why This Works (Mechanism)
SiGIR works by addressing a fundamental challenge in multi-hop QA: the compounding effect of errors in multi-step reasoning. Traditional approaches often fail when early reasoning steps are incorrect, leading to cascading errors. SiGIR mitigates this by introducing self-critique at each reasoning step, allowing the model to detect and correct errors early in the process. The iterative exploration mechanism enables the model to consider multiple reasoning paths in parallel, while the reward-guided selection ensures that only the most promising trajectories are pursued further. This combination of self-evaluation and selective refinement creates a more robust reasoning process that can recover from initial mistakes and converge on correct answers even in complex scenarios requiring multiple reasoning steps across different documents.

## Foundational Learning
- **Iterative Reasoning**: The process of breaking down complex problems into sequential steps and refining answers through multiple passes. Why needed: Multi-hop questions require chaining multiple reasoning steps, and single-pass approaches often miss critical connections. Quick check: Can the model correctly answer questions requiring 3+ reasoning steps?
- **Self-Critique Mechanisms**: The ability for models to evaluate their own intermediate outputs and identify potential errors. Why needed: Without self-evaluation, models cannot detect when they've taken incorrect reasoning paths early enough to course-correct. Quick check: Does the model identify and correct its own reasoning errors during the process?
- **Reward-Guided Selection**: Using performance feedback to guide the selection of promising reasoning paths over less effective ones. Why needed: In iterative exploration, not all reasoning paths are equally valuable, and the model needs a way to focus computational resources on the most promising trajectories. Quick check: Does the model converge faster when using reward signals compared to random selection?

## Architecture Onboarding

Component map: Question Decomposition -> Document Retrieval -> Iterative Reasoning Loop -> Self-Critique -> Reward Evaluation -> Trajectory Selection

Critical path: Question -> Decomposition -> Retrieval -> Reasoning Iteration -> Critique -> Reward -> Answer Selection

Design tradeoffs: The framework balances exploration (trying multiple reasoning paths) against exploitation (focusing on the most promising paths based on rewards). This creates tension between computational efficiency and thoroughness. The self-critique mechanism adds computational overhead but provides error correction benefits. The reward function design is critical - too strict and the model may miss creative solutions, too lenient and it may waste resources on poor trajectories.

Failure signatures: Poor question decomposition leads to irrelevant document retrieval. Weak self-critique results in continued pursuit of incorrect reasoning paths. Inadequate reward signals cause the model to explore unpromising trajectories excessively. Over-aggressive trajectory selection may cause premature convergence on suboptimal answers.

First experiments to run:
1. Ablation study removing self-critique to measure its contribution to performance gains
2. Analysis of iteration count vs. performance to determine optimal computational budget
3. Error analysis on cases where SiGIR fails compared to baseline methods to identify remaining weaknesses

## Open Questions the Paper Calls Out
- How can the self-critique mechanism be further improved to handle more complex reasoning errors?
- What are the optimal strategies for balancing exploration and exploitation in the iterative reasoning process?
- How can the framework be extended to handle even more complex reasoning scenarios involving temporal reasoning or numerical inference?

## Limitations
- The computational cost increases with the number of reasoning paths explored, potentially limiting scalability
- Performance depends heavily on the quality of the reward function design, which may require domain-specific tuning
- The method may struggle with questions requiring commonsense reasoning that goes beyond document content

## Confidence
High: Performance improvements are statistically significant and consistent across multiple datasets
Medium: Computational efficiency claims need more thorough benchmarking across different hardware configurations
Medium: Generalization to other types of complex reasoning tasks beyond multi-hop QA remains to be fully validated

## Next Checks
1. Implement ablation studies to quantify the contribution of each component (self-critique, reward-guided selection, iterative exploration)
2. Test the framework on additional multi-hop QA datasets not included in the original evaluation
3. Benchmark computational efficiency and memory usage compared to baseline methods across different hardware configurations