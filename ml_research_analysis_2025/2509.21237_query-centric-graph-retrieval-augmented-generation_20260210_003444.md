---
ver: rpa2
title: Query-Centric Graph Retrieval Augmented Generation
arxiv_id: '2509.21237'
source_url: https://arxiv.org/abs/2509.21237
tags:
- query
- queries
- answer
- graph
- chunk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QCG-RAG, a query-centric graph-based retrieval-augmented
  generation framework that addresses the granularity dilemma in existing graph-based
  RAG methods. By constructing controllable-granularity query-centric graphs using
  Doc2Query and Doc2Query-- techniques, QCG-RAG enables more effective multi-hop reasoning
  compared to both fine-grained entity-level and coarse document-level approaches.
---

# Query-Centric Graph Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2509.21237
- Source URL: https://arxiv.org/abs/2509.21237
- Authors: Yaxiong Wu; Jianyuan Bo; Yongyue Zhang; Sheng Liang; Yong Liu
- Reference count: 37
- Key outcome: QCG-RAG outperforms chunk-based and graph-based RAG methods on multi-hop QA tasks by constructing controllable-granularity query-centric graphs using Doc2Query and Doc2Query-- techniques

## Executive Summary
QCG-RAG addresses the granularity dilemma in graph-based RAG by constructing query-centric graphs that balance fine-grained entity-level and coarse document-level approaches. The method uses Doc2Query to generate query-answer pairs that serve as intermediate nodes, enabling more effective multi-hop reasoning by improving semantic alignment between queries and evidence. Experiments on LiHuaWorld and MultiHop-RAG datasets demonstrate consistent state-of-the-art performance on multi-hop and long-context question answering tasks.

## Method Summary
QCG-RAG constructs controllable-granularity query-centric graphs using Doc2Query and Doc2Query-- techniques to address the granularity dilemma in existing graph-based RAG methods. The approach involves chunking documents into 1200-token segments with 100-token overlap, generating 20 query-answer pairs per chunk using an LLM, and filtering these pairs by cosine similarity to retain the top 80%. A graph is built where nodes are the filtered QA pairs, connected via KNN relationships (k=2 or 3) and linked to their source chunks. During retrieval, the user query is embedded and used to retrieve top-similar QA nodes, which are then expanded to 1-hop neighbors to aggregate and rank relevant chunks for the LLM.

## Key Results
- QCG-RAG achieves state-of-the-art performance on LiHuaWorld and MultiHop-RAG datasets
- Outperforms both fine-grained entity-level and coarse document-level graph-based RAG approaches
- Demonstrates superior performance on multi-hop reasoning tasks compared to chunk-based RAG methods

## Why This Works (Mechanism)
The query-centric graph approach bridges the granularity gap by using generated queries as intermediate nodes that capture semantic relationships more precisely than either document-level or entity-level approaches. By filtering generated QA pairs based on their similarity to source chunks, the method ensures high-quality semantic anchors in the graph. The multi-hop retrieval mechanism leverages these query nodes to expand search context while maintaining semantic relevance, allowing the model to capture complex reasoning paths that would be missed by simpler approaches.

## Foundational Learning
- **Doc2Query technique**: LLM-generated question-answer pairs from text chunks; needed to create semantic nodes that bridge granularity gaps, check by verifying generated pairs capture chunk content
- **Query-centric graph construction**: Building graphs where queries serve as nodes rather than documents or entities; needed to enable semantic multi-hop reasoning, check by examining node connectivity patterns
- **Multi-hop retrieval with KNN expansion**: Using k-nearest neighbors to expand retrieval context while maintaining relevance; needed to balance breadth and precision in evidence gathering, check by measuring retrieval precision vs recall tradeoff
- **Cosine similarity thresholding**: Using shifted cosine similarity (cosine + 1) for node selection; needed to normalize similarity scores for threshold-based retrieval, check by analyzing threshold sensitivity
- **Graph connectivity optimization**: Ensuring sufficient edge density between QA nodes; needed to enable effective hop expansion, check by measuring average node degree
- **Chunk-based document preprocessing**: Splitting documents into 1200-token chunks with overlap; needed to create manageable semantic units while preserving context, check by verifying overlap consistency

## Architecture Onboarding

**Component Map:** Document Chunks -> Doc2Query Generation -> Similarity Filtering -> Graph Construction -> KNN Edge Creation -> Query Retrieval -> Hop Expansion -> Chunk Aggregation -> LLM

**Critical Path:** User Query → Similarity Search → KNN Expansion → Chunk Ranking → LLM Input

**Design Tradeoffs:** The method balances granularity by using query nodes instead of documents (better semantic alignment but higher computational cost) or entities (better precision but limited context). The Doc2Query-- filtering step reduces noise but may discard potentially useful connections. KNN-based expansion provides controlled hop growth but requires careful parameter tuning.

**Failure Signatures:** 
- Poor retrieval quality when generated queries are generic or hallucinated
- Isolated nodes preventing effective hop expansion due to insufficient KNN connections
- Semantic drift when filtered QA pairs lose connection to source chunk meaning
- Performance degradation with sparse graph connectivity or inappropriate similarity thresholds

**3 First Experiments:**
1. Verify Doc2Query generation produces factually consistent QA pairs by checking answers against source chunks
2. Test graph connectivity by measuring average node degree and ensuring sufficient KNN edges
3. Validate retrieval threshold sensitivity by testing different similarity threshold values

## Open Questions the Paper Calls Out

**Open Question 1:** Can QCG-RAG maintain high performance when applied to web-scale corpora without prohibitive computational costs?
- Basis in paper: [explicit] The authors state that "constructing and maintaining large-scale QCGs remains computationally costly when applied to web-scale corpora."
- Why unresolved: The experiments were conducted on datasets with relatively small document counts (LiHuaWorld: 442 docs; MultiHop-RAG: 609 docs), leaving the efficiency and latency of the query generation and graph construction pipeline untested at massive scales.
- What evidence would resolve it: Benchmarking results on datasets containing millions of documents, reporting indexing time, memory usage, and retrieval latency.

**Open Question 2:** How does QCG-RAG perform in multi-lingual contexts or specialized domains such as legal or biomedical texts?
- Basis in paper: [explicit] The authors note that "experiments are limited to English QA benchmarks; extending QCG-RAG to multi-lingual or domain-specific scenarios... requires further validation."
- Why unresolved: The current evaluation relies solely on general-domain English datasets, and the effectiveness of the Doc2Query expansion step may vary with different languages or technical terminologies.
- What evidence would resolve it: Evaluation metrics (Accuracy/LLM-as-a-Judge) on multi-lingual benchmarks (e.g., C-Eval) or domain-specific corpora (e.g., legal case reports).

**Open Question 3:** Would integrating advanced reasoning strategies like reinforcement learning or self-reflection improve the retrieval accuracy of QCG-RAG?
- Basis in paper: [explicit] The paper lists as a limitation that the retrieval mechanism "does not incorporate advanced reasoning strategies such as reinforcement learning or self-reflection, which may further enhance complex reasoning."
- Why unresolved: The current retrieval relies primarily on structural and semantic similarity (cosine similarity), which may be suboptimal for complex multi-hop logic that requires dynamic planning.
- What evidence would resolve it: A comparative study where an RL agent or self-reflection module guides the graph traversal, showing statistically significant improvements over the current KNN-based expansion.

## Limitations

- Computational cost of constructing and maintaining large-scale query-centric graphs for web-scale corpora
- Limited evaluation to English QA benchmarks without validation in multi-lingual or domain-specific contexts
- Retrieval mechanism lacks advanced reasoning strategies like reinforcement learning or self-reflection that could enhance complex reasoning

## Confidence

- **High confidence**: The core methodology of constructing query-centric graphs using generated QA pairs as nodes, with multi-hop retrieval through KNN connections, is clearly specified and represents a novel contribution to the RAG literature.
- **Medium confidence**: The experimental results showing state-of-the-art performance on LiHuaWorld and MultiHop-RAG datasets are reproducible given the provided dataset sizes and metrics, though exact replication depends on resolving implementation ambiguities.
- **Low confidence**: The specific implementation details around the Doc2Query filtering mechanism and threshold calculations cannot be fully verified without clarification from the authors, potentially affecting the exact performance numbers.

## Next Checks

1. **Prompt Output Validation:** Run the provided Doc2Query prompt on sample chunks and verify whether the LLM output includes a relevance score field as requested, or if the JSON format matches what is actually used in the method.

2. **Threshold Calculation Verification:** Test the retrieval mechanism using both the reported shifted similarity calculation (cosine + 1) and standard cosine similarity to confirm if performance differs significantly.

3. **Graph Connectivity Analysis:** Measure the average degree and clustering coefficient of the constructed query-centric graphs to ensure sufficient connectivity for effective multi-hop retrieval.