---
ver: rpa2
title: 'SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts'
arxiv_id: '2505.21828'
source_url: https://arxiv.org/abs/2505.21828
tags:
- safety
- prompt
- fact
- prompts
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAGE-Eval, a benchmark testing whether large
  language models (LLMs) systematically generalize safety facts to novel, real-world
  scenarios. The authors manually curated 104 safety facts across seven domains and
  generated 10,428 test prompts by systematically augmenting user queries to probe
  implicit safety risks.
---

# SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts

## Quick Facts
- **arXiv ID:** 2505.21828
- **Source URL:** https://arxiv.org/abs/2505.21828
- **Authors:** Chen Yueh-Han; Guy Davidson; Brenden M. Lake
- **Reference count:** 40
- **Primary result:** Best model (Claude-3.7-sonnet) achieves only 57.69% safety score on systematic safety fact generalization test

## Executive Summary
SAGE-Eval is a benchmark testing whether large language models systematically generalize safety knowledge to novel, real-world scenarios. The authors curated 104 safety facts across seven domains and generated 10,428 test prompts by systematically augmenting user queries to probe implicit safety risks. Evaluation on 16 frontier models reveals that even top-performing models achieve only moderate safety scores, with strong models like o1 and o3-mini scoring below 45%. Performance correlates weakly with model capability and training compute, indicating that scaling alone does not solve safety generalization. The benchmark reveals a fundamental gap between knowledge possession and systematic application of safety facts.

## Method Summary
The authors manually curated 104 safety facts from reputable sources (CDC, FDA, AAP, etc.) across seven domains. They generated 10,428 test prompts through systematic augmentation of 1,724 human-verified base questions, including variations in tone, spelling, and context length. Models are evaluated using an LLM-as-a-judge pipeline where o3-mini screens all responses, with flagged cases re-evaluated by Gemini-2.0-Flash and Gemini-1.5-Pro. The primary metric is Model-level Safety Score (fraction of facts where all variant prompts pass), with secondary metrics including Area Under Safety Curve and Fact-level Safety Score. Evaluation uses greedy decoding with a canary string to prevent contamination.

## Key Results
- Claude-3.7-sonnet achieves highest safety score of 57.69%, while o1 and o3-mini score only 41.35% and 34.62% respectively
- All models possess safety knowledge (100% pass at 5% threshold) but fail systematic application across diverse prompts
- Longer contexts (100 words) reduce safety performance by 9.3 percentage points due to "lost in the middle" effect
- Performance correlates weakly with model capability and training compute, suggesting scaling alone doesn't solve safety generalization
- Forecasting analysis shows safety scores decline predictably with prompt diversity following power-law distribution

## Why This Works (Mechanism)

### Mechanism 1: Safety Fact Knowledge vs. Systematic Application Gap
LLMs possess safety knowledge but fail to retrieve and apply it systematically when risks appear implicitly in novel contexts. Models achieve 100% at the 5% pass threshold (proving knowledge exists) but fail when requiring consistent application across diverse prompt variants, suggesting a retrieval-composition gap rather than a knowledge gap.

### Mechanism 2: Context Length and "Lost in the Middle" Dilution
Safety risks embedded within longer contexts receive reduced attention, lowering retrieval probability. 100-word contexts yield 0.803 fact-level safety score vs. 0.896 for instruction-only prompts, with critical safety signals competing with irrelevant context for model attention.

### Mechanism 3: RLHF Preference Trade-offs Partially Mitigate but Don't Eliminate Failures
Direct Preference Optimization improves safety generalization measurably but leaves substantial residual failures. OLMo-2-32B-DPO achieves 0.226 Area Under Safety Curve vs. 0.132 for SFT-only baseline—a 71% relative improvement, yet both models score zero on the strict model-level metric.

## Foundational Learning

- **Systematic Generalization**: The capacity to recombine known components (safety facts) into novel combinations (user queries). Why needed: The benchmark explicitly tests whether models apply known facts to structurally novel situations, not whether facts are memorized. Quick check: Can you explain why a model that correctly warns about grapes for dogs might fail to warn about grapes in a fruit salad recipe for dogs?

- **Compositional vs. Piecemeal Cognition**: Fodor & Pylyshyn's critique that neural networks lack algebraic capacity to recombine knowledge components. Why needed: The paper frames LLM safety failures as "piecemeal safety"—recognizing facts in isolation but failing compositional application. Quick check: How would a systematically generalizing system differ from a pattern-matching system when encountering "diced grapes in dog food"?

- **LLM-as-a-Judge Reliability**: Using frontier models to evaluate other models' outputs with structured criteria. Why needed: SAGE-Eval achieves 100% alignment between human labels and LLM-as-a-judge on Safety Naive questions, enabling scalable evaluation. Quick check: What biases might an LLM judge introduce when evaluating safety warnings from the same model family?

## Architecture Onboarding

- **Component map**: Safety Fact Database (104 facts) → Base Question Generator (GPT-4o with pattern templates) → Human Validation (144 annotators) → Augmentation Layer (typos, tone shifts) → Test Set (10,428 prompts) → Model Under Test → LLM-as-a-Judge (o3-mini + Gemini verification) → Safety Score Computation

- **Critical path**: 1) Ensure facts are uncontaminated using canary string 2) Run evaluation with model under test using greedy decoding 3) Pass (prompt, response, fact) to judge with criteria: warn, offer alternative, or refuse 4) Re-verify failures with two additional judge models

- **Design tradeoffs**: Strict metric (100% pass per fact) vs. Area Under Safety Curve (captures partial performance); Human-validated base questions vs. synthetic augmentation coverage; Bias-corrected power-law forecasting (accurate at n=100→1000) vs. raw predictions (systematically overestimate)

- **Failure signatures**: Model provides direct answer without warning on Safety Naive questions; Model refuses Safety OK questions (over-cautiousness); Safety score degrades predictably with prompt count following power-law

- **First 3 experiments**: 1) Baseline evaluation: Run SAGE-Eval on your model with all 10,428 prompts 2) Context-length ablation: Compare instruction-only vs. 100-word context prompts 3) Forecasting calibration: Train bias-corrected power-law model on 50% of facts, validate on remaining 50% at n=100, extrapolate to n=1,000

## Open Questions the Paper Calls Out

- **Open Question 1**: What system prompt designs or prompting strategies can improve systematic safety generalization without causing models to become "overly vigilant"? The tested reflection prompts showed inconsistent improvement across models and remained far from deployment-ready performance.

- **Open Question 2**: What factors beyond pre-training data frequency and RLHF explain the variance in safety generalization performance across facts and models? Neither Google search frequency nor WIMBD corpus frequency correlated with fact-level safety scores, and DPO only partially improved performance.

- **Open Question 3**: How can alignment objectives be modified to explicitly incentivize risk awareness as a distinct competency alongside helpfulness, honesty, and harmlessness? Models can satisfy HHH while failing to surface implicit risks in naive queries, indicating risk awareness is not captured by current objectives.

## Limitations

- **Knowledge Contamination Risk**: Despite canary string filtering, the 104 safety facts were sourced from web content that may have appeared in model pre-training corpora, introducing potential ceiling effects.

- **Single-Decoding Evaluation**: The study uses greedy decoding without exploring temperature or sampling variations, which may affect safety-critical responses in models with stochastic safety behaviors.

- **Context Length Generalization**: While the "lost in the middle" effect is documented, the study only tests 100-word contexts, leaving extrapolation to longer real-world contexts untested.

## Confidence

- **Model Capability vs. Safety Generalization Correlation**: High confidence - well-supported by systematic evaluation across 16 models
- **Systematic Generalization Failure**: High confidence - convincingly demonstrated through 100% vs. systematic failure contrast
- **RLHF Preference Transfer**: Low confidence - rests on single model comparison without ablation studies

## Next Checks

1. **Contamination Verification**: Extract model activations or attention patterns for safety-critical terms to verify whether models retrieve facts from memory or apply systematic reasoning.

2. **Decoding Strategy Sensitivity**: Re-run evaluations using multiple decoding temperatures (0.0, 0.7, 1.0) and nucleus sampling (p=0.9) to determine if safety responses are decoding-dependent.

3. **Cross-Domain Transfer Test**: Create novel safety scenarios that combine facts from different domains to test true compositional generalization beyond single-domain augmentation patterns.