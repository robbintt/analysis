---
ver: rpa2
title: BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025
arxiv_id: '2509.09725'
source_url: https://arxiv.org/abs/2509.09725
tags:
- biomedical
- entity
- linking
- train
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles multilingual biomedical nested entity linking\u2014\
  a task requiring mapping entity mentions in English and Russian biomedical text\
  \ to UMLS concept IDs, while handling overlapping mentions and cross-lingual ambiguities.\
  \ Existing models struggle with nesting and lack multilingual support."
---

# BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025

## Quick Facts
- arXiv ID: 2509.09725
- Source URL: https://arxiv.org/abs/2509.09725
- Reference count: 22
- Key outcome: BIBERT-Pipe achieved 0.6370 Acc@1 on English, 0.6497 on Russian, and 0.6370 bilingual BioNNE 2025 tracks, placing third in multilingual.

## Executive Summary
The paper addresses multilingual biomedical nested entity linking—mapping entity mentions in English and Russian biomedical text to UMLS concept IDs while handling overlapping mentions and cross-lingual ambiguities. Existing models struggle with nesting and lack multilingual support. The authors propose BIBERT-Pipe, a two-stage encoder-agnostic pipeline: a retrieval stage using SapBERT encoders with boundary cues ([Ms]/[Me] tokens), followed by a cross-encoder rank stage. The ranker uses either listwise or contrastive learning architectures, the latter yielding better accuracy. They also augment training with MedMentions (English) and MCN (Russian) data. On BioNNE 2025, their system achieved 0.6370 Acc@1 on the English track, 0.6497 on Russian, and 0.6370 bilingual, placing third in the multilingual track. The approach demonstrates that minimal architectural changes—boundary cues, contrastive ranking, and data augmentation—can significantly boost performance in multilingual nested entity linking.

## Method Summary
BIBERT-Pipe is a two-stage pipeline for biomedical nested entity linking. The retrieval stage uses multilingual SapBERT encoders enhanced with boundary tokens ([Ms]/[Me]) to generate candidate UMLS concept lists. The rank stage employs a cross-encoder that re-ranks candidates using either a listwise or contrastive learning architecture. The contrastive variant performs better but is computationally more expensive. Training leverages both English (MedMentions) and Russian (MCN) datasets for multilingual robustness.

## Key Results
- Achieved 0.6370 Acc@1 on English, 0.6497 on Russian, and 0.6370 bilingual BioNNE 2025 tracks.
- Contrastive learning ranker outperformed listwise, achieving 0.6604 vs 0.5918 on English dev.
- Boundary cues improved Russian accuracy by 6.60% but only 1.24% for English, attributed to richer morphology.

## Why This Works (Mechanism)
Boundary cues help the model disambiguate entity spans in morphologically rich languages like Russian. Contrastive learning enables better discrimination between similar UMLS concepts by comparing positive and negative pairs. Multilingual SapBERT encoders provide strong cross-lingual embeddings for both English and Russian biomedical text. Data augmentation with MedMentions and MCN expands training coverage across languages and entity types.

## Foundational Learning
- **UMLS Concept Linking**: Why needed - maps text spans to standardized biomedical concepts. Quick check - does the system output valid UMLS CUI codes?
- **Nested Entity Recognition**: Why needed - biomedical text often contains overlapping entity mentions. Quick check - can the model handle nested spans like "breast cancer" within "metastatic breast cancer"?
- **SapBERT Embeddings**: Why needed - provides pretrained biomedical representations. Quick check - are embeddings fine-tuned for entity linking or used off-the-shelf?
- **Contrastive Learning**: Why needed - improves ranking by learning to distinguish similar concepts. Quick check - is the margin between positive and negative pairs stable during training?
- **Boundary Tokenization ([Ms]/[Me])**: Why needed - clarifies entity span boundaries, especially in inflected languages. Quick check - does adding/removing these tokens significantly change accuracy?
- **Cross-lingual Transfer**: Why needed - enables performance on languages without large labeled datasets. Quick check - does Russian performance degrade without Russian-specific training data?

## Architecture Onboarding

**Component Map**
SapBERT Encoder -> Boundary Tokenization -> Candidate Retrieval -> Cross-Encoder Ranker -> Final UMLS Prediction

**Critical Path**
Retrieval stage must include gold concept in top-10 candidates for ranker to succeed. Retrieval accuracy caps theoretical maximum system accuracy.

**Design Tradeoffs**
Contrastive ranker: higher accuracy, k times more computation. Listwise ranker: faster, single-pass, lower accuracy. Boundary cues: significant boost for Russian, minimal for English.

**Failure Signatures**
- Retrieval fails to include gold concept → ranker cannot recover.
- Nested mentions not properly tokenized → boundary cues ineffective.
- Cross-lingual transfer insufficient → Russian accuracy lags English.

**3 First Experiments**
1. Remove boundary cues and measure impact on Russian vs English accuracy.
2. Replace contrastive ranker with listwise and compare runtime and accuracy.
3. Test retrieval-only pipeline to quantify upper bound of ranker performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the significant performance boost from boundary cues in Russian generalize to other morphologically rich or agglutinative languages, or is it specific to the syntactic structure of Russian?
- Basis in paper: [inferred] The ablation study (Table 5) reveals that boundary cues ([Ms]/[Me]) improved Russian accuracy by 6.60% but only 1.24% for English. The authors attribute this to the "richer morphology" of Russian, where inflectional endings may obscure entity spans.
- Why unresolved: The study is restricted to English and Russian. It remains unclear if this heuristic is universally beneficial for complex morphology (e.g., Finnish, Turkish) or if it merely compensates for specific tokenizer limitations in Cyrillic scripts.
- What evidence would resolve it: Applying the BIBERT-Pipe model to a multilingual dataset containing diverse morphological typologies would confirm if boundary cues serve as a universal solution for span delineation in non-analytic languages.

### Open Question 2
- Question: Can the accuracy-efficiency trade-off between Contrastive Learning (CL) and Listwise ranking be resolved without incurring the "k times" computational cost?
- Basis in paper: [explicit] The authors note in Section 4.2 that while the CL architecture yields better accuracy (0.6604 vs 0.5918 on English dev), it "inherently requires k times more computation during both training and inference" compared to the single-pass Listwise method.
- Why unresolved: The paper accepts the higher computational cost of CL as necessary for performance. It does not explore optimization techniques, such as knowledge distillation or hybrid cascades, that might retain CL accuracy with Listwise efficiency.
- What evidence would resolve it: Experiments distilling the CL ranker into a Listwise model, or utilizing efficient attention mechanisms to reduce the overhead of processing k candidates independently, would address this bottleneck.

### Open Question 3
- Question: How can the retrieval ceiling be lifted to prevent error propagation, given that the current best retriever fails to include the gold concept in the top-10 candidates for roughly 20-25% of instances?
- Basis in paper: [inferred] Table 1 shows the best retrieval Acc@10 is 0.8184 for English and 0.7500 for Bilingual tracks. Since the ranking stage (Eq. 2) is restricted to re-ordering the output of the retrieval stage, the system's theoretical maximum accuracy is strictly capped by these retrieval limits.
- Why unresolved: The paper focuses on "minimal architectural changes" to the ranking and encoding components but relies on off-the-shelf SapBERT for retrieval without addressing the false negatives in the candidate generation phase.
- What evidence would resolve it: An analysis of the "unreachable" instances (where the gold label is not in the top-10) and experiments with hybrid retrieval (e.g., combining BM25 with dense retrieval) would determine if the 0.63 accuracy ceiling is primarily a retrieval or a ranking failure.

## Limitations
- No clear comparison with previous BioNNE systems due to absence of leaderboard at submission.
- Cross-encoder ranker's dependency on labeled training data may limit scalability to additional languages.
- Two-stage pipeline may introduce error propagation from retrieval to ranking.
- Impact of boundary cue tokenization on nested mention extraction was not thoroughly evaluated.

## Confidence
- **High**: BIBERT-Pipe achieves competitive accuracy on BioNNE 2025 English and Russian tracks (0.6370 and 0.6497 Acc@1).
- **Medium**: The contrastive learning ranker outperforms the listwise approach, but comparative ablation studies are missing.
- **Medium**: Data augmentation with MedMentions and MCN improves model performance, though the contribution of each is not quantified.

## Next Checks
1. Conduct ablation studies comparing boundary cue tokenization, retrieval vs. end-to-end models, and contrastive vs. listwise ranking.
2. Test BIBERT-Pipe on additional biomedical NER datasets (e.g., NCBI, JNLPBA) to evaluate generalizability beyond BioNNE.
3. Perform runtime and memory efficiency benchmarking for both ranker architectures under nested entity scenarios.