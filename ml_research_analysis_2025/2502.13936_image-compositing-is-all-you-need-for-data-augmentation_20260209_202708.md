---
ver: rpa2
title: Image compositing is all you need for data augmentation
arxiv_id: '2502.13936'
source_url: https://arxiv.org/abs/2502.13936
tags:
- data
- image
- augmentation
- diffusion
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates various data augmentation techniques for
  object detection, focusing on aircraft detection with limited annotated data. The
  authors evaluate classical augmentation methods, image compositing, and advanced
  generative models (Stable Diffusion XL and ControlNet) using YOLOv8 on a custom
  dataset of commercial and military aircraft.
---

# Image compositing is all you need for data augmentation

## Quick Facts
- arXiv ID: 2502.13936
- Source URL: https://arxiv.org/abs/2502.13936
- Authors: Ang Jia Ning Shermaine; Michalis Lazarou; Tania Stathaki
- Reference count: 2
- Primary result: Image compositing achieved mAP@0.50 of 0.911, outperforming classical augmentation (0.821), SD (0.808), and SD+ControlNet (0.854)

## Executive Summary
This paper investigates data augmentation techniques for aircraft detection with limited annotated data. The authors evaluate classical augmentation, image compositing, and generative models (Stable Diffusion XL and ControlNet) using YOLOv8 on a custom dataset of commercial and military aircraft. Results demonstrate that image compositing achieves superior performance with mAP@0.50 of 0.911, precision of 0.904, and recall of 0.907, outperforming both baseline methods and advanced generative approaches. The study concludes that domain-aligned synthetic data creation through compositing is more effective than generative synthesis for object detection in constrained data scenarios.

## Method Summary
The study evaluates multiple augmentation strategies on a custom aircraft detection dataset using YOLOv8s with pretrained CSPDarknet53 backbone. Classical augmentation applies horizontal flips, Gaussian blur, and exposure adjustments. Image compositing extracts aircraft foregrounds and composites them onto sky backgrounds with 0-10° rotation and Gaussian boundary smoothing. Stable Diffusion XL generates synthetic aircraft using ControlNet with Canny edge conditioning to preserve spatial layout. All methods are trained with AdamW optimizer (lr=0.001667, batch_size=16, weight_decay=0.0005) and early stopping (patience=10) on 500 epochs. The dataset contains 240 training samples (218 commercial, 22 military) with semi-automated labeling via Grounding DINO followed by manual verification.

## Key Results
- Image compositing achieved highest performance: mAP@0.50 of 0.911, precision of 0.904, and recall of 0.907
- Classical augmentation improved baseline mAP from 0.654 to 0.821
- Stable Diffusion XL with ControlNet achieved mAP@0.50 of 0.854
- Vanilla Stable Diffusion generated images achieved mAP@0.50 of 0.808
- Baseline model without augmentation achieved mAP@0.50 of 0.654

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image compositing outperforms generative models by minimizing distribution shift between synthetic and real data
- Mechanism: Real aircraft foregrounds are extracted and composited onto domain-matched sky backgrounds, preserving authentic object appearance while varying context
- Core assumption: Target deployment domain has consistent background characteristics that can be captured and reused
- Evidence anchors: Abstract states compositing "better matches the target domain distribution"; section 5.3 attributes superior performance to reduced distribution shift compared to Stable Diffusion
- Break condition: If foreground objects have significant appearance variation not represented in source images

### Mechanism 2
- Claim: Canny edge conditioning with ControlNet preserves spatial layout for bounding box label reuse
- Mechanism: Edge maps extracted from training images constrain diffusion generation to produce aircraft at identical image coordinates
- Core assumption: Edge structure sufficiently constrains generation to produce spatially consistent outputs across diverse textures
- Evidence anchors: Section 3.2 explains label reuse through spatial consistency; SD+ControlNet achieved mAP@0.854, outperforming vanilla SD
- Break condition: If generated content deviates significantly from edge constraints

### Mechanism 3
- Claim: Gaussian filtering at composite boundaries reduces edge artifacts that could be learned as spurious detection features
- Mechanism: Foreground-background seams are smoothed via Gaussian convolution, creating gradual intensity transitions
- Core assumption: Visible seams would otherwise be exploited by the detector as discriminative features that don't generalize
- Evidence anchors: Section 3.3 describes boundary smoothing; image compositing achieved highest precision (0.904)
- Break condition: If smoothing is excessive, aircraft edges become indistinct

## Foundational Learning

- **Object Detection Metrics (mAP, Precision, Recall, IoU)**: Understanding IoU thresholds is essential to interpret mAP@0.50 differences from stricter metrics. Quick check: If a model predicts 10 bounding boxes for 5 ground-truth objects, and 4 predictions correctly overlap (IoU > 0.5), what are precision and recall?

- **Transfer Learning with YOLOv8**: Fine-tuning pretrained weights on small custom datasets requires understanding frozen vs. trainable layers. Quick check: Why might fine-tuning only detection heads (not backbone) be preferable with extremely limited training data?

- **Domain Shift in Synthetic Data**: The central finding attributes performance differences to distribution shift. Understanding covariate shift explains why compositing generalizes better. Quick check: If Stable Diffusion generates photorealistic aircraft but with unrealistic lighting for ground-based observation, what type of distribution shift occurs?

## Architecture Onboarding

- **Component map**: Raw images → Grounding DINO auto-labeling → Manual correction → Augmentation (classical / SD / SD+ControlNet / compositing) → YOLOv8 training → Evaluation

- **Critical path**: 1. Data curation (ground-based aircraft, not aerial) 2. Semi-automated labeling with verification 3. Augmentation strategy selection 4. Fine-tuning with early stopping 5. Evaluation on held-out test set

- **Design tradeoffs**: Classical augmentation: low cost, moderate gain (mAP 0.654 → 0.821); Stable Diffusion: high compute, label reuse requires edge conditioning (mAP 0.808); SD+ControlNet: higher fidelity, still shows distribution shift (mAP 0.854); Compositing: manual background curation required, highest performance (mAP 0.911)

- **Failure signatures**: Low recall with high precision: model overfitting to specific backgrounds; High recall with low precision: model detecting artifacts as objects; Early stopping at epoch 2: insufficient data diversity

- **First 3 experiments**: 1. Reproduce baseline: Train YOLOv8s on original dataset without augmentation (target mAP ~0.65) 2. Implement classical augmentation: horizontal flip, Gaussian blur, exposure adjustment (target mAP ~0.82) 3. Build image compositing pipeline: extract aircraft masks, collect sky backgrounds, composite with 0-10° rotation and Gaussian blur (target mAP >0.90)

## Open Questions the Paper Calls Out
- Can image compositing maintain its performance advantage when applied to larger datasets and more complex domains like medical imaging or autonomous driving?
- Does the integration of semi-supervised learning methods with image compositing yield further improvements when labeled data is extremely scarce?
- Would fine-tuning Stable Diffusion models on the specific target domain eliminate the distribution shift and close the performance gap with image compositing?

## Limitations
- Study relies on a custom dataset with significant class imbalance (22 military vs 218 commercial samples)
- Several implementation details remain unspecified including exact segmentation methods and Gaussian filter parameters
- Evaluation uses only mAP@0.50 without testing stricter IoU thresholds
- Background image sourcing and quantity are undocumented, making exact reproduction difficult

## Confidence
- **High confidence**: Image compositing outperforming baseline and classical augmentation methods
- **Medium confidence**: Image compositing outperforming generative models specifically due to distribution shift
- **Low confidence**: The relative performance ranking would hold on larger, more balanced datasets or different detection tasks

## Next Checks
1. **Dataset balance validation**: Retrain the best-performing image compositing model while equalizing class representation to isolate whether performance gains stem from augmentation quality or class distribution effects
2. **Distribution shift quantification**: Measure the covariate shift between composited images, SD-generated images, and real test images using KL divergence on feature representations
3. **Stricter IoU evaluation**: Repeat all experiments evaluating mAP@0.75 and mAP@0.95 to determine whether compositing's advantage persists under stricter localization requirements