---
ver: rpa2
title: 'AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models
  for Personalized Image Aesthetic Assessment'
arxiv_id: '2509.11620'
source_url: https://arxiv.org/abs/2509.11620
tags:
- aesthetic
- bias
- image
- arxiv
- assessment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes AesBiasBench, a benchmark designed to evaluate\
  \ stereotype bias and alignment in multimodal large language models (MLLMs) for\
  \ personalized image aesthetic assessment (PIAA). It introduces structured metrics\u2014\
  Identity Frequency Disparity (IFD), Normalized Representation Disparity (NRD), and\
  \ Aesthetic Alignment Score (AAS)\u2014to quantify demographic biases across three\
  \ tasks: Aesthetic Perception, Aesthetic Assessment, and Aesthetic Empathy."
---

# AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment

## Quick Facts
- arXiv ID: 2509.11620
- Source URL: https://arxiv.org/abs/2509.11620
- Reference count: 22
- Primary result: Smaller MLLMs show stronger stereotype bias; identity-aware prompts amplify bias, especially in emotional judgments.

## Executive Summary
This paper introduces AesBiasBench, a benchmark to evaluate stereotype bias and alignment in multimodal large language models (MLLMs) for personalized image aesthetic assessment (PIAA). The benchmark defines three tasks—Aesthetic Perception, Aesthetic Assessment, and Aesthetic Empathy—and introduces three structured metrics: Identity Frequency Disparity (IFD), Normalized Representation Disparity (NRD), and Aesthetic Alignment Score (AAS). Experiments on 19 models, including InternVL-2.5 and Qwen2.5-VL, show that smaller models exhibit stronger demographic biases, and that incorporating identity information often amplifies bias, particularly in emotional judgments. Results underscore the importance of identity-aware evaluation in subjective vision-language tasks.

## Method Summary
AesBiasBench evaluates MLLMs using two prompt variants (default and identity-aware) across three tasks. The PARA and LAPIS datasets provide image and demographic metadata, with annotator ratings converted to discrete labels. Models are queried for single-token outputs, which are compared across demographic groups using IFD (measuring output distribution differences), NRD (measuring conditional preference skew), and AAS (measuring alignment with human preferences via Jensen-Shannon Divergence). Inference is conducted without additional training, focusing on model behavior in realistic deployment settings.

## Key Results
- Smaller MLLM variants exhibit significantly higher stereotype bias (higher IFD) than larger variants.
- Incorporating identity information in prompts amplifies bias, especially in aesthetic empathy tasks.
- Model alignment with human preferences (AAS) varies by demographic group, with some models reversing alignment when identity is specified.

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its structured evaluation of both bias (via IFD and NRD) and alignment (via AAS) across controlled prompt conditions. By isolating the effect of demographic identity specification, it reveals how MLLMs' internal representations and output distributions shift in response to identity cues, exposing hidden biases in subjective judgment tasks.

## Foundational Learning
- **Aesthetic Perception vs. Assessment vs. Empathy**: Distinguishing low-level quality from subjective appeal and emotional response is essential for targeted bias analysis. *Quick check*: Verify that label distributions differ meaningfully across these tasks.
- **Identity Frequency Disparity (IFD)**: Measures distributional differences in model outputs across demographic groups, exposing representational bias. *Quick check*: Confirm IFD > 0 indicates group-level preference skew.
- **Normalized Representation Disparity (NRD)**: Normalizes conditional preference scores to compare bias across groups with different base rates. *Quick check*: Ensure normalization accounts for demographic group sizes.
- **Aesthetic Alignment Score (AAS)**: Uses Jensen-Shannon Divergence to quantify model-human label agreement, capturing subjective alignment. *Quick check*: AAS close to 1 indicates strong alignment.
- **Identity-Aware Prompting**: Adding demographic cues ("As a [demographic], please...") tests whether models' outputs are sensitive to identity framing. *Quick check*: Compare outputs with and without identity cues.
- **Single-Token Parsing**: Enforcing constrained output simplifies metric computation but may not capture all model behaviors. *Quick check*: Validate parsing robustness across model families.

## Architecture Onboarding
- **Component Map**: Datasets (PARA/LAPIS) -> Image Preprocessing -> Prompt Templates -> MLLM Inference -> Output Parsing -> Metric Computation (IFD/NRD/AAS)
- **Critical Path**: Prompt generation → Model inference → Response parsing → Metric calculation. Any failure here propagates to final results.
- **Design Tradeoffs**: Single-token outputs simplify metrics but may underrepresent nuanced judgments; identity-aware prompts risk overfitting to demographic cues.
- **Failure Signatures**: Free-text responses break parsing; sparse demographic groups cause unstable IFD/NRD; mismatched label aggregation yields invalid AAS.
- **First Experiments**:
  1. Run inference on a single model and image with both prompt variants; check output format and parsing.
  2. Compute IFD, NRD, AAS for one demographic group; verify against hand calculations.
  3. Compare metric values across model sizes on a small subset to confirm expected bias trends.

## Open Questions the Paper Calls Out
- **Open Question 1**: How do aesthetic biases manifest across identity dimensions beyond age, gender, and education—such as culture, race, and religion? The study restricted analysis to three attributes; no experiments were conducted on cultural, racial, or religious demographic axes.
- **Open Question 2**: What causal mechanism explains the inverse relationship between model size and stereotype bias? The paper observes correlation but does not isolate whether reduced bias stems from parameter count, training data composition, architectural differences, or their interaction.
- **Open Question 3**: Why does explicitly specifying gender identity cause a complete reversal in aesthetic empathy alignment from female to male preferences? The paper documents this shift but offers only speculative explanations without empirical validation.
- **Open Question 4**: How effective are the proposed mitigation strategies—concept editing, data re-balancing, and fairness-aware post-training—at reducing the documented aesthetic biases? The paper introduces these directions but does not implement or validate any intervention.

## Limitations
- Inference hyperparameters (temperature, top-p) are unspecified, affecting output consistency and metric values.
- Human label aggregation method for AAS is not detailed, introducing ambiguity in ground truth creation.
- Normalization and handling of sparse demographic groups across image types is not fully specified, potentially affecting cross-model comparisons.

## Confidence
- **High Confidence**: Core experimental design (three tasks, two prompt variants, three metrics) is clearly specified and reproducible.
- **Medium Confidence**: Trend that smaller models exhibit stronger stereotype bias is methodologically sound but depends on exact inference settings and label aggregation.
- **Medium Confidence**: Finding that identity-aware prompts amplify bias, especially in emotional judgments, is plausible but requires careful control of prompt engineering and response parsing to validate.

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary temperature and top-p across a subset of models to assess stability of IFD, NRD, and AAS values.
2. **Label Aggregation Method**: Test multiple aggregation strategies (majority vote vs. mean-based thresholding) on human ratings to confirm robustness of AAS results.
3. **Prompt Parsing Robustness**: Validate that all models consistently output single-token responses; if not, implement constrained decoding or robust parsing to ensure metric comparability.