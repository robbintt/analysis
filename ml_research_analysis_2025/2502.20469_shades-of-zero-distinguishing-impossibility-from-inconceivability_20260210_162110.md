---
ver: rpa2
title: 'Shades of Zero: Distinguishing Impossibility from Inconceivability'
arxiv_id: '2502.20469'
source_url: https://arxiv.org/abs/2502.20469
tags:
- impossible
- inconceivable
- language
- people
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores how people distinguish between impossible\
  \ and inconceivable events\u2014such as levitating a feather with one's mind versus\
  \ using the number five\u2014and whether statistical language models (LMs) learn\
  \ similar distinctions. Through a categorization task, participants reliably separated\
  \ inconceivable events from impossible ones, though their subjective likelihood\
  \ ratings for both were near zero and indistinguishable."
---

# Shades of Zero: Distinguishing Impossibility from Inconceivability

## Quick Facts
- **arXiv ID:** 2502.20469
- **Source URL:** https://arxiv.org/abs/2502.20469
- **Reference count:** 40
- **Primary result:** Participants distinguish impossible from inconceivable events, but assign both near-zero likelihood; LMs reflect these distinctions through different string probabilities

## Executive Summary
This study investigates how humans and neural language models (LMs) differentiate between impossible events (e.g., levitating a feather with the mind) and inconceivable events (e.g., using the number five). Through a categorization task, participants reliably separated inconceivable from impossible events, despite both receiving near-zero likelihood ratings that were statistically indistinguishable. Neural LMs assigned significantly different string probabilities to these categories, broadly aligning with human ratings. The findings suggest that humans may employ structured, non-probabilistic knowledge for modal distinctions, while LMs rely on statistical patterns in language. This work illuminates both parallels and divergences in human and machine modal reasoning.

## Method Summary
The study employed a categorization task where participants evaluated various events to determine whether they were impossible or inconceivable. Participants also provided subjective likelihood ratings for each event. The same events were then evaluated by neural LMs, which assigned string probabilities. The researchers compared human categorization patterns and likelihood ratings with LM predictions to assess whether LMs captured similar modal distinctions as humans.

## Key Results
- Participants consistently categorized inconceivable events differently from impossible ones, despite both receiving near-zero likelihood ratings
- Neural LMs assigned significantly different string probabilities to impossible versus inconceivable events
- LM predictions broadly aligned with human likelihood ratings, though not perfectly

## Why This Works (Mechanism)
The study demonstrates that humans can make categorical distinctions between types of non-realizable events (impossible vs. inconceivable) even when their graded likelihood assessments are indistinguishable. This suggests that modal reasoning involves structured, non-probabilistic knowledge systems that operate alongside or separately from probabilistic reasoning. Neural LMs appear to capture some of these distinctions through statistical patterns in language, though the mechanism differs from human reasoning.

## Foundational Learning
- **Modal reasoning:** Understanding the distinction between impossibility and inconceivability is fundamental to how humans reason about what is possible, necessary, or impossible in thought and action
- **Subjective likelihood vs. categorical judgment:** The dissociation between near-zero likelihood ratings and reliable categorical distinctions suggests humans use multiple cognitive systems for modal evaluation
- **Language model grounding:** LMs must learn to represent modal concepts through statistical patterns rather than explicit knowledge of possibility and necessity

## Architecture Onboarding
- **Component map:** Human cognitive system (categorization + likelihood rating) -> Neural LM (string probability generation) -> Comparison of outputs
- **Critical path:** Event presentation → Human categorization and rating → LM probability assignment → Statistical comparison of patterns
- **Design tradeoffs:** The study prioritizes ecological validity (using natural language events) over controlled laboratory paradigms, trading experimental control for real-world relevance
- **Failure signatures:** If LMs perfectly matched human ratings, it might indicate simple statistical learning rather than genuine modal understanding; if completely mismatched, it would suggest LMs cannot capture human modal distinctions
- **First experiments:**
  1. Replicate with larger, more diverse samples to test generalizability
  2. Add reaction time measurements to assess cognitive processing differences
  3. Test multiple LM architectures to determine if results generalize across model types

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The dissociation between categorical judgments and likelihood ratings remains unexplained, raising questions about the cognitive mechanisms underlying modal reasoning
- LM predictions aligned broadly but not perfectly with human ratings, leaving unclear whether LMs capture the same conceptual distinctions as humans
- No hypotheses or reproduction notes are provided, limiting assessment of theoretical grounding and replicability

## Confidence
- Human ability to categorically distinguish impossible from inconceivable events: **Medium**
- Neural LMs learn similar modal distinctions as humans: **Low**
- People use structured, non-probabilistic knowledge for modal reasoning: **Low**

## Next Checks
1. Replicate the study with larger sample sizes and diverse populations to test generalizability of the categorical distinction
2. Conduct follow-up experiments measuring reaction times and confidence ratings to better understand the dissociation between categorization and likelihood judgments
3. Test additional LMs (including smaller models and non-transformer architectures) on the same stimuli to determine whether the pattern of results generalizes across model types