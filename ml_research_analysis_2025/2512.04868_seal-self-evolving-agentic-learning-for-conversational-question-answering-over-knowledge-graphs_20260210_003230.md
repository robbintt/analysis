---
ver: rpa2
title: 'SEAL: Self-Evolving Agentic Learning for Conversational Question Answering
  over Knowledge Graphs'
arxiv_id: '2512.04868'
source_url: https://arxiv.org/abs/2512.04868
tags:
- question
- join
- s-expression
- knowledge
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEAL, a two-stage semantic parsing framework
  for conversational question answering over knowledge graphs. It decomposes complex
  query generation into S-expression core extraction followed by agent-driven calibration
  and template-based composition, significantly improving structural accuracy and
  computational efficiency.
---

# SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2512.04868
- **Source URL:** https://arxiv.org/abs/2512.04868
- **Reference count:** 40
- **Primary result:** Introduces SEAL, a two-stage semantic parsing framework for conversational QA over knowledge graphs that significantly improves structural accuracy and computational efficiency.

## Executive Summary
SEAL introduces a novel two-stage semantic parsing framework for conversational question answering over knowledge graphs (KGs). It decomposes complex query generation into S-expression core extraction followed by agent-driven calibration and template-based composition. This approach significantly improves structural accuracy and computational efficiency. The system also incorporates a self-evolving mechanism with local/global memory and reflection to continuously adapt from dialog history and execution feedback without retraining, achieving state-of-the-art performance on the SPICE benchmark.

## Method Summary
SEAL employs a two-stage semantic parsing approach. First, an LLM generates a simplified S-expression core using basic functions (JOIN, R, AND, VALUES, IS_TRUE) that captures the query's essential semantics. This core is then calibrated for syntax and KG alignment by an agentic module. In the second stage, a question-type prediction module selects a predefined template, and placeholders are instantiated with the calibrated core, functions, or constants. The framework also incorporates a self-evolving mechanism using local memory for short-term context and global memory for storing validated S-expression patterns, with a reflection module that analyzes execution results to update the memory base.

## Key Results
- SEAL achieves state-of-the-art performance on the SPICE benchmark, especially in multi-hop reasoning, comparison, and aggregation tasks
- Significant gains in both structural accuracy and computational efficiency compared to baseline methods
- The self-evolving mechanism improves performance over time, with SEAL showing 56% higher F1 than memory-ablated versions in mid-to-late dialog turns

## Why This Works (Mechanism)

### Mechanism 1: S-expression Core Decomposition
The framework decomposes complex logical form generation into minimal S-expression core extraction followed by template-based composition. This improves structural accuracy because LLMs can generate semantically accurate simplified substructures more reliably than full complex logical forms, while templates enforce structural validity.

### Mechanism 2: Agentic Calibration with Lightweight Linking
An agent performs syntactic correction and single-candidate KG alignment on the S-expression core. A lightweight linking strategy retains only the top-1 most semantically similar entity/relation candidate, reducing noise and computational cost while maintaining precision due to the presumed semantic precision of LLM-generated cores.

### Mechanism 3: Self-Evolving Memory and Reflection
The system uses local memory for short-term context management and global memory for storing validated S-expression patterns. A reflection module post-analyzes execution results, adding successful patterns to global memory. This closed-loop process enables adaptive learning without explicit retraining.

## Foundational Learning

**Concept:** S-expressions & Semantic Parsing
- **Why needed here:** The entire framework parses natural language into S-expressions and then into executable SPARQL queries. Understanding the syntax and semantics of S-expression functions is critical for debugging core extraction and template composition.
- **Quick check question:** Given the S-expression `(AND (JOIN (R father) Q1063295) (JOIN P21 Q6581097))`, what logical operation does it represent on a knowledge graph?

**Concept:** Conversational Context & Coreference Resolution
- **Why needed here:** A core challenge is resolving pronouns and ellipsis across dialog turns. Understanding how SEAL uses local memory and an LLM to rewrite incomplete questions into standalone forms is critical for first-stage processing.
- **Quick check question:** In the dialog: "Who are the children of X?" -> "Who are siblings of that one?", how does the system identify the referent for "that one"?

**Concept:** Agent-Based Calibration & Knowledge Graph Alignment
- **Why needed here:** The system uses an agent to interface with the KG, correct syntax, and perform "light linking." Understanding this agentic step—its corrections for relation inversion, type constraints, and top-1 linking strategy—is essential for understanding structural accuracy.
- **Quick check question:** Why does the calibration agent prefer a "top-1" linking strategy over traditional "top-k" candidate approaches, and what assumption does this rest on?

## Architecture Onboarding

**Component map:** User Query -> [Local Memory/LLM for Rewriting] -> [LLM for Core Generation] -> [Calibration Agent for Syntax+KG Alignment] -> [Question Type Prediction] -> [Template Selection from Global Memory] -> [Template Instantiation] -> [SPARQL Execution] -> [Reflection Module] -> (if valid) Update Global Memory

**Critical path:** The user query first undergoes coreference resolution using local memory, then the LLM generates an S-expression core. The calibration agent corrects syntax and performs KG alignment, followed by question type prediction and template selection. The final S-expression is composed and executed, with the reflection module validating results and updating global memory.

**Design tradeoffs:** The top-1 linking strategy prioritizes efficiency over robustness to ambiguous mentions. The template-based approach ensures structural correctness but may struggle with novel query types. The self-evolving memory enables adaptation without retraining but cannot learn new reasoning primitives, only new combinations of existing ones.

**Failure signatures:** Empty/wrong answers indicate calibration failures or template mismatches. Stagnant performance suggests global memory not updating or being polluted with noisy patterns. Coreference errors in multi-turn dialogs point to local memory failures in resolving pronouns.

**First 3 experiments:**
1. End-to-End Evaluation: Run SEAL on SPICE benchmark comparing macro-F1 and accuracy against baselines across different question types.
2. Ablation Studies: Run SEAL with components removed (core extraction, calibration, local/global memory) to quantify individual contributions.
3. Self-Evolution Analysis: Evaluate performance over long dialogs by turn count, comparing SEAL with memory-ablated version to validate performance improvement over time.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the calibration module be refined to distinguish between semantically valid empty query results and structural errors? The current calibration strategy may inadvertently revise valid empty queries, reducing precision. Evidence would come from evaluating on datasets with known valid zero-answer queries.

**Open Question 2:** Can specialized smaller models replace the general-purpose LLM in specific sub-tasks to reduce computational overhead without significant performance degradation? The current implementation's computational overhead from multiple LLM invocations could be mitigated by employing smaller models for subtasks like coreference resolution or question type classification.

**Open Question 3:** To what extent does the S-expression to SPARQL transformation logic generalize to Knowledge Graphs with diverse schemas beyond Wikidata? The current transformation is constrained by specific syntactic patterns and datasets, requiring versatile transformation functions validated across diverse benchmarks.

## Limitations
- The top-1 lightweight linking strategy may fail with ambiguous surface names, potentially limiting robustness in complex multi-hop reasoning
- Fixed template library approach could restrict the system's ability to handle novel query structures not represented in existing templates
- Self-evolving memory mechanism depends heavily on reflection module quality; spurious queries incorrectly validated could pollute global memory and degrade performance over time

## Confidence
- **High Confidence:** Two-stage semantic parsing framework and demonstrated improvement in structural accuracy and computational efficiency
- **Medium Confidence:** Efficacy of top-1 lightweight linking strategy and its contribution to efficiency gains
- **Medium Confidence:** Self-evolving memory mechanism's ability to continuously adapt without retraining

## Next Checks
1. Conduct controlled experiment comparing SEAL's top-1 linking strategy against top-3 baseline across all SPICE question types, measuring accuracy and linking error frequency.
2. Design complex multi-hop reasoning queries with structures not in current template library to quantify trade-off between structural validity and expressiveness.
3. Extend dialog length in experiments to observe self-evolving mechanism behavior over many turns, monitoring global memory growth and query success rates to assess long-term stability.