---
ver: rpa2
title: Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference
  Optimization
arxiv_id: '2508.10164'
source_url: https://arxiv.org/abs/2508.10164
tags:
- reasoning
- length
- preference
- arxiv
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates reducing the output length of Large Reasoning
  Models (LRMs) while maintaining reasoning performance. The authors analyze the generation
  space of LRMs and find that shorter yet effective reasoning trajectories exist within
  the model's generation distribution.
---

# Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization

## Quick Facts
- arXiv ID: 2508.10164
- Source URL: https://arxiv.org/abs/2508.10164
- Authors: Bin Hong; Jiayu Liu; Zhenya Huang; Kai Zhang; Mengdi Zhang
- Reference count: 40
- One-line primary result: LCPO reduces LRM output length by >50% with 0.8k samples while maintaining reasoning accuracy

## Executive Summary
This paper addresses the inefficiency of Large Reasoning Models (LRMs) that generate excessively long Chain-of-Thought (CoT) responses through overthinking. The authors propose a novel preference optimization method called Length Controlled Preference Optimization (LCPO) that leverages existing shorter reasoning trajectories within the model's generation distribution. By filtering training data to focus on "easy" problems and explicitly balancing NLL-related rewards, LCPO achieves significant length reduction while maintaining reasoning performance across multiple math benchmarks.

## Method Summary
The method samples 16 outputs per question, filters to "easy" problems where all outputs are correct, then pairs the shortest correct response (chosen) with the longest response (rejected) for preference optimization. LCPO introduces a log-odds reward formulation that explicitly balances NLL-related rewards, enabling effective length preference learning with limited training data (0.8k samples, 50 steps). The approach is evaluated on DeepSeek-R1-Distill-Qwen models (1.5B and 7B) across six math reasoning benchmarks.

## Key Results
- Achieves >50% average output length reduction while maintaining reasoning accuracy
- Requires only 0.8k training samples and 50 training steps
- Consistent effectiveness across different model sizes (1.5B and 7B)
- Maintains performance on out-of-distribution scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRMs already contain shorter effective reasoning trajectories within their generation distribution that can be surfaced through selective data filtering and preference optimization.
- Mechanism: The model samples 16 outputs per question, sorts by length, and finds that shorter outputs (rankings 0-6) maintain accuracy while longer outputs show degradation—indicating existing concise reasoning paths are accessible without architectural changes.
- Core assumption: The base model has already learned both verbose and concise reasoning modes during pretraining/RLVR, making length reduction a distribution-shifting problem rather than capability injection.
- Evidence anchors:
  - [abstract] "shorter yet effective reasoning trajectories exist within the model's generation distribution"
  - [section 3.2] Figure 2a shows stable performance for outputs ranking 0-6, decline at higher rankings
  - [corpus] Moderate evidence: related work on CoT compression (ConCISE, Pruning the Unsurprising) assumes similar premise but doesn't directly validate the existence claim
- Break condition: If base model was trained exclusively with length-maximizing RLVR and lacks shorter modes, filtering won't surface them; expect flat accuracy-length curves across all rankings.

### Mechanism 2
- Claim: The implicit NLL-related reward in standard preference optimization methods creates a convergence conflict that prevents effective length preference learning.
- Mechanism: Under Bradley-Terry framework, methods like ORPO incorporate NLL loss (log pθ(y|x)) which optimizes for token probability regardless of length—counteracting the length preference signal and causing slower/instable convergence on small data.
- Core assumption: The gradient from NLL loss dominates over the preference signal when data is limited (<1k samples), which the paper demonstrates theoretically (Appendix D) but doesn't empirically ablate in isolation.
- Evidence anchors:
  - [abstract] "Theoretical analysis... reveals that the implicit NLL-related reward can hinder length preference learning"
  - [section 3.3.2] "ORPO fits worse than pure log odds ratio loss" due to NLL component
  - [corpus] Weak evidence: no corpus papers directly address NLL-reward conflict in preference optimization for length control
- Break condition: If using >10k samples or longer training (>500 steps), NLL conflict may diminish as preference signal accumulates; check if SimPO/DPO close the gap with LCPO at scale.

### Mechanism 3
- Claim: Explicitly balancing the NLL-related reward with a counterpart term in LCPO enables faster convergence on length preferences with limited training data.
- Mechanism: LCPO reformulates the reward as rθ(y|x) = log(pθ(y|x)/(1-pθ(y|x))), using the log-odds ratio which naturally bounds the NLL influence while maintaining gradient signal for preference learning.
- Core assumption: The log-odds formulation provides sufficient gradient signal for length preference while avoiding the convergence saturation of standard NLL—validated empirically (50 steps sufficient) but not directly compared against ablation of just removing NLL.
- Evidence anchors:
  - [abstract] "LCPO... explicitly balances the implicit reward related to NLL loss"
  - [section 3.3.3] Equation 11-12 shows the log-odds reward and balanced objective
  - [corpus] No corpus evidence for this specific formulation; appears novel to this work
- Break condition: If log-odds formulation causes numerical instability (when pθ approaches 0 or 1) on harder datasets, expect gradient explosion or training divergence.

## Foundational Learning

- Concept: **Bradley-Terry Model**
  - Why needed here: LCPO derives from BT loss framework; understanding how BT models preference probability via ability differences (βi - βj) is essential for grasping why NLL conflicts arise and how LCPO resolves them.
  - Quick check question: Can you explain why σ(βi - βj) in BT model becomes σ(r(x,yw) - r(x,yl)) in preference optimization, and what happens when the reward function includes conflicting terms?

- Concept: **Preference Optimization (DPO/SimPO variants)**
  - Why needed here: LCPO is a modification of existing PO methods; you need to understand how implicit rewards are derived from policy ratios (DPO) or log-probabilities (SimPO) to see why NLL creates interference.
  - Quick check question: Compare DPO's reward rDPO = β log(πθ/πref) with SimPO's length-normalized reward—why does SimPO work better for long sequences, and what limitation does LCPO address that SimPO doesn't?

- Concept: **Chain-of-Thought Reasoning in LRMs**
  - Why needed here: The entire approach assumes LRMs generate variable-length reasoning traces with "overthinking" tendencies; understanding test-time scaling and RLVR training helps contextualize why shorter paths exist and how length correlates with difficulty.
  - Quick check question: Why do LRMs trained with RLVR (e.g., DeepSeek-R1) tend toward longer outputs, and how does pass-rate-based difficulty estimation help identify which problems can tolerate shorter reasoning?

## Architecture Onboarding

- Component map:
  - Data Pipeline: Sample generation (16 outputs/question) → Sort by length → Difficulty estimation (pass rate si) → Filter to "easy" split → Select shortest as chosen yw, longest as rejected yl
  - LCPO Loss: Compute log-odds reward rθ(y|x) = log(pθ/(1-pθ)) for both responses → Apply Bradley-Terry loss with margin ϵ=0 → Scale by λ hyperparameter
  - Training Loop: Standard offline fine-tuning on 0.8k filtered pairs for 50 steps with AdamW, cosine LR schedule

- Critical path:
  1. Rollout quality: If initial 16 samples don't include correct short solutions, the chosen/rejected pairing will be noisy
  2. Difficulty filtering: Using wrong split (medium/hard) reduces length reduction by 15-25% (Table 4)
  3. Early stopping: Training beyond 50 steps (350 steps tested) shows no improvement and may overfit

- Design tradeoffs:
  - Data volume vs. stability: Paper uses only 0.8k samples for 50% length reduction but trades generalization—MMLU shows 42-51% reduction but 3% accuracy drop (Table 5)
  - Length reduction vs. task difficulty: GSM8K (easy) only achieves 13% reduction while maintaining accuracy; AIME24 (hard) achieves 51% reduction—suggests adaptive behavior but limits compression on simple tasks
  - Margin ϵ=0 vs. ϵ>0: Setting margin to zero speeds convergence but may reduce robustness to noisy preference pairs

- Failure signatures:
  - Accuracy degradation on easy tasks: If accuracy drops >2% on benchmarks like GSM8K, likely over-filtered to "easy" split or trained too long
  - Insufficient length reduction (<30%): Check if using "medium" or "hard" splits; verify shortest response in pair is actually correct
  - Training instability: Log-odds computation can produce NaN when pθ approaches 1; add numerical stability (log(pθ + ε))

- First 3 experiments:
  1. Validate generation space hypothesis: Run 16-sample rollout on 50 questions, plot accuracy vs. length ranking to confirm shorter paths maintain performance (replicate Figure 2a)
  2. Ablate difficulty filter: Train LCPO on easy/medium/hard/unfiltered splits, compare length reduction and accuracy to quantify filtering contribution (replicate Table 4 pattern)
  3. Compare convergence speed: Train DPO, SimPO, and LCPO with identical data, evaluate at steps 50/100/200/350 to validate LCPO's faster convergence claim and check if other methods catch up given more steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of LCPO in balancing NLL-related rewards and length preferences scale to significantly larger reasoning models (e.g., 70B+ parameters)?
- Basis in paper: [explicit] The authors state, "Due to limited computational resources, we only conduct experiments on 1.5B and 7B models."
- Why unresolved: Training dynamics and the interaction between implicit rewards and length constraints may shift unpredictably with increased model capacity.
- What evidence would resolve it: Performance metrics (accuracy vs. length reduction) of LCPO applied to larger open-source reasoning models.

### Open Question 2
- Question: Can a mixed training set of different reasoning modes improve the universality and stability of length reduction compared to the math-only training used?
- Basis in paper: [explicit] The authors note their training set focuses on math tasks and suggest "better results [can] achieve... by carefully design a mixed training set of different reasoning modes."
- Why unresolved: The current experiments rely on math-specific datasets (LIMR), leaving the impact of domain diversity on the pruning of reasoning trajectories unexplored.
- What evidence would resolve it: Experiments comparing model performance when trained on a heterogeneous dataset (e.g., math, coding, logic) versus a single-domain dataset.

### Open Question 3
- Question: What specific semantic reasoning behaviors (e.g., self-verification, planning) are preferentially pruned by LCPO, and does this vary by problem difficulty?
- Basis in paper: [inferred] The paper demonstrates a quantitative reduction in length while maintaining accuracy but lacks a qualitative analysis of which reasoning steps are removed as "overthinking."
- Why unresolved: It is unclear if the model learns to skip redundant loops or if it truncates distinct cognitive phases like error checking.
- What evidence would resolve it: A qualitative study or automated analysis tagging specific reasoning behaviors in outputs before and after LCPO optimization.

## Limitations

- Theoretical analysis of NLL interference is internally consistent but lacks direct empirical ablation to isolate the NLL component's contribution
- Approach relies on existence of shorter effective reasoning trajectories that may not generalize to LRMs trained with different RLVR objectives
- 3% accuracy drop on MMLU suggests potential loss of general reasoning capabilities when applying LCPO-trained models beyond math domains

## Confidence

**High Confidence**: The empirical demonstration that LCPO achieves >50% length reduction while maintaining accuracy on math benchmarks (MATH-500, AIME24, AMC23, OlympiadBench) is robust. The training procedure, hyperparameters, and evaluation methodology are clearly specified, enabling reproducible results.

**Medium Confidence**: The theoretical analysis of NLL interference in preference optimization is internally consistent but relies on assumptions about gradient dominance in small-data regimes that aren't fully empirically validated. The log-odds formulation appears effective but alternative approaches aren't systematically compared.

**Low Confidence**: Claims about LCPO's superiority in out-of-distribution generalization and its effectiveness across all model sizes require further validation. The paper shows consistent results on held-out test sets but doesn't test on truly unseen domains or qualitatively different reasoning tasks.

## Next Checks

1. **Ablation Study of NLL Interference**: Train three variants—standard ORPO, LCPO without NLL component, and LCPO with NLL component. Compare convergence curves and final performance at multiple training steps (50, 100, 200, 350) to empirically validate whether NLL removal is the primary driver of LCPO's efficiency.

2. **Cross-Domain Generalization Test**: Apply LCPO-trained models to non-math reasoning tasks (legal reasoning, code generation, commonsense QA) to quantify the generalization drop observed on MMLU. This would validate whether length reduction trades off general reasoning capabilities.

3. **Rollout Quality Analysis**: Systematically vary the number of samples per question (4, 8, 16, 32) in the data generation phase to determine the minimum required for effective LCPO training. This would establish whether 16 samples is optimal or if fewer samples suffice, reducing computational overhead.