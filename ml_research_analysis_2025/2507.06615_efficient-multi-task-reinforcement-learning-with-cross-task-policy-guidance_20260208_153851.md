---
ver: rpa2
title: Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance
arxiv_id: '2507.06615'
source_url: https://arxiv.org/abs/2507.06615
tags:
- policy
- task
- guide
- tasks
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Cross-Task Policy Guidance (CTPG), a framework
  that improves multi-task reinforcement learning by enabling explicit policy sharing
  between tasks. Instead of each task learning in isolation, CTPG trains a guide policy
  to select the most beneficial control policy from other tasks to generate training
  trajectories, accelerating skill acquisition.
---

# Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance

## Quick Facts
- arXiv ID: 2507.06615
- Source URL: https://arxiv.org/abs/2507.06615
- Reference count: 40
- Primary result: Explicit policy sharing via guide policy selection improves multi-task RL sample efficiency and performance

## Executive Summary
Cross-Task Policy Guidance (CTPG) introduces a novel framework for multi-task reinforcement learning that enables explicit policy sharing between tasks through a learned guide policy. Instead of each task learning in isolation, CTPG trains a guide policy to select the most beneficial control policy from other tasks for trajectory generation, accelerating skill acquisition. The framework incorporates two gating mechanisms—policy-filter and guide-block—that enhance efficiency by filtering out non-beneficial policies and preventing guidance for already-mastered tasks. Experiments demonstrate significant improvements in sample efficiency and final performance on both manipulation (MetaWorld) and locomotion (HalfCheetah) benchmarks.

## Method Summary
CTPG operates through a bi-level learning structure where a guide policy selects which task's control policy to use for behavior generation every K timesteps. The framework builds on SAC for both control and guide policies, with guide policies trained via discrete SAC to maximize expected return by choosing among all available control policies. Two gating mechanisms enhance efficiency: a policy-filter gate uses value function comparisons to mask out non-beneficial policies, and a guide-block gate uses SAC temperature parameters to identify tasks that don't require cross-task guidance. Hindsight off-policy correction enables learning from past guidance decisions. The method requires MTSAC/MHSAC baseline with task conditioning, guide policy networks, comparable Q-networks for policy-filter, and temperature-based blocking logic.

## Key Results
- Combining CTPG with parameter-sharing approaches significantly improves sample efficiency on MetaWorld MT10/MT50 and HalfCheetah MT5/MT8 benchmarks
- Guide policy effectively transfers skills between tasks, learning to select beneficial control policies
- Policy-filter and guide-block gates improve final performance by 10-15% over baselines without gating
- K=10 guide step duration provides robust performance across tested environments

## Why This Works (Mechanism)

### Mechanism 1: Guide Policy for Long-Term Cross-Task Policy Selection
A learned guide policy identifies which task's control policy to use for trajectory generation, providing more effective guidance than single-step action selection. The guide policy operates at a higher temporal abstraction, selecting a behavior policy every K timesteps, creating a bi-level learning structure where the guide policy learns discrete policy selection while control policies learn continuous actions. This works because tasks share transferable skills that manifest over multiple timesteps, not just single actions.

### Mechanism 2: Policy-Filter Gate via Value-Based Candidate Pruning
The policy-filter gate improves guidance quality by filtering based on value function comparisons. It computes a mask where policies are included only if their expected return (via comparable guide Q-value) exceeds the task's own value function. This requires learning a "comparable guide Q-value" that incorporates the current task's entropy term, enabling fair comparison between guide's expected return and task's own value function.

### Mechanism 3: Guide-Block Gate for Adaptive Guidance-Withholding
The guide-block gate uses SAC's temperature parameter as a proxy for task mastery, withholding guidance from already-mastered or easy tasks. Tasks are added to the guidance subset only if their temperature indicates they still benefit from cross-task guidance. This prevents interference when tasks have already learned sufficient skills independently.

## Foundational Learning

- **Concept: Off-Policy Reinforcement Learning with Replay Buffers**
  - Why needed here: CTPG uses hindsight off-policy correction to reassign past guide policy actions based on updated control policies. Understanding replay buffers and distribution shift is essential.
  - Quick check question: Can you explain why the "behavior policy" used for data collection can differ from the "target policy" being optimized, and what conditions make this stable?

- **Concept: Soft Actor-Critic (SAC) and Maximum Entropy RL**
  - Why needed here: CTPG builds on SAC for both control and guide policies. The temperature parameter, entropy regularization, and soft Bellman equation are core to understanding how guide-block gate and comparable Q-values work.
  - Quick check question: In SAC, what happens to the temperature if the policy entropy is higher than the target entropy, and how does this relate to exploration?

- **Concept: Multi-Task RL and Negative Transfer**
  - Why needed here: CTPG is designed to mitigate negative transfer through selective guidance. Understanding why indiscriminate sharing can hurt performance explains why the gates are necessary.
  - Quick check question: Give an example of two tasks where sharing the same policy would harm both, and explain what type of information sharing might still help?

## Architecture Onboarding

- **Component map:**
  - Control policies (per task) -> Guide policy (per task) -> Guide Q-networks -> Replay buffer -> Policy-filter gate -> Guide-block gate -> Hindsight correction

- **Critical path:**
  1. Initialize control policies, guide policies, critics, and replay buffer
  2. Each epoch: compute guidance subset from current temperatures
  3. For each task: every K steps, check guide-block; if task in subset, compute policy-filter mask, sample behavior policy via guide policy; else use own control policy
  4. Roll out K steps with selected behavior policy, store transitions with guide action
  5. Training: update control policies; every K steps, apply hindsight correction to reassigned guide actions, then update guide policies

- **Design tradeoffs:**
  - Guide step K: Small K increases flexibility but reduces temporal coherence; large K provides stable guidance but risks misalignment with skill boundaries. K=10 is robust across tested environments.
  - Monte Carlo samples for V-value estimation: More samples reduce variance but add compute; H=5 is sufficient.
  - Unified vs separate networks: Paper uses unified network with task conditioning for control policies; guide policies use multi-head structure.
  - SAC vs other RL algorithms: CTPG works with TD3+DQN combinations, but temperature-based guide-block gate requires adaptation for non-SAC methods.

- **Failure signatures:**
  - Guide policy oscillation: If K too small or learning rate too high, guide policy switches rapidly, producing incoherent trajectories.
  - Premature blocking: If temperatures converge at different rates, guide-block may withhold guidance from tasks that still need it.
  - Hindsight correction divergence: If control policies change too quickly, reassigned guide actions may not match any policy well.
  - Negative transfer through filter failure: If value estimation is poor early in training, policy-filter admits harmful policies.

- **First 3 experiments:**
  1. Sanity check on single task with self-guidance: Set N=1, verify guide policy learns to always select own control policy (trivial case).
  2. Two-task transfer with known similarity: Use MetaWorld Button-Press and Drawer-Close, train with CTPG, visualize guide policy selections over time.
  3. Ablation sweep on K with dissimilar tasks: Use HalfCheetah-MT8, test K in {1, 5, 10, 20, 50}, confirm intermediate K works best and document variance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the guide step K be automated to adapt to varying skill durations without manual hyperparameter tuning?
- **Basis in paper:** Section 6 identifies the reliance on a predetermined, fixed guide step K as a limitation and suggests automating its selection as a future research avenue.
- **Why unresolved:** A fixed K lacks the flexibility to accommodate the inconsistent execution times of shared skills across different tasks, necessitating environment-specific tuning.
- **What evidence would resolve it:** A mechanism that dynamically adjusts the guidance duration based on task progress, demonstrating superior performance over fixed-K baselines on benchmarks with diverse temporal structures.

### Open Question 2
- **Question:** Can alternative metrics for the guide-block gate improve generalizability beyond the current reliance on SAC-specific temperature parameters?
- **Basis in paper:** Section 6 highlights the need to investigate alternative metrics for the guide-block gate that do not depend on human-defined win rates or the specific temperature parameter of SAC.
- **Why unresolved:** The current reliance on SAC's entropy target may limit the framework's compatibility with other RL algorithms that do not utilize similar temperature scaling.
- **What evidence would resolve it:** Identification of a universal metric (e.g., gradient-based uncertainty or learning progress) that effectively gates guidance across various off-policy RL algorithms without algorithm-specific tuning.

### Open Question 3
- **Question:** Can the CTPG framework be extended to handle tasks with heterogeneous state and action spaces?
- **Basis in paper:** Section 3 formally defines the MTRL setting as tasks sharing the same state space and action space, and the method relies on selecting a policy from one task to execute directly in another.
- **Why unresolved:** The mechanism assumes policies are directly executable across tasks, which fails if tasks have different observation dimensions or action morphologies.
- **What evidence would resolve it:** An extension of CTPG incorporating latent space alignment or adaptor networks that enables effective policy guidance between tasks with mismatched action dimensions.

## Limitations
- Guide step K requires manual tuning and may not generalize across task families with different skill duration characteristics
- Temperature-based guide-block gate relies on SAC-specific entropy parameters, limiting compatibility with other RL algorithms
- Policy-filter gate performance depends on accurate value estimation, which may be challenging early in training

## Confidence
- **High confidence**: The core bi-level learning architecture (guide policy selecting control policies) is well-specified and theoretically grounded in discrete SAC principles
- **Medium confidence**: The gating mechanisms (policy-filter and guide-block) are clearly described, but their practical effectiveness depends on accurate value estimation and temperature calibration
- **Low confidence**: The claim that CTPG "significantly improves sample efficiency" is supported by experimental results but lacks ablation studies showing individual gate contributions across all task families

## Next Checks
1. **Cross-algorithm validation**: Implement CTPG with TD3+DQN and verify guide-block gate requires modification. Document how entropy-based gating translates to non-SAC methods.
2. **Gate ablation study**: Systematically disable each gate (policy-filter, guide-block) and measure performance degradation on MT10 and HalfCheetah-MT8 to isolate individual contributions.
3. **Temperature correlation analysis**: Collect empirical data on SAC temperature convergence rates across task families to test whether temperature thresholds need task-family-specific calibration.