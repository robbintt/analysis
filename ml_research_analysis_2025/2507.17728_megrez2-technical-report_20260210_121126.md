---
ver: rpa2
title: Megrez2 Technical Report
arxiv_id: '2507.17728'
source_url: https://arxiv.org/abs/2507.17728
tags:
- arxiv
- megrez2
- expert
- parameters
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Megrez2, a lightweight, high-performance language
  model architecture optimized for device-native deployment. The key innovation is
  cross-layer expert sharing, where expert modules are reused across adjacent transformer
  layers to significantly reduce total parameters while maintaining capacity.
---

# Megrez2 Technical Report

## Quick Facts
- arXiv ID: 2507.17728
- Source URL: https://arxiv.org/abs/2507.17728
- Reference count: 32
- Primary result: Megrez2-Preview achieves strong performance across language understanding, instruction following, math, and code tasks using only 3B activated and 7.5B stored parameters, outperforming or matching much larger models.

## Executive Summary
Megrez2 introduces a lightweight, high-performance language model architecture optimized for device-native deployment. The key innovation is cross-layer expert sharing, where expert modules are reused across adjacent transformer layers to significantly reduce total parameters while maintaining capacity. Combined with pre-gated routing for memory-efficient expert loading and faster inference, the Megrez2-Preview model, pre-trained on 5 trillion tokens and fine-tuned with RL, demonstrates a successful balance between efficiency, accuracy, and deployability.

## Method Summary
Megrez2 uses a 31-layer transformer architecture with MoE blocks grouped every 3 layers sharing 64 experts, plus 4 shared experts per layer. The model employs pre-gated routing where the router at layer i-1 selects experts for layer i, enabling asynchronous loading. Training occurred in three stages: 1.5T tokens for foundation knowledge, 3T tokens for reasoning and knowledge, and 0.6T tokens with 32K context extension. The model was trained on 5T tokens total using a three-stage approach, followed by supervised fine-tuning and RLVR optimization on math/reasoning tasks.

## Key Results
- Achieves strong performance on C-EVAL, MMLU-Pro, IFEval, MATH-500, GSM8K, HumanEval, and MBPP benchmarks
- Uses only 3B activated and 7.5B stored parameters while outperforming or matching much larger models
- Demonstrates efficient device-native deployment with cross-layer expert sharing and pre-gated routing

## Why This Works (Mechanism)

### Mechanism 1: Cross-layer expert sharing
- **Claim:** Cross-layer expert sharing reduces total storage requirements by a factor of n while maintaining per-token activation capacity
- **Mechanism:** Partitions transformer layers into groups of size n (n=3 in Megrez2-Preview), with all layers within a group referencing the same shared expert pool while maintaining layer-specific routing decisions
- **Core assumption:** Adjacent transformer layers learn sufficiently distinct routing policies that they don't require distinct expert weights
- **Evidence anchors:** [abstract], [section 3.2], and corpus evidence from HunyuanOCR supporting lightweight architectures
- **Break condition:** Performance degradation on deep reasoning tasks where later layers require fundamentally different feature transformations

### Mechanism 2: Pre-gated routing
- **Claim:** Pre-gated routing enables memory-efficient expert loading by shifting routing computation to the preceding layer
- **Mechanism:** Router for layer i computes routing decisions for layer i+1, allowing asynchronous loading of necessary expert weights while previous layer executes
- **Core assumption:** Semantic context at layer i-1 is predictive enough of layer i context for accurate routing decisions
- **Evidence anchors:** [abstract], [section 3.3], and corpus evidence from TeleChat3-MoE and Ring-linear supporting system-level MoE optimizations
- **Break condition:** If layer representations shift drastically, lookahead routing may select irrelevant experts, causing accuracy collapse

### Mechanism 3: Dense-first architecture with shared experts
- **Claim:** Dense-first architecture combined with shared experts stabilizes training and preserves generalization capacity in parameter-constrained regimes
- **Mechanism:** First layer is dense (non-MoE), and 4 shared experts are active in every layer alongside routed experts, ensuring baseline dense computation
- **Core assumption:** Mix of dense and sparse computation provides better accuracy-efficiency trade-off than pure sparsity
- **Evidence anchors:** [section 3.4] and corpus evidence from Motif 2.6B supporting architectural guardrails in lightweight LLMs
- **Break condition:** Excessive latency or memory overhead if ratio of shared-to-routed experts is too high

## Foundational Learning

- **Mixture-of-Experts (MoE) Sparsity:** Megrez2 is fundamentally an MoE architecture. Understanding that "activated parameters" (compute cost) differs from "stored parameters" (memory cost) is crucial to grasp efficiency claims.
  - Quick check: If a model has 7.5B stored parameters but only 3B activated, does it require a GPU with 7.5B parameter capacity to run? (Yes, for memory, though compute is lower)

- **Memory Bandwidth vs. Compute Bound:** The primary justification for Pre-gated routing is overcoming memory bandwidth limits on devices. Understanding that loading weights is often slower than calculating them is essential.
  - Quick check: On a mobile chip, why is "prefetching" data often more important than optimizing the number of FLOPs?

- **Pipeline Parallelism/Overlap:** Section 3.3 claims advantage by overlapping "computation and weights loading." Understanding instruction-level parallelism is required to implement the system co-design.
  - Quick check: Can we load expert weights for Layer 5 while Layer 4 is calculating? What pre-condition must be met? (The router for Layer 5 must have already run)

## Architecture Onboarding

- **Component map:** Input/Embedding -> Dense Feed-Forward Network (Layer 0) -> MoE Layers (Layers 1-30, grouped in sets of 3) -> Output
  - MoE Layers: 4 shared experts always active per layer + 64 routed experts per group (shared across 3 layers), top-6 selected + pre-gated routing

- **Critical path:** The latency path is defined by the critical path of memory loading: Router_i runs → Asynchronous Load of Experts for L_{i+1} begins → Compute L_i (using previously loaded experts) → Compute L_{i+1}

- **Design tradeoffs:**
  - Sharing Factor (n=3): Higher n reduces memory footprint (good for devices) but reduces total knowledge capacity and layer distinctiveness
  - Pre-gating Granularity: Predicting experts one layer ahead is safer but offers less lookahead buffer than predicting 2-3 layers ahead
  - Activated Params (3B): Keeps inference fast/cheap but limits complex reasoning compared to 8B+ models

- **Failure signatures:**
  - Cache Thrashing: If pre-gated router frequently switches between disparate experts, prefetching fails, stalling the pipeline
  - Routing Collapse: If router ignores shared experts and over-selects static "shared experts," model becomes dense 3B model
  - Layer Confusion: Degraded performance on tasks requiring hierarchical reasoning if shared experts cannot specialize sufficiently

- **First 3 experiments:**
  1. Ablation on Group Size: Train small variants with group size n=1 (standard MoE), n=2, and n=4 to validate the 3B active/7.5B stored efficiency sweet spot
  2. Latency Profiling: Deploy on target device and measure "stall time" with Pre-gated routing ON vs. OFF to validate the mechanism
  3. Routing Visualization: Visualize expert activation heatmaps for Layers i, i+1, i+2 to verify routers select different experts within shared pool

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several emerge from the limitations in the technical report regarding device-specific performance validation, scalability of the sharing mechanism, and representational conflicts from cross-layer sharing.

## Limitations
- Training protocol gaps including exact learning rates, batch sizes, and optimizer configurations remain unspecified
- Performance claims lack independent benchmarking and third-party validation
- Memory efficiency claims require device-specific latency measurements to validate actual hardware benefits
- Generalization beyond English web text, code, and synthetic reasoning data remains unvalidated

## Confidence

**High Confidence (8/10):**
- Cross-layer expert sharing mechanism is theoretically sound and implementable
- Dense-first architecture with shared experts provides stability benefits
- Pre-gated routing concept is valid for memory-efficient expert loading

**Medium Confidence (5/10):**
- Specific performance metrics achieved by Megrez2-Preview
- The 3B activated/7.5B stored parameter efficiency ratio claims
- The specific architectural hyperparameters being optimal

**Low Confidence (3/10):**
- Generalization claims beyond the reported benchmark suite
- Device-native deployment performance without hardware-specific validation
- Long-term stability and scaling properties for extended use

## Next Checks

**Validation Check 1: Ablation Study Implementation**
Implement and train three variants of Megrez2 with different sharing group sizes (n=1, n=2, n=4) using the same training protocol. Compare performance and efficiency metrics to verify the claimed optimal configuration (n=3) and quantify trade-offs between parameter efficiency and task performance.

**Validation Check 2: Independent Benchmarking**
Evaluate Megrez2-Preview on standardized benchmarks using independent infrastructure, including HELM evaluation framework for comprehensive assessment across capability, fairness, and efficiency dimensions. Compare against contemporaneous models of similar size using consistent evaluation protocols.

**Validation Check 3: Memory Profiling on Target Hardware**
Deploy Megrez2-Preview on representative device hardware (e.g., mobile SoC or edge device) and measure actual memory bandwidth utilization, prefetch success rates, and inference latency with pre-gated routing both enabled and disabled. Quantify real-world efficiency gains versus theoretical predictions.