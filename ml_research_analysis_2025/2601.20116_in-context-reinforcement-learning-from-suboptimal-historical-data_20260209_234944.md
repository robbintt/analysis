---
ver: rpa2
title: In-Context Reinforcement Learning From Suboptimal Historical Data
arxiv_id: '2601.20116'
source_url: https://arxiv.org/abs/2601.20116
tags:
- learning
- pretraining
- policy
- optimal
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of in-context reinforcement
  learning (ICRL) from suboptimal historical data, where standard autoregressive transformer
  training corresponds to imitation learning and yields suboptimal performance. The
  authors propose the Decision Importance Transformer (DIT) framework, which emulates
  the actor-critic algorithm in an in-context manner.
---

# In-Context Reinforcement Learning From Suboptimal Historical Data

## Quick Facts
- **arXiv ID:** 2601.20116
- **Source URL:** https://arxiv.org/abs/2601.20116
- **Reference count:** 40
- **Primary result:** Decision Importance Transformer (DIT) achieves superior in-context RL performance from suboptimal historical data, comparable to supervised DPT despite lacking optimal action labels.

## Executive Summary
This paper addresses in-context reinforcement learning (ICRL) from suboptimal historical data, where standard transformer training via imitation learning yields poor performance. The authors propose the Decision Importance Transformer (DIT) framework, which emulates actor-critic algorithms in-context. DIT first trains a transformer-based value function to estimate advantage functions of behavior policies, then trains a transformer-based policy via weighted maximum likelihood estimation where weights are constructed from the trained value function to steer suboptimal policies toward optimal ones. Experiments on bandit and MDP problems show DIT achieves superior performance, particularly when offline datasets contain suboptimal historical data, and is comparable to DPT despite being pretrained without optimal action labels.

## Method Summary
DIT addresses ICRL from suboptimal data through a two-stage training process. First, it trains transformer-based value estimators ($\hat{Q}$ and $\hat{V}$) to predict discounted returns from context datasets, computing advantage estimates $\hat{A} = \hat{Q} - \hat{V}$. Second, it trains a policy transformer via weighted maximum likelihood estimation, where weights are computed as $w = \exp(\hat{A}/\eta)$, effectively reweighting the loss to emphasize actions with higher advantage values. This approach theoretically steers the policy toward optimality while remaining close to the data distribution, approximating a KL-constrained optimization. The method is evaluated across linear bandits, dark room navigation, Miniworld, Meta-World, and Half-Cheetah environments using offline trajectories from suboptimal behavior policies.

## Key Results
- DIT outperforms standard transformer pretraining (behavioral cloning) on suboptimal data across all tested environments
- DIT achieves performance comparable to DPT (which uses optimal action labels) despite not having access to such labels
- The advantage of DIT is most pronounced when offline datasets contain highly suboptimal trajectories
- Performance gap between DIT and DPT widens in more complex environments like Meta-World compared to Half-Cheetah

## Why This Works (Mechanism)

### Mechanism 1: Exponential Advantage Reweighting
The framework replaces standard behavioral cloning with weighted maximum likelihood estimation, where actions are weighted by $w = \exp(A(s,a)/\eta)$. This upweights actions with higher advantage values (better than average for that state), pushing the policy toward high-utility regions while remaining grounded in historical data. The core assumption is that optimal actions appear with non-zero probability in the behavioral policy.

### Mechanism 2: In-Context Value Function Estimation
A transformer learns to estimate advantage functions for new tasks by modeling differences between action-values ($Q$) and state-values ($V$) based on provided interaction histories. This allows the system to compute weights for unseen tasks without gradient updates, assuming sufficient structural similarity across tasks.

### Mechanism 3: Implicit KL Constraint (Trust Region)
The weighted loss function is mathematically equivalent to optimizing a policy that maximizes advantage while constrained by KL-divergence against the behavioral policy. The weight $\eta$ acts as an inverse temperature, preventing policy collapse and distributional shift common in offline RL.

## Foundational Learning

- **Concept:** Advantage Actor-Critic (A2C)
  - **Why needed here:** DIT distills Actor-Critic into a transformer; understanding why we subtract $V(s)$ from $Q(s,a)$ (to reduce variance) and why this signals relative action goodness is crucial
  - **Quick check question:** If the Advantage function is always zero for a specific state, how would the DIT model behave regarding that state? (Answer: It would default to standard behavioral cloning/unweighted likelihood)

- **Concept:** Offline RL & Distributional Shift
  - **Why needed here:** Standard training fails on suboptimal data because the model learns to mimic bad actions; understanding this failure motivates the reweighting mechanism
  - **Quick check question:** Why does standard supervised learning on suboptimal data result in a suboptimal policy?

- **Concept:** In-Context Learning (ICL)
  - **Why needed here:** The model learns new tasks by conditioning on context datasets without weight updates; it must infer task identity and dynamics from provided interactions
  - **Quick check question:** How does the model handle a task it has never seen before without gradient descent? (Answer: It conditions predictions on embeddings of the context dataset provided as input)

## Architecture Onboarding

- **Component map:** Context Encoder -> Critic Branch ($\hat{Q}$ and $\hat{V}$) -> Weight Generator -> Actor Branch ($T_\theta$)

- **Critical path:**
  1. Stage 1 (Critic Training): Pre-train $\hat{Q}$ and $\hat{V}$ using MSE loss against discounted returns and Bellman consistency regularizers
  2. Stage 2 (Weight Computation): Freeze Critic, compute scalar weight $w = \exp((\hat{Q} - \hat{V})/\eta)$ for every sample
  3. Stage 3 (Actor Training): Train $T_\theta$ using weighted loss $w \cdot \log T_\theta(a|s)$

- **Design tradeoffs:**
  - Two-stage training enables theoretical justification but may be less expressive than end-to-end approaches
  - Temperature $\eta$ is critical: low $\eta$ risks optimism/overfitting, high $\eta$ approximates standard behavioral cloning

- **Failure signatures:**
  - Collapse to Mean: If advantage estimator is untrained, weights become uniform â†’ model performs like Behavior Cloning
  - Value Overestimation: Poor regularization causes critics to overestimate bad actions, degrading performance
  - Context Misalignment: Performance drops significantly when context dataset is from different task than query

- **First 3 experiments:**
  1. Train DIT and BC (weights fixed to 1) on same suboptimal data; plot performance gap
  2. Before training actor, plot predicted $\hat{V}$ vs ground-truth returns on validation set
  3. Train on tasks 1..M, test on M+1; compare against DPT to measure cost of no optimal labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical and empirical limits of data quality (ratio of optimal to random actions) required for DIT to successfully learn a near-optimal policy?
- Basis: Authors plan to explore weighted pretraining limits and note inferring optimal actions from random trajectories is unlikely
- Why unresolved: Experiments used mixed optimal/suboptimal data but failure threshold wasn't established
- Resolution: Ablation studies varying suboptimality ratio to identify failure threshold

### Open Question 2
- Question: Does performance gap between DIT and supervised baselines (like DPT) widen or narrow as model scale increases?
- Basis: Authors used GPT-2 due to "limited computation resource" and acknowledged larger models may improve performance
- Why unresolved: Unknown if lack of optimal labels is fundamental bottleneck or solvable with better estimation
- Resolution: Scaling laws analysis comparing DIT and DPT across varying parameter counts

### Open Question 3
- Question: Can the weighted pretraining framework close performance gap with DPT in highly complex environments without oracle optimal action labels?
- Basis: Performance gap larger in Meta-World than Half-Cheetah, suggesting absence of labels has greater impact in complex settings
- Why unresolved: Unclear if current weighting mechanism suffices for high-complexity tasks or if architectural changes needed
- Resolution: Improvements in architecture/weighting achieving parity with DPT on complex continuous control benchmarks

## Limitations
- Value estimation generalization assumes single meta-learned estimator can transfer across diverse task families without empirical quantification
- KL constraint approximation relies on $Z_\tau(s) \approx 1$ assumption justified via Taylor expansion but not validated
- Key hyperparameters ($\lambda$ in Eq. 8, exact suboptimal sampling strategy) are unspecified, limiting reproducibility
- Empirical superiority claims lack detailed ablation studies against standard baselines

## Confidence
- **High Confidence:** Weighted loss framework and KL-constrained optimization connection (Proposition 4.1) - mathematically sound if approximation holds
- **Medium Confidence:** In-context value estimation approach - conceptually similar to prior work but specific details sparse
- **Low Confidence:** Empirical claims without knowing critical hyperparameters or detailed ablation studies

## Next Checks
1. **Ablation on Value Estimation Quality:** Train DIT with randomly initialized vs pretrained advantage estimator; measure performance gap to isolate contribution of accurate value estimates
2. **Sensitivity to Temperature $\eta$:** Systematically vary $\eta$ and plot performance vs distributional shift Pareto frontier; identify optimal range or brittleness
3. **Cross-Domain Transfer:** Test DIT on task distribution disjoint from pretraining (e.g., train on locomotion, test on manipulation); measure performance degradation as function of task dissimilarity