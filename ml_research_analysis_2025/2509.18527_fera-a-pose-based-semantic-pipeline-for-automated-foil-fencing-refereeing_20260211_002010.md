---
ver: rpa2
title: 'FERA: A Pose-Based Semantic Pipeline for Automated Foil Fencing Refereeing'
arxiv_id: '2509.18527'
source_url: https://arxiv.org/abs/2509.18527
tags:
- blade
- pose
- move
- temporal
- fera-mdt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FERA, a pose-based semantic pipeline for automated
  foil fencing refereeing that converts broadcast video into structured action tokens
  and rule-grounded decisions. The method extracts 2D poses from monocular footage,
  converts them into 101-dimensional kinematic features, and applies an encoder-only
  transformer (FERA-MDT) to recognize multi-label footwork and blade actions for each
  fencer.
---

# FERA: A Pose-Based Semantic Pipeline for Automated Foil Fencing Refereeing

## Quick Facts
- arXiv ID: 2509.18527
- Source URL: https://arxiv.org/abs/2509.18527
- Reference count: 31
- Key outcome: Pose-based semantic pipeline converts broadcast video into structured tokens and rule-grounded referee decisions, achieving 77.7% priority accuracy on 969 exchanges.

## Executive Summary
FERA presents a pose-based semantic pipeline for automated foil fencing refereeing that bridges video understanding and rule-based reasoning. The method extracts 2D poses from monocular broadcast footage, converts them into a 101-dimensional kinematic representation, and applies an encoder-only transformer to recognize multi-label footwork and blade actions. To avoid fencer-specific logic, FERA processes each clip and a horizontally flipped copy, yielding time-aligned left/right predictions without requiring multi-person pose pipelines. These structured predictions serve as tokens for a language model that applies simplified right-of-way rules to generate textual decisions.

On 1,734 clips with 2,386 annotated actions, FERA achieves macro-F1 of 0.549 under 5-fold cross-validation, outperforming BiLSTM and TCN baselines. Combined with a language model, the full pipeline recovers referee priority with 77.7% accuracy on 969 exchanges. The work provides a case-study benchmark for pose-based semantic grounding in two-person sports and illustrates a general pipeline for connecting video understanding with rule-based reasoning.

## Method Summary
FERA processes monocular broadcast video through a three-stage pipeline: (1) detect-2D-pose-track each fencer using RTMDet-Tiny and RTMPose-M, run twice (original + flipped) to get canonical left-facing tracks; (2) engineer 101-dimensional kinematic features from 12 body joints (49 static + 52 motion dimensions) and apply FERA-MDT, an encoder-only transformer with per-class thresholds; (3) serialize predictions into tokens, retrieve rule snippets via FAISS, and pass to FERA-LM (DeepSeek-R1-Distill-Qwen-7B) to generate referee decisions. The system handles untrimmed tracks via dynamic temporal windowing and uses weighted BCE/CE losses with per-class calibration.

## Key Results
- FERA-MDT achieves macro-F1 of 0.549 for action recognition on 1,734 clips under 5-fold cross-validation
- Full pipeline recovers referee priority with 77.7% accuracy on 969 exchanges
- Ablation shows engineered 101D features improve macro-F1 from 0.534 (raw joints) and per-class thresholds add 0.038 macro-F1

## Why This Works (Mechanism)

### Mechanism 1
Processing the original clip and a horizontally flipped copy yields consistent left-facing representations for both fencers, eliminating the need for multi-person pose pipelines. A single-fencer detection and pose extraction pipeline runs twice—once on the original footage, once on the mirrored version. Each output is mapped to a canonical left-facing coordinate system. This produces two time-aligned pose tracks sharing the same frame indices, enabling downstream comparison without identity tracking or cross-person synchronization.

### Mechanism 2
Engineered 101-dimensional kinematic features provide fencing-specific inductive biases that improve recognition over raw joint coordinates alone. Each frame is encoded as 49 static dimensions (normalized joints, pairwise distances, joint angles, torso orientation, arm extension) plus 52 motion dimensions (first- and second-order finite differences on joints and center of mass). This explicitly captures blade-relevant cues like arm extension and attack timing without direct blade detection.

### Mechanism 3
Structured action tokens plus simplified right-of-way rules enable a language model to recover referee priority decisions with 77.7% accuracy, despite moderate token-level recognition quality. FERA-MDT outputs ordered segment lists with multi-label move sets and blade-line positions for each fencer. These are serialized into text, combined with hit labels from scoreboard signals, and passed to FERA-LM with retrieved rule snippets. The LM applies rule logic to output a priority decision and justification.

## Foundational Learning

- **Multi-label temporal classification with class imbalance**: Fencing actions co-occur (step + beat) and follow long-tail distributions; per-class thresholding is critical. Quick check: Given a segment with probabilities [step: 0.72, beat: 0.48, parry: 0.31] and tuned thresholds [0.5, 0.4, 0.6], which actions are predicted?
- **Encoder-only transformer pooling for variable-length sequences**: FERA-MDT uses masked mean pooling over frame embeddings to produce a sequence-level representation. Quick check: If frames 10–15 are masked out (invalid), how does the pooled representation change compared to using all frames?
- **Probability calibration for downstream decision-making**: Uncalibrated probabilities can mislead threshold selection and rule-reasoning confidence. Quick check: A model outputs probability 0.85 for "lunge" but the true positive rate at that confidence is only 0.60. Is this overconfident or underconfident?

## Architecture Onboarding

- **Component map**: Input video -> RTMDet-Tiny + RTMPose-M -> Norfair tracking -> 101-dim kinematic features -> FERA-MDT transformer -> dynamic windowing -> token serialization -> FAISS rule retrieval -> FERA-LM -> priority decision
- **Critical path**: Correct fencer track selection in original and flipped views (Section 4.1 foreground scoring) -> Per-class threshold tuning on validation data -> Valid segment boundary detection via dynamic windowing (Section 4.3) -> Token serialization format that the LM can parse reliably (Section 4.5)
- **Design tradeoffs**: 2D pose vs. 3D pose (simpler but loses depth information; blade orientation must be inferred implicitly) vs. single-fencer flip approach vs. multi-person pose (avoids cross-person association complexity but doubles inference cost) vs. off-the-shelf LM vs. fine-tuned reasoning (faster iteration, no explanation supervision data)
- **Failure signatures**: Identity swap (left/right predictions correspond to wrong fencers -> inverted priority decisions) -> Over-segmentation (dynamic windowing produces many short segments with noisy labels) -> Missing rare actions (flèche, positions 4/7/other) -> Calibration drift (thresholds tuned on validation become suboptimal on new competitions)
- **First 3 experiments**: Validate pose extraction and flip consistency (run detection–pose–track on 10 annotated clips, verify opposite fencers selected) -> Threshold sensitivity analysis (sweep thresholds on validation, plot F1 vs. threshold, replicate −0.038 macro-F1 gap) -> Token-level error injection test (corrupt ground-truth tokens, measure priority accuracy drop, quantify robustness margin)

## Open Questions the Paper Calls Out

### Open Question 1
Can fencing-specific perception modules (e.g., 3D pose estimation or explicit blade detection) significantly improve robustness to camera viewpoint and occlusion compared to the current generic 2D pose approach? The authors suggest this could improve robustness but leave it untested, as the current architecture deliberately infers blade-line implicitly from body dynamics.

### Open Question 2
How would the collection of ground-truth textual explanations from referees impact the semantic quality and accuracy of the FERA-LM reasoning module? The current FERA-LM uses an off-the-shelf model with lightweight rule retrieval; the potential gains from fine-tuning on actual referee justifications remain unknown.

### Open Question 3
To what degree does the lack of athlete-disjoint data splits inflate the reported macro-F1 and priority accuracy scores? Without ensuring the model is tested on fencers it has never seen during training, it is difficult to determine if the system has learned general fencing semantics or merely memorized individual movement patterns.

## Limitations
- Identity swaps due to rapid lateral movement or occlusions can corrupt downstream reasoning
- Rare classes (flèche, positions 4/7/other) are poorly represented, risking critical decision errors
- Simplified right-of-way rules may not cover all refereeing nuances, and the language model's reasoning is untested against edge-case rule interpretations

## Confidence

- **High Confidence**: Engineered 101D kinematic features improve recognition over raw joints (macro-F1 0.549 vs. 0.534); calibration and per-class threshold tuning significantly boost performance; end-to-end pipeline recovers referee priority with 77.7% accuracy
- **Medium Confidence**: Flip-based single-fencer pipeline reliably yields canonical left-facing representations; simplified rule subset captures dominant decision factors for right-of-way
- **Low Confidence**: Method generalizes to new competitions with different camera angles or fencer behaviors; language model robustly applies rules without fine-tuning on explanation data; rare but decisive actions are reliably detected

## Next Checks

1. **Validate pose extraction and flip consistency**: Run the detection–pose–track pipeline on 10 manually annotated clips. Verify that original and flipped views correctly select opposite fencers and that frame indices align. Flag any identity swaps or tracking losses.

2. **Threshold sensitivity analysis**: For each move class, sweep decision thresholds on held-out validation and plot F1 vs. threshold. Compare macro-F1 using per-class tuned thresholds vs. a fixed 0.5 threshold. Replicate the −0.038 gap from Table 3 as a sanity check.

3. **Token-level error injection test**: Starting from ground-truth annotations, artificially corrupt move/blade tokens (e.g., drop 10% of actions, flip 5% of blade positions) and measure the resulting priority accuracy drop. Quantify robustness margin between 0.549 macro-F1 and 77.7% end-to-end accuracy.