---
ver: rpa2
title: Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems
arxiv_id: '2509.20989'
source_url: https://arxiv.org/abs/2509.20989
tags:
- loss
- items
- teacher
- student
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes Cross-Entropy (CE) loss in knowledge distillation\
  \ (KD) for recommender systems. It proves that CE loss maximizes a lower bound of\
  \ NDCG on a given item subset, but only if the subset satisfies an assumption of\
  \ closure\u2014requiring the student\u2019s top items to be included."
---

# Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems

## Quick Facts
- arXiv ID: 2509.20989
- Source URL: https://arxiv.org/abs/2509.20989
- Reference count: 40
- Primary result: RCE-KD outperforms state-of-the-art KD methods in both homogeneous and heterogeneous settings

## Executive Summary
This paper addresses limitations in using Cross-Entropy (CE) loss for knowledge distillation (KD) in recommender systems. The authors prove that CE loss maximizes a lower bound of NDCG on item subsets only when the student's top items include the teacher's top itemsâ€”an assumption often violated in practice. To overcome this, they propose Rejuvenated Cross-Entropy for Knowledge Distillation (RCE-KD), which splits teacher's top items into two subsets based on student rankings and applies adaptive loss combinations.

## Method Summary
RCE-KD introduces a novel approach to knowledge distillation by addressing the theoretical limitations of standard CE loss. The method first identifies the teacher's top-ranked items and divides them into two subsets: those also ranked highly by the student, and those not. For items in the first subset, CE loss is applied directly. For items in the second subset, a sampling strategy approximates the closure assumption required for CE loss effectiveness. The losses from both subsets are then adaptively combined based on their relative sizes, creating a more robust distillation framework that maintains theoretical guarantees while addressing practical limitations.

## Key Results
- RCE-KD significantly outperforms state-of-the-art KD methods in both homogeneous and heterogeneous settings
- The proposed method addresses the practical violation of closure assumptions in CE loss for KD
- Adaptive combination of losses based on subset sizes improves overall recommendation performance

## Why This Works (Mechanism)
The mechanism works by recognizing that standard CE loss has theoretical guarantees (maximizing NDCG lower bound) only under specific conditions. By splitting items into subsets where these conditions are partially met and applying different strategies accordingly, RCE-KD maintains the benefits of CE loss where applicable while mitigating its limitations elsewhere. The adaptive combination ensures that the method responds to the actual distribution of items rather than relying on a one-size-fits-all approach.

## Foundational Learning

**Knowledge Distillation**: A technique where a smaller "student" model learns from a larger "teacher" model. Needed because training large models from scratch is computationally expensive. Quick check: Can the student model achieve comparable performance to the teacher with fewer parameters?

**NDCG (Normalized Discounted Cumulative Gain)**: A ranking metric that emphasizes the importance of relevant items appearing at higher positions. Needed because recommender systems prioritize top-ranked items. Quick check: Does the method improve the ranking of truly relevant items in top positions?

**Closure Assumption**: The theoretical requirement that the student's top items must include the teacher's top items for CE loss to maximize NDCG bounds. Needed to understand the limitations of standard CE loss in KD. Quick check: How often is this assumption violated in practice across different datasets?

## Architecture Onboarding

**Component Map**: Student Model -> RCE-KD Module -> Teacher Model's Top Items -> Subset Division -> Adaptive Loss Combination

**Critical Path**: The core process flows from the teacher's top items through subset identification, where items are classified based on student rankings. The sampling strategy for the second subset and the adaptive combination of losses form the critical decision points that determine the final distilled model's performance.

**Design Tradeoffs**: The method trades computational overhead (additional subset processing and sampling) for improved theoretical guarantees and practical performance. The adaptive combination introduces complexity but enables better handling of real-world scenarios where assumptions are violated.

**Failure Signatures**: Performance degradation may occur when the student model has fundamentally different preferences from the teacher, making subset division less meaningful. The sampling strategy could introduce bias if not properly calibrated, and the adaptive combination might overweight poorly performing subsets.

**First Experiments**:
1. Compare RCE-KD performance against standard CE loss on datasets where closure assumption is violated
2. Test different sampling strategies for the second subset to identify optimal approaches
3. Evaluate the impact of subset size ratios on adaptive loss combination effectiveness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the general applicability of the closure assumption in real-world scenarios.

## Limitations
- The closure assumption remains a theoretical limitation that may not hold consistently across all datasets
- The adaptive combination of losses based on subset sizes may not always yield optimal results depending on item distributions
- The sampling strategy for the second subset may introduce bias or noise affecting overall performance

## Confidence

High: The theoretical analysis proving that CE loss maximizes a lower bound of NDCG under the closure assumption is well-founded.

Medium: The effectiveness of the RCE-KD method in outperforming state-of-the-art KD methods is supported by experimental results, but the extent of improvement may vary across different datasets and scenarios.

Low: The practical applicability of the closure assumption in real-world recommender systems is uncertain, as it may not hold consistently across all datasets and use cases.

## Next Checks

1. Conduct experiments on a diverse set of datasets to evaluate the robustness of RCE-KD across different domains and user behaviors.

2. Investigate the impact of different sampling strategies on the performance of RCE-KD to determine the most effective approach for the second subset.

3. Analyze the theoretical conditions under which the closure assumption holds and explore alternative assumptions or relaxations that could enhance the applicability of CE loss in KD.