---
ver: rpa2
title: 'CLAX: Fast and Flexible Neural Click Models in JAX'
arxiv_id: '2511.03620'
source_url: https://arxiv.org/abs/2511.03620
tags:
- click
- clax
- document
- optimization
- clicks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLAX is a JAX-based library that implements classic click models
  using gradient-based optimization, replacing traditional EM methods. It enables
  modern deep learning frameworks while preserving the interpretability of classic
  probabilistic graphical models.
---

# CLAX: Fast and Flexible Neural Click Models in JAX

## Quick Facts
- arXiv ID: 2511.03620
- Source URL: https://arxiv.org/abs/2511.03620
- Reference count: 40
- Primary result: Gradient-based optimization of probabilistic click models in JAX achieves orders of magnitude faster training than traditional EM methods while preserving interpretability.

## Executive Summary
CLAX is a JAX-based library that implements classic click models using gradient-based optimization instead of traditional EM methods. It achieves significant speed improvements, training over 1 billion user sessions in approximately 2 hours on a single GPU, while maintaining the interpretability of classic probabilistic graphical models. The framework enables integration of embeddings, deep networks, and custom modules into classic click models for end-to-end optimization, demonstrating that neural parameterizations can surpass widely-used two-tower models in ranking performance.

## Method Summary
CLAX replaces traditional EM-based optimization of probabilistic click models with direct gradient-based optimization using automatic differentiation. The library computes gradients of the marginal log-likelihood directly via JAX's automatic differentiation, leveraging mini-batch processing for scalability. All probability computations are performed in log-space using specialized numerical tricks (log-sum-exp, log1mexp) to prevent underflow, overflow, and catastrophic cancellation. The framework decouples model structure from parameterization, allowing flexible composition of models while maintaining interpretable latent variable structure.

## Key Results
- Trains over 1 billion user sessions in approximately 2 hours on a single GPU, orders of magnitude faster than traditional EM approaches
- Implements ten classic click models with modular design enabling integration of embeddings, deep networks, and custom modules
- Neural parameterizations of sophisticated models can surpass widely-used two-tower models in ranking performance

## Why This Works (Mechanism)

### Mechanism 1
Direct gradient-based optimization of the marginal log-likelihood can replace EM for training PGM-based click models while achieving comparable or better performance. Instead of alternating between E-steps and M-steps, CLAX computes gradients of the marginal log-likelihood directly via automatic differentiation. The mathematical equivalence shows that a single gradient step with current parameters is equivalent to direct marginal likelihood optimization. This approach exploits mini-batch processing, which scales well to large datasets.

### Mechanism 2
Performing all probability computations in log-space with specialized numerical tricks prevents underflow, overflow, and catastrophic cancellation during gradient optimization. CLAX computes log-probabilities using log-sum-exp for stable addition, and log1mexp for stable complement computation. This avoids multiplying small probabilities (underflow) and subtracting nearly equal floats (cancellation), which are common sources of numerical instability in click model training.

### Mechanism 3
Decoupling model structure from parameterization enables flexible customization while preserving interpretability of classic PGMs. CLAX separates the probabilistic model logic from parameter modules, allowing any Flax module matching expected output shapes to substitute default embeddings. This allows gradient-based end-to-end training while maintaining interpretable latent variable structure, giving practitioners both the interpretability of PGMs and the predictive power of neural networks.

## Foundational Learning

- **Probabilistic Graphical Models (PGMs) with latent variables**: Click models represent unobservable user states (examination, satisfaction) as latent variables; understanding conditional independence assumptions is essential for model selection. *Quick check: Can you explain why the PBM assumes clicks at different positions are independent, while cascade models do not?*

- **Expectation Maximization vs. Gradient Descent**: CLAX's core innovation is replacing EM; understanding what EM does (impute missing values, maximize) clarifies why gradient descent is faster but lacks monotonic convergence guarantees. *Quick check: What does the E-step compute, and why does EM guarantee monotonic improvement while gradient descent does not?*

- **Log-space computation and numerical stability**: Implementing click model likelihoods requires log-sum-exp and log1mexp; not understanding these leads to NaN losses during training. *Quick check: Why does computing log(1 - exp(log p)) directly cause numerical instability, and what is the solution?*

## Architecture Onboarding

- **Component map**: Model classes (PBM, UBM, DBN, etc.) -> Parameter configs (EmbeddingParameterConfig, LinearParameterConfig, DeepCrossParameterConfig) -> Trainer -> Metrics -> MixtureModel

- **Critical path**: 1. Choose model class (e.g., UserBrowsingModel) -> 2. Configure parameterization (embeddings vs. features) -> 3. Initialize Trainer with optimizer -> 4. Call trainer.train() with data loaders -> 5. Evaluate with metrics.compute()

- **Design tradeoffs**: Embeddings vs. features (memorization vs. generalization); Compression ratio (memory savings vs. perplexity degradation at extreme ratios); Conditional vs. unconditional perplexity (sequential browsing assumption vs. flexibility)

- **Failure signatures**: NaN losses (numerical instability in log-space operations); Training time explosion on CPU (dense gradient computation for large embedding tables); Wrong model rankings with compression (high compression changes model ordering); Mismatched conditional/unconditional predictions (using conditional perplexity for UBM/DCM on non-sequential browsing data)

- **First 3 experiments**: 1. Reproduce PyClick comparison on WSCD-2012 subset (10M sessions) to validate gradient-based optimization produces equivalent results; 2. Test embedding compression impact on Baidu-ULTR subset with hashing-trick at different compression ratios to find sweet spot; 3. Build a two-tower feature-based model with DeepCrossV2 for attractiveness and linear features for examination to determine if feature generalization improves ranking

## Open Questions the Paper Calls Out
- How does the implementation of sparse embeddings in future versions affect computational efficiency and memory usage for CPU-based training compared to the current dense implementation?
- Can the CLAX framework effectively integrate and optimize non-PGM neural click models, such as attention-based or recurrent architectures, without sacrificing the modularity designed for classic models?
- Are the relative performance rankings between different click models robust to hyperparameter optimization, or are they artifacts of the default AdamW configuration used in the study?
- Does the lack of monotonic improvement guarantees in gradient-based optimization lead to convergence instability or suboptimal local minima in specific data regimes where EM is traditionally more stable?

## Limitations
- Scalability to extreme vocabulary sizes remains untested, with 100x compression experiments showing degraded perplexity
- Distribution shift sensitivity is not evaluated; models trained on commercial search data may fail on specialized domains
- Optimization stability for deep parameterizations is not thoroughly investigated, introducing uncertainty about finding global optima

## Confidence
- **High confidence**: Core claim of gradient-based optimization replacing EM is well-supported by mathematical derivation and empirical validation against PyClick
- **Medium confidence**: Ranking performance claims rely on comparison against two-tower models without detailed hyperparameter tuning for all models
- **Low confidence**: Claims about preserving interpretability while enabling end-to-end optimization require more scrutiny and quantification

## Next Checks
1. **Cross-dataset generalization test**: Train CLAX models on AOL data and evaluate on Baidu-ULTR (and vice versa) to measure performance degradation and quantify transfer learning capabilities
2. **Convergence analysis for deep parameterizations**: Systematically compare training curves of embedding-based versus DeepCrossV2 parameterizations across multiple random seeds to measure final log-likelihood and training stability
3. **Ablation study on numerical stability**: Implement a "naive" version of each click model without log-space computations and compare training success rates, convergence speed, and final performance to quantify the practical impact of numerical stability improvements