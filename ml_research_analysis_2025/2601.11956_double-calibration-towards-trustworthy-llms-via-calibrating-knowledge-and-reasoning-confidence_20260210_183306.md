---
ver: rpa2
title: 'Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and
  Reasoning Confidence'
arxiv_id: '2601.11956'
source_url: https://arxiv.org/abs/2601.11956
tags:
- confidence
- evidence
- reasoning
- calibration
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DoublyCal, a framework that implements double-calibration
  for KG-augmented LLMs by jointly calibrating the confidence of retrieved Knowledge
  Graph evidence and the final LLM predictions. The core idea is to use a lightweight
  proxy model to generate KG evidence alongside a calibrated evidence confidence score,
  which then guides a black-box LLM to produce more accurate and better-calibrated
  predictions with traceable confidence.
---

# Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence

## Quick Facts
- arXiv ID: 2601.11956
- Source URL: https://arxiv.org/abs/2601.11956
- Reference count: 17
- Primary result: DoublyCal achieves F1 up to 76.8 and ECE down to 3.5 on KGQA benchmarks while being highly cost-effective

## Executive Summary
DoublyCal introduces a double-calibration framework for KG-augmented LLMs that jointly calibrates knowledge graph evidence confidence and final LLM predictions. The core innovation uses a lightweight proxy model to generate KG evidence with calibrated confidence scores, which then guide a black-box LLM to produce more accurate and better-calibrated answers. Experiments on WebQSP and CWQ datasets show significant improvements in both accuracy (F1 scores up to 76.8) and calibration (ECE down to 3.5) while maintaining cost-effectiveness across diverse black-box LLMs.

## Method Summary
DoublyCal employs a two-stage training pipeline for a lightweight proxy model (Llama2-7B). First, supervised fine-tuning (SFT) trains the proxy to generate constrained relational paths with Bayesian confidence scores using a Jeffreys prior. Second, reinforcement learning with GRPO optimizes a reward combining inferential quality and calibration alignment. The proxy generates evidence-confidence pairs that are verbalized into prompts for black-box LLMs, creating a traceable confidence propagation chain. The framework uses A* search for path extraction, top-K=3 inference, and verbalized uncertainty quantification methods.

## Key Results
- Achieves F1 scores up to 76.8 on WebQSP and CWQ benchmarks
- Reduces Expected Calibration Error (ECE) to as low as 3.5, compared to 10.6-38.8 for baselines
- Maintains robust performance across GPT-3.5-turbo, GPT-4o-mini, DeepSeek-V3, and Gemini-2.5-flash
- Demonstrates cost-effectiveness through lightweight proxy model and black-box LLM integration

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Confidence Estimation for KG Evidence
A Beta-Bernoulli model assigns statistically grounded confidence scores to retrieved KG evidence paths. Each evidence path is treated as a Bernoulli trial where the MAP estimate blends empirical accuracy with a Jeffreys prior (α=β=0.5), yielding robust posterior means that mitigate overconfidence propagation. This creates a reliable uncertainty anchor for the calibration chain.

### Mechanism 2: Two-Stage Proxy Training for Evidence-Calibration Alignment
Stage 1 trains Llama2-7B to generate structured evidence sequences with Bayesian confidence supervision. Stage 2 uses GRPO to optimize a reward combining inferential quality (F1 × match score) and calibration alignment (|predicted confidence – target confidence|). This decouples evidence generation from the primary LLM while aligning evidence quality with calibration goals.

### Mechanism 3: Traceable Confidence Propagation via Evidence-Anchored Prompting
Evidence-confidence pairs are verbalized into natural language and injected into prompts for black-box LLMs. The primary LLM produces final answers with self-reported confidence via verbalized UQ methods. This creates a two-stage calibration chain where externally verifiable evidence confidence informs final verbalized confidence, enabling traceability.

## Foundational Learning

- **Bayesian Inference with Conjugate Priors**
  - Why needed here: The Beta-Bernoulli model for evidence confidence estimation requires understanding how priors combine with likelihoods to produce posterior distributions
  - Quick check question: Given α=0.5, β=0.5, and evidence that grounds to 4 candidates with 3 correct, what is the posterior mean confidence?

- **Expected Calibration Error (ECE)**
  - Why needed here: The primary evaluation metric for calibration quality; measures the weighted gap between predicted confidence and empirical accuracy across confidence bins
  - Quick check question: If a model assigns 0.9 confidence to 100 predictions that are 70% accurate, what is the ECE contribution from this bin?

- **Knowledge Graph Triple Structure and Relational Paths**
  - Why needed here: Evidence is formalized as constrained relational paths (h, r, t) sequences; understanding KG topology is essential for implementing the evidence retrieval and grounding mechanisms
  - Quick check question: For the query "Snoopy's brother," what relational path would filter candidates by gender constraint?

## Architecture Onboarding

- **Component map**: KG Store -> Evidence Generator (Proxy) -> Bayesian Calibration Module -> Prompt Constructor -> Primary LLM Reasoner

- **Critical path**: 
  1. Query entity linking → Bayesian evidence path generation → Proxy model generates evidence-confidence pairs
  2. Evidence grounding in KG → Candidate answer extraction
  3. Prompt construction with evidence-confidence context → Black-box LLM inference
  4. Verbalized confidence extraction from LLM output → Final answer-confidence pairs

- **Design tradeoffs**:
  - Constrained vs. unconstrained relational paths: Constrained paths improve precision but may miss valid answers lacking constraint relations
  - Proxy model size vs. capability: Llama2-7B balances cost and evidence quality
  - RL reward weight (λ): Higher λ (0.85) emphasizes evidence quality over calibration
  - Number of evidence paths (K=3): More paths provide richer context but increase token costs

- **Failure signatures**:
  - ECE spike without accuracy drop: Evidence confidence is being generated but not influencing final confidence
  - Low F1 with low ECE: Proxy generating low-confidence evidence for correct answers
  - High token cost with low F1: Evidence paths are verbose but uninformative
  - Cross-model inconsistency: DoublyCal works for GPT-3.5 but not Gemini

- **First 3 experiments**:
  1. Sanity check: Run proxy on WebQSP validation set; verify evidence grounding produces non-empty candidate sets for >90% of queries
  2. Ablation baseline: Compare DoublyCal vs. SingleCal on 100-sample subset; target ECE reduction of at least 10 points
  3. Cross-model validation: Test DoublyCal with GPT-3.5-turbo and GPT-4o-mini; confirm ECE <10 for both models

## Open Questions the Paper Calls Out

- Can DoublyCal be effectively extended to open-ended question answering or creative generation tasks where clear reasoning paths are not available?
- How can KG evidence and confidence scores be dynamically updated in evolving knowledge environments?
- How robust is DoublyCal when the underlying knowledge graph is significantly incomplete or contains noisy/incorrect triples?

## Limitations

- Performance fundamentally depends on the quality of the underlying KG, which may be incomplete or noisy
- The framework focuses on well-defined KGQA tasks with clear reasoning paths, limiting extension to open-ended tasks
- Black-box LLM reliance means calibration effectiveness depends on verbalized uncertainty quantification responsiveness

## Confidence

- KG Evidence Calibration (High): The Bayesian MAP estimation with Jeffreys prior is mathematically sound and well-specified
- Two-Stage Proxy Training (Medium): The SFT+RL pipeline is clearly described but implementation details are sparse
- Traceable Confidence Propagation (Medium): The verbalized uncertainty mechanism is common in LLM calibration but assumes verbalized confidence equals reasoning confidence
- Cost-Effectiveness Claims (Low): Token counts are reported but actual API costs and latency comparisons are not measured

## Next Checks

1. **KG Grounding Validation**: Run the proxy on 50 WebQSP validation samples and manually verify that generated evidence paths correctly ground to Freebase triples and produce non-empty candidate answer sets. Target >90% successful grounding.

2. **Calibration-Ablation Fidelity**: Implement a strict SingleCal baseline (remove evidence confidence from prompts) and run on 100 test samples. Verify DoublyCal achieves at least 10-point ECE reduction without F1 degradation >2 points.

3. **Cross-Model Robustness Test**: Test DoublyCal with two black-box models (GPT-3.5 and Gemini) on the same 50-query subset. Confirm both achieve ECE <10 and compare verbalized confidence distributions to assess model-dependent behavior.