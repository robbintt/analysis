---
ver: rpa2
title: A Survey of QUD Models for Discourse Processing
arxiv_id: '2502.15573'
source_url: https://arxiv.org/abs/2502.15573
tags:
- discourse
- questions
- question
- which
- quds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey reviews three models for implementing the Question
  Under Discussion (QUD) framework in discourse processing: the QUD-tree approach,
  expectation-driven approach, and dependency-based approach. The QUD-tree approach
  uses hierarchical structures based on linguistic constraints to organize discourse
  segments, while the expectation-driven approach annotates questions evoked at discourse
  units and their answeredness in subsequent context.'
---

# A Survey of QUD Models for Discourse Processing

## Quick Facts
- arXiv ID: 2502.15573
- Source URL: https://arxiv.org/abs/2502.15573
- Authors: Yingxue Fu
- Reference count: 12
- Primary result: Survey of three QUD models (QUD-tree, expectation-driven, dependency-based) for discourse processing

## Executive Summary
This survey provides a comprehensive overview of Question Under Discussion (QUD) models for discourse processing, reviewing three distinct approaches to implementing the QUD framework. The QUD-tree approach uses hierarchical structures with linguistic constraints, the expectation-driven approach tracks question answeredness in local context, and the dependency-based approach creates sentence-to-sentence question-answer links. The paper examines how these models relate to mainstream discourse frameworks (RST, PDTB, SDRT) and discusses current benchmarks and annotation protocols.

## Method Summary
The survey synthesizes existing literature on QUD models through systematic analysis of three distinct approaches: the constraint-based QUD-tree method, the expectation-driven annotation protocol, and the dependency-based sentence-linking framework. Each approach is examined through its theoretical foundations, annotation procedures, and empirical results from published studies.

## Key Results
- QUD-tree approach applies four linguistic constraints to infer implicit questions, achieving moderate inter-annotator agreement (κ=0.52)
- Expectation-driven approach detects weak correlation between question reliability and answeredness in TED-Q corpus
- Dependency-based approach creates sentence-level Q-A pairs with 41.8% high similarity and 40.7% semantic difference in annotator responses

## Why This Works (Mechanism)

### Mechanism 1: QUD-Tree Hierarchical Constraint Propagation
The QUD-tree approach uses four linguistic constraints to reduce the search space of possible questions: Q-A-Congruence (questions must be answerable by dominated segments), Q-Givenness (implicit QUDs contain only given materials), Maximize-Q-Anaphoricity (maximize given material in QUDs), and Back-to-the-Roots (right frontier attachment). These constraints transform unconstrained generation into a constrained inference task following cooperative principles.

### Mechanism 2: Expectation-Driven Question Answeredness Tracking
Annotators incrementally view text excerpts and generate questions at probe points without seeing future context. Questions are tracked across subsequent probe points for answeredness on a 5-point Likert scale. The correlation between question reliability (inter-annotator semantic similarity) and answeredness validates QUD identification, though limited to two consecutive probe points.

### Mechanism 3: Dependency-Based Anchor-Question Linking
Each sentence S (except first) connects to a previous anchor sentence A through a free-form question Q that S answers, creating a dependency graph rather than a tree. The approach jointly predicts anchor sentences and generates connecting questions, though it lacks hierarchical depth and struggles with anchor prediction ambiguity.

## Foundational Learning

- **Rhetorical Structure Theory (RST) / PDTB / SDRT**: Why needed - QUD is positioned as an alternative to mainstream coherence-relation frameworks; prerequisite for comparative evaluation. Quick check - Can you explain why RST distinguishes nuclearity while QUD does not?

- **Information Structure (Focus/Background/Contrastive Topic)**: Why needed - QUD-tree annotation requires labeling focus domains; Q-A-Congruence constraint depends on focus analysis matching question alternative sets. Quick check - In "Mary invited nobody," what changes in the focal alternative set if "nobody" vs. "Mary" is the focus?

- **Question Semantics (Q-Alternative Sets)**: Why needed - Roberts' theory defines questions as partitioning possible worlds into alternative sets; evaluating answer completeness requires this formalization. Quick check - What is the q-alternative set for "Who did Mary invite?" given P={Mary, Alice, Grace}?

## Architecture Onboarding

- **Component map**:
  - QUD-tree: Segmenter → Elision Reconstructor → QUD Inferrer → Tree Builder → Info Structure Annotator (F/BG/CT/non-at-issue)
  - Expectation-driven: Context Window → Question Generator → Answeredness Tracker → Reliability Scorer
  - Dependency-based: Anchor Predictor → Question Generator → Reranker (pipeline per Ko et al. 2023) OR Joint Predictor (instruction-tuned per Suvarna et al. 2024)

- **Critical path**:
  1. Choose approach based on task: QUD-tree for hierarchical discourse structure, dependency-based for sentence-level QA links, expectation-driven for local coherence evaluation
  2. For dependency-based: Start with DCQA corpus (22,394 Q-A pairs, 606 articles) → train anchor predictor → train question generator → evaluate with QUDEVAL criteria
  3. For QUD-tree: Apply Riester's four constraints to segment-level question inference; validate via PARSEVAL against RST structures (74% similarity per Shahmohammadi et al.)

- **Design tradeoffs**:
  - QUD-tree: Theoretically grounded but moderate inter-annotator agreement (κ=0.52); captures hierarchy but struggles with Contrast/Concession relations
  - Expectation-driven: Simpler annotation but limited to local relations; no global structure
  - Dependency-based: Practical for QA tasks but shallow hierarchy; anchor prediction ambiguity

- **Failure signatures**:
  - Low question reliability scores indicate unconstrained generation (expectation-driven)
  - QUD-tree produces invalid structures when non-at-issue content is mishandled
  - Dependency parser predicts anchors with low semantic relatedness to generated questions

- **First 3 experiments**:
  1. Reproduce DCQA annotation protocol on 10 news articles; measure anchor agreement and question similarity to establish baseline variance
  2. Compare QUD-tree vs. RST parsing on parallel-annotated German podcast transcripts; quantify structural divergence on monologue vs. dialogue
  3. Fine-tune instruction-tuned LLM on QSALIENCE-data (1,766 questions with salience ratings); evaluate correlation between predicted salience and human QUD judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hierarchical relationships between QUDs be effectively captured in the dependency-based approach?
- Basis in paper: Section 2.3 states, "A question that remains to be studied with this approach is how to capture the hierarchical relationship between QUDs," noting that current dependency links result in a shallower structure than the QUD-tree approach.
- Why unresolved: The dependency-based model (Ko et al., 2022) currently relies on flat dependency links between sentences, lacking the nested tree structure required to represent subordination between high-level and low-level questions.
- What evidence would resolve it: A modified parsing algorithm or annotation schema that successfully integrates vertical depth (subquestions) into the horizontal dependency structure without losing the model's simplicity.

### Open Question 2
- Question: Do specific question types (e.g., "why" questions) map consistently to coherence relations (e.g., causal relations) across different frameworks?
- Basis in paper: Section 5 highlights that "it remains an under-studied question whether QUDs represent the same type of discourse information as that encoded by discourse relations," specifically asking if "why questions encode causal relations consistently."
- Why unresolved: While Westera et al. (2020) found correlations between why-questions and causal relations, the paper notes that mapping other relations like Background or Restatement to QUDs is "not straightforward" (Section 4.1).
- What evidence would resolve it: A comprehensive parallel corpus study showing statistical mapping rules between specific QUD types and RST/PDTB/SDRT relation labels.

### Open Question 3
- Question: Can a two-step annotation process (elicitation followed by categorization) improve inter-annotator agreement for the QUD-tree approach?
- Basis in paper: Section 5 notes that "it is still challenging to achieve high inter-annotator agreement" and suggests future work could adopt "a two-step approach... where QUDs are elicited first and then categorized."
- Why unresolved: The free-form nature of questions leads to variability (Section 1), and current results show only moderate agreement (Cohen's Kappa 0.52) for the QUD-tree approach.
- What evidence would resolve it: An empirical study comparing agreement scores between direct tree-annotation and a two-step pipeline using predefined question templates.

## Limitations

- Moderate inter-annotator agreement (κ=0.52) for QUD-tree annotation suggests inherent subjectivity in implicit question reconstruction
- Expectation-driven approach's reliance on local context limits its ability to capture long-distance discourse relations
- Dependency-based approach sacrifices hierarchical depth for simplicity, leaving open questions about representing nested discourse structures

## Confidence

- Theoretical foundations: High - Clear articulation of Roberts' framework and relationship to established discourse theories
- Comparative analysis: Medium - Relies primarily on existing literature rather than direct experimental validation
- Practical applicability: Low - Limited empirical validation across diverse discourse genres and domains

## Next Checks

1. Conduct parallel annotation experiments on the same corpus using all three QUD approaches to enable direct performance comparison
2. Test the expectation-driven approach's ability to handle discourse-level phenomena like cataphora and topic shift
3. Evaluate the dependency-based approach on genres beyond news articles (e.g., narrative fiction, academic writing) to assess generalizability