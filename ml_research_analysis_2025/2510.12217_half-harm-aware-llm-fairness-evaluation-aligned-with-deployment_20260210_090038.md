---
ver: rpa2
title: 'HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment'
arxiv_id: '2510.12217'
source_url: https://arxiv.org/abs/2510.12217
tags:
- bias
- across
- fairness
- gender
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HALF introduces a harm-aware evaluation framework that assesses
  LLM fairness across nine application domains by weighting outcomes according to
  real-world consequences. Using a five-stage pipeline, HALF aggregates fairness scores
  with domain-specific harm weights (Severe: 3, Moderate: 2, Mild: 1), enabling deployment-relevant
  comparisons.'
---

# HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment

## Quick Facts
- arXiv ID: 2510.12217
- Source URL: https://arxiv.org/abs/2510.12217
- Reference count: 40
- One-line primary result: Evaluates LLM fairness across nine domains with harm-weighted aggregation showing no universally fair model exists

## Executive Summary
HALF introduces a harm-aware evaluation framework that assesses LLM fairness across nine application domains by weighting outcomes according to real-world consequences. Using a five-stage pipeline, HALF aggregates fairness scores with domain-specific harm weights (Severe: 3, Moderate: 2, Mild: 1), enabling deployment-relevant comparisons. Across eight models, results show that performance does not guarantee fairness, reasoning models excel in high-stakes domains but underperform in low-stakes tasks, and no universally fair model exists. Claude 4 achieved the highest overall score (60.7 weighted), while open-weight models showed lower fairness despite competitive performance in severe harm contexts. Bias patterns were inconsistent across domains, underscoring the need for domain-specific evaluation aligned with deployment risks.

## Method Summary
HALF employs a five-stage pipeline: domain selection across nine application areas, dataset search/adaptation with demographic augmentation, task and metric definition, evaluation execution with controlled demographic perturbations, and per-dataset score aggregation using harm-weighted averaging. The framework creates demographic variants (e.g., "Western male," "Arab female") for each input and measures output deviation from neutral baselines. Domain-specific metrics include accuracy gaps, flip rates, and demographic perturbation magnitude, normalized via sigmoid(z-score) and aggregated with 3:2:1 weights for Severe:Moderate:Mild domains. Zero-shot evaluation is conducted across eight models including GPT-4o, Claude 4, and open-weight models.

## Key Results
- No model achieves universal fairness; Claude 4 ranks highest (60.7 weighted) but shows high hallucination bias
- Performance-fairness decoupling: DeepSeek-V3 achieves 77.5% accuracy but highest demographic sensitivity (14.39% perturbation)
- Open-weight models show higher refusal rates and lower fairness despite competitive performance in severe harm contexts
- Bias patterns are inconsistent across domains, with no universal ranking across all nine applications

## Why This Works (Mechanism)

### Mechanism 1: Harm-Severity Weighting Reorders Model Rankings
Applying domain-specific harm weights changes which models appear "best," revealing deployment-readiness gaps that uniform averaging obscures. The HALF score aggregates per-dataset fairness scores using a weighted average where Severe domains contribute 3×, Moderate 2×, and Mild 1× to the final score. This prioritizes high-stakes failures over low-stakes stylistic biases. Core assumption: The 3:2:1 weighting scheme reflects relative real-world consequence magnitude; different stakeholders may legitimately choose different weights. Evidence: GPT-4.1 ranks second in unweighted evaluation (60.3) but drops to fourth under harm weighting (47.5), with a 12.8 point drop exposing weaknesses in high-stakes domains.

### Mechanism 2: Demographic Counterfactual Probing Isolates Causal Effects
Creating controlled input variants that differ only in demographic attributes reveals how models respond to identity cues independent of task content. For each input, the framework generates demographic variants (e.g., "Western male," "Arab female") and computes the output deviation from the neutral baseline. Lower perturbation magnitude indicates more consistent, fairer behavior. Core assumption: Demographic markers can be cleanly isolated and perturbed without changing task semantics; intersectional combinations capture compounding effects. Evidence: Fairness measured by "average absolute demographic perturbation (|∆|, pp)" with models showing 0.16–14.62 pp variation across domains.

### Mechanism 3: Performance-Fairness Decoupling Exposes Hidden Risks
High accuracy on neutral benchmarks does not predict robustness to demographic variation; models can excel at tasks while exhibiting severe bias. The framework separately measures (a) task performance on neutral inputs and (b) output consistency across demographic variants. The lack of correlation between these metrics reveals that standard benchmarks miss deployment-critical fairness failures. Core assumption: Neutral benchmark performance is currently used as a proxy for deployment readiness; this proxy is systematically inadequate. Evidence: DeepSeek-V3 achieves 77.5% accuracy on BiasMedQA but exhibits the highest bias by 14.39% average perturbation, indicating that strong medical reasoning coexists with severe sensitivity to cognitive bias framing.

## Foundational Learning

- **Counterfactual fairness and demographic perturbation**
  - Why needed here: HALF relies on creating controlled demographic variants to measure how outputs shift. Without understanding counterfactual analysis, you cannot interpret whether |∆| scores reflect bias or legitimate variation.
  - Quick check question: If you swap "he" to "she" in a medical case and model predictions change, what additional checks do you need before concluding gender bias?

- **Harm taxonomies in AI governance (e.g., EU AI Act risk tiers)**
  - Why needed here: The 3:2:1 weighting is grounded in regulatory frameworks. Understanding why medical AI is "high-risk" vs. chatbots as "limited-risk" helps you adapt weights to your context.
  - Quick check question: Under what conditions would a translation system escalate from Moderate (weight=2) to Severe (weight=3)?

- **Sigmoid normalization for cross-metric aggregation**
  - Why needed here: The framework uses z-score normalization followed by sigmoid transformation to convert heterogeneous metrics (accuracy gaps, flip rates, F1 disparities) onto a common [0,1] scale.
  - Quick check question: Why is inverted sigmoid (σ(−z)) used for metrics where lower values indicate better fairness?

## Architecture Onboarding

- **Component map**: Domain Selection → Dataset Search/Adaptation → Task & Metric Definition → Evaluation Execution (demographic variants) → Per-Dataset Scores → Harm-Weighted HALF Aggregation
- **Critical path**: Dataset adaptation is most labor-intensive, requiring demographic augmentation for task-specific datasets. Demographic variant generation drives evaluation cost with 18 variants per input × multiple datasets. Aggregation weight assignment determines final rankings and must be documented and justified.
- **Design tradeoffs**: Breadth vs. depth—nine domains provide cross-domain insights but reduce per-domain statistical power; focused evaluation on 1–2 high-stakes domains may be more actionable. Closed vs. open models—closed models show more stable cross-domain fairness but lack transparency; open models allow weight inspection but exhibit higher variance and refusal rates. Weight customization vs. comparability—custom weights enable context-specific assessment but reduce cross-study comparability.
- **Failure signatures**: Safety refusals masquerading as fairness—LLaMA-3B refused 6,200+ prompts on mental health tasks, producing empty outputs that register as low fairness scores. Invalid predictions in structured tasks—legal evaluation saw models predict articles outside valid label sets, requiring exclusion before fairness analysis. Hallucinated demographic details—Claude 4 showed highest hallucination bias (0.500) in summarization despite lowest word-list bias—different bias dimensions can conflict.
- **First 3 experiments**: Reproduce weighted vs. unweighted ranking shift by recomputing HALF scores with equal weights (1:1:1) to verify GPT-4.1 rises and o4-mini falls. Single-domain deep dive by picking one Severe domain (e.g., medical) and running full demographic perturbation analysis on 2–3 models to compute both accuracy and |∆| scores. Weight sensitivity analysis by recomputing HALF scores with alternative weight schemes (e.g., 5:3:1 for higher Severe emphasis, or 2:2:2 for uniform) to document how rankings shift and which models are robust to weight changes.

## Open Questions the Paper Calls Out

### Open Question 1
How should harm weights be calibrated for different stakeholder perspectives (hospitals, schools, platforms, regulators), and does weight variation systematically change model rankings? The paper provides default weights but acknowledges this is subjective; no empirical analysis of how rankings shift under alternative weightings is conducted. Systematic comparison of model rankings across multiple stakeholder-defined weight configurations, with sensitivity analysis showing which domains most influence ranking changes, would resolve this.

### Open Question 2
Why does increasing model scale improve fairness in low-stakes conversational tasks but degrade it in certain deployment contexts like recommendation and translation? The paper documents the non-monotonic scaling phenomenon but does not investigate the underlying mechanisms causing 8B models to fail on specific moderate-harm tasks. Controlled experiments probing whether increased model capacity amplifies demographic sensitivity through attention patterns, or whether alignment procedures inadvertently introduce task-specific biases at larger scales, would resolve this.

### Open Question 3
What explains the inverse performance-fairness relationship in legal judgment, where models with higher legal reasoning capability show 3-4× greater demographic perturbation? The paper identifies this phenomenon but does not determine whether higher capability models learn spurious correlations between case metadata (state, demographics) and legal outcomes from training data. Analysis of whether high-performing models rely on demographic cues as predictive features, versus probing whether capability improvements increase sensitivity to context-embedded bias signals, would resolve this.

## Limitations
- Framework assumes demographic perturbation can cleanly isolate bias without altering task semantics, which may not hold in domains where cultural identity directly affects content
- 3:2:1 harm weighting scheme represents one stakeholder perspective and may not align with different contexts or risk profiles
- Open-weight models show higher refusal rates and hallucination bias, suggesting they may not be directly comparable to closed models under current evaluation protocols
- Framework assumes existing bias benchmarks can be meaningfully repurposed through demographic augmentation, which may not capture domain-specific fairness concerns

## Confidence

- **High confidence**: The harm-weighting mechanism's effect on model rankings is empirically validated through direct comparison of weighted vs. unweighted scores. The performance-fairness decoupling observation is robustly demonstrated across multiple domains.
- **Medium confidence**: The demographic perturbation methodology's validity depends on context—while it isolates identity effects in controlled settings, real-world deployment may involve confounding factors. The choice of harm tiers (Severe/Moderate/Mild) is well-motivated but not universally applicable.
- **Low confidence**: Cross-domain fairness patterns are inconsistent, making it difficult to predict model behavior in unseen domains. The framework's scalability to new domains requires significant domain expertise for metric selection and harm tier assignment.

## Next Checks

1. **Weight sensitivity validation**: Recompute HALF scores with alternative harm weightings (e.g., 5:3:1 vs 3:2:1) to determine which models maintain robust rankings across different stakeholder perspectives.
2. **Domain-specific bias analysis**: For one high-stakes domain (e.g., medical), conduct intersectional demographic perturbation (combining gender, ethnicity, and age) to detect compounding bias effects not visible in single-attribute analysis.
3. **Refusal rate correction**: Implement a methodology to distinguish between model refusals due to genuine safety concerns versus those masking underlying bias, then re-analyze open-weight model scores with this correction applied.