---
ver: rpa2
title: Guiding Evolution of Artificial Life Using Vision-Language Models
arxiv_id: '2509.22447'
source_url: https://arxiv.org/abs/2509.22447
tags:
- asal
- prompts
- life
- simulation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ASAL++, a method that uses foundation models
  to guide the evolution of artificial life simulations in an open-ended manner. The
  key innovation is using a second foundation model to propose new evolutionary targets
  based on a simulation's visual history, creating increasingly complex objectives.
---

# Guiding Evolution of Artificial Life Using Vision-Language Models

## Quick Facts
- arXiv ID: 2509.22447
- Source URL: https://arxiv.org/abs/2509.22447
- Reference count: 7
- Primary result: ASAL++ achieves open-ended evolution of artificial life by using foundation models to generate evolutionary targets based on visual history

## Executive Summary
This paper introduces ASAL++, a method that uses foundation models to guide the evolution of artificial life simulations in an open-ended manner. The key innovation is using a second foundation model to propose new evolutionary targets based on a simulation's visual history, creating increasingly complex objectives. Two strategies are explored: Evolved Supervised Targets (EST), which promotes greater visual novelty by optimizing for single new prompts, and Evolved Temporal Targets (ETT), which fosters more coherent and interpretable evolutionary sequences by optimizing for the entire sequence of generated prompts. Experiments in the Lenia substrate using Gemma-3 show that ASAL++ can discover novel behaviors and generate diverse evolutionary trajectories.

## Method Summary
ASAL++ extends ASAL by introducing a dual-foundation model architecture where a second model (EVOLVERMODEL) proposes new evolutionary targets based on visual history. The method uses CLIP embeddings to align simulation parameters with text prompts, optimizing via CMA-ES to maximize cosine similarity. Each iteration takes the best simulation parameters from the previous iteration (warm start) and generates a new target prompt. EST optimizes for single new prompts while ETT optimizes for the entire temporal sequence. The process runs for N iterations, with I inner CMA-ES steps per iteration, generating increasingly complex evolutionary targets.

## Key Results
- EST achieves a mean open-endedness score of 0.052 compared to ETT's 0.049, demonstrating increased diversity
- ETT produces more interpretable sequences with coherent evolutionary trajectories
- The method generates phylogenetic tree visualizations showing exploratory capacity
- Both strategies successfully discover novel behaviors in the Lenia substrate
- Gemma-3 shows biological bias, drifting toward cellular terminology regardless of seed prompts

## Why This Works (Mechanism)

### Mechanism 1: Dual-FM Architecture for Autonomous Target Generation
Using a second foundation model to propose evolutionary targets based on visual history creates open-ended-like search trajectories. The EVOLVERMODEL (Gemma-3) receives simulation videos and prompt history, then generates new target prompts. This induces "an evolutionary trajectory with increasingly complex targets" rather than relying on pre-specified objectives. Core assumption: The FM can propose meaningfully novel targets that lead to interesting behaviors, rather than cycling through similar prompts.

### Mechanism 2: CLIP-based Vision-Language Alignment for Simulation Optimization
VLM embeddings provide a differentiable signal for aligning simulation parameters with semantic text targets. CLIP encodes both rendered simulation frames and text prompts into a shared embedding space. CMA-ES optimizes substrate parameters θ to maximize cosine similarity. Core assumption: CLIP's embedding space captures semantically meaningful correspondences between Lenia patterns and natural language descriptions.

### Mechanism 3: Warm-Start Iterative Refinement Creates Evolutionary Continuity
Initializing each ASAL++ iteration with the previous iteration's best parameters enables cumulative complexity building. "We use warm starts: the initial parameters for each iteration are the best parameters found by the previous iteration." This allows ETT to build coherent sequences (e.g., "a microbe" → "clusters" → "microbe motility"). Core assumption: The parameter space supports smooth transitions between successive targets without catastrophic forgetting.

## Foundational Learning

- **Concept: CLIP Embeddings & Contrastive Learning**
  - Why needed here: The entire alignment mechanism depends on understanding how CLIP maps images and text to a shared space where cosine similarity measures semantic correspondence.
  - Quick check question: Can you explain why `cosine_similarity(VLM_img(image), VLM_txt("a cell"))` might be higher for a Lenia pattern resembling a cell than for random noise?

- **Concept: CMA-ES (Covariance Matrix Adaptation Evolution Strategy)**
  - Why needed here: ASAL uses Sep-CMA-ES to optimize simulation parameters; understanding black-box optimization helps diagnose convergence issues and population sizing.
  - Quick check question: Why would a gradient-free optimizer like CMA-ES be preferred over gradient descent for Lenia parameter optimization?

- **Concept: Lenia Substrate & Continuous Cellular Automata**
  - Why needed here: The substrate defines the parameter space (θ) being optimized; Lenia's radial symmetry and Gaussian kernels constrain expressivity.
  - Quick check question: What is the key difference between Conway's Game of Life and Lenia that enables more "lifelike" behaviors in the latter?

## Architecture Onboarding

- **Component map:**
  Human Prompt → [ASAL++ Loop: N iterations]
  ├─► Lenia Substrate (renders frames from parameters θ)
  ├─► CLIP Encoder (embeds frames → VLM_img)
  ├─► CMA-ES Optimizer (maximizes similarity over I inner iterations)
  └─► Gemma-3 EVOLVERMODEL (takes video + prompt history → generates next prompt)
  Output: Sequence of N prompts + optimized θ

- **Critical path:**
  1. Initialize θ randomly, embed seed prompt with CLIP
  2. Run I=2000 CMA-ES iterations: sample population → simulate T=256 timesteps → render → embed → compute loss
  3. Pass best video + prompt history to Gemma-3 → get new prompt
  4. Append prompt (ETT) or replace (EST), repeat from step 2
  5. After N=8 outer iterations, evaluate with OE score: 1 - max[similarity(frame_T, frame_T'<T)]

- **Design tradeoffs:**
  | Choice | EST (single prompt) | ETT (temporal prompts) |
  |--------|---------------------|------------------------|
  | Diversity | Higher (OE: 0.052) | Lower (OE: 0.049) |
  | Coherence | Low—jumps between patterns | High—builds on history |
  | Local minima risk | Lower (perturbation allowed) | Higher (prompt similarity) |
  | Vanishing activations | Recovers better | Can persist |

- **Failure signatures:**
  - Repetitive prompts: Gemma-3 stuck in vocabulary loops (e.g., "fragment" repeated 4×)
  - Vanishing activations: Simulation renders go blank; consider FlowLenia (mass conservation) as alternative
  - Biological bias: Prompts drift toward cellular/biological vocabulary regardless of seed (e.g., "pepperoni pizza" → "pulsating bioluminescence")
  - Convergence before I iterations: Increase inner iterations (4000 vs 2000 improved caterpillar shape)

- **First 3 experiments:**
  1. Replicate single EST run: Use initial prompt "a microbe," N=4 iterations, I=1000 inner steps. Verify that OE score increases (target: ΔOE > 0). Check prompt diversity.
  2. Compare EST vs ETT on same seed: Run both strategies from "a flower" for N=6 iterations. Measure: (a) final OE scores, (b) qualitative coherence of frame sequences. Expect EST higher OE, ETT more interpretable.
  3. Phylogenetic tree exploration: From "a caterpillar," sample K=3 alternative prompts per iteration with temperature T=1. Visualize branching diversity. If branches converge visually, increase temperature or add environmental pressure prompts.

## Open Questions the Paper Calls Out

### Open Question 1
Can larger or fine-tuned foundation models overcome the biological vocabulary bias when proposing evolutionary targets? The authors note that Gemma-3 ignores non-biological initial prompts and suggest that "Larger FMs, or FMs fine-tuned for our purposes, could address this." Running ASAL++ with significantly larger FMs (e.g., GPT-4o) or models fine-tuned on non-biological simulation data, and measuring the semantic distance of generated prompts from biological concepts would resolve this.

### Open Question 2
Do ASAL++ trajectories align with other quantitative benchmarks of open-endedness beyond the visual OE score? The authors state, "future work could incorporate additional quantitative benchmarks from recent literature (e.g. measures proposed in (Michel et al., 2025)) to provide complementary perspectives." Evaluating the evolved simulations using diverse statistical complexity measures and analyzing the correlation with the visual OE score would resolve this.

### Open Question 3
Can integrating quality-diversity algorithms with ASAL++ prevent the loss of diversity seen in local minima convergence? The authors propose that "An interesting direction would be to use a quality-diversity algorithm such as MAP-Elites... and use ASAL++ to generate a vast archive of diverse simulations." Implementing a hybrid ASAL++/MAP-Elites system and measuring the coverage and diversity of the resulting behavioral archive compared to the standard trajectory approach would resolve this.

## Limitations
- Biological bias in FM target generation causes drift toward cellular terminology regardless of seed prompts
- Computational cost and scalability issues with 2000+ CMA-ES steps per iteration
- Local minima and vanishing activations can cause premature convergence, especially in ETT
- CLIP's single-frame processing may inadequately capture temporal dynamics

## Confidence
- **High Confidence**: The dual-FM architecture for autonomous target generation is well-supported by methodology and experimental design
- **Medium Confidence**: Quantitative OE score comparisons are valid, but biological bias suggests metrics may not fully capture true open-ended exploration
- **Low Confidence**: The claim of achieving "open-ended evolution" is questionable given biological bias evidence suggests semantically constrained rather than genuinely open-ended novelty

## Next Checks
1. **Temporal CLIP evaluation**: Replace single-frame CLIP embeddings with a temporal encoder (e.g., TimeSformer or 3D CNN) to assess whether improved temporal representation changes OE scores or mitigates vanishing activation issues.
2. **Prompt diversity audit**: Log all generated prompts across multiple ASAL++ runs to quantify vocabulary overlap and biological bias. Compute Jaccard similarity between prompt sets to measure true diversity versus semantic clustering.
3. **Temperature sensitivity analysis**: Systematically vary Gemma-3 temperature (T=0.5, 1.0, 1.5, 2.0) across EST and ETT conditions to identify optimal trade-off between exploration (diversity) and exploitation (coherence), measuring both OE scores and prompt uniqueness.