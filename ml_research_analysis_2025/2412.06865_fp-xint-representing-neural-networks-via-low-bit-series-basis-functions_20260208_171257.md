---
ver: rpa2
title: FP=xINT:Representing Neural Networks via Low-Bit Series Basis Functions
arxiv_id: '2412.06865'
source_url: https://arxiv.org/abs/2412.06865
tags:
- quantization
- expansion
- sparse
- intx
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a series expansion framework for post-training
  quantization (PTQ) that converts full-precision neural networks into low-bit integer
  models without calibration or fine-tuning. The core idea is to expand the original
  model into multiple low-bit basis models through tensor, layer, and model-level
  expansions, proving convergence to the original network.
---

# FP=xINT:Representing Neural Networks via Low-Bit Series Basis Functions

## Quick Facts
- arXiv ID: 2412.06865
- Source URL: https://arxiv.org/abs/2412.06865
- Authors: Boyang Zhang; Daning Cheng; Yunquan Zhang; Jiake Tian; Jing Li; Fangming Liu
- Reference count: 26
- Primary result: Converts FP models to INT2-4 without calibration/fine-tuning via series expansion

## Executive Summary
This paper introduces a series expansion framework for post-training quantization (PTQ) that converts full-precision neural networks into low-bit integer models without calibration or fine-tuning. The core idea is to expand the original model into multiple low-bit basis models through tensor, layer, and model-level expansions, proving convergence to the original network. AbelianAdd and AbelianMul operations enable parallel computation across isomorphic models, forming an Abelian group for efficient inference. Experiments show state-of-the-art performance in extremely low-bit settings, with 4-bit ResNet-50 achieving 77.03% accuracy, surpassing the original model. The method also achieves over 60% accuracy on ResNet-101 at 2-bit quantization and maintains high performance in NLP tasks with BERT models, demonstrating broad applicability and efficiency.

## Method Summary
The method converts full-precision neural networks to low-bit integer representations through a three-level expansion: tensor-level decomposition into residual basis terms, layer-level expansion of weight/activation products, and model-level combination via Abelian group operations. Tensors are expanded as M = M_sa + bias × M_nsy + Σ scale_i × fM_i where each fM_i is a low-bit integer tensor with exponentially decreasing scales. Layer expansion applies this to weight/activation tensor products, and model expansion combines basis models using AbelianAdd and AbelianMul operations that form an Abelian group for parallel reduction. Weights expand 2-3 terms while activations expand 4-8 terms based on loss-gradient arguments, enabling asymmetric expansion that minimizes compute while preserving accuracy.

## Key Results
- ResNet-50 4-bit achieves 77.03% accuracy, surpassing original model
- ResNet-101 maintains >60% accuracy at 2-bit quantization
- BERT-base achieves 91.40 F1 on SQuAD with W4A8
- Qwen2.5-7B achieves 69.1% on MMLU with W4A8
- No calibration or fine-tuning required for any experiments

## Why This Works (Mechanism)

### Mechanism 1
Residual decomposition of tensors enables convergence to full-precision values through iterated low-bit quantization. A tensor M is expanded as M = M_sa + bias × M_nsy + Σ scale_i × fMi, where each fMi is a low-bit integer tensor and scales decrease by powers of 2^X. Residuals R_i = M - Σ(scale_j × fMj) converge exponentially as i increases, since max(R_{n+1}) < scale / 2^{Xn}. When n→∞, the maximum value in R_{n+1} is converged to zero.

### Mechanism 2
Abelian group operations over isomorphic models enable lossless parallel inference without sequential accumulation errors. AbelianAdd (⊎) combines model outputs with scale-weighted aggregation, satisfying Model(W1, A) ⊎ Model(W2, A) = Model(W1+W2, A). AbelianMul (ˆ*) applies scale vectors to layer weights. These operations form an Abelian group, enabling AllReduce-style parallel reduction across t² basis models without order-dependent errors.

### Mechanism 3
Asymmetric expansion depth for weights versus activations minimizes compute while preserving accuracy, leveraging trained weight stability. For a trained model, ∂ℓ/∂W ≈ 0, so small weight quantization errors have first-order-zero impact on loss. This allows weights to expand only 2-3 terms while activations require 4-8 terms. Complexity reduces from O(t²) to O(k×t) where k≪t.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed: The paper's contribution is specifically within PTQ (no calibration set, no fine-tuning). Understanding this distinction clarifies why convergence guarantees matter—there's no training loop to correct quantization errors.
  - Quick check: Given a model with 76% accuracy, would you use PTQ or QAT if you had 1M labeled samples and 100 GPU-hours? What if you had 0 samples and 1 GPU-hour?

- **Concept: Scale factor in integer quantization**
  - Why needed: The entire series expansion relies on the relationship scale_i = 2^X × scale_{i+1}. Without understanding how scale maps floating-point ranges to integer ranges, the residual decomposition mechanism is opaque.
  - Quick check: If a tensor has values in [-3.2, 2.8] and you quantize to INT4 (range [-8, 7]), what is the scale factor? What happens to a value of 2.9?

- **Concept: Abelian group properties (commutativity, associativity, identity, inverse)**
  - Why needed: The paper's parallelization claim rests on AbelianAdd/AbelianMul forming an Abelian group compatible with AllReduce. Without this algebraic structure, parallel reduction would have order-dependent results.
  - Quick check: Why does commutativity (a ⊎ b = b ⊎ a) matter for distributed inference across 8 GPUs? What would happen if the operation were non-commutative?

## Architecture Onboarding

- **Component map**: FP Model → [Tensor Expansion] → {fM₁, fM₂, ... fMₙ, M_sa, M_nsy} → [Layer Expansion] → {L̂_{i,j} for each layer} → [Model Expansion] → {model̂_{i,j} basis models} → [AbelianOps] → Parallel inference → Aggregated output

- **Critical path**: Activation expansion at runtime is the bottleneck. Weights are pre-expanded offline; activations must be quantized and broadcast for each inference request. The number of activation expansion terms (4-8 empirically) directly determines latency.

- **Design tradeoffs**:
  - More expansion terms → higher accuracy, but O(n) more compute and memory
  - Saturation clipping → smaller M_sa sparse tensor, but requires distribution estimation (Laplace assumption)
  - Parallel basis models → faster wall-clock with sufficient hardware, but t× memory footprint
  - INT2 basis → maximum throughput on INT2-capable hardware, but needs 8+ expansion terms

- **Failure signatures**:
  - Accuracy degrades sharply below 60% → likely activation expansion insufficient; check max_diff threshold
  - Inference slower than FP → insufficient parallelism; reduce expansion terms or verify AllReduce working
  - M_sa grows >1% of tensor size → clipping thresholds wrong; recalibrate saturation bounds
  - Non-Abelian behavior (results vary with GPU count) → check for non-linear layers (softmax, layernorm) incorrectly expanded

- **First 3 experiments**:
  1. Validate tensor expansion convergence: Take a single 512×512 weight matrix, expand to INT4 basis with n=1 to 8 terms, plot ||M - Σ(scale_i × fMi)|| vs. n. Expect exponential decay; if not, check scale factor computation.
  2. Single-layer ablation: Quantize only the first conv layer of ResNet-18 with full expansion, measure output MSE vs. FP for 100 ImageNet samples. This isolates expansion error from layer-to-layer accumulation.
  3. Parallel scaling test: Run 4-bit ResNet-50 inference with 4 vs. 8 vs. 16 parallel workers, measure wall-clock time and verify output identical (Abelian property). If outputs differ, identify non-Abelian layer handling.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the series expansion framework perform when using alternative basis functions, such as sparse models, rather than low-bit integers? The authors state, "In this study, we used integer models as basis functions, but in practice, we could also use sparse models or other computation-friendly models as the base functions."

- **Open Question 2**: Can the series expansion approach maintain its convergence properties and efficiency when applied to the complex attention mechanisms of Large Language Models (LLMs)? The authors note, "In the future, we will focus on the quantization of LLMs," despite providing preliminary results in Table 6.

- **Open Question 3**: What are the practical latency and energy implications of implementing the custom AbelianAdd and AbelianMul operations on real-world hardware accelerators? The paper claims efficiency through "parallel-friendly" operations forming an Abelian group and analyzes theoretical complexity ($O(n^2)$), but relies on PyTorch runtime for experiments.

## Limitations

- The claim of "state-of-the-art" performance in extremely low-bit settings rests on comparisons against calibration-dependent methods, but lacks ablation studies on how much the Abelian algebraic structure versus simple residual quantization contributes to gains
- The asymmetric expansion strategy is justified through loss-gradient arguments, but sensitivity analysis across different model architectures and training regimes is absent
- Laplace distribution assumptions for saturation clipping may not hold for activation distributions in transformers, potentially causing the M_sa sparse tensor to grow and negate computational savings

## Confidence

- **High confidence**: The series expansion convergence proof (Theorem 1) and basic tensor expansion mechanism are mathematically sound and directly supported by the paper's derivations
- **Medium confidence**: The Abelian group operations and parallel inference claims work for standard CNN architectures, but edge cases with non-linear layers need validation
- **Low confidence**: The claim that this method works "without calibration or fine-tuning" may overstate practical applicability, as Laplace clipping requires distribution estimation that functions as implicit calibration

## Next Checks

1. Test non-Laplacian distributions: Run the same expansion on activations with bimodal or uniform distributions to verify convergence holds; measure M_sa growth rate as a function of distribution shape
2. Single-layer Abelian property validation: Implement a two-GPU parallel reduction on a single linear layer with known non-commutative operations inserted; verify outputs differ, confirming the Abelian property is actively enforced
3. Weight expansion sensitivity sweep: Quantize ResNet-50 with weight expansion terms varied from 1 to 5 (activations fixed at 4 terms) across multiple random initializations; plot accuracy vs. expansion depth to empirically validate the loss-gradient truncation argument