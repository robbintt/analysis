---
ver: rpa2
title: 'Revisiting Social Welfare in Bandits: UCB is (Nearly) All You Need'
arxiv_id: '2510.21312'
source_url: https://arxiv.org/abs/2510.21312
tags:
- logt
- regret
- bound
- nash
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in stochastic multi-armed bandits
  by proposing a new algorithm, Welfarist-UCB, that achieves near-optimal regret under
  both Nash and p-mean social welfare metrics. The key idea is to use a data-adaptive
  initial exploration phase followed by a standard UCB algorithm.
---

# Revisiting Social Welfare in Bandits: UCB is (Nearly) All You Need

## Quick Facts
- arXiv ID: 2510.21312
- Source URL: https://arxiv.org/abs/2510.21312
- Authors: Dhruv Sarkar; Nishant Pandey; Sayak Ray Chowdhury
- Reference count: 40
- Primary result: Welfarist-UCB achieves near-optimal regret under Nash and p-mean social welfare metrics without restrictive assumptions

## Executive Summary
This paper addresses fairness in stochastic multi-armed bandits by proposing Welfarist-UCB, a novel algorithm that achieves near-optimal regret under both Nash and p-mean social welfare metrics. The key innovation is using a data-adaptive initial exploration phase followed by standard UCB, eliminating the need for restrictive assumptions required by prior work such as bounded rewards or minimum mean reward thresholds. The approach naturally extends to sub-Gaussian reward distributions without requiring upper bounds on optimal rewards.

## Method Summary
The Welfarist-UCB algorithm combines an initial exploration phase with a standard UCB algorithm. During the exploration phase, the algorithm pulls each arm a polynomial number of times (depending on the number of arms and sub-Gaussian variance) to gather sufficient statistics. After this exploration, the algorithm switches to standard UCB for the remaining rounds. This simple yet effective approach allows the algorithm to achieve near-optimal regret bounds for both Nash and p-mean regret across all values of the fairness parameter p. The method works by ensuring that during the exploration phase, the algorithm gathers enough information to make informed decisions in the exploitation phase, while the UCB component provides strong regret guarantees.

## Key Results
- Welfarist-UCB achieves near-optimal regret bounds for both Nash and p-mean regret
- Regret bounds depend polynomially on the number of arms and sub-Gaussian reward variance
- Algorithm works for all values of the fairness parameter p without restrictive assumptions
- Natural extension to sub-Gaussian reward distributions without upper bound requirements

## Why This Works (Mechanism)
The algorithm works by leveraging the strengths of both exploration and exploitation. The initial exploration phase ensures that each arm is sampled sufficiently to obtain reliable estimates of their rewards, which is crucial for making fair decisions. The polynomial dependence on the number of arms and sub-Gaussian variance ensures that the exploration phase is neither too short (risking poor estimates) nor too long (wasting resources). After this phase, the standard UCB algorithm takes over, which is known to provide strong regret guarantees. The combination effectively balances the need for fairness (through adequate exploration) with the desire for low regret (through exploitation via UCB).

## Foundational Learning

**Multi-armed bandit problem** - Sequential decision making framework where an agent chooses from multiple arms with unknown reward distributions
*Why needed:* Forms the fundamental problem setting being addressed
*Quick check:* Understand the tradeoff between exploration and exploitation

**Social welfare metrics** - Measures like Nash and p-mean that quantify fairness across arms
*Why needed:* Provides the fairness objective that traditional bandit algorithms don't optimize for
*Quick check:* Know how Nash and p-mean differ in their fairness implications

**Sub-Gaussian distributions** - Reward distributions with bounded moment generating functions
*Why needed:* Allows theoretical analysis with general reward distributions
*Quick check:* Verify that common distributions (Gaussian, bounded) are sub-Gaussian

**Regret analysis** - Framework for measuring algorithm performance against optimal decisions
*Why needed:* Provides the theoretical foundation for proving algorithm effectiveness
*Quick check:* Understand the difference between regret and simple reward maximization

## Architecture Onboarding

**Component map:** Initial exploration phase -> Data collection and estimation -> Standard UCB exploitation

**Critical path:** The algorithm proceeds through three main phases: (1) polynomial exploration of all arms, (2) reward estimation and threshold computation, (3) UCB-based exploitation using the gathered information.

**Design tradeoffs:** The main tradeoff is between exploration depth (polynomial dependence on parameters) and computational efficiency. Longer exploration provides better estimates but increases initial cost.

**Failure signatures:** Poor performance occurs when: (1) exploration polynomial is too small leading to inaccurate estimates, (2) reward distributions have heavy tails violating sub-Gaussian assumptions, (3) the number of arms is extremely large making polynomial exploration infeasible.

**First experiments:**
1. Compare regret on synthetic data with known parameters against theoretical bounds
2. Test algorithm sensitivity to different polynomial exponents in the exploration phase
3. Evaluate performance under varying reward distribution shapes within the sub-Gaussian class

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on specific assumptions about initial exploration phase that may not hold in all practical scenarios
- No empirical validation or experiments provided to demonstrate real-world performance
- Algorithm assumes access to problem parameters or their estimates, which may not be feasible in practice

## Confidence

**Theoretical regret bounds:** High - The paper provides rigorous mathematical proofs for the regret bounds

**Practical applicability without parameter knowledge:** Low - The algorithm requires knowledge or estimates of problem parameters for the exploration phase

**Extension to general sub-Gaussian rewards:** Medium - While theoretically sound, practical challenges in parameter estimation remain

## Next Checks

1. Conduct empirical experiments comparing Welfarist-UCB against existing fairness-aware bandit algorithms on synthetic and real-world datasets to validate theoretical performance claims

2. Implement and test the algorithm in scenarios where problem parameters are unknown or must be estimated online to assess practical limitations

3. Evaluate the algorithm's performance under different reward distributions (beyond sub-Gaussian) to test the robustness of the theoretical guarantees