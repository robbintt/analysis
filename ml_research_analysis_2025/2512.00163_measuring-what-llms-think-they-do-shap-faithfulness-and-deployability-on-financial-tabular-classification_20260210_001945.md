---
ver: rpa2
title: 'Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on
  Financial Tabular Classification'
arxiv_id: '2512.00163'
source_url: https://arxiv.org/abs/2512.00163
tags:
- llms
- feature
- shap
- classification
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) can
  serve as reliable zero-shot classifiers for structured financial data. We evaluate
  four open-source LLMs on three financial classification tasks, generating SHAP values
  to assess feature importance and prompting LLMs to self-explain their predictions.
---

# Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification

## Quick Facts
- arXiv ID: 2512.00163
- Source URL: https://arxiv.org/abs/2512.00163
- Authors: Saeed AlMarri; Mathieu Ravaut; Kristof Juhasz; Gautier Marti; Hamdan Al Ahbabi; Ibrahim Elfadel
- Reference count: 4
- Primary result: LLM self-explanations diverge from SHAP-based attributions, with agreement barely above random chance

## Executive Summary
This study investigates whether Large Language Models (LLMs) can serve as reliable zero-shot classifiers for structured financial data. We evaluate four open-source LLMs on three financial classification tasks, generating SHAP values to assess feature importance and prompting LLMs to self-explain their predictions. Our results reveal a significant divergence between LLMs' self-explanations and their actual SHAP-based feature impacts, with agreement barely above random chance. Additionally, LLM SHAP values show poor alignment with those from LightGBM, indicating distinct classification reasoning. While LLMs demonstrate modest predictive validity, their lack of faithful explainability limits deployability in high-stakes financial applications. The findings suggest that few-shot prompting, hybrid model integration, or improved explanation mechanisms are necessary for trustworthy LLM use in regulated domains.

## Method Summary
The authors evaluate four open-source LLMs (Gemma-2-9B, Llama-3.2-3B, Qwen-2.5-7B, Mistral-7B-v0.3) on three financial classification tasks using zero-shot prompting. They serialize tabular data into natural language prompts, obtain probability predictions via vLLM, and generate SHAP values using PermutationExplainer with k-means masking (C=5 centroids, T=4 permutations, max_evals=200). They then prompt LLMs to self-explain feature impacts and compare these explanations against SHAP-derived attributions using Pearson correlation thresholds. The study also compares LLM SHAP values against LightGBM SHAP values to assess alignment in reasoning patterns.

## Key Results
- LLM self-explanations show only 50-57% agreement with SHAP-derived feature impacts, barely above random chance (33%)
- LLM SHAP values poorly align with LightGBM SHAP values (Kendall τ near 0), suggesting fundamentally different reasoning
- Best-performing LLM (Gemma-2-9B) achieves 57.2% average agreement with rationale prompting
- LLMs show modest predictive validity with PR-AUC lift ranging from 0.86× to 1.24× baseline

## Why This Works (Mechanism)

### Mechanism 1: SHAP-Based Feature Attribution for LLM Predictions
- Claim: Permutation-based SHAP can approximate feature importance for LLM classifiers at tractable cost.
- Mechanism: The authors use `PermutationExplainer` with k-means background masking (C=5 centroids, T=4 permutations). This yields O(K×T×M×B) model calls instead of O(K×C×M²) for KernelExplainer, achieving ~5× speedup while preserving attribution fidelity for tabular inputs.
- Core assumption: Permutation-based SHAP with limited background samples captures meaningful feature contributions for autoregressive LLM outputs.
- Evidence anchors:
  - [section] Section 3, equations (1)-(4) detail complexity reduction and speedup calculation.
  - [section] Figure 1 and Figure 2 show SHAP dependence plots revealing actual feature impact directions.
  - [corpus] TokenSHAP (Goldshmidt & Horovicz 2024) uses Monte Carlo sampling for similar efficiency gains, supporting the approximation approach.
- Break condition: If max_evals is too low or background clustering poorly represents the data distribution, attributions may become unstable or misleading.

### Mechanism 2: Self-Explanation vs. Actual Attribution Divergence
- Claim: LLM self-explanations of feature impact do not faithfully reflect their actual predictive behavior.
- Mechanism: The authors prompt LLMs to classify each feature's impact (positive/neutral/negative), then compare against SHAP-derived impact via Pearson correlation thresholds (r < -0.1 = negative, |r| ≤ 0.1 = neutral, r > 0.1 = positive). Agreement remains at 50-57% average (vs. 33% random baseline).
- Core assumption: SHAP values represent the "ground truth" of feature influence on model predictions.
- Evidence anchors:
  - [abstract] "agreement barely above random chance" between self-explanations and SHAP values.
  - [section] Table 2: Even Gemma-2-9B (best performer) achieves only 57.2% average agreement with rationale prompting.
  - [corpus] "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations" (arXiv:2504.14150) finds similar unfaithfulness in LLM explanations.
- Break condition: If LLMs are prompted with anonymized feature names (f1, f2...), lexical priors may be reduced but semantic context is lost.

### Mechanism 3: Lexical Prior Hypothesis for Explanation Errors
- Claim: LLM self-explanations are influenced by feature name semantics rather than dataset-specific conditional relationships.
- Mechanism: Feature names like "cash," "profit," "liabilities" carry pre-trained associations that bias LLMs toward "plausible" but incorrect impact predictions. SHAP captures actual conditional dependencies, revealing misalignment.
- Core assumption: The divergence stems from pre-training priors, not from SHAP estimation error or prompt format issues.
- Evidence anchors:
  - [section] Page 6: "LLM self-explanations are shaped by lexical priors in feature names rather than the dataset-specific conditional relationships captured by SHAP."
  - [section] Figure 3: Disagreements persist even for top-k most important features.
  - [corpus] Limited direct corpus evidence; primarily an inference from the observed divergence pattern.
- Break condition: If feature-name neutralization experiments show improved agreement, the hypothesis gains support; if not, other causes (e.g., serialization sensitivity) may dominate.

## Foundational Learning

- **Concept: SHAP (Shapley Additive Explanations)**
  - Why needed here: SHAP provides the ground-truth attribution baseline against which LLM self-explanations are evaluated. Understanding how SHAP allocates feature credit is essential to interpret the divergence findings.
  - Quick check question: Given a model with 3 features where features A and B contribute positively and C contributes negatively to a prediction, how would SHAP represent this?

- **Concept: Zero-shot Tabular Classification via Serialization**
  - Why needed here: The entire experimental setup depends on converting tabular rows into natural language prompts. Serialization choices (feature order, delimiters, descriptions) can affect both predictions and explanations.
  - Quick check question: If you serialize the same row with features in different orders, should the LLM produce the same probability? Why might it not?

- **Concept: PR-AUC vs. ROC-AUC Under Class Imbalance**
  - Why needed here: All three datasets are imbalanced (2.1%-80.3% positive rates). ROC-AUC can appear optimistic; PR-AUC is more informative for true deployability assessment.
  - Quick check question: On a dataset with 2% positives, a model that always predicts "negative" achieves what ROC-AUC approximately?

## Architecture Onboarding

- **Component map:**
  1. Data serialization layer: Converts tabular rows → natural language prompts (Template 1 for instance-level, Templates 2-3 for feature-level)
  2. LLM inference engine: vLLM on 2× NVIDIA A10G 24GB; outputs JSON probability estimates
  3. SHAP attribution layer: `PermutationExplainer` with k-means masker (C=5), max_evals=200, T=4 permutations
  4. Explanation comparison layer: Pearson correlation binning vs. LLM self-reported impact labels

- **Critical path:**
  1. Serialize tabular row → prompt
  2. LLM inference → probability output
  3. SHAP permutation sampling → feature attributions
  4. Correlation binning → impact classification
  5. Agreement calculation (self-explanation vs. SHAP)

- **Design tradeoffs:**
  - **Speed vs. attribution fidelity**: Lower max_evals reduces cost but may yield noisier SHAP values. Authors chose T=4 as balance point.
  - **Prompt detail vs. generalization**: Detailed feature descriptions may improve LLM understanding but introduce prompt-specific biases.
  - **Background sample size (C)**: More centroids better represent data distribution but increase computation linearly.

- **Failure signatures:**
  - Agreement rates at or below 33% (random baseline) indicate complete explanation failure.
  - High variance in SHAP values across permutations suggests insufficient max_evals.
  - Large disagreement between LLM SHAP and LightGBM SHAP (Kendall τ near 0) confirms fundamentally different reasoning patterns.
  - Sub-baseline PR-AUC lift (e.g., Qwen on License at 0.86×) indicates model failure on that task.

- **First 3 experiments:**
  1. **Baseline replication**: Run the four LLMs (Gemma-2-9B, Llama-3.2-3B, Qwen-2.5-7B, Mistral-7B-v0.3) on one dataset with the exact prompt templates; verify ROC-AUC and PR-AUC match Table 1 within tolerance.
  2. **Feature-name neutralization test**: Replace semantic feature names with anonymous labels (f1, f2...), re-run self-explanation prompts, and measure whether agreement with SHAP improves—directly testing the lexical prior hypothesis.
  3. **Serialization robustness check**: Vary feature ordering and delimiter formats on a subset of instances; report prediction and explanation variance to quantify prompt sensitivity.

## Open Questions the Paper Calls Out
1. **Few-shot prompting alignment**: Does few-shot or many-shot prompting improve the alignment between LLM self-explanations and their actual SHAP feature impacts? The study evaluated only zero-shot baselines, finding poor alignment. It's unknown if providing examples grounds the model's internal reasoning closer to its actual mechanistic behavior.

2. **Prompt serialization sensitivity**: To what extent does prompt serialization sensitivity (feature order and phrasing) alter the stability of LLM explanations and SHAP values? The authors acknowledge that variations can affect the reasoning trace but did not measure this variance systematically.

3. **Feature-name neutralization impact**: Does feature-name neutralization (anonymization) reduce lexical prior bias and improve the faithfulness of LLM self-explanations? The Discussion hypothesizes that disagreements arise because explanations are shaped by "lexical priors in feature names," suggesting this as a mitigation strategy.

## Limitations
- The equivalence between SHAP attribution and "true" feature importance for LLM classifiers is assumed rather than established.
- The lexical prior hypothesis for explanation errors remains speculative without direct feature-name neutralization experiments.
- Exact LightGBM training configuration for SHAP comparison baseline is unspecified, affecting interpretation of alignment values.

## Confidence
- **High confidence**: The computational efficiency improvement from PermutationExplainer (5× speedup) is verifiable and well-documented; the agreement rates (50-57%) between self-explanations and SHAP are directly measurable and reproducible.
- **Medium confidence**: The claim that LLMs show "poor alignment" with LightGBM SHAP values is supported by the data but requires careful interpretation given unknown LightGBM hyperparameters and the possibility that different models genuinely reason differently.
- **Low confidence**: The lexical prior hypothesis remains speculative without feature-name neutralization experiments; the equivalence of SHAP values to "ground truth" feature impact for LLM classifiers is assumed rather than proven.

## Next Checks
1. **Feature-name neutralization test**: Replace all semantic feature names with anonymous labels (f1, f2...) and re-run self-explanation prompts. If agreement with SHAP improves, this supports the lexical prior hypothesis; if not, other causes (e.g., serialization sensitivity) dominate.
2. **Serialization robustness check**: Vary feature ordering and delimiter formats on a subset of instances; report prediction and explanation variance to quantify prompt sensitivity and determine whether the divergence is prompt-format dependent.
3. **SHAP ground truth validation**: Compare SHAP values against ablation studies (removing individual features) on a small sample to verify that SHAP attributions align with actual prediction changes when features are omitted.