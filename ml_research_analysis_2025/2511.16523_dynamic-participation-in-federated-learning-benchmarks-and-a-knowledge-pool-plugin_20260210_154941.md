---
ver: rpa2
title: 'Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool
  Plugin'
arxiv_id: '2511.16523'
source_url: https://arxiv.org/abs/2511.16523
tags:
- learning
- federated
- knowledge
- kpfl
- participation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first open-source benchmarking framework
  specifically designed for federated learning under dynamic client participation
  (DPFL). The authors identify that most existing FL research assumes consistent client
  participation, overlooking realistic scenarios where clients may intermittently
  join or leave during training.
---

# Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin

## Quick Facts
- arXiv ID: 2511.16523
- Source URL: https://arxiv.org/abs/2511.16523
- Authors: Ming-Lun Lee; Fu-Shiang Yang; Cheng-Kuan Lin; Yan-Ann Chen; Chih-Yu Lin; Yu-Chee Tseng
- Reference count: 8
- This paper introduces the first open-source benchmarking framework specifically designed for federated learning under dynamic client participation (DPFL).

## Executive Summary
This paper introduces the first open-source benchmarking framework specifically designed for federated learning under dynamic client participation (DPFL). The authors identify that most existing FL research assumes consistent client participation, overlooking realistic scenarios where clients may intermittently join or leave during training. To address this gap, they propose a framework that supports configurable data distributions (IID, light-NIID, heavy-NIID), realistic participation patterns (timed-random, Markovian, programmed), and DPFL-specific evaluation metrics including windowed evaluation, intransigence to DP, and instability due to DP. Using this platform, they benchmark nine state-of-the-art FL models across four categories and demonstrate substantial performance degradation under dynamic participation. To mitigate these challenges, the authors propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate that KPFL significantly improves model robustness and generalization under various DP scenarios, with more than 12% gain in accuracy under timed-random participation patterns and narrowing the convergence gap by over 70% in some cases.

## Method Summary
The paper proposes KPFL as a server-side plugin that addresses DPFL challenges through a knowledge pool storing per-client states, dual-age weighting mechanisms that track active and idle client freshness, and generative knowledge profiling that creates synthetic samples from collective client knowledge. The method operates in two stages: weighted aggregation using age and data-bias weights, followed by knowledge distillation using a conditional generator trained on the knowledge pool. The framework is evaluated using a comprehensive DPFL benchmark supporting three data heterogeneity levels (IID, Light-NIID, Heavy-NIID) and three participation patterns (timed-random, Markovian, programmed), with nine state-of-the-art FL models.

## Key Results
- KPFL achieved over 12% accuracy gain compared to baseline methods under timed-random participation patterns
- The framework reduced instability (ID) from 1.57 to 0.45 under Timed-Random participation, approaching static-participation levels
- KPFL narrowed the convergence gap by over 70% in some cases compared to standard FL methods under dynamic participation

## Why This Works (Mechanism)

### Mechanism 1: Dual-age weighting
- Claim: Dual-age weighting preserves knowledge from both active and idle clients by explicitly modeling staleness.
- Mechanism: The server tracks two age counters per client: active age ($aa_i = t - \tau_i$ for currently participating clients) and idle age ($ia_i = t - \tau_i$ for disconnected clients). These feed into exponential decay weights $aw_i = e^{\lambda_{aa} \cdot aa_i}$ (active) or $aw_i = e^{\lambda_{ia} \cdot ia_i}$ (idle), which then combine with data-bias weights during aggregation.
- Core assumption: Older idle clients contribute less reliable gradients; fresher active clients should be weighted more heavily.
- Evidence anchors:
  - [section 4.1] "We propose a dual-age weighting mechanism to address adaptiveness and forgetfulness... A smaller $aa_i$ implies greater freshness... a larger idle age indicates more severe staleness."
  - [section 5.4/Table 3] Ablation shows KPFL's dual-age strategy outperforms MIFA (which applies static weights) on Instability metric under both Timed-Random and Markovian participation.
  - [corpus] Corpus papers on partial participation confirm staleness handling is an open problem but do not provide comparative baselines for dual-age schemes.

### Mechanism 2: Generative knowledge profiling
- Claim: Generative knowledge profiling consolidates fragmented client knowledge into a synthetic dataset for stable distillation.
- Mechanism: A conditional generator $G_{ens}$ is trained to produce class-conditional samples $\hat{x} = G_{ens}(z, y)$ by minimizing (1) age-weighted cross-entropy across all clients' models, (2) contrastive loss against class centroids in feature space, and (3) diversity loss to prevent mode collapse. The generator thus encodes collective knowledge even from currently absent clients.
- Core assumption: The generator can faithfully capture the decision boundaries of the ensemble without accessing raw data.
- Evidence anchors:
  - [section 4.3] "We introduce an age-aware generative knowledge profiling method that mitigates knowledge degradation under DPFL... optimize $G_{ens}$ using three loss functions."
  - [section 5.3/Table 1] KPFL reduces Instability (ID) from 1.57 to 0.45 under Timed-Random participation, near static-participation levels.
  - [corpus] HFedCKD and FedGen-related work use generative/distillation approaches for heterogeneity, supporting plausibility but without DP-specific benchmarks.

### Mechanism 3: Two-stage global model refinement
- Claim: Two-stage global model refinement (aggregation + distillation) improves upon standard one-shot averaging.
- Mechanism: Stage 1 produces initial global model $\Theta$ via weighted aggregation (Eq. 4). Stage 2 fine-tunes $\Theta$ via knowledge distillation using the generator's ensemble predictions $\hat{y}_{ens}$ with KL divergence loss $L_{ens}$ (Eq. 8), yielding refined model $\Theta'$.
- Core assumption: The distilled model $\Theta'$ captures ensemble knowledge more robustly than aggregated weights $\Theta$ alone.
- Evidence anchors:
  - [section 4.3] "After training the generator, we fine-tune $\Theta$ by distilling age-weighted knowledge... to obtain an improved global model $\Theta'$."
  - [section 5.4/Table 3] $\Theta'$ consistently outperforms $\Theta$ on WE and ID metrics across FedProx and MOON backbones.
  - [corpus] Corpus has limited direct comparisons of two-stage vs. single-stage FL training under DP; evidence is primarily internal to this paper.

## Foundational Learning

- Concept: **Federated Averaging (FedAvg) and Client Sampling**
  - Why needed here: DPFL benchmarks assume familiarity with baseline FL aggregation; KPFL is a plugin that modifies aggregation weighting.
  - Quick check question: Can you explain why random client sampling in standard FedAvg differs from the dynamic participation modeled here?

- Concept: **Knowledge Distillation in FL**
  - Why needed here: KPFL's generative distillation (Section 4.3) assumes understanding of teacher-student transfer and KL divergence.
  - Quick check question: How does data-free knowledge distillation differ from requiring a public dataset?

- Concept: **Dirichlet Distribution for Non-IID Data Partitioning**
  - Why needed here: The benchmark uses concentration parameter $\alpha$ to control heterogeneity levels (IID, Light-NIID, Heavy-NIID).
  - Quick check question: What happens to client label distributions as $\alpha \to 0$?

## Architecture Onboarding

- Component map:
  - Knowledge Pool (KP) -> Age-Aware Aggregation Module -> Generative Profiling Module -> Distillation Module -> Participation Pattern Controller

- Critical path:
  1. Initialize KP with first-round client states.
  2. Each round: update ages $aa_i, ia_i$ based on participation status.
  3. Aggregate $\Theta$ using age + data-bias weights.
  4. Train $G_{ens}$ on synthetic data.
  5. Distill $\Theta'$ from $G_{ens}$.
  6. Distribute $\Theta'$ to active clients; store updates in KP.

- Design tradeoffs:
  - **Memory vs. robustness**: Larger KP (more clients tracked) improves stability but increases server storage.
  - **Generator complexity vs. communication**: More powerful $G_{ens}$ improves distillation quality but adds server-side compute.
  - **Decay rates $\lambda_{aa}, \lambda_{ia}$**: Faster decay prioritizes freshness; may discard useful stale knowledge.

- Failure signatures:
  - **ID metric remains high (>1.0)**: Age decay rates may be too aggressive; generator not converging.
  - **WE drops under Markovian but not Timed-Random**: Stateful participation patterns may require adjusted transition probability handling.
  - **Distillation loss plateaus high**: Generator may have mode collapse; check diversity loss weight $\gamma_{div}$.

- First 3 experiments:
  1. Replicate baseline degradation: Run FedAvg under Heavy-NIID with Timed-Random participation; confirm ~15% accuracy drop vs. static (Table 1).
  2. Ablate age weighting: Disable dual-age (set all $aw_i = 1$); compare WE and ID to full KPFL to isolate contribution.
  3. Scale test: Run KPFL + FedProx with 50 clients (Table 2 settings); verify KPFL maintains gains as pool size increases.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations for future research.

## Limitations
- The framework assumes homogeneous client models, which may not reflect heterogeneous hardware scenarios common in real-world deployments.
- The knowledge pool creates linear memory scaling with client count, potentially limiting scalability to thousands of clients.
- The generative knowledge profiling relies on external work for diversity loss formulation, creating dependency on implementation details.

## Confidence
- **High confidence**: KPFL's effectiveness under timed-random and Markovian participation patterns (well-validated in Tables 1-2)
- **Medium confidence**: KPFL's performance under Programmed participation (less extensive benchmarking)
- **Medium confidence**: Scalability beyond 50 clients (not extensively tested)

## Next Checks
1. Conduct ablation studies varying age decay coefficients λ_aa and λ_ia across orders of magnitude to establish robustness boundaries.
2. Test KPFL under extreme participation skew (p=0.1 and p=0.9) to evaluate performance in highly unstable environments.
3. Implement KPFL with heterogeneous client model architectures (e.g., ResNet-18 vs ResNet-10) to assess generalization beyond homogeneous settings.