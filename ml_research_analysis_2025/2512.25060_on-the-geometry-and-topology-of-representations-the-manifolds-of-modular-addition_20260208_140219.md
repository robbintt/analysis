---
ver: rpa2
title: 'On the geometry and topology of representations: the manifolds of modular
  addition'
arxiv_id: '2512.25060'
source_url: https://arxiv.org/abs/2512.25060
tags:
- attention
- difference
- highly
- significant
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the geometry and topology of neural representations
  learned by networks trained on modular addition. It challenges the claim that different
  architectures (uniform vs.
---

# On the geometry and topology of representations: the manifolds of modular addition

## Quick Facts
- arXiv ID: 2512.25060
- Source URL: https://arxiv.org/abs/2512.25060
- Reference count: 26
- This paper shows that different neural network architectures (uniform vs. learnable attention) learn topologically equivalent representations for modular addition, challenging previous claims about distinct Clock vs. Pizza circuits.

## Executive Summary
This paper investigates the geometry and topology of neural representations learned by networks trained on modular addition. Through topological data analysis, it demonstrates that both uniform attention (Pizza) and learnable attention (Clock) architectures learn representation manifolds that are either a torus or linear projections of it. The core finding challenges the claim that different architectures learn fundamentally distinct circuits, showing instead that they implement the same underlying algorithm via topologically and geometrically equivalent representations. This supports the universality hypothesis that different architectures can recover the same abstract algorithmic solution.

## Method Summary
The methodology involves training multiple architectures (MLP-Add, MLP-Concat, Attention 0.0/1.0) on the modular addition task with n=59. After training, the researchers extract first-layer pre-activations, cluster neurons by frequency using 2D DFT, and compute Phase Alignment Distributions (PADs). They then apply PCA and persistent homology (using Ripser) to determine manifold topology through Betti numbers. The analysis compares statistical similarity of phase distributions using Maximum Mean Discrepancy (MMD) and examines how different architectures factorize the function geometrically.

## Key Results
- All studied architectures learn representation manifolds that are either a torus or a linear projection of it
- Both uniform attention (Pizza) and learnable attention (Clock) architectures implement the same algorithm via topologically equivalent representations
- The learned phases dictate the manifold structure, with correlated phases yielding rank-2 discs and independent phases yielding rank-4 tori
- MLP-Concat preserves the full torus structure (Rank 4, more efficient) while MLP-Add collapses it to a disc (Rank 2)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The geometry of representation manifolds in modular addition networks is determined by the statistical correlation of learned phases in "simple neurons."
- **Mechanism:** If phases for inputs $a$ and $b$ ($\Phi_L, \Phi_R$) are perfectly correlated, the pre-activation matrix factorizes into a rank-2 "vector addition disc" (Pizza). If phases are independent, it factorizes into a rank-4 "torus."
- **Core assumption:** First-layer neurons are well-approximated by the "simple neuron" model (linear superposition of sinusoids) as defined in Eq. 4.
- **Evidence anchors:**
  - [section]: Theorem 4.1 proves rank-2 (Disc) vs rank-4 (Torus) factorization based on phase correlation.
  - [abstract]: Claims that learned phases dictate the manifold structure.
  - [corpus]: "Uncovering a Universal Abstract Algorithm..." (arXiv:2505.18266) supports the underlying simple neuron abstraction across architectures.
- **Break condition:** If first-layer neurons in new architectures require degree-2 trigonometric polynomials (violating the simple neuron assumption), the rank-2/4 factorization may not hold.

### Mechanism 2
- **Claim:** "Clock" (Attention 1.0) and "Pizza" (Attention 0.0) circuits are not distinct algorithms but rather different instances of the same topological structure.
- **Mechanism:** Both architectures learn phases that are statistically highly similar (low MMD distance) and concentrated near the $a=b$ diagonal, resulting in topologically equivalent representations (Disc) rather than the distinct "angle summation" circle previously theorized for Clocks.
- **Core assumption:** Maximum Mean Discrepancy (MMD) on Phase Alignment Distributions (PAD) is a sufficient metric to validate representational similarity.
- **Evidence anchors:**
  - [section]: Figure 5 and Table 2 show extremely low MMD (0.02 - 0.07) between Attention 0.0 and 1.0.
  - [abstract]: "Both uniform attention and trainable attention architectures implement the same algorithm via topologically and geometrically equivalent representations."
  - [corpus]: Weak direct corpus support for this specific "Clock vs. Pizza" resolution, though "Uncovering a Universal Abstract Algorithm..." supports the broader universality hypothesis.
- **Break condition:** If attention 1.0 layers learn significantly higher-order features (degree > 1) in the first layer, breaking the "simple neuron" symmetry.

### Mechanism 3
- **Claim:** Modular addition is solved by approximating a map from a torus (input structure) to a circle (output structure), regardless of intermediate representations.
- **Mechanism:** Networks factor the function $a+b \mod n$ geometrically: embeddings define a torus $T^2$ in $\mathbb{R}^4$, which is projected (as a Disc or Torus) through layers before converging to a circular logit manifold.
- **Core assumption:** The topology of the logit manifold accurately reflects the algorithmic solution and is reachable via persistent homology analysis.
- **Evidence anchors:**
  - [section]: Appendix H explicitly hypothesizes modular addition as a factored map from Torus to Circle.
  - [section]: Figure 6 shows Betti number distributions converging to circle-like or disc-like structures across layers.
  - [corpus]: "Learning geometry and topology via multi-chart flows" (arXiv:2505.24665) discusses handling non-trivial topology (like torus) in generative models, providing mathematical context for such manifold learning.
- **Break condition:** If the logit manifold topology differs significantly from a circle (e.g., disjoint components), the torus-to-circle factorization hypothesis is invalid.

## Foundational Learning

- **Concept: Betti Numbers & Persistent Homology**
  - **Why needed here:** The paper relies on Betti numbers ($\beta_0, \beta_1, \beta_2$) to rigorously distinguish between Discs (1,0,0), Circles (1,1,0), and Tori (1,2,1).
  - **Quick check question:** Given a point cloud with one loop and one enclosed void, what is the Betti vector?

- **Concept: Trigonometric Polynomials (Fourier Features)**
  - **Why needed here:** The "simple neuron" model assumes pre-activations are degree-1 trigonometric polynomials ($\cos(2\pi f a/n + \phi)$). Understanding frequency $f$ and phase $\phi$ is essential for Theorem 4.1.
  - **Quick check question:** How does shifting the phase $\phi$ change the activation peak of a cosine neuron?

- **Concept: Maximum Mean Discrepancy (MMD)**
  - **Why needed here:** Used to quantitatively prove that "Clock" and "Pizza" phase distributions are statistically indistinguishable (p-value $\approx 0$).
  - **Quick check question:** What does a low MMD score between two distributions imply about their similarity?

## Architecture Onboarding

- **Component map:** Embedding Layer -> Attention Layer (Uniform or Learnable) -> MLP -> Analysis Pipeline
- **Critical path:**
  1. Train the model (MLP or Transformer) on $(a+b) \mod 59$.
  2. Extract first-layer pre-activations and cluster neurons by dominant frequency (using 2D DFT).
  3. Compute PAD (location of max activation/center of mass) for the cluster.
  4. Run PCA and Persistent Homology on the cluster to determine if it is a Disc (Rank 2) or Torus (Rank 4).

- **Design tradeoffs:**
  - **MLP-Concat vs. MLP-Add:** Concat preserves the full Torus structure (Rank 4, more efficient); Add collapses it to a Disc (Rank 2).
  - **Uniform vs. Learnable Attention:** Trade-off is minimal in final representation; learnable attention acts as a "weak non-linearity" but converges to similar phases as uniform attention.

- **Failure signatures:**
  - **Loss of Structure:** If PCA requires $>4$ components to explain variance, or if Betti numbers are ambiguous, the "simple neuron" assumption may be invalid.
  - **Uniform Phase Distribution:** If PADs are spread uniformly across the torus rather than on the diagonal, the network is likely memorizing rather than learning the algorithmic circuit.

- **First 3 experiments:**
  1. **Verify Simple Neuron Model:** Train an MLP-Add model, extract neurons, and fit them to degree-1 trigonometric polynomials to confirm the "simple neuron" baseline.
  2. **Compute PAD Similarity:** Train 100 seeds of Attention 0.0 and 1.0, compute the MMD distance between their Phase Alignment Distributions to verify they are statistically identical.
  3. **Check Topology:** Use Ripser (TDA library) on the pre-activation point cloud of a high-frequency cluster to confirm the Betti numbers match a Torus (1,2,1) or Disc (1,0,0).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does universality in deep learning manifest as networks recovering a universal manifold or linear projections of it?
- **Basis in paper:** [explicit] The authors explicitly ask, "is the nature of universality that DNNs recover either a universal manifold, or linear projections of it in order to fit data?"
- **Why unresolved:** This hypothesis is supported only by the modular addition results in this study; it remains unknown if this pattern holds for diverse tasks and architectures.
- **What evidence would resolve it:** Applying topological data analysis (specifically Betti numbers) to a wider range of tasks to verify if representations consistently converge to topologically equivalent manifolds.

### Open Question 2
- **Question:** Can the methodology of isolating degrees of freedom in hidden representations characterize circuits in more complex, non-algorithmic domains?
- **Basis in paper:** [explicit] The authors state, "To interpret large-scale neural networks, it will be necessary to derive strategies for characterizing circuits across domains."
- **Why unresolved:** The current precise methodology is tailored to modular addition and relies on the "simple neuron" model, which may not hold in complex domains like natural language processing.
- **What evidence would resolve it:** Demonstrating that estimating statistics of learned geometric values (like phases) can successfully characterize circuits in large-scale models such as LLMs.

### Open Question 3
- **Question:** How does the representation manifold structure change for non-commutative operations where input symmetry is absent?
- **Basis in paper:** [inferred] Theorem 4.1 relies on the symmetry assumption that $\phi_L \equiv \phi_R$ due to the commutativity of modular addition ($a+b=b+a$).
- **Why unresolved:** If the operation is not commutative (e.g., matrix multiplication), the symmetry assumption breaks, potentially altering the disc/torus manifold distinction.
- **What evidence would resolve it:** Training networks on non-commutative group operations (e.g., dihedral groups) and analyzing the resulting phase alignment distributions and manifold topologies.

## Limitations

- The "simple neuron" assumption may not generalize to new architectures with different activation functions or layer compositions
- The Phase Alignment Distribution (PAD) metric may not capture all relevant aspects of representational differences between architectures
- The Betti number analysis relies on specific parameter choices (k=250 neighbors) that could affect topology estimation

## Confidence

- **High confidence:** The claim that both MLP and Transformer architectures learn equivalent topological structures (torus or linear projections) is well-supported by Betti number analysis across multiple seeds and architectures.
- **Medium confidence:** The resolution of the Clock vs. Pizza distinction through statistical similarity of phase distributions is compelling but relies on the sufficiency of MMD as a metric for representational equivalence.
- **Medium confidence:** The factorization hypothesis (modular addition as a map from torus to circle) is theoretically sound but remains largely a hypothesis without rigorous proof of the algorithmic universality.

## Next Checks

1. Test the "simple neuron" model assumption on architectures with different activation functions (e.g., GeLU, Swish) to verify the degree-1 trigonometric polynomial approximation holds.
2. Apply the PAD analysis to larger modular addition problems (n > 100) to check if the statistical similarity between Clock and Pizza circuits persists at scale.
3. Verify Betti number stability by systematically varying the k-NN parameter in persistent homology computation and confirming the torus/disc topology remains consistent.