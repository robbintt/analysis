---
ver: rpa2
title: Do What? Teaching Vision-Language-Action Models to Reject the Impossible
arxiv_id: '2508.16292'
source_url: https://arxiv.org/abs/2508.16292
tags:
- 'false'
- visual
- instructions
- language
- false-premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling Vision-Language-Action
  (VLA) models to handle false-premise instructions, where natural language commands
  reference objects or conditions not present in the environment. The authors propose
  Instruct-Verify-and-Act (IVA), a unified framework that detects when an instruction
  cannot be executed due to a false premise, engages in language-based clarification
  or correction, and grounds plausible alternatives in perception and action.
---

# Do What? Teaching Vision-Language-Action Models to Reject the Impossible

## Quick Facts
- **arXiv ID**: 2508.16292
- **Source URL**: https://arxiv.org/abs/2508.16292
- **Reference count**: 40
- **Primary result**: IVA achieves 97.56% improvement in false-premise detection and 50.78% increase in successful responses while maintaining TP performance.

## Executive Summary
This paper addresses the challenge of enabling Vision-Language-Action (VLA) models to handle false-premise instructions, where natural language commands reference objects or conditions not present in the environment. The authors propose Instruct-Verify-and-Act (IVA), a unified framework that detects when an instruction cannot be executed due to a false premise, engages in language-based clarification or correction, and grounds plausible alternatives in perception and action. IVA uses a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, training a VLA model to handle both accurate and erroneous requests. The approach achieves a 97.56% improvement in false premise detection accuracy over baselines and increases successful responses in false-premise scenarios by 50.78%, while maintaining stable performance on true-premise tasks.

## Method Summary
IVA builds on the LLARVA VLA architecture with frozen vision and language encoders, fine-tuning only the autoregressive decoder with LoRA adapters. The model is trained on a semi-synthetic dataset derived from RLBench, containing paired positive and false-premise instructions (65% In-Domain, 20% Out-of-Domain). Input includes RGB observations and previous 5 proprioceptive states. The decoder first generates a verification step (clarification or refusal), then conditionally generates actions. Training uses cross-entropy loss predicting actions and visual traces.

## Key Results
- IVA achieves 97.56% improvement in false premise detection accuracy over baselines
- 50.78% increase in successful responses for false-premise scenarios
- Maintains stable true-premise task performance while improving FP handling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paired positive/false-premise instruction tuning enables the model to learn a verification behavior before action generation.
- Mechanism: By presenting the model with semantically similar but ungroundable instructions, the autoregressive decoder learns to output a textual verification step first, conditioning action prediction on successful grounding checks.
- Core assumption: The model can generalize the verify-then-act pattern from synthetic false-premise examples to novel object references at test time.
- Evidence anchors: [abstract] "semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction"; [section 3] "approximately 20% of episodes containing Out-of-Domain false premises, and 65% of episodes containing In-Domain false premises injected into 10% of their respective steps"
- Break condition: If false-premise instructions in deployment differ structurally from the synthetic set, detection accuracy will degrade.

### Mechanism 2
- Claim: Structured proprioceptive context in the prompt provides a grounded reference state for detecting object absence.
- Mechanism: The instruction template explicitly encodes previous proprioceptive states alongside visual observation, giving the model access to embodied history that reduces hallucination of object presence.
- Core assumption: Proprioceptive history contains implicit information about reachable workspace and recently interacted objects.
- Evidence anchors: [section 3] "proprioceptive states (internally-sensed joint-angle vectors that indicate the robot's current position before it plans the next move) from the previous h timesteps"; [section 4] "the model received the front camera view and the previous 5 joint positions as input"
- Break condition: In tasks where object presence is independent of recent proprioceptive history, this signal provides limited grounding value.

### Mechanism 3
- Claim: Separate handling of In-Domain vs. Out-of-Domain false premises allows calibrated clarification vs. refusal.
- Mechanism: In-Domain false premises trigger clarification responses suggesting alternatives, while Out-of-Domain false premises trigger refusal and termination, preventing the model from attempting to ground absurd references.
- Core assumption: The training distribution's In-/Out-of-Domain split generalizes to user intent patterns in real deployments.
- Evidence anchors: [section 3] "In-Domain False Premise... the model is expected to respond, 'I don't see a safe in the current scene. Do you mean jar?'; Out-of-Domain... subsequently terminating the interaction"; [section 4] "IVA achieved perfect detection (100%) on In-Domain... For Out-of-Domain false premises, IVA reached a detection accuracy of 97.78%"
- Break condition: Ambiguous cases at the In-/Out-of-Domain boundary will produce inconsistent clarification/refusal behavior.

## Foundational Learning

- **Vision-Language-Action (VLA) Models**
  - Why needed here: IVA builds directly on the LLARVA architecture; understanding how VLAs map (image, instruction) pairs to action sequences is prerequisite.
  - Quick check question: Can you explain how a VLA differs from a standard visual Q&A model in its output space?

- **Instruction Tuning / LoRA Fine-Tuning**
  - Why needed here: IVA uses LoRA adapters to fine-tune only the decoder while keeping vision and language encoders frozen.
  - Quick check question: What is the computational benefit of LoRA over full fine-tuning, and what representational capacity is lost?

- **Proprioception in Robotics**
  - Why needed here: The model conditions on joint-angle history; understanding what this signal provides is essential for debugging grounding failures.
  - Quick check question: If proprioceptive input were corrupted, would you expect false-positive or false-negative detection errors?

## Architecture Onboarding

- **Component map**:
  Vision Encoder (CLIP ViT-L/14) -> Language Encoder (structured prompt) -> Multimodal Decoder (LoRA) -> (text response, visual trace, 8D action vector)

- **Critical path**:
  1. RGB observation + structured prompt â†’ encoder tokens
  2. Decoder generates textual response first (verification)
  3. If accepted, decoder generates action + visual trace
  4. If clarification/refusal, interaction terminates or loops

- **Design tradeoffs**:
  - Frozen encoders limit adaptability to novel visual domains but reduce training compute
  - Binary In-/Out-of-Domain taxonomy is simple but may not capture nuanced user intent
  - Single-pass evaluation avoids multi-turn dialogue complexity but limits correction loops

- **Failure signatures**:
  - False-negative detection: Model executes action despite absent object (indicates under-trained verification)
  - Over-refusal: Model refuses valid true-premise instructions (indicates FP training over-representation)
  - Irrelevant clarification: Model suggests implausible alternatives (indicates weak visual grounding)

- **First 3 experiments**:
  1. Baseline comparison: Run LLARVA (no FP training) on the same FP evaluation set to confirm 0% detection baseline.
  2. Ablate proprioceptive history: Remove previous joint states from the prompt and measure FP detection delta.
  3. Generalization test: Evaluate on held-out RLBench tasks not seen during FP training to assess out-of-distribution robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the IVA framework maintain high false-premise detection accuracy when transferred from the RLBench simulation to physical robotic hardware?
- Basis in paper: [explicit] The "Limitations" section states that "robustness in real-world deployments is not yet validated" and explicitly cites potential "domain shift" regarding visual appearance and sensor noise.
- Why unresolved: The current evaluation is restricted entirely to the RLBench simulation environment, leaving the sim-to-real gap unexplored.
- What evidence would resolve it: Benchmarks on physical robots showing detection rates comparable to simulation results under varied lighting conditions and sensor noise.

### Open Question 2
- Question: How can the framework be extended to maintain context and resolve ambiguity over multi-turn clarification dialogues?
- Basis in paper: [explicit] The authors note in the "Limitations" section that the "current framework does not explicitly handle multi-turn dialogues, implicit user intent, or ambiguous references."
- Why unresolved: The current model operates in a single-pass manner, lacking memory mechanisms to handle iterative back-and-forth interactions required for complex clarifications.
- What evidence would resolve it: Evaluation of success rates in scenarios where the robot must engage in a sequence of questions and answers to fully resolve a false premise.

### Open Question 3
- Question: How does IVA perform when exposed to the full complexity of unscripted, conversational human language?
- Basis in paper: [inferred] The paper notes that "The instructions used for evaluation are relatively short and structured" and the semi-synthetic dataset may not capture the diversity of "real human instructions."
- Why unresolved: The training data relies on template-based instructions and LLM-generated distractors, which may lack the nuance, slang, or disfluencies of natural human speech.
- What evidence would resolve it: Comparative performance metrics using a dataset of spontaneous, unscripted human commands collected via user studies rather than generated templates.

## Limitations
- **Dataset representativeness**: Semi-synthetic dataset covers only RLBench tasks and two distractor types, limiting generalization to arbitrary environments.
- **Single-step evaluation**: Evaluation uses single-pass inference without multi-turn dialogue correction, potentially underestimating recovery ability.
- **Evaluation granularity**: Success measured at episode level using binary detector, but intermediate clarification quality and user satisfaction are not assessed.

## Confidence

- **High confidence**: False-premise detection accuracy improvements (97.56% over baseline) and maintained TP success rates are well-supported by controlled RLBench evaluation.
- **Medium confidence**: The mechanism linking proprioceptive history to grounding quality is plausible but lacks ablation studies comparing proprioceptive vs. non-proprioceptive variants.
- **Low confidence**: Claims about user experience improvements (e.g., "natural language correction") are inferred from generated text quality but not validated through human preference studies.

## Next Checks
1. **Out-of-distribution generalization**: Evaluate IVA on robot tasks from a different benchmark (e.g., Meta-World) to measure cross-task false-premise detection robustness.
2. **Proprioception ablation**: Train and evaluate a variant without the previous 5 joint states to quantify the contribution of proprioceptive context to FP detection accuracy.
3. **Multi-turn interaction test**: Implement a dialogue loop where the model's clarification responses are fed back as revised instructions, measuring successful resolution rates for initially false-premise commands.