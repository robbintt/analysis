---
ver: rpa2
title: Arrhythmia Classification from 12-Lead ECG Signals Using Convolutional and
  Transformer-Based Deep Learning Models
arxiv_id: '2502.17887'
source_url: https://arxiv.org/abs/2502.17887
tags:
- arrhythmia
- classification
- signal
- signals
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores deep learning models for arrhythmia classification
  using 12-lead ECG signals. Due to limited Romanian medical data, the authors combined
  five international ECG databases and used signal processing and deep learning techniques,
  including 1D/2D CNNs, ResNet, and Vision Transformers.
---

# Arrhythmia Classification from 12-Lead ECG Signals Using Convolutional and Transformer-Based Deep Learning Models

## Quick Facts
- **arXiv ID**: 2502.17887
- **Source URL**: https://arxiv.org/abs/2502.17887
- **Reference count**: 40
- **Primary result**: GRU-based 1D CNN model achieved 93.4% accuracy for 5-class arrhythmia classification

## Executive Summary
This study explores deep learning models for arrhythmia classification using 12-lead ECG signals. Due to limited Romanian medical data, the authors combined five international ECG databases and used signal processing and deep learning techniques, including 1D/2D CNNs, ResNet, and Vision Transformers. ECG signals were processed using a variant of the Pan-Tompkins algorithm to detect QRS complexes. The data was analyzed in two ways: feeding raw ECG signals into 1D CNN-based models and converting signals into images for 2D CNN models. The GRU-based 1D CNN model achieved the highest accuracy of 93.4%, while the CNN2D model achieved 92.16% accuracy. These results demonstrate effective automated arrhythmia classification, offering a promising tool for real-world healthcare applications, especially in resource-limited settings.

## Method Summary
The study combined five public ECG datasets (PTB-XL, PTB Diagnostic, China 12-Lead, Georgia 12-Lead, St. Petersburg INCART) totaling 8,360 recordings across 5 balanced arrhythmia classes. Two processing pipelines were developed: (1) raw 12-lead ECG signals processed with a modified Pan-Tompkins algorithm for QRS detection fed into 1D CNN/GRU models, and (2) ECG signals converted to 506×187 grayscale images fed into 2D CNN/ResNet/ViT models. Models were trained with Adam optimizer (learning rate 0.001 → 1e-6), batch size 50, and evaluated using 10-fold cross-validation on 80/20 train-test splits. The 1D CNN+GRU architecture emerged as the top performer at 93.42% accuracy.

## Key Results
- CNN1D+GRU model achieved highest accuracy of 93.42% with F1 score of 0.9338
- CNN2D model achieved 92.16% accuracy on converted ECG images
- Vision Transformer models showed lower performance (73% untrained, 84% pretrained)
- Model struggles particularly with distinguishing First-Degree AV Block from normal rhythms

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Temporal-Spatial Feature Extraction
The integration of Convolutional layers with Gated Recurrent Units (GRU) outperforms single-architecture approaches by capturing both local morphology and sequential dependencies. 1D CNN layers act as feature extractors for QRS morphology, while the subsequent GRU layer models temporal relationships between these features across the signal duration. This is particularly effective for arrhythmias like AF that depend on temporal patterns spanning multiple beats.

### Mechanism 2: Signal-to-Image Transposition for Spatial Context
Converting raw 12-lead vector data into 2D grayscale images allows standard vision architectures to extract features from the relative spatial layout of the leads. By plotting the 12 leads onto a single image canvas, the model can treat the ECG as a texture map, detecting simultaneous deviations across multiple leads that indicate specific cardiac vectors.

### Mechanism 3: Targeted QRS Enhancement via Pan-Tompkins
Pre-processing signals with a variant of the Pan-Tompkins algorithm focuses the model's attention on the QRS complex, acting as a noise filter and region-of-interest selector. The algorithm bandpass filters (5-15 Hz), differentiates, and squares the signal to highlight R-peaks, suppressing baseline wander and muscle noise.

## Foundational Learning

- **Concept: Pan-Tompkins Algorithm**
  - Why needed here: This is the primary data sanitation step. Understanding that it bandpass filters (5-15 Hz) and integrates the signal to find peaks is crucial for debugging why certain noisy signals might fail classification or generate false QRS triggers.
  - Quick check question: Does the algorithm output a cleaned signal or the specific indices (timestamps) of the R-peaks? (Answer: Indices/peaks, per Section 4.1).

- **Concept: Recurrent Neural Networks (GRU/LSTM)**
  - Why needed here: The winning architecture uses GRUs. You must understand that GRUs maintain a "hidden state" that carries information from previous time steps to the current one, which is how the model "remembers" the rhythm (e.g., irregularity in AF) rather than just analyzing a static snapshot.
  - Quick check question: Why might a GRU be preferred over a simple 1D CNN for detecting Atrial Fibrillation? (Answer: AF is defined by irregularity over time, a temporal dependency that GRUs are explicitly designed to capture).

- **Concept: 12-Lead ECG Topology**
  - Why needed here: The paper treats this as a multi-variate time series or a 2D image. Knowing that leads I, II, III are limb leads and V1-V6 are precordial helps in understanding why an image-based approach might capture "spatial" heart vector information that a flattened 1D array might obscure.
  - Quick check question: In the image conversion step, are the 12 leads stacked as channels (like RGB) or plotted sequentially on a canvas? (Answer: Plotted sequentially on a single grayscale canvas, per Section 4.2).

## Architecture Onboarding

- **Component map**: Input (12-lead ECG vectors or 506×187 images) → Pan-Tompkins preprocessing or image conversion → 1D CNN layers → GRU/LSTM → Dense layers → Softmax output

- **Critical path**: The CNN1D + GRU pathway is the priority. The authors explicitly state it achieved 93.4% accuracy. The interaction between the 1D convolutional feature maps feeding into the GRU's temporal processing is the key to replicate.

- **Design tradeoffs**:
  - 1D vs. 2D: 1D models are computationally lighter and slightly more accurate (93.4% vs 92.16%). 2D models allow transfer learning but require more data/hardware and showed lower performance (73% untrained, 84% pretrained) in this specific study.
  - GRU vs. LSTM: The study found GRU-based models performed better than LSTM-based models (93.4% vs 83.9%), likely due to the dataset size or the simpler gating mechanism of GRU being sufficient for the task.

- **Failure signatures**:
  - Class Confusion (IAVB): The model struggles to distinguish First-Degree AV Block (IAVB) from Sinus Rhythm (SNR) and Sinus Bradycardia (SB). This is likely because the primary differentiator (prolonged PR interval) is subtle.
  - Noise Sensitivity: The simplified Pan-Tompkins struggles with extreme noise. If inputs are not quality-checked, the QRS detection step will fail, propagating errors.

- **First 3 experiments**:
  1. **Baseline Replication**: Implement the 1D CNN + GRU model on the raw signal data using the 5-class split to verify the 93.4% benchmark.
  2. **Ablation on Input**: Compare performance of the GRU model on Raw Signal vs. Pan-Tompkins Enhanced Signal to validate the algorithm's contribution.
  3. **Image vs. Signal**: Train the 2D CNN on the generated images and compare confusion matrices against the 1D model, specifically checking if the 2D spatial view improves IAVB detection.

## Open Questions the Paper Calls Out

- **Question**: Can the proposed lightweight models maintain reported accuracy levels when deployed on resource-constrained wearable devices for real-time monitoring?
  - Basis in paper: The conclusion states the approach "could be integrated into wearable ECG monitoring systems" for "real-time arrhythmia detection."
  - Why unresolved: The experiments utilized an NVIDIA GTX 1080 Ti GPU, but the study did not evaluate inference latency, power consumption, or performance degradation on embedded hardware.
  - What evidence would resolve it: Benchmarks of inference speed and accuracy running locally on edge devices.

- **Question**: How does the classification performance degrade when exposed to the "extreme noise" artifacts that were excluded during the image preprocessing stage?
  - Basis in paper: Section 4.2 notes that images with "visible instances of abnormal noise" were "excluded from the classification process" to maintain integrity.
  - Why unresolved: The study filtered out challenging noisy data rather than solving the segmentation failure, leaving the model's robustness to severe artifacts unknown.
  - What evidence would resolve it: Accuracy and F1 scores on a test set containing the previously discarded high-noise ECG recordings.

- **Question**: Would explicitly extracting and feeding PR interval duration features to the model significantly improve the classification of First-degree Atrioventricular Block (IAVB)?
  - Basis in paper: Section 7 attributes the poor IAVB classification to "subtle characteristics, such as a prolonged PR interval," which the network struggles to differentiate from normal rhythms.
  - Why unresolved: The current "black box" feature extraction may miss this specific temporal nuance, leading to misclassification as Sinus Rhythm or Bradycardia.
  - What evidence would resolve it: Comparative results showing IAVB F1 scores for the current model versus a model augmented with explicit PR interval metadata.

## Limitations
- Simplified Pan-Tompkins implementation shows sensitivity to extreme noise, potentially compromising QRS detection accuracy
- Image conversion at 506×187 resolution may lose critical diagnostic information for certain arrhythmias
- Model shows particular weakness in distinguishing First-Degree AV Block from normal rhythms due to subtle PR interval differences

## Confidence
- **High Confidence**: The CNN1D+GRU architecture's superiority (93.4% accuracy) and the general feasibility of ECG classification using deep learning
- **Medium Confidence**: The specific contribution of the Pan-Tompkins preprocessing and the image conversion methodology
- **Low Confidence**: The exact architectural details and hyperparameters, which are not fully specified

## Next Checks
1. **Noise Robustness Test**: Systematically inject varying levels of Gaussian noise into the ECG signals and measure classification performance degradation
2. **Architectural Ablation**: Remove the Pan-Tompkins preprocessing step and compare results against the claimed 93.4% accuracy to quantify its contribution
3. **Class-Specific Analysis**: Generate confusion matrices focusing on IAVB classification to identify specific failure patterns and potential feature engineering opportunities