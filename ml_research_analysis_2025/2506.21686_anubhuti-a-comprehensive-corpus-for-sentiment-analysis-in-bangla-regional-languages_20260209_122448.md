---
ver: rpa2
title: 'ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional
  Languages'
arxiv_id: '2506.21686'
source_url: https://arxiv.org/abs/2506.21686
tags:
- sentiment
- dataset
- bangla
- regional
- dialects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents ANUBHUTI, a novel dataset for sentiment analysis
  in Bangla regional dialects. The dataset consists of 10,000 sentences manually translated
  from standard Bangla into four major regional dialects: Mymensingh, Noakhali, Sylhet,
  and Chittagong.'
---

# ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages

## Quick Facts
- arXiv ID: 2506.21686
- Source URL: https://arxiv.org/abs/2506.21686
- Reference count: 13
- 10,000 manually translated Bangla sentences across 4 regional dialects with dual-layer sentiment annotations

## Executive Summary
This paper introduces ANUBHUTI, a novel dataset for sentiment analysis in four major Bangla regional dialects (Mymensingh, Noakhali, Sylhet, Chittagong). The corpus contains 10,000 sentences manually translated from standard Bangla and annotated with both thematic labels (Political, Religious, Neutral) and multilabel emotion tags (7 categories). Expert native translators performed the work, achieving substantial inter-annotator agreement (Cohen's Kappa 0.76-0.84). The dataset addresses a critical gap in resources for low-resource dialects and supports development of dialect-aware NLP systems, though it is currently domain-limited to political and religious content.

## Method Summary
The dataset was created by sourcing 10,000 sentences from the MONOVAB corpus, then having two expert native translators per region manually translate them into their respective dialects while preserving sentiment polarity. Each sentence received dual-layer annotation: a multiclass thematic label and multilabel emotion tags. Quality assurance involved semantic verification against source text, anomaly detection for formatting issues, and dialectal spelling standardization. The final corpus is provided as CSV with text, dialect ID, thematic label, and binary emotion indicators.

## Key Results
- 10,000 sentences manually translated into 4 Bangla regional dialects
- Dual-layer annotation: multiclass thematic (Political/Religious/Neutral) + multilabel emotions (7 categories)
- Strong inter-annotator agreement: Cohen's Kappa 0.76-0.84 across dialects
- Balanced regional distribution (2,500 sentences per dialect)
- Domain skew: 64% Neutral, 19% Political, 17% Religious content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native dialect speakers can preserve sentiment polarity during translation better than automated methods.
- Mechanism: Expert translators with regional linguistic familiarity maintain semantic alignment and emotional tone through contextual understanding of idiomatic expressions and dialect-specific vocabulary.
- Core assumption: Regional dialect experts can consistently translate sentiment-bearing content without introducing bias or losing emotional nuance.
- Evidence anchors:
  - [abstract] "Expert native translators performed the translation and annotation, achieving strong inter-annotator agreement (Cohen's Kappa 0.76–0.84 across dialects)"
  - [section 3.2] "These experts' linguistic familiarity and contextual understanding of local expressions allowed them to preserve each sentence's semantic integrity and sentiment polarity during translation"
  - [corpus] Neighbor paper "Bridging Dialects" explores neural translation to regional variants, suggesting this remains an active challenge; no direct comparison to automated translation performance is provided in ANUBHUTI
- Break condition: If translators introduce systematic regional bias or if dialectal sentiment expressions map poorly to standard Bangla emotion categories.

### Mechanism 2
- Claim: Dual-layer annotation (thematic + emotional) captures sentiment complexity that single-label schemes miss.
- Mechanism: Multiclass thematic labels provide domain context (Political/Religious/Neutral) while multilabel emotion tags allow overlapping emotional states, reflecting that dialectal speech often expresses mixed sentiments.
- Core assumption: Emotions in regional dialects follow the same categorical framework as standard Bangla; the seven emotion classes are culturally appropriate.
- Evidence anchors:
  - [abstract] "Each sentence is annotated with both a multiclass thematic label (Political, Religious, or Neutral) and a multilabel emotion tag"
  - [section 3.6] "A sentence could exhibit one, multiple, or even no emotion labels depending on its emotional tone"
  - [corpus] Weak corpus evidence—no neighboring papers specifically validate this dual-annotation approach for Bangla dialects
- Break condition: If thematic and emotional labels correlate strongly enough that one predicts the other, reducing annotation value.

### Mechanism 3
- Claim: Multi-stage quality assurance (semantic verification + anomaly detection + spelling standardization) reduces labeling noise in dialect corpora without standardized orthography.
- Mechanism: Manual cross-checking against source text, automated detection of formatting anomalies, and dialect-specific spelling verification together filter translation errors and inconsistencies.
- Core assumption: Anomalies (excessive punctuation, mixed dialects, non-Bangla scripts) correlate with lower annotation quality.
- Evidence anchors:
  - [section 3.5] "This framework consists of four primary stages: Semantic and Sentiment Verification, Missing Data Detection, Anomaly Detection, Dialectal Spelling Standardization"
  - [section 3.5.3] Anomaly detection flagged entries with "excessive punctuation (e.g., '!!!!', '....'), non-Bangla characters or scripts"
  - [corpus] No corpus evidence on effectiveness of these specific quality control measures for dialect datasets
- Break condition: If manual review introduces subjectivity that outweighs noise reduction benefits.

## Foundational Learning

- Concept: **Cohen's Kappa for inter-annotator agreement**
  - Why needed here: Quantifies translation/annotation consistency beyond chance; scores of 0.76-0.84 indicate "substantial to almost perfect" agreement per the paper's interpretation
  - Quick check question: If two annotators agree 90% of the time but expected chance agreement is 80%, what is Cohen's Kappa? (Answer: 0.50)

- Concept: **Multilabel vs. multiclass classification**
  - Why needed here: ANUBHUTI uses both—thematic labels are mutually exclusive (multiclass), but emotion labels can co-occur (multilabel, e.g., Anger + Contempt)
  - Quick check question: Can a sentence be labeled both Political and Religious? (Answer: No—these are multiclass and mutually exclusive)

- Concept: **Low-resource dialect NLP challenges**
  - Why needed here: Regional Bangla dialects lack standardized orthography, annotated corpora, and pre-trained models; this motivates the dataset's creation
  - Quick check question: Why might a model trained on standard Bangla fail on Sylhet dialect? (Answer: Vocabulary differences, syntactic variations, lack of dialect-specific embeddings)

## Architecture Onboarding

- Component map: Source corpus (MONOVAB) -> Standard Bangla sentences -> Translation layer -> 4 regional dialects (2 expert translators each) -> Quality assurance -> Semantic check, anomaly detection, spelling verification -> Annotation layer -> Multiclass (thematic) + Multilabel (emotions) -> Output -> CSV with columns: [text, dialect_id, target, anger, contempt, disgust, enjoyment, fear, sadness, surprise]

- Critical path: Translation quality determines downstream model performance; poor dialectal authenticity cannot be recovered by annotation or modeling.

- Design tradeoffs:
  - Domain focus (political/religious) vs. generalizability to other domains
  - Manual translation vs. scalability (10K sentences required ~8 expert translators)
  - 7 emotion categories vs. finer-grained or culture-specific emotion taxonomies

- Failure signatures:
  - Class imbalance: Figure 2 shows 64% Neutral, 19% Political, 17% Religious—may bias classifiers toward Neutral
  - Emotion distribution skew: Anger (732), Contempt (921), Disgust (871) dominate; Fear (47), Surprise (56) are rare
  - Cohen's Kappa variation: Chittagong (0.76) lower than Mymensingh (0.84)—suggests inconsistent difficulty across dialects

- First 3 experiments:
  1. **Baseline benchmarking**: Train BanglaBERT on ANUBHUTI for both multiclass thematic and multilabel emotion tasks; report macro-F1 to handle class imbalance
  2. **Cross-dialect transfer**: Train on 3 dialects, test on held-out 4th to measure dialect-specific vs. generalizable patterns
  3. **Annotation ablation**: Compare models using only thematic labels vs. only emotion labels vs. both to validate the dual-annotation design choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the translation-based methodology of ANUBHUTI capture the sentiment nuances of naturally occurring dialectal text?
- Basis in paper: [explicit] Section 4 states that future improvements involve "collecting more naturally occurring dialectal text," acknowledging that the current translated sentences may differ from organic usage.
- Why unresolved: The current corpus relies on manual translation from Standard Bangla rather than sourcing original, informal dialectal posts, potentially missing spontaneous idioms.
- What evidence would resolve it: A comparative analysis of sentiment classification performance between this dataset and a newly curated corpus of scraped, native dialectal social media posts.

### Open Question 2
- Question: How does model performance vary when applied to domains outside the political and religious contexts dominant in this corpus?
- Basis in paper: [explicit] Section 4 highlights "Limited Domain Coverage" as a limitation and suggests future work should include "diverse domains such as sports, entertainment, or daily conversation."
- Why unresolved: The dataset is heavily skewed toward the socio-political landscape, potentially biasing models against lexical patterns found in casual or non-controversial topics.
- What evidence would resolve it: Training models on ANUBHUTI and evaluating them on out-of-domain dialect datasets to measure generalization capabilities.

### Open Question 3
- Question: What are the baseline performance benchmarks for transformer-based models on this specific multilabel emotion detection task?
- Basis in paper: [inferred] The paper focuses on data curation and quality assurance (Cohen's Kappa) but does not report experimental results or classification benchmarks.
- Why unresolved: Without baseline experiments, the dataset's efficacy for training Deep Learning or transformer models (like BanglaBERT) remains unquantified.
- What evidence would resolve it: Conducting experiments using standard NLP architectures to establish F1-scores for both thematic and emotional classification across the four dialects.

## Limitations

- Domain limitation: Corpus heavily skewed toward political and religious content (36% combined), limiting generalizability to other sentiment-bearing domains
- Emotion category validity: Seven emotion categories not empirically validated for regional dialect usage across all four dialects
- Translation methodology: Manual translation from standard Bangla may not capture naturally occurring dialectal sentiment expressions

## Confidence

**High confidence**: Dataset creation methodology is well-documented with clear annotation guidelines, quality control procedures, and inter-annotator agreement metrics.

**Medium confidence**: Dual-layer annotation design is theoretically sound but lacks empirical validation through model performance comparisons.

**Low confidence**: Representativeness of 10,000 sentences across full spectrum of regional dialect usage is unclear due to domain skew.

## Next Checks

1. **Domain generalization test**: Train sentiment models on ANUBHUTI and evaluate on dialectal text from non-political/religious domains (e.g., social media, product reviews) to assess domain transfer limitations.

2. **Emotion category validation**: Conduct a cultural appropriateness study where regional speakers evaluate whether the seven emotion categories capture their dialectal emotional expression patterns, particularly for less-represented emotions like Fear and Surprise.

3. **Quality assurance effectiveness audit**: Systematically analyze samples that passed the three-stage quality control to identify any remaining translation inconsistencies, dialect mixing, or annotation errors that escaped detection.