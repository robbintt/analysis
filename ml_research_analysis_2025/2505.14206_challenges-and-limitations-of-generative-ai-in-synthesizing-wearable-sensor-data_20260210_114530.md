---
ver: rpa2
title: Challenges and Limitations of Generative AI in Synthesizing Wearable Sensor
  Data
arxiv_id: '2505.14206'
source_url: https://arxiv.org/abs/2505.14206
tags:
- data
- synthetic
- generation
- generative
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates generative AI models for synthesizing wearable\
  \ sensor data, focusing on multimodal, long-range, and conditional generation challenges.\
  \ The study assesses state-of-the-art GAN and diffusion models (BioDiffusion, TTS-CGAN,\
  \ SSSD, P2PCON D, WaveGAN \u2217CON D) using three real-world datasets (WESAD, SWELL,\
  \ CASE) with ECG and EDA signals."
---

# Challenges and Limitations of Generative AI in Synthesizing Wearable Sensor Data

## Quick Facts
- **arXiv ID:** 2505.14206
- **Source URL:** https://arxiv.org/abs/2505.14206
- **Reference count:** 40
- **Primary result:** Diffusion models (especially BioDiffusion) outperform GANs for wearable sensor synthesis, but cross-modal quality gaps and limited data augmentation utility persist.

## Executive Summary
This paper evaluates state-of-the-art generative models for synthesizing wearable sensor data, focusing on multimodal, long-range, and conditional generation challenges. Using three real-world datasets (WESAD, SWELL, CASE) with ECG and EDA signals, the study assesses BioDiffusion, TTS-CGAN, SSSD, P2PCON D, and WaveGAN ∗CON D models. A comprehensive evaluation framework combines task-independent quality metrics (MMD, discriminative score, DTWD, entropy) and task-dependent utility assessment via downstream classification tasks. BioDiffusion consistently outperforms other models, achieving the lowest MMD values (1.47-1.96) and discriminative scores (0.23-0.17) while maintaining minimal performance degradation in train-on-synthetic, test-on-real scenarios (drops of only 0.7-6.2%). However, data augmentation improvements remain modest (2-3%), and cross-modal quality gaps persist, with EDA synthesis outperforming ECG.

## Method Summary
The study evaluates five generative models across three real-world wearable sensor datasets. Models are assessed using task-independent metrics (MMD, discriminative score, DTWD, entropy) and task-dependent utility via downstream classification tasks. The evaluation framework includes visual inspection, t-SNE plots, and train-on-synthetic-test-on-real scenarios. BioDiffusion, a diffusion-based model with U-Net backbone and attention layers, is compared against GAN variants including TTS-CGAN, SSSD, P2PCON D, and WaveGAN ∗CON D.

## Key Results
- BioDiffusion achieves lowest MMD values (1.47-1.96) and discriminative scores (0.23-0.17) across all datasets
- Conditional models significantly outperform unconditional variants in capturing class-specific characteristics
- EDA synthesis consistently outperforms ECG synthesis, with MMD differences of +1.01 to +0.6 and discriminative score increases of +11% to +23%
- Data augmentation improves classification accuracy by only 2-3%, indicating limited practical utility
- Cross-modal generation via channel concatenation creates quality gaps between modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models outperform GANs for wearable sensor data synthesis under multimodal, long-range, conditional generation constraints.
- Mechanism: BioDiffusion uses iterative denoising through a U-Net backbone with 1D convolutions and multi-head attention layers. The forward process gradually adds Gaussian noise over T=1000 timesteps; the reverse process learns to denoise. Attention layers capture long-range dependencies (LRD) that CNN-based GANs miss due to limited receptive fields. Classifier-free guidance conditions generation by training on both conditional and unconditional samples, then combining their scores during inference.
- Core assumption: Denoising score matching effectively captures the joint distribution of multimodal physiological signals with complex temporal dependencies.
- Evidence anchors:
  - [abstract] "BioDiffusion consistently outperforms other models, achieving the lowest MMD values (1.47-1.96) and discriminative scores (0.23-0.17)"
  - [section V-A] "BioDiffusion achieves the most effective modeling in 4 out of 6 dataset-signal pairs" for DTWD (temporal coherence metric)
  - [corpus] DiM-TS paper confirms diffusion models struggle with long-range dependencies in time series, suggesting BioDiffusion's attention mechanism addresses this gap
- Break condition: When sequence lengths exceed model's effective context window, or when cross-modal correlations are too complex for implicit joint learning through channel concatenation.

### Mechanism 2
- Claim: Conditional generation enables single-model multi-class synthesis and outperforms training separate unconditional models per class.
- Mechanism: Conditional models (TTS-CGAN, BioDiffusion, SSSD, P2P CON D, WaveGAN* CON D) incorporate class labels through embedding layers combined with generator/denoiser inputs. This allows knowledge transfer across classes via shared latent space. For diffusion, classifier-free guidance balances conditional vs. unconditional generation. For GANs, conditional batch normalization or auxiliary classifiers inject class information.
- Core assumption: Classes share underlying signal structure that can be captured in a joint latent representation while preserving class-specific characteristics.
- Evidence anchors:
  - [section V-A] "TimeGAN fails to generate meaningful results... as an unconditional generative model, TimeGAN lacks the ability to leverage transfer learning effects between classes"
  - [section V-A] "conditional models often learn a shared latent space that may capture common information among classes, facilitating knowledge transfer"
  - [corpus] No direct corpus evidence on conditional vs. unconditional comparison for sensor data; this is a paper-specific finding
- Break condition: When minority classes have insufficient samples to learn robust class-conditional distributions, or when high-dimensional conditioning spaces create sparse coverage.

### Mechanism 3
- Claim: Joint multimodal generation via channel concatenation produces cross-modal quality gaps, with EDA synthesis consistently outperforming ECG.
- Mechanism: Current models concatenate ECG and EDA as separate input channels processed by unified networks. ECG has quasi-periodic waveforms requiring precise morphological fidelity; EDA has slow tonic baselines with phasic peaks. The unified optimization objective may prioritize easier signal dynamics (EDA) over complex ones (ECG), or the network lacks mechanisms to enforce cross-modal temporal consistency.
- Core assumption: Processing modalities together implicitly learns their joint distribution without explicit correlation modeling.
- Evidence anchors:
  - [abstract] "cross-modal quality gaps persist, with EDA synthesis outperforming ECG"
  - [section V-B] "MMD differences are +1.01, +0.6, and +0.47, while the discriminative scores show increases of +11%, +23%, and +20% for WESAD, SWELL, and CASE datasets, respectively. These results underscore the lower realism of synthetic ECG compared to EDA."
  - [section V-B] "configurations of TTS-CGAN and SSSD optimized for ECG quality consistently yield the best data utility"
  - [corpus] DiM-TS paper mentions "complex channel interrelations" as a challenge for diffusion models, supporting the cross-modal difficulty finding
- Break condition: When signals have fundamentally different timescales or when precise phase alignment between modalities is required.

## Foundational Learning

- Concept: **Denoising Diffusion Probabilistic Models (DDPM)**
  - Why needed here: BioDiffusion builds on DDPM principles. Understanding the forward/reeverse process, noise schedulers, and the role of timesteps is essential before implementing or debugging diffusion-based synthesis.
  - Quick check question: Can you explain why the paper uses T=1000 timesteps and what happens if you reduce this for faster sampling?

- Concept: **Time Series Evaluation Metrics (beyond visual inspection)**
  - Why needed here: No FID equivalent exists for time series. The paper introduces multiple metrics (MMD, discriminative score, DTWD, entropy) that capture different quality aspects. Understanding what each measures prevents over-reliance on single metrics.
  - Quick check question: Why might low feature-based distances (CD, L2) show good quality while MMD and discriminative scores reveal problems?

- Concept: **Multimodal Fusion Architectures**
  - Why needed here: The paper reveals that simple channel concatenation has limitations. Understanding alternatives (multi-agent GANs with separate discriminators, cross-attention mechanisms, hierarchical architectures) is necessary for advancing beyond current performance.
  - Quick check question: How might a multi-discriminator architecture balance ECG vs. EDA quality differently than single-discriminator channel concatenation?

## Architecture Onboarding

- Component map:
  - **Input Processing**: Resampling (100Hz/128Hz), 10-s windowing (1000-1280 samples), normalization (critical—unnormalized data caused failures), class label encoding
  - **Generator/Denoiser**: U-Net with 1D convolutions (BioDiffusion, P2P CON D), Transformer encoder (TTS-CGAN), or SSSM backbone (SSSD); conditional embedding integration
  - **Discriminator/Classifier**: GAN discriminator (often shared across modalities), auxiliary classifier for conditional variants
  - **Training Loop**: WGAN-GP objective for GANs (λ=10), reconstruction loss for diffusion (averaged across timesteps), guidance scale factor (3.0 for conditional diffusion)
  - **Evaluation**: t-SNE visualization, MMD, discriminative score (multi-classifier ensemble), DTWD, entropy, downstream TSTR/DA tasks

- Critical path:
  1. Data normalization is non-negotiable—multiple models failed with unnormalized data (Table IV)
  2. Checkpoint selection requires balancing generator/discriminator losses for GANs; reconstruction loss alone is insufficient for diffusion (Section V-D warns of deceptive low errors at high timesteps)
  3. Conditional embedding must be properly integrated before training—classifier-free guidance requires training on both conditional and unconditional samples

- Design tradeoffs:
  - **Sequence length vs. computational cost**: Longer sequences (1280 samples) test LRD modeling but require more memory and attention complexity
  - **Single-model multimodal vs. separate models**: Unified model enables cross-modal learning but risks quality gaps; separate models optimize per-modality but lose joint distribution information
  - **Checkpoint selection criteria**: Generator loss minimum may coincide with discriminator weakness; reconstruction loss averaging masks early-timestep problems

- Failure signatures:
  - **Mode collapse**: P2P CON D and WaveGAN* CON D failed to generate class 0 with unnormalized data (Table IV); TimeGAN produced "extremely low-quality data" across all configurations
  - **Cross-modal imbalance**: Configuration "optimized for ECG fail to produce comparable results for EDA, and vice versa" (Section V-B)
  - **Degraded downstream utility**: TSTR drops exceeding 10% indicate synthetic data fails to capture task-relevant patterns

- First 3 experiments:
  1. Reproduce BioDiffusion baseline on single dataset (WESAD recommended—smallest, fastest) with normalized data, evaluating MMD and discriminative score; verify checkpoint selection doesn't produce mode collapse
  2. Ablate conditioning: train unconditional BioDiffusion per-class vs. conditional single model; compare sample quality and TSTR performance to quantify transfer learning benefit
  3. Test cross-modal sensitivity: train BioDiffusion on ECG-only vs. EDA-only vs. joint; compare per-modality quality metrics to isolate whether joint training degrades either modality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can leading generative models efficiently synthesize multimodal sensing data in parallel, or are specialized approaches necessary?
- **Basis in paper:** [explicit] Section II-A, 1) explicitly states, "the question remains open as to whether leading generative models can efficiently synthesize multimodal sensing data in parallel or whether new specialized approaches are necessary."
- **Why unresolved:** Current standard methods (e.g., channel concatenation) often fail to capture intrinsic multimodal distributions, leading to cross-modal inconsistencies.
- **What evidence would resolve it:** A comparative study demonstrating that a specialized architecture significantly outperforms standard channel-concatenation baselines in cross-modal coherence metrics.

### Open Question 2
- **Question:** How can a universally accepted, standardized evaluation metric (analogous to FID for images) be established for synthetic time series?
- **Basis in paper:** [explicit] Section V-D highlights the "urgent need for standardized evaluation metrics, analogous to FID for synthetic images, to seamlessly integrate inference within training."
- **Why unresolved:** Existing metrics are fragmented, and proxies like Context-FID rely on time series embeddings that are poorly understood and lack transferability to wearable data.
- **What evidence would resolve it:** The proposal and validation of a metric that correlates strongly with both human expert assessment and downstream utility across diverse wearable sensor datasets.

### Open Question 3
- **Question:** How can text-conditioned generation be effectively adapted for wearable sensor data?
- **Basis in paper:** [explicit] Section VI lists text-conditioned generation as a future direction but notes, "generating informative and contextually relevant text for these signals remains an open challenge."
- **Why unresolved:** Unlike images, wearable sensor data lacks direct semantic correspondence to natural language, and current generalist text-to-time-series models are limited to simple attributes like trend.
- **What evidence would resolve it:** A generative model that successfully utilizes text prompts to control specific physiological attributes (e.g., "high stress ECG") with high fidelity.

## Limitations

- Cross-modal quality gaps persist between ECG and EDA synthesis, with EDA consistently outperforming ECG
- Data augmentation improvements are modest (2-3%), indicating limited practical utility for downstream tasks
- Current evaluation framework relies on task-specific classification accuracy, which may not capture all relevant quality aspects

## Confidence

- **High confidence**: Diffusion models outperform GANs (BioDiffusion achieves lowest MMD 1.47-1.96 and discriminative scores 0.23-0.17)
- **Medium confidence**: Conditional generation provides meaningful transfer learning benefits (though specific quantitative comparisons between conditional vs. unconditional variants are limited)
- **Medium confidence**: EDA synthesis consistently outperforms ECG (supported by multiple metrics but lacking architectural explanation)
- **Low confidence**: Current limitations represent fundamental barriers rather than opportunities for architectural improvements

## Next Checks

1. **Architectural ablation study**: Replace BioDiffusion's U-Net with pure Transformer architecture (similar to TTS-CGAN) while maintaining diffusion training; compare ECG vs. EDA quality to isolate whether U-Net vs. Transformer design drives cross-modal differences

2. **Cross-task generalization test**: Evaluate synthetic data utility beyond binary classification (e.g., multi-class stress detection, regression of stress intensity, anomaly detection); determine if 2-3% DA gains are task-specific or represent fundamental utility ceiling

3. **Controlled modality separation**: Train separate BioDiffusion models for ECG and EDA (not jointly), then compare joint vs. separate model quality and downstream performance; quantify trade-off between cross-modal learning benefits and quality degradation