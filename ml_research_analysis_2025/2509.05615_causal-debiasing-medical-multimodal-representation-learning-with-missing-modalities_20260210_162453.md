---
ver: rpa2
title: Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities
arxiv_id: '2509.05615'
source_url: https://arxiv.org/abs/2509.05615
tags:
- uni0000018f
- data
- causal
- uni0000013a
- uni00000002
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CaD, a causal debiasing framework for medical
  multimodal representation learning under missing modalities. The method addresses
  two key biases: missingness bias from non-random modality availability and distribution
  bias from latent confounders affecting both features and outcomes.'
---

# Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities

## Quick Facts
- arXiv ID: 2509.05615
- Source URL: https://arxiv.org/abs/2509.05615
- Reference count: 40
- Primary result: Up to 3.19% AUC-PRC gains on ICU mortality prediction using causal debiasing for missing modalities

## Executive Summary
This paper introduces CaD, a causal debiasing framework for medical multimodal representation learning under missing modalities. The method addresses two key biases: missingness bias from non-random modality availability and distribution bias from latent confounders affecting both features and outcomes. CaD uses a dual-branch neural network to disentangle causal from spurious features, combined with a missingness deconfounding module that approximates causal intervention through backdoor adjustment. The framework is model-agnostic and can be integrated with existing direct prediction approaches. Experiments on four real-world medical datasets show consistent improvements: up to 3.19% AUC-PRC gains on mortality prediction and 5.12% on readmission tasks, with robust performance across different missingness patterns including MCAR and MNAR settings.

## Method Summary
CaD implements a dual-branch GNN with edge-wise gating to disentangle causal and bias features, paired with a missingness deconfounding module (MDM) that approximates causal intervention. The framework first trains with disentanglement loss (L_dis) for 15 epochs, then adds counterfactual supervision for up to 100 total epochs. The MDM constructs a confounder dictionary by clustering features from artificially masked samples, then applies NWGM attention to adjust representations. The method uses Generalized Cross Entropy (GCE) to amplify bias learning in one branch while standard Cross Entropy trains the causal branch. Performance is evaluated across four medical datasets using AUC-ROC and AUC-PRC metrics.

## Key Results
- CaD improves ICU mortality prediction by up to 3.19% AUC-PRC compared to state-of-the-art baselines
- Consistent gains across different missingness patterns including MCAR and MNAR settings
- Robust performance on diverse medical tasks: ICU readmission, AD progression, and AFib classification
- Ablation studies show both CBDM and MDM modules contribute significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1
Modeling missingness patterns as a latent confounder and applying backdoor adjustment mitigates bias from non-random data acquisition. The framework assumes modality availability is influenced by patient status, constructing a "confounder dictionary" by clustering features from samples with artificially masked modalities. During prediction, it uses attention (Normalized Weighted Geometric Mean) to approximate the interventional distribution P(Y|do(X_O)) by marginalizing over these confounder prototypes, effectively severing the link between the missing pattern and the outcome. Core assumption: the missingness pattern carries semantic information correlated with the outcome (MNAR or MAR), rather than being purely random noise. Break condition: fails if missingness is completely at random (MCAR) with zero correlation to patient covariates.

### Mechanism 2
Disentangling representations into "causal" and "bias" branches using specialized loss functions isolates invariant features from spurious correlations. A dual-branch Graph Neural Network (GNN) splits the information flow, with the "bias" branch trained with Generalized Cross Entropy (GCE) that amplifies gradients for "easy" (high-confidence/biased) samples, forcing this branch to capture shortcuts. The "causal" branch uses standard Cross Entropy to learn residual, robust features. Core assumption: the dataset contains spurious correlations that are easier to learn than true causal signals and can be mathematically separated. Break condition: fails if the "bias" features are the only predictive features available, leaving the causal branch with no signal.

### Mechanism 3
Counterfactual sample augmentation enforces independence between causal and bias embeddings. To ensure the causal branch Z_c doesn't rely on information leaking from the bias branch Z_b, the method creates counterfactual samples by combining a patient's causal features Z_c with a random patient's bias features Z_b-hat. The model is penalized if the prediction changes based on the swapped bias features. Core assumption: bias features are exchangeable across patients and distinct enough from causal features that swapping them disrupts spurious prediction without destroying label logic. Break condition: fails if bias and causal features are inextricably entangled in the latent space, making the swap operation infeasible or nonsensical.

## Foundational Learning

**Concept: Structural Causal Models (SCM) & Backdoor Adjustment**
Why needed: The paper frames the missing modality problem as a causal inference problem where "missingness" confounds the relationship between data and outcome. You must understand do-calculus to grasp why the MDM module calculates P(Y|do(X_O)). Quick check: Can you explain why simply conditioning on the observed data X_O fails if the missingness mechanism M is influenced by the outcome Y?

**Concept: Generalized Cross Entropy (GCE) Loss**
Why needed: The CBDM module relies on GCE to selectively amplify learning of "biased" samples. Unlike standard CE, GCE behaves like mean absolute error for easy samples, preventing the bias branch from being overwhelmed by hard examples. Quick check: How does the gradient behavior of GCE differ from standard Cross Entropy for samples with high prediction confidence?

**Concept: Graph Neural Networks (GNN) on Bipartite Graphs**
Why needed: The base architecture encodes patients and modalities as nodes in a bipartite graph. Understanding message passing is required to see how missing modalities are handled naturally by the graph structure (as missing edges). Quick check: In a patient-modality bipartite graph, how does the representation update for a patient node with missing modalities compared to one with full modalities?

## Architecture Onboarding

**Component map:** Raw modalities -> Backbone Encoder -> Bipartite Graph -> CBDM (Dual-Branch GNN) -> MDM (Confounder Module) -> Prediction Head

**Critical path:** The success of the model hinges on the confounder dictionary construction in MDM. If the prototypes (clusters of masked features) do not represent the true latent confounders (e.g., clinical severity), the backdoor adjustment will be ineffective.

**Design tradeoffs:**
- Dictionary Size (K): Small K (e.g., 16) limits granularity (worse on large datasets like MIMIC); Large K (e.g., 128) risks overfitting or noise (worse on small datasets like AFib). Paper finds K=64 optimal for large datasets.
- Pre-trained Backbone: MDM requires a pre-trained encoder to build the dictionary. Choice of backbone (GRAPE vs. MUSE) has minimal impact, but it must be pre-trained.

**Failure signatures:**
- Performance Plateau: If MDM improves performance but CBDM does not, the dataset likely suffers primarily from missingness bias rather than distribution bias.
- AFib Result Pattern: Smaller datasets with fewer observable patterns show modest gains; expect higher variance and reduced effectiveness of complex debiasing here.

**First 3 experiments:**
1. Entropy Analysis: Replicate the Normalized Conditional Entropy (NCE) analysis on your target dataset to confirm that missingness patterns correlate with labels.
2. Ablation MDM Only: Run the model with only the MDM module active (disable CBDM) to isolate the impact of missingness correction.
3. Visualization: Plot t-SNE of the Z_c (Causal) vs Z_b (Bias) embeddings. If Z_c separates classes cleanly while Z_b does not, the disentanglement is successful.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the integration of explicit causal structures derived from domain knowledge improve the CaD framework's ability to handle missing modalities? Basis: The authors state that "the integration of an explicit causal structure from multimodal data with the proposed model would be the subject of future work." Why unresolved: The current framework relies on representation-based disentanglement rather than enforcing a fixed, domain-specific structural causal model. What evidence would resolve it: A modified CaD architecture that ingests known biological causal graphs, demonstrating improved performance over the purely data-driven approach.

**Open Question 2:** How can semi-supervised learning strategies be effectively combined with the CaD framework from a causal perspective? Basis: The supplementary material notes, "We plan to further investigate semi-supervised learning methods from a causal perspective in future work." Why unresolved: The current study focuses on direct prediction with observed labels, explicitly excluding semi-supervised methods despite label missingness being a common clinical challenge. What evidence would resolve it: A theoretical extension of the CaD loss function that accounts for missing labels Y, validated on datasets with high rates of unlabeled patient data.

**Open Question 3:** To what extent do the learned "causal" representations correspond to actual physiological disease mechanisms? Basis: The limitations section states the approach "does not aim to explicitly uncover mechanistic or physiological disease pathways," despite learning "causal" features. Why unresolved: The model defines "causal" features as those invariant to distribution shifts and missingness patterns, but this mathematical definition may not align with clinical ground truth regarding disease etiology. What evidence would resolve it: A clinical study where the disentangled embeddings Z_c are validated against independent clinical biomarkers to confirm they capture true disease signals.

## Limitations

- Strong causal assumptions about missingness patterns forming confounders may not hold in all clinical settings
- Effectiveness depends heavily on accurate representation learning and successful disentanglement
- Dictionary-based confounder adjustment requires sufficient data to build meaningful clusters and may struggle with small datasets

## Confidence

- High confidence: The existence of modality missingness bias in medical datasets (supported by entropy analysis and baseline performance gaps)
- Medium confidence: The causal interpretation of the framework's mechanisms (the backdoor adjustment assumption is theoretically sound but hard to validate empirically)
- Medium confidence: The quantitative performance improvements (gains are consistent but baselines show convergence issues on small datasets)

## Next Checks

1. Conduct sensitivity analysis varying dictionary size K across different dataset scales to validate the claimed optimal settings
2. Test the counterfactual loss mechanism by training with swapped bias features from the same patient versus different patients to verify the bias/causal separation
3. Evaluate performance under different missingness mechanisms (MCAR vs MNAR) with varying levels of correlation between missingness and outcomes to test the framework's assumptions