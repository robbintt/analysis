---
ver: rpa2
title: 'CoLLM: A Large Language Model for Composed Image Retrieval'
arxiv_id: '2503.19910'
source_url: https://arxiv.org/abs/2503.19910
tags:
- image
- text
- modification
- mtcir
- collm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoLLM introduces a method for composed image retrieval (CIR) that
  eliminates the need for expensive triplet datasets by synthesizing triplets on-the-fly
  from image-caption pairs. The approach combines spherical linear interpolation for
  reference image embeddings and LLM-based generation of modification text, enabling
  supervised training without manual annotations.
---

# CoLLM: A Large Language Model for Composed Image Retrieval

## Quick Facts
- arXiv ID: 2503.19910
- Source URL: https://arxiv.org/abs/2503.19910
- Reference count: 40
- Primary result: Introduces on-the-fly triplet synthesis for composed image retrieval, achieving up to 15% improvement in recall over state-of-the-art methods

## Executive Summary
CoLLM presents a novel approach to composed image retrieval that eliminates the need for expensive manually annotated triplet datasets. The method synthesizes triplets on-the-fly from abundant image-caption pairs using spherical linear interpolation (Slerp) for reference image embeddings and LLM-based generation of modification text. By leveraging Large Language Embedding Models (LLEMs) for multimodal query fusion, CoLLM achieves state-of-the-art performance on refined CIR benchmarks. The approach is validated on newly refined versions of CIRR and Fashion-IQ, demonstrating significant improvements in recall metrics while reducing dependence on costly annotations.

## Method Summary
CoLLM employs a two-stage training process. During pre-training, it synthesizes CIR triplets on-the-fly from image-caption pairs: it finds nearest neighbors via CLIP similarity, applies Slerp interpolation (α=0.5) to create reference image embeddings, and generates modification text through template-based caption interpolation. The model uses three query embeddings (image-only, text-only, and composed) with a combined contrastive loss. In fine-tuning, CoLLM is trained on the MTCIR dataset using standard CIR triplet loss. The architecture maps visual features to LLM semantic space via an adapter, processes composed queries through an LLM, and projects the output for retrieval. The approach eliminates expensive triplet annotations while achieving superior multimodal understanding compared to shallow transformers.

## Key Results
- Achieves up to 15% improvement in recall over state-of-the-art methods on refined benchmarks
- Eliminates need for expensive triplet datasets through on-the-fly synthesis from image-caption pairs
- Introduces MTCIR, a large-scale dataset with 3.4M image pairs and 17.7M modification texts
- LLEM-based composed query embeddings outperform shallow transformers, validated through controlled comparisons

## Why This Works (Mechanism)

### Mechanism 1: On-the-Fly Triplet Synthesis via Slerp Interpolation
Synthesizing CIR triplets from abundant image-caption pairs enables supervised training without manual annotation. For each target image, the method finds in-batch nearest neighbors via CLIP similarity and uses Slerp interpolation (α=0.5) to synthesize reference image embeddings between the augmented image and its neighbor. Template-based interpolation generates modification text between captions. This approach leverages the abundance of image-caption data while creating realistic composition scenarios. Performance degrades when α approaches 0 or 1, as shown in Figure 7a.

### Mechanism 2: LLM/LLEM-based Composed Query Embedding
Large Language Embedding Models (LLEMs) outperform shallow transformers for multimodal query fusion due to embedded world knowledge. The method maps visual features to LLM semantic space via adapter g(·), constructs composed queries as instruction templates combining adapted image embedding and modification text, and processes these through the LLM to generate unified embeddings. This approach facilitates deeper multimodal fusion compared to LaSCo's single embedding for both image and text. Table 7 shows SFR-Embedding-2 (LLEM) outperforms base Mistral-7B across all benchmarks.

### Mechanism 3: MTCIR Dataset Quality via Two-Stage MLLM+LLM Pipeline
Multiple short modification texts per image pair improve training diversity and alignment with human query patterns. The pipeline first uses MLLM (LLaVA-Next-34B) to generate detailed captions for each image, then employs LLM (Claude 3 Sonnet) to generate multiple modification texts across 6 categories with in-context learning from CIRR examples. This hierarchical text generation captures diverse attributes more effectively than single long descriptions. Table 1 shows MTCIR averages 5.18 texts/pair versus 1 for other datasets.

## Foundational Learning

- **Contrastive Learning for Retrieval**
  - Why needed here: Core training objective aligns query embeddings with target image embeddings while pushing apart negatives. Loss combines image-to-image, text-to-image, and composed queries.
  - Quick check question: Given a batch of N image-caption pairs, how would you compute in-batch negatives for contrastive loss?

- **Spherical Linear Interpolation (Slerp)**
  - Why needed here: Used to synthesize reference image embeddings that smoothly interpolate between two real image embeddings on the unit hypersphere.
  - Quick check question: Why is Slerp preferred over linear interpolation for normalized embeddings? (Hint: consider angular vs. Euclidean distance)

- **Vision-Language Model Alignment**
  - Why needed here: Adapter g(·) must map vision encoder output to LLM embedding space. Understanding CLIP/BLIP pre-training helps diagnose alignment failures.
  - Quick check question: If your adapter produces embeddings with low cosine similarity to target image embeddings, which component would you debug first—vision encoder, adapter, or LLM?

## Architecture Onboarding

- **Component map:** Vision encoder f(·) -> Image adapter g(·) -> LLM/LLEM Φ(·) -> Projection p(·)
- **Critical path:** 1. Pre-training: 5M image-caption pairs → on-the-fly triplet synthesis → LLM training with 3-way contrastive loss. 2. Fine-tuning: MTCIR triplets → composed query only → single contrastive loss. 3. Inference: (reference_image, modification_text) → adapted visual token + text → LLM → retrieval embedding → similarity search
- **Design tradeoffs:** CLIP requires vision encoder tuning; BLIP-L performs better frozen (Table 14). LoRA rank 64 for pre-training, 16 for fine-tuning (reduces overfitting). α=0.5 and 75% text synthesis ratio optimal (Figure 7). LLEM (SFR-Embedding-2) outperforms base LLM (Mistral-7B) for retrieval.
- **Failure signatures:** Recall@1 low but Recall@50 high → query embedding captures coarse semantics but lacks discriminative detail. Text-only query outperforms composed query → image adapter not aligning properly (Table 20 diagnostic). Overfitting on synthetic datasets → reduce LoRA rank or training epochs (Table 15 shows LaSCo overfits after epoch 1). CIRR subset recall contradicts full index recall → unreliable metric (Section 9.1).
- **First 3 experiments:** 1. Validate triplet synthesis: Train with/without Slerp interpolation and text synthesis on held-out image-caption pairs. Compare recall metrics to establish baseline contribution of each component. 2. Ablate LLM choice: Swap SFR-Embedding-2 for base Mistral-7B with identical training setup. Measure gap on CIRCO/CIRR to quantify LLEM benefit (expect ~2-3% improvement from Table 7). 3. Sanity check on refined benchmarks: Evaluate pre-trained model on original vs. refined CIRR validation. If recall improves significantly on refined set, original benchmark has ambiguity confounding your metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does increasing the number of visual tokens for the reference image improve retrieval performance compared to the single synthesized token approach?
- **Basis in paper:** The authors note their triplet synthesis method generates a single visual token, which "constrains the model's ability to process detailed image information."
- **Why unresolved:** It is unclear if the bottleneck is the semantic compression of the synthesis method or the limited capacity of a single token to represent complex visual features.
- **What evidence would resolve it:** Ablation studies comparing the performance of single-token versus multi-token reference embeddings on tasks requiring fine-grained visual discrimination.

### Open Question 2
- **Question:** Can natively multimodal models (MLLMs) outperform the proposed adapter-based LLM approach for composed query understanding?
- **Basis in paper:** The paper explicitly suggests that future work could benefit from "exploring pre-trained MLLMs" rather than limiting the architecture to LLEMs.
- **Why unresolved:** The current architecture maps visual features to a frozen LLM via an adapter; it is unknown if models with native visual grounding would handle composition more effectively.
- **What evidence would resolve it:** Benchmarking a fine-tuned MLLM (e.g., LLaVA) using the CoLLM training strategy against the current adapter-based CoLLM on refined CIRR and Fashion-IQ benchmarks.

### Open Question 3
- **Question:** Does incorporating text category information (e.g., object added vs. attribute changed) as auxiliary supervision improve model generalization?
- **Basis in paper:** The authors identify the "underutilized text category information in our synthetic datasets" as an area for future work to improve generalization.
- **Why unresolved:** The MTCIR dataset includes category labels, but the current training objective does not explicitly leverage them to guide the composition process.
- **What evidence would resolve it:** Experiments adding category-conditioned prompts or auxiliary classification losses during training and evaluating performance across distinct modification types.

## Limitations

- MTCIR dataset release timeline is unclear, creating significant barrier to reproducing fine-tuning results
- Adapter architecture specification is ambiguous (linear layer, MLP, or other not detailed)
- Direct comparison of synthetic vs. human-annotated triplet quality remains unverified
- Single visual token for reference image may constrain detailed visual understanding

## Confidence

**CoLLM achieves state-of-the-art performance with up to 15% improvement in recall** - Medium confidence
Supported by Table 6 but depends on refined benchmarks not publicly available for independent verification

**LLM-based composed query embeddings outperform shallow transformers** - High confidence
Clear evidence in Table 7 with controlled comparisons and ablation studies

**On-the-fly triplet synthesis eliminates need for expensive annotations** - Medium confidence
Demonstrates successful training on synthetic triplets but lacks direct comparison against human-annotated triplets

## Next Checks

1. **Verify Triplet Synthesis Quality** - When MTCIR becomes available, conduct controlled experiment training CoLLM with and without Slerp interpolation and text synthesis. Measure contribution of each component to final recall metrics on held-out validation set.

2. **Independent Benchmark Reproduction** - Once refined benchmarks are released, reproduce reported results on CIRR and Fashion-IQ using exact same evaluation protocols. Pay attention to "CIRR subset" metric warning in Section 9.1.

3. **Adapter Architecture Sensitivity Analysis** - Systematically vary adapter g() architecture (linear vs. MLP, different hidden dimensions) while keeping other components constant. Measure impact on retrieval performance to establish sensitivity to this critical component.