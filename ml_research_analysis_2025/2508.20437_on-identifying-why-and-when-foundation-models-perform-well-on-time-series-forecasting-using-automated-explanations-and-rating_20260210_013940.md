---
ver: rpa2
title: On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting
  Using Automated Explanations and Rating
arxiv_id: '2508.20437'
source_url: https://arxiv.org/abs/2508.20437
tags:
- series
- forecasting
- finance
- time-series
- chronos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates four forecasting models\u2014ARIMA, Gradient\
  \ Boosting, Chronos, and LLaMA (fine-tuned and base)\u2014across four time-series\
  \ domains: finance, power, pedestrian, and car parts. Gradient Boosting consistently\
  \ outperformed others, especially in volatile or sparse domains, while Chronos excelled\
  \ in stable, trend-driven contexts like finance."
---

# On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating

## Quick Facts
- arXiv ID: 2508.20437
- Source URL: https://arxiv.org/abs/2508.20437
- Authors: Michael Widener; Kausik Lakkaraju; John Aydin; Biplav Srivastava
- Reference count: 22
- Primary result: Feature-engineered models (Gradient Boosting) outperform foundation models in volatile/sparse domains, while foundation models excel only in stable/trend-driven contexts like finance.

## Executive Summary
This paper evaluates four forecasting models—ARIMA, Gradient Boosting, Chronos, and LLaMA (fine-tuned and base)—across four time-series domains: finance, power, pedestrian, and car parts. Gradient Boosting consistently outperformed others, especially in volatile or sparse domains, while Chronos excelled in stable, trend-driven contexts like finance. LLaMA without fine-tuning failed in most settings. SHAP and LIME analyses revealed that Gradient Boosting relied heavily on engineered features, while Chronos defaulted to statistical aggregates in complex domains. Rating-Driven Explanations (RDE) showed that Gradient Boosting achieved uniform errors across series but was sensitive to temporal groups, whereas Chronos provided consistent errors across time periods. The findings emphasize the need for domain-specific feature engineering and robustness checks beyond accuracy metrics.

## Method Summary
The study compares four forecasting approaches across four heterogeneous time-series domains using standardized preprocessing, context/horizon windows, and evaluation metrics (sMAPE, MASE). Models include ARIMA (statsmodels with grid search), Gradient Boosting (LightGBM with extensive feature engineering), Chronos (AutoGluon TimeSeries), and LLaMA (LoRA fine-tuning with NF4 quantization). Explainability uses TreeSHAP for GBoost, surrogate models for Chronos, and LIME for temporal patterns. RDE framework provides causal robustness analysis via ATE and WRS metrics. Experiments use 80-20 train/test splits with domain-specific temporal windows.

## Key Results
- Gradient Boosting achieved superior MASE scores across all domains, particularly excelling in volatile (Power: 0.84) and sparse (Car Parts: 0.91) datasets
- Chronos performed best in stable, trend-driven finance data (MASE: 5.48) but failed on sparse car parts (infinite MASE)
- Unmodified LLaMA-3.1-8B failed dramatically across all domains (MASE up to 88.66 in finance)
- SHAP analysis showed GBoost relied on engineered features while Chronos defaulted to statistical aggregates in complex domains
- RDE revealed GBoost's uniform accuracy across series but temporal sensitivity, while foundation models showed series-specific consistency

## Why This Works (Mechanism)

### Mechanism 1
Feature-engineered models outperform foundation models in volatile/sparse domains because domain-specific transformations capture meaningful patterns that pretrained representations cannot leverage without explicit engineering. Gradient Boosting uses engineered features (time-based, Fourier terms for seasonality, domain-specific statistics like log returns/volatility/moving averages, tailored lags) that directly encode domain knowledge. When data has irregular seasonality, extreme sparsity (75.9% zeros in Car Parts), or high-frequency volatility, these explicit features provide signal that foundation models' learned representations fail to extract.

### Mechanism 2
Foundation models succeed in stable, trend-driven contexts because their pretraining inductive bias aligns with data exhibiting consistent temporal structures (autocorrelation, regular seasonality). Chronos discretizes time-series through scaling and quantization, learning general temporal patterns from large-scale pretraining. In finance (daily closing prices with trend momentum), these learned patterns transfer well. The surrogate model analysis shows Chronos defaults to statistical aggregates (expanding means/standard deviations) when patterns become complex—adequate for trend-driven data but insufficient for irregular dynamics.

### Mechanism 3
Rating-Driven Explanations (RDE) reveal model reliability dimensions (series-specific consistency via ATE, temporal stability via WRS) that traditional XAI methods cannot capture because SHAP/LIME focus on feature attribution rather than robustness under intervention. RDE uses causal reasoning with defined treatment (T), outcome (O), and protected attributes (Z). ATE estimates causal effect of series identity on residuals after confounder adjustment (G-computation). WRS measures statistical differences in outcomes across protected groups via weighted t-tests. This quantifies whether models perform fairly across subgroups—information SHAP/LIME do not provide.

## Foundational Learning

- **SHAP (SHapley Additive exPlanations)**: Used for feature attribution analysis across all models (TreeSHAP for Gradient Boosting, surrogate models for Chronos). Understanding SHAP values is essential to interpret why models succeed/fail. *Quick check*: Given a model prediction, can you explain what SHAP values represent and how to interpret the direction/magnitude of feature contributions?

- **Time-series forecasting fundamentals (context window, horizon, autoregressive vs. direct inference)**: Paper defines domain-specific context/horizon windows (e.g., Finance: C=20, H=5; Power: C=1440, H=360) and uses both autoregressive (iterative rollout) and direct inference setups. Understanding these is critical for reproducing experiments. *Quick check*: For a minutely power consumption series, why might autoregressive rollout accumulate error compared to direct multi-step prediction?

- **Causal inference basics (treatment, outcome, confounders, ATE)**: RDE framework is "causally grounded"—ATE uses G-computation to adjust for confounders when estimating treatment effects. Without this foundation, rating metrics cannot be properly interpreted. *Quick check*: If a model produces higher errors for certain car parts, how would you determine whether this is due to the part type itself vs. a confounding factor like month-of-year sales patterns?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> Model-specific feature engineering/windowing -> Forecasting with iterative/direct inference -> Residual computation -> SHAP/LIME analysis -> RDE hypothesis testing -> Rating comparison against biased/random baselines
- **Critical path**: Data preprocessing → Model-specific feature engineering (for GBoost) or windowing (for FMs) → Forecasting with iterative/direct inference → Residual computation → SHAP/LIME analysis → RDE hypothesis testing → Rating comparison against biased/random baselines
- **Design tradeoffs**: Autoregressive inference offers flexibility but accumulates error over long horizons vs. direct inference (computationally expensive for large datasets); feature engineering investment has high upfront cost but enables GBoost dominance vs. foundation models (zero-shot but require pretraining alignment); XAI vs. RDE: SHAP/LIME provide local interpretability but miss global fairness/robustness vs. RDE (causal robustness but requires hypothesis specification)
- **Failure signatures**: ARIMA on high-frequency data: MASE explodes (Power: 2.89 vs. GBoost 0.84) due to linearity assumption violations; unmodified LLaMA on temporal data: MASE=88.66 (Finance) indicates complete failure—pretraining distribution mismatch; Chronos on sparse data: Infinite MASE for Car Parts—model cannot handle extreme zero-inflation without domain adaptation; GBoost temporal sensitivity: Low ATE but high WRS means accurate on average but unreliable for specific months/days; LIME segmentation instability: Different segment boundaries produce substantially different importance patterns
- **First 3 experiments**:
  1. Reproduce baseline comparison: Run all four models (ARIMA, GBoost, Chronos, LLaMA-base) on Finance dataset with paper's C=20/H=5 windowing. Verify MASE rankings (GBoost≈4.32, Chronos≈5.48, ARIMA≈7.69) match reported values within standard deviation.
  2. Feature engineering ablation: Train GBoost on Power dataset with progressively reduced feature sets (remove Fourier terms, then moving averages, then lag features). Measure MASE degradation to quantify contribution of each feature category to the 0.84 baseline performance.
  3. RDE sensitivity analysis: Compute ATE and WRS for GBoost on Car Parts dataset while varying the protected attribute definition (month vs. quarter vs. part category). Document whether series-specific vs. temporal sensitivity rankings change significantly.

## Open Questions the Paper Calls Out

- Can advanced segmentation techniques, such as dynamic or saliency-based sampling, improve the stability and reliability of LIME explanations for time-series forecasting models compared to the uniform segmentation used in current studies? (Section 6.2: "LIME adaptations could be more thoroughly explored," specifically suggesting that advanced techniques like dynamic segmentation "may improve explanation stability for temporal data.")

- How do Rating-Driven Explanations (RDE) metrics compare against other fairness-oriented XAI approaches or synthetic benchmarks with ground-truth fairness properties in detecting model bias? (Section 6.1: "are not directly compared against other fairness-oriented XAI approaches" and were not evaluated on "synthetic benchmarks with ground-truth fairness properties," leaving this comparison for future work.)

- Does incorporating physics-informed constraints into temporal foundation models (TFMs) improve their predictive performance in volatile or sparse domains (e.g., power, car parts) while maintaining interpretability? (Section 6.2: "incorporating physics-informed constraints into temporal foundation models (TFMs) may improve their ability to capture domain-specific dynamics while maintaining interpretability.")

- Does evaluating forecast outputs beyond point predictions (e.g., analyzing the midpoint or maximum/minimum values of the horizon) reveal distinct insights into model failure modes that are missed by current evaluation protocols? (Section 6.2: "expanding the evaluation of forecast outputs beyond point predictions, such as analyzing the first predicted value, midpoint of the forecast horizon, or derived features... could reveal new insights into model behavior.")

## Limitations

- Exact feature engineering specifications for Gradient Boosting are incomplete, making precise reproduction challenging
- Foundation model performance depends heavily on pretraining data overlap with target domains (data contamination suspected in pedestrian/car datasets)
- RDE framework assumes correct causal structure specification, which may not hold in all time-series contexts
- No explicit comparison with state-of-the-art domain-specific models that might outperform foundation models
- Limited ablation studies on foundation model architecture choices (model size, fine-tuning depth)

## Confidence

- High confidence in Gradient Boosting feature engineering advantage in volatile/sparse domains (supported by multiple experiments and clear mechanism)
- Medium confidence in foundation model success conditions (Chronos performed well only in finance, but data contamination issues complicate interpretation)
- Medium confidence in RDE framework utility (methodologically sound but no direct comparison with alternative robustness metrics)
- Low confidence in LLaMA performance conclusions (single run, potential implementation variations, no sensitivity analysis)

## Next Checks

1. Conduct data contamination verification by testing Chronos on completely out-of-distribution time-series domains not present in its pretraining corpus
2. Implement systematic feature ablation for Gradient Boosting across all four domains to quantify individual feature contributions to performance gains
3. Compare RDE metrics against alternative robustness measures (cross-validation stability, bootstrap confidence intervals) to validate their unique contribution