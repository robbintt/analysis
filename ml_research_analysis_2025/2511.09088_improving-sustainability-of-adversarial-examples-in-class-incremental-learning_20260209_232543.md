---
ver: rpa2
title: Improving Sustainability of Adversarial Examples in Class-Incremental Learning
arxiv_id: '2511.09088'
source_url: https://arxiv.org/abs/2511.09088
tags:
- learning
- classes
- semantics
- attack
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adversarial example (AE) sustainability
  in class-incremental learning (CIL), where traditional AEs fail due to domain drift
  across model updates. The authors propose SAE, which enhances AE semantics by making
  them similar to the target class while distinguishing them from all other classes.
---

# Improving Sustainability of Adversarial Examples in Class-Incremental Learning

## Quick Facts
- arXiv ID: 2511.09088
- Source URL: https://arxiv.org/abs/2511.09088
- Reference count: 15
- Primary result: SAE achieves 31.28% average improvement in Attack Success Rate over baselines when CIL models update with 9-fold increase in classes

## Executive Summary
This paper addresses the fundamental challenge of adversarial example sustainability in class-incremental learning (CIL), where traditional adversarial attacks fail due to domain drift across model updates. The authors propose SAE (Sustainable Adversarial Examples), which enhances AE semantics by making them similar to the target class while distinguishing them from all other classes. SAE combines semantic information from a visual-language model (CLIP) with CIL model gradients to guide optimization, ensuring target-class consistency even as the model evolves through incremental learning tasks.

## Method Summary
SAE employs a dual-module approach: the Semantic Correction Module (SCM) optimizes perturbations by maximizing similarity with target class text embeddings while minimizing similarity with non-target embeddings in CLIP's shared latent space, and incorporating CIL model gradients as correction signals. The Filtering-and-Augmentation Module (FAM) removes non-target examples containing target-class features by filtering based on cosine similarity in the model's latent space, then augments remaining samples through rotation and scaling. The method is evaluated on CIFAR-100 and ImageNet-100 datasets, showing significant improvements in Attack Success Rate sustainability across multiple CIL methods including LwF, EWC, and ER-Reservoir.

## Key Results
- SAE achieves 31.28% average improvement in Attack Success Rate when CIL models update with 9-fold increase in classes
- Outperforms baseline attacks significantly in SASR across various CIL methods and target classes
- Demonstrates enhanced sustainability compared to traditional adversarial attacks that fail due to CIL domain drift

## Why This Works (Mechanism)

### Mechanism 1
Aligning adversarial perturbations with universal semantic embeddings (CLIP) reduces overfitting to specific parameters of the initial surrogate model. The Semantic Correction Module optimizes perturbations by maximizing similarity with target class text embeddings and minimizing similarity with non-target embeddings in a shared latent space, rather than relying solely on classification logits. This assumes semantic features extracted by CLIP are sufficiently universal to remain valid anchors even as the CIL model undergoes domain drift.

### Mechanism 2
Incorporating gradients from the initial CIL model corrects the optimization trajectory to ensure the perturbation is effective against the specific target model architecture, not just the semantic space. The SCM adds a Surrogate Loss derived from the initial model's logits, acting as a regularizer to ensure the perturbation actually moves the decision boundary for the specific CIL architecture. This assumes the initial model retains enough structural similarity to future models that its gradients remain relevant guides.

### Mechanism 3
Removing non-target data samples that accidentally contain target-class features reduces semantic fluctuation during optimization. The Filtering-and-Augmentation Module calculates cosine similarity between latent embeddings of POOD samples and the target class, discarding samples with similarity above threshold. This prevents the optimizer from being confused by samples that visually match the target class but are labeled as non-target.

## Foundational Learning

- **Concept: Class-Incremental Learning (CIL) & Catastrophic Forgetting**
  - Why needed here: The paper addresses the failure of AEs specifically due to CIL domain drift. You must understand that CIL models change their decision boundaries over time to understand why "static" AEs fail.
  - Quick check question: Can you explain why a model that learns "Task B" might lose accuracy on "Task A," and how this affects the decision boundary?

- **Concept: Targeted vs. Untargeted Adversarial Attacks**
  - Why needed here: The paper focuses on targeted attacks (forcing a specific misclassification), which is harder to sustain than untargeted attacks under domain drift.
  - Quick check question: What is the optimization objective difference between maximizing loss (untargeted) and minimizing loss for a specific target class (targeted)?

- **Concept: Vision-Language Models (CLIP) & Joint Embeddings**
  - Why needed here: The core "Semantic Correction" relies on CLIP providing a stable text-image embedding space.
  - Quick check question: In a joint embedding space, how does the distance between a text vector "car" and an image vector of a car compare to the distance between "car" and "truck"?

## Architecture Onboarding

- **Component map:** Inputs (POOD Dataset, Target Class Label, Initial CIL Model) -> FAM (Filter samples based on cosine similarity, Augment) -> SCM (Optimization Loop: CLIP forward pass + CIL model forward pass, Compute combined loss, Update perturbation) -> Output (Universal Perturbation)

- **Critical path:** The Filtering-and-Augmentation Module (FAM) is critical. If "confusing" samples are not removed, the semantic direction calculated by SCM becomes noisy, reducing the sustainability of the attack.

- **Design tradeoffs:**
  - Filtering Threshold (σ): Higher threshold retains more data but risks including confusing semantics; the paper suggests 0.7 works well but this is dataset-dependent
  - Loss Weighting: Relying too much on CLIP ignores the specific CIL model; relying too much on the initial model risks overfitting to the initial state

- **Failure signatures:**
  - High Initial ASR, Rapid Decay: Suggests the perturbation overfitted to the initial model and lacked universal semantic features
  - Low ASR everywhere: Suggests the semantic anchor (CLIP) failed to align or the perturbation budget was too constrained
  - High Variance across target classes: Suggests the Filtering module is struggling to find "clean" non-target samples for specific classes with high visual overlap

- **First 3 experiments:**
  1. Baseline Reproduction: Run SAE vs. MIFGSM on a simple CIL setup (e.g., CIFAR-100, 10 classes per step) to verify the "sustainability gap" (31.28% improvement claim)
  2. Ablation Study: Run SAE with SCM-only vs. FAM-only to isolate the contribution of semantic correction vs. data filtering
  3. Hyperparameter Sensitivity: Vary the filtering threshold σ to observe the trade-off between data volume and semantic noise

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SAE effectively target classes introduced in later incremental tasks, given its reliance on the initial model for gradient correction? The experimental setup restricts target classes to the first 10 classes, and the Semantic Correction Module depends on the initial model. If the target class is unknown to the initial model, the optimization guidance is undefined.

- **Open Question 2:** How does the sustainability of untargeted adversarial examples compare to targeted AEs under CIL domain drift? The authors state "Untargeted AEs are not considered in this work," assuming sufficient perturbation is effective.

- **Open Question 3:** Can CIL-specific defense mechanisms be developed to resist sustainable AEs without the severe trade-offs in clean accuracy observed in the study? Results show standard defenses like Adversarial Training lower clean accuracy by ~13-15%.

## Limitations
- The paper's reliance on CLIP's universal semantic space introduces uncertainty about performance on datasets with highly domain-specific visual concepts not well-represented in CLIP's training corpus.
- The filtering threshold (σ=0.7) appears empirically determined without systematic sensitivity analysis across different datasets or class types.
- The paper does not address computational overhead—running SAE requires multiple forward passes through both CLIP and the CIL model during optimization.

## Confidence

- **High Confidence**: The core claim that semantic alignment via CLIP improves AE sustainability is well-supported by quantitative results showing consistent 20-40% improvements across multiple CIL methods and datasets.
- **Medium Confidence**: The claim that filtering non-target samples with target-class features improves stability is supported by ablation studies, though the optimal threshold appears dataset-dependent.
- **Low Confidence**: The paper's assertion that SAE generalizes to arbitrary CIL methods without modification is based on testing only three specific methods, leaving open questions about performance with dynamic architecture methods.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary σ from 0.5 to 0.9 and measure SASR impact across different target classes to determine if the optimal threshold correlates with class semantic similarity.

2. **CLIP Model Ablation**: Replace CLIP ViT-B-32 with smaller/larger variants (ViT-B-16, ResNet-50) to test whether the semantic space universality claim holds across different visual-language model architectures.

3. **Dynamic Architecture Test**: Implement SAE against a CIL method with dynamic network expansion (e.g., DER) to validate whether the initial model's gradient guidance remains effective when the architecture changes significantly between tasks.