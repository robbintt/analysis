---
ver: rpa2
title: Large Language Models Predict Human Well-being -- But Not Equally Everywhere
arxiv_id: '2507.06141'
source_url: https://arxiv.org/abs/2507.06141
tags:
- life
- satisfaction
- llms
- these
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study benchmarks four large language models (LLMs) in predicting
  individual life satisfaction across 64 countries, using a balanced dataset of 64,000
  individuals from the World Values Survey. LLMs generally captured broad well-being
  correlates like income and health, but their predictive accuracy was lower than
  traditional statistical models, with mean absolute errors up to 31% higher than
  OLS regression.
---

# Large Language Models Predict Human Well-being -- But Not Equally Everywhere

## Quick Facts
- arXiv ID: 2507.06141
- Source URL: https://arxiv.org/abs/2507.06141
- Reference count: 0
- LLMs predict life satisfaction with up to 31% higher mean absolute error than traditional models, especially in underrepresented countries

## Executive Summary
This study benchmarks four large language models (GPT-4o mini, Claude 3.5 Haiku, LLaMA 3.3 70B, Gemma 3 27B) in predicting individual life satisfaction across 64 countries using World Values Survey data. While LLMs generally captured broad well-being correlates like income and health, their predictive accuracy was significantly lower than traditional statistical models. The study reveals systematic geographic biases, with performance degrading most severely in underrepresented and low-resource countries. LLMs underweighted empirically strong predictors like income and perceived freedom while overemphasizing factors like education and democracy.

## Method Summary
The study used World Values Survey Wave 7 data from 64 countries, constructing a balanced dataset of 64,000 individuals (1,000 per country). LLMs were prompted with natural-language demographic and attitudinal profiles to predict life satisfaction scores (0-10 scale). Performance was benchmarked against OLS and Lasso regression models using identical covariates. A pre-registered experiment tested semantic generalization through synthetic variables and evaluated context injection strategies to improve predictions in specific regions.

## Key Results
- LLM mean absolute errors were up to 31% higher than OLS regression baselines
- Performance degraded systematically in countries underrepresented in training data, correlating with HDI and internet usage
- LLMs underweighted income and perceived freedom while overemphasizing education and democracy
- Country fixed effects were compressed, failing to capture real cross-national variation
- Semantic generalization experiments revealed models interpolate based on linguistic similarity rather than conceptual understanding

## Why This Works (Mechanism)

### Mechanism 1: Training Data Geographic Bias
- Claim: LLM prediction accuracy degrades systematically in countries underrepresented in training corpora, correlating with digital infrastructure and economic development indicators.
- Mechanism: Training corpora are dominated by content from high-income, English-speaking, digitally connected regions. Models internalize well-being associations from these contexts and misapply them to underrepresented regions where income gradients are steeper and different predictors matter.
- Core assumption: Web-scraped training data reflects digital participation patterns, creating systematic gaps for low-internet-penetration regions.
- Evidence anchors: [abstract] prediction accuracy decreases in countries underrepresented in training data; [section] Figure 5 Panel A shows negative correlation with HDI, log GDP per capita, and internet usage.

### Mechanism 2: Semantic Proximity Substitution for Conceptual Understanding
- Claim: LLMs interpolate predictions along linguistic similarity gradients rather than reasoning from causal structure, producing smooth but empirically unfounded predictions.
- Mechanism: When presented with semantically adjacent prompts, models generalize from trained endpoints to untrained intermediate conditions, creating plausible-looking outputs that diverge from empirical relationships.
- Core assumption: The interpolation behavior observed with synthetic variables generalizes to real-world concept gradients.
- Evidence anchors: [abstract] LLMs rely on surface-level linguistic similarity rather than conceptual understanding; [section] Figure 6 shows clear semantic interpolation across unseen conditions with effects reversing when endpoint mappings were reversed.

### Mechanism 3: Discourse-Driven Feature Weighting
- Claim: LLMs overweight predictors that are rhetorically prominent in training discourse (education, democracy) and underweight empirically strong but less-discussed predictors (income, perceived freedom).
- Mechanism: Models learn associations from text where education and democracy are framed as markers of success, even when survey data shows weak or flat relationships after controlling for confounders.
- Core assumption: The frequency and framing of concepts in training text influences model weighting independent of empirical validity.
- Evidence anchors: [abstract] LLMs underweighted important predictors such as income and perceived freedom while overemphasizing factors like education and democracy; [section] Figure 3 shows perceived freedom consistently underweighted across all models while education is predicted to have strong upward relationships.

## Foundational Learning

- **Concept: Distributional Robustness / Subgroup Performance**
  - Why needed here: The core finding is that aggregate performance masks severe degradation for specific subgroups (low-HDI countries, extreme satisfaction values). Understanding how to measure and report performance across strata is essential.
  - Quick check question: If your model achieves 85% accuracy overall, what additional analyses would you need before deploying it across 64 countries?

- **Concept: Embedding Space Geometry and Semantic Similarity**
  - Why needed here: The semantic generalization experiment relies on cosine similarity between prompt embeddings. Understanding how models organize conceptual space explains why linguistically adjacent prompts receive similar predictions.
  - Quick check question: Why might two prompts with high cosine similarity in embedding space require fundamentally different outputs in a well-being prediction task?

- **Concept: Fixed Effects and Unexplained Variation**
  - Why needed here: The paper uses country fixed effects to capture cultural, institutional, and linguistic variation beyond individual predictors. LLMs compressed these effects, revealing they don't meaningfully integrate national context.
  - Quick check question: After controlling for income, health, and employment, what might cause life satisfaction to differ systematically between Mexico and Zimbabwe?

## Architecture Onboarding

- **Component map:** Structured demographic/attitudinal features → natural language prompts → LLM API calls → numeric predictions (0-10) → comparison against self-reported values → regression analysis → coefficient and fixed effects comparison

- **Critical path:** 1. Prompt construction from WVS variables → 2. LLM API calls for predictions → 3. Extract numeric scores (0-10) → 4. Compare against self-reported values → 5. Regress predictions on covariates to recover implied coefficients → 6. Compare coefficient structure and fixed effects between LLM and OLS

- **Design tradeoffs:**
  - Generalization strength vs. spillover risk: GPT/Gemma show strong interpolation (responsive to injection) but high unintended spillover; Claude shows conservative generalization (limited spillover) but reduced responsiveness
  - Injection specificity vs. scope: Targeted regional facts improve specific predictions but alter model behavior for unrelated regions
  - Prompt format: Natural language prompts enable zero-shot prediction but obscure feature contributions compared to structured inputs

- **Failure signatures:**
  - U-shaped error distribution: highest errors at extreme satisfaction values (≤4, ≈10), lowest at mid-range (6-8)
  - Compressed country fixed effects: range ±0.2 to ±0.5 vs. actual range of -2.1 to +1.0
  - Homogenized income gradients: identical slopes across high-HDI and low-HDI countries despite empirically steeper gradients in low-HDI contexts
  - Smooth interpolation across untrained semantic conditions (indicates surface-level generalization)

- **First 3 experiments:**
  1. **Geographic holdout validation:** Train traditional models on high-HDI countries only, evaluate on low-HDI countries. Compare degradation pattern to LLM performance gap. Isolates training data bias from model architecture effects.
  2. **Semantic gradient stress test:** Create novel semantic continua with known ground-truth relationships (not synthetic/fabricated). Test whether interpolation respects or violates empirical structure. Quantifies semantic overgeneralization in real domains.
  3. **Targeted injection with spillover measurement:** Inject region-specific facts for one country, measure prediction changes across all 64 countries. Map spillover radius and identify conditions that contain effects to intended targets. Establishes practical limits of context-based correction.

## Open Questions the Paper Calls Out
None

## Limitations
- Geographic distribution of training data within LLMs is not publicly available, limiting causal inference about bias sources
- Prompt sensitivity to minor wording changes may affect reproducibility and robustness
- Semantic generalization patterns from synthetic variables may not fully extend to real-world concept spaces
- Discourse-driven feature weighting lacks direct corpus evidence linking training text patterns to model behavior

## Confidence
- **High confidence:** Core empirical findings (systematic performance gaps between LLMs and OLS, geographic bias patterns, feature weighting discrepancies)
- **Medium confidence:** Proposed mechanisms linking findings to training data bias and semantic generalization (correlational evidence and controlled experiments, but cannot definitively prove causation)
- **Low confidence:** Discourse-driven feature weighting mechanism (lacks direct corpus evidence)

## Next Checks
1. Geographic holdout validation to isolate training data bias effects from model architecture differences
2. Semantic gradient stress testing with real-world concept continua to validate interpolation mechanism beyond synthetic variables
3. Targeted injection experiments with spillover measurement to establish practical limits of context-based correction strategies