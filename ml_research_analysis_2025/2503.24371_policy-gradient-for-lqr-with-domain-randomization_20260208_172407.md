---
ver: rpa2
title: Policy Gradient for LQR with Domain Randomization
arxiv_id: '2503.24371'
source_url: https://arxiv.org/abs/2503.24371
tags:
- gradient
- lemma
- policy
- controller
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first convergence analysis of policy
  gradient methods for domain randomization (DR) in linear quadratic regulation (LQR).
  The authors show that policy gradient converges globally to the optimal solution
  of a finite-sample approximation of the DR objective when the heterogeneity of sampled
  systems is bounded.
---

# Policy Gradient for LQR with Domain Randomization

## Quick Facts
- arXiv ID: 2503.24371
- Source URL: https://arxiv.org/abs/2503.24371
- Reference count: 40
- Policy gradient converges globally for domain-randomized LQR under bounded heterogeneity

## Executive Summary
This paper establishes the first convergence analysis of policy gradient methods for domain randomization in linear quadratic regulation. The authors prove that policy gradient converges globally to the optimal solution of a finite-sample approximation of the domain randomization objective when the heterogeneity of sampled systems is bounded. They quantify the sample complexity needed to achieve a small performance gap between sample-average and population objectives. The paper introduces a discount-factor annealing algorithm that eliminates the need for an initial jointly stabilizing controller.

## Method Summary
The paper applies policy gradient to minimize a finite-sample approximation of the domain randomization objective for LQR control. Systems are sampled from a distribution, and the policy is updated via gradient descent on the sample average cost. The key innovation is a discount factor annealing algorithm that starts with a small discount factor where any controller is stabilizing, then gradually increases the discount while maintaining cost bounds. This eliminates the need for an initial jointly stabilizing controller. The gradient computation involves solving Lyapunov equations for each sampled system.

## Key Results
- Policy gradient converges globally to the minimizer of the sample-average DR objective under bounded heterogeneity
- Sample complexity of O(1/√M) to achieve ε accuracy in the population objective
- Discount annealing algorithm eliminates need for initial jointly stabilizing controller
- Empirical validation on inverted pendulum systems and hardware experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy gradient converges to the global minimizer of the sample-average DR objective when sampled systems are sufficiently similar
- Mechanism: The sample-average LQR objective satisfies an approximate gradient domination condition that ensures driving the gradient to zero suffices for global optimality
- Core assumption: The collection of scenarios is (B,s)-bounded and the remainder satisfies ‖R(K,K*)‖²_F ≤ μ(J_SA(K) - J_SA(K*))
- Evidence anchors: [abstract], [Section III, Lemma III.4], [corpus]

### Mechanism 2
- Claim: The sample-average objective J_SA(K) approximates the population DR objective J_DR(K) with error O(1/√M)
- Mechanism: By Hoeffding's inequality, with M samples, the population objective is approximated within O(1/√M) with high probability
- Core assumption: The heterogeneity bound ensures uniform cost bounds across all systems
- Evidence anchors: [Section V, Theorem V.1], [Section II, Assumption II.1], [corpus]

### Mechanism 3
- Claim: Discount factor annealing eliminates the need for an initial jointly stabilizing controller
- Mechanism: Scaling (A,B) by √γ < min ρ(A+BK)⁻² ensures K stabilizes the discounted system even if unstable on the original
- Core assumption: Assumption II.1 holds throughout the annealing process
- Evidence anchors: [Section IV, Theorem IV.1], [Section IV, Algorithm 1], [corpus]

## Foundational Learning

- **Concept: LQR Cost and Riccati Equation**
  - Why needed here: The DR objective is an expectation over LQR costs; gradient expressions require solving Lyapunov equations
  - Quick check question: Can you derive why J(K,θ) = tr(P_K Σ_w) where P_K solves the discrete Lyapunov equation?

- **Concept: Gradient Domination (Polyak-Łojasiewicz Condition)**
  - Why needed here: Standard convergence results for non-convex optimization require this condition to guarantee that gradient descent finds the global optimum
  - Quick check question: For single-system LQR, why does gradient domination hold despite the objective being non-convex in K?

- **Concept: Sample Average Approximation**
  - Why needed here: The theoretical analysis centers on proving convergence for the finite-sample objective J_SA(K), then bounding the gap to J_DR(K)
  - Quick check question: How does the concentration bound change if individual costs J(K,θᵢ) are unbounded?

## Architecture Onboarding

- **Component map:** Sampling Layer -> Discount Module -> Cost Evaluator -> Gradient Computer -> Policy Updater -> Annealing Scheduler

- **Critical path:** The discount annealing loop (lines 4-7 of Algorithm 1) is the non-obvious component. Incorrect γ-scheduling causes divergence.

- **Design tradeoffs:**
  - M (sample count): Larger M reduces population gap but increases per-iteration compute
  - Step size α: Must satisfy α < 2/L where L is the smoothness constant; too large causes instability
  - γ scheduling: Aggressive increase (large γ'/γ ratio) reduces iterations but may violate the cost bound

- **Failure signatures:**
  - Gradient norm plateaus but cost remains high → heterogeneity bound violated, remainder term doesn't shrink
  - Iterates become unstable (cost → ∞) → discount factor increased too aggressively or step size too large
  - Poor sim-to-real performance despite convergence → sampling distribution doesn't capture true uncertainty

- **First 3 experiments:**
  1. **Single-system sanity check:** Set M=1 with known optimal K*. Verify policy gradient recovers K* from random K₀
  2. **Heterogeneity sweep:** Fix M=10, vary ε_HET by scaling parameter ranges. Plot convergence rate vs. ε_HET
  3. **Sample complexity curve:** For fixed ε_HET, vary M ∈ {10, 50, 100, 500}. Measure J_DR(K̃) - J_DR(K*_DR)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the heterogeneity bound (Assumption II.1) be substantially relaxed while preserving global convergence guarantees, or is it fundamental to the domain randomization setting?
- Basis in paper: [explicit] Section III states: "The heterogeneity requirements of Assumption II.1 are stronger than desired... We leave relaxing this condition or determining if it is fundamental to future work."
- Why unresolved: The bound excludes cases where systems can be jointly stabilized despite being distant in parameter space
- What evidence would resolve it: Either a counterexample showing divergence when heterogeneity exceeds the bound but joint stabilization is possible, or a proof establishing convergence under weaker assumptions

### Open Question 2
- Question: Does stochastic policy gradient (sampling a new system at each iteration) converge globally for domain randomization, and at what rate?
- Basis in paper: [explicit] Abstract lists "stochastic PG algorithms" as a promising direction. Section VI(c) demonstrates empirical convergence of Algorithm 2 but states: "we lack theoretical convergence guarantees."
- Why unresolved: Standard stochastic optimization analysis does not directly apply due to the non-convex LQR landscape
- What evidence would resolve it: A convergence proof with rate bounds for Algorithm 2, potentially requiring additional assumptions

### Open Question 3
- Question: Can policy gradient methods be proven to converge for risk-sensitive domain randomization objectives such as the entropic risk measure?
- Basis in paper: [explicit] Abstract lists "risk-sensitive DR formulations" as future work. Section VI(b) shows empirical convergence for entropic risk but notes: "We lack theoretical convergence guarantees for this approach."
- Why unresolved: The reweighted gradient changes the optimization landscape, and gradient domination properties do not directly transfer
- What evidence would resolve it: A proof establishing gradient domination or similar properties for the entropic risk objective

## Limitations

- The heterogeneity bound (Assumption II.1) is critical but difficult to verify in practice and excludes cases where systems can be jointly stabilized
- Sample complexity results depend on optimal cost being finite and heterogeneity bound being small, which may fail silently for poorly conditioned systems
- Discount annealing mechanism is theoretically sound but lacks extensive empirical validation across diverse system classes

## Confidence

- **High Confidence:** Convergence of policy gradient to sample-average minimizer and sample-average approximation bound
- **Medium Confidence:** Discount annealing algorithm's practical effectiveness and iteration complexity
- **Medium Confidence:** Simulation and hardware results demonstrate approach works for inverted pendulum system

## Next Checks

1. **Heterogeneity Threshold Experiment:** Systematically vary parameter ranges for m and ℓ in the inverted pendulum and measure convergence rate vs. ε_HET to identify theoretical bound

2. **Robustness to Initial Conditions:** Run Algorithm 1 from multiple random initializations of K and γ₀. Measure variance in final performance to assess initialization sensitivity

3. **Sample Efficiency Scaling:** For a fixed heterogeneity level, vary M from 10 to 1000 and measure population excess cost J_DR(K̃) - J_DR(K*_DR). Plot against 1/√M to validate theoretical sample complexity bound