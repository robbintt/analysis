---
ver: rpa2
title: Revisiting the Role of Relearning in Semantic Dementia
arxiv_id: '2503.03545'
source_url: https://arxiv.org/abs/2503.03545
tags:
- semantic
- relearning
- errors
- network
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the behavioral patterns observed in semantic
  dementia (SD), specifically the progression of semantic knowledge loss from fine-grained
  distinctions to broader categories. The authors propose that relearning, rather
  than atrophy alone, may explain the specific error patterns seen in SD patients.
---

# Revisiting the Role of Relearning in Semantic Dementia

## Quick Facts
- arXiv ID: 2503.03545
- Source URL: https://arxiv.org/abs/2503.03545
- Reference count: 4
- Primary result: Relearning after neuron deletion, not atrophy alone, produces the staged semantic loss pattern characteristic of Semantic Dementia in a deep linear network

## Executive Summary
This paper addresses the behavioral patterns observed in semantic dementia (SD), specifically the progression of semantic knowledge loss from fine-grained distinctions to broader categories. The authors propose that relearning, rather than atrophy alone, may explain the specific error patterns seen in SD patients. Using a deep linear neural network trained on hierarchically structured semantic data, they simulate atrophy by deleting hidden neurons and compare models with and without relearning. Without relearning, the model loses knowledge across all hierarchy levels simultaneously. With relearning, the model exhibits the characteristic SD pattern: category coordinate errors first, followed by cross-category and superordinate errors, along with prototyping effects. This demonstrates that relearning is necessary to reproduce SD-specific behaviors in artificial neural networks, suggesting that relearning after atrophy—not atrophy itself—drives the progression of memory degradation in SD. The results call for revisiting the role of relearning in chronic cognitive diseases.

## Method Summary
The study trains a deep linear network to generate semantic features for hierarchically organized objects, then simulates semantic dementia by successively deleting hidden neurons. After each deletion, the model either retrains (relearning) or does not. The key comparison is between n=0 (no relearning) and n=200 (with relearning) epochs. The model uses 8 objects organized across 4 hierarchy levels with shared semantic features. Hidden neuron deletion represents ATL atrophy, and retraining represents ongoing plasticity. The network is trained to convergence from small random initialization, then neurons are deleted sequentially with retraining after each deletion. Error rates at each hierarchy level are measured throughout.

## Key Results
- Without relearning (n=0), the model loses semantic knowledge across all hierarchy levels simultaneously
- With relearning (n=200), the model shows sequential degradation: category coordinate errors first, then cross-category errors, then superordinate errors
- Relearning produces prototyping effects where less frequent objects are confused with prototypical category members
- The sequential degradation pattern is also observed with ReLU hidden layers, not just linear

## Why This Works (Mechanism)

### Mechanism 1: Relearning-induced Sequential Semantic Degradation
- Claim: Relearning after neuron deletion, rather than atrophy alone, produces the staged semantic loss pattern observed in semantic dementia.
- Mechanism: When hidden neurons are deleted and gradient descent retraining occurs, the network preferentially recovers high-variance (shared, general) features before low-variance (fine-grained) features. This creates a progression: category coordinate errors → cross-category errors → superordinate errors.
- Core assumption: The brain continuously attempts to recover lost representations through plasticity during chronic neurodegeneration.
- Evidence anchors:
  - [abstract]: "relearning during disease progression rather than particular atrophy cause the specific behavioural patterns associated with SD"
  - [section]: "A model with no relearning loses information across all levels of the hierarchy at once, contradicting the patterns associated to SD"
  - [corpus]: Weak direct evidence; related work on relearning attacks in LLMs (NeuRel-Attack) shows relearning can restore suppressed knowledge, but this concerns safety alignment, not semantic hierarchies.
- Break condition: If retraining epochs n = 0 after each deletion, sequential degradation collapses into simultaneous loss across all hierarchy levels.

### Mechanism 2: Capacity-dependent Hierarchical Feature Distribution
- Claim: Fine-grained semantic distinctions require more hidden layer capacity to maintain than general features.
- Mechanism: During initial training, deep linear networks learn hierarchical structure via singular value decomposition-like dynamics, with low-variance fine-grained features encoded in later-developing, more capacity-sensitive dimensions. Deletion preferentially destroys these fragile representations first when relearning is present.
- Core assumption: The hidden layer dimensionality bounds the number of distinguishable semantic distinctions at each hierarchy level.
- Evidence anchors:
  - [abstract]: "After training the network to generate the common semantic features of various hierarchically organised objects"
  - [section]: Table 1 shows n=200 retraining yields 0% Level 1 error vs. 62.5% Level 4 error—fine-grained features degrade first.
  - [corpus]: Weak corpus evidence; no direct neighbors address hierarchical capacity constraints in linear networks.
- Break condition: If initial weights are too large or network depth is insufficient, hierarchical learning dynamics may not emerge.

### Mechanism 3: Frequency-biased Prototyping via Gradient Accumulation
- Claim: During relearning, higher-frequency features dominate gradient updates, causing low-frequency objects to be misidentified as prototypical category members.
- Mechanism: Constrained capacity after neuron deletion forces the network to allocate remaining representational space proportionally to training frequency. The statistical mode of each category dominates, producing prototyping errors.
- Core assumption: Real-world typicality effects arise from differential environmental exposure frequencies.
- Evidence anchors:
  - [abstract]: "confusing less frequent objects with prototypical ones, replicating SD's prototyping errors"
  - [section]: "if odd data points are shown twice as often during relearning...the more frequently seen features will dominate the representations"
  - [corpus]: Weak corpus evidence; COLUR addresses relearning with noisy labels but not prototyping dynamics.
- Break condition: If training frequencies are uniform across objects, prototyping effects should not emerge.

## Foundational Learning

- Concept: **Deep Linear Network Learning Dynamics**
  - Why needed here: The paper relies on the mathematical property that deep linear networks trained with gradient descent learn hierarchical features sequentially (high-variance first), despite being architecturally linear.
  - Quick check question: Why does a network with no non-linearities still exhibit staged learning rather than instantaneous convergence?

- Concept: **Semantic Feature Hierarchies**
  - Why needed here: The model's behavior depends on objects sharing features at multiple abstraction levels (e.g., "has leaves" for all plants, "has thorns" only for roses).
  - Quick check question: How does feature variance at each hierarchy level relate to learning order?

- Concept: **Neuron Deletion as Neurodegeneration Model**
  - Why needed here: The experimental design treats hidden neuron removal as a computational analog of ATL atrophy.
  - Quick check question: What behavioral signature distinguishes atrophy-only models from atrophy-plus-relearning models?

## Architecture Onboarding

- Component map: Input X -> Hidden layer -> Output Y
  - Input X: Object identity vectors (16 objects across 4 hierarchy levels)
  - Hidden layer: Fully-connected, represents ATL semantic hub; subjected to sequential neuron deletion
  - Output Y: Real-valued semantic feature vectors (continuous confidence values, not thresholded binary)
  - Training: Full-batch gradient descent from small random initialization

- Critical path:
  1. Initialize small weights → train to convergence on semantic feature prediction
  2. Delete subset of hidden neurons (model atrophy event)
  3. Retrain for n epochs (model plasticity/relearning)
  4. Repeat deletion-retraining until hidden layer depleted
  5. Measure error rates at each hierarchy level after each deletion round

- Design tradeoffs:
  - Linear output vs. thresholded output: Linear enables fine-grained confidence comparison; thresholding would obscure degradation gradients
  - Full-batch vs. stochastic gradient descent: Full-batch ensures deterministic, comparable learning dynamics
  - Deletion rate: Too fast → no time for relearning effects; too slow → computationally expensive with unclear additional insight

- Failure signatures:
  - Uniform degradation across all hierarchy levels → relearning disabled (n=0) or insufficient epochs
  - No prototyping effect → training frequencies uniform or relearning absent
  - Random error patterns → hierarchy not encoded in data structure or network failed to learn hierarchical structure initially

- First 3 experiments:
  1. **Ablation control**: Run with n=0 retraining epochs; confirm simultaneous degradation across levels (validates relearning necessity).
  2. **Frequency manipulation**: Double training frequency for "prototypical" objects during retraining phase only; verify enhanced prototyping errors.
  3. **Activation variant**: Replace linear hidden layer with ReLU; confirm sequential degradation pattern persists (as shown in Table 1), establishing mechanism generalizes beyond pure linearity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the relearning mechanism observed in this model drive behavioral progression in other chronic cognitive diseases beyond Semantic Dementia?
- Basis in paper: [explicit] The authors conclude that "future research should revisit the role of relearning as a contributing factor to cognitive diseases" generally, noting it is currently unexamined in most chronic conditions.
- Why unresolved: This study isolated Semantic Dementia, and while it draws parallels to herpes simplex virus encephalitis, the broader applicability to other neurodegenerative pathologies remains theoretical.
- What evidence would resolve it: Applying the atrophy-relearning protocol to computational models of other diseases (e.g., Alzheimer's) and verifying if characteristic patient error patterns emerge.

### Open Question 2
- Question: Is relearning necessary to reproduce Semantic Dementia deficits if the model includes output non-linearities or recurrent connections?
- Basis in paper: [inferred] The authors state their results demonstrate necessity "in the absence of output non-linearities," explicitly contrasting their approach with prior models (e.g., Rogers et al.) that used thresholded outputs.
- Why unresolved: The study establishes relearning as the driver in a linear system, but it leaves open whether complex, biologically realistic architectures might produce the same error gradients through structural constraints alone.
- What evidence would resolve it: Implementing successive neuron deletion with relearning in non-linear or recurrent neural networks to see if the specific SD error patterns persist or change.

### Open Question 3
- Question: Do biological constraints on neural plasticity support the continuous relearning capacity assumed by the model?
- Basis in paper: [inferred] The model assumes the network can actively retrain (optimize) after every instance of neuron deletion, effectively "relearning" lost information despite progressive damage.
- Why unresolved: The paper verifies computational plausibility but does not address biological feasibility, such as whether the atrophying Anterior Temporal Lobe retains sufficient plasticity to support the required reweighting.
- What evidence would resolve it: Longitudinal imaging or electrophysiological data tracking synaptic changes in patients to confirm active reorganization occurs during atrophy.

## Limitations
- The hierarchical feature structure in the synthetic dataset may not fully capture the complexity of real semantic memory
- The linear network approximation, while mathematically tractable, may miss nonlinear dynamics important for modeling human semantic processing
- The exact parameters for training (learning rate, initialization scale, deletion schedule) are not specified, which could affect reproducibility

## Confidence

**High**: The core claim that relearning after atrophy produces sequential semantic degradation, distinct from atrophy-only models

**Medium**: The claim that capacity constraints determine which hierarchy levels degrade first

**Medium**: The prototyping mechanism driven by frequency-biased gradient updates

## Next Checks

1. Verify the sequential degradation pattern persists across different network widths and deletion rates
2. Test whether similar behavioral patterns emerge in nonlinear (ReLU) networks with the same architecture
3. Implement frequency manipulation experiments to confirm the prototyping effect depends on differential training frequencies during relearning