---
ver: rpa2
title: Stochastic Sample Approximations of (Local) Moduli of Continuity
arxiv_id: '2509.15368'
source_url: https://arxiv.org/abs/2509.15368
tags:
- neural
- lemma
- definable
- lipschitz
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of estimating the modulus of
  local continuity (e.g., Lipschitz constant) for neural networks, a key quantitative
  property important for evaluating robustness and fairness. The authors revisit the
  connection between generalized derivatives and moduli of local continuity, and present
  a non-uniform stochastic sample approximation approach.
---

# Stochastic Sample Approximations of (Local) Moduli of Continuity

## Quick Facts
- **arXiv ID:** 2509.15368
- **Source URL:** https://arxiv.org/abs/2509.15368
- **Reference count:** 40
- **Primary result:** A non-uniform stochastic sampling method using UCB policies estimates Lipschitz constants for deep ReLU networks, scaling to depth-11 networks (704 neurons) within 60 seconds with relative errors often below 0.3%.

## Executive Summary
This work introduces a scalable, non-uniform stochastic sampling approach for estimating the Lipschitz constant of neural networks, a key metric for robustness and fairness. By sampling elements of the Clarke subdifferential using PyTorch's automatic differentiation, the method approximates the global or local modulus of continuity without solving computationally expensive optimization problems. The core innovation is an adaptive sampling strategy using Upper Confidence Bound (UCB) policies that preferentially explores regions likely to contain high gradients, improving sample efficiency compared to uniform sampling. Theoretical guarantees include consistency, asymptotic unbiasedness, and asymptotic optimality within a class of sampling policies.

## Method Summary
The method estimates the Lipschitz constant by sampling norms of generalized Jacobians (Clarke subdifferentials) over the input domain. Three algorithms are presented: (1) uniform sampling of points and Jacobians, (2) uniform partitioning of the input space, and (3) UCB-based non-uniform sampling that adaptively refines regions with high gradient potential. The UCB score balances exploitation of known high-gradient regions with exploration of uncertain areas. Experiments use ReLU networks trained on binary MNIST (digits 1 vs 7) and synthetic datasets, with performance compared against LipMIP and LipSDP baselines.

## Key Results
- UCB-based sampling strictly outperforms uniform sampling in convergence speed and final estimate accuracy (Figure 4).
- The method scales to depth-11 networks (704 neurons) within a 60-second time limit, where LipSDP scales to depth 9 (576 neurons) and LipMIP to depth 3 (192 neurons).
- Relative errors are often below 0.3%, with runtimes significantly faster than LipSDP and LipMIP for deeper networks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** For definable non-smooth functions like ReLU networks, the modulus of continuity equals the supremum of Clarke Jacobian norms, which can be estimated by sampling.
- **Core assumption:** The network is definable in an o-minimal structure and autograd provides a "good" approximation of the Clarke subdifferential.
- **Evidence anchors:** Theorem 3 and 6 (Page 2, 6-7), Remark 9 on AD behavior (Page 7).
- **Break condition:** Fails if autograd produces sporadic non-zero gradients on null sets or uses non-definable activations.

### Mechanism 2
- **Claim:** Autograd efficiently samples Clarke subdifferentials, offering scalable lower-bound estimation vs SDP/MIP.
- **Core assumption:** Single backpropagation is much cheaper than solving SDP/MIP relaxations.
- **Evidence anchors:** Abstract mentions PyTorch autograd; Section 3.1 describes sampling (Page 3).
- **Break condition:** Scalability gains diminish with extremely high dimensions and vanishingly small high-gradient regions.

### Mechanism 3
- **Claim:** UCB policies reduce sample complexity by adaptively focusing on high-gradient regions.
- **Core assumption:** High-gradient regions are spatially localized, allowing effective pruning via partitioning.
- **Evidence anchors:** Abstract mentions UCB; Section 3.3 and Algorithm 3 (Page 5-6); Figure 4 shows improvement (Page 8).
- **Break condition:** Fails if gradient landscape is adversarially noisy or maximum lies in a basin surrounded by flat zero-gradients without a structural path for subdivision.

## Foundational Learning

- **Concept:** Clarke Subdifferential / Generalized Jacobian
  - **Why needed here:** Standard derivatives don't exist at ReLU kinks; Clarke subdifferential generalizes derivatives for non-smooth points, enabling theoretical Lipschitz guarantees.
  - **Quick check question:** Can you explain why taking the standard gradient at a ReLU kink (input=0) is theoretically insufficient for proving global robustness?

- **Concept:** Modulus of Local Continuity (Lipschitz Constant)
  - **Why needed here:** Quantifies worst-case sensitivity (output change / input change), connecting math (Jacobians) to applications (robustness/fairness).
  - **Quick check question:** If a network has a local Lipschitz constant of 10 over domain X, and you perturb an input by ε, what is the maximum change in the output?

- **Concept:** Multi-Armed Bandits & Upper Confidence Bound (UCB)
  - **Why needed here:** Algorithm 3 uses UCB to balance exploration (sampling new areas) vs exploitation (refining known high-gradient areas).
  - **Quick check question:** In the context of this paper, what specifically does the "uncertainty" term in the UCB score (the √(ln(t+1)/s_n) part) encourage the algorithm to do?

## Architecture Onboarding

- **Component map:** Domain Manager -> Scoring Engine -> Sampler -> Evaluator -> State Updater
- **Critical path:** Evaluator (PyTorch autograd) runtime per sample; Scoring Engine logic determines convergence speed.
- **Design tradeoffs:**
  - **Partitioning Strategy:** Axis-aligned hyperplanes (halving longest side). *Tradeoff:* Simple, but may create unnecessary subdivisions in high dimensions vs diagonal cuts.
  - **UCB Constant c:** High c encourages exploration (slower but safer); low c risks local maxima. Paper suggests c=10.
  - **Batch vs. Sequential:** Sequential updates UCB dynamically; batching improves GPU utilization but may degrade adaptive benefits.
- **Failure signatures:**
  - **Stagnation:** Estimate stops increasing early, subdivisions concentrate on one region. *Cause:* UCB constant too low or critical region missed initially.
  - **Memory Explosion:** Too many subregions generated. *Cause:* Subdivision multiplier too low or high-dimensional input without sparsity.
  - **Under-estimation:** Result significantly lower than LipSDP. *Cause:* Insufficient total samples N relative to gradient landscape complexity.
- **First 3 experiments:**
  1. **Sanity Check (2D):** Replicate Figure 1/3 on [2, 16, 16, 1] network; verify algorithm identifies same maximum region as grid search.
  2. **Hyperparameter Sweep:** Run Algorithm 3 on depth-4 network with varying c ∈ [1, 5, 10, 20]; plot convergence rate to find optimal exploration.
  3. **Scalability Benchmark:** Compare Algorithm 3 vs uniform sampling on depth 5, 7, 9 networks (width 64); measure time to reach 1% error or compare final estimates at 60s (replicate Figure 5).

## Open Questions the Paper Calls Out
- **Open Question 1:** Can finite-time, non-asymptotic bounds be derived for the convergence rates of the UCB-based sampling policies?
  - **Basis in paper:** Section 6 states, "One may wish to bound the rates of convergence of the UCB-based policies..."
  - **Why unresolved:** Paper establishes consistency and asymptotic optimality but lacks finite-sample error guarantees.
  - **What evidence would resolve it:** Formal proof providing specific convergence rate bounds for non-uniform sampling.

- **Open Question 2:** How can the UCB exploration parameter c be selected automatically or adaptively?
  - **Basis in paper:** Section 6 notes, "It is not clear what value to choose [for c] without a general idea of the gradient space."
  - **Why unresolved:** Poor c choice increases iterations; authors rely on heuristics or pre-sampling.
  - **What evidence would resolve it:** Algorithmic modification dynamically adjusting c or theoretical guideline linking c to dataset statistics, validated by sensitivity analysis.

- **Open Question 3:** Can per-iteration runtime be reduced sufficiently for foundational models?
  - **Basis in paper:** Section 6 lists scalability as a key limitation; suggests JAX or Enzyme for better performance.
  - **Why unresolved:** Current PyTorch implementation limits depth/width of verifiable networks.
  - **What evidence would resolve it:** High-performance implementation (e.g., Enzyme) demonstrating estimation for large-scale, industry-standard networks.

## Limitations
- Theoretical foundation relies on definable functions; unclear if method generalizes to non-standard activations (Leaky ReLU, ELU) or hybrid architectures.
- UCB-based adaptive sampling shown to outperform uniform sampling in limited experiments; no evidence for extremely high-dimensional inputs or pathological gradient landscapes.
- Assumes "good" autograd behavior on ReLU boundaries; boundary behavior of PyTorch's autograd on multi-valued Clarke subdifferentials not rigorously characterized.

## Confidence
- **High Confidence:** Core theoretical result (Theorem 3) connecting Clarke Jacobians to modulus of continuity is well-established and correctly applied.
- **Medium Confidence:** Experimental results showing runtime/accuracy improvements are convincing for tested architectures, but sample sizes and hyperparameter choices (e.g., c=10) lack full justification.
- **Low Confidence:** Claim of "asymptotically optimal" sampling within UCB-like policies is stated but not empirically validated against other adaptive strategies (Thompson sampling, Bayesian optimization).

## Next Checks
1. **Generalization Test:** Replicate main experiments using networks with Leaky ReLU activations; compare Algorithm 3 vs uniform sampling to test robustness to activation perturbations.
2. **Boundary Behavior Analysis:** For a small [2, 4, 4, 1] network, sample points near ReLU boundaries (x_i = 0 ± ε); analyze variance in computed Jacobian norms to validate autograd's approximation of Clarke subdifferential.
3. **Adversarial Landscape Test:** Construct synthetic network with known adversarial gradient landscape (high-gradient region surrounded by zero-gradient moat); run Algorithm 3 with varying c values to observe if subdivision logic discovers isolated high-gradient region or gets trapped in flat area.