---
ver: rpa2
title: Why Chain of Thought Fails in Clinical Text Understanding
arxiv_id: '2509.21933'
source_url: https://arxiv.org/abs/2509.21933
tags:
- clinical
- performance
- general
- reasoning
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates Chain-of-Thought (CoT) prompting
  on 95 large language models across 87 real-world clinical text tasks spanning 9
  languages and 8 task types. Contrary to prior findings in other domains, 86.3% of
  models showed consistent performance degradation under CoT, with weaker models experiencing
  larger drops.
---

# Why Chain of Thought Fails in Clinical Text Understanding

## Quick Facts
- arXiv ID: 2509.21933
- Source URL: https://arxiv.org/abs/2509.21933
- Authors: Jiageng Wu; Kevin Xie; Bowen Gu; Nils Krüger; Kueiyu Joshua Lin; Jie Yang
- Reference count: 40
- Primary result: 86.3% of 95 LLMs show consistent performance degradation under Chain-of-Thought prompting on 87 real-world clinical text tasks spanning 9 languages and 8 task types.

## Executive Summary
This paper systematically evaluates Chain-of-Thought (CoT) prompting on 95 large language models across 87 real-world clinical text tasks spanning 9 languages and 8 task types. Contrary to prior findings in other domains, 86.3% of models showed consistent performance degradation under CoT, with weaker models experiencing larger drops. The study found that longer reasoning traces and weaker alignment with medical concepts correlate with higher performance decline. Error analysis revealed that hallucination, omission, and incompleteness are the dominant failure modes of CoT in clinical contexts. The results highlight a critical paradox: while CoT enhances interpretability, it may undermine reliability in clinical text understanding, calling for more robust reasoning strategies grounded in clinical knowledge.

## Method Summary
The study systematically compares zero-shot (direct answer) vs. Chain-of-Thought prompting across 95 LLMs on 87 clinical text datasets from the BRIDGE benchmark. Models are evaluated on 8 task types (classification, NER, event extraction, QA, summarization, normalization/coding, semantic similarity, NLI) in 9 languages. Zero-shot prompts request only final answers, while CoT prompts use "Solve it step by step" followed by final answer. Performance is measured using task-specific metrics (accuracy, F1, ROUGE) and ΔScore = (CoT performance - Zero-shot performance)/Zero-shot performance × 100. Reasoning traces are analyzed for length, medical concept alignment (CUI overlap via cTAKES), and error types via LLM-as-a-Judge (OpenAI-o3) validated against 200 expert samples.

## Key Results
- 86.3% of 95 models showed performance degradation under CoT prompting (ΔScore < 0)
- Weaker models experienced larger drops (high-capacity: -1.95%, medium: -2.66%, low: -7.35%)
- Longer reasoning traces correlate with higher performance decline across all models
- Models with weaker alignment to medical concepts showed greater degradation
- Hallucination, omission, and incompleteness are dominant failure modes in clinical CoT

## Why This Works (Mechanism)

### Mechanism 1: Error Accumulation in Longer Reasoning Traces
CoT-induced performance degradation intensifies as reasoning chain length increases. Extended reasoning traces create more opportunities for error propagation—each step introduces potential for misinterpretation of clinical abbreviations, numeric values, or temporal relationships, compounding into incorrect final outputs. Error probability per reasoning step is non-trivial in clinical text, and errors are not self-correcting. Tasks with inherently short reasoning requirements (e.g., simple normalization/coding) show reduced degradation.

### Mechanism 2: Weak Clinical Concept Grounding
CoT degradation is attenuated when reasoning traces remain tightly grounded in input medical concepts. Models that generate reasoning which stays within the conceptual space of the input—measured via CUI overlap and coverage—produce more faithful rationales. Ungrounded reasoning introduces external knowledge or distortions that diverge from the specific clinical context. Clinical text requires strict factual grounding; introducing external associations increases hallucination risk.

### Mechanism 3: Clinical Abbreviation and Numeric Misinterpretation
CoT traces disproportionately fail on quantitative data and domain-specific abbreviations. Clinical text contains dense shorthand (e.g., "NRB," "plt," "pft") and numeric values with implicit clinical significance. CoT prompts models to elaborate, but without robust clinical knowledge, elaboration becomes confabulation—misreading "O2sat 100% NRB" as "room air" or misclassifying blood pressure ranges. Models lack sufficient clinical pretraining to reliably expand abbreviations and interpret numeric ranges in context.

## Foundational Learning

- **Concept: Chain-of-Thought Prompting**
  - **Why needed here:** The paper's central intervention; understanding what CoT is designed to do (elicit stepwise reasoning) versus what it actually does in clinical contexts (introduces error pathways) is prerequisite to interpreting results.
  - **Quick check question:** Can you explain why CoT improves performance on mathematical reasoning but, per this paper, degrades it on clinical text tasks?

- **Concept: Medical Concept Unique Identifiers (CUIs) via cTAKES**
  - **Why needed here:** The paper quantifies "grounding" using CUI overlap between input and CoT traces; understanding this NLP pipeline is necessary to interpret the alignment analysis.
  - **Quick check question:** What does a high Alignment-Coverage score indicate about a model's reasoning trace relative to the input clinical text?

- **Concept: LLM-as-a-Judge Evaluation**
  - **Why needed here:** The error taxonomy (hallucination, omission, incompleteness) is derived via an automated judge validated against clinical experts; understanding this methodology is critical for assessing result reliability.
  - **Quick check question:** What validation metric did the authors report for the LLM-judge's hallucination detection against human experts?

## Architecture Onboarding

- **Component map:**
  Input layer (BRIDGE benchmark datasets) -> Model layer (95 LLMs) -> Prompting layer (Zero-shot vs. CoT) -> Evaluation layer (task metrics + LLM-judge) -> Analysis layer (reasoning length, CUI alignment, lexical analysis)

- **Critical path:**
  1. Run identical clinical tasks under both prompting conditions
  2. Extract and score final answers only (reasoning traces excluded from scoring)
  3. Compute ΔScore = CoT performance − Zero-shot performance
  4. Stratify by model capability, reasoning length, and concept alignment
  5. Validate failure modes via automated judge + expert subset review

- **Design tradeoffs:**
  - Breadth vs. depth: 95 models × 87 tasks provides generalizability but limits per-task expert validation (n=200 for judge validation)
  - Prompt standardization: Using vanilla CoT ("Let's think step by step") avoids prompt engineering confounds but may underrepresent optimized clinical prompting strategies
  - Scoring methodology: Excluding reasoning traces from scoring isolates final-answer accuracy but cannot capture partial-credit reasoning quality

- **Failure signatures:**
  - Hallucination flag: Model claims "O2sat 100% on room air" when input specifies "NRB"
  - Omission flag: MeLLaMA-70B-chat shows 87.8% omission rate—fails to extract patient-specific evidence from longitudinal notes
  - Lexical signal: High frequency of numeric tokens and abbreviations in incorrect CoT traces

- **First 3 experiments:**
  1. Reproduce main finding on a subset: Select 3 models (high/medium/low capability) and 5 tasks across different types; confirm ΔScore is negative in ≥80% of model-task pairs.
  2. Test grounding intervention: Manually constrain CoT prompts to require explicit citation of input concepts; measure whether Alignment-Coverage increases and ΔScore improves.
  3. Validate abbreviation handling: Create a controlled test set with common clinical abbreviations and compare CoT vs. zero-shot accuracy on abbreviation interpretation alone.

## Open Questions the Paper Calls Out

- **Can retrieval-augmented generation (RAG) or similar grounding techniques mitigate the hallucination and omission errors prevalent in clinical CoT?**
  - **Basis in paper:** The authors explicitly list "retrieval-augmented generation (RAG)" as an engineering intervention not evaluated in this study, despite identifying hallucination as a dominant failure mode.
  - **Why unresolved:** It remains unclear if external knowledge retrieval can fix the "weak alignment with medical concepts" identified as a cause of performance degradation.
  - **What evidence would resolve it:** A comparative study on the BRIDGE benchmark measuring CoT vs. RAG-CoT performance, specifically tracking hallucination rates and factual grounding.

- **How can clinical reasoning strategies be designed to reconcile the transparency of CoT with the need for trustworthy accuracy?**
  - **Basis in paper:** The Conclusion highlights the "critical paradox" where CoT enhances interpretability but undermines reliability, calling for "clinically aligned reasoning strategies."
  - **Why unresolved:** The paper establishes the trade-off but does not propose or validate a specific method that achieves high transparency without triggering error accumulation.
  - **What evidence would resolve it:** The development of a novel prompting strategy that maintains CoT's interpretability while matching or exceeding the zero-shot accuracy of high-performing models like GPT-4o.

- **Does enforcing structured intermediate steps (e.g., evidence tables) prevent the performance drop associated with long, free-form CoT reasoning traces?**
  - **Basis in paper:** The paper notes a negative correlation between reasoning length and performance, attributing it to "error accumulation," yet does not test if rigid structuring of the trace mitigates this.
  - **Why unresolved:** It is unknown if the length itself is the issue, or if the unstructured, verbose nature of the reasoning allows errors to propagate.
  - **What evidence would resolve it:** An analysis comparing free-form CoT against schema-constrained CoT across different trace lengths.

## Limitations

- The use of vanilla CoT prompting without domain-specific optimization means results may not generalize to clinical-specific reasoning strategies.
- The LLM-as-a-judge methodology, while validated on 200 samples, may not fully capture the nuance of clinical reasoning failures, particularly for rare error types.
- The study focuses on English-centric models even when evaluating non-English clinical texts, which may confound cross-lingual findings.

## Confidence

- **High Confidence:** The core finding that CoT degrades performance in clinical text understanding across most models (86.3% showing negative ΔScore).
- **Medium Confidence:** The mechanisms of error accumulation and clinical concept grounding.
- **Low Confidence:** The generalizability to clinical-specific CoT variants or models with extensive medical pretraining.

## Next Checks

1. **Controlled Abbreviation Test:** Create a benchmark isolating common clinical abbreviations (NRB, plt, pft, etc.) and measure CoT vs. zero-shot accuracy specifically on abbreviation interpretation to determine if this drives the broader degradation pattern.

2. **Medical Pretraining Validation:** Test the most capable medical model (MeLLaMA-70B) on a subset of tasks with extended clinical training to determine if additional medical exposure mitigates CoT failures.

3. **Clinical CoT Prompting:** Implement a domain-specific CoT variant that explicitly requires citation of input text and grounding in medical concepts, then measure whether this improves alignment scores and reduces ΔScore.