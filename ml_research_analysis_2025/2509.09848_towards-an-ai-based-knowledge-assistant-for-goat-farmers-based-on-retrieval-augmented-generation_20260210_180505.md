---
ver: rpa2
title: Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented
  Generation
arxiv_id: '2509.09848'
source_url: https://arxiv.org/abs/2509.09848
tags:
- knowledge
- goat
- farming
- system
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an intelligent knowledge assistant system
  for goat farming that leverages Retrieval-Augmented Generation (RAG) to support
  health management. Two structured knowledge processing methods, table textualization
  and decision-tree textualization, were proposed to enhance large language models'
  (LLMs) understanding of heterogeneous data formats.
---

# Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2509.09848
- Source URL: https://arxiv.org/abs/2509.09848
- Reference count: 9
- Mean accuracies: 87.90% (validation), 84.22% (test) with >85% across text, table, and decision-tree Q&A

## Executive Summary
This study presents an intelligent knowledge assistant system for goat farming that leverages Retrieval-Augmented Generation (RAG) to support health management. The system introduces two structured knowledge processing methods—table textualization and decision-tree textualization—to enhance large language models' understanding of heterogeneous data formats. By converting tabular data and diagnostic decision trees into natural language descriptions, the system achieved high accuracy across different knowledge types, with performance exceeding 85% on validation and test sets.

## Method Summary
The system employs a RAG architecture with hybrid retrieval combining BM25 and dense embeddings (BGE-M3), processed by Qwen3-8B LLM. Two novel textualization methods convert structured knowledge (tables, decision trees) into natural language for LLM consumption. Tables are transformed via rule-based extraction and semantic parsing, while decision trees become interactive conditional chains. Hybrid retrieval uses α=0.3 weighting between BM25 and cosine similarity, with online search as fallback for low-confidence cases. The dynamic augmented prompt module integrates retrieved context with user queries for generation.

## Key Results
- Achieved 87.90% accuracy on validation set and 84.22% on test set
- Accuracy improvements: +30.17% from table textualization, +35.89% from decision-tree textualization
- Error analysis shows omission as dominant category (50%), with hallucination at 18.75%

## Why This Works (Mechanism)

### Mechanism 1: Structured Knowledge Textualization (Tables)
- Claim: Converting tabular data to natural language descriptions enables LLMs to process structured information they otherwise struggle with.
- Mechanism: A rule engine extracts headers and cell content with validation (Eq. 2), then an LLM performs semantic parsing to generate coherent natural language descriptions (Eq. 3), preserving semantic equivalence (Eq. 4).
- Core assumption: Row-by-row textualization maintains sufficient semantic relationships for downstream reasoning tasks.
- Evidence anchors: Table textualization increased accuracy to 88.79% (+30.17%); however, direct corpus evidence for table textualization specifically is weak.

### Mechanism 2: Decision-Tree Textualization with Interactive Clarification
- Claim: Converting hierarchical diagnostic trees into natural language conditional chains enables multi-step reasoning and interactive information gathering.
- Mechanism: Each root-to-leaf path is converted to a natural language sequence (Eq. 6-7). When user input is incomplete, the system identifies unresolved nodes and prompts for missing information (Eq. 8-9), enabling multi-turn diagnostic dialogues.
- Core assumption: Users can accurately provide the requested clinical information when prompted.
- Evidence anchors: Decision tree conversion increased accuracy to 87.03%; however, deep nesting or ambiguous symptoms may require excessive interaction turns.

### Mechanism 3: Hybrid Retrieval with Confidence-Triggered Online Search
- Claim: Combining keyword-precise BM25 retrieval with semantic dense embeddings, plus online search for low-confidence cases, addresses both domain-specific accuracy and knowledge currency.
- Mechanism: Hybrid score combines BM25 and cosine similarity (Eq. 11, α=0.3). Online search activates when local retrieval confidence falls below threshold or real-time information is explicitly requested.
- Core assumption: The confidence threshold correctly identifies knowledge gaps rather than retrieval quality issues.
- Evidence anchors: Hybrid search improves accuracy (50%→60% for novel queries), but external sources introduce hallucination risk (18.75% errors noted).

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: Core to RAG architecture—understanding how text chunks become vectors and how cosine similarity retrieves relevant context.
  - Quick check question: Can you explain why BGE-M3 was chosen over monolingual embedding models for this system?

- Concept: Precision vs. recall in retrieval evaluation
  - Why needed here: Error analysis shows omission (50% of errors) stems from retrieval coverage gaps—understanding this tradeoff informs tuning Top-K and α parameters.
  - Quick check question: If you increase α from 0.3 to 0.7, what happens to semantic vs. lexical matching balance?

- Concept: Prompt engineering with retrieved context
  - Why needed here: The dynamic augmented prompt module (section 2.3) requires understanding how to structure user queries + retrieved context + instructions.
  - Quick check question: How would you modify the prompt template to reduce omission errors identified in the analysis?

## Architecture Onboarding

- Component map: Capripedia knowledge base (125 articles) → Table/Tree textualization → Vector database (FAISS) → Hybrid retrieval (BM25 + cosine, α=0.3) → Online search fallback → Context fusion → Qwen3-8B generation → BERTScore-F1 evaluation

- Critical path: User query → BGE-M3 embedding → Hybrid retrieval (Top-K=3) → Confidence check → [if low] Online search → Context fusion → Prompt assembly → LLM generation

- Design tradeoffs:
  - Qwen3-8B chosen over larger models for computational efficiency (<1s latency on RTX 4080) but may sacrifice reasoning depth
  - α=0.3 favors semantic similarity over keyword matching; may underperform on precise terminology queries
  - Online search improves novel queries (50%→60%) but introduces hallucination risk

- Failure signatures:
  - **Omission errors** (50%): Retrieval misses relevant chunks—consider increasing Top-K or improving chunk segmentation
  - **Hallucination** (18.75%): External source quality issues—implement source credibility filtering
  - **Unsupported reasoning** (12.5%): Fragmented retrieval—restructure knowledge storage for semantic cohesion

- First 3 experiments:
  1. Replicate Experiment 2 baseline: RAG with hybrid retrieval only, measure accuracy gap from paper's 77.44% validation result
  2. Ablate α parameter: Test α ∈ {0.0, 0.3, 0.5, 0.7, 1.0} on validation set to verify optimal balance
  3. Analyze omission cases: Manually review 8 omission errors to identify whether they stem from chunking, embedding quality, or Top-K threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system architecture be extended to effectively process and reason with multimodal inputs, specifically visual and auditory data?
- Basis in paper: The authors state in the Conclusion and Discussion that future work will prioritize the development of a "multimodal intelligent system" to support comprehensive decision-making, as current evaluation is "limited to text-based data."
- Why unresolved: The current framework processes only text-based Q&A pairs and textualized tables/trees; it lacks modules to ingest or interpret sensor data, images, or audio.
- What evidence would resolve it: A functional prototype where image inputs (e.g., photos of goat lesions) or audio inputs are successfully encoded and retrieved alongside text chunks to generate accurate diagnostic advice.

### Open Question 2
- Question: What specific filtering mechanisms or credibility scoring algorithms can minimize hallucinations caused by low-quality external web sources?
- Basis in paper: The Discussion notes that hallucination errors in "Goat Milk Management" stemmed from irrelevant web search results, and suggests future work should explore "dynamic filtering mechanisms based on source reliability scoring" or knowledge graphs.
- Why unresolved: The current online search module retrieves content with "variable" reliability, leading to occasional factual inaccuracies when the local knowledge base is insufficient.
- What evidence would resolve it: A comparative study showing a reduction in hallucination rates (currently ~18.75%) when using a filtered or graph-enhanced retrieval method versus the standard Google Custom Search API.

### Open Question 3
- Question: How can retrieval coverage be optimized to reduce the prevalence of "omission" errors, which currently constitute 50% of all system mistakes?
- Basis in paper: The authors identify omission as the predominant error category and state that "optimising vector retrieval strategies, fine-tuning the Top‑ K threshold, and refining prompt templates" are necessary steps to address this.
- Why unresolved: The existing hybrid retrieval strategy (BM25 + cosine similarity) fails to capture all relevant knowledge points in a significant portion of queries, leading to incomplete answers.
- What evidence would resolve it: Ablation experiments demonstrating that increasing the Top-K threshold or restructuring the vector database yields a statistically significant decrease in omission errors on the test set.

## Limitations

- Data confidentiality prevents exact replication and limits external validation of the 87.90% accuracy claim
- Omission errors (50%) indicate fundamental retrieval coverage issues that may persist even with perfect implementation
- Decision-tree textualization assumes users can accurately provide requested clinical information, which may not hold in real-world scenarios

## Confidence

- **High confidence:** The core RAG architecture with hybrid BM25+dense retrieval is well-established and the implementation details are standard
- **Medium confidence:** The specific α=0.3 weighting and Top-K=3 parameters appear effective but may be dataset-specific
- **Low confidence:** The generalization of the 87.90% accuracy to real-world deployment without further tuning

## Next Checks

1. **Ablation study on retrieval parameters:** Systematically vary α ∈ {0.0, 0.3, 0.5, 0.7, 1.0} and Top-K ∈ {3, 5, 10} on the validation set to identify optimal settings for different query types (text vs. table vs. tree).

2. **Manual analysis of omission errors:** Review 20 randomly selected omission errors to determine whether they stem from chunking strategy, embedding quality, or knowledge base gaps—this will reveal whether the 50% omission rate is fundamentally solvable.

3. **External knowledge source evaluation:** Test the online search component with a diverse set of novel queries to quantify hallucination rates and assess whether source credibility filtering is necessary before production deployment.