---
ver: rpa2
title: 'NGRPO: Negative-enhanced Group Relative Policy Optimization'
arxiv_id: '2509.18851'
source_url: https://arxiv.org/abs/2509.18851
tags:
- ngrpo
- advantage
- grpo
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NGRPO addresses the problem of GRPO\u2019s inability to learn\
  \ from homogeneous groups (all correct or all incorrect) by introducing a virtual\
  \ maximum-reward sample to the advantage calculation, ensuring non-zero advantages\
  \ even for uniformly incorrect groups. This \u201CAdvantage Calibration\u201D mechanism\
  \ promotes exploration by making the gradient magnitude for incorrect samples larger,\
  \ while an \u201CAsymmetric Clipping\u201D mechanism stabilizes training by constraining\
  \ negative updates more tightly than positive ones."
---

# NGRPO: Negative-enhanced Group Relative Policy Optimization

## Quick Facts
- arXiv ID: 2509.18851
- Source URL: https://arxiv.org/abs/2509.18851
- Reference count: 14
- Primary result: NGRPO achieves state-of-the-art Pass@kAUC scores of 31.28% on AIME2025, 86.09% on AMC, and 90.31% on MATH for Qwen2.5-Math-7B

## Executive Summary
NGRPO introduces a novel reinforcement learning approach that addresses the limitations of Group Relative Policy Optimization (GRPO) when dealing with homogeneous groups of samples (all correct or all incorrect). The method employs an "Advantage Calibration" mechanism that introduces a virtual maximum-reward sample to ensure non-zero advantages even for uniformly incorrect groups, promoting exploration. Additionally, an "Asymmetric Clipping" mechanism stabilizes training by constraining negative updates more tightly than positive ones, preventing catastrophic performance degradation.

The approach is specifically designed for mathematical reasoning tasks and has been evaluated on challenging benchmarks including AIME2025, AMC, and MATH using the Qwen2.5-Math-7B model. NGRPO demonstrates superior performance compared to established methods like PPO, GRPO, DAPO, and PSR-NSR, achieving new state-of-the-art results in mathematical problem-solving. The method's effectiveness stems from its ability to learn from homogeneous groups while maintaining training stability through carefully designed gradient clipping strategies.

## Method Summary
NGRPO enhances the standard Group Relative Policy Optimization (GRPO) framework by introducing two key mechanisms: Advantage Calibration and Asymmetric Clipping. The Advantage Calibration mechanism addresses GRPO's inability to learn from homogeneous groups by introducing a virtual maximum-reward sample into the advantage calculation, ensuring non-zero advantages even when all samples in a group are incorrect. This virtual sample effectively provides a reference point that allows the policy to distinguish between different levels of incorrectness and promotes exploration.

The Asymmetric Clipping mechanism complements this by implementing different clipping thresholds for positive and negative policy updates. While positive updates (improving correct answers) are allowed to have larger magnitudes, negative updates (penalizing incorrect answers) are constrained more tightly. This asymmetric treatment prevents the policy from being overly punished for incorrect predictions while still maintaining the ability to learn from mistakes. Together, these mechanisms enable NGRPO to effectively handle the challenges of mathematical reasoning tasks where homogeneous groups are common and where maintaining a balance between exploration and stability is crucial for achieving high performance.

## Key Results
- Achieves Pass@kAUC of 31.28% on AIME2025 benchmark, outperforming PPO, GRPO, DAPO, and PSR-NSR
- Reaches 86.09% Pass@kAUC on AMC and 90.31% on MATH benchmarks using Qwen2.5-Math-7B model
- Demonstrates superior performance in handling homogeneous groups through Advantage Calibration mechanism

## Why This Works (Mechanism)
NGRPO's effectiveness stems from its ability to address GRPO's fundamental limitation with homogeneous groups. In standard GRPO, when all samples in a group are either correct or incorrect, the relative advantage calculation produces zero or near-zero values, preventing meaningful learning signals. NGRPO solves this by introducing a virtual maximum-reward sample that creates a reference point, ensuring non-zero advantages even in these challenging scenarios. This mechanism allows the policy to distinguish between different incorrect answers and learn which ones are closer to the correct solution, effectively promoting exploration in the solution space.

The Asymmetric Clipping mechanism provides crucial stability by preventing overly aggressive negative updates that could destabilize training. By constraining negative updates more tightly than positive ones, NGRPO maintains a healthy learning dynamic where the policy can still improve correct answers substantially while being protected from catastrophic degradation due to incorrect predictions. This balanced approach is particularly important for mathematical reasoning tasks where the solution space is discrete and errors can be severe, requiring careful control over the learning process to achieve optimal results.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: A variant of PPO that uses group-wise advantage estimation, reducing variance but failing with homogeneous groups where all samples are equally correct or incorrect.
- **Advantage Estimation**: The core mechanism for determining which actions are better than others, requiring careful design to provide meaningful signals even in edge cases like homogeneous groups.
- **Asymmetric Gradient Clipping**: A technique that applies different clipping thresholds to positive and negative updates, balancing exploration with training stability.
- **Virtual Sample Injection**: The practice of introducing synthetic data points (in this case, a maximum-reward sample) to improve learning dynamics and ensure non-zero advantages.
- **Mathematical Reasoning Benchmarks**: Standardized tests like AIME, AMC, and MATH that require complex multi-step reasoning, serving as challenging evaluation environments for RL methods.

## Architecture Onboarding

Component Map:
NGRPO -> Advantage Calibration -> Asymmetric Clipping -> Policy Update

Critical Path:
1. Collect trajectory data and group samples
2. Compute advantages with virtual maximum-reward sample
3. Apply asymmetric clipping to advantages
4. Update policy using clipped advantages
5. Repeat with new data

Design Tradeoffs:
- Virtual sample introduces slight bias but enables learning from homogeneous groups
- Asymmetric clipping prevents catastrophic negative updates at cost of slower negative learning
- Group-based processing reduces variance but requires careful handling of edge cases

Failure Signatures:
- Training instability when asymmetric clipping thresholds are poorly chosen
- Plateau in performance when virtual sample mechanism is ineffective
- Suboptimal exploration if advantage calibration is too conservative

First Experiments:
1. Compare NGRPO vs GRPO on synthetic homogeneous group data
2. Test different asymmetric clipping ratios (1:1, 2:1, 3:1) for negative:positive updates
3. Evaluate virtual sample effectiveness by varying its reward value relative to actual maximum rewards

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Limited ablation studies to isolate the individual contributions of Advantage Calibration and Asymmetric Clipping mechanisms
- No sensitivity analysis of the asymmetric clipping hyperparameter to determine optimal configurations
- Lack of qualitative analysis showing how NGRPO specifically behaves differently from GRPO on homogeneous incorrect groups

## Confidence

High confidence:
- Experimental results showing NGRPO outperforming baselines on Qwen2.5-Math-7B are well-documented with specific metrics

Medium confidence:
- Theoretical justification for combining Advantage Calibration with Asymmetric Clipping is plausible but not rigorously proven

Low confidence:
- Claim that NGRPO "effectively tackles" the homogeneous group problem has lowest confidence due to lack of direct mechanistic analysis

## Next Checks
1. Conduct ablation study with NGRPO components disabled individually to quantify specific contributions of Advantage Calibration and Asymmetric Clipping mechanisms

2. Perform systematic sensitivity analysis of asymmetric clipping ratios across multiple orders of magnitude to determine robustness and optimal configuration

3. Design controlled synthetic experiments with known homogeneous incorrect groups to directly compare gradient dynamics and update directions between GRPO and NGRPO