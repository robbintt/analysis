---
ver: rpa2
title: RGBD Gaze Tracking Using Transformer for Feature Fusion
arxiv_id: '2510.06298'
source_url: https://arxiv.org/abs/2510.06298
tags:
- gaze
- training
- dataset
- depth
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates RGBD gaze tracking using transformer-based
  feature fusion. A custom dataset (OTH-Gaze-Estimation) with 132K RGBD samples is
  created due to the lack of suitable public datasets.
---

# RGBD Gaze Tracking Using Transformer for Feature Fusion

## Quick Facts
- arXiv ID: 2510.06298
- Source URL: https://arxiv.org/abs/2510.06298
- Reference count: 0
- RGBD gaze tracking model achieves 4.84° mean angular error on custom dataset, 3.59° on ETH-XGaze

## Executive Summary
This thesis investigates RGBD gaze tracking using transformer-based feature fusion. A custom dataset (OTH-Gaze-Estimation) with 132K RGBD samples was created due to the lack of suitable public datasets. The study develops a deep learning model combining CNNs and transformers, with extensive experiments on three datasets. The research found that depth maps consistently improved accuracy, while the transformer-based fusion module degraded performance compared to a simple MLP substitute. The best model achieved state-of-the-art results on ETH-XGaze (3.59° error) and strong performance on other datasets.

## Method Summary
The method uses a deep learning architecture combining CNNs for feature extraction with a fusion module (originally transformer, but MLP found superior). The model processes face patches (448×448), eye patches (224×224), normalized 5-point facial landmarks, and depth maps. Training uses Adam optimizer without weight decay, with 25 epochs and learning rate decay. The best-performing configuration ("4-x-1") uses a 4-layer CNN encoder for face and depth, ResNet-18 for eyes, and MLP fusion instead of the transformer module. The GAN depth reconstruction backbone was found to hurt performance and is omitted in the best model.

## Key Results
- Best model achieves 4.84° mean angular error on OTH-Gaze-Estimation dataset
- RGBD models consistently outperform RGB-only across all tested datasets
- MLP fusion module outperforms transformer fusion (4.71° vs 4.84° on OTH-Gaze-Estimation)
- Real-time pipeline achieves 25 fps on a single GPU

## Why This Works (Mechanism)

### Mechanism 1
- Depth information provides geometric constraints that improve gaze estimation accuracy
- Core assumption: Depth maps encode 3D structural information about eye/face geometry that cannot be reliably inferred from RGB alone
- Evidence: RGBD models outperformed RGB-only models across datasets (6.29° vs 4.84° on OTH-Gaze-Estimation)
- Break condition: If depth sensor noise exceeds geometric information gain

### Mechanism 2
- Pre-trained GAN backbone for depth reconstruction degrades gaze estimation performance
- Core assumption: Depth reconstruction loss doesn't align with gaze-relevant features
- Evidence: Multi-task learning objective creates conflicting gradients (30.1mm vs 55.3mm on ShanghaiTechGaze+)
- Break condition: If depth artifacts are dominant error source AND reconstruction preserves gaze-relevant features

### Mechanism 3
- Specific Transformer fusion implementation is suboptimal for gaze estimation
- Core assumption: Gaze feature fusion is a relatively simple aggregation problem
- Evidence: Replacing Transformer with MLP improved performance (4.71° vs 4.84° on OTH-Gaze-Estimation)
- Break condition: If token count increases significantly or attention captures meaningful spatial relationships

## Foundational Learning

- **Appearance-based gaze estimation**: Gaze predicted from visual appearance rather than geometric ray tracing. Quick check: Can you explain why head pose normalization is required before gaze estimation?

- **Subject-specific calibration/bias terms**: Learnable per-subject bias (offset + scale) addresses inter-person anatomical variation. Quick check: Why might the same eye appearance correspond to different gaze directions for different people?

- **Face normalization**: Warping input images to canonical pose/distance reduces learning burden. Quick check: What happens to gaze labels when you normalize the face image?

## Architecture Onboarding

- **Component map**: Face/landmark detection → normalization → CNN feature extraction → fusion → regression
- **Critical path**: Head pose feature extractor is CRITICAL; its removal causes 2-4x error increase
- **Design tradeoffs**: GAN backbone (artifacts removed vs conflicting objectives), Fusion module (Transformer vs MLP), Input resolution (448×448 vs 224×224)
- **Failure signatures**: GAN discriminator too strong → mode collapse, BatchNorm before final discriminator activation → constant 0.5 output
- **First 3 experiments**: 1) Establish RGB-only baseline with MLP fusion, 2) Ablate GAN backbone early, 3) Compare Transformer vs MLP fusion with identical token inputs

## Open Questions the Paper Calls Out

### Open Question 1
Is the Transformer fusion module degradation attributable to limited training dataset size relative to model capacity? The author replaced Transformer with MLP (reducing error by 0.33° on ETH-XGaze) but didn't isolate whether this was due to specific fusion mechanism or overfitting from high parameter-to-data ratio.

### Open Question 2
Can a purely CNN-based head pose feature extractor provide critical head pose information without the negative performance impact of the GAN-based depth reconstruction backbone? The ablation study showed removing head pose module increased error massively, but the pre-trained GAN backbone itself hurt performance.

### Open Question 3
To what extent does calibration strategy (linear regression vs fine-tuning) account for performance gaps compared to state-of-the-art baselines? The author's model achieved 3.59° error using linear regression, whereas baselines achieved 2.04° using fine-tuning, leaving unclear whether architectural or calibration differences are primary drivers.

## Limitations
- Custom dataset collection methodology hard to replicate without hardware setup
- Transformer fusion module architecture underspecified in implementation details
- Real-time pipeline lacks detailed latency profiling per component

## Confidence

- **High confidence**: Depth channel consistently improves accuracy; MLP fusion outperforms Transformer; subject-specific bias terms effectively reduce inter-person variation
- **Medium confidence**: GAN backbone consistently degrades performance; 4-layer CNN encoder architecture is optimal
- **Low confidence**: Real-time pipeline achieves 25 fps without detailed timing analysis; custom dataset collection methodology can be fully replicated

## Next Checks

1. Validate MLP vs Transformer fusion: Implement both strategies with identical token inputs on ETH-XGaze to confirm MLP's consistent 0.1-0.2° advantage

2. Test depth channel sensitivity: Systematically degrade depth map quality to quantify geometric information gain threshold where RGB-only becomes competitive

3. Replicate subject bias calibration: Implement learnable offset+scale parameters on ShanghaiTechGaze+ to measure reproducibility of 1-2mm improvement in Euclidean error