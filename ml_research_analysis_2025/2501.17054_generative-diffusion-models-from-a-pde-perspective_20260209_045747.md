---
ver: rpa2
title: Generative diffusion models from a PDE perspective
arxiv_id: '2501.17054'
source_url: https://arxiv.org/abs/2501.17054
tags:
- diffusion
- reverse
- process
- distribution
- sinh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies generative diffusion models from a partial differential
  equation (PDE) perspective, deriving both forward and reverse diffusion dynamics.
  The authors show that exact reverse diffusion has its support contained within the
  original data distribution, meaning it cannot generate new samples - a phenomenon
  known as lack of generalization.
---

# Generative diffusion models from a PDE perspective

## Quick Facts
- arXiv ID: 2501.17054
- Source URL: https://arxiv.org/abs/2501.17054
- Reference count: 40
- Primary result: Exact reverse diffusion has support contained within original data distribution, showing lack of generalization

## Executive Summary
This paper provides a theoretical analysis of generative diffusion models through the lens of partial differential equations, demonstrating that exact reverse diffusion dynamics cannot generate novel samples beyond the original training distribution. The authors derive explicit solutions for both forward and reverse stochastic differential equations, showing that when using perfect score-matching approximations, the reverse process will only reconstruct existing training samples rather than generate new ones. This fundamental limitation reveals that the generalization capability of practical diffusion models arises not from the diffusion process itself, but from the imperfections in the score-matching approximation used during training.

## Method Summary
The paper analyzes generative diffusion models by studying the Ornstein-Uhlenbeck forward process dXₜ = -Xₜdt + √2dBₜ and its corresponding reverse dynamics. The theoretical framework shows that exact reverse diffusion has support contained within the original data distribution, leading to an overfitting regime where samples converge to training data. The authors provide explicit solutions for the reverse SDE when starting from a fixed point and demonstrate that practical implementations must use imperfect score-matching approximations to generate novel samples. For finite datasets, they derive a kernel formulation for the expected value in the overfitting regime and show how stable diffusion discrete updates lead to Voronoi cell behavior where generated samples converge to nearest training points.

## Key Results
- Exact reverse diffusion has support contained within original data distribution, proving lack of generalization
- In the overfitting regime, reverse dynamics converge to original training samples rather than generating new ones
- Practical diffusion models achieve generalization through imperfections in score-matching approximations, not through the diffusion process itself
- The kernel formulation for finite datasets shows Voronoi cell behavior where generated samples converge to nearest training points

## Why This Works (Mechanism)
The paper demonstrates that exact reverse diffusion is fundamentally constrained by the original data distribution. When the score network is trained to perfect accuracy using score-matching loss, the reverse process can only reconstruct samples from the training set. The mechanism relies on the mathematical property that the support of the reverse process is contained within the support of the original data distribution. This theoretical limitation is only overcome in practice through the use of imperfect score-matching approximations, which introduce small errors that enable the generation of novel samples outside the original training distribution.

## Foundational Learning

Ornstein-Uhlenbeck Process
- Why needed: Provides the forward diffusion dynamics used throughout the analysis
- Quick check: Verify the explicit solution Xₜ = e⁻ᵗX₀ + √(1-e⁻²ᵗ)ε satisfies the SDE

Score-Matching Loss
- Why needed: The training objective that measures the quality of score network approximations
- Quick check: Confirm that ℓ[θ] = E[|sθ(Xₜ,t) - ∇lnρ(Xₜ,t)|²] drives convergence to true scores

Support Containment Theorem
- Why needed: Central theoretical result showing exact reverse diffusion cannot generalize
- Quick check: Verify that q(·,T*) ⊂ support(ρ₀) for the given forward process

Kernel Formulation for Finite Samples
- Why needed: Provides explicit solution for expected origin in the overfitting regime
- Quick check: Confirm softmax weights λᵢ(x,t) = exp(2⟨x,xᵢ⟩/√(1-e⁻²ᵗ))/∑ⱼ exp(2⟨x,xⱼ⟩/√(1-e⁻²ᵗ)) sum to 1

## Architecture Onboarding

Component Map: Forward OU Process -> Score Network -> Reverse SDE -> Generated Samples

Critical Path: Forward process simulation → Score network training → Reverse process implementation → Sample generation and analysis

Design Tradeoffs:
- Perfect score-matching gives exact reconstruction but no generalization
- Imperfect approximations enable novel generation but sacrifice reconstruction accuracy
- Kernel-based approaches provide explicit solutions but require full dataset access

Failure Signatures:
- Overfitting: Generated samples exactly match training data points
- Numerical instability: Large errors when t → 0 due to 1/t relaxation term
- Poor generalization: Generated samples clustered in low-density regions of original distribution

First Experiments:
1. Implement forward OU process with exact solution and verify kernel-based expected origin produces Voronoi cell behavior
2. Train score network with varying levels of accuracy and measure overfitting by comparing generated samples to training data
3. Implement stable diffusion reverse updates and test convergence to nearest training points in different noise regimes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes perfect score-matching, while practical implementations use approximate methods
- The analysis focuses on Ornstein-Uhlenbeck process, which may not capture all diffusion model variants
- Implementation details for demonstrating theoretical results are sparse, particularly network architectures and training hyperparameters

## Confidence

High confidence in the theoretical derivation showing exact reverse diffusion cannot generalize beyond the original data distribution
Medium confidence in the explicit solutions for reverse SDEs and kernel formulations, pending numerical verification
Low confidence in practical implications without complete implementation details

## Next Checks

1. Implement the forward OU process with the exact solution Xₜ = e⁻ᵗX₀ + √(1-e⁻²ᵗ)ε and verify the kernel-based expected origin x₀(x,t) = ∑λᵢ(x,t)xᵢ produces Voronoi cell behavior where generated samples converge to training points

2. Run stable diffusion reverse updates with the explicit discrete formulation (Eq. 4.22-4.24) from pure noise and measure the degree of overfitting by computing the percentage of generated samples that exactly match training data points

3. Test the sensitivity of the overfitting behavior to the quality of the score network by training multiple networks with varying levels of score-matching accuracy and measuring how closely generated samples replicate the training distribution