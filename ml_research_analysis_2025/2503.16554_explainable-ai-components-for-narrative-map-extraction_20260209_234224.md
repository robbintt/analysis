---
ver: rpa2
title: Explainable AI Components for Narrative Map Extraction
arxiv_id: '2503.16554'
source_url: https://arxiv.org/abs/2503.16554
tags:
- narrative
- explanations
- system
- extraction
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an explainable AI (XAI) system for narrative
  map extraction that provides multi-level explanations to enhance user trust and
  understanding. The system integrates three components: keyword-based explanations
  from topical clusters for low-level document relationships, connection explanations
  using SHAP values and shared topics/entities for event relationships, and high-level
  structure explanations with storyline names and important event detection.'
---

# Explainable AI Components for Narrative Map Extraction

## Quick Facts
- arXiv ID: 2503.16554
- Source URL: https://arxiv.org/abs/2503.16554
- Authors: Brian Keith; Fausto German; Eric Krokos; Sarah Joseph; Chris North
- Reference count: 36
- Primary result: Multi-level XAI system enhances user trust and understanding in narrative map extraction through keyword-based topical clusters, SHAP connection explanations, and high-level storyline detection

## Executive Summary
This paper presents an explainable AI (XAI) system for narrative map extraction that provides multi-level explanations to enhance user trust and understanding. The system integrates three components: keyword-based explanations from topical clusters for low-level document relationships, connection explanations using SHAP values and shared topics/entities for event relationships, and high-level structure explanations with storyline names and important event detection. A user study with 10 participants analyzing 2021 Cuban protests narratives showed that connection explanations and important event detection were particularly effective at building user confidence, with participants reporting increased trust in the system's decisions. The multi-level explanation approach helped users develop appropriate trust in the narrative extraction capabilities, supporting more effective human-AI collaboration in complex analytical tasks.

## Method Summary
The system implements a three-tiered XAI approach for narrative map extraction. First, it generates narrative maps using HDBSCAN clustering on document embeddings, then applies modified TF-IDF to extract topical keywords for each cluster. Connection explanations use SHAP values to identify contributing terms and classify relationships as Topical, Similarity, or Entity-based using coherence score decomposition. High-level structure explanations employ noun phrase extraction and scoring functions to generate storyline names, while important event detection combines content-based centroid similarity with structural degree centrality. The system was evaluated through a user study where participants analyzed 2021 Cuban protests narratives, reporting trust and usefulness via Likert scales across 11 questionnaire items.

## Key Results
- Connection explanations using SHAP values and shared topics/entities proved particularly effective at building user confidence
- Important event detection was rated as highly relevant (M = 4.3) and useful (M = 4.2) by participants
- Multi-level explanations helped users develop appropriate trust in narrative extraction capabilities
- User trust in the system's decisions increased significantly when connection explanations were available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level explanations spanning low-level document relationships through high-level narrative structures support user sensemaking by matching cognitive needs at different granularities.
- Mechanism: The system provides three tiers—(1) keyword-based topical cluster explanations for document space, (2) connection explanations with SHAP-derived term contributions for edge verification, and (3) storyline names plus important event detection for global patterns. Users self-select the explanation level appropriate to their current analytical focus.
- Core assumption: Users require different explanation types at different stages of sensemaking; a single-level approach would leave gaps in understanding.
- Evidence anchors:
  - [abstract] "Our system integrates explanations based on topical clusters for low-level document relationships, connection explanations for event relationships, and high-level structure explanations for overall narrative patterns."
  - [section 3.2] "We accomplish this through a three-tiered approach that provides explanations for the document space... the narrative structure... and the connection between these two."
  - [corpus] Related work on multi-layered XAI frameworks (arXiv:2506.05887, arXiv:2509.08989) similarly emphasizes explanation granularity, though direct corpus evidence for narrative-specific multi-level systems is limited.
- Break condition: If users consistently rely on only one explanation level while ignoring others, the multi-level design adds complexity without benefit.

### Mechanism 2
- Claim: SHAP-based keyword explanations combined with connection type labels enhance user confidence by enabling direct verification of event linkage decisions.
- Mechanism: For each narrative edge, the system (a) classifies the connection type (Topical, Similarity, or Entity-based) using coherence score decomposition and entity overlap, and (b) computes SHAP values to identify terms with highest positive/negative contribution to the connection strength. Users can inspect these to confirm or question the system's reasoning.
- Core assumption: Users are more likely to trust algorithmic decisions when they can independently verify specific contributing factors rather than accepting opaque outputs.
- Evidence anchors:
  - [abstract] "connection explanations using SHAP values and shared topics/entities for event relationships... proving particularly effective at building user confidence."
  - [section 5.1] "The SHAP-based keyword explanations helped the participants verify connection validity, increasing confidence in the system's linking decisions."
  - [corpus] arXiv:2509.08989 discusses trust calibration through local and global explanations, supporting the verification-trust link, though narrative-specific SHAP applications remain underexplored.
- Break condition: If SHAP explanations highlight irrelevant terms or fail to surface causally meaningful features, users may experience "explanation backfire" and reduced trust.

### Mechanism 3
- Claim: Important event detection combining content-based centroid similarity and structure-based degree centrality helps users efficiently identify narrative-critical nodes.
- Mechanism: For each event, the system computes (1) content importance via cosine similarity to storyline centroid, and (2) structural importance via weighted degree centrality in the narrative graph. Events scoring highly on both receive visual emphasis, reducing search burden.
- Core assumption: Narrative-critical events can be approximated through embedding-space centrality and graph-theoretic connectivity; user attention should be guided accordingly.
- Evidence anchors:
  - [section 3.5.2] "Important event detection combines both content-based and structural approaches to identify key events in the narrative."
  - [section 4] "The important events detected by the system were considered relevant (M = 4.3, SD = 0.67) and useful (M = 4.2, SD = 0.79)."
  - [corpus] No direct corpus evidence found for this specific dual-criterion importance scoring in narrative extraction; this represents a gap in comparative validation.
- Break condition: If centroid similarity or degree centrality correlate poorly with human judgments of narrative importance, the highlighted events may misguide rather than assist users.

## Foundational Learning

- Concept: **SHAP (Shapley Additive Explanations)**
  - Why needed here: Core to connection explanations; interprets feature contributions to similarity scores.
  - Quick check question: Can you explain why SHAP values can be both positive and negative for the same prediction?

- Concept: **TF-IDF with cluster-aware modifications**
  - Why needed here: Generates keyword-based topic explanations for low-level document space understanding.
  - Quick check question: How does adding a local IDF component (cluster-specific) change the keyword ranking compared to standard TF-IDF?

- Concept: **Narrative maps as graph structures**
  - Why needed here: Foundational representation—events as nodes, relationships as edges—underlies all explanation components.
  - Quick check question: What types of relationships can connect events in a narrative map (per the paper's taxonomy)?

## Architecture Onboarding

- Component map:
  Input: News articles (headlines + content) -> Embedding generation -> HDBSCAN clustering -> Coherence computation -> Linear programming optimization -> Post-processing -> XAI Layer (Low-level: cluster keywords via modified TF-IDF, UMAP projection; Connection: label classification + SHAP term analysis + entity overlap; High-level: storyline name extraction + important event detection) -> Interactive narrative map with layered explanations

- Critical path: Extraction parameters (map size, story coverage, temporal sensitivity) -> narrative graph structure -> XAI component generation. If extraction produces poor coherence, downstream explanations will rationalize incorrect structures.

- Design tradeoffs:
  - Modified TF-IDF vs. neural topic models: simpler, more interpretable, but may miss latent semantic themes.
  - SHAP permutation approach vs. gradient-based: model-agnostic but computationally heavier; focuses on headline + first 30 words for efficiency vs. full-document coverage.
  - Static explanation thresholds (e.g., 50% clustering contribution for "Topical" label) vs. adaptive: current design is interpretable but may misclassify borderline cases.

- Failure signatures:
  - Storyline names with high variance in correctness ratings (M=3.0, SD=1.41) indicate the ranking-based extraction may produce irrelevant names for certain narrative structures.
  - Connection labels considered correct (M=4.0) but less useful (M=3.6) suggest labels may be accurate but insufficiently actionable for sensemaking.
  - Information overload when explanations are "too technical" (Section 5.1) -> users disengage.

- First 3 experiments:
  1. **Baseline comparison**: Run the same user study tasks with XAI components disabled; measure trust and insight counts to quantify explanation contribution.
  2. **Explanation-level ablation**: Disable one component at a time (e.g., remove SHAP explanations only) to isolate which elements drive the M=4.5 trust score.
  3. **Scalability stress test**: Apply to a corpus 5-10× larger than the 160-article test set; measure explanation generation latency and user comprehension degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would an abstractive strategy using generative neural networks for storyline naming be more effective than the current extractive method?
- Basis in paper: [explicit] Section 5.2 states that while the current method is extractive, "a shift towards an abstractive strategy... achieved with generative neural networks" might be more effective.
- Why unresolved: Current storyline naming received mixed feedback on correctness (Mean=3.0) and relevance (Mean=3.7), suggesting the extractive approach may fail to capture the essence of complex narratives.
- What evidence would resolve it: A user study comparing user-rated correctness and relevance of names generated by extractive vs. abstractive models on the same narrative maps.

### Open Question 2
- Question: How do specific temporal and causal explanation strategies impact user trust compared to the current similarity and entity-based explanations?
- Basis in paper: [explicit] Section 6 identifies the development of "more sophisticated temporal and causal explanation strategies" as a direction for future research.
- Why unresolved: The current system relies largely on topical/similarity connections; it does not explicitly visualize or explain the underlying temporal or causal logic inherent to narrative structures.
- What evidence would resolve it: An evaluation measuring user trust and sensemaking accuracy when using a system enhanced with explicit causal explanation components versus the current configuration.

### Open Question 3
- Question: Does the inclusion of these XAI components improve objective task performance or insight generation compared to a non-explainable baseline?
- Basis in paper: [inferred] Section 5.3 notes the study did not compare the system with a baseline (e.g., the same system without XAI), limiting conclusions to user perception rather than performance gains.
- Why unresolved: While users reported increased trust, it remains unclear if the explanations actually helped them generate more insights or complete tasks faster than a standard "black box" interface.
- What evidence would resolve it: A controlled experiment comparing the number, depth, and accuracy of insights generated by users using the XAI system versus a control group using a non-explainable version.

### Open Question 4
- Question: Can hierarchical explanation strategies maintain explanation quality while scaling to narrative collections significantly larger than the 160-article dataset used?
- Basis in paper: [inferred] Section 5.3 lists scalability as a limitation, noting that computational complexity may become prohibitive for larger collections.
- Why unresolved: The current "multi-level" approach might suffer from information overload or latency issues when applied to massive document sets, requiring new summarization methods.
- What evidence would resolve it: Performance benchmarks (latency) and user usability studies applying the system to datasets an order of magnitude larger (e.g., >10,000 articles).

## Limitations

- Small sample size (n=10) in user study limits generalizability of trust and usefulness findings
- No statistical significance testing reported for Likert-scale results, making it difficult to distinguish true effects from sampling variation
- System hyperparameters and embedding choices underspecified, potentially leading to variation in reproduced explanations
- Single-domain focus (2021 Cuban protests) means performance on other narrative types remains unknown

## Confidence

- **High confidence**: The multi-level explanation framework concept is well-specified and logically structured. The SHAP-based connection verification mechanism is clearly described.
- **Medium confidence**: The user study results showing trust improvements are plausible but not robustly supported due to sample size and lack of significance testing.
- **Low confidence**: The exact implementation details for several components (NER system, embedding models, hyperparameter values) are unspecified, making faithful reproduction challenging.

## Next Checks

1. Conduct statistical significance testing on the user study results to determine if reported differences in trust (M=4.5) and usefulness ratings are reliable beyond sampling variation.
2. Perform cross-domain validation by applying the system to a different narrative corpus (e.g., COVID-19 pandemic narratives) and measuring explanation effectiveness and user trust in that context.
3. Run an ablation study removing each XAI component individually to quantify their specific contributions to user trust and insight discovery, beyond the current aggregate reporting.