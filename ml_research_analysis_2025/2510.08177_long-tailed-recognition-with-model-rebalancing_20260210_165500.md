---
ver: rpa2
title: Long-tailed Recognition with Model Rebalancing
arxiv_id: '2510.08177'
source_url: https://arxiv.org/abs/2510.08177
tags:
- more
- classes
- learning
- long-tailed
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses long-tailed recognition challenges in deep
  learning by introducing MOdel REbalancing (MORE), which manipulates the model's
  parameter space to improve minority class representation without increasing model
  complexity or inference costs. The core idea involves decomposing neural network
  parameters into generalizable and minority-specific components using low-rank adaptation,
  guided by a discrepancy-based loss and sinusoidal reweighting schedule.
---

# Long-tailed Recognition with Model Rebalancing

## Quick Facts
- arXiv ID: 2510.08177
- Source URL: https://arxiv.org/abs/2510.08177
- Reference count: 40
- Primary result: Introduces MORE framework that improves minority class representation in long-tailed recognition through parameter space manipulation without increasing model complexity

## Executive Summary
This paper addresses the fundamental challenge of long-tailed recognition in deep learning by introducing MOdel REbalancing (MORE), a novel framework that manipulates the model's parameter space to improve minority class representation. The core innovation involves decomposing neural network parameters into generalizable and minority-specific components using low-rank adaptation, guided by a discrepancy-based loss and sinusoidal reweighting schedule. This approach theoretically tightens generalization bounds for long-tailed learning by redistributing modeling capacity toward underrepresented classes while maintaining efficiency during inference.

## Method Summary
MORE decomposes neural network weights into generalizable parameters (θ_g) and minority-specific parameters (θ_t) using low-rank adaptation. The method computes a discrepancy-based loss measuring the difference between full-parameter logits and general-only logits, weighted by class frequency to emphasize minority classes. A sinusoidal reweighting schedule balances early general feature learning with mid-training tail specialization. At inference, parameters are merged for zero overhead. The framework is theoretically justified by showing that restricting majority class hypothesis space while maintaining full capacity for minority classes tightens the balanced risk generalization bound.

## Key Results
- MORE consistently improves tail-class accuracy across multiple long-tailed datasets including CIFAR-100-LT, Places-LT, and ImageNet-LT
- The sinusoidal reweighting schedule outperforms constant and cosine alternatives for balancing general and tail-specific learning
- Integration with CLIP-based finetuning demonstrates effectiveness in multi-label settings across diverse benchmarks
- Theoretical analysis shows parameter decomposition can tighten generalization bounds compared to standard baselines

## Why This Works (Mechanism)

### Mechanism 1
Decomposing model parameters into generalizable (θ_g) and minority-specific (θ_t) components can tighten the balanced risk generalization bound compared to a standard baseline. The decomposition restricts the hypothesis space for majority classes to a subset G_maj ⊆ G (via the low-rank constraint on θ_t), reducing Rademacher complexity for those classes while maintaining full expressive capacity for minority classes via the complete space G_min = G. This asymmetrical capacity redistribution yields a tighter upper bound on balanced risk R^L_bal(f).

### Mechanism 2
The discrepancy-based loss L_MORE with class-proportional weighting encourages θ_t to specialize on minority classes while θ_g captures majority knowledge. The loss computes the ℓ2 distance between logits from full parameters (θ_g ⊕ θ_t) and general-only parameters (θ_g): M(x;θ) = ||f(x; θ_g ⊕ θ_t) - f(x; θ_g)||². Weighting by π_y assigns larger penalties to majority-class discrepancies, pushing θ_t to minimize influence on majority predictions while retaining expressive contribution for minority classes.

### Mechanism 3
A sinusoidal reweighting schedule for α(τ) balances early general feature learning with mid-training tail specialization. α(τ) = A·sin(πτ/T) starts low (allowing θ_g to learn coarse-grained features), peaks mid-training (enabling θ_t to learn from minority classes), and reduces late (preventing overfitting to the reallocation signal).

## Foundational Learning

- **Rademacher Complexity and Generalization Bounds:** Why needed here: The theoretical justification (Theorem 1) relies on class-specific Rademacher complexity to prove tighter balanced risk bounds. Quick check question: Can you explain how restricting the hypothesis space for majority classes reduces Rademacher complexity without harming empirical risk?

- **Low-Rank Decomposition (LoRA-style):** Why needed here: MORE's parameter decomposition (W_i = W^g_i + B^t_i A^t_i) mirrors LoRA but differs in optimization objective. Understanding LoRA helps distinguish structural similarity from functional difference. Quick check question: How does MORE's use of low-rank decomposition differ from LoRA's adaptation objective?

- **Long-Tailed Learning Basics:** Why needed here: The method targets balanced risk R^L_bal(f) and evaluates on Many/Medium/Few splits. Familiarity with imbalance factors, tail-class degradation, and existing approaches (reweighting, decoupled training) is essential. Quick check question: Why do standard empirical risk minimization and balanced accuracy diverge under class imbalance?

## Architecture Onboarding

- **Component map:** Base model (e.g., ResNet) with decomposed weights θ = θ_g ⊕ θ_t -> Dual forward pass (f(x; θ_g ⊕ θ_t) and f(x; θ_g)) -> Loss modules (L_base + α(τ)·L_MORE) -> Sinusoidal scheduler (α(τ) = A·sin(πτ/T)) -> Merged parameters for inference

- **Critical path:** 1) Initialize θ_g from pretrained or random; initialize θ_t as low-rank (B^t_i, A^t_i with rank r) 2) For each batch: compute L_base using full forward pass 3) Compute L_MORE via logit discrepancy M(x;θ) and class weights π_y 4) Apply sinusoidal α(τ) and backpropagate L = L_base + α(τ)·L_MORE 5) At inference, merge parameters; no separate low-rank computation

- **Design tradeoffs:** Rank r: Lower r reduces capacity for θ_t but improves efficiency; paper finds r ≈ 0.1×min(m, k) works well (Fig. 2d). Amplitude A: Controls L_MORE influence; normalized A' ≈ 2.0 is optimal (Fig. 2c). Schedule choice: Sinusoidal > cosine > constant per Table 5; but may require tuning per dataset

- **Failure signatures:** Head-class accuracy drops sharply: θ_t may be interfering with majority knowledge; check if π_y weighting is inverted or α(τ) is too high early. Tail-class gains negligible: θ_t may be under-capacity (rank r too low) or discrepancy metric is uninformative (try ℓ2 vs KL per Fig. 3). Training instability: Sinusoidal schedule may peak too early/late; adjust T or A

- **First 3 experiments:** 1) Sanity check on CIFAR-100-LT (IF=10): Implement MORE with BCE baseline, rank r=0.1, A'=2.0. Verify Many/Medium/Few accuracy improvements match Table 1 (~1% overall gain expected). 2) Ablation on discrepancy metric: Compare ℓ2 distance vs KL divergence on a multi-label dataset (e.g., MIML). Confirm ℓ2 outperforms per Fig. 3a,b. 3) Schedule comparison: Test constant, cosine, and sinusoidal α(τ) on Places-LT with LA baseline. Verify sinusoidal yields best Few-class improvement per Table 5 pattern.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does partitioning tail classes into semantically coherent groups to learn dedicated low-rank parameters mitigate representational trade-offs better than a unified tail space? The Conclusion proposes "mitigating representational trade-offs for semantically disparate tail classes via semantic grouping."

- **Open Question 2:** Would enforcing an orthogonality constraint between general (θ_g) and tail-specific (θ_t) parameters enhance model modularity and performance? The Conclusion and Appendix D identify "a stricter decoupling... such as through orthogonality constraints" as a promising direction to reduce redundancy.

- **Open Question 3:** Can a dynamic decomposition strategy that allocates capacity based on layer-specific importance for minority classes outperform the current static decomposition? Appendix F states the limitation that "current static parameter decomposition applies uniformly across layers" and suggests dynamic allocation could be beneficial.

## Limitations

- Theoretical assumptions (uniform class priors, empirical risk preservation under decomposition) lack comprehensive empirical validation across diverse datasets
- The unified low-rank space for all tail classes may create competition among semantically dissimilar classes
- Current static parameter decomposition applies uniformly across layers, potentially ignoring layer-specific importance for tail class learning

## Confidence

- **High Confidence:** Experimental results showing consistent improvements across multiple datasets and baselines, particularly the effectiveness of sinusoidal scheduling and class-weighted discrepancy loss
- **Medium Confidence:** The theoretical bound tightening mechanism, as it relies on assumptions that need broader empirical validation
- **Medium Confidence:** The claim that low-rank adaptation specifically benefits tail classes without harming head classes, as this depends on proper calibration of rank r and scheduling parameters

## Next Checks

1. **Assumption Testing:** Systematically test Assumptions (1) and (2) across multiple long-tailed datasets with varying imbalance factors to verify the theoretical bound holds beyond the initial CIFAR-100-LT validation

2. **Rank Sensitivity Analysis:** Conduct experiments varying rank r across a wider range (0.01 to 0.5×min(m,k)) on multiple datasets to identify the precise relationship between capacity allocation and tail-class performance

3. **Schedule Robustness:** Test the sinusoidal schedule against real-world data distribution shifts where class frequencies change during training, validating whether the temporal separation of general and tail-specific learning remains optimal