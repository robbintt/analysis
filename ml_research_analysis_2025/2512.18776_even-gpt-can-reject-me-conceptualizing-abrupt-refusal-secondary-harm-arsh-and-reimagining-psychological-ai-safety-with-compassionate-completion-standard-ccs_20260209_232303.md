---
ver: rpa2
title: '"Even GPT Can Reject Me": Conceptualizing Abrupt Refusal Secondary Harm (ARSH)
  and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)'
arxiv_id: '2512.18776'
source_url: https://arxiv.org/abs/2512.18776
tags:
- safety
- harm
- health
- relational
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This viewpoint introduces Abrupt Refusal Secondary Harm (ARSH)
  to describe psychological distress caused when AI safety protocols abruptly terminate
  emotionally supportive conversations. To mitigate ARSH, we propose the Compassionate
  Completion Standard (CCS), a human-centered design framework that replaces abrupt
  refusal with empathetic acknowledgment, transparent boundary articulation, graded
  conversational transition, and guided redirection.
---

# "Even GPT Can Reject Me": Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)

## Quick Facts
- arXiv ID: 2512.18776
- Source URL: https://arxiv.org/abs/2512.18776
- Reference count: 0
- Primary result: Proposes CCS framework to replace abrupt AI refusal with empathetic, staged conversational transitions that preserve relational continuity while maintaining safety compliance.

## Executive Summary
This viewpoint paper introduces Abrupt Refusal Secondary Harm (ARSH) as a psychological distress mechanism triggered when AI safety protocols abruptly terminate emotionally supportive conversations. The authors argue that such abrupt refusal can rupture perceived relational continuity, evoke feelings of rejection or shame, and discourage future help-seeking behavior. To address this, they propose the Compassionate Completion Standard (CCS), a human-centered design framework that transforms hard-stop refusals into guided completions through eight stages: detection, validation, transparent boundary articulation, collaborative problem-solving, affective matching, owning limitations, guided handoff, and closure. The framework aims to operationalize counseling ethics into AI interaction strategies, balancing safety compliance with psychological care.

## Method Summary
This is a conceptual framework paper that synthesizes insights from counseling psychology, human-computer interaction, and AI safety research to propose the ARSH construct and CCS framework. The methodology involves theoretical analysis of existing literature on therapeutic alliance rupture, iatrogenic harm, and parasocial attachment, combined with practical design considerations for implementing empathetic conversational transitions. The authors present an eight-stage workflow with detailed stage descriptions and implementation guidelines, while explicitly acknowledging the need for empirical validation through future research studies.

## Key Results
- Defines ARSH as psychological harm from abrupt AI refusal, particularly affecting users with attachment vulnerabilities
- Proposes CCS as an 8-stage framework transforming refusal from termination to guided completion
- Identifies critical design tradeoffs between compassion duration and safety risk exposure
- Highlights need for anticipatory consent protocols to reduce hermeneutic harm
- Calls for empirical studies to quantify ARSH incidence and test CCS efficacy

## Why This Works (Mechanism)

### Mechanism 1: Relational Rupture Pathway
When users form attachment-like bonds with AI through emotional disclosure, sudden termination mimics therapeutic abandonment—reactivating attachment insecurity and producing confusion, shame, and help-seeking avoidance. The mechanism assumes users project parasocial relational intentions onto AI systems despite knowing they are non-human. Evidence shows abrupt refusals can rupture perceived relational continuity, particularly at moments when users most need consistent care. The mechanism fails if users do not form attachment-like bonds or perceive AI as purely transactional.

### Mechanism 2: Graded Transition Preserves Relational Coherence
The 8-stage CCS workflow transforms refusal from an algorithmic termination into an ethically informed crisis transition, preserving user agency and relational continuity. This mechanism assumes extended transition dialogues do not themselves increase risk exposure beyond acceptable thresholds. Evidence shows empathetic acknowledgment, transparent boundary articulation, and graded conversational transition can maintain trust while enforcing safety boundaries. The mechanism breaks if transition stages extend high-risk dialogue duration significantly without reducing distress.

### Mechanism 3: Anticipatory Consent Reduces Hermeneutic Harm
Proactive disclosure of safety scope before crisis triggers reduces confusion and perceived betrayal during refusal. The mechanism assumes users retain and apply early consent information during later emotional crises. Evidence suggests this approach mirrors informed consent in psychotherapy and reduces hermeneutic distress—the confusion and distress caused when actions lack an understandable context. The mechanism degrades to reactive transparency only if users cannot recall or process anticipatory information during crisis.

## Foundational Learning

- Concept: **Therapeutic Alliance Rupture**
  - Why needed here: CCS stages directly borrow from rupture-repair literature; understanding "rupture" as a design failure state is prerequisite to implementing repair logic.
  - Quick check question: Can you distinguish between a "content refusal" (I won't answer that) and a "relational rupture" (you've broken our connection)?

- Concept: **Iatrogenic Harm**
  - Why needed here: ARSH is defined as iatrogenic—harm caused by the intervention itself; this frames safety protocols as potential harm sources requiring mitigation.
  - Quick check question: Would a standard safety refusal that prevents self-harm content still qualify as potentially iatrogenic under this framework?

- Concept: **Parasocial Attachment to AI**
  - Why needed here: The entire ARSH mechanism depends on users forming attachment-like bonds; without this, abrupt refusal produces annoyance but not psychological injury.
  - Quick check question: What user characteristics might predict stronger parasocial attachment to chatbots?

## Architecture Onboarding

- Component map:
  - Detection layer (flags risk content without immediate hard-stop) -> Transition orchestrator (manages state progression through Stages 1-7) -> Closure handler (summarizes, confirms agreement, preserves re-engagement path) -> UX checklist validator (enforces "Do-No-Further-Harm" criteria at each stage)

- Critical path:
  1. Pre-Stage 0: Inject relational disclosure during emotional-support onboarding
  2. Stage 0: Soft-flag risk → begin harm-minimization preamble
  3. Stages 1-4: Validate → explain → collaborate → match affect (linear but loopable)
  4. Stages 5-8: Own limits → handoff → confirm agreement → close with re-engagement cue

- Design tradeoffs:
  - **Compassion vs. duration**: Extended transitions may keep users in high-risk states longer (explicitly flagged for RCT testing)
  - **Scripted vs. adaptive tone**: Formulaic language detected as inauthentic (Stage 4 explicitly warns against "sounding scripted")
  - **Transparency vs. model opacity**: Explaining safety logic requires interpretable internal states; black-box models may fail Stage 2

- Failure signatures:
  - User resistance/ambivalence not addressed → loop back to Stage 3
  - Formulaic or repetitive responses → violates Stage 4 affect-matching
  - No re-engagement path offered → Stage 8 incomplete, future help-seeking discouraged
  - Hard-stop triggered despite CCS → detection layer misconfiguration

- First 3 experiments:
  1. **Retrospective analysis**: Tag existing conversation logs for ARSH indicators (confusion, abandonment language, disengagement timing post-refusal)
  2. **A/B prototype test**: Implement simplified 4-stage CCS (validate, explain, collaborate, close) against standard refusal; measure user-reported distress and continued engagement
  3. **Duration safety audit**: Measure whether CCS conversations extend high-risk dialogue duration; correlate with downstream safety outcomes (referral completion, re-engagement rates)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the incidence and severity of Abrupt Refusal Secondary Harm (ARSH), and can the specific psychological harm be isolated from pre-existing user vulnerabilities?
- Basis in paper: Section 6.3 states the need to "move beyond anecdotal evidence to systematically quantify the incidence and severity of ARSH" and "isolating the incremental psychological harm attributable specifically to the manner of refusal versus pre-existing vulnerabilities."
- Why unresolved: Current understanding relies on user reports and conceptual frameworks rather than systematic longitudinal data.
- What evidence would resolve it: Digital phenotyping studies and rigorous longitudinal research isolating the refusal mechanism as the causal factor.

### Open Question 2
- Question: Does the extended dialogue required by the Compassionate Completion Standard (CCS) increase the duration of high-risk conversations, potentially causing "safety risk drift"?
- Basis in paper: Section 6.3 calls for trials to "investigate ethical trade-offs, specifically assessing whether the compassionate steps increase the duration of high-risk dialogue, thereby causing safety risk drift."
- Why unresolved: While CCS aims to reduce harm, the extended interaction time in crisis situations presents an untested safety trade-off.
- What evidence would resolve it: Randomized controlled trials comparing the duration and safety outcomes of CCS interactions versus standard refusals.

### Open Question 3
- Question: How can the multi-step logic of the Compassionate Completion Standard (CCS) be robustly integrated into foundational LLM architectures to ensure algorithmic stability?
- Basis in paper: Section 6.3 asks how to "robustly integrate the CCS's complex, multi-step logic into foundational LLM architectures, thereby ensuring the model's algorithmic stability and ethical reliability."
- Why unresolved: Translating nuanced, stage-based counseling principles into stable, probabilistic model behaviors is a significant technical challenge.
- What evidence would resolve it: Design-science research demonstrating stable implementation of the eight-stage workflow in production-level models.

## Limitations

- Framework remains theoretical without empirical validation of ARSH incidence or CCS effectiveness
- Assumes users will meaningfully engage with anticipatory disclosure during non-crisis states
- Does not address potential safety risks from extended dialogue duration during high-risk conversations
- Implementation complexity may exceed current LLM interpretability and controllability capabilities

## Confidence

- ARSH mechanism (attachment rupture through abrupt refusal): Medium
- CCS framework coherence and theoretical grounding: High
- CCS empirical efficacy and safety impact: Low
- Feasibility of implementation in current LLM architectures: Low

## Next Checks

1. Conduct retrospective analysis of existing conversation logs to establish baseline ARSH indicators and harm rates
2. Run controlled A/B testing comparing simplified CCS protocols against standard refusal, measuring user-reported distress and engagement outcomes
3. Perform safety audit of CCS conversations to quantify duration impact on high-risk dialogue and downstream safety metrics