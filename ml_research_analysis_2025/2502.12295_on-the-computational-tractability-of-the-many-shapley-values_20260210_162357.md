---
ver: rpa2
title: On the Computational Tractability of the (Many) Shapley Values
arxiv_id: '2502.12295'
source_url: https://arxiv.org/abs/2502.12295
tags:
- shap
- complexity
- state
- distributions
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes the computational complexity of computing various
  Shapley value variants, including Conditional, Interventional, and Baseline SHAP,
  for different machine learning models under various distributions. The authors establish
  both tractable and intractable results.
---

# On the Computational Tractability of the (Many) Shapley Values

## Quick Facts
- arXiv ID: 2502.12295
- Source URL: https://arxiv.org/abs/2502.12295
- Reference count: 40
- Primary result: Establishes both tractable and intractable complexity results for computing various Shapley value variants (Conditional, Interventional, Baseline SHAP) across different ML models and distributions.

## Executive Summary
This paper provides a comprehensive computational complexity analysis of Shapley value variants used in explainable AI. The authors establish when computing different SHAP variants is tractable versus intractable, showing that Baseline and Interventional SHAP can be computed in polynomial time for certain models (weighted automata, decision trees, linear regression) under Hidden Markov Model distributions, while Conditional SHAP and some variants for neural networks and tree ensemble classifiers are generally NP-hard. The results demonstrate strict complexity gaps between variants, enabling practitioners to choose tractable alternatives when axiomatic properties permit.

## Method Summary
The paper employs computational complexity theory to analyze SHAP variants across different model families and distribution classes. The authors use polynomial-time reductions to show tractability for weighted automata and decision trees under HMM distributions, leveraging algebraic operations like Kronecker products and projections on N-Alphabet Weighted Automata. For intractability results, they reduce from well-known hard problems like Closest String. The analysis covers both local (instance-specific) and global (distribution-aggregated) variants across Baseline, Interventional, and Conditional SHAP definitions.

## Key Results
- Local and global Interventional and Baseline SHAP can be computed in polynomial time for weighted automata, decision trees, and linear regression models under HMM distributions.
- Computing SHAP for neural networks and tree ensemble classifiers is generally intractable, even under simplified settings.
- Strict complexity gaps exist between SHAP variants, with Conditional SHAP being strictly harder than Interventional and Baseline variants for certain model-distribution pairs.
- The HMM distribution class subsumes empirical, independent, and Markovian distributions, allowing tractability results to extend to simpler distributional assumptions.

## Why This Works (Mechanism)

### Mechanism 1: Weighted Automata as a Unifying Computational Substrate
Weighted Automata (WA) enable polynomial-time computation of Interventional and Baseline SHAP because they support efficient algebraic operations (Kronecker product, projection) that directly encode the combinatorial SHAP formulation. The algorithm reduces SHAP computation to iterative applications of projection operators Π₁, Π₂, Π₃ on N-Alphabet WAs. These operators aggregate over coalitions (patterns), reference distributions, and model outputs without explicit enumeration. The polynomial-time construction relies on building auxiliary WAs (A_w,i, T_w, T_w,i) whose sizes are O(|w|³) relative to sequence length.

### Mechanism 2: Hidden Markov Model Distribution Abstraction
HMMs subsume empirical, independent, and Markovian distributions, allowing tractability results proven for HMMs to extend to simpler distributional assumptions. The paper proves EMP ⪯_P HMM and IND ⪯_P MARKOV ⪯_P HMM via polynomial-time reductions. An empirical distribution D is converted to a sequentialized dataset SEQ(D), then encoded as an HMM where states are prefixes and transition probabilities reflect empirical frequencies.

### Mechanism 3: Complexity Gap Between SHAP Variants via Reduction
Conditional SHAP is strictly harder than Interventional and Baseline SHAP for certain model-distribution pairs, enabling practitioners to choose tractable variants when axiomatic properties allow. The paper establishes LOC-I-SHAP(WA,HMM) ∈ PTIME while LOC-C-SHAP(DT,EMP) is NP-Hard. Since IND distributions make Interventional and Conditional equivalent, the gap manifests precisely when features are dependent. Hardness for Interventional SHAP (e.g., for RNN-ReLU) is proven via reduction from the Closest String Problem.

## Foundational Learning

**Concept: Shapley Value Variants (Conditional, Interventional, Baseline)**
Why needed: The paper's central claim is that computational tractability depends critically on which variant is computed; understanding the definitions (Eqs. 1-3) is prerequisite to interpreting the results.
Quick check: For a feature j ∉ S, does Baseline SHAP replace its value with a fixed reference or a sampled value from a distribution?

**Concept: Computational Complexity Classes (PTIME, NP-Hard, coNP-Hard, #P-Hard)**
Why needed: The paper's results are stated in terms of these classes; distinguishing polynomial-time tractability from intractability is essential for practical deployment decisions.
Quick check: If a problem is NP-Hard, can we efficiently compute exact solutions for large instances without approximation?

**Concept: Weighted Automata and N-Alphabet Operations**
Why needed: The tractability proofs rely on WA closure under Kronecker products and projections (Definition 3); understanding these operations is necessary to follow the algorithmic constructions.
Quick check: What does the projection operation Πᵢ(A, T) compute in terms of the functions implemented by A and T?

## Architecture Onboarding

**Component map:**
Model Families: DT (decision trees), ENS-DTR (regression ensembles), ENS-DTC (classification ensembles), LINR (linear regression), WA (weighted automata), NN-SIGMOID, RNN-ReLU
Distribution Families: IND (independent), EMP (empirical), MARKOV (Markovian), HMM (Hidden Markov Model), with inclusions IND ⊂ MARKOV ⊂ HMM and EMP ⊂ HMM
SHAP Variants: Baseline (B), Interventional (I), Conditional (C)
Scope: Local (LOC, per-instance) vs. Global (GLO, distribution-aggregated)

**Critical path:**
1. Identify model family and distribution for your use case
2. Check Table 1 for the complexity of S-V-SHAP(F, P) for your S (scope), V (variant), F (model), P (distribution)
3. If PTIME, use the projection/Kronecker construction (Appendix 4) for exact computation; if intractable, consider approximation or model simplification (e.g., use ENS-DTR instead of ENS-DTC)

**Design tradeoffs:**
- Baseline vs. Interventional SHAP: Baseline is simpler (fixed reference) but may not reflect data distribution; Interventional accounts for distribution but requires HMM modeling
- Regression ensembles vs. Classification ensembles: ENS-DTR is tractable under HMM; ENS-DTC is NP-Hard—prefer soft-voting (regression-style) outputs for tractability
- Model expressiveness vs. tractability: WAs and tree-based models are tractable; neural networks are generally intractable except under very restrictive conditions

**Failure signatures:**
- Intractable inputs: Neural networks (NN-SIGMOID, RNN-ReLU) and hard-voting classification ensembles (ENS-DTC) will cause exponential runtime if exact SHAP is attempted
- Distribution mismatch: If the true data distribution is not well-modeled by HMM, the computed SHAP values may be unfaithful to the actual feature contributions
- Complexity class misinterpretation: Confusing NP-Hardness (decision problem hardness) with #P-Hardness (counting problem hardness) may lead to inappropriate algorithm choices

**First 3 experiments:**
1. Replicate the WA tractability result: Implement the projection and Kronecker operations for a small WA (n=3-5 features) under an HMM distribution and verify polynomial scaling
2. Test the complexity gap: Compare runtime of Interventional vs. Conditional SHAP for a decision tree under an HMM distribution; confirm exponential blowup for Conditional
3. Validate the ENS-DTR tractability claim: Compute Interventional SHAP for an XGBoost regression model under an empirical distribution approximated by HMM; compare against TreeSHAP results for consistency

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the computation of SHAP values for intractable models, such as neural networks, be efficiently approximated using approximation and parameterized complexity theory?
- Basis: The authors state that "investigating the approximability of SHAP metrics through approximation and parameterized complexity theory... is an important direction."
- Why unresolved: The paper establishes intractability for exact computation in certain settings but does not explore whether efficient approximation algorithms with formal guarantees exist.
- What evidence would resolve it: Algorithms that provide bounded approximation guarantees for SHAP values on neural networks or parameterized tractability results for specific structural properties.

**Open Question 2**
- Question: How does the computational complexity change when computing other SHAP variants, such as Asymmetric SHAP and SAGE?
- Basis: The conclusion lists "expanding our computational analysis to other SHAP-related metrics, such as asymmetric SHAP and SAGE" as a direction for future research.
- Why unresolved: The current analysis is restricted to Conditional, Interventional, and Baseline SHAP, leaving the complexity of these other variants uncharacterized.
- What evidence would resolve it: A complexity analysis establishing tractability or intractability results for Asymmetric SHAP and SAGE across the model and distribution classes discussed.

**Open Question 3**
- Question: Do the tractability results for Weighted Automata and linear models under HMM distributions extend to continuous input domains?
- Basis: Appendix 1 notes the use of a discrete input assumption and identifies extending membership proofs for linear models and neural networks to continuous domains as a "promising direction."
- Why unresolved: The constructive proofs for polynomial-time algorithms rely on discrete formalizations, and it is unclear if the same logic holds for continuous feature spaces.
- What evidence would resolve it: A proof demonstrating that the proposed algorithms for Interventional and Baseline SHAP on Weighted Automata remain polynomial-time computable with continuous inputs.

## Limitations
- The complexity results hinge on worst-case theoretical bounds, which may not reflect practical performance for small or structured instances
- The paper's tractability claims assume models and distributions can be efficiently encoded as weighted automata, but constructing such automata for complex real-world models may be non-trivial
- The intractability results for neural networks and tree ensemble classifiers rely on reductions from well-known hard problems, but the constants and practical impact of these reductions are not explored

## Confidence
- High: The polynomial-time algorithms for weighted automata are well-defined and constructive, with explicit operator formulations
- Medium: The complexity gaps between SHAP variants are proven under standard complexity-theoretic assumptions (P ≠ NP), but practical significance varies by use case
- Medium: The intractability results for neural networks and tree ensembles are theoretically sound, but real-world performance may depend on instance structure and approximation methods

## Next Checks
1. Implement and benchmark the WA-based SHAP computation for a small, structured dataset to empirically verify polynomial scaling and compare against approximation methods
2. Analyze the faithfulness of Interventional vs. Conditional SHAP on a real-world dataset with dependent features to quantify the practical impact of the complexity gap
3. Test the tractability claim for XGBoost regression under HMM distributions by comparing exact Interventional SHAP computation time against TreeSHAP and approximation baselines