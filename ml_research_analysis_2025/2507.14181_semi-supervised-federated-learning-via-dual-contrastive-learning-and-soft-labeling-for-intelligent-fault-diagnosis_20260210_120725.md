---
ver: rpa2
title: Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling
  for Intelligent Fault Diagnosis
arxiv_id: '2507.14181'
source_url: https://arxiv.org/abs/2507.14181
tags:
- data
- learning
- fault
- local
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a semi-supervised federated learning framework
  (SSFL-DCSL) for intelligent fault diagnosis (IFD) in industrial machinery. The framework
  addresses challenges of data and label scarcity across distributed clients while
  preserving privacy.
---

# Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis

## Quick Facts
- arXiv ID: 2507.14181
- Source URL: https://arxiv.org/abs/2507.14181
- Reference count: 40
- This paper proposes a semi-supervised federated learning framework (SSFL-DCSL) for intelligent fault diagnosis (IFD) in industrial machinery that significantly outperforms state-of-the-art methods, achieving 1.15% to 7.85% accuracy improvements in the most challenging scenario with only 10% labeled data.

## Executive Summary
This paper addresses the challenge of intelligent fault diagnosis in industrial machinery where data and labels are scarce and distributed across multiple clients. The proposed SSFL-DCSL framework combines semi-supervised learning with federated learning to enable collaborative model training while preserving data privacy. The framework leverages dual contrastive learning and a novel truncated Laplace-based adaptive sample weighting function to effectively utilize both labeled and unlabeled data, achieving state-of-the-art performance on three public datasets and a factory-collected motor dataset.

## Method Summary
SSFL-DCSL employs a semi-supervised federated learning framework where clients collaboratively train a fault diagnosis model using both labeled and unlabeled data while preserving privacy. The method introduces three key innovations: a truncated Laplace-based adaptive sample weighting function to mitigate bias from low-confidence pseudo-labels, a dual contrastive loss combining local contrastive learning for self-supervised consistency and global contrastive loss for alignment with global prototypes, and prototype aggregation with weighted averaging and momentum updates for efficient knowledge sharing. The framework significantly reduces communication overhead by transmitting compact prototypes instead of full model parameters, achieving over 99% reduction in communication costs while maintaining or improving model performance.

## Key Results
- Achieves 1.15% to 7.85% accuracy improvements over state-of-the-art methods in the most challenging scenario with only 10% labeled data
- Reduces communication overhead by over 99% by transmitting compact prototypes (0.06 MB) instead of full model parameters (8.6 MB)
- Demonstrates significant performance gains across three public datasets (CWRU, PU, CWRU-6Channel) and a factory-collected motor dataset
- Ablation studies confirm the effectiveness of each component, with PTA+GCL+LCL(Naive) achieving 88.72% accuracy on PU dataset (10% labels) versus 80.39% with PTA alone

## Why This Works (Mechanism)

### Mechanism 1: Truncated Laplace-based Adaptive Sample Weighting (TLAW)
The TLAW mechanism weights pseudo-labels by confidence deviation to reduce training bias from noisy labels. It estimates confidence distribution statistics (mean $\hat{\mu}_t$, variance $\hat{\sigma}^2_t$) via exponential moving average across batches. Samples with confidence below $\hat{\mu}_t$ receive weights from a truncated Laplace distribution, while high-confidence samples receive maximum weight. This creates a soft curriculum that progressively upweights reliable pseudo-labels as model confidence improves. The mechanism assumes pseudo-label confidence correlates with correctness, and Theorem 1 guarantees minimum pseudo-label quantity and quality bounds. However, if the confidence-correction correlation is weak due to systematic overconfidence from domain shift, the weighting scheme may amplify rather than reduce noise.

### Mechanism 2: Dual Contrastive Loss for Local-Global Feature Alignment
The dual contrastive loss combines local self-supervised contrastive learning with global prototype alignment to improve representation quality under label scarcity and non-IID data. The local contrastive loss uses weak/strong augmented views of unlabeled data with dynamic temperature adaptation to batch confidence variance. The global contrastive loss pulls local prototypes toward global prototypes while pushing away from other classes' global prototypes. Joint optimization of these losses improves feature learning when labeled data is limited. The mechanism assumes unlabeled data contains learnable structure that contrastive objectives can exploit and that global prototypes represent meaningful class centers across heterogeneous clients. If pseudo-labels are systematically incorrect, the local contrastive loss may reinforce wrong clustering, and the global contrastive loss may propagate biased prototypes if majority clients have skewed distributions.

### Mechanism 3: Prototype Aggregation with Momentum Updates
Prototype aggregation transmits class prototypes instead of full model parameters to reduce communication overhead while maintaining performance. Clients compute local prototypes as class-conditional embedding means, and the server aggregates via sample-weighted averaging with momentum updates to prevent oscillation from noisy local prototypes. The mechanism assumes prototypes capture sufficient class information and that the prototype space is more stable than parameter space under heterogeneous data. This approach achieves over 99% communication overhead reduction by transmitting compact prototype representations (0.06 MB) versus full model parameters (8.6 MB). However, if the embedding space dimension is too low or intra-class variance is high, prototypes may not represent class distributions adequately, and extreme class imbalance may cause minority class prototypes to be dominated by majority clients during aggregation.

## Foundational Learning

- **Concept: Semi-supervised learning with pseudo-labeling**
  - Why needed here: The framework trains on both labeled and unlabeled data, requiring understanding of how pseudo-labels are generated and why they can be noisy
  - Quick check question: Given an unlabeled sample with prediction probabilities [0.45, 0.35, 0.20], what pseudo-label would be assigned, and would TLAW weight it high or low?

- **Concept: Contrastive learning fundamentals**
  - Why needed here: The dual contrastive loss is central to the method, requiring understanding of positive/negative pair construction and temperature's role in softmax-based contrastive objectives
  - Quick check question: In SimCLR-style contrastive learning, what constitutes a positive pair, and how does the LCL's pseudo-label-based positive/negative selection differ?

- **Concept: Federated learning aggregation strategies**
  - Why needed here: The method deviates from FedAvg's parameter averaging, requiring understanding of why prototype aggregation might outperform parameter averaging under non-IID data
  - Quick check question: If Client A has 1000 samples of Class 1 and Client B has 100 samples of Class 1, how would weighted prototype aggregation vs. FedAvg handle their contributions differently?

## Architecture Onboarding

- **Component map:**
  Data → [Augmentation: weak α(·), strong A(·)] → [CNN+Transformer backbone f(ϑ)] → Embeddings c → [Projection head] → Predictions p → [Prototype computation P^k_j = mean(c|class j)]
  Loss computation: L_s (supervised CE) ← labeled data; L_u (weighted CE) ← unlabeled data + TLAW weights λ(p); L_LC (local contrastive) ← augmented views + pseudo-label pairs; L_GC (global contrastive) ← local vs. global prototypes
  Server-side: Collect {P^k} from clients → [Weighted averaging Eq.16] → [Momentum update Eq.18] → Broadcast {P̃}

- **Critical path:**
  1. Initialize global prototypes (random or from first-round local prototypes)
  2. Per round: clients compute TLAW statistics → generate pseudo-labels → compute all losses → update local models → recompute prototypes
  3. Server aggregates prototypes with momentum
  4. Fine-tuning stage: train classifier only on labeled data (backbone frozen or low learning rate)

- **Design tradeoffs:**
  - Prototype dimension vs. communication cost: Higher-dimensional embeddings capture more information but increase transmission size (128-dim transformer output → 64-dim projection)
  - Momentum κ: Higher κ increases stability but may slow adaptation to new patterns (paper doesn't specify value)
  - TLAW λ_max: Controls maximum pseudo-label influence (paper uses default values; sensitivity not extensively analyzed)
  - Temperature scaling factor α: Controls dynamic temperature sensitivity to confidence variance (paper doesn't specify—requires tuning)

- **Failure signatures:**
  - Prototype collapse: All global prototypes converge to similar values → GCL provides no signal (check pairwise cosine similarity of global prototypes after each round)
  - Pseudo-label confirmation bias: Accuracy plateaus or degrades after early rounds → TLAW not filtering effectively (monitor pseudo-label accuracy on held-out validation set)
  - Client drift: Local models diverge significantly → GCL insufficient (compare local prototype distances to global prototypes per client)
  - Communication bottleneck: Prototype aggregation fails when clients drop (Table VII shows 3/5 stragglers → 3.14% accuracy drop: 96.71% → 93.57%)

- **First 3 experiments:**
  1. Sanity check with centralized data: Train the full model on pooled data (no federation) with 10% labels. Verify TLAW + dual contrastive improves over supervised baseline. Expected: ~85-90% accuracy on CWRU (paper reports 91.50% with federation).
  2. Ablation on single mechanism: Start with FedAvg-Supervised → add PTA only → add LCL → add GCL → add TLAW. Replicate Table V progression to validate each component on your hardware/dataset.
  3. Non-IID severity stress test: Vary Dirichlet concentration parameter ν (paper uses 0.5). Test ν ∈ {0.1, 0.3, 0.5, 1.0} to characterize performance degradation under increasing heterogeneity. Expected: larger performance gap between SSFL-DCSL and FedAvg baselines at lower ν.

## Open Questions the Paper Calls Out
None

## Limitations
- The truncated Laplace weighting mechanism lacks direct validation against established pseudo-label filtering methods, with empirical superiority over simpler methods remaining unclear
- Prototype aggregation assumes embedding space is rich enough for class separation, but the paper doesn't analyze embedding dimensionality sensitivity or compare against parameter-based federation in detail
- Non-IID experiments focus on Dirichlet allocation with ν=0.5, but don't explore extreme heterogeneity (ν<0.1) where prototype aggregation might fail

## Confidence
- **High Confidence:** Local contrastive learning effectiveness, communication overhead reduction, and basic semi-supervised framework design
- **Medium Confidence:** TLAW weighting mechanism, prototype aggregation stability, and dual contrastive synergy
- **Low Confidence:** Generalization to extremely unbalanced or highly heterogeneous client distributions, and scalability to higher-dimensional embedding spaces

## Next Checks
1. Compare TLAW against confidence thresholding baseline on the same three datasets to quantify the claimed 1.15-7.85% accuracy improvements
2. Test prototype aggregation robustness by simulating 50% client dropout scenarios and measuring accuracy degradation curves
3. Evaluate model performance on a held-out validation set after each federated round to detect potential confirmation bias from incorrect pseudo-labels