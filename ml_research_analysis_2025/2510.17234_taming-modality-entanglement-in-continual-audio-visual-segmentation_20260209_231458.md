---
ver: rpa2
title: Taming Modality Entanglement in Continual Audio-Visual Segmentation
arxiv_id: '2510.17234'
source_url: https://arxiv.org/abs/2510.17234
tags:
- learning
- segmentation
- samples
- classes
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses continual audio-visual segmentation (CAVS),
  a novel task requiring continuous learning of new object classes while maintaining
  performance on previously learned ones, guided by audio cues. Two key challenges
  are identified: multi-modal semantic drift (learned objects mislabeled as background)
  and co-occurrence confusion (confusion between frequently co-occurring classes).'
---

# Taming Modality Entanglement in Continual Audio-Visual Segmentation

## Quick Facts
- **arXiv ID**: 2510.17234
- **Source URL**: https://arxiv.org/abs/2510.17234
- **Reference count**: 40
- **Primary result**: CMR achieves state-of-the-art mIoU on AVSBench datasets, outperforming baselines by 2.3-8.7%

## Executive Summary
This paper addresses continual audio-visual segmentation (CAVS), a novel task requiring continuous learning of new object classes while maintaining performance on previously learned ones, guided by audio cues. Two key challenges are identified: multi-modal semantic drift (learned objects mislabeled as background) and co-occurrence confusion (confusion between frequently co-occurring classes). To tackle these issues, the authors propose a Collision-based Multi-modal Rehearsal (CMR) framework. CMR consists of two main components: Multi-modal Sample Selection (MSS), which selects high-consistency audio-visual samples for rehearsal, and Collision-based Sample Rehearsal (CSR), which dynamically adjusts rehearsal frequency based on collision frequency between old model predictions and current ground truth. Experiments on three AVSBench datasets demonstrate that CMR significantly outperforms single-modal continual learning methods, achieving state-of-the-art performance.

## Method Summary
The paper proposes a Collision-based Multi-modal Rehearsal (CMR) framework for continual audio-visual segmentation. CMR includes two strategies: Multi-modal Sample Selection (MSS) and Collision-based Sample Rehearsal (CSR). MSS selects rehearsal samples with high modality consistency by computing the mIoU difference between visual-only and audio-visual models, prioritizing samples with smaller deviations. CSR dynamically adjusts rehearsal frequency based on collision frequency between old model predictions and current ground truth, increasing rehearsal for confusable classes. The framework uses a memory buffer of 5 samples per class, with 20% resampling based on collision frequency. The model employs ResNet-50 backbone with ASPP fusion, trained for 30 epochs per task on AVSBench datasets with mIoU as the evaluation metric.

## Key Results
- CMR achieves state-of-the-art performance on AVSBench datasets, with mIoU scores significantly higher than baseline methods like FT, LwF, ILT, MIB, PLOP, and EIR
- On AVSBench-CI 60-10 overlapped setting, CMR outperforms PLOP by 2.3-8.7% mIoU across different task increments
- Ablation studies confirm both MSS and CSR components contribute to performance improvements, with MSS showing more consistent gains

## Why This Works (Mechanism)

### Mechanism 1: Multi-modal Sample Selection (MSS)
- Claim: Selecting rehearsal samples with high modality consistency reduces multi-modal semantic drift during continual learning
- Mechanism: The method quantifies the audio modality's contribution by computing the mIoU difference between a visual-only model and an audio-visual model. Samples with smaller absolute deviation (|∆(S a)|) are prioritized for the memory buffer, reinforcing correct cross-modal associations
- Core assumption: Samples where audio and visual predictions align closely with ground truth exhibit less multi-modal semantic drift and are thus better candidates for rehearsal
- Break condition: If visual-only and audio-visual models have comparable performance across most samples, ∆(S a) will not distinguish high-consistency samples, rendering selection ineffective

### Mechanism 2: Collision-based Sample Rehearsal (CSR)
- Claim: Dynamically adjusting rehearsal frequency based on collision frequency mitigates co-occurrence confusion
- Mechanism: Collisions (mismatches between old model predictions and current ground truth) are counted per class. The normalized collision frequency (F') guides resampling of the memory buffer so that classes prone to confusion are rehearsed more often, helping the model disentangle incorrect audio-visual associations
- Core assumption: Classes with high collision counts are those most affected by co-occurrence-induced modality entanglement, and increased rehearsal corrects these associations
- Break condition: If collisions are dominated by noise (e.g., annotation errors) rather than genuine co-occurrence confusion, resampling may amplify incorrect biases

### Mechanism 3: Memory Buffer Construction with Modality-Consistent Samples
- Claim: A memory buffer populated with modality-consistent samples and dynamically resampled using collision frequencies improves continual audio-visual segmentation performance
- Mechanism: MSS selects top-k samples per class with minimal |∆(S a)|. CSR resamples 20% of this buffer based on collision frequency, creating an updated buffer (M̂ t-1) used during training alongside new task data
- Core assumption: A small, well-curated memory buffer can effectively preserve past knowledge without requiring full data rehearsal
- Break condition: If the memory buffer is too small to capture class diversity, or if collision-driven resampling oversamples rare but confusing classes, overall performance may degrade

## Foundational Learning
- **Concept: Continual Learning (CL) / Class-Incremental Learning**
  - Why needed here: CAVS extends continual learning to audio-visual segmentation, requiring sequential task learning without catastrophic forgetting
  - Quick check question: Can you explain why storing a small memory buffer is a common strategy to mitigate forgetting?
- **Concept: Audio-Visual Segmentation (AVS)**
  - Why needed here: The base task requires pixel-level segmentation of sounding objects guided by audio signals, which is extended to a continual setting
  - Quick check question: How does audio-visual segmentation differ from standard semantic segmentation?
- **Concept: Modality Entanglement**
  - Why needed here: The paper identifies modality entanglement (multi-modal semantic drift and co-occurrence confusion) as a key challenge unique to multi-modal continual learning
  - Quick check question: What is modality entanglement, and why does it complicate continual learning in multi-modal settings?

## Architecture Onboarding
- **Component map**: ResNet-50 encoders -> ASPP fusion -> Decoder; MSS module (visual-only + audio-visual models) -> CSR module (collision counting + resampling) -> Memory buffer
- **Critical path**: 1) Train visual-only and audio-visual models on current task D t to compute ∆(S a) for MSS 2) Select top-k samples per class with minimal |∆(S a)| to populate M t 3) At next task D t+1, infer with old model on D t+1 to count collisions (CSR) 4) Resample M t based on collision frequency to create M̂ t 5) Train on D t+1 ∪ M̂ t using PLOP-based losses
- **Design tradeoffs**: Memory buffer size (samples per class) vs. memory budget and rehearsal diversity; Resampling ratio (20% default) vs. emphasis on confusing classes; Single-target vs. multi-target sample selection (MSS favors single-target)
- **Failure signatures**: Low mIoU on old classes: Inadequate memory buffer or ineffective CSR; High confusion between specific class pairs: Co-occurrence confusion not sufficiently addressed; Performance degradation on new classes: Over-rehearsal of old samples, inhibiting plasticity
- **First 3 experiments**: 1) Reproduce baseline comparison on AVSBench-CI 60-10 overlapped setting to validate reported mIoU improvements over PLOP, MiB, etc. 2) Ablate MSS vs. random sample selection to confirm the importance of modality consistency 3) Ablate CSR (collision-based resampling) to verify its contribution over uniform rehearsal

## Open Questions the Paper Calls Out
- **Open Question 1**: How can multi-target audio-visual samples be effectively preprocessed or decomposed into single-target instances to enhance rehearsal quality in complex multi-object scenarios?
- **Open Question 2**: Can effective multi-modal sample selection be achieved without the computational overhead of training separate uni-modal and multi-modal models?
- **Open Question 3**: How can the identified issues of multi-modal semantic drift and co-occurrence confusion be addressed in rehearsal-free continual learning paradigms?

## Limitations
- Performance gains on multi-object datasets (AVSBench-CIM) are significantly smaller (+1.5 mIoU) compared to single-object datasets (+11.3 mIoU), suggesting limited effectiveness for complex scenes
- The method requires training separate visual-only and audio-visual models for sample selection, doubling computational costs during the selection phase
- The framework relies on rehearsal-based continual learning, which may face privacy and memory constraints in real-world applications

## Confidence
- **High**: MSS effectiveness in selecting high-consistency samples; CSR framework design
- **Medium**: Absolute performance gains across all settings; collision frequency normalization implementation
- **Low**: Generalization to non-overlapping class scenarios; memory buffer size optimization

## Next Checks
1. Verify collision frequency computation by manually checking collision counts on a small validation set and confirming the sigmoid normalization produces the expected distribution
2. Test CSR robustness by varying the resampling ratio (5-30%) to determine optimal balance between rehearsal stability and plasticity
3. Evaluate MSS sample quality by visualizing audio-visual predictions on selected vs. rejected samples to confirm reduced modality drift