---
ver: rpa2
title: Knowledge-based learning in Text-RAG and Image-RAG
arxiv_id: '2601.08226'
source_url: https://arxiv.org/abs/2601.08226
tags:
- epochs
- hallucination
- baseline
- image
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated retrieval-augmented generation (RAG) strategies\u2014\
  text-based and image-based\u2014for reducing hallucinations and improving disease\
  \ detection in chest X-ray images. Using the NIH Chest X-ray dataset, the research\
  \ compared performance across baseline, text-based RAG, and image-based RAG models,\
  \ implemented with both GPT and Llama LLMs."
---

# Knowledge-based learning in Text-RAG and Image-RAG

## Quick Facts
- **arXiv ID:** 2601.08226
- **Source URL:** https://arxiv.org/abs/2601.08226
- **Reference count:** 4
- **Primary result:** Image-based RAG achieved highest accuracy and stability in chest X-ray disease detection while text-based RAG reduced hallucinations through external medical knowledge

## Executive Summary
This study evaluated retrieval-augmented generation (RAG) strategies—text-based and image-based—for reducing hallucinations and improving disease detection in chest X-ray images. Using the NIH Chest X-ray dataset, the research compared performance across baseline, text-based RAG, and image-based RAG models, implemented with both GPT and Llama LLMs. Text-based RAG effectively reduced hallucinations by incorporating external medical knowledge, while image-based RAG improved prediction confidence and calibration via KNN-based similarity retrieval. The GPT LLM consistently outperformed Llama, showing lower hallucination rates and better Expected Calibration Error (ECE). Image-based RAG achieved the highest accuracy and stability, though the study was limited by class imbalance and hardware constraints.

## Method Summary
The method employs a Vision Transformer (EVA-ViT) to encode chest X-ray images into tokens, which are then processed by an LLM (GPT-2 mini or Llama-3-8B) with RAG augmentation. Three variants were tested: baseline (no RAG), text-based RAG (retrieves Wikipedia summaries using disease keywords), and image-based RAG (retrieves similar X-rays using KNN with FAISS index). The model addresses class imbalance through WeightedRandomSampler and weighted cross-entropy loss. Training used Adam optimizer for 20 epochs on the NIH Chest X-ray dataset, filtered to 84,053 single-label samples across 6 classes.

## Key Results
- Text-based RAG reduced hallucination rates compared to baseline by grounding outputs in external medical knowledge
- Image-based RAG improved prediction confidence and calibration, achieving highest accuracy and stability
- GPT consistently outperformed Llama on hallucination reduction and ECE metrics
- Class weighting improved macro-F1 scores for rare classes while reducing overall accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Text-based RAG reduces hallucinations by grounding model outputs in external medical knowledge.
- **Mechanism:** The system retrieves relevant medical corpus content (via Wikipedia API summaries) based on feature keywords, then injects this context into the model input. This external domain knowledge constrains reasoning to verified facts rather than allowing unconstrained generation.
- **Core assumption:** Retrieved text is accurate, relevant to the specific case, and not contradictory to the input image.
- **Evidence anchors:**
  - [abstract] "the text-based RAG effectively reduces the hallucination problem by using external knowledge information"
  - [section 4] "it uses the external medical corpus by referencing the Wikipedia API summaries...reduces the hallucination and improves the reasoning quality"
  - [corpus] Related work confirms RAG as a primary hallucination mitigation strategy (Mitigating Hallucination in LLMs survey, fmr=0.55)
- **Break condition:** If retrieved passages are irrelevant, outdated, or contradict visual evidence, hallucination rates may spike (observed in epochs 8-14 with 40-60 hallucination spikes for text-RAG).

### Mechanism 2
- **Claim:** Image-based RAG improves prediction confidence and calibration through visual similarity anchoring.
- **Mechanism:** KNN-based retrieval (k=3) using FAISS index finds visually similar X-ray images from the training corpus. These retrieved examples serve as visual reference anchors, helping the model calibrate predictions against known cases rather than reasoning in isolation.
- **Core assumption:** Visual similarity in embedding space correlates with diagnostic similarity; the feature encoder captures diagnostically relevant patterns.
- **Evidence anchors:**
  - [abstract] "the image-based RAG improved the prediction confidence and calibration by using the KNN methods"
  - [section 4] "It finds out similar x-ray images with the KNN technique...reduces the false positives and improves the visual consistency"
  - [corpus] GroundSight paper (fmr=0.70) demonstrates grounding information reduces VQA hallucinations via object localization—conceptually similar visual grounding principle
- **Break condition:** If the embedding space does not align with disease semantics (e.g., visually similar but diagnostically different cases), retrieval may introduce noise rather than clarity.

### Mechanism 3
- **Claim:** WeightedRandomSampler combined with weighted cross-entropy loss mitigates class imbalance effects on rare disease learning.
- **Mechanism:** Instead of oversampling (which causes overfitting to duplicated rare cases) or undersampling (which loses normal sample variability), WeightedRandomSampler adjusts selection probability per class. Weighted cross-entropy further increases gradient contribution from rare classes while reducing dominant class influence.
- **Core assumption:** Rare class examples are representative; increasing their gradient contribution leads to meaningful feature learning rather than memorization.
- **Evidence anchors:**
  - [section 3] "By using WeightedRandomSampler, we could increase the selection probability for rare classes...gives a balanced gradient update with better training results"
  - [section 3.1] "increasing class weights consistently improved macro-F1 scores while reducing accuracy, highlighting the trade-off"
  - [corpus] No direct corpus evidence on this specific sampling technique for medical imaging
- **Break condition:** If rare classes have insufficient sample diversity, increased weighting may cause overfitting rather than generalization; the trade-off between macro-F1 and accuracy signals this tension.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architectural choice; understanding how retrieval grounds generation is essential for debugging hallucination and confidence issues.
  - Quick check question: Given a chest X-ray with ambiguous features, would you expect text-RAG or image-RAG to provide more reliable grounding—and why?

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: Primary evaluation metric beyond accuracy; measures whether model confidence aligns with actual correctness.
  - Quick check question: A model predicts "Effusion" with 90% confidence but is correct only 60% of the time. Is this well-calibrated? What does high ECE indicate?

- **Concept: Vision Transformer Tokenization**
  - Why needed here: EVA-ViT converts images to tokens that LLMs can process; understanding this bridge is critical for debugging multi-modal failures.
  - Quick check question: How does patch-based tokenization differ from CNN feature extraction, and what might cause token-image alignment failures?

## Architecture Onboarding

- **Component map:**
  Input X-ray → Pre-processing (single-label filter, resize) → EVA-ViT Encoder (image → tokens) → RAG Branch (parallel): Text-RAG: Keyword → Wikipedia API → context text; Image-RAG: Embedding → FAISS KNN(k=3) → similar images → LLM (GPT-mini or LLaMA-2) with concatenated context → Classification head (6 disease classes) → Weighted Cross-Entropy Loss → Adam Optimizer

- **Critical path:** Image embedding quality → retrieval relevance → context grounding → LLM prediction stability. Failures cascade; poor embeddings produce irrelevant retrieval, which increases hallucination risk.

- **Design tradeoffs:**
  - Text-RAG: Better hallucination reduction but higher sensitivity to irrelevant retrieval (large spikes observed)
  - Image-RAG: More stable but limited to intra-domain knowledge; cannot introduce external medical facts
  - GPT vs LLaMA: GPT showed lower hallucination and better ECE, but paper used GPT-mini due to resource constraints—full GPT behavior may differ
  - Macro-F1 vs Accuracy: Class weighting improves rare-class F1 but reduces overall accuracy

- **Failure signatures:**
  - Sudden hallucination spikes (40-60 count) → likely text retrieval returned irrelevant/contradictory passages
  - Accuracy oscillation without convergence (0.40-0.60 range) → baseline lacking grounding; check retrieval pipeline
  - Near-zero F1 on specific classes → class imbalance not adequately addressed; review WeightedRandomSampler probabilities

- **First 3 experiments:**
  1. **Ablate retrieval source:** Run image-RAG with k=1, 3, 5 to characterize sensitivity to neighbor count; expect stability to degrade if k too high (noisy neighbors) or too low (insufficient context).
  2. **Calibration diagnostic:** Plot reliability diagrams for all three model variants; quantify whether high confidence correlates with correctness or if systematic overconfidence exists.
  3. **Retrieval quality audit:** Manually inspect 50 random text-RAG retrievals; label relevance/accuracy to establish upper bound on text-RAG effectiveness and identify failure modes in the Wikipedia API pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would deploying full-scale LLMs (e.g., GPT-4) instead of resource-constrained versions (GPT-2 mini) on a balanced dataset eliminate the hallucination spikes observed in text-based RAG?
- **Basis in paper:** [explicit] The authors state that "a more balanced dataset and majorly improved hardware would allow us to use a larger version of GPT... potentially improving the results."
- **Why unresolved:** Hardware constraints and financial limitations forced the use of smaller models ("GPT mini"), leaving the performance ceiling of larger models untested.
- **What evidence would resolve it:** A comparison of hallucination rates and ECE between the "mini" models and full-scale models on the same NIH dataset.

### Open Question 2
- **Question:** What specific retrieval filtering mechanisms are required to prevent the "significant spikes" in hallucinations when external text contradicts visual input in Text-RAG?
- **Basis in paper:** [inferred] The discussion notes that text-based RAG experienced massive hallucination spikes (40-60 counts) because "retrieved text could likely have contradicted the image observed."
- **Why unresolved:** The study utilized a Wikipedia API without detailing a mechanism to verify consistency between retrieved text and image features.
- **What evidence would resolve it:** Ablation studies testing retrieval filters that cross-check text relevance against visual embeddings before generation.

### Open Question 3
- **Question:** Does a combined "Hybrid-RAG" approach, integrating both KNN-based image retrieval and external medical text, outperform single-modality RAG in diagnostic accuracy?
- **Basis in paper:** [inferred] The paper evaluates Text-RAG and Image-RAG in isolation, noting Image-RAG aids calibration while Text-RAG aids reasoning, but never combines them.
- **Why unresolved:** The methodology and experiments were strictly segmented into three conditions (Baseline, Text-RAG, Image-RAG) without a fourth hybrid condition.
- **What evidence would resolve it:** Implementation of a unified model retrieving both similar images and relevant text passages simultaneously.

## Limitations

- Hallucination detection lacks explicit definition or standardized scoring methodology
- Limited evaluation to single dataset (NIH Chest X-ray) without external validation
- Hardware constraints prevented testing full GPT models, only using GPT-mini

## Confidence

**High confidence:**
- Text-based RAG reduces hallucinations when retrieval is relevant
- WeightedRandomSampler combined with weighted loss mitigates class imbalance effects
- GPT outperforms Llama on hallucination and calibration metrics

**Medium confidence:**
- Image-based RAG improves prediction confidence and calibration through visual similarity anchoring
- The 6-class formulation (removing Pneumonia) is necessary for stable training
- KNN with k=3 is optimal for retrieval quality

**Low confidence:**
- Specific mechanisms driving hallucination spikes in text-RAG (epochs 8-14)
- Generalization of results beyond NIH Chest X-ray dataset
- Long-term stability of calibration improvements across model updates

## Next Checks

1. **Retrieval quality audit:** Manually inspect 100 random text-RAG retrievals, labeling relevance and accuracy. Compare hallucination rates when retrievals are highly relevant vs irrelevant to establish causal relationship between retrieval quality and hallucination reduction.

2. **Cross-dataset validation:** Evaluate the best-performing model (image-based RAG with GPT) on an independent chest X-ray dataset (e.g., CheXpert or MIMIC-CXR). Report accuracy, ECE, and hallucination rates to assess generalization beyond NIH data.

3. **Ablation of sampling strategy:** Run controlled experiments comparing WeightedRandomSampler with standard oversampling and undersampling approaches on the same training data. Measure macro-F1, accuracy, and calibration to quantify the specific contribution of the sampling method versus weighting alone.