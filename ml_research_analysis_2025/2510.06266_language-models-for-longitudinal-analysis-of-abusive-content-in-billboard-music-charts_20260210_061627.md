---
ver: rpa2
title: Language models for longitudinal analysis of abusive content in Billboard Music
  Charts
arxiv_id: '2510.06266'
source_url: https://arxiv.org/abs/2510.06266
tags:
- content
- lyrics
- songs
- music
- explicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed abusive content in Billboard music charts using
  deep learning and language models over seven decades. The researchers extracted
  and processed lyrics from 10,600 unique songs released from 1990-2024, focusing
  on English-language tracks.
---

# Language models for longitudinal analysis of abusive content in Billboard Music Charts

## Quick Facts
- arXiv ID: 2510.06266
- Source URL: https://arxiv.org/abs/2510.06266
- Reference count: 40
- Primary result: BERT and RoBERTa models fine-tuned on sentiment and abuse datasets detect explicit content in Billboard lyrics with GloVe embeddings achieving 98.52% accuracy

## Executive Summary
This study analyzes abusive content in Billboard music charts using deep learning and language models over seven decades. The researchers extracted and processed lyrics from 10,600 unique songs released from 1990-2024, focusing on English-language tracks. They employed BERT and RoBERTa models fine-tuned with sentiment analysis and abuse detection datasets to identify explicit content. N-gram analysis revealed increasing explicit language, particularly from 2017-2024, with themes shifting from love to more profane content. Sentiment analysis showed hip-hop had the highest proportion of negative sentiments, while country had the lowest. The models achieved high accuracy in detecting explicit content, with GloVe embeddings performing best. The study demonstrates the effectiveness of language models in capturing nuanced patterns in lyrical content, reflecting societal changes over time, and provides a framework for automated content filtering systems to help safeguard young audiences from inappropriate material.

## Method Summary
The researchers analyzed Billboard Hot 100 lyrics from 1990-2024 by extracting 342K chart entries and deduplicating to 10,600 unique English songs. They collected lyrics via Genius API, explicit labels from Spotify API, and genres from multiple sources (Spotify, Deezer, Last.FM) with ChatGPT fallback. After preprocessing to remove non-English content and cleaning structural tags, they applied BERT and RoBERTa models fine-tuned on SenWave sentiment data and HateBERT abuse detection. GloVe embeddings achieved highest explicit content detection accuracy (98.52%). They conducted n-gram analysis to track lexical changes and custom sentiment scoring to quantify emotional polarity shifts across genres and decades.

## Key Results
- GloVe embeddings achieved 98.52% accuracy in explicit content detection, outperforming BERT (91.4%) and RoBERTa models
- N-gram analysis showed increasing explicit language from 2017-2024, with themes shifting from love to profane content
- Hip-hop exhibited the highest proportion of negative sentiments while country showed the lowest
- N-grams like "f*ck, f*ck, f*ck" and "trap, trap, trap" became prominent in recent years alongside persistent love themes

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning with Domain-Specific Fine-Tuning
- Claim: Pre-trained language models can be adapted to music content analysis through fine-tuning on domain-relevant datasets for sentiment and abuse detection.
- Mechanism: Base LLMs (BERT/RoBERTa) capture general language patterns; fine-tuning on SenWave (multi-label sentiment from COVID-19 tweets) and HateBERT's Reddit-based abuse corpus adapts the models to lyrical content classification.
- Core assumption: Linguistic patterns in song lyrics share sufficient semantic overlap with social media text and Reddit comments used in fine-tuning datasets.
- Evidence anchors: [abstract] "They employed BERT and RoBERTa models fine-tuned with sentiment analysis and abuse detection datasets to identify explicit content." [section 3.4] "HateBERT is a fine-tuned variation of the BERT base-uncased model for abuse detection in text data, fine-tuned using RAL-E (Reddit Abusive Language English)... suitable for our classification task of explicit song identification."

### Mechanism 2: Static Embeddings for Explicit Content Classification
- Claim: Pre-trained static word embeddings can outperform contextual embeddings for explicit content detection in song lyrics.
- Mechanism: GloVe embeddings encode global word co-occurrence statistics; the model maps profane/explicit terms to consistent vector representations, enabling robust classification even with limited context windows.
- Core assumption: Explicit and abusive words exhibit relatively consistent semantic properties across lyrical contexts, reducing the need for contextual disambiguation.
- Evidence anchors: [abstract] "The models achieved high accuracy in detecting explicit content, with GloVe embeddings performing best." [section 4.2] "Our findings indicate that the model utilising GloVe embeddings achieved the highest accuracy in this classification task with an accuracy of 98.52%... The variation in accuracy among different model emphasises the importance of embedding selection."

### Mechanism 3: Multi-Signal Temporal Pattern Detection
- Claim: Combining lexical frequency analysis (n-grams) with neural model predictions enables detection of longitudinal content evolution across decades.
- Mechanism: N-gram analysis captures surface-level vocabulary shifts; sentiment/abuse models capture semantic-level changes. Together they reveal both what words are used and their emotional/harmful content trajectories.
- Core assumption: Explicit content manifests both lexically (word frequency changes) and semantically (sentiment polarity shifts), requiring multi-signal detection.
- Evidence anchors: [abstract] "N-gram analysis revealed increasing explicit language, particularly from 2017-2024, with themes shifting from love to more profane content." [section 4.1] "There is a significant shift towards more explicit content in the latest period (2017-2024). Trigrams such as 'f*ck, f*ck, f*ck' and 'trap, trap, trap' become prominent... themes of love remain present."

## Foundational Learning

- Concept: Transfer Learning in NLP
  - Why needed here: Understanding how pre-trained models adapt to specific domains through fine-tuning is essential for reproducing the paper's approach and debugging poor performance.
  - Quick check question: Why might a model pre-trained on Wikipedia struggle with hip-hop slang without domain-specific fine-tuning?

- Concept: Static vs. Contextual Word Embeddings
  - Why needed here: The counter-intuitive finding that GloVe outperforms BERT requires understanding the trade-offs between embedding types.
  - Quick check question: What task characteristics might favor static embeddings (GloVe) over contextual embeddings (BERT)?

- Concept: Multi-label vs. Binary Classification
  - Why needed here: Sentiment analysis uses multi-label classification (songs can express multiple emotions) while abuse detection uses binary classification.
  - Quick check question: Why might the same song simultaneously express "optimistic" and "anxious" sentiments, and how does the SenWave dataset handle this?

## Architecture Onboarding

- Component map:
  - Data Ingestion: Kaggle Billboard charts (342K entries) → Deduplication (31K unique) → Spotify API (explicit labels) → Genius API (lyrics)
  - Genre Enrichment Pipeline: Spotify → Deezer → Last.FM → ChatGPT fallback for missing genres
  - Preprocessing: Language detection (langdetect), regex cleaning (remove "[Chorus]" tags), non-English filtering
  - Model Layer: GloVe+LSTM, BERT-base-uncased, RoBERTa, HateBERT (for abuse), SenWave-fine-tuned BERT (for sentiment)
  - Analysis Layer: N-gram extraction, custom polarity scoring (weighted emotion averaging), longitudinal aggregation

- Critical path:
  1. Standardize artist/title formatting (handle "Featuring," "x," brackets in titles)
  2. Filter non-English lyrics via chunked langdetect (5000 char limit per chunk)
  3. Map sub-genres to major categories ("electro"→"electronic", "hip-hop"→"rap")
  4. Fine-tune BERT/RoBERTa on SenWave (sentiment) and apply pre-trained HateBERT (abuse)
  5. Chunk long lyrics by section (verse/chorus) respecting token limits
  6. Aggregate chunk-level predictions; compute custom polarity scores using weighted emotions

- Design tradeoffs:
  - GloVe (98.52% accuracy, faster inference, limited context) vs. BERT (91.4% accuracy, more compute, richer context)
  - Spotify labels (convenient, potentially noisy) vs. human annotation (accurate, expensive)
  - Verse-level chunking (granular sentiment tracking) vs. full-song analysis (contextual coherence)

- Failure signatures:
  - "Joking" sentiment overprediction (>90% of dataset)—rhyming/lyrical structure mimics humor patterns from SenWave tweets
  - Zero abusive word count for dark thematic content (e.g., "Who Is It"—betrayal/depression without profanity)
  - Slang/euphemism misses: "do him dirty" (violence), "blrrrd" (ad-lib), "shiesty" (untrustworthy), "glock" (gun)
  - Genre bias: Hip-hop over-flagged due to higher explicit word rates; country under-flagged despite mature themes

- First 3 experiments:
  1. Embedding comparison: Train GloVe+LSTM, BERT-base, and RoBERTa on identical train/test splits (Table 1); measure accuracy, F1-micro, F1-macro on explicit detection task.
  2. Chunking ablation: Compare verse-level vs. full-song vs. sliding-window analysis on sentiment consistency; measure variance in polarity scores across chunks within same song.
  3. Genre-stratified error analysis: For each primary genre (pop, hip-hop, country, rock, R&B), compute false positive/negative rates against Spotify labels; identify systematic bias patterns in slang handling.

## Open Questions the Paper Calls Out

- Would incorporating multiple labeling sources beyond Spotify reduce annotation bias in explicit content detection?
- Can models be trained to detect euphemisms and genre-specific slang (e.g., "do him dirty," "shiesty") without sacrificing generalization?
- What is the causal impact of exposure to explicit lyrical content on youth behavior and mental health outcomes?
- Would training sentiment models on music-specific corpora rather than social media data reduce genre-related prediction bias?

## Limitations

- Domain Transfer Validity: The study relies on fine-tuning models pre-trained on social media data (tweets, Reddit) for music lyric analysis, representing a significant domain shift that may introduce systematic biases.
- Static Embedding Performance: The finding that GloVe embeddings outperform contextual embeddings lacks strong supporting evidence and theoretical explanation.
- Temporal Granularity: The analysis aggregates data by decade and broad time periods, potentially masking important year-to-year variations in lyrical content.

## Confidence

- High Confidence: The general finding of increased explicit content over time, particularly from 2017-2024, is well-supported by n-gram analysis and aligns with broader cultural observations about music trends.
- Medium Confidence: The effectiveness of language models for longitudinal analysis is demonstrated, but the specific superiority of GloVe embeddings and the transferability of social media-trained models to music lyrics require further validation.
- Low Confidence: The claim that hip-hop has the "highest proportion of negative sentiments" while country has the "lowest" may be influenced by genre-specific slang handling and explicit word detection biases rather than true sentiment differences.

## Next Checks

1. Embedding Ablation Study: Systematically compare GloVe, BERT, and RoBERTa embeddings on the same explicit content detection task using identical train/test splits and hyperparameters to validate the claimed performance differences.

2. Temporal Resolution Analysis: Re-run the longitudinal analysis using yearly (or even quarterly) data points to determine if the 2017-2024 trend is consistent at finer temporal resolutions and identify any earlier inflection points.

3. Genre-Specific Error Analysis: For each major genre, compute false positive and false negative rates against Spotify labels to quantify systematic biases in slang handling and explicit content detection, particularly for genre-specific euphemisms and ad-libs.