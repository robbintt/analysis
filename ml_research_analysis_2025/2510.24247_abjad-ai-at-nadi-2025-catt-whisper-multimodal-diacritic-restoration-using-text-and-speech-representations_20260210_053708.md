---
ver: rpa2
title: 'Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using
  Text and Speech Representations'
arxiv_id: '2510.24247'
source_url: https://arxiv.org/abs/2510.24247
tags:
- speech
- text
- arabic
- fusion
- catt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents CATT-Whisper, a multimodal model for Arabic\
  \ diacritic restoration that combines a pre-trained text encoder (CATT) with a speech\
  \ encoder (Whisper). The approach integrates text and speech through two fusion\
  \ strategies\u2014early fusion and cross-attention\u2014with early fusion selected\
  \ for its efficiency."
---

# Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations

## Quick Facts
- arXiv ID: 2510.24247
- Source URL: https://arxiv.org/abs/2510.24247
- Reference count: 10
- Achieved WER 0.25 / CER 0.09 on NADI 2025 dev set, WER 0.55 / CER 0.13 on test set

## Executive Summary
This paper presents CATT-Whisper, a multimodal model for Arabic diacritic restoration that combines text and speech representations. The approach uses early fusion to integrate Whisper speech embeddings with CATT text embeddings, trained with a modality-robust strategy that allows performance with or without speech input. Experiments on the NADI 2025 dataset demonstrate state-of-the-art results, with speech providing crucial disambiguation cues for diacritic placement in dialectal Arabic.

## Method Summary
CATT-Whisper fuses a pre-trained CATT text encoder with a Whisper speech encoder using early token-level fusion. The method compresses 1500 Whisper frames to 150 tokens via averaging, projects these to match CATT's embedding dimension, and concatenates them with text tokens before encoding. Training employs a two-phase approach: first freezing the speech encoder for 5 epochs, then jointly fine-tuning both encoders for 5 epochs. Random speech dropout during training creates a modality-robust model that performs well with or without speech input.

## Key Results
- Dev set: WER 0.25, CER 0.09
- Test set: WER 0.55, CER 0.13
- Outperforms baseline and other NADI 2025 submissions
- Speech integration provides prosodic and phonetic cues for diacritic disambiguation

## Why This Works (Mechanism)

### Mechanism 1: Speech-Aided Disambiguation
Speech signals provide phonetic and prosodic cues that resolve diacritic ambiguities unresolvable from text alone. Whisper encoder extracts acoustic features → linear projection aligns dimensions → fused representation enables CATT to access pronunciation information during diacritic prediction. Core assumption: Acoustic signals carry reliable phonetic correlates to diacritic patterns in dialectal Arabic.

### Mechanism 2: Early Token-Level Fusion via Soft Prompting
Concatenating projected speech tokens with text tokens before encoding enables contextual integration without architectural changes to the text encoder. 1500 Whisper frames averaged to 150 tokens → linear projection to text embedding dimension → concatenation with text tokens → CATT processes unified sequence. Core assumption: Averaging 10 consecutive frames preserves sufficient phonetic detail while matching typical text sequence lengths.

### Mechanism 3: Modality-Robust Training
Randomly deactivating speech input during training creates a model that degrades gracefully when speech is unavailable. During training, speech is randomly masked → model learns both speech-conditioned and text-only predictions → inference works with or without speech. Core assumption: Text-only and speech-enhanced modes share sufficient representational structure for joint optimization.

## Foundational Learning

- **Transformer Cross-Modal Fusion**
  - Why needed: Understanding how to combine different modalities (text embeddings vs. speech spectrograms) at various architectural depths
  - Quick check: Can you explain why early fusion requires dimension matching but cross-attention does not?

- **Whisper Encoder Architecture**
  - Why needed: Whisper's encoder produces 1500-frame outputs from log-mel spectrograms; understanding this compression is critical for the averaging strategy
  - Quick check: What is the relationship between Whisper's 1500 output frames and typical speech segment duration?

- **Diacritic Restoration as Sequence Labeling**
  - Why needed: The task maps undiacritized tokens to diacritized tokens at character/word level—a token classification problem, not generation
  - Quick check: How does CATT's classification head differ from autoregressive generation?

## Architecture Onboarding

- **Component map:**
  Raw Audio → Whisper Encoder → Average Pooling (10 frames → 1 token) → Linear Projection
                                                                                ↓
  Undiacritized Text → Tokenizer → Text Embeddings ─────────────────→ Concatenation
                                                                                ↓
                                                                    CATT Encoder
                                                                                ↓
                                                                    Classification Head
                                                                                ↓
                                                                    Diacritized Output

- **Critical path:**
  1. Ensure Whisper encoder outputs are correctly extracted (not full model, just encoder)
  2. Verify projection layer matches CATT's hidden dimension exactly
  3. Confirm speech dropout is implemented at batch level, not token level

- **Design tradeoffs:**
  - Early fusion vs. cross-attention: Paper reports comparable accuracy; early fusion chosen for lower compute. Cross-attention may capture finer-grained text-speech alignments
  - Frame averaging: 10x compression is aggressive; may smooth over short phonemes but reduces sequence length bloat
  - Frozen-then-unfrozen training: 5 epochs frozen projection stabilizes alignment before joint fine-tuning

- **Failure signatures:**
  - WER spikes when speech is present but text-only training was insufficient
  - CER stays low while WER rises → word-level diacritic errors (case endings) rather than character errors
  - Test WER (0.55) much higher than dev (0.25) suggests domain mismatch or overfitting to dev distribution

- **First 3 experiments:**
  1. Baseline replication: Run CATT text-only on NADI 2025 dev set; verify WER ~0.46 before adding speech
  2. Ablation on speech dropout rate: Test 0%, 25%, 50%, 75% dropout; measure both speech-present and speech-absent performance
  3. Fusion comparison: Implement cross-attention variant; compare WER/CER and inference latency against early fusion on same dev split

## Open Questions the Paper Calls Out

- **Can replacing the Whisper encoder with phoneme-level encoders (e.g., CTC-based models) resolve the ambiguous sequences that the current model struggles with?**
  - Basis: Conclusion states "ambiguous sequences remain challenging, suggesting the need for stronger phoneme-level encoders"
  - Evidence needed: Comparative study benchmarking current CATT-Whisper against CATT-Conformer variant on specific challenging test cases

- **Does training on datasets significantly larger than the NADI 2025 corpus yield diminishing returns or continued improvements for multimodal diacritic restoration?**
  - Basis: Authors list "larger-scale training" as primary direction for future work
  - Evidence needed: Performance metrics evaluating model when fine-tuned on extended corpora of dialectal Arabic speech and text

- **Does the heuristic averaging of 1500 audio frames into 150 tokens result in loss of essential temporal cues required for accurate diacritization?**
  - Basis: Methodology mentions averaging 10 consecutive frames to match text token counts
  - Evidence needed: Ablation study comparing fixed averaging against adaptive pooling or attention-based compression methods

## Limitations

- Architecture-specific limitations: Lack of ablation studies on critical choices like 10x frame averaging, which may discard phonetic details essential for certain diacritic distinctions
- Generalizability concerns: Computational trade-offs of early fusion not fully quantified; cross-attention baseline performance numbers missing
- Data dependency issues: Random speech dropout rate not specified; modality-robust benefits not empirically validated

## Confidence

- **High confidence:** Reported WER/CER scores (0.25/0.09 dev, 0.55/0.13 test) are methodologically sound and represent state-of-the-art performance on NADI 2025 benchmark
- **Medium confidence:** Mechanism explanations for why speech helps (prosodic disambiguation) are plausible but lack direct error analysis evidence
- **Low confidence:** Modality-robust training benefits are asserted but not empirically validated; text-only performance comparison missing

## Next Checks

1. **Error pattern analysis:** Extract and categorize specific diacritic errors made by CATT-Whisper versus text-only baseline to determine which diacritic categories benefit most from speech integration

2. **Speech dropout sensitivity:** Systematically vary random speech dropout rate during training (0%, 25%, 50%, 75%) and measure both text-only and speech-enhanced performance to quantify modality-robust training benefits

3. **Cross-attention baseline implementation:** Implement and evaluate cross-attention fusion strategy as alternative to early fusion, comparing WER/CER and inference latency to determine if efficiency gains justify accuracy trade-offs