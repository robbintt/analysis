---
ver: rpa2
title: 'LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction'
arxiv_id: '2512.18623'
source_url: https://arxiv.org/abs/2512.18623
tags:
- arxiv
- dynamic
- wang
- policy
- llm-cas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-CAS addresses the hallucination problem in large language models
  by framing real-time correction as a hierarchical reinforcement learning task. Instead
  of static parameter editing, it trains an agent to dynamically select temporary
  neuron perturbations during inference based on the current context.
---

# LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction

## Quick Facts
- arXiv ID: 2512.18623
- Source URL: https://arxiv.org/abs/2512.18623
- Reference count: 15
- Primary result: LLM-CAS achieves 10.98 percentage points improvement on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on TruthfulQA's MC1 score

## Executive Summary
LLM-CAS introduces a dynamic neuron perturbation framework for real-time hallucination correction in large language models. Rather than static model editing, it employs hierarchical reinforcement learning to dynamically select temporary activation perturbations during inference based on current context. The system combines adaptive masking with neuron-level causal tracing to target only the activations causing errors, guided by a PPO-based policy that balances factuality, relevance, and fluency. Experiments demonstrate consistent improvements across multiple benchmarks and model architectures while avoiding catastrophic forgetting.

## Method Summary
LLM-CAS frames hallucination correction as a hierarchical reinforcement learning task where an agent dynamically selects temporary neuron perturbations during inference. The framework uses a two-stage adaptive masking mechanism: first learning a sparse general mask via L1/L0-regularized optimization, then adapting it with input-specific causal information from Integrated Gradients attribution. The agent's architecture is bifurcated into high-level and low-level tiers, each with its own actor-critic networks, to select functional neuron clusters and perturbation types/magnitudes respectively. Perturbations are applied only during inference without modifying model weights, preserving base capabilities while enabling context-specific correction. The system is trained using PPO with GAE advantage estimation and evaluated using an LLM-as-judge approach.

## Key Results
- Achieves 10.98 percentage points improvement on StoryCloze benchmark
- Demonstrates 2.71 points improvement on TriviaQA and 2.06 points on TruthfulQA MC1 score
- Shows strong generalization across different model architectures including LLaMA2-7B, Mistral-7B, and Gemma-1.1-7b-it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical policy decomposition enables efficient navigation of the high-dimensional neuron perturbation space.
- Mechanism: A high-level PPO policy selects a functional neuron cluster (macro-target), conditioning a low-level policy that chooses perturbation type (noise, zero, scale) and magnitude. This factorization reduces the combinatorial action space into tractable sub-problems, allowing structured exploration via PPO's clipped surrogate objective with GAE advantage estimation.
- Core assumption: Hallucination-correction interventions can be decomposed into independent "where" (cluster) and "how" (type/magnitude) decisions that optimize jointly under a shared reward.
- Evidence anchors:
  - [abstract]: "hierarchical reinforcement learning to correct hallucinations... guided by a hierarchical policy that combines high-level and low-level decisions"
  - [section, Dynamic Neuron Perturbation]: "The agent's architecture is bifurcated into two tiers... each implemented with its own actor (policy) and critic (value) networks"
  - [corpus]: No direct corpus validation of HRL for neuron perturbation; related work (Hydra) uses agentic reasoning for hallucination mitigation but without hierarchical decomposition.
- Break condition: If hallucination patterns require joint optimization of cluster and magnitude (non-decomposable), hierarchical factorization may yield suboptimal policies.

### Mechanism 2
- Claim: Two-stage adaptive masking localizes intervention to causally-relevant neurons while maintaining sparsity.
- Mechanism: Stage 1 learns a general sparse mask Mk,l via L1/L0-regularized optimization over episode rewards. Stage 2 computes input-specific attribution scores using Integrated Gradients, then modulates the learned mask via element-wise multiplication: Mop,k,l = Mk,l ⊙ normalize(|Attr_l(x)|). This combines learned priors with real-time causal traces.
- Core assumption: Neurons contributing to hallucinations are identifiable via gradient-based attribution, and their correction-relevant subset is consistent enough to learn a sparse prior.
- Evidence anchors:
  - [section, Adaptive Masking]: "two-stage adaptive masking mechanism... learns a general, input-agnostic pattern... then adapts it with input-specific causal information"
  - [Table 4 ablation]: Random mask causes -5.84 (StoryCloze) and -7.37 (BoolQ) accuracy drops versus full LLM-CAS.
  - [corpus]: "Shadows in the Attention" links hallucination to representation drift, supporting causal trace relevance; no direct validation of two-stage masking.
- Break condition: If attribution methods fail to capture hallucination-relevant neurons (e.g., distributed representations without clear gradients), mask modulation becomes noise.

### Mechanism 3
- Claim: Temporary activation perturbation preserves base model capabilities while enabling context-specific correction.
- Mechanism: Perturbations Δdyn are applied to activations Act(x;W) during inference only, yielding Actperturbed ← Act ⊕ Δdyn without modifying W. The perturbation is discarded after each forward pass, avoiding permanent weight changes that accumulate into catastrophic forgetting.
- Core assumption: Hallucinations arise from transient activation patterns correctable via additive perturbations, rather than fundamental weight-space errors requiring permanent edits.
- Evidence anchors:
  - [abstract]: "avoids permanent parameter changes and catastrophic forgetting, unlike static model editing"
  - [section, Model Editing]: "static edits prove brittle when faced with widespread, context-dependent hallucinations"
  - [corpus]: AutoRAG-LoRA uses lightweight adapters for hallucination-triggered retuning; DSVD uses decoding-time verification—both align with inference-time intervention philosophy.
- Break condition: If hallucinations stem from architectural or weight-level deficits, activation perturbation provides only superficial correction without addressing root cause.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: LLM-CAS uses PPO to train both hierarchical policy levels; understanding clipped surrogate objectives and GAE is essential for debugging training instability.
  - Quick check question: Can you explain why PPO's clipping prevents excessive policy updates compared to vanilla policy gradient?

- Concept: **Integrated Gradients (attribution)**
  - Why needed here: The adaptive masking module relies on attribution scores to localize hallucination-causing neurons.
  - Quick check question: How does Integrated Gradients satisfy the sensitivity and implementation invariance axioms for attribution?

- Concept: **Catastrophic forgetting in model editing**
  - Why needed here: LLM-CAS explicitly avoids this failure mode; understanding why static edits cause forgetting clarifies the design rationale.
  - Quick check question: Why do sequential "locate-then-edit" interventions accumulate negative side effects on unrelated knowledge?

## Architecture Onboarding

- Component map:
  - Input x -> Target LLM (receives activation perturbations) -> corrected output yc
  - Input x -> State construction (Emb(x), Scores_baseline, Scores_best, Stepsnorm) -> Hierarchical RL Agent (high-level MLP + low-level MLP, each with actor-critic) -> actions (cluster, type/magnitude)
  - Actions + Adaptive Masking Module (learns sparse masks, computes attribution, generates operational mask) -> perturbation application -> Target LLM
  - Target LLM -> Evaluation Module (LLM-as-judge scores hallucination/relevance/fluency) -> reward calculation -> PPO update + mask update

- Critical path: Input x → baseline output yh → state construction → high-level action (cluster) → low-level action (type/magnitude) → adaptive mask generation → perturbation application → corrected output yc → evaluation → reward → PPO update + mask update

- Design tradeoffs:
  - Sparsity (λ_sparse, λ_L0) vs. correction effectiveness: tighter masks reduce interference but may miss relevant neurons.
  - Reward weights (wh, wr, wf): prioritize hallucination reduction vs. fluency/relevance preservation.
  - Number of neurons selected (Figure 3): more neurons increase intervention capacity but risk semantic drift.

- Failure signatures:
  - Accuracy drops below baseline: check mask sparsity settings; over-sparse masks may disable effective interventions.
  - High variance in training rewards: inspect PPO clipping threshold ε and GAE λ parameters.
  - Fluency degradation despite hallucination improvement: increase wf relative to wh in reward function.

- First 3 experiments:
  1. **Ablation validation**: Replicate Table 4 (random mask, random action, both removed) on a held-out dataset to confirm component contributions.
  2. **Hyperparameter sweep**: Vary number of selected neurons (200–5000 per Figure 3) and plot accuracy curves to identify optimal sparsity for your target model.
  3. **Cross-architecture test**: Apply trained LLM-CAS agent (trained on LLaMA2-7B) to Mistral-7B without retraining to assess policy transferability; compare against Table 3 results.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- The hierarchical decomposition's effectiveness depends on the assumption that hallucination-correction can be factorized into independent cluster and magnitude decisions.
- The two-stage adaptive masking mechanism relies on Integrated Gradients attribution, which may not capture distributed or attention-based hallucination patterns.
- The temporary perturbation approach assumes hallucinations arise from transient activation patterns rather than fundamental weight-space deficits.

## Confidence
- **High confidence**: The experimental results showing consistent accuracy improvements (10.98 points on StoryCloze, 2.71 on TriviaQA) and the mechanism of avoiding permanent weight changes to prevent catastrophic forgetting.
- **Medium confidence**: The hierarchical policy decomposition's ability to navigate the high-dimensional perturbation space efficiently, as this depends on the validity of the factorization assumption.
- **Medium confidence**: The two-stage adaptive masking's ability to localize causally-relevant neurons, as this relies on attribution methods that may not capture all hallucination patterns.

## Next Checks
1. **Factorization validation**: Systematically test whether joint optimization of cluster and magnitude decisions outperforms the hierarchical decomposition on a controlled synthetic hallucination dataset where the ground truth intervention pattern is known.

2. **Attribution robustness**: Compare Integrated Gradients-based masking against alternative attribution methods (e.g., SHAP, attention-based) on the same hallucination detection task to quantify sensitivity to attribution choice.

3. **Architecture transfer stress test**: Apply the trained LLM-CAS agent across significantly different model architectures (e.g., encoder-decoder vs decoder-only, varying attention patterns) to test the generalization limits of the learned perturbation policy.