---
ver: rpa2
title: Tiny-QMoE
arxiv_id: '2509.22951'
source_url: https://arxiv.org/abs/2509.22951
tags:
- quantization
- compression
- which
- llama3
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tiny-QMoE addresses the challenge of deploying large language models
  on memory-constrained devices by introducing a novel quantization and compression
  method. The core idea involves quantizing LLAMA 3.2 models to 8-bit precision and
  then compressing them using a dictionary-based LZW approach, storing frequently
  occurring sequences of quantized weights as codewords.
---

# Tiny-QMoE

## Quick Facts
- **arXiv ID:** 2509.22951
- **Source URL:** https://arxiv.org/abs/2509.22951
- **Reference count:** 7
- **Primary result:** Achieves 23-35x compression of LLAMA 3.2 models while maintaining near-baseline accuracy

## Executive Summary
Tiny-QMoE introduces a novel quantization and compression method for deploying large language models on memory-constrained devices. The approach quantizes LLAMA 3.2 models to 8-bit precision and compresses them using dictionary-based LZW encoding of frequently occurring weight sequences. This enables models significantly larger than typical memory limits to run on constrained systems with minimal performance and latency impact. The method achieves compression rates of approximately 23x for the 1B model and 35x for the 3B model while maintaining accuracy close to uncompressed baselines.

## Method Summary
The Tiny-QMoE method consists of three main stages: First, pretrained LLAMA 3.2 models (1B, 3B) are quantized from FP16 to 8-bit using uniform quantization with scale-factor-based mapping, optionally enhanced with GPTQ calibration using C4 dataset. Second, the quantized weights undergo dictionary-based compression where the system identifies the most frequent 4-byte sequences (top 65,535 patterns) and maps them to 16-bit codewords, replacing known sequences with codewords and prefixing unknown sequences with 0xFFFF. Third, during inference, weights are decompressed layer-by-layer on the CPU just before use, allowing models to exceed physical memory limits while maintaining acceptable latency overhead compared to network access to cloud-based models.

## Key Results
- Achieves 23x compression for 1B model (1469 MB → 125.29 MB) and 35x for 3B model (3522 MB → 187.97 MB)
- Maintains MMLU accuracy close to baselines: 29.3% → 29.25% for 1B, 35.34% → 35.31% for 3B
- Latency overhead remains below network latency to cloud models: 0.1346s → 0.2114s (1B), 0.3292s → 0.5575s (3B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** 8-bit quantization preserves model accuracy while reducing memory footprint by ~50% compared to FP16, enabling subsequent compression without catastrophic representational loss.
- **Mechanism:** The paper maps FP16 weights to 256 discrete levels using scale-factor-based uniform quantization, then applies GPTQ (data-dependent calibration using C4 dataset) to preserve critical weights. This maintains weight distribution granularity that smaller models (1B-3B parameters) rely on for pattern capture—unlike ternary quantization which collapsed model functionality during experiments.
- **Core assumption:** The quantized weight distribution retains sufficient entropy and structure to support meaningful pattern recognition in the compression stage.
- **Evidence anchors:**
  - [abstract]** "quantizing LLAMA 3.2 models to 8-bit precision"
  - [Section 3]** "ternary, 2-bit, and 4-bit quantization caused a severe loss of model functionality, 6-bit and 8-bit quantization retained the model's ability to generate coherent outputs"
  - [Section 5, Table 2]** MMLU accuracy: 29.3% → 29.25% (1B), 35.34% → 35.31% (3B)
- **Break condition:** Models with <1B parameters may not have sufficient representational redundancy to survive 8-bit quantization without significant accuracy degradation—paper does not test below 1B.

### Mechanism 2
- **Claim:** Dictionary-based compression of quantized weight sequences exploits recurring patterns to achieve 23-35x compression with exact reconstruction (lossless decompression).
- **Mechanism:** The system scans all quantized weights, identifies the most frequent 4-byte sequences (default `sequence_length=4`, top 65,535 patterns), and builds a lookup table mapping each sequence to a 16-bit codeword. During compression, known sequences are replaced by their codeword; unknown sequences are prefixed with `0xFFFF` and stored raw. Decompression reverses this via table lookup, preserving exact weight values.
- **Core assumption:** Quantized weight matrices contain sufficient repeated 4-byte patterns to justify dictionary overhead and achieve net compression—empirically validated by 23x/35x rates reported.
- **Evidence anchors:**
  - [abstract]** "storing frequently occurring sequences of quantized weights as codewords"
  - [Section 4, Listing 2-3]** Code shows `find_frequent_sequences()` using `top_k=2**16-1` and `compress_model()` replacing matches with table indices
  - [Section 4, Table 1]** llama3.2-1B: 1469 MB → 125.29 MB; llama3.2-3B: 3522 MB → 187.97 MB
  - [corpus]** Weak corpus corroboration—neighbor papers focus on MoE-specific compression, not LZW-based weight sequence encoding on dense models
- **Break condition:** If weight distributions have near-uniform entropy (no dominant patterns), dictionary approach degrades to overhead-plus-raw-storage, potentially increasing size. Paper does not characterize entropy thresholds.

### Mechanism 3
- **Claim:** Layer-by-layer decompression during inference enables running models exceeding physical memory limits, with latency overhead remaining below typical network round-trips.
- **Mechanism:** Weights remain compressed on disk/in memory; during forward pass, each layer's compressed parameters are decompressed just before use, then discarded. This trades compute (decompression CPU cycles) for memory footprint reduction. The CPU-focused implementation avoids GPU memory bottlenecks and proprietary dependencies.
- **Core assumption:** Decompression latency per layer is acceptable relative to inference compute time, and CPU cache/memory bandwidth can sustain the decompressed weights during layer execution.
- **Evidence anchors:**
  - [Section 2.3]** "parameters are not decompressed yet and must do so on a layer by layer basis"
  - [Section 5, Tables 2-4]** Latency overhead: 1B model 0.1346s → 0.2114s (MMLU), 3B model 0.3292s → 0.5575s
  - [Section 5]** "accessing online language models... incurred 697ms which is much higher than the latency incurred through decompression"
  - [corpus]** Related work (FloE, MiLo) confirms layer-wise/offloading strategies as viable for memory-constrained inference
- **Break condition:** For autoregressive generation with many tokens, cumulative decompression overhead across all layers for each token may exceed network latency for very long sequences—paper only reports per-example latency on benchmark tasks.

## Foundational Learning

- **Quantization-aware weight representation**
  - Why needed here: Understanding how FP16→INT8 mapping affects weight distribution entropy directly predicts compressibility. Naive uniform quantization can destroy pattern structure; GPTQ calibration mitigates this.
  - Quick check question: Given a layer with weights ranging [-0.5, 0.3], what scale factor and zero point would map these to [0, 255] for 8-bit quantization?

- **Dictionary-based compression fundamentals (LZW-style)**
  - Why needed here: The compression stage relies on sequence frequency analysis and codeword assignment. Misunderstanding dictionary growth/lookup costs leads to incorrect latency expectations.
  - Quick check question: If a 1GB model has 70% of its quantized weight bytes covered by the top 10,000 4-byte sequences, what's the approximate compressed size assuming 2-byte codewords?

- **Layer-wise memory management in transformers**
  - Why needed here: Decompression-on-demand requires understanding when weights are needed (during matrix multiplication for each layer) and how long they must persist in memory.
  - Quick check question: During autoregressive token generation, must layer weights remain in memory across all tokens, or can they be decompressed once per forward pass and discarded?

## Architecture Onboarding

- **Component map:**
  - Quantizer class: find_params() computes scale/zero-point per layer; quantize() applies mapping
  - Compression module: find_frequent_sequences() builds dictionary; compress_model() encodes weights to .npy files per parameter
  - Decompression module: decompress_model() reverse-lookup, called during inference
  - Inference runtime: Modified forward pass that loads compressed weights, decompresses per layer, executes standard transformer ops
  - Evaluation harness: PyTorch + HuggingFace Transformers integration with MMLU/ARC benchmarks

- **Critical path:**
  1. Load pretrained LLAMA 3.2 model (FP16)
  2. Apply 8-bit quantization with GPTQ calibration (C4 dataset)
  3. Scan all quantized weights → build frequency table → create compression dictionary
  4. Encode weights: replace frequent sequences with codewords, save per-parameter .npy files
  5. At inference: for each layer, load compressed data → decompress → execute layer → discard weights
  6. Aggregate predictions, measure accuracy and latency

- **Design tradeoffs:**
  - 8-bit vs 4-bit quantization: 8-bit preserves accuracy (29.3%→29.25% MMLU) but yields ~2x compression; 4-bit + GPTQ still underperforms (Section 3)
  - Sequence length (4 bytes): Longer sequences improve compression if patterns repeat; shorter sequences increase dictionary hit rate. Paper uses 4 as default without ablation.
  - Dictionary size (65,535 entries): Larger dictionary captures more patterns but increases memory overhead; smaller risks more raw encodings (0xFFFF escapes)
  - CPU-only execution: Hardware-agnostic but slower; GPU decompression (future work with OpenCL/OpenXLA) could reduce latency at cost of portability

- **Failure signatures:**
  - Ternary/2-4 bit quantization: Model generates incoherent text (Section 3: "failed to generate coherent English responses")
  - Insufficient dictionary coverage: Compressed size approaches or exceeds quantized size (too many 0xFFFF prefixes)
  - Decompression bottleneck: Per-token latency grows unacceptable for long sequences; watch for decompress time > compute time
  - Memory pressure during decompression: Layer weights + activations exceed available RAM → OOM errors

- **First 3 experiments:**
  1. Reproduce baseline compression: Run `find_frequent_sequences()` + `compress_model()` on llama3.2-1B, verify ~23x compression ratio and exact reconstruction via round-trip test (compress → decompress → compare to quantized original)
  2. Ablate sequence length: Test sequence_length ∈ {2, 4, 8, 16} on 1B model; plot compression ratio vs. dictionary hit rate to find optimal pattern granularity
  3. Profile decompression overhead: Instrument inference loop to measure per-layer decompress time vs. compute time; identify if latency is dominated by I/O, dictionary lookup, or weight reconstruction

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's code only demonstrates naive uniform quantization, lacking implementation details for GPTQ calibration that is mentioned as enhancing accuracy preservation
- Dictionary-based compression performance depends on weight distribution entropy patterns that aren't characterized, with no established failure thresholds for compression degradation
- Layer-by-layer decompression latency scaling is only evaluated for single examples, not multi-token generations where cumulative overhead could become problematic

## Confidence
- **High Confidence:** The compression methodology (LZW-style dictionary encoding of quantized weights) is clearly described and produces verifiable size reductions (23x for 1B, 35x for 3B). The accuracy retention claims (MMLU 29.3%→29.25% for 1B) are directly supported by benchmark results in Tables 2-4.
- **Medium Confidence:** The mechanism by which 8-bit quantization preserves model accuracy is plausible based on experimental observations, but the paper's code only demonstrates naive quantization without GPTQ implementation details, creating uncertainty about the calibration process.
- **Low Confidence:** Claims about hardware-agnostic CPU execution avoiding proprietary dependencies are partially undermined by the reliance on PyTorch and HuggingFace frameworks, which may have underlying hardware dependencies not addressed in the paper.

## Next Checks
1. Round-trip compression validation: Compress and decompress the quantized 1B model weights, then verify exact byte-for-byte reconstruction to confirm lossless decompression works as specified before proceeding to inference experiments.
2. Sequence length ablation study: Systematically test dictionary compression with sequence lengths {2, 4, 8, 16} on the 1B model to empirically determine the optimal pattern granularity that balances compression ratio against dictionary complexity.
3. Per-token latency scaling test: Measure inference latency for autoregressive generation over 50-100 tokens to quantify how cumulative decompression overhead scales with sequence length and compare against network latency to cloud models for the same task.