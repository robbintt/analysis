---
ver: rpa2
title: 'Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social
  Deduction Game'
arxiv_id: '2601.09017'
source_url: https://arxiv.org/abs/2601.09017
tags:
- game
- guess
- player
- vote
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic, game-based benchmark for evaluating
  multilingual and multicultural capabilities of LLMs using Spyfall. The game-based
  rankings align closely with the Chatbot Arena, validating social deduction as an
  effective proxy for general model capability.
---

# Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social Deduction Game

## Quick Facts
- arXiv ID: 2601.09017
- Source URL: https://arxiv.org/abs/2601.09017
- Reference count: 39
- Primary result: Game-based rankings align with Chatbot Arena, validating social deduction as effective proxy for general model capability while revealing significant multilingual and cultural performance gaps

## Executive Summary
This paper introduces a dynamic, game-based benchmark for evaluating multilingual and multicultural capabilities of LLMs using Spyfall. The framework reveals that models show significant performance degradation in non-English contexts and when handling locally specific cultural entities, with notable challenges in Arabic scenarios. While game-based rankings align closely with Chatbot Arena, validating social deduction as an effective proxy for general model capability, the study also identifies critical bottlenecks including format compliance issues and dialect adherence failures. The framework is leakage-resistant, scalable, and extensible to new languages, offering a culturally nuanced alternative to traditional benchmarks.

## Method Summary
The framework implements turn-based Spyfall gameplay across 3 scenarios (Generic, Local Location, Local Food) in 4 languages (en, id, zh, arz) using 5 players (4 non-spies + 1 spy). Models receive role-specific prompts with game history and must respond in strict JSON format. Each model plays 600 matches across all role permutations, with games ending when correct spy guess, majority vote, or surrender occurs. Evaluation uses Bradley-Terry ratings, win rates, leakage rates, Shannon entropy for vote/guess dispersion, and spy guess accuracy. The framework supports future extensions to interactive tasks and heterogeneous model configurations.

## Key Results
- Game-based rankings align closely with Chatbot Arena, validating social deduction as effective proxy for general model capability
- Significant performance degradation observed in non-English contexts, particularly for Arabic scenarios
- Models struggle with locally specific cultural entities (Local Location and Local Food scenarios)
- Format compliance issues emerge as critical bottleneck, with many models failing to adhere to strict JSON output requirements
- Spy models show higher variance in guess accuracy across languages, suggesting challenges with dialect and cultural nuances

## Why This Works (Mechanism)
The Spyfall framework works because social deduction games create high-stakes environments requiring both language proficiency and cultural knowledge. The game's asymmetric information structure forces models to demonstrate genuine understanding rather than memorized responses. The strict JSON format requirement ensures computational feasibility while maintaining game integrity. The three-scenario structure (Generic, Local Location, Local Food) progressively tests general language ability versus cultural specificity, creating a gradient of difficulty that reveals nuanced capability gaps.

## Foundational Learning
The framework leverages models' pretraining on diverse multilingual corpora, but reveals significant gaps in culturally specific knowledge. Models trained primarily on English data show systematic degradation when required to reason about local contexts in Indonesian, Chinese, or Egyptian Arabic. The framework suggests that current multilingual pretraining approaches may not adequately capture the cultural nuances necessary for authentic cross-cultural reasoning, particularly for low-resource languages and dialects.

## Architecture Onboarding
The framework is architecture-agnostic, supporting various model families including GPT-4, Claude-3, and other commercial APIs. The JSON format requirement serves as a standardization mechanism, allowing heterogeneous models to participate while maintaining comparability. The framework's modular design permits easy integration of new models and languages, with the turn-based structure naturally accommodating different inference speeds and token limits.

## Open Questions the Paper Calls Out
- How do model performance patterns change with larger, more diverse language sets beyond the four tested languages?
- Can the framework effectively evaluate interactive, real-time capabilities rather than turn-based interactions?
- What are the long-term effects of cultural bias in training data on model performance across different linguistic communities?
- How can the framework be extended to test collaborative versus competitive dynamics in multilingual settings?
- What specific pretraining strategies could address the observed gaps in cultural and dialectal reasoning?

## Limitations
- Limited to four languages and three scenario types, restricting generalizability to broader multilingual contexts
- Turn-based format may not capture real-time interaction capabilities critical for practical applications
- Strict JSON format requirement may disadvantage models with different architectural strengths
- Reliance on commercial API models limits reproducibility and access to model internals
- Cultural scenarios may not fully represent the complexity of real-world multilingual interactions
- Performance degradation in non-English contexts could reflect evaluation bias rather than actual capability gaps

## Confidence
Medium-High. The framework demonstrates strong alignment with established benchmarks while revealing novel insights about multilingual capabilities. However, the limited language scope and reliance on commercial models introduce uncertainty about broader applicability. The JSON format requirement, while necessary for computational feasibility, may create artificial constraints that affect model performance assessment.

## Next Checks
- Test framework with additional languages, particularly low-resource and dialectal variants
- Evaluate interactive versus turn-based performance differences
- Analyze training data composition to identify specific cultural knowledge gaps
- Compare performance across different architectural families beyond commercial APIs
- Investigate the relationship between pretraining data diversity and cultural reasoning capabilities
- Assess the impact of format compliance requirements on different model types