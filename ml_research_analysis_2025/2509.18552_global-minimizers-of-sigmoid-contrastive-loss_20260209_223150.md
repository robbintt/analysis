---
ver: rpa2
title: Global Minimizers of Sigmoid Contrastive Loss
arxiv_id: '2509.18552'
source_url: https://arxiv.org/abs/2509.18552
tags:
- loss
- bias
- learning
- relative
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the global minimizers of the sigmoid contrastive
  loss used in recent vision-language models like SigLIP. It shows that global minima
  occur when the embedding pairs form an (m,b rel)-constellation - a geometric configuration
  where matching pairs are separated from mismatching pairs by a margin m and relative
  bias b rel.
---

# Global Minimizers of Sigmoid Contrastive Loss

## Quick Facts
- arXiv ID: 2509.18552
- Source URL: https://arxiv.org/abs/2509.18552
- Reference count: 40
- Key outcome: Global minima of sigmoid contrastive loss occur when embedding pairs form (m,b rel)-constellations with explicit constructions and bounds

## Executive Summary
This paper provides a complete theoretical characterization of global minimizers for the sigmoid contrastive loss used in vision-language models like SigLIP. The authors show that zero-loss configurations correspond to geometric arrangements called (m, b_rel)-constellations, where matching pairs are separated from non-matching pairs by specific margins. The work establishes explicit constructions of these constellations, proves bounds on their size in terms of dimension, and demonstrates their optimality properties. The paper also introduces a relative bias parameterization that improves training dynamics and explains the emergence of the modality gap observed in practice.

## Method Summary
The paper analyzes the sigmoid contrastive loss L_Sig = Σ_i log(1 + exp(-t⟨U_i,V_i⟩+b)) + Σ_{i≠j} log(1 + exp(t⟨U_i,V_j⟩-b)) with trainable temperature t and bias b. Through mathematical analysis, it characterizes when this loss approaches zero as requiring embeddings to form (m, b_rel)-constellations with specific margin and relative bias conditions. The authors prove necessity and sufficiency of these conditions for global minima, establish bounds on constellation size via spherical code arguments, and propose a relative bias parameterization L_RB-Sig where b = t·b_rel. Synthetic experiments with N=100 pairs in d=10 dimensions validate the theoretical predictions about convergence, margin size, and the advantages of the relative bias parameterization.

## Key Results
- Global minima of sigmoid contrastive loss occur precisely when embeddings form (m, b_rel)-constellations
- Explicit constructions achieve N = O(d·log d) pairs for m=0, N = O(d) pairs for m>0 in d dimensions
- Modality gap emerges necessarily when N > d and |b_rel| < m, explaining empirical observations
- Relative bias parameterization improves training dynamics and allows control over margin-retrieval tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-loss configurations of sigmoid contrastive loss form (m, b_rel)-constellations—geometric arrangements where matching embedding pairs are separated from non-matching pairs by a margin m around a relative bias b_rel.
- Mechanism: The sigmoid loss approaches zero if and only if there exist m ≥ 0 and b_rel such that ⟨U_i,V_i⟩ ≥ m + b_rel for all i, and ⟨U_i,V_j⟩ ≤ -m + b_rel for all i≠j. Trainable t and b allow the optimization to scale up t while maintaining b/t = b_rel, asymptotically achieving zero loss for any constellation.
- Core assumption: Embeddings are unit-norm vectors on the hypersphere S^{d-1}; the encoders are sufficiently expressive to realize any target configuration.
- Evidence anchors:
  - [abstract] "Temperature and bias can drive the loss function to zero for a rich class of configurations that we call (m, b_rel)-Constellations."
  - [Section 3.1, Theorems 3.1–3.2] Characterizes necessity and sufficiency of constellation conditions for global minima.
  - [corpus] Limited direct corpus validation; nearest neighbors focus on contrastive complexity and calibration rather than constellation geometry.
- Break condition: If t is fixed or b is not proportional to t, the scaling property fails and zero loss is only achievable for special configurations (e.g., perfect alignment or antipodal).

### Mechanism 2
- Claim: The modality gap—linear separability between image and text embedding regions—emerges necessarily when N > d and |b_rel| < m.
- Mechanism: Under constellation conditions with m > |b_rel|, matching pairs have positive inner products while non-matching pairs have negative inner products. By Helly's theorem and Carathéodory's theorem, there exists a hyperplane h such that ⟨h, U_i⟩ > 0 for all image embeddings and ⟨h, V_j⟩ < 0 for at least N-d text embeddings. This is tight—d-1 points may violate separation.
- Core assumption: N > d (practical regime for large-scale pretraining); the constellation has strictly positive margin relative to relative bias.
- Evidence anchors:
  - [abstract] "which explains empirical observations in practice" regarding modality gap.
  - [Section 3.3, Theorem 3.6] Proves linear separability for all but d points when m > |b_rel|.
  - [corpus] ClearVision paper uses SigLIP-2 for weather classification but does not analyze modality gap; corpus evidence is indirect.
- Break condition: If |b_rel| ≥ m or N ≤ d, the geometric conditions for guaranteed modality gap do not hold; separation may be data-dependent.

### Mechanism 3
- Claim: Explicit relative bias parameterization L_RB-Sig (where b = t·b_rel) yields faster convergence and allows control over the margin-retrieval robustness tradeoff.
- Mechanism: In standard parameterization, Adam tends to drive b_rel → 0, limiting the family of attainable constellations. Fixing b_rel allows practitioners to target specific regions of the (m, b_rel) plane: higher m improves retrieval robustness to approximate nearest neighbor errors; b_rel ≠ 0 enables richer embedding geometries including the modality gap. The reparameterization implicitly corresponds to adding linear adapters A_δ that rotate locked and trainable representations.
- Core assumption: First-order optimizers (Adam) are used; the desired b_rel is known or can be searched.
- Evidence anchors:
  - [abstract] "Finally, we propose a reparameterization of the sigmoid loss with explicit relative bias, which improves training dynamics."
  - [Section 3.4, Figures 10–11] Synthetic experiments show faster loss convergence and larger margins with L_RB-Sig vs. L_Sig.
  - [corpus] SCS-SupCon proposes sigmoid-based contrastive learning with adaptive boundaries but does not analyze relative bias; corpus provides no direct evidence.
- Break condition: If b_rel is initialized or fixed to an infeasible value (outside the green region in Figure 2), no constellation exists and the loss cannot converge to zero.

## Foundational Learning

- Concept: Spherical codes
  - Why needed here: Constellations are a variant of spherical codes—sets of unit vectors with bounded pairwise inner products. Understanding packing bounds (e.g., Shannon-Wyner) is essential for interpreting the N vs. d tradeoff in Theorems 3.3–3.5.
  - Quick check question: Given d=768 and b_rel=0, what is the approximate maximum N for m=0.01 using the Shannon lower bound?

- Concept: Global vs. local minima in non-convex optimization
  - Why needed here: The paper characterizes global minimizers but does not prove that Adam finds them; understanding convergence assumptions is critical for applying these results.
  - Quick check question: Does Theorem 3.1 guarantee that gradient descent with trainable t, b will find a constellation, or only that any convergent subsequence must be one?

- Concept: Nearest neighbor retrieval and approximate search
  - Why needed here: Corollary 1 and Proposition 1 link margin m to retrieval robustness under approximate nearest neighbor search—a key practical concern for billion-scale retrieval systems.
  - Quick check question: If m=0.01 and approximate search introduces up to ε=0.005 perturbation in inner products, will retrieval remain correct?

## Architecture Onboarding

- Component map:
  - Encoder f_θ(X) → U ∈ S^{d-1} (image)
  - Encoder g_φ(Y) → V ∈ S^{d-1} (text)
  - Trainable parameters: θ, φ, t > 0 (inverse temperature), b_rel ∈ [-1,1] (relative bias)
  - Loss: L_RB-Sig = Σ_i log(1 + exp(-t(⟨U_i,V_i⟩ - b_rel))) + Σ_{i≠j} log(1 + exp(t(⟨U_i,V_j⟩ - b_rel)))

- Critical path:
  1. Initialize t moderately (e.g., 10–30), b_rel near target (e.g., 0.6–0.7 for large margin).
  2. Train with Adam on embeddings and (t, b_rel), using per-batch normalization to maintain unit norm.
  3. Monitor margin m_t = (min_i⟨U_i,V_i⟩ - max_{i≠j}⟨U_i,V_j⟩)/2 and effective b_rel = b/t.
  4. Stop when loss is near zero and margin stabilizes.

- Design tradeoffs:
  - High m → robust retrieval but requires larger d for same N (Theorem 3.5).
  - Non-zero b_rel → enables modality gap and richer geometries but restricts feasible (m, b_rel) region (Figure 2).
  - Fixed b_rel → explicit control but requires hyperparameter search; trainable b_rel → defaults toward zero.

- Failure signatures:
  - Loss plateaus above zero: (m, b_rel) may be in infeasible region; check if m + b_rel ≤ 1 and 3m ≤ 1 + b_rel.
  - Margin collapses to near zero: b_rel may have drifted to zero; fix b_rel or use explicit reparameterization.
  - Retrieval fails on held-out data: margin too small for approximate search errors; increase d or target higher m.

- First 3 experiments:
  1. Synthetic locked-encoder: Fix random {U_i}, train {V_i} with L_RB-Sig vs. L_Sig; plot loss convergence, margin, and b_rel over time.
  2. Multi-modality (k=4): Synchronize {U^(1)_i,...,U^(4)_i} with pairwise L_RB-Sig; measure final margin vs. k and compare to Construction 2 predictions.
  3. Modality gap verification: Train with fixed b_rel values spanning [-0.5, 0.7]; for each, fit a linear classifier on U vs. V embeddings and report separation accuracy; correlate with |b_rel| < m condition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are tight bounds for NMRB(d, m, b_rel), the maximum size of (m, b_rel)-constellations in dimension d?
- Basis in paper: [explicit] "we do not fully resolve the combinatorial Problem 1, which as we point out is practically relevant for choosing the embedding dimension of encoders"
- Why unresolved: Theorem 3.3 and 3.5 provide lower and upper bounds respectively, but there is a gap between them; the exact capacity remains unknown.
- What evidence would resolve it: Tight characterizations of NMRB matching upper and lower bounds, or explicit constructions achieving the upper bounds.

### Open Question 2
- Question: Do first-order optimization methods (e.g., Adam, SGD) provably converge to global minima of the sigmoid contrastive loss with trainable temperature and bias?
- Basis in paper: [explicit] "we do not prove rigorous performance guarantees for first-order methods"
- Why unresolved: Empirical results (Figures 10, 11) show Adam finds (m, b_rel)-constellations, but no theoretical convergence guarantees exist in the regime d ≪ N ≪ 2^d.
- What evidence would resolve it: Convergence rate bounds for Adam or SGD on sigmoid loss, possibly with assumptions on initialization or data distribution.

### Open Question 3
- Question: Can the relative bias parameterization L_RB-Sig improve training dynamics on real large-scale vision-language datasets compared to standard SigLIP?
- Basis in paper: [explicit] "we show that the parametrization of sigmoid loss with relative bias leads to more flexibility and faster convergence on synthetic data, but do not perform experiments with it on real data"
- Why unresolved: Experiments only cover synthetic data (N=100, d=10) and ImageNet analysis of pre-trained models, not training from scratch.
- What evidence would resolve it: Training SigLIP-scale models with L_RB-Sig on web-scale image-text data and comparing convergence speed, final retrieval accuracy, and margin.

### Open Question 4
- Question: Can training with an explicit ξ-based regularization term enable lower-dimensional representations while preserving retrieval quality?
- Basis in paper: [explicit] "It is an interesting open direction whether we can train models in lower dimension utilizing the fact that ξ → 0 in that case. For example, one can explicitly add ξ in the loss function."
- Why unresolved: ξ=0 for synthetic training (Figure 20) but ξ≈0.55-0.62 for real models (Table 6), suggesting real models don't achieve optimal capacity-utilizing configurations.
- What evidence would resolve it: Experiments training with ξ-regularized loss, measuring resulting dimension requirements and downstream task performance.

## Limitations

- Theoretical completeness gap: While global minimizers are characterized, the paper does not prove that gradient-based optimizers reliably find these minima in practice.
- Practical relevance gap: The theoretical framework assumes perfect embedding synchronization through encoders with sufficient capacity, which may not hold in practice due to representation bottlenecks and architectural constraints.
- Dataset-specific phenomena: The empirical validation focuses on SigLIP models trained on large-scale web datasets without investigating how constellation properties transfer across different pretraining objectives and domains.

## Confidence

- High confidence in the mathematical characterization of (m, b_rel)-constellations as global minimizers, supported by rigorous proofs in Theorems 3.1-3.2.
- Medium confidence in the practical implications regarding retrieval robustness and modality gap emergence, as these rely on the assumption that trained models achieve the theoretical constellation conditions.
- Low confidence in the claim that the relative bias parameterization universally improves training dynamics, as this is demonstrated only on synthetic experiments.

## Next Checks

1. **Convergence to global minima**: Train a SigLIP-like model from scratch using both standard and relative bias parameterizations, monitoring whether the final embedding configuration satisfies the (m, b_rel)-constellation conditions. Measure the gap between theoretical and achieved margins.

2. **Cross-model generalization**: Evaluate the constellation properties (margin, relative bias, modality gap) across diverse vision-language models beyond SigLIP, including models trained with different objectives (e.g., CLIP, BLIP) and on different domains.

3. **Approximate search robustness**: Implement retrieval experiments that systematically vary the approximation error ε in inner products, measuring the empirical probability of retrieval errors as a function of the theoretical margin m. This would validate Corollary 1's prediction about the m ≥ ε requirement for reliable retrieval.