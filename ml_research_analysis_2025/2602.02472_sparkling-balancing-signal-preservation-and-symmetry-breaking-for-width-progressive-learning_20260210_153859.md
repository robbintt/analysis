---
ver: rpa2
title: 'SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive
  Learning'
arxiv_id: '2602.02472'
source_url: https://arxiv.org/abs/2602.02472
tags:
- expansion
- re-warmup
- training
- learning
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of mid-stage width expansion in
  progressive learning, where naive approaches cause loss spikes and gradient symmetry
  that prevent new capacity from being utilized. The authors propose SPARKLING, a
  framework balancing signal preservation (via RMS-scale consistency) and symmetry
  breaking (via asymmetric optimizer state resetting and learning rate re-warmup).
---

# SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning

## Quick Facts
- arXiv ID: 2602.02472
- Source URL: https://arxiv.org/abs/2602.02472
- Reference count: 40
- Primary result: Up to 35% training cost reduction with improved downstream performance vs training from scratch under 2× width expansion

## Executive Summary
This paper addresses the challenge of mid-stage width expansion in progressive learning, where naive approaches cause loss spikes and gradient symmetry that prevent new capacity from being utilized. The authors propose SPARKLING, a framework balancing signal preservation (via RMS-scale consistency) and symmetry breaking (via asymmetric optimizer state resetting and learning rate re-warmup). Experiments on Mixture-of-Experts models show SPARKLING consistently outperforms training from scratch, reducing training costs by up to 35% under 2× width expansion while improving downstream performance. The method generalizes across multiple width axes and optimizer families including AdamW and Muon.

## Method Summary
SPARKLING tackles mid-stage width expansion by balancing two competing needs: preserving existing signal through RMS-scale weight preservation, and breaking gradient symmetry through asymmetric parameter initialization. The framework uses RMS-preserved scaling to maintain activation distributions, asymmetric optimizer resets to prevent new parameters from copying old ones, and asymmetric LR re-warmup to accelerate new parameter training. Tested on MoE language models expanding at ~50% training progress, the method shows consistent improvements over from-scratch training across expert dimensions, hidden dimensions, and joint expansions.

## Key Results
- 35% training cost reduction vs from-scratch training under 2× width expansion
- Consistent downstream performance improvements across ARC-C/E, MMLU, HellaSwag benchmarks
- Generalizes across multiple width axes (expert dimension, hidden dimension, joint expansion)
- Works with both AdamW and Muon optimizer families

## Why This Works (Mechanism)
SPARKLING works by solving the fundamental tension in progressive width expansion between preserving learned signal and breaking symmetry for new parameters. Naive copying creates gradient symmetry where copied parameters remain identical, preventing effective use of expanded capacity. SPARKLING's RMS-scale preservation maintains activation distribution consistency while asymmetric optimizer resetting and LR re-warmup ensure new parameters can learn distinct features. This allows models to recover faster from expansion and ultimately achieve better performance than training from scratch.

## Foundational Learning
- **RMS preservation**: Normalizes weights to maintain activation distribution consistency across expansion - needed to prevent catastrophic loss spikes
- **Gradient symmetry breaking**: Prevents copied parameters from remaining identical through asymmetric initialization - needed to ensure new capacity is actually utilized
- **Asymmetric optimization**: Different learning dynamics for old vs new parameters - needed to accelerate recovery after expansion
- **Progressive learning timing**: Expansion at mid-training (~50%) rather than early stages - needed to leverage learned representations while still benefiting from expansion

## Architecture Onboarding

**Component map:** Input data → MoE layers (expert dimension/hidden dimension) → RMS scaling → Asymmetric optimizer states → Asymmetric LR schedules → Output

**Critical path:** Weight scaling → Optimizer state initialization → LR schedule application → Parameter update loop

**Design tradeoffs:** Signal preservation vs symmetry breaking, computational cost vs performance gain, expansion timing vs model capacity

**Failure signatures:**
- Loss spike immediately after expansion → RMS scale mismatch
- Copied channels never diversify → symmetry lock due to symmetric optimizer initialization
- Slow post-expansion recovery → incorrect asymmetric LR application

**First experiments:**
1. Implement RMS-preserved weight scaling for 2× expansion (w' = w/2 for fan-in)
2. Test asymmetric optimizer reset (preserve states for original, zero for new)
3. Validate asymmetric LR re-warmup (new params 1.3× current LR over 250 steps)

## Open Questions the Paper Calls Out

**Open Question 1:** Can a unified principle be established for simultaneous width and depth expansion in progressive learning? The paper addresses width expansion in isolation; combining width and depth expansion introduces additional complexity in coordinating signal preservation and symmetry breaking across both dimensions simultaneously.

**Open Question 2:** Does the RMS preservation strategy satisfy the μP (maximal update parametrization) condition, enabling zero-shot hyperparameter transfer after expansion? μP provides theoretical guarantees for hyperparameter transfer across model scales, but SPARKLING's RMS-scale constraints may interact with μP requirements in unknown ways.

**Open Question 3:** How does SPARKLING generalize to dense (non-MoE) transformer architectures? All experiments use Mixture-of-Experts models; the paper does not validate on dense architectures where gradient symmetry dynamics may differ due to uniform parameter usage.

**Open Question 4:** How does expansion magnitude (beyond 2×) affect SPARKLING's effectiveness and stability? All experiments use 2× width expansion; larger expansion ratios may require different re-warmup schedules or cause more severe activation distribution shifts.

## Limitations

- Experimental validation limited to a single 0.5B active/2.5B total parameter MoE architecture
- Asymmetric LR re-warmup schedule (ρ=1.3, τ_w=250) lacks theoretical grounding
- Method's behavior on width reductions or non-uniform expansion patterns unexplored
- Claims about generalization across "multiple width axes" based on limited evidence

## Confidence

**High confidence:** The signal preservation mechanism via RMS scaling is mathematically sound and empirical results show consistent improvement over training from scratch.

**Medium confidence:** The symmetry breaking claims are well-supported for the tested MoE architecture, but generalizability to other model architectures needs broader validation.

**Medium confidence:** The 35% training cost reduction claim is specific to the tested 2× width expansion scenario and may not scale linearly to larger expansion ratios.

## Next Checks

1. **Cross-architecture validation:** Test SPARKLING on non-MoE architectures (standard transformers, state-space models) to verify the symmetry breaking mechanism generalizes beyond expert-based models.

2. **Scaling analysis:** Evaluate SPARKLING at different model scales (1B-10B+ parameters) and expansion ratios (1.5×, 3×, 4×) to validate the claimed training cost reductions hold across scales.

3. **Ablation on asymmetry parameters:** Systematically vary ρ (1.1-2.0) and τ_w (100-1000 steps) to determine if the specific values chosen (1.3, 250) are optimal or if the approach is robust to parameter variations.