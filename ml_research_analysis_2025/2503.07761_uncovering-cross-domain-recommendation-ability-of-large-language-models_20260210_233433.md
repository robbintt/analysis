---
ver: rpa2
title: Uncovering Cross-Domain Recommendation Ability of Large Language Models
arxiv_id: '2503.07761'
source_url: https://arxiv.org/abs/2503.07761
tags:
- domain
- recommendation
- source
- llm4cdr
- cross-domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM4CDR, a pipeline using large language
  models for cross-domain recommendation tasks where target domains have no historical
  user data. The approach constructs context-aware prompts using users' purchase history
  from source domains along with shared features between source and target domains.
---

# Uncovering Cross-Domain Recommendation Ability of Large Language Models

## Quick Facts
- arXiv ID: 2503.07761
- Source URL: https://arxiv.org/abs/2503.07761
- Reference count: 29
- Key outcome: LLM4CDR achieves strong cross-domain recommendation performance when source and target domains share semantic features, with GPT-4 outperforming smaller models.

## Executive Summary
This paper introduces LLM4CDR, a pipeline that uses large language models for zero-shot cross-domain recommendation tasks where target domains lack historical user data. The method constructs context-aware prompts using users' purchase history from source domains and shared features between source and target domains. Tested on Amazon Review datasets across four domains, LLM4CDR shows strong performance when domains have small gaps, with performance positively correlating with LLM parameter size. For example, using CD & Vinyl purchase history for Movies & TV recommendations improved MAP@1 by 64.28%.

## Method Summary
LLM4CDR addresses cold-start cross-domain recommendation by leveraging LLMs to map user preferences from source domains to target domains without requiring historical data in the target. The pipeline generates "Recommendation Guidance" by identifying common features between domains, then constructs prompts combining task adaptation, user history, guidance, and candidate lists. The LLM re-ranks candidates based on this context, and a parser handles formatting mismatches. The method uses strict filtering to create datasets of active users and items, constructs candidate lists with ground truth items and negatives, and evaluates using HIT@K, MAP@K, and NDCG@K metrics.

## Key Results
- LLM4CDR achieves strong performance when source and target domains have small domain gaps
- Performance positively correlates with LLM parameter size, with GPT-4 outperforming smaller models
- Domain-specific recommendation guidance and closer domain relationships enhance effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Semantic Feature Mapping via LLM Knowledge Base
- Claim: LLMs improve cross-domain recommendations by identifying latent shared features between domains when they are semantically close.
- Mechanism: The pipeline instructs the LLM to generate "Recommendation Guidance" by identifying commonalities between domains, guiding the model to map user preferences using semantic bridges rather than collaborative filtering.
- Core assumption: The LLM possesses sufficient pre-trained world knowledge to recognize valid semantic correlations between specific item categories.
- Evidence anchors: [abstract] "constructs context-aware prompts by leveraging... shared features between source and target domains."
- Break condition: Fails if source and target domains lack shared semantic features in the LLM's training data.

### Mechanism 2: Conditional Inference via In-Context Learning
- Claim: Providing user history as a conditional sequence in the prompt allows the LLM to perform zero-shot ranking by attending to specific user preferences.
- Mechanism: The prompt structures the task as a conditional prediction problem where the LLM uses its attention mechanism to weigh items in the history against items in the candidate list.
- Core assumption: The LLM's context window is sufficient to hold the history sequence and candidate list, and its attention mechanism effectively captures sequential preference signals.
- Evidence anchors: [abstract] "...leverages users' purchase history sequences from a source domain..."
- Break condition: Performance degrades if the history sequence is too short or if the context window is exceeded.

### Mechanism 3: Capacity-Dependent Reasoning
- Claim: The success of cross-domain knowledge transfer correlates positively with the LLM's parameter size.
- Mechanism: Larger models outperform smaller ones because they likely possess better abstract reasoning capabilities to generalize user intent from one domain to another without explicit fine-tuning.
- Core assumption: The performance gain is due to reasoning capability rather than simply more memorized item correlations.
- Evidence anchors: [abstract] "...performance positively correlates with LLM parameter size, with GPT-4 outperforming smaller models."
- Break condition: If the model size is below a critical threshold, the model fails to utilize the cross-domain history effectively.

## Foundational Learning

- Concept: **Cross-Domain Recommendation (CDR) & Cold Start**
  - Why needed here: This is the core problem definition - fixing the "cold start" problem in a new domain by stealing knowledge from an old domain.
  - Quick check question: If a user has only bought books (Source), how do we recommend movies (Target) without any movie data?

- Concept: **Zero-Shot Prompting**
  - Why needed here: The method relies on prompting a pre-trained LLM to perform a task it wasn't explicitly trained for, using natural language instructions.
  - Quick check question: Can the model make predictions without updating its weights?

- Concept: **Negative Sampling & Candidate Lists**
  - Why needed here: The LLM does not rank the entire catalog but re-ranks a small list containing "Ground Truth" and "Negative Samples."
  - Quick check question: Why is the "Ground Truth" included in the prompt input for evaluation?

## Architecture Onboarding

- Component map:
  Preprocessor -> Guidance Generator -> Prompt Constructor -> LLM Engine -> Parser

- Critical path:
  Preprocessing (Data Prep) -> Guidance Generation (Feature Selection) -> Prompting (Inference) -> Parsing (Output Cleaning)

- Design tradeoffs:
  - History Length vs. User Coverage: Increasing history length improves accuracy but drastically reduces eligible users
  - Candidate Size vs. Difficulty: Larger candidate lists lower accuracy/recall compared to smaller lists
  - Model Size vs. Cost: High performance requires expensive proprietary models; cheaper models show inconsistent results

- Failure signatures:
  - Negative Transfer: When domain gaps are large, adding source history decreases performance vs. no history
  - Policy Refusal: LLM outputs "I cannot fulfill this request"
  - Item Hallucination: LLM generates items not in the provided candidate list

- First 3 experiments:
  1. Sanity Check (Zero-Shot): Run pipeline on close domain pair (CDs → Movies) using GPT-3.5 with/without source history
  2. Guidance Ablation: Run same task with and without "Recommendation Guidance" component
  3. Domain Gap Stress Test: Run pipeline on disjoint pair (Electronics → Movies) to confirm negative transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM4CDR be adapted to effectively leverage multiple source domains simultaneously without introducing conflicting noise?
- Basis in paper: Introduction notes multiple source domains can alleviate sparsity but may introduce challenges in understanding distributions and relationships.
- Why unresolved: Paper does not test pipeline's ability to synthesize overlapping or contradictory history sequences from distinct categories.
- What evidence would resolve it: Experiments demonstrating performance stability when N>1 source domains are included in the prompt context.

### Open Question 2
- Question: Can the pipeline be modified to rank items against the full target domain catalog rather than a limited candidate list?
- Basis in paper: Authors state that due to "token size limit of LLMs, it is difficult to include all candidates," necessitating a filtered candidate list.
- Why unresolved: Real-world recommender systems must score thousands of items; current method relies on pre-filtering or random sampling.
- What evidence would resolve it: A retrieval-augmented generation (RAG) approach or chunking strategy that successfully ranks relevant items from a corpus of 10,000+ items.

### Open Question 3
- Question: What specific prompt engineering or feature alignment techniques can mitigate performance degradation caused by large domain gaps?
- Basis in paper: Results show that introducing source history from a different subgroup negatively impacts performance compared to using no history at all.
- Why unresolved: Current method assumes shared features are beneficial but fails to identify when source data becomes irrelevant or distracting.
- What evidence would resolve it: A comparative study of feature-filtering mechanisms that results in positive transfer gains even when source and target domains are semantically distant.

## Limitations
- Performance depends heavily on semantic similarity between source and target domains
- Requires expensive proprietary models (GPT-4) for optimal performance
- Limited to ranking small candidate lists due to LLM context window constraints

## Confidence

- **High Confidence:** The positive correlation between LLM parameter size and performance is well-established through systematic ablation
- **Medium Confidence:** The semantic feature mapping mechanism assumes LLMs possess sufficient world knowledge about domain relationships
- **Medium Confidence:** The claim that domain-specific recommendation guidance improves effectiveness is supported by experimental design

## Next Checks

1. **Domain Gap Stress Test:** Systematically evaluate the pipeline across domain pairs with varying similarity scores to quantify the threshold at which negative transfer begins.

2. **Open-Source Model Scaling Study:** Conduct a controlled experiment varying parameter counts across multiple open-source models to precisely identify the capacity threshold where cross-domain reasoning emerges.

3. **Prompt Template Ablation:** Test multiple prompt formulations for the "Task domain adaptation" and "Recommendation Guidance" sections to determine sensitivity of performance to natural language instruction quality.