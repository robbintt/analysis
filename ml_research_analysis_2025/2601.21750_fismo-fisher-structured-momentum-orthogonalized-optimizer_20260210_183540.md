---
ver: rpa2
title: 'FISMO: Fisher-Structured Momentum-Orthogonalized Optimizer'
arxiv_id: '2601.21750'
source_url: https://arxiv.org/abs/2601.21750
tags:
- fismo
- muon
- optimizer
- have
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FISMO addresses the challenge of optimizing large-scale neural
  networks by introducing a Fisher-structured momentum-orthogonalized optimizer that
  balances geometric adaptivity with computational tractability. The core method reformulates
  optimizer updates as trust-region problems constrained by Kronecker-factored Fisher
  metrics, allowing structured preconditioning that adapts to local loss landscape
  geometry while maintaining computational efficiency through matrix-geometry preservation.
---

# FISMO: Fisher-Structured Momentum-Orthogonalized Optimizer

## Quick Facts
- arXiv ID: 2601.21750
- Source URL: https://arxiv.org/abs/2601.21750
- Reference count: 40
- One-line primary result: FISMO achieves superior training efficiency and final performance compared to AdamW, Muon, and Shampoo by balancing geometric adaptivity with computational tractability through Fisher-structured momentum-orthogonalized updates.

## Executive Summary
FISMO addresses the challenge of optimizing large-scale neural networks by introducing a Fisher-structured momentum-orthogonalized optimizer that balances geometric adaptivity with computational tractability. The core method reformulates optimizer updates as trust-region problems constrained by Kronecker-factored Fisher metrics, allowing structured preconditioning that adapts to local loss landscape geometry while maintaining computational efficiency through matrix-geometry preservation. The theoretical framework establishes an O(1/√T) convergence rate for expected squared gradient norm in stochastic nonconvex optimization, matching standard nonconvex rates while characterizing variance reduction through mini-batching. Empirical evaluation on language modeling and image classification benchmarks demonstrates that FISMO achieves superior training efficiency and final performance compared to established baselines including AdamW, Muon, and Shampoo, with the method showing improved training stability and smoother validation trajectories.

## Method Summary
FISMO implements a Fisher-structured optimizer that reformulates weight updates as trust-region problems constrained by Kronecker-factored Fisher metrics. The algorithm uses Gauss-Seidel alternating updates for preconditioners P and Q, applies gradient whitening, accumulates momentum in whitened space, and extracts orthogonal directions via approximate polar decomposition using Newton-Schulz iterations. The method achieves partial spectrum homogenization by maintaining a condition number between Adam's high anisotropy and Muon's rigid isotropy, preserving curvature information while enabling efficient orthogonalized updates.

## Key Results
- Achieves O(1/√T) convergence rate for expected squared gradient norm in stochastic nonconvex optimization
- Maintains optimal conditioning trade-off with condition number κ ≈ 10²–10³, avoiding Adam's pathological anisotropy and Muon's rigid isotropy
- Demonstrates superior training efficiency and final performance on GPT-2 language modeling and CIFAR-10 image classification compared to AdamW, Muon, and Shampoo baselines
- Shows improved training stability with smoother validation trajectories due to partial spectrum homogenization

## Why This Works (Mechanism)

### Mechanism 1: Fisher-Structured Trust Region with Kronecker Geometry
- Claim: Replacing Muon's uniform spectral constraint with a Fisher-metric trust region preserves curvature information while maintaining tractability.
- Mechanism: The update $\Delta W^* = -\eta P^{-1/2} \text{Polar}(\tilde{G}) Q^{-1/2}$ solves $\min \langle G, \Delta W \rangle_F$ subject to $\|P^{1/2} \Delta W Q^{1/2}\|_2 \leq \eta$, where $P, Q$ approximate row/column Fisher blocks. This allows anisotropic steps scaled by curvature rather than forcing uniform singular values.
- Core assumption: The Kronecker approximation $F_W \approx Q \otimes P$ captures sufficient geometric structure; mini-batch gradient statistics provide meaningful curvature estimates.
- Evidence anchors:
  - [abstract] "reformulates the optimizer update as a trust-region problem constrained by a Kronecker-factored Fisher metric"
  - [Section 4.1, Eq. 3] Trust-region formulation with $(P, Q)$-preconditioned spectral norm
  - [corpus] Related work on Muon convergence (arXiv:2601.19400) provides context for LMO-based optimizers but does not validate Fisher-structured variants
- Break condition: If preconditioners $P, Q$ become rank-deficient or drift faster than gradient statistics, the trust-region geometry may misalign with true curvature, potentially harming convergence.

### Mechanism 2: Gauss-Seidel Preconditioner Updates with Stabilization
- Claim: Sequential alternating updates of $P$ and $Q$ with EMA smoothing and trace normalization yield stable curvature estimates under stochastic gradients.
- Mechanism: Theorem 4.1 shows optimal $P^*(Q) = \frac{1}{n}\mathbb{E}[GQ^{-1}G^\top] + \frac{\mu \text{tr}(Q^{-1})}{n}I_m$. Algorithm 1 implements this via: (1) EMA smoothing $\hat{P}_t = \gamma P_{t-1} + (1-\gamma)L_t$, (2) trace normalization $\text{tr}(P_t) = m$ to fix scale non-identifiability, (3) $\mu$-damping to ensure positive definiteness. Gauss-Seidel (update $P_t$ then $Q_t$) outperforms Jacobi-style simultaneous updates.
- Core assumption: EMA decay $\gamma$ and damping $\mu$ are tuned such that preconditioner drift is bounded by step size (Lemma D.5 requires $1-\gamma \leq c_\gamma \eta$).
- Evidence anchors:
  - [Section 5.1] "Gauss–Seidel Preconditioner Updates" and "Preconditioner Stabilization"
  - [Algorithm 1, Steps 4-5] Explicit update rules with symmetrization and normalization
  - [corpus] No direct corpus validation for Gauss-Seidel vs. Jacobi in this context
- Break condition: If $\gamma$ is too small (rapid preconditioner changes) or $\mu$ is insufficient (near-singular estimates), Lemma D.2's boundedness condition $b = G^2/(mn\mu) < 1$ fails, destabilizing convergence.

### Mechanism 3: Partial Spectrum Homogenization via Whitened Polar Decomposition
- Claim: FISMO achieves an optimal conditioning trade-off ($\kappa \approx 10^2$–$10^3$) between Adam's pathological anisotropy and Muon's rigid isotropy.
- Mechanism: Gradients are whitened as $\tilde{G}_t = P_t^{-1/2} G_t Q_t^{-1/2}$, momentum accumulated in whitened space, then polar decomposition extracts the orthogonal direction. Unlike Muon's strict $\kappa=1$, FISMO's preconditioning partially compresses the spectrum while retaining relative curvature information. Figure 3 shows FISMO maintains $\kappa$ in a "sweet spot" throughout training.
- Core assumption: The Isotropic Curvature Model (Su, 2025) correctly predicts that optimal updates require partial—not complete—spectrum homogenization for realistic loss landscapes.
- Evidence anchors:
  - [Section 7, Figure 3] Condition number trajectories showing $\kappa_{\text{Adam}} \gg \kappa_{\text{FISMO}} > \kappa_{\text{Muon}}$
  - [Section 4.3, Theorem 4.2] Optimal update via polar factor of preconditioned gradient
  - [corpus] Corpus does not provide independent validation of the partial homogenization hypothesis
- Break condition: If the true loss curvature demands either more aggressive isotropy (rapid curvature transitions) or more preserved anisotropy (highly structured landscapes), FISMO's fixed conditioning regime may be suboptimal.

## Foundational Learning

- **Concept: Fisher Information Matrix and Natural Gradient**
  - Why needed here: FISMO's core idea is constraining updates under Fisher geometry; understanding $F(\theta) = \mathbb{E}[\nabla \log p_\theta \nabla \log p_\theta^\top]$ as a Riemannian metric explains why preconditioning adapts steps to curvature.
  - Quick check question: Why does natural gradient descent take larger steps along flat directions and smaller steps along steep ones?

- **Concept: Trust Region Methods and Linear Minimization Oracle (LMO)**
  - Why needed here: FISMO formulates updates as LMO problems (Eq. 1, 3); understanding trust-region geometry clarifies why spectral-norm constraints yield orthogonalized solutions.
  - Quick check question: What is the solution to $\min_{\|\Delta W\|_2 \leq \eta} \langle G, \Delta W \rangle_F$ and why does it involve the polar factor?

- **Concept: Polar Decomposition and Newton-Schulz Iteration**
  - Why needed here: FISMO uses Newton-Schulz to approximate $\text{Polar}(M) = UV^\top$ without full SVD; this is the computational workhorse enabling efficient orthogonalization.
  - Quick check question: Why does Newton-Schulz iteration converge to the orthogonal polar factor, and what matrix operations does it require?

## Architecture Onboarding

- **Component map:** Preconditioner Estimation -> Gradient Whitening -> Momentum Buffer -> Polar Decomposition -> Update Application
- **Critical path:** Preconditioner quality → whitening effectiveness → momentum direction quality. If $P, Q$ poorly approximate Fisher geometry, whitening distorts gradients, harming downstream orthogonalization.
- **Design tradeoffs:**
  - **EMA decay $\gamma$:** Higher $\gamma$ = more stable but slower adaptation to curvature changes. Lemma D.5 requires $1-\gamma \leq c_\gamma \eta$ for bounded drift.
  - **Damping $\mu$:** Larger $\mu$ = more regularization, ensuring positive definiteness but potentially over-smoothing curvature. Condition $b = G^2/(mn\mu) < 1$ must hold (Lemma D.2).
  - **Newton-Schulz iterations:** More iterations = better orthogonalization approximation but higher compute. Muon typically uses 3-5 iterations; FISMO likely similar.
- **Failure signatures:**
  - **Preconditioner explosion:** $\|P_t^{-1}\|_2$ or $\|Q_t^{-1}\|_2$ grows unboundedly → check $\mu$ is sufficiently large relative to gradient magnitude $G$.
  - **No convergence improvement over Muon:** May indicate Fisher approximation is poor for the architecture; consider increasing EMA responsiveness or checking matrix structure assumptions.
  - **Numerical instability in whitening:** $P_t$ or $Q_t$ losing positive definiteness → verify symmetrization is applied and $\mu > 0$.
- **First 3 experiments:**
  1. **Ablation on $\gamma$ and $\mu$:** Train a small transformer (e.g., GPT-2 124M) with FISMO, sweeping $\gamma \in \{0.9, 0.95, 0.99\}$ and $\mu \in \{10^{-4}, 10^{-3}, 10^{-2}\}$. Monitor preconditioner eigenvalue bounds and convergence rate vs. AdamW/Muon baselines.
  2. **Condition number trajectory validation:** Replicate Figure 3 on a different architecture (e.g., CNN on CIFAR-10) to verify that FISMO maintains $\kappa \in [10^2, 10^3]$ and correlates with smoother validation curves. Plot training/validation loss and accuracy side-by-side.
  3. **Gauss-Seidel vs. Jacobi comparison:** Modify Algorithm 1 to use Jacobi-style updates (compute $P_t, Q_t$ simultaneously from $P_{t-1}, Q_{t-1}$). Compare convergence speed, final performance, and memory usage to validate the claimed advantages of sequential updates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does FISMO maintain computational efficiency and convergence advantages when scaling to multi-billion parameter Large Language Models compared to AdamW or Muon?
- **Basis in paper:** [inferred] The Introduction emphasizes the challenge of training models with "billions of parameters," yet the Empirical Validation (Section 6) is restricted to a 124M-parameter GPT-2 and CIFAR-10.
- **Why unresolved:** The algorithm requires computing inverse square roots of preconditioners $P$ and $Q$ and sequential Gauss-Seidel updates (Algorithm 1), which introduce computational overhead and sequential dependencies that may become bottlenecks in large-scale distributed training setups.
- **What evidence would resolve it:** Benchmarks on models exceeding 1B parameters (e.g., Llama-scale) reporting wall-clock time per step, memory footprint, and throughput scaling across multiple GPUs/TPUs.

### Open Question 2
- **Question:** How should FISMO be theoretically generalized to handle 4D convolutional kernels while preserving the Kronecker-factored Fisher geometry assumptions?
- **Basis in paper:** [inferred] The theoretical framework (Section 4) strictly defines parameters as matrices $W \in \mathbb{R}^{m \times n}$, while the experiments apply the method to CNNs (SimpleDLA) without detailing the 4D-to-2D mapping strategy.
- **Why unresolved:** Simply reshaping convolutional tensors into matrices may violate the input/output statistical independence assumptions required for the Kronecker approximation in Theorem 4.1, potentially degrading the optimizer's geometric alignment in vision architectures.
- **What evidence would resolve it:** A formal derivation of the FISMO update for 4D tensors or an ablation study comparing performance on CNNs against optimizers like Shampoo or K-FAC that use explicit convolutional approximations.

### Open Question 3
- **Question:** Is the observed condition number range of $10^2–10^3$ universally optimal, or does the ideal degree of "partial spectrum homogenization" vary across architectures or training phases?
- **Basis in paper:** [inferred] Section 7 characterizes the stable spectral profile of FISMO as an "optimal conditioning trade-off" lying between Adam and Muon, but does not investigate if this specific range is a fixed emergent property or a tunable target.
- **Why unresolved:** While the paper argues that strict isotropy ($\kappa=1$) discards curvature, it does not establish if the condition number resulting from the default hyperparameters is the theoretical optimum for minimizing loss across diverse nonconvex landscapes.
- **What evidence would resolve it:** Studies analyzing the relationship between the damping factor $\mu$ and the resulting condition number, testing if explicitly tuning the condition number yields better convergence than the default behavior.

## Limitations
- Theoretical claims rely heavily on Kronecker-factored Fisher approximation accuracy, which may degrade for complex architectures
- Empirical validation limited to specific architectures (GPT-2 and SimpleDLA) without comprehensive ablation studies
- Hyperparameter values (EMA decay γ, damping µ, Newton-Schulz iterations) not specified, requiring extensive tuning for different tasks
- No independent validation of FISMO's performance or the partial spectrum homogenization hypothesis from external sources

## Confidence

**High confidence**: The theoretical framework for O(1/√T) convergence rate in stochastic nonconvex optimization; the core mechanism of Fisher-structured trust regions with Kronecker geometry.

**Medium confidence**: The empirical superiority of FISMO over AdamW, Muon, and Shampoo on language modeling and image classification; the claim that FISMO achieves an optimal conditioning trade-off between Adam's anisotropy and Muon's isotropy.

**Low confidence**: The broader applicability of FISMO across diverse neural architectures; the assertion that partial spectrum homogenization is universally optimal for realistic loss landscapes.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary γ, µ, and the number of Newton-Schulz iterations to identify the robust operating regime for FISMO across different architectures and tasks.

2. **Condition number correlation study**: Quantify the relationship between FISMO's maintained condition number (κ ∈ [10², 10³]) and training stability/optimization efficiency on a wider range of models (e.g., ResNets, LSTMs) and datasets.

3. **Fisher approximation fidelity**: Evaluate the accuracy of the Kronecker-factored approximation F_W ≈ Q ⊗ P for different layer types (MLPs, attention layers, convolutions) and quantify the impact of approximation errors on optimization performance.