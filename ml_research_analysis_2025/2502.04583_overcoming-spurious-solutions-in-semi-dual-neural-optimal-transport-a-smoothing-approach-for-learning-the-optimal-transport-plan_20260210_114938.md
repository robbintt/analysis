---
ver: rpa2
title: 'Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing
  Approach for Learning the Optimal Transport Plan'
arxiv_id: '2502.04583'
source_url: https://arxiv.org/abs/2502.04583
tags:
- transport
- optimal
- neural
- plan
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical issue in existing semi-dual neural
  optimal transport (SNOT) frameworks: spurious solutions that fail to recover the
  correct optimal transport map. The authors prove that a sufficient condition to
  avoid this problem is when the source distribution does not assign positive mass
  to sets of Hausdorff dimension at most d-1.'
---

# Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan

## Quick Facts
- arXiv ID: 2502.04583
- Source URL: https://arxiv.org/abs/2502.04583
- Reference count: 39
- Primary result: OTP model successfully recovers optimal transport plans where existing SNOT methods fail, particularly for one-to-many mappings and image-to-image translation tasks.

## Executive Summary
This paper addresses a critical issue in semi-dual neural optimal transport (SNOT) frameworks: spurious solutions that fail to recover the correct optimal transport map. The authors prove that when the source distribution doesn't assign mass to sets of Hausdorff dimension at most d-1, SNOT recovers the true OT map. When this condition isn't met, they propose a novel OTP model that learns both the OT map and OT plan by smoothing the source distribution with Gaussian noise, applying SNOT to recover the correct OT plan, then gradually restoring the original source distribution. The method achieves state-of-the-art performance on unpaired image-to-image translation, including successfully learning stochastic transport mappings in challenging cases like MNIST-to-colored MNIST colorization.

## Method Summary
The OTP model addresses spurious solutions in SNOT by smoothing the source distribution μ with Gaussian noise to create μ_ε = μ * N(0,εI), ensuring the smoothed distribution satisfies the sufficient condition for unique OT map recovery. The method applies SNOT to (μ_ε, ν) to recover the correct OT plan π*_ε, then gradually decreases ε through an annealing schedule while fine-tuning the transport network. The approach uses alternating updates: the potential network V_ϕ maximizes the semi-dual objective while the transport network T_θ minimizes the transport cost. The noise schedule progresses from ε_max to ε_min > 0, with variance-preserving variants used for image tasks to maintain signal scale.

## Key Results
- OTP successfully recovers optimal transport plans in synthetic cases where SNOT fails, particularly for one-to-many mappings and perpendicular distribution alignments
- The method achieves state-of-the-art performance on unpaired image-to-image translation tasks, including MNIST-to-colored MNIST colorization
- Ablation studies confirm that decreasing noise schedules outperform constant noise, preventing mode collapse and ensuring full target distribution coverage
- OTP learns stochastic transport mappings in cases where deterministic maps would fail, demonstrating superior flexibility

## Why This Works (Mechanism)

### Mechanism 1: Source Distribution Conditioning Eliminates Spurious Solutions
When the source distribution μ doesn't assign positive mass to measurable sets of Hausdorff dimension ≤ d-1, the max-min solution of SNOT recovers the true OT map. This condition ensures the c-subdifferential ∂_c(V^*)_c(x) is uniquely determined μ-almost surely. When V^* is differentiable μ-a.s., the minimization arg min_y[c(x,y) - V^*(y)] yields a unique y for almost every x, eliminating the ambiguity that allows spurious solutions.

### Mechanism 2: Distribution Smoothing Restores Uniqueness
Convolving the source distribution with Gaussian noise produces an absolutely continuous μ_ε that satisfies the sufficient condition. Gaussian convolution μ_ε = μ * N(0, εI) ensures μ_ε has positive density everywhere w.r.t. Lebesgue measure. This guarantees the c-subdifferential is unique μ_ε-a.s., so SNOT applied to (μ_ε, ν) recovers the correct OT plan π^*_ε.

### Mechanism 3: Gradual Noise Annealing Preserves Solution Quality
Progressively decreasing noise from ε_max to ε_min > 0 maintains convergence to π^* while providing training stability. Each noise level ε_k defines a valid OT problem with unique solution. The transport network T_θ and potential V_ϕ are progressively fine-tuned as ε_k ↘ ε_min, tracking the family of OT plans π^*_{ε_k}. The composition x → x_{ε_min} → T_θ(x_{ε_min}) approximates the true OT map.

## Foundational Learning

- **Concept: Monge vs Kantorovich OT Formulations**
  - Why needed here: The paper's core insight is that SNOT targets the Monge problem (deterministic map T), but spurious solutions arise when no unique Monge map exists. Understanding that Kantorovich's plan π(y|x) generalizes to stochastic mappings is essential.
  - Quick check question: Given source μ on a line segment and target ν on a perpendicular line segment, does a unique Monge map exist?

- **Concept: c-Transform and Semi-Dual Formulation**
  - Why needed here: The SNOT objective derives from the semi-dual form S(μ,ν) = sup_V[∫V^c dμ + ∫V dν]. The T_θ-parametrization T_θ(x) = arg min_y[c(x,y) - V_ϕ(y)] is the computational realization of the c-transform.
  - Quick check question: For quadratic cost, what is the relationship between V^c(x) and the convex conjugate?

- **Concept: Hausdorff Dimension and Measure-Theoretic Support**
  - Why needed here: The sufficient condition requires μ not charge sets of dimension ≤ d-1. Data concentrated on manifolds (lines, surfaces) violates this, explaining why image data often triggers spurious solutions.
  - Quick check question: Does a uniform distribution on the unit circle in R² satisfy or violate the dimension condition?

## Architecture Onboarding

- **Component map:** Sample x ~ μ, y ~ ν, z ~ N(0,I) → x̃ = x + √ε_k z → Update V_ϕ to maximize L_ϕ → Update T_θ to minimize L_θ → Decay ε_k

- **Critical path:**
  1. Sample x ~ μ, y ~ ν, z ~ N(0,I)
  2. Construct x̃ = x + √ε_k z (or VP variant)
  3. Update V_ϕ to maximize L_ϕ = -V_ϕ(T_θ(x̃)) + V_ϕ(y)
  4. Update T_θ to minimize L_θ = c(x̃, T_θ(x̃)) - V_ϕ(T_θ(x̃))
  5. Decay ε_k according to schedule

- **Design tradeoffs:**
  - ε_min selection: Lower values better approximate true OT but risk instability. Paper uses ε_min ∈ [0.05, 0.5] depending on task.
  - Inner loop iterations K_T: More T_θ updates per V_ϕ update improves inner optimization but slows training. Paper uses K_T ∈ {1, 10, 20}.
  - VP vs. additive noise: Variance-preserving (x̃ = √(1-ε_k)x + √ε_k z) maintains signal scale; used for image tasks.

- **Failure signatures:**
  - Mode collapse: Transport maps all source samples to a subset of target support (indicates ε too small or constant scheduling).
  - Target distribution error >> transport cost error: SNOT converged to spurious solution where T_θ minimizes cost but T_θ#μ ≠ ν.
  - Training divergence at low ε: Indicates ε_min too small; increase to restore stability.

- **First 3 experiments:**
  1. **2D Perpendicular case validation**: Replicate Example 1 with source on [-1,1]×{0} and target on {0}×[-1,1]. Verify OTM fails (T collapses to origin) while OTP recovers transport to target.
  2. **Noise schedule ablation**: Compare decreasing noise (ε_max=0.2→ε_min=0.05) vs. constant noise (ε=0.05) on d=64 synthetic data. Measure D_target to confirm mode collapse with constant scheduling.
  3. **MNIST→CMNIST stochastic mapping**: Train on grayscale-to-color task where each digit has 3 color variants. Confirm OTP learns stochastic π(y|x) mapping to all three colors, while OTM-s collapses to single color mode.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sufficient condition for the source distribution (not assigning positive mass to sets of Hausdorff dimension $\le d-1$) be refined into a necessary and sufficient condition for recovering the true OT Map in SNOT?
- Basis in paper: The authors state in the Conclusion: "our analysis provides a sufficient condition, rather than a necessary and sufficient one (Thm. 3.1), leaving room for further refinement in understanding the exact conditions under which spurious solutions occur."
- Why unresolved: The current theory guarantees success under the specific geometric constraint but does not fully characterize all scenarios (e.g., specific singular distributions) where the max-min solution might still succeed or fail.
- What evidence would resolve it: A theoretical proof establishing the equivalence between the condition and the unique recovery of the OT Map, or specific counter-examples showing failure/success on the boundary of the current assumptions.

### Open Question 2
- Question: Does the proposed OTP model converge to the optimal transport plan globally, rather than merely along a subsequence?
- Basis in paper: The Conclusion notes: "One limitation of our work is that our convergence theorem holds up to a subsequence (Thm. 4.1)."
- Why unresolved: Theoretical guarantees for the full sequence would require stricter controls on the trajectory of the transport map as the smoothing noise vanishes, which the current weak convergence analysis does not provide.
- What evidence would resolve it: A proof of global convergence for the sequence $\pi_k^\star$ or empirical studies demonstrating that the algorithm does not oscillate between multiple valid transport plans during the denoising process.

### Open Question 3
- Question: What is the optimal scheduling strategy for the noise level $\epsilon_k$ to balance training stability and convergence speed?
- Basis in paper: The method relies on a manually defined decreasing sequence $\{\epsilon_k\}$ (Algorithm 1). Appendix F.2 shows that a decreasing schedule outperforms constant noise, but the specific decay rate is chosen heuristically.
- Why unresolved: The paper demonstrates that the schedule matters but does not provide a theoretical or empirical optimization for the rate of decay relative to the complexity of the source/target distributions.
- What evidence would resolve it: An ablation study comparing various decay functions (e.g., linear vs. exponential vs. cosine) across diverse datasets to identify a robust or adaptive scheduling rule.

## Limitations
- The theoretical guarantee depends on source distributions not assigning mass to sets of Hausdorff dimension ≤ d-1, which is often violated in practice through manifold-structured data
- The smoothing approach's convergence proof assumes infinite capacity networks and perfect optimization at each noise level, which may not hold in finite-sample, finite-capacity settings
- Empirical evaluation focuses heavily on image-to-image translation tasks, leaving open questions about OTP's performance on non-image data with complex structure

## Confidence
- **High Confidence**: The spurious solution problem identification (Section 3) and the smoothing mechanism (Section 4.1) are theoretically grounded with clear proofs under stated assumptions.
- **Medium Confidence**: The annealing schedule and empirical superiority over baselines are well-supported, though the theoretical convergence rate for the finite-ε case remains open.
- **Low Confidence**: Claims about superiority on "challenging" tasks beyond the evaluated examples lack systematic ablation studies across diverse data modalities.

## Next Checks
1. **Hausdorff Dimension Ablation**: Test OTP on synthetic data with controlled Hausdorff dimension (line, circle, sphere in R³) to quantify the trade-off between dimension condition violations and smoothing requirements.
2. **Finite-Sample Convergence Analysis**: Evaluate OTP's transport cost and target distribution error as a function of batch size and network capacity to assess when the theoretical guarantees break down in practice.
3. **Cross-Domain Generalization**: Apply OTP to non-image tasks (e.g., tabular data, point clouds, or temporal sequences) to verify the method's broader applicability beyond the image domain focus.