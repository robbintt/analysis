---
ver: rpa2
title: 'TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large Language
  Models in Traditional Chinese Medicine'
arxiv_id: '2503.07041'
source_url: https://arxiv.org/abs/2503.07041
tags:
- medicine
- chinese
- clinical
- traditional
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces TCM-3CEval, a comprehensive benchmark designed
  to evaluate large language models (LLMs) in Traditional Chinese Medicine (TCM).
  The benchmark addresses the gap in standardized evaluation tools for TCM by assessing
  models across three critical dimensions: Core Knowledge mastery, Classical Literacy,
  and Clinical Decision-Making.'
---

# TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large Language Models in Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2503.07041
- Source URL: https://arxiv.org/abs/2503.07041
- Reference count: 40
- Primary result: Comprehensive benchmark reveals significant performance gaps between general and TCM-specialized models, with all models showing limitations in Meridian & Acupoint theory

## Executive Summary
This study introduces TCM-3CEval, a comprehensive benchmark designed to evaluate large language models (LLMs) in Traditional Chinese Medicine (TCM). The benchmark addresses the gap in standardized evaluation tools for TCM by assessing models across three critical dimensions: Core Knowledge mastery, Classical Literacy, and Clinical Decision-Making. By testing models on fundamental theories, classical texts, and clinical scenarios, TCM-3CEval provides a structured framework for measuring LLM performance in TCM. Results reveal that models with specialized TCM training, such as DeepSeek, significantly outperform general-purpose models, particularly in classical text interpretation and clinical reasoning. However, all models exhibit limitations in specialized areas like Meridian & Acupoint theory, highlighting gaps between current AI capabilities and clinical needs. TCM-3CEval sets a new standard for evaluating AI in culturally grounded medical domains, offering insights for optimizing LLMs in TCM and supporting its modernization and internationalization.

## Method Summary
TCM-3CEval is a 450-question benchmark divided into three dimensions: TCM-Exam (150 questions on core knowledge), TCM-LitQA (150 questions on classical text interpretation), and TCM-MRCD (150 questions on clinical decision-making). Each dimension contains five subdomains with 30-40 questions each. Questions use multiple-choice, fill-in-blank, short-answer, and case analysis formats. For robust evaluation, multiple-choice options are shuffled, and responses are only counted correct if the model consistently selects the same answer across all shuffles. Models tested include DeepSeek, GPT-4o, o1-mini, InternLM2.5, Llama3, Claude, and PULSE. The benchmark is available through Medbench's TCM track.

## Key Results
- DeepSeek, with specialized TCM training, outperforms general-purpose models by 50 percentage points in classical text interpretation
- All models show significant weaknesses in Meridian & Acupoint theory and Various TCM Schools subdomains
- Clinical Decision-Making scores lag behind Core Knowledge scores, indicating gaps in practical reasoning
- International models exhibit critical errors in cultural context interpretation, particularly with polysemous Chinese medical terminology

## Why This Works (Mechanism)

### Mechanism 1: Cultural-Linguistic Prior Alignment
- Claim: Models pre-trained on Chinese corpora with subsequent TCM domain fine-tuning perform better because they internalize TCM's unique conceptual framework.
- Mechanism: Chinese language pre-training establishes linguistic foundations; TCM domain fine-tuning then builds conceptual alignment with pattern-based reasoning (Yin-Yang, Five Elements, Zang-Fu organ systems) rather than linear diagnostic logic.
- Core assumption: TCM reasoning patterns are epistemologically distinct from Western medical reasoning and require culturally-grounded training data.
- Evidence anchors:
  - [abstract] "Models with Chinese linguistic and cultural priors perform better in classical text interpretation and clinical reasoning."
  - [section 4.2] "DeepSeek's superior performance in classical text interpretation (50 percentage points higher than general-purpose models) suggests successful internalization of TCM's unique 'way of knowing'"
  - [corpus] Tianyi paper confirms importance of specialized TCM training data and cultural context for effective TCM applications
- Break condition: If TCM reasoning could be effectively captured through general medical training without Chinese cultural context, this mechanism would not hold.

### Mechanism 2: Triaxial Competency Decomposition
- Claim: The three-dimensional framework (Core Knowledge, Classical Literacy, Clinical Decision-Making) captures partially independent competency dimensions that reveal distinct model weaknesses.
- Mechanism: Each dimension tests different cognitive demands—factual recall (Core Knowledge), classical text interpretation (Classical Literacy), and holistic pattern recognition for syndrome differentiation (Clinical Decision-Making).
- Core assumption: TCM competency requires distinct skills that do not fully transfer across dimensions.
- Evidence anchors:
  - [abstract] "Results show a performance hierarchy: all models have limitations in specialized subdomains like Meridian & Acupoint theory and Various TCM Schools"
  - [section 3.2] Shows differentiated performance patterns—models showed "relatively lower accuracy in Meridians & Acupoints compared to the other four aspects"
  - [corpus] MTCMB benchmark paper similarly employs multi-dimensional evaluation across knowledge, reasoning, and safety
- Break condition: If performance on one dimension reliably predicted performance on others, the triaxial structure would be redundant.

### Mechanism 3: Robustness Through Option Permutation
- Claim: Shuffling multiple-choice options and requiring consistent correct responses reduces positional bias and false positive accuracy.
- Mechanism: LLMs may exhibit positional biases toward certain option labels; requiring the same correct answer across shuffled presentations validates genuine comprehension over heuristic exploitation.
- Core assumption: Models may exploit positional or formatting patterns rather than demonstrating actual knowledge.
- Evidence anchors:
  - [section 2.3] "Only if all model outputs consistently point to the same correct answer is the response considered as passed"
  - [section 4.3] International models exhibited "critical errors in cultural context interpretation, including misunderstandings of polysemous Chinese medical terminology whose meanings shift based on classical text context"
  - [corpus] LingLanMiDian paper notes similar challenges with TCM terminology ambiguity and evaluation reliability
- Break condition: If models demonstrated zero positional bias, this robustness measure would add computational cost without benefit.

## Foundational Learning

- Concept: Syndrome Differentiation (辨证论治)
  - Why needed here: TCM's core diagnostic approach requires recognizing patterns across seemingly unrelated symptoms guided by theoretical constructs, not linear disease-symptom mapping.
  - Quick check question: Given fever, aversion to cold, and floating pulse, can you identify the syndrome pattern and explain the theoretical basis for this symptom cluster?

- Concept: Classical Text as Living Clinical Guide
  - Why needed here: Unlike modern medicine where classical texts are historical references, TCM practitioners actively derive contemporary treatment guidance from ancient texts like Huangdi Neijing and Shang Han Lun.
  - Quick check question: Why might a TCM practitioner consult the 3rd-century Shang Han Lun for a contemporary patient's treatment plan?

- Concept: Polysemous TCM Terminology
  - Why needed here: Terms like "Qi" (气) and "Zhi" (治) have context-dependent meanings across classical texts, creating disambiguation challenges for models.
  - Quick check question: How might the term "Qi" differ in meaning between a discussion of respiratory physiology versus pathogenic factors?

## Architecture Onboarding

- Component map:
  - Dataset: 450 questions across 3 dimensions (TCM-Exam: 150, TCM-LitQA: 150, TCM-MRCD: 150)
  - Each dimension contains 5 subdomains with 30-40 questions each
  - Question types: multiple-choice, fill-in-blank, short-answer, case analysis
  - Evaluation pipeline: Question presentation → Option permutation → Model inference → Consistency check → Accuracy aggregation

- Critical path:
  1. Load benchmark questions with metadata (dimension, subdomain, question type)
  2. For each question, generate N shuffled option variants
  3. Run inference on all variants; mark correct only if model selects same answer across all shuffles
  4. Aggregate accuracy by dimension, subdomain, and question type
  5. Compare against baseline models (DeepSeek, GPT-4o, InternLM, PULSE)

- Design tradeoffs:
  - Textbook-derived cases vs. real clinical EMRs: Paper explicitly notes need for real-world validation
  - Standardized multiple-choice vs. open-ended generation: Enables comparison but may miss nuanced reasoning
  - 450 questions balances coverage with evaluation cost

- Failure signatures:
  - Universal low accuracy in Meridian & Acupoint theory (noted weakness across all models)
  - Low accuracy in Various TCM Schools (consistently poorest subdomain)
  - Large gap between Core Knowledge and Clinical Decision-Making scores
  - Inconsistent responses across shuffled option variants indicating positional bias

- First 3 experiments:
  1. Baseline profiling: Run your model on full 450-question benchmark with option shuffling to establish dimensional and subdomain performance baseline
  2. Gap analysis: Compare subdomain performance vs. DeepSeek to identify highest-impact improvement targets (likely Meridians, TCM Schools)
  3. Cultural prior ablation: Test whether gains come from Chinese language pre-training vs. TCM-specific fine-tuning by comparing Chinese-general vs. Chinese-medical model variants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance on TCM-3CEval benchmark questions correlate with performance on real-world clinical electronic medical records in actual practice settings?
- Basis in paper: [explicit] "Study limitations include test sets primarily derived from literature cases, requiring future validation through real-world clinical electronic medical records."
- Why unresolved: The current benchmark uses textbook-derived questions and cases, not authentic clinical data; the relationship between benchmark accuracy and real clinical utility remains unverified.
- What evidence would resolve it: A comparative study evaluating the same LLMs on both TCM-3CEval questions and de-identified real clinical cases from TCM hospitals, with expert assessment of diagnostic/treatment appropriateness.

### Open Question 2
- Question: Can concept disambiguation mechanisms with expert annotation improve model comprehension of polysemous TCM terminology that shifts meaning based on classical text context?
- Basis in paper: [explicit] "Polysemous TCM terminology... remains challenging for model comprehension. We suggest implementing a concept disambiguation mechanism with expert annotation in future research."
- Why unresolved: Context-dependent meanings of TCM terms (e.g., "Qi Zhi" variations across classical texts) are not currently handled by any evaluated model architecture; no disambiguation approach has been tested.
- What evidence would resolve it: An intervention study comparing baseline model performance against models augmented with expert-annotated concept disambiguation modules on ambiguous TCM terminology questions.

### Open Question 3
- Question: What structured knowledge graph representations of TCM theoretical-conceptual interrelationships would best improve models' clinical reasoning capabilities in specialized subdomains like Meridian & Acupoint theory?
- Basis in paper: [inferred] The paper notes that "merely expanding corpus size with TCM-related texts creates factual knowledge but fails to develop the deeper clinical reasoning capabilities" and states "what appears missing is the integration of structured knowledge graphs that capture the complex interrelationships between TCM theoretical concepts and their clinical manifestations," with all models showing particular weakness in Meridian & Acupoint theory.
- Why unresolved: The exact structure, granularity, and content of knowledge graphs needed to support TCM-specific reasoning patterns has not been empirically determined.
- What evidence would resolve it: A controlled comparison of models trained with different knowledge graph architectures (varying in relationship types, ontology depth, classical-modern concept linkages) evaluated on the Meridian & Acupoint subdomain.

## Limitations
- The evaluation relies entirely on textbook-derived cases rather than real clinical EMRs, limiting external validity for actual clinical deployment
- The comparison is limited to a small set of existing models without exploring the full landscape of TCM-specialized models or fine-tuning strategies
- The claim that Chinese linguistic priors are essential for TCM reasoning lacks ablation studies isolating the effect of Chinese vs. medical vs. general pre-training

## Confidence
- High confidence: The benchmark successfully creates a structured triaxial evaluation framework with standardized questions across three dimensions
- Medium confidence: The performance gaps between Chinese-trained and international models reflect genuine differences in TCM capability rather than prompt engineering artifacts
- Low confidence: The specific claim that models with Chinese cultural priors "internalize" TCM's unique way of knowing, as this requires deeper qualitative analysis beyond multiple-choice accuracy

## Next Checks
1. Conduct real-world validation using actual TCM clinical EMRs to test whether benchmark performance correlates with clinical utility
2. Implement ablation studies comparing models with Chinese language pre-training, TCM medical training, and both to isolate which priors drive performance gains
3. Extend evaluation to include TCM-specialized models beyond the initial six tested, particularly those explicitly trained on TCM corpora