---
ver: rpa2
title: Reliable End-to-End Material Information Extraction from the Literature with
  Source-Tracked Multi-Stage Large Language Models
arxiv_id: '2510.05142'
source_url: https://arxiv.org/abs/2510.05142
tags:
- extraction
- materials
- information
- source
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of extracting comprehensive
  materials information from unstructured literature, focusing on the composition-processing-microstructure-property
  relationships essential for data-driven discovery. We propose a multi-stage information
  extraction pipeline powered by large language models, capturing 47 features across
  four categories.
---

# Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models

## Quick Facts
- arXiv ID: 2510.05142
- Source URL: https://arxiv.org/abs/2510.05142
- Authors: Xin Wang; Anshu Raj; Matthew Luebbe; Haiming Wen; Shuozhi Xu; Kun Lu
- Reference count: 40
- One-line primary result: Multi-stage extraction with source tracking achieves F1 scores of ~0.96 while reducing missed materials from 49 to 13 out of 396 materials.

## Executive Summary
This study addresses the challenge of extracting comprehensive materials information from unstructured literature, focusing on the composition-processing-microstructure-property relationships essential for data-driven discovery. We propose a multi-stage information extraction pipeline powered by large language models, capturing 47 features across four categories. The pipeline integrates iterative extraction with source tracking to enhance accuracy and reliability. Evaluations yielded F1 scores around 0.96 at both feature and tuple levels. Compared to single-pass extraction, our approach improved microstructure category F1 scores by 10.0% (feature level) and 13.7% (tuple level), reducing missed materials from 49 to 13 out of 396 materials. The pipeline enables scalable literature mining, producing databases with high precision, minimal omissions, and zero false positives.

## Method Summary
The method employs a four-stage pipeline using OpenAI's o3-mini model with reasoning_effort="high" to extract 47 structured features from materials science literature. Stage 1 identifies all materials and extracts composition and processing information. Stage 2 iteratively extracts microstructure details for each material using accumulated context. Stage 3 extracts properties using the same iterative approach. Stage 4 performs global validation across all materials. The system processes text extracted via PyMuPDF from PDFs, with each stage requiring the model to output source text excerpts for validation. The approach achieves high precision by requiring evidence for every extracted value and uses iterative processing to reduce cognitive load on the LLM.

## Key Results
- Feature-level F1 score: 0.959; tuple-level F1 score: 0.962
- Material miss rate reduced from 49 to 13 out of 396 materials (3.3%)
- Zero false positives achieved across the evaluation
- Microstructure category F1 scores improved by 10.0% (feature level) and 13.7% (tuple level) compared to single-pass extraction

## Why This Works (Mechanism)

### Mechanism 1
Sequential multi-stage extraction improves recall on complex hierarchical information while reducing cognitive load on the LLM. The pipeline decomposes extraction into four stages mirroring materials science paper structure—composition/processing first, then microstructure per material, then properties per material, then global validation. Each stage receives accumulated context from prior stages, allowing the model to focus attention on specific feature categories rather than tracking 47 features simultaneously across thousands of tokens.

### Mechanism 2
Source tracking (requiring the LLM to output original text snippets for each extracted value) improves precision by anchoring extractions to verifiable evidence and enabling retrospective validation. By forcing the model to retrieve and display source text for every value, the pipeline reduces hallucination, maintains entity identity across stages, and allows Stage 4 validation to cross-reference extracted values against original sources.

### Mechanism 3
Structured flexibility through "Additional" fields prevents information loss from rigid categorization while maintaining schema consistency. The schema defines primary fields plus catch-all "Other Composition," "Additional Processing," "Additional Microstructure," and "Additional Properties" fields, allowing capture of non-standard information without forcing data into inappropriate categories or discarding it entirely.

## Foundational Learning

- **Materials tetrahedron (composition-processing-microstructure-property relationships)**: Why needed here: The entire extraction schema and stage ordering depend on understanding that composition and processing determine microstructure, which governs properties. Without this, the hierarchical decomposition appears arbitrary. Quick check: Can you explain why microstructure extraction (Stage 2) should precede property extraction (Stage 3) rather than run in parallel?

- **LLM attention and context window limitations**: Why needed here: The paper explicitly attributes single-pass failures to "cognitive load and task complexity issues" and "attention mechanism limitations" when tracking multiple materials across thousands of words. Understanding this motivates the multi-stage decomposition. Quick check: Why would extracting 47 features for 5+ materials in a single pass cause more errors than extracting the same features across 4 sequential stages?

- **Tuple-level vs feature-level evaluation**: Why needed here: The paper evaluates both individual features and interdependent feature groups (tuples). Tuple evaluation is stricter—a tuple is correct only if ALL components match. This distinction is critical for interpreting the ~0.96 F1 scores correctly. Quick check: If a system correctly extracts precipitate type and size but misses volume fraction, would this count as a true positive, false positive, or false negative in tuple-level evaluation?

## Architecture Onboarding

- **Component map**: PyMuPDF text extraction -> Stage 1 material identification -> Stage 2 microstructure extraction (per material) -> Stage 3 property extraction (per material) -> Stage 4 global validation
- **Critical path**: Stage 1 material identification → Stage 2-3 per-material iteration → Stage 4 validation. Errors in Stage 1 (missed or hallucinated materials) cascade to all downstream stages and are penalized heavily (47 features counted as FN/FP per material error).
- **Design tradeoffs**: Accuracy vs cost (each stage requires re-submitting full article text), precision vs recall (multi-stage without source tracking improved recall but decreased precision), standardization vs completeness (rigid schema vs "Additional" fields).
- **Failure signatures**: Missed materials (13/396 = 3.3%), confusion errors (62.5% of property errors), inappropriate inference (12.5% of property errors), cascade losses without source tracking.
- **First 3 experiments**: 1) Reproduce three-pipeline comparison on 10-paper subset, 2) Ablate source tracking while keeping multi-stage decomposition, 3) Test generalization on different material class (e.g., polymers).

## Open Questions the Paper Calls Out

1. **Theoretical vs experimental data extraction**: How can LLM behavior be controlled to distinguish between experimental measurements and theoretical predictions, ensuring strict adherence to experimental data extraction without unauthorized theoretical modeling? The authors observed 12.5% of property extraction errors involved inappropriate theoretical modeling (e.g., applying linear addition strengthening models to derive values not experimentally measured).

2. **Multimodal integration**: How can multimodal capabilities be integrated to capture microstructure information embedded in images, which the current text-only pipeline misses? The current pipeline relies on text extraction and misses information embedded in figures and microscopy images.

3. **Cost-effective processing**: Can selective context windowing or incremental processing strategies achieve comparable extraction quality while reducing the current ~21,500 tokens per paper cost? The current multi-stage design requires inputting complete original text plus all previously extracted information at each stage.

4. **Completion bias**: How should extraction systems handle "completion bias"—the tendency for LLMs to treat unreported values as non-existent rather than explicitly unknown? When texts report partial phase fractions (e.g., "precipitate A comprises 20%"), models incorrectly assume the remainder rather than marking it unreported.

## Limitations
- The zero false positive claim lacks methodological detail on ground truth construction, making it difficult to assess whether borderline cases were handled consistently.
- The 47-feature schema, while comprehensive for precipitate-containing multi-principal element alloys, may not generalize to other material classes without modification.
- The study's evaluation relies on a ground truth constructed by multiple reviewers, but inter-reviewer agreement rates and individual reviewer accuracy are not reported.

## Confidence

- **High Confidence**: The multi-stage decomposition mechanism is well-supported by documented performance improvements and logical dependence of microstructure on composition/processing. The source tracking mechanism's role in reducing hallucination is strongly evidenced by precision improvements from 0.854 to 0.934 when source tracking is added.
- **Medium Confidence**: The claim that hierarchical information extraction improves recall is supported by the 10.0% microstructure F1 improvement, but this could be partially attributed to source tracking rather than decomposition alone.
- **Low Confidence**: The zero false positive claim lacks methodological detail on ground truth construction. The generalization claim to other material classes is unsupported by experimental evidence.

## Next Checks

1. **Ground Truth Quality Validation**: Reconstruct the evaluation by having three independent reviewers extract features from 10 randomly selected papers, calculating inter-reviewer agreement rates. Compare these to the reported pipeline F1 scores to assess ground truth reliability.

2. **Ablation of Source Tracking**: Implement the multi-stage pipeline without source tracking requirements on the full 100-paper corpus. Measure the change in precision/recall to isolate the source tracking mechanism's contribution from the multi-stage decomposition effect.

3. **Schema Transferability Test**: Apply the unmodified pipeline to a corpus of polymer or ceramic literature. Identify which of the 47 features transfer without modification and which require schema updates, quantifying the fraction of extractions that fall into the "Additional" fields.