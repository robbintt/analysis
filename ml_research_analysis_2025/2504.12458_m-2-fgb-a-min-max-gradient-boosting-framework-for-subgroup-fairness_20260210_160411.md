---
ver: rpa2
title: 'M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness'
arxiv_id: '2504.12458'
source_url: https://arxiv.org/abs/2504.12458
tags:
- fairness
- loss
- learning
- group
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces M\xB2FGB, a novel min-max gradient boosting\
  \ framework designed to address subgroup fairness in machine learning. The method\
  \ extends gradient boosting to optimize a min-max objective that maximizes the worst-group\
  \ utility while maintaining overall performance."
---

# M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness

## Quick Facts
- **arXiv ID**: 2504.12458
- **Source URL**: https://arxiv.org/abs/2504.12458
- **Reference count**: 40
- **Primary result**: Novel gradient boosting framework that optimizes a min-max objective for subgroup fairness, achieving competitive performance-fairness trade-offs

## Executive Summary
This paper introduces M²FGB, a min-max gradient boosting framework designed to address subgroup fairness in machine learning. The method extends gradient boosting by incorporating primal-dual optimization to maximize the worst-group utility while maintaining overall performance. Theoretical analysis demonstrates convergence under mild conditions, and empirical evaluation on four benchmark datasets shows that M²FGB achieves superior or competitive performance-fairness trade-offs compared to existing methods while being computationally efficient. The framework is flexible, supporting various fairness metrics and tasks including classification and regression.

## Method Summary
M²FGB is a gradient boosting framework that extends standard boosting by incorporating primal-dual optimization to address subgroup fairness. The algorithm iteratively updates dual variables (μ) associated with each subgroup via projected gradient ascent, then computes weighted gradients (Lagrangian gradients) to fit weak learners (decision trees). The framework uses differentiable proxy functions for non-differentiable fairness metrics, enabling gradient-based optimization. At each boosting round, the algorithm prioritizes the worst-performing subgroup by scaling their gradients, effectively focusing the model's learning capacity on improving performance for marginalized groups. The method supports both classification and regression tasks and can handle intersectional subgroups defined by multiple protected attributes.

## Key Results
- M²FGB achieves competitive or superior performance-fairness trade-offs compared to existing fairness-aware boosting methods
- The framework converges under mild conditions (convex, K-smooth loss functions) as proven in theoretical analysis
- Empirical evaluation on four benchmark datasets (German Credit, COMPAS, ENEM, ACSIncome) demonstrates effectiveness across diverse fairness metrics
- Computational efficiency is maintained relative to standard gradient boosting approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating primal-dual optimization into gradient boosting iterations allows the model to dynamically prioritize the worst-performing subgroup without training separate models
- Mechanism: The framework treats min-max fairness as a constrained optimization task, performing gradient ascent on dual variables (weights) associated with each subgroup at each boosting round. These weights scale the gradients of the loss function, forcing the next weak learner to focus more heavily on correcting errors for high-loss groups
- Core assumption: Loss functions and fairness metrics must be convex and smooth (K-smooth) to guarantee convergence, or suitable differentiable proxy functions must exist
- Evidence anchors:
  - [abstract] "The optimization process explored the primal-dual problems at each boosting round"
  - [section 4] "At each boosting iteration, μ moves in the direction of the gradient... while f is updated in the negative direction of the Lagrangian gradient"
- Break condition: Convergence fails if learning rates for primal (γ) and dual (ζ) steps are not appropriately small, preventing the duality gap from closing

### Mechanism 2
- Claim: Optimizing for min-max fairness avoids the "unnecessary harm" pitfall common in disparity-reduction methods
- Mechanism: The objective function minimizes maximum group-level loss (Rawlsian approach), seeking to raise the floor of performance rather than equalizing by degrading privileged groups
- Core assumption: Stakeholders value raising absolute performance of worst-off groups over strictly minimizing mathematical variance between groups
- Evidence anchors:
  - [section 1] "This is a common situation where e4 might be the best loss possible... the second classifier will not be evaluated as an improvement... in the min-max setting"
  - [section 4] "Theorem 1 (No unnecessary harm)... the min-max solution always dominates the predictor with equal losses"
- Break condition: If worst-performing group data is fundamentally limited or noisy, strictly optimizing for that group may cause overfitting and degrade generalization

### Mechanism 3
- Claim: Using differentiable proxy functions for non-differentiable fairness metrics enables gradient-based optimization while still improving target metrics
- Mechanism: Standard fairness metrics often rely on discrete counts (non-differentiable). M²FGB substitutes these with differentiable surrogates (e.g., cross-entropy loss restricted to specific groups) to compute gradients, empirically showing that minimizing proxy correlates with improvements in actual metric
- Core assumption: Proxy function is sufficiently accurate stand-in for true metric such that gradient descent on proxy translates to gains in true metric
- Evidence anchors:
  - [section 4.1] "M²FGB needs that Lz is differentiable... we present diverse proxy functions"
  - [section 5.1] "M²FGB optimization will increase the weights μz for the groups that have the worse performance... improvement obtained... is also observed in the TP rate"
- Break condition: Mechanism may fail if proxy function creates landscape where local minimum for proxy doesn't align with local minimum for actual fairness metric

## Foundational Learning

- **Primal-Dual Optimization (Lagrangian Multipliers)**: Mathematical engine of M²FGB. Explains why algorithm maintains set of weights (μ) alongside model weights and how constraints (fairness) are converted to penalties in loss function
  - Quick check question: If constraint "Loss for Group A must be < ϵ" is violated, should the dual variable μ for Group A increase or decrease?

- **Gradient Boosting Descent**: M²FGB modifies standard boosting. Understanding that boosting fits new trees to residuals (gradients) of previous ensemble reveals how M²FGB warps these residuals to prioritize specific groups
  - Quick check question: In standard gradient boosting, target for next tree is negative gradient of loss. In M²FGB, how does dual variable μ alter this target?

- **Subgroup Fairness vs. Group Fairness**: Paper explicitly addresses "gerrymandering" fairness issues where model might be fair on gender and race separately but unfair at intersections (e.g., "Black Women")
  - Quick check question: Why would model that is fair for "All Women" and fair for "All Black People" potentially be unfair specifically to "Black Women"?

## Architecture Onboarding

- **Component map**: Input (X, Y, Z) -> Initialization (Base predictor, μ uniform) -> Primal-Dual Loop (T iterations) -> Output (Final ensemble model f)
  - Dual Step: Calculate loss per group -> Update μ via projected gradient ascent onto simplex
  - Primal Step: Compute weighted gradients (Lagrangian gradient) -> Fit weak learner (tree) -> Update ensemble
- **Critical path**: Lagrangian Gradient Computation. Where standard gradient of loss is scaled by dual variables. If calculation is incorrect, model won't prioritize worst-group and reverts to standard risk minimization
- **Design tradeoffs**:
  - λ (Fairness Strength): High λ maximizes worst-group utility but risks overfitting to small subgroups and hurting overall accuracy. Low λ acts like standard boosting
  - Proxy vs. True Metric: Optimizing proxy is computationally feasible but approximation; monitoring true non-differentiable metric is required for validation
- **Failure signatures**:
  - Degenerate Solutions: In "Positive Rate" optimization, model may learn to simply predict positive class for everyone (WG Pos Rate = 1.0, Accuracy low)
  - Overfitting Small Groups: Validation loss increases while training loss for worst group decreases significantly (high variance in small subgroups)
  - Divergence: If dual learning rate (ζ) is too high, weights μ may oscillate rather than settle on stable weighting for groups
- **First 3 experiments**:
  1. Lambda Sweep: Run M²FGB with λ ∈ [0.0, 0.2, 0.5, 0.8, 1.0] on validation set to visualize Pareto frontier between Overall Accuracy and Worst-Group Accuracy
  2. Proxy Correlation Check: During training, log both differentiable "TP Loss" and discrete "TP Rate." Plot to verify as proxy loss decreases, actual metric increases (validating proxy assumption)
  3. Comparison vs. Disparity Method: Compare M²FGB against disparity-based method (like FairGBM) specifically looking for "unnecessary harm"—check if privileged group's performance drops significantly in baseline compared to M²FGB

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can M²FGB framework be modified or regularized to explicitly prevent overfitting on worst-group loss, particularly when subgroup sample size is small?
- Basis in paper: [explicit] Authors state in Discussion that increased number of iterations can lead to overfitting, noting "It is important to tune M²FGB hyperparameters, allowing it to learn more rounds with simple weak learners," but don't offer mechanism within optimization to handle small sample size of worst-performing group directly
- Why unresolved: While standard gradient boosting prevents overfitting through regularization (shrinkage, depth limits), min-max objective explicitly focuses on potentially small subset of data (worst group), which statistically increases variance and overfitting risk without specific countermeasures
- What evidence would resolve it: Theoretical analysis or empirical study showing specific regularization term (e.g., group-specific regularization) or early stopping criterion based on worst-group validation loss effectively closes generalization gap

### Open Question 2
- Question: To what extent does choice of differentiable proxy function (e.g., cross-entropy for accuracy) bias optimization landscape compared to true non-differentiable fairness metric?
- Basis in paper: [inferred] Paper notes in Methodology and Discussion sections that M