---
ver: rpa2
title: 'TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers'
arxiv_id: '2502.04056'
source_url: https://arxiv.org/abs/2502.04056
tags:
- quantization
- diffusion
- tq-dit
- performance
- dits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenges of diffusion
  transformers (DiTs) by proposing an efficient post-training quantization method.
  The core approach introduces time-grouping quantization (TGQ) to handle timestep-dependent
  activation variance, multi-region quantization (MRQ) to address asymmetric activation
  distributions, and Hessian-guided optimization (HO) to improve quantization accuracy.
---

# TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers

## Quick Facts
- arXiv ID: 2502.04056
- Source URL: https://arxiv.org/abs/2502.04056
- Authors: Younghye Hwang; Hyojin Lee; Joonhyuk Kang
- Reference count: 33
- Primary result: Achieves 0.29 FID increase at 8-bit quantization with 45.4% memory reduction and 89.3% faster calibration

## Executive Summary
This paper addresses the computational challenges of diffusion transformers (DiTs) by proposing an efficient post-training quantization method. The core approach introduces time-grouping quantization (TGQ) to handle timestep-dependent activation variance, multi-region quantization (MRQ) to address asymmetric activation distributions, and Hessian-guided optimization (HO) to improve quantization accuracy. The proposed TQ-DiT framework achieves near-full-precision performance with 0.29 FID increase at 8-bit quantization and significantly outperforms baselines at 6-bit precision. Additionally, it reduces GPU memory usage by 45.4% and calibration time by 89.3% compared to existing methods, supporting more sustainable AI deployment.

## Method Summary
TQ-DiT is a three-phase post-training quantization framework for DiTs. Phase 1 partitions timesteps into G contiguous groups and samples n calibration images per group. Phase 2 performs forward/backward passes to compute layer outputs and gradients. Phase 3 applies time-aware quantization using TGQ for timestep-specific parameter groups, MRQ for asymmetric distributions (splitting post-softmax/post-GELU ranges), and HO that minimizes weighted quantization error using diagonal Fisher information. The method optimizes CNN/linear layers with HO and MRQ, and MatMul layers with TGQ+MRQ for post-softmax inputs.

## Key Results
- Achieves 0.29 FID increase at W8A8 quantization compared to full precision
- Outperforms PTQ4DiT by 3.85 FID at W6A6 quantization
- Reduces GPU memory by 45.4% and calibration time by 89.3% versus baselines

## Why This Works (Mechanism)

### Mechanism 1: Time-Grouping Quantization (TGQ)
Grouping timesteps into G contiguous groups with per-group quantization parameters reduces temporal activation variance errors. By sampling n calibration points per group and optimizing step sizes and zero points separately, TGQ captures timestep-specific activation distributions rather than using global parameters.

### Mechanism 2: Multi-Region Quantization (MRQ)
Allocating separate scaling parameters to asymmetric distribution sub-regions improves quantization accuracy for non-uniform activations. For post-softmax (concentrated near [0,1]), range splits into two regions with distinct step sizes. For post-GELU (negative skew), separate handling of positive/negative values reduces quantization error.

### Mechanism 3: Hessian-Guided Optimization (HO)
Using diagonal Fisher information matrix to approximate pre-activation Hessian weights quantization errors by their impact on task loss. This emphasizes quantization errors at layers with higher gradient magnitudes, improving overall generation quality through second-order sensitivity analysis.

## Foundational Learning

- **Diffusion Models (DDPM)**: Why needed - DiTs implement the denoising network ε_θ(x_t, t) that TGQ must optimize across timesteps. Quick check - Can you explain why the noise prediction network shares weights across all timesteps, and how this creates the temporal variance challenge?

- **Uniform Quantization (k-bit asymmetric)**: Why needed - MRQ extends uniform quantization by partitioning the range. Quick check - Given values in [0.01, 0.95] for 4-bit quantization, what percentage of quantization levels are wasted if using full-range uniform quantization?

- **Fisher Information Matrix (diagonal approximation)**: Why needed - HO uses diagonal FIM to weight quantization errors. Quick check - Why does the diagonal approximation discard inter-layer correlations, and when might this be problematic?

## Architecture Onboarding

- **Component map**: Calibration Dataset Generation -> Layer-Wise Computation -> Time-Aware Quantization (per layer)
- **Critical path**: Post-softmax MatMul layers require all three mechanisms (HO + MRQ + TGQ). Errors here cascade through attention. Post-GELU layers use HO + MRQ only.
- **Design tradeoffs**: More groups (G↑) → better temporal coverage but more parameters/calibration time; More calibration samples (n↑) → better optimization but higher memory; Lower bits (k↓) → more compression but MRQ becomes critical
- **Failure signatures**: FID spike at specific timestep ranges → group boundaries misaligned; Systematic under-reconstruction of small activations → MRQ region boundary too high; HO diverges → gradient explosion
- **First 3 experiments**:
  1. Baseline sanity check: Run TQ-DiT at W8A8 on DiT-XL-2 with T=250, G=10, n=32. Verify FID < 5.5
  2. Ablation validation: Disable TGQ (set G=1) at W6A6. Confirm FID degrades significantly
  3. Efficiency benchmark: Compare GPU memory and calibration time against PTQ4DiT with identical settings

## Open Questions the Paper Calls Out

- How can the time-grouping quantization strategy be adapted for video generation tasks where temporal dynamics involve both diffusion timesteps and video frame sequences?
- Can adaptive calibration techniques be developed for TQ-DiT to support real-time applications without requiring fixed pre-calibration groups?
- To what extent does TQ-DiT mitigate performance degradation at 4-bit quantization (W4A4) compared to the reported 6-bit and 8-bit results?

## Limitations
- TGQ effectiveness depends on timestep grouping strategy that may not align with actual distribution shifts
- MRQ assumes stable asymmetric distributions that may vary during inference
- HO optimization quality depends on gradient quality and diagonal approximation validity

## Confidence

- **High confidence**: Computational efficiency claims (45.4% memory reduction, 89.3% calibration time reduction)
- **Medium confidence**: TGQ mechanism effectiveness relies on assumptions about temporal correlation within groups
- **Medium confidence**: MRQ effectiveness assumes stable asymmetric distributions
- **Medium confidence**: HO optimization quality depends on gradient quality and diagonal approximation

## Next Checks
1. Measure actual activation distribution shifts across timesteps in DiT-XL-2 to verify TGQ group boundaries capture temporal variance patterns effectively
2. Apply TQ-DiT to different DiT variants (DiT-Small, DiT-Large) and datasets (LSUN, CelebA) to assess calibration stability and parameter sensitivity
3. Systematically induce distribution shifts (out-of-distribution inputs, temperature scaling) to identify conditions where MRQ region boundaries fail and HO optimization breaks down