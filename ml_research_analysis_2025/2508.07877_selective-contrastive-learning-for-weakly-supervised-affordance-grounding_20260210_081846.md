---
ver: rpa2
title: Selective Contrastive Learning for Weakly Supervised Affordance Grounding
arxiv_id: '2508.07877'
source_url: https://arxiv.org/abs/2508.07877
tags:
- learning
- object
- images
- parts
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses weakly supervised affordance grounding, aiming
  to identify functional object parts relevant to specific actions using only image-level
  labels and action text prompts. The core method introduces selective prototypical
  and pixel contrastive learning that adaptively leverages affordance-relevant cues
  at both part and object levels, depending on the granularity of available information.
---

# Selective Contrastive Learning for Weakly Supervised Affordance Grounding

## Quick Facts
- **arXiv ID:** 2508.07877
- **Source URL:** https://arxiv.org/abs/2508.07877
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art KLD scores of 1.124 (seen) and 1.243 (unseen) on AGD20K, outperforming baselines by significant margins.

## Executive Summary
This paper addresses weakly supervised affordance grounding, aiming to identify functional object parts relevant to specific actions using only image-level labels and action text prompts. The core method introduces selective prototypical and pixel contrastive learning that adaptively leverages affordance-relevant cues at both part and object levels, depending on the granularity of available information. Object-level clues are first discovered using CLIP, then refined to part-level clues. Two contrastive objectives—prototypical (for object/part-level alignment) and pixel (for fine-grained localization)—are applied selectively based on the reliability of discovered parts. The approach is evaluated on AGD20K and HICO-IIF datasets, achieving state-of-the-art performance with KLD scores of 1.124 (seen) and 1.243 (unseen) on AGD20K, and demonstrating significant improvements in challenging unseen scenarios that reflect real-world conditions.

## Method Summary
The method employs a dual-view framework using egocentric and exocentric images. CLIP generates Object Affinity Maps to identify objects in exocentric views, which are then refined through K-means clustering to discover part-level prototypes. A reliability assessment using pIoU thresholds determines whether to apply part-level or object-level contrastive learning. The model trains with three losses: classification loss (L_ce), prototypical contrastive loss (L_proto) for semantic separation, and pixel contrastive loss (L_pix) for fine-grained localization. During inference, CAMs are calibrated using binarized Object Affinity Maps to produce precise affordance heatmaps.

## Key Results
- Achieves KLD scores of 1.124 (seen) and 1.243 (unseen) on AGD20K, outperforming state-of-the-art methods
- Demonstrates significant improvements on HICO-IIF dataset with 1.186 KLD (seen) and 1.196 KLD (unseen)
- Ablation studies confirm the effectiveness of selective contrastive learning, with selective object-level learning showing particular benefit for unseen data
- Outperforms WSCD, a distillation-based method, by large margins across all metrics

## Why This Works (Mechanism)

### Mechanism 1: Selective Granularity Switching
The model dynamically switches between object-level and part-level supervision based on reliability assessment. It generates candidate part prototypes from exocentric images and evaluates them using pIoU thresholds against DINO attention maps. If parts are unreliable, the system defaults to object-level contrastive learning, preventing overfitting to spurious features.

### Mechanism 2: Cross-View Activation Thresholding
Exploits CLIP's stronger activation on salient, unoccluded objects in egocentric views compared to exocentric views. Sets pixel-inclusion thresholds based on minimum maximum activations across paired exocentric images, enabling unsupervised isolation of affordance-relevant pixels.

### Mechanism 3: Multi-Class Prototypical Contrast
Structures contrastive learning around prototypes rather than pairwise distillation. Constructs positive prototypes (affordance regions) and negative prototypes (background/other classes) within batches, enforcing better semantic separation and more discriminative representations.

## Foundational Learning

- **Concept: Weakly Supervised Affordance Grounding (WSAG)**
  - Why needed: Core task requiring localization without pixel-level masks, only image-level action labels and context images
  - Quick check: How does the model know where to look without explicit mask supervision? (Answer: Uses CAM and CLIP heatmaps as proxy supervision)

- **Concept: Class Activation Mapping (CAM)**
  - Why needed: Primary localization tool that creates "shortcut learning" problem by highlighting object identity rather than interaction potential
  - Quick check: Why is standard CAM insufficient for affordance grounding? (Answer: It focuses on object identity rather than interaction potential)

- **Concept: Contrastive Learning**
  - Why needed: Replaces distillation with contrastive losses that pull positive pairs and push negative pairs
  - Quick check: What serves as the "negative" in prototypical contrastive loss? (Answer: Background regions and regions belonging to different action classes)

## Architecture Onboarding

- **Component map:** DINO (feature extraction) -> CLIP (Object Affinity Map) -> Discovery Modules (K-means + Thresholding) -> Projection MLPs -> Loss Aggregation (L_ce + L_proto + L_pix)

- **Critical path:** Input (Egocentric + Exocentric images) -> CLIP Module (generates Object Affinity Maps) -> Discovery (finds reliable parts or falls back to object level) -> Training (computes L_proto and L_pix based on discovered reliability) -> Inference (calibrates CAM using binarized Object Affinity Map)

- **Design tradeoffs:** Threshold α (Reliability) - high values ensure quality but reduce training signal; Threshold ρ (Pixel selection) - robust to noise but assumes consistent camera scaling

- **Failure signatures:** "Blob" predictions (model highlights whole object) -> Check if α is too high; No activation (predicts background) -> Check if ρ calculation is incorrect

- **First 3 experiments:** 1) Sanity Check: Run on single image pair to verify part discovery; 2) Ablation: Disable selective logic, compare object-only vs part-only learning; 3) Hyperparameter Sweep: Vary α and γ on validation set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved:
1. How to make the reliability assessment adaptive rather than relying on fixed thresholds
2. Impact of CLIP's salience bias in cluttered environments
3. Treatment of multi-affordance parts in prototypical contrastive learning

## Limitations
- Fixed thresholds (α=0.6, γ=0.6) may not generalize across datasets with varying object scales and part granularities
- Assumes consistent camera scaling between egocentric and exocentric views for cross-view activation thresholding
- Underspecified MLP architecture dimensions create potential reproducibility gaps

## Confidence
- **High Confidence:** Core contrastive learning framework and integration with CLIP/DINO backbones
- **Medium Confidence:** Selective gating mechanism's effectiveness across diverse scenarios
- **Low Confidence:** Part discovery pipeline's reliability with highly occluded or small objects

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary α and γ across validation set to determine optimal values and assess robustness
2. **Cross-View Consistency Test:** Evaluate performance when exocentric images have varying quality and scale
3. **Part Discovery Reliability Audit:** Visualize and quantify success rate across different object categories and occlusion levels