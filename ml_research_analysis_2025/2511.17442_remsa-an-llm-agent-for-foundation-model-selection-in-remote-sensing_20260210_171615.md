---
ver: rpa2
title: 'REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing'
arxiv_id: '2511.17442'
source_url: https://arxiv.org/abs/2511.17442
tags:
- data
- selection
- string
- constraints
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REMSA, the first LLM-based agent for automated
  foundation model selection in remote sensing. The authors build a structured database
  of over 150 remote sensing foundation models and develop an agent that interprets
  natural language queries, retrieves candidate models via dense retrieval, ranks
  them using in-context learning, and provides transparent explanations.
---

# REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing

## Quick Facts
- arXiv ID: 2511.17442
- Source URL: https://arxiv.org/abs/2511.17442
- Authors: Binger Chen; Tacettin Emre Bök; Behnood Rasti; Volker Markl; Begüm Demir
- Reference count: 40
- Key outcome: REMSA outperforms baselines across multiple metrics on 75 expert-verified queries

## Executive Summary
This paper introduces REMSA, the first LLM-based agent for automated foundation model selection in remote sensing. The system builds on a structured database of over 150 remote sensing foundation models and uses natural language query interpretation, dense retrieval, in-context learning ranking, and iterative clarification to recommend appropriate models. Evaluated on a benchmark of 75 expert-verified queries producing 900 model-query configurations, REMSA achieves state-of-the-art performance across multiple metrics.

## Method Summary
REMSA operates through an LLM agent core with an Interpreter and Task Orchestrator, supported by external tools including a Retrieval Tool (SBERT+FAISS), Ranking Tool (rule-based filtering plus in-context LLM ranking), Clarification Generator, and Explanation Generator. The system uses a structured database (RS-FMD) encoding each foundation model into typed metadata fields. When processing a query, the orchestrator determines whether clarification is needed, retrieves candidates via dense retrieval, applies hard constraint filtering, ranks remaining candidates using few-shot LLM examples, and provides explanations. The system includes memory augmentation for personalized selection and operates entirely on public metadata.

## Key Results
- REMSA achieves 75.76 average top-1 score vs. 67.37 for DB-RETRIEVAL baseline
- REMSA attains 22.67% hit rate and 0.38 MRR across benchmark queries
- System processes queries in 31.7s on average vs. 0.77s for retrieval-only baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured schema grounding improves retrieval relevance over unstructured text search.
- **Mechanism:** RS-FMD encodes each foundation model into typed metadata fields (e.g., `[APPLICATION]`, `[MODALITY]`), then Sentence-BERT embeds these prefixed fields. FAISS retrieves candidates via cosine similarity.
- **Core assumption:** The schema captures decision-relevant properties that users implicitly query.
- **Evidence anchors:** [Section 3] "Each metadata field is prefixed with a type-indicator (e.g., [APPLICATION], [MODALITY]) before encoding." [Table 4] Complete schema with 40+ fields across model, pretraining, and benchmark substructures. [Corpus] Neighbor paper "Towards Efficient Benchmarking of Foundation Models in Remote Sensing" addresses capability encoding but not automated selection—REMSA fills this gap.
- **Break condition:** If user constraints span properties not represented in the schema (e.g., specific license terms), retrieval will miss relevant models.

### Mechanism 2
- **Claim:** Hybrid rule-based + in-context LLM ranking outperforms retrieval-only or pure LLM baselines.
- **Mechanism:** Hard constraints (modality, sensor) are enforced deterministically; remaining candidates are re-ranked by an LLM prompted with structured query + metadata + few-shot ranking examples.
- **Core assumption:** LLM can generalize ranking behavior from few-shot examples without fine-tuning.
- **Evidence anchors:** [Section 6.1] REMSA achieves 75.76 Avg Top-1 Score vs. DB-RETRIEVAL at 67.37 and UNSTR.-RAG at 71.23. [Section 4.2] "Rule-Based Filtering eliminates candidates that violate hard constraints... In-Context LLM Ranking returns an ordered list with brief justifications." [Corpus] No corpus papers evaluate LLM-based ranking for FM selection; REMSA is novel in this domain.
- **Break condition:** If candidate models lack clear metadata for comparison dimensions (e.g., missing benchmark results), LLM ranking degrades to surface-level heuristics.

### Mechanism 3
- **Claim:** Iterative clarification with confidence-gated orchestration improves constraint coverage without user fatigue.
- **Mechanism:** Task Orchestrator monitors constraint completeness and ranking confidence. If below thresholds, it invokes Clarification Generator for missing fields (max 3 rounds). User responses merge into evolving query.
- **Core assumption:** Users can articulate constraints when prompted; 3 rounds suffices for most cases.
- **Evidence anchors:** [Section 4.1] "If there are too many candidates or if ranking results yield low confidence scores, the orchestrator calls the Clarification Generator Tool." [Algorithm 1] Explicit loop with `ClarifyCounter < MaxClarify` and confidence threshold checks. [Corpus] GeoLLM-Squad paper uses multi-agent orchestration but targets task execution, not model selection—REMSA adapts orchestration to selection workflows.
- **Break condition:** If users cannot answer clarification questions accurately (e.g., unsure of their compute budget), the system may converge on suboptimal recommendations.

## Foundational Learning

- **Concept: Dense Retrieval / RAG**
  - **Why needed here:** REMSA's Retrieval Tool uses Sentence-BERT + FAISS to find candidate models; understanding embedding-based search is prerequisite to debugging retrieval quality.
  - **Quick check question:** Can you explain why prefixing metadata fields with `[MODALITY]` before embedding might improve retrieval over raw text?

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The Ranking Tool uses few-shot examples in the LLM prompt to guide ranking behavior without training.
  - **Quick check question:** What happens to ICL performance if the few-shot examples don't cover the user's application domain?

- **Concept: Agentic Orchestration**
  - **Why needed here:** Task Orchestrator decides which tool to invoke based on task state—understanding control flow is essential for extending the system.
  - **Quick check question:** In Algorithm 1, what triggers a return to line 3 (restart with clarified query)?

## Architecture Onboarding

- **Component map:** LLM Agent Core (Interpreter + Task Orchestrator) -> External Tools (Retrieval Tool + Ranking Tool + Clarification Generator + Explanation Generator) -> Knowledge Base (RS-FMD) -> Memory (Vector DB)

- **Critical path:**
  1. User query → Interpreter → Structured constraints
  2. Task Orchestrator → Retrieval Tool → Candidate set
  3. Rule-based filter → Pruned candidates
  4. Ranking Tool (LLM) → Scored list
  5. Confidence check → Clarification loop OR Explanation output

- **Design tradeoffs:**
  - Latency vs. accuracy: REMSA takes 31.7s/query vs. DB-Retrieval at 0.77s (Section 6.1)
  - Schema completeness vs. maintenance burden: 40+ fields require extraction and verification
  - Clarification rounds vs. user patience: Capped at 3 to avoid fatigue

- **Failure signatures:**
  - Empty candidate set after filtering → triggers "closest-match" fallback
  - Low confidence scores after ranking → triggers clarification round
  - Missing mandatory fields → blocks retrieval until clarified or max rounds reached

- **First 3 experiments:**
  1. **Ablate orchestration:** Run REMSA-NAIVE (sequential tool calls) on benchmark queries to isolate orchestrator contribution—expect ~3-point drop in Avg Top-1 Score per Table 2.
  2. **Probe retrieval quality:** Manually inspect top-10 retrieved candidates for 5 queries with low MRR; check if relevant models exist in RS-FMD but aren't retrieved (embedding issue) or aren't in the database (coverage gap).
  3. **Stress-test clarification:** Run queries with intentionally ambiguous constraints (e.g., "I need a model for disaster response") and measure how many clarification rounds are triggered; verify final recommendations improve over unclarified baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Schema coverage may miss emerging capabilities or properties not represented in the 40+ field structure
- Clarification mechanism assumes users can accurately respond to prompts about constraints they may not fully understand
- 3-round clarification limit may truncate necessary dialogue for complex queries

## Confidence
- **High Confidence:** Retrieval and ranking mechanics (Sections 4.2, 6.1) - performance improvements over baselines are well-quantified with statistical backing across 900 query-model configurations.
- **Medium Confidence:** Clarification orchestration effectiveness - while Algorithm 1 and confidence thresholds are specified, user study data on clarification quality and fatigue is absent.
- **Medium Confidence:** Schema sufficiency - the comprehensive field list suggests thoroughness, but no formal coverage analysis or user validation of schema relevance is provided.

## Next Checks
1. **Schema Coverage Validation:** Conduct user interviews with 10 remote sensing practitioners to identify query properties not captured in RS-FMD. For each missing property, measure how many relevant FMs become undiscoverable and estimate impact on hit rate.

2. **Clarification Loop Robustness Test:** Deploy REMSA with modified thresholds (confidence=0.3, max rounds=5) on a subset of benchmark queries. Compare recommendation accuracy against the published configuration to determine optimal tradeoff between user effort and selection quality.

3. **Long-Tail Model Performance:** Identify FMs in RS-FMD with sparse benchmark data (<3 entries). Run REMSA queries that should retrieve these models and measure whether they appear in top-5 results. If not, investigate whether ranking relies too heavily on benchmark density versus intrinsic capabilities.