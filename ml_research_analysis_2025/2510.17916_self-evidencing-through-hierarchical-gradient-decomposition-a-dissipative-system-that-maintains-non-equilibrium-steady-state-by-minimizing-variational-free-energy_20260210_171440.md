---
ver: rpa2
title: 'Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative
  System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free
  Energy'
arxiv_id: '2510.17916'
source_url: https://arxiv.org/abs/2510.17916
tags:
- learning
- credit
- structural
- system
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a constructive proof that the Free Energy
  Principle (FEP) can be implemented as a scalable algorithm for self-organizing systems.
  The key innovation is a hierarchical credit assignment mechanism that decomposes
  gradient computation into three nested inference loops: spatial credit via feedback
  alignment, temporal credit via eligibility traces, and structural credit via a Trophic
  Field Map (TFM) that estimates expected gradient magnitude for each connection block.'
---

# Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy

## Quick Facts
- arXiv ID: 2510.17916
- Source URL: https://arxiv.org/abs/2510.17916
- Reference count: 40
- Self-organizing system achieves 98.6% task retention after interference through hierarchical gradient decomposition

## Executive Summary
This paper presents a constructive proof that the Free Energy Principle (FEP) can be implemented as a scalable algorithm for self-organizing systems. The key innovation is a hierarchical credit assignment mechanism that decomposes gradient computation into three nested inference loops: spatial credit via feedback alignment, temporal credit via eligibility traces, and structural credit via a Trophic Field Map (TFM) that estimates expected gradient magnitude for each connection block. This exact hierarchical inference produces emergent capabilities including autonomous recovery from structural damage, self-organized criticality, and sample-efficient reinforcement learning without replay buffers.

## Method Summary
The system implements hierarchical gradient decomposition through three nested inference loops operating at different timescales. Spatial credit is computed via feedback alignment where a learned feedback projection converges to exact spatial gradients in the relevant error subspace. Temporal credit is handled by eligibility traces that implement optimal exponential filtering for expected temporal gradients under timescale separation. Structural credit is computed by the Trophic Field Map (TFM), which uses block-averaged eligibility traces and Jacobian-gated error signals combined via exponential moving average to estimate which connection blocks would most reduce free energy. The TFM achieves a Pearson correlation of 0.9693 with oracle gradients, validating the exactness of structural credit assignment.

## Key Results
- TFM achieves Pearson correlation of 0.9693 with oracle gradients for structural credit assignment
- 98.6% task retention after interference in continual learning scenarios
- Autonomous recovery from 75% structural damage
- Self-organized criticality with spectral radius ρ≈1.0
- Sample-efficient reinforcement learning without replay buffers

## Why This Works (Mechanism)

### Mechanism 1: Trophic Field Map (TFM) for Structural Credit Assignment
The TFM computes the exact expected block-level gradient magnitude using only local signals. Block-averaged eligibility traces (temporal) and Jacobian-gated error signals (spatial) are combined via exponential moving average (α ≈ 10⁻⁶) to estimate which connection blocks would most reduce free energy. This integrates spatial and temporal exactness into a structural prior. The mechanism assumes gradient signals are stationary enough over the TFM's glacial timescale (~10⁶ steps) that their expected magnitude is a reliable proxy for structural utility.

### Mechanism 2: Feedback Alignment for Spatial Credit Assignment
A learned feedback projection (W_fb) converges to exact spatial gradients in the relevant error subspace without weight transport. The feedback pathway adapts via ∆W_fb ∝ -(W_fb·δ - R^T·δ)·δ^T, minimizing alignment error with the true gradient projection. Over time, W_fb aligns with R^T in the subspace spanned by error signals. This mechanism assumes the error subspace is sufficiently explored during training for alignment to generalize.

### Mechanism 3: Eligibility Traces for Temporal Credit Assignment
Eligibility traces implement optimal exponential filtering that provides exact expected temporal gradients under timescale separation. trc_i(t+1) = α_elig·trc_i(t) + (1-α_fast)·x_i(t) implements a kernel K(τ) ∝ exp(-τ/τ_elig) that weights past activity by causal influence. Combined with the Jacobian term (1-x²), this approximates forward-mode gradients locally. The mechanism assumes learning timescales are slow relative to eligibility decay (η·τ_elig ≪ 1).

## Foundational Learning

- **Variational Free Energy (FEP basics)**: The entire architecture is framed as minimizing free energy to maintain non-equilibrium steady-state. Without this, the credit assignment mechanisms lack unifying rationale. Quick check: Can you explain why free energy upper-bounds surprise, and how minimizing it performs approximate Bayesian inference?

- **Three-Factor Learning Rules**: The synaptic plasticity rule (presynaptic trace × postsynaptic error × neuromodulatory signal) is the computational primitive enabling local gradient approximation. Quick check: How does error-gating differ from vanilla Hebbian learning, and why is it necessary for task-directed plasticity?

- **Self-Organized Criticality**: The system autonomously maintains spectral radius ρ≈1.0, which maximizes computational capacity. This emerges from TFM-driven growth/pruning dynamics. Quick check: Why does criticality matter for information processing, and what happens if ρ ≫ 1 or ρ ≪ 1?

## Architecture Onboarding

- **Component map**: Recurrent core (W, x(t), tanh, noise ξ) → Spatial pathway (R, δ, W_fb → ϵ) → Temporal pathway (trc τ=200ms, a τ=1000s) → Structural pathway (TFM T α≈10⁻⁶, viability, prune/growth) → Timescales (Fast 20ms → Eligibility 200ms → Homeostatic 1000s → TFM 10⁶ steps)

- **Critical path**: TFM update (Eq. 24-26) depends on block-averaged eligibility and gated error; structural plasticity decisions depend on TFM values; synaptic updates depend on error-modulated traces. If TFM correlation degrades, structural inference fails.

- **Design tradeoffs**: Block granularity (ℓ=32 neurons) vs. fine-grained structural credit; NLMS normalization enables online robustness but weakens direct gradient correlation (Pearson 0.195 vs. e-prop); TFM's glacial timescale preserves memory but slows adaptation to novel tasks.

- **Failure signatures**: Cosine similarity < 0.5 after 10K steps → spatial alignment failure; Spectral radius >> 1.0 → runaway dynamics (pruning insufficient); Spectral radius << 1.0 → dead network (over-pruning); NLMS removal → MSE stuck at baseline.

- **First 3 experiments**: 1) Reproduce TFM-oracle correlation: Freeze plasticity, compute H_post vs. G_post over 100 timesteps, verify Pearson > 0.95. 2) Ablate NLMS normalization: Train on Mackey-Glass, confirm learning fails (MSE unchanged from initialization). 3) Test continual learning: Train Task A → Task B → Task A with single-step relearning, verify >95% retention and rapid recovery.

## Open Questions the Paper Calls Out

### Open Question 1
Can formal convergence rates be established for the coupled synaptic and structural dynamics, given the singular perturbation problem caused by vastly different timescales? The system operates on timescales separated by orders of magnitude (fast state τ_fast=20ms vs. glacial structural α ≈ 10⁻⁶), creating a difficult singular perturbation problem for standard dynamical systems analysis.

### Open Question 2
Can structural credit assignment be made exact at the individual synapse level rather than the block level (ℓ=32) without sacrificing computational tractability? The current Trophic Field Map (TFM) aggregates gradients over blocks of 32 neurons to manage complexity, which precludes finer-grained topological adaptations.

### Open Question 3
Can the system be adapted to achieve state-of-the-art performance on static benchmarks, or is the architecture strictly specialized for non-stationary domains? The model currently trades peak single-task accuracy for continual learning capabilities and robustness.

## Limitations

- TFM correlation generalization is uncertain - strong results on Mackey-Glass may not generalize across diverse task distributions
- Biological plausibility boundary - TFM update operations may exceed realistic cellular computation capabilities
- Catastrophic interference claims need stress-testing with more severe interference scenarios and higher task similarity

## Confidence

**High Confidence (8-10/10)**: Basic gradient approximation mechanisms (feedback alignment achieving cosine similarity >0.9, eligibility traces providing diagonal gradient approximation with Pearson >0.8).

**Medium Confidence (5-7/10)**: TFM's exact structural credit assignment and the emergent criticality maintenance.

**Low Confidence (1-4/10)**: Claims about biological implementation feasibility and generalization to arbitrary task distributions.

## Next Checks

1. **Cross-Task TFM Validation**: Test TFM correlation on at least 5 diverse dynamical systems (chaotic, periodic, stochastic) to establish the 0.9693 correlation is not Mackey-Glass specific.

2. **Interference Stress Test**: Design a continual learning benchmark with high task similarity (>80% overlap in input/output space) and evaluate whether structural reorganization still prevents catastrophic forgetting.

3. **Biological Feasibility Audit**: Map each computational operation in the TFM update equations to realistic cellular mechanisms to identify which components exceed biological constraints.