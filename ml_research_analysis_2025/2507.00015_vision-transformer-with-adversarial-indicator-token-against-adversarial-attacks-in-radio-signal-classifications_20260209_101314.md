---
ver: rpa2
title: Vision Transformer with Adversarial Indicator Token against Adversarial Attacks
  in Radio Signal Classifications
arxiv_id: '2507.00015'
source_url: https://arxiv.org/abs/2507.00015
tags:
- adversarial
- attacks
- proposed
- token
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel adversarial defense mechanism for
  radio signal classification by integrating an adversarial indicator (AdvI) token
  into a vision transformer (ViT) architecture. The AdvI token works alongside the
  standard classification token to detect adversarial attacks during both training
  and inference, enabling the model to either correctly classify signals or flag them
  as adversarial.
---

# Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications

## Quick Facts
- arXiv ID: 2507.00015
- Source URL: https://arxiv.org/abs/2507.00015
- Reference count: 40
- Primary result: ViT with adversarial indicator token achieves 10-30% higher accuracy and lower false negative rates against white-box attacks compared to state-of-the-art baselines

## Executive Summary
This paper introduces a novel adversarial defense mechanism for radio signal classification by integrating an adversarial indicator (AdvI) token into a vision transformer (ViT) architecture. The AdvI token works alongside the standard classification token to detect adversarial attacks during both training and inference, enabling the model to either correctly classify signals or flag them as adversarial. The approach combines adversarial training with a detection mechanism in a single unified network, reducing architectural complexity compared to using separate models. Experiments on radio signal datasets show the method achieves significantly higher accuracy and lower false negative rates against white-box adversarial attacks (FGM, PGD, BIM) compared to state-of-the-art baselines, particularly under strong perturbations.

## Method Summary
The method adds a learnable adversarial indicator token to the standard ViT architecture alongside the classification token. During training, both tokens participate in self-attention, but the AdvI token is connected to a separate detection head that outputs binary predictions (benign vs adversarial). The model is trained using a combined loss that balances classification accuracy and detection capability, with adversarial examples generated using adaptive PGD attacks that scale with SNR. The unified architecture allows simultaneous classification and detection in a single forward pass, with the model outputting either a modulation class or flagging the input as adversarial based on the AdvI token's prediction.

## Key Results
- Achieves 10-30% higher accuracy than AT-ViT at PNR=-10dB against PGD attacks
- Reduces false negative rate significantly compared to baselines, particularly at low PNR
- Attention analysis shows AdvI token focuses on suspicious signal features with ~4× fewer high-attention elements than standard AT-trained ViT
- Maintains competitive performance across multiple attack types (FGM, PGD, BIM) in white-box scenarios

## Why This Works (Mechanism)

### Mechanism 1: Dual-Token Detection via Learned Separation
- Claim: Adding a dedicated adversarial indicator token enables binary classification of adversarial vs. benign inputs within a single forward pass.
- Mechanism: The AdvI token is a learnable embedding positioned alongside the CLS token. Through self-attention, it aggregates global signal features. Its output passes through two dense layers (128→32→2) trained to output Class 0 (benign) or Class 1 (adversarial). The detection loss (loss2) explicitly optimizes this separation.
- Core assumption: Adversarial perturbations produce distinguishable patterns in the attention/representation space that a dedicated token can learn to capture.
- Evidence anchors: [abstract] "Integrating an adversarial training method with a detection mechanism using AdvI token"; [section III.B] "loss2 = L(f²θ(x), yb) + L(f²θ(xadv), ya)"; [corpus] Related work addresses robustness through distillation but does not propose a dedicated detection token.

### Mechanism 2: Attention Modulation to Highlight Anomalous Features
- Claim: The AdvI token redirects attention toward suspicious signal regions, improving robustness by focusing classification on less-corrupted features.
- Mechanism: During training, AdvI participates in multi-head self-attention. The detection loss gradients propagate through attention weights, causing the model to attend differently to adversarial vs. clean inputs. Visualization shows AiTViT has ~4× fewer high-attention elements (38.21 vs 165.48) compared to AT-trained ViT.
- Core assumption: Adversarial perturbations create detectable anomalies in time-frequency features that attention can isolate.
- Evidence anchors: [abstract] "Attention analysis reveals the AdvI token helps the model focus on suspicious signal features"; [section III.A] "the AdvI token influences attention weights"; [section IV.B] "average number of attention elements with high attention weights... for AT trained conventional ViT was 165.48, while for our proposed AiTViT, it was significantly lower at 38.21".

### Mechanism 3: Joint Optimization via Weighted Dual Loss
- Claim: Simultaneously optimizing classification accuracy and detection capability through a weighted combined loss maintains clean accuracy while improving adversarial robustness.
- Mechanism: Total loss = β·loss1 + (1-β)·loss2, where β=0.1 heavily weights detection. Loss1 ensures correct modulation classification for both clean and adversarial samples; loss2 trains the AdvI head for binary detection. Pretraining on clean data (NT) before adversarial fine-tuning preserves baseline accuracy.
- Core assumption: The two objectives (classification + detection) are not fundamentally conflicting, and a shared representation can serve both.
- Evidence anchors: [section III.B] "loss = βloss1 + (1-β)loss2, where β is a hyperparameter that balances the importance of two loss functions... we use β = 0.1"; [section III.B] "we pretrained models using the normal training (NT) approach".

## Foundational Learning

- **Vision Transformer (ViT) Architecture**: Understanding patch embeddings, CLS token function, and self-attention is prerequisite to comprehending the AdvI token modification. Can you explain how the CLS token aggregates information from all patches through self-attention layers?

- **Adversarial Attack Methods (FGM, PGD, BIM)**: The defense is evaluated against these specific white-box attacks; understanding gradient-based perturbation generation is essential to interpret attack algorithms and defense evaluation. What is the key difference between single-step (FGM) and iterative (PGD, BIM) adversarial attacks in terms of perturbation generation?

- **Multi-Task Learning with Combined Loss**: The method jointly optimizes 11-class modulation classification and binary adversarial detection; understanding loss balancing (β weighting) is critical for implementation. Why might heavily weighting one task (β=0.1 for classification) still improve overall robustness metrics?

## Architecture Onboarding

- **Component map**: Input I/Q signal → Conv layer → Patch embeddings (N0 × 128) → Tokens (CLS + AdvI) prepended → (N0+2) × 128 → 4 transformer layers with MSA + FFN + LayerNorm → CLS → Dense → 11-class softmax; AdvI → Dense(32, GeLU) → Dense(2) → binary softmax

- **Critical path**: 1) Signal preprocessing (I/Q → 2D representation, normalization for RDL dataset); 2) Adaptive PGD generation during training (perturbation scales with SNR); 3) Dual-loss backpropagation (β=0.1 weighting critical); 4) Inference: Both heads run; if AdvI predicts Class 1, flag as adversarial

- **Design tradeoffs**: Detection vs. classification balance (β=0.1 emphasizes detection; may reduce clean accuracy if increased); Unified vs. separate models (~10× faster than NR but harder to debug); White-box assumption (attackers know AdvI exists; defense may degrade under gray/black-box)

- **Failure signatures**: High FNR at low PNR (< -15dB): Detection fails; model reverts to vulnerable classification; Attention remains dispersed: AdvI not influencing attention → check loss2 gradients; Clean accuracy drops >5%: Pretraining may be insufficient or β needs adjustment

- **First 3 experiments**: 1) Reproduce baseline comparison: Train AiTViT on RML2016.10a with β=0.1, evaluate against PGD attacks at PNR=[-20, -15, -10, -5, 0] dB; compare accuracy and FNR to AT-ViT and NR baselines (target: ≥10% accuracy gain at PNR=-10dB); 2) Ablation on loss weighting: Train variants with β=[0.01, 0.1, 0.3, 0.5]; plot clean accuracy vs. FNR trade-off to validate β=0.1 choice; 3) Attention visualization sanity check: For 10 PGD samples, extract and visualize AdvI attention weights; confirm focused attention (<50 high-weight elements on average) vs. AT-ViT baseline (>150 elements)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the AiTViT architecture perform against black-box transfer attacks where the attacker has no knowledge of the AdvI token mechanism? The authors focus exclusively on white-box attacks to establish a "worst-case scenario" benchmark, while noting that related works often address black-box transferability.

- **Open Question 2**: Does the AdvI token generalize to detect unseen adversarial attacks that differ significantly from the PGD-based examples used during training? The training method generates adversarial samples solely using adaptive PGD, but the evaluation tests only similar gradient-based methods.

- **Open Question 3**: What are the energy consumption and latency trade-offs of the AiTViT model when deployed on actual resource-constrained IoT edge hardware? The introduction claims suitability for "resource-constrained environments" and IoT, but computational costs are only reported for an NVIDIA RTX 4080 desktop GPU.

## Limitations

- Critical hyperparameters (number of attention heads, patch size, FFN dimensions) are not specified, requiring guesses that may affect performance
- Training schedule details (optimizer, learning rate, batch size) are unspecified, making exact reproduction difficult
- Evaluation assumes white-box attacks; performance under gray-box or black-box scenarios is not assessed
- The claim of "significantly higher accuracy" lacks quantitative architectural comparison to modern defenses in the radio signal domain

## Confidence

- **High Confidence**: The dual-token mechanism and combined loss formulation are clearly specified and implemented with well-supported accuracy improvements
- **Medium Confidence**: The attention-based explanation for AdvI effectiveness is plausible but could be strengthened with more systematic analysis
- **Low Confidence**: The claim of superior performance compared to state-of-the-art baselines is difficult to fully verify due to missing architectural details

## Next Checks

1. **Ablation on Loss Weighting**: Train AiTViT variants with β=[0.01, 0.1, 0.3, 0.5] on RML2016.19a. Plot clean accuracy versus FNR trade-off curves to empirically validate that β=0.1 optimally balances classification accuracy and detection capability.

2. **Gray-Box Attack Evaluation**: Implement attacks where the adversary knows the ViT architecture but not the presence of the AdvI token. Generate adversarial examples targeting only the CLS classification head, then evaluate both classification accuracy and AdvI detection performance.

3. **Attention Pattern Reproducibility**: For 10 PGD-adversarial samples at PNR=-10dB, extract and visualize the AdvI token's attention weights across all heads and layers. Quantify the number of attention elements with weights >0.1 and compare against the AT-ViT baseline.