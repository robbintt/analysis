---
ver: rpa2
title: Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences
arxiv_id: '2504.00473'
source_url: https://arxiv.org/abs/2504.00473
tags:
- step
- answer
- rose
- each
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces RoSE, a framework that enhances reasoning\
  \ performance of large language models (LLMs) without manual demonstrations, labeled\
  \ data, or external tools. RoSE collects streaming experiences\u2014answered questions\
  \ and their reasoning paths\u2014into a dynamic experience pool and orchestrates\
  \ them to assist in answering new questions."
---

# Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences

## Quick Facts
- arXiv ID: 2504.00473
- Source URL: https://arxiv.org/abs/2504.00473
- Authors: Xiangyang Liu; Junliang He; Xipeng Qiu
- Reference count: 33
- Primary result: RoSE framework enhances reasoning performance of LLMs without manual demonstrations, labeled data, or external tools, achieving strong results across 9 reasoning tasks.

## Executive Summary
RoSE is a framework that enhances the reasoning capabilities of large language models by orchestrating a dynamic pool of streaming experiences. It collects answered questions and their reasoning paths, then uses diversity, uncertainty, and complexity filtering to select the most effective demonstrations for new questions. The method is evaluated across multiple reasoning tasks and model sizes, showing consistent improvements over established baselines.

## Method Summary
RoSE operates in a streaming setting where test questions arrive one-by-one. For each question, the model generates multiple reasoning paths to compute uncertainty (entropy over answers) and complexity (average reasoning step count). These experiences are stored in a dynamic pool. When answering a new question, RoSE retrieves and sorts pool questions by semantic similarity, partitions them into buckets, filters by uncertainty (using a dynamic threshold), and selects the most complex question from each bucket to form demonstrations. The final answer is obtained via few-shot prompting with these demonstrations, and the new question is added to the pool.

## Key Results
- RoSE consistently outperforms baselines like zero-shot/few-shot CoT and Auto-CoT across 9 reasoning tasks and 2 LLMs
- LLaMA2-13B-Chat approaches GPT-3.5-Turbo performance after applying RoSE
- Ablation studies confirm the importance of each orchestration step (diversity, uncertainty, complexity)
- RoSE demonstrates stability across different temperatures, reasoning path counts, and demonstration numbers

## Why This Works (Mechanism)

### Mechanism 1: Diversity-Aware Demonstration Selection
- Claim: When demonstrations span a range of similarity to a new question, models are less prone to incorrectly copying answers from highly similar but flawed examples.
- Mechanism: Questions in the experience pool are sorted by semantic similarity to the test question and partitioned into k equal-width buckets. One question is selected from each bucket, ensuring the final demonstration set covers both similar and dissimilar examples.
- Core assumption: The model's tendency to copy errors is strongest when demonstrations are clustered in high similarity; distributing examples across the similarity spectrum reduces this bias.
- Evidence anchors:
  - [abstract] "RoSE will sort the questions according to their similarity with the new question, and then uniformly divide them into multiple buckets. It finally extracts one question from each bucket to make these extracted questions more diverse."
  - [section] Section 3.2 describes the partitioning algorithm (Algorithm 1) and states that this method outperforms the k-means clustering used in Auto-CoT.

### Mechanism 2: Uncertainty-Based Quality Filtering
- Claim: Filtering demonstrations to retain only those with relatively low model-estimated uncertainty improves the overall correctness of the demonstration set.
- Mechanism: For each candidate question in a bucket, uncertainty is computed as the entropy over answers from multiple sampled reasoning paths. A dynamic threshold (λ times the minimum uncertainty in the bucket) removes high-uncertainty candidates.
- Core assumption: Low uncertainty (measured via self-consistency) correlates with a higher probability of the answer being correct.
- Evidence anchors:
  - [abstract] "RoSE will preferentially select the questions with low uncertainty and high complexity from each bucket."
  - [section] Section 3.1 defines uncertainty (Equations 1-3) and Figure 2 shows a negative correlation between uncertainty and accuracy on SVAMP.

### Mechanism 3: Complexity-Based Demonstration Prioritization
- Claim: Selecting the most complex available questions (those with longer reasoning paths) as demonstrations provides richer procedural examples that can improve transfer to new reasoning tasks.
- Mechanism: After uncertainty filtering, the question with the highest complexity (measured by the average number of reasoning steps in paths leading to the most frequent answer) is chosen from each bucket.
- Core assumption: Questions with more reasoning steps encode more detailed solution patterns that are useful for teaching general reasoning skills.
- Evidence anchors:
  - [abstract] "RoSE will preferentially select the questions with low uncertainty and high complexity from each bucket."
  - [section] Section 3.1 defines complexity (Equation 4) and Section 4.3 ablation (Figure 4) shows higher accuracy when selecting "Hard" (complex) questions.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: RoSE operates entirely via in-context learning, where the model's behavior is guided by demonstrations provided in the prompt, without any weight updates.
  - Quick check question: How does a model use the provided (question, rationale, answer) triplets in the prompt to infer the answer to a new question? Is this process robust to variations in demonstration order or quality?

- **Concept: Entropy-Based Uncertainty Estimation**
  - Why needed here: RoSE relies on model self-consistency (via multiple samples) to estimate question difficulty and answer reliability.
  - Quick check question: If a model generates 10 reasoning paths for a question and 9 lead to answer "A" while 1 leads to "B", what is the approximate entropy? How does this compare to a case with a 5-5 split?

- **Concept: Complexity-Based Prompting**
  - Why needed here: Selecting complex demonstrations is a core heuristic in RoSE.
  - Quick check question: Why might a math word problem requiring two arithmetic steps be a better demonstration than one requiring only a single step? What are the potential downsides of always choosing the longest chain?

## Architecture Onboarding

- **Component map**: Streaming Experience Pool -> Experience Orchestration Module -> Inference Module -> Post-Processing
- **Critical path**: Test question arrival → Similarity computation & sorting → Bucket partitioning → Per-bucket uncertainty filtering → Per-bucket complexity selection → Prompt assembly → LLM inference → Pool update
- **Design tradeoffs**:
  - **Efficiency vs. Effectiveness**: Generating multiple reasoning paths per question (for uncertainty calculation) and storing all experiences increases compute and memory. The tradeoff is improved demonstration quality.
  - **Dynamic vs. Static Thresholds**: Using a dynamic λ-based threshold per bucket adapts to pool state but adds complexity. Fixed thresholds are simpler but may fail across diverse tasks.
  - **Complexity Metric**: Using reasoning step count is simple but may not capture true cognitive complexity. Alternative metrics (e.g., logical depth) could be more accurate but harder to compute.
- **Failure signatures**:
  1. **Empty buckets after filtering**: If uncertainty thresholds are too aggressive, buckets may become empty, breaking the orchestration.
  2. **Performance collapse early in stream**: If initial pool entries are low-quality (high uncertainty), they may provide poor demonstrations, creating a negative feedback loop.
  3. **Stagnant pool quality**: If the model's uncertainty estimates are miscalibrated, low-uncertainty but incorrect entries may accumulate, degrading future demonstrations.
- **First 3 experiments**:
  1. **Ablation study**: Implement RoSE with each orchestration component (diversity, uncertainty, complexity) removed individually. Compare accuracy on a held-out subset of a reasoning benchmark (e.g., GSM8K).
  2. **Threshold sensitivity analysis**: Vary the λ parameter for dynamic uncertainty thresholding (e.g., λ = 1.0, 1.2, 1.4, 1.6) and measure impact on both accuracy and the percentage of questions retained in the pool.
  3. **Baselines in streaming setting**: Compare RoSE against Auto-CoT adapted for streaming (where the pool grows as questions are answered). Use the same base LLM and evaluation protocol to ensure a fair comparison.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the complexity-based selection mechanism be optimized to prevent significant latency increases during inference?
- **Basis in paper:** [Explicit] The "Limitations" section states: "Since we estimate the complexity of a question using the number of reasoning steps... this may lead to a longer length of demonstrations and thus lead to slower efficiency."
- **Why unresolved:** The current method preferentially selects long reasoning paths to maximize detail, creating a direct trade-off between reasoning accuracy and inference speed that the paper identifies but does not solve.
- **What evidence would resolve it:** A modified ablation study measuring accuracy vs. latency trade-offs, potentially using a semantic density metric rather than raw step count to determine complexity.

### Open Question 2
- **Question:** Does RoSE exacerbate error propagation when the model is "confidently wrong" (hallucination)?
- **Basis in paper:** [Inferred] The methodology filters experiences based on "uncertainty" (low entropy). This assumes low uncertainty correlates with correctness.
- **Why unresolved:** The paper does not address scenarios where an LLM produces a low-entropy (confident) but incorrect reasoning path. If such an experience is stored, the orchestration mechanism might prioritize it as a "high quality" demonstration, potentially poisoning the experience pool.
- **What evidence would resolve it:** An analysis of performance on adversarial datasets designed to elicit high-confidence hallucinations, tracking the rate at which incorrect "low uncertainty" experiences are retrieved.

### Open Question 3
- **Question:** Is the performance gain of RoSE inversely correlated with the base model's intrinsic capability?
- **Basis in paper:** [Inferred] Section 4.2 notes that "compared to GPT-3.5-Turbo... the improvement becomes larger than it on GPT-3.5-Turbo" when applied to the weaker LLaMA2-13B-Chat.
- **Why unresolved:** It is unclear if RoSE acts as a fundamental reasoning booster or primarily as a crutch for models with weaker intrinsic reasoning capabilities.
- **What evidence would resolve it:** A scaling law analysis applying RoSE to a spectrum of model sizes (e.g., 7B, 13B, 34B, and 70B parameters) to plot the delta improvement against base model size.

## Limitations
- The empirical validation relies heavily on semantic similarity and uncertainty estimates, but their robustness across diverse reasoning domains is not systematically evaluated.
- The complexity metric (reasoning step count) is simplistic and may not correlate with true cognitive complexity.
- The streaming setting assumes a growing, noise-free experience pool, but no experiments probe degradation under early poor-quality demonstrations or concept drift in question distributions.

## Confidence

**High Confidence**: The core ablation study and performance comparisons against established baselines robustly support the overall effectiveness of RoSE's orchestration approach.

**Medium Confidence**: The individual contributions of diversity, uncertainty, and complexity filtering are validated, but the paper does not isolate their marginal gains in a controlled, head-to-head manner or test robustness to prompt template variations.

**Low Confidence**: The underlying assumptions about why diversity, uncertainty, and complexity work (e.g., "copy effect" mitigation, entropy-accuracy correlation, richer procedural examples) are grounded in intuition and limited empirical evidence rather than rigorous causal analysis.

## Next Checks

1. **Robustness to Prompt Templates**: Repeat the ablation and main experiments with alternative few-shot prompt formats (e.g., "Q: … A: …" vs. "Question: … Reasoning: … Answer: …") to assess sensitivity to surface form.

2. **Pool Quality Over Time**: Log the accuracy and uncertainty of demonstrations added to the pool after each question. Plot pool-level accuracy vs. time to detect stagnation or drift.

3. **Alternative Complexity Metrics**: Replace step count with a proxy for logical depth (e.g., number of unique intermediate conclusions) and re-run ablation to see if gains persist.