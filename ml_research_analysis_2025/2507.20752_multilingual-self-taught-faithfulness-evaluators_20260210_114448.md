---
ver: rpa2
title: Multilingual Self-Taught Faithfulness Evaluators
arxiv_id: '2507.20752'
source_url: https://arxiv.org/abs/2507.20752
tags:
- sentence
- training
- languages
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STEMF, a framework for training multilingual
  faithfulness evaluators for large language models using synthetic data and self-supervised
  learning. The approach addresses the challenge of extending faithfulness evaluation
  to multiple languages without relying on expensive human-labeled data.
---

# Multilingual Self-Taught Faithfulness Evaluators

## Quick Facts
- arXiv ID: 2507.20752
- Source URL: https://arxiv.org/abs/2507.20752
- Reference count: 30
- Key outcome: STEMF framework trains multilingual faithfulness evaluators using synthetic data, achieving 6.9% average balanced accuracy improvement and matching 72B models with 9B models

## Executive Summary
This paper introduces STEMF (Self-Taught Evaluators for Multilingual Faithfulness), a framework that addresses the challenge of extending faithfulness evaluation to multiple languages without relying on expensive human-labeled data. STEMF generates synthetic training data by creating both faithful and corrupted summaries from documents, then fine-tunes evaluator models to distinguish between them. The approach enables the development of high-performance faithfulness evaluators across seven languages, demonstrating significant improvements over existing methods while requiring minimal human annotation effort.

The framework shows that models trained on English data often yield the best cross-lingual results, and that focusing fine-tuning on central model layers provides computational efficiency without sacrificing performance. Notably, a 9B-parameter STEMF-trained model matches the performance of much larger 72B-parameter models and even outperforms GPT-4 on certain benchmarks, making it a practical solution for real-world deployment in multilingual settings.

## Method Summary
STEMF addresses the challenge of multilingual faithfulness evaluation by generating synthetic training data through a self-supervised learning approach. The method creates both faithful and corrupted versions of summaries from source documents, using techniques like mask-filling and entity substitution to introduce controlled errors. An evaluator model is then fine-tuned to distinguish between faithful and unfaithful summaries. The framework supports multiple languages and demonstrates that training on English data often yields the best cross-lingual performance. STEMF also identifies that focusing fine-tuning on central model layers provides computational efficiency without sacrificing evaluation quality, making it a practical solution for multilingual faithfulness assessment.

## Key Results
- STEMF achieves 6.9% average balanced accuracy improvement across seven languages compared to baseline methods
- 9B-parameter STEMF models match the performance of 72B-parameter models and outperform GPT-4 on certain benchmarks
- English training data consistently yields the best cross-lingual results across different evaluation settings

## Why This Works (Mechanism)
STEMF leverages the abundance of multilingual text data to generate synthetic training examples that capture the characteristics of faithful and unfaithful summaries. By systematically corrupting summaries through controlled modifications like entity substitution and mask-filling, the framework creates diverse training signals that help evaluator models learn to detect faithfulness violations. The self-supervised nature eliminates the need for expensive human annotations while the multilingual approach ensures broad language coverage. The focus on central model layers during fine-tuning provides an efficient training strategy that maintains performance while reducing computational costs.

## Foundational Learning
- Synthetic data generation for training evaluators: Needed to create training data without human annotation; quick check: verify corruption methods produce realistic faithfulness errors
- Cross-lingual transfer learning: Required for multilingual evaluation without language-specific training data; quick check: test performance when training on one language and evaluating on another
- Model fine-tuning optimization: Essential for computational efficiency; quick check: compare performance when fine-tuning different model layers
- Faithfulness evaluation metrics: Fundamental for measuring summary quality; quick check: ensure metrics align with human judgment of faithfulness
- Multilingual text processing: Critical for handling diverse languages; quick check: validate tokenization and processing across all target languages

## Architecture Onboarding

Component Map: Document -> Summary Generator -> Faithful/Corrupted Summaries -> Evaluator Model -> Faithfulness Score

Critical Path: The core workflow involves generating synthetic summaries (both faithful and corrupted versions) from source documents, then training an evaluator model to distinguish between them. The synthetic data generation pipeline includes document processing, summary creation, and controlled corruption techniques. The evaluator training process involves fine-tuning on the synthetic data with focus on central model layers for efficiency.

Design Tradeoffs: STEMF trades potential data diversity limitations from synthetic generation against the practical benefits of eliminating human annotation costs. The framework prioritizes computational efficiency through selective fine-tuning of central layers rather than full model training. It also balances the need for comprehensive multilingual coverage against the reality that most available training data is in English.

Failure Signatures: Models may overfit to synthetic patterns rather than genuine faithfulness errors, potentially missing real-world evaluation scenarios. Performance may degrade significantly when evaluating summaries from different generation paradigms than those used in training data synthesis. Cross-lingual transfer may fail when source and target languages have significantly different linguistic structures or when training data quality varies across languages.

First Experiments:
1. Train STEMF on English data only and evaluate cross-lingual performance to verify the paper's finding about English as optimal source language
2. Compare STEMF performance when fine-tuning all model layers versus only central layers to validate computational efficiency claims
3. Test STEMF against human judgment on real multilingual summaries to assess practical applicability beyond synthetic benchmarks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions arise from the research. These include the framework's performance on other summary quality dimensions beyond faithfulness, its generalization to different summary generation paradigms, and the potential for extending the approach to even more languages. The paper also leaves open questions about the optimal corruption techniques for generating synthetic training data and how to best balance computational efficiency with evaluation performance.

## Limitations
- Focus exclusively on faithfulness evaluation, ignoring other summary quality dimensions like relevance, coherence, and fluency
- Synthetic data generation may not fully capture the complexity and diversity of real-world faithfulness errors across languages
- Performance gains measured primarily against other synthetic evaluation methods rather than human judgment

## Confidence
- STEMF significantly improves multilingual faithfulness evaluation performance - Medium Confidence
- English training data yields the best cross-lingual results - Medium Confidence
- 9B STEMF models match 72B models and GPT-4 performance - Low-Medium Confidence

## Next Checks
1. Conduct human evaluation studies comparing STEMF-trained models against existing faithfulness evaluators on real multilingual summaries to validate synthetic benchmark performance in practical settings.

2. Test STEMF's generalization by evaluating on faithfulness errors from different generation paradigms (e.g., different summarization models, prompting strategies) than those used in training data synthesis.

3. Extend evaluation to other aspects of summary quality beyond faithfulness, including relevance, coherence, and fluency, to assess STEMF's broader applicability for comprehensive summary evaluation.