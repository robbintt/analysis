---
ver: rpa2
title: 'VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent
  AI Architecture'
arxiv_id: '2509.22218'
source_url: https://arxiv.org/abs/2509.22218
tags:
- data
- visualization
- language
- user
- vizgen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VizGen is an AI-driven system for data visualization that translates
  natural language queries into SQL and generates appropriate visualizations. Built
  on a multi-agent architecture, it uses specialized agents for SQL generation, graph
  recommendation, customization, and insight extraction.
---

# VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture

## Quick Facts
- arXiv ID: 2509.22218
- Source URL: https://arxiv.org/abs/2509.22218
- Reference count: 13
- Primary result: AI-driven system translating natural language to visualizations with 4.4/5 usability rating from 19 participants

## Executive Summary
VizGen is an AI-driven data visualization system that translates natural language queries into SQL and generates appropriate visualizations through a multi-agent architecture. The system uses specialized agents for SQL generation, graph recommendation, customization, and insight extraction, supporting real-time interaction with SQL databases and conversational refinement of visualizations. Evaluation with 19 participants showed an average usability rating of 4.4 out of 5, with users finding the tool intuitive and effective for creating visualizations without technical expertise. Key features like data summarization, auto chart recommendation, and text-to-graph generation were highly valued.

## Method Summary
VizGen employs a multi-agent architecture built on LangGraph to process natural language queries through specialized agents. The system uses an Intent Classifier to route queries to appropriate agents including SQL Agent (with metadata retrieval and validation), Visualization Agent (with data preprocessing and graph ranking), Analysis Agent, Explanation Agent, and Customizer Node. The architecture supports multiple database types and allows sequential activation of agents for multi-intent queries. The system translates user queries into SQL, retrieves and processes data, recommends suitable graph types based on data characteristics, and generates visualizations with conversational refinement capabilities.

## Key Results
- Average usability rating of 4.4 out of 5 from 19 participants
- 74% of users valued the auto chart recommendation feature
- Successfully bridges technical complexity and user accessibility for data visualization
- Navigation received lower rating of 3.3/5, indicating UI improvement opportunities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intent classification enables efficient agent routing by parsing natural language into discrete task categories.
- Mechanism: The Intent Classifier analyzes user queries and maps them to one of six intents (Visualization, Insight, Explanation, Customization, System, Other), triggering only the necessary agents. Multi-intent queries activate agents sequentially.
- Core assumption: Natural language queries can be reliably decomposed into discrete, classifiable intents without significant ambiguity loss.
- Evidence anchors:
  - [section] "The workflow begins with a user's natural language query, which is first processed by the Intent Classifier. This node analyzes the input to determine the core intent(s) and routes the request to the appropriate agent(s)."
  - [section] "For queries with multiple intents... the workflow will sequentially activate the necessary agents."
  - [corpus] THOR and Chain-of-Query papers similarly use decomposition/routing patterns for Text-to-SQL, suggesting this is a convergent design pattern.
- Break condition: Ambiguous or compound queries that don't map cleanly to single intents; classifier errors propagating to wrong agents.

### Mechanism 2
- Claim: Multi-stage SQL generation with validation improves query reliability over end-to-end generation.
- Mechanism: SQL Agent chains Metadata Retriever → SQL Generator → SQL Validator → SQL Executor. Schema context is retrieved first, then the LLM generates SQL, which is validated before execution.
- Core assumption: Providing schema metadata to LLMs and validating output reduces SQL errors compared to direct generation.
- Evidence anchors:
  - [section] "This agent incorporates a Metadata Retriever that fetches database schema information... The SQL Validator verifies the correctness and efficiency of the generated SQL query before execution."
  - [corpus] TiInsight and BAPPA papers similarly emphasize schema-aware SQL generation for accuracy.
  - [abstract] "it translates user queries into SQL and recommends suitable graph types."
- Break condition: Schema retrieval failures; validator false-positives approving invalid SQL; complex joins exceeding LLM reasoning capacity.

### Mechanism 3
- Claim: LLM-based graph ranking recommends chart types by analyzing data characteristics and user intent.
- Mechanism: Graph Ranker component receives preprocessed data and outputs ranked graph recommendations based on data properties (cardinality, distribution, temporal structure) and visualization best practices encoded in the LLM.
- Core assumption: LLMs can internalize sufficient visualization heuristics to make appropriate chart recommendations without explicit rule-based systems.
- Evidence anchors:
  - [section] "The Graph Ranker component analyzes the preprocessed data to recommend and rank the most suitable graph types based on data characteristics and visualization best practices."
  - [section] 74% of evaluation participants valued "Auto chart recommendation."
  - [corpus] Multi-Agent Data Visualization and Narrative Generation paper uses similar LLM-driven visualization selection.
- Break condition: Novel data types or edge cases not well-represented in LLM training data; user intent conflicting with data characteristics.

## Foundational Learning

- Concept: **LangGraph state machines for agent orchestration**
  - Why needed here: VizGen uses LangGraph to route queries through a state-driven graph with conditional branching between agents.
  - Quick check question: Can you sketch a state machine where a query can branch to either SQL Agent or directly to Customizer based on intent?

- Concept: **Text-to-SQL with schema grounding**
  - Why needed here: The system must retrieve database schema before generating queries to ensure column/table names match reality.
  - Quick check question: What metadata must be extracted from a database schema to enable accurate SQL generation?

- Concept: **Visualization grammar fundamentals (data types, encodings, chart taxonomies)**
  - Why needed here: Graph Ranker needs to map data characteristics to appropriate visual encodings (e.g., temporal → line chart, categorical comparison → bar chart).
  - Quick check question: Given a dataset with one categorical column and one continuous metric, which chart types are appropriate?

## Architecture Onboarding

- Component map:
  - **Intent Classifier** → routes to one or more of:
    - **SQL Agent** (Metadata Retriever → SQL Generator → Validator → Executor)
    - **Visualization Agent** (Data Preprocessor → Graph Ranker)
    - **Analysis Agent** (Insight Generator)
    - **Explanation Agent** (Query Generator → Search Engine → Explanation Generator)
    - **Customizer Node** (handles modification requests)
    - **System Node** (database connections, auth)
  - **Response Generator** ← compiles outputs from all agents

- Critical path: User query → Intent Classifier → SQL Agent (if data retrieval needed) → Visualization Agent → Response Generator. Most user flows require this sequence.

- Design tradeoffs:
  - Accuracy vs. latency: More capable text-to-SQL models introduced latency; required balancing (Section VI-B).
  - Flexibility vs. complexity: Supporting multiple database types (MySQL, PostgreSQL, MariaDB, MS SQL, Oracle) increased integration complexity (Section VI-B).
  - Single vs. multi-LLM: Uses Claude 3.7 Sonnet and Gemini 2.0 Flash—unclear if for different tasks or redundancy.

- Failure signatures:
  - Database connection failures due to driver incompatibility (Section VI-B).
  - Inconsistent graph recommendations requiring fine-tuning (Section VI-B).
  - Navigation rated 3.3/5—UI routing between features is a known weak point (Section V).
  - Multi-intent queries may produce incomplete responses if sequential agent activation fails silently.

- First 3 experiments:
  1. **Intent classification accuracy test**: Feed 50 sample queries with ground-truth intent labels; measure classifier precision/recall per intent category.
  2. **SQL generation validation rate**: Execute 100 generated SQL queries against a test database; log validation pass rate, execution success rate, and error types.
  3. **Graph recommendation consistency check**: Submit the same dataset with identical queries 10 times; measure recommendation stability (do top suggestions converge?).

## Open Questions the Paper Calls Out
None

## Limitations

- User Study Representativeness: Evaluation involved only 19 participants without demographic diversity details or professional background disclosure.
- SQL Generation Reliability: Paper doesn't provide quantitative metrics on SQL generation accuracy or failure rates.
- Graph Recommendation Quality: Lacks measurement of recommendation accuracy against ground truth or user satisfaction with specific recommendations.

## Confidence

- **High Confidence**: The architectural framework and agent orchestration approach are well-described and technically coherent. The multi-agent design pattern follows established principles in the field.
- **Medium Confidence**: Usability results are promising but limited by small sample size. The qualitative feedback is valuable but insufficient for strong claims about democratization.
- **Low Confidence**: Quantitative performance metrics for core technical components (SQL accuracy, visualization recommendation quality) are largely absent, making it difficult to assess real-world effectiveness.

## Next Checks

1. **SQL Generation Benchmark Test**: Execute 200 diverse natural language queries against databases with varying schema complexity, measuring generation accuracy, validation pass rates, and execution success rates compared to baseline end-to-end approaches.

2. **Visualization Recommendation Validation**: Conduct a controlled study where participants rate graph recommendations against expert-curated ground truth for 50 datasets, measuring recommendation accuracy and user satisfaction with specific suggestions.

3. **Stress Test Multi-Intent Handling**: Systematically test compound queries requiring sequential agent activation (e.g., "Show sales trends and highlight anomalies") to identify failure modes in the sequential processing pipeline and measure response completeness and coherence.