---
ver: rpa2
title: 'Behind Maya: Building a Multilingual Vision Language Model'
arxiv_id: '2505.08910'
source_url: https://arxiv.org/abs/2505.08910
tags:
- multilingual
- arxiv
- dataset
- maya
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Maya is a multilingual vision-language model built to address\
  \ performance gaps in low-resource languages and cultural contexts. The authors\
  \ created a multilingual image-text pretraining dataset by translating and extending\
  \ LLaVA\u2019s English dataset into eight languages, using a hybrid translation\
  \ pipeline with human review."
---

# Behind Maya: Building a Multilingual Vision Language Model

## Quick Facts
- arXiv ID: 2505.08910
- Source URL: https://arxiv.org/abs/2505.08910
- Reference count: 40
- Key outcome: Maya is a multilingual vision-language model built to address performance gaps in low-resource languages and cultural contexts

## Executive Summary
Maya is a multilingual vision-language model (VLM) designed to improve performance in low-resource languages and cultural contexts. The authors created a 4.4 million sample pretraining dataset by translating and extending LLaVA's English dataset into eight languages using a hybrid translation pipeline with human review. Maya employs SigLIP as the vision encoder and fine-tunes the multilingual LLM Aya-23 8B, achieving competitive performance on multilingual benchmarks and outperforming 7B models while matching 13B models on average, with notable gains in Arabic.

## Method Summary
The authors built Maya by first creating a multilingual pretraining dataset through translating LLaVA's 550K English samples into eight languages (Chinese, French, Spanish, Russian, Hindi, Japanese, Arabic) using a hybrid pipeline with human review, resulting in 4.4M samples. They employed SigLIP-base-patch16-256-multilingual as the vision encoder and fine-tuned the multilingual LLM Aya-23 8B on this dataset, followed by instruction tuning on a 150K sample multilingual dataset. The training used a 2-layer MLP projection layer with GELU activation, full fine-tuning rather than LoRA due to suboptimal results with equal learning rates for adapter matrices, and achieved competitive performance on multilingual benchmarks.

## Key Results
- Maya achieved competitive performance on multilingual benchmarks, outperforming 7B models and matching 13B models on average
- Notable performance gains in Arabic (63.4% vs PALO-7B's 57.8%)
- Demonstrated effective cross-lingual understanding on diverse VQA tasks
- Multilingual pretraining showed clear advantage over English-only pretraining with multilingual fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Multilingual pretraining data improves cross-lingual VLM performance compared to English-only pretraining with multilingual fine-tuning. By exposing the projection layer to aligned image-text pairs across eight languages during pretraining, the model learns language-agnostic visual representations that transfer more effectively during instruction tuning, even to unseen languages (Bengali, Urdu). Core assumption: Translation quality preserves sufficient semantic correspondence between images and non-English captions for alignment learning.

### Mechanism 2
SigLIP's multilingual adaptability and variable-resolution support provides better vision encoding for diverse cultural contexts than CLIP. SigLIP's scalable positional embeddings handle variable input dimensions through interpolation, while its multilingual training enables more culturally grounded visual feature extraction that aligns better with multilingual LLM representations. Core assumption: The vision encoder's multilingual pretraining transfers to cultural visual concepts, not just textual multilinguality.

### Mechanism 3
Full fine-tuning outperforms LoRA for multilingual VLM adaptation when adapter matrices share learning rates. Full parameter updates allow the LLM to adjust its multilingual representations more comprehensively for vision-grounded tasks, whereas LoRA's low-rank constraints may limit cross-modal adaptation capacity—particularly when both adapter matrices use identical learning rates. Core assumption: The observed suboptimality is primarily due to learning rate configuration rather than LoRA's fundamental rank constraints.

## Foundational Learning

- **Vision-Language Projection Layers**: Maya uses a 2-layer MLP to bridge SigLIP embeddings to Aya-23's embedding space; understanding this alignment is essential for modifying architecture or debugging cross-modal transfer. *Quick check: Can you explain why the projection layer is trained during pretraining while both encoder and LLM remain frozen?*

- **Cross-Lingual Transfer in Multilingual Models**: Maya fine-tunes on 10 languages but pretrains on 8; understanding why it still performs reasonably on Bengali and Urdu requires grasping transfer from related languages or from multilingual LLM backbone. *Quick check: Why might Arabic show particularly strong gains (63.4% vs PALO-7B's 57.8%) given its root-based morphology?*

- **BLEU Score for Translation Quality**: The paper uses BLEU thresholds (~0.47 average) to validate translation pipeline quality before committing to large-scale dataset generation. *Quick check: What are the limitations of BLEU for evaluating translation quality in a vision-language context where cultural nuance matters?*

## Architecture Onboarding

- **Component map**: Image → SigLIP (256×256 crop) → visual features Zv → Projection W → language tokens Hv → Aya-23 → response

- **Critical path**: Image → SigLIP → visual features Zv → Projection W → language tokens Hv → Aya-23 → response

- **Design tradeoffs**: 2-layer vs 4/8-layer MLP (paper tested deeper projections; 2-layer achieved lowest training loss); Full fine-tuning vs LoRA (LoRA failed with equal learning rates; full fine-tuning chosen at higher compute cost); 8 languages vs Aya's 23 (focused pretraining data quality over breadth; fine-tuning covered 10 languages including 2 unseen during pretraining)

- **Failure signatures**: If Arabic underperforms despite multilingual focus: check prompt template (Preamble 6) application consistency; If cross-lingual transfer fails for Bengali/Urdu: verify pretraining dataset language coverage and translation quality; If training loss plateaus early: examine projection layer initialization and learning rate schedule (1e-3 with cosine scheduler used)

- **First 3 experiments**: 1) Reproduce pretraining on a 10K sample subset across all 8 languages, monitoring per-language loss curves to verify balanced learning; 2) Ablate the projection layer depth (2 vs 4 layers) on a held-out language pair to confirm paper's finding that 2-layer is optimal; 3) Test LoRA with differential learning rates for matrices A and B to determine if full fine-tuning can be avoided under compute constraints

## Open Questions the Paper Calls Out

- Would advanced cross-modal alignment techniques (e.g., Q-Former or gated soft-attention) significantly outperform the 2-layer MLP projection used in Maya? *Basis: Section 3.2 states that techniques like Q-Former (BLIP-2) and gated soft-attention (Flamingo) were "set aside for future work."*

- How does unfreezing the LLM decoder layers during fine-tuning impact multilingual comprehension compared to the frozen-encoder approach? *Basis: Section 5 lists "unfreezing decoder layers for fine-tuning" as a specific avenue for future investigation.*

- Does expanding the pretraining phase to include Bengali and Urdu improve performance on these languages compared to the current instruction-tuning-only approach? *Basis: Section 5 proposes "expanding pretraining to Bengali and Urdu," which were included in fine-tuning but not the initial pretraining dataset.*

## Limitations

- Translation quality validation relies on BLEU scores (0.4-0.5) without confirming preservation of semantic-visual alignment necessary for effective vision-language training
- SigLIP vs CLIP superiority is asserted but not empirically validated through comparative experiments using identical training protocols
- Ablation testing scope limited to full 4.4M dataset without exploring statistical significance or smaller dataset generalization

## Confidence

- **High Confidence**: Experimental methodology for multilingual pretraining and instruction tuning is clearly described and reproducible; PALO benchmark evaluation methodology is sound
- **Medium Confidence**: Claim that multilingual pretraining improves cross-lingual transfer is well-supported, but exact contribution of pretraining vs instruction tuning remains unclear
- **Low Confidence**: Superiority of SigLIP over alternative vision encoders is asserted but not empirically validated; LoRA failure demonstrated but alternative adapter approaches unexplored

## Next Checks

1. **Translation Quality Impact Study**: Conduct controlled experiment varying translation quality (professional vs automated) on dataset subset, measuring impact on downstream VQA performance across languages to quantify translation quality threshold

2. **SigLIP vs CLIP Comparative Study**: Replicate full training pipeline using CLIP as vision encoder while holding all other variables constant, comparing performance on multilingual and English-only benchmarks to isolate vision encoder contribution

3. **Cross-Lingual Transfer Analysis**: Design experiments measuring how effectively Maya transfers from 8 pretraining languages to 2 unseen languages, including zero-shot transfer to languages not present in either pretraining or fine-tuning datasets