---
ver: rpa2
title: 'Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection
  Framework'
arxiv_id: '2507.16414'
source_url: https://arxiv.org/abs/2507.16414
tags:
- data
- training
- arxiv
- language
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting whether specific
  text data was included in the pre-training corpus of large language models (LLMs),
  which is important for copyright verification and privacy protection. The authors
  propose NA-PDD, a novel algorithm that analyzes differential neuron activation patterns
  between training and non-training data during LLM inference.
---

# Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection Framework

## Quick Facts
- arXiv ID: 2507.16414
- Source URL: https://arxiv.org/abs/2507.16414
- Reference count: 27
- Outperforms nine baselines across three benchmarks with up to 99.7% AUC score

## Executive Summary
This paper addresses the problem of detecting whether specific text data was included in the pre-training corpus of large language models (LLMs), which is important for copyright verification and privacy protection. The authors propose NA-PDD, a novel algorithm that analyzes differential neuron activation patterns between training and non-training data during LLM inference. NA-PDD identifies "member neurons" that are predominantly activated by training data and "non-member neurons" that respond more to non-training data, then uses these patterns to detect if a given text was part of the training corpus. The method achieves state-of-the-art performance, outperforming nine baseline methods across three benchmarks with up to 99.7% AUC score, particularly showing significant improvements over the second-best method (DC-PDD) by 27.9% AUC points. The paper also introduces CCNewsPDD, a temporally unbiased benchmark that addresses limitations in previous evaluation methods.

## Method Summary
NA-PDD is a white-box method that detects pre-training data by analyzing differential neuron activation patterns in LLMs. The algorithm requires white-box access to the target model with forward hooks on FFN layers to capture post-activation outputs. It uses a small reference corpus (100 training + 100 non-training samples) to identify "member neurons" (activated more by training data) and "non-member neurons" (activated more by non-training data) based on activation frequency ratios. The method computes discriminative scores for each layer, selects top-K layers with highest discrimination, and calculates a membership score as the ratio of average member-to-non-member similarity. Classification is performed using a threshold θ. The approach requires computing activation frequencies across reference data, ranking layers by discriminative scores, and aggregating member/non-member neuron responses for final inference.

## Key Results
- Achieves up to 99.7% AUC score across three benchmarks (WikiMIA, ArxivMIA, CCNewsPDD)
- Outperforms second-best method (DC-PDD) by 27.9% AUC points
- Introduces CCNewsPDD benchmark that addresses temporal bias in previous evaluation methods
- Shows stable performance across α dominance thresholds in [1.2, 2.0] range

## Why This Works (Mechanism)

### Mechanism 1: Differential Neuron Activation
Text samples included in the pre-training corpus activate a distinct set of "member neurons" compared to non-training data, which activates "non-member neurons." LLMs internalize training data by developing specific neural pathways, and during inference, inputs that were seen during training trigger these pathways, leading to higher activation values in specific neurons within the Feed-Forward Networks (FFN). The method detects these differential firing patterns rather than relying on surface-level loss metrics.

### Mechanism 2: Relative Dominance Filtering (Alpha Threshold)
Identifying useful neurons requires filtering for relative dominance rather than raw activation frequency. Many neurons activate frequently for all text, so the algorithm uses a dominance threshold α to classify a neuron as "member" only if its activation frequency on training data is significantly higher (e.g., 1.5×) than on non-training data. This suppresses noise from general-purpose neurons.

### Mechanism 3: Layer-Wise Discriminative Selection
Pre-training data detection is most effective when focusing on specific layers with high "member neuron" density. Different layers perform different functions, and the algorithm calculates a discriminative score for each layer based on the imbalance between member and non-member neurons. It selects the top K layers to aggregate the final membership score, ignoring layers that do not differentiate well.

## Foundational Learning

**White-box Inference Access**
Why needed here: Unlike standard membership inference (MIA) which often assumes black-box (output-only) access, NA-PDD requires attaching "hooks" to internal FFN layers to read activation values (a_n(x)).
Quick check question: Do you have the model weights/architecture access to instrument the forward pass, or only an API endpoint?

**Reference Corpus Construction**
Why needed here: The method relies on "Neuronal Identity Discrimination" which requires a small set of known training and known non-training data to calibrate the "member" vs. "non-member" neuron thresholds.
Quick check question: Can you supply 100-200 samples that are definitively in the training set and 100-200 that are definitively out?

**Temporal Distribution Drift**
Why needed here: The paper introduces CCNewsPDD to address a failure mode in prior benchmarks where "non-training" data was simply newer data. The model might recognize the style of new data differently, leading to false positives.
Quick check question: Is your "non-training" control set temporally aligned with the training set, or is it from a different time period (e.g., strictly future data)?

## Architecture Onboarding

**Component map:**
Input Processor -> Instrumented LLM (with forward hooks on FFN layers) -> Neuron Analyzer (computes activation states and frequency ratios) -> Layer Selector (ranks layers by discriminative score) -> Membership Scorer (calculates R(x,M) ratio)

**Critical path:** The setup of the Neuron Identity Discrimination (Section 3.4). If the reference data is poor, the neuron labels (Member vs. Non-Member) will be garbage, rendering the final inference useless regardless of the threshold θ.

**Design tradeoffs:**
- Accuracy vs. Applicability: Achieves high AUC (up to 99.7%) but strictly requires white-box access (model weights). Cannot run on closed APIs (e.g., GPT-4).
- Efficiency vs. Granularity: Requires storing activation states for thousands of neurons. Memory overhead is higher than loss-based methods.

**Failure signatures:**
- High False Positives on Technical Data: Lower performance on ArxivMIA suggests technical/scientific text may be harder to distinguish via this mechanism than news text.
- Breakdown on Small Models: Performance generally improves with model size; very small models (< 1B params) may lack distinct enough neuron specialization.

**First 3 experiments:**
1. Sanity Check (Activation Hook): Run a single batch through the target model and verify that the hook captures non-zero FFN activation tensors.
2. Parameter Sweep (Alpha/Tau): Validate the dominance threshold (α) on a held-out validation set. The paper suggests α=1.5, but verify if f_train > 1.5 · f_non actually yields distinct neuron sets for your specific model.
3. Benchmark Reproduction: Run NA-PDD on the WikiMIA or CCNewsPDD subset using Pythia-2.8B. Confirm you achieve >75% AUC before testing on proprietary data.

## Open Questions the Paper Calls Out

**Open Question 1**
Does NA-PDD maintain its performance advantage when applied to LLMs significantly larger than 13 billion parameters?
Basis in paper: [explicit] The authors state in the Limitations section that due to computational constraints, NA-PDD "has only been evaluated on LLMs with up to 13 billion parameters" and they "plan to explore this in future work."
Why unresolved: It is unclear if the distinct neuron activation patterns hold, degrade, or improve in much larger models (e.g., 70B+ parameters) where representations are more distributed.
What evidence would resolve it: Evaluation of NA-PDD on open-source models with parameter counts exceeding 13B (e.g., LLaMA-2 70B) using the CCNewsPDD benchmark.

**Open Question 2**
Can modeling complex neuronal activation pathways, rather than just individual neuron states, further enhance detection accuracy?
Basis in paper: [explicit] The Conclusion states, "In future work, we aim to explore more complex neuronal activation patterns, such as how activation pathways can further enhance PDD."
Why unresolved: The current method aggregates activation frequencies of individual neurons ("member" vs "non-member") but ignores the structural connectivity or sequential activation circuits within the network.
What evidence would resolve it: A comparative study showing AUC improvements when using graph-based path analysis of activations versus the current frequency-based aggregation method.

**Open Question 3**
What architectural or data-driven factors cause the significant performance drop in technical domains like arXiv papers?
Basis in paper: [inferred] The results show NA-PDD achieves only 57.2% AUC on ArxivMIA compared to ~99% on CCNewsPDD. The authors speculate this is due to the "technical nature" of the text, but the precise mechanism (e.g., neuron saturation, distinct tokenization) is not investigated.
Why unresolved: It is undetermined if the lower performance is an inherent limitation of the neuron activation approach for specialized text or a hyperparameter sensitivity issue.
What evidence would resolve it: Ablation studies on technical datasets analyzing neuron overlap (member vs. non-member) to see if the "dominance" separation fails in specific domains.

## Limitations
- Requires white-box access to target models, limiting applicability to closed APIs like GPT-4
- CCNewsPDD benchmark is not publicly available at time of writing
- Performance drops significantly on technical/scientific text domains (57.2% AUC on ArxivMIA vs ~99% on CCNewsPDD)
- Higher computational overhead than loss-based methods due to activation state storage

## Confidence

**High Confidence (80-100%)**: The differential activation mechanism and layer-wise selection approach are well-supported by empirical results. The 27.9% AUC improvement over DC-PDD is robust across multiple benchmarks.

**Medium Confidence (50-80%)**: The general applicability across diverse model architectures and text domains. While results are strong on news text, the reduced performance on technical domains suggests potential domain-specific limitations.

**Low Confidence (0-50%)**: Long-term stability of neuron activation patterns across model updates and fine-tuning scenarios. The paper doesn't address how performance degrades when models are fine-tuned or when training data distribution shifts significantly.

## Next Checks

1. **Activation Hook Verification**: Run a single inference pass through the target model and verify that forward hooks correctly capture non-zero FFN activation tensors after the activation function (GELU/ReLU).

2. **Parameter Calibration**: Perform a grid search on α dominance threshold (1.2-2.0) and τ activation threshold (1.0-2.0) using a held-out validation set to verify optimal values match paper specifications.

3. **Temporal Bias Test**: Construct a simple test comparing performance when non-training data is strictly future data versus temporally matched control data to reproduce the CCNewsPDD motivation.