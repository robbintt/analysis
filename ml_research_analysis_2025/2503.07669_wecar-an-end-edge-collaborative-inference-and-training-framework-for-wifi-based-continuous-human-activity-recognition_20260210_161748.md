---
ver: rpa2
title: 'WECAR: An End-Edge Collaborative Inference and Training Framework for WiFi-Based
  Continuous Human Activity Recognition'
arxiv_id: '2503.07669'
source_url: https://arxiv.org/abs/2503.07669
tags:
- task
- distillation
- wecar
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WECAR, an end-edge collaborative framework
  for WiFi-based continuous human activity recognition that tackles the dual challenges
  of incremental learning and resource constraints. By decoupling training and inference
  across edge and end devices, WECAR enables adaptive recognition of new activities
  without catastrophic forgetting while preserving privacy and reducing bandwidth
  usage.
---

# WECAR: An End-Edge Collaborative Inference and Training Framework for WiFi-Based Continuous Human Activity Recognition

## Quick Facts
- arXiv ID: 2503.07669
- Source URL: https://arxiv.org/abs/2503.07669
- Reference count: 40
- Primary result: Up to 40% higher accuracy than SOTA methods with 90% reduction in model parameters

## Executive Summary
WECAR is an end-edge collaborative framework for WiFi-based continuous human activity recognition that tackles the dual challenges of incremental learning and resource constraints. By decoupling training and inference across edge and end devices, WECAR enables adaptive recognition of new activities without catastrophic forgetting while preserving privacy and reducing bandwidth usage. The framework integrates a transformer-based architecture with dynamic model expansion via task-specific prefixes and stability-aware selective retraining, alongside a dual-phase distillation mechanism for lightweight deployment.

## Method Summary
WECAR uses a transformer encoder with Gaussian range encoding for WiFi CSI feature extraction. The key innovation is task-specific prefix expansion in multi-head self-attention (MHSA) layers, where new tasks learn only prefix parameters while base weights remain frozen. Stability-aware selective MLP retraining freezes neurons with stable activation patterns across tasks. Dual-phase knowledge distillation compresses the Full-Scale Model (FSM) to a Lightweight Model (LWM) for deployment on resource-constrained end devices. The edge device (Jetson Nano) handles training and optimization, while the end device (ESP32) performs real-time inference without storing raw CSI data.

## Key Results
- Achieves up to 40% higher accuracy than state-of-the-art methods on three public WiFi datasets
- Reduces model parameters by 90% while maintaining performance within 2-4% of full model
- Maintains low forgetting rates (<21%) across incremental task sequences
- Reduces GPU memory usage by 25% through distillation

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Prefix Expansion in MHSA Layers
Prefix-based expansion enables new task learning without overwriting frozen base weights, preserving prior knowledge while adding plasticity. Trainable key/value prefixes (P^t_K, P^t_V) are concatenated with frozen historical prefixes and frozen W_K/W_V weights. New tasks learn only prefix parameters; base transformer weights remain fixed. Parallel adapter initializes prefixes via down/up projections for stability. Core assumption: WiFi CSI features for different tasks share a common temporal representation base that prefixes can specialize without interference.

### Mechanism 2: Stability-Aware Selective MLP Retraining
Freezing neurons with stable activation patterns across tasks preserves historical knowledge while allowing unstable neurons to adapt. Compute per-neuron average activation per layer; classify as stable if ||ā^(t) - ā^(t-1)||₂ ≤ ε. Generate freeze masks M_W, M_b; zero gradients at stable positions during backprop. Core assumption: Neuron activation stability correlates with importance for prior tasks; unstable neurons are safe to modify.

### Mechanism 3: Dual-Phase Knowledge Distillation
Two-stage distillation (MHSA relation → prefix relation) enables 60-70% parameter reduction with only 2-4% accuracy drop. Phase 1 (initial): Attention relation loss L_AT + value relation loss L_VR transfer MHSA patterns. Phase 2 (incremental): Prefix relation loss L_P compresses historical prefix knowledge into student's single-task prefixes; student does not store frozen historical prefixes. Core assumption: Student can approximate teacher's multi-task prefix behavior through contrastive alignment without explicit prefix storage.

## Foundational Learning

- **Catastrophic Forgetting in Incremental Learning**
  - Why needed here: WECAR's entire design addresses this; without understanding it, the prefix/selective retraining rationale is opaque.
  - Quick check question: If you fine-tune a model on Task B after training on Task A without any mitigation, what happens to Task A accuracy?

- **Transformer Multi-Head Self-Attention (MHSA)**
  - Why needed here: Core architecture; prefixes modify Q/K/V projections in attention heads.
  - Quick check question: In scaled dot-product attention, why is √d_h used as a scaling factor?

- **Knowledge Distillation**
  - Why needed here: Enables FSM→LWM compression for ESP32 deployment.
  - Quick check question: What is the difference between logits distillation and feature/attention distillation?

## Architecture Onboarding

- **Component map:** CSI data → Edge (Jetson Nano) trains FSM with prefix expansion → Dual-phase distillation → LWM deployment → End (ESP32) performs real-time inference

- **Critical path:**
  1. Initial training: Train full FSM on C₁; freeze MHSA weights
  2. Initial lightweight: MHSA relation distillation → deploy LWM
  3. Incremental task t: Expand prefixes (P^t), selective MLP retrain, prefix distillation → deploy updated LWM
  4. Repeat for each new activity class set

- **Design tradeoffs:**
  - Prefix length vs. memory: Longer prefixes = better plasticity but more edge storage; paper uses adapter bottleneck
  - ε threshold tuning: Task-specific; WiAR/MMFi/XRF may need different values
  - Student MLP width: Directly controls ESP32 feasibility (paper: 0.58M-1.33M params final)

- **Failure signatures:**
  - Accuracy crashes on Task 1 after Task 3 training → prefix expansion not isolating, or ε too loose
  - ESP32 OOM → student dimension still too large; check ONNX binary size
  - L_VR/L_P not decreasing → distillation hyperparameters need scaling

- **First 3 experiments:**
  1. **Sanity check:** Train FSM on WiAR Task 1 only, verify baseline accuracy; confirm freezing mechanism (no gradient updates to W_Q, W_K, W_V after epoch 1)
  2. **Forgetting baseline:** Add Task 2 without prefix expansion or selective retraining; measure accuracy drop on Task 1 (expect catastrophic forgetting per Fig. 1)
  3. **Ablation on ε:** Run full WECAR on WiAR long task (N=8) with ε ∈ {0.01, 0.05, 0.1}; plot Task 1 accuracy retention vs. final task accuracy to find stability-plasticity balance

## Open Questions the Paper Calls Out

- **How does WECAR perform when deployed with real-time CSI data collection and transmission, rather than simulated datasets?**
  - Basis: The implementation section states, "We use the existing dataset to simulate the data collected by the ESP32," implying the validation was not performed on a fully live, end-to-end data stream.
  - Why unresolved: The experimental validation relies on pre-processed, static public datasets (WiAR, MMFi, XRF) rather than live signal capture which introduces latency, packet loss, and synchronization issues.
  - What evidence would resolve it: An experiment deploying WECAR in a live smart home setting with real-time activity execution and online model updates.

- **How does the accumulation of task-specific prefixes in the Full-Scale Model (FSM) impact training efficiency and memory on edge devices over very long task sequences?**
  - Basis: The paper defines the frozen prefix update as a concatenation $P_{frozen}^t = [P_K^t, P_{K,frozen}^{t-1}]$, which increases the FSM's parameter count linearly with the number of tasks.
  - Why unresolved: While the Lightweight Model (LWM) is compressed via distillation, the FSM on the Jetson Nano must still load and process these growing prefixes during the incremental training phase, potentially exceeding edge memory limits over "lifelong" horizons.
  - What evidence would resolve it: Analysis of training memory consumption and latency on the Jetson Nano across a significantly larger number of incremental tasks (e.g., >50 tasks).

- **Is the linear interpolation method for handling empty frames sufficient for high-noise or highly unstable WiFi environments?**
  - Basis: The paper states, "Due to environmental interference or signal instability... we employ linear interpolation to fill the missing values," but does not evaluate the system's robustness against severe data loss.
  - Why unresolved: Linear interpolation assumes a relatively continuous signal trend; it may fail to accurately reconstruct complex activity patterns if large segments of data are missing due to severe interference.
  - What evidence would resolve it: Ablation studies injecting varying percentages of empty frames or noise into the CSI stream to measure recognition accuracy degradation.

## Limitations

- Several critical hyperparameters are unspecified, including MLP dimensions, stability threshold ε, and distillation loss weights, which could significantly impact reproducibility
- The reliance on adapter-based prefix initialization may limit scalability to very large task sets
- The evaluation focuses on three specific datasets with predefined task splits, leaving generalizability to other HAR domains unclear

## Confidence

- **High Confidence**: The core architecture of prefix-based expansion with frozen base weights is well-specified and mechanistically sound. The dual-phase distillation approach for model compression is explicitly detailed and experimentally validated.
- **Medium Confidence**: The stability-aware selective retraining mechanism is formally defined but lacks empirical justification for the activation stability criterion. The choice of ε threshold is critical but unspecified.
- **Low Confidence**: Claims about the superiority of Prefix-A (adapter initialization) over Prefix-Z/R lack rigorous ablation; the 3% improvement is stated without detailed experimental validation.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary ε (stability threshold) across {0.01, 0.05, 0.1} on WiAR long task (N=8) to quantify the stability-plasticity tradeoff curve.

2. **Prefix Initialization Ablation**: Implement and compare Prefix-A (adapter), Prefix-Z (zero), and Prefix-R (random) initialization across all three datasets to verify the claimed 3% accuracy advantage.

3. **Model Capacity Scaling**: Vary the student MLP dimension (below and above the 0.58M-1.33M range) to determine the minimum viable model size that maintains <4% accuracy drop post-distillation.