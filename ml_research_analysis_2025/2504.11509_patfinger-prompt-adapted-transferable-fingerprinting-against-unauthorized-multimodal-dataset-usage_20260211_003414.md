---
ver: rpa2
title: 'PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized
  Multimodal Dataset Usage'
arxiv_id: '2504.11509'
source_url: https://arxiv.org/abs/2504.11509
tags:
- dataset
- retrieval
- verification
- prompt
- patfinger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unauthorized usage of multimodal
  datasets in vision-language models, where dataset owners invest significant effort
  in data collection and cleaning. Existing methods for dataset ownership verification
  are either intrusive (degrading model accuracy) or non-intrusive (relying on label-driven
  boundaries, ineffective for cross-modal data).
---

# PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage

## Quick Facts
- arXiv ID: 2504.11509
- Source URL: https://arxiv.org/abs/2504.11509
- Authors: Wenyi Zhang; Ju Jia; Xiaojun Jia; Yihao Huang; Xinfeng Li; Cong Wu; Lina Wang
- Reference count: 40
- One-line primary result: Non-intrusive fingerprinting scheme achieves >90% verification confidence on multimodal datasets while maintaining model performance and robustness to pruning.

## Executive Summary
PATFinger addresses unauthorized usage of multimodal datasets in vision-language models by creating a non-intrusive ownership verification mechanism. Unlike previous intrusive methods that degrade model accuracy or non-intrusive methods that fail on cross-modal data, PATFinger uses Global Optimal Perturbation (GOP) and adaptive prompts to capture dataset-specific distribution characteristics without modifying original data. The method generates a perturbation that maximizes embedding drifts between modalities and aligns it with an adaptive prompt to profile cross-modal decision boundaries, achieving over 90% verification confidence while outperforming state-of-the-art baselines by 30%.

## Method Summary
PATFinger generates a dataset fingerprint through three stages: First, a surrogate CLIP model is fine-tuned on the protected dataset. Second, a GOP generator is trained to create perturbations that maximize semantic drift between image and text embeddings while maintaining intra-modal consistency. Third, adaptive prompts are optimized to align with GOP samples using a token network that projects continuous prompts to discrete, interpretable tokens. Verification is performed by applying the GOP to images and concatenating the adaptive prompt to target labels, checking if the model retrieves the target label with high confidence. The method achieves transferability across different model architectures through the textual constraint mechanism.

## Key Results
- Achieves over 90% verification confidence on four benchmark datasets (COCO, Flickr, Open-images, TextCaps)
- Outperforms state-of-the-art baselines by 30% in verification success rate
- Maintains model performance without degrading accuracy
- Demonstrates robustness against dataset pruning up to 80% and fine-tuning
- Provides non-intrusive verification without modifying training data

## Why This Works (Mechanism)

### Mechanism 1
Dataset ownership can be verified by generating a Global Optimal Perturbation (GOP) that maximizes semantic drift between modalities. The system trains a generator to create a perturbation that pushes image embeddings away from original text embeddings while pulling toward adversarial embeddings, reinforced by intra-modal and inter-modal discriminators. **Assumption:** A perturbation optimized to drift across dataset-specific decision boundaries will trigger consistent behavior in models fine-tuned on that data. **Evidence:** [abstract] "GOP maximizes embedding drifts between different modalities." **Break condition:** If the generator overfits to the surrogate architecture, the perturbation will fail to transfer to black-box models.

### Mechanism 2
Continuous prompts can be constrained to discrete, interpretable tokens to profile cross-modal interactions while maintaining transferability. An adaptive prompt is optimized to align with GOP samples, with a token network projecting continuous prompt vectors to the nearest readable discrete tokens. **Assumption:** Mapping continuous prompt embeddings to discrete tokens preserves fingerprint information while ensuring readability across different tokenizers. **Evidence:** [section 4.3] "We design a token network to additionally impose constraints on readability." **Break condition:** If projection to discrete tokens drifts too far from the continuous optimum, verification confidence will drop in black-box settings.

### Mechanism 3
Models fine-tuned on the same dataset share consistent "perturbation affinities," allowing the GOP and adaptive prompt pair to act as a non-intrusive fingerprint. Verification queries are constructed by applying the GOP to images and concatenating the adaptive prompt to target labels. **Assumption:** The dataset fingerprint is an intrinsic property of the data distribution manifesting in model decision boundaries. **Evidence:** [abstract] "Utilizes inherent dataset attributes as fingerprints instead of compelling the model to learn triggers." **Break condition:** If the target model uses significantly different pre-training objectives, the alignment of decision boundaries may not hold.

## Foundational Learning

- **Concept**: **Contrastive Language-Image Pre-training (CLIP)**
  - **Why needed here**: GOP generation relies on maximizing distance between image and text embeddings within CLIP's joint embedding space. **Quick check**: If you maximize cosine similarity between a perturbed image embedding and a random text embedding, what happens to their semantic relationship in a CLIP model?

- **Concept**: **Universal Adversarial Perturbations (UAP)**
  - **Why needed here**: GOP is essentially a UAP tailored for dataset fingerprinting - a single perturbation vector that generalizes across dataset samples to cause specific model behavior. **Quick check**: How does a UAP differ from a standard adversarial example generated for a single specific image?

- **Concept**: **Prompt Tuning (Continuous vs. Discrete)**
  - **Why needed here**: The method bridges continuous prompts (high performance, low transferability) and discrete prompts (high transferability, low performance) to create a "transferable fingerprint." **Quick check**: Why do continuous prompts typically fail to transfer between different model architectures?

## Architecture Onboarding

- **Component map**: Surrogate Model -> GOP Generator -> Discriminators -> Prompt Learner -> Token Network -> Suspicious Model
- **Critical path**:
  1. Optimize GOP: Train generator against discriminators using surrogate model to minimize transferability and adversarial losses
  2. Freeze GOP: Fix the perturbation
  3. Optimize Prompt: Train adaptive prompt on surrogate model using frozen GOP samples with textual constraint
  4. Verify: Export static perturbation and discrete text prompt to query suspicious black-box model

- **Design tradeoffs**:
  - **Perturbation Budget (σ)**: Higher budget increases verification success but risks visual detectability
  - **Prompt Length**: Length 4 is sufficient; longer prompts may overfit surrogate model
  - **Surrogate Selection**: ViT-B/32 surrogate provides best balance for transfer to other architectures

- **Failure signatures**:
  - **Unverifiable Ownership**: Model returns true label despite prompt, indicating no dataset training
  - **False Positive**: Benign model matches adversarial retrieval, suggesting GOP overfitted to general features

- **First 3 experiments**:
  1. **Sanity Check (White-box)**: Train surrogate CLIP on COCO, generate PATFinger, verify surrogate retrieves target label with >90% confidence
  2. **Transferability Test (Black-box)**: Test fingerprint from step 1 on BLIP model fine-tuned on COCO, check if R@1 drops significantly
  3. **Robustness to Pruning**: Randomly remove 20-80% of training data, attempt verification, observe if ΔR remains stable

## Open Questions the Paper Calls Out

### Open Question 1
Can PATFinger be adapted for ownership verification in generative Vision-Language Models (e.g., LLaVA, Stable Diffusion) where output is free-form text or synthesized images rather than retrieval rankings? The methodology is strictly confined to discriminative cross-modal retrieval tasks, but LLaVA-Med is mentioned as a VLM application. This requires testing on generative models where verification is triggered by specific textual outputs or image generation patterns.

### Open Question 2
Is PATFinger robust against adaptive attacks designed to smooth decision boundaries or obfuscate the GOP without degrading model utility? Section 5.4 evaluates robustness against pruning, fine-tuning, and Clean-CLIP defense, but not against adversaries who might detect and remove the GOP signature using gradient masking or boundary tilting techniques.

### Open Question 3
How does semantic similarity between surrogate and suspicious models affect the theoretical lower bound of verification confidence? Table 4 shows high variance in black-box transferability, with fingerprints from CLIPVL14 performing significantly worse on other architectures than CLIPVT32. The paper states this is "possibly due to the proximity of the semantic space" but does not formalize this dependency.

## Limitations
- Architecture dependence shows significant performance degradation when transferring between different model architectures (R@1 drops from ~90% to ~50-60%)
- Real-world applicability limited to controlled scenarios; not tested against sophisticated adversarial defenses like gradient masking
- Generalization scope restricted to vision-language retrieval tasks; effectiveness on other multimodal tasks like visual question answering remains unproven

## Confidence

**High Confidence (8/10)**: Core mechanism of using GOP to maximize semantic drift and adaptive prompts for boundary profiling is technically sound and well-supported by CLIP's contrastive learning framework.

**Medium Confidence (6/10)**: "Non-intrusive" fingerprinting claim partially supported - method doesn't modify training data, but GOP perturbation may be visually perceptible in some regions.

**Low Confidence (4/10)**: "Robust against dataset pruning and fine-tuning" needs qualification - verification confidence degrades linearly with pruning ratio and extreme pruning (>90%) would likely break the fingerprint.

## Next Checks

1. **Transferability Stress Test**: Evaluate PATFinger against 10+ diverse VLM architectures (ALIGN, Florence, Flamingo) to quantify true cross-architecture transferability limits and identify which architectural differences most disrupt the fingerprint.

2. **Adversarial Defense Evaluation**: Test PATFinger against common adversarial defenses including JPEG compression, Gaussian noise addition, and feature squeezing to establish practical robustness thresholds and identify potential countermeasures.

3. **Long-Term Stability Assessment**: Train models on datasets with PATFinger protection, then subject them to 6+ months of continuous fine-tuning on unrelated data to measure how quickly the fingerprint degrades and whether periodic regeneration is necessary.