---
ver: rpa2
title: 'TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained
  Evaluation'
arxiv_id: '2505.20016'
source_url: https://arxiv.org/abs/2505.20016
tags:
- tool
- tool-use
- ttpa
- tools
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TTPA, a training framework for tool-use in
  large language models that addresses limitations in fine-grained optimization and
  preference alignment. The core method uses reversed dataset construction to generate
  high-quality multi-turn tool-use data, token-level preference sampling to capture
  fine-grained differences during generation, and an error-oriented scoring mechanism
  to quantify tool-call errors.
---

# TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation

## Quick Facts
- arXiv ID: 2505.20016
- Source URL: https://arxiv.org/abs/2505.20016
- Authors: Chengrui Huang; Shen Gao; Zhengliang Shi; Dongsheng Wang; Shuo Shang
- Reference count: 40
- Primary result: Up to 39.7% accuracy improvement on tool selection, parameter filling, and return value parsing

## Executive Summary
TTPA introduces a training framework for fine-grained tool-use alignment in large language models, addressing limitations in current preference optimization methods. The framework combines reversed dataset construction, token-level preference sampling, and an error-oriented scoring mechanism to create high-quality preference pairs for direct preference optimization. Experiments show significant improvements in tool selection accuracy, parameter filling, and error discrimination across multiple benchmark datasets, with the approach demonstrating strong generalization capabilities.

## Method Summary
TTPA employs a three-stage approach: first, reversed dataset construction generates high-quality tool-use scenarios by deriving queries from answers rather than generating queries first; second, token-level preference sampling captures fine-grained differences during generation by sampling multiple variants when model uncertainty is high; third, an error-oriented scoring mechanism evaluates tool calls using a rule-based taxonomy to create preference pairs for direct preference optimization training. The framework is implemented on top of DPO with specific hyperparameters including learning rate 1e-4, cosine scheduler, and LoRA fine-tuning.

## Key Results
- 39.7% accuracy improvement on ToolBench compared to baseline models
- Significant gains in tool selection (up to 39.7%), parameter filling (up to 30.6%), and return value parsing (up to 21.3%)
- Strong generalization across different base models (Qwen2.5-7B, Llama3-8B) and datasets
- Competitive performance on general capability benchmarks while improving tool-use specific tasks

## Why This Works (Mechanism)

### Mechanism 1: Reversed Dataset Construction
- Generates queries from answer-tool call sequences rather than forward generation
- Ensures every query is answerable and prevents tool-name/parameter leakage
- Addresses quality bottleneck of unanswerable queries and information leakage in forward methods

### Mechanism 2: Token-level Preference Sampling (TPS)
- Samples at token boundaries where model uncertainty is high (probability gap < threshold ε)
- Creates more discriminative preference pairs than trajectory-level sampling
- Addresses fine-grained token-level errors that can invalidate entire structured tool calls

### Mechanism 3: Error-oriented Scoring Mechanism (ESM)
- Uses rule-based error taxonomy across six error types with weighted aggregation
- Provides reliable training signal without noisy LLM judgments
- Decomposes tool-call correctness into enumerable error types

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: TTPA uses DPO to align model with token-level preferences without separate reward model
  - Quick check question: Can you explain why DPO avoids training a separate reward model compared to RLHF?

- Concept: Structured Output Generation (JSON/function calling)
  - Why needed here: Tool calls require strict syntactic correctness where single token errors can invalidate entire call
  - Quick check question: What types of errors in JSON generation would cause a tool call to fail entirely?

- Concept: Preference Pair Construction
  - Why needed here: TPS samples multiple variants scored by ESM to create (preferred, dispreferred) pairs
  - Quick check question: If all sampled variants have the same ESM score, what happens to the preference signal?

## Architecture Onboarding

- Component map: Generator LLM → Scenario → Tool calls → Answer → Query → Token-level Preference Sampling → Error-oriented Scoring → DPO Training
- Critical path:
  1. Ensure reversed construction produces diverse, answerable queries
  2. Validate ESM scoring against manually labeled tool calls
  3. Run TPS only on tokens with uncertainty > threshold ε; verify preference pairs are discriminative
- Design tradeoffs:
  - Sampling granularity vs. compute cost: Token-level sampling is expensive
  - Rule-based vs. learned scoring: ESM is deterministic but may not generalize to complex validation
  - Reversed vs. forward data: Reversed avoids unanswerable queries but may produce less diverse scenarios
- Failure signatures:
  - Low preference pair discriminability → ESM weights may need adjustment or ε is too low
  - High variance in tool-call success rate across categories → Error taxonomy may be incomplete
  - Model overfits to training tool names → Check queries for implicit hints
- First 3 experiments:
  1. Ablation on data construction: Train with forward vs. reversed data on ToolBench subsets
  2. ESM weight sensitivity: Vary error-type weights and measure impact on fine-grained accuracy
  3. Sampling threshold ε sweep: Run TPS with different ε values; plot preference pair discriminability vs. number of pairs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the computational overhead of Token-level Preference Sampling be reduced to ensure scalability without compromising alignment quality?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that "conducting fine-grained token-level preference sampling may lead to an increase in computational complexity, requiring higher computational resources and extending the overall training time."
- **Why unresolved:** The current framework necessitates sampling multiple candidates based on probability distributions during generation, which is resource-intensive compared to standard SFT.
- **What evidence would resolve it:** Successful integration of efficient inference methods or optimized sampling algorithms that maintain the fine-grained alignment benefits while significantly lowering training duration and resource usage.

### Open Question 2
- **Question:** Can the TTPA framework be effectively adapted to dynamic environments where the toolset evolves in real-time?
- **Basis in paper:** [explicit] The paper notes that "training data is based on a predefined static set of tools, whereas in practical applications, the external environment is dynamically changing."
- **Why unresolved:** The current Reversed Dataset Construction relies on a static candidate tool set ($T_{can}$), and the model's ability to adapt to new tools or APIs without re-training is unverified.
- **What evidence would resolve it:** Extension of the method to a dynamic tool library setting and evaluation of the model's adaptability to unseen tools or changing API structures.

### Open Question 3
- **Question:** Are the empirically set error weights in the Error-oriented Scoring Mechanism (ESM) optimal across different domains, or do they require adaptive tuning?
- **Basis in paper:** [inferred] While the paper introduces specific error weights ($\omega_i$), Appendix A.3 states they were "empirically set based on preliminary observations" and require "more thorough investigation."
- **Why unresolved:** It is unclear if the fixed hierarchy of error severity (e.g., format vs. value errors) generalizes well to all tool-use scenarios or if it introduces bias.
- **What evidence would resolve it:** A sensitivity analysis showing the impact of different weight configurations on various domains, or the development of a mechanism to learn these weights dynamically.

## Limitations

- Computational overhead of token-level preference sampling requiring higher resources and extended training time
- Reliance on static predefined tool set limiting adaptation to dynamic environments with evolving toolsets
- Empirically set error weights in ESM that may not generalize across different domains without further investigation

## Confidence

**High Confidence Claims**:
- TTPA improves tool-use performance metrics on tested benchmarks
- Token-level alignment is more discriminative than trajectory-level alignment for structured outputs
- ESM provides deterministic alternative to LLM-based scoring for tool calls

**Medium Confidence Claims**:
- Reversed dataset construction produces higher-quality training data than forward methods
- TPS captures meaningful fine-grained preferences (limited validation shown)
- TTPA generalizes across different base models

**Low Confidence Claims**:
- TTPA will generalize to toolsets significantly larger or different from the 114 APIs used
- The method scales to real-time API environments with dynamic tool definitions
- ESM weights are optimal and will transfer to other tool-use domains

## Next Checks

1. **Ablation Study on Data Construction**: Implement both forward and reversed dataset construction methods with identical tool sets and training procedures. Measure not just pass rates but also query-answer pair quality metrics (e.g., fraction of unanswerable queries, tool-name leakage) to quantify the claimed advantage of reversed construction.

2. **ESM Robustness Testing**: Create a held-out test set with tools and tool configurations not present in training data. Manually label tool-call correctness and compare against ESM scores. Measure inter-annotator agreement and ESM's ability to detect edge cases like optional parameters, complex type hierarchies, or tools with conditional parameter dependencies.

3. **Sampling Threshold Sensitivity Analysis**: Systematically vary ε (e.g., 0.05, 0.1, 0.2, 0.3) and measure the resulting preference pair quality (score gap distribution, fraction of pairs with score difference > 0.5) and downstream performance. Plot preference pair discriminability vs. performance to identify the optimal operating point and determine if the dynamic threshold strategy is necessary.