---
ver: rpa2
title: Gender Fairness of Machine Learning Algorithms for Pain Detection
arxiv_id: '2506.11132'
source_url: https://arxiv.org/abs/2506.11132
tags:
- pain
- fairness
- learning
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender fairness in machine learning and deep
  learning models for automated pain detection. The authors evaluate two classical
  ML algorithms (Linear SVM and RBF SVM) and two deep learning models (CNN and Vision
  Transformer) using the UNBC-McMaster Shoulder Pain Expression Archive Database.
---

# Gender Fairness of Machine Learning Algorithms for Pain Detection

## Quick Facts
- arXiv ID: 2506.11132
- Source URL: https://arxiv.org/abs/2506.11132
- Authors: Dylan Green; Yuting Shang; Jiaee Cheong; Yang Liu; Hatice Gunes
- Reference count: 40
- Primary result: ViT achieved highest accuracy (0.9806) but all models exhibited gender-based biases

## Executive Summary
This paper evaluates gender fairness in machine learning and deep learning models for automated pain detection using the UNBC-McMaster Shoulder Pain Expression Archive Database. The authors assess two classical ML algorithms (Linear SVM and RBF SVM) and two deep learning models (CNN and Vision Transformer) across multiple performance and fairness metrics. While the Vision Transformer achieved the highest overall performance, all models exhibited gender-based biases, highlighting the persistent trade-off between accuracy and fairness. The study suggests ViT's architecture may be less prone to overfitting dataset biases, though no model excelled across all metrics simultaneously.

## Method Summary
The study uses the UNBC-McMaster Shoulder Pain Expression Archive Database with 48,398 facial images from 25 subjects. Data is pre-processed through stratified 60/20/20 splitting, SMOTE oversampling of the minority pain class, and data augmentation (flip, rotate, crop, histogram equalization). Four models are evaluated: Linear SVM and RBF SVM using HOG features, and CNN (ResNet-50) and ViT using ImageNet normalization. Performance is measured using Accuracy, F1 Score, and ROC AUC, while fairness is assessed through seven metrics including Equal Accuracy, Equal Opportunity, Equalised Odds, and Demographic Parity.

## Key Results
- Vision Transformer achieved highest overall performance (Accuracy 0.9806, F1 Score 0.9808, ROC AUC 0.9969)
- All models exhibited gender-based biases despite high accuracy scores
- ViT demonstrated relatively better fairness across multiple metrics compared to other models
- SMOTE oversampling increased gender imbalance despite initial balanced distribution
- No model excelled across all fairness metrics simultaneously, confirming impossibility results in fairness research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformers (ViT) can achieve better fairness performance alongside higher accuracy compared to CNNs and SVMs for pain detection tasks.
- Mechanism: ViT's architecture with pre-training on diverse datasets and global self-attention mechanism is less likely to overfit to specific biases present in the limited training dataset compared to local-feature focused models like CNNs or simpler models like SVMs.
- Core assumption: Pre-training on large, diverse datasets (ImageNet) transfers representations that are more robust and less sensitive to the specific gender imbalance or bias present in the target pain detection dataset.
- Evidence anchors: ViT achieved highest accuracy and selection of fairness metrics; architecture with pre-training on diverse datasets and ability to capture global relationships is less likely to overfit to biases in the dataset.

### Mechanism 2
- Claim: Balancing class distributions via oversampling (SMOTE) is a necessary pre-processing step to mitigate dataset bias, but it introduces its own limitations and may not solve demographic imbalance.
- Mechanism: Severe imbalance in "pain" vs. "no-pain" classes leads models to favor majority class. SMOTE generates synthetic samples of minority class to balance this, preventing trivial "always predict no-pain" strategy.
- Core assumption: Synthetic samples generated by SMOTE are representative and do not introduce significant noise or artifacts that the model overfits to.
- Evidence anchors: SMOTE oversampling increased gender imbalance (20,630 male vs. 27,384 female images) despite initial balanced distribution.

### Mechanism 3
- Claim: No single model architecture (SVM vs. CNN vs. ViT) or fairness metric captures all aspects of equitable performance; there is an inherent trade-off between different fairness definitions and between accuracy and fairness.
- Mechanism: Different fairness metrics mathematically capture different aspects of classification parity across groups. Optimizing for one often comes at cost of another, as these definitions can be mutually exclusive.
- Core assumption: It is possible to mathematically define and measure "fairness" using group-based metrics, and moving model's predictions closer to one of these mathematical ideals makes it "fairer" in societal or clinical context.
- Evidence anchors: No model excels across all metrics; recent studies have revealed it is impossible to satisfy all fairness metrics simultaneously, notion reflected in results table.

## Foundational Learning

- Concept: **Vision Transformer (ViT) Architecture**
  - Why needed here: Paper identifies ViT as top performer and hypothesizes its architecture is key to relative fairness. Understanding how it processes images as sequences of patches with global self-attention is essential to grasp proposed mechanism for its performance.
  - Quick check question: How does ViT's method of processing an image (patch embeddings + self-attention) fundamentally differ from a CNN's convolutional approach, and how might that relate to bias?

- Concept: **Algorithmic vs. Dataset Bias**
  - Why needed here: Paper explicitly focuses on "algorithmic bias" introduced by models, distinct from "dataset bias" in input data. Distinguishing these sources is critical for designing mitigation strategies.
  - Quick check question: If a model trained on a perfectly balanced dataset still shows difference in accuracy between two demographic groups, is that dataset bias or algorithmic bias?

- Concept: **SMOTE (Synthetic Minority Over-sampling Technique)**
  - Why needed here: Authors use SMOTE as core pre-processing step to address class imbalance. Understanding what it does (interpolating new samples) and its limitations (creating artifacts) is crucial for critiquing their methodology.
  - Quick check question: What is a potential risk of using SMOTE on image data by interpolating pixel values, and how might that affect model training?

## Architecture Onboarding

- Component map: Input (UNBC-McMaster database) -> Pre-processing (stratified split -> SMOTE oversampling -> Data Augmentation -> Feature Extraction) -> Model Zoo (Linear SVM, RBF SVM, CNN, ViT) -> Evaluation (Performance + Fairness Metrics) -> Output (Binary classification + fairness scores)

- Critical path: The most critical path for a new engineer is the pre-processing and evaluation pipeline. SMOTE can inadvertently introduce gender imbalance. An engineer must first understand how data was split, processed, and augmented. Secondly, they must understand the fairness metrics, as interpreting model performance requires balancing multiple, sometimes conflicting, mathematical definitions of fairness.

- Design tradeoffs:
  - Accuracy vs. Fairness: Central trade-off. ViT was best on accuracy but not all fairness metrics; Linear SVM showed better Demographic Parity but worse accuracy.
  - Data Augmentation (SMOTE) vs. Data Integrity: SMOTE balances class sizes but risks creating artifacts and can worsen demographic imbalance.
  - Model Complexity (SVM vs. DL) vs. Interpretability: SVMs are simpler and more interpretable but may capture fewer complex patterns than DL models like ViT.

- Failure signatures:
  - Overfitting to artifacts: Model might show high training accuracy but perform poorly on real-world data if it learns from distorted features introduced by aggressive SMOTE or augmentation.
  - Fairness Metric Collapse: Model that achieves near-perfect accuracy by always predicting majority class ("No Pain"). Would fail on fairness metrics like Equal Opportunity (TPR for 'pain' would be 0).
  - Hidden Demographic Drift: Model that appears balanced on class labels but, due to SMOTE, is now trained on dataset with skewed gender representation, leading to poor generalization for under-represented gender in synthetic set.

- First 3 experiments:
  1. Reproduce the baseline: Implement data pipeline (including SMOTE and augmentation steps) and train Linear SVM and CNN models. Verify similar accuracy and fairness metrics to those reported in Table IV. This validates the setup.
  2. Ablate the pre-processing: Train same models without SMOTE oversampling step. Compare performance and fairness metrics. This directly tests claim that SMOTE is beneficial and reveals its impact on both accuracy and demographic balance.
  3. Fairness-Aware Loss Function: Take best-performing architecture (ViT) and modify training by adding fairness-aware loss term (e.g., penalty for violating Equalized Odds). Compare new model's accuracy and fairness scores to baseline ViT. This moves from diagnosis to mitigation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can post-hoc fairness interventions (e.g., adversarial de-biasing, fairness-aware loss functions) effectively mitigate bias in pain detection without compromising the high accuracy achieved by models like Vision Transformers?
- Basis in paper: The Conclusion states, "Further research is needed to explore post-hoc fairness interventions, such as adversarial de-biasing... to enhance the fairness of pain detection systems without compromising accuracy."
- Why unresolved: This study focused on evaluating standard models without applying specific fairness mitigation algorithms.
- What evidence would resolve it: A comparative study applying these specific interventions to the models used in this paper, measuring the resulting shift in the accuracy-fairness trade-off.

### Open Question 2
- Question: How can dataset balancing techniques be improved to maintain demographic parity while rectifying class imbalance?
- Basis in paper: Authors note that "SMOTE oversampling increased gender imbalance... despite initial balanced distribution," and suggest the "need for more sophisticated data balancing techniques."
- Why unresolved: Standard pre-processing methods like SMOTE inadvertently distorted the gender distribution while trying to fix the pain/no-pain class imbalance.
- What evidence would resolve it: Development and validation of pre-processing pipeline that yields equal demographic ratios post-augmentation.

### Open Question 3
- Question: To what extent do intersectional biases (e.g., gender combined with race) affect pain detection models?
- Basis in paper: Discussion lists as limitation that current metrics "may not capture all dimensions of bias, particularly in intersectional contexts involving multiple sensitive attributes."
- Why unresolved: Study was restricted to analyzing gender in isolation due to dataset and metric constraints.
- What evidence would resolve it: Evaluating these models on datasets with intersectional labels using fairness metrics designed for multiple protected attributes.

## Limitations
- SMOTE oversampling increased gender imbalance despite initial balanced distribution, revealing unintended consequences of standard bias mitigation techniques
- No single model architecture or fairness metric captures all aspects of equitable performance, highlighting fundamental trade-offs between different fairness definitions
- Study restricted to analyzing gender bias in isolation, unable to address intersectional biases involving multiple sensitive attributes

## Confidence

| Claim | Confidence |
|-------|------------|
| All evaluated models exhibit gender-based biases | High |
| ViT's architecture contributes to better fairness performance | Medium |
| SMOTE effectively mitigates bias without demographic consequences | Low |

## Next Checks
1. Implement a per-gender SMOTE oversampling strategy to maintain demographic balance while addressing class imbalance, then re-evaluate all models' fairness metrics.
2. Conduct ablation studies removing data augmentation and SMOTE to quantify their individual contributions to both accuracy and fairness outcomes.
3. Test additional fairness-aware training objectives (e.g., adversarial debiasing or equalized odds regularization) on the best-performing ViT architecture to directly optimize for multiple fairness metrics simultaneously.