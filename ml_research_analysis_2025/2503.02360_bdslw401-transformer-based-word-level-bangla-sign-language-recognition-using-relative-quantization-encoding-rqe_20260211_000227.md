---
ver: rpa2
title: 'BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using
  Relative Quantization Encoding (RQE)'
arxiv_id: '2503.02360'
source_url: https://arxiv.org/abs/2503.02360
tags:
- recognition
- sign
- attention
- language
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BdSLW401, a large-scale, multi-view Bangla
  Sign Language dataset with 401 signs and 102,176 video samples, and proposes Relative
  Quantization Encoding (RQE) to improve transformer-based SLR. RQE reduces spatial
  variability by anchoring landmarks to physiological reference points and quantizing
  motion trajectories, leading to 44.3% WER reduction in WLASL100, 21.0% in SignBD-200,
  and significant gains in BdSLW60 and SignBD-90.
---

# BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)

## Quick Facts
- arXiv ID: 2503.02360
- Source URL: https://arxiv.org/abs/2503.02360
- Reference count: 27
- Key outcome: RQE reduces WER by 44.3% on WLASL100, 21.0% on SignBD-200, and significantly improves BdSLW401 recognition by anchoring landmarks and quantizing motion

## Executive Summary
This paper introduces BdSLW401, a large-scale multi-view Bangla Sign Language dataset with 401 signs and 102,176 video samples, and proposes Relative Quantization Encoding (RQE) to improve transformer-based SLR. RQE reduces spatial variability by anchoring landmarks to physiological reference points and quantizing motion trajectories, leading to substantial WER reductions across multiple datasets. An extended variant, RQE-SF, stabilizes shoulder landmarks for improved pose consistency but with minor trade-offs in lateral view recognition.

## Method Summary
The method uses MediaPipe Holistic landmarks (224-dim tensor: 75 points × 3 - 1) as input, then applies RQE preprocessing which anchors hand landmarks to wrists, wrists to elbows, elbows to shoulders, and shoulders to the mid-shoulder point of the first frame. Continuous trajectories are quantized into fixed levels (10 per coordinate axis) while lower body landmarks are set to a single level. RQE-SF optionally stabilizes shoulder landmarks to their first-frame positions. The transformed embeddings feed a transformer encoder (3-4 layers, 7-8 heads) with sinusoidal positional encoding, producing vocabulary-level classifications evaluated by WER.

## Key Results
- RQE achieves 44.3% relative WER reduction on WLASL100 compared to raw embeddings
- RQE-SF improves front-view WER from 30.12% to 28.19% but slightly increases lateral-view WER from 70.67% to 72.08%
- Fixed quantization becomes insufficient for large-scale datasets like WLASL2000, where performance gains diminish

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured landmark embeddings reduce attention misallocation in transformers.
- **Mechanism:** Raw pose coordinates introduce variability from signer proportions, hand dominance, and camera angles. By anchoring hand landmarks to wrists and upper-body landmarks to the mid-shoulder point of the first frame, the model receives input where spatial relationships are consistent across signers. This allows the self-attention mechanism to focus on semantically relevant motion patterns rather than normalizing physiological differences.
- **Core assumption:** The transformer's self-attention layers distribute focus more effectively when the input space has lower spatial variance across signers.
- **Evidence anchors:**
  - [abstract] "RQE improves attention allocation by decreasing spatial variability"
  - [Page 5, Section IV.B] "Hand coordinates are expressed as offsets from the wrist, ensuring consistent representation of finger positions across signers"
  - [corpus] No direct corpus validation of this specific attention-mechanism claim for Bangla SLR; related work mentions transformer struggles with unprocessed landmarks but does not confirm causality
- **Break condition:** If the target gesture vocabulary relies heavily on absolute spatial positioning (e.g., signs referencing body-external locations), physiological anchoring may discard discriminative information.

### Mechanism 2
- **Claim:** Discretizing continuous motion trajectories into quantized levels improves classification stability.
- **Mechanism:** Continuous landmark coordinates contain micro-variations and noise. Quantization maps these to a fixed set of levels (e.g., 10 per coordinate axis), reducing sensitivity to small tracking errors while preserving the major trajectory shape. This compresses spatio-temporal noise without learning a noise model.
- **Core assumption:** Micro-variations in landmark trajectories are predominantly noise rather than linguistically meaningful distinctions for the target vocabulary.
- **Evidence anchors:**
  - [Page 5, Section IV.C] "Continuous trajectories are discretized into interpretable levels (e.g. 10 levels x coordinate), compressing spatio-temporal noise"
  - [Page 9, Table IX] RQE achieves 44.3% relative WER reduction on WLASL100 but negligible gains on WLASL2000, suggesting fixed quantization helps when vocabulary complexity is limited
  - [corpus] No corpus papers validate quantization specifically; low-resource SLR papers focus on dataset creation and lightweight architectures, not input encoding
- **Break condition:** On ultra-large vocabularies with fine-grained motion distinctions, fixed quantization resolution becomes insufficient (observed: WER increases slightly on WLASL2000).

### Mechanism 3
- **Claim:** Stabilizing shoulder landmarks reduces torso-movement noise at the cost of viewpoint adaptability.
- **Mechanism:** RQE-SF anchors shoulder landmarks to their first-frame positions (single quantization level), preventing torso sway or posture shifts from affecting the embedding. This improves consistency for frontal-view recognition where shoulder movement is typically irrelevant. However, lateral views rely more on torso orientation cues, which RQE-SF suppresses.
- **Core assumption:** Torso movement is primarily noise for word-level sign recognition and not a core articulatory feature.
- **Evidence anchors:**
  - [Page 6, Section IV.D] "shoulder landmarks are anchored to their first-frame positions, stabilizing torso movements while preserving gesture dynamics"
  - [Page 8, Table VII] RQE-SF improves front-view WER (30.12% → 28.19%) but slightly increases lateral-view WER (70.67% → 72.08%)
  - [corpus] No corpus validation; related papers do not address shoulder stabilization strategies
- **Break condition:** For sign languages or vocabulary domains where torso orientation is linguistically contrastive, RQE-SF would suppress meaningful features.

## Foundational Learning

- **Concept: Transformer Self-Attention for Sequence Modeling**
  - **Why needed here:** RQE's design rationale is based on making attention allocation more efficient. Without understanding that transformers distribute "focus" across input tokens via learned attention weights, the benefit of reducing spatial variability is unclear.
  - **Quick check question:** Given an input sequence of pose embeddings per frame, which component determines how much each frame influences the final classification?

- **Concept: Keypoint / Landmark-Based Pose Representation**
  - **Why needed here:** The entire pipeline operates on MediaPipe-extracted landmarks (x, y, depth), not video pixels. Understanding that pose is represented as sparse 3D coordinates is essential to grasping why physiological anchoring is even possible.
  - **Quick check question:** If a wrist landmark is occluded in a lateral view, what does the pipeline set its coordinates to?

- **Concept: Word Error Rate (WER) for Sequence Classification**
  - **Why needed here:** WER accounts for substitution, deletion, and insertion errors in predicted sign sequences, which is more informative than simple accuracy for structured prediction tasks.
  - **Quick check question:** If a model correctly predicts 9 out of 10 signs but inserts one spurious sign, what happens to WER?

## Architecture Onboarding

- **Component map:** Landmark extraction → RQE preprocessing → positional encoding → transformer encoder → classification
- **Critical path:** Landmark extraction → RQE preprocessing → positional encoding → transformer encoder → classification. If RQE preprocessing is skipped (raw mode), the model receives unnormalized coordinates and attention is less focused (observed in attention graphs).
- **Design tradeoffs:**
  - RQE vs. Raw: RQE reduces WER on small/medium vocabularies but may degrade on ultra-large vocabularies with fine distinctions.
  - RQE-SF vs. RQE: RQE-SF improves front-view consistency but slightly hurts lateral-view performance; choose based on deployment viewpoint.
  - Fixed quantization vs. adaptive: Current implementation uses fixed 10-level quantization; paper explicitly states this is insufficient for WLASL2000-scale datasets.
- **Failure signatures:**
  - Lateral-view WER remains high (70%+) even with RQE-RQE-SF, indicating depth ambiguity and occlusion are not resolved by coordinate normalization alone.
  - Left-handed signing introduces mirror-image variance (~17% of BdSLW401); without hand-dominance correction, accuracy drops.
  - Temporal ambiguities (pauses within signs) cause quantization to misinterpret pauses as low-magnitude motion.
- **First 3 experiments:**
  1. **Baseline sanity check:** Train SLRT on BdSLW60 with raw embeddings (no RQE). Reproduce reported WER. Then enable RQE and confirm WER reduction. This validates the preprocessing pipeline.
  2. **Ablation on quantization levels:** Run RQE with 5, 10, 20, and 50 quantization levels on WLASL100. Plot WER vs. levels to find the inflection point where quantization resolution no longer helps.
  3. **Viewpoint stress test:** Train on BdSLW401 front-view only, lateral-view only, and combined. Measure the delta between RQE and RQE-SF in each condition to quantify the trade-off documented in Table VII.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adaptive or dynamic quantization strategies be developed to maintain the benefits of RQE in large-scale datasets with complex motion patterns?
- **Basis in paper:** [explicit] The authors state that fixed quantization becomes insufficient on large-scale datasets like WLASL2000, where performance gains diminish or reverse.
- **Why unresolved:** The current fixed-level quantization likely discards subtle kinematic differences necessary for distinguishing between thousands of signs.
- **What evidence would resolve it:** A revised RQE method with variable quantization steps that achieves a lower Word Error Rate (WER) on WLASL2000 compared to the current baseline.

### Open Question 2
- **Question:** How can geometric transformations or multi-view fusion be integrated with RQE to close the performance gap between frontal and lateral viewpoints?
- **Basis in paper:** [explicit] The paper suggests future work should integrate geometric transformations or multi-view fusion to improve cross-perspective generalization.
- **Why unresolved:** Current results show a massive performance disparity, with lateral-view WER (75.43%) remaining significantly higher than front-view WER (28.10%).
- **What evidence would resolve it:** Demonstration of a unified model where lateral-view WER is statistically comparable to front-view WER on the BdSLW401 dataset.

### Open Question 3
- **Question:** Is it possible to design a context-aware shoulder stabilization mechanism that improves pose consistency without sacrificing lateral-view adaptability?
- **Basis in paper:** [inferred] The RQE-SF variant improves pose consistency but introduces "minor trade-offs in lateral view recognition" by statically anchoring shoulders.
- **Why unresolved:** The static anchoring restricts the natural movement variance required to interpret gestures accurately from side angles.
- **What evidence would resolve it:** An adaptive stabilization model that lowers lateral WER below standard RQE while maintaining the stability benefits of RQE-SF.

## Limitations
- Fixed quantization level (10) is insufficient for large-scale vocabularies (WLASL2000), but adaptive quantization strategy is not defined or validated
- Shoulder landmark stabilization (RQE-SF) improves frontal views but degrades lateral views, yet no ablation study isolates the contribution of shoulder anchoring from other RQE components
- Paper does not report confidence intervals or statistical significance tests for WER improvements across datasets

## Confidence
- **High confidence:** BdSLW401 dataset construction, basic RQE framework (physiological anchoring + quantization), WER metric definition
- **Medium confidence:** Attention mechanism improvements, RQE-SF design rationale, cross-dataset generalization patterns
- **Low confidence:** Exact quantization boundaries, optimal quantization levels per dataset, statistical significance of reported gains

## Next Checks
1. **Quantization resolution ablation:** Systematically vary quantization levels (5, 10, 20, 50) on WLASL100 and plot WER vs. level count to identify optimal resolution and degradation point
2. **Viewpoint-specific training:** Train separate models on frontal-only, lateral-only, and combined views of BdSLW401; measure RQE vs. RQE-SF performance deltas to quantify viewpoint trade-offs
3. **Statistical significance testing:** Apply paired t-tests or bootstrap confidence intervals on WER improvements across multiple random seeds to validate reported gains are not due to chance