---
ver: rpa2
title: 'HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs'
arxiv_id: '2405.00645'
source_url: https://arxiv.org/abs/2405.00645
tags:
- neural
- quantization
- networks
- latency
- resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HGQ (High Granularity Quantization), a quantization-aware
  training framework for FPGA-targeted neural networks that learns optimal per-parameter
  bit-widths through gradient descent. Unlike conventional quantization methods that
  use uniform or per-layer/channel bit-widths, HGQ determines optimal bit-widths for
  each parameter independently, making it suitable for hardware platforms supporting
  heterogeneous arbitrary precision arithmetic.
---

# HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs

## Quick Facts
- arXiv ID: 2405.00645
- Source URL: https://arxiv.org/abs/2405.00645
- Reference count: 40
- Primary result: Learns per-parameter bit-widths for FPGA deployment, achieving orders-of-magnitude resource reduction while maintaining accuracy.

## Executive Summary
HGQ introduces a quantization-aware training framework that learns optimal bit-widths for each neural network parameter independently. Unlike conventional methods using uniform or per-layer quantization, HGQ treats bit-widths as learnable parameters optimized via gradient descent. The framework combines differentiable fixed-point quantization with a hardware-aware resource estimator (EBOPs) that acts as a regularizer during training, encouraging accuracy where it matters while penalizing resource-heavy configurations.

The method has been validated on jet substructure classification, SVHN image classification, and muon tracking tasks, showing superior performance compared to existing compression methods. HGQ achieves significant reductions in FPGA resource consumption and latency while maintaining model accuracy. The framework is open-source and has been integrated with popular FPGA deployment tools da4ml and hls4ml, enabling efficient deployment of complex models for real-time data selection at the CERN ATLAS and CMS experiments.

## Method Summary
HGQ is a quantization-aware training framework for FPGA-targeted neural networks that learns per-parameter bit-widths through gradient descent. The method uses differentiable fixed-point quantization with learnable bit-widths optimized at arbitrary granularity, combined with a differentiable on-chip resource usage estimator (EBOPs) that acts as a regularizer during training. The loss function combines task loss, EBOPs regularization, and L1 regularization on bit-widths. Training employs Adam optimizer with cosine annealing, exponential scheduling of the hardware penalty coefficient β, and fixed γ=2e-8. The framework supports both da4ml and hls4ml backends for FPGA synthesis and has been used for developing next-generation trigger systems at CERN experiments.

## Key Results
- Achieves orders-of-magnitude reduction in FPGA resource consumption (LUTs, DSPs) while maintaining accuracy
- Enables deployment of complex neural networks on FPGAs with sub-microsecond latency constraints
- Successfully integrated with da4ml and hls4ml tools for efficient FPGA synthesis
- Applied to real-world high-energy physics applications at CERN ATLAS and CMS experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-parameter bit-widths can be optimized via gradient descent using surrogate gradients that approximate the relationship between quantization error and fractional bit-width.
- Mechanism: The method relaxes inherently discrete bit-widths to continuous surrogates during training. It derives a surrogate gradient ∂δf/∂f ← -log(2)·δf by taking the expectation over the uniform quantization error distribution, which smooths the loss landscape and makes optimization tractable.
- Core assumption: The quantization error δf follows a uniform distribution when the parameter variance is significantly larger than the quantization step size.
- Evidence anchors:
  - [section 3.2]: "We propose a surrogate gradient method that assigns a gradient to f only on integer values... E|δf|[log|δf+1|/|δf|] = -log(2)"
  - [section 3.1.1]: "During training, we use the Straight Through Estimator (STE) to approximate the gradients of the quantization operation"
  - [corpus]: Weak direct support—neighbor papers focus on deployment but not on the surrogate gradient derivation.
- Break condition: If parameter distributions become highly concentrated relative to quantization step size, the uniform error assumption breaks down and gradient estimates may become biased.

### Mechanism 2
- Claim: Effective Bit Operations (EBOPs) serves as a differentiable proxy for post-place-and-route FPGA resource consumption, enabling hardware-aware regularization during training.
- Mechanism: EBOPs sums multiplication complexity (b_i × b_j) and addition complexity (max(b_k, b_l)) across all operations. This is added to the loss as L = L_base + β·EBOPs + γ·∑bit-widths, penalizing resource-heavy configurations while maintaining task accuracy.
- Core assumption: EBOPs correlates monotonically with actual LUT/DSP consumption despite backend optimizations like common subexpression elimination.
- Evidence anchors:
  - [section 3.3]: "EBOPs is typically dominated by the first term... constructed based on a simplified model of the resource consumption of a constant multiplier on FPGAs"
  - [section 5.2, Figure 3]: Shows strong correlation between EBOPs and actual LUT consumption; relative error within 20% for most models.
  - [corpus]: da4ml paper (neighbor) provides complementary backend validation.
- Break condition: For architectures requiring significant non-arithmetic logic (e.g., FIFOs, muxing for SVHN classifier), EBOPs underestimates actual resource consumption.

### Mechanism 3
- Claim: Zero bit-width assignment provides automatic, unstructured pruning without explicit pruning algorithms.
- Mechanism: When fractional bits f is reduced below a threshold, |x| < 2^(-f-1) causes the quantized value to round to zero, effectively removing the parameter from computation.
- Core assumption: The regularization coefficient β is sufficient to drive unnecessary parameters to zero bit-width.
- Evidence anchors:
  - [section 3.1.2]: "As f∈Z, a sufficiently small f will cause the corresponding parameters in the network to vanish, effectively pruning these parameters"
  - [section 5.2, Figures 4-5]: Weight bit-width distributions show majority of weights quantized to zero bits across tasks.
  - [corpus]: No direct corpus support for this specific pruning mechanism.
- Break condition: If β is too small, bit-widths remain above pruning threshold; if too large, critical parameters may be over-pruned, degrading accuracy.

## Foundational Learning

- Concept: Fixed-point representation (signed, integer bits, fractional bits)
  - Why needed here: HGQ uses hardware-compatible ap_fixed/ac_fixed schemes; understanding Q = [-s·2^i, 2^i - 2^(-f)] is essential for interpreting quantization constraints.
  - Quick check question: Given signed=true, i=4, f=8, what is the maximum representable value?

- Concept: Straight-Through Estimator (STE) for non-differentiable operations
  - Why needed here: The rounding operation is non-differentiable; STE (∂q(x)/∂x = 1) enables backpropagation through quantization.
  - Quick check question: Why does STE work despite being a biased gradient estimator?

- Concept: Regularization trade-offs (L1 vs. hardware-aware)
  - Why needed here: The loss function combines task loss, EBOPs regularization (β), and L1 on bit-widths (γ)—understanding their interaction is critical for hyperparameter tuning.
  - Quick check question: What happens to bit-widths if γ is set to zero?

## Architecture Onboarding

- Component map: Keras HGQ layers -> Surrogate Gradient Module -> EBOPs Calculator -> Loss Aggregator -> Export Backend -> Vivado/Quartus synthesis
- Critical path:
  1. Define model architecture using HGQ layers (HGROUP, HDense, HConv2D, etc.)
  2. Configure β schedule (exponential ramp from small to target value) and fixed γ
  3. Train with Adam + cosine annealing, checkpointing Pareto-front models
  4. Profile activation ranges on calibration dataset (for WRAP overflow mode)
  5. Export to da4ml (low-latency, LUT-optimized) or hls4ml (flexible, DSP-capable)
  6. Synthesize with Vivado/Quartus; verify functional correctness with Verilator

- Design tradeoffs:
  - SAT vs. WRAP overflow: SAT is training-stable but requires comparators; WRAP has minimal overhead but needs post-training range profiling.
  - da4ml vs. hls4ml backend: da4ml optimizes CMVM via distributed arithmetic and common subexpression elimination (lower LUT, lower latency); hls4ml supports broader architectures and DSP utilization.
  - Granularity choice: Per-parameter offers maximal compression but may increase routing complexity; per-channel balances efficiency with implementation regularity.
  - RND vs. TRN rounding: RND is unbiased and often fuses with bias addition; TRN has systematic bias but minimal overhead.

- Failure signatures:
  - Accuracy collapse with high β: Over-regularization drives too many bit-widths to zero; reduce β_max or increase β ramp duration.
  - Timing violations at target clock: High bit-widths in accumulation cause long combinational paths; reduce precision or add pipeline registers.
  - EBOPs-resource mismatch: For architectures with significant buffering/muxing (e.g., CNNs with reuse), actual resources exceed predictions; validate early with synthesis.
  - Training instability with WRAP mode: Periodic overflow gradients cause oscillation; use SAT mode during training or profile carefully.
  - Backend export errors: Unsupported layer types or dynamic operations; verify compatibility with hls4ml/da4ml supported ops list.

- First 3 experiments:
  1. **Baseline accuracy vs. β sweep**: Train on a simple task (e.g., HLF JSC) with β varying from 1e-7 to 1e-3. Plot accuracy vs. EBOPs to identify Pareto front and calibrate β range for target domain.
  2. **Backend comparison**: Export the same HGQ-trained model to both da4ml and hls4ml. Compare post-place-and-route LUT, DSP, latency, and F_max to understand backend-specific optimizations.
  3. **Bit-width distribution analysis**: For a moderate-complexity model, visualize per-layer weight and activation bit-width distributions. Identify layers with high pruning (zero bit-width concentration) and correlate with EBOPs contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimization methodology of HGQ be effectively integrated with LUT-based neural network methods (e.g., NeuraLUT) to enhance hardware efficiency beyond current quantization or LUT-based approaches alone?
- Basis in paper: [explicit] Page 10, Section 6: "Future work includes extending the optimization methodology used in HGQ to LUT-based methods, such as NeuraLUT or NeuraLUT-Assemble."
- Why unresolved: It is unclear if the per-parameter bit-width optimization used in HGQ translates directly to the sub-network logic optimization domain of LUT-based methods, or if the sparse gradients in LUT space would destabilize HGQ's training dynamics.
- What evidence would resolve it: A hybrid framework demonstrating improved resource-accuracy trade-offs compared to standalone HGQ or NeuraLUT baselines on standard benchmarks.

### Open Question 2
- Question: How can HGQ be adapted to optimize the deployment of complex attention-based architectures, such as Transformers, on FPGAs?
- Basis in paper: [explicit] Page 10, Section 6: "While more complex models such as transformers are supported in HGQ, more work is required for efficient FPGA deployment of these models."
- Why unresolved: While HGQ supports the layers structurally, the specific memory access patterns and computational complexities of attention mechanisms may not be fully captured by the current EBOPs estimator, potentially leading to suboptimal hardware mappings.
- What evidence would resolve it: Successful deployment of an HGQ-optimized Transformer model on an FPGA with resource consumption and latency comparable to current CNN/MLP benchmarks.

### Open Question 3
- Question: Can the EBOPs resource estimator be refined to accurately account for non-arithmetic hardware overheads, such as FIFO buffering and control logic?
- Basis in paper: [inferred] Page 7, Section 5.2: "Other non-arithmetic operations... such as those arising from the control logic... or FIFOs... are not currently accounted for in EBOPs."
- Why unresolved: The current metric correlates well with arithmetic logic but fails to predict resource usage for models requiring extensive data buffering (e.g., the SVHN classifier), causing discrepancies between estimated and actual post-place-and-route resource consumption.
- What evidence would resolve it: A modified differentiable estimator that maintains high correlation with actual resource usage across architectures requiring significant buffering or control logic.

## Limitations
- The uniform quantization error assumption may break down for highly concentrated parameter distributions, affecting surrogate gradient accuracy.
- EBOPs underestimates hardware resources for architectures requiring significant non-arithmetic components like FIFOs and control logic.
- Method effectiveness depends heavily on careful hyperparameter tuning that may not transfer well across different domains.

## Confidence
- High: The surrogate gradient mechanism is mathematically sound under uniform quantization error assumption.
- Medium: EBOPs correlates well with resource consumption for arithmetic-intensive architectures.
- Low: The unstructured pruning mechanism lacks extensive validation across diverse architectures.

## Next Checks
1. Test surrogate gradient accuracy by comparing training dynamics with exact gradients on small networks where exhaustive search is feasible.
2. Validate EBOPs predictions against actual post-place-and-route resource consumption for architectures with significant non-arithmetic components.
3. Evaluate HGQ's performance on diverse tasks beyond high-energy physics applications to assess generalizability across domains.