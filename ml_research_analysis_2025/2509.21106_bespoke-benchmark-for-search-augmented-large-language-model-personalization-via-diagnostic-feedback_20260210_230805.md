---
ver: rpa2
title: 'BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization
  via Diagnostic Feedback'
arxiv_id: '2509.21106'
source_url: https://arxiv.org/abs/2509.21106
tags:
- user
- information
- query
- history
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BESPOKE is a benchmark for evaluating personalization in search-augmented
  LLMs. It uses authentic user histories collected over three weeks, where annotators
  engaged in natural conversations and searches.
---

# BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback

## Quick Facts
- arXiv ID: 2509.21106
- Source URL: https://arxiv.org/abs/2509.21106
- Reference count: 40
- Key outcome: Benchmark for evaluating personalization in search-augmented LLMs using authentic user histories

## Executive Summary
BESPOKE introduces a benchmark for evaluating personalization in search-augmented large language models. The benchmark uses authentic user histories collected over three weeks from annotators who engaged in natural conversations and searches. For each query, it provides gold information needs and response-judgment pairs with fine-grained scores and feedback across four criteria: need alignment, content depth, tone, and explanation style. The evaluation framework leverages human-annotated feedback to assess both factual coverage and personalization quality.

The benchmark demonstrates that leveraging user history improves personalization, especially when user contexts are query-aware and constructed as structured profiles from selectively chosen histories. However, the study reveals significant challenges in inferring implicit preferences from noisy, long-term interaction data, with none of the evaluated models surpassing an average score of 60 across evaluation criteria.

## Method Summary
BESPOKE constructs a benchmark by collecting authentic user histories over three weeks from annotators who naturally engage in conversations and searches. The dataset captures real-world interaction patterns and user preferences. For each query, the benchmark provides gold information needs and response-judgment pairs with fine-grained scores across four evaluation criteria: need alignment, content depth, tone, and explanation style. The evaluation framework uses human-annotated feedback to assess both factual coverage and personalization quality. The benchmark focuses on query-aware context construction, where user histories are selectively chosen and structured into profiles to improve personalization performance.

## Key Results
- Leveraging user history improves personalization in search-augmented LLMs
- Query-aware contexts and structured profile construction from user histories yield the best personalization performance
- None of the evaluated models surpassed an average score of 60 across evaluation criteria
- The benchmark achieves strong alignment with human judgments for assessing personalization quality

## Why This Works (Mechanism)
BESPOKE works by creating a realistic evaluation framework that captures authentic user behavior over time. The benchmark collects genuine user histories through natural interactions rather than artificial or simulated data, providing a more accurate representation of how users actually search and communicate. By structuring these histories into query-aware profiles and providing fine-grained feedback across multiple criteria, the benchmark enables detailed assessment of both factual accuracy and personalization quality. The human-annotated feedback mechanism ensures that evaluation aligns with human judgment of what constitutes good personalization.

## Foundational Learning
- **Authentic User History Collection**: Captures real user behavior over extended periods to understand natural interaction patterns
- **Query-Aware Context Construction**: Selects and structures relevant user histories based on current query context rather than using all available history
- **Fine-Grained Evaluation Criteria**: Assesses personalization across multiple dimensions (need alignment, content depth, tone, explanation style) for comprehensive quality measurement
- **Human-Annotated Feedback**: Provides ground truth judgments that align with human expectations for personalized responses
- **Structured Profile Creation**: Transforms raw user interaction data into organized profiles that models can effectively use for personalization

## Architecture Onboarding

**Component Map:** User History Collection -> Profile Construction -> Context Generation -> Model Response -> Evaluation Framework

**Critical Path:** User history collection and annotation → Structured profile construction → Query-aware context generation → Model inference → Multi-criteria evaluation with human feedback

**Design Tradeoffs:** 
- Tradeoff between comprehensive history coverage vs. relevance filtering for context construction
- Balance between automated evaluation metrics and human judgment for accuracy vs. efficiency
- Choice between raw history vs. structured profiles for model input flexibility vs. performance

**Failure Signatures:** 
- Low need alignment scores indicate context relevance problems
- Poor content depth suggests insufficient information retrieval from search augmentation
- Inconsistent tone reveals failure in capturing user communication patterns
- Mismatched explanation style indicates misunderstanding of user preference patterns

**3 First Experiments:**
1. Test profile construction methods with varying history selection strategies on a small subset of queries
2. Evaluate baseline model performance with different context sizes and structures
3. Validate human annotation consistency by having multiple annotators rate the same responses

## Open Questions the Paper Calls Out
None

## Limitations
- Small annotator cohort (10 participants) collected over a three-week period limits generalizability and introduces potential sampling bias
- Maximum average score of 60 across evaluation criteria indicates substantial room for improvement in personalization performance
- The four evaluation criteria may not comprehensively capture all dimensions of effective personalization, potentially missing critical aspects like privacy considerations

## Confidence
- **High confidence**: The benchmark's methodology for collecting authentic user histories and the technical approach to query-aware context construction
- **Medium confidence**: The effectiveness of structured profile construction from user histories for personalization
- **Low confidence**: The completeness of the four evaluation criteria and the benchmark's generalizability beyond the specific annotator cohort

## Next Checks
1. Conduct inter-annotator agreement analysis to quantify the reliability of the gold standard judgments and feedback annotations
2. Test the benchmark's effectiveness across diverse user populations with varying demographics, interests, and interaction patterns
3. Implement longitudinal studies to assess whether the benchmark captures evolving user preferences and personalization needs over extended time periods beyond three weeks