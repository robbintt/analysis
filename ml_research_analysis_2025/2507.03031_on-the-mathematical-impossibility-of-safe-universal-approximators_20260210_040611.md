---
ver: rpa2
title: On the Mathematical Impossibility of Safe Universal Approximators
arxiv_id: '2507.03031'
source_url: https://arxiv.org/abs/2507.03031
tags:
- universal
- catastrophic
- network
- networks
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes fundamental mathematical limits on the controllability
  of universal approximators by proving that catastrophic failures are an inescapable
  feature of any useful computational system. The authors demonstrate that for any
  universal approximator complex enough to be useful, perfect reliable control is
  mathematically impossible due to dense sets of instabilities that are inextricably
  linked to the system's expressive power.
---

# On the Mathematical Impossibility of Safe Universal Approximators

## Quick Facts
- arXiv ID: 2507.03031
- Source URL: https://arxiv.org/abs/2507.03031
- Authors: Jasper Yao
- Reference count: 20
- Primary result: Perfect reliable control of universal approximators is mathematically impossible due to dense sets of instabilities inextricably linked to expressive power.

## Executive Summary
This paper establishes fundamental mathematical limits on the controllability of universal approximators by proving that catastrophic failures are an inescapable feature of any useful computational system. The authors demonstrate that for any universal approximator complex enough to be useful, perfect reliable control is mathematically impossible due to dense sets of instabilities that are inextricably linked to the system's expressive power.

The core argument proceeds through three complementary levels: (1) combinatorial necessity showing that practical networks like those using ReLU activations have catastrophe density directly proportional to their expressive power; (2) topological necessity proving that universal approximation capability requires the ability to implement dense catastrophic singularities; and (3) empirical necessity demonstrating that the universal existence of adversarial examples proves real-world tasks are themselves catastrophic, forcing any successful model to replicate these instabilities.

## Method Summary
The paper establishes impossibility through three mathematical pillars: (1) combinatorial analysis of ReLU networks showing catastrophe density ρ(δ) ≥ 1 − exp(−αC/δ^d) where C is complexity and d is input dimension; (2) topological argument via Whitney's theorem proving that universal approximation requires implementing dense catastrophic singularities; and (3) Fisher Information Matrix analysis showing real-world tasks force pathological eigenvalue spectra that guarantee behavioral instability. Quantitative bounds demonstrate that modern systems like GPT-4 with ~10^12 parameters have catastrophe density indistinguishable from 1, exceeding safe complexity thresholds by factors of 10^13 to 10^17.

## Key Results
- Catastrophe density in ReLU networks grows exponentially with parameter count and inversely with perturbation scale
- Universal approximation capability mathematically requires the ability to implement dense catastrophic singularities
- Modern large language models exceed safe complexity thresholds by factors of 10^13-10^17
- The paper reframes UAT safety from achieving perfect control to operating safely within irreducible uncontrollability

## Why This Works (Mechanism)

### Mechanism 1: Combinatorial Density of ReLU Boundaries
- **Claim:** For piecewise-linear networks, catastrophe density ρ(δ) ≥ 1 − exp(−αC/δ^d) where C is complexity and d is input dimension.
- **Mechanism:** ReLU neurons partition input space into linear regions; the hyperplane boundaries between regions are loci of non-differentiability where small input perturbations can cause large output changes.
- **Core assumption:** Assumption: Crossing a ReLU boundary corresponds to (ε, δ)-catastrophic behavior in practice.
- **Evidence anchors:**
  - [abstract]: "density of catastrophic failure points is directly proportional to the network's expressive power"
  - [section 5.1, Theorem 5.1]: Exact bound formula with geometric constants c_k
  - [corpus]: No direct corpus support for this specific catastrophe-boundary equivalence
- **Break condition:** If smooth activations (GELU, Swish) avoid discrete boundaries; if boundary crossings rarely cause ε-scale output changes in trained networks.

### Mechanism 2: Whitney Singularity Theory Applied to Function Space
- **Claim:** "Generic" smooth functions have dense singularities; universal approximators must implement these to be useful.
- **Mechanism:** Whitney's theorem states that functions with dense singularities form a full-measure set in smooth function space. UAT guarantees networks can approximate any function, including catastrophe-dense ones.
- **Core assumption:** Assumption: Real-world tasks require approximating "generic" functions rather than measure-zero smooth subclasses.
- **Evidence anchors:**
  - [abstract]: "ability to approximate generic functions requires the ability to implement the dense, catastrophic singularities"
  - [section 3.1, Theorem 3.1]: Measure-theoretic argument about FC → F∞ density
  - [corpus]: Weak—no corpus papers on singularity theory in neural approximation
- **Break condition:** If practical tasks lie on low-dimensional smooth manifolds; if training dynamics avoid catastrophe-dense regions.

### Mechanism 3: FIM Eigenvalue Pathology from Task Complexity
- **Claim:** Real-world tasks force Fisher Information Matrix spectra with extreme eigenvalue ratios (10^6–10^8), which mathematically guarantees behavioral instability.
- **Mechanism:** Complex tasks require some parameters to be highly sensitive (fine-grained features) while others become redundant (broad patterns), creating pathological FIM spectra. The natural gradient becomes explosive.
- **Core assumption:** Assumption: High FIM condition number directly causes input-space catastrophes (not just training instability).
- **Evidence anchors:**
  - [section 3.4, Theorem 3.5]: "Real Tasks Force Catastrophic FIM Structure"
  - [section 3.4]: Cites Karakida et al. [4] on empirical FIM spectra
  - [corpus]: Corpus includes Karakida paper on FIM statistics (FMR=0.39)
- **Break condition:** If FIM pathology correlates with but doesn't cause input-space catastrophes; if regularization can bound eigenvalue ratios.

## Foundational Learning

- **Universal Approximation Theorem (UAT)**
  - Why needed: The entire argument hinges on what UAT guarantees (existence of approximators) vs. what it doesn't (safety of approximators).
  - Quick check: Can you explain why UAT says nothing about the *behavior* of approximators between training points?

- **(ε, δ)-Safety Definition**
  - Why needed: This is the paper's core formalism—a function is (ε, δ)-safe at x if perturbations smaller than δ cause output changes smaller than ε.
  - Quick check: For a given ε=0.1, δ=0.01, what would ρ(δ)=0.9 mean?

- **Fisher Information Matrix (FIM)**
  - Why needed: The empirical mechanism linking task complexity to instability.
  - Quick check: What does an eigenvalue ratio of 10^8 tell you about parameter sensitivity in different directions?

- **Singularity Theory / Whitney's Theorem**
  - Why needed: The topological mechanism claiming "generic" functions are catastrophe-dense.
  - Quick check: Why does "full measure" not mean "all practical tasks"?

## Architecture Onboarding

- **Component map:**
  - Pillar 1 (Combinatorial): Applies to ReLU/piecewise-linear networks → direct boundary-counting argument
  - Pillar 2 (Topological): Applies to any universal approximator → singularity-theoretic argument
  - Pillar 3 (Empirical): FIM-based argument connecting task complexity to observed instability
  - Quantitative bounds: C₀ (safe complexity) vs. C_min (useful complexity) → "Impossibility Sandwich"

- **Critical path:**
  1. Understand (ε, δ)-formalism and catastrophe density ρ
  2. Trace ReLU boundary counting → exponential growth in boundaries
  3. Understand how Whitney extends this to smooth activations
  4. Connect FIM eigenvalue spectra to behavioral instability

- **Design tradeoffs:**
  - ReLU vs. smooth activations: ReLU has clearer catastrophe mechanism but smooth activations inherit via Pillar 2
  - Architecture choice: Attention mechanisms, normalization layers may *amplify* catastrophes (Section 4.3)
  - The paper claims no architecture escapes—the tradeoff is between different amplification factors

- **Failure signatures:**
  - If adversarial robustness improves substantially without capability loss → questions Pillar 3
  - If smooth-activation networks show qualitatively different failure modes → tests Pillar 1 generality
  - If FIM regularization demonstrably reduces catastrophe density → challenges core causal chain

- **First 3 experiments:**
  1. **Boundary density measurement:** Count actual ReLU activation boundaries in trained networks; measure ρ(δ) empirically vs. theoretical bound
  2. **FIM-to-catastrophe correlation:** Measure FIM condition number and adversarial vulnerability across architectures; test claimed causal relationship
  3. **Smooth activation comparison:** Compare catastrophe density between ReLU and GELU/Swish networks of equivalent capacity; test if Pillar 2 prediction holds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specific regularization methods targeting Fisher Information Matrix (FIM) singularities effectively reduce catastrophe density without destroying the universal approximation capability of the model?
- **Basis in paper:** [explicit] The authors explicitly call for "new methods for regularizing the FIM and avoiding its singularities during training" to potentially create more stable models.
- **Why unresolved:** While the paper proves that pathologies are necessary for learning complex tasks, it is unknown if the *degree* of pathology can be minimized while retaining sufficient utility.
- **What evidence would resolve it:** A formal proof or empirical demonstration of a regularization technique that lowers FIM eigenvalue ratios while maintaining comparable task performance.

### Open Question 2
- **Question:** What mathematical frameworks can define "safety" under the assumption of irreducible uncontrollability, replacing standard $(\epsilon, \delta)$-guarantees?
- **Basis in paper:** [explicit] Section 8.3 outlines a "Strategic Trilemma" and calls for the development of "new paradigms that assume irreducible uncontrollability as a fundamental mathematical feature."
- **Why unresolved:** Current safety research focuses on elimination or verification of failures; the paper proves this impossible, requiring a foundational shift in the definition of "safe operation."
- **What evidence would resolve it:** The formulation of a new safety theory that provides probabilistic or containment-based guarantees for systems known to possess dense catastrophic failures.

### Open Question 3
- **Question:** Can the geometry of natural data manifolds be formally characterized to guarantee they avoid the dense catastrophe regions of the ambient input space?
- **Basis in paper:** [inferred] The paper notes in Section 5.9 that networks remain useful because data manifolds are lower-dimensional and may not trigger latent catastrophes, but offers no formal proof that these manifolds are safe.
- **Why unresolved:** The paper proves catastrophes are dense in the *general* input space, but the interaction between these catastrophic hyperplanes and the specific sub-space of valid data remains empirically observed but theoretically under-constrained.
- **What evidence would resolve it:** Theorems establishing bounds on the probability of intersection between low-dimensional data manifolds and the network's catastrophic boundaries.

## Limitations

- The equivalence between ReLU boundary crossings and practical catastrophes lacks direct empirical evidence
- The topological argument assumes real-world tasks require approximating "generic" functions, but practical datasets may lie on low-dimensional smooth manifolds
- The FIM-based mechanism shows correlation but doesn't conclusively prove causation between eigenvalue pathology and input-space catastrophes

## Confidence

- **High confidence**: Mathematical bounds on ReLU catastrophe density (Pillar 1 combinatorial analysis)
- **Medium confidence**: Universal approximation implies ability to implement catastrophe-dense functions (Pillar 2 topological necessity)
- **Low confidence**: FIM eigenvalue pathology directly causes adversarial vulnerability (Pillar 3 causal mechanism)

## Next Checks

1. **Boundary density vs. actual failures**: Train small ReLU networks on benchmark tasks, systematically measure ReLU boundary density, and correlate with empirical adversarial vulnerability. Test if theoretical ρ(δ) bounds predict actual (ε, δ)-safety violations.

2. **FIM regularization experiment**: Apply explicit FIM regularization to large networks and measure effects on both training stability and adversarial robustness. If reducing FIM condition number improves robustness without hurting capability, this validates the causal chain.

3. **Smooth activation comparison**: Compare catastrophe density and adversarial vulnerability between ReLU and smooth-activation (GELU/Swish) networks of equivalent capacity trained on identical tasks. This tests whether the topological argument in Pillar 2 accurately predicts behavioral differences.