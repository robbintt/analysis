---
ver: rpa2
title: 'Foundation models for time series forecasting: Application in conformal prediction'
arxiv_id: '2507.08858'
source_url: https://arxiv.org/abs/2507.08858
tags:
- series
- data
- time
- prediction
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of time series foundation
  models (TSFMs) within conformal prediction (CP) for forecasting, particularly in
  data-constrained scenarios. Traditional CP requires a calibration set, which can
  limit model performance when data is scarce.
---

# Foundation models for time series forecasting: Application in conformal prediction

## Quick Facts
- arXiv ID: 2507.08858
- Source URL: https://arxiv.org/abs/2507.08858
- Reference count: 14
- Primary result: TSFMs achieve better MASE and MSIW than traditional models, with Chronos-Bolt, TimesFM, and TimesFM2 showing superior performance in data-constrained settings.

## Executive Summary
This study investigates the application of time series foundation models (TSFMs) within conformal prediction (CP) for forecasting, particularly in data-constrained scenarios. Traditional CP requires a calibration set, which can limit model performance when data is scarce. TSFMs, with their zero-shot capabilities, enable most available data to be allocated for calibration, addressing this limitation.

The study compares TSFMs—Lag-Llama, Chronos, Chronos-Bolt, TimesFM, and TimesFM2—against traditional statistical models and gradient boosting methods using three metrics: mean coverage rate (MCR), mean scaled interval width (MSIW), and mean absolute scaled error (MASE). Experiments are conducted on ERCOT (univariate) and NN5 Daily, NN5 Weekly, and M3 Monthly (multivariate) datasets across short, medium, and long forecasting horizons.

## Method Summary
The study employs Split Conformal Prediction (SCP) with local quantiles for time series forecasting. For each series, the first C points serve as context for TSFMs, with remaining data used for rolling window calibration. Conformity scores are computed as absolute errors between predictions and actual values on calibration data, and quantiles are calculated to generate prediction intervals. Five TSFMs are tested in zero-shot mode: Lag-Llama, Chronos-t5-small, Chronos-Bolt-small, TimesFM-1.0-200m, and TimesFM-2.0-500m. Baselines include Naive, SeasonalNaive, StatisticalEnsemble light (AutoETS + AutoCES + DynamicOptimizedTheta), and LightGBM with lag features. Three metrics evaluate performance: MCR (target 90%), MSIW (normalized by Naive), and MASE (normalized by Naive).

## Key Results
- TSFMs, especially Chronos-Bolt, TimesFM, and TimesFM2, consistently achieve better MASE and MSIW compared to traditional methods.
- While some TSFMs did not always meet the target coverage rate, their prediction intervals were narrower and more meaningful than those of traditional models.
- TSFMs demonstrated superior performance particularly when data was limited, as they require less data for training and can allocate more for calibration.
- Chronos-Bolt achieved inference speeds comparable to or better than traditional statistical models.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Conformal Prediction**: A framework for quantifying uncertainty by constructing prediction intervals with statistical guarantees. Needed to ensure reliable uncertainty quantification in time series forecasting. Quick check: Verify target coverage rate (90%) is achieved across datasets.
- **Split Conformal Prediction**: A practical CP method that splits data into training and calibration sets. Needed to adapt CP for time series with rolling windows. Quick check: Confirm rolling window implementation maintains temporal order.
- **Zero-shot Learning**: Model inference without task-specific training. Needed to leverage pre-trained TSFMs for time series forecasting. Quick check: Verify all TSFMs are used without fine-tuning.
- **Local Quantiles**: Calibration-specific quantile calculation per series. Needed to reduce exchangeability assumptions in non-stationary time series. Quick check: Compare local vs global quantile results for coverage and interval width.

## Architecture Onboarding

**Component Map**: Data -> Context Window (C) -> Rolling Window Calibration -> Conformity Scores -> Quantile Calculation -> Prediction Intervals

**Critical Path**: The zero-shot TSFM inference combined with local quantile SCP forms the critical path. Context window size directly impacts calibration data availability, which affects coverage rate and interval width.

**Design Tradeoffs**: Zero-shot TSFMs vs. fine-tuned models (generalization vs. adaptation), local vs. global quantiles (conservative vs. data-efficient), context window size (calibration data vs. historical information).

**Failure Signatures**: 
- MCR < 90%: Exchangeability assumption violated or insufficient calibration data
- High MSIW (>1.0): Model overcompensates with wide intervals (common in traditional models)
- Poor MASE: Model fails to capture underlying patterns effectively

**3 First Experiments**:
1. Run SCP with varying context window sizes (256, 512, 768) on ERCOT to assess sensitivity to calibration data.
2. Compare local vs global quantile approaches on NN5 Daily to quantify exchangeability assumption impact.
3. Implement Adaptive Conformal Inference on non-stationary datasets to address coverage failures.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's performance claims depend critically on specific calibration window sizes, which are not thoroughly explored for sensitivity.
- Computational efficiency comparisons may not fully account for model loading and preprocessing overhead, particularly for larger TSFMs.
- The assertion that TSFMs' narrower intervals are "more meaningful" is subjective and not rigorously quantified.

## Confidence

**High Confidence**: TSFMs achieve better MASE and MSIW than traditional methods, particularly Chronos-Bolt, TimesFM, and TimesFM2. SCP methodology is standard and correctly implemented. TSFMs require less training data while allocating more for calibration.

**Medium Confidence**: TSFMs consistently outperform traditional models across all datasets and horizons is somewhat overstated. Some TSFMs failed to meet target coverage rates. Inference speed comparisons are context-dependent.

**Low Confidence**: The claim that TSFMs' narrower intervals are "more meaningful" is subjective. The comparison between local and global quantile approaches is based on limited datasets.

## Next Checks
1. **Calibration Sensitivity Analysis**: Systematically vary calibration window sizes (C = 256, 512, 768, 1024) for each dataset to assess robustness of MCR, MSIW, and MASE.
2. **Stationarity Testing**: Apply statistical tests (Augmented Dickey-Fuller, KPSS) to quantify non-stationarity and explain coverage failures.
3. **Fine-tuning Baseline**: Implement minimal fine-tuning (5-10% training data) for best-performing TSFMs to determine if zero-shot constraint artificially limits performance.