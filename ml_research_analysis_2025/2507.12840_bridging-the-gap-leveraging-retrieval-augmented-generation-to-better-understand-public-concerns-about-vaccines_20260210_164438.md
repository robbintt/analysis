---
ver: rpa2
title: 'Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand
  Public Concerns about Vaccines'
arxiv_id: '2507.12840'
source_url: https://arxiv.org/abs/2507.12840
tags:
- public
- health
- concerns
- information
- vaccine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VaxPulse Query Corner (VaxPulse QC), a tool
  that leverages Retrieval Augmented Generation (RAG) to address complex queries about
  public vaccine concerns on social media. The tool analyzes large volumes of social
  media data to provide insights for public health administrators and stakeholders.
---

# Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand Public Concerns about Vaccines

## Quick Facts
- arXiv ID: 2507.12840
- Source URL: https://arxiv.org/abs/2507.12840
- Reference count: 20
- Primary result: VaxPulse QC achieves 0.90 faithfulness and 0.89 relevance for vaccine-related question answering using RAG

## Executive Summary
This paper introduces VaxPulse Query Corner (VaxPulse QC), a tool that leverages Retrieval Augmented Generation (RAG) to address complex queries about public vaccine concerns from social media. The system analyzes large volumes of vaccine-related social media data to provide insights for public health administrators and stakeholders. By using a two-iteration retrieval process with contextual compression, the tool aims to bridge the gap between traditional sentiment analysis methods and the nuanced understanding needed for effective public health communication.

## Method Summary
VaxPulse QC implements a two-iteration RAG pipeline. First, it retrieves vaccine-related social media comments (35,103 Shingrix posts from multiple platforms) using semantic similarity search with text-embedding-ada-002 embeddings stored in a LangChain vector database. The system applies ranking and re-ranking with FlashRank models, then performs contextual compression to extract query-relevant document portions. The second iteration refines retrieval before generating answers with GPT-4o. The system was evaluated using the RAGAs framework, measuring faithfulness and relevance across different query types.

## Key Results
- VaxPulse QC achieves average faithfulness score of 0.90 and relevance score of 0.89 for question answering
- Context precision improved from 0.56 to 0.66 and recall from 0.85 to 0.91 through two-iteration retrieval
- Summarization tasks achieved highest scores with 0.96 faithfulness and 0.94 relevance
- The tool successfully mitigates LLM hallucinations by grounding responses in retrieved social media posts

## Why This Works (Mechanism)

### Mechanism 1
Two-iteration retrieval with contextual compression improves both context precision (0.56→0.66) and context recall (0.85→0.91) compared to single-pass retrieval. The first iteration retrieves a broad candidate set from the vector database. The second iteration applies contextual compression to extract query-relevant portions of documents, reducing noise before final answer generation.

### Mechanism 2
Re-ranking mitigates the "lost in the middle" phenomenon where LLMs overlook information in the middle of long contexts. Initial ranking uses LangChain's longcontextreorder module to place most-similar documents at top and bottom positions. FlashRank models further reorder by relevance, with early exit for small sets.

### Mechanism 3
RAG reduces hallucinations compared to pure LLM generation by grounding responses in retrieved social media posts. The LLM (gpt-4o) generates answers conditioned on retrieved context chunks rather than parametric knowledge alone. Faithfulness scores (0.88-0.96 across query types) indicate outputs are attributable to source documents.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Pure LLMs lack access to current social media discussions and may hallucinate vaccine safety information. RAG grounds responses in actual user-generated content.
  - Quick check question: Can you explain why a standard LLM would fail to answer "What are people saying about Shingrix side effects this week?"

- **Concept: Contextual Compression**
  - Why needed here: Social media posts contain irrelevant content (URLs, mentions, non-vaccine discussion). Compression extracts only query-relevant segments before answer generation.
  - Quick check question: Given a 500-character Reddit comment about Shingrix, what would compression preserve versus discard for a query about "arm pain after second dose"?

- **Concept: Lost-in-the-Middle Phenomenon**
  - Why needed here: When retrieving 50+ documents, LLMs disproportionately attend to beginning and end positions, missing middle content. Re-ranking addresses this attention bias.
  - Quick check question: If you retrieve 30 documents and place the 15th most relevant document in position 15, what retrieval failure mode might occur?

## Architecture Onboarding

- **Component map:** Data ingestion (Tweepy/PRAW/Google API) → Preprocessing → BERT classification → GPT-4o filtering → text-embedding-ada-002 → Vector DB (LangChain) → Query embedding → Top-k semantic search → Ranking (longcontextreorder) → Re-ranking (FlashRank) → Contextual compression → Second re-ranking → gpt-4o generation → Formatted response

- **Critical path:** First iteration retrieval quality determines candidate pool; compression quality determines final answer relevance. Both iterations must complete before answer formulation.

- **Design tradeoffs:** Latency vs. quality (two-iteration prioritizes quality over speed), precision vs. recall (50% selection trades recall for precision), cost vs. accuracy (gpt-4o increases API costs).

- **Failure signatures:** Low context precision (<0.5) + high recall (compression threshold too permissive), high faithfulness + low relevance (retrieved context factually grounded but topically mismatched), "I don't have enough information" responses (check embedding quality, query formulation).

- **First 3 experiments:**
  1. Ablation study: Disable second iteration and measure faithfulness/relevance degradation
  2. Threshold sweep: Vary compression similarity threshold (60%, 70%, 80%, 90%) and document selection rate (25%, 50%, 75%)
  3. Misinformation handling test: Inject known vaccine misinformation posts and measure propagation

## Open Questions the Paper Calls Out

### Open Question 1
How does VaxPulse QC perform when applied to vaccines other than Shingrix, particularly those with different demographic targets or controversy levels? The evaluation is restricted to a case study of 35,103 posts regarding the Shingrix zoster vaccine, limiting generalizability.

### Open Question 2
To what extent do the automated RAGAs metrics (faithfulness, relevance) align with assessments from human public health experts? The authors utilized automated evaluation because generating manual ground truth is "time-consuming and laborious," but automated metrics may not capture nuanced aspects of public health communication quality.

### Open Question 3
Does the integration of VaxPulse QC into public health workflows lead to measurable improvements in communication strategies or vaccine confidence? High faithfulness scores do not guarantee that public health administrators will successfully use the insights to alter public sentiment.

## Limitations
- The study focuses exclusively on Shingrix vaccine discussions, limiting generalizability to other vaccines or health topics
- The 80% similarity threshold and 50% selection rate for contextual compression are presented without sensitivity analysis
- The RAGAs framework provides automated evaluation scores that may not fully capture nuanced aspects of public health communication quality

## Confidence

- **High confidence**: Overall RAG framework architecture and hallucination reduction through document grounding
- **Medium confidence**: Effectiveness of two-iteration retrieval process for improving context quality metrics
- **Medium confidence**: Mitigation of lost-in-the-middle phenomenon through re-ranking strategies
- **Low confidence**: Optimal parameter settings for contextual compression thresholds and selection rates

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary the contextual compression threshold (60%, 70%, 80%, 90%) and document selection rate (25%, 50%, 75%) to identify optimal settings and quantify their impact on answer quality across different query types.

2. **Cross-vaccine generalization test**: Apply VaxPulse QC to social media data for a different vaccine (e.g., HPV or COVID-19) and compare performance metrics to ensure the system generalizes beyond Shingrix-specific discussions.

3. **Human evaluation validation**: Conduct expert human review of a stratified sample of generated answers to validate that RAGAs faithfulness and relevance scores correlate with actual factual accuracy and public health communication effectiveness in real-world contexts.