---
ver: rpa2
title: Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators
arxiv_id: '2503.19877'
source_url: https://arxiv.org/abs/2503.19877
tags:
- reasoning
- process
- evaluators
- outcome
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores scaling evaluation-time compute for language
  models by using reasoning models as evaluators that generate chain-of-thought reasoning.
  The core idea is to prompt reasoning models to evaluate not just the final answer
  (outcome evaluation) but also assess each step in the solution separately (process
  evaluation).
---

# Scaling Evaluation-time Compute with Reasoning Models as Process Evaluators

## Quick Facts
- **arXiv ID:** 2503.19877
- **Source URL:** https://arxiv.org/abs/2503.19877
- **Reference count:** 40
- **One-line result:** Reasoning evaluators that generate chain-of-thought before judging outperform direct evaluators in Best-of-N reranking, with process+outcome combination yielding best results

## Executive Summary
This work demonstrates that evaluation-time compute can be scaled effectively by using reasoning models to evaluate not just final answers but also intermediate solution steps. The core innovation is prompting reasoning models to generate extended chain-of-thought reasoning before rendering judgments, which improves evaluator accuracy monotonically with more reasoning tokens. Experiments show that spending inference compute on enhanced evaluation (fewer candidates, more deliberative scoring) can match or exceed spending it on generating more candidates. Notably, reasoning process+outcome evaluation with fewer candidates (Best-of-8) outperforms direct evaluators (Best-of-64) while using less total compute, with gains especially pronounced in coding tasks where traditional process evaluators struggle.

## Method Summary
The approach prompts reasoning models (like DeepSeek-R1-Distill or QwQ) to act as evaluators by generating chain-of-thought before judging responses. For outcome evaluation, models verify final answers by extracting token probabilities for "1" (correct) or "0" (incorrect). For process evaluation, responses are split into steps using an auxiliary model, then each step is evaluated sequentially with chain-of-thought reasoning conditioned on previous steps. Scores are aggregated using mean_logit across steps. The final score combines outcome and process evaluations with weight α (0.5 in experiments), and Best-of-N reranking selects the highest-scoring response. This contrasts with direct evaluators that score without extended reasoning.

## Key Results
- Evaluator performance improves monotonically with more reasoning tokens generated, paralleling generation-time scaling trends
- Best-of-8 with reasoning process+outcome evaluator (32B) achieves ~50.5 avg score vs. ~48.5 for Best-of-64 with 72B direct PRM, using less total compute
- Reasoning outcome evaluators outperform trained outcome reward models
- Process evaluators are conservative (high precision, lower recall) but excel at breaking ties among outcome-scored candidates
- Gains are especially pronounced in coding tasks where traditional process evaluators struggle

## Why This Works (Mechanism)

### Mechanism 1: Evaluation-Time Compute Scaling via Chain-of-Thought
Reasoning models produce extended chain-of-thought sequences prior to final evaluation, allocating additional inference compute to deliberation. This parallels generation-time scaling where longer CoT improves problem-solving—the paper demonstrates the same principle applies to evaluation. Monotonic improvement holds across model scales and task domains beyond tested benchmarks.

### Mechanism 2: Complementary Precision-Recall from Process + Outcome Combination
Process evaluators exhibit high precision but lower recall (conservative—flag many correct responses as having step errors), while outcome evaluators achieve higher recall but can assign high scores to responses with flawed reasoning. Combining scores uses process evaluation as a tie-breaker among responses with similar outcome scores, filtering false positives.

### Mechanism 3: Compute Reallocation—Better Selection Over More Generation
Allocating inference compute to enhanced evaluation (small N, expensive evaluator) can match or exceed allocating it to more candidate generation (large N, cheap evaluator). More accurate evaluators extract higher quality from smaller candidate pools. Direct evaluators suffer from reward model overoptimization—as N increases, they increasingly select suboptimal responses they incorrectly scored highly.

## Foundational Learning

- **Process Reward Models (PRMs)**
  - Why needed: Primary baseline this work compares against; understand what "direct" process evaluation means and why reasoning-based process evaluation differs.
  - Quick check: Given a 5-step solution with PRM scores [0.9, 0.8, 0.3, 0.7, 0.9], what final score do min-aggregation vs. mean_logit-aggregation produce?

- **Outcome Reward Models (ORMs)**
  - Why needed: Contrasting evaluation paradigm; the paper shows reasoning outcome evaluators outperform trained ORMs, and combining with process evaluation yields further gains.
  - Quick check: Why might an ORM assign a high score to a response that contains reasoning errors?

- **Test-Time Compute Scaling**
  - Why needed: Core paradigm—understanding that inference-time compute is a resource to be allocated, analogous to training compute.
  - Quick check: If doubling N in Best-of-N sampling costs 2× generation compute, what two factors determine whether this improves final accuracy?

- **Reasoning Models (Long CoT Models)**
  - Why needed: The key enabler—models like DeepSeek-R1-Distill or QwQ that natively generate extended deliberation chains before outputs.
  - Quick check: What is the difference between an instruction-tuned model prompted to "think step by step" vs. a reasoning model trained with RL for long CoT?

- **Self-Consistency**
  - Why needed: The paper compares process evaluation against self-consistency as alternative strategies for spending evaluation compute.
  - Quick check: Why does majority voting over 8 CoTs provide different signal than evaluating 8 steps individually?

## Architecture Onboarding

**Component map:**
┌─────────────────────────────────────────────────────────────┐
│ Generator LM ──► N candidate responses per problem          │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┴─────────────────────┐
        ▼                                           ▼
┌───────────────────┐                    ┌────────────────────┐
│ Reasoning Outcome │                    │ Response Splitter  │
│ Evaluator         │                    │ (Msplit model)     │
│ (CoT + judgment)  │                    └────────────────────┘
└───────────────────┘                              │
        │                                          ▼
        │                               ┌────────────────────┐
        │                               │ Reasoning Process  │
        │                               │ Evaluator          │
        │                               │ (per-step CoT +    │
        │                               │  judgment)         │
        │                               └────────────────────┘
        │                                          │
        │                                          ▼
        │                               ┌────────────────────┐
        │                               │ Score Aggregator   │
        │                               │ (mean_logit)       │
        │                               └────────────────────┘
        │                                          │
        └─────────────────────┬───────────────────┘
                              ▼
                    ┌────────────────────┐
                    │ Score Combiner     │
                    │ α×outcome +        │
                    │ (1-α)×process      │
                    └────────────────────┘
                              │
                              ▼
                    ┌────────────────────┐
                    │ Select argmax      │
                    └────────────────────┘

**Critical path:**
1. Generate N responses (N=8 for reasoning evaluators vs. N=64 for direct evaluators at comparable compute)
2. For each response:
   - Outcome branch: Prompt reasoning model with (problem, response) → extract token probability for "1" (correct) as score
   - Process branch: Split response into steps via Msplit → for each step k, prompt reasoning model with (problem, steps 1..k) → extract step scores → aggregate via mean_logit
   - Combine: `final = 0.5 × outcome + 0.5 × process` (α=0.5 default)
3. Rank by final score, return top response

**Design tradeoffs:**
- **N vs. evaluator cost:** Lower N (8) with expensive reasoning evaluator can beat higher N (64) with cheap direct evaluator—provided evaluator accuracy is sufficiently higher
- **α tuning:** Paper uses 0.5 but optimal skews toward outcome; process primarily breaks ties rather than dominates scoring
- **Multi-step vs. single-step process eval:** Multi-step (separate inference per step) outperforms single-step (all steps in one prompt) by 2-7% absolute—more compute, avoids context limits
- **Model-based vs. heuristic splitting:** Model-based (Msplit inserting "[SPLIT]" tokens) handles edge cases (code, unstructured outputs) that heuristics miss

**Failure signatures:**
- **Reward model overoptimization:** As N increases, direct evaluator performance plateaus or declines—evaluator selects responses it incorrectly scored high
- **Unfaithful reasoning false positives:** Process evaluator flags steps in correct-final-answer responses; 44.4% are genuine errors but remainder are conservative over-flagging
- **Domain mismatch:** Direct PRMs trained on math (PRM800K) perform poorly on code—process evaluation on code requires model-based splitting
- **Recall collapse on hard problems:** When few correct candidates exist, process evaluator's conservatism excludes valid solutions

**First 3 experiments:**
1. **Scaling validation:** On ProcessBench, measure F1 vs. reasoning tokens generated by sweeping temperature/max_tokens for reasoning outcome evaluator; verify monotonic improvement
2. **Compute budget equivalence:** Run Best-of-8 with reasoning process+outcome evaluator (32B) vs. Best-of-64 with direct PRM (72B) on 7 benchmarks; compute total inference FLOPs to confirm similar budget with superior accuracy
3. **Ablation on mixing strategy:** Compare α-weighted averaging vs. 2-stage filtering (outcome threshold → process rerank) to validate that process evaluation's value is tie-breaking, not independent selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning utilizing reasoning process evaluators effectively mitigate reward hacking during generator training?
- Basis in paper: The conclusion states that investigating whether reasoning process evaluators can mitigate reward hacking represents a promising direction for future research.
- Why unresolved: The current work applies evaluators only at inference time (Best-of-N reranking) and does not test their stability or robustness when used as reward signals for training loops.
- What evidence would resolve it: Experiments training generators via RL using reasoning evaluators as the reward model, measuring the incidence of reward hacking compared to traditional ORMs.

### Open Question 2
- Question: Does fine-tuning reasoning models specifically for evaluation yield significant performance gains over off-the-shelf prompting?
- Basis in paper: The authors explicitly ask if reasoning evaluators can be improved through training in addition to prompting, noting existing trained evaluators do not leverage long CoTs.
- Why unresolved: The study relies on prompting off-the-shelf models; the potential benefits of training a model to generate evaluation-specific CoTs remain unquantified.
- What evidence would resolve it: A comparison of prompted reasoning evaluators against models fine-tuned on evaluation datasets using long CoT supervision.

### Open Question 3
- Question: How should self-evaluation strategies be optimized to handle the trade-off between evaluating full Chain-of-Thought traces and condensed summaries?
- Basis in paper: Section 5.1 discusses the challenge of evaluating lengthy CoTs and suggests evaluating summaries, encouraging further investigation into self-evaluation strategies.
- Why unresolved: Preliminary results show mixed effectiveness between evaluating full thoughts versus summaries, leaving the optimal strategy for self-correction undefined.
- What evidence would resolve it: Ablation studies on self-evaluation accuracy across varying CoT lengths and summary compression levels.

## Limitations

- **Domain dependence:** Direct PRMs fail on code tasks when trained only on math data, limiting compute reallocation benefits across domains
- **Conservative recall:** Process evaluators' high precision comes at the cost of lower recall, potentially excluding valid solutions when correct candidates are rare
- **Unfaithful reasoning challenge:** Models reaching correct conclusions through flawed intermediate steps can cause process evaluators to flag otherwise correct responses

## Confidence

- **High Confidence:** Monotonic improvement of reasoning evaluator performance with more reasoning tokens; compute-equivalence claim demonstrated on specific benchmarks
- **Medium Confidence:** Process+outcome combination effectiveness depends on optimal α tuning and sufficient correct candidates for tie-breaking
- **Low Confidence:** Generalizability of compute-reallocation tradeoff across budget regimes and stability of α mixing weights across diverse domains remain untested

## Next Checks

1. **Domain Transfer Validation:** Test Best-of-8 reasoning evaluator against Best-of-64 direct evaluator on non-math, non-code reasoning benchmarks (e.g., commonsense reasoning, symbolic manipulation) to verify the compute-reallocation benefit holds across domains.

2. **Hard Problem Stress Test:** Construct or identify problems where correct solutions are extremely rare (<5% in candidate pools). Measure whether process evaluator's conservatism causes catastrophic recall collapse, breaking the Best-of-8 advantage.

3. **Budget Regime Sweep:** Vary the total compute budget (e.g., 10× more/less) and test whether the N=8 reasoning vs. N=64 direct equivalence holds, or if different optimal N/evaluator-cost tradeoffs emerge at different scales.