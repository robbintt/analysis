---
ver: rpa2
title: Surprisal reveals diversity gaps in image captioning and different scorers
  change the story
arxiv_id: '2511.04754'
source_url: https://arxiv.org/abs/2511.04754
tags:
- surprisal
- captions
- image
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates linguistic diversity in image captioning\
  \ by measuring surprisal variance\u2014the spread of token-level negative log-probabilities\
  \ within a caption set. On the MSCOCO test set, five state-of-the-art vision-and-language\
  \ models are compared to human captions under two decoding strategies (greedy and\
  \ nucleus sampling)."
---

# Surprisal reveals diversity gaps in image captioning and different scorers change the story
## Quick Facts
- **arXiv ID:** 2511.04754
- **Source URL:** https://arxiv.org/abs/2511.04754
- **Reference count:** 24
- **Key outcome:** Model caption diversity is lower than human under caption-trained scoring but higher under GPT-2 rescoring, showing scorer-dependent conclusions.

## Executive Summary
This paper examines linguistic diversity in image captioning by measuring the variance of surprisal (negative log-probability) across captions for the same image. Using five state-of-the-art vision-and-language models and human captions on the MSCOCO test set, it finds that human captions exhibit about twice the surprisal variance of model captions when evaluated with a caption-trained n-gram language model. However, when the same captions are rescored using GPT-2, the models display higher variance than humans. This reversal demonstrates that conclusions about caption diversity are highly sensitive to the choice of surprisal scorer.

## Method Summary
The study compares linguistic diversity in image captioning by calculating the variance of token-level surprisal scores across captions generated for the same image. Five state-of-the-art vision-and-language models and human captions are evaluated on the MSCOCO test set using both greedy and nucleus sampling decoding strategies. Surprisal is computed with a caption-trained n-gram language model and, for comparison, with GPT-2. The main diversity metric is the variance of surprisal values across captions, reflecting the spread of information content.

## Key Results
- Human captions show about twice the surprisal variance of model captions under a caption-trained n-gram language model.
- Rescoring with GPT-2 reverses this pattern: models show higher variance than humans.
- The choice of surprisal scorer significantly affects conclusions about caption diversity.

## Why This Works (Mechanism)
Diversity in image captioning is measured by the spread of information content, captured via surprisal variance. When models and humans generate captions for the same image, humans tend to produce captions with more varied and less predictable word choices, reflected in higher surprisal variance under a caption-specific scorer. However, when a general-language model like GPT-2 is used, the relative ordering changes, suggesting that scorer bias or domain mismatch can alter perceived diversity.

## Foundational Learning
- **Surprisal (negative log-probability)**: quantifies how unexpected a word is given the context; higher values mean more surprising, less predictable language.
  - *Why needed*: central metric for measuring linguistic diversity in captions.
  - *Quick check*: verify that surprisal variance increases with more diverse, less repetitive language.

- **Language model scoring**: uses n-gram or neural models to assign probabilities to word sequences, influencing diversity metrics.
  - *Why needed*: the choice of scorer can bias results toward or against certain types of diversity.
  - *Quick check*: compare outputs of caption-trained vs. general-language scorers on the same set.

- **Diversity metrics in captioning**: go beyond accuracy to assess the richness and variety of generated captions.
  - *Why needed*: standard metrics like BLEU don't capture how varied captions are for the same image.
  - *Quick check*: ensure variance is only one of several diversity measures considered.

## Architecture Onboarding
- **Component map**: Image features → Vision-Language Model → Captions → Language Model Scorer → Surprisal Variance
- **Critical path**: Image → VL Model → Captions → Surprisal Scoring → Variance Calculation
- **Design tradeoffs**: caption-trained scorer favors domain-specific diversity; general scorer (GPT-2) may capture broader language patterns but introduce bias.
- **Failure signatures**: inconsistent results across scorers, high variance with low novelty, or scorer overfitting to domain.
- **3 first experiments**:
  1. Compare surprisal variance across multiple human reference sets to test robustness.
  2. Test additional language models (e.g., BERT, RoBERTa) as scorers to see if reversal effect persists.
  3. Supplement variance analysis with other diversity metrics (e.g., n-gram uniqueness, semantic coverage).

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses only a single human reference set, making it unclear if the diversity gap is consistent across different human groups or annotation conditions.
- The n-gram language model's training details and domain match are not fully specified, potentially affecting the reliability of surprisal variance as a diversity measure.
- Only variance is used as a diversity metric, which may not fully capture novelty, coverage, or topical range.

## Confidence
- **High confidence**: Human captions have higher surprisal variance than models under caption-trained scoring, and this reverses under GPT-2 scoring.
- **Medium confidence**: The reversal reflects scorer bias or domain mismatch, but this is not fully validated with ablation studies.
- **Low confidence**: Broader implications for diversity evaluation or model development are speculative due to limited scope and dataset.

## Next Checks
1. Replicate the analysis using multiple human reference sets (e.g., from different annotation rounds or datasets) to test the robustness of the human-model diversity gap.
2. Evaluate additional, diverse surprisal scorers (e.g., other LMs, transformer-based, or task-specific models) to determine whether the observed reversal is consistent or scorer-dependent.
3. Supplement variance analysis with other diversity metrics (e.g., n-gram uniqueness, semantic coverage, or novelty scoring) to provide a more complete picture of caption diversity.