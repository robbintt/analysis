---
ver: rpa2
title: Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light
  Curves
arxiv_id: '2507.19520'
source_url: https://arxiv.org/abs/2507.19520
tags:
- data
- exoplanets
- exoplanet
- augmentation
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores the use of common machine learning models\u2014\
  logistic regression, k-nearest neighbors, and random forest\u2014to detect exoplanets\
  \ from synthetic light curve data. The primary challenge addressed is the severe\
  \ class imbalance in the dataset, with less than 1% of cases representing true exoplanets."
---

# Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves

## Quick Facts
- **arXiv ID:** 2507.19520
- **Source URL:** https://arxiv.org/abs/2507.19520
- **Reference count:** 24
- **One-line primary result:** Simple ML models (logistic regression, KNN, random forest) can detect exoplanets from synthetic light curves with up to 90.2% F1-score when paired with SMOTE data augmentation.

## Executive Summary
This study demonstrates that common machine learning models can effectively detect exoplanets from synthetic light curve data when combined with appropriate data augmentation. The primary challenge addressed is severe class imbalance, with less than 1% of cases representing true exoplanets. Through SMOTE-based data augmentation, the researchers transformed a severely imbalanced dataset into a balanced one, enabling simpler classifiers to achieve strong performance metrics. The work provides a cost-effective and efficient alternative to complex deep learning models for exoplanet detection, with logistic regression achieving the highest F1-score of 90.2%.

## Method Summary
The study employed logistic regression, k-nearest neighbors, and random forest classifiers on synthetic light curve data from the Kepler mission. After preprocessing (Fourier-based augmentation, Savitzky-Golay filtering, normalization, and RobustScalar transformation), SMOTE was applied to balance the severely imbalanced dataset (37 exoplanets vs. 5,050 non-exoplanets). The models were trained on the augmented dataset and evaluated using accuracy, recall, precision, and F1-score metrics. The approach focused on achieving balanced performance across both classes rather than maximizing accuracy alone.

## Key Results
- Logistic regression achieved the highest F1-score of 90.2% after data augmentation
- All models showed significantly improved recall and precision after SMOTE application
- Without augmentation, models achieved ~99% accuracy but 0% recall due to majority class bias
- The study demonstrates simpler ML models can match complex deep learning approaches when properly augmented

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic minority oversampling enables simple classifiers to learn meaningful decision boundaries from severely imbalanced astronomical data.
- Mechanism: SMOTE interpolates between existing minority class samples and their k-nearest neighbors to generate synthetic examples, expanding the decision region for the positive class.
- Core assumption: The interpolated synthetic samples preserve the underlying feature distribution of true exoplanet signals.
- Evidence anchors: Abstract states all models showed improved recall and precision after SMOTE; Section VI details SMOTE's interpolation mechanism.
- Break condition: If synthetic samples diverge significantly from real transit signatures, models may overfit to artificial patterns.

### Mechanism 2
- Claim: Standard classification accuracy is misleading for imbalanced domains; recall and precision must guide model selection.
- Mechanism: In the raw dataset, KNN and random forest achieved 99.2% accuracy but 0% recall—they simply predicted the majority class for every instance.
- Core assumption: The cost of missing a true exoplanet is comparable to or greater than the cost of false positives.
- Evidence anchors: Section V.D shows KNN's 0% recall despite 99.2% accuracy; Section VII.D emphasizes F1-score as the primary metric.
- Break condition: If downstream workflows prioritize different tradeoffs, F1-score optimization may not align with operational goals.

### Mechanism 3
- Claim: Logistic regression can capture transit signal patterns when decision boundaries are approximately linear in the normalized feature space.
- Mechanism: After normalization and SMOTE augmentation, logistic regression achieved 91.0% accuracy and 90.2% F1-score—outperforming more complex ensemble methods.
- Core assumption: The preprocessing pipeline transforms raw flux data into a representation where exoplanet vs. non-exoplanet classes are approximately linearly separable.
- Evidence anchors: Section VII.A and Table I show logistic regression's superior F1-score; Section VII.D compares model performances.
- Break condition: If real-world light curves contain non-linear noise patterns not represented in synthetic samples, logistic regression's linear decision boundary may fail to generalize.

## Foundational Learning

- **Concept: Transit Photometry & Light Curves**
  - Why needed here: The core input data is flux (brightness) over time; exoplanets cause periodic dips that must be distinguished from noise.
  - Quick check question: Can you sketch how a transit light curve differs from a binary star eclipse pattern?

- **Concept: Class Imbalance in Binary Classification**
  - Why needed here: With <1% positive class, models can achieve high accuracy while failing completely on the task of interest.
  - Quick check question: If a dataset has 99% negatives and 1% positives, what accuracy would a model achieve by always predicting "negative"?

- **Concept: SMOTE (Synthetic Minority Oversampling Technique)**
  - Why needed here: Simply duplicating minority samples causes overfitting; SMOTE creates interpolated synthetic samples along decision boundaries.
  - Quick check question: How does SMOTE differ from random oversampling, and what assumption does it make about feature space geometry?

## Architecture Onboarding

- **Component map:** Raw flux vectors (3,197 features per star) → preprocessing pipeline (Fourier, Savitzky-Golay, normalization, RobustScaler) → SMOTE augmentation → Logistic Regression/KNN/Random Forest classifiers → Confusion matrix → Accuracy, Recall, Precision, F1-score

- **Critical path:** Load Kepler flux data (5,087 samples × 3,197 time points) → Apply preprocessing transforms sequentially → Apply SMOTE to training split only → Train classifier on augmented training set → Evaluate on unaugmented test set

- **Design tradeoffs:**
  - Synthetic data fidelity vs. quantity: SMOTE generated ~5,000 synthetic exoplanet samples from only 37 real instances
  - Model complexity vs. interpretability: Logistic regression offers transparent coefficients; random forest captures non-linear interactions
  - Precision vs. recall: Random Forest achieved 99.7% precision but only 74.8% recall; KNN had more balanced 88.4% precision / 83.6% recall

- **Failure signatures:**
  - High accuracy, zero recall: Model predicts only majority class (indicates unresolved class imbalance)
  - Training accuracy >> test accuracy: Overfitting to synthetic samples rather than learning generalizable patterns
  - High false negatives after augmentation: Model still biased toward majority class

- **First 3 experiments:**
  1. Baseline diagnostic: Train all three models on raw data without augmentation; confirm near-zero recall to establish the problem
  2. Ablation study: Apply each preprocessing step individually to isolate which augmentation contributes most to F1 improvement
  3. Cross-validation on augmented data: Use stratified k-fold cross-validation to assess variance in F1-scores across different train/test splits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the generation of synthetic samples via SMOTE introduce artificial artifacts or overfitting that fails to generalize to real, unseen astrophysical data?
- Basis in paper: The authors note that the generation of new exoplanet data is "dependent on a limited pool of variability" derived from only 37 original positive instances.
- Why unresolved: The study reports high F1-scores on the test set, but the test set was derived from the same synthetic distribution as the training set.
- What evidence would resolve it: Validating the trained models on a separate, hold-out set of confirmed exoplanets from different observation runs or telescopes.

### Open Question 2
- Question: To what extent can the performance of these simple models be further improved through rigorous hyper-parameter tuning?
- Basis in paper: The conclusion states that "hyper-parameter tuning can be employed" to further improve accuracy.
- Why unresolved: The experiments utilized default or arbitrarily chosen parameters to establish a baseline rather than optimizing for the specific feature space.
- What evidence would resolve it: A comparative study using grid search or Bayesian optimization on the augmented dataset.

### Open Question 3
- Question: Do the models learn domain-specific noise profiles of the Kepler telescope rather than universal transit signals?
- Basis in paper: The paper mentions TESS and K2 as distinct missions but relies exclusively on Kepler data for training and testing.
- Why unresolved: The preprocessing steps may not fully decouple the transit signal from the specific instrumental noise of the Kepler sensor.
- What evidence would resolve it: A cross-dataset evaluation where the model trained on Kepler data is tested on TESS light curves without retraining.

## Limitations

- The study relies on synthetic data augmentation from only 37 real exoplanet instances, raising questions about real-world generalization
- High F1-scores may be inflated due to data leakage from applying SMOTE before train-test splitting
- The preprocessing pipeline lacks sufficient specification for exact reproduction
- No comparison with state-of-the-art deep learning approaches limits assessment of simple models' advantages

## Confidence

**High Confidence:** The core finding that class imbalance severely biases standard classifiers toward the majority class is well-established and reproducible.

**Medium Confidence:** The specific F1-score values are questionable due to potential data leakage and small sample size of real positive examples.

**Low Confidence:** Claims about the superiority of simple models over complex deep learning approaches lack direct validation, as no such comparison was conducted.

## Next Checks

1. **Data Leakage Validation:** Reproduce the analysis with SMOTE applied only to training folds and compare F1-scores to quantify inflation from data leakage.

2. **Real Data Generalization:** Apply the trained models to a separate, independent exoplanet dataset (e.g., from TESS mission) to test whether high performance on synthetic-augmented data translates to real astronomical observations.

3. **Baseline Comparison:** Implement and evaluate at least one CNN or transformer model on the same dataset to directly assess whether simple ML approaches genuinely outperform or underperform state-of-the-art methods for exoplanet detection.