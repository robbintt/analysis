---
ver: rpa2
title: Fair Representation Learning for Continuous Sensitive Attributes using Expectation
  of Integral Probability Metrics
arxiv_id: '2505.06435'
source_url: https://arxiv.org/abs/2505.06435
tags:
- eipm
- frem
- sensitive
- fair
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fair Representation using EIPM (FREM), an algorithm
  for learning fair representations with continuous sensitive attributes. FREM introduces
  a new metric called Expectation of Integral Probability Metrics (EIPM) to measure
  the fairness level of representation space for continuous sensitive attributes.
---

# Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics

## Quick Facts
- **arXiv ID**: 2505.06435
- **Source URL**: https://arxiv.org/abs/2505.06435
- **Reference count**: 40
- **Primary result**: FREM outperforms state-of-the-art methods for fair representation learning with continuous sensitive attributes, achieving better fairness-prediction trade-offs.

## Executive Summary
This paper introduces Fair Representation using EIPM (FREM), a novel algorithm for learning fair representations when sensitive attributes are continuous. The key innovation is the Expectation of Integral Probability Metrics (EIPM), a metric that measures the statistical distance between conditional and marginal distributions of representations across different sensitive attribute values. By minimizing EIPM, FREM ensures that any prediction head built on top of the learned representation inherits fairness properties, regardless of the head selection. The method uses kernel smoothing to create a computationally feasible estimator and demonstrates superior performance on both tabular and graph datasets compared to existing approaches.

## Method Summary
FREM learns fair representations by minimizing a supervised risk subject to an EIPM constraint, which is relaxed to a Lagrangian formulation. The method uses an encoder to map inputs to representations and a prediction head for the target task. The EIPM metric is estimated using kernel smoothing (Nadaraya-Watson estimator) combined with Maximum Mean Discrepancy (MMD) as the specific Integral Probability Metric. This approach avoids the instability of adversarial training while maintaining computational tractability. The representation distribution for any specific sensitive value is forced to match the overall representation distribution, effectively removing sensitive information while preserving predictive utility.

## Key Results
- FREM outperforms existing state-of-the-art methods including regularization approaches and variants of fair representation learning for binary attributes
- The method achieves better fairness-prediction trade-offs on both tabular (ADULT, CRIME) and graph (POKEC) datasets
- FREM learns more fair representations as measured by mutual information between the representation and sensitive attribute
- The approach demonstrates robust performance without requiring arbitrary discretization of continuous sensitive attributes

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment via Expectation of IPMs (EIPM)
If the representation $Z$ is learned such that the EIPM value is low, the representation distribution becomes conditionally independent of the continuous sensitive attribute $S$. The EIPM metric measures the expected distance between conditional distributions $P_{Z|S=s}$ and the marginal distribution $P_Z$. By minimizing this metric, the algorithm forces the distribution of representations for any specific sensitive value $s$ to match the overall distribution of representations, thereby removing information about $S$ from $Z$.

### Mechanism 2: Kernel Smoothing for Finite Sample Estimation
Weighted empirical distributions using kernel smoothing allow for statistically consistent estimation of EIPM even when individual values of $S$ appear rarely (or once) in the dataset. The paper uses a Nadaraya-Watson style estimator where samples $Z_j$ with sensitive attributes $S_j$ close to the target $S_i$ are weighted higher using a kernel function $K_\gamma$. This creates a "soft" conditional distribution estimate rather than a hard binning approach.

### Mechanism 3: Tractable Optimization via MMD
Using the Maximum Mean Discrepancy (MMD) as the specific Integral Probability Metric allows for a closed-form solution to the EIPM estimation, avoiding the instability of adversarial training. By choosing the Reproducing Kernel Hilbert Space (RKHS) unit ball as the discriminator set, the IPM becomes MMD. MMD has a closed-form expression based on kernel matrices, converting a complex saddle-point optimization problem into a standard minimization problem.

## Foundational Learning

- **Concept: Integral Probability Metrics (IPM)**
  - Why needed here: The paper defines its core fairness metric (EIPM) based on IPM. You must understand that IPM measures the distance between two probability distributions by finding the maximum difference in expectations over a class of test functions.
  - Quick check question: Can you explain why IPM is preferred over KL-divergence when densities are unknown or hard to estimate?

- **Concept: Kernel Density Estimation (KDE) & Nadaraya-Watson**
  - Why needed here: The paper's primary technical contribution for *continuous* attributes is the estimator. You need to understand how weighting neighbors by kernel functions creates a "soft" conditional probability estimate.
  - Quick check question: If you have a sample with $S=50$ and neighbors $S=49, 51$, how does an RBF kernel assign weights compared to a Box kernel?

- **Concept: Reproducing Kernel Hilbert Space (RKHS)**
  - Why needed here: The paper leverages properties of RKHS to use MMD as the IPM. Understanding that functions in RKHS can be evaluated via inner products (kernel trick) explains why the optimization is tractable.
  - Quick check question: Why does the "kernel trick" allow us to compute distances between distributions in high/infinite-dimensional spaces without explicitly mapping data to those spaces?

## Architecture Onboarding

- **Component map:** Inputs -> Encoder (X → Z) -> Prediction Head (Z → Ŷ) -> EIPM Estimator (Z, S) -> Total Loss
- **Critical path:** Compute representation $Z_i = h(X_i)$ for a batch → Compute the "Kernel Weight Matrix" $A_\gamma$ based on $S$ distances → Compute the MMD between weighted empirical distributions → Backpropagate total Loss = Supervised Loss + $\lambda \times$ Estimated EIPM
- **Design tradeoffs:** Binning vs. Smoothing (binning is brittle; smoothing preserves continuity but introduces bandwidth hyperparameters); Adversarial vs. MMD (adversarial is flexible but unstable; MMD is stable but restricted to kernel-defined topology)
- **Failure signatures:** Unstable Training (NaNs from degenerate $A_\gamma$ matrix); High Bias (Over-smoothing from too large $\gamma$); Collapsed Representation (from too high $\lambda$)
- **First 3 experiments:** 1) Estimator Validation on Gaussian data with known EIPM values; 2) Ablation on Binning comparing FREM against "MMD with Binning" on ADULT; 3) Hyperparameter Sensitivity on $\gamma$ plotting Pareto frontier of Accuracy vs. Fairness

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the FREM framework be extended to handle multivariate continuous or mixed-type (continuous and categorical) sensitive attributes?
  - Basis in paper: [explicit] The authors state: "Extensions for multivariate continuous or mix-typed... sensitive attributes would be useful."
  - Why unresolved: The current formulation assumes a one-dimensional sensitive attribute $S \subset \mathbb{R}$, and kernel smoothing suffers from the curse of dimensionality in higher dimensions.
  - What evidence would resolve it: A modified formulation for vector-valued $S$ and empirical validation of fairness-accuracy trade-offs on datasets with multiple sensitive attributes.

- **Open Question 2:** Can a scalable set of discriminators encompassing non-smooth functions be constructed while maintaining the computational feasibility of the EIPM estimator?
  - Basis in paper: [explicit] The authors note: "It would be useful to construct a set of discriminators such that it includes less smooth functions but computation of FREM is feasible."
  - Why unresolved: The current choice (MMD/RKHS) favors smooth functions, whereas many practical prediction heads (e.g., ReLU networks) are less smooth.
  - What evidence would resolve it: Derivation of a tractable EIPM estimator for a Lipschitz discriminator class and comparison of performance on non-smooth prediction tasks.

- **Open Question 3:** Can alternative fairness measures such as KL divergence or Mutual Information be effectively estimated and optimized for continuous sensitive attributes within this framework?
  - Basis in paper: [explicit] The authors state: "Searching for other fairness measures for FRL with continuous sensitive attributes is worth pursuing."
  - Why unresolved: Estimating these metrics typically requires difficult density estimation for continuous attributes; IPM was chosen specifically to avoid this issue.
  - What evidence would resolve it: A new kernel-smoothed estimator for an alternative metric (e.g., KL divergence) with theoretical convergence guarantees and empirical validation.

## Limitations

- The paper does not provide ablation studies on the choice of kernel for EIPM estimation, leaving the robustness to kernel selection unclear.
- While Proposition 3 gives a closed-form estimator, the paper lacks experiments showing how the choice of bandwidth γ affects the fairness-performance trade-off.
- The method assumes continuous attributes are normalized to [0,1], but real-world sensitive attributes may have complex distributions that could challenge the kernel smoothing approach.

## Confidence

- **High confidence**: The core theoretical framework connecting EIPM to conditional independence is well-grounded (Section 3.1, Proposition 1). The use of MMD as a tractable IPM is standard and well-established in the literature.
- **Medium confidence**: The empirical validation on tabular and graph datasets shows FREM outperforms baselines, but the paper does not explore edge cases (e.g., highly imbalanced S values, non-smooth attribute distributions).
- **Low confidence**: The convergence rate analysis (Theorem 4) assumes smooth joint densities, which may not hold in practice. The paper does not test the estimator's robustness to violations of this assumption.

## Next Checks

1. **Robustness to Attribute Distribution**: Test FREM on synthetic datasets where the sensitive attribute S has a multi-modal distribution. Compare performance against a baseline that discretizes S into bins.
2. **Kernel Sensitivity Analysis**: Systematically vary the bandwidth γ and kernel type (RBF vs. Box vs. Epanechnikov) on a held-out validation set. Plot the Pareto frontier of fairness vs. accuracy for each kernel configuration.
3. **Edge Case Stress Test**: Create a dataset where most S values are unique (e.g., S sampled from a uniform continuous distribution). Measure how FREM's EIPM estimator degrades compared to a baseline that uses k-NN instead of kernel smoothing.