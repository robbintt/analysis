---
ver: rpa2
title: Improving Adaptive Moment Optimization via Preconditioner Diagonalization
arxiv_id: '2502.07488'
source_url: https://arxiv.org/abs/2502.07488
tags:
- adam
- gradient
- matrix
- adadiag
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for enhancing adaptive moment
  optimization algorithms by improving the diagonal approximation of the preconditioner
  matrix. The key idea is to apply an invertible transformation to the gradient space,
  making the preconditioner approximately diagonal in the transformed space, and then
  project back to the original space for parameter updates.
---

# Improving Adaptive Moment Optimization via Preconditioner Diagonalization

## Quick Facts
- **arXiv ID:** 2502.07488
- **Source URL:** https://arxiv.org/abs/2502.07488
- **Reference count:** 40
- **Primary result:** AdaDiag achieves 1.8x-2x sample efficiency speedup over Adam on LLaMA models via improved preconditioner diagonalization

## Executive Summary
This paper introduces AdaDiag, an optimization algorithm that enhances adaptive moment optimizers by making the preconditioner matrix approximately diagonal through gradient space transformations. The method applies periodic singular value decomposition to extract projection matrices that diagonalize the gradient statistics, then performs updates in this transformed space before projecting back. Experiments demonstrate consistent improvements over Adam on image classification (ResNet50, ViT-B/32, ViT-S/16 on ImageNet1k) and language modeling (LLaMA models on C4 dataset), with 1.8x-2x speedup in sample efficiency for language tasks.

## Method Summary
AdaDiag improves Adam by applying an invertible transformation to make the preconditioner approximately diagonal. It computes full SVD of gradients every T steps to extract projection matrices P and Q, then projects gradients into a space where the preconditioner becomes approximately diagonal. Updates are performed in this transformed space using standard Adam moment estimation, then projected back to the original parameter space. The method supports both one-sided (P^T G) and two-sided (P^T G Q) projections, with periodic updates every 200-500 steps providing the best trade-off between adaptation accuracy and computational overhead.

## Key Results
- Achieves 1.8x-2x sample efficiency speedup over Adam on LLaMA 60M/130M/350M models for language modeling
- Consistently improves top-1 accuracy on ImageNet1k across ResNet50 and ViT architectures
- Demonstrates competitive performance when combined with memory-efficient optimizers like Adafactor and Hfac
- Shows optimal SVD update frequency at T=200-500 steps, balancing adaptation quality with computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Preconditioner Diagonalization via Gradient Projection
The core insight is that projecting gradients through orthogonal transformation makes the preconditioner matrix approximately diagonal, improving second-moment estimation accuracy. By computing SVD G_τ = P_τ Σ_τ Q_τ^⊤ and projecting gradients to Ĝ_τ = P_τ^⊤ G_τ Q_τ, the vec(Σ_τ)vec(Σ_τ)^⊤ becomes sparse, making C(Ĝ_τ) approximately diagonal. This means the diagonal approximation used in Adam becomes more faithful, leading to better preconditioning.

### Mechanism 2: Periodic Subspace Projection for Computational Efficiency
Full SVD at every iteration is computationally prohibitive, so periodic updates with fixed projection matrices across T steps provide similar benefits at manageable cost. The paper empirically shows T=200-500 balances computational overhead against adaptation quality, with frequent updates (T=2, 10) degrading performance due to excessive adaptation to minibatch noise.

### Mechanism 3: Network Reparameterization Interpretation
The gradient projection implicitly defines a network reparameterization W̃ = P_κ^⊤ W Q_κ that leaves the forward pass unchanged but changes gradient structure. Since P and Q are orthogonal, the transformation is invertible, guaranteeing that updates in projected space map back to optimize the original loss landscape.

## Foundational Learning

- **Preconditioned Gradient Descent:** Understanding why preconditioners accelerate convergence by scaling gradient directions by curvature inverse is prerequisite. Quick check: Can you explain why multiplying the gradient by C^-1/2 before the update helps navigate ill-conditioned loss surfaces?

- **SVD and Orthogonal Transformations:** The method relies on SVD decomposition G = PΣQ^⊤ and properties of orthogonal matrices (P^⊤P = I, determinant ±1, norm preservation). Quick check: Why does applying orthogonal transformations preserve gradient magnitude but change coordinate alignment?

- **Exponential Moving Average (EMA):** Adam-family optimizers accumulate first and second moments via EMA; AdaDiag operates on these same statistics but in projected space. Quick check: How do β1, β2 decay coefficients control the effective window size for gradient statistics?

## Architecture Onboarding

- **Component map:** Input Gradient G_t → Periodic SVD Module → Gradient Projector → EMA Accumulator → Preconditioner Application → Back-Projection → Output Parameter update ΔW_t

- **Critical path:** The SVD operation on large weight matrices is the computational bottleneck. Use torch.linalg.svd(..., full_matrices=True) and ensure sufficient GPU memory for storing P (m×m) and Q (n×n) for weight matrix W (m×n).

- **Design tradeoffs:** One-sided (AdaDiag) vs. two-sided (AdaDiag++) projection: two-sided provides better diagonalization but requires m² + n² extra memory. Update frequency T: lower T = more accurate subspaces but higher compute overhead. Integration with memory-efficient optimizers reduces memory but may slightly reduce convergence acceleration.

- **Failure signatures:** Training instability with small T (<100) due to excessive adaptation to minibatch noise. OOM on large layers due to full-rank SVD memory requirements. No improvement over Adam if SVD uses truncated matrices instead of full_matrices=True.

- **First 3 experiments:** 1) Sanity check on small MLP/CNN on CIFAR-10 comparing Adam vs. AdaDiag with T=500, verifying off-diagonal element histograms show sparsity. 2) Frequency sweep on LLaMA-60M testing T ∈ {50, 200, 500, 1000, 5000} and plotting validation perplexity vs. steps. 3) Memory-accuracy tradeoff comparing AdaDiag vs. AdaDiag++ vs. AdafacDiag on same task, measuring wall-clock time, peak memory, and final metric.

## Open Questions the Paper Calls Out

### Open Question 1
Does the accumulation of momentum estimates across dynamic subspaces maintain consistency and theoretical advantages during subspace transitions? The paper notes that the complex interactions between model parameters (W_t) and optimization states (S_t) caused by changing projection matrices make standard analysis difficult, and it's unclear whether momentum estimates accumulated across previous subspaces would be consistent with each other.

### Open Question 2
Can the reported 2x sample efficiency speedup be reliably replicated on multi-billion parameter models given the computational constraints of full-rank SVD? The paper was unable to conduct experiments with billion-parameter models due to resource constraints, and the O(min(m,n)² max(m,n)) complexity of full-rank SVD may become a wall-clock bottleneck at the 70B+ scale.

### Open Question 3
Can the optimization process be simplified by identifying a fixed, optimal subspace that eliminates the need for periodic SVD updates? The paper observes that fixed projection matrices also showed improved results, suggesting that if the optimal subspace is constant or predictable, the computational overhead of SVD could be removed entirely.

## Limitations
- Performance depends critically on the assumption that gradient subspaces evolve slowly enough for periodic SVD to remain valid
- Memory overhead from storing full projection matrices (O(m² + n²)) may be prohibitive for extremely large layers
- Optimal SVD update frequency (T=200-500) appears empirically determined without theoretical justification
- Theoretical convergence guarantees are mentioned but not rigorously established

## Confidence
- **High Confidence:** Core mechanism of preconditioner diagonalization through gradient projection is mathematically sound and well-illustrated
- **Medium Confidence:** Network reparameterization interpretation is internally consistent but lacks direct experimental validation
- **Low Confidence:** Theoretical convergence guarantees are mentioned but not rigorously established

## Next Checks
1. **Subspace Stability Analysis:** Track the Frobenius norm of changes in projection matrices P_t across SVD updates to quantify subspace drift and correlate with training loss to identify optimization instability patterns.

2. **Mixed-Precision Accuracy:** Evaluate AdaDiag's performance using mixed-precision training (FP16 weights, FP32 moments) to assess robustness to numerical precision trade-offs common in large-scale training.

3. **Cross-Domain Generalization:** Test AdaDiag on non-transformer architectures (e.g., LSTMs, MLPs) and non-vision/non-language tasks to verify the preconditioner diagonalization benefit extends beyond the reported domains.