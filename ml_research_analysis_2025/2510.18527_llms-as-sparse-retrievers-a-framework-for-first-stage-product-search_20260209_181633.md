---
ver: rpa2
title: LLMs as Sparse Retrievers:A Framework for First-Stage Product Search
arxiv_id: '2510.18527'
source_url: https://arxiv.org/abs/2510.18527
tags:
- retrieval
- product
- prosper
- sparse
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses vocabulary mismatch in product search by proposing
  PROSPER, a framework that leverages large language models (LLMs) as sparse retrievers.
  The core method introduces a literal residual network (LRN) to reinforce literal
  terms like brand names and model numbers, and a lexical focusing window (LFW) for
  coarse-to-fine sparsification during training.
---

# LLMs as Sparse Retrievers:A Framework for First-Stage Product Search

## Quick Facts
- arXiv ID: 2510.18527
- Source URL: https://arxiv.org/abs/2510.18527
- Authors: Hongru Song; Yu-an Liu; Ruqing Zhang; Jiafeng Guo; Maarten de Rijke; Sen Li; Wenjun Peng; Fuyu Lv; Xueqi Cheng
- Reference count: 40
- Primary result: PROSPER achieves 93.9% Hit@1000 and improves Taobao search GMV by 0.64%

## Executive Summary
PROSPER addresses vocabulary mismatch in product search by using LLMs as sparse retrievers with literal residual network (LRN) and lexical focusing window (LFW). The framework reinforces critical literal terms like brand names through compensatory weighting while enabling stable training in ultra-high-dimensional vocabularies via coarse-to-fine sparsification. Extensive experiments show PROSPER significantly outperforms sparse baselines and achieves recall comparable to dense retrievers, with successful online deployment in Taobao search.

## Method Summary
PROSPER extends the SPLADE framework for LLMs by introducing a Literal Residual Network that reinforces underweighted literal terms through compensatory weighting, and a Lexical Focusing Window that enables coarse-to-fine sparsification during training. The method uses last-token pooling for LLM causal attention, asymmetric query-document normalization, and InfoNCE loss with FLOPS regularization. Training employs conditional TopK masking with warmup, and inference uses Top-16 query terms with inverted index lookup.

## Key Results
- PROSPER achieves 93.9% Hit@1000 on Multi-CPR E-commerce dataset
- Improves Taobao search gross merchandise volume by 0.64%
- Outperforms SPLADE-v2 by 5.6pp Hit@1 on product search task
- LFW critical: removing it drops Hit@1000 from 93.9% to 87.2%

## Why This Works (Mechanism)

### Mechanism 1: Literal Residual Network for Hallucination Mitigation
- **Claim:** LRN reduces lexical expansion hallucination by reinforcing underweighted literal terms through compensatory weighting
- **Core assumption:** Product queries and titles are keyword aggregations where literal terms (brand, model) carry critical intent
- **Evidence anchors:** Table 1 shows hallucination examples before LRN; LRN removal drops performance by ~6.7pp Hit@1000
- **Break condition:** If literal terms are noisy rather than critical, LRN may overemphasize terms that should be filtered

### Mechanism 2: Lexical Focusing Window for Coarse-to-Fine Sparsification
- **Claim:** LFW stabilizes training in ultra-high-dimensional LLM vocabularies by enforcing rapid early-stage sparsification
- **Core assumption:** Soft FLOPS regularization alone cannot provide sufficient guidance in 150,000+ dimensional spaces
- **Evidence anchors:** Table 3 shows PROSPER without LFW drops from 93.9% to 87.2% Hit@1000
- **Break condition:** Window size k must balance sparsity and coverage; too small discards important terms

### Mechanism 3: Asymmetric Query-Document Normalization
- **Claim:** Applying ℓ2 normalization only to query-side representations preserves item-side absolute weights
- **Core assumption:** Product search exhibits inherent asymmetry—queries require precise intent expression, items require broad coverage
- **Evidence anchors:** Table 6 shows query-only ℓ2 norm achieves 25.3% Hit@1 vs 17.0% for no normalization
- **Break condition:** In symmetric retrieval scenarios, this asymmetric normalization may not apply

## Foundational Learning

- **Concept: Sparse vs. Dense Retrieval Trade-offs**
  - Why needed here: PROSPER chooses sparse retrieval for interpretability and storage efficiency despite dense retrieval's semantic power
  - Quick check question: Why does PROSPER achieve recall comparable to dense methods with better interpretability? (Sparse vectors map to vocabulary terms with explicit weights, enabling inverted index usage and human-readable explanations)

- **Concept: SPLADE Framework**
  - Why needed here: PROSPER builds on SPLADE's log-saturation transformation, pooling, and FLOPS regularization
  - Quick check question: What does w_j = max_i log(1 + ReLU(w_ij)) achieve? (Converts logits to sparse term weights with non-linear scaling that encourages sparsity while preserving importance ranking)

- **Concept: LLM Causal Attention Limitation**
  - Why needed here: PROSPER uses last-pooling instead of SPLADE's max-pooling due to LLM causal attention
  - Quick check question: Why last-pooling over max-pooling for LLMs? (Causal attention means each token attends only to previous tokens; the last token's representation aggregates full sequence information)

## Architecture Onboarding

- **Component map:** Tokenized sequences (max 64 tokens) -> Qwen2.5-3B backbone -> LM Head (vocabulary logits) -> LRN (log(1+ReLU) + last-pooling + FC layer) -> LFW (conditional TopK + FLOPS) -> Training (InfoNCE loss) -> Inference (Top-16 query terms -> inverted index -> Block-Max Maxscore)
- **Critical path:** Training: Query-item pairs → LLM → LRN → LFW-constrained loss; Offline inference: Products → LRN → sparse weights → inverted index; Online inference: Query → LRN → top-16 terms → index lookup → candidates
- **Design tradeoffs:** Model size 1.5B/3B/7B tested; minimal scaling benefit, chose 1.5B for cost; LFW window k_q=256, k_d=512 optimal; Echo embedding improves performance but doubles computation; Literal vs. expansion terms: literal for precision, expansion for recall
- **Failure signatures:** LRN removed: ~6.7pp Hit@1000 drop; LFW removed: Hit@1000 drops from 93.9% to 87.2%; Wrong normalization: Cosine similarity drops to 87.2% Hit@1000; Brand underweighting: Literal brands expanded as irrelevant terms
- **First 3 experiments:** 1) Run PROSPER_BERT vs. SPLADE-v2 on your product data to validate LRN transfer across backbones (paper: 23.6% vs 18.4% Hit@1); 2) Ablate LRN (PROSPER vs. w/o-LRN vs. LRN-add) to quantify compensatory weighting contribution (Figure 2); 3) Sweep LFW window sizes (64/32 through 1024/512) to find optimal sparsity-efficiency point for your latency budget (Table 3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Chain-of-Thought (CoT) reasoning effectively filter and refine noisy expansion terms generated by the model logits?
- Basis in paper: [explicit] The authors state, "In future work, we plan to explore incorporating CoT reasoning to filter and refine expansion terms" to address the limitation that logits inevitably introduce noise
- Why unresolved: The current Literal Residual Network (LRN) compensates for weights but does not explicitly reason about term relevance to remove hallucinations
- Evidence would resolve it: A comparative study measuring the reduction in irrelevant expansion terms and changes in retrieval precision when using a CoT-guided filter

### Open Question 2
- Question: How can learned sparse representations be effectively integrated into the ranking stages of product search pipelines?
- Basis in paper: [explicit] The authors plan to "investigate the application of learned sparse retrieval in ranking stages"
- Why unresolved: The paper currently validates PROSPER only for first-stage retrieval, leaving the utility of these representations for re-ranking undefined
- Evidence would resolve it: Experiments using PROSPER's sparse weights as features in a learning-to-rank model or cross-encoder, showing improvements in final ranking metrics like NDCG

### Open Question 3
- Question: Does the performance saturation observed when scaling from 1.5B to 7B parameters persist across different LLM architectures or domains?
- Basis in paper: [inferred] The authors report "no obvious scaling law" and "no significant performance gains" when increasing model size, but they do not analyze the cause or test larger scales
- Why unresolved: It is unclear if the saturation is an artifact of the specific Qwen architecture or the limited complexity of the retrieval task
- Evidence would resolve it: Evaluation of the framework on significantly larger models (e.g., 70B+) or alternative architectures (e.g., Llama) to determine if the performance plateau shifts

## Limitations

- LRN implementation specificity requires assumptions about literal term indicator vector construction
- Transferability to non-product domains uncertain due to core assumption about literal terms being critical for intent capture
- Vocabulary scale generalization unclear as performance on smaller vocabularies or different tokenizers not tested

## Confidence

- **High Confidence:** PROSPER's superior performance over sparse baselines; LFW's critical role in training stability; Asymmetric normalization effectiveness
- **Medium Confidence:** LRN's hallucination mitigation; Coarse-to-fine sparsification necessity; Product search asymmetry assumption

## Next Checks

1. **Cross-Domain LRN Transfer:** Test PROSPER with LRN disabled/enabled on non-product datasets (e.g., MS MARCO passage retrieval) to validate whether compensatory weighting for literal terms generalizes beyond e-commerce

2. **Vocabulary Size Sensitivity:** Sweep LFW window sizes and FLOPS regularization parameters across different vocabulary scales (10k, 50k, 150k) to determine if the coarse-to-fine approach remains effective at smaller scales

3. **Online A/B Test Duration:** Extend online evaluation beyond the 10-day test mentioned to assess whether the 0.64% GMV improvement is sustained over longer periods and across different shopping seasons