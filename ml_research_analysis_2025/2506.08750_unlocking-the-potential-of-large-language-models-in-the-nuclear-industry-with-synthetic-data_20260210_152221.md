---
ver: rpa2
title: Unlocking the Potential of Large Language Models in the Nuclear Industry with
  Synthetic Data
arxiv_id: '2506.08750'
source_url: https://arxiv.org/abs/2506.08750
tags:
- data
- synthetic
- nuclear
- text
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of leveraging unstructured text
  data from the nuclear industry for advanced Large Language Model (LLM) applications.
  The authors propose a synthetic data generation pipeline that transforms raw nuclear
  text into structured question-answer pairs.
---

# Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data

## Quick Facts
- arXiv ID: 2506.08750
- Source URL: https://arxiv.org/abs/2506.08750
- Reference count: 0
- Primary result: Synthetic data pipeline transforms nuclear text into high-quality QnA pairs with cosine similarity >0.80 and Shannon entropy 6.63

## Executive Summary
This paper presents a synthetic data generation pipeline to address data scarcity and privacy constraints in the nuclear industry. The method converts unstructured nuclear text into structured question-answer pairs using text chunking, semantic embedding generation with Azure OpenAI's text-embedding-ada-002, K-Means clustering for context preservation, and carefully engineered prompts. The approach enables scalable, high-quality dataset creation for LLM training and evaluation in nuclear applications. Results demonstrate strong document relevance (cosine similarity scores mostly above 0.80) and question diversity (Shannon entropy of 6.63), validated through t-SNE visualizations showing semantic separation between domain-relevant and unrelated questions.

## Method Summary
The pipeline transforms nuclear industry text into structured QnA pairs through a multi-stage process. First, unstructured text is chunked using Azure Document Intelligence at paragraph, section, and chapter boundaries. Key concepts are extracted from each chunk via LLM summarization using categorized prompts. Text chunks and questions are then embedded using Azure OpenAI's text-embedding-ada-002 (1536-dimensional vectors). K-Means clustering groups semantically similar chunks to preserve contextual coherence. Finally, QnA pairs are generated through prompt engineering that specifies extraction categories and cognitive levels, with JSON-structured output. Quality evaluation uses cosine similarity (threshold >0.80) and Shannon entropy to assess relevance and diversity.

## Key Results
- Document relevance scores show cosine similarity mostly above 0.80, indicating strong semantic alignment between questions and source material
- High question diversity achieved with Shannon entropy of 6.63, demonstrating effective coverage of cognitive levels and topics
- t-SNE visualizations confirm clear semantic separation between domain-relevant questions and unrelated benchmark questions

## Why This Works (Mechanism)

### Mechanism 1: Semantic Clustering Preserves Contextual Coherence
- Claim: Grouping semantically similar text chunks before QnA generation maintains contextual relationships that improve downstream question relevance and coherence.
- Mechanism: K-Means partitions 1536-dimensional embeddings into clusters where topically related chunks co-locate, preventing fragmented questions spanning disconnected source material.
- Core assumption: Semantic proximity in embedding space correlates with the topical coherence required for generating meaningful domain questions.
- Evidence anchors:
  - Explicitly cites "K-Means clustering for context preservation" as a core pipeline component
  - t-SNE plot shows effective grouping with distinct regions corresponding to different clusters
  - Related work confirms synthetic data diversity impacts fine-tuning outcomes
- Break condition: Clusters become too large (diluting specificity) or too small (insufficient context)

### Mechanism 2: Structured Prompt Engineering Guides Domain Extraction
- Claim: Explicitly categorized prompts with domain context yield higher-quality, more diverse QnA pairs than open-ended generation.
- Mechanism: Prompts specify extraction categories (technical concepts, system components, operational processes, safety protocols), reference source textbook identity, and request multiple cognitive levels.
- Core assumption: The LLM has sufficient pre-existing knowledge of nuclear terminology to accurately interpret and extract domain-specific content when given structured guidance.
- Evidence anchors:
  - Prompt explicitly specifies categories of information to extract
  - Design encourages broad range of question types covering multiple cognitive levels
  - Best practices emphasize factual accuracy and dataset diversity requirements
- Break condition: Prompt complexity exceeds LLM's instruction-following capacity or domain-specific terms are misinterpreted

### Mechanism 3: Embedding-Based Metrics Enable Scalable Quality Filtering
- Claim: Cosine similarity and Shannon entropy provide automated, scalable proxies for human evaluation of synthetic QnA quality.
- Mechanism: Questions are embedded and compared to source chunk embeddings via cosine similarity (>0.80 threshold flags for review). Shannon entropy quantifies lexical diversity across the question set.
- Core assumption: High embedding similarity to source text correlates with factual grounding; high entropy correlates with genuine question diversity useful for training.
- Evidence anchors:
  - Most questions exhibit high similarity to corresponding chunks, with low-similarity questions flagged for manual inspection
  - Calculated question entropy of 6.63 suggests good variation in questions
  - Embedding-based evaluation similarly used for domain-specific synthetic datasets
- Break condition: Low-similarity questions may still be valid if targeting narrow details within large chunks

## Foundational Learning

- Concept: **Text Embeddings as Semantic Representations**
  - Why needed here: The entire pipeline—clustering, retrieval, and quality evaluation—depends on representing text as vectors where semantic meaning maps to spatial proximity.
  - Quick check question: Why does cosine similarity outperform Euclidean distance for comparing text embeddings of varying lengths?

- Concept: **K-Means Clustering and Dimensionality Reduction Visualization**
  - Why needed here: The pipeline validates cluster quality via t-SNE; understanding this relationship is essential for interpreting visualizations and tuning k.
  - Quick check question: If t-SNE shows significant cluster overlap, what does this imply about the embedding model's ability to distinguish domain topics?

- Concept: **Shannon Entropy as a Diversity Metric**
  - Why needed here: The paper relies on entropy to claim question diversity; understanding what entropy measures is crucial for assessing whether 6.63 is meaningful.
  - Quick check question: If entropy dropped from 6.63 to 3.0 after a prompt change, what hypothesis would you form about the new prompt's effect?

## Architecture Onboarding

- Component map: Document Intelligence (Azure) -> Text Chunking -> Summarization LLM -> text-embedding-ada-002 -> K-Means Clustering -> QnA Generation LLM -> Evaluation Layer

- Critical path: Raw document -> Chunking -> Summarization -> Embedding -> Clustering -> Prompt construction (chunk + summary) -> QnA generation -> Quality filtering (cosine < 0.80 -> manual review queue)

- Design tradeoffs:
  - Chunk granularity: Larger chunks preserve context but dilute relevance signals; smaller chunks improve precision but may fragment coherent concepts
  - Cluster count (k): Requires domain intuition; under-clustering mixes disparate topics, over-clustering isolates related content
  - Human-in-the-loop placement: Post-hoc quality gate vs. earlier intervention (prompt iteration)

- Failure signatures:
  - Cosine similarity distribution centered below 0.75 -> Prompt generates tangential or overly abstract questions
  - Entropy significantly below 6.0 -> Repetitive phrasing patterns; prompt may over-constrain output format
  - t-SNE shows no clear cluster separation -> Embedding model may lack domain sensitivity
  - Hallucinated technical details in answers -> Add explicit citation requirements to prompt

- First 3 experiments:
  1. **Baseline calibration**: Run full pipeline on one textbook chapter with manually verified ground truth; establish cosine similarity distribution and entropy baseline; validate >90% of QnA pairs pass human spot-check
  2. **Cluster count sensitivity**: Vary k (e.g., 5, 10, 20 for single-chapter test); measure t-SNE visual separation and sample question coherence per cluster; select k where clusters are distinct but not sparse
  3. **Prompt diversity ablation**: Generate QnA sets with (a) single question-type prompts vs. (b) multi-type prompts; compare entropy scores and manually assess whether higher entropy correlates with broader cognitive coverage

## Open Questions the Paper Calls Out

- Question: Does combining synthetic datasets with limited high-quality real-world nuclear samples result in more robust training frameworks than synthetic-only approaches?
  - Basis in paper: The "Future work" section identifies "integration with real-world data" as a necessary step to improve model proficiency and adaptability.
  - Why unresolved: The current study relied exclusively on the "Essential CANDU" textbook, leaving the marginal benefit of mixing real and synthetic data unquantified.
  - What evidence would resolve it: Comparative performance benchmarks of models trained on mixed datasets versus synthetic-only datasets in operational scenarios.

- Question: To what extent does domain-specific fine-tuning with reinforcement learning improve LLM accuracy on safety-critical nuclear tasks?
  - Basis in paper: The authors explicitly state that future research should investigate coupling synthetic data with reinforcement learning to handle safety-critical tasks with greater reliability.
  - Why unresolved: The current paper focuses on the generation pipeline and automated evaluation metrics, stopping short of training or fine-tuning the models themselves.
  - What evidence would resolve it: Error rate reductions in safety protocol adherence or operational reasoning tasks following reinforcement learning fine-tuning.

- Question: How can human-in-the-loop validation be optimally integrated with automated metrics to ensure high data quality without incurring excessive resource costs?
  - Basis in paper: The "Future work" section calls for "hybrid evaluation strategies" that balance the resource intensity of human review with scalable automated metrics.
  - Why unresolved: The paper notes that while human evaluation is the gold standard, it is currently too resource-intensive for large-scale data, and the proposed automated metrics are only partially validated.
  - What evidence would resolve it: A study correlating automated cosine similarity scores with domain expert satisfaction ratings across large datasets.

## Limitations

- The approach depends heavily on the quality of the source embedding model and prompt engineering; inadequate domain sensitivity in text-embedding-ada-002 could group unrelated topics together
- Cosine similarity thresholds (>0.80) may not capture valid but narrow-domain questions, and Shannon entropy doesn't directly assess factual correctness
- Human-in-the-loop review is positioned post-generation rather than as iterative prompt refinement, potentially reducing efficiency

## Confidence

- **High**: Effectiveness of structured prompt engineering in guiding domain-relevant QnA extraction (supported by high cosine similarity scores and explicit categorization)
- **Medium**: Utility of cosine similarity and Shannon entropy as quality metrics (indirect relationship to factual accuracy acknowledged)
- **Medium**: Semantic clustering's ability to preserve contextual coherence (evidence based on visual t-SNE validation rather than quantitative metrics)

## Next Checks

1. Conduct a controlled study comparing QnA quality generated from clustered chunks versus non-clustered chunks to quantify the impact of semantic clustering on question coherence
2. Implement a pilot where a subset of QnA pairs is validated by nuclear domain experts for factual accuracy and relevance, independent of embedding similarity scores
3. Test the pipeline's robustness by applying it to a different nuclear domain document (e.g., safety protocols) and measuring changes in cosine similarity distribution and Shannon entropy to assess generalizability