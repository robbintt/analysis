---
ver: rpa2
title: 'WebNovelBench: Placing LLM Novelists on the Web Novel Distribution'
arxiv_id: '2505.14818'
source_url: https://arxiv.org/abs/2505.14818
tags:
- evaluation
- dataset
- novels
- benchmark
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WebNovelBench, a benchmark for evaluating
  long-form story generation by LLMs using 4,000+ Chinese web novels. It frames evaluation
  as a synopsis-to-story generation task and employs an LLM-as-Judge approach to assess
  eight narrative quality dimensions.
---

# WebNovelBench: Placing LLM Novelists on the Web Novel Distribution
## Quick Facts
- **arXiv ID:** 2505.14818
- **Source URL:** https://arxiv.org/abs/2505.14818
- **Reference count:** 32
- **Primary result:** Benchmark evaluates LLM story generation using 4,000+ Chinese web novels with 8 narrative dimensions, showing top models achieve near-human scores

## Executive Summary
This paper introduces WebNovelBench, a benchmark for evaluating long-form story generation by LLMs using 4,000+ Chinese web novels. It frames evaluation as a synopsis-to-story generation task and employs an LLM-as-Judge approach to assess eight narrative quality dimensions. Scores are aggregated via PCA and mapped to percentile ranks against human-authored works. Experiments with 24 state-of-the-art LLMs show effective differentiation between human masterpieces, popular web novels, and LLM-generated content, with top models like Qwen3-235B-A22B achieving near-perfect scores. The benchmark provides a scalable, automated, and data-driven methodology for assessing and advancing LLM-driven narrative generation.

## Method Summary
The benchmark constructs a Chinese web novel dataset (4,000+ works with >10k readers each, 2013-2020), extracts 10 synopsis-story pairs per novel using Doubao-pro-32k, and creates a reference distribution by scoring these works on eight narrative dimensions (1-5 scale) using DeepSeek-V3. For evaluation, target LLMs generate stories from sampled synopses (max 4096 tokens, temp 0.6), which are scored by the same judge. Scores are normalized, weighted by PCA first-component loadings (explaining 75.6% variance), and mapped to percentiles via ECDF against the reference distribution, yielding interpretable "better than X% of human writing" metrics.

## Key Results
- Eight narrative dimensions effectively differentiate human masterpieces, popular web novels, and LLM-generated content
- Top models (Qwen3-235B-A22B, GPT-4o) achieve near-perfect scores (99.9%), while others range 20-60%
- The benchmark provides scalable, automated evaluation that correlates with human literary judgment
- Generation quality correlates with model scale and architectural sophistication

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synopsis-to-story generation provides a constrained yet creative task for evaluating narrative capabilities.
- **Mechanism:** Given extracted synopses (main characters, plot points, scenes), LLMs must expand into full chapters. This tests both adherence to provided structure and creative elaboration, reducing evaluation variance from open-ended prompts.
- **Core assumption:** Synopses derived from human-authored works represent valid test cases that correlate with general storytelling ability.
- **Evidence anchors:**
  - [abstract] "framing evaluation as a synopsis-to-story generation task"
  - [Section 3.1] Details the 4,000 novel curation and synopsis extraction using Doubao-pro-32k
  - [corpus] Related work (NexusSum) confirms long-form narrative summarization remains challenging for LLMs
- **Break condition:** If synopses are too detailed (over-constraining) or too sparse (under-specified), the task may not differentiate model capabilities meaningfully.

### Mechanism 2
- **Claim:** Eight-dimensional LLM-as-Judge scoring captures narrative quality better than single-metric evaluation.
- **Mechanism:** DeepSeek-V3 evaluates outputs on literary devices, sensory detail, character balance, dialogue distinctiveness, character consistency, atmospheric alignment, contextual appropriateness, and scene coherence (1-5 scale each). Direct scoring avoids position bias inherent in pairwise comparisons.
- **Core assumption:** The judge model's scoring correlates with human literary judgment.
- **Evidence anchors:**
  - [Section 3.2/3.3] Table 1 lists all eight dimensions with PCA-derived weights
  - [Section 5.2] Mao Dun Literature Prize novels scored in the high range, validating alignment
  - [Section 5.3] Eleven repeated evaluations showed IQR <0.05, variance ~0.001
  - [corpus] Limited direct corpus validation; related work (Ismayilzada et al.) used only 4 stories per LLM
- **Break condition:** If judge model has systematic bias toward certain writing styles or its own outputs, rankings become circular.

### Mechanism 3
- **Claim:** PCA aggregation with ECDF percentile ranking enables interpretable comparison against human baselines.
- **Mechanism:** Eight dimension scores are z-normalized, then weighted by PCA first-component loadings (explaining 75.6% variance). Aggregated scores map to percentiles via ECDF over 4,000 human-authored works, yielding intuitive "better than X% of human writing" metric.
- **Core assumption:** The first principal component represents a meaningful "quality" axis rather than an artifact.
- **Evidence anchors:**
  - [Section 3.3] Equations 1-4 formalize normalization, PCA weighting, and ECDF percentile computation
  - [Section 5.1] First component explains 75.6% variance; weights range 11.5%-13.8%
  - [corpus] No direct corpus evidence on PCA validity for narrative quality; this is a methodological assumption
- **Break condition:** If dimensions are highly correlated (multicollinearity) or the quality construct is multidimensional, PCA first component may conflate distinct quality aspects.

## Foundational Learning

- **Concept: LLM-as-Judge Paradigm**
  - **Why needed here:** Core evaluation mechanism; understanding its biases and reliability is critical for interpreting results.
  - **Quick check question:** Can you explain why direct scoring reduces position bias compared to pairwise comparison?

- **Concept: Principal Component Analysis (PCA)**
  - **Why needed here:** Aggregates eight correlated dimensions into a single score; understanding loadings informs which aspects matter most.
  - **Quick check question:** If the first PC explains only 40% of variance, would using it as a single quality metric still be justified?

- **Concept: Empirical CDF and Percentile Ranking**
  - **Why needed here:** Translates raw scores to interpretable human-comparable metrics.
  - **Quick check question:** Given ECDF formula in Equation 3, what percentile would a score equal to the median of the reference distribution receive?

## Architecture Onboarding

- **Component map:** Data Pipeline (Crawl → Deduplicate → Parse chapters → Filter authors → Extract synopses) → Distribution Builder (Score 4,000 novels → PCA weights → ECDF mapping) → Evaluator (Sample 100 novels → Generate stories → Score → Map to distribution) → Validator (Classic literature as anchor)

- **Critical path:** Data curation quality → Synopsis extraction fidelity → Judge prompt design → PCA weight stability. Errors propagate; poor synopses yield uninformative generation tasks.

- **Design tradeoffs:**
  - 100-novel subset vs. full 4,000: Resource efficiency vs. statistical robustness
  - Single judge (DeepSeek-V3) vs. ensemble: Consistency vs. bias mitigation
  - 4096-token cap: Standardization vs. constraining long-form capability
  - Chinese-only: Deep domain coverage vs. language generalizability

- **Failure signatures:**
  - All LLMs cluster near same percentile → dimensions may lack discriminative power or judge is unreliable
  - Generated stories consistently outscore classics → distribution skewed or judge biased toward LLM style
  - High variance across repeated runs → temperature or prompt instability

- **First 3 experiments:**
  1. **Reproduce baseline**: Run Qwen3-235B-A22B and GPT-4o on 10-novel subset; verify ranking order matches paper
  2. **Judge ablation**: Compare DeepSeek-V3 vs. Claude-3.7-Sonnet as judge on same 20 outputs; measure correlation
  3. **Length control**: Test whether normalizing scores by output length changes rankings (address Section 5.4 concern)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the benchmark exhibit ceiling effects that limit its ability to differentiate among top-tier models?
- **Basis in paper:** [inferred] Top models like Qwen3-235B-A22B achieve near-perfect scores (99.9%), and the paper notes "for more fine-grained evaluation, future work may involve collecting higher-quality reference texts or designing more nuanced evaluation dimensions."
- **Why unresolved:** The current evaluation may saturate at the high end, making it difficult to distinguish incremental improvements in state-of-the-art models.
- **What evidence would resolve it:** Correlation analysis between benchmark scores and independent human evaluations specifically for top-performing models; introduction of harder test cases that stratify current ceiling-scoring models.

### Open Question 2
- **Question:** How robust are the rankings when using different LLM judges beyond DeepSeek-V3?
- **Basis in paper:** [explicit] "evaluating additional subsets with multiple judge models in future studies would further strengthen and validate our conclusions."
- **Why unresolved:** Using only DeepSeek-V3 as judge may introduce systematic bias; different judges may have varying sensitivities to the eight narrative dimensions.
- **What evidence would resolve it:** Cross-judge agreement analysis using multiple evaluator models (e.g., GPT-4o, Claude) on the same test set, with inter-rater reliability metrics.

### Open Question 3
- **Question:** Does output length systematically bias LLM-as-Judge scores?
- **Basis in paper:** [inferred] Section 5.4 notes notable length outliers (Claude 3.7 Sonnet ~2,700 words, Gemini 2.5 Pro ~2,000 words) and states "a scoring regularization term based on output length may be introduced to enhance robustness."
- **Why unresolved:** Longer outputs may receive higher scores regardless of actual narrative quality, confounding the evaluation.
- **What evidence would resolve it:** Controlled experiments varying output length while holding narrative content constant; correlation analysis between output length and scores across models.

## Limitations
- Chinese-language focus limits generalizability to other linguistic and cultural contexts
- Single judge model (DeepSeek-V3) raises concerns about systematic bias and circular evaluation
- 4096-token generation limit constrains assessment of truly long-form capabilities

## Confidence
- **High Confidence**: Benchmark construction methodology is clearly specified and reproducible; experimental results showing differentiation between human masterpieces, popular web novels, and LLM-generated content are methodologically sound
- **Medium Confidence**: Eight narrative dimensions capture comprehensive storytelling quality is reasonable but not extensively validated against human expert judgment
- **Low Confidence**: Generalizability to non-Chinese contexts and different narrative styles remains untested; potential bias in LLM-as-Judge evaluation is acknowledged but not empirically measured

## Next Checks
1. **Judge Bias Analysis**: Run the same 20 evaluation samples through multiple judge models (Claude-3.7-Sonnet, GPT-4o) and measure inter-judge correlation and systematic bias patterns
2. **Cross-Lingual Transfer**: Apply the benchmark methodology to English web novels (e.g., from Royal Road or Wattpad) and compare whether the same eight dimensions and PCA weights produce meaningful rankings
3. **Expert Validation**: Have human literary experts evaluate a subset of generated stories and judge-scored human works to measure correlation with the LLM-as-Judge scores and validate the narrative quality construct