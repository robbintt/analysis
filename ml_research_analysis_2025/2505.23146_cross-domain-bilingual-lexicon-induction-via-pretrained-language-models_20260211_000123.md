---
ver: rpa2
title: Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models
arxiv_id: '2505.23146'
source_url: https://arxiv.org/abs/2505.23146
tags:
- word
- embeddings
- domain
- language
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new task of Bilingual Lexicon Induction (BLI)
  that extracts domain-specific bilingual dictionaries using monolingual corpora from
  both general and target domains. The core idea is to leverage pre-trained language
  models and introduce a Code Switch method that replaces words with their translations
  during model fine-tuning, enhanced by context-based word replacement strategies.
---

# Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models

## Quick Facts
- **arXiv ID**: 2505.23146
- **Source URL**: https://arxiv.org/abs/2505.23146
- **Reference count**: 8
- **Primary result**: Code-switched fine-tuning with domain vocabulary targeting improves BLI accuracy by 0.78 percentage points on average

## Executive Summary
This paper addresses the challenge of extracting domain-specific bilingual dictionaries using only monolingual corpora from general and target domains. The authors propose a novel approach that combines static and contextual word representations through code-switched fine-tuning of pre-trained language models. By strategically replacing words with their translations during training and using frequency-based domain vocabulary detection, the method effectively improves cross-lingual word embedding quality for specialized domains like Medicine, Law, and Financial. Experimental results demonstrate consistent improvements over robust baselines, particularly for low-frequency domain-specific vocabulary.

## Method Summary
The approach involves training static embeddings (FastText, 300-dim) and contextual embeddings (XLM, 1024-dim) separately, then combining them through similarity interpolation. Code Switch randomly replaces source-language words with target-language translations during XLM fine-tuning using an external bilingual dictionary. A word replacement strategy classifies vocabulary as domain vs general using relative frequency ratios, applying full replacement for high-density domain sentences. Contextual representations are extracted via average anchor from 10 sentences per word. Both embedding types are aligned to cross-lingual space using Vecmap, with static embeddings further refined by a spring network with adversarial training. Final translation scores combine static and mapped contextual similarities with a tuned weight λ.

## Key Results
- Average BLI accuracy improvement of 0.78 percentage points across three domains
- Code Switch alone improves average score from 27.63 to 27.84 (+0.21 points)
- Full method with domain strategy reaches 27.84 average, up from baseline 27.08 (+0.76)
- Layer 1 of XLM encoder outperforms Layer 0 (0 vs non-zero accuracy on Medicine)
- Frequency-based domain vocabulary targeting enhances code-switching effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Code-Switched Fine-Tuning for Cross-Lingual Context Injection
Randomly replacing source-language words with target-language translations during PLM fine-tuning creates implicit cross-lingual associations that improve bilingual lexicon extraction. During fine-tuning on domain corpora, the Code Switch method uses an external bilingual dictionary to substitute selected words with their translations. This produces mixed-language training data where contextual representations naturally encode cross-lingual signals, helping corresponding word pairs cluster closer in the resulting embedding space. The pre-trained model can learn to associate words across languages when they appear in similar contextual environments during fine-tuning.

### Mechanism 2: Frequency-Ratio Domain Vocabulary Detection
Distinguishing domain-specific from general vocabulary using relative frequency ratios enables more targeted code-switching, improving coverage of specialized terms. A word x is classified as "domain vocabulary" when FDx/D > FGx/G (relative frequency in domain corpus exceeds that in general corpus). For sentences where domain vocabulary ratio exceeds threshold γ, ALL replaceable words are switched rather than random sampling—ensuring high-density domain sentences receive full cross-lingual exposure. Words with elevated relative frequency in domain corpora carry domain-specific semantics that benefit disproportionately from cross-lingual context during fine-tuning.

### Mechanism 3: Static-Contextual Similarity Interpolation
Combining similarity scores from refined static embeddings and mapped contextual representations produces better translation rankings than either representation type alone. Final similarity S = cos(ux, uy) + λ·cos(a′x, a′y) where u represents unified static embeddings (refined by spring network) and a′ represents aligned contextual representations. Weight λ is tuned via unsupervised bidirectional alignment validation. Static embeddings capture stable cross-lingual semantic structure while contextual representations capture domain-specific usage; their combination is complementary rather than redundant.

## Foundational Learning

- **Static vs. Contextual Word Embeddings**
  - Why needed here: The architecture combines both representation types. Static embeddings (FastText, Word2Vec) map each word type to ONE vector regardless of context; contextual embeddings (XLM, BERT) produce different vectors for the same word type in different contexts.
  - Quick check question: For the word "cold" appearing in "cold temperature" vs. "cold virus," would static or contextual embeddings produce different representations?

- **Bilingual Lexicon Induction (BLI)**
  - Why needed here: This is the core task—automatically extracting translation pairs by aligning separately-trained monolingual embedding spaces, typically without parallel corpora.
  - Quick check question: Given English word vectors and Chinese word vectors trained on separate monolingual data, how would you find the Chinese translation of "hospital" without any parallel sentences?

- **Cross-Lingual Space Alignment via Linear Mapping**
  - Why needed here: The paper uses Vecmap to transform monolingual spaces into a shared cross-lingual space through learned matrices (WX, WY). Alignment quality directly determines BLI accuracy.
  - Quick check question: If two languages have isometric embedding spaces (same geometric structure), what mathematical property should the optimal mapping matrix satisfy?

## Architecture Onboarding

- **Component map**: General corpus + Domain corpora → Code-switching augmentation → Training data → Static Branch (FastText → Vecmap → Spring network) → Contextual Branch (Code-switched XLM → Average anchor → Vecmap) → Similarity interpolation → Translation ranking

- **Critical path**: 1) Collect/preprocess general + domain monolingual corpora; 2) Obtain seed bilingual dictionary for code-switching; 3) Apply Code Switch + word replacement strategy to create training data; 4) Fine-tune XLM on augmented data; 5) Extract contextual representations via average anchor; 6) Train static embeddings via FastText + Vecmap + spring network; 7) Align both representation types to cross-lingual space; 8) Tune interpolation weight λ on validation set; 9) Evaluate using combined similarity scoring

- **Design tradeoffs**:
  - Sentence replacement ratio (α): Paper tests 0.4–1.0. Higher = more cross-lingual exposure but risk of unnatural sequences
  - Token replacement ratio (β): Controls density of switches within selected sentences
  - Domain threshold (γ): Paper tests 0.1–1.0. Lower = more sentences classified as domain-heavy → aggressive full replacement
  - Context sample size: Paper uses ≤10 sentences for average anchor. More samples improve quality but increase compute
  - XLM layer: Layer 1 outperforms Layer 0, suggesting encoder processing matters

- **Failure signatures**:
  - Zero accuracy: Unsupervised Muse on medical en-zh scores 0—indicates initialization failure or poor embedding quality
  - Supervised < unsupervised: Observed in Vecmap on medicine—suggests seed dictionary contains out-of-domain vocabulary that misleads alignment
  - Negligible improvement (< 0.5 points): If code switch gains are tiny, check dictionary coverage for domain terms and whether fine-tuning actually modified XLM weights
  - Layer 0 > Layer 1: Paper doesn't observe this, but if seen, suggests fine-tuning degraded contextual quality

- **First 3 experiments**:
  1. **Reproduce CSCBLI baseline**: Run static + contextual combination WITHOUT code switching on one domain (Medicine). Target: ~29.85 en-zh accuracy. This validates your pipeline before modifications.
  2. **Ablate Code Switch only**: Add basic Code Switch (random replacement) without domain vocabulary strategy. Target: ~30.45 en-zh on Medicine (+0.6 points). Validates core mechanism.
  3. **Full system with domain strategy**: Add word replacement strategy targeting domain-heavy sentences. Target: ~30.68 en-zh on Medicine. Then test generalization to Law and Financial domains.

## Open Questions the Paper Calls Out

### Open Question 1
How can supervised BLI methods be modified to prevent performance degradation caused by seed dictionaries containing general-domain vocabulary instead of domain-specific terminology? The Discussion section highlights that supervised methods often underperform unsupervised ones on domain datasets, explicitly stating this is because "the seed dictionary is given words that belong to a non-domain or general domain." The authors identify this "negative impact" and the need for "improvement in terms of domain vocabulary extraction," but explicitly state they "did not investigate this phenomenon in depth" as it was not the focus of the work.

### Open Question 2
Under what specific conditions do static word representations from Pre-trained Language Models (PLMs) perform comparably to contextual representations in domain-specific BLI? Section 5.2 notes that for Law and Financial datasets, the static word representations from XLM Layer 0 are "not much worse" than the contextual Layer 1 representations, suggesting the simpler static approach might suffice in certain professional domains. The paper validates the efficacy of Layer 0 but does not analyze why it works well for Law/Finance but results in zero accuracy for Medicine, leaving the domain-specific criteria for layer selection undefined.

### Open Question 3
Can the Code Switch strategy provide significant gains when applied to the training of static word embeddings, or is it primarily effective only for fine-tuning Pre-trained Language Models? Section 5.3 briefly describes testing Code Switch during the training of static embeddings (FastText) before running Vecmap, but the analysis is truncated and lacks the depth of the PLM experiments. The paper introduces this variation but does not clearly quantify its effectiveness relative to the primary proposed method, leaving the utility of Code Switch for static embedding training ambiguous.

## Limitations

- The method relies heavily on domain-specific corpora size and quality, using only 2,000 training pairs and 800 test pairs per domain
- Code-switching effectiveness depends on an external bilingual dictionary whose coverage, quality, and domain appropriateness are not specified
- Spring network refinement and adversarial training components add complexity without thorough ablation studies showing their individual contributions

## Confidence

- **High Confidence**: The core observation that combining static and contextual embeddings improves BLI performance (average 0.78 percentage point improvement over baselines) is well-supported by experimental results across three domains
- **Medium Confidence**: The specific mechanisms of code-switched fine-tuning and frequency-based domain vocabulary targeting are supported by results, but the magnitude of improvements (0.21-0.76 points) suggests these components provide incremental rather than transformative gains
- **Low Confidence**: The effectiveness of the spring network refinement and adversarial training components is not independently validated, making it unclear whether these complex additions provide meaningful benefit beyond simpler baseline combinations

## Next Checks

1. **Dictionary Coverage Analysis**: Systematically evaluate how BLI performance scales with external bilingual dictionary size and domain coverage. Test scenarios where the dictionary contains only general vocabulary versus domain-specific terms, and measure the resulting impact on code-switching effectiveness.

2. **Cross-Lingual Transfer Robustness**: Validate whether improvements transfer across different language pairs beyond English-Chinese. Test the method on high-resource (e.g., English-French) and low-resource (e.g., English-Finnish) pairs to assess generalizability of the code-switching and frequency-based targeting approaches.

3. **Component Ablation with Statistical Significance**: Conduct formal statistical tests (e.g., paired t-tests) on ablation studies that isolate the contributions of code-switching, frequency-based targeting, and spring network refinement. This would quantify whether observed improvements are statistically significant or could be attributed to random variation.