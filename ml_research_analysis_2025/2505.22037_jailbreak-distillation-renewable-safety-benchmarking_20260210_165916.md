---
ver: rpa2
title: 'Jailbreak Distillation: Renewable Safety Benchmarking'
arxiv_id: '2505.22037'
source_url: https://arxiv.org/abs/2505.22037
tags:
- benchmark
- prompts
- safety
- attack
- istill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose Jailbreak Distillation (JBDistill), a framework that
  distills jailbreak attacks into safety benchmarks by using development models and
  prompt selection algorithms to identify effective evaluation prompts. JBDistill
  addresses key challenges in safety evaluation including fair comparisons, reproducibility,
  and benchmark saturation.
---

# Jailbreak Distillation: Renewable Safety Benchmarking

## Quick Facts
- **arXiv ID:** 2505.22037
- **Source URL:** https://arxiv.org/abs/2505.22037
- **Reference count:** 40
- **Key outcome:** JBDistill framework achieves up to 81.8% effectiveness and generalizes to 13 diverse held-out models, significantly outperforming existing benchmarks while maintaining high separability and diversity.

## Executive Summary
Jailbreak Distillation (JBDistill) is a framework that constructs renewable safety benchmarks by distilling jailbreak attacks using development models and prompt selection algorithms. The approach addresses key challenges in LLM safety evaluation including fair comparisons, reproducibility, and benchmark saturation. By generating attacks on multiple development models and selecting prompts that transfer effectively across them, JBDistill creates benchmarks that generalize to held-out models while maintaining high attack success rates and model separability. The framework amortizes computational costs by decoupling attack generation from evaluation, providing an effective, sustainable, and adaptable solution for safety benchmarking.

## Method Summary
JBDistill generates adversarial prompts by running multiple transformation functions (TAP, PAP, AutoDAN-Turbo, Adversarial Reasoning) on four development models using attacker models Mixtral-8x7B and GPT-4o. The framework then applies selection algorithms (Rank By Success, Best Per Goal, or Combined Selection) to filter the candidate pool down to a fixed size (e.g., 500 prompts). These selected prompts form the benchmark, which is evaluated on held-out models using a Llama-3.3-70B-Instruct judge with the AdvPrefix method. The approach relies on the hypothesis that attacks effective across multiple development models will generalize well to diverse evaluation models.

## Key Results
- **Effectiveness:** JBDistill benchmarks achieve up to 81.8% attack success rate on held-out models, significantly outperforming existing benchmarks
- **Generalization:** The framework successfully transfers to 13 diverse held-out models, validating the cross-model transferability assumption
- **Trade-offs:** Different selection algorithms provide controllable trade-offs between effectiveness (81.8% vs 73.3%) and separability (71.1% vs 84.4%), allowing users to prioritize finding flaws or distinguishing model safety levels

## Why This Works (Mechanism)

### Mechanism 1
Aggregating attack effectiveness across multiple development models selects prompts that transfer better to unseen evaluation models than attacks generated on a single model. The framework over-generates attacks using diverse algorithms and filters this pool using the Rank By Success algorithm, selecting prompts that jailbreak the highest number of development models. This filters out model-specific noise and retains attacks exploiting shared vulnerabilities.

### Mechanism 2
The framework amortizes computational cost and ensures reproducibility by decoupling the search (attack generation) from evaluation (benchmark execution). Instead of running expensive search algorithms individually on every new model, JBDistill runs them once during construction to build a static benchmark, allowing for consistent evaluation across models.

### Mechanism 3
Prompt selection algorithms control the trade-off between benchmark effectiveness (finding any flaw) and separability (distinguishing which model is safer). Algorithms like Rank By Success maximize total Attack Success Rate, potentially making all models look unsafe, while Best Per Goal enforces diversity by selecting the best prompt for each seed goal, spreading the difficulty and better separating model performance.

## Foundational Learning

**Concept: Adversarial Transferability**
*Why needed here:* The entire framework relies on the premise that an attack generated on Model A will likely succeed on Model B. *Quick check:* If adversarial transferability did not exist, would the "distillation" of a benchmark on development models be useful for held-out models? (Answer: No)

**Concept: Benchmark Separability**
*Why needed here:* A safety benchmark is useless if it gives every model a 100% failure rate or 0% failure rate. You must understand how confidence intervals determine if one model is statistically significantly safer than another. *Quick check:* If Benchmark A gives Model X 90% ASR and Model Y 92% ASR, can you confidently claim Model Y is less safe? What metric determines this? (Answer: Separability, specifically via confidence interval overlap)

**Concept: Dataset Distillation**
*Why needed here:* The paper draws an analogy to dataset distillation (compressing knowledge into a smaller set). Understanding this helps explain why we select a subset of prompts rather than using the raw pool. *Quick check:* Why not just use the entire "candidate prompt pool" generated by the attacks? (Answer: It would be computationally prohibitive to evaluate and might contain low-quality or redundant noise)

## Architecture Onboarding

**Component map:** Seed Goals -> Attack Generation -> Candidate Pool -> Selection Algorithm -> Fixed Benchmark -> Evaluation

**Critical path:** The Selection Algorithm is where the "magic" happens. If this component simply selects randomly, the benchmark fails to generalize effectively. The logic must successfully identify which attacks are robust transfer vectors.

**Design tradeoffs:**
- **Renewability vs. Stability:** Rerunning the pipeline creates a new benchmark, but historical comparisons become invalid
- **Effectiveness vs. Separability:** Rank By Success creates a "harder" benchmark (higher ASR) but worse ranking capability; Best Per Goal creates a "fairer" benchmark but might miss some extreme vulnerabilities

**Failure signatures:**
- **Low Coverage:** The benchmark only tests 10 specific harm types because the selection algorithm was too greedy for effectiveness
- **Overfitting:** The benchmark is highly effective on development models but fails entirely on held-out models
- **Saturation:** All evaluation models score 95%+ ASR, rendering the benchmark useless for comparing model safety

**First 3 experiments:**
1. **Sanity Check (Transferability):** Generate attacks on a single small model. Test on a similar model vs. a very different model to confirm transferability
2. **Algorithm Ablation:** Compare "Random Selection" vs. "Rank By Success" vs. "Best Per Goal" on a held-out validation set
3. **Separability Test:** Run the generated benchmark on a pair of models known to have different safety levels and check if 95% confidence intervals overlap

## Open Questions the Paper Calls Out

**Open Question 1:** Can the JBDistill framework be effectively extended to multimodal safety benchmarking? The current implementation relies on text-based jailbreaks and LLM-based judges, leaving multimodal interactions for future work.

**Open Question 2:** Does developing customized transformation functions specifically for JBDistill yield more effective benchmarks than off-the-shelf attacks? The current study uses generic, off-the-shelf attacks as transformation functions.

**Open Question 3:** Can JBDistill be adapted to evaluate the trade-off between safety and overrefusal? The current framework optimizes exclusively for safety violations, ignoring the model's utility and tendency to refuse benign requests.

## Limitations

- **Cross-model generalizability** is limited to the specific model families used in development, with uncertainty about effectiveness for extremely different architectures
- **Judge subjectivity** introduces a single point of evaluation subjectivity that could affect benchmark validity and cross-study comparisons
- **Benchmark staleness risk** exists as static benchmarks may become saturated over time as models evolve and new vulnerabilities emerge

## Confidence

**Effectiveness claims:** High - Well-supported by experimental results across multiple evaluation models and attack types
**Separability improvements:** Medium-High - Demonstrated empirically, though practical significance could be more thoroughly explored
**Renewable benchmarking framework:** Medium - Design is sound but long-term evidence of renewability advantage is limited

## Next Checks

1. **Architecture stress test:** Apply JBDistill to a development set containing highly diverse model architectures (including reasoning models, specialized safety models, and different pretraining paradigms)
2. **Judge robustness analysis:** Evaluate the same benchmark using multiple independent judging methods to quantify the impact of judge subjectivity on effectiveness scores
3. **Longitudinal stability study:** Generate JBDistill benchmarks at multiple time points as new model versions are released, tracking how effectiveness and separability metrics evolve over time