---
ver: rpa2
title: A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical
  Decision Support Systems
arxiv_id: '2502.09849'
source_url: https://arxiv.org/abs/2502.09849
tags:
- cdss
- clinical
- learning
- evaluation
- clinicians
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic PRISMA-guided survey reviews 31 human-centered
  evaluations (HCE) of XAI methods in Clinical Decision Support Systems (CDSS), categorizing
  them by methodology, evaluation design, and adoption barriers. Most studies use
  post-hoc, model-agnostic approaches like SHAP and Grad-CAM, typically evaluated
  through small clinician studies (median n=16).
---

# A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems

## Quick Facts
- arXiv ID: 2502.09849
- Source URL: https://arxiv.org/abs/2502.09849
- Reference count: 25
- This systematic PRISMA-guided survey reviews 31 human-centered evaluations (HCE) of XAI methods in Clinical Decision Support Systems (CDSS), categorizing them by methodology, evaluation design, and adoption barriers.

## Executive Summary
This survey systematically reviews 31 human-centered evaluations of XAI methods in CDSS, revealing that post-hoc, model-agnostic approaches like SHAP and Grad-CAM are most prevalent. While explanations generally improve clinician trust and diagnostic confidence, they often increase cognitive load and misalign with clinical reasoning processes. A stakeholder-centric evaluation framework is proposed to bridge the socio-technical gap, emphasizing iterative development, diverse stakeholder engagement, and alignment with clinical workflows. The review identifies significant barriers including small sample sizes (median n=16), misalignment between developer and clinician interpretability goals, and lack of patient perspectives in evaluation protocols.

## Method Summary
The authors conducted a systematic review following PRISMA guidelines across ACM Digital Library, Web of Science, and PubMed databases, identifying 3,126 initial records (including 12 from author knowledge). After deduplication and screening, 73 full-text articles were eligible, with 31 studies meeting inclusion criteria of top-tier venues (CORE A*/A or SJR Q1), post-2017 publication, and explicit clarity on XAI and HCE methodologies in medical fields. Papers were classified by XAI method type (intrinsic vs post-hoc, model-specific vs agnostic, local vs global), HCE method (Think-Aloud, Interview, Survey, Focus Group), sample size, and clinician perception categories.

## Key Results
- Post-hoc feature attribution methods (SHAP, Grad-CAM, LIME) are used in 80%+ of CDSS, with SHAP appearing in 12 studies
- Explanations improve clinician trust and diagnostic confidence but increase cognitive load and misalign with clinical reasoning
- Median sample size is 16 clinicians (only 9/31 studies met threshold of n≥30)
- No studies reported negative perceptions; positive perceptions relate to improved risk assessments, neutral views reflect preference for established practice and role disparities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc feature attribution methods (SHAP, LIME) improve clinician trust by revealing which input variables drive individual predictions.
- Mechanism: These methods compute contribution scores per feature after model training, presenting them as visualizations alongside predictions. Clinicians verify whether highlighted features align with their clinical knowledge, reducing perceived opacity.
- Core assumption: Clinicians can accurately interpret feature importance scores and map them to pathophysiological reasoning without extensive training.
- Evidence anchors: [abstract] "explanations generally improve clinician trust and diagnostic confidence"; [Section 3.2] "Twelve studies employed SHAP... This functionality, particularly in time-sensitive domains such as critical care, appeared valuable in augmenting clinicians' confidence"; [corpus] Weak direct validation; "Explainable AI for Clinical Outcome Prediction" surveys clinician preferences but doesn't confirm trust improvement causally.
- Break condition: When cognitive load exceeds interpretive benefit, or when feature attributions conflict with established clinical heuristics without resolution path.

### Mechanism 2
- Claim: The socio-technical gap between rigid algorithmic outputs and flexible clinical reasoning limits CDSS effectiveness.
- Mechanism: Clinical decisions integrate patient context, experience, and nuance that computational systems cannot fully capture. When XAI outputs present fixed feature rankings without contextual adaptation, clinicians experience misalignment and increased cognitive burden.
- Core assumption: Clinical expertise involves judgment patterns that resist formalization into discrete feature contributions.
- Evidence anchors: [abstract] "explanations... often increase cognitive load and exhibit misalignment with domain reasoning processes"; [Section 4.2] Citing Ackerman (2000), "human activity is 'flexible and nuanced,' while computational mechanisms are 'rigid and brittle'"; [corpus] "Before the Clinic" addresses bridging XAI theory with clinician expectations, supporting the gap concept.
- Break condition: If XAI explanations are designed to be context-adaptive, role-specific, or provide actionable recommendations rather than pure attributions.

### Mechanism 3
- Claim: Iterative, stakeholder-centric development with early clinician involvement produces more clinically viable XAI-CDSS.
- Mechanism: A four-phase protocol—(1) stakeholder identification, (2) goal refinement, (3) prototyping, (4) evaluation—creates feedback loops that surface misalignments before deployment. Early engagement reduces "frictions between stakeholders" (Theme 4).
- Core assumption: Stakeholder requirements can be elicited, prioritized, and translated into design specifications without infinite iteration.
- Evidence anchors: [abstract] "stakeholder-centric evaluation framework... emphasizing iterative development, diverse stakeholder engagement"; [Section 4.3] Details the four-phase protocol; notes only 12/31 studies gathered early clinician feedback; [corpus] "On the Design and Evaluation of Human-centered Explainable AI Systems" supports human-centered evaluation frameworks broadly.
- Break condition: If stakeholder goals are fundamentally irreconcilable (e.g., developer interpretability metrics vs. clinician actionability needs) or iteration cycles exceed clinical workflow change tolerance.

## Foundational Learning

- Concept: **Post-hoc vs. Intrinsic Interpretability**
  - Why needed here: The survey finds 80%+ of CDSS use post-hoc methods (SHAP, Grad-CAM), which apply after model training rather than being built into architecture. This affects explanation stability and computational cost.
  - Quick check question: Would a logistic regression coefficient be considered intrinsic or post-hoc interpretability?

- Concept: **Local vs. Global Explanations**
  - Why needed here: Clinical CDSS predominantly use local explanations (individual patient-level) via SHAP/LIME, but global model behavior may be needed for regulatory trust. The survey notes all 31 studies used at least one local method.
  - Quick check question: If a CDSS shows which features contributed to one patient's sepsis risk score, is that local or global?

- Concept: **Application-Grounded Evaluation**
  - Why needed here: The paper emphasizes that medical XAI must be evaluated with real clinicians on real tasks, not proxy metrics. Only application-grounded HCE provides sufficient fidelity for high-stakes domains.
  - Quick check question: Why would a proxy evaluation (e.g., simulated user study with non-experts) be insufficient for validating a CDSS used in ICU decision-making?

## Architecture Onboarding

- Component map:
  - Backend ML Layer: Models (XGBoost, Random Forest, CNNs) trained on structured EHR data or unstructured imaging/text
  - XAI Explanation Layer: Post-hoc generators (SHAP, LIME, Grad-CAM, Integrated Gradients) producing feature attributions or saliency maps
  - Frontend Presentation Layer: UI displaying predictions, risk scores, and visual explanations; must support multiple explanation formats
  - Evaluation Infrastructure: HCE methods (surveys, think-aloud protocols, interviews) integrated into development cycle

- Critical path:
  1. Identify stakeholders (clinicians, developers, administrators, regulators) and elicit requirements
  2. Define clinical task, prediction target, and evaluation criteria collaboratively
  3. Build prototype with XAI integrated—not bolted on post-hoc as afterthought
  4. Conduct application-grounded HCE with n≥30 clinicians (median was 16; only 9/31 met threshold)
  5. Iterate based on trust, cognitive load, and actionability metrics

- Design tradeoffs:
  - Model complexity vs. interpretability: Deeper models (CNNs, LSTMs) require post-hoc XAI that may increase cognitive load
  - Sample size vs. feasibility: Larger clinician samples improve statistical power but are costly; most studies were underpowered
  - Explanation detail vs. actionability: Detailed SHAP plots may be informative but less actionable than counterfactual recommendations

- Failure signatures:
  - Explanations increase cognitive load without improving diagnostic accuracy (Theme 2: "Preference for Established Clinical Practice")
  - Clinician-developer misalignment on what constitutes "interpretable" vs. "clinically plausible" (Theme 4)
  - Senior clinicians struggle more than juniors (Theme 3: role/seniority disparities)
  - Explanations are "not always actionable"—provide feature importance but no clear next step (Sub-theme 2.1)

- First 3 experiments:
  1. Baseline trust comparison: Randomize clinicians to prediction-only vs. prediction+SHAP conditions; measure trust via validated scales and decision concordance with AI (target: n=30+ clinicians, same clinical domain)
  2. Cognitive load think-aloud: Have clinicians verbalize reasoning while using XAI-CDSS on 5-10 cases; code for confusion points, explanation helpfulness, and workflow disruption
  3. Stakeholder alignment workshop: Run parallel focus groups with 3-5 clinicians and 3-5 developers; present same XAI outputs and document where interpretability goals diverge; use to inform design specs before prototyping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can human-centered evaluation protocols be effectively designed to incorporate patient perspectives, ensuring explanations are communicable to non-experts?
- Basis in paper: [explicit] Section 4.3 explicitly identifies a research gap: "future work may design evaluation protocols that may incorporate patient perspectives, ensuring that explanations are not only useful by clinicians but also easily communicable to patients."
- Why unresolved: The review finds that current evaluations focus almost exclusively on clinicians and developers, neglecting the patient as an active stakeholder in the decision-making loop.
- What evidence would resolve it: New HCE studies that successfully validate XAI interfaces with patient groups, measuring metrics such as understanding and communicability.

### Open Question 2
- Question: What is the required sample size for human-centered evaluations (HCE) to achieve statistical significance and generalizability in clinical settings?
- Basis in paper: [inferred] Section 3 and Section 4.1 highlight a methodological limitation where clinician sample sizes remain small (median n=16) and "question the statistical significance validity of the HCE evaluations."
- Why unresolved: The reviewed studies exhibit heterogeneity in scale, with the majority utilizing cohorts too small to support robust statistical claims or widespread generalization.
- What evidence would resolve it: Large-scale, multi-center studies with power analyses defining minimum participant thresholds for reliable effect detection.

### Open Question 3
- Question: Do counterfactual or directive explanation formats provide higher clinical utility and actionability compared to standard feature-importance methods like SHAP?
- Basis in paper: [inferred] Section 3.2 notes that while SHAP is widely used, clinicians often found it "less actionable" than counterfactual explanations, which were valued for "actionable insights."
- Why unresolved: There is a misalignment (Theme 2) where the most popular XAI methods (SHAP/LIME) do not always provide the actionable reasoning clinicians need for established practice.
- What evidence would resolve it: Comparative studies directly contrasting feature-attribution tools against counterfactual/directive tools on metrics of clinical decision speed and appropriateness.

## Limitations
- The survey's inclusion criteria may have excluded relevant grey literature and industry case studies that could reveal additional adoption barriers.
- The median sample size of 16 clinicians is below the recommended threshold for generalizable findings, and no negative perceptions were reported—potentially indicating publication bias.
- The analysis relies heavily on categorical coding of perceptions without quantitative effect sizes, limiting conclusions about relative impact across different XAI methods or clinical specialties.

## Confidence

**High confidence**: Post-hoc feature attribution methods are predominant in CDSS, explanations improve trust but increase cognitive load, and stakeholder misalignment exists between developers and clinicians.

**Medium confidence**: The four-phase stakeholder-centric evaluation framework is necessary but not necessarily sufficient; the reported perception patterns are descriptive rather than causal.

**Low confidence**: Specific claims about which XAI methods work best for particular clinical tasks, and whether proposed framework phases will resolve identified gaps in practice.

## Next Checks

1. Replicate the systematic search with current database access to verify PRISMA screening results and inclusion consistency.
2. Conduct a grey literature search (clinical trial registries, conference proceedings, industry white papers) to identify XAI-CDSS evaluations with negative or mixed perceptions that may have been excluded.
3. Design a multi-site clinician study (n≥50) comparing SHAP, LIME, and counterfactual explanations on identical clinical tasks, measuring both trust and cognitive load to quantify trade-offs beyond binary positive/neutral/negative perceptions.