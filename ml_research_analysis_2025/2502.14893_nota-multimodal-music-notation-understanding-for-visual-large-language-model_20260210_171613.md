---
ver: rpa2
title: 'NOTA: Multimodal Music Notation Understanding for Visual Large Language Model'
arxiv_id: '2502.14893'
source_url: https://arxiv.org/abs/2502.14893
tags:
- music
- information
- dataset
- extraction
- notation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NOTA, the first large-scale multimodal dataset
  for music notation understanding, comprising 1,019,237 records from three global
  regions across three tasks: cross-modal alignment, music information extraction,
  and music notation analysis. The authors developed NotaGPT-7B, a music notation
  visual large language model trained using a three-phase pipeline involving cross-modal
  alignment, foundational music information extraction, and music notation analysis.'
---

# NOTA: Multimodal Music Notation Understanding for Visual Large Language Model

## Quick Facts
- arXiv ID: 2502.14893
- Source URL: https://arxiv.org/abs/2502.14893
- Reference count: 25
- Primary result: NotaGPT-7B achieves 67.84% music information extraction rate, outperforming GPT-4V (33.33%) and Gemini-pro-vision (33.34%)

## Executive Summary
This paper introduces NOTA, the first large-scale multimodal dataset for music notation understanding, comprising 1,019,237 records from three global regions across three tasks: cross-modal alignment, music information extraction, and music notation analysis. The authors developed NotaGPT-7B, a music notation visual large language model trained using a three-phase pipeline involving cross-modal alignment, foundational music information extraction, and music notation analysis. Experimental results show that NotaGPT-7B achieves a 67.84% music information extraction rate, significantly outperforming 17 other MLLMs, including GPT-4V (33.33%) and Gemini-pro-vision (33.34%). The dataset and training pipeline effectively address the limitations of existing models in understanding visual music scores.

## Method Summary
The authors created NotaGPT-7B by adapting the LLaVA architecture, connecting a CLIP vision encoder to a Mistral-7B language model via a 2-layer MLP connector. The model was trained using a three-phase pipeline: first pre-aligning visual music scores with their ABC notation representations using connector-only training; then extracting musical metadata (title, key, meter, composer, etc.) from scores; and finally performing music notation analysis. Training was conducted on 8×80GB A100 GPUs with progressive freezing strategies: freezing all but the connector in phase 1, freezing only the vision encoder in phase 2, and full fine-tuning in phase 3.

## Key Results
- NotaGPT-7B achieves 67.84% music information extraction rate, significantly outperforming baselines
- Lowest Levenshtein distance (59.47) for ABC notation transcription accuracy
- Outperforms 17 other MLLMs including GPT-4V and Gemini-pro-vision on music notation tasks

## Why This Works (Mechanism)

### Mechanism 1: Progressive Cross-Modal Alignment via Connector-Only Pre-Training
- Claim: Freezing both the vision encoder and language model while training only the vision-language connector enables efficient alignment between visual music notation and textual ABC representations.
- Mechanism: A two-layer MLP projects CLIP vision embeddings into the language model's embedding space. By freezing pretrained components and training only this connector on 28,125 cross-modal alignment samples (highlighted score regions → ABC notation), the model learns to map visual note symbols to their textual counterparts without disrupting general visual or linguistic capabilities.
- Core assumption: The pretrained CLIP encoder already extracts meaningful features from music score images, and Mistral-7B already understands ABC notation when presented textually—the gap is purely in the projection between modalities.
- Evidence anchors: [abstract] "we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation" [section 4] "We have frozen the visual encoder and the language model components, focusing solely on training the two-layer MLP vision-language connector"
- Break condition: If CLIP's pretrained features fail to capture musically-relevant visual structure (e.g., staff lines, note heads, clefs), the connector cannot bridge modalities regardless of training. Evidence of this: consistently low alignment performance on out-of-distribution score styles not represented in pretraining.

### Mechanism 2: Curriculum-Based Skill Progression
- Claim: Ordering training in three phases—cross-modal alignment → foundational information extraction → music notation analysis—produces better final performance than joint training.
- Mechanism: Each phase builds on the previous. Phase 1 (alignment) establishes symbol-to-text correspondence. Phase 2 (extraction: 161K samples per subtask) trains the model to identify structural elements (key, meter, note length, composer, title, full ABC). Phase 3 (analysis: 700 samples) develops higher-order reasoning about style and structure. The full model (unfrozen except vision encoder) is fine-tuned in phases 2-3.
- Core assumption: Music notation understanding decomposes into hierarchically dependent skills: you cannot analyze style without first extracting the notes, and you cannot extract notes without recognizing symbol-text correspondence.
- Evidence anchors: [abstract] "Specifically, we involve a pre-alignment training phase... Subsequent training phases focus on foundational music information extraction, followed by training on music notation analysis" [section 4] Describes the three-stage training with different freezing strategies per stage
- Break condition: If later tasks don't actually depend on earlier skills (e.g., if style analysis can be learned from metadata without note extraction), the curriculum provides no benefit. Ablation showing joint training matches or exceeds phased training would refute this.

### Mechanism 3: ABC Notation as Structured Intermediate Representation
- Claim: Using ABC notation—a compact, text-based music format—as the target representation bridges the gap between 2D visual scores and 1D language model processing.
- Mechanism: ABC notation encodes both metadata (headers: T=title, K=key, M=meter, L=unit length, C=composer) and note sequences in plaintext. The model learns to output this structured format, which is both human-readable and machine-parseable. The Levenshtein distance metric (NotaGPT: 59.47 vs. GPT-4V: 655.45) captures character-level transcription accuracy.
- Core assumption: ABC notation is sufficiently expressive to represent the musical information present in score images, and language models can learn to generate valid ABC syntax from visual input.
- Evidence anchors: [section 3] "We choose to use ABC notation to represent music scores. ABC notation encodes music into two parts: header and body" [section 5.2] Levenshtein distance used for cross-modal alignment evaluation; NotaGPT achieves 59.47 vs. next best 308.27
- Break condition: If ABC notation cannot capture essential visual score information (e.g., precise positioning, performance markings, multi-staff layouts), performance gains are limited to ABC-representable content only.

## Foundational Learning

- **Concept: CLIP Vision-Language Pretraining**
  - Why needed here: NotaGPT builds on CLIP's vision encoder. Understanding that CLIP learns aligned image-text embeddings through contrastive learning on 400M image-text pairs explains why it can extract features from music scores despite never seeing music notation during pretraining.
  - Quick check question: Can you explain why CLIP features might transfer to music scores despite no music-specific pretraining data?

- **Concept: LLaVA-style Connector Architecture**
  - Why needed here: NotaGPT uses the same architecture as LLaVA—a linear projection (here, 2-layer MLP) connecting a frozen vision encoder to a frozen/unfrozen LLM. This is the standard approach for vision-language models.
  - Quick check question: Why might a simple linear projection be sufficient to connect pretrained vision and language models?

- **Concept: ABC Music Notation Format**
  - Why needed here: All training targets use ABC notation. Understanding its structure (X:reference, T:title, K:key, M:meter, L=length, then note body) is necessary to interpret model outputs and evaluation metrics.
  - Quick check question: Given an ABC excerpt `K:D Major\nM:4/4\n|:A2 Bc|`, can you identify the key, meter, and first two notes?

## Architecture Onboarding

- **Component map:** Music Score Image → CLIP Vision Encoder - ViT-L/14, frozen → 2-Layer MLP Connector - trainable in all phases → Mistral-7B LLM - frozen in phase 1, trained in phases 2-3 → ABC Notation Output

- **Critical path:**
  1. Phase 1 (Cross-modal Alignment): ~28K samples, 10 epochs, lr=2e-4, connector-only training
  2. Phase 2 (Information Extraction): ~970K samples across 6 subtasks, 3 epochs, lr=2e-5, full model training (vision encoder frozen)
  3. Phase 3 (Music Analysis): ~700 samples, supervised fine-tuning, full model training

- **Design tradeoffs:**
  - **7B vs. larger models**: Paper shows 7B parameter limit constrains LSA semantic similarity scores (NotaGPT ranks mid-pack), but excels at extraction accuracy. Consider larger base model if semantic analysis quality is critical.
  - **ABC vs. MusicXML vs. MIDI**: ABC is chosen for compactness and text-friendliness, but may lose information compared to MusicXML. Evaluate whether your task requires information ABC cannot encode.
  - **Freezing vision encoder**: Reduces compute and preserves general visual capabilities, but may limit adaptation to domain-specific visual patterns in scores.

- **Failure signatures:**
  - Title/author confusion: Paper notes NotaGPT sometimes extracts author as title (Section 6.1). This suggests imperfect header-field discrimination.
  - Region bias: Table 5 shows performance varies significantly across China/Europe/America regions (GPT-4V: 16.19/12.31/11.27), suggesting training data geographic distribution affects generalization.
  - Low LSA with high extraction: NotaGPT outperforms on extraction but not semantic similarity, indicating analysis capability may require larger models or more analysis training data.

- **First 3 experiments:**
  1. **Ablation: Phase order**: Train with phases 2→1→3 or jointly to validate curriculum hypothesis. Compare extraction rates and Levenshtein distances.
  2. **Connector architecture test**: Compare 1-layer vs. 2-layer MLP vs. transformer connector on cross-modal alignment (Levenshtein distance).
  3. **Region generalization**: Evaluate on held-out regional data (e.g., train on Europe/China, test on America) to quantify geographic bias and data diversity requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the fidelity and dependability of music notation MLLMs be improved to effectively mitigate the issues of hallucination and shallow reasoning?
- Basis in paper: [explicit] The "Limitations" section explicitly states the authors are aware of "typical limitations in MLLMs, including hallucinations and shallow reasoning," and identify future efforts in improving "fidelity and dependability."
- Why unresolved: The current NotaGPT-7B model inherits general MLLM weaknesses, and the paper does not propose a specific architectural or data-driven solution to hallucinations beyond the standard training pipeline.
- What evidence would resolve it: A comparative study showing a reduction in factuality errors or "hallucinated" musical elements in NotaGPT's output compared to baselines, potentially using a specialized fact-checking metric for music theory.

### Open Question 2
- Question: Can the specific confusion between "Composer" and "Title" fields be resolved through targeted data augmentation or attention mechanism adjustments?
- Basis in paper: [explicit] Section 6.1 states that NotaGPT-7B "does not perform very well on the title extraction task" specifically because it "mistakenly extracts author information as title information."
- Why unresolved: The authors identify the error mode (confusing metadata fields) but do not offer a solution for this semantic disambiguation within the current model architecture.
- What evidence would resolve it: Ablation studies demonstrating improved distinctiveness between Title and Author extraction scores after implementing specific semantic separation techniques.

### Open Question 3
- Question: Does balancing the regional distribution of training data eliminate the geographic bias observed in the zero-shot evaluations of existing MLLMs?
- Basis in paper: [inferred] Appendix A.2 analyzes a "Region Bias Test," noting that models show distinct preferences and performance drops across China, Europe, and America.
- Why unresolved: While the authors identify regional bias in existing models, they do not explicitly demonstrate that the NOTA training set resolves this bias in NotaGPT, nor do they analyze NotaGPT's performance on this specific subset in the appendix.
- What evidence would resolve it: A detailed performance breakdown of NotaGPT-7B on the Region Bias Test subset, showing consistent extraction rates across the three labeled regions.

## Limitations

- **Title/author confusion**: The model sometimes extracts author names as titles, indicating imperfect metadata field discrimination
- **Geographic performance variation**: Significant performance differences across China/Europe/America regions suggest potential geographic bias in training data
- **Limited semantic understanding**: While excelling at extraction, the model shows weaker performance on music analysis and semantic similarity tasks compared to its extraction capabilities

## Confidence

- **High Confidence**: The experimental results showing NotaGPT's superior extraction rates (67.84% vs. 33.33% for GPT-4V) and lower Levenshtein distances are well-supported by the reported metrics and methodology.
- **Medium Confidence**: The progressive training curriculum and connector-only pre-training mechanism are theoretically sound and show effectiveness, but lack comparative ablation studies against alternatives.
- **Low Confidence**: The semantic understanding capabilities and generalization across geographic regions remain questionable given the performance gaps observed in LSA scores and regional variations.

## Next Checks

1. **Ablation Study**: Compare the three-phase training pipeline against joint training on all tasks to validate whether the curriculum approach provides measurable benefits beyond simpler training strategies.
2. **Connector Architecture Test**: Evaluate whether the 2-layer MLP connector provides advantages over simpler 1-layer projections or transformer-based connectors on the cross-modal alignment task.
3. **Regional Generalization Test**: Train on two geographic regions (e.g., Europe/China) and test on the held-out third region (America) to quantify and address geographic data distribution effects on model performance.