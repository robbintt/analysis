---
ver: rpa2
title: 'Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource
  Canonical Morpheme Segmentation'
arxiv_id: '2505.16800'
source_url: https://arxiv.org/abs/2505.16800
tags:
- data
- segmentation
- learning
- multitask
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of low-resource canonical morpheme
  segmentation, a critical task for linguistic documentation of morphologically complex
  languages. The authors propose a multitask learning framework that jointly predicts
  morphological segments and glosses from orthographic input, leveraging shared linguistic
  representations to enhance model generalization.
---

# Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation

## Quick Facts
- arXiv ID: 2505.16800
- Source URL: https://arxiv.org/abs/2505.16800
- Reference count: 9
- Key outcome: Multitask learning with synthetic data improves low-resource canonical morpheme segmentation, achieving 81.35% F1-score vs 77.71% baseline

## Executive Summary
This paper addresses the challenge of low-resource canonical morpheme segmentation by proposing a multitask learning framework that jointly predicts morphological segments and glosses from orthographic input. The approach leverages shared linguistic representations to enhance model generalization and integrates synthetic training data generated by large language models using in-context learning. Experimental results on the SIGMORPHON 2023 dataset demonstrate significant improvements in word-level segmentation accuracy and morpheme-level F1-score across multiple low-resource languages, with the multitask model achieving an average F1-score of 81.35% compared to 77.71% for the single-task baseline.

## Method Summary
The proposed method combines multitask learning with synthetic data augmentation for low-resource canonical morpheme segmentation. The multitask framework jointly predicts morphological segments and glosses, sharing representations between tasks to improve generalization. Synthetic data is generated using large language models with in-context learning, where the LLM generates morphological annotations based on available stems. The approach processes orthographic input through a shared encoder, with task-specific decoders for segmentation and gloss prediction. The synthetic data generation addresses the scarcity of annotated morphological data in low-resource languages.

## Key Results
- Multitask model achieves average F1-score of 81.35% compared to 77.71% for single-task baseline
- Synthetic data augmentation significantly improves performance across multiple low-resource languages
- The approach demonstrates particular effectiveness for morphologically complex languages in the SIGMORPHON 2023 dataset

## Why This Works (Mechanism)
The multitask learning framework works by sharing linguistic representations between morphological segmentation and gloss prediction tasks. This shared representation forces the model to learn more general morphological patterns rather than task-specific features. The synthetic data augmentation expands the training distribution, exposing the model to a wider variety of morphological constructions and reducing overfitting to limited training data. The LLM-based synthetic data generation leverages the model's understanding of morphological patterns to create plausible examples that complement the scarce human-annotated data.

## Foundational Learning
- **Canonical morpheme segmentation**: Morpheme segmentation without considering surface forms - why needed: Provides a clean evaluation of morphological understanding without orthographic variation; quick check: Verify segmentation aligns with linguistic morpheme boundaries
- **Multitask learning**: Jointly optimizing multiple related tasks - why needed: Leverages shared linguistic representations to improve generalization; quick check: Compare shared vs separate encoders
- **In-context learning**: LLMs generating outputs conditioned on prompt examples - why needed: Enables synthetic data generation without fine-tuning; quick check: Test with varying prompt example counts
- **Morphological glossing**: Providing semantic labels for morphemes - why needed: Adds semantic constraints to improve segmentation; quick check: Verify gloss accuracy correlates with segmentation quality

## Architecture Onboarding
Component map: Input text -> Shared encoder -> Segmentation decoder + Gloss decoder -> Output
Critical path: Input → Shared encoder → Task-specific decoders → Loss computation
Design tradeoffs: Multitask sharing vs. task specialization; synthetic data quality vs. quantity
Failure signatures: Over-reliance on surface patterns; poor generalization to unseen morphological combinations
First experiments:
1. Baseline single-task model performance on held-out validation set
2. Ablation study removing synthetic data to measure its contribution
3. Error analysis on OOV morphological combinations to assess generalization

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can multilingual training frameworks enhance cross-linguistic generalization for low-resource canonical segmentation?
- Basis in paper: [explicit] The authors explicitly state that future research should "investigate multilingual training frameworks to improve cross-linguistic generalization."
- Why unresolved: The current study evaluates languages individually using a shared model architecture but does not leverage shared parameters or transfer learning between languages.
- What evidence would resolve it: Experiments utilizing a shared encoder across multiple low-resource languages, measuring performance improvements (F1-score) resulting from cross-lingual parameter sharing.

### Open Question 2
- Question: How can active learning strategies be integrated with LLM-based synthetic data generation?
- Basis in paper: [explicit] The conclusion identifies the need to "explore active learning strategies" to refine data augmentation techniques.
- Why unresolved: The current approach generates synthetic data based on available stems, but lacks a mechanism to iteratively identify and target specific morphological gaps or uncertainties in the model.
- What evidence would resolve it: A comparative study where an active learning loop selects specific prompts for synthetic data generation, measuring efficiency gains in data annotation or model convergence speed.

### Open Question 3
- Question: Do performance gains from synthetic data stem from genuine morphological generalization or merely increased exposure to frequent character sequences?
- Basis in paper: [inferred] The authors note in the Limitations section that "it is difficult to determine whether the improvements stem from genuinely better morphological generalization or simply from increased exposure to frequent patterns."
- Why unresolved: The quantitative evaluation confirms higher accuracy but does not perform a linguistic analysis of the errors to distinguish structural understanding from pattern matching.
- What evidence would resolve it: A qualitative error analysis focused on "unseen" morphological combinations (OOV) to verify if the model applies rules or simply replicates surface statistics.

## Limitations
- The synthetic data generation approach's quality and linguistic validity across diverse language families remains unclear
- The multitask formulation's underlying mechanisms and alternative task combinations require further investigation
- Performance gains' source (genuine morphological generalization vs. pattern exposure) cannot be definitively determined from current evaluation

## Confidence
- **High confidence**: The empirical methodology and evaluation framework are sound, with clear baselines and statistically significant improvements over single-task models
- **Medium confidence**: The synthetic data augmentation strategy is effective for the tested languages, though broader validation is needed
- **Medium confidence**: The multitask learning framework improves performance, but the underlying mechanisms require further investigation

## Next Checks
1. Test the synthetic data generation approach on morphologically diverse language families beyond the SIGMORPHON 2023 dataset, including polysynthetic languages
2. Conduct ablation studies to isolate which components of the multitask framework contribute most to performance gains
3. Evaluate model robustness to noise and domain shift by testing on held-out synthetic data versus human-annotated data from the same languages