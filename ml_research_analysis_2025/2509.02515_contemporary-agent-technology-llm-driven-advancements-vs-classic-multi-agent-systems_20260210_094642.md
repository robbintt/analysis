---
ver: rpa2
title: 'Contemporary Agent Technology: LLM-Driven Advancements vs Classic Multi-Agent
  Systems'
arxiv_id: '2509.02515'
source_url: https://arxiv.org/abs/2509.02515
tags:
- agents
- agent
- language
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive reflection on contemporary
  agent technology, focusing on LLM-driven advancements versus classic Multi-Agent
  Systems (MAS). It examines architectural models of LLM-agents, comparing them with
  foundational MAS concepts like the BDI model, Agents & Artifacts (A&A) meta-model,
  and Speech Act Theory.
---

# Contemporary Agent Technology: LLM-Driven Advancements vs Classic Multi-Agent Systems

## Quick Facts
- arXiv ID: 2509.02515
- Source URL: https://arxiv.org/abs/2509.02515
- Reference count: 40
- Classic MAS principles remain relevant despite LLM advancements, with open challenges in standardization and scalability

## Executive Summary
This paper provides a comprehensive reflection on contemporary agent technology, examining the intersection of Large Language Model (LLM)-driven agents and classic Multi-Agent Systems (MAS). The authors systematically analyze how modern LLM-agents compare to traditional MAS frameworks like BDI, Agents & Artifacts, and Speech Act Theory. While LLM-agents offer unprecedented natural language capabilities and flexibility, they often lack the formal rigor and predictability of established MAS approaches. The study identifies critical challenges including the lack of standardization, hallucinations, non-determinism, and scalability issues in natural language communication. Future research directions emphasize the need to build upon established MAS principles while leveraging LLM capabilities to create robust, reliable, and scalable multi-agent systems.

## Method Summary
The paper employs a comprehensive literature review and conceptual analysis approach, examining contemporary LLM-driven agent technologies alongside classic MAS frameworks. The authors systematically compare architectural models, communication protocols, and development environments, drawing on established MAS concepts like BDI models, Agents & Artifacts meta-models, and Speech Act Theory. Through this comparative analysis, they identify gaps between modern LLM approaches and traditional MAS principles, highlighting areas where LLM capabilities either complement or conflict with established agent system design patterns.

## Key Results
- LLM-agents demonstrate superior natural language processing but lack the formal predictability of classic MAS frameworks
- Natural language communication scalability remains an open challenge for large-scale MAS deployments
- Development environments for LLM-agents lack standardization from a software engineering perspective

## Why This Works (Mechanism)
The effectiveness of LLM-driven agents stems from their ability to process and generate natural language, enabling more intuitive human-agent interaction and flexible communication between agents. However, this flexibility comes at the cost of the formal guarantees provided by classic MAS approaches. Traditional MAS frameworks like BDI and A&A offer structured reasoning and environmental interaction models that ensure predictable agent behavior. The paper reveals that while LLM-agents can approximate these capabilities through natural language processing, they often fail to provide the same level of reliability and formal verification that MAS developers have relied upon for decades.

## Foundational Learning
1. BDI Architecture (Belief-Desire-Intention)
   - Why needed: Provides formal model for rational agent decision-making
   - Quick check: Can the agent explain its reasoning chain from beliefs to intentions?

2. Speech Act Theory
   - Why needed: Formalizes communication acts between agents
   - Quick check: Are message types (assert, request, promise) explicitly defined?

3. Agents & Artifacts (A&A) Meta-model
   - Why needed: Standardizes agent-environment interaction patterns
   - Quick check: Do artifacts maintain persistent state across agent interactions?

4. FIPA-ACL Protocol
   - Why needed: Ensures interoperable agent communication
   - Quick check: Can agents from different frameworks exchange messages reliably?

5. Utility Function Optimization
   - Why needed: Provides mathematical basis for rational decision-making
   - Quick check: Does agent behavior converge to optimal strategies over time?

## Architecture Onboarding

Component Map:
LLM Agent Core -> Natural Language Processor -> Tool/Function Executor -> Environment Interface

Critical Path:
User Request -> LLM Reasoning -> Tool Selection -> API Call -> Result Processing -> Response Generation

Design Tradeoffs:
- Flexibility vs. Predictability: LLM natural language capabilities sacrifice formal guarantees
- Expressiveness vs. Efficiency: Natural language communication is more expressive but less efficient than formal protocols
- Adaptability vs. Standardization: LLM agents adapt easily but lack standardized interfaces

Failure Signatures:
- Hallucinations in tool execution results
- Non-deterministic responses to identical inputs
- Communication overhead in large-scale deployments
- Inconsistent environmental state management

First Experiments:
1. Deploy a simple LLM agent with tool calling to perform basic arithmetic operations
2. Compare response consistency across 100 identical queries to measure non-determinism
3. Measure message processing time for natural language vs. formal protocol communication

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-driven agent behaviors compare to established game-theoretic approaches in strategic interactions?
- Basis in paper: Section 4.2 "Future Works" explicitly calls for a "deeper comparative analysis of LLM-driven agent behaviors against established game-theoretic approaches."
- Why unresolved: LLMs currently rely on probabilistic reasoning rather than the formal utility functions and rational choice models used in classic game theory.
- What evidence would resolve it: Studies demonstrating LLM-agents approximating Nash equilibria or optimizing utility functions in complex MAS settings without explicit programming.

### Open Question 2
- Question: Is natural language efficient and scalable enough for communication in large-scale Multi-Agent Systems?
- Basis in paper: Section 3.4 states, "The question of efficiency of natural language based communication... remains open," specifically regarding systems with thousands of agents.
- Why unresolved: Natural language introduces message size overhead and processing latency compared to formal, concise languages like FIPA-ACL.
- What evidence would resolve it: Performance benchmarks showing message processing speeds and system stability in large-scale deployments (e.g., 1,000+ agents) using natural language versus formal protocols.

### Open Question 3
- Question: Can the rigorous "Agents & Artifacts" (A&A) meta-model be effectively synthesized with flexible LLM tool usage?
- Basis in paper: Section 3.3 notes that while some frameworks borrow A&A terminology, "far more research work in this direction is expected" to combine A&A robustness with LLM flexibility.
- Why unresolved: LLM tools are often stateless API calls, lacking the stateful, persistent properties required by the formal A&A definition of artifacts.
- What evidence would resolve it: The development of a hybrid architecture where LLM agents interact with stateful artifacts via standardized protocols (like MCP) that guarantee environmental consistency.

### Open Question 4
- Question: How can development environments for LLM-agents be classified and standardized from a software engineering perspective?
- Basis in paper: Section 4.2 identifies the need for a "software engineering-centric classification and analysis of development environments and APIs."
- Why unresolved: The rapid proliferation of frameworks has led to fragmentation, lacking standardization in architecture, interoperability, and maintenance processes.
- What evidence would resolve it: A comprehensive taxonomy of LLM-agent frameworks based on software engineering metrics (e.g., modularity, scalability, compliance with modern development standards).

## Limitations
- Analysis remains largely conceptual without systematic performance evaluations across common benchmarks
- Comparison between LLM-based and traditional MAS approaches lacks empirical quantification
- Treatment of natural language communication scalability is theoretical with limited practical constraints discussion

## Confidence

- LLM flexibility versus MAS rigor: Medium
- Natural language scalability challenges: Low
- Need for standardization: Medium
- Real-world deployment readiness: Low

## Next Checks
1. Conduct controlled experiments comparing LLM-based agents with classic BDI agents on standardized MAS benchmarks (e.g., RoboCup, JADE scenarios) to quantify performance differences in reliability, response time, and resource consumption.

2. Implement a prototype multi-agent system using both LLM-driven and traditional MAS approaches to measure natural language communication overhead, including message size, processing latency, and network bandwidth requirements at different agent population scales.

3. Survey 50+ active developers of both LLM-based and traditional MAS frameworks to collect empirical data on development challenges, debugging difficulties, and production deployment experiences, focusing on standardization needs and reliability concerns.