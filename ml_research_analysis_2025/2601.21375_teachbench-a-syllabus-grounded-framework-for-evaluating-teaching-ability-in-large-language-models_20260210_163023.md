---
ver: rpa2
title: 'TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability
  in Large Language Models'
arxiv_id: '2601.21375'
source_url: https://arxiv.org/abs/2601.21375
tags:
- teaching
- knowledge
- student
- points
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a syllabus-grounded framework for evaluating
  the teaching ability of large language models (LLMs). The method measures student
  performance improvement after multi-turn instruction, using structured knowledge
  points and example problems to prevent information leakage.
---

# TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models

## Quick Facts
- arXiv ID: 2601.21375
- Source URL: https://arxiv.org/abs/2601.21375
- Reference count: 37
- Primary result: Qwen3-235B-A22B-Instruct achieves highest teaching effectiveness, improving student Pass@1 accuracy by 7.63 points in mathematics.

## Executive Summary
This work introduces TeachBench, a syllabus-grounded framework for evaluating the teaching ability of large language models (LLMs). The method measures student performance improvement after multi-turn instruction, using structured knowledge points and example problems to prevent information leakage. Experiments on Gaokao data across seven subjects show that teaching effectiveness varies significantly by domain and that incorporating example problems does not consistently enhance teaching, as models often shift to example-specific error correction rather than systematic knowledge instruction. These findings demonstrate that teaching ability is a distinct, measurable dimension of LLM behavior, separate from problem-solving performance.

## Method Summary
The TeachBench framework evaluates LLM teaching ability through a three-stage process: knowledge structuring (syllabus → knowledge tree), a teaching loop where a teacher agent instructs a student agent using only knowledge points and optionally example problems (not the target question), and evaluation via pre/post-instruction student performance. The student agent (Qwen2.5-7B-Instruct) first answers target questions to establish a baseline Pass@1. After a multi-turn teaching dialogue with a teacher agent, the student re-attempts the same questions. The delta in Pass@1 (post - pre) is the primary teaching score. The framework restricts teacher access to structured knowledge points and example problems rather than target test questions to enable controlled measurement of genuine teaching ability.

## Key Results
- Qwen3-235B-A22B-Instruct achieves the highest teaching effectiveness, improving student Pass@1 accuracy by 7.63 points in mathematics.
- Teaching effectiveness varies significantly by domain: models perform better in subjects with direct knowledge application (e.g., mathematics, history) and struggle in domains requiring complex contextual integration (e.g., physics, chemistry).
- Incorporating example problems does not consistently enhance teaching, as models often shift to example-specific error correction rather than systematic knowledge instruction.

## Why This Works (Mechanism)

### Mechanism 1: Syllabus-Grounded, Leakage-Controlled Instruction
The framework uses structured knowledge points and separate example problems (rather than target test questions) to force teachers to generalize knowledge rather than memorize or fit to a specific test item. This three-stage process (knowledge structuring → teaching loop → evaluation) enables controlled measurement of genuine teaching ability.

### Mechanism 2: Measuring Teaching via Student Performance Delta (Pass@1)
Teaching effectiveness is quantified as the improvement in a student agent's Pass@1 accuracy on held-out test set after multi-turn instruction. A student agent first answers target questions to establish a baseline Pass@1, then re-attempts after instruction, with the delta in Pass@1 serving as the primary teaching score.

### Mechanism 3: Negative Impact of Example-Driven Instruction on Systematic Teaching
Providing example problems to teacher LLMs often degrades teaching effectiveness by causing a shift from syllabus-grounded conceptual instruction to example-specific error correction. This fragments instruction, reduces time spent on generalizing concepts, and leads to lower post-instruction Pass@1 scores compared to teaching from knowledge points alone.

## Foundational Learning

- **Concept: Multi-turn, Role-based Dialogue Simulation**
  - Why needed: The core evaluation mechanism relies on simulating a teacher-student interaction. Understanding how to design system prompts for distinct roles (teacher, student) and manage dialogue history is essential for implementing and extending this framework.
  - Quick check: If you were to add a "teaching assistant" role to mediate between teacher and student, what key state or context would its system prompt need to manage?

- **Concept: Pass@k Metric**
  - Why needed: The framework's primary evaluation metric is Pass@1, with Pass@4/16/64 used for analysis. Understanding this metric is critical for interpreting results and designing new experiments.
  - Quick check: Why does a large gap between Pass@1 and Pass@64 for a student agent suggest that a teacher could be more effective?

- **Concept: Knowledge Graph/Tree Structuring**
  - Why needed: The entire approach depends on converting a syllabus into a structured knowledge tree and tagging questions to leaf nodes. Without this, the "syllabus-grounded" leakage control fails.
  - Quick check: A question is tagged with knowledge path A -> B -> C. If a teacher teaches only A and B, what is a likely reason the student's Pass@1 might not improve on this question?

## Architecture Onboarding

- **Component map:** Knowledge Structuring Pipeline (Offline) -> Question Tagger (Offline) -> Question Generator (Offline) -> Teaching Loop (Online) -> Evaluation Module

- **Critical path:** The path from Knowledge Point Tagging to Teaching Dialogue Execution. If questions are mis-tagged or the knowledge tree is incomplete, the teacher will be given irrelevant instructional material, making the entire evaluation invalid.

- **Design tradeoffs:**
  1. Student Agent Choice: Using a more capable student may reduce room for improvement; using a weaker one may not be a realistic proxy.
  2. Dialogue Turn Limit: Capping at 30 turns prevents deadlocks but may truncate effective instruction.
  3. Example Problem Integration: The paper shows including examples without explicit pedagogical scaffolding hurts performance.

- **Failure signatures:**
  1. Dialogue Deadlock: Models enter repetitive acknowledgment loops without emitting termination token.
  2. Example-Specific Overfit: Teacher focuses exclusively on solving/correcting provided examples without generalizing concepts.
  3. Knowledge Mismatch: Teacher is given a knowledge point unrelated to the target question due to upstream tagging errors.

- **First 3 experiments:**
  1. Baseline Reproduction (Mathematics): Run the evaluation on Gaokao Math subset to validate the pipeline and reproduce the ∆Acc@Pass@1 metric.
  2. Ablation on Example Quality: Rerun the Math evaluation with examples but modify the teacher's system prompt to explicitly forbid using examples for error correction until conceptual explanation is complete.
  3. Cross-Domain Generalization: Evaluate a top-performing teacher model on a domain where it performed well (e.g., History) and one where it struggled (e.g., Physics), analyzing dialogue logs for contextual integration difficulties.

## Open Questions the Paper Calls Out
- The evaluation does not include human teachers as a baseline, which could provide additional insights and is left for future work.
- The current framework relies on LLM-based student agents, which may not fully reflect the diversity and complexity of human learning behaviors.
- Alternative instructional protocols may lead to different outcomes when incorporating example problems and warrant further investigation.

## Limitations
- The framework uses LLM-based student agents as proxies for human learning, which may not fully capture human learning behaviors and diversity.
- The evaluation does not include human teachers as a baseline for comparison, limiting the context for interpreting LLM teaching effectiveness.
- The effectiveness of knowledge tree extraction and question tagging relies on LLM performance and manual review without public validation datasets.

## Confidence
- **High Confidence:** Technical implementation of the syllabus-grounded framework and domain-specific findings are well-specified and robustly supported.
- **Medium Confidence:** Core claims about teaching ability being distinct from problem-solving are supported but rely on assumptions about student agent validity.
- **Low Confidence:** Generalizability beyond Gaokao dataset and specific subjects remains uncertain.

## Next Checks
1. Conduct a human validation study comparing Pass@1 improvement from the student agent with actual learning outcomes from human participants taught by the same LLM models.
2. Design and test teacher prompts that explicitly instruct models to use examples only after systematic conceptual explanation to mitigate negative impact on Pass@1 scores.
3. Repeat the evaluation with student agents of varying capabilities to determine how the student's latent capability ceiling affects measured teaching effectiveness.