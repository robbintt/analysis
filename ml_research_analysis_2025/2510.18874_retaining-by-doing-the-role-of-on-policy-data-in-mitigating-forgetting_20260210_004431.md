---
ver: rpa2
title: 'Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting'
arxiv_id: '2510.18874'
source_url: https://arxiv.org/abs/2510.18874
tags:
- forgetting
- data
- policy
- drop
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting in language model
  post-training by comparing supervised fine-tuning (SFT) and reinforcement learning
  (RL). The authors find that RL consistently exhibits less forgetting than SFT while
  achieving comparable or higher target task performance across multiple model families
  (Llama, Qwen), scales, and tasks (instruction following, general knowledge, arithmetic
  reasoning).
---

# Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting

## Quick Facts
- arXiv ID: 2510.18874
- Source URL: https://arxiv.org/abs/2510.18874
- Reference count: 40
- Key outcome: RL shows consistently less forgetting than SFT while achieving comparable or higher target task performance across multiple model families, scales, and tasks.

## Executive Summary
This paper investigates catastrophic forgetting in language model post-training by comparing supervised fine-tuning (SFT) and reinforcement learning (RL). The authors find that RL consistently exhibits less forgetting than SFT while achieving comparable or higher target task performance across multiple model families (Llama, Qwen), scales, and tasks (instruction following, general knowledge, arithmetic reasoning). Through theoretical analysis using mixture-of-Gaussians models and extensive ablations, they identify that RL's robustness stems from its use of on-policy data, which enables mode-seeking behavior that preserves prior knowledge. The study shows that approximately on-policy data, such as data generated at the start of each epoch, can substantially reduce forgetting in SFT, suggesting a practical guideline for mitigating forgetting in language model post-training.

## Method Summary
The study compares SFT and GRPO (RL with group reward policy optimization) on three target tasks using models ranging from 1B to 8B parameters. Training uses AdamW optimizer with cosine learning rate scheduling, 2 epochs, and batch size 128. GRPO employs group size 5 and optional KL regularization. Non-target task performance is tracked to measure forgetting. Theoretical analysis uses mixture-of-Gaussians models to explain why on-policy data reduces forgetting. Ablations test KL regularization, learning rates, and data generation strategies including Self-SFT (initial policy only) and Iterative-SFT (epoch-level regeneration).

## Key Results
- RL (GRPO) consistently shows less forgetting than SFT while achieving comparable or higher target task performance across all tested models and tasks
- On-policy data is identified as the primary driver of RL's forgetting resistance, not KL regularization or advantage estimation
- Approximately on-policy data (generated at epoch boundaries) substantially reduces forgetting in SFT, approaching GRPO's performance
- Theoretical analysis explains why reverse KL (RL) preserves prior knowledge better than forward KL (SFT) in multi-modal settings through mode-seeking behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: On-policy data is the primary driver of reduced forgetting, not KL regularization or advantage estimation.
- Mechanism: RL generates data from the current policy πθ during training. This creates a feedback loop where the model learns from outputs it actually produces, rather than from fixed expert demonstrations. The on-policy sampling causes gradient updates that preserve existing modes rather than redistributing probability mass from prior knowledge to cover new targets.
- Core assumption: The model's prior knowledge is encoded in a multi-modal distribution, which is characteristic of practical LMs.
- Evidence anchors:
  - [abstract] "We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation."
  - [section 4.1] Figure 6 shows non-regularized GRPO (β=0.0) achieves similar gain-drop tradeoff as KL-regularized GRPO across most models and datasets.
  - [corpus] Self-Distillation paper confirms on-policy RL reduces forgetting but requires explicit reward functions.
- Break condition: If the target distribution is drastically far from initial modes, even on-policy RL experiences forgetting (Figure 8 shows this with distance ≥4.0 between modes).

### Mechanism 2
- Claim: Reverse KL (RL) preserves prior knowledge better than forward KL (SFT) in multi-modal settings, contrary to uni-modal intuition.
- Mechanism: In a bi-modal training policy, reverse KL shifts the "new" mode toward the target while keeping the "old" mode intact. Forward KL stretches probability mass to cover the new target mode, drawing mass away from the old mode. This occurs because forward KL penalizes the model for assigning zero probability to any region where the target has mass.
- Core assumption: The training policy has multiple modes that can be independently adjusted without full distributional overlap.
- Evidence anchors:
  - [abstract] "We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task."
  - [section 3.3] Figure 5 demonstrates forward KL with high LR causes drop=0.12, while reverse KL achieves the same gain (0.90) with only drop=0.03.
  - [corpus] Per-parameter Task Arithmetic paper notes over-forgetting risks when parameter updates disrupt unrelated capabilities.
- Break condition: In uni-modal settings (Section 3.2), forward KL actually forgets less (drop=0.64) than reverse KL (drop=0.70), reversing the pattern.

### Mechanism 3
- Claim: Approximately on-policy data, generated at the start of each epoch, suffices to mitigate forgetting without requiring per-step sampling.
- Mechanism: Iterative-SFT regenerates training data from the current model checkpoint at each epoch boundary. This provides "approximately on-policy" signals that maintain the mode-seeking benefits of RL without the computational cost of continuous sampling. The data remains closer to the model's current distribution than fixed expert data.
- Core assumption: The model's distribution does not drift too far within a single epoch to negate the on-policy benefits.
- Evidence anchors:
  - [abstract] "Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data."
  - [section 4.2] Figure 7 shows Iterative-SFT reaches higher target accuracy than SFT with mild to no forgetting, while Self-SFT (initial policy only) suffers severe forgetting.
  - [corpus] Weak or missing evidence—no direct corpus papers evaluate epoch-boundary regeneration strategies.
- Break condition: Self-SFT (data only from initial policy) fails to mitigate forgetting, showing that single-timepoint on-policy data is insufficient.

## Foundational Learning

- Concept: Forward vs Reverse KL Divergence
  - Why needed here: The entire theoretical explanation hinges on understanding why forward KL (mode-covering) and reverse KL (mode-seeking) produce different forgetting behaviors.
  - Quick check question: Given a multi-modal target distribution, which KL direction would assign some probability mass to all modes, and which might ignore low-probability modes entirely?

- Concept: On-Policy vs Off-Policy Data
  - Why needed here: The paper's central finding is that on-policy data—not algorithmic differences—drives RL's forgetting resistance.
  - Quick check question: In GRPO, is the training data generated from a fixed expert model or from the model being trained? How does this differ from standard SFT?

- Concept: Catastrophic Forgetting Metrics (Gain vs Drop)
  - Why needed here: The paper quantifies forgetting through non-target task drop (∆d) and learning through target task gain (∆g), measuring the tradeoff.
  - Quick check question: If a model improves 15% on a target task but degrades 8% on average across non-target tasks, would you consider this an acceptable tradeoff for a deployed system?

## Architecture Onboarding

- Component map:
  - SFT Pipeline: Fixed expert responses → Cross-entropy loss → Parameter update
  - Self-SFT Pipeline: Initial model responses → Filter correct → Cross-entropy loss
  - Iterative-SFT Pipeline: Current model responses (per epoch) → Filter correct → Cross-entropy loss
  - GRPO Pipeline: Current model generates group → Compute rewards → Advantage-weighted policy gradient with optional KL penalty

- Critical path: Start with GRPO implementation. The key departure from standard RL is ensuring data generation happens from πθ at each optimization step, not from a frozen reference. Verify the sampling loop generates fresh completions before each gradient update.

- Design tradeoffs:
  - Fully on-policy (GRPO) vs approximately on-policy (Iterative-SFT): GRPO achieves best forgetting mitigation but requires continuous sampling overhead. Iterative-SFT offers ~80% of the benefit with epoch-level regeneration.
  - Learning rate selection: High LR needed for SFT to reach target performance but causes severe forgetting. RL achieves gains with lower, more stable LR (Figure 3).
  - KL regularization (β): Paper shows β=0.0 and β=0.05 produce similar forgetting patterns, suggesting this is not a primary lever.

- Failure signatures:
  - Self-SFT shows severe forgetting despite using model-generated data—single-timepoint sampling is insufficient.
  - SFT with low LR fails to reach target performance even with extended epochs (Figure 3).
  - Uni-modal models would show reversed forgetting patterns (Section 3.2), though this is rare in practice.

- First 3 experiments:
  1. Reproduce the GRPO vs SFT comparison on IFEval using Llama-3.2-1B-Instruct. Measure both target accuracy gain and non-target task drop (include MATH and at least one safety benchmark). Expect GRPO drop < 5% while SFT drop > 15%.
  2. Ablate KL regularization by running GRPO with β=0.0 vs β=0.05. Verify that forgetting remains similar across both settings (Figure 6 pattern).
  3. Implement Iterative-SFT with epoch-level regeneration. Compare forgetting against Self-SFT (single regeneration) and standard SFT (expert data). Expect Iterative-SFT drop to approach GRPO levels while Self-SFT and SFT show elevated forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do forgetting patterns compare between SFT and RL when scaling model and dataset sizes significantly beyond the 8B parameter limit?
- Basis in paper: [explicit] The authors state in the Limitations section that "investigating how forgetting patterns vary as the model and dataset sizes are further scaled... remains a valuable direction for future work."
- Why unresolved: The study was constrained by compute budget to models up to 8B parameters (Llama 3.1, Qwen 2.5).
- What evidence would resolve it: Empirical comparisons of SFT vs. RL forgetting rates on frontier-scale models (e.g., 70B+ parameters) and larger post-training datasets.

### Open Question 2
- Question: Can the theoretical relationship between on-policy data and forgetting be formally established beyond the simplified mixture-of-Gaussians model?
- Basis in paper: [explicit] The authors acknowledge that "additional research is necessary to theoretically establish the role of on-policy data in mitigating forgetting" beyond the intuition provided by their simplified analysis.
- Why unresolved: The paper provides intuition using univariate Gaussians, but lacks a formal theoretical guarantee for complex, high-dimensional language model distributions.
- What evidence would resolve it: A formal theoretical framework or proof generalizing the mode-seeking benefits of on-policy data to high-dimensional policy distributions.

### Open Question 3
- Question: What is the precise trade-off between the frequency of data generation and the mitigation of forgetting when using "approximately on-policy" data?
- Basis in paper: [inferred] While the authors ask "what degree of 'on-policyness' is required," they only test the endpoints (initial policy vs. start of each epoch) and do not fully map the efficiency trade-off.
- Why unresolved: It is unclear if refreshing data less frequently than every epoch (e.g., every 2-3 epochs) retains the anti-forgetting benefits or if strictly step-level on-policy data is required for optimal retention.
- What evidence would resolve it: Ablation studies measuring target gain vs. non-target drop across a spectrum of data refresh frequencies (e.g., refreshing data every $k$ steps vs. every epoch).

## Limitations

- The theoretical analysis using mixture-of-Gaussians models relies on idealized assumptions about mode separation that may not fully capture complex LLM representation geometry
- Empirical validation focuses on small to medium-sized models (1B-8B parameters), leaving uncertainty about whether patterns hold at frontier scales
- Study uses binary reward signals, simplifying the reward landscape compared to more nuanced continuous reward functions used in practical RLHF systems

## Confidence

- **High confidence**: The empirical observation that RL exhibits less forgetting than SFT across multiple model families and tasks is well-supported by the experimental results presented in Figures 1-3
- **Medium confidence**: The theoretical explanation of why on-policy data reduces forgetting through mode-seeking behavior is logically consistent and supported by ablation studies, but relies on simplified assumptions about underlying distributions
- **Medium confidence**: The claim that KL regularization is not the primary factor in RL's forgetting resistance is supported by β ablation experiments, though the mechanism connecting on-policy data to forgetting mitigation could benefit from additional theoretical grounding

## Next Checks

1. **Scale validation**: Reproduce the GRPO vs SFT comparison on a 70B parameter model (e.g., Llama-3.1-70B-Instruct) to verify that forgetting patterns observed in smaller models hold at frontier scales

2. **Reward function ablation**: Test GRPO with continuous reward functions (e.g., using a learned reward model rather than binary correctness) to determine whether on-policy forgetting mitigation holds with more realistic reward landscapes

3. **Cross-task forgetting patterns**: Extend evaluation to include domain-specific tasks beyond current benchmarks (e.g., code generation, multilingual capabilities, long-form reasoning) to assess whether forgetting patterns hold across full spectrum of LLM capabilities and identify potential failure modes in specialized domains