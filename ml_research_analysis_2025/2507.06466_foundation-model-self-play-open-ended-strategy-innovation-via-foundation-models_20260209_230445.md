---
ver: rpa2
title: 'Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation
  Models'
arxiv_id: '2507.06466'
source_url: https://arxiv.org/abs/2507.06466
tags:
- self
- prompt
- password
- policies
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Foundation-Model Self-Play (FMSP) is a new family of algorithms
  that combine self-play with foundation model code generation to create high-quality,
  diverse strategies in multi-agent domains. Three variants are proposed: Vanilla
  FMSP for pure exploitation, Novelty-Search FMSP for diversity, and Quality-Diversity
  FMSP for balancing both.'
---

# Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models

## Quick Facts
- arXiv ID: 2507.06466
- Source URL: https://arxiv.org/abs/2507.06466
- Reference count: 40
- Primary result: QDSP achieves highest quality-diversity scores and outperforms human-designed strategies in multi-agent domains

## Executive Summary
Foundation-Model Self-Play (FMSP) combines self-play with foundation model code generation to discover high-quality, diverse strategies in multi-agent domains. Three variants are proposed: Vanilla FMSP for pure exploitation, Novelty-Search FMSP for diversity, and Quality-Diversity Self-Play (QDSP) for balancing both. Evaluated in Car Tag and Gandalf, QDSP demonstrates superior performance, discovering strategies that outperform human-designed baselines and successfully red-teaming and patching LLM defenses across six levels.

## Method Summary
FMSP operates by having a foundation model generate Python class files that define agent policies. These policies compete against each other in a secure sandbox environment, with their performance and code embeddings stored in an archive. The system iteratively generates new policies by providing the FM with context from previous matches and similar policies, creating a co-evolutionary dynamic. QDSP specifically uses code embeddings to automatically discover behavioral dimensions, eliminating the need for manual feature engineering required by traditional quality-diversity algorithms.

## Key Results
- QDSP achieves the highest quality-diversity scores compared to human-designed and other FM-generated strategies in Car Tag
- In Gandalf, FMSPs successfully red-team LLM defenses and automatically generate patches to discovered vulnerabilities across six levels
- FMSP discovers strategies that outperform human-designed baselines in both domains, demonstrating the potential for open-ended strategy discovery

## Why This Works (Mechanism)

### Mechanism 1
FMSP overcomes local optima common in standard self-play by operating at a higher level of abstraction (code vs. parameters), allowing for discontinuous "leaps" in strategy space. Instead of gradient descent on weights, the Foundation Model acts as a search operator generating Python classes, leveraging its pretraining on human knowledge to propose entirely new algorithms in a single step.

### Mechanism 2
Quality-Diversity Self-Play (QDSP) automates the discovery of behavioral dimensions, removing the need for manual feature engineering required by traditional Quality-Diversity (QD) algorithms. Instead of hand-crafting behavioral descriptors, QDSP uses the embedding of the generated code itself to determine novelty, where code embedding distance correlates with behavioral similarity.

### Mechanism 3
Co-evolutionary dynamics between attackers and defenders create a self-generated curriculum that exposes and patches system vulnerabilities. In Gandalf, the system alternates between generating "Red Team" attackers to exploit defenses and generating "Blue Team" defenders to patch specific exploits found in the previous round, forcing the FM to react to its own best strategies.

## Foundational Learning

- **Concept: Self-Play & Implicit Curricula**
  - Why needed here: The paper builds directly on self-play fundamentals (pitting agents against past versions). Understanding how this creates an automatic difficulty ramp is necessary to grasp why FMSP adds an FM on top.
  - Quick check question: Can you explain why training an agent against a fixed opponent often leads to overfitting, and how self-play theoretically solves this?

- **Concept: Quality-Diversity (QD) / MAP-Elites**
  - Why needed here: QDSP is the paper's primary contribution. You must understand the distinction between optimization (finding the *one* best solution) and QD (finding a *map* of high-performing, diverse solutions).
  - Quick check question: In a standard MAP-Elites setup for a walking robot, what are the "dimensions of variation," and how does QDSP change this requirement?

- **Concept: Sandboxing & Code Execution**
  - Why needed here: FMSP generates executable Python code. Understanding the risks (infinite loops, malicious code) and the solution (containerization/sandboxing) is critical for implementation.
  - Quick check question: Why can't we just ask the LLM to output the action directly (e.g., "turn left") instead of generating a class file, and what security risks arise when executing generated code?

## Architecture Onboarding

- **Component map:** Context Builder -> Foundation Model -> Sandbox/Environment -> Evaluation Engine -> Archives (two databases storing code strings and embeddings)

- **Critical path:** The Propose-Execute-Evaluate loop: 1) Sample parent policy, 2) FM generates new code, 3) Sandbox execution (must compile and run without crashing), 4) If successful, calculate score and extract embedding

- **Design tradeoffs:**
  - vFMSP vs. QDSP: vFMSP is simpler (single-threaded, pure exploitation) but prone to local optima. QDSP is complex (requires embedding DB, niche finding) but produces diverse, robust strategies
  - GPT-4o vs. Claude: Paper notes switching to Claude Sonnet 3.5 for Gandalf because complex coding tasks required higher coding proficiency

- **Failure signatures:**
  - Strategy Collapse: Archive fills with nearly identical policies (indicates embedding space is too tight or FM is failing to innovate)
  - Execution Errors: High rate of syntax errors or runtime exceptions (indicates FM coding capability is insufficient or prompt is confusing)
  - Level 7 Stagnation: System cannot combine primitive strategies into a composite attack

- **First 3 experiments:**
  1. Open-Loop Baseline (Car Tag): Implement simplest version - ask FM to generate pursuer, run against fixed heuristic evader without score or previous code feedback
  2. vFMSP Implementation (Car Tag): Close the loop - give FM previous code and score, let it "hill-climb" to improve performance
  3. QDSP Integration (Gandalf Level 1-4): Implement embedding-based archive, run loop for 50 generations, visualize QD-Map (PCA of embeddings) to check coverage expansion

## Open Questions the Paper Calls Out

### Open Question 1
Can FMSPs effectively synthesize multiple "specialist" strategies into a single generalized policy to solve complex, multi-constraint problems? Current mechanism struggles to combine orthogonal strategies (e.g., input obfuscation and output parsing) into one code block within the FM's context window.

### Open Question 2
How can FMSPs be extended to optimize neural network policies via generated reward functions rather than direct code generation? Code-based policies may struggle with high-dimensional sensory inputs where neural networks excel.

### Open Question 3
To what extent does the embedding model's capacity constrain QDSP's ability to distinguish novel strategies? If the embedding space is low-dimensional or biased, the "dimensionless" search may fail to recognize significant conceptual leaps in strategy.

## Limitations

- FM dependency and brittleness: Performance heavily relies on FM coding ability, with current mechanisms failing on complex multi-constraint problems like Gandalf Level 7
- Embedding space assumptions: QDSP assumes code embeddings correlate with behavioral diversity, which may break if syntactically different codes produce similar behaviors
- Sandboxing overhead: Requires secure execution environments (Docker containers), adding computational and implementation complexity

## Confidence

- **High**: QDSP's ability to discover diverse strategies in Car Tag and patch Gandalf vulnerabilities; co-evolutionary dynamics creating self-generated curricula
- **Medium**: Claims about overcoming local optima through higher abstraction levels; depends heavily on FM pretraining coverage
- **Medium**: Quality-diversity scores and behavioral analysis in Gandalf; embedding-based novelty detection shows promise but hasn't been rigorously validated across diverse domains

## Next Checks

1. **Cross-domain robustness test**: Apply QDSP to a completely different multi-agent domain (e.g., predator-prey or resource collection) to verify FM can generate viable strategies without domain-specific pretraining
2. **Embedding behavior correlation**: Conduct controlled experiment where code pairs with similar embeddings are run in environment to verify they produce similar behaviors, and vice versa
3. **FM model ablation**: Systematically test different FM models (GPT-4, Claude, smaller open models) on same tasks to quantify minimum model capability required for successful FMSP operation