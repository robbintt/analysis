---
ver: rpa2
title: Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning
arxiv_id: '2506.10664'
source_url: https://arxiv.org/abs/2506.10664
tags:
- learning
- policy
- adaptive
- data
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work extends Logarithmic Smoothing (LS) PAC-Bayes off-policy
  learning to the adaptive setting, where policies are iteratively refined and redeployed
  to collect higher-quality data. The authors develop two adaptive algorithms: one
  based on the standard LS estimator and another using an adjusted estimator that
  avoids pseudo-variance terms and achieves faster convergence rates.'
---

# Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning

## Quick Facts
- arXiv ID: 2506.10664
- Source URL: https://arxiv.org/abs/2506.10664
- Authors: Maxime Haddouche; Otmane Sakhi
- Reference count: 40
- Primary result: Adaptive PAC-Bayesian algorithms outperform static baselines and improve with deployment count across multiple datasets

## Executive Summary
This paper extends Logarithmic Smoothing (LS) PAC-Bayes off-policy learning to adaptive settings where policies are iteratively refined and redeployed to collect higher-quality data. The authors develop two adaptive algorithms: one based on the standard LS estimator and another using an adjusted estimator that avoids pseudo-variance terms and achieves faster convergence rates. Both algorithms are theoretically grounded with PAC-Bayes guarantees, and the adjusted version shows accelerated convergence under margin and coverage conditions. Empirically, the methods outperform state-of-the-art offline approaches in static settings and significantly improve when intermediate policy deployments are allowed, with the adjusted estimator showing particularly strong performance.

## Method Summary
The method builds on inverse propensity scoring (IPS) with logarithmic smoothing regularization to control variance in off-policy evaluation. For adaptive settings, the authors replace batch Markov inequality with Ville's inequality on a non-negative martingale, allowing bounds to hold across countably many deployment rounds while adapting to incoming data batches. Two estimators are proposed: the standard LS estimator and an adjusted version that eliminates persistent pseudo-variance terms. Both are combined with PAC-Bayes optimization using KL divergence complexity penalties. The algorithms iteratively deploy policies, collect new data, update estimators, and re-optimize, with the adjusted estimator achieving faster convergence rates under margin and coverage conditions.

## Key Results
- Adaptive methods consistently outperform state-of-the-art offline approaches in static settings
- Performance improves significantly with increasing number of intermediate deployments
- The adjusted estimator (AdaAdjLS) achieves faster convergence rates and stronger empirical performance
- Across MNIST, FashionMNIST, EMNIST, and CIFAR100 datasets, adaptive methods yield better policies than static approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online PAC-Bayes with supermartingales extends static off-policy learning to adaptive sequential deployments.
- Mechanism: The method replaces batch Markov inequality with Ville's inequality on a non-negative martingale (Mk = Eθ∼P[exp(fk(S,θ))]), allowing the bound to hold simultaneously across countably many deployment rounds while adapting to incoming data batches.
- Core assumption: The prior P is data-independent; contexts within each batch are conditionally independent given the filtration Fk−1.
- Evidence anchors:
  - [abstract] "using tools from online PAC-Bayesian theory"
  - [section 3.2] "controlling the expansion of a supermartingale through Ville's inequality"
  - [corpus] Related PAC-Bayesian RL paper mentions similar Markov-dependency challenges but in full RL settings (FMR=0.66)
- Break condition: If behavior policies become data-dependent in ways that violate the conditional independence assumption within batches, the martingale property fails.

### Mechanism 2
- Claim: Adjusting the LS regularizer eliminates persistent pseudo-variance terms, enabling accelerated convergence.
- Mechanism: The adjusted regularizer hadjλ(p,q,c) = −p/λ · log(1−λc/(q(1+λc))) replaces the standard LS term, yielding L(π,πj) which decreases to zero as πj→π⋆, unlike S(π) which remains positive even in the oracle case.
- Core assumption: The optimal policy π⋆ is deterministic; the policy class is rich enough to represent π⋆; the learning algorithm outputs policies satisfying γk ≤ γ (bounded coverage of optimal actions).
- Evidence anchors:
  - [abstract] "principled adjustment to the LS estimator naturally accommodates multiple rounds of deployment and yields faster convergence rates"
  - [section 4] "L(π⋆,πj)→0 if πj→π⋆. Such desirable behavior leaves room for accelerated convergence rate"
  - [corpus] No direct corpus evidence on this specific estimator adjustment (weakly connected)
- Break condition: If the optimal policy is stochastic or the policy class cannot represent π⋆, the acceleration guarantee does not hold.

### Mechanism 3
- Claim: Margin-like separability (Δu>0) and minimal optimal-action coverage (C⋆k>0) create a recursive contraction enabling O(1/(km)^α) rates.
- Mechanism: Lemma 4.3 shows L(π⋆,πk) ≤ γk(R(πk)−R(π⋆)) where γk decreases with better coverage and larger action gaps; this converts the pseudo-variance bound into a risk-difference bound, yielding recursive decay.
- Core assumption: There exists u∈[0,1) such that Δu>0 (optimal action is distinguishable from suboptimal ones with positive probability); C⋆k = inf_x πk(a⋆(x)|x) > 0 for all k.
- Evidence anchors:
  - [section 4, Lemma 4.3] "The existence of a Δu>0 is similar to margin conditions used in multi-arm contextual bandits"
  - [section 4, Theorem 4.4] "provides an accelerated rate of O(1/√mk^α) compared to O(1/√mk)"
  - [corpus] No corpus papers directly address this specific margin-coverage coupling
- Break condition: If costs are nearly constant (Δu≈0) or policies stop covering optimal actions (C⋆k→0), γk explodes and the recursion fails.

## Foundational Learning
- Concept: Inverse Propensity Scoring (IPS) with importance weights
  - Why needed here: The entire estimator framework builds on IPS, with regularization as the key modification to control variance.
  - Quick check question: Can you explain why IPS is unbiased under common support but has high variance?
- Concept: PAC-Bayesian bounds with KL divergence complexity penalty
  - Why needed here: All generalization guarantees in this paper follow the form R(πQ) ≤ estimator + KL(Q||P)/n.
  - Quick check question: What role does the prior P play in controlling generalization vs. data-fitting?
- Concept: Supermartingales and Ville's inequality
  - Why needed here: Extending batch bounds to sequential deployments requires martingale concentration instead of single-shot Markov inequality.
  - Quick check question: Why does Ville's inequality allow bounds that hold uniformly over time?

## Architecture Onboarding
- Component map: Data collector deploying πk and logging (x,a,c) tuples -> Regularized IPS estimator (either LS or AdjLS) -> PAC-Bayes optimizer solving argmin_Q[estimator + KL(Q||P)/Nk] -> Policy extraction as πQ(a|x) = Eθ∼Q[1{fθ(x)=a}]
- Critical path: Data arrival → Update estimator with new batch → Re-solve optimization → Deploy new πk. The adjusted estimator (AdaAdjLS) is preferred for accelerated convergence but requires checking coverage assumptions.
- Design tradeoffs: (1) Standard LS (adaLS) is simpler but limited to O(1/√n) rates; (2) Adjusted LS (adaAdjLS) achieves faster rates but requires margin and coverage conditions; (3) Larger λ speeds convergence but increases constants in bounds.
- Failure signatures: (1) Risk stagnating despite more data → check if optimal actions are covered (C⋆k≈0); (2) Policy becoming deterministic prematurely → increase prior variance or reduce λ; (3) Variance explosion in importance weights → behavior policy not improving.
- First 3 experiments:
  1. Replicate static batch setting (k=1) on MNIST with adaLS vs. adaAdjLS to validate baseline matching per Table 1.
  2. Sweep k∈{1,5,10,100} with fixed total samples N to quantify adaptive gains per deployment count.
  3. Vary initial policy quality (α∈{0,0.5,1}) on CIFAR100 to test sensitivity to starting coverage per Appendix F.2.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the adaptive PAC-Bayesian framework be extended to handle non-stationary environments where the context or cost distributions shift during training?
  - Basis in paper: [explicit] The Conclusion and Appendix A state that the current assumption of a fixed context distribution is inadequate for real-world applications where user behavior changes, identifying distribution shift as a key avenue for future research.
  - Why unresolved: The current theoretical guarantees rely on Ville's inequality for supermartingales adapted to a fixed data stream distribution, which breaks down if the underlying distribution $\nu$ evolves.
  - What evidence would resolve it: Theoretical generalization bounds that hold under specific distribution drift models, alongside empirical validation showing the estimator remains unbiased or robust in non-stationary settings.

- **Open Question 2**: How can the convergence parameter $\alpha$ be optimally selected or adapted online to balance the theoretical convergence rate against the deterioration of the bound's constant $C_\alpha$?
  - Basis in paper: [explicit] The discussion following Theorem 4.4 notes that while larger $\alpha$ values yield faster rates ($O(1/n^\alpha)$), the constant $C_\alpha$ deteriorates as $\alpha$ approaches 1, creating a difficult trade-off for practitioners.
  - Why unresolved: The optimal $\alpha$ depends on unknown properties of the optimal policy and the margin conditions ($\Delta_u$), and the paper uses fixed values in the experimental setup.
  - What evidence would resolve it: An adaptive hyperparameter schedule for $\alpha$ (or $\lambda$) that provably minimizes the generalization bound at each step, or empirical results showing sensitivity to this parameter across diverse data regimes.

- **Open Question 3**: Can the algorithm be modified to guarantee convergence or recovery if the initial behavior policy $\pi_0$ fails to adequately cover the optimal actions?
  - Basis in paper: [explicit] Appendix A lists as a limitation that the algorithm is reliant on $\pi_0$ and "may not converge to the optimal policy if the starting policy fails to explore optimal actions."
  - Why unresolved: The fast convergence rates rely on the condition $C^\star_k > 0$ (minimal plausibility of optimal actions), which is not guaranteed if the initial deterministic policy is suboptimal.
  - What evidence would resolve it: A mechanism for safe exploration or a theoretical proof showing convergence under relaxed coverage assumptions (e.g., weak regularity conditions rather than positive probability on optimal actions).

## Limitations
- The accelerated convergence guarantees require strong assumptions about deterministic optimal policies and sufficient policy class expressiveness that may not hold in practice
- The theoretical analysis relies on conditional independence within data batches, which may be violated by data-dependent behavior policies
- The paper does not address computational complexity of solving PAC-Bayes optimization at each deployment round as k increases

## Confidence
- **High Confidence**: The core mechanism of using Ville's inequality to extend static PAC-Bayes bounds to sequential deployments is well-established in the online learning literature and the implementation details are clearly specified.
- **Medium Confidence**: The accelerated convergence rates for adaAdjLS are mathematically sound under stated assumptions, but the empirical validation is limited to moderate-sized datasets where these assumptions may hold trivially.
- **Low Confidence**: The practical impact of the accelerated rates in real-world settings with large action spaces and complex optimal policies is uncertain, as the experiments do not stress-test the coverage and margin assumptions.

## Next Checks
1. **Coverage Verification**: For each deployment k, compute and report C⋆k = inf_x πk(a⋆(x)|x) to empirically verify that optimal actions maintain positive coverage throughout the adaptive process.
2. **Action Gap Analysis**: Measure Δu = inf_{x,a≠a⋆(x)} P(c(x,a⋆(x)) - c(x,a) > 0) on the logged data to assess whether the margin condition required for accelerated convergence holds in practice.
3. **Scaling Experiment**: Evaluate adaLS and adaAdjLS on a dataset with 10× larger action space (e.g., ImageNet32 reduced to 1000 classes) to test whether the theoretical advantages persist when the policy class becomes less expressive relative to the optimal policy.