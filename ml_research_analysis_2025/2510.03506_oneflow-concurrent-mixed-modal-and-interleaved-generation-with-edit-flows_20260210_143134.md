---
ver: rpa2
title: 'OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows'
arxiv_id: '2510.03506'
source_url: https://arxiv.org/abs/2510.03506
tags:
- image
- generation
- text
- oneflow
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OneFlow is the first non-autoregressive multimodal model enabling
  concurrent mixed-modal generation with variable-length and interleaved text and
  image synthesis. It combines Edit Flows for discrete text tokens with Flow Matching
  for image latents, using an insertion-based framework that allows images to be inserted
  as part of the generation process.
---

# OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows

## Quick Facts
- arXiv ID: 2510.03506
- Source URL: https://arxiv.org/abs/2510.03506
- Reference count: 40
- Primary result: First non-autoregressive multimodal model enabling concurrent mixed-modal generation with variable-length and interleaved text and image synthesis, outperforming autoregressive baselines while using up to 50% fewer training FLOPs.

## Executive Summary
OneFlow is a non-autoregressive multimodal model that enables concurrent text and image generation through a unified sequence space with specialized generative processes for each modality. It combines Edit Flows for discrete text tokens with Flow Matching for image latents, using an insertion-based framework that allows images to be inserted as part of the generation process. The model achieves state-of-the-art or competitive performance on benchmarks including DPG-Bench, FID, and VQA, while reducing training compute by approximately 50% compared to autoregressive baselines.

## Method Summary
OneFlow treats discrete text tokens and continuous image embeddings as elements in a single sequence processed by a shared bidirectional Transformer backbone. For text generation, it uses Edit Flows with Poisson rate parameterization to predict both the number of missing tokens and their values, enabling hierarchical generation where content precedes structure. For images, it applies Flow Matching to denoise continuous latent embeddings, with an interleaved time schedule ensuring proper temporal alignment between modalities. The model uses dual encoders (SigLIP for understanding, SD3 VAE for generation) and trains in two stages: multimodal pretraining followed by instruction finetuning on VQA and interleaved datasets.

## Key Results
- Outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs
- Achieves state-of-the-art performance on DPG-Bench, FID, and VQA benchmarks
- Enables new capabilities for concurrent generation, iterative refinement, and reasoning-like text generation
- Mixed-modal pretraining yields 4% relative improvement on VQA and 1.5% on image generation over sequential pretraining

## Why This Works (Mechanism)

### Mechanism 1: Unified Sequence Space with Typed Generative Processes
OneFlow handles multiple modalities through a sequence model where elements can be discrete tokens or continuous embeddings, combining discrete-valued and continuous-valued generative processes. The shared Transformer backbone processes this sequence, but output heads diverge: one head predicts discrete insertion rates for text while another predicts velocity fields for continuous image denoising. This specialization allows the model to effectively process both modalities within a unified framework.

### Mechanism 2: Hierarchical Insertion via Poisson Rate Parameterization
The model generates text non-autoregressively by learning to predict the number of missing tokens and their values, allowing content to be generated before grammatical structure. Instead of predicting a single token, the model predicts λ_i (rate of insertions) and Q_i (distribution of tokens), with the loss explicitly modeling the count of missing tokens. This enables hierarchical sampling where the most difficult answer tokens are generated later.

### Mechanism 3: Interleaved Time Schedule for Asynchronous Denoising
OneFlow allows images to be inserted mid-text and denoised concurrently by enforcing a distributional dependency between text generation time (t_text) and image generation time (t_img). Images are inserted as noise (tokens <img>), and because images start denoising later than the text prompt, the model uses an interleaved time schedule where t_img ≤ t_text. This ensures the model sees a consistent noise distribution during training even when insertion times are random.

## Foundational Learning

- **Concept: Continuous-Time Markov Chains (CTMC)**
  - Why needed here: Edit Flows model text generation as a continuous jump process where insertions occur at random times, unlike standard Diffusion (discrete steps) or AR (discrete tokens).
  - Quick check question: Can you explain how the "rate" λ in a CTMC differs from a standard probability P in an autoregressive model?

- **Concept: Flow Matching (Rectified Flows)**
  - Why needed here: This is the mathematical framework for generating images, defining an Ordinary Differential Equation to transport noise to data.
  - Quick check question: What is the target vector v_t that the model tries to predict in Flow Matching (specifically, what is the relationship between Y_0, Y_1, and Y_t)?

- **Concept: Bidirectional Attention**
  - Why needed here: OneFlow is non-autoregressive and needs to see the full context (future and past tokens) to decide what to insert, unlike AR models which mask the future.
  - Quick check question: Why does the lack of KV-caching (a consequence of bidirectional attention) impact inference latency, and how does OneFlow's variable length property mitigate this?

## Architecture Onboarding

- **Component map:** Data → Tokenizer (text) / VAE Encoder (image) → Random Deletion/Noising (creating X_t) → Transformer Forward Pass → Multi-head Loss (Poisson + CE + FM MSE) → Backprop

- **Critical path:** The model takes a sequence of discrete tokens plus continuous latent embeddings, applies random deletion/noising to create a corrupted input X_t, processes it through the bidirectional Transformer, and applies specialized heads for text insertion (Poisson rate + vocabulary distribution) and image denoising (velocity field prediction).

- **Design tradeoffs:** OneFlow saves ~50% training FLOPs because it predicts on ~50% of tokens (deleted ones) compared to AR, but inference requires multiple denoising steps without KV-caching, potentially increasing latency per token compared to cached AR inference.

- **Failure signatures:** Training instability with interleaved time schedule, poor image insertion during generation, quality gap vs AR baseline, text collapse (infinite token insertion), modal amnesia (images don't match text), CFG over-hallucination.

- **First 3 experiments:**
  1. Overfit Single Instance: Train on one image-text pair to verify perfect reconstruction via insertion and denoising, validating data pipeline and loss logic.
  2. Ablate Time Schedule: Train with independent time schedules vs. interleaved schedule to check if image generation quality drops or text-image alignment degrades.
  3. Scaling Check: Train 1B model for 10k steps on controlled dataset, compare FID/DPG against AR+FM baseline to ensure scaling trend before scaling to 8B.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can semi-autoregressive or block-structured decoding methods be integrated into OneFlow to mitigate the lack of key-value (KV) caching while preserving concurrent mixed-modal capabilities?
- Basis in paper: [explicit] The authors identify the "lack of key-value caching" due to bidirectional attention as a limitation that increases inference cost, suggesting that "semi-autoregressive models... or more sophisticated methods, would be an exciting research direction" (Section 5).
- Why unresolved: The current OneFlow formulation relies on full bidirectional attention to enable concurrent refinement of text and images, which is fundamentally incompatible with standard causal KV-caching used in autoregressive models.
- What evidence would resolve it: A modified OneFlow architecture implementing block-wise or semi-autoregressive attention, demonstrating reduced inference latency and memory footprint on long-sequence interleaved generation without sacrificing quality or concurrency.

### Open Question 2
- Question: How does the performance and capability of OneFlow scale when trained on significantly larger, high-quality interleaved datasets (e.g., trillion-token scale)?
- Basis in paper: [explicit] The authors state that "Interleaved generation is still in its infancy" and call for "incoming research efforts in constructing large-scale data sets... and designing comprehensive benchmarks" (Section 5).
- Why unresolved: The experiments utilize a specific mixture of datasets (400M examples for pretraining), but it remains unclear if the observed "emergent reasoning" and concurrent generation capabilities saturate or improve exponentially with significantly more diverse interleaved data.
- What evidence would resolve it: Training runs on newly curated, massive-scale interleaved corpora showing scaling curves that exceed current baselines.

### Open Question 3
- Question: Why does the time-independent parameterization for insertion rates (λ) outperform time-dependent formulations in practice, despite lacking theoretical justification?
- Basis in paper: [inferred] In Section 2.1.1, the authors note they "factor out this ratio... and use a simplified model that is independent of t," admitting "While not theoretically justified, we found this t-independence assumption to work better in practice."
- Why unresolved: The theory of continuous-time Markov chains and Edit Flows typically suggests dependence on noise level (time), yet the empirical success of the time-independent approach suggests the noisy sequence X_t might encode sufficient signal, challenging current theoretical understandings.
- What evidence would resolve it: A theoretical analysis or ablation study isolating information content within X_t at various noise levels, potentially showing that position of missing tokens implicitly encodes the time information usually provided by explicit time embeddings.

### Open Question 4
- Question: Can adaptive or learned deletion schedulers (κ_t) outperform the linear schedule to further reduce training FLOPs without compromising generation quality?
- Basis in paper: [explicit] Section 2.1 mentions that "tuning the scheduler can save even more [FLOPS] if desired," but the authors defaulted to a linear schedule (κ_t = t) because it "works most consistently" in preliminary experiments.
- Why unresolved: The linear schedule is a heuristic choice; a scheduler that adaptively keeps difficult tokens or deletes easy tokens more aggressively could theoretically optimize the trade-off between compute efficiency and gradient signal.
- What evidence would resolve it: Experiments comparing the linear schedule against curvature-based or loss-based adaptive schedulers, reporting the Pareto frontier of validation loss vs. training FLOPs.

## Limitations
- Theoretical grounding for hierarchical generation and "implicit reasoning" is primarily qualitative rather than rigorously proven
- Computational efficiency claims may not translate to practical gains due to lack of KV-caching increasing inference latency
- Modality alignment robustness depends heavily on the interleaved time schedule mechanism, which hasn't been tested across diverse scenarios
- Training stability appears sensitive to hyperparameters, particularly around Poisson rate parameterization and time embeddings

## Confidence

**High Confidence:**
- Technical feasibility of combining Edit Flows with Flow Matching in unified framework
- Existence of performance improvements on controlled experiments and benchmark datasets
- Qualitative demonstration of new capabilities (concurrent generation, iterative refinement)

**Medium Confidence:**
- 50% training FLOPs reduction mathematically sound but may not translate to practical efficiency gains
- Mixed-modal pretraining providing 4% relative improvement on VQA supported by controlled experiments
- Model's ability to generate images interleaved with text demonstrated but not systematically evaluated

**Low Confidence:**
- Hierarchical generation mechanism producing "implicit reasoning" is primarily qualitative
- Claim that OneFlow "outperforms autoregressive baselines" lacks comprehensive ablation studies
- Generalization of controlled experiment results to full 400M pretraining corpus is assumed

## Next Checks

1. **Controlled Ablation Study**: Systematically ablate the Poisson rate parameterization while keeping all other components constant. Train three variants: (a) standard AR with fixed deletion schedule, (b) Edit Flows with learned deletion but no Poisson rate prediction, and (c) full Edit Flows with Poisson rates. Compare not just final performance but generation patterns (token insertion sequences, reasoning quality) to isolate the contribution of hierarchical generation.

2. **Inference Latency Benchmarking**: Implement cached inference for the autoregressive baseline and measure wall-clock time for generating sequences of varying lengths (100, 500, 1000 tokens) with and without image insertion. Compare against OneFlow's non-cached inference to quantify the actual practical efficiency gains beyond FLOPs reduction.

3. **Robustness to Time Schedule Perturbations**: Design an experiment where the interleaved time schedule is systematically varied: (a) standard schedule with t_img ≤ t_text, (b) independent schedules, and (c) reversed schedule with t_img ≥ t_text. Evaluate image-text alignment quality using automated metrics (CLIP similarity between generated text context and inserted images) and qualitative analysis of generation coherence.