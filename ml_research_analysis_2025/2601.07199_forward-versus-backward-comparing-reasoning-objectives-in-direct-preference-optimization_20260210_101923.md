---
ver: rpa2
title: 'Forward versus Backward: Comparing Reasoning Objectives in Direct Preference
  Optimization'
arxiv_id: '2601.07199'
source_url: https://arxiv.org/abs/2601.07199
tags:
- training
- reasoning
- verification
- preference
- forward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how different training objectives affect
  reasoning reliability in large language models. Two training approaches are compared:
  forward chain-of-thought generation (training models to produce correct reasoning
  traces) and backward verification (training models to verify and acknowledge errors
  in candidate solutions).'
---

# Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization

## Quick Facts
- **arXiv ID:** 2601.07199
- **Source URL:** https://arxiv.org/abs/2601.07199
- **Reference count:** 18
- **Key outcome:** Forward-only DPO improves accuracy (+3.5pp) while backward-only DPO reduces false positive rate (13.4% → 4.3%) with minimal accuracy gains; both reduce error acknowledgement

## Executive Summary
This paper systematically compares forward chain-of-thought generation and backward verification training objectives in Direct Preference Optimization (DPO) for mathematical reasoning. Experiments on GSM8K show that forward-only training achieves the highest accuracy improvement (+3.5 percentage points), while backward-only training substantially reduces false positive rate with minimal accuracy gains. Both training variants reduce error acknowledgement rates compared to baseline, indicating that preference optimization increases model confidence. The findings suggest forward and backward reasoning objectives provide distinct learning signals: forward training improves problem-solving capability while backward training improves verification calibration.

## Method Summary
The study trains LLaMA 3.1 8B-Instruct models using LoRA adapters with DPO on GSM8K problems. Forward training uses preference pairs of correct versus incorrect reasoning traces, while backward training uses preference pairs of accurate versus inaccurate verification verdicts. Models are trained for one epoch with weighted DPO (β=0.1, ω=1.2 for real negative pairs). Evaluation measures accuracy, acknowledgement rate, false positive rate, and calibration F1. The complete training and evaluation pipeline is released for reproducibility.

## Key Results
- Forward-only DPO training achieves highest accuracy improvement (+3.5pp, from 83.1% to 86.6%)
- Backward-only training substantially reduces false positive rate (from 13.4% to 4.3%) with minimal accuracy gains
- Both training variants reduce acknowledgement rate compared to baseline (44.7% and 46.3% vs 67.8%), suggesting increased model confidence
- Forward training signal does not transfer directly to problem-solving capability when trained backward-only

## Why This Works (Mechanism)

### Mechanism 1: Forward Preference Optimization Improves Solution Generation
- **Claim:** Training on preference pairs of correct versus incorrect reasoning traces improves problem-solving accuracy.
- **Mechanism:** DPO directly optimizes the policy to increase likelihood of chosen (correct) traces while decreasing likelihood of rejected (incorrect) traces, without requiring a separate reward model. The model learns to produce reasoning patterns that lead to correct answers.
- **Core assumption:** Correct and incorrect reasoning traces contain learnable differences in structure or reasoning steps that generalize beyond the training distribution.
- **Evidence anchors:**
  - [abstract] "Forward-only DPO training achieves the highest accuracy improvement, increasing from 83.1% to 86.6% (+3.5 percentage points)"
  - [Section 5.1] "Forward-only DPO training achieves the highest accuracy at 86.6%, representing a 3.5 percentage point improvement over the baseline"
  - [corpus] Related work (MDPO) confirms multi-granularity DPO improves mathematical reasoning, supporting the general mechanism
- **Break condition:** If incorrect traces in preference pairs are artificially constructed rather than realistic model failures, the learned preference may not generalize to actual error modes.

### Mechanism 2: Backward Verification Training Calibrates Error Detection
- **Claim:** Training on verification verdicts improves the model's ability to correctly judge candidate solutions without improving its problem-solving capability.
- **Mechanism:** DPO trains the model to output PASS for correct answers and FAIL for incorrect ones. This optimizes a separate verification capability that operates on given inputs rather than generating solutions from scratch.
- **Core assumption:** Verification is a distinct cognitive skill from generation; learning to judge solutions does not automatically improve the ability to produce them.
- **Evidence anchors:**
  - [abstract] "backward-only training yields minimal accuracy gains but substantially reduces the false positive rate from 13.4% to 4.3%"
  - [Section 5.1] "Training exclusively on backward verification produces minimal impact on forward reasoning performance, confirming that backward training signal does not transfer directly to problem-solving capability"
  - [corpus] Weak direct evidence; neighbor papers focus on generation objectives rather than verification-objective isolation
- **Break condition:** If verification and generation share more underlying representations than assumed, separate training may yield unexpected transfer effects.

### Mechanism 3: Preference Optimization Increases Output Confidence
- **Claim:** DPO training systematically increases model confidence in outputs, reducing error acknowledgement rates even when uncertainty would be appropriate.
- **Mechanism:** DPO explicitly trains the model to prefer certain outputs over alternatives, which may reduce output diversity and increase certainty in selected responses. This generalizes beyond training examples to situations where the model should express uncertainty.
- **Core assumption:** The confidence effect is inherent to preference optimization rather than specific to the training data composition or hyperparameters.
- **Evidence anchors:**
  - [abstract] "Notably, both training variants reduce acknowledgement rate compared to the baseline, suggesting that preference optimization increases model confidence in its outputs"
  - [Section 6.1] "The baseline model, without any preference optimization, correctly flags 67.8% of its errors during backward verification. After training, both forward-only and backward-only models flag fewer errors (44.7% and 46.3% respectively)"
  - [corpus] Related work (Objective Matters: Fine-Tuning Objectives) examines how fine-tuning objectives shape model behavior, supporting that objective choice affects calibration
- **Break condition:** If the confidence effect is driven by the specific preference data distribution rather than DPO itself, alternative data curation strategies might preserve acknowledgement rates.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** Core training methodology. Unlike RLHF which learns a separate reward model, DPO directly optimizes policy on preference pairs using the reference policy ratio.
  - **Quick check question:** Can you explain why DPO eliminates the need for a reward model and what the β parameter controls?

- **Concept: Chain-of-Thought Reasoning**
  - **Why needed here:** Forward reasoning traces are chain-of-thought sequences from problem to solution. Understanding what makes a good CoT trace is essential for data generation.
  - **Quick check question:** What distinguishes a high-quality chain-of-thought trace from a shallow solution dump?

- **Concept: Verification Calibration vs. Accuracy**
  - **Why needed here:** The paper explicitly separates accuracy (getting the right answer) from calibration (knowing when you're wrong). These are distinct objectives with different training requirements.
  - **Quick check question:** Why might a model achieve high accuracy while having poor error acknowledgement?

## Architecture Onboarding

- **Component map:** LLaMA 3.1 8B-Instruct → LoRA adapters (rank=16, alpha=32, dropout=0.05) → DPO training → evaluation
- **Critical path:**
  1. Generate forward traces with rejection sampling until both correct and incorrect solutions obtained per problem
  2. Generate backward verification traces for each forward trace
  3. Construct preference pairs (correct vs. incorrect for forward; accurate vs. inaccurate verdict for backward)
  4. Train with weighted DPO for one epoch
  5. Evaluate using accuracy, acknowledgement rate, false positive rate, and calibration F1
- **Design tradeoffs:**
  - Forward-only: Maximizes accuracy (+3.5pp) but reduces error acknowledgement (44.7% vs 67.8% baseline)
  - Backward-only: Minimizes false positives (4.3%) but no accuracy gain
  - Single model vs. two-model system: Paper suggests combining forward-trained generator with backward-trained verifier may achieve both objectives
- **Failure signatures:**
  - High accuracy but low acknowledgement rate: Model is overconfident, may have been preference-optimized without calibration preservation
  - High false positive rate: Model rejects correct answers; may need backward verification training
  - No accuracy improvement from backward training: Expected behavior — verification skill does not transfer to generation
- **First 3 experiments:**
  1. **Replicate baseline metrics** on a held-out GSM8K subset to establish accuracy (83.1%), acknowledgement rate (67.8%), and FPR (13.4%) before any training.
  2. **Run forward-only DPO** with rejection-sampled preference pairs; verify accuracy improvement (~86.6%) and observe acknowledgement rate reduction.
  3. **Run backward-only DPO** on the same data split; confirm FPR reduction (~4.3%) and minimal accuracy change, validating the separation of training signals.

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on GSM8K limits generalizability to other reasoning domains
- Theoretical mechanisms for why forward/backward training affect different capabilities are not fully proven
- Confidence effects from preference optimization may not be inherent to DPO itself

## Confidence
- **High confidence** in the empirical observations (accuracy improvements, false positive rate reduction, acknowledgement rate changes) - these are directly measured from experiments with clear numerical results.
- **Medium confidence** in the proposed mechanisms - while the evidence supports the directional effects, the theoretical explanations are not fully proven.
- **Low confidence** in the transferability of findings to other domains or model scales - GSM8K and LLaMA 3.1 8B represent specific conditions.

## Next Checks
1. **Domain Transfer Validation**: Test the same forward/backward DPO training objectives on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference) to verify whether the observed effects persist across domains.
2. **Multi-Objective Training Experiment**: Train a single model using both forward and backward preference pairs simultaneously to determine whether this achieves the benefits of both approaches (accuracy improvement + reduced false positives) without the confidence degradation observed in separate training.
3. **Long-Context and Multi-Step Reasoning Test**: Evaluate the trained models on problems requiring longer reasoning chains or multiple solution attempts to determine whether the confidence effects scale with problem complexity, and whether backward training provides greater benefits for harder problems.