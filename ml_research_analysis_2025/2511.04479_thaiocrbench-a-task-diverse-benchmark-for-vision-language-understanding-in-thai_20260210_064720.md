---
ver: rpa2
title: 'ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in
  Thai'
arxiv_id: '2511.04479'
source_url: https://arxiv.org/abs/2511.04479
tags:
- tasks
- question
- answer
- document
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ThaiOCRBench, the first comprehensive benchmark
  for evaluating vision-language models on Thai text-rich visual tasks. The dataset
  includes 2,808 human-annotated samples across 13 diverse task categories such as
  chart parsing, table parsing, document classification, and handwritten content extraction.
---

# ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai

## Quick Facts
- arXiv ID: 2511.04479
- Source URL: https://arxiv.org/abs/2511.04479
- Reference count: 40
- Primary result: First comprehensive benchmark for evaluating vision-language models on Thai text-rich visual tasks

## Executive Summary
ThaiOCRBench introduces a novel benchmark for evaluating vision-language models on Thai text-rich visual tasks, addressing the gap in low-resource language document understanding. The benchmark comprises 2,808 human-annotated samples across 13 diverse task categories including chart parsing, table parsing, document classification, and handwritten content extraction. Systematic zero-shot evaluation reveals significant performance gaps between proprietary models like Gemini 2.5 Pro and open-source alternatives, with all models struggling notably on fine-grained text recognition and handwritten content tasks.

The benchmark provides actionable insights through detailed error analysis, identifying three main failure modes in open-source models: language bias and code-switching, structural mismatch, and hallucinated content. These findings establish ThaiOCRBench as a standardized framework for assessing VLMs in complex script environments and guide future improvements in Thai-language document understanding systems.

## Method Summary
ThaiOCRBench evaluates vision-language models through zero-shot inference on 2,808 human-annotated Thai document images across 13 task categories. The benchmark uses vLLM with greedy decoding and task-specific metrics including Tree Edit Distance for structural parsing, BMFL composite (average of BLEU, METEOR, F1, NLS) for text generation, F1 for key information extraction, and ANLS for VQA tasks. Models evaluated include both proprietary (Gemini 2.5 Pro, GPT-4o, Claude Sonnet 4) and open-source (Qwen2.5-VL, Gemma3, InternVL3) variants. The dataset and evaluation code are available at https://huggingface.co/datasets/scb10x/ThaiOCRBench and https://github.com/scb-10x/ThaiOCRBench respectively.

## Key Results
- Proprietary models (e.g., Gemini 2.5 Pro) significantly outperform open-source counterparts, achieving highest overall average score (0.777) and ranking first in 11 out of 13 tasks
- Among open-source models, Qwen2.5-VL 72B performs best but still shows substantial performance drops on fine-grained text recognition and handwritten content extraction
- Error analysis identifies three main failure modes: language bias/code-switching (0.87-5.48% depending on task/model), structural mismatch (47-55% of structural tasks), and hallucinated content with model-family-specific patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model scale correlates with reduced language bias and improved cross-modal instruction following in Thai script processing.
- Mechanism: Larger models likely have more robust multilingual tokenization and pretraining coverage, enabling better script fidelity when processing complex non-Latin scripts like Thai.
- Core assumption: The correlation between model size and reduced code-switching implies architectural capacity constraints rather than training data issues alone.
- Evidence anchors:
  - [abstract] "fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models"
  - [section 5.2] "larger models exhibit lower code-switching rates, indicating improved linguistic stability"
  - [corpus] Typhoon OCR paper explicitly addresses Thai script complexity and document extraction challenges

### Mechanism 2
- Claim: Proprietary models outperform open-source counterparts due to superior multilingual grounding and training data diversity.
- Mechanism: Proprietary systems likely incorporate targeted multilingual document corpora and optimized vision-language alignment strategies during training.
- Core assumption: The performance gap persists across model families and scales, suggesting training methodology differences beyond parameter count.
- Evidence anchors:
  - [abstract] "proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts"
  - [section 5.1] "Gemini 2.5 Pro achieves the highest overall average score (0.777), ranking first in 11 out of 13 tasks"
  - [corpus] Limited direct corpus evidence; related VMMU benchmark shows similar multilingual challenges

### Mechanism 3
- Claim: Model family-specific generation strategies create trade-offs between precision and recall in document understanding tasks.
- Mechanism: Different model families optimize for different generation objectives—some favor conservative outputs with more deletions, others prioritize comprehensive extraction with more insertions.
- Core assumption: The observed error patterns reflect deliberate design choices rather than random variation.
- Evidence anchors:
  - [section 5.2] "Gemma3 27B produces the highest overall deletion count (42.38)... reflecting a concise generation style"
  - [section 5.2] "Qwen2.5-VL-32B produces the highest number of insertions (172.48)"
  - [corpus] CropVLM paper addresses fine-grained perception limitations in VLMs

## Foundational Learning

- **Tree Edit Distance (TED) for hierarchical content evaluation**
  - Why needed here: Used to evaluate structural similarity between predicted and reference outputs in chart, table, and document parsing tasks where layout consistency matters more than exact text matching.
  - Quick check question: Can you explain why TED is more appropriate than character-level accuracy for evaluating whether a model correctly captured a table's row-column structure?

- **Character Error Rate (CER) decomposition into edit operations**
  - Why needed here: Enables fine-grained analysis of whether models are omitting content (deletions), adding spurious content (insertions), or misrecognizing characters (substitutions).
  - Quick check question: Given Thai's script complexity, why might deletion errors be more problematic than insertion errors for downstream applications?

- **Code-switching detection and language bias in multilingual VLMs**
  - Why needed here: The benchmark reveals that models often inappropriately mix Thai and English outputs or default to non-Thai responses, indicating fundamental grounding failures.
  - Quick check question: Why would code-switching be more prevalent in structured reasoning tasks like chart parsing compared to simple text recognition?

## Architecture Onboarding

- **Component map:** Thai document images → Vision encoder → Multimodal fusion layer → Language decoder → Structured output (JSON, Markdown, HTML)
- **Critical path:**
  1. Image preprocessing must preserve Thai script detail (headed vs. headless variants, diacritics)
  2. Vision encoder must capture fine-grained text regions (critical for fine-grained recognition tasks)
  3. Multimodal alignment must handle script-specific tokenization challenges
  4. Output generation must maintain language consistency (Thai-only responses for Thai inputs)

- **Design tradeoffs:**
  - **Scale vs. accessibility:** 72B models perform best but require significant compute; 3-7B models accessible but show 2-3x higher code-switching rates
  - **Precision vs. recall:** Model families show distinct tendencies—Gemma minimizes hallucinations through deletions, Qwen maximizes coverage at insertion cost
  - **Task-specific vs. unified metrics:** BMFL composite averages BLEU, METEOR, F1, NLS but may not capture task-specific semantic adequacy

- **Failure signatures:**
  - **Language bias:** Model defaults to English or mixes languages despite Thai input (prevalence: 0.87-5.48% depending on task/model)
  - **Structural mismatch:** Format-related errors in 47-55% of structural tasks; key-value extraction errors more scale-sensitive (4.10% at 7B vs. 1.15% at 72B)
  - **Content hallucination:** High insertion counts in table/document parsing (Qwen2.5-VL-32B: 41.24 insertions in document parsing vs. Gemma3 27B's higher deletions)

- **First 3 experiments:**
  1. **Establish baseline on subset:** Run zero-shot evaluation on 3 diverse tasks using 2-3 open-source models at different scales to characterize performance-scale relationship
  2. **Isolate language bias:** Test whether prompting with explicit language constraints ("Respond in Thai only") reduces code-switching rates
  3. **Diagnose structural failure modes:** Compare format-related vs. key-value extraction errors across tasks to determine if structural understanding requires different interventions

## Open Questions the Paper Calls Out

- **How would fine-tuning or instruction tuning on Thai-specific data affect VLM performance on ThaiOCRBench compared to zero-shot evaluation?**
  - Basis in paper: "our experiments are conducted exclusively under a zero-shot setting... This does not account for performance gains achievable through fine-tuning or instruction tuning"
  - Why unresolved: The benchmark only evaluates out-of-the-box generalization; no fine-tuning experiments were conducted.
  - What evidence would resolve it: Compare zero-shot vs. fine-tuned performance on ThaiOCRBench across multiple model families with controlled Thai-language training data.

- **Which LLM evaluator best correlates with human judgments for Thai document understanding tasks?**
  - Basis in paper: "the choice of which LLM to serve as the 'judge' remains an open question requiring further systematic analysis"
  - Why unresolved: Only GPT-4o-mini was tested; correlation with traditional metrics was moderate (Pearson 0.651, Spearman 0.559), and alignment with human judgment was not assessed.
  - What evidence would resolve it: Systematic comparison of multiple LLM judges against human annotations across ThaiOCRBench tasks.

- **How do the three identified failure modes differ in their sensitivity to model scaling versus training data composition?**
  - Basis in paper: Error analysis shows model-family-specific tendencies (Qwen overgenerates, Gemma omits), and code-switching decreases with scale, but the relative contribution of architecture vs. training data remains unexplored.
  - Why unresolved: The study compares model sizes but cannot disentangle whether failures stem from architectural limitations or Thai-language pretraining gaps.
  - What evidence would resolve it: Controlled experiments varying training data composition while holding architecture constant, or vice versa.

## Limitations
- Benchmark relies on zero-shot evaluation, potentially underestimating model capabilities achievable through fine-tuning or instruction tuning
- Dataset size (2,808 samples) may not capture full diversity of real-world Thai document scenarios, especially rare or domain-specific formats
- Proprietary models may have undisclosed training data including Thai content, potentially biasing comparison with open-source models

## Confidence

- **High Confidence**: The systematic performance gap between proprietary and open-source models is well-established and reproducible across multiple model families and task types
- **Medium Confidence**: The correlation between model scale and reduced language bias is observed consistently but may be confounded by training data differences rather than purely architectural capacity
- **Medium Confidence**: The trade-off between precision and recall across model families is well-documented but may reflect task-specific optimization strategies rather than fundamental architectural tendencies

## Next Checks

1. **Fine-tuning validation**: Replicate the zero-shot evaluation with the same open-source models after fine-tuning on a subset of ThaiOCRBench data to determine whether the performance gap with proprietary models is primarily due to training methodology versus architectural advantages

2. **Language constraint testing**: Systematically test whether explicit language constraints in prompts ("Respond in Thai only") reduce code-switching rates, or whether the issue persists at the tokenization/alignment layer regardless of prompting strategy

3. **Domain transfer assessment**: Evaluate model performance on ThaiOCRBench using models pre-trained on similar but distinct script systems (e.g., Lao, Khmer) to determine whether script-specific challenges or general multilingual document understanding capabilities drive performance differences