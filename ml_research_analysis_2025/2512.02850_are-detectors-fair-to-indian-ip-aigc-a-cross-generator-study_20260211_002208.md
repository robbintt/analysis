---
ver: rpa2
title: Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study
arxiv_id: '2512.02850'
source_url: https://arxiv.org/abs/2512.02850
tags:
- indian
- ip-aigc
- aide
- effort
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting identity-preserving
  AI-generated content (IP-AIGC) for Indian and South-Asian faces, a critical fairness
  issue as current detectors often underperform on under-represented populations and
  specific IP-AIGC scenarios. The authors conduct what they believe is the first systematic
  study of IP-AIGC detection for Indian faces, assembling Indian-focused training
  data from FairFD and HAV-DF, and constructing held-out test sets using commercial
  web-UI generators (Gemini and ChatGPT) with identity-preserving prompts.
---

# Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study

## Quick Facts
- **arXiv ID:** 2512.02850
- **Source URL:** https://arxiv.org/abs/2512.02850
- **Reference count:** 21
- **Primary result:** Fine-tuning improves in-domain AIGC detection for Indian faces but degrades performance on held-out identity-preserving AIGC, indicating overfitting to training-generator cues.

## Executive Summary
This paper addresses fairness in AIGC detection for under-represented populations by conducting what the authors believe is the first systematic study of IP-AIGC detection for Indian faces. They find that while fine-tuning state-of-the-art detectors on Indian-focused datasets significantly improves in-domain performance, it consistently degrades performance on held-out IP-AIGC generated by commercial models like Gemini and ChatGPT. This reveals a specific brittleness to identity-preserving edits rather than a generic distribution shift, highlighting the need for representation-preserving adaptation and India-aware benchmark curation to close generalization gaps.

## Method Summary
The study evaluates two SOTA detectors (AIDE and Effort) in pre-trained and fine-tuned regimes using Indian-focused training data from FairFD and HAV-DF. Training sets include FairFD-Indian (10308 real / 61678 fake) and HAV-DF-train (2444 real / 3759 fake). Testing uses HAV-DF-test, HIDF-img, HIDF-vid, plus held-out IP-AIGC sets (HIDF-img-ip-genai: 747 real / 535 fake; HIDF-vid-ip-genai: 142 real / 108 fake) generated via Gemini-2.5-Flash and ChatGPT web UIs with identity-preserving prompts. Evaluation metrics include Accuracy, AP, AUC, and EER. Fine-tuning uses specified hyperparameters (AIDE: batch 32, lr 1e-6; Effort: batch 40, lr 2e-4) on a Colab L4 GPU.

## Key Results
- Fine-tuned Effort achieves strong in-domain performance (AUC 0.944 on HAV-DF-test) but fails on held-out IP-AIGC (AUC 0.249 on HIDF-vid-ip-genai)
- Pre-trained Effort maintains better cross-generator generalization (AUC 0.986 on HIDF-img vs 0.740 on HIDF-img-ip-genai)
- AIDE fine-tuned performance drops from 0.923 to 0.563 AUC on HIDF-img-ip-genai
- The performance degradation is specific to IP-AIGC scenarios rather than generic distribution shifts

## Why This Works (Mechanism)

### Mechanism 1: Generator-Specific Artifact Overfitting During Fine-Tuning
Fine-tuning causes detectors to overfit to generator-specific artifacts in the training data, degrading performance on unseen generators. The model learns low-level generator signatures that are absent in held-out data, causing the decision boundary to fail on new IP-AIGC content.

### Mechanism 2: Specific Brittleness to Identity-Preserving Edits
IP-AIGC introduces more subtle, localized artifacts compared to standard synthetic content. These artifacts exist in a feature space that detectors are not calibrated for after fine-tuning on more pronounced artifacts, making identity-preserving edits inherently harder to detect.

### Mechanism 3: Representation Mismatch in Training Data
Training datasets inadequately represent the target population and the specific artifacts produced by state-of-the-art commercial generators for Indian subjects. This representation gap means the model's learned decision boundary is poorly placed for the target domain, especially for challenging IP-AIGC cases.

## Foundational Learning
- **Concept: Overfitting to Generator Artifacts** - Why needed: Core diagnosis for why fine-tuning improves in-domain but destroys cross-generator performance. Quick check: Would a detector trained only on GAN images generalize well to diffusion model images?
- **Concept: The IP-AIGC Challenge** - Why needed: IP-AIGC is a distinct, more difficult sub-problem. Quick check: Why might detecting face-swapped images be easier than detecting clothing changes with preserved faces?
- **Concept: Demographic Fairness in AIGC** - Why needed: Failures are not uniform and disproportionately affect under-represented populations. Quick check: How can a "diverse" dataset still fail to support fair performance for specific demographic groups?

## Architecture Onboarding
- **Component map:** Input datasets → AIDE/Effort detectors → Evaluation pipeline (Accuracy/AP/AUC/EER)
- **Critical path:** Fine-tuning loop: Data from FairFD/HAV-DF-train → Fine-tune AIDE/Effort → Evaluate on HAV-DF-test vs. held-out HIDF sets
- **Design tradeoffs:** Fine-tuning vs. generalization (fine-tuning provides in-domain gains but creates cross-generator failures)
- **Failure signatures:** High AUC on in-domain data coupled with near-random AUC on IP-AIGC test sets indicates generator overfitting
- **First 3 experiments:**
  1. Reproduce overfitting effect by training Effort on HAV-DF-train and evaluating on both HAV-DF-test and HIDF-img-ip-genai
  2. Ablate training data by adding small portion of IP-AIGC data to mixed dataset to test mitigation
  3. Test non-Indian IP-AIGC to isolate fairness aspect from general IP-AIGC difficulty

## Open Questions the Paper Calls Out
1. Can fairness-constrained adaptation methods maintain both in-domain performance and cross-generator IP-AIGC generalization?
2. How do controlled expression-only edits affect detector generalization compared to full identity-preserving AIGC edits?
3. Which specific training-generator cues or artifacts cause detectors to overfit and lose IP-AIGC generalization?
4. Does IP-AIGC brittleness extend to other under-represented populations beyond Indian/South-Asian faces?

## Limitations
- Small held-out IP-AIGC test sets (747 real / 535 fake images) may limit statistical robustness
- Prompt templates and generation procedures for held-out test sets are not fully specified
- Convergence criteria for Effort fine-tuning is not clearly defined
- Training data representativeness for target distribution is not explicitly validated

## Confidence
- **High confidence**: Core finding of fine-tuning improving in-domain but degrading on held-out IP-AIGC is well-supported
- **Medium confidence**: Generator-specific artifact overfitting mechanism is logically sound but needs feature attribution analysis
- **Low confidence**: Claim about specific brittleness to identity-preserving edits lacks direct corpus support

## Next Checks
1. Perform significance tests on performance differences between PT and FT models across all test sets
2. Apply feature attribution techniques (Grad-CAM, integrated gradients) to confirm FT models focus on generator-specific features
3. Generate held-out IP-AIGC test set for well-represented demographic using same generators to test generalization patterns