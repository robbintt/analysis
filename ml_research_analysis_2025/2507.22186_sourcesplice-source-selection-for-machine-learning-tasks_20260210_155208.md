---
ver: rpa2
title: 'SourceSplice: Source Selection for Machine Learning Tasks'
arxiv_id: '2507.22186'
source_url: https://arxiv.org/abs/2507.22186
tags:
- sources
- subset
- data
- source
- sourcesplice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting a subset of data
  sources to construct an effective training dataset for machine learning tasks. The
  authors propose two algorithms, SourceGrasp and SourceSplice, which iteratively
  select sources based on their contribution to task utility.
---

# SourceSplice: Source Selection for Machine Learning Tasks

## Quick Facts
- arXiv ID: 2507.22186
- Source URL: https://arxiv.org/abs/2507.22186
- Reference count: 40
- Key outcome: SourceSplice identifies high-utility source subsets with up to 16× fewer model trainings than baselines while achieving comparable or better performance.

## Executive Summary
This paper addresses the problem of selecting an optimal subset of data sources to construct an effective training dataset for machine learning tasks. The authors propose SourceSplice, an algorithm inspired by gene splicing that iteratively evolves an initial set of sources into the optimal subset. The approach is compared against SourceGrasp (a GRASP metaheuristic) and other baseline methods on real-world ACS PUMS datasets. Experimental results demonstrate that SourceSplice can efficiently identify subsets of sources leading to high task utility with significantly fewer subset explorations, requiring up to 16 times fewer model trainings to find the best solution while achieving comparable or better performance in terms of subset percentile and profit improvement.

## Method Summary
SourceSplice is a source selection algorithm that iteratively refines an active set of data sources to maximize task utility (profit). The method initializes by ranking sources by individual profit and populating an initial active set. It then computes marginal valuations for active sources (rmVal) and inactive sources (addVal) by training models and evaluating the impact of adding or removing each source. The algorithm swaps the least valuable active source with the most valuable inactive source, repeating this process until the active set stabilizes (no further swaps improve profit). The approach is evaluated against SourceGrasp (GRASP metaheuristic) and DsDM on ACS PUMS datasets using logistic regression for classification and linear regression for MSE tasks, with profit defined as G(S) - C(S) where G is task utility and C is subset cost.

## Key Results
- SourceSplice requires up to 16× fewer model trainings than SourceGrasp to find optimal subsets
- Achieves comparable or better performance in terms of subset percentile and profit improvement
- Demonstrates efficient search by exploring <1% of possible subsets in experiments
- Maintains effectiveness across different cost functions (linear, quadratic, step functions)

## Why This Works (Mechanism)

### Mechanism 1: Marginal Utility Gradient Ascent
SourceSplice identifies high-utility subsets by valuing sources based on their marginal contribution to the current active set. It computes addVal(s) for inactive sources (marginal benefit of inclusion) and rmVal(s) for active sources (marginal cost of exclusion), then swaps the least valuable active source with the most valuable inactive source. This performs guided ascent on the profit landscape, assuming source contributions are interdependent rather than strictly additive.

### Mechanism 2: Active Set Stabilization (Fixed Point Iteration)
The algorithm converges to a stable optimal subset by iteratively refining an "active set" until further swaps yield no profit increase. The fixedSupport procedure repeatedly invokes the splicing logic, continuing as long as the active set changes (S_prev ≠ S_curr). This ensures convergence to a local equilibrium where the active set is satisfied with its current composition, assuming the search space is smooth enough to avoid oscillation.

### Mechanism 3: Resource-Efficient Search Pruning
SourceSplice achieves efficiency by restricting the search space to the neighborhood of the current active set, avoiding the exponential explosion of evaluating all 2^m subsets. Unlike SourceGrasp's randomized construction or Naïve search, it limits exploration to specific swap operations defined by the k_max parameter, explicitly tracking "Models explored" to demonstrate finding optimal solutions by evaluating a tiny fraction of the possibility space.

## Foundational Learning

- **Concept: Submodularity and Marginal Gain** - Understanding that P(A ∪ s) ≠ P(A) + P(s) is critical to seeing why addVal must be computed dynamically based on the current active set rather than source standalone quality. Quick check: If source A has 80% accuracy and source B has 80% accuracy, is the accuracy of {A, B} guaranteed to be 80%? (No, it depends on data overlap/distribution).

- **Concept: Metaheuristics (GRASP vs. Local Search)** - To differentiate SourceGrasp (randomized multi-start) from SourceSplice (iterative refinement). Quick check: Why might a "greedy" approach get stuck in a local optimum compared to a "randomized" search? (Greedy follows the steepest immediate slope; randomization allows jumping to new regions).

- **Concept: Profit vs. Gain (Cost-aware Optimization)** - The paper explicitly defines Profit P(S) = G(S) - C(S), optimizing for net benefit rather than raw model performance. Quick check: If a source increases model accuracy by 1% but costs $1M, would SourceSplice likely include it? (Likely no, unless the gain function G is scaled to outweigh the cost C).

## Architecture Onboarding

- **Component map:** Source List S & Cost Function C -> Initialization Module (ranks by profit, populates Active Set A) -> Valuation Engine (computes rmVal and addVal via model training) -> Splicing Operator (swaps sources) -> Convergence Monitor (checks A_new == A_old)

- **Critical path:** The fixedSupport loop (Algorithm 2, lines 9-15). This is where the majority of model trainings occur, making it the primary target for optimization.

- **Design tradeoffs:** s_max (Max Subset Size) - higher values find better solutions but linearly increase runtime; k_max (Max Swaps) - determines search aggressiveness; Initialization - using top-i greedy sources provides warm start but may bias away from synergistic subsets.

- **Failure signatures:** Empty Active Set (costs too high relative to gains), Oscillation (identical utility profiles cause swapping), Stagnation (negative addVal and positive rmVal prevent progress).

- **First 3 experiments:** 1) Smoke Test - run SourceSplice on synthetic dataset with known optimal subset; 2) Efficiency Baseline - measure "Models Explored" vs "Subset Percentile" on medium dataset comparing against Naïve search; 3) Cost Sensitivity - rerun with linear vs quadratic cost functions to observe profit improvement changes.

## Open Questions the Paper Calls Out
1. Can the gain of source subsets be estimated accurately without retraining the model for every candidate? (Section 7 identifies this as primary future research).
2. How can internal model details (gradients, embeddings) be utilized to expedite source utility estimation? (Suggested as deferred research direction).
3. How can source selection be integrated with upstream data preparation tasks like cleaning and feature engineering? (Identified as future work).

## Limitations
- The convergence guarantees for the splicing procedure are not formally proven
- Performance on smaller datasets (<15 sources) is not characterized, limiting scalability understanding
- The impact of cost functions beyond linear case is mentioned but not deeply explored

## Confidence
- Efficiency claims: High (supported by concrete experimental metrics)
- Mechanism claims: Medium (logically coherent but rely on unstated assumptions)
- Foundational learning: Medium (pedagogically sound but needs more explicit submodularity treatment)

## Next Checks
1. Instrument SourceSplice to log active set changes per iteration and verify convergence within bounded steps across all experimental datasets
2. Replicate experiments with quadratic and step-function cost models to determine if 16× efficiency improvement holds under cost heterogeneity
3. Compare SourceSplice's performance when initialized with random subsets versus greedy top-i initialization to quantify warm start impact