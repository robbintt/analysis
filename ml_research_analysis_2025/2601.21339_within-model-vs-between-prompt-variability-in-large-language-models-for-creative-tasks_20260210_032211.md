---
ver: rpa2
title: Within-Model vs Between-Prompt Variability in Large Language Models for Creative
  Tasks
arxiv_id: '2601.21339'
source_url: https://arxiv.org/abs/2601.21339
tags:
- prompt
- variance
- prompts
- creative
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantifies how variance in LLM creative outputs partitions
  across prompt, model, and within-LLM sources. Using 12 models, 10 creativity prompts,
  and 100 samples each (12,000 total generations), it reveals that for output quality
  (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%),
  while for output quantity (fluency), model choice (51.25%) and within-LLM variance
  (33.70%) dominate with prompts explaining only 4.22%.
---

# Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks

## Quick Facts
- arXiv ID: 2601.21339
- Source URL: https://arxiv.org/abs/2601.21339
- Reference count: 40
- Primary result: Prompts explain 36.43% of output quality variance vs. model choice at 40.94%; for output quantity, model choice dominates at 51.25% with prompts at only 4.22%

## Executive Summary
This study quantifies how variance in LLM creative outputs partitions across prompt, model, and within-LLM sources. Using 12 models, 10 creativity prompts, and 100 samples each (12,000 total generations), it reveals that for output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%), while for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate with prompts explaining only 4.22%. Strategic prompts like "Creative Persona" can elicit high-quality responses from capable models, but the substantial within-LLM variance (10-34%) means single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

## Method Summary
The study uses 12 LLMs (proprietary APIs + open-weights via Ollama) with 10 prompts across 3 categories (baseline, strategic, minor variations) to generate 100 samples each per model-prompt pair (N=12,000). Ideas are collected for the Alternate Uses Task (plastic bottle) with JSON output format enforcement. Originality is scored via automated AUT scoring (1-5 scale), fluency is counted as idea count, and variance decomposition is performed using linear mixed-effects models with REML estimation and bootstrap confidence intervals.

## Key Results
- Prompts explain 36.43% of originality variance, comparable to model choice (40.94%)
- For fluency, model choice explains 51.25% of variance while prompts explain only 4.22%
- Within-LLM variance ranges from 10-34% depending on metric and prompt
- Persona prompt yields highest between-model variance (0.112 vs. baseline P1 variance of 0.0337)

## Why This Works (Mechanism)

### Mechanism 1: Discriminative Capability Elicitation
Strategic prompts (e.g., Persona) act as capability stress-tests that reveal latent model differences rather than uniformly improving performance. Identity-based framing shifts the output distribution upward for models capable of contextual self-modification, but also expands variance. Capable models respond with large effect sizes (Cohen's d > 2.0), while less capable models show negligible improvement.

### Mechanism 2: Structural Constraint Collapse
Format constraints reduce semantic diversity by restricting the syntactic search space, inadvertently causing idea convergence. Compliance with formatting rules ("no titles or colons") forces models to abandon default output structures, collapsing toward high-probability phrasings and reducing uniqueness.

### Mechanism 3: Metric-Divergent Variance Partitioning
Variance partitions differently for output quality (originality) vs. quantity (fluency), with prompts controlling quality but model choice and stochasticity controlling quantity. Prompts steer the semantic center of the output distribution (originality), but the breadth of exploration (fluency) is constrained by model architecture, alignment, and sampling dynamics.

## Foundational Learning

- **Variance Decomposition via Linear Mixed-Effects Models**: Needed to partition output variance into prompt, model, interaction, and residual (within-LLM) components; if you run the same prompt on the same model 100 times and get different outputs, where does that variance appear in the model?

- **Conditional Distribution p(y|x)**: LLMs implement probability distributions over outputs, not deterministic mappings; sampling yields realizations, not fixed responses; why does setting temperature=0 not guarantee deterministic output according to the paper?

- **The Sampling Fallacy (N=1 Evaluation)**: Single-generation evaluation conflates sampling noise with genuine prompt/model effects; the paper quantifies this risk at 10-34% of total variance; if within-LLM variance is 33.7% for fluency, what is the minimum sample size needed to distinguish prompt effects from noise?

## Architecture Onboarding

- **Component map**: Stimuli Layer (10 prompts) -> Model Layer (12 LLMs) -> Sampling Layer (100 samples) -> Evaluation Layer (AUT scoring) -> Analysis Layer (LMM with REML)

- **Critical path**: Define prompt variants and validation criteria → Configure inference with provider defaults → Collect 100 samples per condition with JSON-enforced output format → Parse and score ideas using automated AUT pipeline → Fit mixed-effects model: Y_mpr = μ + α_m + β_p + (αβ)_mp + ε_mpr → Compute ICCs via bootstrap resampling

- **Design tradeoffs**: Defaults maximize ecological validity but conflate architecture+alignment+settings; single-task depth enables variance quantification but limits generalizability; automated scoring enables scale but introduces verbosity bias

- **Failure signatures**: Mode collapse (high repetition rates), format non-compliance (JSON parsing failures), rating timeout (scoring service failures), rank instability (high variance across prompts)

- **First 3 experiments**: Baseline variance quantification (3 models, 50 samples each) → Discriminative prompt test (baseline vs. persona on 2 models) → Sample complexity curve (plot mean vs. sample size N=1,5,10,25,50,100)

## Open Questions the Paper Calls Out
None

## Limitations
- Single creative task (plastic bottle AUT) limits generalizability across domains
- Automated evaluation introduces potential biases through length-originality correlation
- Sampling assumptions may underestimate true within-model variance due to GPU parallelism

## Confidence

**High confidence** (direct variance decomposition evidence):
- Prompts explain 36.43% of originality variance vs. model choice at 40.94%
- Model choice explains 51.25% of fluency variance vs. prompts at 4.22%
- Within-LLM variance ranges from 10-34% depending on metric and prompt

**Medium confidence** (mechanism supported but single-task dependent):
- Strategic prompts act as discriminative capability elicitors
- Format constraints cause coupled semantic-syntactic collapse
- Quality vs. quantity variance partitioning reflects different control mechanisms

**Low confidence** (requires additional validation):
- Single-sample evaluations reliably conflate noise with genuine effects
- Persona prompt mechanism generalizes across identity-based framing strategies
- Variance partitioning patterns hold across creative task domains

## Next Checks

1. **Cross-task variance generalization**: Replicate variance decomposition on 3-5 different creative tasks (story generation, product design, metaphor creation) using same 12 models and 100-sample protocol to verify 36.43% vs 40.94% prompt-model split for originality.

2. **Controlled hyperparameter sweep**: For 2 representative models, systematically vary temperature (0.0, 0.5, 1.0, 1.5) and top-p (0.5, 0.9, 0.95) while keeping prompt constant to quantify hyperparameter variation vs. prompt and model variance components.

3. **Multi-sample evaluation protocol validation**: Implement framework where models are judged on 5-10 samples per prompt rather than 1 to test whether this reduces rank instability and improves reliability of between-model comparisons.