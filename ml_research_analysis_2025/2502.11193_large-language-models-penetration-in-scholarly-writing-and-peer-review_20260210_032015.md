---
ver: rpa2
title: Large Language Models Penetration in Scholarly Writing and Peer Review
arxiv_id: '2502.11193'
source_url: https://arxiv.org/abs/2502.11193
tags:
- review
- llm-generated
- data
- reviews
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study reveals increasing Large Language Model (LLM) penetration
  in scholarly writing and peer review, raising concerns about academic credibility.
  Researchers developed a two-component framework: ScholarLens, a curated dataset
  of human-written and LLM-generated scholarly content, and LLMetrica, a tool combining
  rule-based linguistic metrics and model-based detectors to assess LLM usage.'
---

# Large Language Models Penetration in Scholarly Writing and Peer Review

## Quick Facts
- arXiv ID: 2502.11193
- Source URL: https://arxiv.org/abs/2502.11193
- Reference count: 40
- Key outcome: LLM penetration is rising in scholarly writing and peer review, with detection tools achieving up to 98% F1 accuracy

## Executive Summary
This study presents evidence of increasing LLM penetration in scholarly writing and peer review, highlighting both the need for detection tools and the risks to academic integrity. Researchers developed ScholarLens, a dataset of human-written and LLM-generated scholarly content, and LLMetrica, a framework for detecting LLM usage through rule-based linguistic metrics and model-based detectors. Results show that LLM-generated texts exhibit consistent linguistic patterns across domains, and detectors trained on scholarly data outperform general baselines. Temporal analysis reveals rising LLM adoption post-2023, with refined LLM roles dominating in peer reviews.

## Method Summary
The study introduces ScholarLens, a dataset containing 2,831 ICLR papers (pre-2019) with human-written abstracts, reviews, and meta-reviews, paired with LLM-generated counterparts from GPT-4o, Gemini-1.5, and Claude-3. LLMetrica combines rule-based linguistic metrics (e.g., AWL, LWR, FRE, sentiment) and model-based detectors (Longformer fine-tuned on ScholarLens). The framework achieves up to 98% F1 accuracy in binary classification of human vs. LLM text, with detectors trained on scholarly data outperforming general baselines. Temporal analysis applies these metrics to published abstracts and reviews to estimate LLM penetration trends.

## Key Results
- LLM-generated texts show consistent linguistic patterns: longer words, lower readability, and more positive sentiment across scholarly domains
- Model-based detectors trained on scholarly data achieve up to 98% F1, outperforming general baselines (52-78% F1)
- LLM penetration in abstracts and reviews increased post-2023, especially after ChatGPT's release, with refined LLM roles dominating in peer reviews

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Fingerprint Detection
- Claim: Rule-based linguistic metrics can distinguish LLM-generated scholarly text from human-written text with consistent directional changes across multiple features.
- Mechanism: The framework captures measurable statistical differences in writing patterns—specifically, LLMs tend to produce longer words (higher AWL, LWR), lower readability (lower FRE), reduced stopword ratios, and more positive sentiment. These patterns emerge from training data regularization and optimization objectives that favor certain linguistic constructions.
- Core assumption: LLMs exhibit consistent, detectable biases in text generation that differ from human writing patterns and persist across scholarly domains.
- Evidence anchors:
  - [abstract]: "LLM-generated texts exhibit consistent linguistic patterns—such as longer words, lower readability, and more positive sentiment—across different scholarly domains."
  - [section] §5.1: "regardless of the data type or LLM used, LLM-generated texts consistently show higher values for Average Word Length (AWL) and Long Word Ratio (LWR), and lower values for Stopword Ratio (SWR) and Readability (FRE)"
  - [corpus]: Corpus shows related work on LLM detection in peer review (e.g., "Detecting AI-Generated Content in Academic Peer Reviews") but limited direct validation of the specific linguistic metrics used here.
- Break condition: If LLMs are specifically fine-tuned on human-written scholarly text with stylistic constraints, or if human authors begin unconsciously mimicking LLM patterns, the statistical separation would diminish.

### Mechanism 2: Domain-Specific Detector Fine-tuning
- Claim: Training detectors on scholarly-domain data (ScholarLens) substantially improves detection accuracy over general-purpose detectors.
- Mechanism: General detectors trained on diverse web text fail to capture domain-specific patterns. By creating paired human/LLM data specifically for abstracts, reviews, and meta-reviews, the Longformer-based ScholarDetect models learn subtle distinctions in scholarly writing conventions, citation patterns, and argumentative structures that general detectors miss.
- Core assumption: The linguistic patterns distinguishing human from LLM text in scholarly domains differ sufficiently from general text to require specialized training.
- Evidence anchors:
  - [abstract]: "Model-based detectors trained on this data achieved high detection accuracy (F1 scores up to 98%), consistently outperforming existing baselines."
  - [section] Table 1: ScholarDetect models achieve 94-99% F1 on abstracts/meta-reviews vs. 52-78% for baselines like RAIDetect and HNDCDetect
  - [corpus]: Related papers like "LLM-REVal" and "Reviewriter" explore LLM roles in review but don't report comparable detection benchmarks.
- Break condition: If domain-specific training data contains systematic biases (e.g., all LLM text from a single model version), detectors may overfit to narrow patterns and fail on new LLM versions or hybrid human-LLM workflows.

### Mechanism 3: Temporal Penetration Tracking via Feature Shifts
- Claim: Tracking aggregate linguistic feature changes in published academic text over time can estimate LLM adoption rates even without per-document labels.
- Mechanism: By applying the validated linguistic metrics to corpora across years, the framework detects population-level shifts consistent with LLM penetration. The post-2023 inflection aligns with ChatGPT's release, and different patterns in abstracts (refinement) vs. reviews (synthesis) reveal distinct LLM roles.
- Core assumption: The observed population-level feature shifts are primarily driven by LLM adoption rather than other temporal confounds (e.g., changing author demographics, editorial policies).
- Evidence anchors:
  - [abstract]: "Temporal analysis indicated rising LLM penetration in abstracts and reviews, especially post-2023, with refined LLM roles dominating in peer reviews."
  - [section] Figure 4/6/7: Shows year-over-year increases in LLM-associated linguistic features starting 2023, with ScholarDetect predicting highest penetration rates in 2024
  - [corpus]: Corpus includes "Detecting AI-Generated Content in Academic Peer Reviews" examining temporal emergence—suggests this is an active research direction but methods and findings vary.
- Break condition: If editorial policies or author awareness campaigns shift human writing patterns independently, or if LLMs become standard accepted tools with disclosure, the signal interpretation becomes confounded.

## Foundational Learning

- Concept: Supervised Text Classification with Class Imbalance
  - Why needed here: ScholarDetect trains binary classifiers on paired human/LLM text. Understanding F1 vs. accuracy, train/test splits, and class balancing (the paper uses 1:1 ratio) is essential for interpreting Table 1 results.
  - Quick check question: Why might a detector achieve high overall accuracy but still fail to catch most LLM-generated text if the test set is 90% human-written?

- Concept: Statistical Hypothesis Testing for Feature Selection
  - Why needed here: The two-sample t-test approach (Appendix E.1) identifies LLM-preferred words by comparing proportions across corpora. This validates the linguistic metrics empirically.
  - Quick check question: If you run 10,000 word-level t-tests with α=0.05, how many significant results would you expect by chance alone, and how might you correct for this?

- Concept: Cross-Domain Generalization in NLP
  - Why needed here: A key finding is that general detectors (MAGE, RAIDetect) underperform on scholarly text. Understanding why models trained on one domain fail on another prevents misapplication.
  - Quick check question: A detector trained on GPT-4 news articles achieves 95% F1. Would you expect similar performance on Claude-generated peer reviews? Why or why not?

## Architecture Onboarding

- Component map:
  ScholarLens Dataset -> LLMetrica Framework -> Binary Classification + Linguistic Analysis
  ├── Human-written: ICLR abstracts/reviews/meta-reviews (pre-2019)
  └── LLM-generated: GPT-4o, Gemini-1.5, Claude-3 versions
       ├── Refined (abstracts): Human text → LLM polish
       └── Synthesized (reviews/meta): Source paper → LLM generation

  LLMetrica Framework
  ├── Rule-Based Metrics
  │   ├── General Linguistic: AWL, LWR, SWR, TTR, ASL, DRV, SCD, FRE, PS, SS
  │   └── Specific Semantic: MRSim, RSim, SF-IRF (review-specific)
  └── Model-Based Detectors
      └── ScholarDetect: Longformer fine-tuned on ScholarLens
           ├── Variants: Abs-only, Meta-only, Hybrid
           └── Training: Mixed-LLM sources for robustness

- Critical path:
  1. Data curation → 2. Metric definition/validation → 3. Detector training → 4. Temporal analysis application

- Design tradeoffs:
  - Pre-2019 data ensures human-only baseline but may not reflect current writing norms
  - Mixed-LLM training improves generalization but may dilute model-specific signatures
  - Focus on ICLR (open review) limits generalization to closed-review venues (acknowledged in Limitations)

- Failure signatures:
  - High false positive rate on non-native English speakers (not tested in paper)
  - Degraded performance on heavily edited LLM outputs (human-LLM hybrid)
  - Baseline detectors failing on long-form reviews (Table 1 shows 25-54% F1)

- First 3 experiments:
  1. Replicate the linguistic feature comparison on a small held-out corpus to validate reproducibility of AWL, LWR, SWR, FRE differences
  2. Train a minimal ScholarDetect variant on abstracts only and compare cross-domain performance vs. baseline to understand domain adaptation gains
  3. Apply the four robust linguistic metrics to a recent conference (2024-2025) not in training data to test temporal drift assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can detection models be refined to distinguish between acceptable LLM assistance (e.g., language polishing) and unacceptable use (e.g., content generation) in peer review?
- Basis in paper: [explicit] The authors suggest developing "fine-grained LLM detection models" to differentiate acceptable roles like "language improvement vs. content creation."
- Why unresolved: Current detection frameworks, including LLMetrica, primarily operate on binary classifications (human vs. LLM) or broad roles (synthesized vs. refined) without granular semantic analysis of contribution types.
- What evidence would resolve it: A labeled dataset categorizing various types of LLM interventions (grammar, style, argument synthesis) and metrics that quantify the extent of conceptual contribution versus stylistic alteration.

### Open Question 2
- Question: To what extent do current detection frameworks generalize to complex human-in-the-loop workflows where humans heavily edit machine-generated drafts?
- Basis in paper: [explicit] The Limitations section notes the "data simulation may not fully capture the intricate dynamics of LLM-human collaboration," potentially lowering detection ability in real-world scenarios.
- Why unresolved: The ScholarLens dataset utilizes direct generation or single-step refinement, failing to simulate the iterative editing cycles common in actual academic writing.
- What evidence would resolve it: Analysis of longitudinal document version histories containing iterative human edits on LLM-generated text to test detector robustness against hybrid authorship.

### Open Question 3
- Question: Is LLM penetration significantly higher in closed peer review systems (e.g., private journals) compared to the open review systems analyzed?
- Basis in paper: [explicit] The authors state that "in many journals and conferences where peer review remains closed, the penetration of LLMs could be even more pronounced" than observed in ICLR.
- Why unresolved: Data access restrictions and the proprietary nature of journal peer review processes prevent the application of tools like LLMetrica to these closed venues.
- What evidence would resolve it: Collaborative studies with private journals to anonymously apply LLMetrica to their review archives to compare penetration rates against the ICLR baseline.

## Limitations

- Generalizability limited to ICLR as a single conference venue, with uncertain applicability to closed-review systems
- Temporal analysis cannot definitively attribute linguistic feature shifts to LLM adoption versus other confounding factors
- Detection accuracy may degrade for hybrid human-LLM workflows not captured in the training data

## Confidence

- Linguistic fingerprint detection mechanism: **High** - Consistent directional patterns across multiple features and scholarly domains are empirically demonstrated
- Domain-specific detector performance: **Medium** - Strong F1 scores on held-out test sets, but real-world generalization to unseen venues and LLM versions remains uncertain
- Temporal penetration estimates: **Low-Medium** - While post-2023 inflection aligns with ChatGPT release, causal attribution to LLM adoption versus other temporal factors cannot be definitively established

## Next Checks

1. Test ScholarDetect on peer review data from venues with different review processes (closed vs. open) to assess cross-venue generalization
2. Evaluate detection performance on human-LLM hybrid texts where human authors heavily edit LLM-generated drafts
3. Apply the framework to a 2024-2025 conference outside the training data to validate temporal predictions and detect potential drift in LLM writing patterns