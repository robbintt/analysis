---
ver: rpa2
title: Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum
arxiv_id: '2508.04293'
source_url: https://arxiv.org/abs/2508.04293
tags:
- nirmal
- momentum
- adam
- learning
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum

## Quick Facts
- arXiv ID: 2508.04293
- Source URL: https://arxiv.org/abs/2508.04293
- Authors: Nirmal Gaud; Surej Mouli; Preeti Katiyar; Vaduguru Venkata Ramya
- Reference count: 1
- Primary result: NIRMAL achieves 92.36% test accuracy on FashionMNIST, 68.91% on CIFAR-100, with ablation suggesting stochastic components improve complex dataset performance

## Executive Summary
This paper introduces NIRMAL, a novel optimizer that combines five distinct update strategies inspired by chess piece movements into a weighted ensemble. The algorithm aggregates direct gradient descent, momentum, adaptive scaling, stochastic perturbations, and non-linear momentum transformations. Experimental results show NIRMAL outperforms Adam and SGD with Momentum on CIFAR-100 (68.91% vs 67.44% Adam) and achieves competitive results on FashionMNIST (92.36%), though it underperforms on CIFAR-10 (80.52% vs 85.26% Adam). The optimizer's performance gains appear most pronounced on complex, high-dimensional datasets where landscape exploration becomes critical.

## Method Summary
NIRMAL combines five update components—Wazir (direct gradient), Elephant (momentum), Knight (stochastic noise), Camel (adaptive scaling), and Horse (non-linear momentum)—into a weighted sum for parameter updates. The optimizer maintains exponential moving averages of gradients and squared gradients like Adam, then computes five distinct update vectors that are linearly combined using fixed weights. Training uses standard image classification datasets (MNIST, FashionMNIST, CIFAR-10, CIFAR-100) with batch size 64, 10 epochs, and data augmentation for CIFAR datasets. The optimizer introduces additional hyperparameters including component weights and scaling factors beyond standard learning rate and decay.

## Key Results
- NIRMAL achieves 92.36% test accuracy on FashionMNIST, outperforming Adam (92.14%) and SGD+M (91.28%)
- On CIFAR-100, NIRMAL reaches 68.91% accuracy versus Adam's 67.44%, suggesting benefits for complex datasets
- On CIFAR-10, NIRMAL underperforms Adam (80.52% vs 85.26%), indicating potential drawbacks on simpler datasets
- The ablation study shows the stochastic Knight component provides measurable benefits on complex datasets

## Why This Works (Mechanism)

### Mechanism 1: Weighted Multi-Strategy Aggregation
- **Claim:** NIRMAL balances bias-variance tradeoff by treating optimization as a weighted ensemble of distinct update logics
- **Mechanism:** Computes five separate update vectors and calculates final parameter change as weighted linear combination
- **Core assumption:** Loss landscape benefits from simultaneous application of exploratory noise and exploitative momentum
- **Evidence anchors:** Abstract mentions "combines multiple strategies inspired by movements of chess piece"; Section 3.1 defines weighted sum of components
- **Break condition:** Misconfigured component weights may cause stagnation where noise cancels directional momentum

### Mechanism 2: Stochastic Perturbation for Landscape Exploration
- **Claim:** Explicit Gaussian noise injection aids escaping sharp minima or saddle points in high-dimensional spaces
- **Mechanism:** Knight component adds scaled random normal value to update
- **Core assumption:** Standard gradient direction insufficient; random perturbations provide necessary exploratory bias
- **Evidence anchors:** Abstract mentions "stochastic perturbations"; Section 3.1 describes Knight introducing noise to escape local minima
- **Break condition:** Excessive perturbation scale causes failure to converge due to high variance

### Mechanism 3: Non-Linear Momentum Transformation
- **Claim:** Tanh squashing function creates self-regulating velocity cap for improved stability
- **Mechanism:** Horse component bounds update step size regardless of accumulated momentum magnitude
- **Core assumption:** Unbounded momentum can lead to overshooting in complex landscapes
- **Evidence anchors:** Abstract mentions "non-linear transformations"; Section 3.1 defines tanh transformation applied to momentum term
- **Break condition:** Tanh saturation slows convergence when rapid large jumps are required

## Foundational Learning

- **Concept: Exponential Moving Average (EMA)**
  - **Why needed here:** NIRMAL relies on two distinct EMAs for first moment (momentum) and second moment (adaptive scaling)
  - **Quick check question:** Can you explain how decay rates μ=0.9 (short memory) and β=0.999 (long memory) differently affect optimizer's sensitivity to recent gradient spikes?

- **Concept: Loss Landscape Geometry**
  - **Why needed here:** Claims improved performance on complex datasets (CIFAR-100) versus simpler ones (MNIST) requires understanding saddle points vs local minima
  - **Quick check question:** Why would adding random noise be detrimental on convex loss surface but beneficial on highly non-convex surface?

- **Concept: Hyperparameter Coupling**
  - **Why needed here:** NIRMAL introduces 5 component weights and 3 scaling factors beyond standard hyperparameters
  - **Quick check question:** If you increase weight of Wazir (direct gradient), how might you need to adjust global learning rate η to maintain stable loss convergence?

## Architecture Onboarding

- **Component map:** Gradients → Momentum EMA → Variance EMA → Five Update Components (Wazir, Elephant, Knight, Camel, Horse) → Weighted Sum → Parameters
- **Critical path:** Calculating m_t and v_t first; five update components are embarrassingly parallel before final weighted reduction
- **Design tradeoffs:** Requires storing two state vectors like Adam but incurs higher FLOPs per step; 5 weights provide versatility but create combinatorial tuning burden
- **Failure signatures:** Divergence on simple data suggests noise or saturation interfering with gradient signals; over-smoothing indicates Horse weight too high
- **First 3 experiments:**
  1. Ablation Study: Run NIRMAL on CIFAR-100 with w_knight=0 to isolate stochastic noise contribution
  2. Baseline Reproduction: Implement from scratch using Section 3.1 equations on FashionMNIST to verify 92.36% accuracy
  3. Hyperparameter Sensitivity: Grid search on Camel (γ) and Horse (λ) scaling factors for different architectures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the theoretical convergence guarantees for the NIRMAL optimizer?
- **Basis in paper:** [explicit] Conclusion states need for "Investigating the theoretical convergence properties... of each 'chess piece' inspired component"
- **Why unresolved:** Current study is purely empirical without mathematical proofs of convergence
- **What evidence would resolve it:** Formal mathematical proofs demonstrating convergence rates in convex and non-convex settings

### Open Question 2
- **Question:** How does removal or adjustment of individual components affect performance?
- **Basis in paper:** [explicit] Conclusion suggests investigating "specific contributions of each 'chess piece' inspired component"
- **Why unresolved:** Experiments use fixed weights without ablation study to isolate utility of specific strategies
- **What evidence would resolve it:** Results from ablation studies where individual components are removed or varied

### Open Question 3
- **Question:** Can NIRMAL effectively optimize models in domains outside computer vision?
- **Basis in paper:** [explicit] Conclusion proposes "exploring NIRMAL's applicability to other deep learning tasks beyond image classification, such as natural language processing (NLP)"
- **Why unresolved:** Evaluation restricted to four image classification datasets
- **What evidence would resolve it:** Benchmarking NIRMAL on standard NLP or time-series prediction tasks

## Limitations

- Exact CNN architectures unspecified, making faithful reproduction impossible
- Component weight configuration appears arbitrary without sensitivity analysis
- Underperformance on CIFAR-10 versus Adam raises questions about when complexity is beneficial
- Existence of "Enhanced NIRMAL" suggests base algorithm may require refinement

## Confidence

- **Mechanism 1 (Weighted Multi-Strategy):** Medium - Mathematical framework sound but untested for optimal weight configurations
- **Mechanism 2 (Stochastic Perturbation):** Medium - Theoretically plausible but may be counterproductive on simple datasets
- **Mechanism 3 (Non-Linear Momentum):** Low - Novel tanh transformation lacks empirical validation or comparison

## Next Checks

1. **Ablation Study on CIFAR-100:** Train NIRMAL with w_knight=0 to quantify stochastic noise contribution on complex dataset
2. **Architecture Sensitivity Analysis:** Systematically vary CNN filter counts and FC layer sizes on FashionMNIST to determine architecture effects
3. **Weight Configuration Grid Search:** Perform sensitivity analysis on five component weights to identify optimal configuration