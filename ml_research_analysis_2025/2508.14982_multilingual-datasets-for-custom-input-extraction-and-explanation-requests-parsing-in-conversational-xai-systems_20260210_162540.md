---
ver: rpa2
title: Multilingual Datasets for Custom Input Extraction and Explanation Requests
  Parsing in Conversational XAI Systems
arxiv_id: '2508.14982'
source_url: https://arxiv.org/abs/2508.14982
tags:
- input
- custom
- user
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multilingual intent recognition
  and custom input extraction in conversational explainable AI (ConvXAI) systems.
  To improve upon the scarcity of multilingual training data, the authors extend the
  CoXQL dataset to five languages (English, German, Chinese, Russian, and Telugu)
  and introduce a new dataset, Compass, for custom input extraction across the same
  languages.
---

# Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems

## Quick Facts
- arXiv ID: 2508.14982
- Source URL: https://arxiv.org/abs/2508.14982
- Reference count: 29
- Primary result: Introduces MultiCoXQL (5 languages) and Compass datasets, with GMP method showing 28.31% average improvement in multilingual parsing

## Executive Summary
This paper addresses the challenge of multilingual intent recognition and custom input extraction in conversational explainable AI (ConvXAI) systems. The authors extend the CoXQL dataset to five languages (English, German, Chinese, Russian, and Telugu) and introduce a new Compass dataset for custom input extraction. They propose Guided Multi-Prompt Parsing (GMP), which combines SBERT-based intent retrieval with grammar-guided generation to significantly improve multilingual parsing performance. The work demonstrates that fine-tuned mBERT and LLMs with appropriate prompting strategies outperform cross-lingual approaches, with GOLLIE excelling for smaller models and naive few-shot prompting best for larger models.

## Method Summary
The paper introduces Guided Multi-Prompt Parsing (GMP), a method that enhances multilingual parsing by integrating intent similarity-based retrieval with grammar-guided generation. SBERT encodes training examples into centroid embeddings for each intent, then user input is encoded and compared via cosine similarity to retrieve top-k candidate intents. These candidates dynamically construct a prompt with relevant demonstrations. A two-stage grammar-constrained decoding process first selects the coarse-grained intent, then generates fine-grained attributes using intent-specific grammars. For custom input extraction, the paper compares GOLLIE (structured annotation guidelines as code-generation) against naive few-shot prompting and other methods across Llama3-8B, Phi4-14B, and Qwen2.5-72B models.

## Key Results
- GMP consistently outperforms existing methods by an average of 28.31%, with substantial gains across non-English languages
- For custom input extraction, GOLLIE excels with smaller models (Llama3-8B, Phi4-14B), while naive few-shot prompting is best for larger models (Qwen2.5-72B)
- Multilingual mBERT consistently outperforms both monolingual BERT and cross-lingual mBERT, with especially compelling performance gains for Telugu (41.82 → 84.55 F1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GMP improves multilingual parsing by combining retrieval-based intent selection with grammar-constrained generation
- Mechanism: SBERT computes centroid embeddings for each intent from training examples. User input is encoded and compared via cosine similarity to retrieve top-k candidate intents. These candidates dynamically construct a prompt with relevant demonstrations. A simplified grammar constrains coarse-grained intent selection, followed by intent-specific grammar for fine-grained attribute generation
- Core assumption: Multilingual SBERT embeddings capture cross-lingual semantic similarity sufficiently for intent retrieval, and grammar constraints reduce invalid outputs without overly limiting generation flexibility
- Evidence anchors: Abstract states GMP "enhances multilingual parsing performance by integrating intent similarity-based retrieval with grammar-guided generation"; Section 5.1.2 shows GMP outperforms existing methods by 28.31%; Related work shows automated benchmark generation benefits from structured constraints
- Break condition: If SBERT embeddings fail to capture semantic similarity for very low-resource languages (e.g., Telugu shows 25.66 F1 baseline), retrieval quality degrades, making grammar constraints insufficient for recovery

### Mechanism 2
- Claim: Structured annotation guidelines (GOLLIE) outperform naive prompting for smaller LLMs on custom input extraction
- Mechanism: GOLLIE reformulates extraction as code-generation by providing annotation guidelines as structured dataclass definitions. This scaffolding reduces ambiguity about what constitutes "custom input" and how to mark boundaries, making the task more interpretable for smaller models
- Core assumption: Smaller models benefit more from explicit structural guidance than larger models, which may be distracted by added complexity
- Evidence anchors: Abstract notes "GOLLIE excels with smaller models, while naive few-shot prompting is best for larger models"; Section 8.3.2 shows GOLLIE outperforms naive approach for Llama3-8B and Phi4-14B; GPT-NER paper similarly reformulates IE as text generation with special tokens
- Break condition: When custom input spans complex linguistic structures (idioms, multi-word expressions) that don't map cleanly to code-like representations, GOLLIE's structured format may fail to capture nuance

### Mechanism 3
- Claim: Multilingual mBERT trained on mixed-language data outperforms cross-lingual (English-only training) and monolingual fine-tuning for both intent recognition and custom input extraction
- Mechanism: Joint training on English + varying proportions of target language data creates shared representations that transfer knowledge across languages. Performance scales with target-language data proportion, with Telugu showing largest gains (41.82 → 84.55 F1)
- Core assumption: Languages share transferable representations for XAI-domain intents; annotation quality is comparable across languages
- Evidence anchors: Section 8.3.1 shows multilingual mBERT consistently outperforms both monolingual BERT and cross-lingual mBERT; Section 8.3.2 shows multilingual mBERT outperforms all LLMs across nearly all languages; LEMONADE corpus demonstrates multilingual annotation challenges
- Break condition: If translation quality is poor (Telugu shows lowest semantic similarity: 53-73% vs. English), multilingual training may propagate noise

## Foundational Learning

- Concept: **Intent recognition as text-to-SQL parsing**
  - Why needed here: The paper frames ConvXAI intent recognition as mapping natural language queries to SQL-like structured outputs (e.g., `nlpattribute(instance, topk, method)`). Understanding this formulation is prerequisite to grasping why grammar constraints and template checking matter
  - Quick check question: Given the query "Show feature importance for ID 5 using LIME," what would the parsed SQL-like output look like?

- Concept: **Cross-lingual vs. multilingual training paradigms**
  - Why needed here: The paper evaluates three settings—monolingual, cross-lingual (train English, test other languages), and multilingual (joint training). Results show cross-lingual underperforms significantly for low-resource languages
  - Quick check question: If you only have English training data but need Telugu inference, which approach should you use? What if you have 25% Telugu data available?

- Concept: **Grammar-constrained decoding**
  - Why needed here: Guided Decoding (GD) and GMP both use grammars to limit output vocabulary to valid operations/attributes. Understanding why this helps—especially for non-English languages where MP/MP+ produce "outputs outside the predefined operation set"—is critical
  - Quick check question: Why might unconstrained multi-prompt parsing generate invalid operations for non-English queries that grammar-constrained approaches avoid?

## Architecture Onboarding

- Component map: User query → multilingual SBERT encoding → intent centroid comparison → top-k candidate selection → dynamic few-shot prompt construction → LLM with grammar-constrained decoding (coarse intent → fine-grained attributes) → structured SQL-like parse

- Critical path: SBERT embedding quality determines retrieval accuracy—this is the bottleneck for low-resource languages; Grammar definition coverage—if grammars don't include all valid operation/attribute combinations, valid outputs will be incorrectly rejected; Translation quality for Compass custom input extraction—must verify translated custom_input remains embedded in translated user_question

- Design tradeoffs: GMP vs. MP+—GMP sacrifices some English performance (occasionally underperforms MP+) for large multilingual gains; Choose based on deployment language distribution. GOLLIE vs. Naive prompting—GOLLIE adds prompt complexity but helps smaller models; Larger models (Qwen2.5-72B) perform best with simpler naive prompting. BERT vs. LLMs—Fine-tuned BERT matches/exceeds LLMs on non-English data at lower cost, but requires per-language fine-tuning; mBERT offers single-model deployment with slightly lower peak performance

- Failure signatures: Low SBERT similarity scores (<60%) indicate retrieval will fail; consider fallback to nearest-neighbor baseline or language-specific embedding models. LLM generates intent labels in target language instead of English; add explicit instruction to output English labels. Custom input extraction includes artifacts/hallucinations; GOLLIE reduces this but doesn't eliminate it. Telugu performance collapse—across all approaches, Telugu shows 2-4x worse F1 than other languages; indicates fundamental embedding/translation issues requiring dedicated data collection

- First 3 experiments: Baseline retrieval analysis—compute SBERT similarity scores between English training queries and translated queries in each target language. Identify which languages fall below retrieval threshold (<70% similarity predicts poor GMP performance). Grammar coverage audit—extract all valid SQL-like outputs from CoXQL training set and verify GMP grammar accepts 100% of them. Identify any operations/attributes missing from grammar definition. Cross-lingual transfer diagnostic—train mBERT on English-only data and evaluate on each target language individually. Compare against monolingual BERT to quantify transfer gap; this establishes expected improvement ceiling for multilingual training

## Open Questions the Paper Calls Out
- **Question:** How can the three new operations identified in the Compass dataset (edit_label, knowledge_edit, feedback) be functionally implemented and integrated into ConvXAI systems?
- **Basis in paper:** The authors state in the Limitations section: "We do not implement the operations highlighted in red... their actual implementation and integration into ConvXAI systems are left for future work."
- **Question:** Why does the GOLLIE prompting strategy, which excels with smaller models, underperform compared to naive few-shot prompting for larger models (72B)?
- **Basis in paper:** The results section notes that "larger models appear more susceptible to distraction from newly introduced patterns" when using GOLLIE, whereas smaller models benefit from the structure, but the paper does not isolate the cause.
- **Question:** To what extent does the reliance on machine-translated training data (via Gemini) degrade parsing accuracy for low-resource languages compared to natively constructed datasets?
- **Basis in paper:** The Limitations section acknowledges a key dependence on a machine translation system, noting that translations "may contain inaccuracies" and that Telugu posed challenges due to "semantic complexity."

## Limitations
- Translation quality uncertainty—automated translation via Gemini shows particularly poor semantic similarity for Telugu (53-73%), potentially limiting downstream model performance
- Grammar coverage completeness—GMP's performance depends on comprehensive grammar definitions that may not capture all valid operation/attribute combinations from CoXQL
- Implementation details unspecified—critical parameters like k value for retrieval, fine-tuning hyperparameters, and decoding parameters remain unclear for exact reproduction

## Confidence
- **High Confidence**: GMP's multilingual performance advantage (28.31% average improvement), GOLLIE's effectiveness for smaller models, and mBERT's superiority over cross-lingual approaches in multilingual settings
- **Medium Confidence**: The mechanism explaining why grammar constraints help non-English languages, and why GOLLIE helps smaller models more than larger ones
- **Low Confidence**: Translation quality impact assessment and whether observed Telugu performance issues stem from data quality or fundamental embedding limitations

## Next Checks
1. **Translation Quality Audit**: Compute SBERT similarity scores between original English queries and their translations in each target language. Languages falling below 70% similarity threshold likely cannot benefit from retrieval-based approaches like GMP
2. **Grammar Coverage Verification**: Extract all SQL-like outputs from the original CoXQL training set and systematically verify that GMP's grammar definitions accept 100% of valid outputs. Document any operations/attributes that require grammar expansion
3. **Cross-Lingual Transfer Benchmark**: Train mBERT on English-only data and evaluate on each target language individually. Compare against monolingual BERT to establish the theoretical maximum transfer performance, then measure how close multilingual training approaches this ceiling