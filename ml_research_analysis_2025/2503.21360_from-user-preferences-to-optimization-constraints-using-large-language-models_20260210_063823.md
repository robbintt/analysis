---
ver: rpa2
title: From User Preferences to Optimization Constraints Using Large Language Models
arxiv_id: '2503.21360'
source_url: https://arxiv.org/abs/2503.21360
tags:
- data
- user
- energy
- task
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for converting
  user preferences expressed in natural language into formal energy optimization constraints
  for household appliances. Using a pilot dataset of 26 Italian utterances, the task
  was to map user-specified time and temperature preferences into structured constraint
  representations.
---

# From User Preferences to Optimization Constraints Using Large Language Models

## Quick Facts
- arXiv ID: 2503.21360
- Source URL: https://arxiv.org/abs/2503.21360
- Authors: Manuela Sanguinetti; Alessandra Perniciano; Luca Zedda; Andrea Loddo; Cecilia Di Ruberto; Maurizio Atzori
- Reference count: 26
- One-line primary result: ChatGPT and LLaMAntino-3-ANITA LLMs can partially convert Italian user preferences into energy optimization constraints, but performance remains limited without fine-tuning.

## Executive Summary
This study evaluates Large Language Models for converting user preferences expressed in natural language into formal energy optimization constraints for household appliances. Using a pilot dataset of 26 Italian utterances, the task was to map user-specified time and temperature preferences into structured constraint representations. Zero-shot, one-shot, and few-shot prompting were tested on four Italian LLMs. ChatGPT and LLaMAntino-3-ANITA were the only models yielding valid results. ChatGPT performed best in zero-shot settings, while LLaMAntino-3-ANITA showed better accuracy with more examples. However, both models struggled with condition generation, indicating that in-context learning alone is insufficient. Future work will expand the dataset and explore supervised fine-tuning for improved performance.

## Method Summary
The study maps Italian user utterances expressing appliance preferences to formal constraint representations using prompt engineering. A pilot dataset of 26 annotated utterances includes XML-tagged preference spans and corresponding constraint annotations. Four Italian LLMs (ChatGPT, LLaMAntino-3-ANITA, LLaMAntino-2-UltraChat-7B, Zefiro-7b-beta-ITA) were evaluated using zero-shot, one-shot, and few-shot prompting. Prompts include task description, XML tag meanings, format specification, and examples. Generated outputs are parsed to extract variables (st, ht) and conditions, with performance measured by ChrF, AccVariables, and AccConditions metrics.

## Key Results
- ChatGPT achieved 78.57% AccVariables in zero-shot prompting but struggled with conditions (21.43% AccConditions).
- LLaMAntino-3-ANITA showed incremental improvement from 28.57% (zero-shot) to 64.29% (few-shot) AccVariables.
- Both models exhibited significantly lower condition accuracy compared to variable accuracy, indicating difficulty with temporal reasoning.
- Additional examples improved LLaMAntino-3-ANITA but had inconsistent effects on ChatGPT performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting improves constraint generation quality over zero-shot for open LLMs, but gains vary by model architecture.
- Mechanism: Providing annotated examples in the prompt gives the model a pattern to imitate, reducing ambiguity in the target output format. LLaMAntino-3-ANITA showed incremental improvement across 0-shot (AccAvg: 28.57%), 1-shot (50.4%), and few-shot (64.29%).
- Core assumption: The model has sufficient instruction-following capacity and Italian language proficiency to leverage in-context examples.
- Evidence anchors:
  - [abstract] "Results show that while LLMs can partially handle this task, performance remains limited without domain-specific fine-tuning."
  - [Section 6, Table 1] LLaMAntino-3-ANITA few-shot: ChrF 74.5%, AccVariables 85.7%, AccConditions 42.9%
  - [corpus] Related work on LLMs for NL2Opt tasks shows similar patterns—examples help but fine-tuning remains necessary.
- Break condition: If the model lacks sufficient instruction-tuning or Italian proficiency (e.g., LLaMAntino-2, Zefiro returned "inconsistent and noise-rich results"), additional examples degrade rather than help.

### Mechanism 2
- Claim: XML tagging of preference spans focuses model attention on constraint-relevant text segments.
- Mechanism: Wrapping text spans expressing user preferences in XML tags provides explicit boundaries, reducing the search space for the sequence-to-sequence mapping task.
- Core assumption: The model can interpret and leverage tagged structure meaningfully.
- Evidence anchors:
  - [Section 5] "the input utterances included in each prompt were labeled with XML tags that incorporate the text spans expressing users' preferences"
  - [Section 5] Prompt structure includes "the meaning of the XML tags" as an explicit component
  - [corpus] Weak direct evidence in neighbors—no direct replication of XML tagging for this specific constraint task.
- Break condition: If tags are inconsistently applied or the model doesn't generalize from examples to unseen tag patterns, performance drops.

### Mechanism 3
- Claim: Constrained decoding parameters reduce randomness in structured output generation.
- Mechanism: Low temperature (0.1), limited top-k (20), and short max tokens (30) constrain the model toward more deterministic, format-compliant outputs.
- Core assumption: The constrained space still contains valid constraint representations.
- Evidence anchors:
  - [Section 5] "temperature = 0.1, top-k = 20, and top-p = 0.9... maximum generation length to 30 tokens"
  - [Section 6] ChatGPT (using OpenAI's interface, different settings) showed different behavior patterns than open models
  - [corpus] No direct corpus evidence on decoding parameters for this specific task type.
- Break condition: If constraints require more than 30 tokens or the valid output lies outside the constrained distribution, generation fails silently.

## Foundational Learning

- Concept: **Sequence-to-sequence mapping with structured outputs**
  - Why needed here: The task fundamentally maps natural language input to a formal constraint representation with specific syntax.
  - Quick check question: Can you explain why standard text generation metrics (BLEU, ROUGE) are unsuitable for this task?

- Concept: **In-context learning (zero/one/few-shot prompting)**
  - Why needed here: The paper evaluates models without fine-tuning, relying entirely on prompt engineering.
  - Quick check question: What is the expected performance trajectory as you add more examples to a prompt?

- Concept: **Constraint satisfaction vs. text generation evaluation**
  - Why needed here: ChrF measures character overlap; AccVariables/AccConditions measure semantic correctness of specific constraint components.
  - Quick check question: Why might a high ChrF score coexist with low condition accuracy?

## Architecture Onboarding

- Component map: Input utterance -> XML tagging -> Prompt assembly -> LLM inference -> Parse variables/conditions -> Validate against annotation schema
- Critical path: Input utterance → XML tagging → Prompt assembly → LLM inference → Parse variables/conditions → Validate against annotation schema
- Design tradeoffs:
  - More examples in prompt → Better accuracy but longer latency and context limits
  - Lower temperature → More deterministic but may miss valid creative outputs
  - Italian-specific models (LLaMAntino) vs. multilingual (ChatGPT) → Trade-off between language specialization and general capability
- Failure signatures:
  - **Noise-rich outputs**: Model generates incoherent text (seen with LLaMAntino-2, Zefiro) → Model lacks sufficient instruction-tuning
  - **High variable accuracy, low condition accuracy**: Model identifies what to constrain but not when → Time expression parsing is the bottleneck
  - **Performance degradation with 1-shot**: Additional context confuses rather than helps (seen with ChatGPT AccConditions: 0.2143 → 0.119) → Prompt design mismatch
- First 3 experiments:
  1. **Baseline replication**: Run LLaMAntino-3-ANITA with the paper's exact prompt template and decoding settings on the 26-utterance pilot set; verify ChrF ~74% and AccConditions ~43%.
  2. **Ablation on XML tags**: Remove XML tagging from prompts; measure impact on AccVariables and AccConditions to quantify the annotation signal contribution.
  3. **Condition-specific error analysis**: Manually inspect failed condition generations; categorize whether failures stem from time parsing, logical operators, or format adherence to guide fine-tuning data collection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning on an expanded dataset significantly outperform the current few-shot baseline?
- Basis in paper: [explicit] The authors state that "performance remains limited without domain-specific fine-tuning" and plan to "further expand the dataset... and annotate the related constraints."
- Why unresolved: The current study was restricted to in-context learning (zero-shot to few-shot) due to the small size of the pilot dataset (26 utterances).
- What evidence would resolve it: Performance metrics from a fine-tuned Italian LLM compared against the current LLaMAntino-3-ANITA few-shot baseline (ChrF 74.5%).

### Open Question 2
- Question: Do functional correctness metrics like pass@k correlate better with optimization success than the currently used ChrF metric?
- Basis in paper: [explicit] The authors propose evaluating "functional correctness... taking inspiration from metrics such as pass@k" once the model is connected to the actual optimizer.
- Why unresolved: Current evaluation relies on surface-level string similarity (ChrF) and parsed accuracy, rather than the executable validity of the constraints.
- What evidence would resolve it: A correlation analysis comparing pass@k scores against the successful execution rate of generated constraints within the optimization module.

### Open Question 3
- Question: Can the performance gap between variable identification and time condition generation be closed through improved modeling?
- Basis in paper: [inferred] Results show a large disparity (e.g., 85.7% variable accuracy vs. 42.9% condition accuracy), which the authors highlight as a specific difficulty.
- Why unresolved: The paper notes that models struggle to generate conditions correctly but does not determine if this is a fundamental limitation of the models or the prompting strategy.
- What evidence would resolve it: Experiments demonstrating that specific architectural changes or fine-tuning can raise condition accuracy to levels comparable with variable accuracy.

## Limitations
- Small pilot dataset (26 utterances) limits generalizability and prevents fine-tuning evaluation.
- Performance gap between variable and condition accuracy indicates fundamental challenges in temporal reasoning.
- Exclusive focus on Italian language restricts applicability to multilingual households or international deployment.

## Confidence
- **High confidence**: ChatGPT outperforms other models in zero-shot settings for constraint variable generation
- **Medium confidence**: Few-shot prompting improves LLaMAntino-3-ANITA performance but has variable effects on ChatGPT
- **Low confidence**: XML tagging significantly improves model focus on constraint-relevant spans

## Next Checks
1. **Dataset expansion validation**: Replicate the study with a 10x larger dataset (minimum 260 utterances) covering diverse time expressions, temperature preferences, and appliance types to assess whether current model limitations persist with more training examples.

2. **Fine-tuning impact study**: Compare in-context learning performance against supervised fine-tuning using the same 26 utterances. Measure whether fine-tuning on domain-specific examples (rather than just prompting) closes the condition accuracy gap observed in zero-shot settings.

3. **Cross-linguistic generalization test**: Evaluate the best-performing models (ChatGPT and LLaMAntino-3-ANITA) on equivalent English utterances to determine if performance patterns are language-specific or reflect broader architectural limitations in constraint generation.