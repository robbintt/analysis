---
ver: rpa2
title: 'Users as Annotators: LLM Preference Learning from Comparison Mode'
arxiv_id: '2510.13830'
source_url: https://arxiv.org/abs/2510.13830
tags:
- users
- user
- data
- preference
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to collect and filter preference data
  for aligning large language models (LLMs) using user annotations from comparison
  mode. It introduces a probabilistic user behavior model to distinguish attentive
  from casual users, an expectation-maximization (EM) algorithm to estimate user attentiveness,
  and a filtering strategy based on inferred attentiveness levels.
---

# Users as Annotators: LLM Preference Learning from Comparison Mode

## Quick Facts
- arXiv ID: 2510.13830
- Source URL: https://arxiv.org/abs/2510.13830
- Authors: Zhongze Cai; Xiaocheng Li
- Reference count: 40
- Key outcome: Filtering user annotations by inferred attentiveness improves DPO performance, achieving up to 73% win rate over baselines

## Executive Summary
This paper proposes a method to collect and filter preference data for aligning large language models (LLMs) using user annotations from comparison mode. It introduces a probabilistic user behavior model to distinguish attentive from casual users, an expectation-maximization (EM) algorithm to estimate user attentiveness, and a filtering strategy based on inferred attentiveness levels. Experiments show that filtering improves downstream direct preference optimization (DPO) performance, with filtered models achieving up to 73% win rate over baselines and higher reward scores compared to unfiltered models.

## Method Summary
The method generates response pairs from two LLMs with a known performance gap (μ), simulates user annotations via a probabilistic model that modulates preferences based on latent attentiveness (η_j), runs EM to estimate user attentiveness and model parameters, filters low-attentiveness annotations, and trains DPO on the filtered dataset. The approach uses synthetic user data where η_j is sampled from P_η distributions (two-point or Beta), and μ is either known or estimated with regularization.

## Key Results
- DPO+Filter achieves 67-73% win rate vs. 58-67% for unfiltered DPO across conditions
- Filtering consistently improves average reward scores on held-out prompts
- "Sweet spot" filtering varies with P_η distribution shape, balancing quality vs. quantity tradeoff

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Model Pairing for Quality Signaling
Generating responses from two models with known performance gap creates a detectable signal for user attentiveness. When Model A consistently outperforms Model B (μ > 0.5), attentive users preferentially select Model A's responses while casual users select randomly near 0.5. This divergence becomes statistically distinguishable given sufficient annotations.

### Mechanism 2: Latent Attentiveness Inference via EM
The EM algorithm recovers each user's latent attentiveness η_j as a posterior distribution without observing ground-truth labels. The E-step computes P(η_j | observations, θ^{(t)}) by combining the prior P_η with the likelihood of observed preferences. The M-step updates population parameters to maximize expected complete-data log-likelihood.

### Mechanism 3: Threshold-Based Filtering for Downstream Alignment
Filtering training data by inferred attentiveness improves DPO performance even with reduced data volume. High-η users provide labels closer to the true preference distribution. Removing low-η data reduces label noise, which prior work shows degrades DPO.

## Foundational Learning

- **Bradley-Terry Preference Model**: Why needed here: The paper's p(x) = P(y^1 > y^2 | x) is a Bradley-Terry probability; σ(r_i - r_j) connects reward scores to binary preferences. Quick check: Given rewards r_A = 2.3 and r_B = 1.1, what is P(A preferred over B)? (Answer: σ(1.2) ≈ 0.77)

- **Expectation-Maximization Algorithm**: Why needed here: The core inference method; requires understanding latent variables, complete-data likelihood, and why EM only guarantees convergence to stationary points. Quick check: Why does EM require marginalization over η_j in the E-step rather than directly optimizing? (Answer: η_j is unobserved; we compute expectations under the posterior)

- **Direct Preference Optimization (DPO)**: Why needed here: The downstream task; DPO implicitly learns a reward function by optimizing policy ratios, avoiding explicit reward model training. Quick check: In DPO loss, what happens when P_θ(y_preferred | x) increases while P_ref stays fixed? (Answer: Loss decreases; the policy shifts toward preferred responses)

## Architecture Onboarding

- **Component map**: Response Generation Module -> User Interaction Logger -> EM Estimator -> Filtering Engine -> DPO Training Pipeline

- **Critical path**: Response pair selection → μ estimation → EM convergence → threshold calibration → DPO training. Errors propagate: poor μ estimates corrupt all downstream η_j inferences.

- **Design tradeoffs**: Two-point vs. Beta P_η (simpler vs. more nuanced); μ known vs. estimated (simpler analysis vs. realistic); model gap (μ) (better inference vs. trivial discrimination).

- **Failure signatures**: All users classified as attentive (likely μ ≈ 0.5); EM oscillation/divergence (check regularization); filtered model underperforms baseline (threshold too aggressive); win rate gains without reward score gains (overfitting to comparison distribution).

- **First 3 experiments**: 1) Validate EM on synthetic data with known ground truth (replicate Figure 3); 2) Ablate μ gap across values to identify optimal performance (replicate Table 2 pattern); 3) Threshold sensitivity analysis across different P_η types to locate "sweet spots" (replicate Figure 6).

## Open Questions the Paper Calls Out

### Open Question 1
How can the user behavioral model be extended to estimate attentiveness at the sample level rather than assuming it is constant for a given user? The current model aggregates user history to infer a single latent attentiveness factor, which ignores the fact that users may be attentive for simple queries but casual for complex ones.

### Open Question 2
Does partitioning prompts and responses into topic-based subgroups improve the inference of attentiveness and preference probabilities? The current framework assumes a universal μ, which may not hold if Model A is better at coding while Model B is better at creative writing.

### Open Question 3
Can the proposed method maintain its performance when applied to real human annotations rather than simulated user behavior? The reported high win rates rely on simulated data where "attentive" behavior follows a mathematically clean probability defined by a reward model.

## Limitations

- The assumption of known μ in synthetic experiments requires careful regularization in real-world deployment
- The two-point P_η model may oversimplify the true distribution of user attentiveness
- The filtering mechanism assumes attentiveness directly translates to annotation quality, which may not hold if users are attentive but systematically biased

## Confidence

**High Confidence**: The EM algorithm correctly recovers latent attentiveness parameters under the stated model. The filtering methodology is well-specified and the DPO training procedure is standard.

**Medium Confidence**: The claim that filtered models achieve 73% win rate over baselines relies on the specific μ gap (0.74) and P_η distribution used. The sweet spot analysis is sensitive to the choice of filtering threshold and P_η parameterization.

**Low Confidence**: The claim that over-aggressive filtering can harm performance depends on domain-specific factors not fully explored. The relationship between μ gap and DPO performance may vary across different model pairs and tasks.

## Next Checks

1. **EM Robustness to μ Estimation**: Validate EM performance when μ is estimated rather than known across multiple μ gaps (0.6, 0.74, 0.98) and compare parameter recovery accuracy.

2. **Threshold Sensitivity Across P_η Types**: Systematically vary filtering thresholds (top 50%, 70%, 90%) across different P_η distributions (two-point, Beta, Gaussian) to identify consistent "sweet spots".

3. **Real-World Deployment Validation**: Test the full pipeline on actual user preference data from UltraFeedback/HelpSteer3 without synthetic user simulation and compare EM-inferred attentiveness against ground-truth annotation quality measures.