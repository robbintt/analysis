---
ver: rpa2
title: 'TS2Vec-Ensemble: An Enhanced Self-Supervised Framework for Time Series Forecasting'
arxiv_id: '2511.22395'
source_url: https://arxiv.org/abs/2511.22395
tags:
- time
- forecasting
- ts2vec
- series
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TS2Vec-Ensemble, a novel framework that enhances
  time series forecasting by combining the implicit dynamics captured by TS2Vec representations
  with explicit time features encoding periodic cycles. The method employs two parallel
  regression heads and an adaptive weighting scheme optimized per forecast horizon.
---

# TS2Vec-Ensemble: An Enhanced Self-Supervised Framework for Time Series Forecasting

## Quick Facts
- arXiv ID: 2511.22395
- Source URL: https://arxiv.org/abs/2511.22395
- Reference count: 15
- Primary result: Reduces MSE by over 16% on long-horizon univariate forecasting (ETTm1, horizon 672: MSE from 0.156 to 0.131)

## Executive Summary
This paper proposes TS2Vec-Ensemble, a novel framework that enhances time series forecasting by combining the implicit dynamics captured by TS2Vec representations with explicit time features encoding periodic cycles. The method employs two parallel regression heads and an adaptive weighting scheme optimized per forecast horizon. Extensive experiments on the ETT benchmark datasets for univariate and multivariate forecasting demonstrate that TS2Vec-Ensemble consistently and significantly outperforms the standard TS2Vec baseline and other state-of-the-art models, particularly in long-horizon scenarios.

## Method Summary
The method builds on the TS2Vec self-supervised encoder, which produces timestamp-level embeddings through contrastive learning. Two parallel Ridge regression heads are trained: one using only the original TS2Vec embeddings (dynamics-focused) and another using embeddings concatenated with sinusoidal time features (seasonality-focused). Ensemble weights are optimized independently for each forecast horizon via grid search on a validation set. The framework is evaluated on ETT datasets with 12/4/4 month train/val/test splits, using MSE and MAE as primary metrics.

## Key Results
- Reduces MSE by over 16% on ETTm1 univariate forecasting at horizon 672 (0.156 → 0.131)
- Consistently outperforms TS2Vec baseline across all ETT datasets and forecast horizons
- Shows particular strength in long-horizon scenarios where explicit seasonal encoding becomes more valuable

## Why This Works (Mechanism)

### Mechanism 1: Complementary Feature Fusion via Dual-Head Architecture
The TS2Vec encoder captures complex, non-linear temporal dynamics through hierarchical contrastive learning, but does not explicitly encode deterministic periodicities. By concatenating sinusoidal features—sin(2πt/24) and cos(2πt/24)—to the embeddings before passing to a parallel regression head, the model gains an explicit anchor for daily cycles. The two heads (dynamics-focused and seasonality-focused) are then combined via adaptive weighting. This decomposes the forecasting problem: one path handles residual/complex patterns, the other grounds predictions in known periodic structure.

### Mechanism 2: Horizon-Optimized Adaptive Weighting
Rather than fixed weights, the framework selects optimal weights (w₁, w₂) from a grid of 17 candidates by minimizing √(MSE+MAE) on a holdout validation set for each horizon. Longer horizons may favor the seasonality-focused head (explicit cycles become more predictive), while shorter horizons may favor dynamics-focused representations.

### Mechanism 3: Contrastive-Generative Objective Conflict (Negative Result)
The paper demonstrates that adding a masked signal modeling (MSM) reconstruction objective to TS2Vec degrades forecasting performance. The contrastive loss encourages scale-invariance and global discriminative representations, while MSM forces fine-grained local reconstruction. These objectives create gradient conflict: the encoder cannot simultaneously learn representations that are both globally discriminative (contrastive) and locally precise (reconstruction).

## Foundational Learning

- **Concept: Contrastive Learning for Time Series**
  - Why needed: TS2Vec's core representation learning uses contrastive objectives (temporal and instance-wise) to learn timestamp-level embeddings. Understanding how positive/negative pairs are constructed via contextual consistency (random cropping, timestamp masking) is essential for debugging representation quality.
  - Quick check: Given a batch of 8 time series, can you identify what constitutes a positive pair vs. negative pair for the temporal contrastive loss at timestamp t?

- **Concept: Time Series Decomposition (Trend-Seasonality-Residual)**
  - Why needed: The ensemble implicitly decomposes forecasting into dynamics (residual-like, captured by TS2Vec) and seasonality (captured by sinusoidal features). Understanding classical decomposition clarifies why the dual-head architecture is theoretically motivated.
  - Quick check: If a series has strong weekly seasonality but you only encode daily cycles (period=24), what component will the seasonality-focused head fail to capture?

- **Concept: Ridge Regression for Downstream Forecasting**
  - Why needed: The "linear protocol" uses Ridge regression on frozen TS2Vec embeddings. Understanding L2 regularization and how α controls bias-variance tradeoff is critical, as α is grid-searched per horizon.
  - Quick check: Why might longer forecast horizons require different Ridge α values than shorter horizons, even with the same embeddings?

## Architecture Onboarding

- **Component map:**
  Raw Time Series X → [TS2Vec Encoder - Frozen/Pretrained] → Timestamp-level Embeddings Z (dim=320) → ├─→ [Head A: Dynamics] → Ridge Regression → Ŷ_A │ └─→ [Head B: Seasonality] → Ridge Regression → Ŷ_B Input: z_{t-h} Input: [z_{t-h} || sin(2πt/24), cos(2πt/24)] Ŷ_final = w₁·Ŷ_A + w₂·Ŷ_B (weights optimized per horizon on validation set)

- **Critical path:**
  1. TS2Vec encoder training (contrastive, hierarchical) → produces 320-dim embeddings
  2. Feature engineering: extract last-timestamp embedding z_{t-h} + construct sinusoidal time features
  3. Train two independent Ridge regression heads on training data
  4. Grid search over 17 weight combinations per horizon on validation set using √(MSE+MAE) criterion
  5. Apply optimal weights to test predictions

- **Design tradeoffs:**
  - Grid search vs. learned weights: Current approach uses discrete grid search (17 candidates); faster but coarser than meta-learning or gradient-based weight optimization
  - Frozen vs. fine-tuned encoder: TS2Vec encoder is frozen; fine-tuning could improve task-specific representations but risks overfitting and loses self-supervised generalization
  - Sinusoidal vs. learned positional features: Fixed sinusoidal features are simple and interpretable but cannot adapt to irregular or non-harmonic periodicity

- **Failure signatures:**
  - MSE doubles relative to baseline: Check if MSM loss was accidentally enabled (λ>0); contrastive-generative conflict
  - Short-horizon performance degrades: Check if seasonality head weight is too high; explicit features may over-constrain short-term predictions
  - Multivariate worse than univariate: Verify concatenation dimension handling; Ridge may struggle with high-dimensional concatenated features without proper regularization scaling
  - Validation-test weight mismatch: If validation set is too small (4 months vs 12 months training), horizon-specific weights may not generalize

- **First 3 experiments:**
  1. Reproduce baseline comparison: Train vanilla TS2Vec + Ridge (no ensemble, no time features) on ETTh1 univariate; verify MSE ≈0.116 average matches Table I before proceeding
  2. Ablate single components: Run (a) dynamics-only head, (b) seasonality-only head, (c) ensemble with fixed 50-50 weights; quantify contribution of each component to isolate mechanism
  3. Weight sensitivity analysis: For horizon H=336, plot validation √(MSE+MAE) across all 17 weight candidates to verify optimal weights are meaningfully different from uniform and that the loss surface is not flat

## Open Questions the Paper Calls Out
- Can TS2Vec-Ensemble maintain its performance superiority when applied to a wider range of benchmark datasets beyond the Electricity Transformer Temperature (ETT) suite?
- Can meta-learned or attention-based weighting schemes outperform the current validation-based grid search used to balance the dynamics-focused and seasonality-focused regression heads?
- Does a fully end-to-end joint optimization of the TS2Vec encoder and the ensemble forecasting heads improve accuracy compared to the current two-stage transfer learning approach?

## Limitations
- The current study validates the method exclusively on ETT datasets, which may share specific structural characteristics or periodicities that favor the proposed sinusoidal time features
- The framework does not explore higher-frequency features or learnable positional encodings, which could capture richer periodic structure beyond daily cycles
- By keeping TS2Vec frozen, the model cannot adapt representations to the specific forecasting task, potentially leaving performance on the table compared to fine-tuned approaches

## Confidence
- High confidence: The dual-head architecture (dynamics vs. seasonality) and its implementation are clearly specified and reproducible. The contrastive loss mechanism is standard and well-validated in TS2Vec literature.
- Medium confidence: Horizon-specific ensemble weighting improves performance relative to fixed weights, but the risk of overfitting to validation data is not quantified. The claim of 16% MSE improvement at horizon 672 is specific but may not generalize to other datasets.
- Low confidence: The negative result that MSM degrades performance is clearly demonstrated, but the mechanism (contrastive-generative conflict) is asserted rather than experimentally validated with gradient analysis or ablation of individual loss components.

## Next Checks
1. Reproduce baseline comparison: Train vanilla TS2Vec + Ridge (no ensemble, no time features) on ETTh1 univariate; verify MSE ≈0.116 average matches Table I before proceeding.
2. Ablate single components: Run (a) dynamics-only head, (b) seasonality-only head, (c) ensemble with fixed 50-50 weights; quantify contribution of each component to isolate mechanism.
3. Weight sensitivity analysis: For horizon H=336, plot validation √(MSE+MAE) across all 17 weight candidates to verify optimal weights are meaningfully different from uniform and that the loss surface is not flat.