---
ver: rpa2
title: Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation
arxiv_id: '2601.22228'
source_url: https://arxiv.org/abs/2601.22228
tags:
- camera
- vlms
- reasoning
- spatial
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates vision-language models (VLMs) on relative
  camera pose estimation (RCPE), a 3D spatial reasoning task requiring inference of
  camera motion between image pairs. The authors introduce VRRPI-Bench, derived from
  unlabeled egocentric videos with verbalized motion annotations, and VRRPI-Diag,
  a diagnostic benchmark isolating individual degrees of freedom.
---

# Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation

## Quick Facts
- arXiv ID: 2601.22228
- Source URL: https://arxiv.org/abs/2601.22228
- Reference count: 40
- Primary result: VLMs significantly underperform classical geometric methods (LoFTR 0.97 F1 vs VLMs 0.64) and humans (0.92) on relative camera pose estimation

## Executive Summary
This paper evaluates vision-language models on relative camera pose estimation (RCPE), a 3D spatial reasoning task requiring inference of camera motion between image pairs. The authors introduce VRRPI-Bench and VRRPI-Diag benchmarks derived from egocentric videos, and find that VLMs significantly underperform both classical geometric methods (LoFTR 0.97 F1 vs VLMs 0.64) and humans (0.92). VLMs exhibit inconsistent reasoning across image pairs (59.7% consistency) and struggle with depth translation and roll motion along the optical axis, indicating they lack robust multi-view spatial understanding despite excelling at single-image tasks.

## Method Summary
The authors created VRRPI-Bench by filtering RGB-D video frames from 7 Scenes and ScanNet datasets to identify pairs with dominant relative camera motion (translation + rotation) while excluding those with ambiguous multiple motions. VRRPI-Diag isolates individual degrees of freedom by filtering for pure single-DoF motions. They evaluate classical baselines (LoFTR+SIFT with RANSAC) and zero-shot VLMs using structured prompts with `<thinking>` and `<ans>` tags. Performance is measured via macro F1-score on binary classification of dominant motion direction, plus consistency rate when swapping source/target images.

## Key Results
- VLMs significantly underperform classical geometric methods (LoFTR 0.97 F1 vs VLMs 0.64) and humans (0.92) on RCPE
- VLMs show poor consistency when image order is swapped (59.7% consistency)
- VLMs struggle specifically with z-axis transformations (depth translation and roll motion)
- Even top models like GPT-5 fail to maintain logical symmetry between swapped viewpoints

## Why This Works (Mechanism)

### Mechanism 1
VLMs succeed on single-image spatial relations by exploiting learned 2D positional patterns rather than grounded 3D understanding. High performance on 2D benchmarks like WhatsUp reflects pattern matching, not geometric reasoning. This breaks down when tasks require inferring camera motion from viewpoint changes, as the inverse mapping (object shift → camera motion) is ambiguous without 3D scene models.

### Mechanism 2
VLMs fail on z-axis transformations (depth/roll) because these require internalizing projective geometry rather than tracking 2D pixel shifts. While small x-y motions manifest as horizontal/vertical pixel shifts, z-axis depth translation induces scale changes and roll rotation alters canonical scene orientation—neither maps to simple 2D translation. VLMs lack the geometric primitives to invert these projections.

### Mechanism 3
VLMs exhibit cross-view correspondence failure because visual representations are not invariant to viewpoint changes. Without explicit 3D object representations or pose-equivariant features, VLMs default to surface-level similarity matching. This is evident in the low consistency (<60%) when image order is swapped and poor cross-view object tracking performance.

## Foundational Learning

- **6-DoF Camera Pose Representation (θ, φ, ψ, tx, ty, tz)**: Needed to decompose camera motion into three rotations and three translations for RCPE. Quick check: Given a camera moving forward while tilting up, which DoFs are nonzero?

- **Epipolar Geometry and Essential Matrix**: Needed to understand how classical baselines solve RCPE by matching keypoints and computing the essential matrix. Quick check: Why can't two arbitrary points in two views always correspond to the same 3D point?

- **Projective Ambiguity of 2D Images**: Needed to understand why a single image cannot resolve depth, and how VLMs conflate camera translation with object motion. Quick check: If an object appears larger in view B than view A, did the camera move closer or did the object grow?

## Architecture Onboarding

- **Component map**: Image Pair → Vision Encoder → Visual Features → Vision-Language Projector → LLM Backbone → Output: Discrete motion classification
- **Critical path**: The vision encoder → LLM interface is the failure point. Features are extracted independently per image, with no geometric consistency constraint linking the two views.
- **Design tradeoffs**: Single-image pretraining optimizes for 2D tasks but creates "pseudo-spatial" representations; discrete classification enables VLM evaluation but discards metric precision; no explicit correspondence module keeps architecture simple but forces LLM to infer matching implicitly.
- **Failure signatures**: Consistency <60% on swapped image pairs; Roll prediction F1 near random (0.47 for GPT-5 vs 0.99 for pitch/yaw); Cross-view object tracking at or below random baseline (0.28).
- **First 3 experiments**: 
  1. Run your VLM on VRRPI-Diag subset to identify which axes fail; expect near-random performance on ψ (roll) and tz (depth translation).
  2. Test with explicit reference object naming (w/ Ref. condition); improvement indicates correspondence bottleneck.
  3. Swap source/target images on 100 pairs; if predictions don't invert logically, visual representations lack ordered spatial encoding.

## Open Questions the Paper Calls Out

**Open Question 1**: Can strong semantic priors enable VLMs to complement or extend classical geometry-based pipelines in texture-poor or wide-baseline scenarios? Unresolved because VLMs rely on "shallow 2D heuristics" and underperform classical methods (LoFTR), failing to exploit global structural cues for spatial ambiguity.

**Open Question 2**: Can specific training regimes bridge the gap between 2D image-plane heuristics and grounded 3D geometric reasoning? Unresolved because this study focused on zero-shot evaluation; it remains unknown if fine-tuning can fix the observed "compositional gap" in degrees of freedom.

**Open Question 3**: How can VLMs internalize the inverse relationship between object-level visual shifts and global camera motion? Unresolved because models struggle to bridge egocentric object shifts and allocentric camera transformations, relying on textual logic rather than geometric understanding.

## Limitations

- The paper doesn't explore whether the VLM performance gap stems from architectural limitations versus training data distribution shifts from the indoor scene datasets used.
- Discrete classification simplification may understate VLM capabilities by masking finer-grained geometric understanding that exists but doesn't align with discrete bins.
- The influence of prompt engineering is significant but not systematically ablated across models.

## Confidence

- **High confidence**: VLMs' underperformance on RCPE relative to classical methods and humans; the consistency gap when swapping image order; the z-axis (depth/roll) specific failures.
- **Medium confidence**: The mechanistic explanation that VLMs rely on 2D heuristics rather than 3D geometric reasoning; the claim that correspondence failure is the primary bottleneck.
- **Low confidence**: The interpretation that discrete classification formulation is optimal for evaluating VLMs on this task; the generalizability of findings to outdoor or more diverse scene types.

## Next Checks

1. **Continuous pose regression test**: Evaluate VLMs on the same image pairs using continuous 6-DoF pose estimation rather than discrete classification. Compare mean angular error against LoFTR to determine if the gap persists in metric space.

2. **Cross-dataset generalization**: Test VLMs on RCPE tasks from outdoor datasets (e.g., KITTI) or synthetic scenes with different visual statistics than 7 Scenes/ScanNet to assess whether failures generalize beyond the training distribution.

3. **Architecture ablation study**: Implement a modified VLM with explicit cross-view correspondence module (e.g., attention between image pair features) and evaluate whether this bridges the performance gap, distinguishing architectural limitations from fundamental reasoning deficits.