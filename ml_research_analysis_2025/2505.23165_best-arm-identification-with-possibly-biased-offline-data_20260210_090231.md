---
ver: rpa2
title: Best Arm Identification with Possibly Biased Offline Data
arxiv_id: '2505.23165'
source_url: https://arxiv.org/abs/2505.23165
tags:
- data
- lucb-h
- online
- bound
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the best arm identification (BAI) problem when
  historical offline data may be biased. Existing BAI algorithms either ignore such
  data or assume it aligns with online distributions, risking poor performance when
  biases exist.
---

# Best Arm Identification with Possibly Biased Offline Data

## Quick Facts
- **arXiv ID**: 2505.23165
- **Source URL**: https://arxiv.org/abs/2505.23165
- **Reference count**: 40
- **Primary result**: Proposes LUCB-H algorithm for BAI with biased offline data, proving impossibility of adaptive algorithms without bias bounds and showing theoretical/sample-complexity gains when bounds are available.

## Executive Summary
This paper addresses the best arm identification (BAI) problem when historical offline data may be biased relative to online distributions. Existing BAI algorithms either ignore such data or assume it aligns with online distributions, risking poor performance when biases exist. The authors prove that without prior knowledge of bias bounds, no adaptive algorithm can consistently outperform purely online methods. To address this, they propose LUCB-H, which uses auxiliary bias bounds to adaptively combine offline and online data within the LUCB framework. The algorithm computes both online-only and offline-corrected confidence bounds, then selects the tighter bounds per arm. Theoretically, LUCB-H matches the sample complexity of standard LUCB when offline data is misleading and improves it when data is helpful, achieving near-optimal performance in certain cases. Experiments on Gaussian bandit problems confirm LUCB-H's adaptability and robustness, showing it closely matches pure LUCB under misleading data and significantly outperforms it when data is beneficial.

## Method Summary
The LUCB-H algorithm extends the standard LUCB framework by computing two sets of confidence bounds per arm: online-only bounds and hybrid bounds that incorporate bias-corrected offline data. For each arm, it calculates both the standard LUCB confidence bounds and augmented bounds that include a bias correction term proportional to the provided bias bound V(i). The algorithm then adaptively selects the tighter (more conservative) bound for each arm by taking the maximum of the lower confidence bounds and minimum of the upper confidence bounds. This conservative selection automatically uses offline data when beneficial and discards it when misleading. The method requires as input a valid bias bound V(i) ≥ |μ^off(i) - μ^on(i)| for each arm. The stopping condition remains the same as standard LUCB: when the lower confidence bound of the arm with highest upper bound exceeds the upper confidence bound of the second-best arm.

## Key Results
- Proves impossibility result: no δ-PAC adaptive algorithm can outperform purely online methods without prior bias bound knowledge
- LUCB-H matches standard LUCB's sample complexity when offline data is misleading, while achieving savings when data is beneficial
- Sample complexity improvements of up to 50% observed when offline data has beneficial bias
- Algorithm remains robust to mild underestimation of bias bounds (Appendix E)

## Why This Works (Mechanism)

### Mechanism 1: Impossibility Result for Bias-Unaware Adaptive Algorithms
- Claim: No δ-PAC algorithm can adaptively decide when to use offline data without prior knowledge of the bias bound V(i) ≥ |μ^off(i) - μ^on(i)|.
- Mechanism: The paper constructs two instances (I_P and I_Q) that are statistically indistinguishable to any online-only policy but have opposite optimal arms. On I_P, offline data is helpful (same optimal arm online); on I_Q, it is misleading (different optimal arm). Any algorithm benefiting on I_P must suffer worse-than-baseline complexity on I_Q.
- Core assumption: The algorithm must be non-anticipatory (cannot know the true bias a priori) and δ-PAC (correct with probability ≥ 1-δ).
- Evidence anchors:
  - [abstract] "We prove an impossibility result for adaptive algorithms without prior knowledge of the bias bound between online and offline distributions."
  - [Section 3, Proposition 3.1] Formal construction showing E[τ_δ(Q)] ≥ Ω(δ^{-2β-ε} log(1/δ)) = ω(E[τ'_δ]) when V(i) is unknown.
  - [corpus] No direct corpus support for this specific impossibility result in BAI; related work (Cheung & Lyu 2024) establishes similar results for regret minimization but not fixed-confidence BAI.
- Break condition: When V(i) = ∞ (no prior knowledge), any algorithm attempting to use offline data risks sample complexity exceeding pure online methods.

### Mechanism 2: Adaptive Confidence Bound Selection via Conservative Tightening
- Claim: LUCB-H adaptively incorporates or discards offline data per-arm by computing confidence bounds both with and without historical data, then selecting the tighter (more conservative) bound.
- Mechanism: For each arm i, compute online-only bounds LCB_t(i), UCB_t(i) and hybrid bounds LCB^S_t(i), UCB^S_t(i) that include a bias-correction term (T_S(i)/(N_t(i)+T_S(i)))·V(i). Then use LCB_mix = max{LCB_t, LCB^S_t} and UCB_mix = min{UCB_t, UCB^S_t}. This automatically selects the bound that better constrains the true mean.
- Core assumption: A valid bias bound V(i) ≥ |μ^off(i) - μ^on(i)| is provided as input.
- Evidence anchors:
  - [abstract] "LUCB-H... introduces adaptive confidence bounds by incorporating an auxiliary bias correction to balance offline and online data."
  - [Section 4, Equations 4.1-4.4] Explicit formulas for both bound types with the bias correction term in rad^S_t(i).
  - [corpus] Batch Track-and-Stop (Agrawal et al. 2023) achieves similar savings but only when P^off = P^on; LUCB-H generalizes to biased cases.
- Break condition: When V(i) is severely underestimated (V(i) << |μ^off(i) - μ^on(i)|), UCBS_t may be tighter than UCB_t even for misleading data, causing over-reliance on biased offline samples.

### Mechanism 3: Sample Complexity Savings via Discrepancy-Dependent Reduction
- Claim: When offline data is "helpful" (discrepancy η(i) = V(i) + μ^off(i) - μ^on(i) < Δ_i/4), LUCB-H achieves sample complexity savings of Sav_u(i) = T_S(i)·max{1 - 4η(i)/Δ_i, 0} per arm.
- Mechanism: The saving term arises because tighter mixed confidence bounds reduce the online samples needed to separate the best arm from suboptimal arms. The condition η(i) < Δ_i/4 ensures the bias-correction penalty is small enough that hybrid bounds remain tighter than online-only bounds.
- Core assumption: The suboptimality gap Δ_i is sufficiently large relative to the bias bound and actual mean shift.
- Evidence anchors:
  - [Section 4, Theorem 4.1] Formal sample complexity bound E[τ_δ] = O(∑_{Δ_i>0}(Δ_i^{-2} log(1/δ) - Sav_u(i))).
  - [Section 6, Figures 1-4] Empirical validation showing LUCB-H matches pure LUCB in misleading cases and significantly outperforms in beneficial cases.
  - [corpus] Shivaswamy & Joachims (2012) showed logarithmic historical data can reduce regret to constant when distributions align; LUCB-H extends this insight to biased settings.
- Break condition: When η(i) ≥ Δ_i/4, Sav_u(i) = 0 and no savings occur—algorithm gracefully defaults to pure LUCB complexity.

## Foundational Learning

- Concept: **Fixed-Confidence Best Arm Identification (BAI)**
  - Why needed here: This is the core problem formulation—identifying the arm with highest expected reward with probability ≥ 1-δ while minimizing online samples. Understanding the distinction from cumulative regret minimization is essential.
  - Quick check question: Why does fixed-confidence BAI optimize for sample complexity rather than cumulative reward?

- Concept: **LUCB (Lower/Upper Confidence Bound) Algorithm**
  - Why needed here: LUCB-H extends the standard LUCB framework. You must understand how LUCB selects two arms per iteration (h_t with highest UCB, l_t with highest UCB among remaining) and stops when LCB(h_t) ≥ UCB(l_t).
  - Quick check question: Why does LUCB pull two arms per round instead of one, and how does this relate to PAC guarantees?

- Concept: **Distribution Shift / Offline-Online Bias**
  - Why needed here: The entire contribution addresses P^off ≠ P^on scenarios. Understanding practical examples (clinical trial demographics changing, recommendation system user preference drift) helps internalize why the impossibility result is unavoidable.
  - Quick check question: In a recommendation system, what factors could cause historical interaction data to become biased relative to current user behavior?

## Architecture Onboarding

- Component map:
  ```
  Inputs: {V(i)}_i (bias bounds), δ (confidence), S (offline samples)
      │
      ├─► Empirical Mean Computation: Ŷ_t(i) from online, X̂(i) from offline
      │
      ├─► Dual Confidence Bound Computation:
      │       ├─ Online-only: LCB_t, UCB_t (Eq. 4.1-4.2)
      │       └─ Hybrid: LCB^S_t, UCB^S_t with bias correction (Eq. 4.3-4.4)
      │
      ├─► Conservative Selection: LCB_mix = max{LCB_t, LCB^S_t}, UCB_mix = min{UCB_t, UCB^S_t}
      │
      ├─► Arm Selection: h_t = argmax UCB_mix, l_t = argmax_{i≠h_t} UCB_mix
      │
      └─► Stopping: if LCB_mix(h_t) ≥ UCB_mix(l_t) → output h_t
  ```

- Critical path:
  1. **Bias bound validation** (pre-deployment): Ensure V(i) ≥ |μ^off(i) - μ^on(i)| for all arms—underestimation is the primary failure mode.
  2. **Hybrid bound computation** (per-round): The bias correction term (T_S(i)/(N_t(i)+T_S(i)))·V(i) must be correctly scaled.
  3. **Conservative bound selection**: The min/max operations determine whether offline data is used—verify these are not inverted.

- Design tradeoffs:
  - **V(i) overestimation**: More conservative; algorithm ignores offline data more often, matching pure LUCB. Safe but may forfeit savings.
  - **V(i) underestimation**: Riskier; may over-rely on misleading data. Appendix E shows mild underestimation is tolerable, but severe cases degrade performance (Figure 10).
  - **Computational overhead**: Negligible—two bound computations instead of one, same O(k) per-round complexity as standard LUCB.

- Failure signatures:
  - **Persistent wrong-arm selection**: If V(best_arm) is severely underestimated, LUCB-H may converge to a suboptimal arm (Figure 10: when V < 0.2 for all arms in Group 2).
  - **No sample savings despite "clean" offline data**: Check if V(i) is too large relative to Δ_i; condition η(i) < Δ_i/4 may not hold.
  - **Stopping too early with high error**: Likely V(i) underestimation causing premature confidence in biased offline means.

- First 3 experiments:
  1. **Reproduce Group 1, Case 2 (staircase + beneficial bias)**: Verify LUCB-H achieves sample complexity reduction vs pure LUCB. Target: ~50% reduction with T_S=200, δ=0.01.
  2. **Reproduce Group 2, Case 1 (linear + misleading bias)**: Confirm LUCB-H matches pure LUCB complexity when offline data suggests wrong best arm. Target: overlapping confidence intervals with pure LUCB.
  3. **V(i) sensitivity stress test**: Systematically vary V from 0.1 to 0.5 for the best arm with true bias 0.2. Identify the underestimation threshold where performance degrades below pure LUCB.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the gap between the LUCB-H upper bound and the instance-dependent lower bound remain non-negative in all cases, not just the special cases analyzed?
- Basis in paper: [explicit] Remark 5.1 states: "While we believe that gap(i) ≥ 0 holds generally, we leave the proof of this to future work."
- Why unresolved: The authors proved gap(i) ≥ 0 only for two special cases (when V(i)'s are equal, and when V(1) = η(1)), but the general case proof remains open.
- What evidence would resolve it: A theoretical proof showing Savl(i) − Savu(i) ≥ 0 holds for all valid bias bounds V(i) and offline sample configurations, or a counterexample instance where the gap becomes negative.

### Open Question 2
- Question: Can the LUCB-H approach be extended to the fixed-budget setting where the stopping time is predetermined and the goal is to minimize error probability?
- Basis in paper: [inferred] The paper focuses exclusively on the fixed-confidence setting. Fixed-budget BAI is a standard variant in the literature (e.g., Successive Halving, Hyperband) where the total sample budget is fixed.
- Why unresolved: The confidence-bound approach and sample complexity analysis in LUCB-H are designed for fixed-confidence guarantees; adapting to fixed-budget requires different algorithmic structure and analysis.
- What evidence would resolve it: An algorithm that incorporates biased offline data in fixed-budget BAI with error probability bounds showing improvement when offline data is helpful.

### Open Question 3
- Question: What are the theoretical performance guarantees for LUCB-H when the bias bound V(i) is misspecified (either under- or over-estimated)?
- Basis in paper: [inferred] Appendix E empirically tests robustness to V(i) misspecification but provides no formal guarantees. The impossibility result (Proposition 3.1) assumes valid V(i), leaving the misspecified case theoretically open.
- Why unresolved: The theoretical analysis requires V(i) ≥ |μoff(i) − μon(i)|. When this is violated, the confidence bounds may not be valid, but the algorithm's failure mode and sample complexity bounds are uncharacterized.
- What evidence would resolve it: Theoretical bounds on error probability and sample complexity when V(i) deviates from the true bias by some factor, showing graceful degradation rather than catastrophic failure.

## Limitations
- Impossibility result assumes worst-case statistical indistinguishability; practical scenarios may exhibit detectable distributional differences
- Performance characterization incomplete when bias bounds are severely underestimated
- Empirical validation limited to Gaussian rewards with specific mean structures; performance on heavy-tailed or non-stationary distributions untested

## Confidence
- **High confidence**: The impossibility result construction and LUCB-H's basic adaptive mechanism (confidence bound selection) are mathematically rigorous and well-supported by the proofs
- **Medium confidence**: The sample complexity savings claims depend on the η(i) < Δ_i/4 condition, which is theoretically derived but may be conservative in practice
- **Low confidence**: The robustness of LUCB-H to partial bias bound knowledge and systematic V(i) underestimation is mentioned but not thoroughly characterized

## Next Checks
1. **Stress test V(i) underestimation**: Systematically evaluate LUCB-H performance as V(i) decreases from 2×|μ^off-μ^on| to 0.5×|μ^off-μ^on|, quantifying the exact performance degradation threshold
2. **Distribution generalization**: Test LUCB-H on heavy-tailed (e.g., Student-t) and bounded (e.g., Bernoulli) reward distributions to verify the algorithm's robustness beyond Gaussian assumptions
3. **Detectability analysis**: Investigate whether simple statistical tests (e.g., Kolmogorov-Smirnov) can distinguish the I_P vs I_Q instances from the impossibility construction, potentially enabling partial adaptation even without exact bias bounds