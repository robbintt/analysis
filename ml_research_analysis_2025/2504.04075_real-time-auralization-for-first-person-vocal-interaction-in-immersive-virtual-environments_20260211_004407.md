---
ver: rpa2
title: Real-Time Auralization for First-Person Vocal Interaction in Immersive Virtual
  Environments
arxiv_id: '2504.04075'
source_url: https://arxiv.org/abs/2504.04075
tags:
- virtual
- audio
- acoustic
- signal
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report presents a real-time auralization pipeline
  for first-person vocal interaction in immersive virtual environments. The system
  enables users to explore acoustic spaces from various positions and orientations
  within a predefined area, supporting three and five degrees of freedom (3DoF and
  5DoF) in audio-visual multimodal perception.
---

# Real-Time Auralization for First-Person Vocal Interaction in Immersive Virtual Environments

## Quick Facts
- arXiv ID: 2504.04075
- Source URL: https://arxiv.org/abs/2504.04075
- Authors: Mauricio Flores-Vargas; Enda Bates; Rachel McDonnell
- Reference count: 28
- Primary result: Real-time vocal auralization pipeline enabling 5DoF acoustic exploration in VR using pre-recorded spatial impulse responses

## Executive Summary
This technical report presents a real-time auralization pipeline for first-person vocal interaction in immersive virtual environments. The system enables users to explore acoustic spaces from various positions and orientations within a predefined area, supporting three and five degrees of freedom (3DoF and 5DoF) in audio-visual multimodal perception. The pipeline captures three-dimensional Spatial Impulse Responses (SIRs) from a first-person perspective using a grid of 20 recording positions with four directional orientations each. Real-time audio signal processing interpolates between these SIRs using directional constant-power panning and translational inverse distance weighting to convolve the user's voice with appropriate acoustic responses. The system addresses latency challenges through buffer optimization and direct monitoring, while compensating for headphone-induced amplitude loss.

## Method Summary
The system uses a grid of 20 recording positions (4×3m, 1m spacing) with four directional orientations each to capture 80 spatial impulse responses in 3rd-order Ambisonics format. These SIRs are processed to remove direct sound and Initial Time Delay Gap (ITDG), then real-time convolution is performed in a DAW with interpolation between responses based on user position (inverse distance weighting) and orientation (constant-power panning). A Unity application tracks HMD position/orientation and sends interpolation parameters via OSC to control the DAW's faders. The system employs direct monitoring for latency compensation and binaural decoding for headphone output.

## Key Results
- Real-time interpolation between 80 pre-recorded SIRs using constant-power panning for rotation and inverse distance weighting for translation
- Latency compensation through direct monitoring and ITDG-aligned delay compensation
- Support for both 3DoF and 5DoF audio-visual multimodal perception
- Headphone amplitude loss compensation for natural self-voice perception

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-time interpolation between discrete Spatial Impulse Responses (SIRs) creates a perceptually continuous acoustic field for a moving user.
- Mechanism: The system dynamically blends the audio output of multiple convolution processes. For rotation (yaw), it applies 4-way constant-power panning to blend between front, right, back, and left SIRs based on head orientation. For translation, it applies Inverse Distance Weighting (IDW) to amplitude-weight the SIRs from the four nearest grid nodes based on the user's (x, z) position.
- Core assumption: The acoustic properties of the environment vary smoothly enough that discrete spatial sampling (1-meter grid) and linear amplitude interpolation suffice for perceptual continuity without spectral artifacts.
- Evidence anchors: [abstract] "Real-time audio signal processing interpolates between these SIRs using directional constant-power panning and translational inverse distance weighting..."; [section 3.2] "IDW computes a weighted average of surrounding node values, where each weight is inversely proportional to the node's distance from the listener."

### Mechanism 3
- Claim: Feeding the user's dry voice via a near-zero latency path (direct monitoring) while perceptually aligning the processed reverb preserves natural self-voice perception and masks processing delay.
- Mechanism: The system uses the audio interface's analog direct monitoring to provide immediate bone-conduction-like feedback, compensating for the isolating effect of noise-canceling headphones. It then removes the direct sound and Initial Time Delay Gap (ITDG) from the SIRs and delays the wet signal so that the first virtual reflection arrives at a physically plausible time relative to the user's voice, preventing a perceptible "slap-back" echo from system latency.
- Core assumption: The user's auditory system will fuse the direct monitored signal and the delayed, convolved signal into a single, coherent acoustic percept of the room.
- Evidence anchors: [abstract] "...addresses latency challenges through buffer optimization and direct monitoring..."; [section 3.1] "...cropped the direct sound and the Initial Time Delay Gap (ITDG) of the SIRs... and then aligned them using a delay plugin, with the delay time compensation obtained by subtracting the round-trip latency from the ITDG."

## Foundational Learning

- Concept: Convolution with Impulse Responses (IRs)
  - Why needed here: This is the core operation for auralization. An IR is an acoustic "fingerprint" of a space. Convolving an input signal (the voice) with an IR applies the reverb, echoes, and frequency coloration of that space to the signal.
  - Quick check question: What is the difference between an impulse response and a reverb plugin preset?

- Concept: Ambisonics and Binaural Decoding
  - Why needed here: The system uses 3rd-order Ambisonics (3OA) as an intermediate format to capture a full 3D sound field. To be heard over headphones, this 16-channel 3OA signal must be decoded to 2-channel binaural stereo, using Head-Related Transfer Functions (HRTFs) to place sounds in 3D space.
  - Quick check question: Why can't you listen to a 3OA (16-channel) audio file directly on standard headphones?

- Concept: Degrees of Freedom (DoF) in VR
  - Why needed here: The paper's goal is 5DoF audio-visual perception. This defines the user's movement capabilities: 3 rotational axes (roll, pitch, yaw) plus 2 translational axes (typically forward/back, left/right). This dictates the complexity of the interpolation system.
  - Quick check question: What is the 6th degree of freedom that this system deliberately omits, and why is it harder to implement?

## Architecture Onboarding

- Component map: Zylia ZM-1 microphone and Genelec 1029A speaker capture SIRs on grid → MATLAB segmentation and Voxengo deconvolution process recordings → Reaper DAW performs multichannel convolution → Unity C# scripts track HMD position/orientation and send OSC control messages → binaural decoder converts 3OA to stereo for headphone output

- Critical path: The round-trip from microphone input, through A/D conversion, software buffering, convolution, binaural decoding, D/A conversion, and headphone output. This path is the primary source of latency and must be kept shorter than the virtual room's ITDG.

- Design tradeoffs:
  - **Grid Density vs. Computational Load:** A denser grid captures more acoustic detail but requires more real-time convolution instances, increasing CPU usage.
  - **Ambisonic Order vs. Spatial Accuracy:** Higher-order Ambisonics (HOA) provide sharper spatial detail but require more recording channels and processing power. 3OA is a chosen balance.
  - **Headphone Type:** Noise-canceling headphones block real room acoustics (good for immersion) but block natural self-voice (bad for naturalness), requiring the direct monitoring workaround.

- Failure signatures:
  - **Audible "Phasiness" or Comb Filtering:** During movement, this indicates a problem with the interpolation algorithm or non-aligned SIRs.
  - **"Slap-back" Echo:** The wet signal is arriving too late, meaning system latency is higher than the virtual ITDG.
  - **Muffled "Boxy" Self-Voice:** The direct monitoring level is incorrect, or the headphone isolation is too high, disrupting natural bone conduction perception.

- First 3 experiments:
  1. **Static Latency Budget Test:** With the real-time processing active but SIRs bypassed, measure the round-trip latency of the entire audio chain (mic in to headphone out) using a test signal. This establishes the baseline delay that must be compensated for.
  2. **Single-Point Rotation Validation:** At a single grid point, verify the directional interpolation works by having a user slowly rotate (yaw) and confirm that sounds pan smoothly and correctly around them without level drops or jumps.
  3. **Grid Interpolation Continuity Check:** Have a user move slowly between adjacent grid points while speaking. Listen for any abrupt changes in reverb character, volume, or spatial position, which would indicate interpolation artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the perceptual validity of the auralization system for vocal self-perception tasks, and how does it compare to real acoustic environments?
- Basis in paper: [explicit] The paper presents the system as a "case study report" with detailed technical implementation but includes no user study, perceptual evaluation, or comparison to real acoustic spaces to validate effectiveness.
- Why unresolved: The technical pipeline is described, but no empirical evidence is provided that users perceive the virtual acoustics as natural or that the system supports realistic vocal adaptation as intended.
- What evidence would resolve it: Controlled perceptual studies comparing singer/speaker performance and self-perception in the virtual system versus the actual recorded space, including measures of presence, acoustic realism, and vocal parameter adjustment.

### Open Question 2
- Question: How does the absence of height variation (the third translational DoF) affect perceptual accuracy and user experience during standing vocal performances or natural postural shifts?
- Basis in paper: [explicit] The authors state the system "is limited to two translational degrees of freedom, as the height remains fixed across all recordings" and that capturing elevation "would incur significant costs," but they justify this by claiming "performers typically maintain a relatively stable vertical position" without empirical validation.
- Why unresolved: The assumption about stable vertical position lacks supporting data; postural changes during singing or speaking could introduce perceptible acoustic differences.
- What evidence would resolve it: Perceptual studies measuring just-noticeable differences for height changes in first-person acoustic perception, and comparison of system performance with and without height interpolation during realistic vocal tasks.

### Open Question 3
- Question: Under what acoustic or spatial conditions should the ITDG calculation include or exclude floor reflections, and how does this choice affect perceived realism?
- Basis in paper: [explicit] The authors state they "did not consider this reflection due to its negligible significance, attributed to the shadowing effect of the singer's torso and the voice's directivity," but explicitly acknowledge that "different scenarios may require estimating the ITDG based on the floor reflection."
- Why unresolved: The floor reflection exclusion was based on assumptions without systematic testing across different room geometries, performer body types, or vocal techniques.
- What evidence would resolve it: Comparative studies measuring the perceptual impact of floor reflections across varied room configurations, speaker directivity patterns, and performer positions, with binaural measurements accounting for torso shadowing.

## Limitations
- No perceptual validation or user studies to verify effectiveness of the auralization system
- Limited to two translational degrees of freedom (height remains fixed)
- Floor reflection handling based on assumptions without systematic testing
- No quantification of acceptable latency thresholds for different vocal tasks

## Confidence
- **High Confidence:** The technical description of the capture process, the use of convolution with pre-recorded SIRs, and the basic architecture (Unity + DAW + OSC) are well-specified and plausible.
- **Medium Confidence:** The interpolation algorithm (constant-power panning, IDW) is theoretically sound for smooth acoustic variations, but its perceptual effectiveness for this specific application is unverified.
- **Low Confidence:** The latency compensation strategy (direct monitoring + delay alignment) is described, but without measured latency values or perceptual testing, it's unclear if the "masking" actually works across different systems and room IRs.

## Next Checks
1. **Perceptual Continuity Test:** Conduct a controlled listening test where users move between adjacent grid points while speaking. Measure the just-noticeable difference (JND) in reverb character and spatial position. This directly validates the core claim of "perceptually continuous" interpolation.

2. **Latency Budget Verification:** Using a calibrated audio interface, measure the actual round-trip latency of the entire processing chain (mic-in to headphone-out) with the real-time convolution active. Verify that this measured latency is indeed less than the ITDG of the shortest virtual room SIR used, confirming the latency compensation is feasible.

3. **Interpolation Artifact Analysis:** At a single grid position, have users rapidly rotate their head (simulating a "fast look around"). Use a high-sample-rate recorder to capture the output and analyze the signal for any clicks, jumps, or spectral discontinuities that would indicate problems with the constant-power panning implementation or gain smoothing.