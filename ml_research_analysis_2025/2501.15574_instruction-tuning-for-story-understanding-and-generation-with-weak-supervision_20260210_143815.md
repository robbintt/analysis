---
ver: rpa2
title: Instruction Tuning for Story Understanding and Generation with Weak Supervision
arxiv_id: '2501.15574'
source_url: https://arxiv.org/abs/2501.15574
tags:
- story
- instruction
- generation
- weak
- strong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for improving story understanding
  and generation by leveraging large language models with weak to strong instruction
  tuning. The proposed approach progressively refines the model's ability to generate
  coherent and engaging narratives by initially using high-level, minimal instructions
  and gradually introducing more specific, detailed guidance.
---

# Instruction Tuning for Story Understanding and Generation with Weak Supervision

## Quick Facts
- **arXiv ID**: 2501.15574
- **Source URL**: https://arxiv.org/abs/2501.15574
- **Reference count**: 33
- **Primary result**: Progressive weak-to-strong instruction tuning significantly improves story generation quality over baseline models

## Executive Summary
This paper introduces a novel method for improving story understanding and generation by leveraging large language models with weak to strong instruction tuning. The proposed approach progressively refines the model's ability to generate coherent and engaging narratives by initially using high-level, minimal instructions and gradually introducing more specific, detailed guidance. The method demonstrates significant improvements in both automatic evaluation metrics and human evaluations, outperforming existing state-of-the-art models in tasks such as BLEU score, ROUGE-L, and perplexity.

## Method Summary
The approach uses a three-phase curriculum: (1) pretraining on general corpus with standard language modeling objective, (2) weak instruction tuning using high-level prompts, and (3) strong instruction fine-tuning with detailed, specific instructions. The model employs a Transformer encoder-decoder architecture where the encoder processes instructions and the decoder generates stories autoregressively. The total loss combines contributions from all phases via weighted sum: L_total = λ₁L_pretrain + λ₂L_weak + λ₃L_strong, balancing general language capabilities with instruction-following specialization.

## Key Results
- Strong instruction BLEU-1 score of 0.79 versus weak instruction score of 0.71, with the gap widening as instruction specificity increases
- Significant improvements in human evaluations across Fluency, Coherence, and Relevance metrics (1-5 scale)
- Outperforms baseline models on automatic metrics including ROUGE-L and perplexity
- Effective balance between structured guidance and creative freedom demonstrated across various story domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Progressive training from weak to strong instructions improves story generation by first establishing foundational narrative structures, then refining details.
- **Mechanism**: Weak instructions (e.g., "generate a story about a dragon") train the model on basic plot/role patterns. Strong instructions (e.g., "dragon overcomes fear of fire") then specialize parameters for nuanced content. The staged approach prevents the model from being overwhelmed by complex guidance before mastering simpler structures.
- **Core assumption**: Foundational patterns learned from weak instructions transfer positively to strong instruction refinement rather than locking in shallow behaviors.
- **Evidence anchors**:
  - [abstract]: "initially using high-level, minimal instructions and gradually introducing more specific, detailed guidance"
  - [section III.B]: "These phases help the model progressively learn how to generate high-quality stories from weak to strong instructional prompts"
  - [corpus]: "Representations Shape Weak-to-Strong Generalization" (arXiv:2502.00620) provides theoretical grounding for W2SG but does not validate curriculum ordering specifically for story generation
- **Break condition**: If weak instruction training causes overfitting to generic patterns that resist specialization during strong instruction phase (no ablation on reverse ordering provided).

### Mechanism 2
- **Claim**: Aggregating losses from all training phases via weighted sum preserves general language capabilities while specializing for instruction-following.
- **Mechanism**: L_total = λ₁L_pretrain + λ₂L_weak + λ₃L_strong combines gradient signals, potentially preventing catastrophic forgetting where strong instruction tuning would otherwise overwrite foundational knowledge.
- **Core assumption**: Hyperparameters λ₁, λ₂, λ₃ can be tuned to achieve stable multi-objective optimization without gradient conflict.
- **Evidence anchors**:
  - [section III.C]: "λ₁, λ₂, and λ₃ are hyperparameters that control the contribution of each phase to the total loss"
  - [abstract]: "effectively balances structured guidance and creative freedom"
  - [corpus]: No direct corpus evidence for this specific loss aggregation strategy; related work on instruction tuning does not address multi-phase loss combination
- **Break condition**: If gradient directions from different phases conflict, causing training instability or suboptimal convergence at any phase boundary.

### Mechanism 3
- **Claim**: Models exposed to varying instruction specificity learn to extract more signal from detailed prompts, with performance gains amplifying as instruction clarity increases.
- **Mechanism**: Separate parameter updates (θ_weak, θ_strong) create specialized representations for different instruction granularities. During inference, the attention mechanism leverages both for instruction-conditioned generation.
- **Core assumption**: The model generalizes its training on weak/strong instruction patterns to appropriately weight new instructions at inference time.
- **Evidence anchors**:
  - [section IV.C, Table II]: Strong instruction BLEU-1 = 0.79 vs Weak = 0.71; gap widens with specificity
  - [section IV.E.1]: "as the instruction specificity increases, the gap between our method and the baseline methods widens"
  - [corpus]: "The Atomic Instruction Gap" (arXiv:2510.17388) suggests instruction-tuned models struggle with simple directives—potentially contradicting robust weak-instruction performance if the model over-relies on specificity cues
- **Break condition**: If the model cannot calibrate appropriately when instruction specificity falls between training extremes (no interpolation analysis provided).

## Foundational Learning

- **Concept: Transformer Encoder-Decoder Architecture**
  - Why needed here: The method builds on standard Transformer with encoder processing instructions and decoder generating stories autoregressively via cross-attention.
  - Quick check question: Can you trace how encoder hidden states (h1...hn) condition decoder token generation through the attention mechanism in Equation 1?

- **Concept: Cross-Entropy Language Modeling Loss**
  - Why needed here: All three phases use next-token prediction loss, differing only in conditioning context (unconditional, weak instruction, strong instruction).
  - Quick check question: For L_weak = -Σ log p(yt|xweak, θweak), what specifically does xweak represent and how does it differ from xstrong?

- **Concept: Autoregressive Decoding**
  - Why needed here: Story generation produces tokens sequentially, each conditioned on prior outputs and encoded instructions.
  - Quick check question: How might the model handle the trade-off between staying faithful to instructions versus maintaining narrative coherence when tokens begin to drift from the prompt?

## Architecture Onboarding

- **Component map**: Pretraining corpus -> Transformer encoder-decoder -> Weak instruction fine-tuning -> Strong instruction fine-tuning -> Evaluation
- **Critical path**:
  1. Pretrain on general corpus with standard LM objective
  2. Fine-tune with weak instructions on STORYWARS/LongForm-C
  3. Fine-tune with strong instructions (detailed prompts for same/augmented data)
  4. Tune λ weights for loss aggregation
  5. Evaluate on both instruction types plus domain generalization
- **Design tradeoffs**:
  - Phase ordering: Paper assumes weak→strong is optimal; no reverse-order ablation
  - λ weight selection: No sensitivity analysis provided
  - Data allocation: Proportions of weak vs strong instruction examples not specified
  - Base model: Transformer seq2seq assumed; no architecture comparison
- **Failure signatures**:
  - Catastrophic forgetting: Strong phase overwrites weak-phase learning (watch for BLEU drop on weak instruction eval)
  - Over-constrained output: Excessive strong training reduces creativity scores in human eval
  - Instruction confusion: Model fails to distinguish instruction types at inference, producing generic outputs regardless of specificity
- **First 3 experiments**:
  1. **Reverse curriculum test**: Train strong→weak ordering to validate curriculum assumption; expect degraded performance if weak-first is mechanistically important
  2. **λ sensitivity sweep**: Grid search λ₁, λ₂, λ₃ to identify stable regions; monitor for training instability at boundary values
  3. **Instruction interpolation**: Create test set with instructions varying continuously in specificity (not binary weak/strong) to assess whether model generalizes or overfits to training extremes

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the integration of multimodal inputs (e.g., images or video) further enhance the model's ability to generate contextually rich narratives compared to text-only instruction tuning?
- **Open Question 2**: How does the model perform in interactive settings where the narrative must adapt dynamically to real-time user feedback or changing constraints?
- **Open Question 3**: What is the optimal scheduling strategy (pacing) for transitioning from weak to strong instruction phases to maximize generalization without overfitting?

## Limitations
- **Curriculum ordering validity**: The paper asserts weak-to-strong is optimal but provides no ablation comparing alternative orderings
- **Loss aggregation stability**: Weighted sum approach assumes gradient compatibility but no sensitivity analysis or stability testing is provided
- **Instruction specificity calibration**: Binary weak/strong distinction may oversimplify; model's ability to handle intermediate specificity is untested

## Confidence
- **High Confidence Claims**:
  - The method improves story generation quality relative to baseline models
  - Progressive instruction tuning enhances both structured guidance and creative freedom
  - The approach generalizes across different domains and instruction types
- **Medium Confidence Claims**:
  - Weak-to-strong curriculum specifically drives improvements (no reverse-order validation)
  - Loss aggregation via weighted sum is optimal (no λ sensitivity analysis)
  - Instruction specificity gap widening is solely attributable to the proposed method (no interpolation analysis)
- **Low Confidence Claims**:
  - The method's behavior on out-of-distribution instruction specificity
  - Long-term stability of multi-phase training across different base models
  - Generalizability beyond story generation to other generation tasks

## Next Checks
1. **Curriculum Order Ablation**: Train the model using strong→weak instruction ordering and compare performance to the proposed weak→strong approach. If weak-first instruction is mechanistically important, the reversed curriculum should show degraded performance on both weak and strong instruction evaluations.

2. **λ Weight Sensitivity Analysis**: Conduct a grid search over λ₁, λ₂, λ₃ values while monitoring training stability, BLEU scores, and human evaluation metrics. Identify regions where loss aggregation causes gradient conflict versus stable multi-objective optimization.

3. **Continuous Instruction Interpolation**: Create a test set with instructions varying continuously in specificity from weak to strong (not binary categories). Evaluate whether the model generalizes to intermediate instruction types or exhibits sensitivity to training extremes, revealing whether it learned genuine instruction understanding or overfitting patterns.