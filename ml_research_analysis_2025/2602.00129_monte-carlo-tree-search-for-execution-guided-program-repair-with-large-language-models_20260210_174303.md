---
ver: rpa2
title: Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language
  Models
arxiv_id: '2602.00129'
source_url: https://arxiv.org/abs/2602.00129
tags:
- search
- code
- language
- tree
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodePilot integrates Monte Carlo Tree Search with Qwen3 to improve
  execution-guided program repair for real-world GitHub issues. The framework addresses
  limitations of autoregressive decoding by exploring diverse patch trajectories with
  MCTS, using execution feedback as a reward signal.
---

# Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models

## Quick Facts
- arXiv ID: 2602.00129
- Source URL: https://arxiv.org/abs/2602.00129
- Authors: Yixuan Liang
- Reference count: 10
- One-line primary result: CodePilot achieves 24.67% issue resolution rate on SWE-bench Lite using MCTS-guided synthesis with Qwen3.

## Executive Summary
CodePilot addresses the limitations of autoregressive decoding in program repair by integrating Monte Carlo Tree Search (MCTS) with execution feedback. The framework explores diverse patch trajectories through a four-phase tree search, using test execution as a reward signal to guide synthesis. It combines hierarchical fault localization, confidence-calibrated generation with Qwen3's thinking mode, and iterative self-refinement to improve patch quality. Experiments on SWE-bench Lite demonstrate state-of-the-art performance among open-weight models.

## Method Summary
CodePilot employs a two-stage training pipeline: supervised fine-tuning on historical GitHub issues followed by reinforcement learning with execution rewards. The system uses hierarchical fault localization (embedding-based file-level, AST-based function-level) to identify buggy code regions, then applies MCTS at the statement level to explore diverse patch trajectories. Each MCTS iteration samples continuations from Qwen3, executes tests, and backpropagates rewards. The thinking mode generates explicit reasoning traces for self-refinement when execution fails. The framework uses LoRA fine-tuning (rank=16) and vLLM for efficient inference with tiered evaluation to reduce computational cost.

## Key Results
- 24.67% issue resolution rate on SWE-bench Lite, outperforming comparable baselines using open-weight models
- 85.33% patch applicability rate, demonstrating syntactic correctness of generated patches
- Ablation studies show MCTS contributes +4.34% and thinking mode adds +3.67% to performance

## Why This Works (Mechanism)

### Mechanism 1
MCTS enables exploration of diverse patch trajectories that autoregressive decoding cannot reach due to greedy token selection. Four-phase tree search operates at the semantic statement level: Selection traverses via UCT scores combining value estimates with model priors; Expansion samples continuations from the LLM; Simulation completes patches via greedy decoding and executes tests; Backpropagation updates node values using execution reward. Execution feedback provides reliable gradient for guiding search toward correct patches. Break condition: If test suites have poor coverage or execution feedback is uncorrelated with actual bug fixes, reward signal becomes noise.

### Mechanism 2
Hierarchical fault localization progressively narrows the search space, reducing burden on patch synthesis. Three-level cascade: (1) file-level hybrid scoring combining dense embeddings with BM25 via weighted sum; (2) function-level extraction via AST parsing; (3) edit-location identification with dependency ordering rather than file order to provide implicit call-graph context. Break condition: If bugs involve subtle logic errors without clear semantic signals, or span multiple distant files, localization accuracy degrades.

### Mechanism 3
Dual-mode reasoning (thinking/non-thinking) combined with execution feedback enables iterative self-correction of patches. Qwen3's thinking mode generates explicit chain-of-thought traces before code; when execution fails, model receives patch and failure output to produce self-critique and revised patch. Loop terminates on test pass, max iterations, or quality plateau. Break condition: If failure modes are cryptic or require domain knowledge not present in model's training, self-critique becomes superficial.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS):** Core search algorithm replacing greedy decoding; understanding UCT, exploration-exploitation, and reward backpropagation is essential. Quick check: Can you explain why UCT balances Q-values with visit counts, and why operating at statement-level rather than token-level matters for code?

- **Transformer Decoding and Autoregressive Limitations:** Paper frames MCTS as addressing greedy decoding's inability to recover from low-probability token choices. Quick check: Why does standard temperature-sampled autoregressive generation struggle with long-horizon reasoning tasks like multi-file bug repair?

- **Execution-Based Verification for Code:** Reward signal R(p) is entirely execution-derived; understanding test pass/fail as sparse reward is critical. Quick check: What are the failure modes when using test execution as sole quality signal for program repair?

## Architecture Onboarding

- **Component map:**
GitHub Issue + Repository → Hierarchical Fault Localization → MCTS-Guided Patch Synthesis → Execution-Driven Self-Refinement → Final Patch

- **Critical path:** Fault localization accuracy → MCTS reward signal quality → Self-refinement effectiveness. Ablation shows localization errors cause 33% of failures; MCTS provides largest single gain.

- **Design tradeoffs:**
MCTS exploration (c=1.4, N=16) vs computational cost—tiered evaluation mitigates but doesn't eliminate overhead. LoRA fine-tuning (r=16) for parameter efficiency vs full fine-tuning for potential performance gains. Thinking mode provides deeper analysis but increases latency per iteration.

- **Failure signatures:**
Low Apply Rate: localization failing to identify correct edit locations. High Apply Rate, low Resolve Rate: patches syntactically valid but semantically wrong (reward hacking). Self-refinement loops without progress: execution feedback unactionable or model lacking domain knowledge.

- **First 3 experiments:**
1. Ablation replication: Run CodePilot with MCTS disabled vs enabled on 50-instance subset to verify +4.34% contribution.
2. Localization accuracy audit: Measure Loc@1, Loc@3, Loc@5 on held-out issues; correlate with final resolve rate.
3. Reward function sensitivity: Test alternative reward formulations to assess robustness of MCTS guidance signal.

## Open Questions the Paper Calls Out
- How can MCTS-guided synthesis be adapted to handle complex multi-file modifications effectively?
- To what extent can static analysis tools be integrated into MCTS loop to improve pruning and reward signals?
- How can hierarchical fault localization accuracy be improved to overcome identified bottleneck where localization errors cause one-third of failures?

## Limitations
- SFT dataset composition and exact training hyperparameters not specified, impacting reproducibility
- Dense embedding model for fault localization unspecified, leaving critical pipeline component ambiguous
- 14.67% gap between Apply Rate and Resolve Rate suggests reward signal may not fully capture semantic correctness

## Confidence
- **High Confidence:** Core contribution of integrating MCTS with execution feedback is well-supported by ablation results (+4.34%)
- **Medium Confidence:** Hierarchical fault localization approach shows promise but lacks direct empirical validation
- **Medium Confidence:** Dual-mode reasoning for self-correction is supported by +3.67% ablation gain but effectiveness depends on execution feedback quality

## Next Checks
1. Ablation replication: Run CodePilot with MCTS disabled vs enabled on 50-instance subset of SWE-bench Lite to verify claimed +4.34% contribution
2. Localization accuracy audit: Measure file-level and function-level Loc@k on held-out validation set to quantify how localization errors correlate with final resolve rate
3. Reward function sensitivity: Test alternative reward formulations on small validation set to assess robustness of current reward design