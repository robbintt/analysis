---
ver: rpa2
title: 'ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search'
arxiv_id: '2505.15259'
source_url: https://arxiv.org/abs/2505.15259
tags:
- reguide
- grounding
- reasoning
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReGUIDE addresses the challenge of data-efficient GUI grounding
  for autonomous agents, which requires accurately localizing interface elements like
  buttons. The method combines reinforcement learning for self-generated reasoning
  with spatial consistency training and test-time spatial search with kernel density
  estimation-based voting.
---

# ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search

## Quick Facts
- arXiv ID: 2505.15259
- Source URL: https://arxiv.org/abs/2505.15259
- Reference count: 40
- Key outcome: ReGUIDE achieves 87.3% accuracy on SCREEN SPOT and 44.3% on SCREEN SPOT-PRO using only 0.2% of baseline training data.

## Executive Summary
ReGUIDE addresses the challenge of data-efficient GUI grounding for autonomous agents, which requires accurately localizing interface elements like buttons. The method combines reinforcement learning for self-generated reasoning with spatial consistency training and test-time spatial search with kernel density estimation-based voting. ReGUIDE significantly improves grounding accuracy—for example, increasing accuracy from 55.5% to 87.3% on SCREEN SPOT and from 23.9% to 44.3% on SCREEN SPOT-PRO—while using only 0.2% of the training data compared to baselines. The improvements in grounding directly enhance downstream agentic task performance.

## Method Summary
ReGUIDE employs a two-stage training pipeline on MLLM backbones (Qwen-2.5-VL-3B/7B). First, GRPO reinforcement learning uses grounding accuracy as reward to evolve self-generated reasoning traces without human annotations. Second, consistency fine-tuning enforces spatial equivariance by training on global-local image view pairs. At inference, a two-stage spatial search samples multiple predictions, uses KDE to identify high-density regions, crops around them, and resamples for final coordinate selection.

## Key Results
- ScreenSpot accuracy improves from 55.5% to 87.3% with full ReGUIDE pipeline
- ScreenSpot-Pro accuracy increases from 23.9% to 44.3% while using only 0.2% of training data
- Test-time spatial search with KDE voting improves accuracy by 3 percentage points over sampling alone
- Consistency training provides additional 1.6 percentage point gain beyond RL alone

## Why This Works (Mechanism)

### Mechanism 1: Reinforcement Learning for Self-Generated Spatial Reasoning
Online RL enables the model to discover useful reasoning strategies for GUI localization without human-annotated reasoning traces. The model generates (reasoning, coordinate) pairs; coordinates within ground-truth bounding boxes yield positive reward. GRPO samples multiple outputs per input and computes group-relative advantages, reinforcing reasoning patterns that correlate with accurate localization.

### Mechanism 2: Spatial Consistency Training via Global-Local View Alignment
Enforcing consistent predictions across transformed image views improves spatial precision and robustness to scale/position variations. After RL training, the model is fine-tuned on (global view, local crop) pairs sharing the same reasoning trace but requiring adjusted coordinates. This enforces equivariance: reasoning remains stable while coordinates adapt to transformed images.

### Mechanism 3: Test-Time Spatial Search with KDE-Based Aggregation
Sampling multiple predictions and aggregating via Kernel Density Estimation improves localization, especially for high-resolution images where initial predictions may be coarse. Two-stage inference: sample N predictions on full image, use KDE to find highest-density region, crop around it; then sample M predictions within crop, apply KDE again to select final coordinate.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: ReGUIDE uses GRPO rather than vanilla REINFORCE or PPO. Understanding how group-relative advantages stabilize training helps diagnose RL divergence.
  - Quick check question: Can you explain why computing advantages relative to a group mean (rather than a learned value function) might be more stable for sparse-reward grounding tasks?

- Concept: **Equivariance vs Invariance in Spatial Transformations**
  - Why needed here: ReGUIDE enforces equivariance (coordinates change predictably under crops) rather than invariance. This distinction is critical for implementing the consistency loss correctly.
  - Quick check question: Given a crop operation, should the *reasoning tokens* or the *coordinate tokens* change? Which should stay invariant?

- Concept: **Kernel Density Estimation for Coordinate Aggregation**
  - Why needed here: The inference strategy relies on KDE rather than simple mean or medoid voting. Understanding bandwidth selection and Gaussian kernels is necessary for tuning.
  - Quick check question: If predictions cluster tightly but one outlier is far away, how does KDE behave differently from taking the arithmetic mean?

## Architecture Onboarding

- Component map: Input: (image, instruction) -> MLLM Backbone -> generates: <reasoning>...</reasoning><answer>(x,y)</answer> -> Training Pipeline (GRPO stage, Consistency SFT stage) -> Inference Pipeline (Sample N predictions, KDE, crop, Sample M predictions, KDE voting)

- Critical path: RL stage must converge before consistency training; consistency SFT requires high-quality reasoning traces; inference requires N≥8 samples for stable KDE

- Design tradeoffs:
  - Data efficiency vs. compute: Uses 0.2% of training data but requires 16 samples at inference per query
  - Reasoning quality vs. reward hacking: λ=0.1 for format reward balances structure enforcement vs. gaming format without accuracy
  - Crop size W_RoI: 840px balances zoom precision vs. risk of excluding target

- Failure signatures:
  - Output length collapses during RL -> REINFORCE++ or insufficient KL penalty
  - Accuracy degrades after consistency training -> crop ratio too aggressive
  - KDE selects outlier -> N too small or temperature too low

- First 3 experiments:
  1. Sanity check on base model: Evaluate unmodified Qwen-2.5-VL-3B on ScreenSpot; should be ~55%
  2. RL convergence diagnosis: Plot reward and response length during GRPO training
  3. Inference ablation: Run with N=1, N=16 without crop, N=16 with crop+vote

## Open Questions the Paper Calls Out

- Question: How can MLLM planners be trained to operate hierarchically with ReGUIDE to fully capitalize on its precise grounding performance?
- Basis in paper: Authors state in Future Works: "We believe it will be an interesting future direction to train MLLM planners that can operate hierarchically with ReGUIDE, fully capitalizing on its precise grounding performance."
- Why unresolved: Current work only demonstrates grounding improvements; hierarchical planning integration remains unexplored.

## Limitations

- RL training stability depends on accurate reward signals and may fail if ground-truth annotations contain errors
- Spatial consistency training requires high-quality reasoning traces from RL stage, amplifying errors if RL produces degenerate patterns
- Test-time spatial search assumes Gaussian-distributed prediction errors, which may not hold for all model architectures

## Confidence

- High confidence: Core performance claims (87.3% ScreenSpot accuracy, 44.3% ScreenSpot-Pro accuracy) are well-supported by ablation studies
- Medium confidence: Claims about RL discovering "useful reasoning strategies" are supported but could be confounded by reward hacking
- Low confidence: Claims about "data efficiency" are relative to unspecified baselines

## Next Checks

1. Ablation of RL training stability: Run GRPO training with KL penalty ablation to verify that response length collapse is a real failure mode and that the specified 0.01 KL coefficient prevents it across different random seeds.

2. Test-time search parameter sensitivity: Systematically vary KDE bandwidth (0.001, 0.01, 0.1) and sample counts (N=8, 16, 32) to determine if the claimed improvements are robust to parameter choices.

3. Cross-dataset generalization: Evaluate ReGUIDE on a held-out GUI dataset (e.g., RICO or AppFluents) without additional training to assess whether the learned reasoning strategies transfer beyond the UGround training distribution.