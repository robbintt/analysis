---
ver: rpa2
title: 'ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping
  for Natural Disaster Damage Assessment'
arxiv_id: '2506.00238'
source_url: https://arxiv.org/abs/2506.00238
tags:
- question
- remote
- visual
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a zero-shot visual question answering (VQA)
  framework for post-disaster damage assessment using large-scale Vision-Language
  Models (VLMs). Traditional VQA models are limited to predefined answer lists, requiring
  costly retraining for new datasets.
---

# ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment

## Quick Facts
- arXiv ID: 2506.00238
- Source URL: https://arxiv.org/abs/2506.00238
- Authors: Ehsan Karimi; Maryam Rahnemoonfar
- Reference count: 0
- Primary result: 89.1% overall accuracy on FloodNet dataset for zero-shot VQA

## Executive Summary
This paper introduces ZeShot-VQA, a zero-shot visual question answering framework for post-disaster damage assessment using large-scale Vision-Language Models (VLMs). Traditional VQA models require predefined answer lists and costly retraining for new datasets, limiting their scalability for disaster response. The proposed method overcomes this by employing zero-shot learning and answer mapping to generate accurate responses for unseen questions without fine-tuning. Tested on the FloodNet dataset, ZeShot-VQA achieves 89.1% overall accuracy, outperforming baseline models across building condition, risk assessment, and density estimation tasks, though counting tasks remain challenging.

## Method Summary
The framework uses a three-stage zero-shot approach: (1) Prompt modification appends all valid answer choices to input questions to guide the generative model's output space, (2) BLIP-2 generates raw answers from the modified question and image, and (3) A BERT-based answer mapping mechanism computes cosine similarity between the generated answer embedding and each valid answer embedding to select the closest match. This approach enables deployment to new disaster types without collecting new annotated training data.

## Key Results
- 89.1% overall accuracy on FloodNet dataset, outperforming ViLT (88.2%) and CLIP (88.4%)
- Counting tasks remain challenging with accuracy below 40%
- Answer mapping successfully bridges vocabulary gaps between generated answers and dataset labels
- Zero-shot transfer enables deployment without fine-tuning on new disaster datasets

## Why This Works (Mechanism)

### Mechanism 1
Prompt modification constrains the generative model's output space toward domain-relevant answers without fine-tuning. The framework appends all valid answer choices directly to the input question, conditioning the VLM's generative process by making the target vocabulary explicit in the context window. Core assumption: The pretrained VLM has sufficient semantic understanding to recognize appended options as valid answer candidates.

### Mechanism 2
Image-grounded text encoding enables answer generation conditioned on visual features from aerial imagery. The image encoder processes the input image and shares the resulting feature vector with the text encoder, which extracts image-grounded features from the modified question. Core assumption: The pretrained VLM's visual encoder has encountered sufficient aerial/satellite-like imagery during pretraining to extract meaningful features from disaster scenes.

### Mechanism 3
Answer mapping bridges vocabulary gaps between generative model outputs and dataset-specific answer labels. After the VLM generates a raw answer, a BERT-based text encoder computes embeddings for the question concatenated with each valid answer and the question concatenated with the raw answer. Cosine similarity identifies the closest valid answer, mapping outputs like "scarce" to acceptable choices like "low." Core assumption: The semantic similarity space captured by the text encoder meaningfully aligns generated synonyms with ground-truth labels.

### Mechanism 4
Zero-shot transfer enables deployment to new disaster types without collecting new annotated training data. By relying entirely on frozen pretrained models with no gradient updates, the framework treats inference as a pure forward-pass operation. Domain adaptation occurs only through prompt engineering rather than weight modification. Core assumption: The combination of prompt modification and answer mapping sufficiently compensates for the domain gap without parameter updates.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs)**
  - Why needed here: The entire framework builds on BLIP-2's pretrained multimodal representations. Understanding how vision and language modalities are aligned during pretraining explains both the model's capabilities and its domain-transfer limitations.
  - Quick check question: Can you explain how BLIP-2's Q-Former bridges image and text representations?

- **Concept: Zero-Shot Learning**
  - Why needed here: The paper's primary contribution is enabling VQA without task-specific training. Distinguishing zero-shot (no gradient updates) from few-shot (in-context examples) is critical for understanding what the framework can and cannot adapt to.
  - Quick check question: What is the difference between zero-shot inference and fine-tuning, and which components in this pipeline could theoretically be fine-tuned?

- **Concept: Semantic Similarity / Embedding Spaces**
  - Why needed here: The answer mapping mechanism depends entirely on cosine similarity in BERT embedding space. Understanding what these embeddings capture (and what they don't) is essential for predicting mapping failures.
  - Quick check question: Why might "scarce" and "low" be close in embedding space while "flooded" and "damaged" might not be?

## Architecture Onboarding

- **Component map:**
  Input image + question → Prompt Modifier → Image Encoder (BLIP-2) → Image-Grounded Text Encoder (BLIP-2 Q-Former + BERT) → Answer Decoder (BLIP-2) → Answer Matcher (BERT + cosine similarity) → Highest-similarity answer

- **Critical path:**
  Input image + question → Prompt modification → BLIP-2 encoding (image + text) → Raw answer generation → BERT embedding of [Q + each valid answer] + BERT embedding of [Q + raw answer] → Cosine similarity → Highest-similarity answer

- **Design tradeoffs:**
  - Generative flexibility vs. output control: Open-ended generation allows unseen answers but requires post-hoc mapping
  - Zero-shot deployment vs. counting accuracy: No fine-tuning enables rapid deployment but counting tasks fail (<40% accuracy)
  - Prompt-based guidance vs. implicit bias: Appending answers constrains output but may bias toward frequent options

- **Failure signatures:**
  - Counting tasks: Systematic underperformance across all model variants (0.0-5.0% for complex counting)
  - Vocabulary mismatch: Generated synonyms not in acceptable answer list (e.g., "scarce" vs. "low/moderate/high")
  - Ambiguous questions: Density questions unclear about vegetation vs. building density
  - Domain shift: General-purpose pretraining doesn't cover disaster-specific visual patterns

- **First 3 experiments:**
  1. Baseline validation: Run zero-shot BLIP-2 and ViLT on FloodNet without prompt modification to quantify the performance gain from each component
  2. Ablation study: Disable the answer mapping module and measure accuracy drop, particularly for density estimation tasks where vocabulary gaps are documented
  3. Error analysis on counting: Manually inspect counting failures to determine whether the issue is visual (object detection), linguistic (number word generation), or both

## Open Questions the Paper Calls Out

### Open Question 1
Can specific prompt engineering strategies resolve the semantic ambiguity in density estimation tasks where model outputs differ from ground truth labels? The authors state they "aim to address these issues in future studies," specifically regarding the unclear definitions of density thresholds and the semantic gap between generated answers (e.g., "scarce") and acceptable dataset labels (e.g., "low").

### Open Question 2
How can the zero-shot framework be modified to improve performance on counting tasks, which currently fail to generalize? Despite high performance in categorical tasks, the results show counting accuracy below 40%. The paper attributes this to the regression nature of counting and the significant imbalance in the distribution of counting classes.

### Open Question 3
To what extent does the ZeShot-VQA framework generalize to other natural disaster types beyond flooding? While the paper claims the method is applicable to new datasets without fine-tuning, all experiments are restricted to the FloodNet dataset (Hurricane Harvey), leaving cross-domain robustness unverified.

## Limitations
- Counting tasks show systematic failure (<40% accuracy) due to the framework's generative approach to numerical estimation
- Answer mapping depends on semantic proximity in BERT embedding space, which may fail for domain-specific terminology
- Zero-shot generalization claims remain theoretical as experiments are limited to a single flood dataset
- Domain shift between pretraining data and disaster imagery may limit feature extraction quality

## Confidence
- **High Confidence:** Framework architecture and prompt modification approach are well-specified and reproducible
- **Medium Confidence:** 89.1% accuracy claim relies on single dataset with potential unknown test/train splits
- **Low Confidence:** Zero-shot generalization claims lack empirical validation across multiple disaster scenarios

## Next Checks
1. Cross-dataset validation: Test the zero-shot framework on a different disaster dataset (e.g., wildfire or earthquake imagery) to verify generalization claims beyond FloodNet
2. Counting task ablation: Implement a dedicated object detection module for counting and compare performance against the current generative approach
3. Answer mapping stress test: Create controlled experiments with semantically distant generated answers to quantify false positive rates in the mapping mechanism