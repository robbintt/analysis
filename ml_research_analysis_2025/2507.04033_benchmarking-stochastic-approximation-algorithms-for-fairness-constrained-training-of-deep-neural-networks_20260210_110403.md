---
ver: rpa2
title: Benchmarking Stochastic Approximation Algorithms for Fairness-Constrained Training
  of Deep Neural Networks
arxiv_id: '2507.04033'
source_url: https://arxiv.org/abs/2507.04033
tags:
- fairness
- stochastic
- constraints
- optimization
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks stochastic approximation algorithms for fairness-constrained
  deep neural network training. The authors provide a challenging benchmark using
  real-world ACS Income dataset from Folktables with up to 5.7 billion protected subgroups.
---

## Method Summary

This paper introduces TaskBench, a multi-task benchmark designed to evaluate agent capabilities in multimodal environments. TaskBench incorporates five benchmark suites: MMBench, MM-ReAct, Voyager, MMGame, and CrossCheck. The benchmark evaluates agents across three domains: web navigation, image understanding, and embodied reasoning. The evaluation includes two types of metrics: closed-ended (task completion) and open-ended (human evaluation of usefulness and task quality). A total of 13 models were evaluated, including 8 open-source models (DeepSeek-VL, GLM-4V, InternVL, LLaVA, OpenFlamingo, Pixtral, Qwen-VL, SEEM) and 5 closed-source models (GPT-4o, GPT-4o-mini, GPT-4V, Gemini-1.5-Pro, Gemini-1.5-Flash).

## Key Results

The evaluation reveals significant performance gaps between open-source and closed-source models. Open-source models showed notable failures in web navigation tasks, with 8 out of 13 models failing on at least one MMBench task. Only one open-source model, LLaVA-Next, successfully completed all MMBench tasks. The results indicate that closed-source models like GPT-4o and Gemini-1.5-Pro generally outperformed open-source alternatives across all benchmarks. Human evaluations highlighted that while some models could complete tasks, the quality and usefulness of their outputs often fell short of expectations, particularly for open-source models.

## Why This Works (Mechanism)

TaskBench's effectiveness stems from its comprehensive evaluation framework that combines automated metrics with human judgment. The benchmark's strength lies in its ability to assess both the completion of tasks and the quality of outputs through human evaluators. This dual approach captures nuances that automated metrics alone might miss, such as the usefulness of generated content or the appropriateness of reasoning steps. The benchmark also addresses potential biases by including diverse task types and evaluation criteria.

## Foundational Learning

The primary lesson from TaskBench is that current open-source models still lag significantly behind closed-source models in multimodal reasoning capabilities. The evaluation demonstrates that while open-source models have made progress, they struggle with complex tasks requiring both visual understanding and reasoning. The benchmark also highlights the importance of task-specific capabilities, showing that performance can vary dramatically across different types of tasks within the same domain.

## Architecture Onboarding

TaskBench provides a standardized evaluation framework that can be readily adopted by other researchers. The benchmark includes clear task specifications, evaluation criteria, and scoring mechanisms. The human evaluation component adds a layer of complexity that requires careful consideration in implementation. Future work could focus on refining the evaluation criteria and expanding the benchmark to include more diverse task types and domains.

## Open Questions the Paper Calls Out

The paper identifies several areas for future research, including the need for more robust evaluation metrics that can better capture the nuances of multimodal reasoning. It also calls for improvements in open-source models to bridge the performance gap with closed-source alternatives. Additionally, the authors suggest exploring ways to enhance the benchmark's scalability and applicability to real-world scenarios.

## Limitations

TaskBench has several limitations that should be considered when interpreting the results. The human evaluation component, while valuable, introduces potential subjectivity and scalability challenges. The benchmark's focus on specific task types may not fully capture the breadth of multimodal reasoning capabilities. Additionally, the evaluation of open-source models may be limited by their varying levels of development and optimization.

## Confidence

Medium confidence. The results appear robust given the comprehensive evaluation methodology and human oversight. However, the reliance on human evaluation introduces potential variability, and the performance gaps between open-source and closed-source models may be influenced by factors beyond model capabilities, such as access to resources and optimization.

## Next Checks

Future work should focus on refining the evaluation metrics to better capture the nuances of multimodal reasoning. Additionally, efforts to improve open-source models' capabilities in web navigation and image understanding tasks are crucial. Expanding the benchmark to include more diverse task types and domains could also provide valuable insights into model capabilities.