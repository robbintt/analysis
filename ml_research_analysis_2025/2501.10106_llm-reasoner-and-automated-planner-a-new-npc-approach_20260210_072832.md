---
ver: rpa2
title: 'LLM Reasoner and Automated Planner: A new NPC approach'
arxiv_id: '2501.10106'
source_url: https://arxiv.org/abs/2501.10106
tags:
- problem
- agent
- state
- goal
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel intelligent agent architecture that
  combines a Large Language Model (LLM) for decision-making with a classical automated
  planner for generating executable plans. The LLM selects contextually appropriate
  goals based on environmental perceptions and personality traits, while the automated
  planner creates specific action sequences to achieve those goals.
---

# LLM Reasoner and Automated Planner: A new NPC approach

## Quick Facts
- arXiv ID: 2501.10106
- Source URL: https://arxiv.org/abs/2501.10106
- Reference count: 31
- One-line primary result: Novel NPC architecture combining LLM goal selection with automated planning demonstrated plausible behavior in firefighter simulation.

## Executive Summary
This paper proposes a new intelligent agent architecture that combines a Large Language Model (LLM) for decision-making with a classical automated planner for generating executable plans. The system was implemented in the Rhymas framework within a firefighter training simulation called "The FireFighter Problem." The agent demonstrated plausible and coherent behavior, autonomously selecting and pursuing goals based on context without explicit instructions. The architecture successfully emulated human-like reasoning, though the LLM's inherent "common sense" sometimes led to decisions not fully aligned with specified personality traits. The system showed promise for creating believable NPCs in simulations and games, though computational limitations required running components on separate machines.

## Method Summary
The architecture consists of three main modules: a Reasoner (LLM-based goal selector), a Planner (automated planner using Fast Downward solver), and an Interface (environment adapter). The Reasoner maintains a memory stream of natural-language perceptions plus personality traits, using the LLM to select contextually appropriate goals from a list of possible goals. The Planner maintains a world-state representation in PDDL-style predicates and generates executable action sequences using the automated planner. The Interface translates world state between the environment and both modules, coordinating the iterative reactive loop where each iteration updates perceptions, re-evaluates goals, replans if needed, and executes the next action. The system was demonstrated in a firefighter training simulation with five agent configurations.

## Key Results
- Agent autonomously selected and pursued goals based on context without explicit instructions
- Demonstrated plausible and coherent behavior across multiple agent configurations
- Successfully emulated human-like reasoning with contextually appropriate decisions
- Showed limitations where LLM's common sense sometimes overrode specified personality traits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can select contextually appropriate goals from environmental perceptions without explicit scenario programming.
- Mechanism: The Reasoner module maintains a memory stream of natural-language perceptions plus a personality traits field. When goal selection is triggered, the LLM receives the memory stream, personality description, and a list of achievable goals, then outputs a single goal selection.
- Core assumption: The LLM's training on diverse human behavior provides sufficient "common sense" to make plausible goal decisions in novel situations.
- Evidence anchors:
  - [abstract] "The LLM selects contextually appropriate goals based on environmental perceptions and personality traits"
  - [section III-A] "This module features a main data structure, called memories, representing the environmental context... The module receives a list of potential goals to pursue and, leveraging the environment context provided by the memories, determines which one to pursue."
  - [corpus] Weak direct corpus support; related work (HCR-Reasoner) discusses LLM reasoning but not specifically for goal selection.
- Break condition: If the environment requires strict adherence to developer-specified priorities, the LLM's implicit common sense may override personality traits, producing misaligned decisions.

### Mechanism 2
- Claim: Classical automated planners can generate sound, executable action sequences from LLM-selected goals.
- Mechanism: The Planner module maintains a world-state representation (AP Problem) using predicates and objects. When triggered by goal changes or world-state updates, it invokes a solver (Fast Downward) to generate a sequential action plan that achieves the selected goal from the current state.
- Core assumption: The environment's action space can be fully modeled in PDDL-style predicates with defined preconditions and effects.
- Evidence anchors:
  - [abstract] "classical automated planner for generating executable plans... creates specific action sequences to achieve those goals"
  - [section III-B] "The Planner module is responsible for generating and maintaining an AP Problem... It is also responsible for making the calls to the AP Solver to generate a plan."
  - [corpus] No direct corpus support for LLM-planner integration specifically.
- Break condition: If the domain cannot be fully specified in formal predicates, or if actions have uncertain effects, the planner will produce invalid or unexecutable plans.

### Mechanism 3
- Claim: A reactive iteration loop enables adaptation to unforeseen scenarios without manual specification.
- Mechanism: Each iteration: (1) Interface generates possible goals from world state, (2) updates Reasoner memories and Planner's AP Problem, (3) triggers goal re-evaluation if memories changed, (4) triggers replanning if goal or world state changed, (5) executes the next action from the plan.
- Core assumption: The environment provides sufficiently frequent state updates for reactive behavior to emerge.
- Evidence anchors:
  - [abstract] "The system... demonstrated plausible and coherent behavior, autonomously selecting and pursuing goals based on context without explicit instructions"
  - [section I-B] "The execution of the system consist on an iterative and reactive process where, on each iteration, the system exhibits the following behaviour..."
  - [corpus] Reason-Plan-ReAct paper proposes similar reasoner-planner-executor decomposition for complex tasks, supporting the architectural pattern.
- Break condition: If LLM inference latency is too high relative to environment dynamics, the agent will exhibit delayed or inconsistent reactive behavior.

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: The Planner module requires domain modeling with types, predicates, actions, preconditions, and effects to generate valid plans.
  - Quick check question: Can you write a simple PDDL domain file with two actions and three predicates?

- Concept: LLM prompt engineering for constrained output
  - Why needed here: The Reasoner must extract a single goal selection from the LLM reliably; inconsistent outputs break the planning pipeline.
  - Quick check question: How would you structure a prompt to force an LLM to output exactly one option from a numbered list?

- Concept: Sense-Plan-Act agent architecture
  - Why needed here: The system follows this classic pattern; understanding the control flow is essential for debugging and extension.
  - Quick check question: Draw the data flow between perception, memory update, goal selection, planning, and action execution.

## Architecture Onboarding

- Component map:
  - World State -> Interface (generate goals + update memories + update AP Problem) -> Reasoner (LLM selects goal) -> Planner (solver generates action sequence) -> Interface (pop and execute next action) -> Environment

- Critical path: World state → Interface (generate goals + update memories + update AP Problem) → Reasoner (LLM selects goal) → Planner (solver generates action sequence) → Interface (pop and execute next action) → Environment

- Design tradeoffs:
  - GPU distribution required: LLM and simulation frontend should run on separate machines to avoid latency bottlenecks
  - Temperature=0 in LLM calls: Reduces randomness for consistent goal selection but limits behavioral variety
  - Manual domain definition: Required for AP but limits portability across environments

- Failure signatures:
  - LLM outputs unparseable goal selection → plan never updates, agent stalls
  - World-state predicate mismatch → AP solver returns no solution
  - Personality traits overridden by LLM common sense → agent ignores specified priorities (observed in firefighter experiments)

- First 3 experiments:
  1. Single-agent "Common Person" scenario: Verify that personality traits ("prioritize safety, call professionals") produce expected non-intervention behavior; confirm LLM does not override with hero impulses.
  2. Two-agent firefighter variants (FP vs FF): Test whether distinct personality prompts ("save people first" vs "extinguish fires first") produce measurably different goal selection patterns across multiple runs.
  3. Latency stress test: Measure iteration time with LLM and planner on same machine vs. distributed; identify the computational threshold where reactive behavior degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a hybrid architecture combining LLMs with decision trees (where specific leaves invoke the LLM) produce more controllable and predictable NPC behavior while retaining flexibility?
- Basis in paper: [explicit] Authors state: "it would be valuable to explore hybridizing this approach with other classical techniques. For instance, an intriguing extension of this work could involve studying the behavior of a decision tree, where some of the leaves are calls to the LLM."
- Why unresolved: The paper only tested a pure LLM-based reasoner; no hybrid structures were implemented or evaluated.
- What evidence would resolve it: Comparative experiments measuring behavioral consistency, controllability, and flexibility between pure LLM, pure decision tree, and hybrid architectures in the same scenarios.

### Open Question 2
- Question: Can the architecture maintain coherent, contextually appropriate behavior in larger, more complex multi-agent environments with multiple simultaneous objectives?
- Basis in paper: [explicit] "investigating the behavior of this architecture in larger and more complex environments would be highly valuable."
- Why unresolved: Testing was limited to a simple scenario (one burning car, one person, one extinguisher) with at most four agents.
- What evidence would resolve it: Demonstrations of the architecture operating successfully in environments with orders of magnitude more objects, agents, and potential goals, with behavioral evaluation metrics.

### Open Question 3
- Question: How can the gap between specified personality traits and LLM "common sense" reasoning be reduced to achieve more predictable agent behavior?
- Basis in paper: [explicit] "The architecture successfully emulated human-like reasoning, though the LLM's inherent 'common sense' sometimes led to decisions not fully aligned with specified personality traits" and "full control over the agent's decisions remains elusive."
- Why unresolved: The paper documents this as an observed limitation but does not propose or test solutions.
- What evidence would resolve it: Experiments with different prompting strategies, fine-tuning approaches, or architectural modifications showing improved alignment between specified traits and actual decisions.

## Limitations

- Architecture requires manual PDDL domain specification, limiting portability across environments
- LLM's inherent "common sense" can override explicitly defined personality traits, creating unpredictability
- Computational demands require distributed GPU resources, presenting practical deployment challenges

## Confidence

- **High confidence**: The core architectural pattern of LLM-reasoner plus automated planner is technically sound and feasible, with clear data flow and well-defined module interfaces.
- **Medium confidence**: The qualitative claim of "plausible and coherent behavior" is supported by the authors' observations but lacks systematic measurement; the firefighter scenario demonstrates the concept but doesn't prove general applicability.
- **Low confidence**: Claims about the system's ability to handle unforeseen scenarios are theoretical—the firefighter environment was controlled and fully specified, so reactive adaptation was never truly tested in novel conditions.

## Next Checks

1. Implement quantitative metrics for goal selection consistency: run the same scenario 50 times with identical personality traits and measure variance in goal selection timing and frequency; test whether LLM's "common sense" overrides personality traits in >30% of trials.
2. Stress test the planning component: systematically remove predicates or actions from the PDDL domain to identify the minimum viable specification needed for the solver to generate valid plans; measure plan quality degradation as domain completeness decreases.
3. Performance benchmarking: measure iteration latency with various LLM model sizes (7B, 13B, 70B) and quantization levels; identify the computational threshold where the reactive loop becomes non-real-time (iteration > 100ms); test distributed vs. single-machine configurations to quantify the claimed GPU distribution benefit.