---
ver: rpa2
title: On the Evaluation of Large Language Models in Multilingual Vulnerability Repair
arxiv_id: '2508.03470'
source_url: https://arxiv.org/abs/2508.03470
tags:
- repair
- vulnerability
- code
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first comprehensive empirical investigation
  into the effectiveness of large language models (LLMs) and existing automated vulnerability
  repair (AVR) techniques across multiple programming languages. The research evaluates
  state-of-the-art AVR approaches, pre-trained language models, and advanced LLMs
  on a multilingual vulnerability dataset containing 10,649 function pairs across
  seven languages (C, C++, C, Go, Java, JavaScript, Python).
---

# On the Evaluation of Large Language Models in Multilingual Vulnerability Repair

## Quick Facts
- arXiv ID: 2508.03470
- Source URL: https://arxiv.org/abs/2508.03470
- Reference count: 40
- Instruction-tuned GPT-4o with few-shot prompting achieves 28.71% exact match rate, competing with best AVR approach VulMaster at 28.94%

## Executive Summary
This study presents the first comprehensive empirical investigation into the effectiveness of large language models (LLMs) and existing automated vulnerability repair (AVR) techniques across multiple programming languages. The research evaluates state-of-the-art AVR approaches, pre-trained language models, and advanced LLMs on a multilingual vulnerability dataset containing 10,649 function pairs across seven languages (C, C++, C#, Go, Java, JavaScript, Python). Results show that instruction-tuned GPT-4o with few-shot prompting achieves a 28.71% exact match rate, competing effectively with the best AVR approach, VulMaster, at 28.94%. Across all models, Go consistently yields the highest effectiveness while C/C++ performs the worst.

## Method Summary
The study constructs a multilingual vulnerability repair dataset (REEF) from existing sources (Big-Vul, CWEfixes) and evaluates state-of-the-art AVR techniques (VulMaster, VRepair), pre-trained language models (CodeT5, Code Llama), and advanced LLMs (GPT-4o, Llama 3, Qwen2) using exact match (EM), BLEU-4, and ROUGE metrics. Three prompting strategies are tested: zero-shot (instruction only), few-shot (instruction + 3 BM25-retrieved examples), and instruction-tuning via LoRA adaptation. The study systematically compares these approaches across seven programming languages and evaluates generalization on previously unseen TypeScript vulnerabilities.

## Key Results
- Instruction-tuned GPT-4o with few-shot prompting achieves 28.71% exact match rate, competing effectively with best AVR approach VulMaster at 28.94%
- Go language consistently yields the highest effectiveness across all models, while C/C++ performs the worst
- GPT-4o demonstrates strong generalization on previously unseen TypeScript vulnerabilities, outperforming VulMaster by 385.88%
- Manual analysis reveals error localization is the primary challenge for LLM-based repair attempts

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Tuning Alignment with In-Context Learning
- Claim: Instruction-tuning aligns the model's latent knowledge to the specific output format of vulnerability repair, while few-shot prompting provides the immediate semantic context required to bridge the gap between general coding ability and specific security patching.
- Mechanism: The model undergoes Low-Rank Adaptation (LoRA) on a multi-task dataset where inputs are structured instructions and outputs are repaired functions. This adapts the attention weights to prioritize repair patterns. Simultaneously, few-shot prompting retrieves similar vulnerability examples (via BM25), activating the model's in-context learning capabilities to mimic the repair logic of the examples without updating weights.
- Core assumption: The model possesses sufficient pre-existing knowledge of secure coding patterns and vulnerability semantics, requiring only "activation" via formatting (instruction-tuning) and examples (few-shot) rather than learning new security logic from scratch.
- Evidence anchors:
  - [abstract] "instruction-tuned GPT-4o with few-shot prompting achieves a 28.71% exact match rate, competing effectively with the best AVR approach, VulMaster, at 28.94%."
  - [section 4.2] "Instruction-tuning boosts the performance of the majority of models... For example, instruction-tuning Code Llama with few-shot prompting outperforms Code Llama with few-shot prompting with an improvement of 624.21%."
  - [corpus] "SPVR: syntax-to-prompt vulnerability repair..." suggests that mapping syntax to prompts is a key mechanism, reinforcing the need for structured prompting strategies.
- Break condition: This mechanism fails if the training data contains insufficient diversity of vulnerability types, leading to overfitting on seen patterns, or if the BM25 retrieval retrieves syntactically similar but semantically irrelevant examples, misleading the few-shot context.

### Mechanism 2: Language-Agnostic Semantic Transfer
- Claim: Large Language Models (LLMs) leverage abstract semantic representations of code control flow and data logic that transfer across programming languages, allowing them to generalize to unseen languages better than models trained on syntax-specific features.
- Mechanism: Unlike traditional AVR techniques (e.g., VulMaster) which may rely heavily on language-specific syntax trees or CWE metadata for specific languages (mostly C/C++), LLMs treat code as multilingual text. The model learns to identify "insecure patterns" (e.g., unchecked inputs, bad sanitization) as a semantic concept independent of the specific language syntax (e.g., Go vs. C++).
- Core assumption: Vulnerability concepts (like "improper authentication" or "injection") share a semantic invariant across different language implementations that the LLM has captured during its massive pre-training phase.
- Evidence anchors:
  - [abstract] "Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language [TypeScript], outperforming existing approaches."
  - [section 4.4] "GPT-4o attains a score of 28.57% [on TypeScript], which is 385.88% higher than VulMaster... This suggests the robust generalization capabilities of GPT-4o."
  - [corpus] Corpus signals regarding "multilingual vulnerability detection" and "reasoning LLMs" support the idea that higher-level reasoning aids cross-language tasks.
- Break condition: This mechanism breaks down for vulnerability classes that are heavily dependent on language-specific memory management (e.g., C/C++ buffer overflows), where semantic reasoning must be precise about memory addresses rather than just logic. The paper notes C/C++ performs worst, supporting this boundary.

### Mechanism 3: Localization Failure as the Primary Bottleneck
- Claim: The primary barrier to higher repair rates is not the generation of code, but the accurate identification (localization) of the vulnerable lines within the context.
- Mechanism: The Sequence-to-Sequence paradigm used for LLMs takes the entire function as input. If the model's attention mechanism fails to assign high weight to the specific vulnerable lines amidst the noise of the full function body, it generates a patch that is either semantically equivalent but misplaced, or introduces logical errors in safe code regions.
- Core assumption: The model knows *how* to fix a vulnerability if it knew *where* it was; the error is primarily one of attention allocation rather than remediation knowledge.
- Evidence anchors:
  - [abstract] "Manual analysis reveals that error localization is the primary challenge for LLM-based repair attempts."
  - [section 5.3] "A significant portion of the errors in the GPT-4o results resulted from localization issues (49% and 43% for Go and C/C++, separately)."
  - [corpus] "Repairing vulnerabilities without invisible hands" and "Localization-Guided Instructions" from the corpus strongly corroborate that localization is a critical independent variable in repair success.
- Break condition: This mechanism implies that simply scaling model size or data volume may yield diminishing returns unless the input method (prompting) explicitly aids localization, e.g., via line-number hints or highlighting (which this study avoided to maintain language-agnosticism).

## Foundational Learning
- Concept: **Exact Match (EM) with Beam Search**
  - Why needed here: The study relies on EM (Exact Match) with beam sizes of 1, 3, and 5 to determine if the generated patch *exactly* matches the ground truth. Understanding beam search is crucial because higher beam sizes usually increase the likelihood of finding the correct patch (as seen in Table 3), but LLMs were evaluated at beam 1 due to API constraints.
  - Quick check question: Why does increasing beam size generally improve EM scores for AVR techniques, and why was this parameter fixed at 1 for LLMs in this study?
- Concept: **Instruction Tuning (LoRA)**
  - Why needed here: The winning strategy (GPT-4o) uses "instruction-tuning" via Low-Rank Adaptation (LoRA). You must understand that this is not full fine-tuning; it freezes the main model weights and only updates small adapter matrices, making it computationally feasible to adapt huge models to the specific "repair" task.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of what parts of the model are updated, and why is this advantageous for the 7B+ parameter models used here?
- Concept: **BM25 Retrieval for Few-Shot Prompting**
  - Why needed here: The study uses BM25 (a lexical matching algorithm) to select the 3 best examples from the training set to feed into the LLM. This creates a dynamic "few-shot" prompt tailored to the specific vulnerability being repaired.
  - Quick check question: Why might BM25 (keyword-based) be a risky choice for retrieving code examples compared to semantic vector search, and how does the choice of "3 examples" impact token limits?

## Architecture Onboarding
- Component map: REEF dataset -> Tree-sitter preprocessing -> Strategy Layer (Zero-shot/Few-shot/Instruction-Tuning) -> Model Layer (LLMs/AVR Baselines) -> Evaluation (EM/BLEU/ROUGE)
- Critical path:
  1. **Data Hygiene:** Start with REEF and rigorously remove duplicates found in Big-Vul/CVEfixes (1,159 pairs removed)
  2. **Prompt Engineering:** Construct the "Instruction" template (Role + Task + Constraints). Do *not* use special tokens (like `<StartLoc>`) for LLMs as it degrades performance (Section 4.2)
  3. **Training:** Apply LoRA to open-source LLMs using the "instruction-filled" dataset (7,448 pairs)
  4. **Inference:** For each test case, retrieve 3 similar examples via BM25, format the prompt, and query the model
- Design tradeoffs:
  - **VulMaster vs. GPT-4o:** VulMaster uses rich inputs (ASTs, CWE metadata) but is brittle to new languages (fails on TypeScript). GPT-4o uses raw text (no metadata) but generalizes better (385.88% better on TypeScript)
  - **Localization vs. Generation:** AVR baselines often use special tokens to show *where* the bug is. LLMs do not use this info, making the task harder but the pipeline simpler (no static analysis tools needed)
- Failure signatures:
  - **Error Localization (43-49%):** Model changes unrelated code or misses the vulnerable line entirely (Section 5.3)
  - **Multiple Chunks Errors:** Model tries to fix one part of a multi-part vulnerability but misses others (e.g., CWE-120 example in Fig 11)
  - **Semantic Equivalence:** Model generates code that is logically correct but syntactically different from the ground truth (counts as a failure in EM, though might be a valid patch)
- First 3 experiments:
  1. **Baseline Validation:** Replicate the "VulMaster" EM score of ~28.94% on the processed REEF dataset to ensure your evaluation pipeline matches the paper's metrics
  2. **Zero vs. Few-Shot Ablation:** Run GPT-4o (or a smaller open model like Llama 3) on a subset of 100 Go vulnerabilities. Compare Zero-shot (0.x% EM) vs. Few-shot with BM25 (expected ~20%+ EM) to quantify the value of retrieval
  3. **Localization Stress Test:** Manually inspect 50 failed cases to verify the "Error Localization" claim. Check if the model produced a *valid* patch that was simply misplaced, versus a hallucinated patch

## Open Questions the Paper Calls Out
- **Open Question 1:** Can advanced prompting strategies like Tree-of-Thought (ToT) or iterative refinement using static analysis feedback significantly improve LLM performance in multilingual vulnerability repair?
  - Basis in paper: [explicit] Section 5.5 (Future Directions) explicitly suggests exploring ToT prompting and iterative refinement based on security tool feedback to enhance current capabilities
  - Why unresolved: Current strategies (zero-shot, few-shot, CoT) struggle with complex localization and logical errors, and CoT alone was shown to perform worse than few-shot prompting
  - What evidence would resolve it: Empirical results showing higher Exact Match (EM) scores or functional correctness rates when feedback loops or reasoning trees are implemented compared to standard few-shot baselines

- **Open Question 2:** To what extent does incorporating multi-modal structural data, such as control-flow graphs (CFG) or data-flow graphs (DFG), enhance the repair of memory-related vulnerabilities?
  - Basis in paper: [explicit] Section 5.5 notes that current single-modality (source code) approaches may limit effectiveness on specific types like memory-related vulnerabilities, suggesting graph-structural data as a solution
  - Why unresolved: The study focused on text-based LLMs and did not evaluate models capable of processing graph-structural information alongside code
  - What evidence would resolve it: A comparative study showing that graph-augmented models outperform text-only models (like GPT-4o) specifically on CWEs related to memory corruption (e.g., Buffer Overflows)

- **Open Question 3:** How do AVR techniques perform on dynamically verifiable datasets regarding functional correctness and the prevention of new security issues?
  - Basis in paper: [explicit] Section 5.5 highlights that current datasets are not executable and calls for evaluation on dynamically verifiable benchmarks to ensure reliability
  - Why unresolved: The current evaluation relies on Exact Match (EM) and textual similarity (BLEU/ROUGE), which do not verify if the patched code compiles, passes tests, or introduces new vulnerabilities
  - What evidence would resolve it: Evaluation results on a benchmark with executable test suites, measuring the "pass rate" of generated patches and the "security score" of the resulting code

- **Open Question 4:** How robust are LLM-based AVR techniques against minor code modifications or semantically equivalent adversarial inputs?
  - Basis in paper: [explicit] Section 5.5 identifies comprehensive robustness testing against code variations and adversarial inputs as a necessary step before real-world deployment
  - Why unresolved: LLMs are highly sensitive to input variations; slight syntactic changes or semantic equivalencies in the input code might lead to significant drops in repair success
  - What evidence would resolve it: Consistency of repair success rates across obfuscated, refactored, or slightly modified versions of the same vulnerable functions

## Limitations
- Exact match evaluation may be overly strict, penalizing semantically equivalent but syntactically different patches that would be functionally correct
- The study focuses on function-level repair without providing explicit vulnerability localization to the model, creating an artificial constraint that doesn't reflect real-world tooling
- Limited evaluation on complex, multi-component vulnerabilities (only 20% of cases) may overstate general effectiveness
- BM25 retrieval for few-shot examples relies on lexical similarity, which may miss semantically relevant examples for code with similar vulnerabilities but different surface syntax

## Confidence
- **High Confidence:** GPT-4o instruction-tuned with few-shot prompting achieves competitive results with state-of-the-art AVR techniques; Go language consistently outperforms others; localization identified as primary failure mode
- **Medium Confidence:** Claims about LLM superiority on unseen languages (TypeScript) require independent replication due to potential data leakage concerns; generalization mechanisms across languages need further validation
- **Low Confidence:** Absolute effectiveness percentages (28% EM) may not translate to real-world deployment due to evaluation methodology constraints

## Next Checks
1. Conduct human evaluation of "failed" patches to distinguish between truly incorrect patches versus semantically equivalent alternatives rejected by exact match criteria
2. Replicate the TypeScript generalization experiment with independently collected vulnerability data to verify the 385.88% improvement claim
3. Test instruction-tuned LLMs with explicit vulnerability localization tokens (currently avoided) to establish upper bounds on potential performance