---
ver: rpa2
title: Black-Box Hallucination Detection via Consistency Under the Uncertain Expression
arxiv_id: '2509.21999'
source_url: https://arxiv.org/abs/2509.21999
tags:
- expression
- llms
- uncertainty
- responses
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates black-box hallucination detection in LLMs
  without requiring internal model states or external resources. The core method uses
  prompt-based uncertainty/certainty expressions (e.g., "I am not sure but it could
  be" vs.
---

# Black-Box Hallucination Detection via Consistency Under the Uncertain Expression

## Quick Facts
- arXiv ID: 2509.21999
- Source URL: https://arxiv.org/abs/2509.21999
- Reference count: 0
- Primary result: AUROC up to 0.746 on HotpotQA and NQ-open datasets

## Executive Summary
This paper presents a black-box hallucination detection method that requires no internal model states or external resources. The core approach tests whether LLM responses remain consistent when prefixed with uncertainty or certainty expressions. Factual responses show high consistency across perturbations, while hallucinated responses demonstrate inconsistency. A simple detection metric compares responses with standard and uncertainty-expressing prompts using an NLI model, achieving strong performance on HotpotQA and NQ-open datasets.

## Method Summary
The method generates two responses per question: one with a standard prompt and one with an uncertainty/certainty expression prefix. It then computes an NLI-based score measuring semantic divergence between responses. The detection metric is the difference between contradiction and entailment logits from a DeBERTa-v3 NLI model. This approach requires exactly 2 LLM calls and 1 NLI call, making it computationally efficient compared to sampling-based alternatives.

## Key Results
- Achieves AUROC of 0.746 on HotpotQA and 0.741 on NQ-open datasets
- Outperforms baselines including log probability, entropy, and SelfCheckGPT
- Shows strong performance without requiring internal model states or external knowledge sources
- Expression sensitivity analysis reveals "It must be" works best for HotpotQA while "I am not sure but it could be" works best for NQ-open

## Why This Works (Mechanism)

### Mechanism 1
Factual responses exhibit higher robustness to prompt perturbations than non-factual responses. When an LLM has strong parametric knowledge, adding uncertainty expressions does not significantly alter the answer. Hallucinations, being weakly grounded, show response instability under perturbation. Evidence shows factual groups generate consistent responses while non-factual groups show inconsistency in over half of cases.

### Mechanism 2
Semantic divergence between original and perturbed responses, measured via NLI, serves as a proxy for hallucination risk. High contradiction scores indicate the perturbation successfully "broke" the generation, flagging it as likely hallucination. The NLI model detects logical contradictions in LLM outputs, with this approach outperforming lexical similarity metrics.

### Mechanism 3
LLMs are not linguistically calibrated with respect to certainty expressions. Forcing an LLM to express certainty or uncertainty disrupts generation primarily when the model lacks underlying knowledge. This perturbation works bidirectionally and affects non-factual generations more than factual ones.

## Foundational Learning

- **Natural Language Inference (NLI)**: Needed because the detection metric relies on an NLI model to determine if responses logically contradict each other. Quick check: Can you explain why simple string overlap (ROUGE/BLEU) is insufficient for detecting semantic inconsistencies?

- **Black-Box Access Constraints**: The motivation relies on lacking access to token log-probabilities. The solution must operate solely on generated text strings. Quick check: Why does the paper argue that SelfCheckGPT is computationally disadvantaged compared to this single-perturbation approach?

- **Closed-Book Question Answering**: The evaluation is performed in closed-book setting where the LLM uses internal parametric knowledge. Quick check: How does "closed-book" differ from RAG, and why does that matter for "factuality" here?

## Architecture Onboarding

- **Component map**: Input Question → Standard Path (Question → LLM → Response R1) → Perturbation Path (Question + Expression → LLM → Response R2) → Detector (NLI Model) → Scorer (logit_contradiction - logit_entailment)

- **Critical path**: Selection of perturbation prefix (e.g., "I am not sure but it could be") must be optimized for specific LLM version and dataset domain.

- **Design tradeoffs**: 
  - Inference Cost vs. Accuracy: 2 LLM calls + 1 NLI call (cheaper than SelfCheckGPT's 8+ samples)
  - Expression Selection: Requires tuning prefix to dataset domain (different expressions work best for different datasets)

- **Failure signatures**:
  - Consistently Wrong: Same hallucination for both prompts yields low NLI score but false answer
  - "I cannot answer": Perturbation may trigger refusal to answer
  - Expression Ineffective: High accuracy with standard prompt but low detection for non-GPT models

- **First 3 experiments**:
  1. Baseline Verification: Implement NLI scoring and verify AUROC on HotpotQA subset using "It must be" prefix
  2. Ablation on Temperature: Test with temperature > 0 to check consistency under non-deterministic generation
  3. Prefix Sensitivity: Test new prefix (e.g., "I guess that") on held-out validation set for generalization

## Open Questions the Paper Calls Out

### Open Question 1
Can unlikelihood training effectively penalize high-confidence non-factual statements to improve factual calibration? The conclusion suggests applying unlikelihood training as a "corrective mechanism" for plausible but incorrect responses. This remains unresolved as the paper focuses on detection rather than model training.

### Open Question 2
How can hallucination detection be improved for non-factual samples that are generated consistently and confidently? The authors identify this as a "critical challenge" where non-factual samples show no distinct features compared to factual ones. This evades inconsistency-based detection.

### Open Question 3
Does the consistency-based detection approach generalize to open-source models or tasks beyond short-form QA? Experiments were restricted to text-davinci-003 and short-form QA datasets. Different model architectures or long-form generation tasks may exhibit different consistency behaviors.

## Limitations
- Dependence on NLI model quality - detection accuracy fundamentally relies on external NLI model's ability to identify semantic contradictions
- Model-specific prefix sensitivity - requires careful tuning of perturbation expressions for different LLM versions
- Inability to detect consistent hallucinations - fails when LLM generates same incorrect response regardless of perturbation

## Confidence
- **High confidence** in core mechanism: Factual responses are more robust to prompt perturbations than non-factual ones
- **Medium confidence** in NLI-based detection: Performance may degrade on out-of-domain questions or different LLM architectures
- **Low confidence** in universal applicability: Requires significant calibration for different model families and domains

## Next Checks
1. Cross-model consistency test: Apply approach to different LLM (LLaMA-2 or Claude) to verify generalizability beyond GPT-3.5-based models
2. Adversarial prompt testing: Generate cases where standard prompt yields correct answers but uncertainty prompt introduces subtle errors
3. Consistency rate analysis: Calculate percentage of factual vs. non-factual responses showing consistency vs. inconsistency under perturbation to validate core assumption