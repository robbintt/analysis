---
ver: rpa2
title: 'PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion
  Prior'
arxiv_id: '2512.18367'
source_url: https://arxiv.org/abs/2512.18367
tags:
- psi3d
- diffusion
- prior
- latexit
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PSI3D, a plug-and-play method for large-scale\
  \ 3D stochastic inference using latent diffusion priors and total variation regularization.\
  \ The method addresses the challenge of scaling diffusion models to massive 3D volumes\
  \ (1024\xD71024\xD7128) by combining pretrained 2D diffusion priors with inter-slice\
  \ consistency constraints."
---

# PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion Prior

## Quick Facts
- arXiv ID: 2512.18367
- Source URL: https://arxiv.org/abs/2512.18367
- Authors: Wenhan Guo; Jinglun Yu; Yaning Wang; Jin U. Kang; Yu Sun
- Reference count: 40
- One-line primary result: PSI3D achieves PSNR 28.98 on OCT super-resolution, outperforming traditional and learning-based baselines

## Executive Summary
PSI3D introduces a scalable approach for large-scale 3D stochastic inference by decomposing the problem into slice-wise 2D diffusion sampling with inter-slice consistency constraints. The method addresses the challenge of applying diffusion models to massive 3D volumes (1024×1024×128) by combining pretrained 2D latent diffusion priors with total variation regularization along the concatenation axis. PSI3D employs a Markov chain Monte Carlo approach to reconstruct each 2D slice, while incorporating stochastic TV regularization to enhance inter-slice consistency and enable uncertainty quantification through posterior sampling.

## Method Summary
PSI3D reconstructs 3D volumes by alternating between three stochastic updates: likelihood sampling from degraded measurements, latent diffusion prior sampling for 2D slices, and TV regularization along the depth axis. Each 2D slice (1024×1024) is projected to a 64×64 latent code via VQGAN, where an EDM diffusion model performs denoising posterior sampling. The method uses a split Gibbs sampler with proximal average decomposition, substantially reducing computational cost per iteration. TV regularization is applied stochastically through noise injection around proximal solutions, enabling posterior sampling while enforcing inter-slice consistency. The approach is evaluated on OCT super-resolution, demonstrating significant improvements over traditional and learning-based baselines.

## Key Results
- PSI3D achieves PSNR 28.98 on OCT super-resolution, outperforming bilinear (27.29), bicubic (27.42), 3D total variation denoising (27.24), and 3D UNet (27.97)
- The method provides robust uncertainty quantification with 95.57% of voxels falling within the 3-standard deviation credible interval
- PSI3D demonstrates scalability to massive 3D volumes (1024×1024×128) through slice-wise latent diffusion

## Why This Works (Mechanism)

### Mechanism 1: Slice-wise Latent Diffusion for Scalable 2D Prior
Decomposing 3D reconstruction into slice-wise 2D diffusion sampling enables tractable inference on massive volumes while maintaining high-fidelity in-plane reconstruction. Each 2D slice is projected to a compact latent code via VQGAN encoder, where EDM diffusion model performs denoising posterior sampling. The 2D latent diffusion prior captures sufficient in-plane structure to generalize to 3D volumes when combined with orthogonal regularization.

### Mechanism 2: Stochastic TV Prior for Inter-Slice Consistency
Injecting noise around a TV proximal solution provides stochastic inter-slice regularization while enabling posterior sampling. TV regularization along the concatenation axis enforces smoothness between slices through noise injection around proximal points, approximating draws from the proximal generator. First-order TV along depth is sufficient to capture inter-slice continuity without modeling higher-order correlations.

### Mechanism 3: Split Gibbs Sampler with Proximal Average
Alternating between three stochastic updates (likelihood, diffusion prior, TV prior) yields posterior samples with credible uncertainty quantification. The augmented posterior is sampled via split Gibbs, with proximal average decomposing batch updates into slice-wise diffusion plus TV, substantially reducing computational cost per iteration.

## Foundational Learning

- **Half-Quadratic Splitting (HQS) and Proximal Operators**: PSI3D builds on HQS to decouple likelihood from priors. Understanding proximal operators is essential for grasping how diffusion and TV steps relate mathematically. Quick check: Can you explain why the proximal of a Gaussian denoising problem connects to diffusion model sampling?

- **EDM Diffusion Parameterization**: The diffusion prior step uses EDM-style sampling with specific noise schedules. You need to understand how noise level σ maps to diffusion timestep. Quick check: Given a noise level ρ_d, how would you determine the corresponding starting timestep for reverse diffusion?

- **Stochastic Proximal Sampling**: Unlike deterministic PnP methods, PSI3D requires sampling from proximal distributions. The noise injection in the TV step is non-obvious. Quick check: Why does adding ρ_tv η_t around the proximal point approximate sampling from the posterior?

## Architecture Onboarding

- **Component map**: Degraded input → downsampled measurements y → VQGAN encoder/decoder (1024×1024 ↔ 64×64 latent) → EDM diffusion model (latent space) → Likelihood module (Cholesky sampling) → TV proximal solver (dual-domain) → Batch selector (randomized rounding)

- **Critical path**: Degrade input → downsampled measurements y → Initialize x_0, z_0, w_0 → For T iterations: Likelihood → Diffusion Prior (latent) → TV Prior (batch) → Average posterior samples for final reconstruction

- **Design tradeoffs**: Latent vs. pixel-space diffusion (latent reduces memory but introduces encoder-decoder nonlinearity requiring separate noise schedules); Batch size vs. coverage (larger batches increase inter-slice context but require more GPU memory); TV strength λ_TV (stronger TV improves consistency but risks over-smoothing)

- **Failure signatures**: Slice-to-slice discontinuities ("stair-step" artifacts) → TV prior too weak; Over-smoothed reconstructions → TV too strong or diffusion schedule decayed too fast; Uncertainty intervals not calibrated (≠95%) → coupling schedules mismatched; Out-of-memory on 1024×1024 slices → latent projection failing or batch too large

- **First 3 experiments**: 1) Validate latent diffusion prior standalone by training VQGAN + EDM on 2D B-scans and verifying reconstruction quality without TV or likelihood steps; 2) Ablate TV prior by running PSI3D with and without TV step to quantify inter-slice smoothness improvements; 3) Calibrate coupling schedules by sweeping ρ_d and ρ_tv decay schedules and measuring PSNR and 3-SD credible interval coverage

## Open Questions the Paper Calls Out

### Open Question 1
How does the non-linearity of the VQGAN encoder-decoder affect the precise mapping of noise levels from image space to latent space during the diffusion prior step? The current reliance on heuristic tuning for the latent noise schedule lacks theoretical guarantee of correctness or stability across different datasets. Resolution would require theoretical derivation linking latent and image noise variances or demonstration of fixed schedules working across diverse datasets.

### Open Question 2
Does the use of first-order Total Variation (TV) regularization limit the reconstruction of high-frequency inter-slice structures compared to learned 3D priors? TV regularization tends to over-smooth edges and textures; this disparity in prior expressiveness between axes could inhibit recovery of complex volumetric geometries. Resolution would require evaluation on datasets with non-piecewise-constant 3D structures comparing PSI3D against methods utilizing deep 3D priors.

### Open Question 3
Can the PSI3D framework maintain its convergence guarantees and reconstruction quality when applied to inverse problems with non-linear or spatially-variant forward operators? While the paper suggests using Langevin dynamics for general cases, the efficiency and mixing of the sampler for highly non-linear or non-convex likelihood landscapes are unverified. Resolution would require application to non-linear inverse problems showing robust convergence and uncertainty quantification.

## Limitations
- The coupling schedules (ρd, ρtv) are tuned empirically for OCT and may not generalize to other modalities
- TV regularization along depth is heuristic and may not capture complex 3D anatomical correlations
- The claim of "plug-and-play" is qualified as PSI3D requires training VQGAN and EDM on domain-specific data
- Uncertainty quantification assumes the forward model is known exactly

## Confidence

- **High confidence**: Feasibility of slice-wise latent diffusion for 2D slices (mechanisms 1 and 3), basic calibration of uncertainty (3-SD coverage target)
- **Medium confidence**: Effectiveness of TV regularization for inter-slice consistency (mechanism 2), scalability to 1024×1024 volumes
- **Low confidence**: Generalizability of PSI3D to arbitrary inverse problems beyond OCT super-resolution

## Next Checks

1. **Cross-Modality Validation**: Apply PSI3D to a different 3D imaging modality (e.g., CT or MRI) with different noise characteristics and verify if the same coupling schedules maintain performance without retraining.

2. **Ablation on Higher-Order Correlations**: Compare PSI3D against a variant that replaces TV regularization with a learned 3D diffusion prior (e.g., a 3D latent diffusion trained on small patches) to quantify the loss from not modeling inter-slice correlations jointly.

3. **Uncertainty Calibration Under Model Mismatch**: Introduce systematic biases in the forward operator A (e.g., mismatched downsampling kernel) and measure whether the 3-SD credible intervals remain calibrated or become overconfident.