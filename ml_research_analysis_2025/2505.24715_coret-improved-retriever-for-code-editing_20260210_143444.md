---
ver: rpa2
title: 'CoRet: Improved Retriever for Code Editing'
arxiv_id: '2505.24715'
source_url: https://arxiv.org/abs/2505.24715
tags:
- code
- retrieval
- file
- chunks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoRet, a dense retrieval model for code-editing
  tasks that improves upon existing models by incorporating code semantics, repository
  structure, and call graph dependencies. The key innovation is a loss function explicitly
  designed for repository-level retrieval, optimized for the likelihood of retrieving
  correct code sections.
---

# CoRet: Improved Retriever for Code Editing

## Quick Facts
- **arXiv ID:** 2505.24715
- **Source URL:** https://arxiv.org/abs/2505.24715
- **Reference count:** 40
- **Primary result:** Dense retrieval model achieving 15+ percentage point improvements over baselines on SWE-bench and Long Code Arena datasets.

## Executive Summary
CoRet is a dense retrieval model specifically designed for code-editing tasks that significantly outperforms existing models by incorporating code semantics, repository structure, and call graph dependencies. The key innovation is a repository-level retrieval loss function optimized for the likelihood of retrieving correct code sections within a single codebase. By using in-instance negative sampling, including file paths in chunk representations, and incorporating call graph context, CoRet achieves substantial improvements in recall@20 (from 25.6% to 40.7% on SWE-bench) and MRR (from 0.27 to 0.53). The model addresses the challenge of retrieving relevant code chunks for bug fixes and feature implementation, demonstrating that existing retrieval models are suboptimal for repository-level code-editing scenarios.

## Method Summary
CoRet fine-tunes the CodeSage Small encoder with a repository-level cross-entropy loss using in-instance negative sampling. The model incorporates file paths as prefixes to code chunks and adds call graph context through string concatenation with downstream neighbor code using special token markers. Training uses up to 1024 in-instance negatives sampled from the same repository, mean pooling over chunk tokens, and tied query/code encoder weights. The model is trained for 4 epochs with RAdam optimizer, batch size of 256, and bf16 mixed precision on 8× A10G GPUs, taking approximately 24 hours to complete.

## Key Results
- **15+ percentage point improvement:** CoRet achieves significantly higher recall@20 compared to baselines like CodeSage, BM25, and other pretrained encoders on SWE-bench and Long Code Arena datasets.
- **Critical design choices validated:** Using in-instance negatives (10-point recall improvement from 8→1024 negatives), including file paths (0.53→0.42 MRR drop when removed), and call graph context (0.47 vs 0.41 MRR on LCA) all contribute to performance gains.
- **Repository-level optimization effective:** The loss function explicitly designed for repository-level retrieval outperforms traditional contrastive approaches for code-editing tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-instance negative sampling better aligns training with repository-level retrieval reality than across-instance negatives.
- **Mechanism:** CoRet uses a multi-class cross-entropy loss that models the probability of retrieving each chunk within a single repository, learning to distinguish relevant chunks from distractors that actually coexist in the codebase.
- **Core assumption:** The normalization factor approximation with up to 1024 in-instance negatives sufficiently approximates the full softmax over thousands of repository chunks.
- **Evidence anchors:** 10-point recall improvement from increasing negatives from 8 to 1024; "loss function explicitly designed for repository-level retrieval."
- **Break condition:** Performance may degrade if deployment repositories have dramatically different size distributions than training.

### Mechanism 2
- **Claim:** Call graph context injection enriches chunk representations with runtime dependency signals.
- **Mechanism:** Each chunk is concatenated with downstream neighbor code using a special `[DOWN]` token, allowing the encoder to internalize structural dependencies within the chunk embedding itself.
- **Core assumption:** Downstream calls are more informative than upstream callers for retrieval, and the 1024-token context window can absorb neighbor code without truncating the primary chunk.
- **Evidence anchors:** CoRet with call graph outperforms CoRet-CG on LCA (0.47 vs 0.41 MRR); "integrates...call graph dependencies."
- **Break condition:** If call graphs are incomplete or neighbors are too large to fit, injected context becomes noisy or truncated.

### Mechanism 3
- **Claim:** File path prefixes enable the model to leverage explicit hierarchy signals present in queries.
- **Mechanism:** The full file path is prepended to each code chunk during tokenization, and attention visualizations confirm the model focuses on path matches between query and chunk.
- **Core assumption:** Queries will continue to mention file paths at similar rates, and the model can generalize path-attention to unseen repositories.
- **Evidence anchors:** 0.53→0.42 MRR drop when file paths are removed at inference; 26-38% of queries contain at least one ground-truth file path.
- **Break condition:** If deployment queries omit file paths, path-attention may not fire, degrading retrieval to semantic similarity alone.

## Foundational Learning

- **Concept: Cross-Entropy (Softmax) Loss for Retrieval**
  - **Why needed here:** CoRet's loss is a multi-class cross-entropy treating each chunk as a class, unlike typical contrastive losses. Understanding softmax normalization clarifies why in-instance negatives matter.
  - **Quick check question:** Given a query and 5000 chunks, why does sampling 1024 negatives approximate the full softmax reasonably well?

- **Concept: Call Graph Extraction in Python**
  - **Why needed here:** To implement call graph context, you must parse ASTs to find function calls. Understanding static vs. dynamic call resolution affects which neighbors you can retrieve.
  - **Quick check question:** In Python, why might `getattr(obj, method_name)()` not appear in a static call graph?

- **Concept: Token Segment Embeddings**
  - **Why needed here:** CoRet uses segment embeddings to differentiate primary chunk vs. call graph neighbor tokens, allowing the model to learn distinct representations for each.
  - **Quick check question:** If segment embeddings are removed but `[DOWN]` token remains, what signal is lost?

## Architecture Onboarding

- **Component map:** CodeSage Small encoder -> Mean pooling -> Cross-entropy loss with in-instance negatives -> File path prefixing -> Call graph context injection

- **Critical path:**
  1. Chunk repository: Parse files → extract functions/classes → prefix file paths.
  2. Build call graph: Static analysis → extract downstream neighbors.
  3. Format inputs: Concatenate chunks with neighbors.
  4. Train: Sample in-instance negatives → compute loss → backprop.
  5. Inference: Embed query + all chunks → rank by cosine similarity.

- **Design tradeoffs:**
  - In-instance vs. across-instance negatives: Better alignment with inference but requires more memory per batch.
  - Mean pooling vs. [CLS]: Mean pooling improved performance but loses position-specific signals.
  - Call graph depth: Only immediate downstream neighbors used; deeper chains may add signal but increase token length and noise.

- **Failure signatures:**
  - Recall plateau: If recall@k stalls despite more negatives, check if negative sampling is biased.
  - Path over-reliance: If removing file paths at inference causes >10 MRR drop, the model may have overfit to path tokens.
  - Call graph noise: If performance degrades on repos with heavy dynamic dispatch, the static call graph may be too sparse.

- **First 3 experiments:**
  1. **Negative ablation:** Train with 8, 64, 256, 1024 in-instance negatives on a hold-out split. Plot recall@20 vs. number of negatives to validate approximation.
  2. **Call graph depth:** Compare immediate downstream neighbors vs. 2-hop neighbors vs. no call graph. Measure token overflow rates and recall.
  3. **Path removal robustness:** Evaluate trained CoRet on a synthetic query set where file paths are stripped. Compare MRR drop vs. baseline CodeSage to quantify path dependency.

## Open Questions the Paper Calls Out

- **Can leveraging topological properties of the call graph improve the selection of relevant context neighbours over random sampling?** The current implementation selects call graph neighbors for context enhancement but does not utilize structural metrics (e.g., centrality, dependency depth) to filter or prioritize these neighbors, potentially including noisy or irrelevant context.

- **Does the proposed loss function and chunking methodology generalize effectively to multi-language code repositories?** The model's chunking strategy relies on Python-specific semantic units (functions, classes), and the loss function is optimized for the specific scale of Python repositories found in SWE-bench.

- **Would feature-level fusion or attention-based mechanisms outperform the current input string concatenation method for incorporating call graph context?** String concatenation increases token count and may dilute semantic focus, whereas architectural fusion (e.g., cross-attention) could integrate structural context more efficiently without expanding the input window.

## Limitations

- **Call graph extraction method:** The paper specifies downstream neighbors are added but does not detail the extraction tool or algorithm, which is critical for reproducibility.
- **Token length handling:** With 1024-token maximum input, the handling of chunks exceeding this limit is unspecified, potentially impacting representation quality.
- **File path dependency:** Performance on deployment scenarios with path-agnostic queries remains untested, creating a potential generalization gap.

## Confidence

- **CoRet outperforms baselines by 15+ percentage points:** High confidence (supported by multiple datasets with consistent improvements)
- **In-instance negatives are critical:** Medium confidence (strong empirical evidence but no direct ablation comparison)
- **Call graph context improves retrieval:** Medium confidence (demonstrated on LCA but no controlled isolation study)
- **File path prefixing contributes 10+ MRR points:** High confidence (clear evidence from path-removal experiment)

## Next Checks

1. **Negative sampling ablation study:** Train CoRet with 8, 64, 256, and 1024 in-instance negatives on a held-out validation split. Plot recall@20 against negative count to empirically verify the approximation quality claim.

2. **Cross-file context comparison:** Implement an alternative context injection mechanism using import/dependency relationships instead of call graphs. Compare against CoRet on the LCA dataset to isolate the contribution of structural vs. dependency-based context.

3. **File path generalization test:** Create a synthetic query set by stripping file paths from SWE-bench queries. Evaluate CoRet's performance drop versus baseline models to quantify path dependency and assess robustness to path-agnostic queries.