---
ver: rpa2
title: Revisiting the Reliability of Language Models in Instruction-Following
arxiv_id: '2512.14754'
source_url: https://arxiv.org/abs/2512.14754
tags:
- prompt
- evaluation
- prompts
- llms
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current LLMs exhibit substantial inconsistency in following closely
  related instructions with subtle variations, despite achieving high accuracy on
  standard benchmarks. To quantify this nuance-oriented reliability, we introduce
  reliable@k, a metric that measures whether models can handle a set of cousin prompts
  simultaneously.
---

# Revisiting the Reliability of Language Models in Instruction-Following

## Quick Facts
- arXiv ID: 2512.14754
- Source URL: https://arxiv.org/abs/2512.14754
- Authors: Jianshuo Dong; Yutong Zhang; Yan Liu; Zhenyu Zhong; Tao Wei; Chao Zhang; Han Qiu
- Reference count: 40
- One-line primary result: Current LLMs exhibit substantial inconsistency in following closely related instructions with subtle variations, despite achieving high accuracy on standard benchmarks.

## Executive Summary
Current LLMs exhibit substantial inconsistency in following closely related instructions with subtle variations, despite achieving high accuracy on standard benchmarks. To quantify this nuance-oriented reliability, we introduce reliable@k, a metric that measures whether models can handle a set of cousin prompts simultaneously. We develop an automated pipeline that generates high-quality cousin prompts via three augmentation strategies: rephrasing, distractor addition, and constraint/task reconfiguration. Using this pipeline, we construct IFEval++, an extended benchmark with 541 test cases each containing 10 nuanced variants. Across 46 models, reliability drops by up to 61.8% compared to standard accuracy, revealing this as a second-order property beyond benchmark performance. We explore three improvement directions: prediction-based methods show limited success, training on curated cousin prompts improves reliability, and parallel test-time scaling through rejection sampling is most effective, enabling weaker models to surpass stronger ones.

## Method Summary
The paper introduces IFEval++, an extended benchmark for evaluating LLM instruction-following reliability, built by augmenting the original IFEval benchmark with nuanced cousin prompts. The augmentation pipeline generates 9 cousin prompts per original test case using three strategies: rephrasing, distractor addition, and constraint/task reconfiguration, validated through code-based checking with GPT-5-mini. The reliable@k metric evaluates whether models can simultaneously handle all k cousin prompts, requiring perfect success across all variants. The paper explores three improvement directions: (1) prediction-based methods using classifiers to identify reliable prompts show limited success, (2) training on curated cousin prompts via SFT with LoRA improves reliability by up to 10.5%, and (3) parallel test-time scaling through rejection sampling with n=12 samples proves most effective, enabling weaker models to surpass stronger ones. The study evaluates 46 models, revealing reliability drops up to 61.8% compared to standard accuracy.

## Key Results
- LLM reliability drops by up to 61.8% when evaluated on nuanced cousin prompts versus standard benchmarks
- Parallel test-time scaling through rejection sampling is most effective improvement, enabling weaker models to surpass stronger ones
- Training on curated cousin prompts improves reliability by up to 10.5%, while prediction-based methods show limited success

## Why This Works (Mechanism)
Unknown: The paper does not explicitly specify why these mechanisms work. Based on the methodology, the reliable@k metric likely works by enforcing strict consistency requirements across nuanced variants, exposing models' sensitivity to subtle instruction variations. The cousin prompt augmentation works by systematically generating semantically equivalent but structurally different prompts that reveal models' brittleness. The rejection sampling mechanism works by leveraging probabilistic sampling to find at least one correct response among multiple attempts, effectively smoothing out model inconsistencies.

## Foundational Learning
- **Reliable@k metric**: Measures whether models can handle all k cousin prompts simultaneously (pass if all succeed, fail if any fail). Needed to quantify nuance-oriented reliability beyond standard accuracy. Quick check: verify that reliable@10 requires all 10 variants to pass IFEval's prompt_strict evaluation mode.
- **Cousin prompt augmentation**: Three strategies (rephrasing, distractor addition, constraint/task reconfiguration) generate nuanced variants of original prompts. Needed to expose models' sensitivity to subtle instruction variations. Quick check: ensure generated cousin prompts maintain semantic equivalence while introducing controlled variations.
- **Rejection sampling**: Generates multiple samples per prompt and selects passing responses to improve reliability. Needed to overcome models' inconsistency on nuanced variants. Quick check: verify that plateau occurs around n=12 samples with temperature=1.0.

## Architecture Onboarding

**Component Map:**
Data (IFEval benchmark) -> Augmentation Pipeline (rephrasing + distractor + C/T reconfiguration) -> IFEval++ (541 test cases × 10 variants) -> Evaluation (reliable@k) -> Improvement Methods (prediction-based, training, rejection sampling)

**Critical Path:**
IFEval benchmark → Cousin prompt generation → reliable@k evaluation → Reliability drop analysis → Improvement direction exploration

**Design Tradeoffs:**
- Full 10-variant evaluation vs. computational cost: comprehensive nuance assessment vs. 10x overhead
- Rejection sampling sample count vs. effectiveness: diminishing returns beyond n=12 samples
- Training on curated prompts vs. general capability: targeted reliability improvement vs. potential overfitting to cousin variants

## Open Questions the Paper Calls Out
- How can models be improved to handle nuanced instruction variations without requiring expensive test-time scaling?
- What architectural modifications could enable more consistent instruction-following across semantically equivalent prompts?
- Can the reliability gap be reduced through more sophisticated prompt engineering techniques?

## Limitations
- The study focuses on English-language prompts and may not generalize to multilingual instruction-following
- Cousin prompt generation relies on GPT-5-mini for validation, introducing potential circularity in evaluation
- The reliable@k metric requires perfect success across all variants, which may be overly strict for some practical applications
- Improvement methods are evaluated primarily on the IFEval++ benchmark, limiting generalization claims to other domains

## Confidence
Moderate: The methodology appears sound with systematic benchmark construction and evaluation across multiple models. However, reliance on GPT-5-mini for cousin prompt validation and the use of rejection sampling as a primary improvement method introduce some concerns about the validity of reliability measurements and practical applicability of findings.

## Next Checks
- Verify that cousin prompts generated through the three augmentation strategies maintain semantic equivalence while introducing meaningful variations
- Confirm that the reliable@k metric implementation correctly handles edge cases where some variants fail while others pass
- Validate that the rejection sampling improvement method doesn't simply mask underlying reliability issues through brute-force sampling