---
ver: rpa2
title: 'Intentionally Unintentional: GenAI Exceptionalism and the First Amendment'
arxiv_id: '2506.05211'
source_url: https://arxiv.org/abs/2506.05211
tags:
- speech
- genai
- first
- amendment
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that generative AI outputs should not receive\
  \ First Amendment protections because AI models lack intentionality, sentience,\
  \ and self-awareness\u2014key elements required for speech under established legal\
  \ precedent. Unlike human speech or corporate speech (which is made up of human\
  \ actions), AI outputs are not intentionally produced and therefore do not qualify\
  \ as speech."
---

# Intentionally Unintentional: GenAI Exceptionalism and the First Amendment

## Quick Facts
- arXiv ID: 2506.05211
- Source URL: https://arxiv.org/abs/2506.05211
- Reference count: 0
- Key outcome: Generative AI outputs should not receive First Amendment protections because AI models lack intentionality, sentience, and self-awareness required for speech under legal precedent.

## Executive Summary
This paper argues that generative AI outputs should not receive First Amendment protections because AI models lack intentionality, sentience, and self-awareness—key elements required for speech under established legal precedent. Unlike human speech or corporate speech (which is made up of human actions), AI outputs are not intentionally produced and therefore do not qualify as speech. The authors assert that extending First Amendment protections to AI would hinder the government's ability to regulate this powerful technology, potentially enabling unchecked spread of misinformation and other harms. They emphasize that users of AI do not have a protected right to receive AI outputs, and any regulations on AI should not trigger strict constitutional scrutiny. The paper concludes that AI should be treated as a tool, not as a speaker, and that democratic governance requires the ability to regulate AI effectively.

## Method Summary
The paper employs conceptual legal analysis using established First Amendment doctrine and precedent to argue that generative AI outputs lack the essential characteristics of speech. The authors examine the technical operation of foundation models, showing they generate outputs through statistical associations rather than intentional communication. They then apply established legal tests requiring intentionality, sentience, and human origin for speech protection, demonstrating how GenAI fails all three criteria. The analysis draws on case law, theoretical frameworks, and technical descriptions of AI behavior to build a comprehensive argument against First Amendment protection for AI outputs.

## Key Results
- Generative AI outputs lack the intentionality, sentience, and self-awareness required for First Amendment speech protection
- Neither developers nor users can be considered the speakers of specific AI outputs due to the stochastic nature of generation
- Extending First Amendment protection to AI would undermine democratic governance and the ability to regulate this powerful technology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GenAI outputs lack the intentionality required for constitutional speech protection.
- Mechanism: The paper establishes a three-part test (sentience, self-awareness, human origin) that must be satisfied for speech to receive First Amendment protection. GenAI fails all three criteria, so outputs are non-expressive conduct rather than speech.
- Core assumption: Courts will maintain that intentionality remains a necessary condition for speech protection even as technology evolves.
- Evidence anchors:
  - [abstract] "because these models lack intentionality, their outputs do not constitute speech as understood in the context of established legal precedent"
  - [section IV] "Intentionality, in turn, requires both sentience... and self-awareness... GenAI lacks intentionality, sentience, self-awareness, and humanness."
  - [corpus] Related paper "Constitutional Law and AI Governance" examines First Amendment constraints on AI regulation but does not directly test the intentionality argument empirically.
- Break condition: If a court finds that listener reception of outputs—rather than speaker intent—is sufficient to trigger speech protection, this mechanism fails.

### Mechanism 2
- Claim: Neither developers nor users can serve as constitutionally recognized speakers for GenAI outputs.
- Mechanism: The paper decouples generation from use. Developers do not intend specific outputs (models are "grown more than programmed"), and users cannot predict outputs deterministically. Without an identifiable speaker, there is no speech to protect.
- Core assumption: The stochastic gap between prompt and output is wide enough that neither party can claim authorship of the specific expression.
- Evidence anchors:
  - [section V.A] "developers do not intend for foundation models to convey any particular message"
  - [section V.G] "the prompter is a cause, but not a creator of the text, and the same may be said of the LLM proprietor"
  - [corpus] Weak corpus evidence on developer intent; related papers focus on copyright rather than speech doctrine.
- Break condition: If courts adopt a "but-for" causation standard (the output would not exist without the prompt), users might gain speaker status.

### Mechanism 3
- Claim: Extending First Amendment protection to GenAI would undermine the doctrinal purposes of free speech.
- Mechanism: The paper maps three classic justifications for speech protection (marketplace of ideas, self-governance, self-expression) and shows GenAI contributes to none—it cannot form beliefs, persuade, or self-actualize. Worse, it can flood the marketplace with misinformation at superhuman scale.
- Core assumption: The "more speech cures bad speech" doctrine breaks down when speech production becomes effectively unlimited and non-agentic.
- Evidence anchors:
  - [section VI.A] "It is not at all clear to us how GenAI that can hallucinate a false output...improves our nation"
  - [section VI.A] "more speech is always the cure for bad speech...overlooks the possibility...of how GenAI can create bad speech more quickly than any human possibly could"
  - [corpus] No direct corpus tests of marketplace effects; this remains a theoretical claim.
- Break condition: If empirical evidence shows GenAI outputs increase information diversity without degrading epistemic quality, the marketplace argument weakens.

## Foundational Learning

- Concept: Strict vs. intermediate scrutiny in First Amendment law
  - Why needed here: The paper's policy stakes depend on which scrutiny level applies—strict scrutiny would largely block regulation, while intermediate scrutiny allows more government flexibility.
  - Quick check question: If a regulation targets the functional effects of code rather than its expressive content, which scrutiny standard applies?

- Concept: Stochastic parrots and the form/meaning distinction
  - Why needed here: The paper relies on the claim that LLMs manipulate form (token probabilities) without accessing meaning, which grounds the intentionality argument.
  - Quick check question: Can a system trained only to predict likely next tokens ever be said to "understand" what it produces?

- Concept: Corporate speech doctrine and aggregation theory
  - Why needed here: The paper distinguishes corporate speech (protected because corporations are composed of humans acting with intent) from GenAI outputs (no human component in the specific expression).
  - Quick check question: Why does the paper argue GenAI is more like a parrot than a corporation for speech doctrine purposes?

## Architecture Onboarding

- Component map: The argument has three layers: (1) technical claim → GenAI outputs are probabilistic token predictions without intent; (2) doctrinal claim → established First Amendment precedent requires intentionality and human origin; (3) policy claim → extending protection would harm democratic governance. Each layer must hold for the conclusion to follow.

- Critical path: The intentionality argument (Mechanism 1) is load-bearing. If outputs can be speech without intent, the developer/user speaker analysis (Mechanism 2) and the marketplace harm argument (Mechanism 3) become secondary.

- Design tradeoffs: The paper explicitly limits its scope to foundation models and acknowledges that more directed/specialized models might warrant different treatment. It also concedes that future self-aware AI would require reanalysis.

- Failure signatures: The argument is vulnerable to (1) courts adopting a listener-centric theory of speech protection; (2) empirical evidence that GenAI outputs enhance rather than degrade information ecosystems; (3) successful analogies to protected non-human expression (e.g., corporate speech, code-as-speech).

- First 3 experiments:
  1. Survey existing case law for any precedent granting speech protection to outputs created without human intent (e.g., random number generators, algorithmic trading signals).
  2. Document the stochastic gap empirically—measure output variability across multiple runs with identical prompts to quantify unpredictability.
  3. Map regulatory proposals (licensing, transparency reports, output restrictions) to scrutiny levels to predict which would survive under intermediate vs. strict scrutiny.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what precise point during model training, if ever, would First Amendment speech protections attach to model outputs?
- Basis in paper: [explicit] Section F asks: "Is the gibberish output from the beginning of training protected speech? If not, when do we decide to attach protections? When the output has a few understandable words? Mostly understandable words? Perfect grammar?"
- Why unresolved: The authors note that models display varying levels of generative capabilities based on factors like size and training cycles, making any claim of when speech rights begin "entirely arbitrary."
- What evidence would resolve it: Legal precedent establishing a clear threshold for when outputs transition from non-speech to protected speech, or a technical standard defining when model outputs achieve sufficient coherence.

### Open Question 2
- Question: What level of judicial scrutiny should apply when the government attempts to regulate GenAI outputs that may affect downstream human speech?
- Basis in paper: [explicit] Page 23-24 states: "Assuming that GenAI outputs likely impact a person's future expression, what level of judicial scrutiny should apply when the government tries to curtail GenAI outputs?" The authors propose "heightened scrutiny akin to a weak version of intermediate scrutiny" but acknowledge this is unresolved.
- Why unresolved: The authors argue rational basis is too low a bar and strict scrutiny is too high, but no clear standard exists for non-expressive conduct that may affect downstream speech.
- What evidence would resolve it: Court rulings applying a specific scrutiny standard to AI output regulations, or legislative frameworks establishing clear standards.

### Open Question 3
- Question: At what point does a technology have a "predictable connection" to a person's potential expression such that regulation triggers First Amendment analysis?
- Basis in paper: [explicit] Page 23 states: "The key question becomes: at what point does some thing have a predictable connection to a person's potential expression?"
- Why unresolved: The authors note that if any effect on downstream speech triggered First Amendment analysis, "anyone claiming any government action disrupted the person's speech in any manner could file a non-frivolous lawsuit," which cannot be the correct standard.
- What evidence would resolve it: Judicial decisions establishing a test for when regulation of non-expressive tools crosses into protected speech territory.

### Open Question 4
- Question: Would transparency reporting requirements for AI models constitute unconstitutional compelled speech under the First Amendment?
- Basis in paper: [inferred] Page 10-11 discusses transparency proposals but concludes that "an analysis of how the First Amendment may apply to transparency reports will require a more nuanced assessment, including examining which models the regulation would apply to...and how burdensome the regulation would be to comply with."
- Why unresolved: The paper notes compelled speech typically triggers strict scrutiny, but transparency reports don't neatly fit existing frameworks since they require factual disclosures rather than viewpoint carriage.
- What evidence would resolve it: Court rulings on AI-specific transparency requirements, or distinguishing precedent from commercial disclosure cases like *Zauderer*.

## Limitations

- The argument relies entirely on theoretical legal analysis without empirical validation of claims about GenAI behavior or marketplace effects
- The paper limits its scope to foundation models, leaving open questions about more specialized or future AI systems
- The stochastic gap between prompt and output, while theoretically significant, requires empirical measurement to establish legal relevance

## Confidence

- **High** for doctrinal analysis using established First Amendment precedent and logical reasoning
- **Medium** for the technical claims about model behavior and their legal implications
- **Low** for empirical claims about marketplace effects and information ecosystem impacts

## Next Checks

1. Survey existing case law for any precedent granting speech protection to outputs created without human intent (e.g., random number generators, algorithmic trading signals) to test the boundaries of current doctrine.

2. Empirically measure the stochastic gap by running multiple identical prompts through current foundation models to quantify output variability and unpredictability.

3. Map regulatory proposals (licensing, transparency reports, output restrictions) to scrutiny levels using established First Amendment doctrine to predict which would survive under intermediate vs. strict scrutiny.