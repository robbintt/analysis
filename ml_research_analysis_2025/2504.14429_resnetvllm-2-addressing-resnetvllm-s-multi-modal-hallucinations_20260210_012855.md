---
ver: rpa2
title: 'ResNetVLLM-2: Addressing ResNetVLLM''s Multi-Modal Hallucinations'
arxiv_id: '2504.14429'
source_url: https://arxiv.org/abs/2504.14429
tags:
- hallucination
- video
- resnetvllm
- generated
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ResNetVLLM-2, an enhanced video-language
  model that addresses hallucination issues in ResNetVLLM. The approach combines a
  faithfulness detection strategy using a modified Lynx model to assess semantic alignment
  between generated captions and ground-truth references, along with a hallucination
  mitigation strategy employing Retrieval-Augmented Generation (RAG) with a dynamically
  constructed ad-hoc knowledge base.
---

# ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations

## Quick Facts
- **arXiv ID:** 2504.14429
- **Source URL:** https://arxiv.org/abs/2504.14429
- **Reference count:** 29
- **Primary result:** Improved ActivityNet-QA accuracy from 54.8% to 65.3% using hallucination detection and RAG mitigation

## Executive Summary
This paper introduces ResNetVLLM-2, an enhanced video-language model that addresses hallucination issues in ResNetVLLM through a two-phase approach combining detection and mitigation. The detection phase uses a modified Lynx model to assess semantic alignment between generated captions and ground-truth references, while the mitigation phase employs Retrieval-Augmented Generation (RAG) with a dynamically constructed ad-hoc knowledge base. Evaluation on ActivityNet-QA demonstrates substantial accuracy improvement from 54.8% to 65.3%, confirming the effectiveness of the hallucination detection and mitigation techniques in enhancing video-language model reliability.

## Method Summary
The approach combines hallucination detection and mitigation strategies. Detection uses a modified Lynx model fine-tuned on ActivityNet Captions, which computes cosine similarity between generated caption and ground-truth embeddings, flagging outputs below 50% as hallucinations. Mitigation employs Post-Hoc RAG with an ad-hoc knowledge base constructed from ResNet projection layer outputs (visual features) and metadata. The generated caption is verified against retrieved evidence and revised if discrepancies are found using the RARR framework. Training involved two phases: warm-up with SGD for 150 epochs followed by joint training with AdamW for 50 epochs, using Video-ChatGPT-100K instruction dataset.

## Key Results
- Improved ActivityNet-QA accuracy from 54.8% to 65.3%
- Faithfulness score increased from 34.2% to 92.7%
- Demonstrated effectiveness of combined hallucination detection and RAG mitigation
- Validated on 5,800 videos with 58K QA pairs from ActivityNet-QA benchmark

## Why This Works (Mechanism)

### Mechanism 1: External Faithfulness Detection
If a model lacks internal self-correction, an external faithfulness detector can identify semantic misalignments between video content and generated text. The system utilizes a modified Lynx model, fine-tuned on ActivityNet Captions, which computes a cosine similarity score between the embeddings of the generated caption and the ground-truth context. A score below 50% flags the output as a hallucination. Core assumption: ground-truth captions fully capture necessary visual semantics, and embedding similarity correlates with factual faithfulness.

### Mechanism 2: Dynamic Visual RAG
If hallucinations stem from lack of visual grounding, dynamically retrieving visual features during generation can anchor the output. The architecture introduces an "ad-hoc knowledge base" constructed during inference from ResNet projection layer outputs and metadata. A Post-Hoc RAG module retrieves relevant features from this database to verify or revise the generated text. Core assumption: projection layer outputs contain sufficient semantic density to act as reliable knowledge for verifying complex textual claims.

### Mechanism 3: Post-Hoc Revision
If a generated statement conflicts with retrieved visual evidence, a post-hoc revision step can improve factual consistency. Implemented via the RARR framework, the Post-Hoc RAG module reviews the generated caption against retrieved ad-hoc knowledge. If discrepancies are found, the response is revised to align with visual evidence. Core assumption: the revision model can distinguish between its own parametric knowledge and provided visual evidence without introducing new logical errors.

## Foundational Learning

- **Concept: Multi-Modal Hallucination (Intrinsic vs. Extrinsic)**
  - Why needed: The paper differentiates intrinsic conflicts (contradicting source video) from extrinsic fabrications (unverifiable info). Understanding this distinction diagnoses why the model failed and whether ad-hoc KB can fix it.
  - Quick check: Does the model's error contradict the video (intrinsic), or invent details unrelated to visual context (extrinsic)?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: The mitigation strategy relies on non-standard RAG where retrieval source is dynamic visual features rather than static text documents.
  - Quick check: How does retrieving "visual projection outputs" differ from retrieving Wikipedia text in standard RAG pipelines?

- **Concept: Projection Layer / Visual Encoder**
  - Why needed: The "ad-hoc knowledge base" is built from ResNet encoder outputs. Understanding what these vectors represent evaluates if they constitute "knowledge."
  - Quick check: What data format is stored in ad-hoc knowledge baseâ€”raw pixels, feature vectors, or textual metadata?

## Architecture Onboarding

- **Component map:** Video + Metadata -> ResNet Encoder -> Projection Layer (Split: to LLM & to KB) -> LLM Generation -> Lynx Evaluation -> RAG Verification -> Output
- **Critical path:** Video -> ResNet Encoder -> Projection Layer (Split: to LLM & to KB) -> LLM Generation -> RAG Verification -> Output
- **Design tradeoffs:** Accuracy vs. Latency (dynamic KB construction increases inference time); Specificity vs. Generalization (highly specific KB reduces generalization errors but adds computational overhead)
- **Failure signatures:** False Positive Detection (Lynx similarity fails to capture nuance); KB Sparsity (RAG fails if projection features are too abstract)
- **First 3 experiments:** 1) Baseline Validation: Run ResNetVLLM on ActivityNet-QA to confirm 54.8% accuracy baseline; 2) Detection Ablation: Evaluate modified Lynx on ActivityNet Captions subset to measure precision in flagging hallucinations; 3) RAG Integration Test: Compare ResNetVLLM-2 with baseline on hallucination-prone examples to verify 65.3% accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the RAG framework be effectively extended to support multi-turn video dialogues while maintaining factual consistency?
- Basis: Conclusion states future work will focus on extending RAG framework to support multi-turn video dialogues.
- Why unresolved: Current RAG relies on dynamically constructed ad-hoc knowledge base for single-turn inference; multi-turn dialogue requires managing continuously evolving context and retrieving temporally relevant evidence across conversation history.
- Evidence to resolve: Demonstrating application on Audio-Visual Scene-Aware Dialog benchmark with metrics showing hallucination rates don't increase as dialogue history grows.

### Open Question 2
- Question: Does incorporating fine-grained temporal alignment into visual encoder provide significant reduction in hallucinations compared to current uniform frame sampling?
- Basis: Authors identify exploring fine-grained temporal alignment to further reduce hallucinations as specific future work direction.
- Why unresolved: Current implementation samples 100 frames uniformly, which may miss rapid events or fail to capture causal sequences, leading to temporal hallucinations where event orders are confused.
- Evidence to resolve: Comparative study integrating temporal attention mechanisms into ResNet encoder, measuring performance changes on temporally-sensitive questions within ActivityNet-QA.

### Open Question 3
- Question: To what extent does the "ad-hoc knowledge base" introduce circular dependency where model retrieves its own potentially hallucinated features to verify output?
- Basis: Method constructs knowledge base from "extracted projection layer outputs from ResNetVLLM" to verify LLM's output.
- Why unresolved: If visual encoder extracts incorrect feature (e.g., detecting non-existent object), this feature populates knowledge base. RAG might then retrieve this incorrect "evidence" to justify hallucinated caption, reinforcing error rather than correcting it.
- Evidence to resolve: Ablation study comparing current "self-generated" knowledge base against knowledge base populated with ground-truth object detections or external video captions to determine if retrieval performance is capped by encoder's initial errors.

### Open Question 4
- Question: Is the 50% faithfulness score threshold optimal for distinguishing hallucinations across diverse video genres, or does it introduce detection bias?
- Basis: Method defines hallucination condition strictly as faithfulness scores "below 50%" but provides no justification for this specific threshold.
- Why unresolved: Fixed threshold may be too lenient for high-stakes scenarios or too strict for ambiguous videos, potentially failing to detect subtle extrinsic hallucinations that score slightly above cutoff.
- Evidence to resolve: Sensitivity analysis plotting ROC curve of modified Lynx model against human-annotated hallucination labels to identify optimal operating point.

## Limitations
- Lynx adaptation for video hallucination detection lacks specific architectural details, making exact replication difficult
- Ad-hoc knowledge base construction from projection layer outputs is underspecified regarding feature extraction and indexing methodology
- Base ResNetVLLM implementation remains partially inaccessible through placeholder arXiv reference
- Fixed 50% faithfulness threshold may not be optimal across diverse video genres

## Confidence

- **High Confidence:** General two-phase approach (detection + mitigation) is sound and aligns with established multi-modal hallucination research. Accuracy improvement from 54.8% to 65.3% is well-documented and significant.
- **Medium Confidence:** Faithfulness detection mechanism using modified Lynx is theoretically valid, though video-specific implementation details are unclear. General concept of using embedding similarity for hallucination detection is established.
- **Low Confidence:** Exact implementation of ad-hoc knowledge base construction and Post-Hoc RAG revision mechanism cannot be fully verified without additional implementation details.

## Next Checks

1. **Implementation Fidelity Test:** Reconstruct modified Lynx model architecture based on T5-encoder specifications and validate faithfulness scoring against held-out ActivityNet Captions subset before integration.

2. **KB Construction Verification:** Implement and benchmark ad-hoc knowledge base construction pipeline using ResNet-50 projection outputs, measuring retrieval recall and precision against ground-truth hallucination examples.

3. **RAG Integration Validation:** Conduct controlled experiments comparing ResNetVLLM-2 outputs against both baseline ResNetVLLM and human-annotated corrections on hallucination-prone ActivityNet-QA examples to verify 65.3% accuracy claim.