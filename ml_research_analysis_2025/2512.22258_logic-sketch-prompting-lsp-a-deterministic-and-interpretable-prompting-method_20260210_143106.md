---
ver: rpa2
title: 'Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting
  Method'
arxiv_id: '2512.22258'
source_url: https://arxiv.org/abs/2512.22258
tags:
- prompting
- logic
- prompt
- language
- sketch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Logic Sketch Prompting (LSP), a structured
  prompting method designed to improve determinism, interpretability, and reliability
  in large language model (LLM) outputs. LSP achieves this by separating condition
  detection from decision logic through typed variables, deterministic condition evaluators,
  and a rule-based validator.
---

# Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method

## Quick Facts
- **arXiv ID:** 2512.22258
- **Source URL:** https://arxiv.org/abs/2512.22258
- **Reference count:** 29
- **Primary result:** LSP achieves 0.83–0.89 accuracy and F1 score on ADE classification tasks, outperforming zero-shot, brief, and chain-of-thought prompting while providing deterministic, interpretable outputs.

## Executive Summary
This paper introduces Logic Sketch Prompting (LSP), a structured prompting method that improves determinism, interpretability, and reliability in LLM outputs by separating condition detection from decision logic. LSP uses typed variables, deterministic condition evaluators, and rule-based validators to produce stable, traceable outputs. In experiments on adverse drug event classification with Gemma, Mistral, and Llama models, LSP consistently outperformed standard prompting approaches across accuracy and F1 metrics. The method is particularly suited for safety-critical applications requiring transparent reasoning without model fine-tuning or external tools.

## Method Summary
LSP works by structuring prompts into three components: a variable store with typed variables (Boolean, Integer, etc.), condition evaluators that detect relevant information from text using regex or semantic similarity, and a deterministic validator that applies logical rules (IF-THEN) to the variable states to produce final outputs. The LLM's role is limited to populating the variable store through evaluators, while the validator performs the final decision-making deterministically. This separation ensures reproducible outputs and creates an execution trace that can be audited for each decision.

## Key Results
- LSP achieved 0.83–0.89 accuracy and F1 scores on ADE classification tasks, significantly outperforming zero-shot (0.76–0.86 accuracy), brief (0.77–0.87 accuracy), and chain-of-thought (0.75–0.86 accuracy) prompting approaches
- Statistical tests confirmed significant performance gains (p < 0.05) across all tested models (Gemma, Mistral, Llama)
- The method demonstrated consistent improvements across different prompt configurations and model sizes while maintaining determinism and interpretability

## Why This Works (Mechanism)

### Mechanism 1: Separation of Pattern Detection and Decision Logic
- **Claim:** Decoupling pattern detection from decision logic reduces cognitive load on LLMs, minimizing reasoning errors
- **Mechanism:** Externalizes decision boundaries by requiring LLMs only to populate typed variables via condition evaluators, while a separate deterministic validator applies logical rules
- **Core assumption:** LLMs are more reliable at feature extraction than multi-step logical deduction
- **Evidence anchors:** Abstract and section 2.1 emphasize separation of detection from decision; *Blueprint First, Model Second* supports separating probabilistic planning from deterministic execution
- **Break condition:** Performance degrades if condition evaluators are poorly defined or extraction exceeds model capability

### Mechanism 2: Deterministic Validation of Variable States
- **Claim:** Deterministic functions over fixed variable states ensure reproducibility compared to stochastic token sampling
- **Mechanism:** Once variables are set, a rule-based validator computes outcomes without stochastic components, ensuring identical variable states yield identical outputs
- **Core assumption:** Logical rules accurately reflect ground truth task requirements
- **Evidence anchors:** Abstract and section 2.1.3 highlight deterministic validators with no side effects or stochastic components; *Valori* emphasizes deterministic substrates for stable reasoning
- **Break condition:** Semantic similarity evaluators may introduce input variance despite deterministic validator

### Mechanism 3: Constrained Context via Typed Variables
- **Claim:** Structured variable stores reduce reasoning paths and mitigate hallucination
- **Mechanism:** Finite typed variables restrict model output format and reasoning scope, preventing irrelevant context invention
- **Core assumption:** Necessary information can be captured by predefined variable schema
- **Evidence anchors:** Abstract and section 2.1.1 describe typed variable stores; *LLM-Assisted Formalization* shows formalization improves consistency
- **Break condition:** Complex nuances that don't fit predefined variable types are lost or misclassified

## Foundational Learning

- **Concept:** Neuro-Symbolic Architectures
  - **Why needed here:** LSP is a hybrid system requiring understanding of where neural extraction ends and symbolic validation begins
  - **Quick check question:** Can you distinguish between probabilistic classification and deterministic logical implication in the LSP pipeline?

- **Concept:** Prompt Engineering vs. Logic Programming
  - **Why needed here:** LSP bridges these fields, requiring translation of natural language requirements into formal logical rules
  - **Quick check question:** How would you express "A is true only if B is false AND C is true" in the LSP validator format?

- **Concept:** Deterministic Decoding
  - **Why needed here:** The paper relies on temperature=0 and disabled sampling; understanding decoder settings is essential for reproducibility
  - **Quick check question:** Does setting temperature=0 guarantee identical outputs across different model versions or hardware?

## Architecture Onboarding

- **Component map:** Input text → Condition Evaluators → Typed Variable Store → Deterministic Validator → Output Label
- **Critical path:** Designing Condition Evaluators - the validator is only as good as the variables populated by evaluators
- **Design tradeoffs:**
  - Rigidity vs. Flexibility: Stable, auditable outputs require manual rule engineering, less adaptable than zero-shot
  - Precision vs. Recall: Strict rules increase precision but may miss temporal/inferred relations if too narrow
- **Failure signatures:**
  - Indication Misclassification: False positives where therapeutic indications are mistaken for adverse events
  - Semantic Drift: Semantic similarity evaluators may produce different variable states across runs
- **First 3 experiments:**
  1. Baseline Comparison: Run Zero-shot vs. LSP on small validation set (N=50) to verify deterministic validator filtering
  2. Ablation on Condition Evaluators: Replace SemanticSim with strict Regex to measure determinism impact (run each input 5 times)
  3. Edge Case Testing: Input sentences with temporal links to confirm LSP captures these while Zero-shot baseline fails

## Open Questions the Paper Calls Out
None

## Limitations
- Framework effectiveness highly dependent on quality and completeness of predefined variable schema and condition evaluators
- Validation limited to two ADE classification datasets, raising questions about generalizability to other domains
- Manual rule engineering required may become prohibitive for complex, multi-step reasoning scenarios

## Confidence
- **High confidence:** Core claim of improved determinism through separation of detection and decision logic
- **Medium confidence:** Interpretability claims - traceable execution paths may still require domain expertise
- **Medium confidence:** Scalability claims - demonstrated on narrow ADE tasks but not complex reasoning scenarios

## Next Checks
1. **Schema Robustness Test:** Systematically vary variable schema complexity across multiple domains to identify breaking point
2. **Cross-Domain Generalization:** Apply LSP to non-medical domain requiring multi-hop reasoning (legal document analysis)
3. **Runtime Efficiency Analysis:** Measure computational overhead of condition evaluators versus pure zero-shot prompting