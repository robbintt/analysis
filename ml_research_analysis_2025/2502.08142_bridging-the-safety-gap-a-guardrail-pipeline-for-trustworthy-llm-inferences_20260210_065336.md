---
ver: rpa2
title: 'Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences'
arxiv_id: '2502.08142'
source_url: https://arxiv.org/abs/2502.08142
tags:
- safety
- arxiv
- hallucination
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Wildflare GuardRail is a comprehensive guardrail pipeline designed
  to enhance the safety and reliability of Large Language Model (LLM) inferences by
  systematically addressing risks across the entire processing workflow. The pipeline
  integrates four core functional modules: Safety Detector identifies unsafe inputs
  and detects hallucinations in model outputs while generating root-cause explanations,
  Grounding contextualizes user queries with information retrieved from vector databases,
  Customizer adjusts outputs in real time using lightweight, rule-based wrappers,
  and Repairer corrects erroneous LLM outputs using hallucination explanations provided
  by Safety Detector.'
---

# Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences

## Quick Facts
- **arXiv ID:** 2502.08142
- **Source URL:** https://arxiv.org/abs/2502.08142
- **Reference count:** 40
- **One-line primary result:** Wildflare GuardRail pipeline enhances LLM safety with 80.7% hallucination repair accuracy and 100% malicious URL detection.

## Executive Summary
Wildflare GuardRail is a comprehensive guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. The pipeline integrates four core functional modules: Safety Detector identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations, Grounding contextualizes user queries with information retrieved from vector databases, Customizer adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector. Results show that the unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7%.

## Method Summary
Wildflare GuardRail is a comprehensive guardrail pipeline for enhancing LLM inference safety and reliability through a sequential four-module architecture. The pipeline processes user queries through: (1) Safety Detector, which identifies unsafe content and hallucinations while generating root-cause explanations, (2) Grounding, which retrieves contextual information from vector databases to augment queries, (3) Customizer, which applies lightweight rule-based wrappers to adjust outputs in real-time, and (4) Repairer, which corrects hallucinated outputs using explanations from Safety Detector. The system uses Fox-1 (1.6B parameters) as the base model, fine-tuned on aggregated safety datasets and hallucination-augmented data generated via GPT-4. Key optimizations include top-k token probability logic for binary classification and efficient regex-based URL filtering for the Customizer module.

## Key Results
- Safety Detector achieves comparable performance to OpenAI API for unsafe content detection
- Customizer wrapper detects malicious URLs with 100% accuracy in 1.06s per query
- Repairer model reduces hallucinations with 80.7% accuracy using GPT-4 generated reasoning
- Pipeline effectively integrates multiple safety checks while maintaining low latency

## Why This Works (Mechanism)
The pipeline works by creating a defense-in-depth architecture where each module addresses specific failure modes in LLM inference. Safety Detector provides upfront risk assessment and generates explanations that enable the Repairer to fix errors systematically. Grounding reduces hallucination likelihood by providing contextual information from external sources. Customizer handles simple, deterministic safety issues through lightweight rules without expensive model calls. The sequential design ensures that safety checks occur at multiple stages, with Repairer leveraging Safety Detector's explanations to make targeted corrections rather than attempting to fix issues blindly.

## Foundational Learning
- **LLM Safety Detection:** Why needed - To identify and filter unsafe or harmful content before it reaches users; Quick check - Evaluate model accuracy on standardized safety benchmarks like ToxicChat
- **Hallucination Detection:** Why needed - To identify when LLMs generate false or unsupported information; Quick check - Measure detection accuracy using datasets like HaluEval with human evaluation
- **Vector Database Grounding:** Why needed - To provide LLMs with relevant context to reduce hallucinations; Quick check - Verify retrieval accuracy and relevance on domain-specific datasets
- **Rule-based Output Filtering:** Why needed - To efficiently handle known safety issues without expensive model inference; Quick check - Test customizer latency and accuracy on malicious URL detection
- **Fine-tuning with GPT-4 Augmentation:** Why needed - To create high-quality training data for hallucination repair; Quick check - Validate the quality of generated reasoning through human review
- **Top-k Token Probability Logic:** Why needed - To implement binary classification through generation models; Quick check - Verify correct token mapping and probability summation

## Architecture Onboarding

**Component Map:** User Query -> Safety Detector -> Grounding -> Customizer -> Repairer -> Final Output

**Critical Path:** The safety-critical path flows sequentially through all four modules, with Repairer depending on Safety Detector's hallucination explanations. Grounding occurs before Customizer to ensure context is available for output adjustments.

**Design Tradeoffs:** The pipeline trades increased latency (from sequential processing) for comprehensive safety coverage. Using lightweight rule-based Customizer instead of model-based filtering significantly reduces computational cost. GPT-4 augmentation for training data creation improves repair accuracy but introduces dependency on external API costs.

**Failure Signatures:** Safety Detector failures manifest as missed unsafe content or false positives. Repairer failures occur when it ignores context and hallucinates new answers instead of correcting specific errors. Grounding failures result in irrelevant retrieved information that doesn't reduce hallucinations.

**First 3 Experiments:**
1. Evaluate Safety Detector accuracy on ToxicChat benchmark to verify baseline safety performance
2. Test Customizer malicious URL detection with diverse test cases to confirm 100% accuracy claim
3. Measure Repairer hallucination consistency using Vectara evaluation on HaluEval test set

## Open Questions the Paper Calls Out
- **Multimodal Extension:** How can the GuardRail architecture be extended to integrate safety checks for multimodal inputs, such as images or audio? The current pipeline is designed exclusively for text-based LLM inferences, relying on text-specific vector databases and language models (Fox-1).
- **Edge Deployment Optimization:** What architectural trade-offs or optimizations are necessary to deploy this sequential pipeline on resource-constrained edge devices with low latency? The pipeline chains multiple modules, which inherently increases inference time and computational load compared to a standalone model.
- **Repairer Failure Analysis:** What specific error types or reasoning failures cause the Repairer to fail on the remaining 19.3% of hallucinated content? The results section quantifies the success rate but does not provide a qualitative breakdown of the failure cases or the limitations of the GPT-generated reasoning used for training.

## Limitations
- Fox-1 model weights are proprietary and unavailable, requiring substitution with open-source alternatives that may impact performance
- Critical training hyperparameters (learning rate, batch size, epochs, LoRA configurations) are omitted from the paper
- Exact sampling strategy for mixing 15 diverse safety datasets is unspecified, potentially affecting model generalization
- Safety evaluation is limited to ToxicChat without broader adversarial testing or cross-dataset validation

## Confidence
- **High Confidence:** Customizer latency and accuracy results (1.06s/query, 100%) are straightforward to replicate using Regex and Google Safe Browsing API
- **Medium Confidence:** Safety Detector accuracy (~0.78) and Repairer hallucination fixing rate (80.7%) are plausible given the described fine-tuning approach, but exact parity with OpenAI API is uncertain due to proprietary baseline differences
- **Low Confidence:** Hallucination detection and Repairer performance are highly sensitive to the quality of GPT-4-generated reasoning in the training data augmentation step, which is not independently verifiable

## Next Checks
1. **Data Augmentation Validation:** Verify the quality and diversity of GPT-4-generated "hallucination_reason" annotations for the HaluEval training set by sampling and manual inspection
2. **Model Substitution Impact:** Train the Safety Detector and Repairer modules using a comparable open-source 1.5B-2B model (e.g., Llama-3.2-1B) and compare performance metrics to reported results
3. **Adversarial Safety Testing:** Evaluate the Safety Detector on an external, adversarial safety dataset (e.g., RealToxicityPrompts) to assess robustness beyond the ToxicChat benchmark