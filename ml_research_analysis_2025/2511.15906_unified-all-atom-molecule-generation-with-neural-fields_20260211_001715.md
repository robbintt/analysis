---
ver: rpa2
title: Unified all-atom molecule generation with neural fields
arxiv_id: '2511.15906'
source_url: https://arxiv.org/abs/2511.15906
tags:
- funcbind
- amino
- molecules
- design
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FuncBind is a unified framework for generating all-atom molecules
  conditioned on target structures, using neural fields and score-based generative
  models. It represents molecules as continuous atomic densities and employs a modality-agnostic
  approach, enabling a single model to handle small molecules, macrocyclic peptides,
  and antibody CDR loops.
---

# Unified all-atom molecule generation with neural fields

## Quick Facts
- **arXiv ID:** 2511.15906
- **Source URL:** https://arxiv.org/abs/2511.15906
- **Reference count:** 40
- **Primary result:** Unified framework for all-atom molecule generation across small molecules, macrocyclic peptides, and antibody CDR loops using neural fields and score-based generative models.

## Executive Summary
FuncBind introduces a unified framework for generating all-atom molecules conditioned on target structures, using neural fields to represent molecules as continuous atomic densities. This modality-agnostic approach enables a single model to handle diverse molecular types including small molecules, macrocyclic peptides, and antibody CDR loops. The method achieves competitive in silico performance across modalities, matching or outperforming specialized baselines on benchmarks for small molecule docking, CDR loop redesign, and macrocyclic peptide generation. In vitro experiments show successful generation of novel antibody binders through CDR H3 loop redesign.

## Method Summary
FuncBind represents molecules as continuous neural fields mapping 3D coordinates to atomic densities, enabling variable atom counts and modality-agnostic processing. The framework uses a 3D CNN encoder to convert voxelized target structures into latent spatial feature maps, which condition a 5B parameter 3D U-Net denoiser that learns to denoise latent representations. A multiplicative filter network decoder with Gabor filters converts these latents back to continuous atomic density fields. The model is trained with aggregated batch size 768 on 8x B200 GPUs, using noise sampled from LogNormal(1.2, 0.8) and EDM2 sampling for generation. The approach handles small molecules, macrocyclic peptides, and antibody CDR loops within a single unified architecture.

## Key Results
- Achieves competitive in silico performance across three molecular modalities, matching or outperforming specialized baselines
- Successfully generates novel antibody binders with 42% binding rate on rigid epitopes and 2% on flexible epitopes in vitro
- Introduces a new dataset and benchmark for structure-conditioned macrocyclic peptide generation with non-canonical amino acids
- Demonstrates unified generation capability while maintaining chemical plausibility and structural validity

## Why This Works (Mechanism)

### Mechanism 1: Continuous Atomic Density Fields
Representing molecules as continuous neural fields (mapping coordinates to atomic densities) allows a single model to process diverse molecular modalities with variable atom counts. The model learns a function v: ℝ³ → [0,1]ⁿ representing atomic occupancy, avoiding fixed memory limits of voxel grids and sparsity issues of point clouds.

### Mechanism 2: Spatially-Arranged Latent Feature Maps
Organizing the latent representation z as a spatial feature map (grid) rather than a global vector enables scalability to larger molecular systems like antibodies. The encoder outputs a grid z ∈ ℝᶜ×ᴸ³ that preserves local spatial correspondence, allowing the decoder to fetch local embeddings zₓ for specific coordinates.

### Mechanism 3: Conditional Latent Denoising
Generating molecules is achieved by learning to denoise latent representations conditioned on a target protein structure. A denoiser ẑθ (3D U-Net) is trained to recover clean latent codes z from noisy versions y = z + σε, conditioned on the target's latent code zₜₐᵣ.

## Foundational Learning

- **Concept: Implicit Neural Representations (Neural Fields)**
  - **Why needed here:** The core engine is an MLP mapping (x,y,z) coordinates to physical properties, requiring understanding of coordinate-based MLPs and positional encoding/filters to debug decoding failures.
  - **Quick check question:** Can you explain why a standard MLP struggles to represent high-frequency details (the "spectral bias" problem) and how Gabor filters might mitigate this?

- **Concept: Score-Based Generative Models (Diffusion & WJS)**
  - **Why needed here:** The model learns by denoising, requiring understanding of how "learning the score" (gradient of log-density) relates to generating samples, and tradeoffs between Diffusion and Walk-Jump Sampling.
  - **Quick check question:** If noise level σ is too high during sampling, what happens to structural integrity? What if too low?

- **Concept: Voxelization & 3D Convolutions**
  - **Why needed here:** Despite continuous output, internal latent encoder and denoiser operate on discretized grids (voxels), requiring understanding of 3D CNN architectures.
  - **Quick check question:** How does resolution of internal voxel grid (L=16) affect tradeoff between receptor's receptive field and computational memory cost?

## Architecture Onboarding

- **Component map:** Voxelization (2.0Å resolution for low-res input, continuous query for output) -> Encoder (3D CNN, 4 residual blocks) -> [Add Noise] -> Denoiser (5B param 3D U-Net) -> Decoder (MFN with Gabor filters) -> Post-Processor (Peak detection + refinement + bond assignment)

- **Critical path:** Target/Input Voxelization -> Encoder -> [Add Noise] -> Denoiser (heavy training loop) -> Decoder -> Post-processing (most likely source of "unreasonable" atom errors)

- **Design tradeoffs:**
  - Continuous vs. Discrete: Uses continuous fields for output to save memory, but discretized grids for internal reasoning to leverage stable CV architectures
  - Augmentation vs. Equivariance: Explicitly removes SE(3) equivariance constraints in favor of massive data augmentation
  - Unified vs. Specialized: Single 5B model for all tasks, trading training complexity for inference utility

- **Failure signatures:**
  - "Unreasonable" Amino Acids: Invalid bond lengths (<0.8Å) or illegal chemistry in non-canonical residues
  - Steric Clashes: Generated atoms overlapping with target pocket
  - Hallucinated Density: Decoder outputs high density in empty space
  - High Strain Energy: Indicates model found local minimum not corresponding to physically relaxed molecule

- **First 3 experiments:**
  1. Auto-encoder Reconstruction: Verify Encoder-Decoder pair by checking if decoded density field accurately reconstructs atom positions
  2. Target-Free Generation (Prior Check): Run denoiser with no target condition to validate latent score-matching
  3. Pocket-Conditioned Inpainting (CDR H3): Run exact benchmark (e.g., target 4cni), focusing on RMSD of backbone, not just AAR

## Open Questions the Paper Calls Out

- **Open Question 1:** Can FuncBind effectively leverage transfer learning to improve data efficiency or generation quality when training across distinct molecular modalities?
- **Open Question 2:** How does performance scale with parameter count beyond 5 billion parameters tested, and does it exhibit saturation?
- **Open Question 3:** Do CDR H3 loop designs with variable lengths maintain structural fidelity and binding affinity comparable to fixed-length designs?
- **Open Question 4:** What architectural modifications are required to improve success rate for antibody generation on flexible epitopes?

## Limitations
- Computational requirements (5B parameter denoiser) create substantial barriers for widespread adoption requiring specialized hardware
- In vitro validation represents relatively small-scale experimental study (15 designed loops) needing broader validation
- Method's generalizability beyond three tested modalities (small molecules, peptides, antibodies) remains untested

## Confidence
- **High Confidence:** Core technical implementation of continuous atomic density fields and basic cross-modal benchmarking results
- **Medium Confidence:** Claim of achieving "unified" generation across modalities
- **Medium Confidence:** In vitro experimental results showing 42% binding rate
- **Low Confidence:** Assertion that this represents a "foundational model" approach

## Next Checks
1. **Ablation Study on Modality-Specific vs. Unified Models:** Train separate specialized models for each modality and quantitatively compare their performance against the unified FuncBind model across all three benchmarks.

2. **Broader In Vitro Validation:** Test CDR H3 loop designs on at least 50 additional antibody-target pairs spanning diverse epitope types to establish robustness of 42% binding rate claim.

3. **Computational Efficiency Analysis:** Conduct systematic scaling experiments comparing inference time and memory usage between FuncBind and specialized baselines, including GPU memory requirements across different batch sizes and resolutions.