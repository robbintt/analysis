---
ver: rpa2
title: 'Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific
  LLM Embeddings'
arxiv_id: '2601.11124'
source_url: https://arxiv.org/abs/2601.11124
tags:
- learning
- knowledge
- domain
- generative
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  (LLMs) for domain-specific text retrieval, where contrastive learning-based methods
  struggle due to insufficient domain knowledge. The authors propose Learn Before
  Represent (LBR), a two-stage framework that first injects domain knowledge through
  Information Bottleneck-constrained generative learning (IB-GL) to compress semantics
  into bottleneck tokens, then refines representations via generative-refined contrastive
  learning (GR-CL).
---

# Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings

## Quick Facts
- arXiv ID: 2601.11124
- Source URL: https://arxiv.org/abs/2601.11124
- Reference count: 9
- Primary result: R@10 scores reach 90.33 (medical), 80.60 (chemistry), 89.85 (code) via LBR framework

## Executive Summary
This paper tackles domain-specific text retrieval with LLMs, where contrastive learning struggles due to insufficient domain knowledge. The authors propose Learn Before Represent (LBR), a two-stage framework that first injects domain knowledge via Information Bottleneck-constrained generative learning (IB-GL), then refines representations using generative-refined contrastive learning (GR-CL). Experiments on medical, chemistry, and code domains show LBR significantly outperforms baselines, achieving state-of-the-art retrieval performance while maintaining causal attention architecture.

## Method Summary
LBR is a two-stage framework for domain-specific text retrieval. Stage 1 (IB-GL) compresses input semantics into bottleneck tokens using an Information Bottleneck constraint, trained autoregressively with attention masks blocking input-to-target flow. Stage 2 (GR-CL) extracts final bottleneck token hidden states and trains InfoNCE contrastive loss with in-batch negatives. The method maintains causal attention throughout and resolves the objective conflict between generative and contrastive learning by separating them into sequential stages.

## Key Results
- LBR achieves R@10 scores of 90.33 (medical), 80.60 (chemistry), and 89.85 (code)
- Outperforms baselines like LLM2Vec by 8-15 points across all domains
- Ablation shows IB-GL stage improves R@10 by 8-12 points over standard GL+CL approaches
- Maintains causal attention architecture while achieving superior retrieval performance

## Why This Works (Mechanism)
The method works by first compressing domain-relevant semantics into bottleneck tokens through Information Bottleneck-constrained generative learning, then using these compressed representations for contrastive learning. This two-stage approach resolves the objective conflict between generative (local) and contrastive (global) losses by separating them temporally. The IB constraint ensures bottleneck tokens contain only the most essential semantic information, preventing representation collapse during the contrastive stage.

## Foundational Learning
- **Information Bottleneck (IB)**: Information-theoretic framework that compresses input while preserving relevant information for target prediction. Needed to ensure bottleneck tokens contain only essential semantic information without redundancy.
- **InfoNCE Loss**: Contrastive loss that pulls positive pairs together and pushes negative pairs apart in embedding space. Needed for effective representation learning in retrieval tasks.
- **Causal Attention**: Attention mechanism that only allows tokens to attend to previous tokens, preserving autoregressive generation capability. Critical for maintaining text generation functionality alongside retrieval.
- **Bottleneck Token Mechanism**: Technique to compress input information into fixed-size representations. Enables efficient processing of variable-length inputs while preserving key semantic content.
- **Attention Masking**: Technique to control information flow in transformer architectures. Essential for enforcing the Information Bottleneck constraint during generative training.

## Architecture Onboarding

**Component Map:** [Input] -> [IB-GL Stage with Bottleneck Tokens] -> [Bottleneck Representations] -> [GR-CL Stage with InfoNCE] -> [Final Embeddings]

**Critical Path:** The bottleneck token extraction and InfoNCE training form the critical path for retrieval performance. The IB-GL stage must effectively compress domain knowledge into bottleneck tokens for the GR-CL stage to work well.

**Design Tradeoffs:** Fixed vs adaptive bottleneck length (R=500 optimal but may be suboptimal for variable information density), sequential vs joint optimization of generative and contrastive objectives, and the balance between semantic compression and information preservation.

**Failure Signatures:** Representation collapse (near-zero embedding variance), catastrophic forgetting of generation capability, or performance degradation when compression ratio is too extreme (R<200 or R>1000).

**First Experiments:** 1) Test with different bottleneck compression ratios (R=100, 500, 1000) on validation set, 2) Verify attention mask implementation blocks input-to-target flow correctly, 3) Monitor embedding variance during contrastive training to detect collapse.

## Open Questions the Paper Calls Out
- Can generative and contrastive objectives be optimized jointly rather than sequentially without causing representation collapse?
- Would an adaptive or hierarchical bottleneck mechanism outperform the current fixed-length token approach for inputs with highly variable information density?
- Can the LBR framework be effectively extended to handle high-level logical deduction by incorporating reasoning-specific supervision (e.g., Chain-of-Thought) into the generative stage?

## Limitations
- Fixed-length bottlenecks may be suboptimal for inputs with highly variable information density
- Complex logical deduction remains challenging, suggesting need for reasoning-specific supervision
- Method validated only on three domains (medical, chemistry, code) with limited baseline comparisons

## Confidence
- **High confidence**: Core methodological insight and framework design are clearly articulated and theoretically sound
- **Medium confidence**: Reported improvements are substantial and consistent, but lack of open data and complete hyperparameter details limits verification
- **Low confidence**: Qualitative claims about semantic information control in bottleneck lack quantitative analysis (e.g., mutual information bounds)

## Next Checks
1. Obtain or simulate domain-specific QA data matching the "unified format" with sufficient scale (150k for GL, 50k for CL, 20k test)
2. Systematically sweep bottleneck compression ratio R across [100, 500, 1000] and InfoNCE temperature in [0.01, 0.02, 0.05] to confirm optimal settings
3. Apply LBR pipeline to a held-out domain (e.g., legal or finance) and measure R@10/NDCG@10 to assess generalizability beyond evaluated domains