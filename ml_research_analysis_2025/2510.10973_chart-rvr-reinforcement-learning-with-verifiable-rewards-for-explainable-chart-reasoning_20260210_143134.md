---
ver: rpa2
title: 'Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable
  Chart Reasoning'
arxiv_id: '2510.10973'
source_url: https://arxiv.org/abs/2510.10973
tags:
- chart
- answer
- table
- type
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Chart-RVR improves chart reasoning accuracy by combining Group
  Relative Policy Optimization with verifiable rewards. The framework enforces three
  verifiable objectives: chart-type classification, table reconstruction, and process
  conformity, encouraging faithful step-by-step reasoning.'
---

# Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning

## Quick Facts
- arXiv ID: 2510.10973
- Source URL: https://arxiv.org/abs/2510.10973
- Reference count: 39
- Improves chart reasoning accuracy by combining Group Relative Policy Optimization with verifiable rewards across three objectives

## Executive Summary
Chart-RVR is a reinforcement learning framework that enhances chart reasoning by enforcing verifiable objectives: chart-type classification, table reconstruction, and process conformity. Applied to models like Qwen2.5VL-3B, it improves out-of-distribution performance and interpretability. The approach combines GRPO with verifiable rewards to produce more accurate and explainable chain-of-thought rationales, surpassing state-of-the-art chart-specific baselines on six benchmarks.

## Method Summary
Chart-RVR uses Group Relative Policy Optimization with verifiable rewards to improve chart reasoning. The framework enforces three verifiable objectives: correct chart-type classification, faithful table reconstruction, and process conformity. GRPO samples multiple rollouts per prompt, converts absolute rewards to relative advantages, and applies clipped surrogate optimization with KL regularization. Process conformity rewards add step-wise embedding similarity checks to ensure faithful intermediate reasoning.

## Key Results
- Achieves best accuracy among 2-3B models on six chart benchmarks
- Closes the out-of-distribution performance gap compared to supervised fine-tuning
- Produces more interpretable chain-of-thought rationales while improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Verifiable surrogate-task rewards reduce reasoning uncertainty by conditioning on explicit chart structure
- Two auxiliary tasks—chart-type classification and table reconstruction—produce intermediate representations that lower conditional entropy for the final answer
- Core assumption: Chart type and table are non-redundant with respect to the query; errors in these propagate to final errors
- Break condition: If chart type or table is irrelevant to most queries, surrogate gradients become noise

### Mechanism 2
- Group Relative Policy Optimization with process-conformity rewards yields denser credit assignment than outcome-only rewards
- For each prompt, sample G rollouts; convert absolute rewards to relative advantages via group mean/variance normalization; apply clipped surrogate with KL penalty
- Process conformity adds step-wise embedding similarity and overall reasoning alignment, penalizing stylistic drift
- Break condition: If ground-truth rationales are low-quality or embedding similarity fails to distinguish correct vs. plausible-but-wrong steps, Rproc reinforces spurious patterns

### Mechanism 3
- Verifiable rewards improve out-of-distribution generalization by rewarding task success over style imitation
- GRPO with verifiable rewards optimizes outcome/process criteria that generalize across styles
- Reports OOD gains: EvoChart +7.28%, ChartQAPro +4.82%, ChartBench +3.68% over SFT
- Break condition: If OOD shifts include novel chart types or operations not covered by verifiable checks, rewards provide no signal

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO) and policy gradients
  - Why needed here: GRPO extends PPO with group-wise advantages and KL regularization; understanding clipping, advantages, and KL penalties is prerequisite
  - Quick check question: Explain why clipping prevents large policy updates and how KL penalty stabilizes training relative to a reference policy

- Concept: Conditional entropy and information theory basics
  - Why needed here: Proposition 1 uses H(Y|X,Q) vs H(Y|X,Q,C*,T*) to justify surrogates; you must interpret entropy reduction as uncertainty decrease
  - Quick check question: If I(X;Y|Z)=0, what does that imply about Y's dependence on X given Z?

- Concept: Embedding-based similarity and its limitations
  - Why needed here: Process conformity uses cosine similarity of sentence embeddings; you must know when this aligns with semantic correctness vs. surface similarity
  - Quick check question: Give a counterexample where two reasoning traces have high embedding similarity but different logical validity

## Architecture Onboarding

- Component map: Prompt formatting with structured tags -> Rollout generator (samples G completions) -> Verifiable reward modules (Rfmt, Rlen, Racc, Rtype, Rtable, Rproc) -> GRPO optimizer (normalizes rewards to advantages, applies clipped surrogate with KL penalty, updates πθ)

- Critical path:
  1. Prompt formatting with structured tags (<type>, <table>, reasoning steps, <answer>)
  2. Generate G rollouts; parse each for type, JSON table, steps, answer
  3. Compute all rewards; aggregate via R = Rschema + λ1 Rsurr + λ2 Rproc
  4. Normalize to advantages; update policy with GRPO objective

- Design tradeoffs:
  - More rollouts (G) improve advantage estimation but increase compute
  - Higher λ2 strengthens process conformity but risks overfitting to ground-truth style
  - Length rewards prevent trivial short answers but can cause "filler" tokens; mitigated by stacked/penalized rewards

- Failure signatures:
  - Reward hacking: repetitive steps, newline spam, unparseable JSON despite format rewards
  - Training collapse: sudden reward spike then zero if early hacking isn't penalized
  - OOD failure: misidentified chart types or missing table entries for novel visual styles

- First 3 experiments:
  1. Ablate Rproc: train with Rschema + Rsurr only; compare OOD accuracy to full Chart-RVR on EvoChart/ChartQAPro
  2. Vary G (2 vs 4 vs 8): measure stability of advantages and final accuracy; identify compute-accuracy tradeoff
  3. Stress-test surrogate necessity: remove Rtype or Rtable individually; quantify impact on both surrogate-task metrics and downstream QA accuracy

## Open Questions the Paper Calls Out
None

## Limitations

- The entropy-reduction argument lacks direct empirical validation; Proposition 1 is theoretical and no ablation tests confirm that surrogate tasks actually reduce uncertainty
- Process conformity rewards depend on embedding similarity, which may not reliably distinguish correct from superficially similar but logically flawed reasoning
- OOD generalization gains assume shifts are primarily stylistic rather than semantic; verifiable rewards may fail on novel chart types or operations

## Confidence

- Mechanism 1 (entropy reduction): Medium - theoretically grounded but under-tested empirically
- Mechanism 2 (GRPO + process rewards): Medium-High - consistent with RLVR literature but chart-specific gains not independently replicated
- Mechanism 3 (OOD generalization): Medium - supported by reported numbers but assumes distribution shift characteristics

## Next Checks

1. Ablate Rproc: Train with Rschema + Rsurr only and compare OOD accuracy on EvoChart/ChartQAPro to isolate process reward impact
2. Vary G (2 vs 4 vs 8): Measure advantage estimation stability and final accuracy to quantify compute-accuracy tradeoffs
3. Stress-test surrogate necessity: Remove Rtype or Rtable individually and measure impact on both surrogate-task metrics and downstream QA accuracy