---
ver: rpa2
title: 'Align Your Query: Representation Alignment for Multimodality Medical Object
  Detection'
arxiv_id: '2510.02789'
source_url: https://arxiv.org/abs/2510.02789
tags:
- modality
- detection
- object
- medical
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of degraded performance in medical
  object detection when training a single detector on mixed imaging modalities (e.g.,
  CXR, CT, MRI) due to heterogeneous statistics and disjoint representation spaces.
  The authors propose a method that leverages representation alignment to inject modality
  context into DETR-style object queries, improving their modality awareness and class
  faithfulness.
---

# Align Your Query: Representation Alignment for Multimodality Medical Object Detection

## Quick Facts
- **arXiv ID**: 2510.02789
- **Source URL**: https://arxiv.org/abs/2510.02789
- **Reference count**: 20
- **Primary result**: DINO+method achieves 41.1 AP vs 37.7 baseline on mixed multimodality medical detection

## Executive Summary
This paper addresses the challenge of degraded performance when training a single object detector on mixed medical imaging modalities (CXR, CT, MRI, etc.) due to heterogeneous statistics and disjoint representation spaces. The authors propose a method that leverages representation alignment to inject modality context into DETR-style object queries, improving their modality awareness and class faithfulness. The core approach involves two key components: Multimodality Context Attention (MoCA) which appends modality tokens to the query set and uses self-attention to mix them, and QueryREPA, a short pretraining stage that aligns query representations to modality tokens using a contrastive objective with modality-balanced batches. The method consistently improves Average Precision across diverse medical imaging modalities without requiring architectural modifications to the underlying detector.

## Method Summary
The method operates on DETR-style object queries by first constructing modality tokens using text encoders with "<CLASS> in <MODALITY>" prompts, extracting [CLS] embeddings and projecting them to model dimension. During pretraining (QueryREPA), object queries are aligned to their corresponding modality tokens via InfoNCE contrastive loss using modality-balanced batches where each sample comes from a distinct modality. The main training then proceeds with MoCA, which appends the modality token to the query set and allows self-attention mixing in decoder layers. This injects modality cues into object queries while preserving DETR architecture and adding negligible latency. The approach is validated on a challenging mixed multimodality dataset achieving 41.1 AP (vs 37.7 baseline) and 65.5 AP50 (vs 58.6 baseline).

## Key Results
- DINO+method achieves 41.1 AP vs 37.7 baseline on mixed multimodality medical detection
- DINO+method achieves 65.5 AP50 vs 58.6 baseline, representing significant improvements
- Method demonstrates robust performance across different object scales without architectural modifications
- MoCA adds only 0.2-0.9 ms overhead per image while improving detection accuracy

## Why This Works (Mechanism)

### Mechanism 1: Modality Token Injection via Self-Attention (MoCA)
Appending compact text-derived modality tokens to the object query set and allowing self-attention mixing enables queries to become modality-aware without architectural changes. The modality token acts as a "semantic anchor" in the query set. During decoder self-attention, each of the N object queries can attend to the modality token, propagating modality-specific context into query representations. Since self-attention already exists in DETR-style architectures, this adds negligible latency (~0.2–0.9 ms overhead per image). Core assumption: Object queries learn to attend to informative signals during training, and the modality token carries sufficient semantic information to influence query refinement.

### Mechanism 2: Contrastive Alignment of Query Statistics (QueryREPA)
Pretraining query representations to align with their corresponding modality tokens via contrastive learning creates modality-aware and class-faithful queries before detection training begins. Before detection training, the model maximizes mutual information between the cluster mean of queries at decoder layer l and their corresponding modality token using InfoNCE loss. Modality-balanced batches ensure negatives come from different modalities, structuring the query manifold by modality and class semantics. Core assumption: The cluster mean of object queries at a decoder layer carries sufficient modality information; this pretraining transfers to improved detection via better query initialization.

### Mechanism 3: Modality-Balanced Batch Sampling for Cross-Modality Separability
Constructing mini-batches where each sample comes from a distinct modality ensures contrastive negatives are explicitly from different modalities, focusing alignment on cross-modality separability. A round-robin sampler cycles over M modalities, drawing one example per modality per batch. For any anchor from modality d, negatives are tokens from d' ≠ d, creating well-conditioned gradients for modality distinction. Core assumption: Cross-modality confusion is a primary degradation source in mixed-modality training; explicit modality contrasting produces transferable representations.

## Foundational Learning

### Concept: DETR-style Object Queries
**Why needed here**: The entire method operates on object queries. Understanding that queries are learnable embeddings informing class prediction and bounding box regression is essential to grasp why aligning them with modality tokens could improve detection.  
**Quick check question**: How do object queries in DETR differ from anchor boxes in Faster R-CNN, and where in the DETR architecture are queries refined?

### Concept: Contrastive Learning (InfoNCE)
**Why needed here**: QueryREPA uses InfoNCE loss to align query representations with modality tokens. Understanding how positive pairs are pulled together and negatives pushed apart—and how this relates to mutual information maximization—is necessary.  
**Quick check question**: Given a batch with images from CXR, CT, and MRI, what are the "positive" and "negative" pairs for a CXR image's query representation in QueryREPA?

### Concept: Self-Attention in Transformer Decoders
**Why needed here**: MoCA injects modality information through existing self-attention. Understanding how each query attends to all queries (including the appended modality token) clarifies information propagation without new cross-attention modules.  
**Quick check question**: In a decoder layer with N object queries and 1 appended modality token, how does the attention weight between query i and the modality token influence query i's output representation?

## Architecture Onboarding

### Component map
Input Image → Image Encoder (ResNet-50) → Features

Modality Token Construction:
"<CLASS> in <MODALITY>" → Text Encoder (CLIP/BiomedCLIP/PubMedCLIP) → [CLS] embedding → Linear projection → m_{d,c}

PRETRAINING (QueryREPA):
- Modality-balanced batch sampling
- Object queries Q at layer l → cluster mean q^(l)
- Projection g_φ(q^(l)) aligned to m_{d,c} via InfoNCE
- Only g_φ and query embeddings trained; backbone frozen

FINETUNING (with MoCA):
- Augmented query set: Q̃ = [Q, f_θ(m_{d,c})]  (N+1 tokens)
- Self-attention over Q̃ in decoder layers
- Standard detection losses (focal, L1, GIoU)

Output: Class predictions + Bounding boxes

### Critical path
1. Modality token quality: Text encoder and prompt format determine token disentanglement across modalities.
2. QueryREPA layer selection: Must be at layer l after cross-attention injects image features (l ≠ 1; paper uses l = 5).
3. Batch balancing during pretraining: Essential for cross-modality contrastive focus.

### Design tradeoffs
- Text encoder choice: CLIP/PubMedCLIP outperform BiomedCLIP in experiments, attributed to better modality token disentanglement (Figure 4 UMAP).
- Separate pretraining vs. joint training: QueryREPA adds a training stage but simplifies optimization; joint training is noted as future work.

### Failure signatures
- Uniform attention to modality token: Modality information not used.
- Collapsed contrastive loss: Trivial projection solution; monitor loss curves and token distances.
- Per-modality degradation: Indicates batch balancing failure or modality dominance.

### First 3 experiments
1. Reproduce single-modality vs. mixed-modality baseline to confirm the degradation this work addresses.
2. Ablate MoCA alone (without QueryREPA) to isolate its contribution (Table 2: +3.1 AP for DINO+CLIP without QueryREPA).
3. Visualize modality token embeddings (UMAP/t-SNE) and compute silhouette scores to confirm disentanglement correlates with performance (Section 4.3).

## Open Questions the Paper Calls Out
1. Can the QueryREPA alignment objective be optimized jointly with the detection loss during end-to-end training to eliminate the need for a separate pretraining stage? The authors note in the Limitations section that QueryREPA adds pretraining before end-to-end detection training; future work could explore joint alignment training.

2. Does the proposed framework maintain its effectiveness and safety profile when deployed on multi-center clinical data with distinct domain shifts? The Limitations section notes that the method is evaluated only on public datasets, thus additional validation on multi-center clinical data must be further explored.

3. Would making the modality tokens learnable or image-adaptive improve performance over the current frozen, text-derived embeddings? The paper defines modality tokens as "compact, text-derived embeddings" projected from a frozen text encoder, but does not explore if these tokens should be optimized to capture visual statistics specific to the detection task.

## Limitations
- The method relies on modality tokens derived from text encoders, making performance highly dependent on the quality of these embeddings and their disentanglement across modalities
- The approach requires domain-specific text encoders for optimal performance in specialized medical subfields
- The method adds computational overhead through MoCA and QueryREPA pretraining, though this is relatively modest (0.2-0.9 ms per image)
- The contrastive alignment in QueryREPA assumes modality-balanced batch sampling is feasible, which may be challenging with highly imbalanced datasets

## Confidence
- **High Confidence**: The core mechanism of MoCA (modality token injection via self-attention) and its latency claims are well-supported by architecture descriptions and ablation studies
- **Medium Confidence**: The effectiveness of QueryREPA pretraining is demonstrated through controlled experiments, but the exact contribution of modality-balanced batch sampling versus the contrastive objective itself remains partially unclear
- **Low Confidence**: The assumption that text-derived modality tokens provide optimal semantic anchors for medical imaging modalities, given the potential domain shift between natural language and medical imaging concepts

## Next Checks
1. **Ablation of modality token quality**: Replace CLIP-derived tokens with randomly initialized modality embeddings and measure performance degradation to isolate the contribution of semantic versus purely discriminative modality information.

2. **Batch balancing stress test**: Systematically vary batch composition (from strictly modality-balanced to imbalanced) during QueryREPA pretraining and measure the impact on final detection performance across all modalities.

3. **Cross-modality generalization**: Train the model on a subset of modalities (e.g., only CXR and CT) and evaluate on held-out modalities (e.g., MRI, pathology) to assess whether QueryREPA provides transfer benefits beyond modality alignment.