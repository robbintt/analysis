---
ver: rpa2
title: Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment
arxiv_id: '2510.05135'
source_url: https://arxiv.org/abs/2510.05135
tags:
- expert
- creativity
- wang
- curiosity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a curiosity-driven LLM-as-a-judge for personalized
  creative evaluation. It addresses the challenge that LLMs struggle with subjective
  tasks like creativity assessment, where human annotators often disagree.
---

# Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment

## Quick Facts
- arXiv ID: 2510.05135
- Source URL: https://arxiv.org/abs/2510.05135
- Reference count: 22
- Key outcome: Curiosity-driven LLM-as-a-judge shows significant improvements over baseline SFT in personalized creative evaluation, with Pearson correlation improvements from 0.170 to 0.524 for 0.5B models and better generalization to out-of-distribution dimensions.

## Executive Summary
This paper addresses the challenge of subjective evaluation in creative writing by proposing a curiosity-driven LLM-as-a-judge. The method reinterprets curiosity as a belief-shift signal, using an Intrinsic Curiosity Model (ICM) that learns from expert explanations and predicts which expert provided them. The resulting curiosity score enhances a supervised fine-tuning (SFT) model for personalized judgment. Experiments on the TTCW dataset across model sizes (0.5B-7B) demonstrate significant improvements over baseline SFT in correlation and F1 scores, with better generalization to out-of-distribution dimensions.

## Method Summary
The method uses a two-stage approach: First, an ICM computes curiosity scores by measuring belief shifts between story+question+expert ID states and story+question+explanation states using cosine loss. Second, a backward classification task predicts which expert authored each explanation. The scalar curiosity score is then used as input to an SFT classifier predicting binary verdicts. This approach conditions on the weight of evidence while filtering out lexical/style noise, theoretically providing variance reduction through control variate effects.

## Key Results
- Pearson correlation improvements: 0.524 vs 0.170 for 0.5B models, 0.495 vs 0.387 for 7B models
- F1 score improvements: 0.616 vs 0.382 for 0.5B models, 0.574 vs 0.493 for 7B models
- Better generalization to out-of-distribution dimensions compared to SFT baseline
- Outperforms GPT-5 in one-shot settings for personalized creative judgment

## Why This Works (Mechanism)

### Mechanism 1
Quantifying "belief shift" effectively measures the information gain an expert explanation provides to the model. The model computes cosine loss between two states: State A (Story + Question + Expert ID) and State B (Story + Question + Explanation). The difference in the model's output logits (belief) between these states generates the "curiosity score." High divergence indicates the explanation contained information the model did not previously infer from the context alone. This assumes the delta in logit space is a proxy for "surprise" or novelty specific to the creative judgment task, rather than just generic linguistic variance.

### Mechanism 2
Forcing the model to attribute explanations to specific experts calibrates the curiosity signal for personalized judgment. An auxiliary "backward" classification task trains the model to predict which expert authored a given explanation. This compels the ICM to learn distinct reasoning styles associated with each annotator, preventing the forward score from being a generic similarity metric. This assumes experts possess distinct, learnable "styles" or "biases" that correlate with their specific creative verdicts.

### Mechanism 3
Using a scalar curiosity score acts as a variance-reducing "control variate" compared to using raw explanation text. Instead of conditioning the final SFT judge on the raw text of the explanation, the method conditions it on the scalar curiosity score. This approximates a sufficient statistic for the "weight of evidence," filtering out irrelevant linguistic variance while keeping the decision-relevant signal. This assumes the curiosity score captures the decision-relevant information of the explanation while discarding stylistic noise.

## Foundational Learning

- **Concept: Intrinsic Curiosity Module (ICM)**
  - Why needed here: Core architectural component borrowed from RL, re-purposed to measure "surprise" in subjective evaluation
  - Quick check question: How does the definition of "curiosity" in this paper differ from the prediction-error definition in standard RL?

- **Concept: Control Variates (Variance Reduction)**
  - Why needed here: Justifies replacing text explanations with a score using control variate theory for better sample efficiency
  - Quick check question: According to Section 3, how does the correlation ρ between the loss and curiosity score theoretically affect the variance of the estimator?

- **Concept: Subjectivity & Disagreement in Evaluation**
  - Why needed here: Method is explicitly designed for cases where "not all annotators agree"
  - Quick check question: Why does the paper use the TTCW dataset instead of a factual reasoning dataset like GSM8K to validate this method?

## Architecture Onboarding

- **Component map:** Base LLM (Frozen/LoRA) -> ICM (computes Forward/Backward Losses) -> Curiosity Scorer (generates scalar score) -> Final Classifier (SFT with input Q_d + S + <CREAT> + Curiosity Score)

- **Critical path:** Training ICM: Input (Story, Question, Expert ID, Explanation) → Compute Forward/Backward Losses → Generate Signal: Pass data through trained ICM to extract scalar Curiosity Scores → Training Judge: Input (Story, Question, Curiosity Score) → Predict Verdict (Yes/No)

- **Design tradeoffs:** The method discards explanation text during final judgment phase, trading richness of Chain-of-Thought for reduced variance of scalar signal. LoRA rank experiments show ICM works best with low ranks (32/16) compared to SFT baselines (256/256), suggesting "belief shift" signal is efficient to learn.

- **Failure signatures:** Mode collapse to "No" (GPT-5 comparisons showed bias toward negative judgments); ID overfitting (baseline "without explanation" degrades with scale due to overfitting on small data)

- **First 3 experiments:**
  1. Ablate the Inverse Model: Train ICM with λ=0 (no backward loss) to verify if expert attribution is strictly necessary for performance lift
  2. OOD Generalization Check: Train on 4 dimensions and test on held-out dimension to replicate generalization claim
  3. Scale Consistency: Run 0.5B vs. 7B comparison to confirm ICM improves with scale while classification baseline degrades

## Open Questions the Paper Calls Out
- Can the learned curiosity score function effectively as a reward signal within a Reinforcement Learning (RL) framework to further optimize personalized judgment?
- Does the curiosity-driven approach generalize to subjective evaluation domains beyond creative writing, such as marketing or scientific novelty assessment?
- Is the "backward score" (expert attribution) strictly necessary for robustness against non-expert annotators, given its negligible impact on expert-only data?

## Limitations
- The sufficiency assumption for curiosity scores as a control variate remains largely theoretical without direct empirical variance evidence
- Expert attribution mechanism effectiveness is assumed rather than empirically proven through ablation studies
- The TTCW dataset (720 examples) is small, raising concerns about overfitting in larger models (0.5B-7B range)

## Confidence
- **High Confidence:** ICM architecture implementation and training procedure, correlation and F1 score improvements on TTCW dataset, better OOD generalization than SFT baseline
- **Medium Confidence:** Superiority over GPT-5 one-shot evaluation, variance reduction benefits from using curiosity scores vs. raw explanations, expert attribution improving personalized calibration
- **Low Confidence:** Theoretical grounding of belief shift as proxy for information gain, claims about control variate sufficiency without empirical variance measurements, scalability to larger model sizes given limited validation

## Next Checks
1. **Ablation of Expert Attribution:** Train ICM with λ=0 (backward loss disabled) and compare performance to full ICM to quantify contribution of expert-specific calibration
2. **Variance Analysis Experiment:** Measure and compare variance of predictions when conditioning on raw explanations vs. curiosity scores across multiple runs to empirically validate control variate claim
3. **Inter-Annotator Disagreement Test:** Identify dimensions with highest expert disagreement in TTCW and evaluate whether ICM performance gains are most pronounced where subjective judgment is most challenging