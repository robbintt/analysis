---
ver: rpa2
title: 'CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code
  Adjustments'
arxiv_id: '2510.27565'
source_url: https://arxiv.org/abs/2510.27565
tags:
- code
- instruction
- instructions
- tasks
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeAlignBench introduces a benchmark for evaluating how well code
  generation models follow developer-provided instructions. The key idea is to collect
  real-world coding preferences from developers across Python, Java, and JavaScript,
  and then systematically test whether models can adjust their code to match those
  preferences.
---

# CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments

## Quick Facts
- **arXiv ID**: 2510.27565
- **Source URL**: https://arxiv.org/abs/2510.27565
- **Reference count**: 14
- **Primary result**: Benchmark shows significant gaps in models' ability to follow developer-provided code instructions, with structural adjustments easiest and semantic hardest.

## Executive Summary
CodeAlignBench introduces a benchmark for evaluating how well code generation models follow developer-provided instructions. The key idea is to collect real-world coding preferences from developers across Python, Java, and JavaScript, and then systematically test whether models can adjust their code to match those preferences. This goes beyond standard correctness checks to assess style, structure, and semantic adjustments. The evaluation pipeline uses a modular framework with applicability and verification functions for each instruction type, enabling scalable and language-agnostic testing.

Experiments on ten models show significant performance gaps: models score about 30% higher on follow-up tasks than predefined ones, with structural instructions being the easiest and semantic ones the hardest. Top models like GPT-5 and GPT-5 mini lead in overall performance, though none achieve saturation. The framework also demonstrates high reliability when using LLM judges, with over 87% agreement with human annotators. Overall, CodeAlignBench provides a realistic, extensible way to measure instruction-following in code generation, revealing that even leading models have substantial room for improvement in aligning with developer intent.

## Method Summary
The benchmark evaluates code generation models on instruction-following capabilities across two settings: predefined instructions embedded in the initial prompt, and follow-up instructions provided after initial code generation. The evaluation pipeline uses LiveBench code generation tasks (LeetCode, AtCoder) translated from Python to Java and JavaScript, along with 228 verified developer instructions collected via user study. The two-stage pipeline consists of Task Construction (applicability checker selects relevant instructions) and IF Evaluation (verifier checks compliance). Each instruction has `is_applicable(code)` and `verify(code_after, code_before: Optional)` functions, implemented via rules or LLM-as-a-judge (Claude Sonnet 4). Binary success rate is used as the primary metric, with high agreement (>87%) between LLM judges and human annotators.

## Key Results
- Models score approximately 30% higher on follow-up tasks compared to predefined tasks
- Structural instructions are easiest to follow; semantic instructions are hardest
- GPT-5 and GPT-5 mini lead in overall performance, but no model achieves saturation
- LLM judges demonstrate over 87% agreement with human annotators

## Why This Works (Mechanism)
The benchmark works by creating a realistic evaluation scenario that mirrors how developers actually work with code generation models - by providing instructions after seeing initial code. The modular evaluation pipeline with applicability and verification functions allows systematic testing of diverse instruction types while maintaining language-agnostic capabilities.

## Foundational Learning
- **LLM-as-a-judge**: Using language models to verify instruction compliance - needed for scalable evaluation of diverse instruction types; quick check: measure agreement between LLM judge and human annotators
- **Modular evaluation pipeline**: Separating task construction from instruction-following evaluation - needed for systematic testing across different instruction categories; quick check: verify balanced sampling across instruction types
- **Binary success rate metric**: Simple pass/fail evaluation for instruction compliance - needed for clear, interpretable results; quick check: calculate inter-annotator agreement for verification

## Architecture Onboarding
- **Component map**: LiveBench tasks -> Instruction catalog -> Applicability checker -> Code generation models -> LLM verifier -> Success rate
- **Critical path**: Task Construction (applicability check) -> IF Evaluation (verification) -> Aggregation
- **Design tradeoffs**: LLM judges vs. human judges (scalability vs. potential bias)
- **Failure signatures**: Low applicability rates may indicate instruction ambiguity; inconsistent verification may indicate judge bias
- **First experiments**: 1) Test a single instruction type across all models; 2) Compare predefined vs. follow-up settings for one model; 3) Verify LLM judge reliability on a small subset

## Open Questions the Paper Calls Out
None

## Limitations
- Instruction implementation completeness is uncertain - exact `is_applicable()` and `verify()` implementations for all 228 instructions are not fully specified
- Heavy reliance on LLM judges may introduce bias, especially toward same-family model outputs
- Performance may vary across programming languages, but language-specific analysis is limited

## Confidence
- **High Confidence**: Core methodology and high agreement between LLM judges and human annotators
- **Medium Confidence**: Overall findings about performance gaps, but uncertainty about disentangling instruction ambiguity from model limitations
- **Low Confidence**: Precise impact of in-context exemplars and prompt engineering on results

## Next Checks
1. Cross-judge validation: Re-run evaluation using multiple LLM judges (Claude Sonnet 4 and GPT-5) on held-out tasks to quantify and mitigate judge bias
2. Instruction implementation audit: Manually verify `is_applicable()` and `verify()` implementations for 20 randomly sampled instructions across all categories
3. Language-specific analysis: Stratify results by programming language to identify consistent performance differences and investigate root causes