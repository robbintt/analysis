---
ver: rpa2
title: Image-Seeking Intent Prediction for Cross-Device Product Search
arxiv_id: '2511.14764'
source_url: https://arxiv.org/abs/2511.14764
tags:
- product
- intent
- user
- utterance
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Image-Seeking Intent Prediction for Cross-Device Product Search
  We introduce Image-Seeking Intent Prediction, a task for LLM-driven e-commerce assistants
  to determine when a voice query requires visual content. Our IRP model uses query
  transcripts and summarized product metadata to predict visual intent with 78.47%
  precision and 60.42% recall, significantly outperforming baselines.
---

# Image-Seeking Intent Prediction for Cross-Device Product Search

## Quick Facts
- arXiv ID: 2511.14764
- Source URL: https://arxiv.org/abs/2511.14764
- Reference count: 40
- Primary result: Image-Seeking Intent Prediction (IRP) model achieves 78.47% precision and 60.42% recall for predicting when voice queries require visual content in cross-device product search.

## Executive Summary
We introduce Image-Seeking Intent Prediction (IRP), a task for LLM-driven e-commerce assistants to determine when a voice query requires visual content. Our IRP model uses query transcripts and summarized product metadata to predict visual intent with 78.47% precision and 60.42% recall, significantly outperforming baselines. By optimizing for precision through a differentiable loss function, we reduce false positives while maintaining strong recall. The model demonstrates that combining utterance semantics with product data, particularly when improved through lightweight summarization, consistently improves prediction accuracy. Our approach enables intelligent, cross-device shopping assistants that anticipate and adapt to user needs, facilitating seamless device transitions when visual content would enhance product discovery.

## Method Summary
The IRP model is a transformer-based binary classifier that predicts whether a voice query requires visual content. It takes as input a user utterance (transcription + intent label) and the top-k retrieved product metadata (attributes like title, brand, color, style). Product metadata is summarized via mean pooling across attributes, then concatenated with the utterance tokens. The model is trained on 900K interactions with labels derived from image carousel taps (proxy for visual intent). Training uses a combined loss function (α·BCE + β·Precision Loss) to optimize for precision, minimizing unnecessary device switches. The model is evaluated on Precision (primary), Recall, and F0.5 score.

## Key Results
- IRP model achieves 78.47% precision and 60.42% recall, outperforming utterance-only baselines
- Combining query semantics with product metadata consistently improves prediction accuracy
- Optimizing for precision through differentiable loss function reduces false positives without sacrificing recall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining user utterance semantics with retrieved product metadata significantly improves visual intent prediction accuracy compared to using utterance features alone.
- **Mechanism:** Voice queries are often ambiguous. By encoding the attributes of the retrieved products (e.g., "color," "style") alongside the user's speech, the model infers visual intent from the *context* of the results, not just the query text.
- **Core assumption:** The retrieved top-k products are relevant to the user's intent; if retrieval fails, the intent signal is noisy.
- **Evidence anchors:**
  - [abstract] "Our experiments show that combining query semantics with product data... consistently improves prediction accuracy."
  - [section 3.4.1] Table 1 shows that combining Utterance, Intent, and Title yields higher Precision (78.11) than Utterance alone (77.83).
  - [corpus] Neighbor paper "Multi-Modality Transformer for E-Commerce" supports the general efficacy of bridging query-product gaps via multi-modal transformers.
- **Break condition:** If the product catalog has sparse metadata or the retrieval system returns irrelevant items, the fusion signal degrades toward noise.

### Mechanism 2
- **Claim:** Lightweight summarization of product metadata (mean pooling) preserves discriminative signals for visual intent while fitting within transformer context limits.
- **Mechanism:** Raw product attributes exceed token limits. Mean pooling aggregates features (e.g., visual attributes like "color" or "style") across top-k products into a compact representation, allowing the model to detect if the *set* of results is highly visual.
- **Core assumption:** Visual intent is correlated with the aggregate visual attributes of the retrieved product set.
- **Evidence anchors:**
  - [section 3.4.2] Table 2 shows Aggregation by mean ($f_{sum}$) achieves the highest Precision (78.47).
  - [section 2.2] Equation (3) defines the summary function $p_{sum}$ used to condense product tokens.
  - [corpus] "Generating Query-Relevant Document Summaries" supports the general approach of condensing document info for query relevance, though not specific to this architecture.
- **Break condition:** If the visual attributes of the top-k products are highly inconsistent (high variance), pooling may dilute the specific visual signal.

### Mechanism 3
- **Claim:** Optimizing the loss function explicitly for precision reduces false positives (unnecessary device switches) without proportionally sacrificing recall.
- **Mechanism:** Standard Binary Cross-Entropy (BCE) treats all errors equally. By introducing a differentiable precision surrogate loss ($L_{Sum}$), the model heavily penalizes false positives, aligning training with the business goal of minimizing user friction.
- **Core assumption:** The primary cost in the system is "user friction" from an unneeded device switch, rather than a missed visual opportunity.
- **Evidence anchors:**
  - [abstract] "Incorporating a differentiable precision-oriented loss further reduces false positives."
  - [section 3.5.1] Table 3 indicates that the Combined Loss ($L_{Sum}$) provides competitive $F_{0.5}$ scores by balancing precision and recall effectively.
  - [corpus] Corpus signals do not explicitly validate precision-loss mechanisms in this specific context; general IR literature suggests optimizing for the metric of interest (e.g., Precision) improves it.
- **Break condition:** If the proxy label (image tap) contains significant noise (accidental taps), optimizing strictly for precision on these labels may lead to a conservative model that misses valid visual intents.

## Foundational Learning

- **Concept: Weak Supervision via Proxy Labels**
  - **Why needed here:** There is no ground-truth dataset for "visual intent" at scale. You must understand that "image carousel taps" are an *implicit* behavioral signal serving as a noisy proxy for the true label.
  - **Quick check question:** If users tap images for unrelated reasons (e.g., boredom), how would this affect your model's "ground truth"?

- **Concept: Precision vs. Recall Trade-offs in UX**
  - **Why needed here:** In cross-device switching, a false positive is "interruptive," while a false negative is just a "missed opportunity." You must understand why the paper optimizes for $F_{0.5}$ (favoring precision) rather than $F_1$.
  - **Quick check question:** Why is high precision prioritized over high recall in this specific assistant interaction?

- **Concept: Token Budgeting and Summarization**
  - **Why needed here:** Transformers have strict input limits. Feeding 10 full product descriptions + a query is impossible. You need to understand pooling/MMR as compression techniques.
  - **Quick check question:** Why does the paper aggregate product attributes via mean pooling rather than concatenating all product titles?

## Architecture Onboarding

- **Component map:**
  Utterance + Intent -> Tokenizer -> Utterance Tokens
  Top-k Products -> Summarizer (Mean Pooling) -> Product Tokens
  Utterance Tokens + Product Tokens -> Transformer Backbone (DistilBERT/RoBERTa/XLNet) -> Binary Classifier

- **Critical path:** The **Product Summarization** module. If the summarization ($f_{sum}$) fails to extract visual cues (color, style) from the metadata, the concatenated input lacks the necessary signal to distinguish a "visual" query from a generic one.

- **Design tradeoffs:**
  - **DistilBERT vs. XLNet:** DistilBERT is 5x smaller and faster with competitive precision (78.47), while XLNet offers slightly better recall (60.64) and $F_{0.5}$ but is heavier (340M params).
  - **Mean Pooling vs. MMR:** Mean Pooling optimizes for **Precision** (less disruption), while MMR optimizes for **Recall** (capturing more visual needs).

- **Failure signatures:**
  - **High False Positive Rate:** The model suggests switching for non-visual queries (e.g., "buy batteries").
    - *Fix:* Increase the $\beta$ weight of the Precision Loss component.
  - **Low Recall on Obvious Visuals:** The model misses "red dress" queries.
    - *Fix:* Check if the retrieval system is passing "color" metadata correctly; ensure the product tokenizer isn't stripping color tokens.

- **First 3 experiments:**
  1. **Baseline Validation:** Replicate the "Utterance Only" vs. "Utterance + Product" experiment (Table 1) to verify that the fusion mechanism provides a lift in your specific environment.
  2. **Loss Function Sensitivity:** Sweep $\alpha$ and $\beta$ in the $L_{Sum}$ function to plot the Precision-Recall curve specific to your latency/UX constraints.
  3. **Summarization Ablation:** Compare Mean Pooling vs. MMR on a held-out set of "known visual" queries (e.g., queries containing color terms) to verify which summarization retains visual attributes better.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the "image carousel tap" proxy signal introduce label noise compared to explicit human annotations of visual intent?
- **Basis in paper:** [explicit] The authors note in the Limitations section that the proxy signal "may not capture all cases of visual intent, leading to potential label noise."
- **Why unresolved:** The study relied entirely on weak supervision via behavioral logs because manual annotation was cost-prohibitive.
- **What evidence would resolve it:** A comparative study benchmarking model performance when trained on behavioral taps versus a gold-standard dataset of human-annotated visual intent.

### Open Question 2
- **Question:** Can recent vision-language or multimodal pretraining approaches improve predictive performance over the text-only transformer backbones used in this study?
- **Basis in paper:** [explicit] The Limitations section states the authors "do not explore recent vision–language or multimodal pretraining approaches that could further improve performance."
- **Why unresolved:** The current IRP model utilizes only text-based backbones (DistilBERT, RoBERTa, XLNet) to process transcribed utterances and summarized product text.
- **What evidence would resolve it:** An ablation study replacing the text-only encoders with multimodal architectures (e.g., CLIP-based models) that can process raw product images directly.

### Open Question 3
- **Question:** How do the offline prediction metrics (Precision/Recall) translate to user satisfaction and system-level trade-offs in a live deployment?
- **Basis in paper:** [explicit] The authors state in Future Work that "evaluation is offline; live deployment studies are required to assess real-world effectiveness... Live A/B testing could provide deeper insights."
- **Why unresolved:** Offline metrics do not capture the "friction" of a real device switch or the long-term impact on user trust and retention.
- **What evidence would resolve it:** Results from a controlled live A/B test measuring conversion rates, user retention, and feedback on proactive device-switching suggestions.

## Limitations
- Model relies on image carousel taps as proxy labels, which may introduce significant noise from unrelated user behaviors
- Performance depends heavily on the quality of the upstream product retrieval system
- Model is trained on proprietary e-commerce data and may not generalize to other domains

## Confidence
**High Confidence:** The architectural approach of combining utterance and product metadata (Mechanism 1) and the use of mean pooling for product summarization (Mechanism 2) are well-supported by the paper's ablation studies (Tables 1 and 2). The core findings—that fusion improves precision and that mean pooling is optimal—are consistently demonstrated.

**Medium Confidence:** The precision-oriented loss function (Mechanism 3) shows theoretical soundness and competitive $F_{0.5}$ scores, but the lack of ablation on loss hyperparameters (α, β) and the absence of validation on a held-out test set leave room for uncertainty about its true impact.

**Low Confidence:** The practical impact of the 78.47% precision in a real-world assistant is unclear. The paper does not report user satisfaction metrics, false positive rates in live deployment, or the cost of missed opportunities, making it difficult to assess the true UX benefit.

## Next Checks
1. **Proxy Label Quality Audit:** Manually sample 100 image carousel taps and verify if they genuinely represent visual intent. Calculate precision@100 of the proxy labels to quantify noise.

2. **Retrieval Failure Simulation:** Deliberately corrupt the top-k product retrieval (e.g., replace with random products) and measure the drop in IRP model performance. This validates the dependency on retrieval quality.

3. **Cross-Domain Transfer Test:** Train the IRP model on the e-commerce dataset and evaluate it on a different visual product domain (e.g., furniture or apparel from a public dataset). Measure precision drop to assess generalizability.