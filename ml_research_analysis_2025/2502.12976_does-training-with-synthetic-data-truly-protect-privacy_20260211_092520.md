---
ver: rpa2
title: Does Training with Synthetic Data Truly Protect Privacy?
arxiv_id: '2502.12976'
source_url: https://arxiv.org/abs/2502.12976
tags:
- data
- privacy
- synthetic
- training
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether training models on synthetic data
  truly protects privacy. Four synthetic data training paradigms are systematically
  evaluated: coreset selection, dataset distillation (with private, noise, and out-of-distribution
  initialization), data-free knowledge distillation, and diffusion model-generated
  data.'
---

# Does Training with Synthetic Data Truly Protect Privacy?

## Quick Facts
- arXiv ID: 2502.12976
- Source URL: https://arxiv.org/abs/2502.12976
- Authors: Yunpeng Zhao; Jie Zhang
- Reference count: 20
- Key outcome: Most empirical synthetic data training methods leak significant privacy on worst-case samples, with only OOD-initialized distillation and text-to-image diffusion showing promising privacy at reduced utility

## Executive Summary
This paper systematically evaluates whether training models on synthetic data truly protects privacy compared to standard training with differential privacy (DPSGD). The authors examine four synthetic data training paradigms: coreset selection, dataset distillation (with private, noise, and out-of-distribution initialization), data-free knowledge distillation, and diffusion model-generated data. Privacy is rigorously measured using membership inference attacks on the most vulnerable samples under worst-case scenarios, with direct comparison to DPSGD baseline. The results show that most empirical synthetic data methods leak significant privacy, especially those using private data initialization, even when synthetic data appears visually dissimilar to private data. Only OOD-initialized dataset distillation and text-to-image diffusion methods show promising privacy, but at the cost of reduced utility. The paper concludes that empirical privacy claims for synthetic data training require rigorous evaluation, as they often provide false privacy assurances.

## Method Summary
The paper evaluates four synthetic data training paradigms on CIFAR-10 using membership inference attacks to measure privacy leakage. A fixed set of 500 random training samples are mislabeled to act as "audit samples" (canaries) for worst-case evaluation. Privacy is measured as True Positive Rate (TPR) at 0.1% False Positive Rate (FPR) using LiRA attacks specifically on these mislabeled canaries, while utility is measured by standard test accuracy. The evaluation includes 32 shadow models per defense (16 with the audit sample, 16 without). The study compares synthetic data methods against a DPSGD baseline and examines privacy-utility tradeoffs across different initialization strategies and generation methods.

## Key Results
- Most empirical synthetic data methods (coreset selection, private-initialized distillation, data-free KD) leak significant privacy on worst-case mislabeled samples
- OOD-initialized dataset distillation and text-to-image diffusion methods show better privacy-utility tradeoffs but with reduced utility
- None of the empirical synthetic data methods achieve better privacy-utility-efficiency tradeoffs than DPSGD
- Visual dissimilarity between synthetic and private data does not guarantee privacy protection
- Privacy leakage occurs through model memorization even when synthetic data appears dissimilar to private data

## Why This Works (Mechanism)
The paper demonstrates that privacy leakage in synthetic data training occurs through model memorization mechanisms that persist even when synthetic data is visually dissimilar to private data. Through logits alignment in knowledge distillation and trajectory matching in dataset distillation, privacy-sensitive information transfers from private data to synthetic data generation, which then leaks to downstream models. The rigorous worst-case evaluation using mislabeled canaries reveals that these memorization patterns enable membership inference attacks despite apparent visual differences.

## Foundational Learning
- **Membership Inference Attacks (MIA)**: Used to measure privacy leakage by determining if specific samples were in the training data. Why needed: Provides quantitative privacy metrics for comparing different training paradigms. Quick check: Can you explain the difference between shadow model and attack model approaches?
- **Differential Privacy (DP)**: Mathematical framework providing provable privacy guarantees through noise addition. Why needed: Serves as the gold standard baseline for privacy protection. Quick check: What is the privacy-utility tradeoff in DPSGD compared to synthetic data methods?
- **Dataset Distillation**: Training tiny synthetic datasets to capture essential information from large datasets. Why needed: Enables privacy evaluation by creating controllable synthetic data with known initialization. Quick check: How do different initialization strategies (private, noise, OOD) affect privacy leakage?

## Architecture Onboarding
- **Component Map**: Private Data -> Synthetic Data Generation -> Model Training -> Privacy Evaluation (MIA)
- **Critical Path**: Synthetic data generation directly determines privacy leakage in downstream model training
- **Design Tradeoffs**: Visual dissimilarity vs privacy protection, utility vs privacy guarantees, initialization strategy choices
- **Failure Signatures**: High TPR@0.1%FPR on canaries indicates privacy leakage; low utility indicates poor synthetic data quality
- **First Experiments**: 1) Train baseline model with mislabeled canaries and verify high TPR; 2) Generate synthetic data using private initialization and test privacy; 3) Compare OOD vs private initialization privacy leakage

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the privacy-utility tradeoff for diffusion models trained from scratch on private data, compared to fine-tuned pretrained models?
- Basis in paper: "A more interesting direction for future research would be to investigate the privacy-utility tradeoff in diffusion models that are trained from scratch."
- Why unresolved: Current experiments only evaluate fine-tuned Stable Diffusion models, which benefit from large-scale public pretraining.
- What evidence would resolve it: Train diffusion models from scratch on private datasets and evaluate privacy leakage, comparing against fine-tuned approaches.

### Open Question 2
- Question: Do stronger canaries beyond mislabeled data exist that could better capture worst-case privacy leakage?
- Basis in paper: Footnote 4 states that even stronger canaries may exist beyond mislabeled data.
- Why unresolved: Mislabeled data approximates vulnerable samples but may not represent the absolute worst case for all synthetic data training paradigms.
- What evidence would resolve it: Systematic search for canary types (outliers, rare subpopulations, adversarially crafted examples) that yield higher MIA success rates.

### Open Question 3
- Question: How can evaluation frameworks capture visual privacy leakage that is not detected by membership inference attacks?
- Basis in paper: Section 4.3 shows trajectory-matching DD methods have low MIA success rates but exhibit visual privacy leakage.
- Why unresolved: Current MIA-based evaluation metrics fail to detect cases where synthetic data visually exposes private information.
- What evidence would resolve it: Develop combined metrics incorporating visual similarity measures alongside MIA, validated against human privacy assessments.

### Open Question 4
- Question: What mechanisms enable privacy leakage from teacher to student models through visually dissimilar synthetic data in knowledge distillation?
- Basis in paper: Section 4.4 shows synthetic images with low SSIM can still trigger teacher memorization, leaking privacy to student models.
- Why unresolved: The transfer of memorization patterns through logits alignment during distillation is not fully understood.
- What evidence would resolve it: Probing experiments analyzing which layers/features encode memorization information, and whether filtering specific logits patterns can prevent privacy transfer while maintaining utility.

## Limitations
- Worst-case evaluation bias: The fixed set of 500 mislabeled audit samples may overestimate leakage for the broader dataset
- Limited dataset scope: All experiments use CIFAR-10, which may not generalize to other domains like medical imaging
- Attack specificity: Only evaluates membership inference attacks, not other privacy threat vectors like model inversion

## Confidence
- **High Confidence**: Most empirical synthetic data methods show significant privacy leakage on worst-case samples
- **Medium Confidence**: OOD-initialized distillation and text-to-image diffusion methods show better privacy-utility tradeoffs
- **Low Confidence**: Statement that no empirical method achieves better privacy-utility-efficiency tradeoffs than DPSGD

## Next Checks
1. **Cross-Dataset Validation**: Replicate core experiments on ImageNet or medical imaging datasets to test generalizability
2. **Attack Diversity Testing**: Evaluate the same synthetic data methods against attribute inference and model inversion attacks
3. **Population-Level Privacy Analysis**: Supplement worst-case canary analysis with population-level privacy metrics (average TPR across all samples)