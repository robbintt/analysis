---
ver: rpa2
title: 'Projected Compression: Trainable Projection for Efficient Transformer Compression'
arxiv_id: '2506.22255'
source_url: https://arxiv.org/abs/2506.22255
tags:
- compression
- pruning
- weights
- base
- projection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Projected Compression (PC) is a model compression method that uses
  trainable projection modules to reduce transformer model size while retaining access
  to all original parameters. Unlike hard pruning, PC keeps frozen base weights and
  learns to project them into a lower-dimensional space through optimized projection
  matrices.
---

# Projected Compression: Trainable Projection for Efficient Transformer Compression

## Quick Facts
- arXiv ID: 2506.22255
- Source URL: https://arxiv.org/abs/2506.22255
- Reference count: 40
- Trains a small projection module to compress frozen transformer weights into a lower-dimensional space while preserving access to all original parameters

## Executive Summary
Projected Compression (PC) is a model compression method that uses trainable projection modules to reduce transformer model size while retaining access to all original parameters. Unlike hard pruning, PC keeps frozen base weights and learns to project them into a lower-dimensional space through optimized projection matrices. This approach enables the model to gradually recover useful information during training rather than permanently discarding it. Experiments show PC outperforms hard pruning and retraining, especially on higher-quality models trained on more tokens.

## Method Summary
PC freezes transformer base weights W and adds trainable projection matrices P₁ and P₂ that downsample input and output dimensions respectively, producing compressed weights W_C = P₁WP₂. During training, gradients optimize only P₁ and P₂, which "blend" influence from all frozen parameters into the active computation. The method can compress feedforward hidden sizes and embedding dimensions, with optional residual weights for late-stage optimization flexibility. Training uses C4 dataset on GPT-2 style models, comparing cross-entropy loss against Hard Pruning with Retraining baseline at 35%, 50%, 65% compression.

## Key Results
- PC outperforms hard pruning with retraining, particularly on higher-quality base models trained on more tokens
- Performance advantage increases with both base model size and training cost
- Consistent improvements across compression levels (35%, 50%, 65%)
- Computational efficiency matches hard pruning when using large batch sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projected Compression preserves representational capacity by maintaining access to frozen base weights while learning to project them into a lower-dimensional active subspace.
- Mechanism: The base weight matrix W remains frozen in memory. Trainable projection matrices P₁ and P₂ learn to downsample input and output dimensions respectively, producing compressed weights W_C = P₁WP₂. During training, gradients optimize only P₁ and P₂, which "blend" influence from all frozen parameters into the active computation.
- Core assumption: Useful information is distributed across the full parameter space rather than concentrated in a subset that can be identified a priori.
- Evidence anchors:
  - [abstract] "preserve access to all the original model parameters"
  - [section 3.2] "less important dimensions are excluded from the initial projection but remain accessible"
  - [corpus] Weak direct evidence; related work on low-rank adaptation (LoRA variants like IPA, LARGO) operates similarly by freezing base weights, but targets adaptation rather than compression.

### Mechanism 2
- Claim: Importance-aware initialization combined with gradient-optimized projections enables gradual recovery of dimensions that would be permanently discarded by hard pruning.
- Mechanism: Projection matrices are initialized using the same importance scores as hard pruning (e.g., magnitude-based criteria), ensuring initially important weights dominate. During training, gradient updates to P₁ and P₂ can progressively reincorporate information from initially suppressed dimensions if they prove useful for the loss.
- Core assumption: The optimization landscape permits recovery of useful dimensions through projection learning; importance scores are imperfect heuristics that benefit from refinement.
- Evidence anchors:
  - [abstract] "enables the model to gradually recover useful information during training rather than permanently discarding it"
  - [section 3.3] "less important dimensions are initially suppressed but remain accessible"
  - [corpus] Table 3 shows magnitude-based initialization underperforms random initialization for both methods, suggesting importance heuristics are imperfect—corpus lacks direct comparison of recovery dynamics.

### Mechanism 3
- Claim: PC achieves training computational efficiency comparable to hard pruning by accumulating gradients in the compressed weight space rather than the full parameter space.
- Mechanism: Forward pass computes xW_C where W_C is the projected (smaller) matrix. Gradients accumulate across batch and sequence dimensions in this reduced space. The backward pass updates only P matrices using these reduced gradients, making overhead negligible for large batches typical in LLM training.
- Core assumption: Batch sizes are sufficiently large that per-sample projection computation amortizes effectively; memory overhead (storing frozen W + optimizer states for P) is acceptable.
- Evidence anchors:
  - [abstract] "matches the base model's per-token computation step in FLOPs"
  - [section 3.4] "overall cost per optimization step is equivalent to that of retraining a transformer model obtained through hard pruning"
  - [corpus] No direct corpus evidence on gradient accumulation efficiency in projection-based compression.

## Foundational Learning

- Concept: **Structured pruning vs. low-rank decomposition**
  - Why needed here: PC occupies a middle ground—unlike structured pruning that removes neurons/heads, PC uses low-rank projections to compress while retaining full parameter access. Understanding both paradigms clarifies what PC gains.
  - Quick check question: Can you explain why removing an attention head provides inference speedup but low-rank projection of a weight matrix may not, unless explicitly structured?

- Concept: **Gradient flow through frozen parameters**
  - Why needed here: PC freezes W but gradients must reach P₁ and P₂. Understanding how automatic differentiation handles this (gradients flow through W_C computation) is essential for implementation and debugging.
  - Quick check question: If W is frozen, how do gradients from the loss reach P₁ and P₂ during backpropagation?

- Concept: **Token-to-parameter ratio as a quality signal**
  - Why needed here: The paper emphasizes that PC advantages emerge with higher-quality base models (80:1 ratio) and more training tokens. This connects to broader scaling literature on when compression is most valuable.
  - Quick check question: Why might a model trained on 80 tokens per parameter respond better to PC than one trained on 20 tokens per parameter?

## Architecture Onboarding

- Component map:
  - Base weights (W) -> Projection matrices (P₁, P₂) -> Compressed weights (W_C) -> Forward pass computation -> Gradients -> P₁, P₂ updates

- Critical path:
  1. Compute importance scores for all target weight matrices in base model.
  2. Initialize P₁, P₂ to project high-importance dimensions into compressed space.
  3. For each training step: recompute W_C = P₁WP₂, run forward pass, accumulate gradients in compressed space, update only P₁, P₂ (and W_r if used).
  4. At deployment: merge projections into final W_C, discard frozen W and projection matrices.

- Design tradeoffs:
  - Memory vs. compute: PC requires storing frozen W (no gradients) + P matrices (with optimizer states). Higher memory than hard pruning, but similar compute per step.
  - Compression axis: Can compress input dimension only (P₁), output dimension only (P₂), or both. Paper targets feedforward hidden size and embedding dimension—attention head removal not directly addressed.
  - Residual inclusion: W_r adds flexibility but increases trainable parameter count; may be unnecessary for shorter training runs.

- Failure signatures:
  - Early-training divergence: If projection initialization is poor (e.g., random vs. importance-based without calibration), initial W_C may be far from useful configurations. Monitor initial loss spike.
  - No convergence advantage over hard pruning: If base model is undertrained (low token-to-parameter ratio) or training budget is minimal, PC's recovery mechanism cannot activate. Check Figure 2: 20:1 ratio shows PC ≈ HPR.
  - Memory overflow: Frozen W + optimizer states for P may exceed GPU memory. Use gradient checkpointing or CPU offloading as noted in Section 3.4.

- First 3 experiments:
  1. Sanity check on small model: Apply PC to a 300M model with 50% compression, training on 1B tokens. Compare cross-entropy loss trajectory against hard pruning baseline. Verify that PC matches or slightly underperforms HPR early but catches up or surpasses with more tokens.
  2. Ablation on initialization: Compare magnitude-based vs. random projection initialization (replicate Table 3). Confirm that magnitude-based is not strictly better—random may work as well or better depending on model.
  3. Scaling test: Run PC on 800M model at 50% compression with 5B+ training tokens and 80:1 base model ratio. This is where paper shows clearest PC > HPR advantage. Measure loss gap at multiple token checkpoints to verify scaling trend.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Projected Compression be effectively adapted to architectures beyond standard Transformer-based language models?
- Basis in paper: [explicit] The conclusion states, "This work opens the possibility of adapting PC method to a broader range of architectures, which we leave for future exploration."
- Why unresolved: The current study restricts experiments to GPT-2 style models, leaving the method's efficacy on other domains (e.g., computer vision) or architectures (e.g., RNNs) unverified.
- What evidence would resolve it: Successful application and benchmarking of PC on non-Transformer architectures or multi-modal models.

### Open Question 2
- Question: Does the performance advantage of Projected Compression over hard pruning persist at significantly larger parameter scales (e.g., 7B+)?
- Basis in paper: [inferred] The paper demonstrates that the performance margin "scales well" with model size and training tokens (Figure 2), but experimental validation stops at 800M parameters.
- Why unresolved: It is unclear if the observed scaling trends continue linearly or if memory overhead and optimization difficulties emerge at billion-parameter scales.
- What evidence would resolve it: Comparative loss scaling curves for multi-billion parameter models compressed using both PC and hard pruning.

### Open Question 3
- Question: What are the practical memory limits and throughput impacts of maintaining frozen base weights during training on hardware with limited VRAM?
- Basis in paper: [inferred] Section 3.4 notes the method incurs "additional memory usage" to store frozen weights and optimizer states, suggesting this overhead is a potential constraint.
- Why unresolved: While the paper suggests offloading to CPU as a mitigation, it does not quantify the impact on training throughput or the feasibility on smaller GPU clusters.
- What evidence would resolve it: A system-level analysis of peak memory usage and tokens-per-second training speed relative to model size.

## Limitations

- Narrow compression target scope: Only tested on feedforward hidden dimensions and embedding projections, not attention heads or layers
- Base model quality dependency: Performance advantage heavily depends on sufficient training quality in base model (80:1 ratio vs 20:1)
- Memory overhead: Requires storing frozen base weights plus projection matrices with optimizer states

## Confidence

- High confidence: Basic PC mechanism (freezing W, learning P₁/P₂ projections) is sound and well-defined; training efficiency claim (matching hard pruning FLOPs) is mathematically verifiable
- Medium confidence: Claim that PC enables gradual recovery of useful dimensions is supported by loss trajectories but lacks direct mechanistic evidence; initialization comparison shows magnitude isn't strictly better but paper doesn't explore why
- Low confidence: Scaling claim that PC advantages increase with base model size is based on two model sizes (300M and 800M) without testing intermediate points or verifying if trend continues at larger scales

## Next Checks

1. **Recovery dynamics validation**: Instrument PC training to track which base weight dimensions contribute most to the compressed weights over training steps. This would provide direct evidence of the gradual recovery mechanism by showing when and how initially suppressed dimensions become active contributors.

2. **Base model quality threshold**: Systematically vary the training tokens for base models (e.g., 20:1, 40:1, 60:1, 80:1) while keeping architecture constant, then apply PC to each. This would isolate the effect of base model quality on PC performance and identify the minimum threshold for PC's advantage.

3. **Compression target ablation**: Apply PC to attention projection matrices and test head-level compression in addition to feedforward and embedding projections. This would validate whether PC's advantages generalize beyond the tested compression axes and reveal if certain transformer components benefit more from the projection approach.