---
ver: rpa2
title: Learning to Guide Local Search for MPE Inference in Probabilistic Graphical
  Models
arxiv_id: '2602.01475'
source_url: https://arxiv.org/abs/2602.01475
tags:
- search
- beacon
- step
- budget
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural network-based method to improve stochastic
  local search for MPE inference in PGMs. It addresses the myopia of best-improvement
  heuristics by learning to score local moves based on their predicted ability to
  reduce Hamming distance to high-quality solutions.
---

# Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models

## Quick Facts
- arXiv ID: 2602.01475
- Source URL: https://arxiv.org/abs/2602.01475
- Authors: Brij Malhotra; Shivvrat Arya; Tahrima Rahman; Vibhav Giridhar Gogate
- Reference count: 40
- Primary result: Neural network guidance improves stochastic local search for MPE inference, consistently outperforming standard greedy search and GLS+ on high-treewidth PGMs with win percentages often exceeding 60% and log-likelihood improvements up to 80%.

## Executive Summary
This paper addresses the myopia of standard best-improvement heuristics in stochastic local search (SLS) for Most Probable Explanation (MPE) inference in probabilistic graphical models (PGMs). The authors propose a neural network-based method that learns to score local moves not by immediate log-likelihood gain, but by their predicted ability to reduce the Hamming distance to high-quality reference solutions. This approach balances short-term likelihood gains with long-term progress toward optimal assignments. Experiments on 25 high-treewidth PGMs show consistent improvements, with BEACON-Greedy and BEACON-GLS+ frequently winning over 60% of comparisons against standard methods, particularly in early search stages.

## Method Summary
The method trains an attention-based neural network to predict whether a local move reduces the Hamming distance to a reference solution generated by an approximate solver. During inference, this learned score is combined with the log-likelihood gradient in a hybrid objective to guide neighbor selection in SLS. The approach amortizes lookahead by learning a proxy for long-term progress rather than relying solely on immediate objective improvement. Theoretical analysis shows convergence is guaranteed if the learned policy selects distance-reducing moves with sufficient probability.

## Key Results
- BEACON-Greedy wins over standard Greedy in 80% of comparisons, with win percentages ranging from 65% to 95% across datasets
- BEACON-GLS+ wins over GLS+ in 64% of comparisons, with improvements from 55% to 85%
- Log-likelihood improvements of 1% to over 80% compared to baselines, especially in early search stages (first 1000-2000 steps)
- The learned guidance is most effective during initial exploration when the search is far from local optima

## Why This Works (Mechanism)

### Mechanism 1: Amortized Lookahead via Distance-Reducing Proxies
Standard local search selects neighbors maximizing immediate log-likelihood, which is myopic. This paper trains a neural network to estimate the probability that a move reduces the Hamming distance to a near-optimal reference solution. By selecting neighbors based on this distance-reduction signal, the search prioritizes moves that make structural progress toward high-quality regions, effectively acting as a multi-step lookahead. Theoretical convergence relies on the assumption that the learned policy selects a distance-reducing move with probability α > 0.5.

### Mechanism 2: Structure-Aware Attention for Contextual Move Scoring
The attention mechanism captures dependencies between the global evidence state and local variable flips. By embedding the current assignment and candidate flips, multi-head attention computes a contextualized embedding for each candidate move based on the entire state. This allows the model to learn that flipping variable X_i is promising given the current values of its neighbors or specific evidence patterns, a dependency missed by 1-flip local heuristics.

### Mechanism 3: Hybrid Objective Regularization
The final score combines neural guidance and log-likelihood gain: S_final = (1-λ)S_LL + λS_NN. This interpolation ensures the search rarely moves to extremely low probability states while encouraging exploration. Neither signal is sufficient alone - S_LL anchors the search in objective-improving directions while S_NN encourages long-term progress.

## Foundational Learning

- **Concept: Most Probable Explanation (MPE) & Inference**
  - Why needed: The entire paper optimizes maximizing the probability assignment of unobserved variables given evidence
  - Quick check: Why does exact MPE inference scale exponentially with treewidth?

- **Concept: Stochastic Local Search (SLS) & 1-flip Neighborhoods**
  - Why needed: The paper modifies neighbor selection within SLS; you must understand the "best-improvement" baseline
  - Quick check: In a 1-flip neighborhood, how many neighbors does a state have for N binary variables?

- **Concept: Hamming Distance as a Supervisory Signal**
  - Why needed: The paper treats optimization as a learning problem where the label is binary: "did this move get us closer to the reference solution?"
  - Quick check: Why is Hamming distance a "surrogate" for likelihood improvement in this context?

## Architecture Onboarding

- **Component map:** Data Generator (Anytime Solver) -> Labeler (Hamming Distance) -> Trainer (Attention Network) -> Inference Engine (Local Search)
- **Critical path:** The Data Generation phase. If the teacher solver (budget B=300s) produces low-quality reference solutions, the neural network learns to mimic suboptimal trajectories.
- **Design tradeoffs:**
  - Teacher Budget (B): Higher B = better labels but slower offline training
  - Lambda (λ): Balances exploration vs. exploitation; paper suggests tuning on validation set
  - Trajectory Sampling: Training on states visited during search (rather than random states) is crucial for distribution matching
- **Failure signatures:**
  - Distribution Shift: Performance degrades on graphs with random or unstructured connectivity
  - Overhead: Scoring all neighbors with a neural net at every step adds latency (~2x slowdown per step per Table 4)
- **First 3 experiments:**
  1. Run standard Greedy vs. BEACON-Greedy on a small Ising grid (e.g., Grid20) to verify log-likelihood improves over 4000 steps
  2. Sweep λ ∈ [0.0, 1.0] to find failure modes (pure neural vs. pure greedy) on a validation set
  3. Vary teacher solver budget B (e.g., 10s vs 300s) during data generation to measure degradation of learned guidance signal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-step lookahead architectures effectively capture long-horizon gains that the current one-step Hamming-based guidance misses?
- Basis: The authors identify "multi-step lookahead architectures" as future work since current one-step guidance may miss longer-horizon gains
- Why unresolved: Current method predicts immediate distance reduction, which may not align with optimal trajectory over many steps
- What evidence would resolve it: Comparing convergence speed and solution quality of n-step prediction models against current one-step model on high-treewidth benchmarks

### Open Question 2
- Question: Would adaptive weighting mechanisms outperform the static convex combination parameter λ?
- Basis: Paper notes performance sensitivity to λ choice and lists "adaptive weighting mechanisms" as specific avenue for future work
- Why unresolved: Static weighting requires validation set tuning and may not be optimal for all stages or instance types
- What evidence would resolve it: Developing method to dynamically adjust λ based on search progress and demonstrating superior performance without manual tuning

### Open Question 3
- Question: How can supervision be modified to reduce the bias introduced by using approximate solvers for training data?
- Basis: Authors identify "richer and less biased supervision sources" as necessary inquiry since training relies on approximate solvers which may introduce bias
- Why unresolved: Ground truth labels depend on solvers that might provide suboptimal reference assignments, potentially capping neural network accuracy
- What evidence would resolve it: Quantifying performance gap when training on exact solutions (for tractable instances) versus approximate solutions, or demonstrating method that corrects for teacher bias

## Limitations
- Teacher Quality Dependence: Method's effectiveness is fundamentally limited by quality of reference solutions generated by approximate solver during training
- Distribution Shift Risk: Performance on unseen graph topologies or different evidence distributions remains unverified
- Computational Overhead: Neural network evaluation adds latency (~2x slower per step), though net speedup versus standard GLS+ is not explicitly quantified

## Confidence
- High Confidence: Core architectural innovation (attention-based contextual scoring + hybrid objective) is sound and well-justified
- Medium Confidence: Theoretical convergence guarantee depends critically on assumption that learned policy selects distance-reducing moves with probability α > 0.5
- Medium Confidence: Claim of balancing short-term gains with long-term trajectory is supported by ablation studies but relies on quality of distance-reduction proxy

## Next Checks
1. Systematically vary the anytime solver budget (B) during data generation and measure degradation in BEACON's final solution quality to quantify labeling bottleneck
2. Evaluate BEACON on PGM instances with graph topologies not present in training corpus to assess robustness to distribution shift
3. Plot cumulative log-likelihood vs. wall-clock time (not just steps) for BEACON versus standard GLS+ to determine if reduced step count translates to net runtime improvement