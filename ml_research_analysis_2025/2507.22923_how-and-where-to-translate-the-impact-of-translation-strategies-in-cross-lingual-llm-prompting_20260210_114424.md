---
ver: rpa2
title: How and Where to Translate? The Impact of Translation Strategies in Cross-lingual
  LLM Prompting
arxiv_id: '2507.22923'
source_url: https://arxiv.org/abs/2507.22923
tags:
- language
- prompt
- english
- arxiv
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates prompt translation strategies
  for cross-lingual LLM prompting in multilingual RAG systems. The research investigates
  how pre-translating different prompt components (identity statements, rules, candidates)
  affects classification performance across Hindi and French languages.
---

# How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting

## Quick Facts
- arXiv ID: 2507.22923
- Source URL: https://arxiv.org/abs/2507.22923
- Reference count: 36
- Key outcome: Translation strategies in cross-lingual LLM prompting show model- and language-dependent effects, with BLOOMZ variants improving from translation while Llama-3.1-8B degrades.

## Executive Summary
This study systematically evaluates prompt translation strategies for cross-lingual LLM prompting in multilingual RAG systems. The research investigates how pre-translating different prompt components (identity statements, rules, candidates) affects classification performance across Hindi and French languages. Experiments with five diverse LLMs show that optimal translation strategies depend on both language resource availability and model architecture. While models like BLOOMZ variants show consistent improvements with translated prompts, others like Llama-3.1-8B perform better with English-only prompts. For low-resource languages like Hindi, English generation is generally preferred, while high-resource languages like French may benefit from source language generation depending on the model. Translating identity and rules to the source language often yields performance improvements.

## Method Summary
The paper evaluates six translation strategies (B/C/I/R/I&R/A) across five multilingual LLMs (Llama-3.1-8B, Qwen2.5-7B-Instruct, BLOOMZ-7b1, Mistral-Nemo-Instruct-2407, BLOOMZ-7b1-mt) using 6,000 Hindi and 6,000 French utterances across 80 intent classes. The task is cross-lingual intent classification where an LLM re-ranks retrieved candidates. Translation is performed via LLM with manual quality inspection. Models are evaluated using temperature=0.1, top-p=0.7, max_tokens=512 on 4 NVIDIA L4 GPUs, with accuracy measured as correct intent verbatim in response.

## Key Results
- BLOOMZ-7b1 and BLOOMZ-7b1-mt show consistent improvements with translated prompts, while Llama-3.1-8B and Mistral-Nemo-Instruct-2407 degrade performance when any component is translated.
- Translating identity and rules to source language (I&R strategy) yields +5.4% to +6.7% improvement for Qwen2.5-7B-Instruct in Hindi and +7.0% for BLOOMZ-7b1 in French.
- Low-resource languages like Hindi benefit from English generation across all models, while high-resource French may benefit from source-language generation depending on model architecture.
- The optimal translation strategy depends on three factors: language resource level, model's multilingual training profile, and whether source-language output is required.

## Why This Works (Mechanism)

### Mechanism 1
Translating identity statements and task rules to the source language while keeping candidates in English improves classification accuracy for many multilingual LLMs. Source-language task framing activates relevant cultural and linguistic context in the model's representation space, improving comprehension of user intent. Meanwhile, keeping candidates in English leverages the model's stronger English generation capabilities, avoiding degradation from lower-quality non-English token distributions. This assumes the model has sufficient cross-lingual alignment in its representations to map source-language understanding to English token selection. The evidence shows Qwen2.5-7B-Instruct and BLOOMZ-7b1 achieve +5.4% to +7.0% improvements with I&R strategy. Break condition: if the model lacks cross-lingual alignment (e.g., Llama-3.1-8B), source-language framing introduces noise without benefiting comprehension, degrading performance.

### Mechanism 2
Low-resource languages benefit more from English generation than source-language generation across all tested models. Training data imbalance means English token sequences have lower perplexity and more reliable copy behavior. For low-resource languages like Hindi, the model's generation capacity is underdeveloped, causing verbatim copying from candidates to fail more frequently. This assumes the task allows English output. The evidence shows all models struggled with generating Hindi text, experiencing performance decline. For low-resource languages lexically dissimilar to English, it's always better to have generation in English unless the task requires Hindi text. Break condition: when the application requires source-language output (e.g., user-facing responses), this strategy is not viable.

### Mechanism 3
Models explicitly trained with multilingual objectives (e.g., BLOOMZ variants) show consistent performance gains from prompt translation, while English-dominant models (e.g., Llama-3.1-8B) degrade. Multilingual fine-tuning creates stronger cross-lingual representation alignment, allowing translated prompts to activate relevant knowledge without introducing semantic drift. English-dominant models treat non-English tokens as out-of-distribution, increasing uncertainty. The evidence shows BLOOMZ-based models show decent improvements while Llama-3.1-8B experiences performance decreases when any component is translated. Break condition: multilingual-trained models may still fail if translation quality is poor or if the specific language pair was underrepresented during training.

## Foundational Learning

- **Cross-lingual Prompting vs. Pre-translation**: The paper compares two strategies—passing mixed-language prompts to multilingual models versus translating prompts to a single language before inference. Quick check: Given a French user query and English candidate labels, should you translate candidates to French, translate the query to English, or keep them mixed?
- **Retrieval-Augmented Generation (RAG) for Classification**: The task structure (retriever generates candidates, LLM re-ranks) is the experimental context. Understanding this pipeline clarifies why candidates might be in a different language than user utterances. Quick check: In a multilingual RAG system, if the knowledge base is in English, what language will retrieved candidates be in for a Hindi user query?
- **Language Resource Levels and Tokenizer Effects**: Hindi (low-resource) vs. French (high-resource) comparison is central to the paper's findings. Tokenizer vocabulary coverage affects both comprehension and generation quality. Quick check: Why might a tokenizer trained primarily on English produce degraded generation for Hindi compared to French?

## Architecture Onboarding

- **Component map**: User Utterance (source language) → Multilingual Retriever → English KB → English Candidates → Prompt Constructor (applies translation strategy) → LLM Re-ranker → Selected Intent
- **Critical path**: Determine (1) source language resource level, (2) inference model's multilingual training profile, (3) whether source-language output is required. These three inputs determine optimal translation strategy.
- **Design tradeoffs**: Pre-translation adds offline compute but enables English-only models to serve multilingual users. Translating candidates enables source-language output but risks generation errors for low-resource languages. Translating identity/rules improves comprehension but may confuse models with weak cross-lingual alignment.
- **Failure signatures**: Llama-3.1-8B with any translated components shows consistent performance drops (6-32% depending on configuration). Any model with full translation (A strategy) for Hindi shows 1-33% degradation. BLOOMZ models with English-only baseline for French leave performance gains unrealized.
- **First 3 experiments**: 1) Baseline audit: Run B strategy (all English) on your target language with your inference model to establish baseline accuracy. 2) I&R test: Translate identity and rules only; compare accuracy. If positive, this hybrid strategy is likely optimal. 3) Language resource check: If target is low-resource, stop—keep generation in English. If high-resource, test C strategy (translated candidates) to assess source-language generation viability.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset remains inaccessible (anonymized and not publicly released), preventing independent verification of experimental conditions.
- Translation quality verification process described as "manual inspection" without specifying criteria or inter-rater reliability.
- Experiments focus exclusively on intent classification tasks, limiting generalizability to other cross-lingual applications.

## Confidence

- **High Confidence**: The general finding that translation strategy effectiveness varies by model architecture and language resource level is well-supported by systematic experimentation across five diverse models and two languages.
- **Medium Confidence**: The specific performance numbers (e.g., +5.4% to +6.7% improvements) are methodologically sound but may not generalize beyond the specific dataset and task.
- **Low Confidence**: The claim that Hindi specifically requires English generation while French may benefit from source-language generation is based on limited language pairs and could reflect dataset-specific patterns.

## Next Checks

1. **Replication with Public Dataset**: Reproduce the core experiments using a publicly available multilingual intent classification dataset (e.g., MASSIVE) to verify whether the observed model-specific translation sensitivities persist across different data distributions.
2. **Translation Quality Impact**: Systematically vary translation quality (using different translation models, with/without post-editing) to determine whether the observed effects are primarily driven by translation accuracy versus model-specific cross-lingual capabilities.
3. **Task Generalization**: Extend the experimental framework to a different cross-lingual task (e.g., sentiment analysis or summarization) to assess whether the model- and language-dependent translation strategy patterns generalize beyond intent classification.