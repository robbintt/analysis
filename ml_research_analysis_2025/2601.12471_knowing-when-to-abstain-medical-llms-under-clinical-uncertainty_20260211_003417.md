---
ver: rpa2
title: 'Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty'
arxiv_id: '2601.12471'
source_url: https://arxiv.org/abs/2601.12471
tags:
- abstention
- size
- accuracy
- rate
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedAbstain, a benchmark and evaluation protocol
  for assessing how well medical large language models recognize uncertainty and abstain
  from answering when unsure. The authors propose a framework that combines conformal
  prediction, adversarial perturbations, and explicit abstention options to evaluate
  uncertainty-aware behavior in medical multiple-choice QA.
---

# Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty

## Quick Facts
- **arXiv ID**: 2601.12471
- **Source URL**: https://arxiv.org/abs/2601.12471
- **Reference count**: 40
- **Primary result**: Explicit abstention options improve uncertainty-aware abstention more than prompting or scaling

## Executive Summary
This paper introduces MedAbstain, a benchmark and evaluation protocol for assessing how well medical large language models recognize uncertainty and abstain from answering when unsure. The authors propose a framework that combines conformal prediction, adversarial perturbations, and explicit abstention options to evaluate uncertainty-aware behavior in medical multiple-choice QA. Across experiments with both open- and closed-source models, they find that simply providing an explicit abstention option consistently increases model uncertainty and safer abstention, far more than perturbing questions or using chain-of-thought or few-shot prompting. Larger or more advanced models do not necessarily improve abstention performance. These findings highlight the importance of abstention mechanisms for trustworthy LLM deployment in high-stakes domains like healthcare.

## Method Summary
The MedAbstain framework evaluates medical LLMs using four dataset variants: original questions (NA), questions with abstention option (A), perturbed questions without abstention (NAP), and perturbed questions with abstention (AP). Perturbations remove critical context using GPT-4.1-mini. The evaluation uses zero-shot, few-shot (k=4 dynamic examples), and chain-of-thought prompting across open-source models (Llama-3.1-8B, Qwen2.5-7B, etc.) and closed-source models (GPT-4o, GPT-4.1). Conformal prediction with 90% coverage (α=0.1) computes prediction sets using LAC and APS scoring functions on a 30% calibration split. Key metrics include accuracy, prediction set size, and abstention rate, with negative correlation between accuracy and set size indicating valid uncertainty signaling.

## Key Results
- Explicit abstention options consistently increase prediction set sizes and abstention rates across all models tested
- Conformal prediction set sizes correlate negatively with accuracy, validating uncertainty signals
- Chain-of-thought prompting and few-shot examples show negligible improvements in abstention behavior compared to abstention option
- Model scaling (8B→32B parameters) and advanced prompting bring little improvement in uncertainty-aware abstention
- GPT-4.1 exhibits counterintuitive patterns where accuracy and set size both increase, indicating potential calibration issues

## Why This Works (Mechanism)

### Mechanism 1: Explicit Abstention Option Recalibrates Model Uncertainty
Adding an abstention token redistributes probability mass across plausible answers and the abstention option, captured as larger conformal prediction sets. This creates a legitimate uncertainty outlet in the output space.

### Mechanism 2: Conformal Prediction Set Size Signals Abstention Appropriateness
CP computes prediction sets with statistical coverage guarantees; larger sets indicate higher uncertainty, which should trigger abstention in high-stakes settings.

### Mechanism 3: Prompting and Scaling Have Limited Effect on Abstention Behavior
CoT and few-shot primarily improve reasoning paths toward answers, not meta-cognitive awareness of uncertainty; scaling increases accuracy but doesn't inherently improve self-knowledge of limitations.

## Foundational Learning

- **Concept: Conformal Prediction**
  - Why needed here: Provides mathematical framework for converting logits into statistically valid prediction sets with coverage guarantees
  - Quick check question: Can you explain why CP requires a separate calibration split and what happens if the calibration distribution differs from test?

- **Concept: Selective Prediction / Abstention**
  - Why needed here: Understanding that abstention is a rejection option that trades off coverage for safety
  - Quick check question: What is the difference between accuracy and selective accuracy when an abstention option is available?

- **Concept: Uncertainty Calibration in LLMs**
  - Why needed here: Raw log-probabilities from LLMs are often miscalibrated
  - Quick check question: Why might a model with high accuracy still have poorly calibrated uncertainty estimates?

## Architecture Onboarding

- **Component map**: Dataset → Variant Generator (NA/A/NAP/AP) → LLM Prompt → Log-probs Extractor → Calibration Split → Conformal Predictor (LAC/APS) → CP Scores → Quantile Threshold → Test Set Prediction Sets → Metrics (Accuracy, Set Size, Abstention Rate)

- **Critical path**: The abstention option must be tokenized as a single, extractable token; log-prob extraction must be exact; calibration must be stratified by difficulty/distribution

- **Design tradeoffs**: α=0.1 (90% coverage) is conservative; lower α increases abstention but may trigger too often; 30/70 calibration/test split balances statistical validity with test power

- **Failure signatures**: Zero abstention rate despite abstention option; maximum set sizes (all options included); inverse correlation between set size and accuracy

- **First 3 experiments**: 1) Baseline zero-shot on NA and A variants with small model to verify abstention option affects set sizes; 2) Run calibration sensitivity: test α∈{0.05, 0.1, 0.2} on held-out validation; 3) Compare GPT-4o-mini vs GPT-4.1 on same splits to assess closed-source calibration differences

## Open Questions the Paper Calls Out

- Does the MedAbstain evaluation framework generalize effectively to multilingual or non-English medical contexts?
- Can the uncertainty quantification and abstention protocols be adapted for generative, free-form, or multi-modal medical tasks?
- Why do specific closed-source models, such as GPT-4.1, exhibit an inverse correlation between accuracy and uncertainty when an abstention option is introduced?
- How do retrieval-augmented generation (RAG) or test-time scaling methods influence abstention rates and uncertainty calibration?

## Limitations

- Dataset representativeness: MedQA and AMBOSS may not capture full spectrum of clinical uncertainty; perturbation method may not simulate realistic clinical uncertainty
- Calibration set size: Only 30% allocated to calibration may compromise statistical validity of conformal prediction sets
- Closed-source model limitations: API log-probabilities may be post-processed; GPT-4.1 shows calibration artifacts undermining universal applicability

## Confidence

- **High Confidence**: Explicit abstention options increase prediction set sizes and abstention rates consistently
- **Medium Confidence**: Scaling model size and advanced prompting have negligible effects on abstention behavior
- **Low Confidence**: Generalizability of perturbation-based uncertainty assessment to real clinical scenarios

## Next Checks

1. Cross-Dataset Validation: Replicate main findings on additional medical QA datasets with different characteristics
2. Calibration Set Sensitivity Analysis: Systematically vary calibration set size and examine threshold stability
3. Real-World Clinical Uncertainty Simulation: Design validation set where uncertainty arises from conflicting evidence or ambiguous symptoms rather than simple context removal