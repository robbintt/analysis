---
ver: rpa2
title: An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage
  Classification
arxiv_id: '2511.19367'
source_url: https://arxiv.org/abs/2511.19367
tags:
- tumor
- lung
- segmentation
- cancer
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurately classifying lung
  cancer tumor stages (T-staging) using medical imaging, which is critical for prognosis
  and treatment planning. Traditional deep learning approaches often overlook essential
  spatial and anatomical information, limiting their clinical applicability.
---

# An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification

## Quick Facts
- arXiv ID: 2511.19367
- Source URL: https://arxiv.org/abs/2511.19367
- Reference count: 40
- Primary result: 91.36% overall accuracy for T-stage classification (F1: T1=0.93, T2=0.89, T3=0.96, T4=0.90)

## Executive Summary
This study addresses the challenge of accurately classifying lung cancer tumor stages (T-staging) using medical imaging, which is critical for prognosis and treatment planning. Traditional deep learning approaches often overlook essential spatial and anatomical information, limiting their clinical applicability. To overcome this, the authors propose a hybrid framework that combines specialized encoder-decoder networks for anatomical segmentation with rule-based classification aligned with medical guidelines. The method explicitly measures tumor size and its proximity to key anatomical structures such as the mediastinum and diaphragm, enabling transparent and clinically grounded staging decisions. Evaluated on the Lung-PET-CT-Dx dataset, the framework achieves an overall accuracy of 91.36%, with per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4). The approach outperforms traditional CNN-based classifiers, offering both state-of-the-art performance and interpretability, addressing a critical gap in current T-staging methodologies.

## Method Summary
The framework uses three specialized encoder-decoder networks for anatomical segmentation: LungNet (DenseNet-121 + U-Net decoder), MediNet (DenseNet-121 + UNet++ decoder), and TumorNet (ResNet-152 + FPN decoder). YOLOv11 detects tumors for ROI cropping. CT images undergo lung window preprocessing (W=1400, C=-700 HU), resize to 256×256, and CLAHE enhancement. After segmentation, contours are extracted and maximum Euclidean distances between tumor and anatomical structures are computed. A rule-based classifier applies IASLC T-stage criteria: T1 (≤3cm, lung-encased), T2 (>3-5cm), T3 (>5-7cm), T4 (>7cm or invasion). The diaphragm is estimated using the lowest 10% of lung mask points due to lack of dedicated segmentation data.

## Key Results
- Overall T-stage classification accuracy of 91.36% on Lung-PET-CT-Dx dataset
- Per-stage F1-scores: T1=0.93, T2=0.89, T3=0.96, T4=0.90
- Segmentation performance: LungNet DSC=97.82%, MediNet DSC=93.39%, TumorNet DSC=89.68%
- Outperforms traditional CNN approaches (37-40% accuracy) by explicitly measuring tumor properties

## Why This Works (Mechanism)

### Mechanism 1
Decomposing staging into segmentation + measurement + rules improves accuracy over end-to-end classification. The framework extracts explicit tumor properties (size, distances to mediastinum/diaphragm/lung walls) from segmentation masks, then applies IASLC-aligned rule-based classification. This forces the model to learn clinically relevant intermediate representations rather than correlating image features directly with stage labels. Core assumption: Segmentation quality is sufficiently high that extracted measurements reflect true anatomical relationships. Evidence anchors: [abstract] "performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task"; [Table 7] Traditional CNN approaches achieve 37-40% accuracy vs. 91.36% for the proposed hybrid approach; [corpus] Weak direct evidence; neighbor papers focus on segmentation but not the hybrid measurement-classification comparison. Break condition: If Dice scores fall below ~0.80 for any anatomical structure, distance measurements become unreliable and staging accuracy degrades non-linearly.

### Mechanism 2
Task-specialized encoder-decoder architectures outperform unified models for multi-organ segmentation. LungNet uses DenseNet121+UNet decoder, MediNet uses DenseNet121+UNet++, TumorNet uses ResNet152+FPN. Each combination is selected for its respective task characteristics—DenseNet's feature reuse benefits lung/mediastinum boundaries, while ResNet's depth and FPN's multi-scale reconstruction capture tumor details from cropped ROIs. Core assumption: The optimal architecture differs across anatomical structures due to varying boundary sharpness, size, and contrast. Evidence anchors: [Section 3.4.1-3.4.3] Explicit architectural choices with rationale for each network; [Table 4] DSC scores: LungNet 97.82%, MediNet 93.39%, TumorNet 89.68%—suggesting task-appropriate but not equal difficulty; [corpus] Neighbor paper "Uncertainty-Guided Coarse-to-Fine Tumor Segmentation" similarly uses multi-stage segmentation, supporting the decomposition approach. Break condition: If cropped tumor ROIs lose spatial context needed for invasion detection, TumorNet's high DSC may not translate to staging accuracy.

### Mechanism 3
Contour-based distance computation from segmentation masks provides clinically interpretable invasion detection. After segmentation, contours are extracted and maximum Euclidean distances between tumor and anatomical structure contours are computed. Zero distance indicates invasion. This mimics radiologist assessment rather than learning invasion patterns implicitly. Core assumption: 2D slice-level distances capture clinically relevant invasion; 3D volumetric analysis would not substantially change staging decisions. Evidence anchors: [Section 3.8] "distances are measured by first extracting the contours... compute the maximum distance between the tumor contour and the corresponding anatomical region"; [Section 5.4] T4 misclassifications partially attributed to structures not segmented (esophagus, trachea, carina), validating that invasion detection drives staging; [corpus] No direct comparison of contour-based vs. learned distance methods in neighbor papers. Break condition: Diaphragm estimation heuristic (lowest 10% of lung mask) fails for atypical anatomies, causing false positive/negative invasion calls.

## Foundational Learning

- **CT image preprocessing (HU windowing, normalization)**: Why needed here: The pipeline relies on consistent intensity ranges across multiple datasets. Lung window settings (WW:1400, WC:-700) must be understood to debug segmentation failures. Quick check question: Can you explain why CLAHE is applied after windowing rather than before?

- **Encoder-decoder segmentation architectures (U-Net, UNet++, FPN decoders)**: Why needed here: Three different architectures are used; understanding skip connections, feature pyramid aggregation, and dense skip pathways is essential for modifying or debugging individual networks. Quick check question: What is the tradeoff between UNet++ nested skip connections and standard U-Net skip connections for small tumor boundaries?

- **TNM staging system and IASLC T-stage criteria**: Why needed here: The rule-based classifier hard-codes medical guidelines. Without understanding size thresholds (≤3cm=T1, >3-5cm=T2, etc.) and invasion criteria, you cannot debug misclassifications or extend the system. Quick check question: Why does mediastinum invasion automatically classify as T4 regardless of tumor size?

## Architecture Onboarding

- **Component map**: Input CT → [Preprocessing: windowing, CLAHE, resize] → [Detection: YOLOv11] → Tumor ROI crop → [Segmentation: LungNet/MediNet/TumorNet in parallel] → [Contour extraction + distance computation] → [Rule-based T-stage classifier per IASLC flowchart] → T-stage output (T1/T2/T3/T4)

- **Critical path**: TumorNet segmentation quality → tumor size measurement → staging accuracy. The largest tumor dimension directly determines T1/T2/T3 boundaries; errors here cascade.

- **Design tradeoffs**:
  - 2D slice-level analysis vs. 3D volumetric: faster inference but may miss depth-wise tumor extent
  - Separate models vs. unified multi-organ segmentation: higher maintenance but allows per-task architecture optimization
  - Rule-based vs. learned classifier: interpretable but cannot capture edge cases outside explicit rules

- **Failure signatures**:
  - T4 underperformance (F1=0.90, lowest precision 88%): multifocal disease and invasion into non-segmented structures (trachea, carina, esophagus) cause false negatives
  - Diaphragm estimation errors for patients with pleural effusion or lung base abnormalities
  - Cross-dataset generalization: models trained on secondary datasets may not transfer to new scanner protocols

- **First 3 experiments**:
  1. Replicate LungNet on the compiled lung database with 5-fold CV; target DSC >0.97 before proceeding
  2. Run inference on 50 held-out primary dataset slices; manually verify tumor size measurements against radiologist annotations to validate distance computation
  3. Ablate the rule-based classifier by replacing with a simple size-only threshold; quantify accuracy drop to isolate the contribution of invasion detection

## Open Questions the Paper Calls Out

### Open Question 1
Would replacing the heuristic diaphragm estimation (lowest 10% of lung mask boundary) with a dedicated segmentation model significantly improve T4 classification accuracy? Basis in paper: [explicit] "Our diaphragm boundary relies on a simple heuristic using the lowest 10% of lung mask points, due to the lack of datasets related to diaphragm segmentation. It may misalign in atypical anatomies." Why unresolved: No public diaphragm segmentation datasets exist; the heuristic was chosen as a practical workaround. What evidence would resolve it: Curating a diaphragm segmentation dataset and comparing T-staging accuracy between heuristic-based and segmentation-based approaches.

### Open Question 2
How much would incorporating detection of multifocal disease (separate tumor nodules in different ipsilateral lobes) reduce T4 misclassification? Basis in paper: [explicit] The authors state T4 misclassification occurs because "A patient with such tumor nodules is classified as T4, regardless of the individual nodule sizes... In our proposed pipeline... a patient with, for instance, a 2 cm nodule in the upper lobe and a 1.5 cm nodule in the lower lobe could be erroneously classified as T1 or T2." Why unresolved: Current pipeline assesses only individual nodule characteristics, not multifocal disease patterns across slices. What evidence would resolve it: Extending the framework to detect multiple nodules across all CT slices and comparing classification metrics before/after.

### Open Question 3
Would transitioning from 2D slice-based analysis to full 3D volumetric segmentation improve tumor size estimation and staging accuracy? Basis in paper: [explicit] "By focusing solely on two-dimensional slices, we may overlook volumetric tumor characteristics that could refine staging." Why unresolved: Current depth estimation multiplies slice count by slice thickness; true 3D segmentation was not implemented. What evidence would resolve it: Comparing staging accuracy between 2D-based and 3D volumetric segmentation pipelines on the same dataset.

### Open Question 4
How robust is the rule-based staging pipeline to segmentation errors from the AnatomicalNets models? Basis in paper: [inferred] The framework depends entirely on accurate segmentation masks for rule-based classification, yet no sensitivity analysis examines how Dice score degradation affects final staging accuracy. Why unresolved: The paper reports segmentation metrics and staging metrics separately without analyzing error propagation. What evidence would resolve it: Systematically degrading segmentation quality and measuring the corresponding drop in T-stage classification accuracy.

## Limitations
- Reliance on 2D slice-level segmentation may miss volumetric tumor extent critical for accurate staging
- Diaphragm estimation heuristic fails on atypical anatomies with effusions or post-surgical changes
- T4 classification performance suffers from inability to segment non-pulmonary structures (trachea, carina, esophagus)

## Confidence

- Mechanism 1 (hybrid segmentation + rules > end-to-end): Medium - supported by 91.36% vs 37-40% accuracy gap, but no ablation study provided
- Mechanism 2 (task-specialized architectures): High - DSC scores show appropriate task difficulty, neighbor papers support decomposition approach
- Mechanism 3 (contour-based invasion detection): Medium - mimics radiologist assessment but 2D distances may miss depth-wise invasion

## Next Checks

1. Perform 3D volumetric measurement comparison on 50 samples to quantify 2D vs 3D staging differences
2. Test diaphragm estimation failure cases by injecting synthetic effusions into validation set
3. Conduct cross-dataset evaluation using MSD tumor or other lung cancer datasets with different scanner protocols