---
ver: rpa2
title: 'PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence
  Completion'
arxiv_id: '2510.22616'
source_url: https://arxiv.org/abs/2510.22616
tags:
- dataset
- reasoning
- persian
- percor
- commonsense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PerCoR is the first large-scale Persian commonsense reasoning benchmark,
  containing 106K multiple-choice sentence completion problems. The authors developed
  a conjunction-based segmentation strategy to extract coherent sentence-completion
  pairs from over 40 diverse Persian sources.
---

# PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion
## Quick Facts
- arXiv ID: 2510.22616
- Source URL: https://arxiv.org/abs/2510.22616
- Reference count: 24
- First large-scale Persian commonsense reasoning benchmark with 106K multiple-choice sentence completion problems

## Executive Summary
PerCoR introduces the first large-scale Persian commonsense reasoning benchmark using a conjunction-based segmentation strategy to create 106K multiple-choice sentence completion problems. The authors developed DRESS-AF, an embedding-based adversarial filtering method that selects challenging distractors from gold continuations without LLM generation. Human annotators achieved 89% accuracy while top models like OpenAI-o3 (92.18%) and Claude-Sonnet-3.7 (91.17%) approached human performance, demonstrating the benchmark's difficulty.

## Method Summary
The authors created PerCoR by segmenting Persian text at conjunctions (50-250 characters from paragraph start) to generate coherent sentence-completion pairs requiring discourse-level reasoning. They introduced DRESS-AF, which selects distractors from gold continuations using a similarity-based scoring function optimized via Bayesian optimization to maximize model confusion while preserving human solvability. The dataset uses 40+ diverse Persian sources and includes 86K training, 10K validation, and 10K test samples.

## Key Results
- Human annotators achieved 89% accuracy on PerCoR
- OpenAI-o3 scored 92.18%, Claude-Sonnet-3.7 reached 91.17%, and DeepSeek-R1 attained 82.51%
- DRESS-AF successfully transferred to English HellaSwag, increasing difficulty without affecting human solvability
- The method avoids LLM-generated distractors, reducing generation artifacts while maintaining challenge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conjunction-based segmentation creates linguistically grounded, semantically coherent sentence–completion pairs that require discourse-level reasoning to solve.
- Mechanism: Splitting Persian text at conjunctions ensures the prefix establishes a discourse relation that must be inferred to select the correct continuation. The prefix provides sufficient context (50–250 characters) while forcing models to reason about causal, contrastive, or elaborative relations rather than relying on surface lexical cues.
- Core assumption: Conjunctions consistently signal predictable discourse relations that constrain plausible continuations; readers must integrate the conjunction's semantic role with world knowledge.
- Evidence anchors:
  - [abstract] "We adopt a linguistically grounded, conjunction-based segmentation strategy to generate coherent prefix–continuation pairs."
  - [section 3.2] "To ensure that the sentence–completion split occurs at an informative and coherent boundary, we define a valid character span within which conjunctions are considered—ranging from a minimum of 50 to a maximum of 250 characters from the start of the paragraph."
  - [corpus] Related work (DiscoSense) validates discourse connectives as reasoning cues in other languages; FarsiMCQGen corpus addresses MCQ generation in Persian but not discourse-level reasoning specifically.
- Break condition: If conjunctions are ambiguous (some Persian connectives have multiple senses), the discourse constraint weakens.

### Mechanism 2
- Claim: DRESS-AF produces challenging distractors by selecting semantically similar gold continuations from other samples, then adversarially optimizing embedding-similarity coefficients to maximize model confusion while preserving human solvability.
- Mechanism: The scoring function s_ij = α·cos(x_i, y_j) + β·cos(y_i, y_j) + (1−α−β)·cos(z_i, y_j) ranks candidate distractors. Bayesian optimization (TPE) searches for (α*, β*) that minimize a discriminator model's accuracy on a held-out set. The top-k candidates are sampled to introduce controlled randomness.
- Core assumption: The pool of gold continuations across all samples is sufficiently diverse to serve as plausible distractors; embedding similarity correlates with "confusability" for models but not for humans.
- Evidence anchors:
  - [abstract] "DRESS-AF...selects distractors from the pool of gold continuations while maximising model confusion."
  - [section 3.3] "We optimize α and β via adversarial filtering: for a given (α, β), we build the provisional dataset from the held-out development set, measure the accuracy of an LLM on it, and use that accuracy as the objective."
  - [corpus] HellaSwag uses adversarial filtering with LM-generated distractors; DRESS-AF differs by using human-written continuations, reducing generation artifacts.
- Break condition: If the gold-continuation pool lacks semantic diversity, distractors become either too obvious or unsolvable.

### Mechanism 3
- Claim: Distractors selected from higher embedding-similarity ranks create harder examples for models without proportionally affecting human performance, because models exploit stylistic artifacts in generated text that humans ignore.
- Mechanism: On HellaSwag, DRESS-AF variants showed that sampling from top-10 candidates (harder) reduced GPT-4o-mini accuracy by 14–16 points while human accuracy dropped only ~7 points (90% → 83%). Sampling from lower ranks (easier) preserved both model and human performance.
- Core assumption: Models rely partially on fluency/style artifacts in LLM-generated distractors; real human-written distractors lack these artifacts, forcing reasoning-based selection.
- Evidence anchors:
  - [section 4.2.2] "This asymmetry indicates that models were partially exploiting stylistic or fluency artifacts present in the original LM-generated distractors—artifacts that humans did not rely on."
  - [section 4.2.2] Human accuracy on original HellaSwag: 90%; easier variant: 89.5%; harder variant: 83%. Model drops were much larger (14–16 points).
  - [corpus] No direct corpus evidence on artifact-exploitation mechanisms; this is an empirical observation specific to this study.
- Break condition: If human-written distractors are too similar to gold answers, human accuracy degrades sharply, making examples unsolvable rather than challenging.

## Foundational Learning

- Concept: **Adversarial Filtering (AF)**
  - Why needed here: DRESS-AF builds on AF principles from HellaSwag/SWAG. Understanding AF explains why iterative discriminator-based selection creates challenging benchmarks.
  - Quick check question: Given a 4-way multiple-choice item, how would you iteratively replace the easiest distractor to reduce a discriminator model's accuracy?

- Concept: **Discourse Relations (Conjunction Semantics)**
  - Why needed here: The dataset's core challenge requires inferring whether a conjunction signals contrast, causation, elaboration, etc., to predict the correct continuation.
  - Quick check question: For the sentence "The weather was pleasant, ___," predict how completions differ after "but" vs. "so" vs. "because."

- Concept: **Embedding Similarity for Distractor Selection**
  - Why needed here: The DRESS-AF scoring function combines three cosine-similarity terms; understanding how each captures different aspects of semantic relatedness is essential for tuning or adapting the method.
  - Quick check question: In s_j = α·cos(x_i, y_j) + β·cos(y_i, y_j) + γ·cos(z_i, y_j), what does increasing β prioritize?

## Architecture Onboarding

- Component map:
  Corpersia corpus (40+ Persian websites) -> Paragraph filtering (≥50 chars) -> Conjunction detection -> Span validation (50–250 chars) -> GPT-4o-mini filtering -> DRESS-AF optimization -> Final dataset assembly

- Critical path:
  1. Conjunction list curation (49 high-frequency Persian conjunctions, filtered to those with ≥500 occurrences)
  2. Sentence–completion pair extraction with length constraints
  3. GPT-4o-mini binary classification for conjunction sense and completion completeness
  4. DRESS-AF optimization loop on held-out set
  5. Final dataset assembly with train/val/test splits (86K/10K/10K)

- Design tradeoffs:
  - **Generation-free vs. LLM-generated distractors**: Avoids LLM biases but limits distractor diversity to available gold continuations
  - **k=20 sampling window**: Balances difficulty (lower k = harder) with diversity (higher k = more varied distractors)
  - **c=30 optimization trials**: More trials may find better parameters but increases compute cost; diminishing returns observed after trial ~20
  - **Non-expert annotators**: Faster/cheaper but potentially noisier human baseline; expert annotation would yield higher-quality gold standard

- Failure signatures:
  - **Low strict vs. post-processed accuracy gap** (e.g., LLaMA-3.3-70B: 11.23% → 79.56%): Model embeds correct answer in prose rather than returning index alone—post-processing recovers latent ability
  - **Conjunction ambiguity**: If filtering fails, semantically invalid splits create incoherent prefixes (Figure 7 shows ~10–50% retention for ambiguous conjunctions)
  - **Excessively similar distractors**: Human accuracy drops sharply (e.g., HellaSwag harder variant: 90% → 83%); if k is too small, examples become unsolvable

- First 3 experiments:
  1. **Reproduce DRESS-AF on a held-out subset**: Implement the scoring function with random (α, β), verify that TPE optimization reduces discriminator accuracy over 30 trials. Confirm the lowest-accuracy trial corresponds to (α*, β*).
  2. **Vary k (distractor sampling window)**: Create variants with k ∈ {10, 20, 50} on the validation set; plot model vs. human accuracy to find the "sweet spot" where model difficulty increases without excessive human degradation.
  3. **Transfer DRESS-AF to another language**: Apply the pipeline to English HellaSwag (as authors did) or another language with a conjunction list; verify that harder variants reduce model accuracy more than human accuracy, confirming language-agnostic applicability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does DRESS-AF generalize across typologically diverse languages beyond Persian and English?
- Basis in paper: [explicit] The authors state: "Future work will (i) extend our language-agnostic pipeline to other languages by adapting conjunction lists and applying DRESS-AF" and "we could have extended the algorithm to other languages to construct a multilingual commonsense reasoning dataset."
- Why unresolved: While DRESS-AF demonstrated transferability to English HellaSwag, it has only been validated on two languages. Languages with different conjunction structures, morphological patterns, or discourse conventions may require substantial adaptation.
- What evidence would resolve it: Application of the pipeline to 3-5 typologically diverse languages (e.g., agglutinative, tonal, or pro-drop languages) with comparative analysis of difficulty levels and human solvability rates.

### Open Question 2
- Question: Would expert annotators achieve significantly higher accuracy than the 89% obtained by non-expert annotators on ambiguous PerCoR instances?
- Basis in paper: [explicit] The Limitations section states: "relying on expert annotators would likely yield more accurate and reliable assessments, particularly for complex or ambiguous cases" and proposes "conduct[ing] expert-based human evaluation to establish a high-quality gold standard for ambiguous cases."
- Why unresolved: Current human evaluation used three non-expert annotators per sample with majority voting, but no comparison to expert performance exists to quantify the annotation quality ceiling.
- What evidence would resolve it: Expert annotation on a stratified sample of PerCoR instances, particularly those flagged as ambiguous, with inter-annotator agreement and accuracy comparisons to the non-expert baseline.

### Open Question 3
- Question: Does training on PerCoR improve performance on diverse Persian downstream tasks beyond extractive QA?
- Basis in paper: [inferred] The authors show preliminary transfer to PQUAD (ROUGE-L improved 10.4 points), but note: "Such evaluation is not currently feasible in Persian, as PERCOR is the first structured commonsense reasoning benchmark in the language. We hope PERCOR will serve as a foundation that enables similar cross-task studies in the future."
- Why unresolved: Only one transfer experiment was conducted (to PQUAD), and the improvement in Exact Match was minimal (1.35 points). The breadth of transferability remains unexplored.
- What evidence would resolve it: Fine-tuning experiments on multiple Persian tasks (e.g., ParsiNLU benchmarks, sentiment analysis, summarization) with analysis of which reasoning capabilities transfer.

### Open Question 4
- Question: How robust is DRESS-AF to the choice of embedding model, and does adversarial optimization against one LLM generalize to others?
- Basis in paper: [inferred] DRESS-AF optimizes hyperparameters using GPT-4o-mini's accuracy as the objective, and relies on HAKIM embeddings for Persian. The paper does not analyze sensitivity to these component choices.
- Why unresolved: If the selected distractors are tuned specifically to confuse GPT-4o-mini with HAKIM embeddings, they may be less effective against other model-embedding combinations.
- What evidence would resolve it: Ablation studies varying the embedding model (e.g., multilingual alternatives) and the adversarial target model, measuring resulting dataset difficulty across diverse LLMs.

## Limitations
- Limited cross-linguistic validation: DRESS-AF only tested on Persian and English, leaving generalizability to other languages uncertain
- Reliance on GPT-4o-mini filtering: Dataset quality depends heavily on LLM's ability to detect discourse coherence, potentially introducing biases
- Potential annotation noise: Non-expert human annotators achieved 89% accuracy, suggesting possible disagreement on edge cases

## Confidence
- **High confidence**: Dataset construction methodology (conjunction-based segmentation, DRESS-AF implementation) is well-specified and reproducible
- **Medium confidence**: DRESS-AF produces more challenging examples without reducing human solvability, but mechanism remains partially speculative
- **Medium confidence**: Models exploit stylistic artifacts in generated distractors, but specific artifacts and prevalence not characterized

## Next Checks
1. **Conjunction ambiguity analysis**: Conduct detailed study of Persian conjunctions with multiple senses to quantify how often GPT-4o-mini filtering fails and whether this introduces systematic bias
2. **Cross-lingual DRESS-AF adaptation**: Apply DRESS-AF pipeline to another language with different conjunction inventory (e.g., Spanish or French) to test generalizability
3. **Artifact characterization study**: Analyze model predictions on HellaSwag variants to identify specific stylistic or fluency patterns that models exploit in LLM-generated distractors but humans ignore