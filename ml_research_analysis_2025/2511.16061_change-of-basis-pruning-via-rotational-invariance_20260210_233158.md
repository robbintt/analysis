---
ver: rpa2
title: Change-of-Basis Pruning via Rotational Invariance
arxiv_id: '2511.16061'
source_url: https://arxiv.org/abs/2511.16061
tags:
- pruning
- importance
- layer
- parameters
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of structured pruning in neural
  networks, where effectiveness depends on how importance is distributed across the
  representation space. The authors introduce two-subspace radial activations (TSRAs)
  to enable change-of-basis (CoB) pruning by making the activation function invariant
  to orthogonal transformations within two separate subspaces.
---

# Change-of-Basis Pruning via Rotational Invariance

## Quick Facts
- arXiv ID: 2511.16061
- Source URL: https://arxiv.org/abs/2511.16061
- Reference count: 21
- Key outcome: TSRA + CoB enables 90%+ pruning with minimal accuracy loss vs 30% without CoB

## Executive Summary
This paper addresses structured pruning effectiveness by introducing Two-Subspace Radial Activations (TSRAs) that enable change-of-basis (CoB) pruning through rotational invariance. By making activations invariant to orthogonal transformations within two separate subspaces, CoB transformations can be merged into surrounding weights without extra parameters. The method uses uncentered PCA to concentrate activation importance along aligned axes, making structured pruning more effective. Demonstrated on VGG-16/CIFAR-10, CoB significantly extends the reliable pruning frontier from ~30% to ~70% of parameters without post-prune fine-tuning.

## Method Summary
The method combines TSRA activations with CoB transformations for structured pruning. TSRAs decompose R^d = U ⊕ V and apply independent scaling functions based on |x_U|, |x_V|, achieving rotational invariance within each subspace. CoB uses uncentered PCA on activation distributions to compute rotation matrices that concentrate importance along axes. The rotation R is merged into the current layer's weights and R^T into the next layer's weights, eliminating parameter overhead. Pruning targets dimensions with low activation magnitudes, applied separately within each subspace. The approach replaces ReLU with TSRA (incurring ~4.5% accuracy drop), replaces BatchNorm with unlearned RMSNorm, and uses average pooling instead of max-pooling.

## Key Results
- CoB extends reliable pruning frontier from ~30% to ~70% parameters without post-prune fine-tuning
- Under threshold-based pruning, CoB achieves 90-96% parameter reduction with only 1-6% accuracy drop after fine-tuning
- Uncentered PCA concentrates importance distributions post-CoB (Figure 5) compared to diffuse pre-CoB distributions
- TSRA width saturation bound is 2(d_{i-1} + 1), strictly greater than radial rescaling functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rotational invariance permits CoB transformations to merge into weights without parameter overhead
- Mechanism: TSRAs satisfy φ(R^T x) = R^T φ(x) within each subspace, allowing R^T to factor into W_{i+1}' = W_{i+1} R^T
- Core assumption: Orthogonal transformation can be block-diagonal with respect to TSRA subspaces
- Evidence anchors: [abstract] "This invariance allows CoB transformations to be merged into surrounding weights"; [Section 3] Derivation of merge; [corpus] DenoiseRotator uses learned rotations with parameter overhead

### Mechanism 2
- Claim: Uncentered PCA concentrates activation-magnitude importance along aligned axes
- Mechanism: Without mean-centering, PCA maximizes L2-norm of projections, aligning axes so few dimensions capture most importance
- Core assumption: Activation L2-norm across samples is valid importance proxy
- Evidence anchors: [Section 4.1] "maximizing the L2 norm of the projections"; [Figure 5] Concentrated importance post-CoB; [corpus] SliceGPT uses similar PCA-based importance concentration

### Mechanism 3
- Claim: Two-subspace factorization avoids radial network width saturation while preserving invariance
- Mechanism: Fully radial activations restrict to input span; TSRAs allow leaving any 1D subspace with saturation bound 2(d_{i-1} + 1)
- Core assumption: Two subspaces sufficient for practical width expansion
- Evidence anchors: [Section 2] "strictly greater width saturation than radial rescaling functions"; [Appendix A.2, Lemma 2] Upper bound derivation; [corpus] No direct evidence on TSRA-style width saturation

## Foundational Learning

- Concept: Orthogonal transformations and change-of-basis
  - Why needed here: Core to understanding how rotations redistribute importance without altering geometry
  - Quick check question: Given R ∈ SO(d), explain why R^T R = I and how this enables insertion of identity into a forward pass

- Concept: Principal Component Analysis (PCA)
  - Why needed here: Method uses uncentered PCA to derive importance-concentrating rotation
  - Quick check question: What does PCA maximize when data is mean-centered vs un-centered?

- Concept: Structured pruning importance metrics
  - Why needed here: Paper uses activation-magnitude (L2-norm) as importance score
  - Quick check question: Compare weight-based (L1 norm) vs activation-based importance; when would each be preferred?

## Architecture Onboarding

- Component map: Input -> VGG-16 layers with TSRA activations -> CoB transformation -> Pruned network
- Critical path:
  1. Train dense TSRA model to convergence (AdamW recommended)
  2. Collect activations on calibration samples
  3. Compute uncentered PCA per layer → construct R within each subspace
  4. Merge R into current layer weights, R^T into next layer weights
  5. Compute importance scores on rotated activations
  6. Prune low-importance dimensions (separately per subspace)
  7. Optionally fine-tune

- Design tradeoffs:
  - TSRA vs ReLU: ~4.5% accuracy drop for invariance property
  - Subspace split ratio: Paper uses 50/50; other splits unexplored
  - Logistic coefficients (a=5, b_U=0.5, b_V=0.7): Arbitrary; no hyperparameter sweep
  - Weight initialization: Not investigated; uses PyTorch defaults

- Failure signatures:
  - Importance remains diffuse post-CoB → check PCA is uncentered and applied per-subspace
  - Accuracy collapse at moderate pruning → verify subspace-wise pruning (not global)
  - Training instability → TSRA prefers AdamW; SGD with momentum may underperform
  - Invariance broken → confirm no BatchNorm or max-pooling remains

- First 3 experiments:
  1. Replicate VGG-16 + CIFAR-10 baseline: Compare TSRA vs ReLU control accuracy to validate ~4.5% gap
  2. Ablate CoB: Compare standard pruning vs CoB-augmented pruning at fixed ratios (10-70%) without fine-tuning
  3. Threshold sweep: Test proportion-of-maximum thresholding (T=0.05-0.17) to verify 90%+ pruning with <6% drop

## Open Questions the Paper Calls Out

- Can optimal TSRA coefficient values (a_U, a_V, b_U, b_V) be determined through systematic analysis to close the 4.52% accuracy gap with ReLU-based models?
- What are the lower bounds on TSRA width saturation, and can the upper bound of 2(d_{i-1} + 1) be tightened under specific conditions?
- Can dedicated weight initialization schemes for TSRAs improve training convergence and final accuracy compared to default PyTorch initializations?
- Does CoB pruning with TSRAs generalize to other architectures (e.g., ResNets, Transformers) and larger-scale datasets beyond VGG-16/CIFAR-10?

## Limitations

- 4.52% accuracy drop versus ReLU controls represents non-trivial performance penalty
- Pruning effectiveness evaluated only on VGG-16/CIFAR-10, limiting generalization claims
- Two-subspace TSRA design appears arbitrary without exploration of alternative decompositions
- Logistic coefficients used without systematic hyperparameter optimization

## Confidence

- **High confidence**: Mathematical derivation of TSRA invariance and width saturation bounds; empirical demonstration of CoB enabling more aggressive pruning
- **Medium confidence**: Claim that uncentered PCA is essential for importance concentration; consistency of 4.52% accuracy penalty
- **Low confidence**: Assertion that two subspaces are sufficient for practical networks; lack of broader experimental scope

## Next Checks

1. Test TSRA + CoB on ResNet and MobileNet architectures on CIFAR-10/CIFAR-100 to assess pruning gains transfer beyond VGG-style networks
2. Vary number of subspaces (2→4→8) and measure impact on both accuracy and pruning effectiveness to determine optimal subspace count
3. Systematically vary logistic scaling parameters (a, b_U, b_V) across a grid to quantify sensitivity of baseline accuracy and CoB effectiveness to design choices