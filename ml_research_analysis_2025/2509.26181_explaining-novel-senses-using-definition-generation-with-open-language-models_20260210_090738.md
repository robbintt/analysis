---
ver: rpa2
title: Explaining novel senses using definition generation with open language models
arxiv_id: '2509.26181'
source_url: https://arxiv.org/abs/2509.26181
tags:
- definition
- axolotl
- definitions
- sense
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the use of open-weight large language models\
  \ (LLMs) for generating definitions of novel word senses, addressing the challenge\
  \ of interpreting semantic change. The authors fine-tune three instruction-tuned\
  \ open-source models\u2014mT0, Aya-101, and TowerInstruct\u2014on datasets from\
  \ the AXOLOTL\u201924 shared task, which includes Finnish, Russian, and German languages."
---

# Explaining novel senses using definition generation with open language models

## Quick Facts
- arXiv ID: 2509.26181
- Source URL: https://arxiv.org/abs/2509.26181
- Reference count: 21
- Primary result: Open-weight LLMs (mT0, Aya-101, TowerInstruct) fine-tuned on AXOLOTL'24 and Dbnary outperform closed proprietary models on definition generation for novel word senses

## Executive Summary
This paper investigates the use of open-weight large language models for generating dictionary-style definitions of novel word senses, addressing the challenge of interpreting semantic change. The authors fine-tune three instruction-tuned open-source models—mT0, Aya-101, and TowerInstruct—on datasets from the AXOLOTL'24 shared task covering Finnish, Russian, and German languages. By combining AXOLOTL'24 data with Dbnary, they achieve higher performance than previous submissions that relied on closed proprietary models. The study finds that encoder-decoder and decoder-only models perform similarly, larger models and datasets improve results, and manual evaluation reveals differences in fluency, adequacy, and circularity of generated definitions.

## Method Summary
The approach involves fine-tuning open-weight LLMs using QLoRA on aligned usage-definition pairs from AXOLOTL'24 and Dbnary. The models generate candidate definitions for each usage example, which are then aggregated by embedding and selecting the definition closest to the centroid. This method is evaluated across three languages using BLEU and BERTScore metrics, with manual evaluation on Russian to assess fluency, adequacy, and circularity.

## Key Results
- Open-weight models (mT0, Aya-101, TowerInstruct) outperform previous closed proprietary model submissions on the AXOLOTL'24 task
- Encoder-decoder and decoder-only architectures perform on par for this task
- Augmenting training data with Dbnary consistently improves results, though language-specific variations exist
- Manual evaluation reveals trade-offs: decoder-only models show better fluency while encoder-decoder models show better adequacy

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Fine-Tuning on Definition-Usage Pairs
Fine-tuning open-weight LLMs on aligned usage-definition pairs enables them to generate human-readable definitions for novel senses by learning to map context-dependent usage examples to dictionary-style definitions. The core assumption is that the dataset provides sufficient signal for the model to generalize from seen definitions to novel senses. Break condition: If test senses are too dissimilar from training data, the model may hallucinate or produce overly generic definitions.

### Mechanism 2: Embedding-Based Definition Aggregation
Aggregating definitions generated for multiple usage examples of the same sense produces a more stable representation by selecting the definition whose embedding is closest to the centroid of all candidates. The core assumption is that the embedding model captures semantic similarity well enough that the centroid represents the most accurate summary. Break condition: If generated definitions are noisy or contradictory, the centroid becomes meaningless.

### Mechanism 3: Extended Lexical Resource Augmentation
Augmenting a small task-specific dataset with a larger general lexical resource (Dbnary) improves model robustness by exposing it to wider variety of lexical semantics. The core assumption is that definition styles in the general resource are compatible with those expected in the target task. Break condition: If the external resource domain is too dissimilar, performance may degrade, as observed with Finnish.

## Foundational Learning

- **Concept: Low-Rank Adaptation (QLoRA)**
  - Why needed here: Enables efficient fine-tuning of large models (7B-13B parameters) on a single GPU with limited memory
  - Quick check question: Why is it possible to fine-tune a massive model with only 16GB of GPU memory available for activations/gradients?

- **Concept: Encoder-Decoder vs. Decoder-Only Architectures**
  - Why needed here: The paper compares mT0/Aya (encoder-decoder) with TowerInstruct (decoder-only) and finds them to perform on par
  - Quick check question: An encoder-decoder model processes the entire input context before generating any output. How might this be different from a decoder-only model processing the same context?

- **Concept: Evaluation Metrics for Generation (BLEU, BERTScore)**
  - Why needed here: The core results are reported using these metrics, but the authors find BLEU can be gamed by outputting dictionary labels
  - Quick check question: Why might a high BLEU score not correlate with a good, fluent definition in this specific task?

## Architecture Onboarding

- **Component map**: AXOLOTL'24 + Dbnary datasets -> QLoRA fine-tuning -> Beam search generation -> Sentence Transformer embedding -> Centroid selection -> Final definition

- **Critical path**:
  1. Data Prep: Align usage-definition pairs from AXOLOTL'24 and Dbnary, removing contaminated data
  2. Model Fine-Tuning: Train QLoRA adapters on (usage + prompt) -> definition pairs
  3. Per-Usage Inference: Generate definitions for all usage examples of a target sense
  4. Definition Aggregation: Embed all generated definitions, compute centroid, select closest, ensure uniqueness

- **Design tradeoffs**:
  - Model Size vs. Resources: Aya-101 (13B) shows better adequacy/fluency but is larger and slower to train than mT0 (3.7B)
  - Architecture: Decoder-only (Tower) shows better fluency; Encoder-decoder (Aya) shows better adequacy
  - Data Source: AXOLOTL'24-only leads to overfitting (low fluency, high BLEU); adding Dbnary improves generalization but may hurt if domains misaligned
  - Aggregation Method: Simple centroid selection used; more complex summarization found insufficient

- **Failure signatures**:
  - Low Fluency/Repetitions: High BLEU but definitions contain excessive dictionary labels (overfitting to small AXOLOTL-only dataset)
  - Low Adequacy/Wrong Sense: Generated definition refers to wrong sense of the word
  - Circular Definitions: Generated definition contains the target word itself
  - Overly Narrow Definitions: Repeating named entities from usage examples
  - Metric Mismatch: High BLEU but poor semantic quality, rewarding surface form over meaning

- **First 3 experiments**:
  1. Baseline Replication: Fine-tune mT0-XL using QLoRA on Russian AXOLOTL'24 dataset only to establish baseline BLEU/BERTScore
  2. Data Augmentation Ablation: Add cleaned Dbnary data and compare metrics and qualitative fluency/adequacy against baseline
  3. Architecture Comparison: Fine-tune decoder-only model on same augmented dataset and compare generated definitions against mT0 encoder-decoder model

## Open Questions the Paper Calls Out

- Can a single end-to-end system be developed to jointly solve both novel sense identification and definition generation?
- Can instruction-tuned models be improved to reliably summarize multiple generated definitions into a single sense label?
- How does the domain of fine-tuning data affect performance relative to raw dataset size?
- How do biases inherent in lexical resources like Wiktionary propagate into the outputs of fine-tuned definition generators?

## Limitations
- Manual evaluation was only performed on Russian, leaving uncertainty about cross-lingual consistency of architecture-specific patterns
- BLEU metric vulnerability to being "gamed" by dictionary labels is acknowledged but not fully resolved in evaluation methodology
- Language-specific variations in Dbnary augmentation effectiveness suggest domain alignment is critical but not well-characterized

## Confidence

- **High Confidence**: Open-weight models can match or exceed closed proprietary model performance; QLoRA enables efficient fine-tuning on consumer hardware
- **Medium Confidence**: Architectural comparison showing similar overall performance with different strengths in fluency vs. adequacy
- **Medium Confidence**: Dataset augmentation with Dbnary improves results, though effectiveness varies by language

## Next Checks

1. **Cross-Lingual Consistency**: Replicate manual evaluation framework on Finnish and German test sets to verify architecture-specific patterns across all three languages
2. **Domain Alignment Analysis**: Systematically evaluate definition quality when training data domains are increasingly misaligned to establish when augmentation helps versus harms
3. **Metric Calibration Study**: Design ablation comparing definitions with and without dictionary label artifacts to quantify divergence between BLEU/BERTScore and human judgments