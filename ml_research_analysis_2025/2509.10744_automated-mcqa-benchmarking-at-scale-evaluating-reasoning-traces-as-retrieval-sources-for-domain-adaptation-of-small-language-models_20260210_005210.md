---
ver: rpa2
title: 'Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval
  Sources for Domain Adaptation of Small Language Models'
arxiv_id: '2509.10744'
source_url: https://arxiv.org/abs/2509.10744
tags:
- reasoning
- retrieval
- baseline
- scientific
- mcqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a scalable, modular pipeline for automated MCQA benchmark
  generation from large scientific corpora, enabling continuous updates to keep pace
  with rapid knowledge growth. Our framework automates PDF parsing, semantic chunking,
  question generation, and quality filtering while preserving provenance links to
  source literature.
---

# Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models

## Quick Facts
- arXiv ID: 2509.10744
- Source URL: https://arxiv.org/abs/2509.10744
- Reference count: 32
- Primary result: Reasoning-trace retrieval consistently outperforms baseline and chunk retrieval for domain adaptation of small language models

## Executive Summary
This paper presents a scalable, modular pipeline for automated multiple-choice question answering (MCQA) benchmark generation from large scientific corpora, specifically targeting domain adaptation of small language models. The framework automates PDF parsing, semantic chunking, question generation, and quality filtering while preserving provenance links to source literature. As a case study, the authors generate 16,679 MCQs from 22,000 open-access radiation and cancer biology papers and evaluate 1.1Bâ€“14B parameter models under baseline, chunk-based RAG, and reasoning-trace RAG conditions.

The key finding demonstrates that retrieval-augmented generation from GPT-4.1 reasoning traces consistently outperforms both baseline and chunk retrieval approaches, with particularly dramatic gains for smaller models. For instance, TinyLlama-1.1B-Chat improves from 17.6% to 71.0% accuracy on the synthetic benchmark. The approach shows effectiveness beyond the synthetic domain, as several small models surpass GPT-4 baseline performance on the expert-annotated 2023 Astro exam when using reasoning-trace retrieval, demonstrating the potential of reasoning distillation for domain adaptation.

## Method Summary
The authors develop a comprehensive automated pipeline for MCQA benchmark generation from scientific literature. The system begins with PDF parsing using Parsr to extract text, followed by semantic chunking with sentence-level separators and duplicate removal. Question generation employs LLMs (GPT-4o and GPT-4.1) with specific prompting strategies that include answer masking and reference retention. Quality filtering uses LLM judges (GPT-4o) with multiple criteria including context-answer consistency, question difficulty, grammatical correctness, and subject relevance. The final benchmark undergoes expert review for validation. For evaluation, the framework tests multiple model sizes (1.1B to 14B parameters) across three conditions: baseline (no retrieval), chunk-based RAG (retrieving from document chunks), and reasoning-trace RAG (retrieving from GPT-4.1 reasoning traces). Experiments use semantic search with max marginal relevance for diverse retrieval and evaluate both the synthetic radiation/cancer biology benchmark and the expert-annotated 2023 Astro exam.

## Key Results
- Reasoning-trace retrieval consistently outperforms baseline and chunk retrieval across all model sizes tested
- TinyLlama-1.1B-Chat accuracy improves from 17.6% to 71.0% when using reasoning-trace retrieval
- Several small models surpass GPT-4 baseline performance on the expert-annotated 2023 Astro exam using reasoning-trace retrieval
- Performance gains are most pronounced for smaller models, demonstrating effectiveness of reasoning distillation for domain adaptation

## Why This Works (Mechanism)
The approach works by leveraging the reasoning capabilities of large language models to generate interpretable reasoning traces that serve as intermediate retrieval sources. Instead of directly retrieving from raw document chunks, the system retrieves from the reasoning traces produced by GPT-4.1, which distill and synthesize the key information needed to answer questions. This creates a more focused and relevant retrieval space that better matches the reasoning requirements of the questions. The modular pipeline design enables continuous updates as new scientific literature becomes available, ensuring benchmarks remain current with rapid knowledge growth. The quality filtering using LLM judges helps maintain benchmark standards despite the automated generation process, while preserving provenance links maintains traceability to source materials.

## Foundational Learning

1. **MCQA Benchmark Generation** - Why needed: Essential for creating domain-specific evaluation datasets for small model adaptation. Quick check: Can the pipeline generate valid MCQs with answer options and explanations from scientific PDFs?

2. **Reasoning Trace Distillation** - Why needed: Provides a method to extract and utilize the reasoning capabilities of large models for improving smaller model performance. Quick check: Do reasoning traces capture relevant information that improves retrieval quality compared to raw chunks?

3. **Retrieval-Augmented Generation (RAG)** - Why needed: Enables small models to access external knowledge without increasing model size. Quick check: Does RAG improve performance over baseline non-retrieval approaches across different model scales?

4. **Semantic Chunking and Search** - Why needed: Allows efficient organization and retrieval of information from large document collections. Quick check: Can the semantic search effectively retrieve relevant passages given domain-specific scientific terminology?

5. **Quality Filtering with LLM Judges** - Why needed: Maintains benchmark quality in automated generation pipelines. Quick check: Do filtered questions show better performance and validity compared to unfiltered outputs?

6. **Domain Adaptation via Retrieval** - Why needed: Addresses the challenge of adapting models to specialized scientific domains without extensive fine-tuning. Quick check: Does retrieval from reasoning traces improve domain-specific performance compared to general training?

## Architecture Onboarding

**Component Map:**
PDF Parsing -> Semantic Chunking -> Question Generation -> Quality Filtering -> Expert Review -> Benchmark Storage

**Critical Path:**
Question Generation -> Quality Filtering -> Retrieval Evaluation -> Model Testing

**Design Tradeoffs:**
The pipeline prioritizes scalability and automation over manual curation, trading some quality control for the ability to generate large benchmarks quickly. Using reasoning traces as retrieval sources adds computational overhead but provides more relevant context than raw chunks. The choice of multiple model sizes allows evaluation of approach effectiveness across the small model spectrum, though larger models may show diminishing returns from retrieval augmentation.

**Failure Signatures:**
Poor question quality from generation phase manifests as low consistency scores during filtering. Retrieval failures appear as degraded performance compared to baseline, particularly for smaller models. Generation failures occur when PDF parsing cannot properly extract text or when semantic chunking breaks context inappropriately. Quality filtering may be too stringent, reducing benchmark size, or too lenient, allowing low-quality questions through.

**3 First Experiments:**
1. Generate a small test set of 100 questions from a single paper to validate the complete pipeline workflow
2. Compare retrieval quality (recall@5) between reasoning-trace retrieval and chunk-based retrieval on a subset of questions
3. Test a single small model (1.1B) across all three conditions (baseline, chunk RAG, reasoning-trace RAG) to establish baseline performance differentials

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Synthetic benchmark nature without extensive expert validation for most generated questions
- Single reasoning trace generation run used for retrieval augmentation without examining stability
- Domain specificity to radiation and cancer biology limits generalizability to other fields
- No detailed analysis of retrieval quality metrics such as recall rates or relevance scoring

## Confidence

- Automated benchmark generation pipeline effectiveness: Medium
- Reasoning trace retrieval superiority: High
- Domain adaptation of small models: Medium
- Scalability claims: High

## Next Checks
1. Conduct expert annotation studies on a random sample of generated MCQs to assess question quality, validity, and difficulty calibration, establishing ground truth for benchmark reliability.

2. Perform ablation studies varying the quality and quantity of reasoning traces used for retrieval, including comparisons with multiple generation runs and alternative reasoning distillation approaches.

3. Extend evaluation to multiple scientific domains beyond radiation and cancer biology to assess the generalizability of reasoning trace retrieval effectiveness across different knowledge domains and question types.