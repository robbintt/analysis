---
ver: rpa2
title: Lost without translation -- Can transformer (language models) understand mood
  states?
arxiv_id: '2512.00274'
source_url: https://arxiv.org/abs/2512.00274
tags:
- been
- depression
- feeling
- euphoric
- euthymic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformer models fail to encode mood states expressed in Indic
  languages, achieving near-zero clustering performance (Composite Score = 0.002).
  All translation-based approaches significantly improved performance, with the highest
  accuracy (84%) achieved when human-translated English text was further translated
  to Chinese and embedded using Qwen-3-embedding-8b (Composite = 0.67).
---

# Lost without translation -- Can transformer (language models) understand mood states?

## Quick Facts
- arXiv ID: 2512.00274
- Source URL: https://arxiv.org/abs/2512.00274
- Reference count: 40
- Direct embedding of Indic-language mood idioms yields near-zero clustering performance (Composite Score = 0.002)

## Executive Summary
Transformer models completely fail to encode mood states expressed in Indic languages, achieving near-zero clustering performance. Translation-based approaches significantly improve performance, with human-translated English text further translated to Chinese achieving 84% accuracy. This reveals a fundamental barrier to AI-based psychiatric applications in India, as current models cannot meaningfully represent mental health idioms of distress in local languages. High-quality translation bridges this gap, but reliance on proprietary models is unsustainable for equitable global mental health implementation.

## Method Summary
The study evaluated 247 mood-state phrases across 11 Indic languages using k-means clustering (k=4) to group phrases by depression, euthymia, euphoric mania, and dysphoric mania. Seven translation and embedding conditions were tested, ranging from direct native/romanized script embedding to cascaded Indic→English→Chinese translation with Qwen embeddings. Performance was measured using a composite score combining ARI (50%), NMI (40%), Homogeneity (5%), and Completeness (5%). Translation quality emerged as the dominant factor, with direct Indic embedding yielding composite scores near zero while translated text achieved substantially higher performance.

## Key Results
- Direct embedding of Indic-language mood phrases yields near-zero clustering performance (Composite Score = 0.002)
- Translation to English significantly improves performance, with Gemini-translated text achieving Composite = 0.60
- Cascaded translation (human English→Chinese) with Qwen embeddings achieves highest accuracy (84%, Composite = 0.67)
- Specialized Indic models (IndicBERT, Sarvam-M) perform poorly on native script idioms of distress
- Higher-dimensional embeddings (3072 vs 768) consistently improve performance across conditions

## Why This Works (Mechanism)

### Mechanism 1: Translation as Semantic Bridge to High-Resource Representations
Translation activates well-trained semantic spaces in high-resource language models, enabling meaningful mood-state clustering. When Indic idioms are translated to English or Chinese, embedding models trained on orders of magnitude more data can map phrases into semantic spaces where mood-state distinctions are well-represented. Direct Indic embedding yields Composite=0.002, while Gemini-translated English yields Composite=0.60.

### Mechanism 2: Training Corpus Domain Mismatch in Indic-Specific Models
Specialized Indic models fail because their training corpora consist of formal text sources lacking vernacular, emotionally-rich expressions used in mental health contexts. Models trained on Wikipedia and news articles learn statistical patterns of formal language, while idioms of distress ("mann bezaar hai" — "my heart is weary") rely on informal, culturally-embedded metaphors absent from these corpora.

### Mechanism 3: Embedding Dimension and Model Capacity for Nuanced Affect Representation
Higher-dimensional embeddings (3072 vs 768) provide greater representational capacity to capture subtle distinctions between related mood states. Gemini-001 (3072 dimensions) consistently outperformed sentence transformers (768 dimensions) across comparable conditions, suggesting additional dimensions encode finer-grained affective features that distinguish mood states sharing surface-level vocabulary.

## Foundational Learning

- Concept: **Semantic Embeddings and Vector Space Structure**
  - Why needed here: The methodology depends on k-means clustering assuming semantically similar phrases cluster together in vector space
  - Quick check question: Given two sentences—"I feel heavy inside" and "My heart is weary"—should their embeddings be closer to each other than to "I feel fantastic"? Explain why or why not.

- Concept: **K-Means Clustering Evaluation Metrics (ARI, NMI)**
  - Why needed here: The paper's composite score is built from ARI (50%), NMI (40%), Homogeneity (5%), Completeness (5%)
  - Quick check question: If a model randomly assigns phrases to four clusters, what ARI value would you expect? What does ARI=0.66 indicate about the human→Chinese translation condition?

- Concept: **Idioms of Distress in Cross-Cultural Psychiatry**
  - Why needed here: Patients express psychological distress through culturally-specific phrases rather than clinical terminology
  - Quick check question: Why might a literal translation of an idiom of distress fail to preserve its diagnostic significance?

## Architecture Onboarding

- Component map: Input (Phrase collection) → Translation Module (Options: none, Sarvam MT, Sarvam-M LLM, Gemini-2.5-Pro, human, cascaded) → Embedding Module (gemini-001, sentence-transformers, IndicBERT, Qwen-3-embedding-8b) → K-means clustering (k=4) → Evaluation (Composite score)

- Critical path: Input → Translation choice → Embedding model selection → K-means clustering → Metric computation. Translation quality dominates performance, with embedding model as secondary factor.

- Design tradeoffs:
  - Accuracy vs. Accessibility: Highest accuracy (84%) requires proprietary Gemini or complex cascaded pipelines; open-source models achieve ~31% accuracy
  - Speed vs. Quality: Human translation outperforms machine translation slightly but is impractical at scale
  - Dimension vs. Compute: Higher-dimensional embeddings (3072) improve performance but increase storage and computation costs

- Failure signatures:
  - Composite score near 0.0 indicates embeddings fail to distinguish mood states
  - ARI near 0.0 indicates clustering is no better than random assignment
  - All phrases assigned to one cluster indicates collapsed representations

- First 3 experiments:
  1. **Reproduce the translation gap**: Embed the 247-phrase dataset directly with paraphrase-multilingual-MiniLM, run k-means with k=4, compute ARI. Expect near-zero performance.
  2. **Test translation quality hypothesis**: Compare three translation paths (Gemini, Sarvam, human) on 50 phrases and measure clustering performance to quantify the translation quality gap.
  3. **Embedding dimension ablation**: Using the same translated text, compare gemini-001 (3072-dim) vs. sentence-transformers (768-dim) embeddings. Project gemini embeddings to 768 dimensions via PCA and re-test to isolate dimension effects.

## Open Questions the Paper Calls Out

### Open Question 1
Can open-source translation models be fine-tuned or developed to match proprietary model performance for preserving mood-state semantics in Indic languages? The authors note that open-source Indic translation models "failed to provide adequate semantic preservation" and reliance on proprietary technology presents challenges for building scalable, equitable systems. This remains unresolved as the paper only tested existing models without fine-tuning.

### Open Question 2
Why does the Indic→English→Chinese translation pipeline outperform direct Indic→English embedding (Composite 0.67 vs. 0.61) for mood state clustering? The authors call this "surprising" and hypothesize it "combined high-fidelity human translation... with a model (Qwen) that has a native, deep understanding of the Chinese language," but this remains speculative. Systematic testing of multiple target languages is needed.

### Open Question 3
What minimum dataset characteristics (size, vernacular content, idiomatic diversity) would enable direct Indic-language mood encoding without translation? The authors note that Indic training corpora from "Wikipedia and online news articles... lack the real-world, vernacular, and emotionally rich data required to represent nuances in idioms of distress." This remains unresolved as the study did not collect or test a vernacular, emotionally-rich Indic corpus.

## Limitations
- The study's findings are based on a specific 247-phrase corpus representing a narrow slice of possible emotional expressions across 11 Indic languages
- Translation-based solutions introduce dependency on proprietary models (Gemini, Qwen) that may not be accessible in low-resource settings
- The cascaded translation pipeline achieving 84% accuracy relies on infrastructure unavailable in most Indian mental health contexts

## Confidence

**High Confidence**: The empirical finding that direct embedding of Indic-language mood idioms yields near-zero clustering performance (Composite = 0.002) is robust and reproducible across multiple model architectures and scripts.

**Medium Confidence**: The proposed mechanisms explaining why translation bridges the semantic gap are plausible but not definitively proven. The paper shows correlation between translation and performance improvement but cannot conclusively distinguish between training corpus effects, embedding dimension effects, and other confounding factors.

**Low Confidence**: Claims about the specific superiority of Chinese embeddings over English embeddings (despite similar performance) lack mechanistic explanation. The cascaded translation pipeline's effectiveness, while demonstrated, has unclear practical implications given its complexity.

## Next Checks

1. **Dataset Expansion and Generalization Test**: Replicate the study with a 10x larger corpus of mood-state phrases across the same 11 languages, including both clinical and vernacular sources. Test whether the translation performance gap persists when the dataset includes phrases from clinical interviews, social media, and traditional literature.

2. **Mechanism Isolation Experiment**: Conduct controlled experiments varying one factor at a time—test the same translated text with embeddings of identical dimensions but different architectures (3072-dim English vs 3072-dim Chinese), and test whether fine-tuning Indic models on translated text closes the performance gap.

3. **Practical Implementation Assessment**: Build a prototype clinical tool using the human→Chinese pipeline and evaluate its performance in a real-world setting with non-English-speaking patients. Measure both classification accuracy and clinical utility, including time-to-diagnosis and patient satisfaction metrics.