---
ver: rpa2
title: 'Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence
  Policy Optimization'
arxiv_id: '2510.22477'
source_url: https://arxiv.org/abs/2510.22477
tags:
- communication
- agent-gspo
- policy
- multi-agent
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agent-GSPO tackles communication inefficiency in multi-agent systems
  by optimizing for token economy through sequence-level reinforcement learning. It
  employs Group Sequence Policy Optimization (GSPO) to train agents on a reward function
  that balances task success against verbosity penalties.
---

# Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization

## Quick Facts
- arXiv ID: 2510.22477
- Source URL: https://arxiv.org/abs/2510.22477
- Reference count: 0
- Primary result: Achieves 96.02% accuracy on GSM8K while reducing communication tokens by 70% compared to prior methods

## Executive Summary
Agent-GSPO addresses communication inefficiency in multi-agent systems through sequence-level reinforcement learning that optimizes for both task success and communication economy. The framework employs Group Sequence Policy Optimization (GSPO) to train agents on a reward function that balances task accuracy against verbosity penalties, achieving state-of-the-art performance across seven reasoning benchmarks while dramatically reducing token consumption. The method demonstrates that agents can learn to communicate strategically, sometimes choosing "strategic silence" when information sharing is not valuable.

## Method Summary
Agent-GSPO optimizes multi-agent communication efficiency through sequence-level policy optimization using GSPO. The method trains agents with a composite reward that combines task accuracy with penalties for verbosity, turns, and repetition. GSPO stabilizes training by operating on groups of entire response sequences rather than individual tokens, using sequence-level importance sampling ratios normalized by sequence length. A dual budget constraint dynamically adjusts the verbosity penalty to meet target token budgets. The framework enables emergent communication strategies where agents learn when to share information and when to remain silent.

## Key Results
- Achieves 96.02% accuracy on GSM8K using only 7.2M tokens versus 22-26M for competing methods
- Reduces communication tokens by up to 70% while maintaining or improving task performance
- Sets new state-of-the-art accuracy on seven reasoning benchmarks (MMLU, GSM8K, MultiArith, SVAMP, AQuA, MATH-500, HumanEval)
- Demonstrates that communication-aware rewards induce agents to learn concise strategies without explicit hard-coding

## Why This Works (Mechanism)

### Mechanism 1: Sequence-Level Policy Optimization via GSPO
Operating at sequence level rather than token level stabilizes training for multi-agent dialogue by reducing variance in importance sampling ratios. GSPO computes clipped importance sampling ratio over entire response sequences, then normalizes by sequence length: `s(θ) = (πθ(y|x)/πθ_old(y|x))^(1/|y|)`. Group-wise reward normalization estimates advantages directly without a critic network. The paper assumes, but does not formally prove, that sequence-level clipping prevents large-variance updates from token-level probability ratios, especially for long sequences.

### Mechanism 2: Communication-Aware Composite Reward
Explicitly penalizing verbosity in the reward signal induces agents to learn concise communication strategies while maintaining task performance. The reward `r(x,y) = r_task(x,y) - λ_tok·tokens(y) - λ_turn·turns(y) - λ_rep·repetition(y)` creates direct trade-off. The dual budget constraint dynamically adjusts `λ_tok` via projected gradient ascent: `λ_tok ← max(0, λ_tok + η·(tokens - B))`. The paper assumes task success and communication efficiency are not fundamentally adversarial, and there exist policies that achieve both.

### Mechanism 3: Emergent Strategic Silence
Through optimization of the communication-aware reward, agents develop the ability to withhold redundant information ("strategic silence") without explicit hard-coding. As `λ_tok` increases, the policy gradient pushes probability mass away from verbose outputs toward concise summaries, keywords, or abstains from communicating altogether. The paper assumes the policy network can learn to estimate information value and relevance implicitly through reward signals.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: GSPO modifies PPO's clipping mechanism from token-level to sequence-level; understanding PPO is prerequisite.
  - Quick check question: Can you explain why PPO uses a clipped importance sampling ratio and what problem this solves?

- **Concept: Importance Sampling in RL**
  - Why needed here: GSPO relies on importance sampling ratios normalized by sequence length to reuse samples from old policies.
  - Quick check question: Why does GSPO normalize the importance ratio by `1/|y|` rather than using raw sequence probabilities?

- **Concept: Multi-Agent Communication Protocols**
  - Why needed here: The paper positions itself against "free-for-all" protocols; understanding MAS communication patterns is essential context.
  - Quick check question: What are the failure modes of unstructured multi-agent dialogue that motivate communication constraints?

## Architecture Onboarding

- **Component map:**
  Input Query → Actor Network (π_θ) → Sample G candidate responses → Reward Function: r_task - λ_tok·tokens - λ_turn·turns - λ_rep·rep → GSPO Module: Group normalization → Sequence-level importance ratios → Clipped objective → Policy Update: θ ← θ + α∇_θ J_GSPO → (Optional) Dual Budget: Update λ_tok if tokens ≠ B

- **Critical path:**
  1. Initialize policy π_θ and frozen old policy π_θ_old
  2. For each query, sample G responses from π_θ_old (paper uses unspecified G value—check appendix)
  3. Compute rewards for all G candidates
  4. Group-normalize advantages (critical for stability—ablation shows ↓2.9 points without this)
  5. Update π_θ via GSPO objective
  6. Periodically sync π_θ_old ← π_θ

- **Design tradeoffs:**
  - **Group size G:** Larger G improves advantage estimation but increases compute; paper doesn't specify optimal values.
  - **λ_tok magnitude:** Higher values increase efficiency but risk under-communication; ablation shows accuracy drops when penalty removed entirely.
  - **Budget B:** Target token budget shapes agent behavior; too aggressive may prevent necessary coordination.

- **Failure signatures:**
  - Token consumption doesn't decrease → λ_tok may be too low or dual update learning rate η too small
  - Accuracy drops significantly → λ_tok too aggressive; check if agents are skipping critical reasoning steps
  - Training instability → Verify group normalization is enabled; check sequence-length normalization in importance ratio
  - No emergent silence → Base model may have strong verbosity bias; consider warmup period before applying full penalty

- **First 3 experiments:**
  1. **Baseline sanity check:** Run Agent-GSPO with λ_tok = 0 on a single benchmark (e.g., MMLU); verify you reproduce the ablation result (~82.5% accuracy, ~25M tokens) to validate implementation.
  2. **Hyperparameter sweep:** Vary λ_tok across {0.01, 0.05, 0.1, 0.5} on GSM8K; plot accuracy vs. token consumption to find the Pareto frontier before committing to dual budget constraints.
  3. **Group size ablation:** Test G ∈ {2, 4, 8, 16} on a held-out validation set; measure both final accuracy and training stability (variance of loss across steps) to determine minimum viable group size.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does Agent-GSPO's efficiency transfer to smaller, open-source models with less inherent reasoning capability than GPT-4?
- **Basis in paper:** The experimental setup states that "all experiments leverage gpt-4-1106-preview as the base model for all agents," leaving the efficacy on weaker models unverified.
- **Why unresolved:** Smaller models may lack the capacity to simultaneously maintain high reasoning accuracy and adhere to the complex conciseness constraints imposed by the GSPO reward function, potentially leading to performance collapse.
- **What evidence would resolve it:** Evaluation of Agent-GSPO on open-source models (e.g., Llama-3-8B or Mistral-7B) to compare the accuracy-efficiency trade-off curve against the GPT-4 baseline.

### Open Question 2
- **Question:** Does the "strategic silence" strategy remain effective or degenerate into coordination failure as the number of agents increases significantly?
- **Basis in paper:** The paper claims the framework provides a "blueprint for developing scalable... systems," but the experiments rely on standard MAS baselines which typically utilize a small, fixed number of agents.
- **Why unresolved:** In larger groups (e.g., 10+ agents), the verbosity penalty might overly discourage necessary information sharing, causing agents to remain silent even when they possess unique, critical data, thereby lowering task success rates.
- **What evidence would resolve it:** A scaling analysis measuring token consumption and accuracy while systematically varying the number of agents (N) from 3 to 20.

### Open Question 3
- **Question:** Is the proposed dual budget constraint (adaptive penalty) superior to a carefully tuned static penalty?
- **Basis in paper:** While Section 3.3 introduces a "Dual Budget Constraint" to dynamically adjust $\lambda_{tok}$, the ablation study only compares the full model against a "w/o Cost Penalty" baseline, without isolating the contribution of the adaptive mechanism.
- **Why unresolved:** It is unclear if the dynamic adjustment mechanism is necessary for optimal performance or if a fixed penalty coefficient suffices, which would simplify the training pipeline.
- **What evidence would resolve it:** An ablation study comparing the dual-update mechanism against a grid-searched static $\lambda_{tok}$ in terms of convergence speed and adherence to the target token budget.

### Open Question 4
- **Question:** How does the verbosity penalty impact performance on open-ended tasks where "conciseness" may conflict with nuance or completeness?
- **Basis in paper:** The method is evaluated exclusively on reasoning benchmarks (Math, Code, MMLU) which utilize verifiable rewards like "exact match" or "pass@1."
- **Why unresolved:** In tasks requiring long-form generation (e.g., creative writing or summarization), the token penalty might incentivize "strategic silence" that results in the omission of essential details rather than the removal of low-value chatter.
- **What evidence would resolve it:** Evaluation on open-ended generation benchmarks using LLM-as-a-judge to score the trade-off between information completeness and token economy.

## Limitations
- The paper lacks critical implementation details including exact hyperparameters (group size G, learning rates, baseline values), the specific multi-agent architecture used, and training dataset specifications.
- The emergent "strategic silence" mechanism is theoretically appealing but lacks direct validation—we cannot verify whether agents truly learned to withhold information strategically versus simply becoming less verbose through optimization pressure.
- The claimed 70% token reduction represents best-case performance that may not generalize across domains or agent configurations.

## Confidence
- **High Confidence**: GSPO's sequence-level optimization improves training stability compared to token-level approaches (supported by theoretical arguments and indirect evidence from related work)
- **Medium Confidence**: The composite reward function successfully induces more efficient communication (supported by ablation showing token reduction when λ_tok > 0, though task-accuracy trade-off is data-limited)
- **Low Confidence**: The emergent strategic silence represents a genuinely novel capability rather than a side effect of aggressive optimization (no direct validation or mechanistic explanation provided)

## Next Checks
1. **Mechanism Validation**: Run controlled experiments varying sequence lengths from 10 to 2000 tokens to empirically measure variance reduction claims. Compare training stability metrics (loss variance, KL divergence between policy updates) between GSPO and token-level approaches across this range.

2. **Ablation Study Replication**: Implement the exact ablation experiments described in Section 4.3—remove group normalization, disable communication penalties, and test with/without dual budget constraints on GSM8K. Verify the claimed 2.9-point accuracy drop without group normalization and the 3× token increase without penalties.

3. **Generalization Test**: Apply Agent-GSPO to a different multi-agent reasoning task not in the original seven benchmarks (e.g., commonsense reasoning or code generation with multiple agents). Measure whether the 70% token reduction and accuracy improvements transfer, or if performance degrades significantly in new domains.