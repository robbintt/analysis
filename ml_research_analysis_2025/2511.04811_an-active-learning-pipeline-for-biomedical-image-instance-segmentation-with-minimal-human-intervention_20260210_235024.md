---
ver: rpa2
title: An Active Learning Pipeline for Biomedical Image Instance Segmentation with
  Minimal Human Intervention
arxiv_id: '2511.04811'
source_url: https://arxiv.org/abs/2511.04811
tags:
- core-set
- segmentation
- nnu-net
- image
- selected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an active learning pipeline that addresses
  the challenge of biomedical image segmentation when labeled data is scarce. The
  method combines pseudo-labeling from foundation models (CellSAM) with core-set selection
  for efficient annotation and fine-tuning of nnU-Net.
---

# An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention

## Quick Facts
- arXiv ID: 2511.04811
- Source URL: https://arxiv.org/abs/2511.04811
- Reference count: 16
- One-line primary result: Achieved 90% of full annotation performance using only 6.25-12.5% of manual labels for mitochondria segmentation

## Executive Summary
This paper presents an active learning pipeline that addresses the challenge of biomedical image segmentation when labeled data is scarce. The method combines pseudo-labeling from foundation models (CellSAM) with core-set selection for efficient annotation and fine-tuning of nnU-Net. The pipeline first generates pseudo-labels from CellSAM for unlabeled images, uses them for nnU-Net pre-training, then selects a representative core-set using features from a self-supervised MAE model. This core-set undergoes minimal manual annotation with microSAM assistance before fine-tuning the pre-trained nnU-Net. When evaluated on the 3D MitoEM dataset for mitochondria segmentation, the approach achieved 90% of full annotation performance using only 6.25-12.5% of manual labels (64-128 samples).

## Method Summary
The pipeline employs a two-stage transfer learning approach for biomedical image instance segmentation. First, CellSAM generates pseudo-labels for unlabeled images, which are used to pre-train nnU-Net and determine its self-configuration parameters. Second, a self-supervised MAE model extracts features from all images, enabling core-set selection through a greedy algorithm that maximizes cosine distance diversity. The selected core-set undergoes minimal manual annotation with microSAM assistance before fine-tuning the pre-trained nnU-Net. This approach reduces annotation burden while maintaining competitive segmentation quality, achieving 90% of full annotation performance with only 6.25-12.5% of labeled data.

## Key Results
- Achieved 90% of full annotation performance using only 6.25-12.5% of manual labels (64-128 samples)
- Core-set selection outperformed random selection, achieving F1=0.6003 vs. F1=0.5773 with 12.5% labels
- Pre-training on pseudo-labels provided 2-4% performance boost across all metrics compared to training from scratch
- The pipeline successfully segmented 3D mitochondria instances from EM images with competitive quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-label pre-training enables nnU-Net self-configuration without ground truth labels
- Mechanism: CellSAM generates approximate segmentation labels for unlabeled images, which nnU-Net uses to determine optimal architecture configuration (patch size, batch size, network topology) and initialize weights. This bypasses nnU-Net's requirement for labeled data during its empirical configuration phase.
- Core assumption: Foundation model pseudo-labels contain sufficient structural signal to guide architecture search, even if boundary precision is imperfect.
- Evidence anchors:
  - [abstract] "The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net's self-configuration"
  - [Page 2, Section 2.1] "Pseudo-labels L_p were generated from CellSAM [4] for unlabeled raw images D. Then D and L_p formed a train set for nnU-Net M_p"
  - [Page 5, Table 2] Shows pre-trained models (w/) consistently outperform non-pretrained (w/o) across all metrics

### Mechanism 2
- Claim: Greedy core-set selection in MAE latent space yields more representative annotation subsets than random sampling
- Mechanism: A self-supervised MAE encoder maps all images to feature vectors. The algorithm initializes with k=3 random samples, then iteratively selects the unlabeled sample with maximum minimum cosine distance to the current core-set—ensuring each new sample covers maximally distinct visual characteristics.
- Core assumption: MAE features capture semantic diversity relevant to segmentation difficulty; distance in latent space correlates with informativeness for the downstream task.
- Evidence anchors:
  - [Page 2, Section 2.1] "According to d = 1 - E·E^T, a cosine distance matrix d was computed... the sample S in D_u with the maximum d_min was selected"
  - [Page 5, Table 2] Core-set selection achieves F1=0.6003 vs. Random's F1=0.5773 with pre-training (12.5% labels)
  - [Page 5, Figure 4] UMAP visualization shows selected patches span diverse regions of latent space

### Mechanism 3
- Claim: Two-stage transfer (pseudo-label pre-training → manual fine-tuning) compounds gains from both weak and strong supervision
- Mechanism: Stage 1 provides broad coverage and architectural initialization; Stage 2 corrects systematic errors through targeted human annotation. The pre-trained model converges faster and generalizes better from limited manual labels.
- Core assumption: Errors in pseudo-labels are partially correctable through fine-tuning on a small but representative manually-labeled subset.
- Evidence anchors:
  - [Page 4, Table 1] F1 score jumps from 0.4025 (core-set=0, pseudo-labels only) to 0.6003 (core-set=128, fine-tuned)
  - [Page 4, Figure 2] Visual comparison shows pre-training detects mitochondria roughly (c), fine-tuning refines boundaries (d,e)
  - [Page 4-5] "Fine-tuning with the selected core-set is effective... At about 12.5% annotations, the model achieves 90% of the full annotation performance"

## Foundational Learning

- **Active Learning (Core-set Selection)**
  - Why needed here: The pipeline must choose which ~100 images to annotate from ~1000+ candidates. Random selection is inefficient; active learning theory provides principled selection criteria.
  - Quick check question: Can you explain why maximizing minimum distance to existing samples promotes diversity? (Answer: Each new sample covers a region of feature space not yet represented.)

- **Self-Supervised Learning (Masked Autoencoders)**
  - Why needed here: No labels exist for feature extraction. MAE learns representations by reconstructing masked image patches, producing features usable for core-set selection without any manual annotation.
  - Quick check question: Why can't we use the nnU-Net encoder directly for features? (Answer: nnU-Net requires labels for training; MAE trains on raw images alone.)

- **Transfer Learning / Fine-tuning Paradigm**
  - Why needed here: The pipeline depends on pre-training on noisy pseudo-labels transferring beneficial inductive biases to the fine-tuning stage.
  - Quick check question: What could cause negative transfer in this setup? (Answer: If pseudo-labels encode dataset-specific artifacts or wrong class boundaries that conflict with ground truth.)

## Architecture Onboarding

- **Component map:**
Raw Images (D)
    ├─→ CellSAM → Pseudo-labels (L_p) → nnU-Net Pre-training (M_p)
    │                                      │
    └─→ MAE (self-supervised) → Features (E) → Core-set Selection → 
                                                              ↓
                                            Manual Annotation (microSAM assist)
                                                              ↓
                                         Fine-tuning (M_p → M_f)
                                                              ↓
                                         Instance Segmentation (+ Connected Components)

- **Critical path:**
  1. CellSAM pseudo-label generation (Stage 1 bottleneck—quality affects all downstream)
  2. MAE feature extraction (requires 400 epochs training on raw images)
  3. Core-set selection (greedy O(n²) distance computation)
  4. Manual annotation with microSAM (human-in-loop)
  5. nnU-Net fine-tuning (automated via nnU-Net framework)

- **Design tradeoffs:**
  - Core-set size: Smaller = less annotation, but diminishing returns below 64 samples (Table 1 shows non-linear scaling)
  - k initialization: Paper uses k=3; larger k may improve diversity but reduces greedy optimization impact
  - MAE mask ratio: Paper uses 0.75; higher ratios force more global reasoning but may lose fine detail
  - Foundation model choice: CellSAM chosen for cells; other domains may need different models (medSAM, SAM2)

- **Failure signatures:**
  - Pre-trained model detects wrong structures → pseudo-labels systematically mislabeled (check CellSAM output visually on sample images)
  - Core-set selection picks visually similar images → MAE features not discriminative (check UMAP visualization for clustering)
  - Fine-tuning doesn't improve over pre-training → core-set not representative or annotation quality issues
  - Large gap between core-set and full-label performance → increase core-set size or check for domain shift

- **First 3 experiments:**
  1. **Validate pseudo-label quality**: Generate CellSAM labels for 10 random images; compare visually against manual annotations to estimate error types (boundary errors vs. false negatives vs. wrong objects)
  2. **Ablate core-set size**: Run pipeline with core-set sizes [16, 32, 64, 128] to establish scaling curve for your dataset; compare against Table 1 patterns
  3. **Compare selection strategies**: Benchmark core-set selection vs. random vs. uncertainty-based selection on a held-out validation set to validate MAE feature quality for your imaging modality

## Open Questions the Paper Calls Out
- **Open Question 1**: How can this pipeline be extended to support continuous learning for dynamic datasets?
  - Basis in paper: [explicit] The authors state in the conclusion, "Future work will enable continuous learning for dynamic datasets."
  - Why unresolved: The current study validates the method on a static dataset (MitoEM) and does not address the mechanisms required to update the model as new data arrives without suffering from catastrophic forgetting.
  - Evidence: Evaluation on a streaming data environment showing performance retention and adaptation over time.

- **Open Question 2**: Does the pipeline generalize to other imaging modalities, such as light microscopy or MRI, with different noise profiles?
  - Basis in paper: [inferred] The validation is limited to a single 3D Electron Microscopy dataset (MitoEM).
  - Why unresolved: The specific hyperparameters (core-set size ratios) and feature extractors (MAE) may be optimized for the high-contrast textures of EM, potentially failing on different contrast mechanisms found in other biomedical domains.
  - Evidence: Successful replication of the 90% efficiency benchmark on diverse biomedical datasets (e.g., fluorescence microscopy).

- **Open Question 3**: Is the pre-training phase robust if the foundation model (CellSAM) produces severely erroneous pseudo-labels?
  - Basis in paper: [inferred] The paper notes foundation models "struggle with specific datasets," yet relies on their pseudo-labels for nnU-Net initialization.
  - Why unresolved: If initial pseudo-labels are highly inaccurate (e.g., containing systematic false negatives), the nnU-Net self-configuration might converge to a poor local minimum that minimal fine-tuning cannot correct.
  - Evidence: Ablation studies on datasets where the foundation model's zero-shot performance is significantly lower than on MitoEM.

## Limitations
- **Pseudo-label quality dependency**: The pipeline's performance critically depends on foundation model accuracy, with domain shift potentially reducing effectiveness substantially.
- **Core-set selection scalability**: The greedy O(n²) distance computation becomes prohibitive for datasets with millions of images, and the k=3 initialization appears arbitrary.
- **Annotation efficiency assumptions**: microSAM assistance effectiveness is not quantified relative to traditional annotation tools, and the annotation time savings vs. quality trade-offs remain unmeasured.

## Confidence
- **High confidence**: The two-stage transfer learning mechanism (pre-training → fine-tuning) is well-supported by ablation studies showing consistent performance gains.
- **Medium confidence**: Claims about achieving 90% of full annotation performance at 12.5% label usage are dataset-specific and may not generalize across imaging modalities.
- **Low confidence**: Generalization to other biomedical domains remains speculative, and the computational efficiency claims lack rigorous benchmarking against established active learning methods.

## Next Checks
1. **Cross-domain foundation model testing**: Apply the pipeline to a different biomedical imaging dataset (e.g., neuronal segmentation) using an appropriate foundation model and measure performance degradation relative to MitoEM results.
2. **Core-set selection ablation**: Compare the MAE-based core-set selection against random sampling and uncertainty-based sampling on the MitoEM dataset using identical annotation budgets and compute resources.
3. **Annotation efficiency quantification**: Measure the actual time and accuracy of microSAM-assisted annotation versus standard polygon/brush tools for the same core-set patches, including inter-annotator agreement metrics.