---
ver: rpa2
title: 'The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors
  with Random Data Weights'
arxiv_id: '2512.10188'
source_url: https://arxiv.org/abs/2512.10188
tags:
- lemma
- gradient
- linear
- random
- slin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies gradient descent with randomly weighted data
  points in a linear regression model. The authors analyze how different random weighting
  distributions affect the convergence behavior and statistical properties of the
  algorithm.
---

# The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors with Random Data Weights

## Quick Facts
- arXiv ID: 2512.10188
- Source URL: https://arxiv.org/abs/2512.10188
- Authors: Gabriel Clara; Yazan Mash'al
- Reference count: 40
- Key outcome: This paper studies gradient descent with randomly weighted data points in a linear regression model, deriving non-asymptotic bounds for convergence and characterizing stationary distributions under various weighting schemes.

## Executive Summary
This paper analyzes stochastic gradient descent with randomly weighted data points in linear regression, showing how different weighting distributions affect both optimization convergence and statistical recovery of the true parameter. The authors establish that iterates converge to the minimum-norm solution of a weighted least squares problem, and under constant step sizes, the system admits a unique stationary distribution. A key finding is the tension between optimization speed and statistical accuracy: weighting schemes that accelerate convergence may lead to poor statistical recovery if they down-weight informative data points. The analysis applies broadly to importance sampling, robustness techniques, and various stochastic optimization algorithms.

## Method Summary
The method analyzes gradient descent updates of the form ŵ_{k+1} = (I - α_k·X^t D²_k X)ŵ_k + α_k·X^t D²_k Y where D_k are random diagonal weighting matrices sampled independently at each iteration. The analysis proceeds by marginalizing over the random weights to study the deterministic evolution of expected iterates, establishing convergence to a weighted least squares solution. Under constant step sizes, the paper proves the existence of a unique stationary distribution using Geometric Moment Contraction (GMC) arguments. The framework accommodates arbitrary continuous weighting distributions and provides non-asymptotic bounds for both first and second moment convergence.

## Key Results
- Expected iterates converge to the minimum-norm solution of the expected weighted least squares problem
- Under constant step sizes, iterates converge to a unique stationary distribution with explicit bounds on Wasserstein distance
- Asymptotic statistical recovery depends primarily on the quality of the weighted least squares estimator, creating tension between optimization speed and statistical accuracy
- Different weighting schemes (importance sampling vs uniform) can dramatically affect both convergence rates and final estimation error

## Why This Works (Mechanism)

### Mechanism 1
The expected value of randomly weighted iterates converges to the minimum-norm solution of the weighted least squares problem. By taking expectation over the random weighting matrices D_k, the stochastic affine recursion transforms into a deterministic linear dynamical system that contracts toward the weighted solution at a rate determined by the smallest non-zero singular value of the expected weighted Hessian. This requires step sizes satisfying α_ℓ · ||X̄|| < 1 where X̄ = X^t M₂ X and M₂ = E[D²].

### Mechanism 2
Under constant step sizes, iterates do not converge to a fixed point but instead diffuse into a unique stationary distribution. The random weightings prevent variance from collapsing to zero, but the system satisfies Geometric Moment Contraction (GMC), meaning the distance between two trajectories using the same noise sequence contracts exponentially. This ensures existence of a unique stationary distribution reflecting the local geometry of the loss, requiring α τ² · ||X|| < 2 where τ bounds the noise support.

### Mechanism 3
Asymptotic statistical quality depends primarily on the quality of the underlying weighted least squares solution, creating tension between optimization speed and statistical accuracy. The iterates cluster around the weighted estimator w̄, so if the weighting scheme prioritizes low-quality data points for statistical recovery (e.g., high variance noise), the algorithm converges quickly to a statistically poor solution. This creates a fundamental tradeoff between fast optimization and statistically optimal estimation.

## Foundational Learning

- **Pseudo-inverse (X⁺) and Minimum Norm Solutions**: Essential for over-parameterized linear regression where standard inverses don't exist; the pseudo-inverse yields the solution with smallest Euclidean norm. Quick check: If matrix X has rank 1, does the pseudo-inverse solution minimize the residual, the norm of weights, or both?

- **Affine Recursion / Vector Auto-regressive (VAR) Process**: The error vector w̃_k - w̄ evolves via a linear transformation plus a random shift, crucial for understanding first/second moment convergence proofs. Quick check: In update rule z_{k+1} = A_k z_k + b_k, if E[A_k] < 1, does the variance of z_k necessarily go to zero?

- **Geometric Moment Contraction (GMC)**: Mathematical tool proving iterates have a unique stationary distribution by linking contraction rate between coupled trajectories to system stability. Quick check: Does GMC imply that the distribution of iterates forgets the initial condition w₁ as k → ∞?

## Architecture Onboarding

- **Component map**: Data matrix X, labels Y -> Random sampler D -> State w̃_k -> Update rule w̃_{k+1} = (I - α X^t D² X)w̃_k + α X^t D² Y

- **Critical path**: The definition of weighting distribution (specifically M₂ = E[D²]) determines the target solution w̄. The variance of weighting Σ_D determines the diffusion around that target.

- **Design tradeoffs**: Importance sampling based on gradient norms accelerates convergence but may bias solution if those points have high statistical noise. Discrete weighting (SGD) is computationally cheap; continuous weighting allows robust down-weighting of outliers but is computationally distinct.

- **Failure signatures**: Fast convergence to high error (training loss drops rapidly but distance to true parameter w* remains high), unbounded variance (constant step sizes too large causing iterates to explode).

- **First 3 experiments**: 1) Replicate "Bad" vs "Good" Weighting with synthetic data using weighting by data norm vs inverse noise variance. 2) Compare constant vs decaying step-size behavior to verify stationary distribution properties. 3) Vary weight variance Σ_D while fixing M₂ to confirm impact on stationary distribution radius.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the relationship between random weighting and weighted least squares extend to deep linear neural networks (matrix factorization)? The authors conjecture a "weighted nuclear norm minimal estimator" plays an analogous role but leave non-convex analysis to future work due to complex loss landscapes and non-affine second moment dynamics.

- **Open Question 2**: How does convergence analysis change with adaptively chosen random data weights based on previous iterations? Adaptive weighting violates i.i.d. assumptions, complicating the law of total covariance used in proofs. The authors note this is desirable but leave details to future work.

- **Open Question 3**: Does the dependence of stationary distribution on batch size in weight-tied auto-encoders extend to biased sampling schemes? The authors suggest this may hold based on related work but note the non-linear activation functions in auto-encoders invalidate their linear recursion methods.

## Limitations

- Analysis relies on strict stability conditions for step sizes relative to data norms and noise support that may be violated in practice
- Assumes i.i.d. sampling of weighting matrices, but practical implementations often use mini-batches or correlated sampling
- Focus on first and second moment convergence doesn't address potential bias or heavy-tailed behavior in iterates

## Confidence

- **High Confidence**: First moment convergence to weighted least squares solution - well-supported by deterministic analysis and explicit error bounds
- **Medium Confidence**: Stationary distribution characterization - proof relies on GMC property which is less commonly encountered in ML literature
- **Medium Confidence**: Statistical tension between optimization and estimation - theoretical bounds exist but empirical validation is limited

## Next Checks

1. **Robustness to Step Size Violations**: Systematically test convergence when step size exceeds theoretical bounds to identify exact failure thresholds and quantify algorithm robustness

2. **Generalization to Non-IID Data**: Extend experiments to include correlated features and non-uniform data distributions to assess real-world applicability beyond i.i.d. assumptions

3. **Heavy-Tailed Weighting Distributions**: Analyze convergence behavior under power-law distributed weights to understand algorithm robustness beyond compact support assumption and identify potential breakdown modes