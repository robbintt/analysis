---
ver: rpa2
title: Sensor-Adaptive Flood Mapping with Pre-trained Multi-Modal Transformers across
  SAR and Multispectral Modalities
arxiv_id: '2509.23035'
source_url: https://arxiv.org/abs/2509.23035
tags:
- flood
- data
- mapping
- sensor
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of rapid flood mapping when
  sensor availability is limited or delayed during disaster events. The authors propose
  a sensor-flexible flood detection method that leverages Presto, a lightweight pixel-based
  pre-trained transformer capable of processing either Sentinel-1 SAR data, Sentinel-2
  multispectral data, or both.
---

# Sensor-Adaptive Flood Mapping with Pre-trained Multi-Modal Transformers across SAR and Multispectral Modalities

## Quick Facts
- arXiv ID: 2509.23035
- Source URL: https://arxiv.org/abs/2509.23035
- Reference count: 6
- Primary result: F1 score of 0.896 and mIoU of 0.886 for sensor-fused flood detection

## Executive Summary
This study addresses the challenge of rapid flood mapping when sensor availability is limited or delayed during disaster events. The authors propose a sensor-flexible flood detection method that leverages Presto, a lightweight pixel-based pre-trained transformer capable of processing either Sentinel-1 SAR data, Sentinel-2 multispectral data, or both. By fine-tuning this model on the Sen1Floods11 dataset, they achieve strong performance across all sensor configurations while maintaining operational utility when only partial data is available.

## Method Summary
The approach fine-tunes a pre-trained pixel-based transformer (Presto) with a linear classification head on the Sen1Floods11 dataset, training only on complete 13-channel fused data (SAR+MS+NDVI). The model leverages channel-group masking from pre-training to enable inference with any sensor combination through a single architecture. Training uses focal loss with AdamW optimizer, and the lightweight 0.4M parameter model achieves sensor-adaptive flood mapping without requiring sensor-specific models.

## Key Results
- F1 score of 0.896 and mIoU of 0.886 for MS+SAR fusion scenario
- Strong single-sensor performance: F1=0.893 (MS-only) and F1=0.718 (SAR-only)
- Superior fine boundary delineation compared to large baseline (Prithvi-100M)
- Effective generalization to missing-sensor inference via pre-trained embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal pre-training with channel-group masking enables single-model inference across variable sensor availability
- Mechan: Systematic masking of entire sensor modalities during pre-training forces the encoder to learn shared representations that remain functional when specific channel groups are absent
- Core assumption: Pre-training on general EO tasks transfers to flood detection with sensor-agnostic representations
- Evidence anchors: Pre-training masks entire modalities to ensure robust performance; neighbor papers explore foundation models but not specific channel-masking transfer

### Mechanism 2
- Claim: Pixel-level processing with linear classification head preserves fine spatial boundaries compared to patch-based aggregation
- Mechan: Each pixel's 128-dimensional encoder output is independently mapped to flood probability via single linear layer, avoiding spatial smoothing from patch-based operations
- Core assumption: Pixel-wise independence acceptable for flood mapping; contextual information encoded in per-pixel feature vector
- Evidence anchors: Pixel-wise head applied to encoder output; MS+SAR and MS-only models more accurately delineated fine water extents; patch-based vs pixel-based tradeoffs underexplored in literature

### Mechanism 3
- Claim: Training on complete fused data generalizes to missing-sensor inference via pre-trained channel embeddings
- Mechan: Model fine-tuned only on full 13-channel inputs; during evaluation, channel masking removes unavailable sensor tokens while pre-trained embeddings maintain consistent feature space
- Core assumption: Embeddings for masked channels from pre-training remain semantically meaningful without fine-tuning on partial inputs
- Evidence anchors: Embedding layers for unused channel groups retain pre-trained weights; MS-only achieved F1=0.893 with minimal degradation; not directly validated in neighbor papers

## Foundational Learning

- **Transformer attention over heterogeneous channels**
  - Why needed here: Presto processes 15+ continuous channels plus categorical inputs; understanding how attention merges SAR backscatter, optical reflectance, and auxiliary variables is essential for debugging missing-sensor behavior
  - Quick check question: Can you explain why masking an input channel during inference doesn't change the output dimension of the encoder?

- **SAR speckle and backscatter interpretation**
  - Why needed here: SAR-only scenario underperforms partly due to speckle noise and low-backscatter confusion; interpreting these artifacts informs preprocessing and post-processing decisions
  - Quick check question: Why does SAR imagery produce "salt-and-pepper" noise in classification outputs, and what simple post-processing could reduce it?

- **Transfer learning with frozen vs. trainable embeddings**
  - Why needed here: Fine-tuning strategy freezes positional/month embeddings but updates channel embeddings for used sensors; knowing which layers to freeze prevents catastrophic forgetting
  - Quick check question: If you fine-tuned on SAR-only data, would you expect the MS embeddings to improve, degrade, or stay unchanged?

## Architecture Onboarding

- **Component map**: Input layer (13 channels + optional static) -> Presto encoder (0.4M params, 128-dim output) -> Classification head (single linear layer, 128→1) -> Output (binary flood map)

- **Critical path**: 1) Prepare pixel-wise time series 2) Apply channel masking based on sensor availability 3) Forward pass through Presto encoder 4) Linear projection to logits per pixel 5) Threshold at 0.5 for binary classification

- **Design tradeoffs**: Parameter efficiency vs. capacity (0.4M vs 100M params); Pixel vs. patch (finer boundaries but internal noise requiring post-processing); Training on full fusion only (simpler pipeline but risks suboptimal single-sensor performance)

- **Failure signatures**: SAR-only speckle (internal noise in flood segments); Low-backscatter confusion (smooth surfaces misclassified as water); Cloud-obscured ground truth mismatch (SAR detects floods under clouds but labels show no flood)

- **First 3 experiments**: 1) Reproduce MS+SAR baseline (F1≈0.896 on Bolivia test set) 2) Ablate to MS-only inference (expect F1≈0.893) 3) Test SAR-only with spatial smoothing (apply median filter to reduce salt-and-pepper noise)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning with SAR-specific ground truth labels resolve the modality mismatch and significantly improve quantitative performance in SAR-only flood mapping scenarios?
- Basis in paper: Discussion proposes "fine-tuning the model using SAR-based ground truth labels for SAR observations" to address the discrepancy where S2-derived labels penalize valid SAR detections under clouds
- Why unresolved: Current study uses Sen1Floods11 labels derived from optical data, inherently biasing evaluation against SAR-only predictions
- What evidence would resolve it: Re-training and evaluating on dataset with hand-labeled flood extents derived specifically from SAR imagery, followed by improved F1 scores in SAR-only scenario

### Open Question 2
- Question: To what extent can spatial post-processing techniques mitigate the "salt-and-pepper" noise inherent in pixel-based SAR predictions without degrading boundary accuracy?
- Basis in paper: Discussion notes pixel-wise approach produces internal noise and suggests "final map's quality could be enhanced with a simple post-processing step, such as a spatial smoothing filter"
- Why unresolved: Qualitative analysis identified this noise as characteristic limitation, but suggested smoothing solution not implemented or tested
- What evidence would resolve it: Application of spatial filters to SAR-only output maps, demonstrating reduction in speckle noise metrics while maintaining IoU for boundary delineation

### Open Question 3
- Question: Does supervised fine-tuning with stochastic channel-group masking provide superior robustness compared to fine-tuning exclusively on complete, fused data?
- Basis in paper: Methodology notes model trained exclusively on "complete 13-channel sensor-fused data," relying on pre-training phase for masking robustness; unclear if task-specific masking during fine-tuning would improve single-sensor performance
- Why unresolved: Paper demonstrates pre-training alone allows handling missing sensors, but does not ablate whether fine-tuning strategy incorporating sensor dropout would yield higher robustness
- What evidence would resolve it: Ablation study comparing current model against version fine-tuned with random sensor masking, showing differences in rate of performance degradation when sensors are removed

## Limitations
- Label mismatch between SAR detections and S2-derived ground truth causes unfair penalty for valid SAR-only predictions
- Pixel-level classification introduces salt-and-pepper noise requiring post-processing for operational use
- Pre-trained embeddings' ability to generalize across sensor configurations remains weakly validated

## Confidence

- High confidence: Sensor fusion performance (F1=0.896, mIoU=0.886) and overall methodology soundness
- Medium confidence: Single-sensor generalization mechanism and frozen embedding strategy
- Low confidence: SAR-only performance interpretation due to label source mismatch

## Next Checks
1. Generate SAR-specific ground truth labels to isolate whether SAR performance limitations stem from model capability or label mismatch
2. Systematically test intermediate masking scenarios (e.g., removing only VV or only VH channels) to validate pre-trained embeddings' robustness to partial sensor availability
3. Apply consistent spatial smoothing to all three sensor configurations and measure impact on mIoU and visual quality, particularly for SAR-only case