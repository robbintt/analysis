---
ver: rpa2
title: Random Direct Preference Optimization for Radiography Report Generation
arxiv_id: '2509.21351'
source_url: https://arxiv.org/abs/2509.21351
tags:
- report
- rdpo
- generation
- mimic-cxr
- chexpert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Random Direct Preference Optimization (RDPO),
  a model-agnostic framework to improve Radiography Report Generation (RRG) by leveraging
  contrastive sampling from existing training data. Unlike prior methods requiring
  human preference annotations or reward models, RDPO uses randomly sampled contrastive
  pairs from ground-truth image-text pairs to optimize RRG outputs via Direct Preference
  Optimization.
---

# Random Direct Preference Optimization for Radiography Report Generation

## Quick Facts
- arXiv ID: 2509.21351
- Source URL: https://arxiv.org/abs/2509.21351
- Reference count: 33
- Primary result: Model-agnostic RDPO improves RRG performance by up to 5% on clinical metrics without requiring human preference annotations

## Executive Summary
This paper introduces Random Direct Preference Optimization (RDPO), a novel framework for improving Radiography Report Generation (RRG) that requires no human preference annotations or reward modeling. RDPO leverages contrastive sampling from existing training data to optimize RRG outputs via Direct Preference Optimization. The method is demonstrated across three publicly available models (Stanford AIMI, CSIRO, and MAIRA-2) trained on MIMIC-CXR, CheXpert Plus, and Interpret-CXR datasets, showing consistent performance gains with statistically significant improvements (P < 0.05) in multiple clinical and semantic metrics. Expert radiologist assessment validated the clinical relevance, with RDPO-aligned reports preferred in 33 out of 100 cases.

## Method Summary
RDPO is a model-agnostic alignment framework that improves RRG by using randomly sampled contrastive pairs from ground-truth image-text pairs during training. For each batch of N pairs, the method samples random y_j ≠ y_i as rejected responses and applies DPO loss to optimize the RRG outputs. The approach uses LoRA fine-tuning with α=16, r=8 on q/v attention weights, learning rate of 1e-6 with cosine scheduler, batch size of 64, and trains for 3 epochs with greedy decoding at inference. Unlike prior methods requiring human preference annotations or reward models, RDPO only requires the image-text pairs already available for supervised fine-tuning.

## Key Results
- Performance improvements up to 5% on clinically relevant metrics including BLEU, Bert-score, and RadCliQ-v1
- Statistically significant gains (P < 0.05) across multiple evaluation metrics
- RDPO-aligned reports preferred by expert radiologists in 33 out of 100 assessment cases
- Consistent performance gains across three different RRG models and multiple datasets

## Why This Works (Mechanism)
RDPO works by leveraging the inherent structure in ground-truth image-text pairs to create contrastive learning signals. By randomly sampling alternative responses from the same training batch as "rejected" examples, the method creates a preference learning scenario without requiring external human annotations. The DPO framework then optimizes the model to produce outputs that are more similar to the ground-truth responses compared to the randomly sampled alternatives, effectively aligning the RRG model with clinically accurate reporting patterns.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A method for aligning language models using preference data by optimizing a KL-regularized reward; needed because it enables preference learning without reward modeling; quick check: verify DPO loss formulation matches standard implementation
- **Radiography Report Generation**: The task of generating medical reports from chest X-ray images; needed as the target application domain; quick check: confirm input image preprocessing matches model requirements
- **Contrastive Sampling**: Technique of comparing a positive example against negative alternatives; needed to create preference pairs from existing data; quick check: ensure random sampling implementation produces valid contrastive pairs
- **LoRA Fine-tuning**: Parameter-efficient method using low-rank adapters for model adaptation; needed for efficient model updates; quick check: verify LoRA configuration matches specified α and r values

## Architecture Onboarding

**Component Map**: Image Input -> RRG Model -> Output Text -> RDPO Loss Computation -> LoRA Parameters -> Updated RRG Model

**Critical Path**: The core training loop where RDPO constructs contrastive pairs (ground truth vs. random sample) and applies the DPO loss to update LoRA parameters, which in turn modifies the RRG model's output distribution toward more clinically accurate reports.

**Design Tradeoffs**: Random sampling is computationally efficient and requires no additional data annotation, but may not always select the most informative negative examples compared to more sophisticated sampling strategies. The approach trades potential optimization quality for simplicity and scalability.

**Failure Signatures**: Performance degradation on out-of-distribution datasets (like IU X-Ray) when trained on stylistically different datasets (MIMIC-CXR or CheXpert Plus), indicating domain shift issues. Catastrophic forgetting if training data differs significantly from pre-training distribution.

**First Experiments**:
1. Verify RDPO training converges by monitoring loss curves and checking that RadCliQ-v1 improves over baseline
2. Test RDPO on a single dataset/model combination (e.g., Stanford AIMI on MIMIC-CXR) to establish baseline performance gains
3. Evaluate statistical significance of improvements using Wilcoxon signed-rank test on held-out validation sets

## Open Questions the Paper Calls Out
- Alternative strategies for sampling rejected responses (beyond random selection) could potentially yield better optimization signals
- The benefit of incorporating stylistically diverse datasets like IU X-Ray into the alignment phase remains untested due to dataset size constraints
- The extent to which RDPO reduces specific clinical error types (factual hallucinations vs. omissions) compared to SFT requires detailed error analysis

## Limitations
- The β parameter in DPO loss is unspecified, affecting reproducibility and optimization dynamics
- Performance degradation on out-of-distribution test sets due to domain shift between datasets
- Limited validation with only 100 radiologist assessments, potentially insufficient for establishing clinical utility

## Confidence
- **High Confidence**: Core RDPO methodology is sound; experimental framework is appropriately designed
- **Medium Confidence**: Performance improvements are statistically significant but clinical relevance needs larger validation
- **Low Confidence**: Claims about generalizability across different RRG models based on limited model diversity

## Next Checks
1. Conduct parameter sensitivity analysis across different β values (0.01, 0.1, 1.0) and reference policy update strategies
2. Perform cross-dataset generalization test by training on MIMIC-CXR and evaluating independently on CheXpert Plus and IU X-Ray
3. Execute larger-scale longitudinal clinical validation with blinded radiologist evaluation (n > 100) comparing RDPO outputs against both baseline models and radiologist-generated reports