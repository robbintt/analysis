---
ver: rpa2
title: 'STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models'
arxiv_id: '2502.13119'
source_url: https://arxiv.org/abs/2502.13119
tags:
- demand
- arxiv
- price
- element
- given
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STEER-ME, a comprehensive benchmark for evaluating
  large language models on non-strategic microeconomic reasoning. The benchmark covers
  58 distinct economic reasoning elements across five settings, including consumption
  decisions, production decisions, and market equilibrium analysis.
---

# STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models

## Quick Facts
- arXiv ID: 2502.13119
- Source URL: https://arxiv.org/abs/2502.13119
- Reference count: 40
- This paper introduces STEER-ME, a comprehensive benchmark for evaluating large language models on non-strategic microeconomic reasoning across 58 distinct economic reasoning elements.

## Executive Summary
STEER-ME is a novel benchmark designed to evaluate large language models' capabilities in non-strategic microeconomic reasoning. The benchmark covers five key settings: consumption decisions, production decisions, market equilibrium analysis, consumer behavior analysis, and firm behavior analysis. Questions are generated using auto-STEER, an LLM-assisted protocol that creates diverse, dynamically generated test items across multiple domains and perspectives. The benchmark was evaluated on 27 LLMs, ranging from small open-source models to state-of-the-art systems like GPT-4o and o1-preview, revealing significant variations in economic reasoning capabilities across different model families.

The evaluation methodology employs a three-step process where models are presented with an economic scenario, required to reason through the problem, and then select an answer from multiple choices. This approach reveals not only whether models can arrive at correct answers but also whether they engage in proper economic reasoning processes. The benchmark and codebase are publicly available to support ongoing evaluation and development of economic reasoning capabilities in LLMs, addressing a critical gap in current AI evaluation frameworks for domain-specific reasoning skills.

## Method Summary
The STEER-ME benchmark employs an innovative LLM-assisted generation protocol called auto-STEER to create diverse, high-quality economic reasoning questions. The process begins with manually curated seed questions that define 58 distinct microeconomic reasoning elements across five settings. These seed questions are then automatically rewritten and expanded through a multi-stage process involving prompt rewriting, manual verification, and iterative refinement. Each generated question follows a three-step evaluation format: first presenting an economic scenario, then requiring the model to reason through the problem, and finally selecting an answer from multiple choices. The benchmark evaluates 27 LLMs ranging from small open-source models (1.5B-8B parameters) to large closed-source systems, measuring both accuracy and reasoning processes.

## Key Results
- All evaluated models struggled with at least some economic reasoning elements, with o1-preview consistently achieving top performance
- Models frequently bypassed intended reasoning processes by using answer choices to solve problems rather than demonstrating proper economic reasoning
- Significant performance gaps exist between closed-source models (GPT-4o, o1) and open-source models, with closed-source models performing substantially better
- Models showed systematic errors in fundamental economic concepts like deadweight loss calculations and market equilibrium analysis

## Why This Works (Mechanism)
The benchmark works by systematically testing whether LLMs can apply microeconomic principles through structured problem-solving rather than pattern matching. The three-step evaluation format forces models to demonstrate reasoning processes, revealing whether they truly understand economic concepts or are simply guessing based on answer choices. The auto-STEER generation protocol ensures diverse question coverage across multiple economic domains and perspectives, reducing the likelihood that models can rely on memorized patterns. By evaluating both the reasoning process and final answer selection, the benchmark distinguishes between models that can parrot economic terminology and those that can actually apply economic principles to novel situations.

## Foundational Learning
- Microeconomic concepts (why needed: benchmark targets microeconomic reasoning; quick check: verify models can define and apply concepts like supply/demand, marginal analysis, elasticity)
- Economic modeling (why needed: understanding how to represent economic scenarios mathematically; quick check: test models on translating word problems into economic equations)
- Reasoning processes (why needed: distinguishing between pattern matching and genuine problem-solving; quick check: evaluate whether models show step-by-step reasoning or jump to conclusions)
- Multiple choice analysis (why needed: understanding how models use answer options to solve problems; quick check: test models with and without answer choices to identify bypassing behavior)

## Architecture Onboarding

**Component Map:**
auto-STEER generation -> Question bank creation -> Three-step evaluation format -> Model testing -> Performance analysis

**Critical Path:**
Seed question identification → Auto-STEER generation → Manual verification → Question bank compilation → Model evaluation → Result analysis

**Design Tradeoffs:**
- Automated generation vs. manual curation balance: auto-STEER provides scalability but requires verification
- Three-step format vs. direct questioning: forces reasoning but may be more complex
- Multiple choice vs. open-ended answers: easier evaluation but may encourage answer choice exploitation

**Failure Signatures:**
- Answer choice exploitation: models use options to reverse-engineer solutions
- Conceptual confusion: models mix up fundamental economic principles
- Missing steps: models skip necessary reasoning steps
- Inconsistent application: models apply correct principles in some contexts but fail in others

**First 3 Experiments:**
1. Test model performance on questions with answer choices removed to identify bypassing behavior
2. Evaluate model performance on mixed-domain questions to test transfer of economic reasoning
3. Compare performance between step-by-step reasoning prompts vs. direct question prompts

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on dynamically generated questions which may have consistency issues in difficulty and clarity
- Three-step evaluation format may not perfectly capture real-world economic reasoning complexity
- Focus on non-strategic microeconomic reasoning excludes strategic decision-making contexts
- Automated evaluation framework may have biases in assessing economic reasoning quality

## Confidence
- **High confidence**: Comprehensive coverage of 58 distinct economic reasoning elements and systematic evaluation methodology
- **Medium confidence**: Comparative performance rankings between different model families, given potential variations in question difficulty
- **Low confidence**: Generalizability of failure patterns to real-world economic reasoning tasks due to simplified, textbook-style problems

## Next Checks
1. Validate the auto-STEER generation process by having independent economists review question variants for consistency in difficulty and clarity
2. Test whether models that bypass reasoning through answer choices can be prompted to demonstrate proper step-by-step economic reasoning when explicitly instructed to do so
3. Evaluate model performance on a subset of questions using human economics experts to establish ground truth difficulty levels and identify potential biases in the automated evaluation framework