---
ver: rpa2
title: Black-Box Privacy Attacks on Shared Representations in Multitask Learning
arxiv_id: '2506.16460'
source_url: https://arxiv.org/abs/2506.16460
tags:
- task
- learning
- adversary
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new black-box task-inference threat model
  to study privacy leakage in multitask learning (MTL). The adversary aims to determine
  whether a specific task was included in MTL training by querying only the shared
  representation, with two variants: a strong adversary with access to training samples
  and a weak adversary with only fresh samples from the task''s distribution.'
---

# Black-Box Privacy Attacks on Shared Representations in Multitask Learning

## Quick Facts
- arXiv ID: 2506.16460
- Source URL: https://arxiv.org/abs/2506.16460
- Reference count: 40
- Primary result: Black-box attacks can infer whether a task was included in multitask training by exploiting embedding-space dependencies, with strong adversaries consistently outperforming weak ones.

## Executive Summary
This paper introduces a novel black-box task-inference threat model for multitask learning (MTL), where an adversary aims to determine whether a specific task was included in MTL training by querying only the shared representation. The authors develop two efficient, purely black-box attacks—coordinate-wise variance and pairwise inner product—that exploit dependencies between embeddings from the same task without requiring shadow models or reference data. Extensive experiments across vision (CelebA, FEMNIST) and language (Stack Overflow) domains show that both strong adversaries (with training samples) and weak adversaries (with fresh samples) can achieve non-trivial success rates. Theoretical analysis of a simplified mean estimation setting demonstrates a strict separation between the two threat models, validating the empirical findings.

## Method Summary
The authors propose two black-box attacks on shared representations in MTL: coordinate-wise variance (computing trace of empirical covariance) and pairwise inner product (computing mean absolute inner product of whitened embeddings). They evaluate these attacks under two threat models: a strong adversary with access to training samples and a weak adversary with only fresh samples from the task's distribution. The attacks are tested on personalization experiments using ResNet on CelebA (256 tasks), FEMNIST (62 tasks), and BERT on Stack Overflow (20 topics). The MTL models are trained using joint training with GradNorm, gradient clipping, and weight decay. Whitening is applied to embeddings to reduce noise and improve attack signal.

## Key Results
- Both strong and weak adversaries achieve non-trivial AUC scores across all three datasets, with strong adversaries consistently outperforming weak ones
- Coordinate-wise variance attack succeeds on vision datasets where task heads are correlated, while pairwise inner product attack works across all domains
- Theoretical analysis in a simplified Gaussian mean-estimation setting demonstrates a strict separation between strong and weak adversary performance
- Embedding dimension has little impact on task-inference AUC, suggesting the attacks are robust to dimensionality changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embeddings from tasks included in multitask training exhibit systematically lower coordinate-wise variance than those from unseen tasks.
- Mechanism: The shared representation overfits to certain embedding-space directions for trained tasks, causing outputs to cluster more tightly. The attack computes the trace of the empirical covariance matrix; lower trace suggests higher likelihood the task was in training.
- Core assumption: The MTL model generalizes well enough that trained-task embeddings are more correlated/less dispersed than untrained-task embeddings, even under regularization.
- Evidence anchors:
  - [abstract] "exploit the dependencies between embeddings from the same task"
  - [section 5.1.1] Algorithm 1 computes trace of empirical covariance as the test statistic.
  - [corpus] Weak/no direct corpus support for task-level variance leakage specifically; related work focuses on sample-level membership.

### Mechanism 2
- Claim: Pairwise inner products among embeddings from the same task are higher for tasks included in training.
- Mechanism: For IN tasks, the shared representation produces more aligned embeddings because the model has seen samples from that task distribution. The attack computes mean absolute inner product (or cosine similarity) across all pairs; higher mean indicates IN.
- Core assumption: The representation encodes task-specific structure that persists across different samples from the same distribution, creating positive codependencies.
- Evidence anchors:
  - [abstract] "purely black-box attacks...that exploit the dependencies between embeddings from the same task"
  - [section 5.1.2] Algorithm 2 computes average pairwise inner product/cosine similarity.
  - [corpus] Related work on encoder membership inference (Liu+21b) uses similarity of augmented embeddings, but task-level generalization is novel here.

### Mechanism 3
- Claim: Theoretical analysis in a simplified Gaussian mean-estimation setting shows a strict separation between strong and weak adversaries, with strong adversaries achieving higher expected test statistic.
- Mechanism: Strong adversaries use actual training samples, which share stochastic dependence with the multitask mean estimator; weak adversaries use fresh samples from the same task distribution, sharing only task-level dependence. The test statistic correlates with the multitask mean only in the IN case; the correlation is stronger for strong adversaries.
- Core assumption: The Gaussian mixture model approximates key properties of MTL representations; the analysis is not a formal proof for deep learning models.
- Evidence anchors:
  - [abstract] "theoretical analysis...demonstrate a strict separation between the two threat models"
  - [section 4.1] Theorems 4.1–4.3 derive expectations and variances for strong vs. weak adversary test statistics.
  - [corpus] Tracing attacks (Dwo+15) are referenced but task-level extension is new.

## Foundational Learning

- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: Task inference generalizes MIAs from samples to entire tasks; understanding sample-level leakage helps interpret task-level risks.
  - Quick check question: Can you explain how MIAs determine if a single data point was in training? How might this extend to groups of correlated samples?

- Concept: Multitask Learning (MTL) and Shared Representations
  - Why needed here: The attack targets the shared representation learned across tasks; understanding its role is essential.
  - Quick check question: In MTL, what is the difference between the shared representation and task-specific heads? Which component leaks information in this threat model?

- Concept: Gaussian Distributions and Covariance
  - Why needed here: The theoretical analysis models tasks as Gaussian mixtures; covariance measures underlie one attack.
  - Quick check question: If two random vectors have covariance matrix Σ, what does a large trace of Σ indicate about their spread?

## Architecture Onboarding

- Component map: MTL model (shared encoder + task heads) -> Black-box query interface -> Attack algorithms (variance, inner product) -> Decision threshold -> Classification (IN/OUT)

- Critical path: 1. Obtain black-box query access to shared representation hθ. 2. For each challenge task τ*, query hθ on k samples from τ* to get embeddings E. 3. Compute test statistic (variance trace or mean pairwise inner product). 4. Apply threshold (e.g., based on percentiles of pooled statistics) to decide IN/OUT.

- Design tradeoffs:
  - Variance attack: Sensitive to embedding-space anisotropy; may fail if task heads are uncorrelated (paper notes BERT heads are near-orthogonal).
  - Inner product attack: Robust to isotropy; cosine similarity preferred for language domains.
  - Whitening: Improves signal by decorrelating embedding coordinates, but requires estimating covariance from available queries.

- Failure signatures:
  - Low AUC (~0.5) suggests embeddings do not encode task-level dependencies.
  - Large overlap in test statistic distributions for IN and OUT tasks.
  - Strong adversary success but weak adversary near random indicates overfitting to specific samples rather than task structure.

- First 3 experiments:
  1. Replicate CelebA personalization experiment (ResNet, 256 tasks) with both attacks, reporting ROC and AUC.
  2. Ablate number of samples per task (k) and observe impact on strong vs. weak adversary AUC.
  3. Compare raw embeddings vs. whitened embeddings on Stack Overflow topic classification to assess whitening utility.

## Open Questions the Paper Calls Out

- Question: Can user-level or group-level differential privacy effectively mitigate task-inference attacks on shared representations without unacceptable utility degradation?
- Basis in paper: [explicit] The authors state in Section 7.1 that "only one work develops algorithms with client-level privacy guarantees [HWS23] when MTL is applied to collaborative learning," and that standard defenses like gradient clipping do not prevent their attacks.
- Why unresolved: The paper demonstrates attacks succeed even with gradient clipping and normalization, but does not evaluate formal privacy mechanisms.
- What evidence would resolve it: Empirical evaluation of task-inference attack success on MTL models trained with user-level DP across different privacy budgets and utility metrics.

- Question: Why does the coordinate-wise variance statistic ordering differ between vision (IN > OUT) and language datasets in personalization settings, and what architectural properties explain this discrepancy?
- Basis in paper: [explicit] Section 6.4.4 and Appendix C.1 note "a discrepancy in the test statistic distributions for vision and language datasets" and correlate it with task head correlation patterns, but do not fully explain the mechanism.
- Why unresolved: The authors observe the correlation between task heads and variance ordering but do not establish causation or a theoretical explanation.
- What evidence would resolve it: Controlled experiments varying model architecture (e.g., orthogonalization of task heads) and measuring the impact on variance statistic ordering.

- Question: How does task-inference vulnerability scale with model complexity and embedding dimensionality in modern large-scale MTL systems?
- Basis in paper: [inferred] The ablation studies (Appendix C.2, Figure 8) show "embedding dimension has little impact on task-inference AUC" on synthetic data, but the models tested are relatively small (29M-41M parameters).
- Why unresolved: The relationship may differ for large foundation models or higher-dimensional embeddings not tested in the paper.
- What evidence would resolve it: Systematic evaluation across varying model sizes and embedding dimensions, particularly on larger transformer-based architectures.

## Limitations

- The variance attack's reliance on embedding anisotropy is not fully validated across diverse MTL architectures; failure cases for uncorrelated task heads are noted but not extensively explored.
- Theoretical analysis assumes Gaussian mixtures, which may not fully capture the behavior of deep neural representations; the strict separation between strong and weak adversaries in practice depends on this approximation holding.
- Regularization hyperparameters (especially λ for whitening) are underspecified, potentially affecting reproducibility and attack stability.

## Confidence

- **High:** Empirical success of both attacks on real MTL benchmarks (CelebA, FEMNIST, Stack Overflow) with consistent AUC improvements over random guessing.
- **Medium:** Theoretical separation between strong and weak adversaries in the Gaussian mean-estimation model, as this is a simplified abstraction.
- **Medium:** Whitening improves attack signal, though optimal λ and sample size for covariance estimation are not fully characterized.

## Next Checks

1. **Ablation of Embedding Dimensionality:** Test whether the variance attack's effectiveness degrades when task heads are orthogonal (e.g., BERT), and whether inner product or cosine similarity remains robust.
2. **Generalization Gap Analysis:** Measure how the strong vs. weak adversary performance gap correlates with the MTL model's generalization gap; expect weak performance if the model overfits to samples rather than tasks.
3. **Theoretical Extension:** Simulate the Gaussian mixture model with parameters estimated from real MTL embeddings to verify if the predicted separation in test statistic distributions holds empirically.