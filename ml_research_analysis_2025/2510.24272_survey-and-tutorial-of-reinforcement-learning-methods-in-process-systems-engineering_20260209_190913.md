---
ver: rpa2
title: Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering
arxiv_id: '2510.24272'
source_url: https://arxiv.org/abs/2510.24272
tags:
- control
- learning
- policy
- process
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys reinforcement learning (RL) methods and their
  applications in process systems engineering (PSE). It presents a tutorial covering
  key RL algorithmic families - value-based, policy-based, and actor-critic methods
  - while surveying existing applications across PSE domains including batch process
  control, regulatory control, setpoint scheduling, and supply chain management.
---

# Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering

## Quick Facts
- arXiv ID: 2510.24272
- Source URL: https://arxiv.org/abs/2510.24272
- Reference count: 40
- Key outcome: This paper surveys reinforcement learning (RL) methods and their applications in process systems engineering (PSE), identifying successes in handling uncertainty while highlighting limitations in sample efficiency, safety constraints, and real-world deployment.

## Executive Summary
This survey provides a comprehensive tutorial of reinforcement learning methods and their applications in process systems engineering. The paper systematically covers three main RL algorithmic families—value-based, policy-based, and actor-critic methods—while surveying existing applications across PSE domains including batch process control, regulatory control, setpoint scheduling, and supply chain management. The authors identify key challenges in PSE applications including sample efficiency, safety constraints, and the gap between simulation studies and real-world deployment. They emphasize that effective RL integration requires augmenting classical control approaches with adaptive, data-driven policies tailored to the structured and constrained nature of process systems.

## Method Summary
The paper presents a tutorial framework covering RL algorithms from fundamental MDP formulations through advanced techniques like model-based RL, constrained RL, and offline RL. The methodology synthesizes existing literature on RL applications in PSE, organizing approaches by control structure (regulatory, supervisory, batch), optimization paradigm (stochastic vs. deterministic), and safety considerations. The tutorial provides algorithmic details including Q-learning, DQN, DDPG, PPO, and SAC, while discussing their adaptation to continuous action spaces, safety constraints, and limited data scenarios typical in industrial settings.

## Key Results
- RL methods show promise in handling uncertainty and optimizing complex PSE systems, particularly in regulatory control and batch process optimization
- Sample efficiency remains a critical limitation, with model-free methods requiring prohibitively many interactions for industrial-scale deployment
- Safety constraints pose significant challenges, as standard RL algorithms focus on expected constraint satisfaction rather than hard pathwise guarantees
- Integration of domain knowledge through model-based approaches and hybrid control structures shows potential for improving data efficiency and safety

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating learned policies with classical control structures (e.g., PID or MPC) may improve stability and sample efficiency compared to purely data-driven approaches.
- Mechanism: The policy network is constrained or augmented by domain-specific inductive biases (e.g., PID structure) or hierarchical decomposition, reducing the effective search space and grounding the exploration in known control behaviors.
- Core assumption: The underlying process dynamics, while complex, retain sufficient structure that can be partially captured by the classical controller or model prior.
- Evidence anchors: [abstract] "effective RL integration in PSE requires augmentation of classical control approaches"; [section 3.2] "strategies employ meta-RL... to enable the rapid online tuning of controllers"; "embeds the PID structure directly into the RL agent's policy network"
- Break condition: If the process is highly non-stationary or the model-plant mismatch is severe, fixed structural priors may degrade performance or limit adaptability.

### Mechanism 2
- Claim: Formulating safety requirements as explicit constraints within the optimization problem (Constrained MDPs) is proposed to provide better guarantees than simple reward shaping.
- Mechanism: Instead of treating safety as a soft penalty in the cost function, algorithms (like CPO or Lagrangian methods) solve a constrained optimization where expected constraint costs must remain below thresholds, theoretically decoupling objective maximization from safety satisfaction.
- Core assumption: The constraint costs can be accurately estimated and the problem is feasible (a feasible safe policy exists).
- Evidence anchors: [section 9] "CMDP... extends the standard MDP tuple... to include m auxiliary cost functions"; "Standard RL algorithms... do not explicitly guarantee that these constraints will be satisfied"
- Break condition: If "hard safety" (pathwise constraints) is required but the algorithm only guarantees "statistical safety" (expectation), the mechanism may fail safety-critical validation.

### Mechanism 3
- Claim: Conservative estimation of value functions in offline settings may prevent the policy from taking unsafe actions that appear erroneously optimal due to distributional shift.
- Mechanism: Algorithms like Conservative Q-Learning (CQL) penalize high values for actions diverging from the offline dataset, pushing the policy to stay "close" to the behavior policy's distribution.
- Core assumption: The historical dataset contains sufficient high-quality transitions to cover the optimal policy's state-action distribution.
- Evidence anchors: [section 10.3] "CQL... explicitly overestimates costs for out-of-distribution controls while underestimating costs for controls observed in the dataset"; [section 10.2] "Fundamental challenge in offline RL stems from the distributional shift"
- Break condition: If the offline dataset is sparse or of poor quality (low coverage), conservative estimates may result in a trivial or overly passive policy.

## Foundational Learning

- Concept: Markov Decision Process (MDP) & Bellman Equations
  - Why needed here: This is the mathematical substrate for all RL methods discussed. Understanding the difference between the Bellman Expectation Equation (for evaluation) and the Bellman Optimality Equation (for control) is required to distinguish between algorithms like SARSA and Q-learning.
  - Quick check question: If an environment has a time-delay in sensing, does it strictly satisfy the Markov property as defined in Section 4?

- Concept: Exploration vs. Exploitation
  - Why needed here: The paper highlights that in PSE, unlike games, "exploration may be prohibitively expensive" [Section 10]. Understanding this tradeoff is essential for selecting between on-policy (high variance, safe exploration needed) and off-policy/offline methods.
  - Quick check question: Why might ε-greedy exploration be dangerous in a chemical reactor compared to a grid world?

- Concept: Actor-Critic Architecture
  - Why needed here: This is identified as the dominant class of algorithms for continuous PSE problems [Section 6.4]. You must understand the distinct roles of the Actor (policy parameterization) and the Critic (value estimation) to debug training instabilities.
  - Quick check question: In an Actor-Critic setup, which component, if inaccurate, would cause the policy to "thrash" or degrade despite receiving good state feedback?

## Architecture Onboarding

- Component map:
  - Process Environment -> Safety Layer -> Agent Core (Critic, Actor) -> Experience Buffer

- Critical path:
  1. Formulation: Map physical process to MDP/CMDP (define state/action spaces, cost function φ, and constraints)
  2. Algorithm Selection: Choose based on action space (Discrete → DQN; Continuous → DDPG/TD3/SAC) and data availability (Sim → Model-free; Historical → Offline RL)
  3. Reward/Cost Engineering: Design φ to balance economic goals and safety; implement constraint layers
  4. Training Loop: Interact → Store → Update Critic → Update Actor

- Design tradeoffs:
  - Model-free vs. Model-based: Model-free is simpler but sample-inefficient; Model-based (Section 8) is efficient but susceptible to model bias ("exploitation of model errors")
  - On-policy vs. Off-policy: On-policy (e.g., PPO) is stable but data-hungry; Off-policy (e.g., DQN/DDPG) reuses data but can be unstable
  - Generic vs. Specialized: Generic algorithms are easier to implement; specialized (e.g., Goal-Conditioned or Distributional RL) handle multi-setpoint or risk-sensitive tasks better [Section 7, 11]

- Failure signatures:
  - Constraint Violation: Agent succeeds at task but violates safety bounds (Indicates need for CMDP or harder penalties)
  - Q-value Explosion: Critic loss diverges; Q-values trend to infinity (Common in off-policy RL without target networks or proper regularization)
  - Freezing: Agent stops exploring and repeats a sub-optimal safe action (Common in sparse reward settings)

- First 3 experiments:
  1. Behavior Cloning / Warm Start: Train a policy using Offline RL (e.g., CQL or Behavioral Cloning) on existing historical PID data to establish a safe baseline before any online interaction [Section 10.5]
  2. Safe Actor-Critic Baseline: Implement a standard continuous control algorithm (e.g., TD3 or SAC) on a simulation with a simple "safety shield" that rejects unsafe actions to verify the learning loop
  3. Constraint Ablation: Compare standard RL (penalties in reward) vs. Constrained RL (Lagrangian) on a setpoint tracking task with a hard "maximum temperature" constraint to measure violation frequency [Section 9]

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Constrained Reinforcement Learning (CRL) methodologies transition from satisfying constraints in expectation (statistical safety) to guaranteeing hard pathwise constraints required for safety-critical process systems?
- Basis in paper: [explicit] The authors state that standard CMDP formulations focus on expected constraint satisfaction, which is "typically insufficient for most engineering systems," and that "significant research is still required" to provide explicit guarantees for hard constraints.
- Why unresolved: Standard RL algorithms and CMDP formulations generally lack the mechanisms to enforce strict constraints at every time step, often relying on penalty methods or Lagrangian relaxation that only ensure average satisfaction.
- What evidence would resolve it: The development of algorithms that mathematically guarantee state constraints (e.g., temperature limits) are never violated during both training and deployment, potentially via control barrier functions or similar control-theoretic integrations.

### Open Question 2
- Question: How can Off-Policy Evaluation (OPE) be adapted to handle deterministic industrial controllers and non-stationary process dynamics, which currently cause existing methods to fail?
- Basis in paper: [explicit] The authors identify OPE as a "complementary but underexplored area in PSE," noting that importance sampling fails when behavior policies are deterministic (standard in industry) or when dynamics shift due to degradation.
- Why unresolved: Standard OPE relies on importance weighting which yields zero probabilities if the learned policy deviates from a deterministic historical policy, and assumes stationary dynamics which historical plant data may not reflect.
- What evidence would resolve it: Novel OPE estimators capable of accurately predicting policy performance from fixed historical datasets generated by deterministic controllers or non-stationary environments.

### Open Question 3
- Question: To what extent can the integration of domain knowledge (via model-based or hybrid approaches) reduce the sample complexity of RL to a level feasible for direct industrial deployment?
- Basis in paper: [explicit] The paper concludes there is a "persistent gap" between simulation studies and deployment "perhaps owing to the prohibitive sample complexity," suggesting that exploiting the inherent structure of PSE problems is necessary to improve data efficiency.
- Why unresolved: Model-free methods require extensive exploration that is dangerous or expensive in real plants, while purely model-based methods may suffer from model bias or lack of robustness.
- What evidence would resolve it: Demonstrations of hybrid RL agents achieving convergence and performance superior to Model Predictive Control (MPC) using only a fraction of the interaction data typically required by standard model-free algorithms.

## Limitations

- Sample efficiency remains a fundamental challenge, with model-free methods requiring potentially prohibitive data volumes for industrial-scale processes
- Safety guarantees are limited to statistical satisfaction rather than hard pathwise constraints, creating uncertainty for safety-critical applications
- The gap between simulation studies and real-world deployment persists, likely due to the prohibitive sample complexity and safety requirements of industrial processes

## Confidence

- High Confidence: The identification of core RL algorithmic families (value-based, policy-based, actor-critic) and their general applicability to PSE problems is well-supported by established literature
- Medium Confidence: Claims about hybrid approaches combining RL with classical control structures are plausible but rely on assumptions about process regularity that may not hold in all PSE applications
- Low Confidence: Specific performance comparisons and quantitative success metrics for RL applications in the cited PSE domains are not provided in sufficient detail for independent validation

## Next Checks

1. Implement and test Constrained MDP algorithms (CPO, Lagrangian PPO) on a benchmark PSE control problem to empirically measure constraint violation rates under different safety requirement specifications
2. Conduct a systematic comparison of model-free vs. model-based RL approaches on a realistic PSE simulation, quantifying sample efficiency gains against potential model bias degradation
3. Evaluate conservative offline RL algorithms (CQL) on historical process control datasets to assess whether distributional shift mitigation improves policy safety without excessive conservatism