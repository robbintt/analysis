---
ver: rpa2
title: 'FACE: Faithful Automatic Concept Extraction'
arxiv_id: '2510.11675'
source_url: https://arxiv.org/abs/2510.11675
tags:
- concept
- face
- c-ins
- c-del
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACE addresses the faithfulness gap in concept-based explanations
  by enforcing alignment between extracted concepts and model predictions. It augments
  NMF with a KL divergence regularization term that minimizes the discrepancy between
  model predictions on original and reconstructed activations.
---

# FACE: Faithful Automatic Concept Extraction
## Quick Facts
- arXiv ID: 2510.11675
- Source URL: https://arxiv.org/abs/2510.11675
- Authors: Dipkamal Bhusal; Michael Clifford; Sara Rampazzi; Nidhi Rastogi
- Reference count: 40
- Primary result: FACE enforces alignment between extracted concepts and model predictions through KL divergence regularization, achieving up to 96.9% Concept Insertion score versus 93.2% for CRAFT.

## Executive Summary
FACE addresses the critical faithfulness gap in concept-based explanations by introducing a novel regularization framework that ensures extracted concepts align with model predictions. The method augments Non-negative Matrix Factorization (NMF) with a KL divergence term that minimizes discrepancies between predictions on original and reconstructed activations. This supervision guarantees that discovered concepts not only reconstruct latent representations but also faithfully reflect the model's true reasoning process. FACE demonstrates consistent superiority across faithfulness metrics and provides theoretical guarantees linking KL divergence minimization to bounded predictive distribution deviations.

## Method Summary
FACE extends traditional NMF-based concept extraction by incorporating a KL divergence regularization term that enforces alignment between extracted concepts and model predictions. The framework operates by first obtaining model activations from intermediate layers, then decomposing these activations into concept coefficients and concept vectors. The key innovation is the KL divergence term that measures the discrepancy between the model's predictive distribution on original activations versus the distribution on reconstructed activations using extracted concepts. This regularization ensures that concepts capture features the model actually relies upon for predictions, rather than merely reconstructing activations. The method provides theoretical bounds showing that minimizing KL divergence constrains the deviation in predictive distributions, promoting faithful local linearity in concept space.

## Key Results
- FACE achieves up to 96.9% Concept Insertion score compared to 93.2% for state-of-the-art CRAFT
- Consistent improvements across faithfulness metrics (Concept Insertion/Deletion) and sparsity metrics on ImageNet, COCO, and CelebA datasets
- Theoretical guarantees showing KL divergence minimization bounds predictive distribution deviations
- Successfully balances the trade-off between faithfulness and reconstruction quality through empirical λ parameter tuning

## Why This Works (Mechanism)
FACE works by directly supervising concept extraction with the model's predictive behavior. The KL divergence regularization term creates a feedback loop where concepts are optimized not just to reconstruct activations, but to preserve the model's decision-making process. When the model makes a prediction based on original activations, FACE ensures that the same prediction is maintained when using only the extracted concepts. This alignment between concept space and prediction space guarantees faithfulness - the concepts genuinely capture what the model uses for its decisions. The theoretical framework shows that minimizing this discrepancy bounds the deviation in predictive distributions, providing mathematical assurance that extracted concepts faithfully represent the model's reasoning.

## Foundational Learning
**Non-negative Matrix Factorization (NMF)**: A dimensionality reduction technique that decomposes data into non-negative components, ideal for extracting interpretable concepts from activations. Why needed: Provides the basic framework for decomposing activations into concept vectors and coefficients. Quick check: Verify that NMF produces sparse, interpretable components when applied to model activations.

**KL Divergence**: A measure of the difference between two probability distributions. Why needed: Quantifies the discrepancy between model predictions on original versus reconstructed activations, enabling faithfulness supervision. Quick check: Confirm that KL divergence correctly measures distribution differences in your specific use case.

**Concept Insertion/Deletion Metrics**: Post-hoc evaluation methods that measure how much prediction changes when concepts are added or removed. Why needed: Provide quantitative measures of faithfulness to evaluate whether extracted concepts actually influence model predictions. Quick check: Validate that these metrics correlate with intuitive notions of concept importance.

## Architecture Onboarding
Component map: Input Activations -> NMF Decomposition -> Concept Extraction -> KL Regularization -> Faithful Concepts
Critical path: Activation extraction → NMF factorization → KL divergence calculation → Concept optimization
Design tradeoffs: FACE trades slight reconstruction accuracy for improved predictive consistency, prioritizing faithfulness over perfect reconstruction
Failure signatures: Poor faithfulness scores indicate misalignment between concepts and predictions; high reconstruction error with low faithfulness suggests over-regularization
First experiments: 1) Test KL divergence regularization strength sensitivity across different λ values, 2) Compare faithfulness scores on simple vs complex datasets, 3) Evaluate concept sparsity and interpretability qualitatively

## Open Questions the Paper Calls Out
The paper acknowledges that the optimal weighting parameter λ for the KL divergence term is determined empirically rather than through theoretical derivation. This empirical determination may affect the trade-off between faithfulness and reconstruction quality across different datasets and model architectures. Additionally, the faithfulness evaluation relies on Concept Insertion/Deletion metrics, which measure post-hoc prediction changes rather than establishing true causal relationships between concepts and model decisions.

## Limitations
- Empirical determination of the regularization parameter λ rather than theoretical derivation
- Reliance on Concept Insertion/Deletion metrics that measure correlation rather than causation
- Assumption of local linearity in concept space that may not hold for complex vision tasks
- Evaluation primarily focused on image classification, limiting generalizability to other modalities

## Confidence
- FACE's superiority over CRAFT (96.9% vs 93.2% Concept Insertion): High
- Theoretical analysis of KL divergence bounds: Medium
- Practical utility for real-world interpretability tasks: Low

## Next Checks
1. Conduct ablation studies varying the λ parameter across a wider range to determine sensitivity of the faithfulness-reconstruction trade-off and identify dataset-specific optimal values.

2. Test FACE's performance on non-image modalities (text, tabular data) and more complex vision tasks (object detection, segmentation) to validate generalizability beyond classification benchmarks.

3. Design and execute causal intervention studies that go beyond Concept Insertion/Deletion metrics to establish whether FACE-extracted concepts truly capture the causal features the model relies upon, rather than merely correlating with predictions.