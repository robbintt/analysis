---
ver: rpa2
title: 'Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents'
arxiv_id: '2512.13704'
source_url: https://arxiv.org/abs/2512.13704
tags:
- error
- data
- graph
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adjudicator, a neuro-symbolic system for
  automated correction of noisy labels in industrial datasets. The core innovation
  is a multi-agent Large Language Model architecture, the "Council of Agents," that
  adjudicates label validity using evidence from a dynamically constructed Knowledge
  Graph.
---

# Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents

## Quick Facts
- **arXiv ID:** 2512.13704
- **Source URL:** https://arxiv.org/abs/2512.13704
- **Reference count:** 40
- **Primary result:** A KG-informed multi-agent LLM system achieved 0.99 F1-score for correcting noisy labels, outperforming single-LLM and non-KG baselines.

## Executive Summary
This paper introduces Adjudicator, a neuro-symbolic system for automated correction of noisy labels in industrial datasets. The core innovation is a multi-agent Large Language Model architecture, the "Council of Agents," that adjudicates label validity using evidence from a dynamically constructed Knowledge Graph. Adjudicator was evaluated on a 1,000-item balanced subset of the AlleNoise benchmark. The KG-informed council achieved a 0.99 F1-score, dramatically outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). The superior performance stems from the KG's ability to perfectly identify complex, structural errors via a novel override logic, demonstrating a robust, explainable method for generating high-quality "golden datasets" in strictly governed environments.

## Method Summary
Adjudicator combines a dynamic Knowledge Graph with a council of three specialized LLM agents (Policy Expert, Data Analyst, Pattern Detector) to verify product labels. The system first constructs a localized taxonomy graph and calculates the Hierarchical Ancestor Distance between the noisy and ground truth labels. Agents then vote on whether a label is correct, with the Data Analyst's structural evidence given double weight. A novel override logic flags errors when the Data Analyst votes "Error" and the hierarchical distance exceeds zero, ensuring perfect precision on semantic errors. The approach specifically addresses LLM sycophancy through "hardened" adversarial prompts that encourage fault-finding rather than agreeableness.

## Key Results
- KG-informed council achieved 0.99 F1-score versus 0.48 for single-LLM baseline and 0.59 for non-KG council
- Perfect precision (1.00) achieved through KG-based structural override mechanism
- System is approximately 4x slower than single LLM inference
- Correctly identifies semantic errors with 100% accuracy when structural evidence exists

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A structural override based on Knowledge Graph (KG) metrics provides higher precision in label correction than semantic reasoning alone.
- **Mechanism:** The system calculates a Hierarchical Ancestor Distance (HAD) between the noisy label and the candidate clean label. If this distance exceeds a threshold, a "strong_kg_signal" triggers an override, forcing an "Error" verdict regardless of other agents' votes.
- **Core assumption:** The domain taxonomy is complete, and a valid label must share a relatively close ancestor with the ground truth.
- **Evidence anchors:** [abstract] "achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors"; [section 3.3] "The final decision... is 'error' (1) if the threshold is met OR the override is triggered"; [corpus] "Identifying and Correcting Label Noise for Robust GNNs" supports graph-based noise identification.
- **Break condition:** Fails when errors are "Hierarchical" (leaf nodes match but paths differ, i.e., `lca_dist` = 0), where the structural signal is absent (Section 5.1).

### Mechanism 2
- **Claim:** Adversarial prompt hardening is required to overcome LLM "agreeableness" bias (sycophancy) in verification tasks.
- **Mechanism:** Agents are not neutral assistants but are assigned "hardened" personas (e.g., "Skeptical Adjudicator") explicitly instructed to find faults and deny the benefit of the doubt.
- **Core assumption:** Standard instruction-tuned LLMs default to confirming user-provided labels unless explicitly restricted.
- **Evidence anchors:** [abstract] "specialized agents debate and vote"; [section 3.2.1] "Initial, neutral prompts resulted in Precision and Recall scores of 0.00... [we] overcame this by developing a 'hardened prompt' strategy"; [corpus] Weak support; corpus focuses on task coordination, not specifically adversarial prompting for verification.
- **Break condition:** If prompts are reverted to "helpful assistant" styles, the system fails to detect errors (0% Recall).

### Mechanism 3
- **Claim:** Differential weighting of agent votes prioritizes verifiable structural evidence over hallucinated semantic reasoning.
- **Mechanism:** The Data Analyst (grounded in KG metrics) receives 2.0 vote weight, while the Pattern Detector (commonsense) receives 0.5. This ensures that a structural contradiction can override a semantic consensus.
- **Core assumption:** Structural graph metrics are more reliable for this specific task than general LLM commonsense.
- **Evidence anchors:** [section 3.3] "The Data Analyst... is given double the weight to prioritize the symbolic signal."; [section 5.3.1] Case 1 demonstrates the weighted vote preventing a False Positive when the Policy Expert was misled; [corpus] "Task-Aware LLM Council" mentions adaptive pathways, generally supporting specialized agent roles.
- **Break condition:** In domains without clear structural schemas (unstructured text), the Data Analyst's weight becomes arbitrary or meaningless.

## Foundational Learning

- **Concept:** Lowest Common Ancestor (LCA) in Tree Structures
  - **Why needed here:** The core metric (HAD) relies on calculating the distance between nodes via their LCA. Without understanding tree traversal, the "override logic" is opaque.
  - **Quick check question:** If Node A is "Fruit/Apple" and Node B is "Fruit/Banana", what is their LCA? (Answer: "Fruit").

- **Concept:** Sycophancy in LLMs
  - **Why needed here:** The paper explicitly combats the LLM bias toward "agreeableness." Understanding this failure mode explains why the complex "Council" architecture is necessary.
  - **Quick check question:** Why would a standard LLM likely agree with a label "Cat" for an image of a "Dog" if the user prompt implies the label is correct?

- **Concept:** Neuro-symbolic Fusion
  - **Why needed here:** The system is not pure AI but a hybrid. Recognizing where the "Neuro" (LLM) ends and the "Symbolic" (KG/Logic) begins is crucial for debugging.
  - **Quick check question:** In Equation 4, which term represents the Neural component ($f_{Ai}$) and which represents the Symbolic context ($C_i$)?

## Architecture Onboarding

- **Component map:** Data Point (Text, Noisy Label) -> KG Builder (Taxonomy Graph) -> Context Retriever (LCA Distance) -> Council (3 Parallel Agents) -> Aggregator (Weighted Vote + Boolean Override)

- **Critical path:** The **Data Analyst agent**. It is the only component with access to the `lca_dist` and the power to trigger the `strong_kg_signal` override. If this agent's query fails, precision drops significantly.

- **Design tradeoffs:** The system trades **latency for precision**. It is ~4x slower than a single LLM (Section 8). It also trades **recall on subtle errors** (hierarchical/leaf-node matches) for **perfect precision** on semantic errors.

- **Failure signatures:**
  - **False Negatives (Missed Errors):** Occurs on "Hierarchical Errors" where `lca_dist` = 0 (Section 5.1).
  - **Hallucination:** Occurs if the KG construction step fails and agents revert to pure semantic reasoning.

- **First 3 experiments:**
  1. **Baseline Test:** Run the Single LLM prompt (Appendix B, lines 3-7) to establish the "agreeableness" floor.
  2. **Ablation (No KG):** Run the Council *without* passing `graph_insight` to the Data Analyst (Listing 1, line 141) to measure the contribution of the KG.
  3. **Threshold Tuning:** Vary the `decision_threshold` (currently 2.0) and `agent_weights` to observe the shift between Precision and Recall.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the agent weighting and logic be optimized to capture ambiguous hierarchical errors ($lca\_dist=0$) without sacrificing the perfect precision achieved on semantic errors?
- Basis in paper: [explicit] Section 6 identifies "tuning the agent weights and logic to capture ambiguous, leaf-matched errors" as a clear area for future work.
- Why unresolved: The current override logic relies on structural signals ($lca\_dist > 0$), which are absent in hierarchical errors, forcing the system to rely on weaker semantic voting that fails 80% of the time.
- What evidence would resolve it: A comparative study on a dataset enriched with hierarchical noise showing improved recall (currently 20%) on leaf-matched errors while maintaining 1.00 Precision.

### Open Question 2
- Question: What is the stability of agent voting and final adjudication across multiple inference runs given the inherent non-determinism of LLMs?
- Basis in paper: [explicit] Section 6 states future work "must rigorously quantify the stability of agent voting across multiple runs, especially for ambiguous classification cases."
- Why unresolved: The reported results are likely from single inference passes; LLM stochasticity could alter the weighted vote outcomes or consensus, particularly in borderline cases.
- What evidence would resolve it: Variance metrics (standard deviation of F1-score) calculated over $k>10$ identical runs on the ambiguous "Hierarchical Error" subset of the data.

### Open Question 3
- Question: Can the dynamic KG construction and override logic be effectively adapted for unstructured domains lacking clear taxonomic hierarchies?
- Basis in paper: [explicit] Section 6 lists "Generalization Beyond Domain" as a threat, noting performance is dependent on a "well-defined domain schema."
- Why unresolved: The system currently relies on graph-based metrics like the Hierarchical Ancestor Distance (HAD), which cannot be computed on raw, unstructured text data.
- What evidence would resolve it: Successful application of the Adjudicator framework on a benchmark without explicit taxonomy (e.g., sentiment analysis of raw text) where the KG must be induced rather than queried.

## Limitations

- The AlleNoise dataset and its associated category taxonomy are not publicly available, creating a fundamental barrier to independent verification.
- The "Hierarchical Error" failure mode, where the system cannot detect errors with `lca_dist = 0`, represents a significant limitation in its applicability to domains with overlapping or ambiguous hierarchical structures.
- The paper's reliance on "hardened" adversarial prompts introduces brittlenessâ€”if these prompts are not perfectly replicated, the entire verification mechanism fails.

## Confidence

- **High Confidence:** The structural override mechanism (Mechanism 1) is well-specified and directly supported by equations and experimental results. The multi-agent architecture with differential voting weights is clearly described.
- **Medium Confidence:** The adversarial prompting approach (Mechanism 2) is well-documented in the paper's methodology, but the specific prompts are lengthy and context-dependent, making exact replication challenging.
- **Low Confidence:** The generalisability of the 0.99 F1-score to different datasets and domains remains uncertain due to the proprietary nature of the evaluation data.

## Next Checks

1. **Dataset Availability:** Attempt to contact the authors for access to the AlleNoise benchmark and the specific 1,000-item balanced subset used in evaluation.

2. **Threshold Sensitivity Analysis:** Replicate the experiments while systematically varying the `decision_threshold` (currently 2.0) and `agent_weights` to quantify the precision-recall tradeoff and identify optimal operating points.

3. **Hierarchical Error Case Study:** Construct a synthetic test case with intentionally designed hierarchical errors (same leaf node, different path) to empirically verify the documented `lca_dist = 0` failure mode and measure the extent of false negatives.