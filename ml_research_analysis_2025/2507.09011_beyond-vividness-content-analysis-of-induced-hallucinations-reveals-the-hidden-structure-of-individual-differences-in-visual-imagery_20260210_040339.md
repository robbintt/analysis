---
ver: rpa2
title: 'Beyond vividness: Content analysis of induced hallucinations reveals the hidden
  structure of individual differences in visual imagery'
arxiv_id: '2507.09011'
source_url: https://arxiv.org/abs/2507.09011
tags:
- imagery
- visual
- content
- vividness
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed over 4,000 free-text descriptions of hallucinations
  induced by a flickering visual stimulus (Ganzflicker) to investigate how individual
  differences in visual imagery ability relate to the content of internally generated
  visual experiences. Using computational methods including topic modeling, language
  model embeddings, and sensorimotor content analysis, the researchers found that
  individuals with stronger visual imagery reported more complex, naturalistic hallucinations
  (e.g., faces, scenes, structured imagery), while those with weaker imagery predominantly
  described simple geometric patterns and visual distortions.
---

# Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery

## Quick Facts
- arXiv ID: 2507.09011
- Source URL: https://arxiv.org/abs/2507.09011
- Reference count: 40
- Key outcome: Language models trained on both text and images outperformed text-only models in capturing individual differences in visual imagery ability from hallucination descriptions

## Executive Summary
This study analyzed over 4,000 free-text descriptions of hallucinations induced by a flickering visual stimulus (Ganzflicker) to investigate how individual differences in visual imagery ability relate to the content of internally generated visual experiences. Using computational methods including topic modeling, language model embeddings, and sensorimotor content analysis, the researchers found that individuals with stronger visual imagery reported more complex, naturalistic hallucinations (e.g., faces, scenes, structured imagery), while those with weaker imagery predominantly described simple geometric patterns and visual distortions. Language models trained on both text and images outperformed text-only models in capturing these differences, and participants with stronger imagery used language with richer sensorimotor associations. These findings support a layered model of visual imagery and demonstrate that language can reveal the hidden structure of individual differences in mental imagery.

## Method Summary
The study analyzed 4,365 English descriptions of Ganzflicker-induced hallucinations from Reeder (2022), with participants rating their imagery vividness on a 0-10 scale. Participants were grouped as weak (0-3, n=1,515), moderate (4-7, n=1,634), or strong (8-10, n=1,216) imagery. The analysis employed three computational approaches: (1) BERTopic-based unsupervised topic modeling (27 topics, C_v=0.50 coherence) to identify hallucination themes, (2) comparison of vision-language model (VLM) versus text-only language model embeddings using representational dissimilarity matrices, and (3) sensorimotor content analysis using Lancaster Norms to examine language patterns. Lasso-regularized logistic regression with 10-fold cross-validation and 1000 bootstraps was used for classification, with permutation tests confirming significance.

## Key Results
- Topic modeling revealed 27 distinct hallucination themes, with weak imagers predominantly describing geometric patterns and visual distortions while strong imagers reported faces, scenes, and complex imagery
- Vision-language models (CLIP r=.76, SigLIP r=.71) outperformed text-only models (BERT r=.67, GPT-2 r=.40, RoBERTa r=.30) in capturing imagery-related differences
- Participants with stronger imagery used language with richer sensorimotor associations, with head- and hand-related content being particularly predictive
- The moderate imagery group showed no reliable classification pattern (F1=0.48, p=.57), suggesting intermediate phenotypes may be heterogeneous

## Why This Works (Mechanism)

### Mechanism 1: Layered Visual Imagery Processing
Individual differences in visual imagery reflect variation in coordination between early visual areas and higher-order regions, affecting the complexity of internally generated visual content. Basic visual features (lines, edges, patterns) are processed in early visual cortex (V1/V2) across all individuals. Complex, naturalistic imagery (faces, scenes) requires top-down integration from higher-order areas (fusiform gyrus, parahippocampal place area). Weak imagers may have intact low-level representations but reduced capacity for top-down feature integration.

### Mechanism 2: Vision-Language Model Sensitivity to Perceptual Content
Vision-language models capture imagery-related distinctions in text better than text-only models because visual grounding provides sensitivity to perceptual features implied in language. VLMs like CLIP and SigLIP are trained on paired image-text data, aligning visual and textual representations. When processing hallucination descriptions, VLMs can leverage visual referential structure to detect perceptual dimensions that text-only models miss.

### Mechanism 3: Sensorimotor Richness as Internal Simulation Marker
Visual imagery vividness correlates with richer sensorimotor language use, suggesting stronger imagers engage broader multisensory simulation networks. Strong imagers access visual, haptic, olfactory, and auditory associations when describing experiences. Head- and hand-related motor content (e.g., "looking around," "reaching") are particularly predictive, possibly reflecting specialized neural circuitry for faces and actions.

## Foundational Learning

- **Concept: Visual imagery spectrum (aphantasia to hyperphantasia)**
  - Why needed here: The entire paper depends on understanding that imagery ability varies dramatically across individuals—not just in vividness but in content complexity.
  - Quick check question: Can you explain why measuring "vividness" alone might miss important structural differences in what people can internally represent?

- **Concept: Top-down vs. bottom-up processing in vision**
  - Why needed here: The layered model hinges on distinguishing early visual cortex (bottom-up feature extraction) from higher-order integration (top-down construction).
  - Quick check question: Why would aphantasics still report seeing geometric patterns during Ganzflicker if they lack voluntary imagery?

- **Concept: Topic modeling with BERTopic**
  - Why needed here: The paper uses unsupervised topic modeling to discover 27 hallucination themes; understanding this is essential for interpreting results.
  - Quick check question: Why use c-TF-IDF weighting instead of standard TF-IDF for topic extraction?

## Architecture Onboarding

- **Component map:** Data ingestion (free-text descriptions + vividness ratings) -> Topic modeling pipeline (Sentence-BERT -> UMAP -> HDBSCAN -> c-TF-IDF) -> Embedding analysis (6 language models -> RDM computation) -> Sensorimotor analysis (Lancaster Norms -> GLM regression)
- **Critical path:** Topic modeling results feed into Lasso classifiers for imagery group prediction (F1 = 0.54 weak, 0.44 strong). Classifier coefficients identify which topics distinguish groups.
- **Design tradeoffs:** Sentence-level vs. document-level analysis: Sentences capture distinct experiential elements but lose document context. VLM selection matters: BLIP failed (r = .03) while CLIP/SigLIP succeeded—architectural differences (contrastive vs. autoregressive) are critical. Binning continuous vividness into 3 groups enables classification but moderate group showed no reliable pattern (p = .57).
- **Failure signatures:** Moderate imagery group classifier performed at chance—intermediate phenotypes may be genuinely heterogeneous. Gustatory dimension showed negative coefficient despite slight upward raw trend—low base rate (mean = 0.30) introduced noise. BLIP VLM failed to recover imagery structure despite being multimodal—objective function matters more than modality.
- **First 3 experiments:**
  1. Replication with neuroimaging: Test whether hallucination content (geometric vs. naturalistic) predicts differential activation in early visual cortex vs. fusiform/parahippocampal regions.
  2. Cross-paradigm validation: Apply the same topic modeling pipeline to voluntary imagery descriptions (not hallucinations) to test generalization of content differences.
  3. VLM ablation study: Systematically compare VLM architectures (contrastive vs. generative, different training objectives) on imagery-related text discrimination to isolate which design choices matter.

## Open Questions the Paper Calls Out

### Open Question 1
Do the content differences identified in hallucination descriptions correspond to differential activation patterns in early visual cortex versus higher-order areas (such as the fusiform face area and parahippocampal place area)? The conclusion explicitly states future neuroimaging studies should examine whether the content differences identified through language analysis correspond to differential activation in specific visual processing areas. This is unresolved because the current study relied on behavioral data rather than direct neural measurement.

### Open Question 2
Do the systematic content differences observed in Ganzflicker-induced hallucinations generalize to other forms of internal visual processing, such as voluntary visual imagery, memory visualization, or dreaming? The discussion states establishing this broader claim would require further work to determine whether the content differences generalize to other forms of internal visual processing. This is unresolved because the study was restricted to a single paradigm (Ganzflicker).

### Open Question 3
Is the reduction in complex hallucination content in weak imagers caused specifically by impaired top-down connectivity between frontoparietal control networks and visual areas? The paper proposes a "layered model" suggesting weak imagers have intact low-level processing but reduced capacity for top-down integration. This is inferred from behavioral text analysis rather than direct neural connectivity measurement.

### Open Question 4
Why did interoceptive content fail to predict visual imagery vividness in this dataset, contrary to recent theoretical proposals linking interoception to aphantasia? The discussion notes interoceptive content was not significantly predictive, contrasting with recent theoretical proposals emphasizing interoception's central role in imagery differences. It is unclear if this null result is due to the specific linguistic norms used, the Ganzflicker paradigm itself, or if the interoceptive theories of aphantasia require revision.

## Limitations

- Findings hinge on whether hallucination content truly reflects stable imagery differences rather than task-specific responses to the flickering stimulus
- The moderate imagery group showed no reliable pattern (F1 = 0.48, p = .57), suggesting the relationship may be non-linear or the intermediate phenotype genuinely heterogeneous
- While sensorimotor language use correlates with imagery strength, this could reflect verbal fluency differences rather than actual perceptual simulation capacity

## Confidence

- **High Confidence:** Topic modeling successfully identified distinct hallucination themes (27 topics, C_v = 0.50) and revealed systematic differences between imagery groups
- **Medium Confidence:** VLM superiority (CLIP r = .76 vs BERT r = .67) reflects genuine perceptual grounding rather than dataset artifacts
- **Medium Confidence:** Sensorimotor associations indicate broader simulation networks in strong imagers, though verbal expressiveness remains an alternative explanation

## Next Checks

1. Test whether hallucination content differences predict differential activation patterns in early visual cortex versus fusiform/parahippocampal regions during neuroimaging
2. Apply the same topic modeling pipeline to voluntary imagery descriptions to determine if content differences generalize beyond the Ganzflicker paradigm
3. Conduct systematic ablation studies comparing VLM architectures to isolate which design choices (contrastive vs generative, training objectives) enable imagery-related text discrimination