---
ver: rpa2
title: Generating time-consistent dynamics with discriminator-guided image diffusion
  models
arxiv_id: '2505.09089'
source_url: https://arxiv.org/abs/2505.09089
tags:
- video
- diffusion
- time
- guidance
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating time-consistent
  spatiotemporal dynamics using image diffusion models (IDMs) without costly retraining.
  The authors propose a novel time-consistency discriminator that guides the sampling
  process of pretrained IDMs to generate realistic video sequences.
---

# Generating time-consistent dynamics with discriminator-guided image diffusion models

## Quick Facts
- arXiv ID: 2505.09089
- Source URL: https://arxiv.org/abs/2505.09089
- Reference count: 40
- Key outcome: This work addresses the challenge of generating time-consistent spatiotemporal dynamics using image diffusion models (IDMs) without costly retraining. The authors propose a novel time-consistency discriminator that guides the sampling process of pretrained IDMs to generate realistic video sequences.

## Executive Summary
This paper introduces a novel approach for generating time-consistent spatiotemporal dynamics using pretrained image diffusion models without requiring costly retraining. The method employs a discriminator trained to distinguish temporally consistent from random samples, which provides gradient guidance during the reverse diffusion process. The approach achieves temporal consistency comparable to video diffusion models while maintaining better uncertainty calibration, lower bias, and notably, stable long-term climate simulations where video diffusion models exhibit drift.

## Method Summary
The method uses a pretrained image diffusion model (IDM) for unconditional frame generation, then applies discriminator guidance during sampling to enforce temporal consistency. A separate discriminator is trained to classify whether a candidate next frame is temporally consistent with previous frames, conditioned on both the diffusion noise level and past frames. During inference, the discriminator's gradient is added to the IDM's score function, effectively shifting sampling toward temporally coherent trajectories. This guidance operates without modifying the base IDM architecture or retraining, enabling stable long-term rollouts on physical systems like 2D Navier-Stokes turbulence and global precipitation data.

## Key Results
- Achieves temporal consistency matching video diffusion models (VDMs) on ACF and Wasserstein metrics
- Demonstrates better uncertainty calibration (spread-skill ratio ~1) and lower bias than VDMs
- Enables stable centennial-scale climate simulations (>100 years) without the drift observed in VDMs
- Adds only 3-8% computational overhead to generation time

## Why This Works (Mechanism)

### Mechanism 1: Discriminator-Guided Score Modification
The discriminator provides gradient guidance that approximates conditional sampling by learning to distinguish temporally consistent samples p(x_{n+1}|x_{n-m:n}) from random samples p(x_{n+1}). The log-odds gradient ∇x_{n+1} log[D_θ/(1-D_θ)] equals ∇x_{n+1} log[p(conditional)/p(unconditional)], which is added to the unconditional score during reverse diffusion. This shifts the sampling trajectory toward temporally coherent regions of the data manifold.

### Mechanism 2: Noise-Conditioned Temporal Discrimination
The discriminator takes (x_{n+1}^t, x_{n-m:n}^0, t) as input—evaluating consistency between a noised future frame and clean past frames at each noise level. This allows the discriminator to provide meaningful gradients early in sampling when images are highly corrupted, preventing accumulation of temporal errors.

### Mechanism 3: Importance Sampling for Hard Negatives
The discriminator is trained with temporally-adjacent hard negatives (rather than random frames) using importance sampling l ~ NZ(μ=1, σ²_step=2) \ {1}. This forces the discriminator to learn fine-grained temporal dynamics rather than trivial differences, improving discrimination at decision boundaries.

## Foundational Learning

- **Score-based diffusion and reverse SDEs**: Why needed - The guidance operates by modifying the score function ∇x_t log p_t(x_t) in the reverse diffusion process. Quick check - Can you explain why adding a term to the score changes the sampling distribution, and how the reverse SDE differs from the forward noising process?

- **Classifier/discriminator guidance in diffusion models**: Why needed - This work extends classifier guidance (Dhariwal & Nichol 2021) to temporal consistency. Quick check - What is the relationship between classifier guidance strength λ and the tradeoff between sample quality and condition satisfaction?

- **GAN discriminator theory (density ratio estimation)**: Why needed - The discriminator implicitly estimates p(consistent)/p(random), and the log-odds transformation yields the guidance gradient. Quick check - Why does an optimal discriminator's log-odds equal the log-density ratio, and what happens if the discriminator is non-optimal?

## Architecture Onboarding

- Component map: Pretrained IDM (frozen) -> provides unconditional score s_ϕ(x_t, t) -> Reverse SDE sampler <- receives guidance term d_θ <- Time-consistency discriminator D_θ

- Critical path: 1) Train unconditional IDM on single frames, 2) Train discriminator on triplet data with consistent/inconsistent labels, 3) At inference, for each autoregressive step: sample initial noise, compute s_ϕ + λ·d_θ at each denoising step, advance reverse SDE, 4) Apply guidance in both 1st and 2nd order solver steps

- Design tradeoffs: Guidance strength λ (higher → more consistency but potential over-constraint), conditioning history m (paper finds m=1 optimal), discriminator size (smaller is faster but may miss dynamics), sampling steps (50-100 used)

- Failure signatures: VDM-style drift (verify via global mean time series), temporal flickering (check autocorrelation decay), over-smoothing (check Wasserstein distance between consecutive frames), discriminator overfitting (monitor training accuracy)

- First 3 experiments: 1) Sanity check on synthetic data (sinusoidal wave), 2) Ablation on guidance strength λ, 3) Long-rollout stability test (10,000+ step sequences comparing global mean over time)

## Open Questions the Paper Calls Out

- Can modifications to the discriminator architecture or training improve short-term forecast skill? Current design prioritizes long-term stability over immediate forecast accuracy.

- Can discriminator guidance stabilize Video Diffusion Models (VDMs) during long-term rollouts? Experiments only applied guidance to Image Diffusion Models, not VDMs.

- How does the method perform on multivariate simulations? The paper only considers univariate simulations, with extensions to multiple variables noted as straightforward.

## Limitations
- The approach assumes the base IDM has learned reasonable spatial structure; poor spatial fidelity may cause guidance to generate temporally consistent but physically unrealistic sequences
- The importance sampling strategy for discriminator training relies on choosing appropriate σ_step; suboptimal choices could lead to either trivial discrimination or unstable training
- While the paper demonstrates stability for >100-year simulations, theoretical guarantees for arbitrary dynamical systems are not established

## Confidence
- High Confidence: The discriminator guidance mechanism is mathematically sound and consistent with established classifier guidance theory. Experimental results on temporal consistency metrics are robust across multiple datasets.
- Medium Confidence: The computational efficiency claims and climate simulation stability are well-supported but depend on specific implementation choices that may vary with different datasets or hardware.
- Low Confidence: The generalization of the negative sampling strategy to datasets with different temporal characteristics has not been thoroughly explored.

## Next Checks
1. Test the approach with different noise schedules (e.g., cosine schedule, VP schedule) to verify that discriminator guidance remains effective when the diffusion process changes.
2. Apply the method to datasets with substantially different temporal characteristics (e.g., astronomical time series, financial data) to assess robustness beyond the current physical systems.
3. Systematically vary the σ_step parameter and sampling strategy for negative examples to determine optimal settings and understand sensitivity to this hyperparameter.