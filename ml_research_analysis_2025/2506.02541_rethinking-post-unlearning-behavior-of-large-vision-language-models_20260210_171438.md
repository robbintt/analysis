---
ver: rpa2
title: Rethinking Post-Unlearning Behavior of Large Vision-Language Models
arxiv_id: '2506.02541'
source_url: https://arxiv.org/abs/2506.02541
tags:
- unlearning
- pubg
- entity
- arxiv
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing LVLM unlearning methods often result in undesirable post-unlearning
  behaviors such as degenerate, hallucinated, or excessively refused responses. We
  address this by introducing a new unlearning task that requires LVLMs to provide
  privacy-preserving yet informative and visually grounded responses.
---

# Rethinking Post-Unlearning Behavior of Large Vision-Language Models

## Quick Facts
- **arXiv ID**: 2506.02541
- **Source URL**: https://arxiv.org/abs/2506.02541
- **Reference count**: 16
- **Primary result**: PUBG achieves high image-text alignment and informativeness while maintaining perfect unlearning success, avoiding degenerate/hallucinated/refusal outputs.

## Executive Summary
Existing LVLM unlearning methods often produce undesirable post-unlearning behaviors such as degenerate, hallucinated, or excessively refused responses. This work introduces PUBG, a method that addresses these issues by guiding models toward a desirable output distribution using in-context prompting and KL divergence minimization. The approach generates informative, visually grounded responses without privacy leakage, achieving strong performance across multiple metrics.

## Method Summary
PUBG combines Gradient Ascent loss (LGA) to suppress private information with Behavior Guidance loss (LBG) using KL divergence to a reference distribution. The reference distribution is obtained via in-context prompting of the original model with instructions for visual-only description. The method uses LoRA fine-tuning with a total loss combining LGA, LBG, and retain-set preservation. Training involves 30 steps with AdamW optimizer on A100 GPU, using celeb-1000 dataset with entities filtered by recognition status.

## Key Results
- PUBG generates informative, visually grounded responses without privacy leakage
- Achieves high image-text alignment (CLIP-SCORE) and informativeness (JUDGE inform)
- Maintains perfect unlearning success rate (USR) while avoiding model collapse and excessive refusal
- Outperforms baselines (NPO, REJECT) on seen and unseen image generalization

## Why This Works (Mechanism)
PUBG works by simultaneously suppressing private information through LGA while guiding the model toward a reference distribution of desirable alternative outputs via LBG. The in-context prompting leverages the original model's instruction-following capabilities to generate privacy-preserving yet informative responses that serve as targets. This dual approach prevents the model from collapsing into degenerate outputs or excessive refusals while maintaining privacy guarantees.

## Foundational Learning
- **LVLM unlearning**: Removing entity-specific knowledge while preserving general functionality - needed because naive unlearning causes behavioral degradation
- **KL divergence minimization**: Measuring and minimizing distribution differences - needed to align generated outputs with reference distribution
- **In-context prompting**: Using examples within prompts to guide model behavior - needed to generate quality reference distributions
- **LoRA fine-tuning**: Parameter-efficient adaptation using low-rank adapters - needed for efficient unlearning without full model retraining
- **CLIP-SCORE**: Measuring image-text alignment via pretrained vision-language model - needed to verify visual grounding of responses
- **TF-IDF similarity**: Quantifying semantic overlap between outputs - needed for measuring unlearning success

## Architecture Onboarding

**Component Map**: Original LVLM -> In-context prompt generator -> Reference distribution -> PUBG loss computation -> LoRA adapter -> Fine-tuned LVLM

**Critical Path**: Image input → LVLM with LoRA adapter → PUBG loss (LGA + LBG + Lretain) → Parameter update → Output generation

**Design Tradeoffs**: LGA alone causes information loss and hallucination; LBG alone fails unlearning; PUBG balances both. Uses LoRA for efficiency vs full fine-tuning. In-context prompting vs manual reference creation.

**Failure Signatures**: Model collapse (empty/repetitive outputs) indicates LGA dominance; hallucination indicates insufficient LGA; over-refusal indicates LBG dominance or poor reference quality.

**First Experiments**:
1. Verify LGA-only baseline produces degenerate outputs and fails unlearning
2. Test LBG-only baseline for hallucination while preserving some privacy
3. Confirm PUBG with balanced losses achieves both privacy and informativeness

## Open Questions the Paper Calls Out
- Can PUBG framework generalize to pure LLMs and more complex unlearning scenarios beyond vision-language tasks?
- How well does PUBG scale to larger forget-entity sets (hundreds/thousands) compared to tested 5-20 entities?
- How does reference distribution quality affect PUBG when original model has weaker instruction-following capabilities?

## Limitations
- Evaluation limited to 5-20 entities; scalability to real-world unlearning demands unexplored
- Method relies on original model's strong instruction-following; may not generalize to weaker models
- Reference distribution generation via in-context prompting may be suboptimal for some model architectures

## Confidence
- **High confidence**: PUBG methodology is sound with substantial, consistent improvements over baselines
- **Medium confidence**: Dataset filtering criteria and LVLM judge evaluation protocol introduce potential subjectivity
- **Medium confidence**: LoRA implementation mostly complete but lacks detailed learning rate sensitivity analysis

## Next Checks
1. Implement systematic ablation varying LGA and LBG weights to confirm both components are necessary
2. Test PUBG on a second entity dataset (CelebA or VGGFace) to validate generalizability
3. Conduct human evaluation of JUDGE scores to assess alignment with automated judgment