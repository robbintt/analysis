---
ver: rpa2
title: Reading Recognition in the Wild
arxiv_id: '2505.24848'
source_url: https://arxiv.org/abs/2505.24848
tags:
- reading
- gaze
- dataset
- modalities
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task called "reading recognition" to
  determine when a user is reading, enabling AI assistants in always-on smart glasses.
  The authors present the first-of-its-kind large-scale multimodal "Reading in the
  Wild" dataset containing 100 hours of reading and non-reading videos in diverse
  real-world scenarios.
---

# Reading Recognition in the Wild

## Quick Facts
- **arXiv ID:** 2505.24848
- **Source URL:** https://arxiv.org/abs/2505.24848
- **Reference count:** 40
- **Primary result:** 86.9% accuracy in reading recognition using RGB, gaze, and IMU modalities combined

## Executive Summary
This paper introduces a novel task called "reading recognition" to determine when users are reading in real-world scenarios using always-on smart glasses. The authors present the first-of-its-kind "Reading in the Wild" dataset containing 100 hours of multimodal data across diverse environments. They demonstrate that combining egocentric RGB, eye gaze, and head pose modalities significantly outperforms single-modality approaches, achieving 86.9% accuracy. The lightweight transformer-based model operates efficiently on-device while generalizing well to unseen scenarios and languages, though performance drops on non-standard reading directions and multitasking scenarios.

## Method Summary
The method employs a multimodal transformer architecture that fuses egocentric RGB, eye gaze, and IMU data to detect reading activity. RGB frames are cropped to 64×64 pixels around the gaze point (5° FoV) for efficiency. Gaze and IMU data are processed through 1D convolutional encoders, while RGB uses 2D convolutions, all feeding into a shared lightweight transformer (3 layers, 32 dims, 2 heads). Training incorporates modality dropout (randomly dropping modalities) and gaze rotation augmentation. The model operates on 2-second temporal windows and achieves real-time performance with 137k parameters.

## Key Results
- 86.9% accuracy when combining all three modalities (RGB, gaze, IMU)
- 82.2% accuracy using gaze-centered RGB crops (5° FoV) with gaze-only features
- Strong generalization to unseen scenarios (Seattle → Columbus) for left-to-right scripts
- Performance drops to 55.5% on "reading while writing/typing" multitasking scenarios

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Disambiguation via Complementary Sensors
Combining eye gaze, RGB, and IMU data resolves ambiguities that cause single-modality models to fail, such as distinguishing "reading" from "looking at text." The model fuses distinct error distributions: Gaze detects reading patterns even if visual text is occluded or low-resolution; RGB identifies short-text reading (e.g., signs) where gaze scanpaths are subtle; IMU disambiguates head motion (e.g., turning vs. reading lines).

### Mechanism 2: Foveal Efficiency via Gaze-Conditioned Cropping
Reducing the input RGB field of view to a small crop around the gaze point retains reading-relevant features while drastically lowering compute. Physiologically, human reading resolution is limited to the fovea (~2°). By cropping the egocentric image to 5° (64px) around the gaze, the model discards background noise and focuses computation on the high-probability text region, avoiding the need for full-frame OCR.

### Mechanism 3: End-to-End Temporal Pattern Recognition
A transformer operating on raw or lightly processed temporal streams (gaze/IMU) outperforms traditional handcrafted features (fixations/saccades) when data is abundant. Instead of manually engineering "fixation" detectors, the model uses 1D convolutions and self-attention to learn the characteristic "left-to-right" sweep of reading directly from time-series data.

## Foundational Learning

- **Concept: Egocentric Geometry & Foveation**
  - **Why needed here:** Understanding why the model crops to 1/484th of the image requires knowing that human vision is high-resolution only at the center (fovea).
  - **Quick check question:** Why does the architecture accept a 64x64 crop rather than the full 1408p frame?

- **Concept: Modality Dropout**
  - **Why needed here:** This training technique is critical for the model to function when a sensor fails or is disabled (e.g., saving power by turning off the RGB camera).
  - **Quick check question:** How does dropping entire modalities during training improve inference robustness?

- **Concept: Sequence Windowing (Latency vs. Duration)**
  - **Why needed here:** The model requires a time window (T=2s) to detect reading patterns, creating a trade-off between detection confidence and real-time responsiveness.
  - **Quick check question:** What happens to detection latency if the input window duration is increased from 1s to 3s?

## Architecture Onboarding

- **Component map:** Project Aria Sensors (RGB Camera, Eye Tracking, IMU) -> Gaze-Centered Crop (64px) + 1D/2D Convs -> Transformer Encoder (3L, 32D, 2H) -> Linear Head -> Sigmoid

- **Critical path:**
  1. Calibration: Eye-tracking must be calibrated to generate meaningful 3D gaze rays
  2. Alignment: Gaze timestamps must sync with RGB frames to extract the correct crop
  3. Fusion: Token concatenation must occur after positional embeddings are applied

- **Design tradeoffs:**
  - Accuracy vs. Efficiency: Using the full RGB frame would likely improve accuracy on "contextual" reading but violates the on-device compute budget (137k params limit)
  - Latency vs. Stability: A longer input window (T) stabilizes predictions (less flickering) but delays the "Reading Start" signal

- **Failure signatures:**
  - Hard Negatives: Model predicts "Reading" when the user is looking at a text-rich environment (e.g., a bookshelf) but not actually parsing words
  - Multitasking: Performance drops (to ~55%) when reading while writing/typing, as gaze patterns become erratic
  - Language Drift: Model fails on Right-to-Left (Arabic) or Vertical (Chinese) text unless specific inference-time flips/rotations are applied

- **First 3 experiments:**
  1. Modality Ablation: Run inference on the Seattle test set using Gaze-only, RGB-only, and Combined models to reproduce the performance gap (Table 5a)
  2. Crop Sensitivity: Test the Gaze+RGB model while intentionally injecting Gaussian noise into the gaze coordinates to measure robustness to eye-tracker jitter (Figure 8)
  3. Language Generalization: Evaluate the trained model on the Columbus subset (unseen scenarios) specifically filtering for non-English scripts to verify the failure modes and augmentation fixes (Table 3b)

## Open Questions the Paper Calls Out
- **Can the model be personalized to individual reading speeds and styles to improve performance on diverse users without extensive calibration?** The authors identify "model personalization to address variations in reading speed and style" as a key area for future work, noting that scaling gaze magnitude empirically resolves some failure cases.
- **Can a dynamic mechanism predict the optimal activation of specific modalities (RGB, Gaze, IMU) to maximize efficiency on resource-constrained devices?** The authors explicitly list "predicting optimal modality activation for enhanced efficiency" as a promising direction for future research.
- **How can the architecture be improved to distinguish reading from "reading while writing/typing" or "browsing," where gaze patterns are ambiguous?** The results show the model achieves only 55.5% accuracy on "reading while writing/typing" and struggles to differentiate scanning from engaged reading.

## Limitations
- Performance degrades significantly on non-standard reading directions (right-to-left, vertical scripts) without specific augmentation
- The "hard negatives" problem (false positives on text-rich environments) remains partially unsolved
- The model relies heavily on high-quality eye-tracking calibration, which may not be practical in all real-world scenarios

## Confidence
- **High Confidence:** Multimodal fusion performance claims (86.9% accuracy with all three modalities) - directly supported by ablation studies in Table 5
- **Medium Confidence:** Real-time capability claims - while the model is computationally efficient, the 2-second window requirement creates inherent detection latency
- **Medium Confidence:** Generalization claims - strong performance on unseen scenarios is demonstrated but primarily for left-to-right scripts

## Next Checks
1. **Multitasking Scenario Test:** Evaluate the model's performance on the specific "Reading + Writing" subset (where accuracy drops to 55%) to quantify the degradation in realistic multitasking scenarios
2. **Cross-Device Transferability:** Test the trained model on eye-tracking hardware with different specifications (sampling rates, accuracy) to assess true generalization beyond the Project Aria device
3. **Temporal Consistency Analysis:** Measure the model's false start/stop rates by analyzing the duration of incorrectly predicted reading segments in the test set, quantifying the practical utility of the 2-second detection window