---
ver: rpa2
title: 'HealthBench: Evaluating Large Language Models Towards Improved Human Health'
arxiv_id: '2505.08775'
source_url: https://arxiv.org/abs/2505.08775
tags:
- healthbench
- response
- health
- user
- physicians
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HealthBench is a comprehensive, open-source benchmark for evaluating
  large language models in healthcare, addressing the need for meaningful, trustworthy,
  and unsaturated evaluations. It consists of 5,000 realistic multi-turn conversations
  spanning seven themes and five behavioral axes, with responses graded against physician-created
  rubrics.
---

# HealthBench: Evaluating Large Language Models Towards Improved Human Health

## Quick Facts
- arXiv ID: 2505.08775
- Source URL: https://arxiv.org/abs/2505.08775
- Authors: Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, Karan Singhal
- Reference count: 40
- Primary result: HealthBench is a comprehensive, open-source benchmark with 5,000 multi-turn health conversations across 7 themes, using physician-written rubrics to evaluate LLM responses against 48,562 criteria.

## Executive Summary
HealthBench is an open-source benchmark designed to evaluate large language models (LLMs) on realistic health conversations. Developed with input from over 262 physicians across 60 countries, it addresses the need for meaningful, trustworthy, and unsaturated evaluations in healthcare AI. The benchmark spans seven themes including emergency referrals, global health, and context-seeking, with responses graded against detailed physician-created rubrics.

Recent model evaluations show substantial improvements: GPT-4.1 nano outperforms GPT-4o while being 25x cheaper, and o3 achieves 60% accuracy on HealthBench. The validated HealthBench Consensus subset of 34 critical criteria shows model error rates have fallen 4x from GPT-3.5 to GPT-4.1. Despite progress, context-seeking remains challenging, with models scoring only 0.01-0.50 on "not enough context" scenarios versus 0.89-0.99 when context is sufficient.

## Method Summary
HealthBench evaluates LLM responses to 5,000 multi-turn health conversations using physician-written rubrics with 48,562 criteria across seven themes and five behavioral axes. Each criterion is scored as met/not-met by a model-based grader (GPT-4.1 by default), with per-example scores calculated from earned points divided by maximum possible points. The benchmark includes both conversation-specific criteria and a validated consensus subset of 34 critical criteria. Model responses are generated for the final user message in each conversation, then graded independently against all applicable rubric criteria.

## Key Results
- GPT-4.1 nano outperforms GPT-4o while being 25x cheaper, demonstrating rapid model improvements
- o3 achieves 60% accuracy, with test-time compute improvements showing the benefit of chain-of-thought reasoning
- HealthBench Consensus shows model error rates have fallen 4x from GPT-3.5 to GPT-4.1
- Context-seeking remains the weakest theme, with scores of only 0.01-0.50 for "not enough context" scenarios
- Worst-at-k reliability shows performance decay, with o3 dropping from 60% to ~40% at worst-at-16

## Why This Works (Mechanism)

### Mechanism 1: Rubric-based evaluation design
- **Claim:** Model-based grading can approximate physician judgment when rubric criteria are specific, objective, and conversation-specific.
- **Mechanism:** Rubrics decompose complex clinical evaluation into discrete binary-checkable criteria, reducing ambiguity and enabling consistent automated scoring. The model grader evaluates each criterion independently rather than making holistic judgments.
- **Core assumption:** Physician judgment about response quality can be operationalized into explicit, gradable criteria that preserve fidelity to expert evaluation.
- **Evidence anchors:** 48,562 criteria across 5,000 conversations; GPT-4.1 grader exceeds average physician score in five of seven themes; Health-SCORE paper validates rubric scalability but notes creation cost challenges.
- **Break condition:** When criteria are inherently subjective (e.g., "appropriate empathy") or when rubrics fail to capture implicit clinical knowledge that physicians apply without articulation.

### Mechanism 2: Test-time compute scaling
- **Claim:** Extended inference allows models to verify factual accuracy, identify missing information, consider multiple clinical scenarios, and iteratively revise responses for completeness and style.
- **Mechanism:** Chain-of-thought reasoning enables self-verification and completeness checking that improves performance on complex health tasks.
- **Core assumption:** Health tasks benefit from explicit multi-step reasoning rather than direct generation; the model can productively use additional computation time.
- **Evidence anchors:** o3 achieves 60% accuracy compared to GPT-3.5 Turbo's 16%; improvements observed across low, medium, and high reasoning settings.
- **Break condition:** When required medical knowledge is absent from training data, or when the conversation requires real-time clinical data access that reasoning cannot substitute for.

### Mechanism 3: Context-seeking challenges
- **Claim:** Models systematically underperform on context-seeking because recognizing missing information requires meta-cognitive uncertainty modeling that current architectures handle poorly.
- **Mechanism:** Effective context-seeking requires detecting insufficient information, identifying most diagnostic missing information, and formulating targeted questions without premature recommendations.
- **Core assumption:** The ability to recognize what you don't know is cognitively distinct from and potentially harder than applying what you do know.
- **Evidence anchors:** Performance on context-seeking lagging behind other themes; GPT-4.1 scores 0.1878 for context-seeking when not enough context provided vs 0.89-0.99 for "enough context" scenarios.
- **Break condition:** When missing context is obvious from conversation structure, or when users explicitly flag uncertainty.

## Foundational Learning

- **Concept: Rubric-based evaluation design**
  - **Why needed here:** Understanding how HealthBench decomposes physician judgment into 48,562 gradable criteria is essential for interpreting scores and identifying improvement areas.
  - **Quick check question:** Can you explain why the paper uses conversation-specific rubrics rather than a single universal scoring rubric?

- **Concept: Test-time compute scaling**
  - **Why needed here:** The performance gains from o3's reasoning (60% vs 32% for GPT-4o) depend on understanding how chain-of-thought improves health task performance.
  - **Quick check question:** What specific benefits does the paper claim extended reasoning provides for health conversations?

- **Concept: Worst-at-k reliability analysis**
  - **Why needed here:** In healthcare, worst-case performance matters more than average performance; the paper shows o3's worst-at-16 score drops by one-third from its overall score.
  - **Quick check question:** Why might a model with high average HealthBench scores still be unsafe for deployment?

## Architecture Onboarding

- **Component map:**
  Conversation dataset (5,000 multi-turn conversations) -> Rubric system (48,562 criteria) -> Model-based grader (GPT-4.1) -> Theme classification (7 themes) -> Axis classification (5 axes) -> Scoring (per-criterion binary scoring → example-level aggregation → clipped mean)

- **Critical path:**
  1. Load conversation and associated rubric criteria
  2. Generate model response to final user message
  3. Grade response against each rubric criterion independently
  4. Compute example score (points earned / max possible points)
  5. Aggregate across examples with clipping to [0, 1]

- **Design tradeoffs:**
  - **Example-specific vs consensus rubrics:** Example-specific provides breadth (48,562 criteria) but lower validation; consensus (34 criteria) provides validated precision but narrow coverage
  - **Grader model selection:** GPT-4.1 chosen for cost-performance; smaller models (nano) substantially worse at grading (MF1 = 0.580)
  - **Length control:** Scores partially correlated with response length (r = 0.123 for o3); length-controlled win rates show 6-9% lower performance than uncontrolled

- **Failure signatures:**
  - **Context-seeking gap:** Models score 0.01-0.50 on "not enough context" vs 0.89-0.99 on "enough context"
  - **Global health adaptation:** Performance drops when healthcare context matters but is unclear (0.47-0.88 range across models)
  - **Reliability decay:** Worst-at-k scores decrease with k; o3 drops from 60% overall to ~40% at worst-at-16
  - **Physician disagreement:** Physician-physician agreement ranges 55-75% on consensus criteria grading

- **First 3 experiments:**
  1. Run baseline evaluation on your target model across all 7 themes to identify which themes have the largest gaps vs frontier performance (focus on context-seeking and global health)
  2. Evaluate worst-at-16 reliability to understand deployment risk profile, not just average performance
  3. Test on HealthBench Hard subset (1,000 examples) to establish ceiling for your model architecture

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does HealthBench performance correlate with real-world health outcomes in deployed clinical settings?
- **Basis in paper:** HealthBench does not measure health outcomes of specific workflows, which depend on both model response quality and implementation. Real-world studies measuring both quality and outcomes will be important future work.
- **Why unresolved:** HealthBench evaluates response quality, not downstream effects like patient outcomes, time savings, or care quality, which require longitudinal deployment studies.
- **What evidence would resolve it:** Correlation analysis between HealthBench scores and measured outcomes (mortality, diagnosis accuracy, clinician time savings, patient satisfaction) in real-world deployments.

### Open Question 2
- **Question:** Can targeted training methods improve context-seeking behavior when users provide underspecified queries?
- **Basis in paper:** There is still substantial room for improvement in context-seeking behavior, with context-seeking showing the lowest performance among themes (GPT-4.1 at 0.1878 for context-seeking when not enough context provided).
- **Why unresolved:** Models frequently fail to seek clarifying context even when critical information is missing.
- **What evidence would resolve it:** Demonstrated improvements in context-seeking scores on HealthBench Consensus criteria after targeted fine-tuning or prompting strategies.

### Open Question 3
- **Question:** What factors explain the variability in physician-physician agreement (55-75%) when evaluating model responses against consensus criteria?
- **Basis in paper:** Reasons for variation could include ambiguity in criteria, ambiguity in conversations and responses, and differences in clinical specialization, risk tolerance, perceived severity, communication style, and interpretation of instructions.
- **Why unresolved:** The paper identifies multiple potential causes but does not isolate their relative contributions to disagreement.
- **What evidence would resolve it:** Controlled experiments varying one factor (e.g., clinical specialty, criteria clarity) at a time to measure impact on inter-rater agreement.

## Limitations

- Clinical validity remains uncertain without real-world outcome validation; synthetic conversations may not fully capture clinical interaction complexity
- Rubric creation is labor-intensive, with consensus rubrics covering only 34 of 48,562 criteria, raising questions about comprehensive coverage
- Model-based grading reliability depends on rubric specificity; subjective criteria like "appropriate empathy" remain challenging to evaluate consistently

## Confidence

- **High confidence:** Performance improvements across model generations (GPT-3.5 to GPT-4.1) and validity of model-based grading as a scalable evaluation method
- **Medium confidence:** Clinical relevance and safety implications of HealthBench scores given lack of real-world outcome validation
- **Medium confidence:** Generalizability across healthcare systems, particularly for global health scenarios where performance varies with context clarity

## Next Checks

1. Conduct external validation with healthcare practitioners from diverse systems to assess whether HealthBench scores correlate with real-world clinical safety and effectiveness across different healthcare contexts.

2. Test the worst-at-k reliability metric with additional model families to determine if the observed reliability decay is architecture-specific or a general characteristic of LLM performance on health tasks.

3. Evaluate the benchmark's sensitivity to cultural and linguistic variations by testing model performance on HealthBench conversations adapted for non-US healthcare contexts and comparing against domain-expert physician judgments.