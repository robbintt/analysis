---
ver: rpa2
title: 'Bridging Accuracy and Interpretability: Deep Learning with XAI for Breast
  Cancer Detection'
arxiv_id: '2510.21780'
source_url: https://arxiv.org/abs/2510.21780
tags:
- cancer
- class
- mean
- value
- breast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a deep learning model for breast cancer detection
  using features extracted from digitized fine needle aspirate images. The model employs
  ReLU activations, Adam optimizer, and binary cross-entropy loss, achieving high
  performance with accuracy of 0.992, precision of 1.000, recall of 0.977, and F1
  score of 0.988.
---

# Bridging Accuracy and Interpretability: Deep Learning with XAI for Breast Cancer Detection

## Quick Facts
- arXiv ID: 2510.21780
- Source URL: https://arxiv.org/abs/2510.21780
- Authors: Bishal Chhetri; B. V. Rathish Kumar
- Reference count: 36
- Primary result: Deep learning model achieves 0.992 accuracy and 1.000 precision on breast cancer detection from FNA features, with SHAP identifying concave points as most influential feature.

## Executive Summary
This study presents a deep learning model for breast cancer detection using features extracted from digitized fine needle aspirate images. The model employs ReLU activations, Adam optimizer, and binary cross-entropy loss, achieving high performance with accuracy of 0.992, precision of 1.000, recall of 0.977, and F1 score of 0.988. These results surpass existing benchmarks. To enhance interpretability, the authors integrate SHAP and LIME techniques to provide feature-level attributions and visualizations. The analysis identifies concave points of cell nuclei as the most influential feature for classification. The work demonstrates the potential of deep learning combined with explainable AI to improve diagnostic accuracy and clinician trust in clinical applications.

## Method Summary
The study uses the Wisconsin Breast Cancer Dataset (569 samples, 357 benign, 212 malignant) with 30 pre-extracted morphological features from digitized FNA images. Features undergo Min-Max scaling and are split using stratified sampling (70/10/20 or 80/10/10 train/validation/test). A feed-forward neural network with 2 hidden layers (30 neurons each, ReLU activations) is trained using Adam optimizer (lr=0.001) for 200 epochs with binary cross-entropy loss. SHAP and LIME techniques provide post-hoc interpretability through feature importance visualizations. The model achieves 0.992 accuracy, 1.000 precision, 0.977 recall, and 0.988 F1 score.

## Key Results
- Achieves 0.992 accuracy, 1.000 precision, 0.977 recall, and 0.988 F1 score on Wisconsin Breast Cancer Dataset
- SHAP analysis identifies concave_points_mean as the most influential feature for malignancy classification
- Outperforms baseline models including logistic regression, decision tree, random forest, SGD, KNN, and XGBoost

## Why This Works (Mechanism)

### Mechanism 1
Pre-extracted morphological features from FNA images enable high-accuracy classification with a compact neural network. The pipeline bypasses end-to-end image learning by using 30 quantitative features extracted via active contour models, reducing the learning problem from high-dimensional pixel space to a structured 30-dimensional feature space where benign/malignant boundaries are more linearly separable.

### Mechanism 2
ReLU-activated feed-forward network with Adam optimization achieves superior performance through efficient gradient flow and adaptive learning rates. Two hidden layers with 30 neurons each, ReLU activations prevent vanishing gradients in shallow networks. Adam optimizer (lr=0.001) provides per-parameter adaptive learning rates, enabling stable convergence over 200 epochs.

### Mechanism 3
SHAP and LIME provide complementary local and global explanations that identify concave points as the primary malignancy indicator. SHAP computes Shapley values showing concave_points_mean has highest mean absolute impact. LIME approximates the neural network locally with interpretable linear models for individual predictions.

## Foundational Learning

- **Concept: Shapley Values and Game-Theoretic Attribution**
  - Why needed here: Understanding how SHAP assigns credit to features requires grasping marginal contributions over all possible feature coalitions.
  - Quick check question: Can you explain why SHAP computes feature importance by averaging marginal contributions across all possible subsets of features?

- **Concept: Min-Max Normalization vs. Standardization**
  - Why needed here: The paper applies Min-Max scaling to address feature scale differences; understanding when this is appropriate versus z-score standardization.
  - Quick check question: Why might Min-Max scaling to [0,1] be preferred over z-score normalization for neural networks with ReLU activations?

- **Concept: Stratified Sampling for Imbalanced Classes**
  - Why needed here: The dataset has 357 benign vs 212 malignant cases; stratified sampling ensures representative class distribution in train/test splits.
  - Quick check question: How does stratified sampling differ from random sampling, and why does it matter when evaluating recall for the minority (malignant) class?

## Architecture Onboarding

- **Component map:** Raw FNA Image → Active Contour Extraction → 30 Nuclear Features → Min-Max Scaler → [Input: 30] → [Hidden: 30, ReLU] → [Hidden: 30, ReLU] → [Output: 1, Sigmoid] → Binary Prediction
- **Critical path:** 1) Feature extraction quality (snake algorithm boundary detection) 2) Data normalization (prevents scale-dominated learning) 3) Train/validation/test split (70:10:20 or 80:10:10, stratified) 4) Model training (200 epochs, Adam, lr=0.001, binary cross-entropy) 5) Threshold application (0.5 default for sigmoid output)
- **Design tradeoffs:** 2 vs 3 hidden layers (paper shows 2 layers perform better, likely due to overfitting risk with limited data); Feature-based vs end-to-end (trading off potential representational power for interpretability and data efficiency); SHAP vs LIME (SHAP provides global consistency at computational cost, LIME provides fast local explanations with potential inconsistency)
- **Failure signatures:** High training accuracy, low test accuracy → Overfitting (try regularization, reduce layers); SHAP and LIME feature rankings disagree → Model may have learned spurious correlations; Precision=1.0 but recall<1.0 → Model is conservative; threshold could be adjusted if false negatives are critical; Loss plateaus early → Learning rate may be too low or features may lack discriminative power
- **First 3 experiments:** 1) Replicate the baseline with 70:10:20 split, 2 hidden layers, verify metrics match (acc=0.992, precision=1.0, recall=0.977) 2) Ablation study: Remove the top SHAP-identified feature (concave_points_mean) and measure performance degradation to validate explanation fidelity 3) Cross-validation stability: Run 5-fold stratified CV and report variance in metrics to assess robustness beyond the single split reported

## Open Questions the Paper Calls Out

### Open Question 1
How does integrating clinical and genetic data with FNA-derived features affect model performance and interpretability? The current study uses only quantitative features from digitized FNA images; no multimodal data integration was attempted.

### Open Question 2
How well does the deep learning model generalize to external datasets from different institutions, imaging equipment, and patient populations? The study uses a single historical dataset (Wisconsin, n=569) with no external validation.

### Open Question 3
Do clinicians find SHAP and LIME explanations actionable and trustworthy for diagnostic decision-making in practice? The paper claims that XAI visualizations "increase clinician trust" but no user study with medical professionals is reported.

## Limitations

- Results are based on a single train/test split without reporting variance across multiple runs or cross-validation stability
- The high precision (1.000) with slightly lower recall (0.977) suggests a potentially conservative threshold that may not be optimal for clinical decision-making where false negatives carry higher cost
- The paper uses a pre-extracted feature set rather than raw images, limiting generalizability to other imaging modalities or datasets

## Confidence

- **High confidence** in performance metrics and architectural claims, as these are directly verifiable from the methodology and match established results on the Wisconsin dataset
- **Medium confidence** in SHAP/LIME interpretability results, as post-hoc explanations can be sensitive to implementation details and may not fully capture model reasoning
- **Low confidence** in clinical applicability claims, as the study uses a public dataset without validation on prospective clinical data or comparison against radiologist performance

## Next Checks

1. Conduct 5-fold stratified cross-validation to quantify performance stability and report mean ± standard deviation across folds
2. Perform ablation studies by removing top-ranked features (e.g., concave points) to test whether performance degrades as expected from SHAP attributions
3. Compare model performance against domain experts using independent test sets or clinical validation studies to establish practical utility beyond benchmark performance