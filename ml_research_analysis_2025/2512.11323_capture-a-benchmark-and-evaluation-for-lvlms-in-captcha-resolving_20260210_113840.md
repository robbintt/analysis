---
ver: rpa2
title: 'CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving'
arxiv_id: '2512.11323'
source_url: https://arxiv.org/abs/2512.11323
tags:
- captcha
- lvlms
- captchas
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAPTURE, the first comprehensive benchmark
  for evaluating Large Vision-Language Models (LVLMs) on CAPTCHA solving. The benchmark
  covers 4 main CAPTCHA types and 25 subtypes from 31 vendors, with 61,464 CAPTCHAs.
---

# CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving

## Quick Facts
- **arXiv ID**: 2512.11323
- **Source URL**: https://arxiv.org/abs/2512.11323
- **Reference count**: 40
- **Primary result**: LVLMs achieve 5-86% accuracy on CAPTURE benchmark, significantly below human performance of 87-99%, but CRRD framework improves accuracy by up to 69%.

## Executive Summary
This paper introduces CAPTURE, the first comprehensive benchmark for evaluating Large Vision-Language Models (LVLMs) on CAPTCHA solving. The benchmark covers 4 main CAPTCHA types and 25 subtypes from 31 vendors, with 61,464 CAPTCHAs. The authors propose a two-stage framework called CRRD (Cropping, Re-reading, and Describing) to improve LVLM performance. When tested with 11 different LVLMs, the models showed poor performance on CAPTCHAs, achieving only 5-86% accuracy compared to human accuracy of 87-99%. The CRRD framework improved accuracy by up to 69%, with the best results from Gemini 2.0 Flash. However, even with these enhancements, LVLMs still cannot match human-level performance in solving CAPTCHAs.

## Method Summary
The authors created CAPTURE benchmark with 61,464 CAPTCHAs across 4 main types (Text, Visual, Games, Behavior) and 25 subtypes from 31 vendors. They evaluated 11 LVLMs using basic prompts and a CRRD framework: (1) Crop instruction from image, (2) Re-reading prompt, (3) Describing prompt. Evaluation used accuracy metrics appropriate to each task type. The framework was implemented via API calls for closed-source models and local inference on NVIDIA 4090 for open-source models.

## Key Results
- LVLMs achieved 5-86% accuracy on CAPTURE benchmark, significantly below human baseline of 87-99%
- CRRD framework improved accuracy by up to 69% across different CAPTCHA types
- Gemini 2.0 Flash achieved the best results at 95% accuracy with CRRD
- LVLMs struggled particularly with 4x4 grid segmentation and Chinese character tasks

## Why This Works (Mechanism)

### Mechanism 1: Attention Modulation via Visual Segmentation (Cropping)
Isolating the CAPTCHA puzzle from its instructional text reduces visual distraction, improving feature extraction. Cropping minimizes cross-modal noise where the model attempts to OCR or analyze instruction text as part of the puzzle pattern.

### Mechanism 2: Iterative Reasoning Grounding (Re-reading & Describing)
Generating intermediate descriptions of visual content grounds the final reasoning step, reducing hallucination. The "Describing" and "Re-reading" stages function as a Chain-of-Thought trigger, moving solution from direct input-output mapping to multi-step inference.

### Mechanism 3: Linguistic-Semantic Mapping for Behavior Tasks
LVLMs solve behavior-based CAPTCHAs by mapping visual states to textual instructions since they lack native motor control. They generate semantic plans (e.g., "The image is rotated 90 degrees counter-clockwise; the correct action is to rotate right").

## Foundational Learning

- **Visual Grounding (Referring Expression Comprehension)**: Needed to link text instructions to specific regions in noisy images. Quick check: Can the model identify coordinates or mask of an object based solely on natural language description?

- **Prompt Engineering / Chain-of-Thought (CoT)**: CRRD framework is structured prompt strategy. Quick check: Does adding "Let's think step by step" change the model's intermediate activation patterns or just its final output distribution?

- **OCR vs. Semantic Reasoning**: Text-based CAPTCHAs rely on OCR, while Game/Behavior CAPTCHAs rely on semantic understanding. Quick check: Is the model failing because it cannot "see" the character (perception) or because it doesn't understand the rule (reasoning)?

## Architecture Onboarding

- **Component map**: Input Processor -> Pre-processor (CRRD Stage 1) -> Inference Engine -> Prompt Manager -> Evaluator
- **Critical path**: The "Slide" and "Behavior" tasks are the stress test. The system must successfully crop the slider gap, describe the displacement, and output a coordinate/percentage.
- **Design tradeoffs**: Latency vs. Accuracy (CRRD adds 2 extra inference steps), Generality vs. Specificity (unified approach performs poorly)
- **Failure signatures**: 4x4 Grid Hallucination (predicting only 9 segments), Font Confusion (1/l/I, 0/O), Game Rule Misinterpretation (Gobang, Match-3)
- **First 3 experiments**: 1) Baseline Performance on Text/Visual with basic prompt, 2) Cropping Ablation on Slide/Rotation CAPTCHAs, 3) Full CRRD Evaluation on Game category

## Open Questions the Paper Calls Out

1. **Physical Interaction Enhancement**: Can LVLMs be enhanced to physically interact with behavior-based CAPTCHA elements using Function Call (FC) or Model Context Protocol (MCP)? The paper notes this as a future direction since current LVLMs only generate semantic descriptions.

2. **Performance Gap Nature**: Is the performance gap between LVLMs and humans a surmountable training issue or due to inherent LVLM architectures? While CRRD improved accuracy by up to 69%, models still failed to reach the 87-99% human baseline.

3. **Training Data Diversity**: To what extent does lack of diverse training data (specifically 4x4 image segmentation) restrict LVLM generalization on visual tasks? The paper infers that near-zero accuracy on 4x4 segmentation is because "training data predominantly consists of 3x3 type data."

## Limitations
- Dataset not publicly available at publication, creating reproducibility barrier
- Performance improvements may not generalize to novel CAPTCHA designs outside the 25 subtypes
- Substantial performance variance across LVLMs suggests CRRD effectiveness may be model-specific

## Confidence
- **High Confidence**: LVLMs perform significantly worse than humans on CAPTCHAs (5-86% vs 87-99%)
- **Medium Confidence**: CRRD framework improves accuracy by up to 69% based on ablation studies
- **Low Confidence**: CRRD represents a "universal solution" for CAPTCHA-solving LVLMs, as effectiveness appears model-specific

## Next Checks
1. **Dataset Replication Test**: Once CAPTURE dataset is released, replicate baseline and CRRD results across all 11 LVLMs to verify reported accuracy improvements.

2. **Adversarial CAPTCHA Generation**: Generate novel CAPTCHA variants not represented in the 25 subtypes and test whether CRRD framework maintains performance gains or overfits to benchmark distribution.

3. **Cross-Model Framework Transfer**: Implement CRRD on a smaller, simpler LVLM with comparable visual capabilities to determine whether performance gains are model-specific or generalizable.