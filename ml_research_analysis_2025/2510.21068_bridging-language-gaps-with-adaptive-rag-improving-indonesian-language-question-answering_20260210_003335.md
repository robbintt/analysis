---
ver: rpa2
title: 'Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question
  Answering'
arxiv_id: '2510.21068'
source_url: https://arxiv.org/abs/2510.21068
tags:
- retrieval
- question
- language
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the scarcity of effective Question Answering
  (QA) systems for low-resource languages, focusing on Indonesian. The authors develop
  an Adaptive RAG system that integrates a classifier to assess question complexity
  and determine the optimal answering strategy (non-retrieval, single-retrieval, or
  multi-retrieval).
---

# Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering

## Quick Facts
- **arXiv ID:** 2510.21068
- **Source URL:** https://arxiv.org/abs/2510.21068
- **Reference count:** 40
- **Primary result:** An Adaptive RAG system with complexity-based strategy selection shows promise but struggles with Indonesian multi-retrieval QA due to LLM and dataset limitations.

## Executive Summary
This study tackles the scarcity of effective Question Answering systems for low-resource languages, specifically Indonesian, by developing an Adaptive RAG system that intelligently selects between non-retrieval, single-retrieval, and multi-retrieval strategies based on question complexity. The authors create a multi-retrieval dataset via machine translation of HotpotQA and demonstrate a reliable question complexity classifier (accuracy up to 0.756), though performance with the multi-retrieval strategy remains inconsistent due to Indonesian language processing limitations in current large language models. The findings highlight both the promise and challenges of Adaptive RAG in low-resource languages, suggesting that improving multi-retrieval methods and developing native Indonesian datasets are critical next steps.

## Method Summary
The authors developed an Adaptive RAG system that first classifies incoming questions into complexity levels (simple, single-hop, multi-hop) using a fine-tuned classifier. Based on this classification, the system selects one of three answering strategies: non-retrieval (answer directly from LLM), single-retrieval (retrieve relevant document and answer), or multi-retrieval (retrieve multiple documents, perform multi-hop reasoning). To address the lack of Indonesian multi-hop datasets, they translated HotpotQA to Indonesian using OPUS-MT, creating a new dataset for training and evaluation. The system was evaluated using Gemma 3 (4B and 27B parameters) and Qwen2.5 (8B and 14B parameters) models.

## Key Results
- A question complexity classifier achieved accuracy up to 0.756, demonstrating reliable performance in categorizing question types.
- The multi-retrieval strategy showed significant performance inconsistencies, with smaller models (4B-8B) frequently failing due to insufficient reasoning capabilities for Indonesian language processing.
- Machine-translated datasets introduced quality issues, with examples showing mistranslations that could add noise to the QA pipeline.

## Why This Works (Mechanism)
The Adaptive RAG system works by intelligently routing questions to appropriate answering strategies based on their complexity. Simple questions bypass retrieval entirely, single-hop questions use targeted retrieval, and complex multi-hop questions employ iterative retrieval and reasoning. This adaptive approach aims to optimize performance by matching the question type with the most suitable answering method, avoiding unnecessary retrieval overhead for simple questions while providing sufficient context for complex reasoning tasks.

## Foundational Learning
- **Question Complexity Classification** - A classifier that categorizes questions into simple, single-hop, or multi-hop types to determine the appropriate answering strategy. Why needed: Different question types require different approaches for optimal performance. Quick check: Test classifier accuracy on held-out Indonesian questions.
- **Multi-hop Reasoning** - The ability to answer questions requiring information from multiple documents through iterative retrieval and reasoning. Why needed: Many complex questions cannot be answered from a single source. Quick check: Verify system can correctly chain reasoning across two documents.
- **Machine Translation for Dataset Creation** - Using automated translation to convert English QA datasets to Indonesian. Why needed: Lack of existing multi-hop QA datasets in Indonesian. Quick check: Human evaluation of translation quality for a sample of questions.
- **LLM Parameter Scaling** - The relationship between model size and reasoning capability, particularly for low-resource languages. Why needed: Smaller models may lack sufficient capacity for complex Indonesian language processing. Quick check: Compare performance of 4B vs 27B models on multi-hop tasks.
- **Retrieval-Augmented Generation** - Combining information retrieval with language model generation to answer questions. Why needed: Provides access to external knowledge while maintaining natural language generation capabilities. Quick check: Measure improvement from adding retrieval vs direct answering.

## Architecture Onboarding

**Component Map:** Question -> Classifier -> Strategy Selector -> [Non-retrieval/Retrieval] -> Answer

**Critical Path:** Question → Complexity Classifier → Multi-retrieval Strategy → Iterative Retrieval → Reasoning LLM → Answer

**Design Tradeoffs:** The system trades computational efficiency (using smaller models when possible) against accuracy (requiring larger models for complex reasoning). Using machine-translated datasets enables research but introduces quality noise. The adaptive approach adds complexity but optimizes performance per question type.

**Failure Signatures:** Multi-retrieval fails when LLMs incorrectly determine information is insufficient, often due to poor keyword extraction for subsequent retrieval steps. Smaller models (4B-8B) are particularly prone to this failure, outputting "Information is not enough" even when sufficient context exists.

**3 First Experiments:**
1. Evaluate classifier accuracy on a held-out test set of Indonesian questions to verify complexity categorization reliability.
2. Compare performance of different model sizes (4B, 8B, 14B, 27B) on multi-hop questions to identify scaling thresholds.
3. Human evaluation of machine-translated questions to assess translation quality and identify common error patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a natively constructed Indonesian multi-hop dataset significantly improve Adaptive RAG performance compared to the machine-translated HotpotQA dataset used in this study?
- Basis in paper: The authors explicitly recommend "developing a dataset that is originally written in Indonesian language rather than translated from English" to avoid mistranslation and meaning drift.
- Why unresolved: The current study relied on OPUS-MT translation, which introduced grammatical and semantic errors (e.g., "three times Tony nomination" vs. "three-time Tony nominee"), potentially adding noise that obscured the system's true capabilities.
- What evidence would resolve it: A comparative benchmark where the proposed Adaptive RAG system is evaluated on a new, human-curated Indonesian multi-hop dataset versus the translated HotpotQA dataset.

### Open Question 2
- Question: How can the multi-retrieval strategy be modified to maintain reasoning fidelity in smaller parameter models (4B–8B) when processing Indonesian prompts?
- Basis in paper: The Conclusion states the need for "improving the multi-retrieval method itself" and notes that current methods are "constrained by LLM's reasoning capabilities," leading to hallucination.
- Why unresolved: The authors found that concatenating multiple documents caused smaller models like Gemma 3 to fail (outputting "Information is not enough"), whereas larger models succeeded, suggesting the current prompting strategy is too brittle for smaller, low-resource deployments.
- What evidence would resolve it: An ablation study testing alternative multi-retrieval prompting techniques (e.g., summarization-based context reduction) on 4B–8B models to see if they match the performance of larger models.

### Open Question 3
- Question: To what extent does utilizing an LLM specifically pre-trained on Indonesian corpora resolve the retrieval guidance failures observed in multilingual models?
- Basis in paper: The authors suggest "utilizing or training an LLM that is specifically pre-trained or fine-tuned on Indonesian language" to better understand context and guide retrieval.
- Why unresolved: The study observed that general multilingual models often failed to extract keywords for subsequent retrieval steps, incorrectly deeming information insufficient, but did not test a model optimized specifically for Indonesian.
- What evidence would resolve it: An experiment replacing the generic Gemma/Qwen models with an Indonesian-native LLM (e.g., a specialized fine-tuned variant) in the multi-retrieval loop to measure the reduction in "insufficient information" errors.

## Limitations
- The performance of the Adaptive RAG system is significantly constrained by the current capabilities of Indonesian language models, particularly for complex reasoning tasks.
- Reliance on machine-translated datasets introduces potential quality issues, as automatic translation may not capture nuanced linguistic features essential for accurate question answering.
- The multi-retrieval strategy shows notable performance inconsistencies, indicating that existing LLM architectures may not be optimally suited for complex reasoning in low-resource languages.

## Confidence
- **High confidence:** Question complexity classifier reliability (accuracy up to 0.756)
- **Medium confidence:** Overall Adaptive RAG framework effectiveness, given mixed results across different retrieval strategies
- **Low confidence:** Scalability and generalizability to other low-resource languages, as the study focuses specifically on Indonesian

## Next Checks
1. Conduct human evaluation of the machine-translated HotpotQA dataset to assess translation quality and its impact on downstream QA performance.
2. Compare the Adaptive RAG performance with native Indonesian question answering models trained on purpose-built datasets to isolate the effect of translation artifacts.
3. Test the Adaptive RAG framework with different Indonesian language models (e.g., IndoBERT, IndoGPT) to determine whether performance issues are model-specific or systemic to Indonesian language processing.