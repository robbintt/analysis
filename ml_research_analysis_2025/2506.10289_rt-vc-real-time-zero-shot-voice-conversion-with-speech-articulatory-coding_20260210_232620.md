---
ver: rpa2
title: 'RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding'
arxiv_id: '2506.10289'
source_url: https://arxiv.org/abs/2506.10289
tags:
- speech
- speaker
- conversion
- voice
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RT-VC, a zero-shot real-time voice conversion
  system that achieves ultra-low CPU latency (61.4ms) while maintaining conversion
  quality comparable to state-of-the-art methods. The core innovation is leveraging
  articulatory feature space through the Speech Articulatory Coding (SPARC) framework
  combined with a streaming architecture.
---

# RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding

## Quick Facts
- arXiv ID: 2506.10289
- Source URL: https://arxiv.org/abs/2506.10289
- Authors: Yisi Liu; Chenyang Wang; Hanjo Kim; Raniya Khan; Gopala Anumanchipalli
- Reference count: 16
- Primary result: Achieves 61.4ms CPU latency while maintaining state-of-the-art conversion quality

## Executive Summary
RT-VC presents a zero-shot real-time voice conversion system that achieves ultra-low CPU latency (61.4ms) while maintaining conversion quality comparable to state-of-the-art methods. The system leverages articulatory feature space through the Speech Articulatory Coding (SPARC) framework combined with a streaming architecture. Key innovations include a causal source extractor, acoustic-to-articulatory inversion model trained on SPARC labels, and a differentiable digital signal processing (DDSP) vocoder for efficient real-time synthesis.

## Method Summary
RT-VC operates by extracting articulatory features from input speech using the SPARC framework, then converting these features to match the target speaker's characteristics. The system employs a streaming architecture with a causal source extractor that processes speech incrementally, followed by an acoustic-to-articulatory inversion model that maps the extracted features to articulatory representations. A DDSP vocoder then synthesizes the converted speech in real-time. The zero-shot capability means the system can convert voices without requiring parallel training data between speakers.

## Key Results
- Achieves 3.81 UTMOS and 3.87 MOS for naturalness
- Maintains 6.69% WER and 2.12% CER for intelligibility
- Achieves 76.65% speaker similarity score and 0.865 f0 consistency
- Reduces latency by 13.3% compared to previous best system

## Why This Works (Mechanism)
RT-VC's effectiveness stems from operating in articulatory feature space rather than direct acoustic conversion. By using SPARC labels to train an acoustic-to-articulatory inversion model, the system captures the underlying speech production mechanisms that are more stable and generalizable across speakers. The streaming architecture with causal processing ensures minimal latency, while the DDSP vocoder provides efficient real-time synthesis without compromising quality.

## Foundational Learning

**Speech Articulatory Coding (SPARC)** - A framework that encodes speech into articulatory feature representations based on how humans produce sounds. *Why needed:* Provides a more stable intermediate representation than raw acoustic features. *Quick check:* Verify SPARC labels capture distinctive speech production features across different phonemes.

**Causal Source Extraction** - A processing approach where each output depends only on current and past inputs. *Why needed:* Essential for real-time processing without looking ahead in the audio stream. *Quick check:* Confirm no future context is used in feature extraction.

**Differentiable Digital Signal Processing (DDSP)** - Combines traditional signal processing with deep learning through differentiable operations. *Why needed:* Enables efficient real-time synthesis while maintaining audio quality. *Quick check:* Validate DDSP vocoder can run at target latency on target hardware.

## Architecture Onboarding

**Component Map:** Input Speech -> Causal Source Extractor -> Acoustic-to-Articulatory Model -> DDSP Vocoder -> Output Speech

**Critical Path:** The bottleneck is the acoustic-to-articulatory inversion model, as it must process articulatory features in real-time before synthesis can begin.

**Design Tradeoffs:** Uses articulatory features instead of direct acoustic conversion to improve stability and generalization, trading some conversion complexity for lower latency and better speaker independence.

**Failure Signatures:** Latency spikes occur if the acoustic-to-articulatory model cannot keep pace with streaming input; quality degradation appears when SPARC labels fail to capture speaker-specific characteristics.

**First 3 Experiments:** 1) Measure actual latency on target hardware under various CPU loads, 2) Test conversion quality with different speaker pairs to verify zero-shot capability, 3) Evaluate robustness across different speaking styles and emotional content.

## Open Questions the Paper Calls Out
None

## Limitations
- Does not address conversion quality degradation under varying acoustic conditions or noisy environments
- SPARC framework's generalizability to diverse linguistic contexts and speaker demographics remains unverified
- Claims of "ultra-low" latency lack clear contextual benchmarks across different application scenarios

## Confidence

**High Confidence:** The reported latency measurements (61.4ms) and the architectural innovations (causal source extraction, streaming framework) are well-documented and reproducible based on the methodology described.

**Medium Confidence:** The comparison metrics (UTMOS, MOS, WER, CER, speaker similarity, f0 consistency) appear rigorous, but the absence of competing baseline implementations makes direct performance validation challenging.

**Low Confidence:** The claim of achieving "comparable" quality to state-of-the-art while reducing latency by 13.3% requires independent verification, as the paper does not provide open-source implementations or detailed ablation studies.

## Next Checks

1. Implement and evaluate RT-VC under varying acoustic noise conditions (SNR ranges from 10dB to 30dB) to assess robustness beyond clean speech scenarios.

2. Conduct cross-linguistic testing across non-English languages to validate the generalizability of the SPARC framework and articulatory feature extraction.

3. Perform real-time deployment testing on resource-constrained edge devices (e.g., mobile phones, embedded systems) to verify the claimed latency benefits in practical applications.