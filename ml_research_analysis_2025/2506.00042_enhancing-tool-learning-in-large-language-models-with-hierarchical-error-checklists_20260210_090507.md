---
ver: rpa2
title: Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists
arxiv_id: '2506.00042'
source_url: https://arxiv.org/abs/2506.00042
tags:
- error
- tool
- parameter
- name
- checklist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Hierarchical Tool Error Checklist (HiTEC)
  framework to improve the reliability of tool calling in large language models (LLMs).
  HiTEC addresses parameter mis-filling errors by combining a global checklist of
  common tool-calling errors with a local checklist tailored to specific tools.
---

# Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists

## Quick Facts
- **arXiv ID**: 2506.00042
- **Source URL**: https://arxiv.org/abs/2506.00042
- **Reference count**: 28
- **Primary result**: HiTEC framework improves tool-calling success rates and parameter-filling accuracy by up to 42% over baselines.

## Executive Summary
This paper introduces HiTEC, a framework to improve tool-calling reliability in large language models by addressing parameter mis-filling errors. HiTEC combines a global checklist of common cross-tool errors with tool-specific local checklists, implemented via two methods: HiTEC-ICL (in-context learning with two-round reflection) and HiTEC-KTO (fine-tuning with Kahneman-Tversky Optimization). Experiments on five public datasets show significant gains in tool-calling accuracy and parameter-filling F1 scores, especially for smaller models. The approach effectively separates general and tool-specific error knowledge, enabling dynamic correction without model retraining or robust fine-tuning when synthetic negatives are available.

## Method Summary
HiTEC uses a two-tiered checklist approach: a global checklist of 8 common tool-calling errors and tool-specific local checklists generated from tool metadata. For HiTEC-ICL, the global checklist is embedded in the initial prompt, followed by a two-round conversation where the local checklist refines the tool call. For HiTEC-KTO, synthetic negative samples are generated by introducing checklist errors into correct tool calls, and the model is fine-tuned using KTO loss to avoid the gradient vanishing problem of DPO on near-identical positive/negative pairs. Both methods significantly improve tool-calling accuracy, with ICL doubling inference time but requiring no training, and KTO offering faster inference after upfront fine-tuning.

## Key Results
- HiTEC-ICL improves parameter-filling F1 scores by up to 42% over baselines (e.g., Llama3-8B on Nexus Raven: 24.86 → 55.49).
- HiTEC-KTO achieves higher F1 scores than Hammer2-3B baseline on Qwen2.5-1.5B (87.18 vs 85.24).
- Removing the local checklist sharply reduces performance (e.g., F1 Name+Param drops from 52.26 to 28.97 on Tool-Alpaca with Qwen2.5-1.5B).
- Two-round HiTEC-ICL increases inference time from 0.036s to 0.132s per query (Llama3-8B).

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Error Structuring Separates General from Tool-Specific Failures
Partitioning error knowledge into global (cross-tool) and local (tool-specific) checklists improves coverage without prompt bloat. The global checklist encodes 8 common failure modes (e.g., Missing Required Parameter, Invalid Parameter Type, Wrong Tool Name). The local checklist provides tool-specific examples with simulated queries, erroneous outputs, error messages, and corrective "Thought of Error" reflections. During inference, the global checklist is embedded upfront; the local checklist is invoked conditionally in a second round. This hierarchy reduces cognitive load on the model by not presenting all tool-specific details simultaneously while ensuring tool-specific edge cases are still addressed. Core assumption: Most tool-calling errors are predictable and can be enumerated in advance; models can generalize corrective patterns from structured examples. Evidence: Ablation shows removing local checklist drops F1 Name+Param from 52.26 to 28.97 on Tool Alpaca (Qwen2.5-1.5B); removing both drops to 16.03. Break condition: If error distribution is highly skewed toward novel, unforeseen errors not captured by static checklists, hierarchical structuring provides diminishing returns.

### Mechanism 2: Two-Round ICL Enables Dynamic Error Reflection Without Fine-Tuning
A conversational two-round interaction with checklist-guided reflection corrects parameter-filling errors at inference time. In Round 1, the global checklist is prepended to the user query, producing an initial tool call. In Round 2, the local checklist for the invoked tool is injected, prompting the model to "avoid similar errors in the checklists and refine your previous answer." The model reflects on its first output against explicit error patterns and corrects parameter values, types, or omissions. This mimics a self-correction loop without requiring model weights to change. Core assumption: The model can perform meaningful error reflection when explicitly shown failure modes and correction logic; the cost of a second round is acceptable. Evidence: HiTEC-ICL improves F1 Name+Param from 24.86 to 55.49 on Nexus Raven (Llama3-8B); GPT-4-Turbo improves from 60.75 to 61.95. Two-round HiTEC-ICL increases inference time from 0.036s to 0.132s per query (Llama3-8B). Break condition: If the model lacks sufficient reasoning capacity to generalize from checklist examples (very small models), or if latency constraints prohibit two rounds, this mechanism underperforms.

### Mechanism 3: KTO Overcomes DPO Failure Mode on Near-Identical Positive/Negative Pairs
Kahneman-Tversky Optimization (KTO) enables effective fine-tuning with negative examples where DPO fails due to gradient vanishing. Negative samples are generated by prompting a model to introduce specific errors from the checklist into correct tool calls. The resulting Paired Tool-Calling (PTC) dataset has positive and negative responses differing by only 1-2 tokens (e.g., one extra erroneous parameter). DPO's gradient approaches zero when the reward difference is small, and DPO can decrease positive sample probability. KTO's loss uses asymmetric weights (aw, al) and a reference point z0, producing stable gradients that increase correct token logits even when pairs are nearly identical. Core assumption: High-quality negative examples can be synthetically generated from checklist specifications; KTO's gradient properties generalize to tool-calling domains. Evidence: HiTEC-KTO on Qwen2.5-1.5B achieves 87.18 F1 Average, surpassing Hammer2-3B baseline (85.24). Break condition: If synthetic negatives fail to capture real-world error distributions, or if preference optimization is incompatible with the target deployment constraints, KTO gains diminish.

## Foundational Learning

- **Concept: Function/Tool Calling in LLMs**
  - **Why needed here:** The entire framework targets parameter mis-filling during tool invocation. You must understand how LLMs map natural language queries to structured API arguments (name, parameters, types) to diagnose where checklists intervene.
  - **Quick check question:** Given a user query "Get weather for Boston tomorrow," can you identify which parameters a weather API would require and where an LLM might fail (e.g., missing date format, wrong coordinate parsing)?

- **Concept: Direct Preference Optimization (DPO) vs. Kahneman-Tversky Optimization (KTO)**
  - **Why needed here:** HiTEC-KTO explicitly rejects DPO due to failure modes on near-identical pairs. Understanding why DPO gradients vanish and how KTO's asymmetric weighting resolves this is critical for implementing the fine-tuning pipeline.
  - **Quick check question:** If positive and negative responses differ by one token, why does DPO's loss gradient approach zero, and what does KTO change to prevent this?

- **Concept: In-Context Learning (ICL) and Multi-Turn Prompting**
  - **Why needed here:** HiTEC-ICL relies on embedding checklists into prompts and conducting a two-round conversation for reflection. Understanding how context window accumulation affects model behavior is essential for prompt design.
  - **Quick check question:** What happens to token costs and model attention when you prepend a 2000-token checklist to every query, and how does a two-round interaction compound this?

## Architecture Onboarding

- **Component map:**
  Global Error Checklist (8 types) -> Local Error Checklist Generator (tool metadata) -> Negative Sample Generator (checklist errors) -> HiTEC-ICL Runtime (two-round inference) OR HiTEC-KTO Trainer (fine-tuning on PTC dataset)

- **Critical path:**
  1. Enumerate global errors (manual, one-time).
  2. For each new tool, generate local checklist using tool metadata + template.
  3. For HiTEC-ICL: at inference, inject global checklist, invoke model, then inject local checklist for round-2 correction.
  4. For HiTEC-KTO: generate negative samples from tool ground-truth + local checklist, construct PTC dataset, run KTO fine-tuning.
  5. Evaluate using F1 Name and F1 Name + Parameter metrics on benchmark datasets (API-Bank, Tool-Alpaca, Seal-Tools, Nexus Raven).

- **Design tradeoffs:**
  - ICL vs. KTO: ICL requires no training but doubles inference latency and increases prompt tokens (320K+ vs. 77K for vanilla on Tool-Alpaca, Table 5). KTO requires upfront fine-tuning cost but produces faster single-pass inference.
  - Checklist comprehensiveness vs. prompt bloat: More error types improve coverage but increase context length. The paper fixes at 8 global errors; local checklists are tool-specific.
  - Synthetic vs. real negative samples: Synthetic negatives from checklists avoid costly API calls (Bing Search: $10-25/1000 transactions) but may not reflect real error distributions (Limitations section).

- **Failure signatures:**
  - DPO gradient vanishing on PTC: If log-prob of chosen samples decreases during training (Figure 8c), switch to KTO.
  - Local checklist omission: F1 Name+Param drops sharply (Table 4: 52.26→28.97 on Qwen2.5-1.5B) if local checklist is removed.
  - Format errors persist: If Error 5 (Invalid Function Calling Output Format) remains frequent after HiTEC, check that the global checklist is correctly prepended and model follows JSON structure.
  - Small models underperform on ICL: Llama3-8B shows large ICL gains; if a smaller model shows minimal improvement, consider KTO fine-tuning instead.

- **First 3 experiments:**
  1. Reproduce HiTEC-ICL on a single dataset: Take Tool-Alpaca, implement two-round inference with global + local checklists, measure F1 Name+Param against vanilla baseline. Verify ~5-30 point improvement (Table 2).
  2. Generate negative samples for one tool and train KTO: Pick a tool from Seal-Tools, generate 100 negative samples using the checklist, fine-tune Qwen2.5-1.5B with KTO, compare against Hammer2 baseline (Table 3).
  3. Ablate local checklist: Run HiTEC-ICL with only global checklist, then with both global and local. Quantify the F1 Name+Param delta (expected: 10-25 point drop without local, Table 4).

## Open Questions the Paper Calls Out

- **Can the HiTEC framework be adapted to dynamically update error checklists in real-time based on user interactions to address evolving API behaviors?**
  - Basis: The authors state that the current manual checklists "may not capture all novel or unforeseen errors in dynamic tool environments" and suggest integrating "mechanisms for continuous monitoring."
  - Why unresolved: The current implementation relies on static, pre-defined global and local checklists that do not automatically adapt during deployment.
  - What evidence would resolve it: Experiments demonstrating that a feedback loop integrating live execution errors into the checklist improves performance over time compared to the static baseline.

- **How does the reliance on simulated negative examples affect the generalization of HiTEC-KTO to real-world API instabilities and failures?**
  - Basis: The limitations section notes that "reliance on simulated error feedback may not fully reflect real-world scenarios, potentially limiting the framework’s generalizability."
  - Why unresolved: The training data for KTO is generated via prompting (simulated), which may lack the noise and unpredictability of actual API rate limits or network errors.
  - What evidence would resolve it: A comparative evaluation of HiTEC-KTO models on live API endpoints versus the static benchmarks used in the paper (e.g., API-Bank).

- **Can the manual effort required for creating tool-specific Local Error Checklists be automated or reduced for large-scale tool libraries?**
  - Basis: The paper notes effectiveness depends on "manually crafted error types," and the methodology requires a specific prompt with tool metadata for every tool. Scaling this to thousands of tools implies a high human overhead.
  - Why unresolved: The paper does not explore methods for auto-generating these checklists without the heavy prompt engineering or manual verification currently described.
  - What evidence would resolve it: A study showing that fully automated LLM-generated checklists achieve statistical parity with the human-verified versions used in the main experiments.

## Limitations

- **Checklist Completeness**: The framework relies on handcrafted global errors and LLM-generated local checklists. If real-world error distributions deviate significantly from these static enumerations, coverage gaps will persist.
- **Synthetic Negative Quality**: KTO depends on synthetic negative samples generated by perturbing ground-truth tool calls. While this avoids costly API calls, it may not capture complex failure modes arising from ambiguous queries, tool ambiguities, or runtime constraints.
- **Generalization Across Domains**: Experiments focus on five structured-tool datasets. The framework's effectiveness on open-domain, unstructured, or multi-modal tool-calling scenarios is untested.

## Confidence

- **High Confidence**: Hierarchical error structuring improves coverage and reduces prompt bloat (supported by ablation in Table 4).
- **Medium Confidence**: Two-round ICL enables dynamic correction at inference time (supported by F1 improvements in Table 2, but latency cost is significant).
- **Medium Confidence**: KTO overcomes DPO failure on near-identical pairs (supported by gradient analysis in Appendix D, but KTO's broader applicability is not independently validated).

## Next Checks

1. **Error Distribution Audit**: Manually annotate a sample of tool-calling failures from a held-out dataset to quantify what fraction fall outside the 8 global error types. This tests checklist completeness.

2. **Real vs. Synthetic Negatives**: Generate a small set of human-annotated negative examples for one tool and compare KTO fine-tuning performance using real negatives versus synthetic ones. This isolates the impact of negative sample quality.

3. **Cross-Domain Transfer**: Apply HiTEC-ICL to a non-structured tool-calling task (e.g., multi-modal tool use or open-domain code generation) and measure whether the global checklist generalizes or requires significant adaptation.