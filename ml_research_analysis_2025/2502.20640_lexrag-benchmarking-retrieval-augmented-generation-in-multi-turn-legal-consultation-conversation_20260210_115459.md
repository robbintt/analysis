---
ver: rpa2
title: 'LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal Consultation
  Conversation'
arxiv_id: '2502.20640'
source_url: https://arxiv.org/abs/2502.20640
tags:
- legal
- retrieval
- lexrag
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LexRAG is the first benchmark specifically designed to evaluate
  Retrieval-Augmented Generation (RAG) systems in multi-turn legal consultation conversations.
  It consists of 1,013 multi-turn dialogue samples and 17,228 candidate legal articles,
  each annotated by legal experts across five rounds of progressive questioning.
---

# LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal Consultation Conversation

## Quick Facts
- **arXiv ID:** 2502.20640
- **Source URL:** https://arxiv.org/abs/2502.20640
- **Reference count:** 40
- **Primary result:** Current RAG systems struggle in multi-turn legal consultation, with best retrieval achieving only 33.33% Recall@10

## Executive Summary
LexRAG introduces the first benchmark specifically designed to evaluate Retrieval-Augmented Generation (RAG) systems in multi-turn legal consultation conversations. The benchmark comprises 1,013 dialogue samples with 17,228 legal articles, each annotated by legal experts across five progressive questioning rounds. The study reveals that even with optimal configurations, RAG systems achieve only 33.33% Recall@10 in retrieval and struggle to meet legal consultation standards in response generation, with the highest LLM judge score being 7.37 out of 10.

## Method Summary
The LexRAG benchmark evaluates two key tasks: conversational knowledge retrieval and response generation. The evaluation uses LexiT, a modular legal RAG toolkit featuring various retriever implementations (BM25, dense embeddings) and generator configurations. An LLM-as-a-judge framework provides detailed assessment across five dimensions: Factuality, User Satisfaction, Clarity, Logical Coherence, and Completeness. The best pipeline uses GTE-Qwen2-1.5B-instruct retriever with Query Rewrite processor and Qwen-2.5-72B-Instruct generator, achieving 33.33% Recall@10 and 7.37/10 LLM judge score.

## Key Results
- Dense retrieval methods outperform traditional lexical matching, with GTE-Qwen2-1.5B-instruct achieving the best results
- Query rewriting improves dense retrieval effectiveness by integrating relevant information while minimizing noise
- Even with optimal retrieval, LLM judge scores remain below 7.5/10, indicating response generation quality falls short of legal consultation standards

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Query rewriting converts multi-turn conversational context into a standalone, decontextualized query that improves dense retrieval effectiveness over raw conversation history.
- **Mechanism:** An LLM (GPT-4o-mini in the paper) processes the full dialogue history and synthesizes the relevant context into a single, self-contained question. This reduces noise from prior turns while preserving the current legal intent, allowing embedding models to match against legal articles more effectively than using partial or unfiltered context.
- **Core assumption:** The rewrite LLM can accurately identify which prior turns are relevant and correctly resolve pronouns and topic shifts without losing the legal focus of the current query.
- **Evidence anchors:**
  - [abstract] "LexRAG consists of 1,013 multi-turn dialogue samples...each annotated by legal experts across five rounds of progressive questioning"
  - [section 5.2] "For dense retrieval methods, the query rewrite strategy typically produces the best results. This is likely because it integrates relevant information while minimizing the influence of irrelevant data."
  - [corpus] MTRAG benchmark (arXiv:2501.03468) confirms multi-turn RAG evaluation is an overlooked task with distinct challenges; corpus provides convergent evidence that context management is critical.
- **Break condition:** If the conversation contains multiple unrelated sub-topics or the rewrite prompt is poorly specified, the rewritten query may over-conflate or over-simplify, degrading retrieval. Lexical methods like BM25 performed better with full context rather than rewriting, suggesting the mechanism is retrieval-model dependent.

### Mechanism 2
- **Claim:** Multi-dimensional LLM-as-a-judge evaluation with chain-of-thought reasoning and reference-based scoring produces more reliable legal response assessment than traditional n-gram metrics.
- **Mechanism:** The evaluator LLM receives: (1) the multi-turn conversation, (2) a human-annotated reference answer, and (3) the candidate response. It performs pointwise scoring across five dimensions—Factuality, User Satisfaction, Clarity, Logical Coherence, and Completeness—using explicit chain-of-thought reasoning before outputting a final 1-10 score. The reference answer is anchored at score 8.
- **Core assumption:** The evaluator LLM (Qwen-2.5-72B-Instruct in experiments) possesses sufficient legal reasoning capability to detect subtle factual errors and logical inconsistencies, and its judgments correlate with human expert evaluation.
- **Evidence anchors:**
  - [abstract] "we introduce an LLM-as-a-judge evaluation pipeline to enable detailed and effective assessment"
  - [section 4, Table 2] Full prompt template specifying five evaluation dimensions with scoring rubrics
  - [corpus] No direct corpus validation of LLM-judge correlation for legal domain was found in neighbors; this mechanism remains unverified beyond the paper's internal design.
- **Break condition:** If the evaluator LLM lacks domain-specific legal knowledge or the reference answer contains errors, scores may be systematically biased. The paper does not report human-LLM correlation metrics for this specific judge configuration.

### Mechanism 3
- **Claim:** Dense embedding retrieval substantially outperforms lexical matching (BM25) for multi-turn legal queries, but retrieval quality remains a bottleneck even with optimal methods.
- **Mechanism:** Dense models (BGE-base, GTE-Qwen2-1.5B, text-embedding-3) encode semantic meaning, enabling matching between colloquial user language and formal legal terminology. This addresses the paper's observation that "the relevance of a question to legal knowledge is not simply determined by lexical or semantic similarity"—legal reasoning is required to identify relevant statutes.
- **Core assumption:** The embedding model's pre-training or fine-tuning has exposed it to legal language patterns, allowing it to bridge layperson queries and formal legal articles.
- **Evidence anchors:**
  - [section 5.2] "Dense retrieval methods outperform traditional lexical matching methods like BM25...GTE-Qwen2-1.5B-instruct achieved the best results."
  - [section 5.2] "Even with the best combination of model and processing strategy, the highest achieved Recall@10 is only 33.33%."
  - [corpus] CCIR CUP 2025 solution (arXiv:2510.15722) reports similar challenges in multi-turn legal RAG, confirming retrieval difficulty in this domain.
- **Break condition:** If legal articles contain highly specialized terminology not represented in the embedding space, or if queries require multi-hop legal reasoning, dense retrieval will miss relevant documents. The 33.33% ceiling suggests this mechanism alone is insufficient for production-grade legal RAG.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG) architecture
  - **Why needed here:** The entire benchmark evaluates RAG systems; you must understand the three-stage pipeline (retrieve → augment → generate) to interpret why retrieval failures propagate to generation quality.
  - **Quick check question:** If retrieval Recall@10 is 33%, what is the maximum possible improvement in response quality from optimizing only the generator?

- **Concept:** Multi-turn dialogue state and context management
  - **Why needed here:** Legal consultations unfold over 5 rounds with pronoun references, clarifications, and topic shifts. Understanding how to represent dialogue history (last query vs. full context vs. rewritten query) is the central experimental variable.
  - **Quick check question:** Why does using "Full Queries" without filtering degrade dense retrieval performance compared to "Last Query"?

- **Concept:** Embedding-based retrieval vs. lexical matching
  - **Why needed here:** The paper's headline finding is that dense retrieval outperforms BM25, but BM25 with full context beats BM25 with query rewriting. You must understand when each approach is appropriate.
  - **Quick check question:** A user asks "Can they take my house?" referring to a creditor mentioned three turns earlier. Which retrieval approach handles this better—BM25 with full context or dense retrieval with query rewrite—and why?

## Architecture Onboarding

- **Component map:** Conversation input (single/multi-turn) + Legal corpus (17,228 articles, with optional Legal Books and Legal Cases extensions) -> Processor (Last Query / Full Context / Full Queries / Query Rewrite strategies) -> Retriever (BM25, QLD via Pyserini / Dense BGE, GTE, text-embedding-3 via Faiss indexing) -> Generator (LLM with vLLM/Huggingface integration, accepts augmented prompt) -> Evaluator (Retrieval metrics + Generation metrics + LLM-as-a-judge)

- **Critical path:** 1. Select processor strategy based on retriever type (query rewrite for dense, full context for lexical) 2. Configure retriever with appropriate index and top-k (paper uses k=5 for generation experiments) 3. Pass retrieved articles to generator with domain-appropriate prompt 4. Evaluate with both traditional metrics and LLM-as-a-judge for actionable feedback

- **Design tradeoffs:** Query rewrite adds latency (extra LLM call) but improves Recall@10 by ~2-4 percentage points for dense retrieval. Dense retrieval requires GPU memory for embedding inference; BM25 is CPU-efficient but caps at ~21% Recall@10. Reference setting boosts LLM judge scores by 0.3-1.0 points vs. retriever setting, indicating retrieval noise is the limiting factor

- **Failure signatures:** Retrieval Recall@10 < 25% with dense models → likely processor mismatch or embedding model not suited for legal domain. Retriever setting produces lower LLM judge scores than zero-shot → retrieved documents contain misleading or contradictory information. Keyword accuracy near 30% → generator lacks legal knowledge grounding even with retrieved context

- **First 3 experiments:** 1. Processor ablation: Run all four processor strategies with GTE-Qwen2-1.5B retriever on a 100-sample subset; confirm query rewrite yields highest Recall@5 and analyze failure cases where full context outperforms. 2. Retrieval ceiling test: Provide the human-annotated relevant articles directly to each LLM (reference setting) and compare LLM judge scores vs. retriever setting; quantify the retrieval-to-generation quality gap. 3. Evaluator calibration: Run LLM-as-a-judge on 50 samples with both Qwen-2.5-72B and a secondary LLM (e.g., GPT-4o-mini); compute inter-annotator agreement to assess judge reliability before relying on automated evaluation

## Open Questions the Paper Calls Out

- **Question:** Can the findings and performance gaps identified in LexRAG be generalized to multilingual contexts and different legal systems, such as Common Law?
- **Basis in paper:** [explicit] The authors state in Section 7 that the current focus on Chinese legal scenarios "limits its applicability in broader multilingual contexts" and list expanding to additional languages as future work.
- **Why unresolved:** The current benchmark data and corpus are exclusively Chinese statutes and consultations, making cross-jurisdictional performance unknown.
- **What evidence would resolve it:** Construction and evaluation of an equivalent benchmark using English or other legal corpora to compare RAG performance across jurisdictions.

## Limitations

- LLM-as-a-judge evaluation lacks external validation through human-LLM correlation studies, leaving reliability uncertain
- Dense retrieval achieves only 33.33% Recall@10, suggesting fundamental limitations in current embedding models' ability to handle legal domain-specific semantic matching
- Benchmark focuses exclusively on Chinese legal scenarios, limiting generalizability to other legal systems and languages

## Confidence

- **High confidence:** Query rewriting improves dense retrieval for multi-turn legal queries (supported by ablation results showing consistent 2-4 percentage point gains)
- **Medium confidence:** Dense retrieval substantially outperforms lexical matching (statistically significant but still leaves 66.67% of relevant articles unfound)
- **Low confidence:** LLM-as-a-judge scores reliably measure legal response quality (no human baseline comparison provided for the specific Qwen-2.5-72B judge configuration)

## Next Checks

1. **Judge calibration:** Run the LLM-as-a-judge evaluation on 50 samples using both Qwen-2.5-72B and GPT-4o-mini; compute inter-annotator agreement to establish judge reliability before relying on automated evaluation.
2. **Retrieval ceiling quantification:** For 100 random samples, provide the human-annotated relevant articles directly to each LLM (reference setting) and compare LLM judge scores vs. retriever setting to quantify the exact quality gap caused by retrieval errors.
3. **Processor strategy ablation:** Execute all four processor strategies (Last Query, Full Context, Full Queries, Query Rewrite) with GTE-Qwen2-1.5B retriever on a 100-sample subset; analyze failure cases where full context outperforms query rewriting to identify query rewrite limitations.