---
ver: rpa2
title: 'PRIM: Towards Practical In-Image Multilingual Machine Translation'
arxiv_id: '2509.05146'
source_url: https://arxiv.org/abs/2509.05146
tags:
- text
- translation
- images
- image
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of In-Image Machine Translation
  (IIMT) under practical conditions, where current research relies on synthetic data
  with simple backgrounds, single fonts, fixed text positions, and bilingual translation,
  creating a significant gap with real-world scenarios. To tackle this, the authors
  introduce the PRIM dataset, a new benchmark containing real-world captured one-line
  text images with complex backgrounds, various fonts, diverse text positions, and
  support for five multilingual translation directions.
---

# PRIM: Towards Practical In-Image Multilingual Machine Translation

## Quick Facts
- arXiv ID: 2509.05146
- Source URL: https://arxiv.org/abs/2509.05146
- Reference count: 28
- Key outcome: VisTrans achieves 11.3 avg BLEU, 47.0 avg COMET, 28.8 FID on PRIM benchmark for real-world in-image multilingual translation

## Executive Summary
This paper addresses the practical challenges of In-Image Machine Translation (IIMT) by introducing the PRIM dataset and VisTrans model. Current IIMT research relies heavily on synthetic data with simplified conditions, creating a gap with real-world scenarios. The authors present VisTrans, an end-to-end model that processes visual text and background information separately using a two-stage training strategy with multi-task learning. The approach demonstrates superior performance in both translation quality and visual preservation compared to existing methods, achieving state-of-the-art results on the new PRIM benchmark.

## Method Summary
VisTrans is a ViT-based end-to-end model that separates visual text processing from background preservation through parallel encoder/decoder branches. The method employs a two-stage training strategy: Stage 1 trains vision and codebook components with auxiliary reconstruction tasks, while Stage 2 focuses on code sequence generation using a subword-to-char (S2C) decoder to bridge granularity mismatches. The model uses vector quantization for discrete code generation and integrates translation, text-image reconstruction, and OCR auxiliary tasks. Training leverages synthetic data rendered via TRDG toolkit, with real-world test images provided by the PRIM dataset.

## Key Results
- VisTrans achieves 12.6 BLEU for En-De translation vs. 10.4 for PEIT-Render baseline
- Lower FID scores demonstrate better visual quality (28.8 vs. 69.1 for TranslatotronV)
- Maintains multilingual capability across five translation directions (En-{Ru, Fr, Ro, De, Cs})
- Superior COMET scores (47.0 avg) indicate better translation quality than baselines

## Why This Works (Mechanism)

### Mechanism 1: Separate Processing of Visual Text and Background
Decoupling background preservation from text generation improves visual quality while maintaining translation capability. The architecture splits input through two parallel ViT-based branches—BackEncoder/BackDecoder captures background features directly from source image; MT-Encoder/1-Pass-Decoder handles translation. Final output combines H_back + H_code before image decoding, avoiding cascade-style inpainting artifacts.

### Mechanism 2: Two-Stage Training with Multi-Task Auxiliary Objectives
Staged training isolates visual representation learning from translation-to-code mapping, improving convergence and modularity. Stage 1 trains both branches with auxiliary decoders for text-image reconstruction and background reconstruction plus TIT/OCR tasks. Stage 2 freezes visual representations and trains S2C + 2-Pass decoders to generate code sequences from 1-Pass hidden states.

### Mechanism 3: Subword-to-Char (S2C) Decoder for Granularity Alignment
Converting subword representations to character-level before code generation bridges granularity mismatch between translation embeddings and visual codebook entries. 1-Pass Decoder produces subword-level H_1pass. S2C Decoder (semi-autoregressive, K=2) expands this to char-level H_char, which conditions 2-Pass Decoder for code sequence generation.

## Foundational Learning

- **Vision Transformer (ViT) patch-based encoding**: All encoders/decoders use ViT with configurable patch sizes. Understanding patch embedding and positional encoding is prerequisite to reading architecture. Quick check: Given a 32×512 image with patch_size=16, how many patch tokens are produced? (Answer: 64)

- **Vector Quantization (VQ) and Codebook Learning**: The codebook maps encoded features to discrete codes. Commitment loss and straight-through estimation are central to Stage 1 training. Quick check: In Eq. 3, what happens when the nearest codebook vector e_k is selected for feature x_i? (Answer: The continuous feature is replaced with the discrete code's vector)

- **Semi-Autoregressive (SAT) Decoding**: S2C Decoder uses SAT with K=2, generating token groups rather than single tokens. The staircase mask enables parallel-in-group generation. Quick check: For K=2 and sequence length 6, how many generation steps does SAT require vs. AT? (Answer: SAT: 3 steps; AT: 6 steps)

## Architecture Onboarding

- **Component map**: Input Image → BackEncoder → BackDecoder1/2 → H_back (parallel path) and MT-Encoder → 1-Pass-Decoder → H_1pass → S2C-Decoder → H_char → 2-Pass-Decoder → Code Sequence → H_code → (+) → ImgDecoder → Output Image

- **Critical path**: Stage 1 trains both branches + codebook with auxiliary reconstruction and TIT/OCR losses; Stage 2 freezes MT-Encoder and 1-Pass-Decoder, trains S2C and 2-Pass to predict codes; Inference: source → MT-Encoder → 1-Pass → S2C → 2-Pass → codes + H_back → ImgDecoder

- **Design tradeoffs**: Patch size 8 for MT-Encoder (finer text detail) vs. 16 for Back-Encoder (efficiency); SAT K=2: balanced BLEU/speed; Codebook 8K entries: sufficient for 5 languages but may not scale to 50+ languages; Single-line constraint: architecture assumes one-line text

- **Failure signatures**: High FID + low BLEU: Background branch not converging; Low FID + low BLEU: Translation failing; Truncated output text: Code sequence generation incomplete; Gibberish glyphs: Codebook undertrained

- **First 3 experiments**: 1) Reproduce Stage 1 with single translation direction (En→De); verify background reconstruction (FID < 50) and TIT auxiliary task (BLEU > 30); 2) Ablate S2C decoder: train with direct subword-to-code mapping; expect >30% BLEU drop; 3) Evaluate on PRIM test set with cascade baseline; confirm VisTrans achieves lower FID (<40) while maintaining comparable BLEU (>10)

## Open Questions the Paper Calls Out

1. **Advanced quantization techniques**: How would advanced quantization techniques or decoders optimized for long-sequence modeling impact visual quality and translation accuracy? The authors note they only conducted experiments with the most basic codebook and decoder, lacking investigation into "more advanced quantization techniques or decoders that better support long-sequence modeling."

2. **Multi-line text generalization**: Can the VisTrans architecture effectively generalize to real-world images containing complex multi-line text layouts without relying on synthetic data for training? While the paper tests multi-line capabilities on synthetic IIMT30k dataset, the PRIM dataset itself is limited to "one-line text images."

3. **Computational efficiency**: Is it possible to reduce the high computational resource costs and hardware requirements of the current two-stage training strategy without sacrificing performance? The authors explicitly list high computational costs and hardware requirements resulting from the two-stage training and multi-task learning strategy as a limitation.

## Limitations

- Dataset scale and generalization: Training data remains synthetic despite real-world test images, creating potential synthetic-to-real gaps
- Multilingual generalization: Model supports only five translation directions, scalability to dozens of languages untested
- Architecture specificity: Single-line text assumption and fixed image dimensions (32×512) limit real-world applicability
- Evaluation metrics: Reliance on EasyOCR introduces potential errors, particularly for non-Latin scripts

## Confidence

- **High Confidence**: Separate processing of visual text and background; two-stage training procedure
- **Medium Confidence**: S2C decoder's effectiveness in bridging granularity mismatch
- **Low Confidence**: Claims about practical real-world applicability beyond PRIM benchmark

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate VisTrans on an entirely different dataset of real-world text images (e.g., captured signage, documents, social media posts) without any synthetic pre-training, measuring both translation quality and visual fidelity.

2. **Multi-Line Text Extension**: Modify the architecture to handle multi-line text images by introducing layout modeling (e.g., line position encoding) and evaluate performance degradation compared to single-line specialization.

3. **Codebook Capacity Scaling**: Train VisTrans with varying codebook sizes (4K, 16K, 32K) and measure the trade-off between multilingual coverage (testing on 10+ languages) and visual quality (FID), identifying the optimal size for practical deployment.