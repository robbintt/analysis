---
ver: rpa2
title: 'Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking'
arxiv_id: '2512.24297'
source_url: https://arxiv.org/abs/2512.24297
tags:
- reasoning
- visual
- code
- aime
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIGR improves reasoning by integrating executable visual construction
  into multi-turn inference. Instead of relying on implicit symbolic states or predefined
  visual tools, it generates and executes code to construct diagrams as intermediate
  reasoning states, enabling precise control and revisable visual feedback.
---

# Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking

## Quick Facts
- arXiv ID: 2512.24297
- Source URL: https://arxiv.org/abs/2512.24297
- Reference count: 24
- FIGR improves reasoning by integrating executable visual construction into multi-turn inference

## Executive Summary
FIGR introduces a novel approach to mathematical reasoning by generating and executing Python code to construct diagrams during problem solving. Instead of relying on implicit symbolic states or predefined visual tools, FIGR produces deterministic visual feedback that enforces geometric consistency. Trained via reinforcement learning on DeepMath-103K, FIGR achieves state-of-the-art results on challenging mathematical benchmarks including AIME 2025 and BeyondAIME.

## Method Summary
FIGR integrates executable visual construction into multi-turn mathematical reasoning using a vision-language policy that generates Python code for diagram creation. The system operates through a loop where the policy produces either text continuations or code blocks, which are executed in a sandboxed interpreter producing both textual feedback and rendered diagrams. Reinforcement learning via Group Relative Policy Optimization (GRPO) trains the policy using adaptive rewards that encourage visual construction only when beneficial. The approach requires no supervised cold-start data and demonstrates significant improvements over text-only and multimodal baselines.

## Key Results
- Improves base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME
- Outperforms strong text-only and multimodal baselines
- Maintains stable ~60% code ratio with high pass rate through adaptive reward mechanism

## Why This Works (Mechanism)

### Mechanism 1
Executable code-based diagrams enforce geometric consistency that implicit text reasoning cannot maintain. The policy generates Python code that executes in a sandboxed interpreter, producing deterministic visual output that externalizes spatial constraints—errors become visible when rendered coordinates violate geometric relations, enabling correction. This assumes rendering engines produce accurate visualizations and code correctly expresses geometric intent.

### Mechanism 2
The adaptive reward mechanism prevents code-overuse collapse and encourages purposeful visual construction. An auxiliary classifier predicts task suitability, and rewards visual construction only when coupled with correct final answers, weighted higher for suitable tasks. This ties visual behavior to outcome success, assuming the classifier is directionally accurate and correct answers indicate beneficial reasoning paths.

### Mechanism 3
Trajectory-level GRPO enables learning long-horizon reasoning policies where individual actions lack local evaluability. The algorithm samples trajectories per prompt, computes group-average baseline, and optimizes relative advantage, reducing variance and emphasizing behavioral differences without step-wise supervision or value networks. This assumes outcome rewards reflect trajectory quality and within-group comparison isolates meaningful behavioral differences.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Core RL algorithm for trajectory-level learning without value functions. Understanding advantage computation via group baselines is essential to debug reward scaling. *Quick check*: Given 8 sampled trajectories with rewards [0, 0, 1, 0, 0, 0, 0, 0], what is the advantage of the third trajectory? (Answer: 1 - 0.125 = 0.875)

- **Executable Reasoning Loops**: FIGR's architecture interleaves policy generation with interpreter execution. Debugging requires tracing state updates across text/code boundaries. *Quick check*: If code c_t produces both textual output T_{t+1} and image I_{t+1}, which components of h_{t+1} change? (Answer: All three—prior context, code, and both feedback modalities are concatenated)

- **Vision-Language Model Policy Initialization**: FIGR initializes from Qwen3-VL-32B-Instruct, inheriting multimodal understanding. Performance gains are measured relative to this base. *Quick check*: Why can't a text-only LLM be used as base policy for FIGR? (Answer: It cannot interpret rendered diagram I_{t+1} as input to subsequent reasoning steps)

## Architecture Onboarding

- **Component map**: Input Question → Suitability Classifier (frozen, DeepSeek-V3) → Policy (Qwen3-VL-32B) ↔ Interpreter (sandboxed Python) → Text Feedback + Rendered Diagram → GRPO Trainer ← Trajectory Buffer ← Reward Computation (R_acc + R_fmt + R_vis)

- **Critical path**: Question enters; classifier tags suitability. Policy samples action (text OR code). If code: interpreter executes, returns (T, I). Context updates; loop repeats until termination. Full trajectory scored; GRPO computes advantages against group mean. Policy updated via clipped surrogate objective.

- **Design tradeoffs**: Reward weighting (1.0 vs. 0.2 for s=1 vs. s=0) encourages visual use on suitable tasks but risks over-reliance if classifier errs. Max 3 interaction rounds limits compute overhead but may truncate complex reasoning. No supervised cold-start removes need for labeled visual-reasoning data but requires more RL samples.

- **Failure signatures**: Code ratio → 0 during training indicates ARM disabled or R_vis weight too low. High code pass rate but low accuracy suggests code executes but doesn't aid reasoning. AIME performance strong, MATH-500 weak indicates overfitting to competition-style geometry.

- **First 3 experiments**: 1) Reproduce ARM ablation: Train with ARM disabled (R_vis = 0 for all trajectories). Confirm code ratio collapses. 2) Suitability classifier oracle test: Replace classifier with ground-truth labels on 100 examples. Measure accuracy gap. 3) Interaction round sweep: Train with max_rounds ∈ {1, 2, 3, 5} on BeyondAIME subset. Plot accuracy vs. rounds.

## Open Questions the Paper Calls Out

- How can we precisely identify and characterize task regimes where executable visual construction provides substantial benefit versus those solvable through purely symbolic reasoning? The paper demonstrates benefits on geometry-heavy problems but lacks a systematic framework for predicting a priori which problems will benefit.

- How can the computational overhead of multi-turn reasoning with code execution be reduced while preserving reasoning quality? The current approach requires executing potentially long code blocks across multiple turns, which is inherently costly.

- To what extent does the FIGR paradigm generalize beyond mathematical reasoning to domains such as scientific reasoning, program synthesis, and long-horizon planning? All experiments are confined to seven mathematical benchmarks; no evidence exists for broader applicability.

## Limitations
- Adaptive reward mechanism's effectiveness depends critically on suitability classifier's accuracy, which is never evaluated against ground truth
- 13.12% AIME 2025 improvement is measured against a specific base model, making generalization to other VLMs uncertain
- Paper does not ablate the maximum 3 interaction rounds constraint, leaving open whether performance could improve with longer reasoning chains

## Confidence
- **High confidence**: Executable visual construction mechanism and GRPO implementation are well-specified and technically sound
- **Medium confidence**: 13.12% improvement on AIME 2025 based on single training run and specific baselines
- **Low confidence**: Suitability classifier's accuracy and impact on learning is not empirically validated

## Next Checks
1. **Classifier Accuracy Validation**: Replace DeepSeek-V3 classifier with ground-truth annotations on 100 examples. Train two versions: one using oracle labels and one using classifier predictions. Measure performance gap.

2. **Interaction Round Ablation**: Systematically vary maximum interaction rounds (1, 2, 3, 5) during training on BeyondAIME validation set. Plot accuracy vs. computational cost to identify optimal trade-off.

3. **Cross-VLM Generalization**: Initialize FIGR from different vision-language model (e.g., LLaVA-Next-34B) and train using same DeepMath-103K dataset. Compare relative improvement over base model performance.