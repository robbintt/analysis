---
ver: rpa2
title: 'Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated
  with Human-Ranked Data'
arxiv_id: '2506.02018'
source_url: https://arxiv.org/abs/2506.02018
tags:
- paraphrase
- human
- apty
- changes
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that using human-ranked data with Direct
  Preference Optimization (DPO) significantly improves paraphrase-type generation
  accuracy by 3 percentage points over a supervised baseline, while human evaluators
  prefer these outputs 7 percentage points more. A newly created human-annotated dataset
  and a paraphrase-type detection model with F1 scores up to 0.91 enable more rigorous
  evaluations.
---

# Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data

## Quick Facts
- **arXiv ID**: 2506.02018
- **Source URL**: https://arxiv.org/abs/2506.02018
- **Reference count**: 40
- **Primary result**: DPO with human-ranked data improves paraphrase-type generation accuracy by 3 percentage points over supervised baseline

## Executive Summary
This study introduces a novel approach to improving paraphrase-type generation by incorporating human-ranked preference data with Direct Preference Optimization (DPO) and Reinforcement Learning from Human Feedback (RLHF). The research demonstrates that DPO significantly outperforms traditional supervised fine-tuning, achieving 3 percentage points higher accuracy in paraphrase-type generation. Human evaluators consistently preferred DPO-generated paraphrases by 7 percentage points. The work establishes a new human-annotated dataset and develops a paraphrase-type detection model with F1 scores up to 0.91, enabling more rigorous evaluation of paraphrase generation systems.

## Method Summary
The study combines traditional supervised fine-tuning with preference-based optimization techniques to improve paraphrase generation. Researchers created a new human-annotated dataset specifically designed for evaluating paraphrase types. They trained a paraphrase-type detection model to classify generated paraphrases, enabling quantitative assessment beyond simple quality metrics. The core innovation lies in using human-ranked preference data with Direct Preference Optimization (DPO) and RLHF, which leverages human judgments to guide the model toward generating more semantically accurate paraphrases that better match human preferences.

## Key Results
- DPO with human-ranked data improved paraphrase-type generation accuracy by 3 percentage points compared to supervised fine-tuning
- Human evaluators preferred DPO-generated paraphrases 7 percentage points more than baseline models
- The newly developed paraphrase-type detection model achieved F1 scores up to 0.91, enabling more rigorous evaluation

## Why This Works (Mechanism)
The effectiveness stems from leveraging human preference signals to guide model optimization. Unlike traditional supervised learning that relies solely on labeled examples, DPO and RLHF incorporate human judgments about which paraphrases better preserve meaning while maintaining linguistic diversity. This approach aligns the model's objectives more closely with human perception of quality and semantic accuracy. The human-ranked data provides richer feedback signals than binary labels, allowing the model to learn nuanced distinctions between paraphrase types and generate outputs that better match human expectations.

## Foundational Learning
- **Paraphrase generation**: Creating alternative expressions of the same meaning is essential for applications like text summarization and dialogue systems
- **Paraphrase-type detection**: Classifying paraphrases by their transformation patterns enables targeted evaluation and improvement
- **Direct Preference Optimization (DPO)**: Uses pairwise preference data to optimize model outputs without requiring explicit reward functions
- **Reinforcement Learning from Human Feedback (RLHF)**: Incorporates human judgments into the learning process to align model behavior with human values
- **Human-annotated datasets**: High-quality labeled data is crucial for training and evaluating NLP models effectively
- **Semantic preservation**: Maintaining meaning across paraphrases is fundamental to generating useful and accurate text transformations

## Architecture Onboarding

**Component Map**: Human Annotation -> Preference Dataset -> DPO/RLHF Training -> Paraphrase-Type Detection -> Evaluation

**Critical Path**: Human annotations provide preference data → DPO optimizes model using preferences → Model generates paraphrases → Detection model evaluates paraphrase types → Human evaluation validates results

**Design Tradeoffs**: The approach prioritizes semantic accuracy and human preference alignment over raw generation speed or diversity. Using human-annotated data increases training time and resource requirements but produces more reliable outputs.

**Failure Signatures**: If the paraphrase-type detection model has low F1 scores, evaluation becomes unreliable. Poor quality human annotations lead to suboptimal preference learning. Overfitting to specific paraphrase types may reduce model generalization.

**3 First Experiments**:
1. Train paraphrase-type detection model on small human-annotated dataset and measure F1 scores
2. Apply DPO to baseline model using synthetic preference pairs to verify optimization works
3. Compare supervised fine-tuning vs DPO on held-out paraphrase generation tasks

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions or areas for future research.

## Limitations
- The newly created human-annotated dataset lacks detailed specifications regarding size, diversity, and annotation quality, limiting assessment of generalizability
- Human evaluation methodology is not clearly described, including annotator expertise and evaluation criteria, introducing potential subjectivity
- The study focuses narrowly on paraphrase-type generation accuracy without addressing fluency, diversity, or creativity of generated paraphrases
- Evaluation is limited to paraphrase-type generation without exploring impact on other NLP tasks

## Confidence
**High**: DPO with human-ranked data improves paraphrase-type generation accuracy and produces more reliable, semantically accurate paraphrases
**Medium**: The newly created human-annotated dataset and paraphrase-type detection model enable more rigorous evaluations
**Low**: The study advances the field toward richer, user-aligned language generation and more robust evaluation frameworks, as these claims require further validation and comparison with other state-of-the-art methods

## Next Checks
1. Conduct comprehensive evaluation of generated paraphrases including fluency, diversity, and creativity measures beyond paraphrase-type generation accuracy
2. Perform detailed analysis of the newly created human-annotated dataset including size, diversity, and annotation quality to assess impact on results and generalizability
3. Compare proposed approach with other state-of-the-art methods for paraphrase generation and evaluation to validate claimed advancements in the field