---
ver: rpa2
title: Megrez-Omni Technical Report
arxiv_id: '2502.15803'
source_url: https://arxiv.org/abs/2502.15803
tags:
- arxiv
- language
- data
- visual
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Megrez-3B-Omni, a compact multimodal language
  model achieving state-of-the-art performance across vision, text, and audio understanding
  tasks. The model combines a 2.29B language module with a SigLip-400M vision encoder
  and Whisper-large-v3 audio encoder, using cross-attention connectors to integrate
  multimodal inputs.
---

# Megrez-Omni Technical Report

## Quick Facts
- arXiv ID: 2502.15803
- Source URL: https://arxiv.org/abs/2502.15803
- Reference count: 40
- Primary result: 3B parameter multimodal model achieving 66.2% on OpenCompass vision tasks and 82.8% on OCRBench while maintaining fast edge device inference

## Executive Summary
Megrez-3B-Omni is a compact multimodal language model combining a 2.29B language module with SigLip-400M vision and Whisper-large-v3 audio encoders. The model achieves state-of-the-art performance among compact multimodal models through cross-attention connectors that efficiently compress multimodal tokens while preserving semantic information. It maintains strong text understanding capabilities while adding vision and audio comprehension, making it suitable for on-device AI applications requiring multimodal processing.

## Method Summary
The model uses a staged training approach with progressive component freezing to preserve text capabilities while adding multimodal understanding. It employs cross-attention perceiver resamplers for vision token compression (64 tokens per image slice) and MLP projectors for audio (50 tokens per second). The architecture processes up to 9 image slices plus a global view for vision tasks, with dynamic slicing preserving fine-grained details needed for OCR without prohibitive token costs. Training progresses through three phases: connector-only training, encoder+connector training, and joint omni instruction tuning with all parameters at low learning rate.

## Key Results
- Achieves 66.2% on OpenCompass vision benchmark, competitive with much larger models
- Scores 82.8% on OCRBench for document understanding and 91.6% on DocVQA
- Maintains text performance with MMLU 73.3% and C-EVAL 84.0%, showing <2% degradation from text-only counterpart
- Demonstrates fast inference speeds suitable for edge device deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention connectors enable efficient compression of multimodal tokens while preserving semantic information
- Core assumption: Token compression can retain sufficient semantic content without requiring full-resolution attention across all modalities
- Evidence: Cross-attention perceiver with single layer compresses visual features to 64 tokens per slice; model achieves strong OCR performance (82.8% OCRBench)
- Break condition: High-resolution images requiring fine detail may exceed the 9-slice token budget

### Mechanism 2
- Claim: Staged training with selective component freezing preserves text capabilities while adding multimodal understanding
- Core assumption: Foundational language abilities must be established and protected before multimodal extensions are layered on top
- Evidence: Three-phase alignment with progressive unfreezing maintains text benchmarks (<2% variation from Megrez-3B-Instruct)
- Break condition: Aggressive learning rates during omni instruction tuning risk degrading text-only task performance

### Mechanism 3
- Claim: Dynamic image slicing preserves fine-grained visual details needed for OCR without prohibitive token costs
- Core assumption: Local detail extraction is more critical for OCR/document tasks than uniform resolution scaling
- Evidence: Images divided into up to 9 local slices (64 tokens each) plus one global 448×448 view enables high-resolution understanding
- Break condition: Extremely large documents may still exceed context window capacity

## Foundational Learning

- **Concept: Cross-Attention for Token Compression**
  - Why needed here: Understanding how the perceiver resampler reduces vision tokens while maintaining semantic alignment with the LLM's embedding space
  - Quick check question: Can you explain why cross-attention enables compression while standard self-attention would preserve all input tokens?

- **Concept: Catastrophic Forgetting in Multi-Stage Fine-Tuning**
  - Why needed here: Critical for understanding why the training freezes components progressively rather than training all parameters from the start
  - Quick check question: What would likely happen to text benchmark scores if all parameters were trained at 1e-4 from initialization?

- **Concept: Modality-Specific Connector Design**
  - Why needed here: The paper uses different connector architectures (cross-attention for vision, linear for audio)—understanding why requires grasping alignment tradeoffs
  - Quick check question: Why might vision benefit from cross-attention compression while audio uses a simpler MLP projection?

## Architecture Onboarding

- **Component map**: Image Input → SigLIP-SO400M → Cross-Attention Perceiver → 64 tokens/slice; Audio Input → Whisper-large-v3 encoder → MLP Projector → 50 tokens/sec; Text Input → Tokenizer → Word Embeddings; → Concatenated Multimodal Tokens → LLaMA-2 backbone with GQA → Output logits

- **Critical path**: Vision encoder forward → perceiver resampler cross-attention → LLM forward. This dominates compute for image inputs.

- **Design tradeoffs**:
  - Vision connector: Cross-attention perceiver (36M params, learnable compression) vs. linear projection (faster but less adaptive)
  - Audio connector: MLP (3M params, simple) vs. transformer adapter (more expressive but adds latency)
  - Vocabulary: 120K tokens (optimized Chinese compression 5.02 bytes/token) vs. Qwen2's 151K (broader multilingual coverage)

- **Failure signatures**:
  - OCR accuracy drops on dense documents → token budget exhausted; check slice count configuration
  - Audio transcription quality varies by accent → Whisper encoder may need unfreezing, not just connector training
  - Text benchmarks degrade after multimodal training → omni-tuning LR too high; verify ≤1e-5

- **First 3 experiments**:
  1. Ablate vision connector architecture: Compare cross-attention perceiver vs. single linear layer on OCRBench and TextVQA
  2. Token budget sweep: Test 32, 64, and 128 vision tokens per slice on document understanding tasks
  3. Omni-tuning learning rate grid: Search {1e-6, 5e-6, 1e-5} while tracking text benchmark degradation (MMLU, C-EVAL)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the architecture be extended to support end-to-end Text-to-Speech (TTS) generation directly within the model?
  - Basis: The Conclusion lists "developing an end-to-end TTS system integrated with LLMs" as a primary direction for future improvement
  - Why unresolved: Model processes audio input but responds exclusively in text, lacking a speech decoder or generator
  - What evidence would resolve it: Updated model variant capable of generating waveform outputs with edge device latency metrics

- **Open Question 2**: Can reinforcement learning (RL) effectively enhance complex reasoning and mathematical capabilities in compact, on-device models?
  - Basis: The Conclusion identifies "solving complex reasoning problems with reinforcement learning" as a specific goal; current model shows performance drop in math (MATH) and coding (HumanEval) between Instruct and Omni versions
  - Why unresolved: Current training relies on SFT, and authors note Omni capabilities trade off slightly with specialized reasoning skills
  - What evidence would resolve it: Training ablation study showing improved MATH and HumanEval scores after RL application without parameter count increase

- **Open Question 3**: What architectural modifications are required to support continuous, real-time video and audio streams for live interaction?
  - Basis: The Conclusion lists "supporting real-time video and audio streams in live interactions" and "multi-image... understanding" as necessary future capabilities
  - Why unresolved: Current architecture processes static visual inputs and truncates audio to 30-second clips, lacking mechanism for continuous state updates
  - What evidence would resolve it: Demonstration of model processing live video feed with corresponding memory usage and latency metrics

## Limitations

- Vision understanding appears strongest on document-based tasks rather than general visual reasoning, suggesting potential weaknesses in abstract visual comprehension
- Audio capabilities only tested on Chinese FLEURS data without evaluation on diverse accents, noise conditions, or non-Chinese languages
- Cross-attention connector architecture adds complexity and potential failure modes that aren't fully explored through ablation studies
- Model's multimodal capabilities primarily evaluated on English and Chinese tasks, with no reported performance on other languages or truly low-resource language pairs

## Confidence

- **High Confidence**: Text-only performance retention (MMLU 73.3%, C-EVAL 84.0%) and basic multimodal integration mechanics
- **Medium Confidence**: Vision OCR/document understanding capabilities (OCRBench 82.8%, DocVQA 91.6%) and the staged training methodology
- **Low Confidence**: General visual reasoning abilities beyond document tasks, audio transcription robustness across diverse conditions, and edge device performance claims without empirical benchmarks

## Next Checks

1. **Ablation study of connector architectures**: Compare cross-attention perceiver vs. linear projection on OCRBench and TextVQA to quantify the benefit of the more complex connector design

2. **Vision token budget sensitivity**: Systematically test 32, 64, and 128 vision tokens per slice on document understanding tasks to determine optimal trade-offs between detail preservation and computational cost

3. **Multi-stage training impact**: Compare model performance when training all parameters from initialization versus the proposed staged approach to quantify catastrophic forgetting prevention