---
ver: rpa2
title: 'Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution
  Environments'
arxiv_id: '2506.23706'
source_url: https://arxiv.org/abs/2506.23706
tags:
- https
- audit
- audits
- arxiv
- attestable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Attestable Audits enable verifiable AI safety benchmarking by running
  audits inside Trusted Execution Environments, ensuring model and data confidentiality
  while providing cryptographic proof of compliance. The system allows auditors to
  verify model behavior without accessing sensitive weights or datasets, using hardware-backed
  attestation to bind audit results to specific model and code versions.
---

# Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments

## Quick Facts
- arXiv ID: 2506.23706
- Source URL: https://arxiv.org/abs/2506.23706
- Reference count: 40
- A TEE-based system enabling verifiable AI safety audits without exposing model weights or training data

## Executive Summary
Attestable Audits presents a novel approach to AI safety benchmarking that addresses the verification gap in AI governance. The system leverages Trusted Execution Environments (TEEs) to run audits on sensitive models while maintaining both model and data confidentiality. By using hardware-backed attestation, auditors can cryptographically verify that specific model and code versions were used in safety evaluations, without requiring direct access to proprietary weights or datasets. This creates a transparent and reproducible audit process that could significantly improve trust in AI safety claims while protecting commercial interests.

## Method Summary
The Attestable Audits system creates a verifiable audit pipeline where models are loaded into secure enclaves (specifically AWS Nitro Enclaves) that provide hardware-enforced isolation. The system generates cryptographic attestations that bind audit results to specific model weights and evaluation code, ensuring results are reproducible and tamper-proof. Auditors can verify these attestations without accessing the model directly, using remote attestation protocols to confirm the integrity of the execution environment. The architecture separates concerns between model owners, auditors, and third-party verifiers, with each role having clearly defined capabilities and access restrictions within the TEE framework.

## Key Results
- Quantized Llama-3.1-8B achieves 51.4% MMLU accuracy and 2.4% toxicity rate in TEE environment
- CPU inference in TEEs costs 21.7× more than GPU inference for the same task
- Prototype demonstrates feasibility of verifiable audits without exposing model weights or training data

## Why This Works (Mechanism)
The system works by leveraging TEE hardware features to create an isolated execution environment where sensitive computations can occur. The TEE provides memory encryption, code integrity verification, and remote attestation capabilities. When an audit is requested, the model owner loads their model into the enclave along with the evaluation code. The TEE generates a cryptographic attestation that proves the specific model version and code were executed in the secure environment. This attestation is then cryptographically bound to the audit results, creating an unbroken chain of trust from the model execution to the final safety assessment.

## Foundational Learning
- **Trusted Execution Environments**: Hardware-isolated execution spaces that protect code and data from the host system. Why needed: Provides the foundational security guarantee that model weights remain confidential. Quick check: Can the TEE prevent even privileged system administrators from accessing protected memory?
- **Remote Attestation**: Cryptographic protocol that proves to remote parties that specific software is running in a genuine TEE. Why needed: Enables auditors to verify the integrity and authenticity of the audit environment. Quick check: Does the attestation include binding to specific model hashes and code versions?
- **Confidential Computing**: Computing paradigm where data remains encrypted during processing. Why needed: Ensures model weights and evaluation inputs remain protected throughout the audit. Quick check: Are all data transfers between TEE and external systems encrypted and integrity-protected?
- **Hardware Security Modules**: Specialized hardware for secure key storage and cryptographic operations. Why needed: Provides root of trust for the entire attestation chain. Quick check: Is the hardware root of trust properly validated and resistant to physical attacks?
- **Side-Channel Resistance**: Design principles that prevent information leakage through timing, power, or other indirect channels. Why needed: TEEs can still be vulnerable to sophisticated side-channel attacks. Quick check: Does the implementation include mitigations for timing and cache-based attacks?

## Architecture Onboarding

**Component Map**: Model Owner -> TEE Enclave -> Evaluation Code -> Audit Results -> Attestation Generator -> Verifier

**Critical Path**: The attestation chain is the critical path - any failure in the cryptographic binding between model, code, and results breaks the entire verification system. This includes secure model loading, integrity verification of evaluation code, secure execution, result generation, and attestation creation.

**Design Tradeoffs**: The system trades performance for security, accepting 21.7× slower inference in exchange for confidentiality guarantees. It also centralizes trust in specific hardware vendors (AWS, Intel, AMD), creating potential single points of failure but simplifying the attestation infrastructure.

**Failure Signatures**: Common failure modes include attestation verification failures (indicating code/model tampering), enclave initialization failures (hardware or configuration issues), and performance bottlenecks that make the system impractical for large-scale deployment. The system should provide clear error codes distinguishing between security failures and operational issues.

**First Experiments**:
1. Verify basic TEE functionality by running a simple hello-world program and generating an attestation
2. Test model loading with different quantization levels to find the performance-security sweet spot
3. Validate the complete audit pipeline with a known model and benchmark to ensure end-to-end functionality

## Open Questions the Paper Calls Out
None explicitly called out in the source material.

## Limitations
- Significant performance degradation (21.7× slower CPU inference) may limit practical deployment
- Scalability concerns for larger frontier models beyond the tested 8B parameter range
- Dependence on specific cloud provider TEEs creates vendor lock-in and centralization risks
- Ultimate security relies on hardware/firmware implementation, which could contain vulnerabilities

## Confidence

**High**: Core technical feasibility of TEE-based attestation for AI audits
**Medium**: Performance claims and scalability to larger models
**Medium**: Long-term security guarantees dependent on hardware implementation

## Next Checks

1. Benchmark scalability by testing larger models (70B+ parameters) and measuring performance degradation, including exploring hybrid CPU-GPU approaches
2. Conduct formal security analysis of the attestation chain, including potential side-channel attacks and hardware vulnerability assessments
3. Implement and test alternative TEE platforms (Intel SGX, AMD SEV) to evaluate portability and compare performance/security trade-offs across implementations