---
ver: rpa2
title: 'G-KV: Decoding-Time KV Cache Eviction with Global Attention'
arxiv_id: '2512.00504'
source_url: https://arxiv.org/abs/2512.00504
tags:
- score
- attention
- cache
- tokens
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: G-KV improves KV cache compression for long-chain-of-thought reasoning
  models by integrating local and historical attention scores into a global scoring
  mechanism, which more accurately captures long-term token importance. A memory decay
  factor and three score aggregation forms (max, mean, sum) are explored, with max
  and sum performing best.
---

# G-KV: Decoding-Time KV Cache Eviction with Global Attention

## Quick Facts
- arXiv ID: 2512.00504
- Source URL: https://arxiv.org/abs/2512.00504
- Reference count: 40
- Primary result: Achieves 5%-20% performance gains over prior methods under low KV cache budgets (e.g., 512 tokens) for long-chain-of-thought reasoning models

## Executive Summary
G-KV introduces a decoding-time KV cache compression method that integrates local and historical attention scores into a global scoring mechanism, achieving 5%-20% performance improvements over prior methods for long-chain-of-thought reasoning models under low KV cache budgets. The method combines local attention scores with decayed historical scores using memory decay and three aggregation forms (max, mean, sum), with max and sum performing best. Post-training techniques including reinforcement learning with sparse attention masks and distillation are introduced to adapt models to compressed KV cache settings. Experiments on AMC-23 and AIME-24 benchmarks show G-KV maintains accuracy while reducing token retention ratios, with RL-Sparse training outperforming RL-Full and demonstrating the importance of training-inference alignment.

## Method Summary
G-KV implements decoding-time KV cache compression by computing a global score for each token that combines normalized local attention scores with decayed historical scores. The method maintains a memory decay factor (α ∈ [0.8, 0.9]) and explores three aggregation forms (max, mean, sum) to combine current and historical attention. During decoding, every 128 tokens the model ranks all cached tokens by their global score and retains the top (b-w) tokens plus the observation window. The approach can be extended with post-training via reinforcement learning using sparse attention masks or distillation to further improve adaptation to compressed cache settings.

## Key Results
- Achieves 5%-20% performance gains over prior methods under low KV cache budgets (e.g., 512 tokens)
- Improves pass@1 by up to 20% compared to MorphKV for 512-token budget
- Achieves 4×-12× higher throughput versus full KV cache
- RL-Sparse training outperforms RL-Full, confirming reduced training-inference mismatch
- Token retention becomes more evenly distributed across sequence, preserving critical prompt context

## Why This Works (Mechanism)

### Mechanism 1: Intermittent Attention Capture
Combining local and historical attention scores captures long-term token importance better than local-only methods by recognizing that token importance is intermittent—tokens not attended in current window may be critical in future windows. The global score combines decayed previous global score with normalized local attention scores using max, mean, or sum aggregation. The decay factor (0.8-0.9) promotes eventual eviction of unattended tokens while preserving tokens that receive attention sporadically.

### Mechanism 2: Training-Inference Alignment
Training with sparse attention masks reduces training-inference mismatch by aligning the training policy directly with inference policy under compressed cache conditions. During RL training, the model generates outputs under compressed KV cache, records evicted token positions, constructs sparse attention masks, and computes gradients only on visible positions. This prevents the performance degradation that occurs when models trained with full attention face sparse attention constraints at inference.

### Mechanism 3: Even Retention Distribution
Global scoring leads to more even retention of tokens across the sequence, preserving prompt context and earlier reasoning steps rather than favoring recent tokens. Local scores are biased toward tokens near the observation window due to semantic similarity and RoPE positional effects, while global scores accumulate attention across all windows, allowing distant but periodically-attended tokens to maintain competitive scores.

## Foundational Learning

- **KV Cache in Autoregressive Decoding**: Understanding what KV cache stores (key and value states from attention layers) and why it grows linearly with sequence length is essential to grasp compression motivation. *Quick check*: Explain why attention computation complexity is quadratic in sequence length while KV cache memory is linear.

- **Attention Score Sparsity**: G-KV relies on the observation that attention is sparse—only a small subset of tokens receive significant attention at each step. *Quick check*: Given a 16k token sequence, what does >90% sparsity (tokens below 1% of max attention) imply for compression potential?

- **Training-Inference Mismatch**: The RL-Sparse vs. RL-Full comparison hinges on understanding that models trained with full attention may underperform when attention is artificially constrained at inference. *Quick check*: Why might a model trained with access to all tokens struggle when some tokens are evicted during inference?

## Architecture Onboarding

- **Component map**: Global Score Calculator -> Compression Scheduler -> Observation Window -> RL Training Loop (optional) -> Distillation Module (optional)

- **Critical path**:
  1. Compute local attention scores between observation window queries and all cached keys
  2. Normalize by max per head; combine with decayed previous global score using chosen aggregation
  3. Rank tokens by global score, retain top (b-w) plus observation window
  4. Store updated global scores for retained tokens only
  5. (Optional) Record eviction pattern, construct sparse mask, apply RL training

- **Design tradeoffs**:
  - Aggregation form: Max preserves peak attention (best for rare-but-critical tokens); sum accumulates (favors frequently-attended); mean smooths (may dilute signal)
  - Decay factor α: Higher α retains historical importance longer but may delay eviction; lower α adapts faster but risks losing long-term signals
  - Compression interval s: Larger s reduces overhead but delays eviction; smaller s is more responsive but costs more compute
  - Budget b: Lower budgets show largest gains over baselines; higher budgets approach full-KV performance

- **Failure signatures**:
  - Excessive tail bias: Check that F_{t-1} is correctly decayed and combined (not overwritten by S_t only)
  - OOM during training: Ensure sparse attention masks are offloaded to CPU and gradient checkpointing is used
  - No improvement over local-only: Verify observation window w is small relative to budget b
  - Training instability in RL-Sparse: Check that truncated outputs have advantage set to zero

- **First 3 experiments**:
  1. Reproduce AMC-23 baseline with local-score eviction (MorphKV-style) with b=512, w=16, s=128
  2. Ablate global score aggregation: compare max, mean, sum with α ∈ {0.6, 0.8, 0.95} on AMC-23 subset
  3. Test RL-Sparse vs. RL-Full: fine-tune small reasoning model on DeepScaleR subset with both methods

## Open Questions the Paper Calls Out

### Open Question 1: Reward-Verifiable Tasks
The current RL framework "may only be suitable for tasks where the rewards of outputs can be easily verified." The method relies on verifiable rewards to calculate advantages, limiting applicability to reasoning tasks. Demonstrating stable training and performance maintenance on non-reasoning benchmarks using an adapted RL-Sparse loss would resolve this.

### Open Question 2: Semantic Carrier Token Optimization
The paper hypothesizes that tokens like "wait" function as high-density semantic carriers where actual thinking occurs, with subsequent outputs externalizing pre-compressed information. However, the paper does not explore training models to optimize this specific compression behavior. Experiments showing that training objectives encouraging high semantic density in specific tokens lead to lower token retention ratios without accuracy loss would resolve this.

### Open Question 3: Large-Scale Exploration Techniques
The current framework shows rapid entropy reduction during training, which might limit exploration and be exacerbated in larger parameter spaces. The paper suggests integrating advanced techniques to encourage more exploration for large-scale reinforcement learning. Identifying specific regularization or exploration strategies that maintain entropy and improve pass@1 scores during large-scale RL-Sparse training would resolve this.

## Limitations

- Domain generalization gap: Strong performance on reasoning tasks but not evaluated on non-reasoning domains where intermittent attention assumption may not hold
- Ablation gaps in training-free method: Does not systematically vary observation window size w or compression interval s across different sequence lengths
- RL training scalability: Limited details on implementation strategies for sparse attention mask management and memory constraints

## Confidence

- **High Confidence**: The core mechanism of combining local and historical attention scores is well-supported by ablation studies showing consistent improvements across aggregation forms and decay factors
- **Medium Confidence**: The training-inference mismatch hypothesis is supported by RL-Sparse outperforming RL-Full, but requires further validation on diverse model architectures
- **Medium Confidence**: The claim that global scoring preserves prompt context is supported by case studies and token retention distribution analysis, but relies on assumption that reasoning tasks uniformly require backward references

## Next Checks

1. **Cross-Domain Evaluation**: Test G-KV on non-reasoning benchmarks (e.g., summarization, translation, dialogue) to assess whether intermittent attention assumption holds and performance gains persist outside mathematical reasoning tasks

2. **Hyperparameter Sensitivity Analysis**: Systematically vary observation window size w and compression interval s across different sequence lengths (1k, 4k, 16k tokens) to determine optimal configurations and identify breaking points

3. **Training-Free vs. RL Performance Gap**: Compare training-free G-KV with RL-Sparse on a held-out reasoning subset using same model architecture to quantify marginal benefit of post-training adaptation and determine if additional training complexity is justified