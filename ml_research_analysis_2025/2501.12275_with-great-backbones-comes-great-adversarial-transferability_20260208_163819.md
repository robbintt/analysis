---
ver: rpa2
title: With Great Backbones Comes Great Adversarial Transferability
arxiv_id: '2501.12275'
source_url: https://arxiv.org/abs/2501.12275
tags:
- adversarial
- backbone
- attacks
- attack
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies adversarial transferability in models built\
  \ on publicly shared pre-trained backbones, introducing a \"grey-box\" setting that\
  \ reflects realistic partial knowledge about target models. It evaluates over 20,000\
  \ combinations of meta-information\u2014including tuning techniques, backbone families,\
  \ datasets, and attack types\u2014by simulating attacks with proxy models and introducing\
  \ a \"backbone attack\" that exploits only the pre-trained feature extractor."
---

# With Great Backbones Comes Great Adversarial Transferability

## Quick Facts
- **arXiv ID:** 2501.12275
- **Source URL:** https://arxiv.org/abs/2501.12275
- **Reference count:** 40
- **Primary result:** Backbone attacks exploiting publicly shared pre-trained weights achieve transfer success rates matching or exceeding full-knowledge proxy attacks.

## Executive Summary
This paper introduces a "grey-box" adversarial attack setting that reflects realistic partial knowledge about target models, focusing on attacks leveraging publicly shared pre-trained backbones. Through extensive experiments over 20,000 combinations of meta-information (tuning techniques, backbone families, datasets, attack types), the authors demonstrate that access to backbone weights alone enables highly effective adversarial attacks, even without knowing other tuning details. The proposed "backbone attack" maximizes representation-space perturbations and outperforms black-box methods while approaching white-box effectiveness, revealing critical vulnerabilities in current model-sharing practices.

## Method Summary
The paper evaluates adversarial transferability by simulating attacks with proxy models trained under various meta-information combinations. The core contribution is the "backbone attack" that exploits only the pre-trained feature extractor, computing adversarial examples by maximizing cosine distance between clean and adversarial representations in the backbone's feature space. The attack uses PGD optimization with stop-gradient on clean representations. The study tests 21 pre-trained backbones across four downstream datasets, varying tuning modes (frozen vs. full), classification head depths, and attack types. Results compare Transfer Success Rates against white-box and black-box baselines.

## Key Results
- Proxy-based attacks achieve near-white-box effectiveness even with minimal tuning knowledge when the target uses frozen-backbone training.
- Backbone attacks surpass black-box methods and match white-box performance across all tested datasets and tuning configurations.
- Knowledge of pre-trained backbone weights alone provides attack effectiveness equivalent to having all other meta-information.
- Full parameter fine-tuning substantially decreases proxy attack efficiency but backbone attacks remain effective.
- Tuning mode knowledge is most critical; depth (1 vs. 3 layers) is least important for attack success.

## Why This Works (Mechanism)

### Mechanism 1: Representation-Space Perturbation Transfer
Adversarial perturbations computed in the backbone's feature space transfer to downstream models regardless of their fine-tuning configurations. The backbone attack maximizes distance between clean and adversarial representations via cosine similarity loss, and since all downstream models share the same frozen or fine-tuned backbone weights, disrupting backbone-level representations propagates through any classification head.

### Mechanism 2: Meta-Information Impact Hierarchy
Knowledge of tuning mode (frozen vs. full) is the most critical meta-information; tuning depth is least important. Frozen backbones with classification-head-only tuning produce decision boundaries highly dependent on backbone features, while full tuning modifies backbone weights creating divergent decision surfaces.

### Mechanism 3: Weight Access ≈ Full Meta-Information Access
Access to backbone weights alone is functionally equivalent to knowing all other meta-information for attack effectiveness. Backbone weights encode the representation space structure, enabling optimization-based attacks without target-specific knowledge.

## Foundational Learning

- **Adversarial Transferability:** Core phenomenon where perturbations crafted on one model fool another without access to its weights. *Why needed:* Central concept being studied.
- **Projected Gradient Descent (PGD):** Attack optimization method using iterative gradient ascent with constraint projection. *Why needed:* Primary optimization method for backbone attack.
- **Cosine Similarity in Representation Space:** Backbone attack loss function that maximizes angular distance between clean and adversarial embeddings. *Why needed:* Enables representation-space perturbation optimization.

## Architecture Onboarding

- **Component map:** Input image → Backbone ($B$) with weights $W_B$ → Classification head (1-3 layers) → Output logits
- **Critical path:** Load backbone weights → For backbone attack: Run Algorithm 1 (PGD + cosine loss) → Generate adversarial examples → Transfer to target model
- **Design tradeoffs:** Attack budget vs. compute cost; proxy knowledge vs. training requirements; frozen vs. full tuning robustness vs. vulnerability
- **Failure signatures:** Low TSR despite high ASR indicates meta-information mismatch; backbone attack underperformance suggests substantial backbone modification
- **First 3 experiments:**
  1. Implement backbone attack on SwAV ResNet-50; verify TSR exceeds Square attack and approaches white-box PGD on CIFAR-10
  2. Compare TSR for backbone attack vs. proxy attacks across frozen-head and full-tuning targets
  3. Train two model sets from same backbone with identical meta-info but different batch sizes; verify backbone attack matches proxy attack transferability

## Open Questions the Paper Calls Out

### Open Question 1
What specific defensive strategies can mitigate the inherent vulnerabilities in shared pre-trained backbones while maintaining their utility for downstream tasks? The paper highlights the need for stricter practices but proposes no technical solution.

### Open Question 2
Can more sophisticated representation-space optimization methods surpass the efficacy of the "naive" backbone attack introduced in this study? The authors note significant room for optimization beyond their simple cosine distance loss.

### Open Question 3
Why does full parameter fine-tuning substantially decrease the efficiency of proxy attacks compared to frozen-backbone approaches? The paper presents this as an empirical finding without theoretical analysis.

## Limitations
- Backbone attack effectiveness assumes pre-trained weights remain largely unchanged during downstream fine-tuning
- "4× budget" PGD configuration is not explicitly defined, complicating practical assessment
- Experiments focus on classification tasks; transferability claims may not generalize to other domains

## Confidence

- **High:** Backbone attacks significantly outperform black-box methods and approach white-box effectiveness on frozen-tuned models
- **Medium:** Backbone weights alone are equivalent to full meta-information access, depending on assumption about representation stability
- **Low:** Generality across different model architectures and datasets given specific combinations tested

## Next Checks

1. Test backbone attack effectiveness on fully fine-tuned models with large learning rates to determine threshold where representation-space perturbations degrade
2. Reproduce weight vs. meta-information equivalence claim using different backbone architecture (ViT) to verify non-architecture-specific finding
3. Implement backbone attack on non-classification task (object detection) to assess transferability beyond studied domain