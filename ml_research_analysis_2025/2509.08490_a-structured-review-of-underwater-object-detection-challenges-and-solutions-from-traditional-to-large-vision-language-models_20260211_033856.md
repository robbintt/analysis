---
ver: rpa2
title: 'A Structured Review of Underwater Object Detection Challenges and Solutions:
  From Traditional to Large Vision Language Models'
arxiv_id: '2509.08490'
source_url: https://arxiv.org/abs/2509.08490
tags:
- underwater
- detection
- image
- object
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review provides a structured analysis of underwater object
  detection (UOD) challenges and solutions, categorizing them into image quality degradation,
  target-related issues, data-related challenges, computational constraints, and detection
  methodology limitations. It explores progress from traditional image processing
  and object detection techniques to modern deep learning and large vision-language
  models (LVLMs).
---

# A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models

## Quick Facts
- arXiv ID: 2509.08490
- Source URL: https://arxiv.org/abs/2509.08490
- Reference count: 40
- Key outcome: Synthetic data augmentation using LVLMs shows marginal UOD performance gains; LoRA fine-tuning of Florence-2 enables efficient adaptation but suffers from class name hallucinations.

## Executive Summary
This review systematically analyzes underwater object detection (UOD) challenges and solutions, spanning traditional image processing through deep learning to large vision-language models (LVLMs). The authors identify five core challenge categories: image quality degradation, target-related issues, data-related challenges, computational constraints, and detection methodology limitations. Two case studies demonstrate practical approaches: synthetic data generation using DALL-E 3 for dataset augmentation and efficient fine-tuning of Florence-2 LVLM using LoRA for UOD tasks. The YOLO11 model trained on combined synthetic and real datasets achieved mAP@50 of 0.796, while Florence-2 fine-tuned with LoRA showed strong localization capabilities but generated hallucinated class names.

## Method Summary
The study employed two primary case studies. First, synthetic data generation used DALL-E 3 with text-to-image and image-to-image methods to create 1,200 underwater scenes, which were enhanced with OpenCV effects (aesthetic enhancement, color tints, Gaussian blur, haze) to simulate degradation. These images were manually annotated and merged with the RF100 dataset (7,600 images) for YOLO11 training. Second, Florence-2 LVLM was fine-tuned using LoRA on the combined dataset, training only 0.7% of parameters (1.9M) while freezing pre-trained weights. The Florence-2 fine-tuning used AdamW optimizer with learning rate 5e-6 for 20 epochs on an A100 GPU, completing in approximately 5 hours and 42 minutes.

## Key Results
- YOLO11 trained on combined synthetic+real dataset achieved mAP@50 of 0.796 vs 0.793 (real only), with recall improving from 0.714 to 0.736
- Florence-2 fine-tuned with LoRA excelled at bounding box generation for challenging classes but suffered from class name hallucinations
- Current UOD methods remain insufficient for dynamic underwater environments requiring real-time processing
- Synthetic data generation using LVLMs shows potential but requires refinement for realism and natural imperfection capture

## Why This Works (Mechanism)

### Mechanism 1
Synthetic data augmentation using LVLMs (DALL-E 3) may marginally improve UOD model performance when combined with real datasets. Text-to-image generation creates diverse underwater scenes with controlled variations, followed by enhancement effects to simulate realistic degradation. The synthetic images are manually annotated and merged with real datasets to improve generalization. Core assumption: DALL-E 3's pre-training includes sufficient underwater imagery to generate plausible scenes. Evidence: YOLO11 achieved mAP@50 of 0.796 vs 0.793 (original only). Break condition: Generated images lack natural imperfections and manual annotation remains a bottleneck.

### Mechanism 2
LoRA-based parameter-efficient fine-tuning can adapt LVLMs (Florence-2) to UOD tasks with minimal computational overhead while preserving localization capabilities. Low-rank matrices are injected into specific layers, training only ~0.7% of total parameters. Core assumption: Frozen pre-trained weights contain transferable spatial reasoning capabilities. Evidence: Fine-tuning completed in ~5h42m on A100 GPU; model excelled at bounding box generation. Break condition: Hallucination of class names due to limited domain-specific terms in pre-training.

### Mechanism 3
Image enhancement as preprocessing can improve detection performance, but may inadvertently distort features critical for object detection. Techniques like color correction, dehazing, and super-resolution are applied before detection. Core assumption: Enhancement improves human-perceived quality AND machine-extractable features. Evidence: Enhancement may introduce artifacts that degrade object detection performance. Break condition: Enhancement optimized for visual appeal may not align with detection-relevant features.

## Foundational Learning

- **Concept: Object Detection Architectures (One-stage vs. Two-stage vs. Transformer-based)**
  - Why needed here: The paper compares YOLO (one-stage, real-time), Faster R-CNN (two-stage, higher accuracy), and DETR (transformer-based, global context). Understanding tradeoffs is essential for selecting appropriate models.
  - Quick check question: Given an AUV with limited compute requiring real-time detection, which architecture family would you start with?

- **Concept: Vision-Language Model Fine-tuning (PEFT, LoRA, Prompt Tuning)**
  - Why needed here: The Florence-2 case study uses LoRA to adapt a 272M+ parameter model by training only ~1.9M parameters. Understanding PEFT is critical for practical LVLM deployment.
  - Quick check question: If full fine-tuning requires 100GB GPU memory but you only have 24GB, which two PEFT strategies could you consider?

- **Concept: Underwater Image Degradation Physics (Scattering, Absorption, Color Cast)**
  - Why needed here: Degradation types drive enhancement and synthesis strategy choices. Red channel attenuates fastest, causing blue-green casts at depth.
  - Quick check question: Why does red channel correction often precede blue-green dehazing in underwater restoration pipelines?

## Architecture Onboarding

- **Component map:** Raw underwater image → Optional enhancement → Detection backbone (CNN/Transformer) → Neck (FPN/BiFPN) → Head (classification + bounding box) OR Vision encoder + Language decoder (LVLM) with task-specific prompts → LoRA adapters in attention layers

- **Critical path:** 1) Assess dataset sufficiency and class balance → If insufficient, consider synthetic augmentation (DALL-E 3 + enhancement effects) → 2) Select architecture based on deployment constraints → 3) Apply domain-appropriate preprocessing (avoid over-enhancement) → 4) Train/evaluate with appropriate metrics → 5) For LVLMs: Use LoRA for efficient adaptation; implement prompt engineering to reduce hallucination risk

- **Design tradeoffs:** Accuracy vs. Speed (YOLO11: 0.796 mAP@50 with real-time inference vs. two-stage models with higher accuracy but latency costs); Enhancement vs. Feature Preservation (strong enhancement improves visibility but risks removing texture cues); Synthetic Diversity vs. Realism (DALL-E 3 generates clean images; enhancement adds degradation but may not match real distribution complexity)

- **Failure signatures:** Hallucinated class names in LVLM outputs (e.g., "echinullop" instead of "echinus") → Address via prompt engineering, instruction tuning, larger training sets; Catastrophic forgetting after fine-tuning → Consider replay buffers or adapter-only tuning; Small object misses → Add dedicated small-object prediction heads, increase feature pyramid resolution; Domain shift between training and deployment environments → Use domain generalization techniques

- **First 3 experiments:** 1) Baseline YOLO11 on real dataset (RF100): Establish mAP@50, mAP@50-95, recall, precision benchmarks; 2) Synthetic augmentation ablation: Train YOLO11 on real-only vs. real+synthetic (1200 images with varied enhancement effects); compare metrics to quantify augmentation value; 3) LVLM localization test: Fine-tune Florence-2 with LoRA on combined dataset; evaluate bounding box IoU separately from classification accuracy to isolate localization capability from hallucination issues

## Open Questions the Paper Calls Out

### Open Question 1
How can Large Vision Language Models (LVLMs) be optimized to perform real-time underwater object detection on resource-constrained platforms like AUVs? Basis: Authors state LVLMs hold significant promise but real-time application remains under-explored. Why unresolved: LVLMs have high computational demands conflicting with limited processing power on embedded marine systems. What evidence would resolve it: Demonstrated deployment on standard AUV hardware achieving >30 FPS without significant accuracy loss.

### Open Question 2
What techniques can effectively mitigate semantic hallucinations (e.g., misspelled class names) and catastrophic forgetting when fine-tuning LVLMs like Florence-2 for domain-specific UOD tasks? Basis: Florence-2 case study noted class name hallucinations and struggled to generalize to objects outside training dataset. Why unresolved: Current PEFT adapts the model but disrupts text generation grounding and causes forgetting. What evidence would resolve it: A fine-tuned LVLM that generates strictly valid class labels and retains general object detection capabilities, validated through reduction in word error rate and standard mAP metrics.

### Open Question 3
Can hybrid generative architectures (e.g., Diffusion models with VAEs) generate synthetic underwater data with sufficient realism and complexity to significantly reduce reliance on manual annotation? Basis: Authors found DALL-E 3 images lacked natural imperfections and yielded only marginal performance gains. Why unresolved: Current synthetic data often fails to capture complex phenomena like light scattering and turbidity, leading to domain gap that limits training efficacy. What evidence would resolve it: A study showing models trained on automatically annotated hybrid synthetic data perform equivalently or better than those trained on manually annotated real data.

## Limitations
- Limited empirical evidence from case studies rather than comprehensive benchmarking across multiple datasets and model architectures
- Effectiveness of DALL-E 3 for generating realistic underwater scenes remains uncertain, as enhancement effects may not fully capture real degradation complexity
- Florence-2 hallucination issue represents significant reliability concern for practical deployment with limited solutions offered

## Confidence
- **High confidence**: Categorization of UOD challenges and general framework for addressing them through traditional to modern approaches
- **Medium confidence**: Specific performance improvements from synthetic data augmentation and LoRA fine-tuning efficiency claims, given limited ablation studies and single-dataset validation
- **Low confidence**: Practical viability of LVLMs for real-time underwater object detection, given hallucination issues and lack of comprehensive latency/accuracy tradeoff analysis

## Next Checks
1. Evaluate the synthetic augmentation approach and Florence-2 LoRA fine-tuning across multiple underwater datasets (e.g., URPC, UdOS-5) to assess generalizability beyond the RF100 dataset
2. Measure inference latency and memory usage of Florence-2 with LoRA on edge hardware (NVIDIA Jetson Orin, Coral TPU) to validate practical deployment feasibility
3. Implement and test prompt engineering strategies, instruction tuning, or larger domain-specific training sets to quantify improvement in reducing class name hallucinations while maintaining localization accuracy