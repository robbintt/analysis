---
ver: rpa2
title: 'Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions'
arxiv_id: '2507.21285'
source_url: https://arxiv.org/abs/2507.21285
tags:
- clarification
- code
- questions
- user
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces an LLM-based coding assistant that addresses
  the problem of ambiguous or under-specified user prompts by asking clarification
  questions before generating code. The system consists of two key components: an
  intent clarity classifier (fine-tuned DistilBERT) that detects under-specified prompts,
  and a fine-tuned Gemma-3-1B-IT model that generates clarification questions.'
---

# Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions

## Quick Facts
- arXiv ID: 2507.21285
- Source URL: https://arxiv.org/abs/2507.21285
- Authors: Harsh Darji; Thibaud Lutellier
- Reference count: 40
- Key outcome: LLM-based coding assistant that asks clarification questions before generating code, outperforming zero-shot prompting and preferred by users (82% for precision, 78% for contextual fit, 80% for answer faithfulness)

## Executive Summary
This paper introduces a two-stage coding assistant pipeline designed to address ambiguous user prompts by first classifying intent clarity and then generating clarification questions when needed. The system uses a fine-tuned DistilBERT classifier to detect under-specified prompts and a fine-tuned Gemma-3-1B-IT model to generate clarification questions from synthetic training data. User studies demonstrate strong preference for the assistant's outputs over baseline models, with 82-80% of users favoring its precision, contextual fit, and faithfulness. The approach significantly outperforms standard zero-shot prompting for clarification generation, though the system faces challenges with inference latency and reliance on synthetic data.

## Method Summary
The system employs a two-stage pipeline where user prompts first pass through an intent clarity classifier (fine-tuned DistilBERT) that detects under-specified queries on a 4-point scale. If flagged as unclear, a fine-tuned Gemma-3-1B-IT model generates clarification questions using synthetic training data (9,969 prompt-clarification pairs). The user's response enriches the context before code generation. Both models are trained on synthetic data generated by GPT-4o-mini, with the classifier achieving 73% accuracy. The pipeline uses Parameter-Efficient Fine-Tuning (LoRA) and mixed precision for efficient training on limited hardware.

## Key Results
- Fine-tuned clarification model significantly outperforms zero-shot prompting in generating useful questions
- User studies show 82% preference for precision and focus, 78% for contextual fit, and 80% for answer faithfulness
- Intent classifier achieves 73% accuracy in detecting unclear prompts
- Clarification module inference time averages 133 seconds, representing a substantial latency bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A dedicated intent classifier may effectively route ambiguous prompts away from immediate code generation, reducing the likelihood of hallucinated or incorrect solutions.
- **Mechanism:** A DistilBERT model is fine-tuned on synthetic data to classify prompts on a 4-point clarity scale. If the prompt is flagged as under-specified, the system invokes a clarification loop rather than proceeding directly to the answering module.
- **Core assumption:** The classifier's 73% accuracy is sufficient to distinguish actionable prompts from vague ones without excessively disrupting clear requests with unnecessary questions.
- **Evidence anchors:** Mentions a "query classifier trained to detect unclear programming-related queries" in abstract; describes fine-tuned DistilBERT and its role in routing in Section 3.2; PRACTIQ highlights real user questions are often ambiguous.

### Mechanism 2
- **Claim:** Fine-tuning a small LLM (Gemma-3-1B-IT) specifically for clarification question generation yields subjectively better questions than generic zero-shot prompting of larger models.
- **Mechanism:** The model is trained on a synthetic dataset of 9,969 prompt-clarification pairs generated by GPT-4o-mini. This specialization aligns the model's objective with information elicitation rather than immediate completion.
- **Core assumption:** Synthetic data generated by a larger model accurately represents the distribution of real-world ambiguity and the structure of helpful clarification questions.
- **Evidence anchors:** "Evaluation shows that the fine-tuned LLM outperforms standard zero-shot prompting" in abstract; details fine-tuning on synthetic prompt-clarification pairs in Section 3.3; InfoQuest notes LLMs struggle with ambiguous requests.

### Mechanism 3
- **Claim:** Enriching the prompt context via a clarification turn significantly improves the perceived quality and faithfulness of the final generated code.
- **Mechanism:** The system loops user answers back into the context. This enriched context lowers the entropy for the code generation module, allowing it to produce more precise solutions.
- **Core assumption:** Users provide accurate and relevant answers to the clarification questions, and the system can effectively integrate this new information.
- **Evidence anchors:** "Users find the clarification questions generated by our model to outperform the baseline" in abstract; reports 82% preference for precision and 80% for faithfulness in Section 4.5; relies primarily on paper's reported user study.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** Authors fine-tune Gemma-3-1B-IT and DistilBERT on limited hardware without full parameter updates.
  - **Quick check question:** How does LoRA reduce the memory footprint during the fine-tuning of the DistilBERT classifier compared to full fine-tuning?

- **Concept: Synthetic Data Generation**
  - **Why needed here:** System relies entirely on GPT-4o-mini generated data for training both the classifier and the clarification module.
  - **Quick check question:** What are the primary risks of "model collapse" or distribution shift when training a smaller student model on data generated by a larger teacher model?

- **Concept: Ambiguity Detection in NLP**
  - **Why needed here:** First step of the pipeline is classifying intent clarity. Understanding how transformers classify "vagueness" versus "specificity" is key to debugging the router.
  - **Quick check question:** Does the 4-point clarity scale map better to a regression or a classification objective, and how might that affect the loss function?

## Architecture Onboarding

- **Component map:** Input -> Router (DistilBERT Intent Clarity Classifier) -> Branch A (Direct to Answering Module) or Branch B (Clarification Module -> User Response -> Merge Context -> Answering Module) -> Output

- **Critical path:** The *Clarification Module inference*. The paper notes this takes ~133 seconds on average, which is a massive latency bottleneck compared to the classifier (<0.1s).

- **Design tradeoffs:**
  - **Synthetic vs. Real Data:** Chose synthetic data to avoid noise from GitHub/StackOverflow, trading off real-world distribution fidelity for data cleanliness.
  - **Latency vs. Quality:** System adds a potentially multi-turn interaction loop (high latency) to improve code correctness. Unsuitable for real-time autocomplete but fits chat-based assistants.

- **Failure signatures:**
  - **Loop Non-termination:** Classifier fails to recognize the prompt as clear even after clarification, trapping the user in a question loop.
  - **False Negative Routing:** Ambiguous prompt is rated "clear," resulting in generic or hallucinated code without a chance for correction.
  - **Context Window Overflow:** Repeated clarification turns could exceed the context window of the 1B parameter model.

- **First 3 experiments:**
  1. **Classifier Threshold Tuning:** Vary the "clear" vs. "unclear" threshold on the 4-point scale to find optimal balance between asking too many questions (annoyance) and too few (errors).
  2. **Latency Profiling:** Isolate cause of 133s inference time for the Clarification Moduleâ€”is it model size, prompt length, or hardware limitation?
  3. **Data Contamination Check:** Verify synthetic evaluation prompts generated by GPT-4o-mini are not semantically leaking into the training set of the Gemma model.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the intent clarification process result in the generation of more secure code?
  - **Basis in paper:** Explicit statement in Discussion: "Further study could assess whether intent clarification such as the one proposed in this work leads to more secure generated code."
  - **Why unresolved:** Only provides anecdotal evidence where a generated question led to a try-catch block, but performs no rigorous security analysis.
  - **What evidence would resolve it:** Empirical evaluation measuring vulnerability density of code generated with clarification pipeline versus baseline using security benchmark.

- **Open Question 2:** Can integrating cleaned, real-world GitHub data with synthetic data improve the model's generalization?
  - **Basis in paper:** Explicit statement in Limitations: "In the future, we will investigate better cleaning and filtering techniques to integrate this data to our synthetic data to extend the generalization of our dataset."
  - **Why unresolved:** Current study relied exclusively on synthetic data because initial attempts to use real-world GitHub data failed due to excessive noise.
  - **What evidence would resolve it:** Comparative study training models on purely synthetic vs. hybrid datasets and evaluating performance on out-of-distribution, real-world queries.

- **Open Question 3:** How does system performance vary when real users, rather than simulated LLMs, provide clarification responses?
  - **Basis in paper:** Inferred from conclusion validity threat: "clarification responses synthetically generated are likely of a lower quality than real user responses," which may misrepresent real interactions.
  - **Why unresolved:** Evaluation methodology relied on a larger LLM to simulate user responses to control for scalability and participant burden.
  - **What evidence would resolve it:** Live user study where human participants interact directly with the clarification loop, measuring impact of real human variability on final code correctness.

## Limitations

- **Synthetic Data Reliance:** The system relies entirely on GPT-4o-mini generated data, which may not capture the full distribution of real-world ambiguity programmers encounter.
- **Latency Bottleneck:** The clarification module inference time of 133 seconds represents a substantial usability barrier that isn't addressed with optimization strategies.
- **Classifier Accuracy Gap:** The 73% classifier accuracy leaves significant room for error in routing decisions, with no detailed error analysis on which prompt types are most frequently misclassified.

## Confidence

- **High Confidence:** The core observation that fine-tuned models outperform zero-shot prompting for clarification questions is well-supported by the reported user study results (82-80% preference rates).
- **Medium Confidence:** The effectiveness of the intent classifier routing mechanism is supported by reported accuracy but lacks detailed analysis of false positive/negative distributions.
- **Medium Confidence:** The synthetic data generation approach is methodologically sound but untested against real user prompt distributions, creating uncertainty about generalization.

## Next Checks

1. **Error Analysis Validation:** Conduct detailed error analysis on the intent classifier to identify which prompt types are most frequently misclassified, then test whether these errors correlate with user-reported dissatisfaction.

2. **Real Data Generalization Test:** Generate a small validation set of 50-100 actual programmer questions from GitHub issues or Stack Overflow, compare the model's clarification questions against human-annotated gold standard questions, and measure distribution shift from synthetic training data.

3. **Latency Optimization Benchmark:** Profile the clarification module's inference pipeline to identify bottlenecks, then implement and measure the impact of quantization or optimized batching on reducing the 133-second latency to under 30 seconds.