---
ver: rpa2
title: 'Always Tell Me The Odds: Fine-grained Conditional Probability Estimation'
arxiv_id: '2505.01595'
source_url: https://arxiv.org/abs/2505.01595
tags:
- probability
- reasoning
- language
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new approach for fine-grained conditional
  probability estimation in language models, addressing the challenge of making accurate
  probabilistic predictions under uncertainty. The method leverages decoder-based
  regression with synthetic data generation and pairwise ranking supervision to produce
  precise probability estimates.
---

# Always Tell Me The Odds: Fine-grained Conditional Probability Estimation

## Quick Facts
- **arXiv ID:** 2505.01595
- **Source URL:** https://arxiv.org/abs/2505.01595
- **Reference count:** 40
- **Key outcome:** New approach for fine-grained conditional probability estimation using decoder-based regression with synthetic data generation and pairwise ranking supervision shows consistent improvements over existing methods across multiple tasks.

## Executive Summary
This paper addresses the challenge of making accurate probabilistic predictions under uncertainty by presenting a novel method for fine-grained conditional probability estimation in language models. The approach converts continuous probability estimation into discrete token prediction with expectation-based recovery, enabling fine-grained regression from decoder LLMs. Through a combination of human and synthetic data creation, scaling to larger models, and better supervision, the method produces precise probability estimates that improve downstream reasoning performance across multiple benchmarks.

## Method Summary
The method leverages decoder-based regression by discretizing the probability interval [0,1] into N bins represented by special tokens, allowing the model to learn continuous regression through token distribution prediction. Training uses a joint objective combining direct distribution matching (forward KL-divergence) on both human-annotated and synthetic data, plus pairwise ranking consistency loss. Synthetic data is generated through reasoning-augmented prompting from multiple LLMs, with high-agreement samples retained directly and low-agreement samples adjudicated by an LLM judge with confidence-weighted aggregation. The final probability estimates are recovered via expected label scoring from the predicted token distributions.

## Key Results
- Consistent improvements over fine-tuned and prompting-based methods across multiple tasks
- Demonstrates effectiveness on intrinsic alignment, plausibility ranking, and structured reasoning benchmarks
- Shows synthetic data generation with LLM-as-judge confidence weighting enhances domain coverage and handles label ambiguity
- Pairwise ranking supervision improves calibration and decision-relevant ordering

## Why This Works (Mechanism)

### Mechanism 1
Converting continuous probability estimation into discrete token prediction with expectation-based recovery enables fine-grained regression from decoder LLMs. The probability interval [0,1] is discretized into N bins with special tokens, and the model learns to predict token distributions via KL-divergence loss against a quantized Gaussian centered at the ground truth. Fine-grained scalar predictions are recovered by taking the weighted expectation over bin values.

### Mechanism 2
Synthetic data from multiple LLMs with judge-based confidence weighting improves domain coverage and handles label ambiguity. Multiple LLMs generate probability estimates with reasoning chains, high-agreement samples are retained directly, and low-agreement samples undergo LLM-as-judge evaluation where confidence scores are assigned to each reasoning chain. Final synthetic labels are weighted mixtures where confidence scores determine the contribution of each LLM's estimate.

### Mechanism 3
Pairwise ranking consistency loss improves calibration and decision-relevant ordering. For pairs of examples, a margin loss enforces relative ordering constraints without requiring absolute probability accuracy. This supervises the relative ordering of probabilities while complementing direct distribution matching losses.

## Foundational Learning

- **Concept: Probability Calibration**
  - **Why needed here:** The entire method assumes better-calibrated probability estimates improve downstream reasoning. Understanding miscalibration explains why direct prompting fails.
  - **Quick check question:** Can you explain why a model that achieves 80% accuracy might still be poorly calibrated if it assigns 99% confidence to most predictions?

- **Concept: Natural Language Inference (NLI) and Conditional Probability**
  - **Why needed here:** Training data is derived from NLI datasets (UNLI, ANLI, WANLI). Understanding how textual entailment maps to conditional probability P(hypothesis|premise) is foundational.
  - **Quick check question:** Given premise "A man is sipping coffee" and hypothesis "The man is awake," what factors should influence P(hypothesis|premise)?

- **Concept: KL-Divergence for Distribution Matching**
  - **Why needed here:** The training uses forward KL-divergence to match predicted token distributions to quantized Gaussian targets. Understanding mode-covering vs. mode-seeking behavior explains design choices.
  - **Quick check question:** Why might forward KL be preferred over reverse KL for this regression task, given the paper's observation that "theoretical mode-seeking behavior of reverse-KL does not always hold for LLMs"?

## Architecture Onboarding

- **Component map:** Synthetic Data Pipeline (4 LLM annotators → Agreement filtering → LLM judge confidence scoring → Weighted distribution aggregation) → Training Data Mixer (Human annotations + Synthetic pseudo-labels + Pairwise ranking) → Model Core (Decoder-only LLM with LoRA adapters) → Output Layer (Expected label scoring)

- **Critical path:** Data preparation (synthetic annotation) → Distribution quantization → Joint training (LDirect + β1·LDirect,synthetic + β2·LRank) → Expectation-based inference

- **Design tradeoffs:**
  - **Bin count (N ∈ {10, 20, 100}):** More bins → finer granularity but potentially noisier training signal
  - **σ for Gaussian quantization (0.01-0.1):** Smaller σ → sharper targets but less tolerance for annotation noise
  - **β2 for rank loss weight (1-100):** Higher β2 → stronger ordering constraints but potential conflict with point estimation
  - **LoRA vs. full fine-tuning:** LoRA chosen for efficiency; may limit expressiveness for novel task

- **Failure signatures:**
  - **Mode collapse:** Predictions cluster at bin centers or common values (0.5, 0.75) → indicates insufficient σ or limited training diversity
  - **Inverted ranking:** Higher scores for less plausible options → rank loss weight too low or pairwise labels noisy
  - **Poor domain transfer:** Accuracy drops on GNLI/Circa → synthetic data coverage insufficient for target domains
  - **Multimodal token distributions:** High entropy predictions without clear peaks → model uncertain, may need more data or sharper targets

- **First 3 experiments:**
  1. **Ablate synthetic data sources:** Train with human-only (UNLI) vs. human + synthetic (ANLI/WANLI) on held-out EntailmentBank and GNLI. Expected: synthetic data improves cross-domain generalization.
  2. **Vary bin count and σ jointly:** Grid search N ∈ {10, 20, 100} × σ ∈ {0.01, 0.05, 0.1}. Measure Spearman correlation on UNLI validation and calibration error. Expected: optimal around N=20-100, σ=0.05.
  3. **Assess pairwise ranking contribution:** Train with vs. without LRank on comparison tasks (δ-SNLI, HellaSwag, COPA). Expected: rank loss improves accuracy, particularly for plausibility ranking.

## Open Questions the Paper Calls Out

### Open Question 1
Does restricting pairwise comparisons to semantically clustered instances improve the reliability of LLM-based probability estimation compared to random sampling? The current experiment showed pairwise comparison underperformed direct prompting, attributed to the difficulty of comparing unrelated contexts, but the clustered hypothesis remains untested.

### Open Question 2
How does the presence of multi-modal predictive distributions affect the calibration guarantees of rank-consistency training in decoder-based regression? Section 3.3 notes that theoretical results regarding calibration depend on a "single-peaked" predictive distribution, which "does not always hold" for their model.

### Open Question 3
Can integrating explicit logical constraints improve the performance of probability estimation models on structural reasoning frameworks like Maieutic Prompting? Section 5 reports that improvements were "less pronounced" because the framework lacks a "fully probabilistic interpretation," suggesting a mismatch between the probabilistic model and the logical structure.

## Limitations
- Critical limitation: The method assumes predictive distributions are sufficiently sharp for expectation-based recovery, which may not hold for ambiguous or under-specified inputs
- Synthetic data quality concerns: Effectiveness depends critically on the quality and calibration of LLM judges, which is not extensively validated
- Under-specified pairwise loss: The margin loss interaction with point estimation objectives is not fully characterized

## Confidence

- **High Confidence:** Overall framework design (discrete token prediction with expectation-based recovery, synthetic data generation pipeline) and improvement over prompting-based methods
- **Medium Confidence:** Effectiveness of synthetic data generation with LLM-as-judge confidence weighting
- **Medium Confidence:** Pairwise ranking consistency loss contribution
- **Low Confidence:** Method's robustness to highly ambiguous inputs producing multimodal distributions and generalization to truly out-of-distribution tasks

## Next Checks
1. **Out-of-Distribution Robustness Test:** Evaluate the method on conditional probability estimation tasks from completely different domains (e.g., medical diagnosis, financial forecasting) where input-output relationship structure differs significantly from NLI.

2. **Judge Calibration Analysis:** Conduct an ablation study removing the LLM-as-judge confidence weighting and compare performance with random weighting, fixed weighting, and ground-truth confidence on a subset.

3. **Multimodal Distribution Handling:** Design experiments where inputs are constructed to produce ambiguous or multimodal conditional probabilities and analyze whether the model produces high-entropy token distributions in these cases.