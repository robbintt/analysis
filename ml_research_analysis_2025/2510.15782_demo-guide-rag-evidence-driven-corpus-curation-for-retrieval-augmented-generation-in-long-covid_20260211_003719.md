---
ver: rpa2
title: 'Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation
  in Long COVID'
arxiv_id: '2510.15782'
source_url: https://arxiv.org/abs/2510.15782
tags:
- long
- covid
- clinical
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates six corpus configurations for Long COVID clinical
  question answering using retrieval-augmented generation. The Guide-RAG framework
  combines a clinical guideline with high-quality systematic reviews, achieving superior
  overall performance (57.5-65% win rates) compared to single guidelines, reference
  citations, or large-scale literature databases.
---

# Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID

## Quick Facts
- arXiv ID: 2510.15782
- Source URL: https://arxiv.org/abs/2510.15782
- Reference count: 40
- Key outcome: Curated corpus combining clinical guidelines with systematic reviews achieves 57.5-65% win rates over broader literature databases for Long COVID clinical question answering

## Executive Summary
This study evaluates six corpus configurations for Long COVID clinical question answering using retrieval-augmented generation (RAG). The Guide-RAG framework combines a clinical guideline with high-quality systematic reviews, achieving superior overall performance (57.5-65% win rates) compared to single guidelines, reference citations, or large-scale literature databases. Using an LLM-as-a-judge framework across faithfulness, relevance, and comprehensiveness metrics, the curated corpus consistently outperformed broader or unfiltered collections. The study introduces LongCOVID-CQ, a novel dataset of expert-generated clinical questions, demonstrating that expert-curated knowledge bases provide optimal balance for emerging disease information needs.

## Method Summary
The study compares six RAG corpus configurations for Long COVID clinical question answering: no retrieval, single guideline, guideline plus three systematic reviews, guideline references, PubMed literature, and web search. Dense retrieval uses text-embedding-3-small with 1200-character chunks and 600-character overlap, retrieving top-25 results via FAISS. PubMed employs hybrid sparse-dense retrieval with GPT-4o query generation and BM25 + LambdaMART ranking. Generation uses GPT-4o with LangChain. Evaluation employs LLM-as-a-judge pairwise comparisons across faithfulness, relevance, comprehensiveness, and overall metrics using 20 expert-generated clinical questions from the LongCOVID-CQ dataset.

## Key Results
- GS-4 (guideline + 3 systematic reviews) achieved 57.5-65% win rates over other configurations
- Despite using only 4 sources, GS-4 achieved greater comprehensiveness than both R-110 and PM (60% win rate for both)
- Curated corpora consistently outperformed broader collections in faithfulness, relevance, and overall metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curated secondary sources (guidelines + systematic reviews) produce higher-quality RAG outputs than either narrow single-source or broad unfiltered corpora for emerging diseases with evolving evidence.
- Mechanism: Synthesized documents pre-filter and reconcile conflicting primary studies, reducing noise that would otherwise require the retrieval system to distinguish quality in real-time.
- Core assumption: Quality of retrieval corpus mediates output quality more than retrieval breadth alone.
- Evidence anchors: [abstract] "Our RAG corpus configuration combining clinical guidelines with high-quality systematic reviews consistently outperformed both narrow single-guideline approaches and large-scale literature databases"; [section 4] "GS-4 achieved superior overall performance... achieving win rates of 57.5-65% in pairwise comparisons."
- Break condition: If primary literature contains critical emerging evidence not yet synthesized, curated sources will miss it.

### Mechanism 2
- Claim: Synthesized consensus documents produce higher faithfulness than their constituent primary references, even when the primary sources contain the same evidence.
- Mechanism: Guidelines impose consistent terminology and framing that aligns better with retrieved chunks during generation. Primary sources vary in language and focus, creating chunk-retrieval mismatches.
- Core assumption: Faithfulness scores depend on alignment between document language and typical clinical query phrasing.
- Evidence anchors: [section 4] "G-1 achieved a 57.5% win rate over R-110 in faithfulness and overall evaluation" despite R-110 containing the guideline's source references.
- Break condition: If queries require evidence excluded from consensus documents, faithfulness gains may come at the cost of missing relevant information.

### Mechanism 3
- Claim: Small, curated corpora can achieve equal or greater comprehensiveness than large-scale databases when sources are strategically selected for domain coverage.
- Mechanism: Systematic reviews aggregate findings across multiple primary studies, providing breadth in fewer documents. Retrieval from 4 well-chosen sources can match 110+ primary references.
- Core assumption: Comprehensiveness in clinical QA depends on coverage of clinical concepts, not document count.
- Evidence anchors: [section 4] "Despite utilizing only 4 sources, GS-4 achieved greater comprehensiveness than both R-110 and PM (60% win rate for both comparisons)."
- Break condition: If no systematic reviews exist for the target domain, this curation strategy cannot be applied.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) architecture**
  - Why needed here: The entire framework evaluates how corpus selection affects RAG output quality. Without understanding chunk retrieval, embedding, and generation grounding, the comparison between configurations is opaque.
  - Quick check question: Can you explain why retrieved chunks are concatenated with the query before generation, rather than used to fine-tune the model?

- Concept: **Clinical evidence hierarchy (primary studies → systematic reviews → guidelines)**
  - Why needed here: The paper's core claim depends on understanding why synthesized sources outperform primary literature. This hierarchy determines what "high-quality" means in corpus curation.
  - Quick check question: Why would a systematic review be preferred over a randomized controlled trial for clinical decision support?

- Concept: **LLM-as-a-judge evaluation**
  - Why needed here: All performance claims derive from pairwise LLM comparisons, not human evaluation. Understanding prompt design, position bias, and length bias mitigation is critical to interpreting results.
  - Quick check question: What biases must be controlled when using an LLM to compare two responses, and how does randomizing presentation order help?

## Architecture Onboarding

- Component map:
  - **Corpus configurations (6):** NR-0 (no retrieval) → G-1 (single guideline) → GS-4 (guideline + 3 systematic reviews) → R-110 (110 guideline references) → PM (PubMed) → WS (web search)
  - **Dense retrieval pipeline:** PyPDF → chunking (1200 char, 600 overlap) → text-embedding-3-small → FAISS → top-25 retrieval → GPT-4o generation
  - **Hybrid sparse-dense (PubMed only):** Query generation → PubMed BM25+LambdaMART → full-text retrieval → dense re-ranking
  - **Evaluation layer:** GPT-4o pairwise comparison across 4 metrics (faithfulness, relevance, comprehensiveness, overall)

- Critical path:
  1. Document preprocessing and chunking quality determines retrieval fidelity
  2. Corpus selection determines evidence quality ceiling
  3. LLM-judge prompt design determines metric validity
  4. Ground truth dataset (LongCOVID-CQ) determines clinical relevance

- Design tradeoffs:
  - **Corpus size vs. noise:** Larger corpora (PM, R-110) provide more coverage but introduce conflicting or low-quality evidence that degrades faithfulness
  - **Synthesis currency vs. stability:** Systematic reviews lag behind primary literature; guidelines lag further. For emerging diseases, this may miss recent findings
  - **Evaluation automation vs. validity:** LLM-as-a-judge scales but lacks human validation (noted as limitation in Section 6)

- Failure signatures:
  - **Over-specificity from broad corpora:** R-110 responses attributed autonomic dysfunction to "vagus nerve imbalance" and recommended "stellate ganglion block"—interventions not supported by consensus guidance
  - **Exercise recommendations from PM:** PubMed corpus endorsed exercise training extrapolated from other diseases, which LC experts caution against
  - **Missing retrieval for NR-0:** Cannot evaluate faithfulness without retrieved context chunks

- First 3 experiments:
  1. **Reproduce GS-4 vs. PM comparison** on LongCOVID-CQ questions. Verify that win rates fall within reported ranges (57.5-65%) using the same evaluation prompts from Appendix A.
  2. **Ablate systematic reviews one at a time** from GS-4 to measure contribution of each review to comprehensiveness scores. This tests whether all three reviews are necessary or if one dominates.
  3. **Add human expert evaluation** on a subset (e.g., 5 questions) to validate LLM-judge alignment. The paper notes this as a limitation; even small-scale human ratings would strengthen claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Guide-RAG corpus curation approach (combining clinical guidelines with systematic reviews) generalize to other emerging or high-uncertainty diseases beyond Long COVID?
- Basis in paper: [explicit] "Future work should... extend testing to additional high-complexity and high-uncertainty medical domains."
- Why unresolved: The study evaluated only Long COVID, a single clinical domain with specific characteristics (200+ symptoms, heterogeneous presentation, consensus-based guidance).
- What evidence would resolve it: Replication studies applying the GS-4 configuration to other emerging diseases, demonstrating comparable win rates (57.5-65%) across faithfulness, relevance, and comprehensiveness metrics.

### Open Question 2
- Question: How well does LLM-as-a-judge evaluation align with human clinical expert preferences for medical RAG system outputs?
- Basis in paper: [explicit] The study was limited by "the use of a single LLM model (GPT-4o) as the evaluation judge without validation against human or model-diverse raters."
- Why unresolved: No human expert ratings were collected to validate whether GPT-4o's pairwise judgments correlate with clinician assessments of faithfulness, relevance, and comprehensiveness.
- What evidence would resolve it: Comparative evaluation where human clinicians rate the same response pairs, with correlation analysis between LLM and human judgments.

### Open Question 3
- Question: How do retrieval hyperparameters (chunk size, embedding model selection, reranking thresholds) affect performance across corpus configurations of varying sizes?
- Basis in paper: [explicit] "The study also did not include systematic exploration of retrieval hyperparameters, including chunk size, embedding model selection, and reranking thresholds."
- Why unresolved: Only one configuration was tested (1200-character chunks, 600-character overlap, text-embedding-3-small).
- What evidence would resolve it: Ablation studies systematically varying each hyperparameter while measuring impact on win rates across G-1, GS-4, R-110, and PM configurations.

## Limitations
- Reliance on LLM-as-a-judge evaluation without human validation, acknowledged as a key limitation
- Corpus curation strategy assumes availability of high-quality systematic reviews, which may not generalize to other disease domains
- Results limited to a single disease domain (Long COVID) with expert-generated questions rather than real-world clinical queries

## Confidence

- **High Confidence**: The finding that curated corpora outperform broad unfiltered collections in RAG contexts is well-supported by systematic pairwise comparisons across multiple metrics.
- **Medium Confidence**: Claims about GS-4 achieving superior comprehensiveness with fewer documents are credible but depend on the specific systematic reviews selected.
- **Low Confidence**: The transferability of this curation strategy to other emerging diseases without high-quality systematic reviews remains unproven.

## Next Checks
1. **Human Expert Validation**: Conduct blinded expert evaluation on a subset (5-10 questions) to verify LLM-judge alignment and establish ground truth reliability.
2. **Cross-Disease Replication**: Apply the same 6 corpus configurations to a different emerging disease domain to test generalizability of the curated-corpus advantage.
3. **Dynamic Evidence Testing**: Simulate a treatment breakthrough by adding recent primary literature to each corpus and measure how quickly each configuration incorporates the new evidence into responses.