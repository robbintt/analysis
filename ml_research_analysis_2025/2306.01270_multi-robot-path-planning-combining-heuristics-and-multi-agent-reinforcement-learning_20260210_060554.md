---
ver: rpa2
title: Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement
  Learning
arxiv_id: '2306.01270'
source_url: https://arxiv.org/abs/2306.01270
tags:
- path
- planning
- learning
- algorithm
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multi-robot path planning in
  dynamic environments where robots must avoid collisions with other moving robots
  while minimizing travel distance. The authors propose a novel method called MAPPOHR
  that combines heuristic search, empirical rules, and multi-agent reinforcement learning
  (MARL).
---

# Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.01270
- Source URL: https://arxiv.org/abs/2306.01270
- Authors: Shaoming Peng
- Reference count: 32
- Primary result: Novel two-layer MAPPOHR method achieves lower total moving costs than pure learning or pure heuristic methods in multi-robot path planning

## Executive Summary
This paper addresses multi-robot path planning in dynamic environments where robots must avoid collisions while minimizing travel distance. The authors propose MAPPOHR, a novel method combining heuristic search, empirical rules, and multi-agent reinforcement learning. The approach uses a two-layer architecture: a real-time planner based on MAPPO with embedded rules, and a heuristic search planner (D* or A*) providing global path guidance. Experiments in airport scenarios with two moving planes show MAPPOHR achieves superior planning performance compared to existing learning and heuristic methods.

## Method Summary
The method consists of two layers: a real-time planner based on the MAPPO algorithm with embedded empirical rules and a heuristic search planner (D* or A*) that provides global path guidance. During movement, the heuristic planner replans paths based on real-time planner instructions. The approach combines centralized training with distributed execution, where the critic network receives all agents' observations plus agent-specific features during training, while actors share parameters to improve training efficiency. Domain-specific heuristic rules are embedded in both the action output layer and reward function to constrain the exploration space and provide shaped rewards.

## Key Results
- MAPPOHR achieved lower total moving costs (mean 37.3) compared to D* Lite (mean 50.9) by effectively combining waiting and replanning strategies
- The method showed higher learning efficiency than pure learning methods due to integration of heuristics and prior rule knowledge
- Success rate was 0.9, slightly lower than D* Lite's 1.0 due to time constraints on replanning
- MAPPOHR rewards were significantly higher than MAPPO and PPOHR in ablation studies, demonstrating the benefit of heuristic integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing path planning into global guidance and local decision-making reduces the RL action space and accelerates learning convergence.
- Mechanism: The heuristic planner (D* Lite) provides a dense, goal-directed baseline path. The real-time planner only learns when to deviate (wait/back/replan) rather than learning full navigation from scratch.
- Core assumption: The global path remains mostly valid; only local adjustments are needed for dynamic obstacles.
- Evidence anchors: [abstract] "The method consists of two layers: a real-time planner based on... MAPPO... and a heuristic search planner used to create a global guiding path." [section 4.1] "The real-time planner treats the heuristic planner as an action and decides whether to use it for re-planning based on its observations."
- Break condition: Highly dynamic environments where global paths become obsolete faster than replanning can occur.

### Mechanism 2
- Claim: Embedding domain-specific heuristic rules into the action output layer and reward function improves sample efficiency and prevents invalid action exploration.
- Mechanism: Rules such as "forbid move when collision risk exists" or "penalize simultaneous wait by all agents" constrain the action probability distribution before sampling.
- Core assumption: The encoded rules are correct and sufficient for the target domain.
- Evidence anchors: [abstract] "MAPPO, which embeds empirical rules in the action output layer and reward functions." [section 4.4] Lists six heuristic rules for action masking and five reward-shaping rules.
- Break condition: Novel environments where domain rules are incorrect or incomplete.

### Mechanism 3
- Claim: Centralized training with distributed execution (CTDE) with shared actor parameters addresses environment non-stationarity in multi-agent settings.
- Mechanism: During training, critic networks access all agents' observations + agent-specific features. Actors share parameters, reducing total parameters and enabling knowledge transfer.
- Core assumption: Agents are homogeneous in capability.
- Evidence anchors: [section 4.3] "To improve the efficiency of training, the actor of each agent in MAPPO shares the network parameters." [section 6.2] MAPPOHR rewards higher than PPOHR.
- Break condition: Heterogeneous robot teams with significantly different kinematics or task roles.

## Foundational Learning

- Concept: **Partial Observability and POMDPs**
  - Why needed here: Each robot observes only local environment; must reason under uncertainty about other agents' states and intentions.
  - Quick check question: Can you explain why adding an RNN layer helps with partially observable environments?

- Concept: **Centralized Training, Distributed Execution (CTDE)**
  - Why needed here: Core architecture pattern for MARL; understanding what information is available during training vs. deployment is critical.
  - Quick check question: What additional information does the critic network receive that the actor network does not?

- Concept: **Heuristic Search (D* Lite)**
  - Why needed here: Provides global guidance path; understanding incremental replanning helps debug when/why the system triggers replan actions.
  - Quick check question: How does D* Lite differ from A* in handling dynamic obstacle updates?

## Architecture Onboarding

- Component map: Environment Model -> Heuristic Planner (D* Lite) -> Real-Time Planner (MAPPO) -> Rule Engine -> Action Selection
- Critical path:
  1. Environment generates global guide paths via D* Lite for all agents
  2. At each timestep, each agent's actor network receives local observation vector
  3. Rule engine masks invalid actions based on domain heuristics
  4. Actor samples from remaining action distribution
  5. If Replan selected, local D* Lite computes new path considering current dynamic obstacles
  6. Reward computed; if collision detected, heavy penalty; goal reached, bonus
- Design tradeoffs:
  - **Waiting vs. Replanning**: MAPPOHR accepts higher waiting cost to reduce planning cost
  - **Success rate vs. Cost**: 90% success vs. D* Lite's 100% but significantly lower total cost
  - **Shared vs. Independent Actors**: Shared parameters reduce training time but assume agent homogeneity
- Failure signatures:
  - **Case 0 failure (MAPPOHR)**: Timeout during replanning; 3-second limit exceeded
  - **D* Lite pathologies**: Frequent replanning in dense conflict scenarios → long detours
  - **Rule mismatch**: If embedded rules don't match environment dynamics, learning plateaus
- First 3 experiments:
  1. **Sanity check**: Run D* Lite alone on all 10 test cases; reproduce Table 1 metrics
  2. **Ablation ladder**: Train MAPPO → MAPPOH → MAPPOHR sequentially; confirm reward curve improvements
  3. **Single scenario deep-dive**: Pick Case 3 (highest conflict); visualize action sequences and replan triggers

## Open Questions the Paper Calls Out

- **Scalability to more agents**: The paper only tested with two planes moving simultaneously. Does MAPPOHR maintain efficiency and success rate when scaled to environments with significantly more agents (e.g., >10 robots)?
- **Computational time constraints**: The method failed Case 0 where D* Lite succeeded due to replanning time costs. Can the method be adapted to guarantee success in scenarios where computational time for replanning exceeds the fixed threshold?
- **Generalization to different environments**: The reliance on handcrafted empirical rules may limit the algorithm's ability to generalize to fundamentally different environments or robot types. To what extent does this reliance constrain adaptability?

## Limitations
- The observation vector dimension (71) and exact encoding method remain underspecified, potentially affecting reproducibility
- Training was conducted on only 10 test cases with 2 robots, limiting generalizability to larger multi-robot systems
- Success rate of 0.9 (vs D* Lite's 1.0) stems from replanning timeout constraints, suggesting scalability issues
- The shared actor parameter assumption may not hold for heterogeneous robot teams with different capabilities

## Confidence
- **Mechanism 1 (Global guidance + local intervention)**: Medium confidence - Supported by architecture description and learning efficiency improvements, but lacks direct ablation evidence isolating the decomposition effect
- **Mechanism 2 (Rule embedding)**: High confidence - Strong ablation evidence shows MAPPOHR outperforms both MAPPO and MAPPOH, and reward curves demonstrate clear improvement with heuristic integration
- **Mechanism 3 (CTDE with shared actors)**: Medium confidence - Training efficiency gains are documented, but the assumption of agent homogeneity requires validation in heterogeneous settings

## Next Checks
1. **Architecture ablation study**: Systematically remove each mechanism (global guidance, rule embedding, CTDE) to quantify individual contributions to the performance gains observed in Table 1
2. **Scalability testing**: Extend experiments to scenarios with 4+ robots and more complex airport layouts to evaluate whether MAPPOHR maintains its advantage over pure heuristic methods as system complexity increases
3. **Rule robustness analysis**: Systematically relax or modify the embedded heuristic rules to determine their necessity and identify scenarios where rule constraints may limit optimal behavior