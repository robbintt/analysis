---
ver: rpa2
title: Exploratory Causal Inference in SAEnce
arxiv_id: '2510.14073'
source_url: https://arxiv.org/abs/2510.14073
tags:
- effect
- causal
- precision
- inference
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Neural Effect Search (NES), a method for discovering
  treatment effects in exploratory experiments where the outcomes of interest are
  unknown. The core idea is to leverage foundation models and sparse autoencoders
  to transform high-dimensional data into interpretable neural representations, then
  recursively test for significant treatment effects while controlling for entanglement
  between neurons.
---

# Exploratory Causal Inference in SAEnce

## Quick Facts
- arXiv ID: 2510.14073
- Source URL: https://arxiv.org/abs/2510.14073
- Reference count: 40
- Key outcome: NES discovers treatment effects in exploratory RCTs with high precision (0.6-1.0) and recall (0.8-1.0), outperforming standard multiple testing corrections by avoiding spurious discoveries due to neural entanglement.

## Executive Summary
This paper introduces Neural Effect Search (NES), a method for discovering statistically significant treatment effects in exploratory randomized controlled trials where outcome variables are unknown. The core innovation is using foundation models and sparse autoencoders to transform high-dimensional data into interpretable neural representations, then recursively testing for treatment effects while controlling for entanglement between neurons. NES addresses the "paradox of exploratory causal inference" where standard multiple testing methods flag all outcome-entangled neurons as significant, hampering interpretation. The method achieves high precision and recall in semi-synthetic experiments and successfully identifies meaningful treatment effects in a real-world ecological trial.

## Method Summary
NES operates by first extracting features from raw observations using foundation models like SigLIP or DINOv2, then training sparse autoencoders with Top-k activation to obtain interpretable neural representations. The core NES algorithm recursively tests neuron-level Average Treatment Effects (ATE) using Bonferroni correction, selects the strongest significant neuron, and adjusts subsequent tests by residualizing or stratifying on previously selected neurons. This recursive procedure continues until no significant neurons remain, effectively controlling for neural entanglement while maintaining statistical power to discover true treatment effects.

## Key Results
- NES achieves precision of 0.6-1.0 and recall of 0.8-1.0 across varying sample sizes and effect magnitudes in semi-synthetic experiments
- Outperforms Bonferroni and FDR corrections, which suffer from low precision (<0.5) in high-power regimes
- Successfully identifies two significant neurons in real-world ecological trial corresponding to grooming behavior and experimental design bias
- Demonstrates ability to discover treatment effects while avoiding spurious discoveries due to neural entanglement

## Why This Works (Mechanism)
NES works by leveraging the sparse, interpretable representations from SAEs to decompose high-dimensional data into monosemantic neurons, then systematically testing for treatment effects while controlling for entanglement. The recursive adjustment procedure ensures that once a neuron is selected, its influence is removed from subsequent tests, preventing double-counting of effects and maintaining statistical power.

## Foundational Learning
- **Sparse Autoencoders (SAEs):** Neural networks trained to compress and reconstruct data with sparse activations, creating interpretable features. Needed because foundation model embeddings are dense and uninterpretable. Quick check: Train SAE on CelebA embeddings and visualize top-k activated neurons.
- **Top-k Non-linearity:** Activation function that selects k largest values per token and zeros others. Needed to enforce sparsity and create monosemantic features. Quick check: Compare SAE performance with different k values (5 vs 20).
- **Recursive Multiple Testing:** Iterative procedure that tests hypotheses while adjusting for previously selected variables. Needed to control family-wise error rate while maintaining power. Quick check: Implement NES recursion on synthetic data with known effects.
- **Neural Entanglement:** Phenomenon where multiple neurons capture overlapping information about outcomes. Needed to understand why standard multiple testing fails. Quick check: Measure neuron correlation with outcomes in SAE.

## Architecture Onboarding
**Component Map:** Foundation Model -> SAE -> NES Algorithm -> Significant Neurons
**Critical Path:** X (raw data) → Foundation Model → H (embeddings) → SAE → Z (sparse codes) → NES → Significant Neurons
**Design Tradeoffs:** Higher SAE sparsity improves interpretability but may miss distributed effects; recursive adjustment trades computational complexity for statistical power.
**Failure Signatures:** Low precision indicates over-identification of spurious effects; low recall suggests missed true effects; unstable recursion indicates poor SAE quality.
**First Experiments:** 1) Train SAE on CelebA embeddings with Top-k activation; 2) Implement NES recursion on semi-synthetic data; 3) Compare NES vs Bonferroni on entangled neuron scenarios.

## Open Questions the Paper Calls Out
- How can NES be adapted to effectively handle continuous outcome concepts?
- Can NES identify causal effects when the outcome information is distributed across multi-modal measurements?
- To what extent do SAE features reflect true underlying causal mechanisms versus spurious post-hoc artifacts?

## Limitations
- Method's effectiveness depends critically on SAE alignment with ground-truth concepts, which may not hold in real-world data
- Recursive adjustment procedure can be unstable with small sample sizes or high-dimensional residuals
- Current formulation assumes localized treatment effects rather than distributed effects across multiple neurons
- Evaluation is limited to semi-synthetic experiments and single real-world trial, requiring broader validation

## Confidence
- **High Confidence:** NES algorithm is clearly defined and implementable; semi-synthetic experimental results are directly measurable
- **Medium Confidence:** SAE training details and NES steps are sufficiently specified with minor ambiguities requiring reasonable defaults
- **Low Confidence:** Robustness under SAE misalignment and scalability to very large SAEs are not thoroughly explored

## Next Checks
1. Test NES on datasets where SAEs are intentionally mis-trained to quantify sensitivity to alignment assumptions
2. Design semi-synthetic experiment where treatment effects are distributed across multiple neurons to evaluate NES's ability to capture complex effect structures
3. Validate NES on datasets with varying small sample sizes (n < 50) to assess stability and identify failure modes in low-data regimes