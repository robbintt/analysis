---
ver: rpa2
title: 'SLM-SQL: An Exploration of Small Language Models for Text-to-SQL'
arxiv_id: '2507.22478'
source_url: https://arxiv.org/abs/2507.22478
tags:
- arxiv
- text-to-sql
- preprint
- table
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the potential of small language models (SLMs)
  with 0.5B-1.5B parameters for text-to-SQL tasks, where they typically underperform
  due to limited reasoning capabilities. To address this, the authors construct two
  datasets from SynSQL-2.5M: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K
  for SQL revision.'
---

# SLM-SQL: An Exploration of Small Language Models for Text-to-SQL

## Quick Facts
- arXiv ID: 2507.22478
- Source URL: https://arxiv.org/abs/2507.22478
- Reference count: 40
- Key outcome: Small language models (0.5B-1.5B parameters) achieve 56.87-67.08% execution accuracy on BIRD dataset after SFT+RL training and corrective self-consistency inference

## Executive Summary
This paper investigates the capability of small language models (SLMs) with 0.5B-1.5B parameters for text-to-SQL tasks, where they typically underperform due to limited reasoning capabilities. The authors construct two synthetic datasets from SynSQL-2.5M and apply supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance SQL generation and revision abilities. Their corrective self-consistency inference approach significantly improves performance, with the 0.5B model achieving 56.87% execution accuracy on BIRD and demonstrating strong generalization to the Spider dataset.

## Method Summary
The approach involves constructing two synthetic datasets from SynSQL-2.5M: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL revision. The models undergo supervised fine-tuning using these datasets, followed by reinforcement learning with a binary execution accuracy reward. A corrective self-consistency inference mechanism samples multiple SQL outputs, revises them, and selects the best-performing query. The method focuses exclusively on SLM enhancement without exploring schema linking, agent-based frameworks, or pipeline-based methods.

## Key Results
- On BIRD dataset: 0.5B model achieves 56.87% execution accuracy (EX), 1.5B model achieves 67.08% EX
- Average improvement of 31.4 points across five evaluated models
- Strong generalization demonstrated on Spider dataset
- Models show significant improvement over baseline SLM performance on text-to-SQL tasks

## Why This Works (Mechanism)
The method works by addressing the core limitation of small language models in complex reasoning tasks through a multi-stage training approach. Supervised fine-tuning on synthetic SQL data provides foundational SQL generation capabilities, while reinforcement learning with execution-based rewards aligns model outputs with actual database query performance. The corrective self-consistency inference mechanism compensates for the limited reasoning capacity of small models by leveraging multiple sampling and revision passes to arrive at correct SQL queries.

## Foundational Learning

**Text-to-SQL Task**: Converting natural language questions into executable SQL queries. Needed to establish the problem domain and evaluation metrics. Quick check: Understanding of SQL syntax and semantic parsing requirements.

**Synthetic Data Generation**: Creating training data programmatically from existing SQL datasets. Needed to scale training data availability for small models. Quick check: Familiarity with template-based SQL generation and data augmentation techniques.

**Supervised Fine-Tuning (SFT)**: Adapting pre-trained models to specific downstream tasks using labeled data. Needed to establish baseline SQL generation capabilities. Quick check: Understanding of loss functions and optimization for sequence generation tasks.

**Reinforcement Learning for Text Generation**: Using environment feedback (execution accuracy) to guide model updates. Needed to optimize for actual SQL query performance rather than just syntactic correctness. Quick check: Knowledge of reward shaping and policy gradient methods.

**Self-Consistency Inference**: Generating multiple outputs and selecting the most consistent or highest-performing result. Needed to compensate for limited reasoning in small models. Quick check: Understanding of ensemble methods and inference-time computation trade-offs.

## Architecture Onboarding

**Component Map**: Natural Language Query -> SQL Generator (SFT-trained) -> SQL Reviser (RL-trained) -> Execution Engine -> Reward Feedback -> Corrective Self-Consistency

**Critical Path**: Input query → multiple SQL generations → execution and scoring → best query selection → final output

**Design Tradeoffs**: The approach prioritizes computational efficiency and model size over absolute performance, trading off the superior reasoning capabilities of large models for deployment practicality. The synthetic data generation approach balances data diversity against potential distribution mismatch with real-world queries.

**Failure Signatures**: Models may struggle with complex multi-table joins, nested queries, or queries requiring deep schema understanding. The binary reward function may limit learning from partially correct attempts, and the corrective self-consistency mechanism increases inference-time computation.

**First 3 Experiments**:
1. Baseline evaluation of SLM performance on BIRD without any fine-tuning
2. Ablation study isolating the impact of SFT vs RL vs corrective self-consistency components
3. Generalization test on Spider dataset to assess cross-dataset performance

## Open Questions the Paper Calls Out

**Open Question 1**: How would integrating advanced approaches like schema linking, agent-based frameworks, or pipeline-based methods affect SLM-SQL performance compared to the current Corrective Self-Consistency approach?
Basis: The authors state they focused "exclusively on evaluating... without exploring other more advanced approaches such as schema linking, agent-based frameworks, or pipeline-based methods."
Evidence: Experiments combining SLM-SQL with schema linking techniques and comparing accuracy against current baseline on BIRD dataset.

**Open Question 2**: Can the supervised fine-tuning and reinforcement learning pipeline designed for Text-to-SQL be generalized effectively to broader code generation tasks?
Basis: The authors note their study is limited to Text-to-SQL domain and that "extending this research to broader code generation tasks represents an important future direction."
Evidence: Applying SLM-SQL training methodology to general coding benchmarks (e.g., HumanEval) and measuring pass rates.

**Open Question 3**: Does the reliance on a binary execution accuracy reward during reinforcement learning limit the model's ability to refine partially correct SQL reasoning?
Basis: The paper utilizes a simple reward function composed of binary execution accuracy and format matching, providing no gradient for semantically close but incorrect outputs.
Evidence: An ablation study comparing current binary reward against continuous reward function (e.g., based on test case coverage) during RL phase.

## Limitations

- Performance gains primarily measured on benchmark datasets, may not represent real-world complexity
- No detailed analysis of error cases or failure modes across different query types
- Dataset construction using synthetic data raises questions about training data quality and diversity
- Evaluation focuses on execution accuracy without comprehensive analysis of SQL correctness or security considerations

## Confidence

- **High confidence**: Experimental methodology and dataset construction process are clearly described
- **Medium confidence**: Reported performance improvements on BIRD and Spider benchmarks
- **Low confidence**: Generalization to real-world applications and handling of complex SQL queries

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (SFT, RL, corrective self-consistency) to reported performance gains
2. Test models on additional real-world databases with varying schema complexities to assess practical applicability
3. Evaluate model outputs for potential SQL injection vulnerabilities and other security concerns in generated queries