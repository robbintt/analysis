---
ver: rpa2
title: A Feasibility Experiment on the Application of Predictive Coding to Instant
  Messaging Corpora
arxiv_id: '2508.11084'
source_url: https://arxiv.org/abs/2508.11084
tags:
- text
- training
- chat
- instant
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of applying predictive coding
  to instant messaging data, which presents unique challenges due to its informal
  nature, small message sizes, and high dimensionality from abbreviations and numbers.
  The authors propose a solution that first groups individual messages into "day chat"
  documents by conversation and time, making manual review more feasible.
---

# A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora

## Quick Facts
- arXiv ID: 2508.11084
- Source URL: https://arxiv.org/abs/2508.11084
- Authors: Thanasis Schoinas; Ghulam Qadir
- Reference count: 8
- Key outcome: Applying predictive coding to instant messaging data by grouping messages into day chats and using numerical token tagging improves precision at 80% recall from 60.7% to 65%

## Executive Summary
This paper addresses the challenge of applying predictive coding to instant messaging corpora for legal document review. The authors propose a solution that groups individual chat messages into "day chat" documents by conversation and time, making manual review economically feasible. They then implement feature selection and logistic regression classification, with a focus on reducing dimensionality by replacing numeric tokens with "[NUM]" and "[TIMESTAMP]" tags. This preprocessing step significantly improves model performance, increasing precision at 80% recall from 60.7% to 65% on a dataset of 5,000 training documents.

## Method Summary
The approach aggregates raw chat messages into day-level documents grouped by conversation ID and calendar date, with deduplication across participants' logs. Text normalization removes participant join/leave messages, silence indicators, and "Participant X Says" phrases. Numerical token tagging replaces consecutive digit sequences with "[NUM]" and timestamp patterns with "[TIMESTAMP]". Feature selection uses Information Gain ranking to keep the top 20,000 tokens, which are then used to train a logistic regression classifier on normalized token frequencies. The model is evaluated on held-out validation sets with precision at 80% recall as the primary metric.

## Key Results
- Grouping messages into day-level documents enables economically feasible predictive coding for instant messaging
- Numerical token tagging (replacing numbers with [NUM] and timestamps with [TIMESTAMP]) improves precision at 80% recall from 60.72% to 65.03%
- Information gain-based feature selection with top 20,000 tokens effectively reduces noise from high-dimensional sparse chat vocabularies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping individual chat lines into time-bounded "day chat" documents makes predictive coding economically feasible for instant messaging corpora.
- Mechanism: Individual chat lines are too granular for meaningful responsiveness coding. By consolidating messages by conversation ID and calendar day, the system creates document units large enough to contain contextual signal while remaining reviewable by humans. This addresses the "fine granularity challenge" where conversations span topics and participants dynamically join/leave.
- Core assumption: Instant communication about a non-casual topic is highly likely to conclude within the day it was initiated.
- Evidence anchors:
  - [abstract]: "we exploit a data management workflow to group messages into day chats... to provide an economically feasible predictive coding solution"
  - [section]: "grouping messages by conversation and time, with day chat collection documents being a realistic option, on the assumption that instant communication about a topic - especially a non-casual one - is highly likely to conclude within the day it was initiated"
  - [corpus]: Limited corpus evidence on temporal granularity optimization for chat classification; related work focuses on author identification using character-segment approaches rather than time-based grouping.
- Break condition: When conversations span multiple days with distinct topics per day, or when single-day chats contain multiple unrelated topics, grouping by day may conflate or fragment relevant discussions.

### Mechanism 2
- Claim: Replacing numeric tokens with standardized semantic tags improves classification precision by reducing dimensionality and preventing overfitting to noise features.
- Mechanism: Instant Bloomberg chats contain many numeric tokens (monetary amounts, rates, timestamps). These inflate vocabulary size and receive spuriously high logistic regression coefficients due to low frequency rather than true predictive signal. Replacing all digit sequences with "[NUM]" and timestamps with "[TIMESTAMP]" collapses many sparse features into denser semantic indicators, improving the signal-to-noise ratio.
- Core assumption: Reviewers care that a number exists in a message (the semantic category) but not its specific value.
- Evidence anchors:
  - [abstract]: "We also improve the solution's baseline model performance by dimensionality reduction, with focus on quantitative features"
  - [section]: TABLE I shows precision @80% recall improved from 60.72% (untagged) to 65.03% (single [NUM] tag) for Raw 5k models; Fig. 5 confirms that tagged text and complete numeric elimination yield nearly identical performance, suggesting the gain comes from dimensionality reduction rather than tag predictive power.
  - [corpus]: No direct corpus evidence for numerical token standardization in legal document review; this appears novel to this domain.
- Break condition: When specific numeric values (e.g., threshold amounts above $10,000, specific dates) are legally determinative for responsiveness decisions.

### Mechanism 3
- Claim: Information gain-based feature ranking prior to logistic regression training reduces noise from high-dimensional sparse chat vocabularies.
- Mechanism: IM text contains many abbreviations, misspellings, and domain-specific tokens that create a large, sparse feature space. Information gain assessment ranks tokens by discriminative value, and only the top N (20,000 in this study) are retained for model training. This filters out features that contribute variance without signal.
- Core assumption: Lower-ranked features are noise rather than rare-but-predictive signal; the optimal feature count can be determined empirically.
- Evidence anchors:
  - [section]: "Prior to training the model with LR, an information gain assessment was performed, based on which the tokens are ranked, so the 'top N' can be subsequently selected for model training"
  - [section]: "substantial number of numeric tokens were being assigned high logistic regression coefficients, indicating that the model might be overfitting by learning from potentially noise features"
  - [corpus]: Weak corpus evidence; no comparative study of feature selection methods for IM classification identified.
- Break condition: When predictive features are rare tokens (e.g., specific code words) that fall below the ranking cutoff.

## Foundational Learning

- **Concept: Precision-Recall Trade-off in Binary Classification**
  - Why needed here: The paper reports performance as "precision @80% recall" rather than accuracy because legal review prioritizes finding responsive documents (recall) while minimizing non-responsive documents to review (precision). Understanding this trade-off is essential for interpreting the claimed 4.31% precision improvement as economically meaningful.
  - Quick check question: If a model achieves 80% recall with 65% precision on a corpus of 100,000 documents with 16% responsiveness rate, how many documents must be reviewed to find 80% of responsive items?

- **Concept: Dimensionality and Sparsity in Text Classification**
  - Why needed here: The paper's core intervention reduces vocabulary size by collapsing numeric variants. Understanding why high dimensionality (many unique tokens) combined with short documents (sparse feature vectors) degrades classifier performance explains the mechanism behind the reported gains.
  - Quick check question: Why would a logistic regression model assign high coefficients to rare numeric tokens, and why is this problematic?

- **Concept: Active Learning vs. Batch Training**
  - Why needed here: The paper tests both scenarios (5k training documents simulating active learning; full 51k for batch). Understanding how training set size affects model performance and the economics of iterative review is necessary to evaluate which regime the proposed improvements benefit most.
  - Quick check question: In an active learning workflow, why would dimensionality reduction show larger performance gains compared to a batch training scenario with abundant labeled data?

## Architecture Onboarding

- **Component map:**
  Raw Chat Data (multiple participants' logs) -> [Deduplication & Compression Workflow] -> Consolidated day-chat documents -> [Text Normalization] -> [Numerical Token Tagging] -> [Tokenization] -> [Feature Selection] -> [Logistic Regression Training] -> [Scoring]

- **Critical path:** The tagging component is the key intervention point. The workflow must preserve chat-to-day-chat mapping integrity through deduplication, as errors here propagate to both training and scoring phases. The 0.3KB minimum document size filter is critical—documents below this threshold lack sufficient signal and should be excluded or handled separately.

- **Design tradeoffs:**
  - Raw vs. normalized text: Normalized text removes participant identifiers (~1% precision drop) but may be required for privacy/anonymization requirements.
  - Single [NUM] vs. dual [NUM]+[TIMESTAMP] tagging: Marginal differences (TABLE I shows 65.03% vs. 65.08% for 5k Raw); single tag is simpler and nearly equivalent.
  - Training set size vs. upfront review cost: Larger training sets improve performance (60.72% → 72.66% precision from 5k to 29k) but require more manual coding before the model becomes useful.

- **Failure signatures:**
  - High logistic regression coefficients on numeric tokens in untagged models → indicates overfitting to sparse features (trigger tagging intervention).
  - Precision drops when using normalized text → participant names carry signal; consider preserving them if privacy constraints permit.
  - Model performance plateaus despite increased training data → vocabulary may be too noisy; consider additional preprocessing (abbreviation normalization, spelling correction per Future Work section).
  - Very short documents (<0.3KB) show erratic predictions → exclude from training/scoring or aggregate into longer time windows.

- **First 3 experiments:**
  1. Establish baseline: Train logistic regression on raw day-chat documents without numerical tagging using 5k training set; record precision at 80% recall as benchmark.
  2. Ablation on tagging: Train identical model with [NUM] tag replacement; compare precision @80% recall and AUROC to quantify dimensionality reduction benefit.
  3. Training size sensitivity: Repeat experiments 1-2 with larger training sets (e.g., 29k) to determine whether tagging benefits diminish with more training data (per Fig. 3 showing reduced gains at larger training sizes).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does replacing generic numeric tags with semantic tags (e.g., `<amount>`, `<rate>`) and modeling their combinations improve classification accuracy?
- Basis in paper: [explicit] The authors propose "tagging numbers per their semantic value" to enable features based on tag combinations, such as an `<amount>` followed by a `<rate>`.
- Why unresolved: The current study only implemented generic `[NUM]` and `[TIMESTAMP]` tags without semantic differentiation.
- What evidence would resolve it: A comparative study measuring precision and recall metrics for models trained on semantically tagged datasets versus the current baseline.

### Open Question 2
- Question: Does normalizing single-character currency symbols to three-letter codes (e.g., converting `$` to `USD`) improve model performance without introducing noise?
- Basis in paper: [explicit] The authors note that lowering the minimum token length to capture symbols deteriorated performance, suggesting symbol conversion as an alternative solution.
- Why unresolved: The authors have not yet tested whether this specific normalization strategy successfully balances feature retention against noise reduction.
- What evidence would resolve it: Evaluation results showing whether symbol normalization increases precision at target recall levels compared to the standard tokenization settings.

### Open Question 3
- Question: Are the observed performance gains from dimensionality reduction statistically significant, or merely random fluctuations?
- Basis in paper: [explicit] The authors acknowledge that observed differences "could be just random fluctuations due to the different validation sets" and state the need for statistical significance testing.
- Why unresolved: The current analysis relies on single-run comparisons of Area Under ROC Curve (AUROC) and precision without p-values or confidence intervals.
- What evidence would resolve it: Statistical tests (e.g., paired t-tests or bootstrapping) across multiple data splits confirming the reliability of the performance increase.

## Limitations

- The Bloomberg chat corpus may not generalize to other instant messaging contexts (e.g., consumer messaging apps, workplace collaboration tools).
- The grouping assumption that day-level aggregation captures topical coherence is plausible but unverified.
- The numerical tagging approach may fail when specific values are legally determinative for responsiveness decisions.
- The paper does not provide confidence intervals or statistical significance testing for the reported performance improvements.

## Confidence

- High confidence in the core claim that day-level document grouping makes predictive coding feasible for instant messaging data, as this is a straightforward data management intervention with clear economic rationale
- Medium confidence in the numerical tagging mechanism improving performance, as results are consistent across experiments but the effect size (4.31% precision gain) may vary with different corpora or classification tasks
- Medium confidence in the information gain feature selection approach, as the optimal cutoff is not validated and the method lacks comparative evaluation against alternatives
- Low confidence in generalizability to non-financial domains or different messaging platforms without empirical validation

## Next Checks

1. Replicate the numerical tagging experiment with a different IM corpus (e.g., workplace Slack data or consumer messaging) to test generalizability of the dimensionality reduction benefit
2. Conduct statistical significance testing on precision improvements using bootstrapping or cross-validation to establish confidence intervals for the 4.31% gain
3. Test alternative feature selection methods (chi-square, mutual information) and different vocabulary size cutoffs to optimize the information gain approach empirically