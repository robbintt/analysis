---
ver: rpa2
title: Which LIME should I trust? Concepts, Challenges, and Solutions
arxiv_id: '2503.24365'
source_url: https://arxiv.org/abs/2503.24365
tags:
- lime
- explanations
- explanation
- data
- techniques
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively examines LIME (Local Interpretable
  Model-agnostic Explanations), a widely used technique for explaining black-box AI
  models. It systematically categorizes LIME adaptations based on their technical
  modifications and the specific issues they address, including locality, fidelity,
  interpretability, stability, and efficiency.
---

# Which LIME should I trust? Concepts, Challenges, and Solutions

## Quick Facts
- arXiv ID: 2503.24365
- Source URL: https://arxiv.org/abs/2503.24365
- Reference count: 40
- Primary result: Systematic taxonomy of LIME adaptations addressing locality, fidelity, stability, and efficiency issues

## Executive Summary
This survey provides a comprehensive analysis of LIME (Local Interpretable Model-agnostic Explanations), a widely used technique for explaining black-box AI models. The authors systematically categorize LIME adaptations based on technical modifications and the specific issues they address, including locality, fidelity, interpretability, stability, and efficiency. The study presents a detailed taxonomy mapping techniques to these issues and the LIME pipeline stages they modify. The survey highlights the importance of reproducibility, evaluation practices, and the need for standardized metrics. It also identifies future research opportunities such as automatic LIME selection, foundation model integration, and user-centered approaches.

## Method Summary
The authors conducted a systematic literature review from 2016-2025 using ArXiv and scholarly databases with keywords "LIME", "LIME issues", and "LIME improvements". They filtered for papers specifically modifying the LIME framework rather than general XAI comparisons. The resulting concept matrix documented each technique's name, modality, domain, issues addressed, pipeline modifications, evaluation methods, and code availability. This data was used to construct a taxonomy categorizing techniques across five issue types (Locality, Fidelity, Interpretability, Stability, Efficiency) and four pipeline stages (Feature/Sample Generation, Attribution, Representation).

## Key Results
- LIME adaptations can be systematically categorized by the specific issues they address and the pipeline stages they modify
- Stability and fidelity are the most prominent challenges in LIME implementations, with multiple techniques proposed to address each
- The survey identifies a critical need for standardized evaluation frameworks and reproducibility practices in LIME research
- An interactive website accompanies the survey, providing a searchable database of LIME techniques and their characteristics

## Why This Works (Mechanism)

### Mechanism 1: Local Surrogate Approximation
- **Claim:** A simple interpretable model can approximate complex black-box model behavior locally
- **Mechanism:** LIME trains a linear surrogate model on perturbed samples around a target instance
- **Core assumption:** The black-box model's local decision boundary is approximately linear near the instance
- **Evidence anchors:** [Section 3] describes training local surrogate models; [Corpus] supports general use but highlights stability issues
- **Break condition:** Fails with highly non-linear local boundaries or inappropriate kernel widths

### Mechanism 2: Perturbation-Based Sensitivity Analysis
- **Claim:** Feature contribution can be inferred by observing output changes when features are masked
- **Mechanism:** Creates perturbed input variations and measures probability changes
- **Core assumption:** Perturbation results in meaningful "missing" data points without creating artifacts
- **Evidence anchors:** [Section 3] describes perturbed sample generation; [Section 4.1] mentions instability from perturbation process
- **Break condition:** Breaks if perturbation creates artifacts the model treats as features

### Mechanism 3: Taxonomy-Driven Optimization
- **Claim:** Selecting LIME variants based on specific failure modes yields more reliable explanations
- **Mechanism:** Categorizes techniques by issue type (Locality, Fidelity, Stability, etc.) for targeted selection
- **Core assumption:** Identified issues are primary bottlenecks and taxonomic modifications effectively address them
- **Evidence anchors:** [Abstract] describes categorization by issues; [Section 4.1] defines issues explicitly
- **Break condition:** Fails if user misdiagnoses their problem or chosen variant lacks reproducible code

## Foundational Learning

- **Concept: Surrogate Models**
  - **Why needed here:** LIME explains a simpler surrogate model that mimics the neural network locally
  - **Quick check question:** Can you distinguish between the "black-box model" being explained and the "interpretable surrogate model" used to generate the explanation?

- **Concept: Model-Agnosticism**
  - **Why needed here:** LIME works on any model by treating it as an input-output oracle, relying on perturbation rather than internal weights
  - **Quick check question:** Does LIME require access to the internal weights of a neural network to generate an explanation?

- **Concept: The Stability-Fidelity Trade-off**
  - **Why needed here:** Modifications to improve stability often degrade fidelity and vice versa
  - **Quick check question:** If you run LIME twice on the same image and get different top features, is this a failure of fidelity or stability?

## Architecture Onboarding

- **Component map:** Input -> Feature Generation -> Sample Generation -> Feature Attribution -> Explanation Representation
- **Critical path:** Kernel Width (Ïƒ) and Perturbation Strategy determine neighborhood size and data quality for surrogate training
- **Design tradeoffs:**
  - Stability vs. Efficiency: Reducing sample count speeds up explanation but increases variance
  - Locality vs. Fidelity: Very small neighborhoods may capture local behavior but be too sparse for faithful surrogates
- **Failure signatures:**
  - Noise: Explanations change completely on repeated runs (Stability Issue)
  - Misalignment: Explanation highlights irrelevant features (Fidelity/Interpretability Issue)
  - Artifacts: Explanation focuses on grayed-out pixels indicating model sees masking pattern as feature
- **First 3 experiments:**
  1. **Stability Stress Test:** Run default LIME 10 times on same image, calculate rank correlation of top 5 features
  2. **Kernel Width Ablation:** Systematically vary kernel width on single prediction, observe explanation shift from object to background
  3. **Segmentation Sensitivity:** Swap segmentation algorithm (e.g., change SLIC superpixel size), check if explanation quality improves with semantic segments

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can optimal LIME variant selection be automated for a given dataset or model?
- **Basis in paper:** [explicit] Section 5.2 notes automated XAI selection is limited to original framework techniques
- **Why unresolved:** Volume of adaptations makes manual navigation overwhelming for practitioners
- **Evidence:** Framework mapping data characteristics to most effective LIME adaptation

### Open Question 2
- **Question:** Can foundation models be integrated into LIME's feature and sample generation stages?
- **Basis in paper:** [explicit] Section 5.2 proposes integrating LLMs into LIME's feature and sample generation
- **Why unresolved:** This is a proposed future direction; current sampling methods lack semantic context
- **Evidence:** Empirical studies showing LLM-guided sampling produces higher fidelity or stability

### Open Question 3
- **Question:** What is the most effective standardized evaluation framework for comparing LIME adaptations?
- **Basis in paper:** [explicit] Section 5.2 states developing tailored evaluation framework would be beneficial
- **Why unresolved:** Lack of unified benchmark hinders rigorous comparison between new adaptations
- **Evidence:** Adoption of benchmark suite quantifying performance across five identified issue categories

## Limitations

- Survey focuses exclusively on LIME without addressing broader explainability techniques that might outperform LIME
- Taxonomy is comprehensive but lacks empirical validation through systematic benchmarking
- Relies heavily on published papers, potentially missing unpublished practical implementations
- Automatic LIME selection remains speculative without concrete evaluation of technical feasibility

## Confidence

- **High Confidence:** Classification of LIME adaptations into five issue categories is well-supported by literature
- **Medium Confidence:** Practical taxonomy mapping modifications to specific issues is methodologically sound
- **Low Confidence:** Assertion that interactive website will serve as valuable resource is forward-looking and depends on community adoption

## Next Checks

1. **Empirical Validation:** Conduct controlled experiments comparing stability and fidelity of different LIME variants on same dataset
2. **Reproducibility Assessment:** Implement 3-5 representative LIME adaptations from survey to verify sufficient implementation details
3. **Community Impact:** Monitor citations and usage of interactive website over 6-12 months to assess widespread adoption