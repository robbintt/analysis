---
ver: rpa2
title: 'D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth
  and Depth Analysis in LLMs'
arxiv_id: '2509.11569'
source_url: https://arxiv.org/abs/2509.11569
tags:
- score
- dispersion
- arxiv
- llms
- drift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes D2HScore, a training-free method for detecting
  hallucinations in large language models by analyzing internal representations. It
  combines two complementary metrics: Intra-Layer Dispersion, which measures semantic
  diversity within each layer, and Inter-Layer Drift, which tracks the evolution of
  key token representations across layers.'
---

# D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs

## Quick Facts
- arXiv ID: 2509.11569
- Source URL: https://arxiv.org/abs/2509.11569
- Reference count: 24
- This paper proposes D2HScore, a training-free method for detecting hallucinations in large language models by analyzing internal representations.

## Executive Summary
This paper introduces D2HScore, a training-free method for detecting hallucinations in large language models by analyzing internal representations. It combines two complementary metrics: Intra-Layer Dispersion, which measures semantic diversity within each layer, and Inter-Layer Drift, which tracks the evolution of key token representations across layers. The method uses attention weights to identify important tokens and captures both the breadth and depth of semantic representations during inference. Extensive experiments on five open-source models and five benchmarks show that D2HScore consistently outperforms existing training-free baselines, achieving up to 99.53 AUPR on some datasets, demonstrating its effectiveness in detecting hallucinated responses across diverse tasks and model architectures.

## Method Summary
D2HScore detects hallucinations by analyzing the geometric properties of token representations in large language models. It uses two complementary metrics: Intra-Layer Dispersion, which measures semantic diversity within each layer by calculating the mean L2 distance of token embeddings from their centroid, and Inter-Layer Drift, which tracks the progressive transformation of key token representations across layers. The method identifies important tokens using attention weights, then computes normalized scores for both metrics and combines them with equal weighting. Being training-free, it directly analyzes hidden states during inference without requiring additional model training.

## Key Results
- Achieves up to 99.53 AUPR on some datasets, significantly outperforming training-free baselines
- Outperforms CoE baseline by up to 20.2% across all benchmarks
- Consistently high performance across five different model architectures and five diverse datasets
- Attention-guided token selection is critical, with optimal performance at 40-50% top-k threshold

## Why This Works (Mechanism)

### Mechanism 1: Intra-Layer Dispersion (Semantic Breadth)
- **Claim:** If a generated response is a hallucination, its token representations within a single layer will exhibit spatial "collapse" (low diversity), whereas faithful responses show wider semantic spread.
- **Mechanism:** For a given layer $l$, the method computes the centroid $c_l$ of all token embeddings. It then calculates the mean L2 distance of all tokens from this centroid. Low average distance implies the model is outputting tokens from a semantically constricted space, which correlates with factual errors or repetitive gibberish.
- **Core assumption:** Hallucinations stem from a lack of distinct semantic differentiation between tokens during generation.
- **Evidence anchors:**
  - [abstract] "quantifies the semantic diversity of token representations within a layer; lower dispersion often correlates with collapsed, low-information representations."
  - [section: Method, Figure 3] Demonstrates that hallucinated samples in GSM8K yield lower dispersion scores with an AUC of 0.74.
  - [corpus] Consistent with literature suggesting uncertainty is detectable via representation geometry.
- **Break condition:** If the model generates a verbose but semantically repetitive hallucination (e.g., loops), dispersion might paradoxically increase if the "loop" drifts, or remain low if strictly identical. If the task requires low vocabulary diversity (e.g., coding tasks with repeated keywords), low dispersion may trigger false positives.

### Mechanism 2: Inter-Layer Drift (Semantic Depth)
- **Claim:** If a response is faithful, key token representations should evolve significantly as they pass through the model's layers (high drift), indicating progressive reasoning. Stagnant trajectories suggest reasoning breakdown.
- **Mechanism:** The method tracks the L2 distance between the "core representation" $\bar{h}_l$ of adjacent layers. A larger average shift implies the model is actively refining the semantic understanding of core concepts.
- **Core assumption:** Effective reasoning in Transformers manifests as distinct geometric movements in the hidden state space across layers.
- **Evidence anchors:**
  - [abstract] "Inter-Layer Drift... tracks the progressive transformation of key token representations... weak or stagnant trajectories suggest semantic inconsistency."
  - [section: Method, Figure 4] Visual and quantitative evidence (AUC 0.76) showing hallucinated responses have lower drift scores.
  - [corpus] "The Geometry of Truth" (corpus neighbor) similarly posits that layer-wise semantic dynamics are crucial for detection.
- **Break condition:** If a model has "skip connections" that dominate the residual stream, drift might naturally be lower even for correct answers. Conversely, a "confused" model might drift wildly without converging on truth.

### Mechanism 3: Attention-Guided Semantic Anchoring
- **Claim:** Using raw averages of token states (as in prior work like CoE) introduces noise; focusing only on tokens with high attention weights isolates the "reasoning trace."
- **Mechanism:** Before computing the Inter-Layer Drift, the method averages attention matrices to identify the top-$k$ tokens receiving the most attention from the final generated token. Only these embeddings constitute the "core representation."
- **Core assumption:** Attention weights serve as a proxy for semantic importance, filtering out stop words or redundant tokens that dilute the signal.
- **Evidence anchors:**
  - [abstract] "To ensure drift reflects the evolution of meaningful semantics... we guide token selection using attention signals."
  - [section: Method, Figure 5] Ablation study shows that using the attention-guided top 40-50% of tokens significantly outperforms random selection or using all tokens (k=1.0).
  - [corpus] "Robust Hallucination Detection..." supports adaptive token selection strategies.
- **Break condition:** If attention heads are "noisy" or misaligned with factual correctness (e.g., attending to hallucinated entities), this mechanism might amplify the wrong signal.

## Foundational Learning

- **Concept: Hidden States (Internal Representations)**
  - **Why needed here:** D2HScore relies entirely on accessing the model's intermediate activations ($h^l_t$) at every layer, not just the final logits. You must understand that these vectors encode the model's "thought process" before the final word is chosen.
  - **Quick check question:** In a 32-layer model, do you know how to extract the specific vector for the word "Paris" at layer 15 versus layer 32?

- **Concept: L2 Distance (Euclidean Distance)**
  - **Why needed here:** Both Dispersion and Drift scores are computed using L2 distanceâ€”first between tokens and their centroid (breadth), and second between centroids of adjacent layers (depth).
  - **Quick check question:** Can you calculate the L2 distance between two vectors $[1, 2]$ and $[4, 6]$? (Answer: $\sqrt{(1-4)^2 + (2-6)^2} = 5$).

- **Concept: Multi-Head Self-Attention Averaging**
  - **Why needed here:** The method requires aggregating attention patterns across multiple heads to determine token importance. You need to understand that raw attention is a 3D tensor (Heads $\times$ Seq $\times$ Seq) that must be reduced.
  - **Quick check question:** If you have 32 attention heads, how do you derive a single "importance score" for a specific token? (Answer: Average the attention weights across the head dimension).

## Architecture Onboarding

- **Component map:** Inference Hook -> Dispersion Calculator -> Attention Filter -> Drift Calculator -> Score Fusion

- **Critical path:** The extraction of accurate attention weights is the most fragile step. If using a quantized model or a specific inference engine (e.g., vLLM, llama.cpp), verify that the `attn_weights` output matches the standard Transformer definition (Query $\times$ Key) and is not fused or hidden by optimization kernels (like FlashAttention).

- **Design tradeoffs:**
  - **Top-k Threshold:** The paper suggests 0.4-0.5. Setting this too low ($<0.1$) loses context; setting it too high ($1.0$) includes noise and degrades performance below the attention-guided baseline.
  - **Equal Weights ($w_1, w_2$):** The paper defaults to $0.5/0.5$. However, Table 4 suggests Drift is stronger on some models (Llama-7B) while Dispersion is stronger on others (DS-8B). Adaptive weighting could improve results but adds complexity.

- **Failure signatures:**
  - **Constant High Score:** If Dispersion is always high, check if the hidden states are unnormalized or if the scale of L2 distance is exploding in later layers.
  - **No Separation in Drift:** If Drift scores are identical for correct/hallucinated, verify that you are tracking the *same* token indices across layers, not re-calculating attention at every layer independently (though the paper computes per-layer attention, the token positions must align).
  - **FlashAttention Incompatibility:** Standard PyTorch hooks often fail to capture attention weights when FlashAttention is enabled. You may need to force-disable FlashAttention for extraction, incurring a memory/speed penalty.

- **First 3 experiments:**
  1. **Single-Metric Reproduction:** Run the code on Llama3.1-8B using the GSM8K subset. Compute *only* the Dispersion Score. Plot the histogram (Hallucinated vs. Faithful) to verify the "collapse" hypothesis (visual check against Figure 3).
  2. **Ablation on Top-k:** Run the Drift Score calculation while varying the top-k threshold from 0.1 to 1.0. Verify that performance peaks around 0.4-0.5 and drops at 1.0, confirming that attention filtering is functionally necessary.
  3. **Cross-Model Normalization:** Apply D2HScore (trained on Llama settings) to Qwen1.5. Check if the raw score magnitudes differ significantly. If they do, implement a per-model normalization layer (z-score normalization) before fusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can D2HScore be effectively utilized as a real-time feedback signal to guide the decoding process and actively mitigate hallucinations during generation?
- Basis in paper: [explicit] The Conclusion states that the approach "opens new directions for hallucination mitigation, such as using D2HScore as a feedback signal to guide generation."
- Why unresolved: The current study validates D2HScore solely as a post-hoc detection metric; the authors have not yet implemented or evaluated mechanisms to integrate the score into the decoding loop to steer the model away from low-confidence states.
- What evidence would resolve it: Experiments demonstrating that minimizing D2HScore during beam search or decoding results in higher factual accuracy without significantly compromising the fluency or diversity of the output.

### Open Question 2
- Question: Is it possible to transfer or approximate the internal dynamics measured by D2HScore to closed-source, black-box models (e.g., GPT-4) where internal hidden states are inaccessible?
- Basis in paper: [explicit] The Limitations section acknowledges that the method's "white-box" nature is a "primary limitation... [that] precludes its use on closed-source models like GPT-4."
- Why unresolved: The method fundamentally relies on accessing specific hidden states and attention matrices, which are unavailable in proprietary APIs. It is unclear if output-probabilities or text-based features can serve as sufficient proxies for these internal geometric dynamics.
- What evidence would resolve it: A study establishing a correlation between D2HScore and output-only metrics, or a transfer learning framework that predicts D2HScore from accessible data on open models and applies it successfully to closed models.

### Open Question 3
- Question: Does the optimal weighting between Intra-Layer Dispersion and Inter-Layer Drift vary dynamically across different model architectures or prompt types?
- Basis in paper: [inferred] The Method section sets weights ($w_1=w_2=0.5$) uniformly, but the ablation study (Table 4) reveals that the relative effectiveness of the Drift Score vs. the Dispersion Score varies significantly across different models (e.g., Drift is stronger on Llama-7B, while Dispersion is more effective on DS-8B).
- Why unresolved: The current implementation assumes a fixed contribution for both scores, potentially leaving performance gains on the table for specific architectures where one semantic dimension is more predictive of hallucination than the other.
- What evidence would resolve it: An adaptive weighting mechanism that conditions on model identity or input context, demonstrating statistically significant improvements over the static linear summation.

### Open Question 4
- Question: Do the semantic breadth and depth dynamics detected by D2HScore generalize to significantly larger parameter scales (e.g., 70B+) or sparse Mixture-of-Experts (MoE) architectures?
- Basis in paper: [inferred] The experiments are restricted to dense models in the 7B-8B range (Llama2, Qwen1.5, etc.), leaving the scaling behavior of these geometric representations unverified.
- Why unresolved: Larger models often exhibit different representation properties (e.g., polysemanticity or sparsity) which might alter the relationship between dispersion, drift, and factual consistency.
- What evidence would resolve it: Benchmark results applying D2HScore to large-scale or MoE models (such as Mixtral 8x7B or Llama-3-70B), confirming that the detection thresholds and performance metrics remain robust.

## Limitations

- **White-box requirement:** The method requires access to internal hidden states and attention weights, making it incompatible with closed-source models like GPT-4
- **Computational overhead:** Accessing intermediate representations introduces significant memory and computational overhead, particularly for large models
- **Task generalizability:** Performance on tasks with inherently low semantic diversity (e.g., code generation) or non-QA tasks remains untested and may produce false positives

## Confidence

**High Confidence:** The core mechanism of using intra-layer dispersion and inter-layer drift as indicators of semantic collapse and reasoning breakdown is well-supported by the experimental results, particularly the consistent outperformance of training-free baselines and the strong AUC scores (up to 0.99) on multiple benchmarks.

**Medium Confidence:** The method's effectiveness across diverse model families and task types is plausible but not fully established. The reliance on attention weights and hidden states is reasonable given the literature, but the method's robustness to architectural variations (e.g., skip connections, FlashAttention) and task diversity requires further validation.

**Low Confidence:** The interpretability of the semantic claims (e.g., what exactly constitutes "semantic collapse" or "reasoning breakdown") is not rigorously tested. The method's performance on non-QA tasks, proprietary models, or tasks with low semantic diversity is speculative. The lack of detailed computational overhead analysis and failure mode mitigation strategies also limits confidence in real-world deployment.

## Next Checks

1. **Cross-Architecture Robustness:** Apply D2HScore to a diverse set of models, including those with skip connections (e.g., GPT-style models) and those using FlashAttention. Measure performance, computational overhead, and any failure modes (e.g., inability to extract attention weights). This will clarify the method's robustness to architectural variations.

2. **Task Diversity and False Positives:** Test D2HScore on tasks with inherently low semantic diversity (e.g., code generation, summarization with repetitive keywords) and on non-QA tasks (e.g., translation, creative writing). Analyze false positive rates and investigate whether low dispersion or high drift is always indicative of hallucination in these contexts.

3. **Adaptive Normalization and Fusion:** Implement and test adaptive weighting for Dispersion and Drift scores based on model or task characteristics. Explore alternative normalization schemes (e.g., per-token, per-layer scaling) and evaluate their impact on performance and robustness. This will address the current limitation of fixed equal weighting and improve generalizability.