---
ver: rpa2
title: Optimal Look-back Horizon for Time Series Forecasting in Federated Learning
arxiv_id: '2511.12791'
source_url: https://arxiv.org/abs/2511.12791
tags:
- loss
- time
- intrinsic
- horizon
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of selecting the optimal look-back
  horizon for time series forecasting in federated learning with non-IID, heterogeneous
  client data. The authors introduce a synthetic data generator (SDG) that captures
  essential temporal patterns (AR dependencies, seasonality, trend) and client-specific
  heterogeneity.
---

# Optimal Look-back Horizon for Time Series Forecasting in Federated Learning

## Quick Facts
- **arXiv ID:** 2511.12791
- **Source URL:** https://arxiv.org/abs/2511.12791
- **Reference count:** 40
- **Primary result:** Provides first theoretically grounded, client-adaptive criterion for horizon selection in federated time series forecasting

## Executive Summary
This paper addresses the critical problem of selecting optimal look-back horizon for time series forecasting in federated learning with non-IID, heterogeneous client data. The authors introduce a synthetic data generator that captures essential temporal patterns (AR dependencies, seasonality, trend) and client-specific heterogeneity. Using an intrinsic space formulation, they map time series windows into a geometry-preserving representation, enabling rigorous decomposition of forecasting loss into irreducible Bayesian and approximation components. Their analysis shows that while longer horizons improve identifiability of deterministic patterns, they also increase approximation error due to higher model complexity and reduced sample efficiency.

## Method Summary
The framework introduces a synthetic data generator (SDG) that models multivariate time series with additive trend, seasonality, and autoregressive components. Time series windows are flattened and projected into an intrinsic space via eigendecomposition of global covariance matrices. The optimal horizon is determined by finding where Bayesian error saturates while approximation error begins to dominate. In federated settings, client-specific horizons are aggregated using a weighted trimmed mean. The method provides a theoretically grounded criterion for horizon selection that adapts to client heterogeneity and data characteristics.

## Key Results
- Total forecasting loss is minimized at the smallest horizon where Bayesian error saturates and approximation error dominates
- Longer horizons improve identifiability of deterministic patterns but increase approximation error due to higher model complexity
- The framework provides the first theoretically grounded, client-adaptive criterion for horizon selection in federated time series forecasting

## Why This Works (Mechanism)
The method works by decomposing forecasting loss into irreducible Bayesian error (from inherent data noise) and reducible approximation error (from model limitations). By projecting data into intrinsic space, the framework captures the essential dimensionality needed for forecasting while filtering noise. The synthetic data generator accurately models real-world temporal patterns, enabling controlled experiments. The weighted trimmed mean aggregation in federated settings ensures robustness to outlier client horizons while respecting data heterogeneity.

## Foundational Learning
- **Synthetic Data Generator (SDG):** Models time series with trend, seasonality, and AR components using structured noise. *Why needed:* Enables controlled experiments and validation on synthetic data that mimics real temporal patterns. *Quick check:* Verify SDG captures AR spectral properties and seasonal periods in real datasets.
- **Intrinsic Space Projection:** Uses eigendecomposition of covariance matrices to reduce dimensionality while preserving essential structure. *Why needed:* Reduces computational complexity and focuses learning on relevant features. *Quick check:* Confirm intrinsic dimension estimates align with actual model performance across horizons.
- **Loss Decomposition:** Separates total loss into Bayesian (irreducible) and approximation (reducible) components. *Why needed:* Provides theoretical foundation for understanding trade-offs in horizon selection. *Quick check:* Verify that approximation error decreases with model capacity while Bayesian error remains constant.
- **Federated Aggregation:** Uses weighted trimmed mean to combine client-specific horizons. *Why needed:* Handles heterogeneity while being robust to outliers. *Quick check:* Test sensitivity to trim fraction and weight assignments on synthetic heterogeneous data.

## Architecture Onboarding

**Component Map:** SDG -> Intrinsic Space Projection -> Horizon Optimization -> Federated Aggregation

**Critical Path:** Data generation and fitting → Intrinsic dimension estimation → Optimal horizon computation → Federated aggregation

**Design Tradeoffs:**
- Horizon length vs. sample efficiency: longer horizons improve pattern identification but reduce effective sample size
- Model complexity vs. approximation error: more complex models can capture longer dependencies but require more data
- Client privacy vs. aggregation accuracy: trimmed mean provides privacy protection but may discard useful information

**Failure Signatures:**
- Overestimated intrinsic dimension → premature approximation error dominance
- Poor seasonal period estimation → incorrect horizon selection
- Serial correlation in overlapping windows → overstated effective sample size

**First Experiments:**
1. Implement SDG and validate on temperature data using ACF L² gap and spectral L² gap metrics
2. Test intrinsic dimension estimation across varying AR spectral radii and seasonal patterns
3. Verify Bayesian loss saturation behavior as horizon increases beyond seasonal periods

## Open Questions the Paper Calls Out

**Open Question 1:** How can the framework be extended to handle regime switches, nonlinear seasonal patterns, or cross-feature interactions beyond what PCA implicitly captures? The current additive SDG formulation with linear PCA projection cannot represent state-dependent dynamics, multiplicative seasonality, or feature coupling, limiting applicability to complex real-world series.

**Open Question 2:** What secure aggregation protocols enable privacy-preserving estimation of global covariance matrices in the intrinsic space construction? The framework requires computing eigendecomposition of a global covariance matrix across clients, but no mechanism is provided to do this without exposing client statistics or incurring prohibitive communication costs.

**Open Question 3:** How does the assumption that overlapping windows are approximately independent affect effective sample size estimates and optimal horizon selection? Overlapping windows create serial correlation in training samples, but the bound uses D_k/H as effective sample size without accounting for this dependence, potentially biasing the approximation loss estimate.

**Open Question 4:** How robust is the optimal horizon selection when data exhibits long-memory behavior or near-unit-root AR dynamics? The analysis assumes local stationarity and stable autoregressive structure, which may be challenged in long-memory or near-unit-root settings.

## Limitations

- The approximation error bound cannot be empirically verified without a specified predictor architecture
- Client-specific horizon selection assumes homogeneous cost-benefit trade-offs across clients
- The framework relies on accurate parameter estimation (ρ_k, T_{f,j,k}, A_{f,j,k}) which is not fully specified

## Confidence

- **High Confidence:** SDG structure and validation approach are clearly specified and reproducible
- **Medium Confidence:** Theoretical proofs are mathematically sound but practical applicability depends on accurate parameter estimation
- **Low Confidence:** Without specified predictor architecture, empirical validation of approximation bounds is impossible

## Next Checks

1. **Parameter Estimation Validation:** Implement full SDG fitting procedure and test on multiple real datasets to verify robustness of ρ_k and T_{f,j,k} estimates

2. **Approximation Bound Verification:** Select concrete piecewise-affine architecture, implement curvature constant estimation, and empirically measure approximation error scaling

3. **Federated Aggregation Sensitivity:** Conduct experiments varying trim fraction and weight assignments to quantify impact on final forecasting performance