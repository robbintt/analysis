---
ver: rpa2
title: 'iTRI-QA: a Toolset for Customized Question-Answer Dataset Generation Using
  Language Models for Enhanced Scientific Research'
arxiv_id: '2502.15721'
source_url: https://arxiv.org/abs/2502.15721
tags:
- database
- data
- research
- step
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: iTRI-QA is a framework for generating customized QA datasets for
  scientific research using fine-tuned language models. It integrates curated QA examples
  with a structured research paper database, employing LoRA-based fine-tuning to improve
  contextual relevance and accuracy.
---

# iTRI-QA: a Toolset for Customized Question-Answer Dataset Generation Using Language Models for Enhanced Scientific Research

## Quick Facts
- arXiv ID: 2502.15721
- Source URL: https://arxiv.org/abs/2502.15721
- Reference count: 0
- Generates customized QA datasets for scientific research using fine-tuned LLMs with improved accuracy and relevance

## Executive Summary
iTRI-QA is a framework for generating customized QA datasets from scientific literature using fine-tuned language models. It integrates curated QA examples with a structured research paper database, employing LoRA-based fine-tuning to improve contextual relevance and accuracy. The system comprises four steps: curating QA examples, collecting paper metadata, fine-tuning models, and generating QA datasets. Benchmarking with Llama-3.2-1B and 3B models on PubMed data showed that larger models and larger QA sizes improved performance, with Llama-3.2-3B achieving lower evaluation loss and higher human evaluation scores.

## Method Summary
The iTRI-QA framework generates customized QA datasets through a four-step process: curating QA examples, collecting paper metadata, fine-tuning models, and generating QA datasets. It uses LoRA-based fine-tuning of Llama-3.2 models with specific hyperparameters (rank=16, alpha=32, dropout=0.1, learning rate=3e-5) on PubMed abstracts. The system employs a prompt template (llama3.2.j2) and evaluates performance using both automated metrics (evaluation loss) and human expert review across five dimensions (format adherence, question accuracy, answer accuracy, length, category alignment).

## Key Results
- Llama-3.2-3B model achieved lower evaluation loss (9.13 vs 9.27) compared to 1B model
- Human evaluation scores were higher for the 3B model across all five assessment dimensions
- Performance improved with larger QA sample sizes (3/5/8/10/25), demonstrating scalability
- Fine-tuned models showed better accuracy and quality than baseline approaches in expert review

## Why This Works (Mechanism)
The framework leverages LoRA-based fine-tuning to adapt large language models to domain-specific scientific literature, enabling more accurate and contextually relevant QA generation. By combining curated QA examples with structured paper metadata, the system can generate high-quality, domain-specific datasets that support knowledge retrieval and research applications.

## Foundational Learning
- **LoRA fine-tuning**: Parameter-efficient adaptation of LLMs; needed for computational efficiency; quick check: verify rank and alpha parameters
- **PubMed metadata processing**: Structured extraction of scientific paper information; needed for domain-specific context; quick check: validate BibTeX to YAML conversion
- **Human evaluation framework**: Multi-dimensional scoring system for QA quality; needed for comprehensive assessment; quick check: ensure all five scoring dimensions are implemented
- **Prompt template engineering**: Structured input format for fine-tuning; needed for consistent model behavior; quick check: verify template syntax and placeholders
- **Data augmentation methods**: Techniques for expanding evaluation datasets; needed for robust testing; quick check: confirm augmentation pipeline exists

## Architecture Onboarding

**Component Map**
BibTeX PubMed data -> YAML conversion -> QA curation webtool -> JSONL formatting -> LoRA fine-tuning -> QA generation -> Human evaluation

**Critical Path**
PubMed abstract collection -> QA example curation (50 samples) -> LoRA fine-tuning (5 epochs) -> QA dataset generation -> Human expert review

**Design Tradeoffs**
- Model size vs. computational resources: 3B model outperforms 1B but requires more GPU memory
- QA sample size vs. overfitting risk: Larger sizes improve performance but increase training complexity
- Automated vs. human evaluation: Automated metrics provide efficiency while human review ensures quality

**Failure Signatures**
- Format adherence failures (format_score=0) indicate prompt template issues
- High train/eval loss divergence suggests overfitting on small datasets
- Low accuracy scores across multiple dimensions point to inadequate fine-tuning

**First Experiments**
1. Test BibTeX to YAML conversion with sample PubMed data
2. Validate QA webtool submission and JSONL formatting
3. Run single-epoch LoRA fine-tuning on minimal dataset to verify pipeline functionality

## Open Questions the Paper Calls Out
None

## Limitations
- Performance highly sensitive to model scale and QA sample size
- Exact prompt template (llama3.2.j2) not fully specified, affecting reproducibility
- Hardware specifications for benchmarking not documented
- Claims about generalizability to other scientific domains not directly tested

## Confidence
- **High Confidence**: Core methodology with clear evaluation metrics and human review validation
- **Medium Confidence**: Benchmarking results showing model and size effects, dependent on unspecified prompt template
- **Low Confidence**: Generalizability claims beyond PubMed use case

## Next Checks
1. Implement and test the exact prompt template (llama3.2.j2) to verify its impact on format adherence and accuracy scores
2. Conduct controlled experiments varying train/eval split ratios and random seeds to assess result stability across different data partitions
3. Benchmark the fine-tuning process on different hardware configurations (GPU memory constraints) to validate the reported scalability with larger models