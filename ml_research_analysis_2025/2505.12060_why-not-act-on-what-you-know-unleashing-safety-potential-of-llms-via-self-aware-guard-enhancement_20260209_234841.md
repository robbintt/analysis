---
ver: rpa2
title: Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware
  Guard Enhancement
arxiv_id: '2505.12060'
source_url: https://arxiv.org/abs/2505.12060
tags:
- sage
- harmful
- safety
- jailbreak
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a safety gap in LLMs where models can detect
  harmful prompts as discriminators but still generate unsafe responses as generators.
  To bridge this gap, the authors propose SAGE, a training-free defense that couples
  a model's discriminative and generative capabilities.
---

# Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement

## Quick Facts
- arXiv ID: 2505.12060
- Source URL: https://arxiv.org/abs/2505.12060
- Reference count: 34
- Primary result: A training-free defense achieving 99% average defense success rate against seven jailbreak methods while maintaining helpfulness

## Executive Summary
This paper identifies a critical safety gap in LLMs where models can successfully detect harmful prompts as discriminators but still generate unsafe responses as generators. The authors propose SAGE, a training-free defense that bridges this discrimination-generation gap by coupling a model's discriminative and generative capabilities through explicit prompt-based instructions. Extensive experiments demonstrate SAGE achieves a 99% average defense success rate against seven jailbreak methods across multiple open-source and closed-source LLMs, while maintaining strong performance on general benchmarks.

## Method Summary
SAGE works by wrapping user inputs with two prompt modules: a Discriminative Analysis Module that performs two-stage safety checking (semantic and task structure analysis) and a Discriminative Response Module that guides appropriate response generation. The system concatenates these instructions before the user prompt at inference time, forcing the model to explicitly evaluate safety before generating any response. This approach leverages the model's latent safety knowledge that manifests during discrimination tasks but fails to transfer to standard generation.

## Key Results
- Achieves 99% average defense success rate against seven jailbreak methods
- Reduces attack success rates from >86% to <2% across tested models
- Maintains high performance on general benchmarks (GSM8K, MMLU, Just-Eval)

## Why This Works (Mechanism)

### Mechanism 1: Discrimination-Generation Gap Exploitation
- **Claim**: LLMs possess latent safety knowledge that manifests during explicit discrimination tasks but fails to transfer to generation without structural intervention.
- **Evidence**: Llama-3.1-8B-Instruct correctly discriminates 100% of harmful requests from AdvBench using DeepInception jailbreak attack, but can only defend against 34% when generating responses.
- **Core assumption**: The model's safety training is present but not activated during standard generation because autoregressive decoding prioritizes fluency over safety checking.

### Mechanism 2: Hidden State Realignment via Discriminative Instructions
- **Claim**: Jailbreak attacks shift prompt representations toward benign regions in hidden state space; discriminative instructions counteract this shift.
- **Evidence**: PCA visualization shows jailbreak samples crossing the decision boundary, then returning when CLS instructions are added. After adding discriminative instruction, the distribution is pulled back towards the harmful request side.
- **Core assumption**: Safety-related behaviors are linearly separable in hidden state space.

### Mechanism 3: Attention Redistribution to Harmful Tokens
- **Claim**: Discrimination instructions increase attention concentration on harmful tokens, which correlates with safer responses.
- **Evidence**: The model, when acting as a discriminator, focuses more on harmful content than as a generator, as indicated by a higher AOR. AOR doubles from 0.16 (generation) to 0.33 (discrimination) for Gemma2 and Qwen2.5.
- **Core assumption**: Attention patterns are causally related to generation behavior.

## Foundational Learning

- **Concept: Autoregressive Generation vs. Classification**
  - **Why needed here**: SAGE fundamentally works by forcing a classification-style evaluation before generation. Understanding that standard generation conditions on all prior context without explicit safety gating explains why the gap exists.
  - **Quick check question**: Can you explain why asking a model to classify an input as harmful/unharmful uses different internal circuits than asking it to complete the input?

- **Concept: Representation Space Geometry**
  - **Why needed here**: The hidden state analysis relies on understanding that semantic properties can cluster in activation space. The PCA visualizations assume harmful/benign are separable.
  - **Quick check question**: If you extracted hidden states from layer 20 of a model for 100 harmful and 100 benign prompts, what would you expect a linear classifier to achieve?

- **Concept: Prompt-Based Intervention at Inference Time**
  - **Why needed here**: SAGE is training-free. It modifies behavior purely through context manipulation. This is distinct from safety fine-tuning approaches.
  - **Quick check question**: What are the tradeoffs of inference-time prompting defenses versus training-time safety alignment?

## Architecture Onboarding

- **Component map**: User input → SAGE prompt wrapper (DAM + DRM) → LLM → Response
- **Critical path**: User input → DAM prompt wrapper → LLM → Response. The DAM must correctly identify harm; the DRM must correctly map that judgment to refusal vs. help.
- **Design tradeoffs**: 
  - Explicit reasoning output vs. silent classification: The paper notes not outputting the discrimination reasoning reduces latency
  - Prompt robustness: Table 4 shows SAGE works across prompt variants, but highly novel attack patterns may require prompt tuning
- **Failure signatures**:
  - False positives: Excessive refusal of benign requests (check MMLU/Just-Eval scores)
  - DAM bypass without DRM: Table 5 shows removing DRM causes defense failure even when DAM is present
  - Attention diffusion: If the model does not concentrate attention on harmful tokens (low ACI), safety degrades
- **First 3 experiments**:
  1. **Baseline gap measurement**: Run discrimination accuracy vs. generation safety on a held-out jailbreak dataset to quantify the gap for your target model.
  2. **Ablation study**: Remove DAM and DRM separately to verify both are necessary for your model architecture.
  3. **Efficiency benchmark**: Measure TCPS (Time Cost Per Sample) with and without SAGE to quantify latency overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the explicit discriminative reasoning process be internalized into the model's architecture or weights to ensure safety without incurring the inference latency of explicit prompting?
- **Basis in paper**: The Limitations section states: "Exploring how to integrate the reasoning and discrimination process into the model without explicitly outputting it is a worthwhile direction for further research."
- **Why unresolved**: SAGE currently relies on input concatenation, which increases context length and processing time; the authors note the similarity to reasoning models (e.g., DeepSeek R1) but have not tested methods to internalize this specific safety reasoning.
- **What evidence would resolve it**: A study showing that fine-tuning a model on SAGE-style reasoning traces allows it to maintain a >95% defense success rate without the auxiliary prompts at inference time.

### Open Question 2
- **Question**: How robust is the SAGE framework against adaptive attacks specifically designed to obfuscate harmful content within the "task structure analysis" or "semantic analysis" components?
- **Basis in paper**: The paper evaluates against established jailbreaks (GCG, AutoDAN) but acknowledges the framework "relies on the model's intrinsic discriminative capabilities," which suggests vulnerability if an adversary optimizes prompts to confuse the discriminator instructions specifically.
- **Why unresolved**: While the paper tests generalization across existing attack types, it does not evaluate "white-box" adaptive attacks where the attacker optimizes the prompt to specifically bypass the SAGE instruction protocol.
- **What evidence would resolve it**: Evaluation results against an attack algorithm that is allowed to optimize against the SAGE prompt.

### Open Question 3
- **Question**: Can the observed shifts in hidden states (from harmful to benign representations) be used as a direct mechanism for intervention, rather than relying solely on input instructions?
- **Basis in paper**: The mechanistic analysis reveals that jailbreaks shift hidden states toward the benign cluster, while discrimination instructions pull them back. The paper suggests these insights contribute to "developing future LLMs."
- **Why unresolved**: The paper identifies the correlation between hidden state movement and safety but leaves unexplored whether this vector space geometry can be manipulated directly via activation steering or weight editing.
- **What evidence would resolve it**: An experiment using activation steering (e.g., adding a "refusal vector") that replicates the SAGE safety performance without any prompt modifications.

## Limitations
- Effectiveness depends on the model's underlying safety training capacity - insufficient base training will prevent activation of safety mechanisms
- Assumes discrimination-generation gap is primary failure mode rather than training artifact
- Novel attack strategies designed to bypass semantic or structural analysis could still succeed

## Confidence

- **High Confidence**: The empirical observation that models exhibit different behaviors as discriminators versus generators is well-supported. The claim that SAGE reduces ASR from >86% to <2% is directly supported by Table 1.
- **Medium Confidence**: The mechanism that discriminative instructions realign hidden state representations is plausible but primarily inferred from PCA visualizations and indirect attention analysis. The causal relationship between attention redistribution and safer generation is not definitively established.
- **Low Confidence**: The generalizability of SAGE across arbitrary model architectures is uncertain. The paper tests three models, but scaling to larger or differently trained models may yield different results.

## Next Checks

1. **Gap Quantification**: Measure discrimination vs. generation performance on a new jailbreak dataset (e.g., ReNeLLM) for your target model to verify the gap exists and is significant.
2. **Ablation of Hidden State Claims**: Use a linear probe on hidden states to test whether safety-relevant activations are linearly separable in your model, as assumed in the representation space geometry mechanism.
3. **Attention Causality Test**: Implement an intervention that directly manipulates attention weights during generation (e.g., via activation steering) to determine if attention redistribution is causally linked to safer outputs.