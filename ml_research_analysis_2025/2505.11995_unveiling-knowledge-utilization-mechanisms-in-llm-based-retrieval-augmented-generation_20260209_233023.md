---
ver: rpa2
title: Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented
  Generation
arxiv_id: '2505.11995'
source_url: https://arxiv.org/abs/2505.11995
tags:
- knowledge
- external
- internal
- information
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic investigation into how large language
  models (LLMs) utilize internal and external knowledge in retrieval-augmented generation
  (RAG) systems. The authors propose a novel framework that analyzes knowledge utilization
  from both macroscopic knowledge streaming perspectives and microscopic module-level
  functions.
---

# Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2505.11995
- **Source URL**: https://arxiv.org/abs/2505.11995
- **Reference count**: 40
- **Primary result**: The paper systematically analyzes how LLMs utilize internal and external knowledge in RAG systems, identifying four sequential stages of knowledge streaming and demonstrating that selectively deactivating knowledge-specific neurons can shift model reliance between knowledge sources.

## Executive Summary
This paper presents a comprehensive investigation into knowledge utilization mechanisms in LLM-based retrieval-augmented generation (RAG) systems. The authors propose a novel framework that analyzes knowledge processing from both macroscopic streaming perspectives and microscopic module-level functions. Through extensive experiments on LLaMA and Qwen models across multiple datasets, they identify four distinct stages of knowledge streaming: refinement, elicitation, expression, and contestation. The study introduces a new metric called Knowledge Activation Probability Entropy (KAPE) to identify neurons specialized in internal or external knowledge, demonstrating that selective deactivation of these neurons can effectively control the model's knowledge source preference. The research reveals that multi-layer perceptron (MLP) modules play a crucial role in factual verification, showing particular sensitivity to the accuracy of external knowledge.

## Method Summary
The authors analyze knowledge utilization through inference-only experiments on LLaMA and Qwen base models using Natural Questions, TriviaQA, and HotpotQA datasets. They construct RAG pipelines with RocketQAv2 retriever and Wikipedia corpus, creating gold and fake passage variants for each query. Information flow metrics (IF_kc, IF_kq, IF_ka) are computed using attention and saliency methods across model layers. The KAPE metric identifies knowledge-specific neurons by computing entropy of activation probabilities across closed-book and RAG settings. Selective neuron deactivation is performed by zeroing activations of identified neurons (typically top 1% lowest KAPE). The analysis focuses on four-stage knowledge streaming patterns and differential contributions of MHA vs. MLP modules to factual accuracy.

## Key Results
- Knowledge streaming in RAG follows four distinct stages: refinement (early layers), elicitation (middle layers), expression (deep layers), and contestation (final layers)
- KAPE metric successfully identifies neurons specialized in internal vs. external knowledge, with selective deactivation effectively shifting model reliance
- MLP layers show particular sensitivity to factual accuracy of external knowledge, acting as verification units distinct from MHA's integration role
- Knowledge streaming patterns remain consistent across different model scales (8B to 70B parameters)
- Relevance of retrieved passages guides knowledge streaming, particularly during the elicitation stage

## Why This Works (Mechanism)

### Mechanism 1: Layered Knowledge Streaming
The paper proposes that RAG knowledge processing is temporally and functionally segregated into four sequential stages across the LLM's layers: Refinement, Elicitation, Expression, and Contestation. In early layers (Refinement), the model processes context tokens to understand external text. In middle layers (Elicitation), information flows from the retrieved key to the query representation. In deeper layers (Expression/Contestation), the model resolves conflicts between internal parametric knowledge and the newly processed external context to generate an answer.

### Mechanism 2: Knowledge-Specific Neuron Routing via KAPE
Distinct neurons within the MLP layers specialize in encoding either internal (parametric) or external (contextual) knowledge, identifiable via entropy metrics. The authors introduce Knowledge Activation Probability Entropy (KAPE) to identify "knowledge-specific neurons"—neurons that activate highly for one source but not the other. By selectively deactivating these neurons, the system can mechanically force the model to ignore its parametric memory or the retrieved context.

### Mechanism 3: MLP-Led Factual Verification
While Multi-Head Attention (MHA) integrates information, the Multi-Layer Perceptron (MLP) modules serve as the factual verification unit, checking external content against internal weights. The MLP layers exhibit high sensitivity to the factual accuracy of retrieved passages (Gold vs. Fake). If the external passage is factually incorrect (Fake), the MLP's contribution to the residual stream diminishes or shifts, whereas the MHA continues to move information regardless of truthfulness.

## Foundational Learning

**Concept: Residual Stream Interpretation**
- **Why needed here**: To understand Mechanism 3, one must grasp that Transformers add the output of layers (MHA/MLP) to a "stream" of information. Analyzing this stream via "Early Decoding" is how the paper proves MLPs check facts.
- **Quick check question**: If you decode the residual stream at Layer 15 and see the answer token, what does that imply about the subsequent layers?

**Concept: Parametric vs. Non-Parametric Knowledge**
- **Why needed here**: The entire paper hinges on the conflict between "Internal" (weights/parametric) and "External" (retrieved/non-parametric) knowledge.
- **Quick check question**: Does increasing model scale increase parametric knowledge or non-parametric reasoning capability?

**Concept: Entropy in Neuron Selection**
- **Why needed here**: Mechanism 2 relies on KAPE. You need to understand why "low entropy" in activation probability indicates a specialized (knowledge-specific) neuron.
- **Quick check question**: A neuron fires 50% of the time for Internal knowledge and 50% for External. Is this a "knowledge-specific" neuron according to KAPE?

## Architecture Onboarding

**Component map:**
Query + Retrieved Passage -> Early Layers (Refinement) -> Middle Layers (Elicitation) -> Deep Layers (Expression/Contestation) -> Answer Generation

**Critical path:**
The Knowledge Elicitation stage (Key → Query flow) is the critical junction. If this flow is blocked or the passage relevance is low, the external knowledge fails to propagate to the deeper contestation layers.

**Design tradeoffs:**
- Refinement vs. Speed: Shallow models may skip the "Refinement" stage, leading to poorer context understanding
- Intervention Strength: Deactivating 1% of neurons is safe; deactivating 10% trades accuracy for controllability (forcing the model to be blind to its own memory)

**Failure signatures:**
- Hallucination with Context: Occurs if the MLP fails to penalize the parametric answer during the Contestation stage
- Sycophancy: Occurs if the MLP accepts "Fake" passages as truth (verification failure)

**First 3 experiments:**
1. Stage Validation: Block attention between Key and Query tokens in middle layers and measure the drop in RAG performance to confirm the Elicitation stage
2. KAPE Stress Test: Identify the top 1% "Internal" neurons on a fact-known dataset and deactivate them; verify if the model now relies solely on the retrieved context
3. Verification Probe: Feed the model Gold vs. Fake passages and plot the unembedding logits of the MLP vs. MHA layers separately to visualize the "verification gap"

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question**: Do the four identified knowledge streaming stages (refinement, elicitation, expression, contestation) persist in RAG systems performing complex reasoning or multi-turn dialogue tasks?
- **Basis in paper**: [inferred] The authors note RAG's utility in "complex tasks and intelligent assistants" in the introduction, but restrict the empirical study in Section 2.1 to open-domain question answering (ODQA) datasets (Natural Questions, TriviaQA, HotpotQA)
- **Why unresolved**: The streaming dynamics and module contributions were validated only on single-hop and multi-hop QA, leaving untested whether these stages generalize to generative tasks requiring sustained reasoning or conversational context
- **What evidence would resolve it**: Applying the attention-based and saliency-based information flow analysis to reasoning benchmarks (e.g., long-form reasoning) or conversational datasets

**Open Question 2**
- **Question**: Are the complementary roles of MHA (integration) and MLP (factual verification) consistent across instruction-tuned variants of the studied base models?
- **Basis in paper**: [inferred] The experimental settings in Section 2.2 explicitly state, "All selected models are Base versions," excluding instruction-tuned variants which are more commonly deployed in real-world RAG
- **Why unresolved**: Instruction tuning significantly alters model behavior and attention patterns; it is unknown if the "knowledge contestation" stage in deep layers functions identically in models optimized for instruction following
- **What evidence would resolve it**: Replicating the residual stream and early decoding analysis on instruction-tuned variants (e.g., LLaMA-3-Instruct) to verify if MLP sensitivity to factual accuracy remains consistent

**Open Question 3**
- **Question**: How can the KAPE metric be refined to handle scenarios where internal and external knowledge are partially contradictory rather than binary?
- **Basis in paper**: [inferred] The methodology constructs "fake passages" by replacing the correct answer with an incorrect one, creating a binary ground truth (Section 2.2), which avoids the complexity of partial conflicts
- **Why unresolved**: Real-world retrieval often involves passages that support a conclusion while containing minor factual errors; the current binary deactivation approach may not allow for the selective nuance required in such cases
- **What evidence would resolve it**: Experiments measuring performance when neuron deactivation is applied selectively to specific semantic units within a passage rather than the entire external knowledge set

## Limitations
- The KAPE metric requires a threshold for activation probability filtering that is not specified, potentially affecting neuron selection consistency
- The study focuses on base models without instruction tuning, limiting generalizability to instruction-tuned systems common in production
- The paper does not address computational efficiency implications of the proposed interventions

## Confidence
- **High confidence**: The four-stage knowledge streaming framework and information flow metrics are well-supported by empirical patterns across model scales
- **Medium confidence**: The KAPE-based neuron identification and selective deactivation approach shows promise but requires further validation on diverse datasets and model architectures
- **Medium confidence**: The MLP's role in factual verification is supported by differential sensitivity to gold vs. fake passages, though MHA contributions are not fully disentangled

## Next Checks
1. **Stage Boundary Validation**: Conduct targeted ablation studies by selectively disabling attention or MLP layers at different depths to confirm the causal necessity of each proposed knowledge streaming stage
2. **KAPE Robustness Test**: Apply the KAPE method across multiple model families (beyond LLaMA and Qwen) and with varying activation probability thresholds to assess the stability of identified knowledge-specific neurons
3. **MHA vs. MLP Isolation**: Design experiments that separately manipulate MHA and MLP contributions (e.g., through targeted deactivation or attention masking) to conclusively establish the distinct verification role attributed to MLP layers