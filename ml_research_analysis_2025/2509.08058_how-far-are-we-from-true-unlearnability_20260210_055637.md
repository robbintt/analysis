---
ver: rpa2
title: How Far Are We from True Unlearnability?
arxiv_id: '2509.08058'
source_url: https://arxiv.org/abs/2509.08058
tags:
- parameters
- training
- unlearnable
- unlearnability
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the multi-task unlearnability of existing
  unlearnable example methods. The authors find that UEs generated by current methods
  (EM, OPS, AR) still perform well in tasks like semantic segmentation on the Taskonomy
  dataset, failing to exhibit cross-task unlearnability.
---

# How Far Are We from True Unlearnability?

## Quick Facts
- arXiv ID: 2509.08058
- Source URL: https://arxiv.org/abs/2509.08058
- Authors: Kai Ye; Liangcai Su; Chenxiong Qian
- Reference count: 24
- Key outcome: TAP shows highest UD (1.639) on CIFAR-10, while OPS shows lowest (0.157), indicating true unlearnability remains far from achieved.

## Executive Summary
This paper investigates whether existing unlearnable example (UE) methods achieve true unlearnability across multiple tasks. Through extensive experiments on CIFAR-10/100 and Taskonomy datasets, the authors demonstrate that current UE methods fail to provide cross-task unlearnability - poisoned examples still perform well in tasks like semantic segmentation despite being designed to prevent standard classification learning. To address this gap, they introduce Sharpness-Aware Learnability (SAL) to quantify parameter unlearnability and Unlearnable Distance (UD) to measure data unlearnability, providing a new framework for benchmarking UE effectiveness.

## Method Summary
The paper proposes two new metrics to quantify unlearnability: SAL measures the maximum loss change within an ε-ball around parameters, capturing local sharpness, while UD computes the ratio of learnable parameters between poisoned and clean models using a K-means threshold. The authors evaluate existing UE methods (EM, OPS, AR, REM, DC, TAP, LSP) on CIFAR-10/100 and ImageNet-100, then test multi-task unlearnability on Taskonomy. They analyze parameter trajectories during training to understand why current methods fail, finding that only critical parameters show significant differences between clean and poisoned models, and that task-specific gradient conflicts prevent consistent parameter suppression across different tasks.

## Key Results
- Existing UE methods (EM, OPS, AR) fail to exhibit cross-task unlearnability, performing well in semantic segmentation on Taskonomy
- SAL reveals that only a few critical parameters show significant differences between clean and poisoned models
- TAP achieves highest UD (1.639) on CIFAR-10, while OPS achieves lowest (0.157), showing significant variation in unlearnability strength
- Multi-task experiments show low cosine similarity between SAL vectors across tasks, indicating inconsistent parameter effects

## Why This Works (Mechanism)

### Mechanism 1
Unlearnable examples reduce parameter learnability by driving critical parameters toward flat loss regions. Perturbations shift model parameters into neighborhoods where local loss changes minimally with updates. When SAL is low, gradient steps produce negligible loss reduction - parameters effectively stall. The paper formalizes this via SAL = max_{||v||_p ≤ ε} |L(θ_l + v) - L(θ_l)|, capturing local sharpness per layer. Core assumption: Learnability requires parameter updates that meaningfully reduce loss; flat regions inhibit this. Break condition: If perturbations are removed or augmented (e.g., JPEG compression, adversarial training), SAL rises and unlearnability degrades.

### Mechanism 2
Multi-task unlearnability fails because task-specific gradient conflicts prevent consistent parameter suppression across tasks. In multi-task learning, different task losses push the same parameters in opposing directions. UEs designed for classification cannot simultaneously suppress learnability for segmentation, depth, and keypoint tasks. The paper shows low cosine similarity between SAL vectors across tasks (Figure 4), indicating inconsistent parameter effects. Core assumption: Effective UEs must suppress learnability across all tasks using the same perturbation. Break condition: If tasks are highly similar, conflicts reduce and cross-task unlearnability may partially recover.

### Mechanism 3
Learnable parameters can be identified via a SAL threshold (β) computed from clean training dynamics. K-means clustering on clean-model SAL values separates learnable vs. unlearnable parameters. The threshold β = (1/2T) Σ_t Σ_i κ_i defines the boundary. Parameters above β contribute to learning; poisoned models rapidly drop below this threshold early in training. Core assumption: Clean training establishes a reliable baseline for parameter learnability. Break condition: If clean training is unstable or uses different architectures, β may not transfer.

## Foundational Learning

- **Loss Landscape Sharpness:** Why needed here: SAL quantifies local curvature; flat regions correlate with unlearnability. Quick check question: Can you compute the maximum loss change within an ε-ball around parameters?
- **Multi-Task Gradient Conflict:** Why needed here: Explains why UEs fail when tasks compete for the same parameters. Quick check question: Do gradients from different tasks point in similar or opposing directions?
- **Bi-Level Optimization for UEs:** Why needed here: UE generation solves max_δ E_test[L(θ(δ))] where θ(δ) = argmin_θ E_train[L(x+δ, y; θ)]. Quick check question: How does perturbation δ affect the inner training loop?

## Architecture Onboarding

- **Component map:** UE Generator -> Clean/Posioned Trainer -> SAL Computer -> Threshold Estimator -> UD Calculator
- **Critical path:** Generate UEs -> Train clean and poisoned models -> Compute SAL per layer -> Derive β -> Count learnable parameters -> Compute UD
- **Design tradeoffs:** SAL search steps (default 10): more steps improve accuracy but increase cost. ε scale (default 0.05): larger ε captures broader sharpness but may overshoot local geometry. PCA dimensions for visualization: top-2 components capture >90% variance in small models; less reliable for large DNNs.
- **Failure signatures:** UD >> 1: model learns normally; UEs ineffective (e.g., TAP = 1.639 on CIFAR-10). High SAL in poisoned model: perturbations not suppressing learnability. Low task similarity in multi-task: gradient conflicts visible in cosine heatmaps.
- **First 3 experiments:** 1) Replicate Table 1: Train ResNet-18 on CIFAR-10/CIFAR-100 with EM, OPS, TAP; compute UD and compare to test accuracy. 2) Ablate SAL search: Vary ε ∈ {0.01, 0.05, 0.1} and steps ∈ {5, 10, 20}; observe UD stability. 3) Multi-task test: Train multi-head ResNet on Taskonomy with EM perturbations; plot SAL heatmaps and task cosine similarity to confirm gradient conflicts.

## Open Questions the Paper Calls Out

### Open Question 1
How can we construct perturbations that achieve robust cross-task unlearnability, given that current methods fail to generalize beyond single-task classification? Basis in paper: The authors explicitly ask, "How far are we from attaining truly unlearnable examples?" after demonstrating that current UEs fail to exhibit "cross-task unlearnability" on the Taskonomy dataset. Why unresolved: Current methods rely on shortcuts or linearly separable perturbations that conflict across different tasks (e.g., segmentation vs. classification), preventing simultaneous protection. What evidence would resolve it: A perturbation method that maintains a low Unlearnable Distance (UD) and high test loss across multiple diverse tasks within a single multi-task model.

### Open Question 2
Can the proposed Sharpness-Aware Learnability (SAL) be utilized as a direct objective function for generating stronger unlearnable examples? Basis in paper: While the paper introduces SAL to explain and benchmark existing methods, it does not explore if maximizing SAL suppression during the perturbation generation process improves results. Why unresolved: The relationship is currently only correlative (post-hoc analysis); it is untested whether explicitly optimizing for low SAL creates perturbations that are harder to defend against or more robust. What evidence would resolve it: Experiments showing that generating UEs by minimizing parameter SAL results in a higher Unlearnable Distance (UD) compared to heuristic methods like OPS or EM.

### Open Question 3
Do the findings regarding parameter convergence and SAL apply to Large Language Models (LLMs) and non-visual domains? Basis in paper: The introduction mentions the threat of unauthorized data usage in the "era of large models" (LLMs), but all empirical validation is restricted to image classification (CIFAR/ImageNet) and specific architectures (ResNet/ViT). Why unresolved: The loss landscape geometry of massive transformer models on sequential data may differ fundamentally from CNNs on images, potentially altering the effectiveness of SAL as a metric. What evidence would resolve it: Benchmarking the Unlearnable Distance (UD) of text-based unlearnable methods on LLMs to verify if low SAL correlates with unlearnability in NLP tasks.

## Limitations

- Critical hyperparameters for SAL computation (optimization algorithm, learning rate, initialization) and K-means configuration are not fully specified
- Taskonomy backbone architecture and loss weighting across tasks are not clearly defined
- Cross-dataset transferability of learnability threshold β is not tested

## Confidence

- **High Confidence:** SAL's effectiveness as a learnability metric, multi-task unlearnability failure of existing methods, UD's ability to distinguish effective from ineffective UEs
- **Medium Confidence:** Exact UD values for specific methods, quantitative ranking of UEs (e.g., TAP vs. OPS), threshold β stability across datasets
- **Low Confidence:** Cross-dataset transferability of β, SAL's behavior in extremely deep or large models, robustness of UD under different perturbation budgets

## Next Checks

1. **Hyperparameter Sensitivity:** Replicate SAL and UD computations across ε ∈ {0.01, 0.05, 0.1} and steps ∈ {5, 10, 20}; confirm UD rankings remain stable
2. **Multi-Task Architecture Verification:** Implement multi-task ResNet as described in Taskonomy experiments; verify SAL heatmap patterns and task cosine similarity align with paper
3. **Cross-Dataset β Transferability:** Train clean models on CIFAR-10 and CIFAR-100; test whether β computed on one dataset transfers to the other; measure UD degradation