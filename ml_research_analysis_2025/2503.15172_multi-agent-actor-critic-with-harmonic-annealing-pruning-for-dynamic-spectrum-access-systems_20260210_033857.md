---
ver: rpa2
title: Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum
  Access Systems
arxiv_id: '2503.15172'
source_url: https://arxiv.org/abs/2503.15172
tags:
- pruning
- networks
- access
- each
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel multi-agent actor-critic framework
  with gradual neural network pruning for distributed dynamic spectrum access (DSA).
  The method integrates magnitude-based unstructured pruning into the independent
  actor global critic paradigm and introduces a harmonic annealing sparsity scheduler
  that enables periodic weight regrowth.
---

# Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic Spectrum Access Systems

## Quick Facts
- **arXiv ID**: 2503.15172
- **Source URL**: https://arxiv.org/abs/2503.15172
- **Reference count**: 29
- **One-line result**: Proposes a multi-agent actor-critic framework with gradual neural network pruning for distributed dynamic spectrum access (DSA), integrating magnitude-based unstructured pruning into the independent actor global critic paradigm and introducing a harmonic annealing sparsity scheduler that enables periodic weight regrowth.

## Executive Summary
This paper introduces a novel approach for dynamic spectrum access (DSA) by integrating neural network pruning into a multi-agent actor-critic framework. The method employs magnitude-based unstructured pruning on actor networks while maintaining a dense global critic, using a harmonic annealing scheduler with periodic weight regrowth. Experiments demonstrate that this approach outperforms conventional DSA methods and state-of-the-art pruning techniques, achieving superior performance at high sparsities (≥90%) while significantly reducing model size for resource-constrained edge devices.

## Method Summary
The framework uses independent actor global critic (IAGC) with LSTM-based actors and a dense LSTM critic for centralized training. Magnitude-based unstructured pruning is applied to actor networks every 5 iterations using a harmonic annealing scheduler that combines cosine annealing with periodic sinusoidal oscillations. The scheduler enables periodic weight regrowth to maintain performance at high sparsities. Training uses PPO with Adam optimization, and the system is evaluated on a DSA scenario with 10 secondary users and 5 orthogonal channels, comparing against dense IAGC-PPO, DQN with parameter sharing, and Slotted Aloha baselines.

## Key Results
- The harmonic annealing scheduler achieves superior performance at high sparsities (≥90%) compared to linear and polynomial schedulers
- Best-case rewards: 1826.86 (Setup A) and 1223.41 (Setup B) at 95% sparsity, versus dense IAGC PPO's 1556.83 and 1115.74 respectively
- Pruning actors while maintaining dense critic reduces deployment footprint without sacrificing training stability
- The approach maintains strong performance while significantly reducing model size, making it suitable for resource-constrained edge devices

## Why This Works (Mechanism)

### Mechanism 1: Harmonic Annealing Scheduler
The proposed scheduler uses a cosine annealing base function with a sinusoidal oscillation term to enable periodic weight regrowth. This oscillation allows previously pruned weights to be reactivated if they become important later in training, expanding the search space and acting as an implicit regularizer. The periodic regrowth window is timed to allow recovery of critical weights that were pruned prematurely.

### Mechanism 2: Independent Actor Pruning with Global Critic
Pruning actor networks while maintaining a dense global critic reduces deployment footprint without sacrificing training stability. Each agent's actor network is independently pruned, removing low-magnitude weights, while the central critic remains dense to provide stable value estimates. This separation leverages the finding that learning value functions is more sensitive to pruning than learning policies.

### Mechanism 3: Recurrent Policies for Partial Observability
LSTM-based actor networks enable agents to infer peer strategies and channel dynamics from limited local observations. Each agent receives only a binary collision signal and SNR, and LSTMs process the sequence of past action-observation pairs to encode historical dependencies. This internal state serves as a proxy for the unobserved states of other agents.

## Foundational Learning

- **Magnitude-based Pruning**: Technique for reducing model size by setting low-magnitude weights to zero. *Why needed*: This is the fundamental technique for reducing model size. *Quick check*: Given a neural network layer's weight matrix, which weights would be set to zero first in a standard magnitude pruning step?

- **Centralized Training, Decentralized Execution (CTDE)**: Training paradigm where agents learn together but act independently. *Why needed*: The entire framework relies on the IAGC paradigm. *Quick check*: In the IAGC architecture, which network(s) are required for a single agent to select an action after training is complete?

- **Sparse Recurrent Networks**: Applying sparsity constraints to LSTM networks. *Why needed*: The paper introduces sparsity into LSTM networks. *Quick check*: If an LSTM's weights connecting the previous hidden state to the current hidden state are heavily pruned, what specific type of information might the agent lose?

## Architecture Onboarding

- **Component map**:
  - DSA Simulator -> Generates ACK/NACK and SNR for all agents
  - N Independent Actors (LSTMs) -> Each agent has policy network g_θn
  - 1 Global Critic (LSTM) -> Centralized network V_ϕ for training only
  - Pruning Engine -> Calculates sparsity and applies magnitude pruning to actors

- **Critical path**:
  1. Initialization: Define N, K, initialize dense LSTM actors and critic
  2. Training Loop: For each iteration, generate trajectories, update actors/critic using PPO, prune actors
  3. Deployment: Export only the pruned actor networks for edge execution

- **Design tradeoffs**:
  - Scheduler Complexity: Harmonic scheduler introduces two extra hyperparameters vs. linear's simplicity
  - Critic Architecture: Dense critic increases training costs to lower deployment costs
  - Pruning Frequency: Pruning every iteration is unstable; paper uses pruning every 5 iterations

- **Failure signatures**:
  - Premature Convergence: Early pruning may cause agents to converge to poor random policy
  - Critic Instability: High critic learning rate can cause value estimates to diverge
  - Mask Leakage: Pruned weights must be actually zeroed, not just masked

- **First 3 experiments**:
  1. Dense Baseline: Train and evaluate IAGC-PPO with p_final=0 to establish upper bound
  2. Scheduler Sweep: Compare linear vs. harmonic annealing schedulers at fixed high sparsity
  3. Ablation on cn_i: Run harmonic scheduler with oscillatory term removed vs. enabled

## Open Questions the Paper Calls Out

1. **Generalization to larger systems**: The authors state that "Studies with larger N values will be presented in a future journal version of this work," noting that current experiments are limited to N=10.

2. **Meta-learning initialization**: The authors "plan to investigate meta-learning techniques to bias training towards promising initialization conditions" to address high variance across random seeds.

3. **Hyperparameter sensitivity**: The specific constants used in the harmonic scheduler are not ablated, with the authors noting that "Further investigations on the pruning hyperparameters... will be provided in the journal version."

4. **Continuous action spaces**: The conclusion lists "transmit power control" as a specific target for future extensions, as the current work focuses on discrete action spaces.

## Limitations

- Implementation details such as initial sparsity level, GAE lambda parameter, and exact pruning mask handling are unspecified
- Harmonic annealing scheduler's effectiveness is validated on a specific DSA scenario with limited generalization testing
- Results may not scale to significantly larger agent/channel counts without additional tuning
- The claim that pruning is insensitive to hyperparameter tuning conflicts with the scheduler's complexity

## Confidence

- **High Confidence**: Dense IAGC-PPO baseline performance, MADRL framework architecture, and fundamental pruning mechanics
- **Medium Confidence**: Harmonic annealing scheduler's superiority over monotonic schedulers at high sparsities
- **Low Confidence**: Generalization of results to different DSA configurations and scalability to larger agent/channel counts

## Next Checks

1. **Scheduler Ablation Study**: Implement and compare harmonic annealing with linear annealing, polynomial annealing, and harmonic annealing without the oscillatory component at 95% target sparsity.

2. **Generalization Across Configurations**: Test the 95%-sparse harmonic-annealed model on DSA environments with doubled agent counts (N=20) and channels (K=10).

3. **Pruning Frequency Sensitivity**: Vary the pruning interval (iprune=1, 5, 10, 20) while keeping harmonic annealing at 95% sparsity to determine the stability-accuracy tradeoff.