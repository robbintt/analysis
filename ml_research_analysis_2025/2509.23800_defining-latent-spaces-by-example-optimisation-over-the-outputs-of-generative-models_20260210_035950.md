---
ver: rpa2
title: 'Defining latent spaces by example: optimisation over the outputs of generative
  models'
arxiv_id: '2509.23800'
source_url: https://arxiv.org/abs/2509.23800
tags:
- latent
- surrogate
- space
- optimisation
- spaces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces surrogate latent spaces, a general framework\
  \ enabling black-box optimisation over the outputs of diffusion and flow-matching\
  \ models. The key insight is constructing low-dimensional, Euclidean embeddings\
  \ from a generative model\u2019s latent space using a small set of example latents\
  \ (\u201Cseeds\u201D) as coordinate axes."
---

# Defining latent spaces by example: optimisation over the outputs of generative models

## Quick Facts
- arXiv ID: 2509.23800
- Source URL: https://arxiv.org/abs/2509.23800
- Reference count: 33
- Primary result: Introduces surrogate latent spaces for black-box optimisation over diffusion/flow-matching models

## Executive Summary
This paper presents surrogate latent spaces, a framework for applying standard optimisation algorithms to the outputs of diffusion and flow-matching models. The key innovation is constructing low-dimensional Euclidean spaces from a small set of example latents ("seeds"), enabling efficient search over high-dimensional generative model outputs. The method works across diverse modalities including images, audio, video, and proteins, providing interpretable control through example-based coordinate systems.

## Method Summary
The method constructs a (K-1)-dimensional surrogate space U=[0,1]^(K-1) from K seed latents using a weight chart (KR preferred) that maps to the positive orthant of the unit hypersphere. Each point in U defines a weighted linear combination of seeds, which is then mapped through transport functions to produce valid model latents. Standard optimisation algorithms like CMA-ES or Bayesian optimisation can then search in this low-dimensional space. The approach requires deterministic generation (DDIM or flow matching) to maintain bijectivity between latents and outputs.

## Key Results
- Surrogate spaces enable CMA-ES to find high-scoring, diverse outputs where direct optimisation in full latent space fails
- In protein design with RFDiffusion, surrogate spaces increased successful generations more than ten-fold compared to standard sampling
- The method works across diverse generative models including images (FLUX, HunyuanVideo), audio (AudioLDM), and proteins (RFDiffusion)
- Random seeds already provide substantial improvements over baseline sampling, with task-specific seeds offering additional gains

## Why This Works (Mechanism)

### Mechanism 1
Linear combinations of latent vectors under unit-L2 weights preserve distribution membership when mapped through an appropriate transport function. The paper transforms seed latents to an "inner latent" distribution that is zero-mean, rotationally invariant, and closed under aggregation. Linear combinations with ||w||₂ = 1 produce valid latents when transported back via T←.

### Mechanism 2
Restricting weights to the positive orthant of the unit hypersphere preserves approximate stationarity between Euclidean distance in U and output similarity. The weight chart φw maps [0,1]^(K-1) to S^+_(K-1). In high latent dimensions, cosine similarity between outputs reduces to the dot product of weight vectors.

### Mechanism 3
Seeds define interpretable coordinate axes that bound the searchable solution space to regions containing outputs similar to the seeds. Each seed contributes to the linear combination; positive weights induce positive similarity to seed outputs. The K seeds span a (K-1)-dimensional manifold in the full latent space, restricting exploration while preserving diversity within that region.

## Foundational Learning

- **Deterministic sampling in diffusion/flow matching**: Required for bidirectional mapping between latents and outputs. Quick check: Can you explain why standard stochastic DDPM sampling would not support this approach?
- **Cosine similarity in latent spaces**: Used as proxy for semantic similarity in the stationarity principle. Quick check: What assumption about the latent distribution makes cosine similarity meaningful?
- **Black-box optimization algorithms**: Target algorithms the surrogate spaces support. Quick check: Why does CMA-ES fail when applied directly to the full latent space Z?

## Architecture Onboarding

- **Component map**: Seed selection → Transport maps → Weight chart → LOL combination → Generator → Objective
- **Critical path**: u → φw(u) → w → l(w, {z_k}) → z → g(z) → x → f(x)
- **Design tradeoffs**: Higher K increases expressiveness but raises optimization difficulty; KR chart preserves stationarity better at high K; task-specific seeds improve results but require prior knowledge
- **Failure signatures**: Black/degenerate outputs indicate optimizer left model support; low diversity suggests similar seeds; optimization stagnation suggests K too high for evaluation budget
- **First 3 experiments**: 1) 2D visualization with 3 seeds to confirm smooth structure, 2) Seed quality ablation comparing random vs top-scoring seeds, 3) Optimizer comparison (CMA-ES, BO, random search) in same surrogate space

## Open Questions the Paper Calls Out

- **Predicting seed utility**: Developing techniques for predicting the utility of individual seed choices before optimization begins
- **Adaptive seed selection**: Integrating adaptive seed-selection strategies to refine the surrogate space dynamically during optimization
- **Stochastic sampling extension**: Extending the framework to stochastic sampling processes where the latent-to-output mapping is not bijective

## Limitations

- Assumes meaningful linear combinations exist in latent space; cannot handle distributions that resist transport to rotationally invariant forms
- Requires deterministic inverse mapping from outputs to latents, excluding purely stochastic generation models
- Protein sequence length extension claims rely on seed selection rather than demonstrating systematic extension capability

## Confidence

- **High confidence**: Surrogate spaces enable standard optimization algorithms to work in low-dimensional Euclidean spaces
- **Medium confidence**: The method generalizes across diverse generative models with consistent improvements
- **Low confidence**: Ten-fold improvement in protein design success rates and generation of previously infeasible longer proteins

## Next Checks

1. **Distribution closure test**: Verify linear combinations of seed latents remain within model's support by sampling and decoding multiple combinations
2. **Stationarity preservation measurement**: Quantify correlation between Euclidean distance in U and output similarity across entire surrogate space
3. **Low-dimensional stress test**: Apply method to 2D latent space (D=2, K=3) and measure approximation quality of w_i^T w_j ≈ cosine similarity