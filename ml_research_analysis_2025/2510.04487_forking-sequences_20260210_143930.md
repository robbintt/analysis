---
ver: rpa2
title: Forking-Sequences
arxiv_id: '2510.04487'
source_url: https://arxiv.org/abs/2510.04487
tags:
- forecast
- encoder
- forking-sequences
- forecasting
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces forking-sequences, a neural network architectural
  design that improves forecast stability across forecast creation dates while maintaining
  or improving accuracy. The key idea is to jointly encode and decode the entire time
  series across all forecast creation dates in a single forward pass, enabling efficient
  ensemble forecasting across multiple dates.
---

# Forking-Sequences

## Quick Facts
- arXiv ID: 2510.04487
- Source URL: https://arxiv.org/abs/2510.04487
- Reference count: 40
- Key outcome: Neural network architecture that improves forecast stability across forecast creation dates while maintaining or improving accuracy

## Executive Summary
Forking-sequences is a neural network architectural design that addresses forecast stability in multi-horizon time series forecasting. The key innovation is jointly encoding and decoding the entire time series across all forecast creation dates in a single forward pass, enabling efficient ensemble forecasting across multiple dates. This approach reduces gradient variance, preserves gradients for early time steps, and achieves computational efficiency by reusing encoder computations. The method shows median accuracy improvements of up to 49.3% across different architectures while improving forecast stability by up to 13.2% without degrading accuracy.

## Method Summary
The method processes all forecast creation dates (FCDs) simultaneously by sharing a single encoder across all positions and attaching individual decoders at each FCD position. During training, it computes losses across all T FCDs in one forward pass, averaging gradient samples to reduce variance (O(1/T)). At inference, it ensembles overlapping forecasts across FCDs using exponential smoothing (α=0.9 recommended). The approach supports multiple architectures (MLP, RNN, LSTM, CNN, Transformer, State Space) and uses multi-quantile loss for probabilistic forecasting. The design enables efficient computation by reusing encoder outputs and provides theoretical guarantees for gradient variance reduction and preservation.

## Key Results
- Median accuracy improvements of up to 49.3% across different architectures
- Forecast ensembling during inference improves median forecast stability by up to 13.2% without degrading accuracy
- Theoretical analysis shows gradient variance reduction O(1/T) with the number of forecast creation dates
- Preserves gradients for early time steps in recurrent architectures, mitigating vanishing gradient problems

## Why This Works (Mechanism)

### Mechanism 1: Gradient Variance Reduction
Forking-sequences reduces stochastic gradient variance linearly with the number of forecast creation dates (O(1/T)). Instead of sampling a single FCD per batch, it computes losses across all T FCDs in one forward pass, averaging T gradient samples. Under M-dependence (short-range correlation), this acts like mini-batch variance reduction. If FCDs exhibit long-range correlation or T is very small, variance reduction may be weak.

### Mechanism 2: Gradient Preservation
Forking-sequences mitigates vanishing gradients in recurrent encoders by providing multiple shorter gradient paths to early timesteps. In LSTM/RNN window-sampling, gradients from the final FCD decay exponentially. Forking-sequences decodes at every FCD, creating T gradient paths that sum terms, preserving earlier signals. Architectures without sequential bottlenecks (e.g., Transformers) see limited gradient benefit.

### Mechanism 3: Ensemble Forecasting
Forking-sequences naturally produces T overlapping predictions per target date. Exponential smoothing (α=0.9) weights recent FCDs more, reducing volatility while preserving accuracy. Over-smoothing (low α) may suppress accuracy-improving revisions.

## Foundational Learning

### Concept: Multi-horizon forecast revisions and overlapping targets
- Why needed here: Stability is defined across FCDs because the same target is predicted multiple times; understanding overlap is key to ensembling and sEV
- Quick check question: If you forecast 12 steps ahead daily, how many times is day D+6 predicted between origins D and D+5?

### Concept: M-dependence in time series
- Why needed here: The variance-reduction proof assumes gradient samples are correlated only within M steps
- Quick check question: What happens to gradient variance if FCDs are perfectly correlated?

### Concept: Quantile loss (QL)
- Why needed here: Training uses multi-QL; sEV is defined using QL properties
- Quick check question: For q=0.9, does QL penalize overprediction or underprediction more heavily?

## Architecture Onboarding

### Component map:
Encoder -> Hidden states h[t][h] for all FCDs -> Decoders (one per FCD) -> Forecasts Ŷ[t][h] -> Loss aggregator -> Training/inference

### Critical path:
1. Input time series Y[t] of length T
2. Encoder forward pass → h[t][h] for t=1..T
3. Decoders generate Ŷ[t][h] for all t, h
4. Training: Aggregate QL across FCDs; backprop with variance-reduced gradients
5. Inference: Ensemble overlapping forecasts per target date

### Design tradeoffs:
- Memory vs. efficiency: Storing h[t][h] for all FCDs increases memory but avoids O(T²) encoder recomputations
- α in ensembling: High α preserves accuracy; low α improves stability but may over-smooth
- Encoder choice: LSTM/RNN benefit more from gradient preservation; Transformers see smaller gains

### Failure signatures:
- Exploding memory on high-frequency data (long T)
- No stability improvement: Check if ensembling is applied; verify α is not too high
- Accuracy drop: α may be too low; recent FCDs underweighted

### First 3 experiments:
1. Validate gradient variance reduction: Train a small LSTM on synthetic AR(1) data with forking-sequences vs. window-sampling; plot gradient variance across steps
2. Ablate ensembling α: On M4 hourly, compare sEV and sCRPS for α ∈ {0.1, 0.5, 0.9, 1.0} to find accuracy-stability sweet spot
3. Cross-architecture comparison: Train MLP, LSTM, Transformer with forking-sequences on M3 monthly; report sCRPS and sEV to verify architecture-dependent benefits

## Open Questions the Paper Calls Out

### Open Question 1
Does the forking-sequences design retain its theoretical and empirical benefits when applied to multivariate time series forecasting? The authors explicitly state the study focused only on univariate time series forecasting and leave "multivariate extensions as a promising direction for future work." Empirical validation on standard multivariate benchmarks showing maintained or improved accuracy (sCRPS) and stability (sEV) compared to window-sampling baselines would resolve this.

### Open Question 2
Can incorporating forecast ensembling directly into the training phase improve the stability-accuracy trade-off? The paper notes that ensembling is currently an inference-time technique, and the authors "defer the exploration of incorporating forecast ensembling directly into the training phase to future work." Ablation studies comparing models trained with ensemble-aware loss functions against the standard forking-sequences training scheme would provide evidence.

### Open Question 3
Do the benefits of forking-sequences extend to foundation models trained on datasets where series vary widely in scale and length? The authors ask whether the benefits extend to scenarios where "series vary widely in scale and length," noting the current analysis was restricted to frequency-specialized models. Experiments on a cross-frequency foundation model (e.g., Chronos or similar) evaluating if stability gains persist across a heterogeneous corpus of series would resolve this.

## Limitations

- Memory requirements scale linearly with T (number of FCDs), potentially limiting application to high-frequency data with many FCDs
- Architecture-specific benefits are not fully characterized - while LSTM/RNN show clear gradient preservation benefits, the magnitude of improvements for Transformers and CNNs is less clear
- Gradient variance reduction analysis assumes M-dependence and stationarity, which may not hold for real-world FCDs with strong seasonality or long-range dependence

## Confidence

- **High confidence:** Accuracy improvements (sCRPS/MAE reductions) are well-documented across 16 datasets and multiple architectures
- **Medium confidence:** Gradient variance reduction mechanism is theoretically sound but empirical validation beyond synthetic data is limited
- **Medium confidence:** Forecast stability improvements via ensembling are demonstrated, but the tradeoff between α and accuracy-stability balance needs more systematic exploration

## Next Checks

1. Test gradient variance reduction on real-world datasets with known correlation structures (e.g., hourly electricity data with strong daily patterns)
2. Evaluate memory scaling empirically by training on increasingly long time series (hourly data with thousands of FCDs) to identify practical limits
3. Conduct systematic ablation studies varying α across a wider range and different dataset characteristics to map the accuracy-stability Pareto frontier