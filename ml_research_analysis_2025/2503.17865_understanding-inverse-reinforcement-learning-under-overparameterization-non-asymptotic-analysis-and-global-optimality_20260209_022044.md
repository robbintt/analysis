---
ver: rpa2
title: 'Understanding Inverse Reinforcement Learning under Overparameterization: Non-Asymptotic
  Analysis and Global Optimality'
arxiv_id: '2503.17865'
source_url: https://arxiv.org/abs/2503.17865
tags:
- reward
- policy
- qsoft
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inverse reinforcement learning
  (IRL) when the reward function is parameterized by neural networks, a scenario where
  prior theoretical guarantees typically fail. The authors propose a two-timescale
  single-loop IRL algorithm that combines finite-step neural soft Q-learning with
  reward parameter updates using gradient ascent.
---

# Understanding Inverse Reinforcement Learning under Overparameterization: Non-Asymptotic Analysis and Global Optimality

## Quick Facts
- arXiv ID: 2503.17865
- Source URL: https://arxiv.org/abs/2503.17865
- Reference count: 40
- Primary result: First non-asymptotic convergence guarantee for IRL with neural network parameterized rewards, achieving O(ε⁻⁸) sample complexity

## Executive Summary
This paper addresses the challenge of inverse reinforcement learning (IRL) when the reward function is parameterized by neural networks, a scenario where prior theoretical guarantees typically fail. The authors propose a two-timescale single-loop IRL algorithm that combines finite-step neural soft Q-learning with reward parameter updates using gradient ascent. The algorithm avoids the computational inefficiency of nested-loop approaches while maintaining global convergence guarantees. The key theoretical contribution is showing that the algorithm achieves O(ε⁻⁸) sample complexity to identify an ε-approximate globally optimal reward estimator under overparameterization.

## Method Summary
The method employs a two-timescale single-loop algorithm where the lower-level policy optimization (via soft Q-learning) uses a faster stepsize than the upper-level reward updates (via gradient ascent). Two-layer ReLU neural networks parameterize both the reward function and soft Q-function, with overparameterization enabling approximate concavity in the likelihood objective. The algorithm alternates between TD updates for policy evaluation and gradient-based updates for reward parameter estimation, using expert demonstrations to guide learning. The key insight is that with sufficient network width, the likelihood objective becomes approximately concave, enabling global optimality despite the inherent nonconvexity of neural network training.

## Key Results
- Achieves O(ε⁻⁸) sample complexity for identifying ε-approximate globally optimal reward and policy
- First non-asymptotic convergence guarantee for IRL with neural network parameterized rewards
- Superior performance on Mujoco tasks compared to existing IRL and imitation learning algorithms, particularly in limited data regimes
- Demonstrates that overparameterization enables global optimality despite neural network nonconvexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-timescale stepsize separation enables single-loop convergence by keeping policy updates faster than reward updates.
- Mechanism: TD updates use stepsize η = min{K^{-3/4}, (1-γ)/8} while reward updates use α = α_0/K^σ. This timescale gap ensures the lower-level policy optimization tracks the current reward parameter closely, approximating the bi-level structure without nested loops.
- Core assumption: Assumption 1 (ergodicity with geometric mixing rate) ensures the Markov chain stabilizes fast enough for TD updates to make progress within each outer iteration.
- Evidence anchors: [abstract] "combines finite-step neural soft Q-learning with reward parameter updates... avoids the computational inefficiency of nested-loop approaches", [Section 4.3] "By carefully selecting two distinct stepsizes, the lower-level policy optimization converges more quickly than the upper-level reward updates"
- Break condition: If η is too large relative to α, policy estimates lag behind reward changes, breaking the alignment that underpins convergence guarantees.

### Mechanism 2
- Claim: Overparameterization (m → ∞) makes the likelihood objective approximately concave in reward parameters, enabling global optimality despite neural network nonconvexity.
- Mechanism: Under infinite width, the two-layer ReLU network admits a local linearization at initialization. Theorem 3 shows the max-min objective ˆL(θ,π) becomes concave in θ up to O(m^{-1/2}) error. This converts the nonconvex problem into one where any stationary point is globally optimal.
- Core assumption: Assumption 2 (regularity of policy) ensures the soft Bellman operator is a contraction, guaranteeing uniqueness of the approximate stationary point W*_θ.
- Evidence anchors: [abstract] "show that our algorithm can identify the globally optimal reward and policy under certain neural network structures", [Section 5.3, Theorem 3] "ˆL(θ,π) is concave in the reward parameter θ as the width m of our neural network tends to infinity"
- Break condition: Insufficient network width makes linearization error O(m^{-1/2}) large enough to destroy approximate concavity; convergence may stall at local optima.

### Mechanism 3
- Claim: The maximum likelihood IRL formulation with entropy regularization yields a tractable gradient estimator via expert-agent trajectory comparison.
- Mechanism: The gradient ∇ˆL(θ) decomposes as E_τE[Σ γ^t ∇θ r̂(s_t,a_t;θ)] − E_τA[Σ γ^t ∇θ r̂(s_t,a_t;θ)] (Lemma 4). Algorithm 3 samples one expert and one agent trajectory per iteration to estimate this difference, then performs gradient ascent on reward parameters.
- Core assumption: Assumption 3 (regularity of stationary distribution) bounds density of state-action pairs, ensuring gradient estimates have bounded variance.
- Evidence anchors: [Section 3.2] Problem formulation (5)-(6) defining the bi-level ML-IRL structure, [Section 5, Lemma 4] "The gradient of the likelihood objective ˆL(θ) can be equivalently expressed as [difference of expert and agent expectations]"
- Break condition: If π_k diverges significantly from π_θ_k, the agent trajectory expectation fails to match the lower-level optimal policy, biasing gradient estimates.

## Foundational Learning

- Concept: Entropy-regularized RL (soft Q-learning / soft actor-critic)
  - Why needed here: The entire ML-IRL formulation relies on soft Bellman operators and softmax policies. Without understanding V_soft, Q_soft, and the entropy term H(π), the lower-level optimization is opaque.
  - Quick check question: Can you explain why π(·|s) ∝ exp(Q_soft(s,·)) is the optimal policy under entropy regularization?

- Concept: Bi-level optimization with stochastic approximation
  - Why needed here: The IRL problem is inherently bi-level (reward maximization upper level, policy optimization lower level). Two-timescale SA is the key tool enabling single-loop solutions.
  - Quick check question: What happens if both levels use the same stepsize in a bi-level problem?

- Concept: Neural tangent kernel / local linearization regime
  - Why needed here: All convergence guarantees depend on the network being wide enough that it behaves like its linearization at initialization. Understanding when this approximation holds is critical for practical deployment.
  - Quick check question: As network width increases, does the NTK-based analysis become more or less accurate?

## Architecture Onboarding

- Component map: Reward network r̂(·;θ) -> Q-network Q̂(·;W) -> Policy π_k (softmax) -> Agent trajectories -> Gradient estimator -> Reward update θ

- Critical path: Sample (s,a,r,s') -> compute Bellman residual δ -> TD update W -> compute softmax Q -> policy improvement -> sample expert/agent trajectories -> estimate gradient g -> reward update θ. The TD update must produce a Q-function accurate enough that π_{k+1} tracks π_{θ_k}.

- Design tradeoffs:
  - Width m vs error: Larger m reduces linearization error O(m^{-1/2}) but increases memory/compute
  - Stepsize ratio η/α: Larger ratio improves convergence rate but requires more careful tuning; η ≈ K^{-3/4}, α ≈ K^{-σ} with σ < 1
  - Projection radius B: Must be large enough to contain optimal W* but small enough to keep iterates near initialization

- Failure signatures:
  - Policy divergence: ||log π_{k+1} − log π_{θ_k}||_{∞} grows rather than shrinks -> η too large or m too small
  - Reward oscillation: θ_k cycles without convergence -> α too large relative to η
  - Gradient explosion: ||g_k|| unbounded -> check data sampling and Assumption 3 violations

- First 3 experiments:
  1. Sanity check on tabular environment: Implement with small state/action spaces, verify convergence to known optimal reward given sufficient expert data.
  2. Width ablation on Mujoco: Test m ∈ {64, 256, 1024, 4096} on HalfCheetah with single expert trajectory; plot final reward vs m to validate O(m^{-1/2}) error scaling.
  3. Stepsize sensitivity: Grid search η ∈ {K^{-1/2}, K^{-3/4}, K^{-1}} and α ∈ {K^{-1/4}, K^{-1/8}, K^{-1}}; identify regimes where Theorem 2 predictions hold vs break.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the global convergence and optimality guarantees be extended to neural networks with finite width, removing the dependency on the overparameterized regime where width $m \to \infty$?
- Basis in paper: [explicit] The Conclusion states: "A limitation of our method is the requirement of a sufficiently large width of the parameterization neural network, so one future direction of this work is to further extend our theoretical analysis to the finite-width case."
- Why unresolved: The theoretical proofs rely on the local linearization property of the neural network which holds asymptotically as width approaches infinity; finite-width networks introduce non-convexities that the current analysis does not address.
- What evidence would resolve it: A proof of convergence for fixed width $m$ showing explicit dependence of the convergence rate on $m$, or demonstrating that the optimality gap decreases as $m$ increases even for finite $m$.

### Open Question 2
- Question: How can the theoretical framework be adapted to provide rigorous global convergence guarantees for environments with continuous action spaces?
- Basis in paper: [inferred] Section 3 (Preliminaries) explicitly states: "We consider $\mathcal{S}$ to be continuous and $\mathcal{A}$ to be finite." However, the numerical experiments in Section 6 are conducted on Mujoco tasks, which possess continuous action spaces, creating a gap between the theoretical setting and practical application.
- Why unresolved: The analysis relies on a finite action space to define the softmax operator and policy improvement steps (e.g., Eq. 3, Assumption 2) in a way that avoids infinite-dimensional measure theory issues.
- What evidence would resolve it: Theoretical extension of Theorems 1 and 2 to continuous action spaces, likely requiring new assumptions regarding the smoothness of the policy or soft Q-function with respect to action inputs.

### Open Question 3
- Question: Is the $O(\epsilon^{-8})$ sample complexity bound tight, or can the iteration complexity for the two-timescale single-loop algorithm be improved to match the $O(\epsilon^{-4})$ or better rates seen in other bi-level optimization settings?
- Basis in paper: [inferred] Theorem 2 establishes a sample complexity of $O(\epsilon^{-8})$ for the single-loop algorithm. While the paper notes this avoids the $O(K^2)$ cost of nested loops, the exponent 8 is relatively high compared to standard RL convergence rates, suggesting potential inefficiency in the coupling of TD updates and reward updates.
- Why unresolved: The analysis aggregates errors from stochastic sampling, policy evaluation approximation, and the neural network linearization, leading to a conservative bound; it is unclear if the fundamental difficulty of the problem or the proof technique drives the rate.
- What evidence would resolve it: A refined analysis achieving a lower sample complexity bound or lower bound proof demonstrating that $O(\epsilon^{-8})$ is necessary for this class of two-timescale IRL algorithms.

## Limitations
- Requires infinite network width for theoretical guarantees, limiting practical applicability
- Assumes fixed expert demonstrations rather than streaming data
- High sample complexity bound O(ε⁻⁸) suggests computational inefficiency
- Theoretical framework assumes finite action spaces while experiments use continuous actions

## Confidence
- Global optimality under overparameterization: High
- Two-timescale convergence analysis: High
- Practical performance on continuous control: Medium
- Finite-width generalization: Low

## Next Checks
1. Width sensitivity analysis: Vary network width m and measure empirical convergence rate and final reward quality to validate the O(m^{-1/2}) error scaling prediction.
2. Stepsize robustness: Systematically test the η/α ratio across different values to identify the stability boundaries predicted by the two-timescale analysis.
3. Multi-expert generalization: Evaluate performance when expert demonstrations come from multiple trajectories with varying quality, testing the algorithm's robustness to expert noise beyond the single-trajectory assumption.