---
ver: rpa2
title: Automated scoring of the Ambiguous Intentions Hostility Questionnaire using
  fine-tuned large language models
arxiv_id: '2508.10007'
source_url: https://arxiv.org/abs/2508.10007
tags:
- aihq
- scenarios
- ratings
- hostility
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that fine-tuned large language models can
  reliably automate scoring of open-ended responses in the Ambiguous Intentions Hostility
  Questionnaire (AIHQ), a key tool for measuring hostile attribution bias. Using data
  from individuals with traumatic brain injury and healthy controls, the models achieved
  correlations with human ratings exceeding 0.85 for both hostility attributions and
  aggression responses, with fine-tuned models outperforming pre-trained versions.
---

# Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models

## Quick Facts
- arXiv ID: 2508.10007
- Source URL: https://arxiv.org/abs/2508.10007
- Reference count: 0
- Primary result: Fine-tuned LLMs achieved correlations exceeding 0.85 with human ratings for AIHQ scoring, reducing labor while maintaining reliability

## Executive Summary
This study demonstrates that fine-tuned large language models can reliably automate scoring of open-ended responses in the Ambiguous Intentions Hostility Questionnaire (AIHQ), a key tool for measuring hostile attribution bias. Using data from individuals with traumatic brain injury and healthy controls, the models achieved correlations with human ratings exceeding 0.85 for both hostility attributions and aggression responses, with fine-tuned models outperforming pre-trained versions. The models accurately reproduced group differences between TBI and healthy control participants and generalized well to an independent nonclinical dataset. These results indicate that LLM-based automated scoring can significantly reduce the time and labor required for AIHQ assessment while maintaining reliability and validity, offering a scalable solution for research and clinical applications.

## Method Summary
The study employed two large language models (GPT-3.5-Turbo and Flan-T5-Large) to automate scoring of AIHQ open-ended responses. Models were fine-tuned on 84 participants (42 TBI + 42 HC) and validated on an independent test set of 86 participants. Human ratings from two trained raters served as ground truth, with inter-rater reliability of ICC=0.80 (hostility) and ICC=0.73 (aggression). The models generated 1-5 ratings for both hostility attribution and aggression response categories across 15 AIHQ scenarios. Performance was evaluated using Pearson correlations between model and human ratings, with additional validation on a separate undergraduate sample.

## Key Results
- Fine-tuned models achieved correlations exceeding 0.85 with human ratings (GPT-3.5-Turbo: r=0.934 for hostility, r=0.962 for aggression)
- Models accurately reproduced group differences between TBI and healthy control participants
- Fine-tuned models generalized well to independent nonclinical dataset, maintaining r>0.75 correlations
- Fine-tuned models outperformed pre-trained versions across all metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on human-rated AIHQ responses produces scoring outputs that align with expert human judgment at inter-rater reliability levels.
- Mechanism: Supervised fine-tuning adjusts model weights to minimize divergence between predicted ratings and human-assigned scores, causing the model to internalize the latent scoring rubric used by trained raters.
- Core assumption: The human raters' scoring patterns are consistent and reflect a learnable latent construct of "hostility" and "aggression" that can be approximated via text patterns.
- Evidence anchors: Table 3 shows fine-tuned GPT-3.5-Turbo achieved r=0.934 (hostility) and r=0.962 (aggression), comparable to human inter-rater reliability of r=0.879 and r=0.929.

### Mechanism 2
- Claim: Pre-trained LLMs possess sufficient semantic understanding of social situations to produce meaningful hostility ratings even without domain-specific training, though fine-tuning substantially improves performance.
- Mechanism: Pre-training on large text corpora enables the model to understand nuanced social narratives, detect hostile intent, and map these to rating scales; fine-tuning refines this mapping to specific scoring conventions.
- Core assumption: The semantic features relevant to hostility attribution are present in general pre-training data and can be transferred to this specialized task.
- Evidence anchors: Pre-trained GPT-3.5-Turbo achieved r=0.840 (hostility) and r=0.889 (aggression); pre-trained Flan-T5-Large showed lower alignment (r=0.476 for hostility).

### Mechanism 3
- Claim: Fine-tuned models generalize across populations and rater groups, preserving construct validity when applied to unseen datasets.
- Mechanism: The model learns generalizable features of hostile attribution (semantic content, situational framing, aggressive response language) rather than dataset-specific artifacts, enabling cross-sample transfer.
- Core assumption: The underlying construct of hostile attribution bias is sufficiently stable across populations that features learned from one group transfer to another.
- Evidence anchors: Figure 4 shows fine-tuned models maintained r>0.75 correlations on the undergraduate dataset; however, correlations were approximately 0.10 lower than the original test set.

## Foundational Learning

- Concept: Hostile Attribution Bias
  - Why needed here: This is the psychological construct being measured; understanding that it represents a tendency to interpret ambiguous social situations as hostile is essential for interpreting what the model scores mean.
  - Quick check question: If a participant interprets a scenario as "they probably didn't see me there" vs. "they did it on purpose to disrespect me," which reflects higher hostile attribution bias?

- Concept: Fine-tuning vs. In-Context Learning
  - Why needed here: The paper uses both approaches differently for each model; understanding this distinction clarifies why Flan-T5-Large required fine-tuning while GPT-3.5-Turbo performed reasonably with prompting alone.
  - Quick check question: What is the practical difference between updating model weights (fine-tuning) vs. providing examples in the prompt (in-context learning) for a classification task?

- Concept: Inter-rater Reliability as a Performance Ceiling
  - Why needed here: Model performance is benchmarked against human inter-rater reliability; if humans disagree (ICC < 0.80), model alignment cannot meaningfully exceed this level.
  - Quick check question: If human raters have ICC=0.73 for aggression responses, what does this imply about the maximum achievable model-human correlation?

## Architecture Onboarding

- Component map:
  - Input Layer (CSV file) -> Prompt Engineering (Standardized prompts) -> Model Layer (GPT-3.5-Turbo or Flan-T5-Large) -> Output Layer (1-5 numeric rating) -> Aggregation (Subscale means)

- Critical path:
  1. Format input data (CSV with required columns)
  2. Select model based on data sensitivity and infrastructure constraints
  3. Run inference with temperature=0 for deterministic outputs
  4. Validate output format (single numeric values)
  5. Aggregate scores and compute subscale means

- Design tradeoffs:
  - GPT-3.5-Turbo: Faster setup, higher base performance, but requires API access, incurs costs, and transmits data to external servers (privacy concern)
  - Flan-T5-Large: Fully local, no data transmission, freely shareable fine-tuned version, but requires local GPU/CPU setup and slightly lower correlations in some conditions
  - Cloud (Colab) vs. Local: Colab requires no installation but sends data to Google; local interface is most secure but requires technical setup

- Failure signatures:
  - Non-numeric or multi-token outputs (mitigated by max_tokens=10 and prompt constraints)
  - Missing responses or empty text fields
  - Correlation drops >0.10 compared to training benchmarks may indicate population shift
  - Lower agreement on ambiguous scenarios vs. intentional/accidental (observed in Dataset 2) suggests inherent task difficulty rather than model failure

- First 3 experiments:
  1. Baseline validation: Run both models on a small held-out subset (n=20-30) from your target population, compute correlations against human raters to establish expected performance levels.
  2. Cross-population stress test: Apply the fine-tuned Flan-T5-Large model to a demographically different sample, measure correlation degradation to assess generalization bounds.
  3. Prompt sensitivity analysis: Test whether modifying prompt wording (e.g., different anchor examples) changes model outputs systematically, to understand whether the model learned the construct or the specific prompt-response mapping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based AIHQ scoring models maintain high accuracy when applied to culturally diverse populations outside the United States?
- Basis in paper: Authors state "Future work should explore fine-tuning on larger, more culturally diverse datasets to enhance generalizability" and cite cross-cultural research showing different hostility attribution patterns across Polish, Japanese, and U.S. participants.
- Why unresolved: Current models were fine-tuned on U.S.-based data with American rater expectations, potentially limiting validity across cultural contexts.
- What evidence would resolve it: Testing fine-tuned models on AIHQ responses from diverse cultural samples with locally-trained human raters, comparing correlation coefficients across populations.

### Open Question 2
- Question: Can LLMs generate multidimensional psychological outputs from AIHQ responses (e.g., proactive vs. reactive aggression, specific emotional themes) that outperform single numerical ratings in predictive validity?
- Basis in paper: Authors state "Future research could investigate whether generating more detailed and multidimensional outputs... may enable more comprehensive psychological assessment and enhance the theoretical utility of the AIHQ."
- Why unresolved: Current approach reduces nuanced language to single scores, potentially losing clinically relevant information about aggression subtypes or cognitive-emotional themes.
- What evidence would resolve it: Developing multi-output models and validating against external criteria (e.g., behavioral observations, treatment outcomes) to compare with single-score approaches.

## Limitations

- Model generalizability is limited by training on relatively homogeneous populations (TBI and healthy controls from same geographic region)
- Performance ceiling is constrained by human inter-rater reliability, which varies by rater training and may not capture the true construct
- The ~0.10 correlation drop on new datasets suggests sensitivity to population differences, though systematic testing across diverse populations was not conducted

## Confidence

- **High Confidence**: Core finding that fine-tuned LLMs achieve correlations exceeding 0.85 with human ratings is well-supported by direct empirical evidence and aligns with established benchmarks
- **Medium Confidence**: Claims about clinical utility and scalability are reasonable but lack direct validation in real-world settings
- **Low Confidence**: Assertion that models "accurately reproduced group differences" is somewhat circular since models were trained on these same groups

## Next Checks

1. **Cross-Cultural Validation**: Test the fine-tuned models on AIHQ data from populations with different cultural backgrounds (e.g., East Asian vs. Western participants) to assess whether the hostility attribution construct transfers across cultural contexts where social norms around conflict resolution differ substantially.

2. **Rater Training Comparison**: Compare model outputs against AIHQ ratings from raters with varying levels of training and experience to determine whether the models replicate expert scoring patterns or capture a more generalizable construct that could be useful even with less-trained human raters.

3. **Longitudinal Stability Assessment**: Apply the models to AIHQ data collected from the same participants at multiple time points to evaluate whether the automated scoring captures stable individual differences in hostile attribution bias or merely reflects situational variability in response patterns.