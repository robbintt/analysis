---
ver: rpa2
title: 'TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language
  Models'
arxiv_id: '2509.25143'
source_url: https://arxiv.org/abs/2509.25143
tags:
- image
- report
- retrieval
- medical
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TemMed-Bench, the first benchmark for evaluating
  temporal medical image reasoning in vision-language models. It addresses the gap
  in current benchmarks that focus on single-visit image analysis, while real-world
  clinical practice requires tracking changes over time.
---

# TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models

## Quick Facts
- arXiv ID: 2509.25143
- Source URL: https://arxiv.org/abs/2509.25143
- Reference count: 32
- Primary result: Most LVLMs perform at random-guessing levels on temporal medical image reasoning tasks

## Executive Summary
This paper introduces TemMed-Bench, the first benchmark specifically designed to evaluate temporal medical image reasoning in vision-language models. Unlike existing benchmarks that focus on single-visit image analysis, TemMed-Bench addresses the real-world clinical need to track changes across multiple patient visits. The benchmark consists of three tasks—visual question answering, report generation, and image-pair selection—using paired images from different clinical visits alongside a knowledge corpus of over 17,000 instances. Experiments with twelve models (six proprietary, six open-source) reveal that most models perform at random-guessing levels, with only GPT o3, o4-mini, and Claude 3.5 Sonnet showing comparatively better results. The study also demonstrates that multi-modal retrieval augmentation significantly improves performance, particularly in the VQA task where it shows an average improvement of 2.59%.

## Method Summary
TemMed-Bench is built using the CheXpert Plus dataset, featuring paired images from different clinical visits of the same patient. The benchmark includes three tasks: VQA (2,000 binary questions), report generation (1,000 instances), and image-pair selection (862 items). A knowledge corpus of 17,144 instances pairs historical and current images with condition-change reports. The evaluation uses 12 LVLMs (6 proprietary like GPT-4o and Claude 3.5 Sonnet, 6 open-source like LLaVA-Med and Qwen2.5-VL). Retrieval-augmented generation employs pairwise image retrieval scoring and multi-modal retrieval that combines both visual and textual information. Top-1 retrieval is used in main experiments with ablation studies exploring top-1 to top-5 retrieval.

## Key Results
- Most LVLMs perform at random-guessing levels (50% VQA accuracy, 33% image-pair selection)
- GPT o3, o4-mini, and Claude 3.5 Sonnet show comparatively better results
- Multi-modal RAG improves VQA performance by an average of 2.59%
- Medical LVLMs underperform general-domain LVLMs despite medical fine-tuning
- Pairwise image retrieval outperforms single-image retrieval methods

## Why This Works (Mechanism)

### Mechanism 1
- Pairwise image retrieval outperforms single-image retrieval because condition-change reports correspond to image pairs, not individual images
- The retriever computes similarity as Sim(Enc_i(i_h), Enc_i(i*_h)) + Sim(Enc_i(i_c), Enc_i(i*_c)), matching both historical and current images to ensure retrieved instances reflect similar condition trajectories
- Core assumption: Temporal reports encode relational information between two timepoints that cannot be decomposed into single-image representations
- Break condition: If report semantics could be fully captured by single-image features (e.g., static pathology labels), pairwise retrieval would offer no advantage

### Mechanism 2
- Multi-modal RAG provides larger performance gains than text-only RAG because visual examples scaffold the model's ability to recognize subtle change patterns
- Retrieved image pairs serve as demonstrations of condition-change analysis, showing the model how similar temporal patterns manifest visually
- Core assumption: Models can perform analogical transfer from retrieved visual exemplars to novel image pairs
- Break condition: If models cannot align retrieved visual patterns with target images (e.g., due to poor multi-image attention), retrieved images become noise

### Mechanism 3
- Medical-specific fine-tuning can degrade temporal reasoning ability by narrowing the model's capabilities to pattern-matching on single-visit images
- Domain adaptation may overwrite general-purpose visual reasoning circuits without replacing them with temporal reasoning capacity
- Core assumption: General pretraining confers reasoning abilities that are fragile under narrow domain fine-tuning
- Break condition: If medical fine-tuning explicitly incorporated temporal contrastive objectives, this degradation could be avoided

## Foundational Learning

- **Longitudinal Clinical Reasoning**
  - Why needed: The benchmark's core premise is that real clinical practice requires tracking condition changes across visits, not single-snapshot diagnosis
  - Quick check: Can you explain why a report stating "worsening atelectasis" requires information from two timepoints, not just the current image?

- **Multi-Image Attention in Vision-Language Models**
  - Why needed: The image-pair selection task requires processing six images (three pairs) simultaneously to select the matching option
  - Quick check: How does the attention mechanism in an LVLM distribute computation when given multiple images versus a single image?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed: The paper demonstrates that RAG significantly improves temporal reasoning, but effectiveness depends on retrieval modality and method
  - Quick check: What is the key difference between text-only RAG and multi-modal RAG in terms of what information the model can access?

## Architecture Onboarding

- **Component map**: Test instances -> Retrieval module -> Prompt formatter -> LVLM -> Answer evaluator
- **Critical path**: Load image pair + question from test set -> (Optional) Retrieve top-k similar instances from corpus using pairwise similarity -> Format prompt with retrieved context (if RAG) and target images -> Model generates answer -> Evaluate against ground truth
- **Design tradeoffs**:
  - Top-k retrieval: Higher k provides more context but increases noise; benefits plateau differently for different models
  - Retrieval modality: Multi-modal RAG outperforms text-only but requires more compute and storage
  - Test set size: 1,000 instances enables statistical significance but may limit coverage of rare pathologies
- **Failure signatures**:
  - Random-guess accuracy (~50% VQA, ~33% selection) indicates model is not leveraging temporal information
  - Medical LVLMs underperforming general LVLMs suggests fine-tuning has narrowed capabilities
  - RAG causing performance drops (e.g., GPT o4-mini: -1.35% with multi-modal RAG) suggests retrieval noise or over-reliance on retrieved content
- **First 3 experiments**:
  1. Run baseline evaluation on all three tasks with closed-book setting to establish floor performance
  2. Compare text-only RAG vs multi-modal RAG on VQA task with top-1 retrieval to validate retrieval benefit
  3. Ablate retrieval method (image-to-text vs image-to-image vs pairwise) to confirm pairwise superiority

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can medical knowledge adaptation frameworks be designed to preserve general temporal reasoning capabilities?
- Basis: The authors note that medical LVLMs underperform compared to general-domain models, suggesting "prevailing medical knowledge fine-tuning schemes often erode the broad reasoning abilities inherited from general-domain pre-training"
- Why unresolved: The paper identifies the performance gap but does not propose a specific fine-tuning or adaptation methodology to solve the "catastrophic forgetting" of general reasoning skills
- What evidence would resolve it: A training paradigm or regularization technique that allows a medical LVLM to maintain or exceed the temporal reasoning scores of its general-domain backbone

### Open Question 2
- Question: How can models be improved to effectively utilize retrieved information in tasks involving multiple distinct target image pairs?
- Basis: The paper highlights that while retrieval helps VQA, it often fails in the image-pair selection task, arguing that "aligning the retrieved information with three separate pairs... substantially amplifies retrieval noise"
- Why unresolved: Current models struggle to split attention and reconcile conflicting cues when multiple visual contexts are presented alongside retrieved data
- What evidence would resolve it: Architectural innovations or attention mechanisms specifically designed for multi-image contexts that show significant performance gains on the image-pair selection task

### Open Question 3
- Question: What mechanisms allow models to optimally balance reliance on retrieved context versus internal parametric knowledge?
- Basis: Appendix C.3 reveals a discrepancy where GPT-4o refuses to trust retrieved information (harming performance), while HealthGPT relies on it more heavily
- Why unresolved: It is unclear how a model can dynamically determine when its internal knowledge is insufficient for temporal reasoning and when to defer to external retrieved evidence
- What evidence would resolve it: A "retrieval confidence" module or training strategy that results in consistent performance improvements by correctly identifying when to utilize RAG versus internal weights

## Limitations

- The study relies on a single medical dataset (CheXpert Plus) without validation on external sources, raising concerns about generalizability across different imaging modalities and pathologies
- The impact of retrieval augmentation varies substantially across models, with some showing performance degradation, suggesting retrieval noise or over-reliance on retrieved content
- The construction of VQA questions using GPT-4o with unspecified hyperparameters and 10.7% requiring manual correction introduces potential bias
- Pairwise retrieval mechanism superiority assumes temporal reports cannot be decomposed into single-image features, which may not hold for all clinical scenarios

## Confidence

- **High Confidence**: Temporal medical reasoning requires tracking changes across visits; multi-modal RAG provides measurable benefits for VQA task
- **Medium Confidence**: Medical fine-tuning can degrade temporal reasoning abilities; pairwise image retrieval outperforms single-image methods
- **Low Confidence**: Generalizability across medical imaging domains beyond chest X-rays; stability of retrieval augmentation benefits across different model architectures and dataset sizes

## Next Checks

1. Test benchmark performance on an external medical imaging dataset (e.g., MIMIC-CXR or PADCHEST) to validate generalizability beyond CheXpert Plus
2. Conduct ablation studies varying retrieval k values (1-10) across all three tasks to determine optimal retrieval scale and identify performance plateau points
3. Evaluate model performance on simplified single-image tasks (ignoring temporal component) to quantify the specific reasoning deficit in temporal analysis versus general medical knowledge