---
ver: rpa2
title: 'Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical
  Fidelity'
arxiv_id: '2512.00552'
source_url: https://arxiv.org/abs/2512.00552
tags:
- reasoning
- mathematical
- logical
- across
- transitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a diagnostic framework to distinguish genuine
  mathematical reasoning from superficial pattern matching in language models. By
  evaluating Qwen3-0.6B on MenatQA, the authors reveal that while the model achieves
  70%+ answer accuracy, it demonstrates poor backward consistency (15%), limited transitivity
  coverage (32.2%), and brittle perturbation sensitivity.
---

# Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity

## Quick Facts
- arXiv ID: 2512.00552
- Source URL: https://arxiv.org/abs/2512.00552
- Authors: Subramanyam Sahoo; Vinija Jain; Saanidhya Vats; Siddharth Mohapatra; Rui Min; Aman Chadha; Divya Chaudhary
- Reference count: 5
- Primary result: Diagnostic framework reveals Qwen3-0.6B achieves 70%+ answer accuracy but demonstrates poor backward consistency (15%) and limited transitivity coverage (32.2%), exposing superficial pattern matching rather than genuine mathematical reasoning.

## Executive Summary
This paper introduces a diagnostic framework to distinguish genuine mathematical reasoning from superficial pattern matching in language models. By evaluating Qwen3-0.6B on MenatQA, the authors demonstrate that high answer accuracy can mask fundamental reasoning failures invisible to traditional metrics. The framework exposes disconnects between surface-level performance and logical fidelity through four evaluation axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Results show the model relies heavily on pattern matching rather than genuine logical computation, with performance degrading significantly as reasoning complexity increases.

## Method Summary
The diagnostic framework evaluates reasoning fidelity through four axes using MenatQA's multi-hop temporal reasoning questions. Forward-backward consistency checks bidirectional reasoning coherence by comparing forward and reverse reasoning chains. Transitivity coverage measures whether intermediate conclusions can be combined to reach final answers. Counterfactual sensitivity tests model responses to modified premises. Perturbation robustness evaluates performance under input variations. The method uses structured prompts to extract reasoning paths, computes reasoning fidelity metrics (faithfulness, plausibility, hallucination rate), and aggregates results by reasoning complexity levels.

## Key Results
- Qwen3-0.6B achieves 70%+ answer accuracy but demonstrates only 15% backward consistency, revealing significant reasoning gaps
- Transitivity coverage is limited to 32.2%, indicating inability to combine intermediate conclusions for final answers
- Performance degrades sharply with reasoning complexity, with 4+-hop questions showing near-zero backward consistency
- Perturbation robustness shows high sensitivity to input modifications, with accuracy drops of up to 40% under semantic noise

## Why This Works (Mechanism)
The framework works by exposing reasoning failures that traditional accuracy metrics cannot detect. Forward-backward consistency reveals when models can answer questions but cannot explain their reasoning backward, indicating memorization over understanding. Transitivity coverage identifies whether models can chain intermediate conclusions into final answers, exposing limitations in logical composition. Counterfactual sensitivity tests whether models can handle modified premises, revealing brittle reasoning patterns. Perturbation robustness exposes sensitivity to input variations that genuine reasoning should withstand.

## Foundational Learning
**Forward-Backward Consistency**: Measures bidirectional coherence between reasoning chains. Why needed: Traditional accuracy ignores reasoning quality. Quick check: Compare forward and reverse reasoning step counts and semantic similarity.

**Transitivity Coverage**: Evaluates ability to combine intermediate conclusions into final answers. Why needed: Exposes limitations in logical composition and chaining. Quick check: Verify that intermediate conclusions logically entail the final answer.

**Counterfactual Sensitivity**: Tests model responses to modified premises. Why needed: Reveals brittle reasoning patterns and memorization. Quick check: Modify numerical entities or temporal relations and compare responses.

**Perturbation Robustness**: Measures performance under input variations. Why needed: Genuine reasoning should withstand reasonable perturbations. Quick check: Apply token shuffling, distractor injection, and semantic noise to inputs.

## Architecture Onboarding

**Component Map**: Data preprocessing -> Structured prompting -> Reasoning extraction -> Fidelity metrics -> Aggregation -> Visualization

**Critical Path**: The evaluation pipeline requires sequential execution: dataset loading → hop categorization → structured inference → reasoning path extraction → metric computation → aggregation. Each step depends on successful completion of the previous one.

**Design Tradeoffs**: The framework trades computational efficiency for diagnostic depth, requiring multiple inference passes per question (forward, backward, perturbed variants). This increases evaluation time but provides comprehensive reasoning assessment.

**Failure Signatures**: High answer accuracy combined with low backward consistency indicates pattern matching over genuine reasoning. Poor transitivity coverage suggests inability to chain logical steps. High counterfactual sensitivity reveals brittle reasoning that fails under premise modifications.

**First Experiments**:
1. Run forward and backward consistency on single-hop questions to establish baseline reasoning quality
2. Test transitivity coverage on questions with explicit intermediate conclusions
3. Apply simple perturbations (token shuffling) to verify robustness sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed disconnects between surface accuracy and reasoning fidelity—specifically low backward consistency and poor transitivity—persist, diminish, or fundamentally change in language models with significantly larger parameter counts (e.g., 7B+)?
- Basis in paper: The authors explicitly note in the Limitations section that generalization is "restricted to larger models" and list "scaling the evaluation to larger reasoning models" as the primary future direction to determine if failures "persist or decrease with scale."
- Why unresolved: The study relies exclusively on a single 600M-parameter model (Qwen3-0.6B). It remains unclear if the high accuracy/low consistency pattern is a structural limitation of small models or a universal failure mode of current training paradigms.
- What evidence would resolve it: Applying the identical four-axis diagnostic framework to larger model classes (e.g., 7B, 70B, 175B parameters) and comparing the trajectories of consistency scores against accuracy gains.

### Open Question 2
- Question: Can the proposed diagnostic metrics (forward-backward consistency, transitivity) be successfully integrated into reinforcement learning or fine-tuning objectives to incentivize genuine logical computation over superficial pattern matching?
- Basis in paper: The Future Directions section suggests "incorporating these assessments into training goals could be beneficial in motivating models to aim for verifiable reasoning as opposed to superficial plausibility."
- Why unresolved: While the paper establishes metrics for evaluation, it does not test whether these metrics are differentiable or robust enough to serve as reward signals without causing reward hacking or optimized deception.
- What evidence would resolve it: Training a model using a loss function weighted by the proposed consistency metrics and evaluating the resulting model on out-of-distribution mathematical reasoning tasks.

### Open Question 3
- Question: How can the current rule-based approximations for transitivity and logical closure be replaced or augmented by symbolic solvers to provide formal guarantees of reasoning correctness?
- Basis in paper: The authors state in the Limitations that their metrics "continue to be rule-based approximations" rather than universal measures of logical entailment, and propose integrating "symbolic solvers and formal verification tools" in Future Directions.
- Why unresolved: Mapping natural language reasoning chains to formal logic representations that symbolic solvers can process remains a brittle and unsolved challenge, limiting the ability to verify reasoning with mathematical certainty.
- What evidence would resolve it: A hybrid framework where generated reasoning steps are parsed into formal logic and verified by an external solver, demonstrating higher correlation with human judgment than the current Jaccard-based consistency scores.

## Limitations
- Analysis restricted to single small model (Qwen3-0.6B), limiting generalizability to other model families
- Dependence on unpublished prompt templates and generation parameters affects reproducibility
- Rule-based approximations for transitivity and logical closure rather than formal verification
- Perturbation strategies may not capture all real-world reasoning disruptions

## Confidence
- **High confidence** in framework's ability to detect reasoning failures invisible to accuracy metrics
- **Medium confidence** in quantitative claims about Qwen3-0.6B due to missing implementation details
- **Low confidence** in framework's generalizability to non-temporal reasoning domains

## Next Checks
1. Reproduce full evaluation pipeline using only published code and any available public prompt templates
2. Apply diagnostic framework to at least two additional models (small and large) on MenatQA
3. Test framework on non-temporal reasoning dataset to assess cross-domain generalizability