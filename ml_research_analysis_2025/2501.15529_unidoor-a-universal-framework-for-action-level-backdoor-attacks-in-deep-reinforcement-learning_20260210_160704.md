---
ver: rpa2
title: 'UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement
  Learning'
arxiv_id: '2501.15529'
source_url: https://arxiv.org/abs/2501.15529
tags:
- backdoor
- reward
- agent
- adversary
- victim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UNIDOOR introduces the first universal framework for action-level
  backdoor attacks in deep reinforcement learning (DRL), addressing the lack of adaptability
  in existing methods. It leverages a multi-task learning paradigm to adaptively adjust
  backdoor reward functions based on performance monitoring, eliminating reliance
  on expert knowledge and extensive trial-and-error.
---

# UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2501.15529
- **Source URL**: https://arxiv.org/abs/2501.15529
- **Reference count**: 40
- **Primary result**: UNIDOOR achieves top-1 CP in 84.6% of single-backdoor scenarios and 61.5% of multiple-backdoor scenarios

## Executive Summary
UNIDOOR introduces the first universal framework for action-level backdoor attacks in deep reinforcement learning, addressing the critical need for adaptability across diverse tasks and algorithms. By leveraging a multi-task learning paradigm with performance monitoring, the framework adaptively adjusts backdoor reward functions without relying on expert knowledge or extensive trial-and-error. The approach demonstrates superior performance across 11 DRL tasks, 53 backdoor designs, and three mainstream algorithms, achieving high attack success rates while maintaining benign task performance.

## Method Summary
UNIDOOR employs a four-module architecture to execute action-level backdoor attacks: Performance Monitoring tracks Benign Task Performance (BTP) and Attack Success Rate (ASR) using Exponential Weighted Averaging; Initial Freezing delays backdoor injection until the agent learns basic behavior; Transition Poisoning injects triggers into states and overwrites actions with target values while setting rewards based on action match; and Adaptive Exploration dynamically adjusts the backdoor reward $r^\dagger_t$ through expansion (increasing reward when ASR is low) and contraction (narrowing reward bounds when BTP drops). This multi-task learning approach eliminates the need for task-specific hyperparameter tuning and ensures cross-task effectiveness.

## Key Results
- Achieved top-1 comprehensive performance (CP) in 84.6% of single-backdoor scenarios and 61.5% of multiple-backdoor scenarios
- Maintained stealthiness as confirmed by visualization results showing no significant performance degradation in benign conditions
- Ablation studies demonstrated the critical role of adaptive exploration and action tampering, particularly in continuous action spaces

## Why This Works (Mechanism)
UNIDOOR's effectiveness stems from its ability to dynamically adapt the backdoor reward signal based on real-time performance monitoring. By tracking both BTP and ASR simultaneously, the framework can identify when to strengthen the backdoor signal (expansion phase) versus when to preserve task performance (contraction phase). The multi-task learning paradigm allows the agent to maintain a balance between benign task learning and backdoor injection, preventing the typical distraction dilemma where agents either learn the backdoor perfectly but forget the task, or vice versa.

## Foundational Learning
- **Performance Monitoring with EWA**: Exponential Weighted Averaging with β=0.99 smooths performance metrics to prevent overreaction to transient changes. *Quick check*: Verify BTP/ASR curves show gradual changes rather than sharp spikes.
- **Action Tampering in Continuous Spaces**: Adding uniform noise U(-0.025, 0.025) to target actions ensures the agent explores the correct region. *Quick check*: Confirm agent outputs actions within ±0.025 of target when triggered.
- **Multi-Task Reward Balancing**: Simultaneously optimizing BTP and ASR requires careful reward signal design. *Quick check*: Monitor both metrics during training to ensure neither collapses.
- **Adaptive Reward Adjustment**: Dynamically modifying $r^\dagger_t$ based on performance trends prevents overfitting to either task. *Quick check*: Plot $r^\dagger_t$ values over time to verify expansion/contraction phases.

## Architecture Onboarding

**Component Map**: Environment -> Performance Monitor -> Initial Freezer -> Transition Poisoner -> Adaptive Explorer -> Agent

**Critical Path**: The core attack pipeline flows from environment observation through trigger injection, action overwriting, reward modification, and adaptive reward adjustment. The performance monitoring module feeds back to the adaptive explorer to adjust $r^\dagger_t$.

**Design Tradeoffs**: UNIDOOR trades off attack effectiveness for stealth by maintaining benign task performance, whereas traditional backdoor methods might achieve higher ASR at the cost of complete task degradation. The adaptive mechanism adds computational overhead but eliminates the need for extensive hyperparameter tuning.

**Failure Signatures**: 
- BTP collapse indicates insufficient initial freezing or overly aggressive backdoor rewards
- Low ASR in continuous spaces suggests inadequate action tampering or reward signal
- Oscillating performance indicates poor parameter tuning in the adaptive mechanism

**3 First Experiments**:
1. CartPole-v1 with binary trigger (state[0] = -4.8) to verify basic poisoning works
2. Pendulum-v1 with target action 0.0 to test continuous action tampering
3. Multiple trigger injection on LunarLander-v2 to validate cross-task adaptability

## Open Questions the Paper Calls Out
- **Open Question 1**: How can action-level backdoor frameworks be adapted for offline reinforcement learning, particularly with architectures like Transformers? The authors note UNIDOOR "has yet to address... offline RL," as action-level backdoor discussions in this context "remain scarce." This remains unresolved because offline RL does not allow interaction-based exploration during training, and Transformer architectures differ significantly from the MLPs tested.
- **Open Question 2**: Are sample filtering or network pruning effective defenses against universal action-level backdoor attacks? The paper tested fine-tuning but found DRL instability limits the direct transfer of DL defenses; other common defense mechanisms were not evaluated. This remains unresolved as empirical evaluation of filtering or pruning rates required to lower ASR without significant BTP degradation has not been conducted.
- **Open Question 3**: Can enhancing neural plasticity improve backdoor injection success rates in post-training scenarios? The authors attribute lower performance in post-training scenarios to "limitations in plasticity" and suggest exploring strategies to enhance it. This remains unresolved as current adaptive mechanisms struggle with the rigidity of converged policies, and specific strategies to overcome this were not implemented or tested.

## Limitations
- Exact training horizon and convergence criteria are not fully specified, requiring assumptions about standard Stable Baselines defaults or manual monitoring
- Implementation details for "outer-loop" poisoning in vectorized environments are not explicitly provided
- Adaptive reward adjustment mechanism's parameter tuning (step sizes, bounds) may require empirical optimization

## Confidence
- **High confidence**: Overall framework design and multi-task learning paradigm
- **Medium confidence**: Reproducibility of specific numerical results without exact hyperparameter values
- **Low confidence**: Exact implementation details for continuous action space handling without code inspection

## Next Checks
1. Implement the framework on CartPole-v1 with a simple binary trigger (e.g., set state[0] = -4.8) and verify BTP/ASR tracking
2. Test the adaptive reward mechanism by manually setting $r^\dagger_t$ to extreme values (1.0 and 0.1) and observing performance curves
3. Validate continuous action tampering by running on Pendulum-v1 with a target action of 0.0 and checking if the agent learns to output values within ±0.025 of the target when triggered