---
ver: rpa2
title: 'Information Representation Fairness in Long-Document Embeddings: The Peculiar
  Interaction of Positional and Language Bias'
arxiv_id: '2601.16934'
source_url: https://arxiv.org/abs/2601.16934
tags:
- document
- embedding
- similarity
- position
- segment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of information representation fairness
  in long-document embeddings, where state-of-the-art models systematically underrepresent
  later segments and segments in lower-resource languages. The authors introduce a
  permutation-based evaluation framework to quantify positional and language biases,
  finding that first-positioned segments and English segments are over-represented
  in document embeddings regardless of their content.
---

# Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias

## Quick Facts
- arXiv ID: 2601.16934
- Source URL: https://arxiv.org/abs/2601.16934
- Reference count: 40
- This work introduces a permutation-based evaluation framework to quantify positional and language biases in long-document embeddings, finding that first-positioned segments and English segments are systematically over-represented regardless of content.

## Executive Summary
This paper addresses fairness in long-document embeddings by revealing systematic positional and language biases where early segments and high-resource language content dominate document representations. Through a novel permutation-based evaluation framework, the authors demonstrate that state-of-the-art models like mGTE exhibit front-loaded attention distributions that over-represent early segments in final embeddings. They propose an inference-time attention calibration method that redistributes attention more evenly across document positions, substantially reducing positional bias while maintaining semantic fidelity. The approach is training-free and generalizable to other `<s>`-pooled embedding models.

## Method Summary
The authors develop a permutation-based evaluation framework to isolate positional effects by randomizing segment order in multilingual Wikipedia documents. They analyze attention distributions in `<s>`-pooled transformer models, identifying front-loaded attention patterns where early tokens receive disproportionate representation in document embeddings. To address this, they propose an inference-time attention calibration method that partitions keys into fixed-size baskets and enforces uniform attention mass across baskets while preserving within-basket relative distributions. The method is applied during inference to specific layers of the transformer, modifying only the `<s>`-token's attention query.

## Key Results
- Positional bias is pervasive: early segments consistently dominate document embeddings regardless of content, with overrepresentation decreasing monotonically for later positions
- Language bias compounds the issue: English and Chinese segments receive preferential representation even when placed in later positions
- Attention calibration effectively mitigates positional bias: calibrated embeddings increase representation of later segments while maintaining semantic fidelity
- Calibration is training-free: the method works at inference time without requiring model retraining or architectural changes

## Why This Works (Mechanism)

### Mechanism 1: Front-Loaded Attention Distribution Creates Positional Bias
Early tokens in long documents receive disproportionately more attention from pooling tokens (e.g., `<s>`), causing early segments to dominate document embeddings. The `<s>`-pooling token's query allocates attention mass in an L-shaped profile—highest at tokens 2–129, declining thereafter. Since the pooling token's representation becomes the document embedding, early content exerts outsized influence on the final vector. This assumes attention weights correlate with representational contribution to the pooled output.

### Mechanism 2: Basket-Level Attention Equalization Redistributes Representational Capacity
Enforcing uniform attention mass across fixed-size positional baskets during inference reduces positional bias without retraining. Keys are partitioned into contiguous baskets (size B, e.g., 128). The calibration redistributes attention so each basket receives equal total mass, preserving within-basket relative weights. This allows later positions to contribute more to the pooling token's representation, assuming within-basket attention patterns encode meaningful semantic relationships.

### Mechanism 3: Language Preference Interacts with Positional Bias
High-resource languages (English, Chinese) receive preferential representation regardless of position, partially counteracting positional bias when placed later. Model training on imbalanced multilingual corpora creates language-specific representation strengths. When English segments appear in later positions, their language advantage compensates for positional disadvantage, yielding flatter representation profiles. This assumes language bias is largely independent of positional bias and can be approximated as an additive effect.

## Foundational Learning

- **Concept: Pooling strategies in transformer embeddings (CLS/special-token vs. mean pooling)**
  - Why needed here: The paper analyzes models using different pooling methods (mGTE uses `<s>`-pooling; jina-v3 uses mean pooling). Understanding how each aggregates token representations is essential to grasp why attention calibration targets the `<s>`-query specifically.
  - Quick check question: Given a 12-layer transformer producing contextualized embeddings, does mean pooling require access to attention weights for calibration?

- **Concept: Positional bias in long-context models ("lost in the middle" / front-loading)**
  - Why needed here: The paper situates its findings within prior work on positional bias. The mechanism differs—front-loaded rather than U-shaped—but the phenomenon of position-dependent performance is the connecting thread.
  - Quick check question: If a retrieval model retrieves relevant documents correctly when evidence is at position 1 but fails when the same evidence is at position 5, what type of bias does this indicate?

- **Concept: Permutation-based fairness evaluation**
  - Why needed here: The paper's evaluation framework permutes segment order to isolate positional effects from content confounds. This design pattern is reusable for testing other bias types.
  - Quick check question: Why permute segments rather than simply measuring performance across documents of varying lengths?

## Architecture Onboarding

- **Component map:** Input document → Multi-segment tokenization → Transformer encoder → `<s>`-token attention computation → Attention calibration (inference-time) → Calibrated `<s>`-token representation → Document embedding

- **Critical path:**
  1. Tokenize document → sequence of token IDs
  2. Forward pass through transformer layers
  3. At each calibrated layer L_c: retrieve attention weights for `<s>`-query, partition keys into baskets, redistribute attention mass uniformly, apply modified attention to value vectors
  4. Extract `<s>`-token hidden state from final layer as document embedding

- **Design tradeoffs:**
  - Basket size (B): Smaller baskets (128) provide finer-grained equalization but may disrupt local semantic patterns; larger baskets (512) are coarser but preserve more within-basket structure
  - Calibrated layer set (L_c): Later layers capture higher-level semantics; calibrating only late layers (e.g., 7–12) may suffice, while early-layer calibration risks disrupting syntactic processing
  - Scope: Calibration currently targets `<s>`-query only; mean-pooled models would require different approach (not evaluated in paper)

- **Failure signatures:**
  - Semantic collapse: If calibration is too aggressive, document embeddings may lose discriminative power (paper's control experiment shows this does not occur with tested settings, but remains a risk)
  - Inverted bias: Over-correction can cause later segments to be overrepresented, creating reverse unfairness (observed in some English mixed-language settings)
  - Language bias persistence: Calibration addresses positional but not language bias—lower-resource languages remain disadvantaged

- **First 3 experiments:**
  1. **Baseline diagnostic:** Run the permutation-based evaluation on your target embedding model without calibration. Measure positional fairness coefficients (β_p for positions 2–n) to quantify bias magnitude.
  2. **Hyperparameter sweep:** Test basket sizes B ∈ {128, 256, 512} and calibrated layer sets L_c ∈ {{12}, {10,11,12}, {7–12}} on a held-out document set. Track both positional fairness improvement and semantic fidelity (cosine similarity between calibrated and uncalibrated embeddings).
  3. **Cross-model validation:** Apply the best hyperparameter configuration to at least one additional `<s>`-pooled embedding model beyond mGTE to assess generalizability. Document any architecture-specific adjustments required.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does representation-level fairness translate to downstream retrieval performance in realistic search pipelines?
- Basis in paper: [explicit] Authors state they "deliberately focus on representation-level effects and do not evaluate downstream task performance" and note that "future work is needed to connect representation-level fairness more directly to end-task outcomes in realistic retrieval systems."
- Why unresolved: The fairness metric (cosine similarity-based) is geometric and task-agnostic; it remains unknown whether reduced positional bias improves recall for later segments in actual retrieval tasks.
- What evidence would resolve it: Benchmark evaluation where queries target specific document segments, comparing calibrated vs. uncalibrated embeddings on segment-level retrieval metrics (recall@k, MRR) across positions and languages.

### Open Question 2
- Question: Do decoder-based and hybrid embedding architectures exhibit similar positional and language bias patterns as encoder-based models?
- Basis in paper: [explicit] Authors state their analysis is "restricted to encoder-based long-context embedding models" and that "decoder-based and hybrid architectures may exhibit different positional and language bias patterns. Extending the proposed evaluation framework to other embedding paradigms... remains an open direction."
- Why unresolved: Only encoder models (mGTE, jina-v3) were tested; decoder-based embedders and hybrid approaches were not examined.
- What evidence would resolve it: Applying the permutation-based framework to instruction-tuned or generative embedding models (e.g., E5-mistral, LLM-based embedders) and comparing bias profiles.

### Open Question 3
- Question: Is the inference-time attention calibration method effective for mean-pooled embeddings and other pooling strategies beyond `<s>`-token pooling?
- Basis in paper: [explicit] Authors state the calibration "is evaluated primarily on a single embedding model (mGTE) that uses pooling-token representations" and "its effectiveness for other architectures and pooling strategies calls for further empirical validation."
- Why unresolved: Mean-pooled models like jina-v3 were analyzed for bias but not calibrated; the calibration specifically targets `<s>`-query attention rows.
- What evidence would resolve it: Developing and testing adapted calibration techniques for mean-pooled models (e.g., token-level attention reweighting) and measuring bias reduction while monitoring semantic fidelity.

### Open Question 4
- Question: How do the observed biases manifest in real-world document types (newspapers, legal texts, reports) with naturally occurring multi-segment structures?
- Basis in paper: [explicit] Authors note that "Wikipedia articles differ from real-world long documents such as newspapers, reports, or legal texts in structure, discourse style, and noise. As a result, the absolute magnitude of the observed biases may differ in applied settings."
- Why unresolved: The controlled Wikipedia corpus enabled content isolation but may not reflect characteristics of practical long-document applications.
- What evidence would resolve it: Experiments on domain-specific corpora with authentic multi-article layouts (e.g., newspaper pages, multi-clause contracts), comparing bias magnitudes to Wikipedia-based benchmarks.

## Limitations
- Method currently limited to `<s>`-pooling transformer models; not validated on mean-pooling architectures
- Multilingual analysis uses Wikipedia corpus which may not represent real-world document distributions or specialized domains
- Inference-time calibration introduces computational overhead without quantified latency or memory impact
- Evaluation focuses on intrinsic measures; downstream task performance effects are not assessed

## Confidence

**High confidence** in the positional bias diagnostic framework and finding that early segments dominate through front-loaded attention patterns. The permutation-based methodology is rigorous and attention analysis provides clear mechanistic evidence.

**Medium confidence** in attention calibration effectiveness. While statistically significant improvements are shown, evaluation is limited to intrinsic measures rather than downstream task performance. Hyperparameter choices appear somewhat arbitrary without systematic ablation.

**Low confidence** in claimed independence of language and positional biases. The interaction effects are complex and the additive model may oversimplify how these biases compound.

## Next Checks
1. **Downstream task evaluation**: Apply calibrated embeddings to a representative retrieval task using the BEIR benchmark or similar. Measure whether improvements in positional fairness translate to better recall@k and mean reciprocal rank for documents where relevant information appears in later segments.

2. **Architecture generalization test**: Implement and evaluate the calibration approach on at least two additional embedding models with different pooling strategies (e.g., mean-pooling models like jina-v3, or other `<s>`-pooled models). Document any architectural modifications required and compare calibration effectiveness across model families.

3. **Cross-lingual fairness analysis**: Conduct a systematic study of how calibration affects lower-resource languages (Korean, Hindi) versus high-resource languages (English, Chinese). Measure not just positional fairness but absolute representation quality for each language, and test whether additional calibration parameters or strategies are needed to achieve equitable performance across the language spectrum.