---
ver: rpa2
title: 'SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate
  Time Series Forecasting'
arxiv_id: '2509.18135'
source_url: https://arxiv.org/abs/2509.18135
tags:
- graph
- series
- time
- forecasting
- static
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multivariate time series forecasting by proposing
  a Static-Dynamic Graph Fusion network (SDGF) that captures multi-scale inter-series
  correlations through a dual-path graph structure learning approach. The method uses
  a static graph based on prior knowledge to model long-term stable dependencies,
  while employing multi-level wavelet decomposition to extract multi-scale features
  and construct an adaptively learned dynamic graph for capturing associations at
  different scales.
---

# SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2509.18135
- Source URL: https://arxiv.org/abs/2509.18135
- Reference count: 0
- Primary result: SDGF achieves state-of-the-art performance on seven benchmark datasets, with best MSE and MAE on ETT* dataset across multiple prediction horizons

## Executive Summary
This paper addresses multivariate time series forecasting by proposing a Static-Dynamic Graph Fusion network (SDGF) that captures multi-scale inter-series correlations through a dual-path graph structure learning approach. The method uses a static graph based on prior knowledge to model long-term stable dependencies, while employing multi-level wavelet decomposition to extract multi-scale features and construct an adaptively learned dynamic graph for capturing associations at different scales. An attention-gated fusion module intelligently combines these complementary information sources, and a multi-kernel dilated convolutional network deepens temporal pattern understanding.

## Method Summary
SDGF is a multivariate time series forecasting model that uses dual-path graph structure learning. The static path uses Pearson Correlation Coefficient (PCC) to build a prior-knowledge-based graph capturing long-term stable dependencies. The dynamic path employs multi-level wavelet decomposition to extract multi-scale features, with each scale having its own adaptively learned adjacency matrix. An attention-gated fusion module combines outputs from both paths, followed by multi-kernel dilated convolutional networks for temporal pattern learning. The model is trained using Adam optimizer with RevIN normalization and outputs predictions through an MLP layer.

## Key Results
- SDGF achieves state-of-the-art performance on seven benchmark datasets including ETT h1/h2/m1/m2, Exchange-Rate, Electricity, and Weather
- The model attains the best MSE and MAE across multiple prediction horizons on the ETT* dataset
- Ablation study confirms the importance of each component: graph structure learning, attention-gated fusion, and temporal feature learning are all essential to the model's superior forecasting accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale wavelet decomposition enables capture of scale-dependent inter-series correlations that single-scale methods miss.
- Mechanism: Multi-level Wavelet Decomposition (MWD) projects input into frequency sub-bands; a dedicated dynamic adjacency matrix is learned per scale via equation (4), allowing the model to represent that variables may correlate differently at hourly vs. weekly scales.
- Core assumption: Inter-series relationships exhibit meaningfully different patterns at different temporal granularities, and wavelet bases appropriately isolate these.
- Evidence anchors:
  - [abstract] "employing Multi-level Wavelet Decomposition to extract multi-scale features and construct an adaptively learned dynamic graph to capture associations at different scales"
  - [section 3.3] "X norm = PLd l=1 X(l),X (l) ∈R B×Ll×N , whereL d denotes the decomposition level" with separate dynamic graphs per level
  - [corpus] Weak direct validation; related work "Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion" suggests community interest in multi-scale graph fusion, but no direct empirical validation of wavelet choice vs. alternatives.

### Mechanism 2
- Claim: Decoupling static (long-term stable) and dynamic (evolving) graph structures improves modeling of both persistent and transient dependencies.
- Mechanism: Static graph uses prior knowledge or PCC over the full history (equations 1-2) to anchor stable relationships; dynamic graphs are learned per wavelet scale from local features (equation 4). Graph convolutions (equations 3, 5) propagate information along each structure separately before fusion.
- Core assumption: Some inter-series dependencies are approximately stationary over long periods while others vary with context; modeling both explicitly is superior to a single graph.
- Evidence anchors:
  - [abstract] "utilizes a static graph based on prior knowledge to anchor long-term, stable dependencies, while concurrently... constructing an adaptively learned dynamic graph"
  - [section 4.3, ablation] "w/o-GSL" removal degrades performance across ETTh1, Exchange, Weather, indicating graph structure learning is necessary
  - [corpus] "Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector" critiques static-only graphs; supports motivation but not direct validation.

### Mechanism 3
- Claim: Attention-gated fusion adaptively weights static and multi-scale dynamic graph outputs, improving over naive combination.
- Mechanism: Pool each graph representation, compute attention weights via learnable query (equation 9), and form weighted sum $H_{fusion}$. This lets the model emphasize whichever path is more informative per input.
- Core assumption: The relative importance of static vs. dynamic information varies across samples and time; a learned gating is better than fixed weighting.
- Evidence anchors:
  - [abstract] "design an attention-gated module to fuse these two complementary sources of information intelligently"
  - [section 4.3, ablation] "w/o-GF" removal increases MSE/MAE (e.g., ETTh1: 0.343→0.371 MSE), verifying fusion contributes to performance
  - [corpus] No direct external validation; attention-based fusion is common but not specifically tested against simpler alternatives in this paper.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) for sequence modeling
  - Why needed here: SDGF applies graph convolutions along learned adjacency matrices to propagate information across variables; understanding message passing and diffusion (e.g., equation 3) is essential.
  - Quick check question: Given an adjacency matrix A and node features H, trace one step of graph convolution output.

- Concept: Wavelet decomposition in time series
  - Why needed here: The method relies on MWD to isolate frequency sub-bands; you should grasp how approximations and details split signal energy across scales.
  - Quick check question: After two-level decomposition of a length-L signal, what are the shapes of the approximation and detail coefficients?

- Concept: Attention-based feature fusion
  - Why needed here: The attention-gated module (equation 9) computes softmax-normalized scores over multiple graph representations; understanding query-key attention clarifies the fusion behavior.
  - Quick check question: With 3 graph representations and a learned query vector q, compute attention weights if query-key dot products are [2.0, 1.0, 0.5].

## Architecture Onboarding

- Component map: RevIN normalization → Multi-level Wavelet Decomposition → [Static Graph (PCC-based) + Dynamic Graphs per scale] → Graph Convolutions (separate per path) → Attention Gated Fusion → Multi-kernel Dilated Convolutions (Inception-style) → LayerNorm → MLP output head → RevIN denormalization
- Critical path: Wavelet decomposition quality and adjacency normalization (softmax over ReLU) directly affect graph learning; fusion attention collapse (all weight on one path) is a key failure mode to monitor
- Design tradeoffs: More decomposition levels increase multi-scale coverage but reduce temporal resolution per scale; deeper graph propagation (larger K) captures longer-range dependencies but risks over-smoothing; larger dilation rates help long-term patterns but may miss fine-grained changes
- Failure signatures:
  - Attention weights consistently near [1, 0, 0, ...] indicating fusion collapse to a single path
  - Dynamic adjacency matrices near-uniform (all entries similar), suggesting learning failure
  - Static PCC graph overly sparse/dense due to threshold or normalization issues
  - MSE/MAE significantly worse than DLinear baseline, indicating possible implementation or hyperparameter errors
- First 3 experiments:
  1. Reproduce ETTh1 results (L=96, T=96) with default settings; log MSE/MAE and attention weight distributions to verify fusion balance
  2. Ablate the dynamic path (use static graph only) to confirm performance drop aligns with Table 2 w/o-GSL values
  3. Vary wavelet decomposition levels (Ld=1 vs. Ld=2 vs. Ld=3) on a validation split; observe whether additional scales yield diminishing returns or degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the model's performance to the specific choice of wavelet basis function and decomposition depth?
- Basis in paper: [inferred] The paper utilizes Multi-level Wavelet Decomposition (MWD) to extract features but does not specify the wavelet family used or analyze the impact of different decomposition levels ($L_d$).
- Why unresolved: Different wavelet bases have distinct time-frequency localization properties; an inappropriate choice may fail to effectively separate multi-scale inter-series correlations.
- What evidence would resolve it: An ablation study comparing forecasting accuracy using different wavelet types (e.g., Haar vs. Daubechies) and varying decomposition depths on datasets with diverse periodicity.

### Open Question 2
- Question: Does the reliance on the Pearson Correlation Coefficient (PCC) for static graph construction limit the capture of non-linear long-term dependencies?
- Basis in paper: [inferred] Equation 1 defines the static graph adjacency matrix using PCC, a linear metric, which may fail to represent complex non-linear relationships inherent in the stable dependencies the model aims to capture.
- Why unresolved: If the true long-term inter-series dependencies are non-linear, the static graph branch may introduce noise or spurious connections that the attention-gated fusion cannot fully correct.
- What evidence would resolve it: Comparative experiments replacing PCC with non-linear dependency metrics (e.g., distance correlation or mutual information) to observe changes in forecasting error.

### Open Question 3
- Question: What is the computational cost of the multi-scale dynamic graph learning relative to single-graph baselines, particularly regarding memory usage?
- Basis in paper: [inferred] The model generates distinct adjacency matrices for each wavelet scale ($A_{dyn}^{(l)}$) and employs multi-branch convolutions, yet the experimental analysis omits training time, inference latency, or memory consumption metrics.
- Why unresolved: Creating and convolving multiple graphs simultaneously increases theoretical complexity, potentially limiting the model's applicability to high-dimensional, real-time forecasting scenarios.
- What evidence would resolve it: Reporting and comparing training/inference time and GPU memory footprint against baselines like MTGNN and TimesNet as the number of variables ($N$) scales.

## Limitations
- The model's computational complexity increases significantly with multi-scale decomposition, potentially limiting scalability to high-dimensional datasets
- Performance depends on the assumption that long-term stable dependencies exist and can be captured by PCC-based static graphs
- The specific choice of wavelet basis function and decomposition depth is not explored, leaving uncertainty about optimal configuration

## Confidence

**Confidence in claims:**
- **High**: Dual-path static/dynamic graph design improves forecasting vs. static-only or dynamic-only models (confirmed by ablation)
- **Medium**: Attention-gated fusion outperforms naive combination (ablation shows benefit, but no external validation)
- **Low**: Multi-scale wavelet decomposition is strictly necessary (no comparison to single-scale or other frequency transforms)

## Next Checks

1. Compare SDGF performance using fixed single-scale dynamic graph vs. multi-scale; measure sensitivity to decomposition depth
2. Replace attention-gated fusion with fixed equal weighting or learned gating without attention; assess trade-off between accuracy and efficiency
3. Test model on a rapidly evolving dataset to quantify impact of static graph assumptions