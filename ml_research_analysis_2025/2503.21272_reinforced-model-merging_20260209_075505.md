---
ver: rpa2
title: Reinforced Model Merging
arxiv_id: '2503.21272'
source_url: https://arxiv.org/abs/2503.21272
tags:
- merging
- agent
- layer
- tasks
- merged
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reinforced Model Merging (RMM), a novel framework
  that addresses the challenges in model merging by leveraging reinforcement learning.
  Traditional model merging methods often suffer from performance degradation due
  to uniform treatment of parameters and inefficiency in search-based algorithms.
---

# Reinforced Model Merging

## Quick Facts
- arXiv ID: 2503.21272
- Source URL: https://arxiv.org/abs/2503.21272
- Reference count: 32
- This paper introduces Reinforced Model Merging (RMM), a novel framework that addresses the challenges in model merging by leveraging reinforcement learning.

## Executive Summary
Reinforced Model Merging (RMM) introduces a novel framework that leverages reinforcement learning to solve the challenges of model merging. Traditional model merging methods often suffer from performance degradation due to uniform treatment of parameters and inefficiency in search-based algorithms. RMM overcomes these issues by modeling the merging task as an agent's decision-making process, enabling automatic layer-wise merging within an extensible environment. The framework operates without gradient computations on the original models, making it feasible for edge devices. Additionally, RMM employs a Dynamic Average Reward (DAR) mechanism to accelerate the evaluation process by using data subsets, achieving up to 100x speedup. Extensive experiments on vision and NLP tasks demonstrate that RMM achieves state-of-the-art performance, improving accuracy by 5.57% and 8.56%, respectively, compared to existing methods.

## Method Summary
RMM treats model merging as a sequential decision process where an RL agent (PPO) traverses model layers, selecting actions like "keep Model A," "apply Task Arithmetic," or "skip layer" at each step. The agent operates on a "Merging Map" state tracking which actions were taken at which layers, assembling a temporary merged model F_M for evaluation on small data subsets. Rewards are calculated using DAR (Dynamic Average Reward), which smooths noisy subset evaluations by weighting current and previous rewards. The agent updates its policy using PPO based on these non-differentiable rewards, leaving original model parameters frozen. This gradient-free approach enables deployment on edge devices while DAR achieves up to 100× speedup by using 1-10% of training data per episode.

## Key Results
- Achieves 5.57% accuracy improvement over existing methods on vision tasks
- Achieves 8.56% accuracy improvement over existing methods on NLP tasks
- Achieves up to 100× speedup through DAR mechanism using data subsets

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Policy Search via RL
Treating model merging as a sequential decision process allows for non-uniform, granular parameter integration that outperforms static merging rules. An RL agent (PPO) traverses the model layers, selecting actions from a defined space (e.g., keep Model A, apply Task Arithmetic, skip layer). This results in a heterogeneous merged architecture where different layers undergo different transformations based on the learned policy, rather than a single mathematical operation applied uniformly.

### Mechanism 2: Gradient-Free Policy Optimization
Effective merging policies can be learned without differentiating through the original model weights, enabling deployment in constrained environments. The framework constructs the merged model using actions sampled by the agent and evaluates it via inference only. The reward is derived from metrics on a validation set, and the agent updates its policy using PPO based on these non-differentiable rewards, leaving the original model parameters frozen and untouched by gradients.

### Mechanism 3: Dynamic Average Reward (DAR) Acceleration
Using small, dynamic subsets of evaluation data stabilizes RL training while drastically reducing computation time. Instead of evaluating the merged model on the full dataset every episode, RMM uses a subset. To counter the noise introduced by small sample sizes, DAR computes the reward as a weighted average of the current subset reward and the previous reward. This smoothing mechanism allows the agent to extract a stable learning signal from noisy estimates.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) & PPO**
  - **Why needed here:** RMM frames merging not as a one-shot calculation but as a sequence of states (layers) and actions (merging ops). Understanding PPO is critical to grasp how the agent optimizes the discrete merging trajectory without differentiable rewards.
  - **Quick check question:** How does the "Actor" determine the probability of selecting a specific merging operator at a given layer?

- **Concept: Model Merging Primitives (e.g., Task Arithmetic, Ties-Merging)**
  - **Why needed here:** These are the atomic actions available to the RL agent. One cannot understand the agent's decision space without knowing what operations it is choosing between.
  - **Quick check question:** In the RMM action space, what is the difference between a "Model Action" and a "Merging Action"?

- **Concept: Inference-Only Optimization**
  - **Why needed here:** RMM optimizes the structure of the merged model using only forward passes. This distinguishes it from knowledge distillation or fine-tuning, which rely on backpropagation.
  - **Quick check question:** Why does the paper claim RMM is suitable for edge devices compared to traditional multi-task learning?

## Architecture Onboarding

- **Component map:** Merging Agent (Actor-Critic network) -> Merging Environment (holds Merging Map state) -> Action Space (discrete set of model/merge/layer actions) -> DAR Module (calculates smoothed reward) -> Reward Signal -> PPO Update
- **Critical path:** 1. State Input: Environment presents the current Merging Map to the Agent. 2. Action Selection: Agent selects action (e.g., "Apply Ties-Merging at layer k"). 3. Model Assembly: The temporary merged model is assembled up to layer k. 4. Evaluation: F_M runs inference on a data subset. 5. Reward Calculation: DAR computes reward based on accuracy and history. 6. Update: PPO updates the Agent's weights using the clipped objective.
- **Design tradeoffs:** Subset Size vs. Stability: Using 1% data maximizes speed (96.5×) but introduces noise. DAR mitigates this, but tuning λ is critical. Action Space Complexity: Adding more merging operators increases the search space. The paper notes RL complexity does not increase significantly, but inference time for evaluation does.
- **Failure signatures:** Reward Hacking: The agent may learn to "Skip" difficult layers excessively to minimize immediate loss, resulting in a shallow model. DAR Drift: If λ is poorly tuned, the "momentum" of bad rewards might prevent the agent from recognizing better merging strategies. Catastrophic Forgetting: If the agent selects "Merge" actions too aggressively for conflicting tasks, performance on individual tasks may degrade despite the average reward looking stable.
- **First 3 experiments:** 1. Baseline Validation: Reproduce static merging baselines (Task Arithmetic, Ties-Merging) on specific datasets to ensure environment's "Merge" actions are implemented correctly. 2. Ablation on DAR: Run RMM with and without DAR on a single task pair to verify the 100× speedup claim and check for performance variance. 3. Layer-wise Policy Inspection: Visualize the Merging Map of a trained agent. Does it learn to keep certain layers static while merging others?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the RMM framework be extended to handle heterogeneous model merging where architectures differ in layer dimensions or structures?
- Basis in paper: The conclusion states that future work will focus on "extending RMM to more complex scenarios, such as... heterogeneous model merging."
- Why unresolved: The current methodology assumes a shared backbone and layer-wise alignment, requiring parameter dimensions to match for the defined action space.
- What evidence would resolve it: A mechanism that allows the agent to map or align parameters across non-isomorphic architectures without manual intervention.

### Open Question 2
- Question: Can RMM effectively generalize to multi-modal merging tasks where distinct data types require different processing pathways?
- Basis in paper: The conclusion identifies "multi-modal... merging" as a specific area for future investigation.
- Why unresolved: The current experimental validation is restricted to uni-modal tasks (vision-only or NLP-only), leaving the management of cross-modal interference untested.
- What evidence would resolve it: Successful application of RMM to merge a vision model and a language model into a single functional multi-modal system.

### Open Question 3
- Question: What adaptive strategies can be integrated to allow the agent to define custom merging configurations beyond the fixed set of pre-defined operators?
- Basis in paper: The authors mention "exploring adaptive strategies to further enhance its versatility" in the conclusion.
- Why unresolved: The current action space is discrete and limited to specific choices like Task Arithmetic or Ties-Merging, preventing the agent from synthesizing novel merging functions.
- What evidence would resolve it: An updated agent policy that generates continuous weighting parameters or novel functional combinations not present in the initial operator set.

## Limitations

- The 100× DAR speedup claim lacks extensive ablation studies across diverse datasets to validate stability of this acceleration
- RL search space complexity and computational overhead are not fully quantified relative to claimed benefits
- Implementation details for PPO hyperparameters and Actor-Critic architecture are not specified, making exact reproduction challenging

## Confidence

- **High**: The core mechanism of layer-wise RL search for merging decisions is technically sound and well-motivated by existing work on search-based merging
- **Medium**: The gradient-free optimization claim is valid but the practical advantages for edge deployment need more rigorous benchmarking
- **Low**: The 100× DAR speedup claim relies heavily on the assumption that small data subsets correlate with full-dataset performance, which may not hold for all data distributions

## Next Checks

1. **Ablation on DAR stability**: Run RMM with DAR on datasets with known distribution skews (e.g., imbalanced CIFAR variants) to test if the smoothing mechanism prevents reward signal degradation
2. **Edge deployment feasibility**: Benchmark the complete RMM pipeline (search + evaluation) on actual edge hardware (e.g., Raspberry Pi 4) and compare to simple fine-tuning baselines
3. **Action space sensitivity**: Systematically vary the number of merging operators M and measure the trade-off between policy expressivity and training stability across different model pairs