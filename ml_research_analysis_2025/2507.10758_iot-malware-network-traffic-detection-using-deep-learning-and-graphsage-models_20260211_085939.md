---
ver: rpa2
title: IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models
arxiv_id: '2507.10758'
source_url: https://arxiv.org/abs/2507.10758
tags:
- network
- attention
- training
- bi-lstm
- graphsage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates deep learning and graph-based models for detecting
  IoT malware in network traffic. Models compared include BERT, TCN, Multi-Head Attention,
  BI-LSTM, LSTM, and GraphSAGE.
---

# IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models

## Quick Facts
- arXiv ID: 2507.10758
- Source URL: https://arxiv.org/abs/2507.10758
- Authors: Nikesh Prajapati; Bimal Karki; Saroj Gopali; Akbar Siami Namin
- Reference count: 13
- Primary result: BERT achieved 99.94% accuracy and 99.99% AUC-ROC on IoT malware detection

## Executive Summary
This paper evaluates seven deep learning and graph-based models for detecting IoT malware in network traffic. The models include BERT, TCN, Multi-Head Attention, BI-LSTM, LSTM, and GraphSAGE. Tested on a dataset of 1,008,748 labeled network traffic records with 23 features, BERT achieved the highest performance with 99.94% accuracy and 99.99% AUC-ROC. TCN ranked second with 99.30% accuracy and 99.70% AUC-ROC. GraphSAGE showed the lowest accuracy (97.16%) but trained fastest (4 minutes). The results indicate that transformer-based models excel at capturing temporal dependencies for malware detection, while graph-based approaches offer computational efficiency at the cost of detection accuracy.

## Method Summary
The study compares seven models on the IoT-23 dataset with 1,008,748 samples and 23 features. Preprocessing included missing value handling, IP conversion to integers, one-hot encoding for categoricals, and numerical normalization. A 70/30 train-test split was used with 20% of training data for validation. Deep learning models (LSTM, BI-LSTM, TCN, Multi-Head Attention) used 20 epochs, batch size 125, and Adam optimizer. BERT used 0.05 epochs with batch size 4. GraphSAGE was implemented with SAGEConv layers on a NetworkX graph constructed from the traffic data. Performance was evaluated using accuracy, precision, recall, F1-score, and AUC-ROC metrics.

## Key Results
- BERT achieved highest performance with 99.94% accuracy and 99.99% AUC-ROC
- TCN ranked second with 99.30% accuracy and 99.70% AUC-ROC
- GraphSAGE had lowest accuracy (97.16%) but fastest training time (4 minutes)
- BERT had lowest false positive rate (159) among all models
- Training times ranged from 4 minutes (GraphSAGE) to 210 minutes (BI-LSTM+Attention)

## Why This Works (Mechanism)

### Mechanism 1: Transformer Self-Attention for Temporal Pattern Capture
BERT's self-attention mechanism enables superior capture of temporal dependencies in network traffic sequences. The bidirectional transformer architecture processes entire sequences simultaneously, computing attention weights across all time steps. This allows the model to learn relationships between distant network events (e.g., initial connection → data exfiltration) without the information bottleneck inherent in sequential processing. The core assumption is that IoT malware exhibits discernible temporal patterns across sequential network packets that distinguish it from benign traffic.

### Mechanism 2: TCN Dilated Convolutions for Efficient Sequence Modeling
TCN achieves near-BERT performance through causal, dilated convolutions that expand receptive fields exponentially. TCN uses 1D convolutions with exponentially increasing dilation rates ([8, 64] per paper), allowing each layer to see further back in the sequence while maintaining causal ordering. This provides parallelizable computation unlike sequential RNN processing. The core assumption is that network traffic malware signatures can be detected through local and medium-range temporal patterns without requiring full global context.

### Mechanism 3: GraphSAGE's Speed-Accuracy Trade-off via Structural Aggregation
GraphSAGE achieves fastest training (4 minutes) but lowest accuracy (97.16%) because graph structure discards temporal sequence information. GraphSAGE constructs a static graph where nodes are IP addresses and edges aggregate all traffic between pairs. Neighbor sampling efficiently propagates information, but this aggregation destroys the temporal ordering of packets—critical for detecting sequential attack patterns like DDoS progression or C2 communication handshakes. The core assumption is that network topology alone contains sufficient signal for malware detection without requiring packet-level temporal dynamics.

## Foundational Learning

- **Concept: Self-Attention and Multi-Head Attention**
  - Why needed here: All top-performing models (BERT, Multi-Head Attention, BI-LSTM+Attention) leverage attention. Understanding how queries, keys, and values compute relevance scores is essential for interpreting why these models detect malware patterns.
  - Quick check question: Can you explain why self-attention allows BERT to process a 100-packet sequence faster than an LSTM while capturing longer-range dependencies?

- **Concept: Graph Neural Networks and Message Passing**
  - Why needed here: GraphSAGE uses SAGEConv layers for neighbor aggregation. Understanding how node features propagate through graph structure clarifies why temporal information is lost and why training is fast.
  - Quick check question: If you add a time-ordered edge attribute to GraphSAGE, would this restore temporal pattern detection? Why or why not?

- **Concept: Receptive Field in Temporal Convolutional Networks**
  - Why needed here: TCN's performance depends on dilation rates determining how far back in the sequence each output can "see." The [8, 64] dilation configuration directly impacts what malware patterns are detectable.
  - Quick check question: With dilation rates [8, 64] and kernel size 3, what is the approximate receptive field? Would this capture a 500-packet DDoS attack sequence?

## Architecture Onboarding

- **Component map:** Tabular network data → preprocessing (missing values, IP conversion, one-hot encoding, normalization) → sequential models (LSTM/BI-LSTM/TCN/Attention) or BERT (tokenized text) or GraphSAGE (graph construction) → binary classification output

- **Critical path:** Data preprocessing → 70/30 train-test split → Graph construction (for GraphSAGE) → Model training (20 epochs, Adam optimizer, binary cross-entropy) → Evaluation (confusion matrix, precision/recall/F1/accuracy, ROC-AUC)

- **Design tradeoffs:** Accuracy vs. Training Time (BERT 99.94%, 34 min vs. GraphSAGE 97.16%, 4 min — 2.8% accuracy gap for 8.5x speedup); Sequential vs. Parallel Processing (BI-LSTM+Attention 210 min vs. BERT 34 min); Temporal Fidelity vs. Structural Abstraction (sequential models outperform graph-based approach)

- **Failure signatures:** High false positives with low false negatives (model too sensitive); Training time explosion with attention + RNN (BI-LSTM+Attention took 210 min); Performance collapse on GraphSAGE (check if edge aggregation destroys critical temporal signals)

- **First 3 experiments:**
  1. **Baseline reproduction**: Train LSTM, TCN, and BERT on the IoT-23 dataset with the paper's hyperparameters. Compare accuracy and training time against reported values to validate your pipeline.
  2. **Ablation on temporal window size**: Vary the sequence length fed to TCN and BERT to determine the minimum temporal context required for >99% accuracy. This tests the receptive field hypothesis.
  3. **Hybrid architecture test**: Implement a GraphSAGE variant that preserves temporal edge ordering (e.g., time-stamped edges or temporal walk sampling) to evaluate if the accuracy gap can be closed while maintaining speed advantages.

## Open Questions the Paper Calls Out
- **Question**: Can hybrid architectures combining BERT's temporal dependency handling with GraphSAGE's computational efficiency optimize the trade-off between detection accuracy and training speed?
- **Basis in paper**: The authors state in the conclusion: "In the future, we would like to consider hybrid models that combine the best attributes of different architectures..."
- **Why unresolved**: The current study evaluates deep learning and graph models in isolation, identifying distinct advantages for each (BERT's accuracy vs. GraphSAGE's speed) without attempting to integrate them.
- **What evidence would resolve it**: Experimental results from a hybrid model architecture evaluated on the same IoT-23 dataset, showing if accuracy can be maintained near BERT's levels while reducing training time closer to GraphSAGE's levels.

## Limitations
- Unknown learning rate for non-BERT deep learning models, which could significantly impact training outcomes
- BERT's input format for tabular data is unspecified—whether features are serialized as text strings or embedded numerically
- GraphSAGE architecture details (number of layers, output channels) are incomplete despite being listed in the configuration table
- The normalization method for numerical features is not stated, affecting model convergence and performance

## Confidence
- **High confidence** in the relative ranking of models (BERT > TCN > BI-LSTM > LSTM > Multi-Head Attention > GraphSAGE) due to consistent performance gaps across metrics
- **Medium confidence** in absolute metric values given potential hyperparameter sensitivity and lack of hyperparameter optimization details
- **Low confidence** in the GraphSAGE comparison validity since temporal sequence information is inherently lost during graph construction, making it an apples-to-oranges comparison with sequential models

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary learning rates for LSTM, TCN, and attention models to determine if reported performance is robust or an artifact of specific optimization settings.
2. **Feature ablation study**: Remove potentially predictive features (specific ports, IPs, or protocols that only appear in malware samples) to test whether models are learning genuine temporal patterns or memorizing dataset-specific signatures.
3. **Temporal context sweep**: Vary sequence lengths for TCN and BERT to empirically determine the minimum temporal window required for >99% accuracy, validating the receptive field mechanism hypothesis.