---
ver: rpa2
title: Context-Awareness and Interpretability of Rare Occurrences for Discovery and
  Formalization of Critical Failure Modes
arxiv_id: '2504.16117'
source_url: https://arxiv.org/abs/2504.16117
tags:
- cairo
- systems
- scene
- object
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CAIRO, a framework that combines ontological
  knowledge graphs with human-in-the-loop validation to detect and formalize critical
  failure modes in vision systems for autonomous vehicles. CAIRO addresses the problem
  of perception failures by mapping AI model outputs to real-world contexts through
  standardized ontologies and formalized rules.
---

# Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes

## Quick Facts
- **arXiv ID**: 2504.16117
- **Source URL**: https://arxiv.org/abs/2504.16117
- **Reference count**: 40
- **Primary result**: Ontology-based knowledge graphs with human-in-the-loop validation can detect and formalize critical failure modes in vision systems for autonomous vehicles.

## Executive Summary
This paper introduces CAIRO, a framework that combines ontological knowledge graphs with human-in-the-loop validation to detect and formalize critical failure modes in vision systems for autonomous vehicles. CAIRO addresses the problem of perception failures by mapping AI model outputs to real-world contexts through standardized ontologies and formalized rules. The approach uses object detection models, segmentation, and reasoning to identify failures such as misdetections, adversarial attacks, and hallucinations. Experiments demonstrate that CAIRO can reliably detect critical phenomena across diverse scenarios, including occluded objects, missing attributes, and adversarial patches. The framework enables scalable, interpretable, and accountable testing, with results stored as explicit knowledge graphs for downstream analysis. CAIRO bridges the gap between AI perception and human cognition, promoting safer and more interpretable autonomous systems.

## Method Summary
CAIRO uses an ensemble of perception models (Faster R-CNN, GroundingDINO, CLIP with SAM segmentation) to detect objects and extract features from images/video frames. These outputs are mapped to a 6-layer ontology model representing the operational design domain, with nodes representing segmented regions and edges capturing spatial/physical relationships. Critical Phenomena (CP) rules written in SWRL and Description Logic are executed using Pellet/HermiT reasoners to detect failures like missing license plates, occluded vehicles, and adversarial patches. The system incorporates human-in-the-loop validation at multiple stages to ground abstract reasoning in domain expertise and reduce false positives.

## Key Results
- CAIRO successfully detects critical phenomena including missing license plates, occluded objects, and adversarial patches in AV perception systems
- The framework generates explicit knowledge graphs (OWL/XML format) that are amenable to sharing, logical reasoning, and accountability
- Human-in-the-loop validation improves precision but creates scalability challenges, with reasoning taking ~865 seconds per frame in experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ontology-based knowledge graphs enable interpretable failure detection by mapping raw perception outputs to formalized, queryable concepts.
- Mechanism: The system constructs directed graphs where nodes represent segmented regions (e.g., Vehicle, Pedestrian) and edges capture spatial/physical relationships (is_occluded_by, has_distance). These are stored as OWL/XML files, allowing logical reasoning over asserted and inferred knowledge to detect implicit critical patterns.
- Core assumption: Failure modes can be expressed as finite combinations of axioms describable in Description Logic (DL).
- Evidence anchors:
  - [abstract] "resulting in test cases stored as explicit knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis, logical reasoning, and accountability."
  - [section III.C] "Nodes (Concepts): Represent segmented regions...Edges (Relations): Capture the relationships between nodes, such as is_left_of, is_right_of, is_occluded_by"
  - [corpus] Weak direct corpus support; neighboring papers address anomaly detection but not ontology-based formalization for AV specifically.
- Break condition: If failure modes require continuous-valued reasoning not reducible to discrete axioms (e.g., subtle texture degradation), ontology expressiveness limits detection.

### Mechanism 2
- Claim: Hybrid Open-World/Closed-World Assumption (OWA-CWA) enables both flexible concept modeling and decisive failure rule evaluation.
- Mechanism: OWA allows multi-class subsumption (e.g., "car" as passenger_car or cargo_car), while CWA enables negation operations in DL queries (e.g., finding missing license plates). Critical Phenomena (CP) rules adopt CWA for "find criteria" while the broader ontology remains OWA-flexible.
- Core assumption: The combinatorial cases triggering CP are finite and enumerable as Horn clauses.
- Evidence anchors:
  - [section II.B] "Although the ODD is convenient to model as OWA, the actual combinatorial number of cases that trigger a CP are finite. The convergence of CP inherently adopts a CWA in the rules."
  - [section II.B] Figure 2 illustrates hybrid approach enabling CP-005-Off-Body-Wheel detection.
  - [corpus] No direct corpus validation of hybrid OWA-CWA for AV testing found.
- Break condition: If novel failure modes emerge outside pre-defined rule scope, they remain undetected until rules are extended.

### Mechanism 3
- Claim: Human-in-the-loop validation at multiple pipeline stages grounds abstract reasoning in domain expertise, reducing false positive CP detections.
- Mechanism: HITL validates segmentation accuracy, refines SWRL rule formulation, and oversees reasoning to resolve ambiguities. This collaborative approach ensures outputs remain actionable and aligned with human perception of criticality.
- Core assumption: Human experts can reliably identify ground-truth failure modes and translate them into formal rules.
- Evidence anchors:
  - [abstract] "CAIRO by design incentivizes human-in-the-loop for testing and evaluation of criticality"
  - [section III.G] "Human oversight is valuable in complex or ambiguous cases, ensuring that the system's outputs remain meaningful and actionable."
  - [corpus] Neighbor paper "Unsupervised Learning for Detection of Rare Driving Scenarios" addresses rare scenario detection but without HITL grounding.
- Break condition: If human validation becomes a bottleneck at scale (Table III: 865s per frame for Pellet reasoner), throughput limitations may reduce practical deployment.

## Foundational Learning

- Concept: **Description Logic (DL) and Semantic Web Rule Language (SWRL)**
  - Why needed here: CP formalization uses DL for terminological assertions (T-Box) and SWRL for Horn-clause rules querying critical phenomena.
  - Quick check question: Can you write a SWRL rule to find vehicles with occlusion >50% near pedestrian crossings?

- Concept: **6-Layer Model (6LM) for Operational Design Domain**
  - Why needed here: CAIRO inherits 6LM architecture for consistent ontology representation across road structure, traffic participants, and environmental conditions.
  - Quick check question: What are the six layers, and which layer would contain "detached wheel near drivable lane"?

- Concept: **Object Detection Model Ensembles (Faster R-CNN + GroundingDINO + CLIP)**
  - Why needed here: Each DM compensates for others' limitations—Faster R-CNN for class-limited detection, GroundingDINO for text-prompted detection, CLIP for open-vocabulary classification of SAM segments.
  - Quick check question: Why does the pipeline use three models instead of one unified detector?

## Architecture Onboarding

- Component map: Input images → Detection/Segmentation (Faster R-CNN, GroundingDINO, SAM, CLIP, Canny edge detector) → Feature Extraction (physical properties, spatial attributes, statistical metrics, ego-centric relationships) → Ontology Construction (T-Box + A-Box → OWL/XML knowledge graphs) → Reasoning (Pellet/HermiT reasoners + SWRL rules → CP detection) → Validation (feature modification module + HITL interface) → Output (flagged CP with root cause hypotheses, stored KGs for downstream analysis)

- Critical path: Detection → Segmentation → Feature Extraction → Ontology Construction → Reasoning → HITL Validation. Failure at any stage propagates; reasoning quality depends entirely on upstream feature fidelity.

- Design tradeoffs:
  - Expressiveness vs. decidability: Full OWL-DL expressiveness increases reasoning time (865s/frame in experiments)
  - OWA flexibility vs. CWA decisiveness: Hybrid approach balances both but increases rule complexity
  - Automation vs. HITL: More human oversight improves precision but reduces scalability

- Failure signatures:
  - **Undetected CP**: Rules too narrow; extend T-Box concepts or add SWRL antecedents
  - **False positive CP**: Feature extraction noise; validate segmentation quality or adjust occlusion/distance thresholds
  - **Reasoning timeout**: Ontology too large; modularize KGs by scenario scope

- First 3 experiments:
  1. Replicate CP_0004 (missing license plate detection) on CityScape validation images to validate SWRL rule correctness.
  2. Run adversarial patch attack (ShapeShifter) on a held-out frame; verify CAIRO triggers "Traffic Sign On Vehicle" CP even when Faster R-CNN misses the vehicle.
  3. Profile end-to-end latency; identify whether Pellet reasoner or CLIP classification is the bottleneck for your target frame-rate requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Large Language Models (LLMs) reliably automate the generation of SWRL rules for Critical Phenomena to replace the current manual curation process?
- Basis in paper: [explicit] The authors state that while the rules are currently generated manually, "it is possible to generate them through prompt engineering using LLMs too."
- Why unresolved: The paper outlines the manual formalization process but does not implement or validate an automated approach using LLMs, leaving the accuracy and formal consistency of such an automation unproven.
- What evidence would resolve it: A comparative study measuring the semantic correctness and logical validity of LLM-generated SWRL rules against human-curated gold standards.

### Open Question 2
- Question: How can the computational latency of the description logic reasoning phase be reduced to enable feasible application in large-scale or real-time testing?
- Basis in paper: [inferred] Table III shows the Pellet Reasoner taking an average of 865.37 seconds per frame, a massive bottleneck compared to the detection models (<3s).
- Why unresolved: The paper demonstrates the feasibility of the logic checks but acknowledges the experimental nature of the setup; the extreme time cost of reasoning makes the current implementation impractical for processing large datasets or real-time verification without significant optimization.
- What evidence would resolve it: Benchmarks showing the integration of approximate or streamlined reasoners that achieve logical validation at speeds comparable to the perception models.

### Open Question 3
- Question: To what extent does the CAIRO framework generalize to safety-critical domains beyond autonomous driving, such as medical imaging or industrial robotics?
- Basis in paper: [explicit] The Conclusion states: "Future work will explore CAIRO's potential in mitigating evolving real-world threats in different domains with collaborative efforts in human-centered AI."
- Why unresolved: The current study validates the framework exclusively on autonomous vehicle datasets (CityScape, driving video) and specific AV-related ontologies.
- What evidence would resolve it: Successful instantiation of the CAIRO pipeline using domain-specific ontologies (e.g., medical anatomy) and detection of failure modes in non-AV vision tasks.

## Limitations

- The 865-second per-frame reasoning time creates a fundamental scalability barrier that isn't adequately addressed for practical deployment
- Success metrics are qualitative rather than quantitative, lacking precision/recall benchmarks against ground truth failure datasets
- Human-in-the-loop validation, while improving precision, introduces a bottleneck that may limit scalability for large-scale testing

## Confidence

**High confidence**: The ontological framework and SWRL rule formulation are technically sound and represent a valid approach to knowledge graph construction for AV testing.

**Medium confidence**: The hybrid OWA-CWA approach is logically coherent, though the paper provides limited empirical validation of its advantages over pure OWA or CWA systems.

**Low confidence**: Claims about scalability and practical deployment are weakly supported given the computational requirements and human oversight dependencies.

## Next Checks

1. **Quantitative benchmark test**: Implement a controlled experiment comparing CAIRO's CP detection against a labeled dataset of known failure modes, measuring precision, recall, and F1-score to establish baseline performance.

2. **Scalability profiling**: Test CAIRO on progressively larger scenes (10, 50, 100+ objects per frame) to measure how reasoning time and memory usage scale, identifying practical limits for real-time deployment.

3. **Rule generalization test**: Apply the existing SWRL rules to a completely different dataset (e.g., nuScenes or KITTI) to evaluate whether the rule set detects failures without modification or requires extensive re-engineering.