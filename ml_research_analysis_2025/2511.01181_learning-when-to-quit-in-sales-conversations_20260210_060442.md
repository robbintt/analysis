---
ver: rpa2
title: Learning When to Quit in Sales Conversations
arxiv_id: '2511.01181'
source_url: https://arxiv.org/abs/2511.01181
tags:
- stopping
- salespeople
- agent
- calls
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency in sales conversations, specifically
  the decision of when to quit a conversation that is unlikely to succeed. The authors
  formalize this as an optimal stopping problem and develop a generative language
  model-based sequential decision agent, a stopping agent, that learns when to quit
  by imitating an inferred optimal stopping policy.
---

# Learning When to Quit in Sales Conversations

## Quick Facts
- **arXiv ID:** 2511.01181
- **Source URL:** https://arxiv.org/abs/2511.01181
- **Reference count:** 24
- **Primary result:** AI stopping agent reduces failed sales call time by up to 54% while preserving nearly all sales, increasing expected sales by up to 37%

## Executive Summary
This paper addresses the inefficiency in sales conversations by formalizing when to quit unproductive calls as an optimal stopping problem. The authors develop a sequential decision agent that learns when to abandon a sales conversation by imitating an inferred optimal stopping policy, using large language models to handle high-dimensional textual states. Applied to real-world sales data from a European telecommunications firm, the stopping agent significantly reduces time spent on failed calls while maintaining sales performance. The analysis reveals that salespeople systematically overweight a few salient expressions of consumer disinterest and mispredict call failure risk, suggesting cognitive bounds on their real-time decision-making.

## Method Summary
The authors formalize sales conversation termination as an optimal stopping problem where an agent observes live transcript prefixes at discrete time points (30, 60, 90 seconds) and decides whether to wait or quit. They use imitation learning: first computing optimal quitting times for each call by maximizing expected cumulative reward, then building a dataset of (transcript prefix, optimal action) pairs. A generative language model (GPT-4.1) is fine-tuned to generate "abandonar" or "esperar" given transcript prefixes. Per-timestep thresholds are tuned via backward induction on validation data to balance time savings against sales preservation. The method leverages LLM's ability to handle high-dimensional textual states and works with both open-source and proprietary models.

## Key Results
- Stopping agent reduces time spent on failed calls by up to 54% while preserving nearly all sales
- Expected sales increase by up to 37% through improved efficiency
- Salespeople systematically overweight a few salient expressions of consumer disinterest and mispredict call failure risk

## Why This Works (Mechanism)
The approach works by combining optimal stopping theory with modern language models to create a decision agent that can process conversational context in real-time. The key insight is that optimal stopping policies can be learned through imitation of computed optimal behaviors, and that large language models provide the necessary representation power to handle the high-dimensional state space of conversation transcripts. By fine-tuning on (transcript prefix, action) pairs derived from optimal stopping solutions, the agent learns to predict when a conversation is unlikely to succeed based on linguistic patterns that may not be immediately apparent to human salespeople.

## Foundational Learning

**Optimal Stopping Theory**
- *Why needed:* Provides mathematical framework for sequential decision-making under uncertainty
- *Quick check:* Verify that τ* computation maximizes expected reward given opportunity cost c and reward b

**Imitation Learning**
- *Why needed:* Enables learning of complex policies from expert demonstrations
- *Quick check:* Confirm that fine-tuned model minimizes log-loss on imitation dataset

**Large Language Model Fine-tuning**
- *Why needed:* Handles high-dimensional textual state representations
- *Quick check:* Validate that model generates only "abandonar" or "esperar" tokens

**Backward Induction for Threshold Calibration**
- *Why needed:* Optimizes per-timestep decision thresholds to balance objectives
- *Quick check:* Plot π_θ(quit|s_t) distribution to verify threshold reasonableness

## Architecture Onboarding

**Component Map:**
Data Preparation -> Optimal τ* Computation -> Imitation Dataset Construction -> LLM Fine-tuning -> Threshold Calibration -> Evaluation

**Critical Path:**
The most critical sequence is Data Preparation → Optimal τ* Computation → Imitation Dataset Construction, as errors in any of these steps propagate through the entire pipeline and cannot be recovered in later stages.

**Design Tradeoffs:**
The choice between open-source and proprietary models involves a tradeoff between customization/control versus immediate performance. Open-source models require more computational resources and fine-tuning expertise but offer better data privacy and customization. Proprietary models provide superior out-of-box performance but raise concerns about data privacy and ongoing costs.

**Failure Signatures:**
- Model generates tokens other than "abandonar"/"esperar" (tokenization issue)
- Backward induction yields degenerate thresholds (calibration problem)
- High variance in stopping decisions across similar conversations (overfitting)

**First Experiments:**
1. Validate optimal τ* computation by comparing against random stopping baseline
2. Test fine-tuned model's calibration on validation set before threshold tuning
3. Perform ablation study on opportunity cost parameter c to understand sensitivity

## Open Questions the Paper Calls Out

**Behavioral Response Question:**
How do salespeople respond behaviorally to AI-generated quitting recommendations in live deployments? The authors note this remains an open managerial and behavioral question that could be addressed via field experimentation, as the current analysis is retrospective.

**Action Space Expansion Question:**
Can the stopping agent's action space be expanded to include conversational guidance (e.g., scripted responses, clarifying questions) rather than just quit/wait decisions? The authors suggest this extension is possible but would require additional interaction data and more complex estimation procedures.

**Causal Effects Question:**
Does the quitting decision causally affect sale outcomes, or does it merely reveal pre-existing likelihood? The observational design cannot distinguish whether longer calls cause sales or simply signal pre-existing interest that would have converted regardless.

## Limitations
- Method assumes salesperson's decision to not quit has no causal effect on sale likelihood
- Limited demonstration of open-source model performance compared to GPT-4.1
- Generalizability to other sales contexts or languages remains uncertain

## Confidence

**High Confidence:**
- Salespeople systematically mispredict call failure risk and overweight salient disinterest signals
- The optimal stopping framework is mathematically sound

**Medium Confidence:**
- The 54% time savings and 37% expected sales increase figures
- The approach works with both open-source and proprietary models (primarily demonstrated with GPT-4.1)

**Low Confidence:**
- N/A (no low confidence claims identified)

## Next Checks
1. **Threshold calibration robustness:** Re-run backward induction algorithm with perturbed validation sets to assess stability of λ_t thresholds and sensitivity to reward parameter variations.
2. **Cross-validation of stopping policy:** Implement k-fold cross-validation on training set to evaluate whether imitation learning consistently learns similar stopping policies across different data partitions.
3. **Ablation on opportunity cost:** Systematically vary opportunity cost parameter c across a reasonable range to quantify its impact on reported efficiency gains and identify break-even point where time savings no longer justify potential sales losses.