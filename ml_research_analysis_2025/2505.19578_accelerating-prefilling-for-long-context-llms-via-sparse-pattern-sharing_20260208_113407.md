---
ver: rpa2
title: Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing
arxiv_id: '2505.19578'
source_url: https://arxiv.org/abs/2505.19578
tags:
- attention
- pattern
- sparse
- patterns
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SharePrefill, a novel sparse attention method
  for accelerating long-context LLM prefilling. The method is based on two key observations:
  (1) attention patterns across different heads are highly similar, and (2) this similarity
  is consistent across diverse inputs.'
---

# Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing

## Quick Facts
- **arXiv ID**: 2505.19578
- **Source URL**: https://arxiv.org/abs/2505.19578
- **Reference count**: 22
- **Primary result**: Introduces SharePrefill, a sparse attention method achieving superior or comparable speedup and accuracy to state-of-the-art methods on long-context LLM prefilling.

## Executive Summary
This paper presents SharePrefill, a novel method for accelerating the prefilling phase of long-context language models through sparse attention with inter-head pattern sharing. The approach leverages two key observations: attention patterns across different heads exhibit high similarity, and this similarity remains consistent across diverse inputs. SharePrefill clusters similar heads offline and dynamically shares accurate attention patterns among them during inference, reducing the need for full attention computation. The method significantly outperforms or matches state-of-the-art approaches in both speedup and accuracy preservation, demonstrating strong performance across the InfiniteBench benchmark and competitive language modeling perplexity scores.

## Method Summary
SharePrefill operates through an offline clustering phase followed by online inference with dynamic pattern sharing. During offline preparation, an autoencoder compresses attention score maps from a calibration dataset into 64-dimensional representations, which are then clustered hierarchically to group similar heads. During inference, the first head in each cluster computes full attention to construct a "pivotal pattern," which is then shared with other cluster members if their estimated attention distributions are sufficiently similar (measured via Jensen-Shannon divergence). Highly sparse heads are excluded from sharing and default to vertical-slash patterns to avoid unnecessary computation overhead. The method implements block-wise sparse attention computation using Triton kernels and maintains a runtime dictionary of pivotal patterns for efficient reuse.

## Key Results
- Achieves 39.05 average score on InfiniteBench across 10 tasks, outperforming MInference (39.14) and FlexPrefill (36.44)
- Demonstrates strong language modeling performance with perplexity scores closely matching FlashAttention 2
- Shows consistent latency improvements over baseline methods while maintaining accuracy
- Ablation studies confirm the effectiveness of head exclusion mechanism for efficiency gains

## Why This Works (Mechanism)

### Mechanism 1: Inter-head Pattern Similarity Clustering
The method exploits quantifiable similarity in attention patterns across heads within and across layers. An autoencoder compresses attention score maps to 64-dimensional representations, which are hierarchically clustered. This clustering persists across diverse inputs, allowing heads in the same cluster to share computed patterns rather than independently estimating them. The core assumption is that similarity relationships discovered offline generalize to unseen inputs during inference.

### Mechanism 2: Dynamic Pivotal Pattern Construction and Sharing
During inference, the first head in each cluster computes dense attention, and its sparse pattern is constructed using a cumulative threshold (γ=0.9) to select minimal blocks covering most attention mass. Subsequent heads in the cluster retrieve this pattern if their Jensen-Shannon distance to the pivotal head's distribution is below threshold (τ=0.2). This dynamic sharing preserves accuracy while reducing computation by avoiding redundant attention calculations.

### Mechanism 3: Selective Head Exclusion for Efficiency
Highly sparse heads incur unnecessary overhead when computing full attention for pattern construction. The method excludes these heads by computing their Jensen-Shannon distance to a uniform distribution—if this sparsity distance exceeds threshold (δ=0.3), the head uses vertical-slash patterns directly. This optimization improves speedup with acceptable accuracy loss, as confirmed by ablation showing 39.35 avg score vs 39.05 baseline but at 20.02s vs 16.92s latency.

## Foundational Learning

- **Concept: Sparse Attention and Block-wise Sparsity**
  - Why needed: The method assumes understanding that attention matrices are sparse, with most weights concentrating on small subsets of (query, key) pairs. SharePrefill operates on block-wise granularity for hardware efficiency.
  - Quick check: Given a 1024-token sequence with block size 128, how many blocks comprise the attention matrix? Explain why block-wise sparse attention reduces memory bandwidth but may miss fine-grained sparsity.

- **Concept: Jaccard Similarity and Jensen-Shannon Divergence**
  - Why needed: Offline clustering uses Jaccard similarity on binary sparse patterns, while online safety checks use JS divergence to compare probability distributions. Distinguish these metrics and understand why JS is appropriate for normalized attention distributions.
  - Quick check: If two attention patterns both have 100 active blocks but only 20 overlap, what is their Jaccard similarity? Why might JS divergence be more sensitive than Jaccard for detecting distributional shifts?

- **Concept: Cumulative Threshold-based Pattern Selection**
  - Why needed: The γ threshold (default 0.9) determines how many blocks to compute by sorting blocks by attention score and selecting the minimum number whose cumulative sum exceeds γ. This is the core accuracy-efficiency knob.
  - Quick check: If sorted block attention scores are [0.4, 0.3, 0.15, 0.1, 0.05] and γ=0.9, how many blocks are selected? What happens to accuracy if γ is reduced to 0.7?

## Architecture Onboarding

- **Component map**: Offline Pipeline (Autoencoder → Hierarchical Clustering → Head Dictionary) -> Online Inference (Determine Sparse Pattern → Share/Construct Pattern → Sparse Attention Kernel) -> Dynamic Pivotal Pattern Dictionary

- **Critical path**: The first head in each cluster triggers dense computation → pattern construction → dictionary update. All subsequent heads in that cluster hit the cache and use sparse computation. Latency gains compound as sequence length increases and cluster reuse rises.

- **Design tradeoffs**:
  - τ (similarity threshold, default 0.2): Lower values enforce stricter similarity, reducing risky shares but increasing dense/vertical-slash fallbacks. Raise τ for more aggressive sharing at potential accuracy cost.
  - δ (sparsity threshold, default 0.3): Higher values include more heads in sharing (better accuracy, slower). Lower values exclude more heads to vertical-slash (faster, potential accuracy loss on sparse-critical tasks).
  - γ (cumulative attention threshold, default 0.9): Higher values compute more blocks (closer to dense attention). Lower values increase sparsity but risk missing important context.

- **Failure signatures**:
  - Accuracy collapse on retrieval tasks while maintaining performance on synthesis tasks suggests over-aggressive sharing or τ too high.
  - High latency despite sparse attention suggests too many heads using dense fallback—check cluster size distribution and δ threshold.
  - Perplexity spike at specific context lengths indicates pattern construction failing at certain sequence positions—verify block alignment and cumulative threshold logic.

- **First 3 experiments**:
  1. **Cluster quality audit**: Visualize clustering result using t-SNE of 64-dim latent representations colored by cluster. Check coherence and identify noise clusters (>30% of heads).
  2. **Safety threshold sweep**: On held-out validation split, sweep τ ∈ {0.1, 0.2, 0.3, 0.5} and plot accuracy vs. latency to identify knee point.
  3. **Per-task ablation**: Run SharePrefill with pattern sharing disabled vs. enabled on individual InfiniteBench tasks to identify tasks where sharing helps most vs. where vertical-slash suffices.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the underlying mechanistic explanation for why attention head similarity relationships remain highly consistent across diverse inputs and tasks? The paper provides observational evidence but not the representational or architectural reasons for this consistency.

- **Open Question 2**: Can the pattern-sharing mechanism be effectively extended to accelerate the decoding phase in addition to prefilling? The current method is designed for prefilling where full KV caches exist; decoding has different computational constraints.

- **Open Question 3**: How can SharePrefill scale to multi-device distributed inference scenarios? The current implementation assumes a single device; distributed settings introduce communication overhead and synchronization challenges.

- **Open Question 4**: Is the autoencoder-based clustering approach necessary, or would simpler similarity metrics yield equivalent clustering quality? The paper does not compare against simpler baselines or justify the autoencoder design choice.

## Limitations
- Sensitivity to clustering quality—if offline clustering produces mixed or noisy clusters, sharing patterns across dissimilar heads will degrade accuracy
- JS distance threshold calibration appears arbitrary with no shown robustness to threshold variations
- Block size for sparse attention computation is not specified, creating ambiguity in kernel implementation
- Training data bias from using attention patterns from a single task may limit cross-task generalization

## Confidence

- **High Confidence**: The core mechanism of offline clustering followed by online pattern sharing is technically sound and the reported InfiniteBench scores (39.05 avg) are specific and verifiable. The ablation on head exclusion provides direct evidence for that design choice.

- **Medium Confidence**: The JS distance-based safety check is a reasonable approach, but threshold values appear arbitrary and the paper does not show robustness to variations. The block-wise pattern transfer assumption is plausible but not rigorously validated.

- **Low Confidence**: Autoencoder architecture details (input/output dimensions, layer configurations) are underspecified. The clustering process (distance metric, linkage method) is mentioned but not detailed. The exact block size for sparse attention is not stated.

## Next Checks

1. **Cluster Quality and Similarity Distribution Audit**: Visualize 64-dim latent representations using t-SNE or UMAP, color-coded by cluster. Plot Jaccard similarity distributions within and across clusters. Verify coherence and low inter-cluster similarities. If clusters show significant overlap or noise dominates, the sharing mechanism foundation is weak.

2. **JS Threshold Sensitivity Analysis**: Sweep τ ∈ {0.1, 0.15, 0.2, 0.25, 0.3} and δ ∈ {0.2, 0.25, 0.3, 0.35, 0.4} on validation split. Plot accuracy vs. latency tradeoff curves to identify knee points where further threshold relaxation yields minimal latency gains but significant accuracy drops.

3. **Cross-Task Pattern Generalization Test**: Train autoencoder on attention patterns from three different tasks (Retr.KV, Code.Debug, En.Dia). Evaluate clustering and sharing quality on held-out fourth task. Compare results to using task-specific autoencoders to test whether inter-head similarity consistency holds across diverse inputs.