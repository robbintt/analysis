---
ver: rpa2
title: The Viability of Crowdsourcing for RAG Evaluation
arxiv_id: '2504.15689'
source_url: https://arxiv.org/abs/2504.15689
tags:
- responses
- evaluation
- utility
- response
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the viability of crowdsourcing for Retrieval-Augmented
  Generation (RAG) evaluation through two complementary studies: response writing
  and response utility judgment. The authors compile the Crowd RAG Corpus 2025 (CrowdRAG-25),
  containing 903 human-written and 903 LLM-generated responses for 301 TREC RAG''24
  topics across three discourse styles, along with 47,320 pairwise human judgments
  and 10,556 pairwise LLM judgments across seven utility dimensions.'
---

# The Viability of Crowdsourcing for RAG Evaluation

## Quick Facts
- **arXiv ID**: 2504.15689
- **Source URL**: https://arxiv.org/abs/2504.15689
- **Reference count**: 40
- **Key outcome**: Crowdsourced pairwise judgments provide reliable and cost-effective RAG evaluation when controlling for worker competence, outperforming both pointwise alternatives and reference-based metrics.

## Executive Summary
This paper investigates crowdsourcing as a viable approach for evaluating Retrieval-Augmented Generation (RAG) systems. Through two complementary studies—response generation and utility judgment—the authors compile the Crowd RAG Corpus 2025 containing 903 human-written and 903 LLM-generated responses across three discourse styles, paired with 47,320 human pairwise judgments. The analysis demonstrates that pairwise judgment designs yield substantially higher reliability than pointwise alternatives, while competency-filtered crowd judgments outperform automated reference-based metrics and provide more cost-effective evaluation than LLM-as-judge approaches.

## Method Summary
The authors conduct two complementary studies to evaluate RAG responses through crowdsourcing. In the first study, human writers and GPT-4o generate responses to 301 TREC RAG'24 topics using retrieved passages, with responses written in three discourse styles (bullet, essay, news). Worker interactions are logged via the BigBro library to detect potential LLM assistance. In the second study, human judges on Prolific evaluate response pairs across seven utility dimensions using pairwise comparisons. The authors employ MACE competency scoring to filter low-quality annotators and use Bradley-Terry modeling to convert pairwise preferences into scalar utility scores. They compare human judgment-based rankings against reference-based metrics (BLEU, ROUGE, BERTScore) and LLM-as-judge outputs.

## Key Results
- Pairwise judgment design shows substantially higher inter-annotator agreement than pointwise alternatives (α = 0.41 vs α = 0.21 after competency correction)
- Human pairwise judgments provide reliable gold labels when controlling for worker competence through MACE filtering
- Reference-based evaluation metrics (BLEU, ROUGE, BERTScore) fail to correlate with human utility judgments across all tested dimensions
- Bullet-style responses exhibit higher preference judgments compared to essay or news styles
- Worker competency filtering more than doubles agreement strength while removing systematic under-performers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise judgment designs yield substantially higher inter-annotator agreement than pointwise alternatives for RAG utility evaluation.
- Mechanism: Comparative framing reduces cognitive load by replacing absolute scoring with relative preference, which aligns better with how humans naturally evaluate text quality. The neutral option (except for overall quality) allows judges to signal genuine indistinguishability rather than forcing arbitrary distinctions.
- Core assumption: Human judges can reliably discriminate between response pairs when quality differences exist, and low agreement primarily reflects either low worker competency or genuinely similar items rather than fundamental task ambiguity.
- Evidence anchors:
  - [abstract] "pairwise judgment design showing substantially higher reliability than pointwise alternatives"
  - [section 5.1] Competency-corrected pairwise agreement reaches α = 0.41 (doubled from 0.19), while pointwise agreement remains at α = 0.21 even after MACE correction
  - [corpus] Weak corpus signal; neighbor papers focus on LLM-as-judge rather than crowdsourcing methodology
- Break condition: If pointwise agreement approached pairwise levels after competency filtering, or if pairwise showed strong order biases (presentation sequence affecting judgments), the comparative advantage would be questionable.

### Mechanism 2
- Claim: Worker competency estimation via MACE and subsequent filtering produces reliable gold labels from noisy crowd judgments.
- Mechanism: MACE estimates per-worker trustworthiness by modeling annotation behavior, enabling competence-weighted majority voting. Removing the lowest-quartile workers (by competency score) while maintaining ≥3 judgments per item filters systematic under-performers without losing coverage.
- Core assumption: Low agreement stems from a minority of low-competency workers rather than task design flaws or inherent subjectivity in utility dimensions.
- Evidence anchors:
  - [abstract] "crowdsourced data can be a reliable source for RAG evaluation when controlling for worker competence"
  - [section 5.1] After competency correction, agreement more than doubles (mean α: 0.19 → 0.41); expert validation on 30-item subsample confirms crowd/expert failure on same minimally-differentiable items
  - [corpus] No direct corpus validation of MACE specifically for RAG evaluation; assumes transfer from other NLP tasks
- Break condition: If low competency correlates with specific utility dimensions (e.g., topical correctness requiring domain expertise), simple filtering could introduce systematic bias.

### Mechanism 3
- Claim: Reference-based evaluation metrics (BLEU, ROUGE, BERTScore) fail to correlate with human utility judgments across all tested dimensions.
- Mechanism: Content overlap metrics capture surface-level similarity but miss nuanced quality dimensions like logical coherence, coverage depth, and internal consistency. These require semantic and structural understanding that n-gram and embedding similarity do not provide.
- Core assumption: Human pairwise judgments represent valid ground truth for utility; low metric-human correlation indicates metric limitation rather than judgment noise.
- Evidence anchors:
  - [abstract] "Human pairwise judgments provide reliable and cost-effective results compared to [...] automated comparisons with human-written reference responses"
  - [section 5.3] Maximum Spearman correlation between metric-induced rankings and human-judgment rankings is 0.477 (BLEU for logical coherence); full ranking correlations remain low across all metrics and dimensions
  - [corpus] Consistent with neighbor paper findings on evaluation challenges (LFQA-E benchmarking shows similar limitations)
- Break condition: If specific utility dimensions showed high metric correlation (>0.7), targeted reference-based evaluation could be viable for those dimensions.

## Foundational Learning

- Concept: **Bradley-Terry model for ranking from pairwise comparisons**
  - Why needed here: The paper uses a Bradley-Terry variant to convert pairwise preference data into scalar utility scores for response ranking. Understanding this probabilistic model is essential for interpreting the judgment-based evaluation results.
  - Quick check question: Given pairwise win probabilities, can you explain why Bradley-Terry produces more robust rankings than simple win-count aggregation?

- Concept: **Krippendorff's α for inter-annotator agreement**
  - Why needed here: The paper relies on Krippendorff's α to establish reliability of crowd judgments. Unlike Cohen's κ, it handles multiple annotators, ordinal data, and missing values—all present in this study.
  - Quick check question: Why does α = 0.19 (raw) vs. α = 0.41 (competency-corrected) matter for trusting the downstream rankings?

- Concept: **Utility dimensions for RAG (coherence, coverage, consistency, correctness, clarity)**
  - Why needed here: The paper evaluates seven specific dimensions derived from Gienapp et al. [13]. Understanding what each dimension captures (e.g., deep vs. broad coverage) is necessary to interpret the finding that dimensions are empirically distinguishable by human judges.
  - Quick check question: Why might "topical correctness" and "coverage" show higher inter-correlation than "topical correctness" and "stylistic coherence"?

## Architecture Onboarding

- Component map:
  - Response Generation Layer -> Judgment Collection Layer -> Aggregation Layer -> Evaluation Layer

- Critical path:
  1. Retrieve top-20 passages per topic using high-recall system (webis-01)
  2. Generate responses in three discourse styles (bullet, essay, news)
  3. Collect 5 pairwise judgments per response pair across 7 utility dimensions
  4. Apply MACE competency filtering (remove lowest quartile workers)
  5. Generate gold labels via competence-weighted majority vote
  6. Rank responses per topic using Bradley-Terry; compare against metric baselines

- Design tradeoffs:
  - Pairwise vs. pointwise: Pairwise costs ~5× more (15 pairs × 5 judgments vs. 6 responses × 5 judgments) but yields α = 0.41 vs. α = 0.21 agreement
  - Human vs. LLM judges: LLM judgments cost ~10× less ($44.70 vs. $3,672.54) but show poor human alignment (max α = 0.37 for deep coverage in individual inference)
  - Topic coverage: 65 judged topics (20% sample) balances cost vs. statistical power; may limit generalizability

- Failure signatures:
  - Low agreement on specific pairs despite competency correction → likely minimally differentiable items (both responses similar quality)
  - High correlation across utility dimensions for a judge → judge may be collapsing to single "overall quality" heuristic
  - Sudden jumps in text length / absence of typing corrections in logs → likely LLM-assisted writing (should be flagged by BigBro)

- First 3 experiments:
  1. Replicate competency filtering on a new topic sample; verify that removing bottom 25% of workers consistently improves α by ~2× without introducing dimension-specific bias
  2. Test alternative aggregation (e.g., ELO ratings vs. Bradley-Terry) to validate ranking stability across methods
  3. Evaluate whether LLM-as-judge accuracy improves with few-shot examples from the human-judged dataset (label transfer experiment suggested in conclusion)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can few-shot learning or label transfer mechanisms improve the reliability of LLMs as utility judges for RAG evaluation?
- Basis in paper: [explicit] The conclusion states, "For future work, we will study label transfer from a pool of existing human-judged responses to a new, unjudged responses, akin to a few-shot setting."
- Why unresolved: The paper finds that zero-shot LLMs fail to produce consistent or correct utility judgments, often defaulting to general preferences rather than differentiating specific utility dimensions.
- What evidence would resolve it: A comparative study measuring the agreement (Krippendorff's $\alpha$) of few-shot LLM judgments against the human gold labels established in the CrowdRAG-25 corpus.

### Open Question 2
- Question: Do the observed differences in writing behavior and judgment capabilities generalize across diverse LLM architectures?
- Basis in paper: [explicit] The limitations section notes that the "investigation of LLM-written responses relies on a single model configuration [GPT-4o], leaving an investigation of other LLM configurations... for future work."
- Why unresolved: The specific findings regarding low readability and source attribution habits are tied to a single model version; it is unclear if these are universal traits of LLMs or specific to GPT-4o.
- What evidence would resolve it: Replicating the CrowdRAG-25 creation process using other state-of-the-art models (e.g., Claude, Llama, Mistral) and comparing the resulting text reuse and citation patterns.

### Open Question 3
- Question: How can RAG systems effectively adapt discourse styles to optimize user utility?
- Basis in paper: [explicit] The conclusion states that "bullet-style responses exhibit higher preference judgments" and explicitly "prompts future work on style adaption for RAG."
- Why unresolved: While the paper identifies that bullet lists are preferred over essays or news styles, current RAG systems typically default to continuous prose (essay style) without dynamic style adaptation.
- What evidence would resolve it: Developing RAG models that generate responses conditioned on specific discourse styles and evaluating if style-matching improves user preference scores compared to a one-size-fits-all approach.

## Limitations

- Findings depend on careful worker competency filtering, suggesting crowdsourcing reliability may not generalize to less controlled settings
- The pairwise design, while showing higher agreement, costs approximately 5× more than pointwise alternatives
- The corpus represents only 20% of available topics, limiting statistical power for rare phenomena

## Confidence

- **High confidence** in pairwise design superiority (α = 0.41 vs. 0.21) and reference-metric irrelevance (max correlation 0.477)
- **Medium confidence** in MACE competency filtering effectiveness, as validation relies on internal consistency and expert subsampling rather than external benchmarks
- **Low confidence** in LLM-as-judge cost-effectiveness given poor human alignment despite 10× cost reduction

## Next Checks

1. Test whether few-shot fine-tuning of LLM judges on the human-labeled dataset substantially improves human-LLM agreement
2. Evaluate alternative competency filtering methods (e.g., hierarchical clustering of judgment patterns) to detect systematic biases beyond simple performance quartiles
3. Conduct inter-corpus validation by comparing pairwise judgment reliability across different RAG datasets to assess generalizability of the methodology