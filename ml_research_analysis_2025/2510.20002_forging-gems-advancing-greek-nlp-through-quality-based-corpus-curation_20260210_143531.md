---
ver: rpa2
title: 'Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation'
arxiv_id: '2510.20002'
source_url: https://arxiv.org/abs/2510.20002
tags:
- greek
- legal
- corpus
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Greek Embedding Models (GEMs), a new family
  of transformer-based language models designed to address data scarcity and architectural
  stagnation in Greek NLP, particularly for specialized domains like law. The authors
  develop a quality-based corpus curation methodology and train GEMs on large-scale,
  meticulously curated corpora, including both general-domain and specialized legal
  datasets.
---

# Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation

## Quick Facts
- arXiv ID: 2510.20002
- Source URL: https://arxiv.org/abs/2510.20002
- Reference count: 40
- Primary result: New transformer-based Greek language models (GEMs) achieve up to 3.6% accuracy gains over state-of-the-art through quality-based corpus curation and architectural diversity.

## Executive Summary
This paper introduces the Greek Embedding Models (GEMs), a comprehensive family of transformer-based language models specifically designed for Modern Greek, addressing data scarcity and architectural stagnation in Greek NLP. The authors develop a quality-based corpus curation methodology that combines legal and general-domain texts, with strategic repetition of high-quality legal sub-corpora to enhance domain adaptation. Through systematic evaluation on three core natural language understanding benchmarks (NER, legal topic classification, NLI), GEM-RoBERTa and GEM-ConvBERT demonstrate statistically significant performance improvements over existing state-of-the-art models, establishing strong new baselines for the Greek NLP community.

## Method Summary
The methodology centers on a two-stage corpus curation process: initial deduplication using Exact LSH followed by MinHashLSH with Jaccard similarity threshold of 0.8, combined with quality filtering via Monocleaner to remove noise and low-quality content. The authors curate four distinct corpora: Legal Corpus (16.75GB), HQ Repeated (21.12GB) with strategic upsampling of high-quality legal sources, General-Domain (59GB) from OSCAR and Wikipedia, and Bilingual Legal (60/40 Greek-English split). Training employs domain-specific WordPiece tokenization (50,264 vocab) optimized for Greek morphology, with five transformer architectures (RoBERTa, ELECTRA, ConvBERT, Longformer, ModernBERT) pre-trained on AWS p4d.24xlarge (8×A100) GPUs using AdamW optimizer, linear scheduler with 6% warmup, and bfloat16 precision.

## Key Results
- GEM-RoBERTa and GEM-ConvBERT achieve statistically significant performance improvements over state-of-the-art models, with accuracy gains up to 3.6% on benchmark tasks.
- Quality-based corpus curation with targeted repetition (up to 4x) of high-quality legal sub-corpora proves more effective than raw corpus size for domain-specific performance.
- GEM-RoBERTa emerges as the most consistently high-performing model across all evaluation tasks, while GEM-ConvBERT excels particularly on hierarchical classification tasks.
- Bilingual Greek-English models show competitive and sometimes superior performance compared to monolingual counterparts, challenging monolingual-only assumptions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted repetition of high-quality legal sub-corpora improves domain-specific performance
- Mechanism: Strategic upsampling of authoritative legal sources (e.g., Raptarchis Legal Dictionary at up to 4x repetition) amplifies exposure to canonical terminology and usage patterns without introducing noise from web-crawled data.
- Core assumption: Data quality and domain relevance outweigh raw corpus size for morphologically rich languages.
- Evidence anchors:
  - [abstract] "targeted repetition of high-quality legal sub-corpora to enhance domain adaptation"
  - [Section 4] "HQ Repeated corpus demonstrates consistent effectiveness, emerging as the (near-)optimal configuration for most GEM architectures across the majority of tasks"
  - [corpus] Neighbor papers on pre-training data curation (Organize the Web) support systematic domain construction, but no direct external validation of repetition factors
- Break condition: Over-repetition beyond 4x showed diminishing returns in preliminary tests; performance degrades if repeated content is low-quality or narrow.

### Mechanism 2
- Claim: Architectural diversity enables complementary strengths across task types
- Mechanism: Different transformer architectures capture distinct linguistic patterns—ConvBERT's hybrid attention for local morphological patterns plus global structure, RoBERTa's optimized training for general robustness.
- Core assumption: No single architecture is optimal for all Greek NLP tasks, especially given morphological complexity.
- Evidence anchors:
  - [Section 4] "GEM-RoBERTa emerges as the most consistently high-performing model across all evaluation tasks, while GEM-ConvBERT presents a compelling alternative particularly excelling on hierarchical classification tasks"
  - [Section 4] Statistical analysis using Friedman Aligned-Ranks confirms GEM-RoBERTa and GEM-ConvBERT superiority
  - [corpus] Limited; neighbor papers focus on LLMs for Greek, not encoder architectures
- Break condition: DeBERTa underperformed in this study, suggesting architectural choice alone is insufficient—hyperparameter tuning and tokenizer compatibility matter.

### Mechanism 3
- Claim: Tokenizer selection impacts efficiency for morphologically rich languages
- Mechanism: WordPiece tokenizer trained on Greek legal corpus achieved optimal fertility (1.334) and bytes/token (9.02), reducing word fragmentation compared to Unigram (1.601 fertility).
- Core assumption: Lower fragmentation preserves morphological units, improving downstream task performance.
- Evidence anchors:
  - [Section 3.4] Table 2 shows WordPiece outperforms BPE and Unigram on efficiency metrics
  - [Section 4] DeBERTa's poor performance was attributed partly to its Unigram tokenizer
  - [corpus] No external papers validating tokenizer impact specifically for Greek
- Break condition: Tokenizer trained on mismatched domain (e.g., general-domain on legal tasks) may underperform regardless of algorithm.

## Foundational Learning

- Concept: **Masked Language Modeling vs. Replaced Token Detection**
  - Why needed here: GEMs use both MLM (RoBERTa, ConvBERT) and RTD (ELECTRA); understanding the difference explains why ELECTRA is more sample-efficient but didn't dominate results.
  - Quick check question: Can you explain why RTD trains on all tokens while MLM only trains on masked positions?

- Concept: **Sparse Attention Mechanisms**
  - Why needed here: Longformer uses local windowed + global attention to handle 1024+ token sequences; understanding this is critical for troubleshooting long-context performance.
  - Quick check question: How does Longformer's attention pattern differ from standard BERT's full attention, and what's the computational tradeoff?

- Concept: **Subword Tokenization Algorithms (BPE, WordPiece, Unigram)**
  - Why needed here: Paper explicitly compares tokenizers; fertility and bytes/token metrics are standard evaluation criteria.
  - Quick check question: Given a morphologically rich language, which tokenizer characteristic (fragmentation vs. vocabulary size) matters more for downstream performance?

## Architecture Onboarding

- Component map: Legal Corpus (16.75GB) -> HQ Repeated (21.12GB) -> General-Domain (59GB) -> Bilingual Legal (60/40 Greek-English split) -> RoBERTa, ELECTRA, ConvBERT, Longformer (1024 tokens), ModernBERT (1024 tokens, phase 1) -> WordPiece (50,264 vocab) or BPE tokenizers -> A100/H100 GPUs, AdamW optimizer, bfloat16 precision, 90/10 train/validation split

- Critical path:
  1. Corpus curation → preprocessing (noise filtering, deduplication via MinHashLSH)
  2. Tokenizer training on target corpus
  3. Pre-training with early