---
ver: rpa2
title: 'LPOI: Listwise Preference Optimization for Vision Language Models'
arxiv_id: '2505.21061'
source_url: https://arxiv.org/abs/2505.21061
tags:
- lpoi
- image
- object
- preference
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LPOI addresses object hallucination in VLMs by introducing the
  first listwise preference optimization framework. It automatically generates ranked
  image samples by masking critical objects and interpolating visibility levels, then
  trains models to rank these samples by object presence.
---

# LPOI: Listwise Preference Optimization for Vision Language Models

## Quick Facts
- **arXiv ID:** 2505.21061
- **Source URL:** https://arxiv.org/abs/2505.21061
- **Reference count:** 9
- **Primary result:** Listwise preference optimization reduces object hallucination in VLMs without extra annotations

## Executive Summary
LPOI introduces a novel listwise preference optimization framework specifically designed to address object hallucination in vision language models. The method automatically generates ranked image samples by strategically masking critical objects and interpolating visibility levels, then trains models to rank these samples by object presence. Unlike traditional pairwise methods, LPOI leverages listwise ranking to capture richer preference structures. The approach requires no additional annotations beyond standard pairwise preference data, making it practical for real-world deployment. Experiments demonstrate significant improvements in reducing hallucination rates across multiple benchmarks while maintaining output quality.

## Method Summary
LPOI operates by first creating a ranked list of image samples through an automated process that masks critical objects at varying levels of visibility. These ranked samples represent different degrees of object presence, from fully visible to completely masked. The model is then trained using listwise ranking objectives that explicitly learn to prefer images with higher object presence. This differs from pairwise approaches by considering the full ordering of samples rather than just binary preferences. The method integrates seamlessly with existing preference optimization pipelines and can be applied to any VLM architecture without requiring architectural modifications or additional training data beyond what's typically used for preference learning.

## Key Results
- LPOI reduces hallucination rates more effectively than DPO and mDPO across three VLM models
- Significant performance improvements on MMHalBench, AMBER, and Object HalBench evaluation benchmarks
- Human evaluators consistently prefer LPOI-generated responses for accuracy and reliability

## Why This Works (Mechanism)
The method works by leveraging listwise ranking to capture richer preference structures than pairwise methods. By automatically generating samples with varying object visibility through masking and interpolation, LPOI creates a more nuanced training signal that helps VLMs better understand the relationship between visual content and textual descriptions. The listwise approach allows the model to learn from the relative ordering of multiple samples simultaneously, providing stronger gradients for correcting hallucination tendencies. This multi-sample optimization targets the specific failure mode of object hallucination more directly than traditional pairwise preference optimization.

## Foundational Learning

**Listwise Ranking** - Why needed: Captures richer preference structures than pairwise methods; Quick check: Can the model rank 5 samples correctly by object presence?

**Automatic Sample Generation** - Why needed: Creates diverse training examples without manual annotation; Quick check: Are masked samples preserving contextual information?

**Object Masking Strategies** - Why needed: Controls visibility levels to create ranked samples; Quick check: Does masking preserve scene coherence?

**Preference Optimization** - Why needed: Trains models to align with human preferences; Quick check: Can the model distinguish between high and low quality outputs?

## Architecture Onboarding

**Component Map:** Input Images → Masking/Interpolation → Ranked Sample Generation → Listwise Ranking Loss → VLM Parameters

**Critical Path:** The core optimization loop where ranked samples flow through the VLM and the listwise ranking loss updates model parameters. This path is critical because it directly addresses the hallucination problem through relative ranking rather than absolute binary preferences.

**Design Tradeoffs:** The method trades computational complexity during training (generating and ranking multiple samples) for improved hallucination resistance. The automatic masking approach avoids annotation costs but may introduce biases based on masking strategies.

**Failure Signatures:** If masking is too aggressive, the model may lose contextual information needed for proper ranking. If the interpolation is too subtle, the model may not learn meaningful distinctions between ranks.

**Three First Experiments:** 1) Test ranking accuracy on synthetically generated ranked samples 2) Ablation study removing listwise component to measure impact 3) Human evaluation comparing LPOI vs baseline outputs on hallucination-prone prompts

## Open Questions the Paper Calls Out

None

## Limitations

- Dependence on automatic sample generation through masking may introduce biases based on specific masking strategies employed
- Evaluation focuses primarily on object hallucination metrics without exploring potential trade-offs with other VLM capabilities
- Method requires careful engineering of ranking system and masking strategies, representing non-trivial additional complexity

## Confidence

**High confidence:** Core claim that LPOI reduces object hallucination rates more effectively than DPO and mDPO is supported by quantitative results across three benchmarks and three VLM models, with ablation studies providing additional validation.

**Medium confidence:** Claim about improving "output quality" relies on human evaluation metrics that are subjective, though human annotators did prefer LPOI responses for accuracy and reliability.

**Low confidence:** Assertion that LPOI requires "no extra annotations" is technically true but potentially misleading, as the method requires non-trivial engineering of ranking systems and masking strategies.

## Next Checks

1. Conduct controlled study applying same masking/interpolation to generate "fake" preference data for DPO/mDPO baselines to determine if ranking methodology contributes to performance improvements

2. Perform comprehensive evaluation of downstream task performance to ensure hallucination reduction doesn't compromise overall VLM capability and generalization

3. Test method's robustness across diverse VLM architectures beyond the three models evaluated, including encoder-decoder and decoder-only transformer-based VLMs