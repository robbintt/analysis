---
ver: rpa2
title: Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs
  through Global Similarity and Weighted Sampling
arxiv_id: '2507.18977'
source_url: https://arxiv.org/abs/2507.18977
tags:
- temporal
- knowledge
- entities
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a model-agnostic framework for incremental
  learning in temporal knowledge graphs (TKGs) that addresses the challenges of long-tail
  and unseen entities. The proposed method combines an enhancement layer that leverages
  global relation-based entity similarity beyond local neighborhoods, and a weighted
  sampling strategy that prioritizes training examples involving infrequent entities.
---

# Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling

## Quick Facts
- arXiv ID: 2507.18977
- Source URL: https://arxiv.org/abs/2507.18977
- Reference count: 40
- Primary result: 10% MRR gain on ICEWS14, 15% on ICEWS18 for long-tail and unseen entity prediction

## Executive Summary
This paper introduces a model-agnostic framework to improve incremental learning in temporal knowledge graphs (TKGs) for long-tail and unseen entities. The approach combines a global relation-based enhancement layer with a frequency-inverse weighted sampling strategy, addressing the natural bias toward high-frequency entities in standard training. Experimental results on ICEWS14 and ICEWS18 datasets show significant improvements over baselines, particularly for entities with sparse neighborhoods or no prior embedding representation.

## Method Summary
The framework augments existing TKG models through two complementary mechanisms: (1) an enhancement layer that constructs entity representations by aggregating embeddings from globally similar entities based on shared relation types, weighted by temporal recency; and (2) a weighted sampling strategy that prioritizes training examples involving infrequent entities. The method operates incrementally across temporal snapshots, using a two-phase sampling mix to balance training stability with long-tail focus. Hyperparameters include $\lambda$ (balancing original vs. enhanced features), $\mu$ (temporal decay rate), and $\alpha$ (sampling ratio).

## Key Results
- 10% MRR improvement on ICEWS14 dataset compared to baseline models
- 15% MRR improvement on ICEWS18 dataset for long-tail entity prediction
- Significant gains in inductive link prediction for unseen entities
- Reduced catastrophic forgetting in incremental learning settings

## Why This Works (Mechanism)

### Mechanism 1: Global Relation-Based Entity Enhancement
The framework computes enhanced entity embeddings by aggregating information from entities that share relation types, rather than relying solely on local graph neighborhoods. This global similarity approach allows the model to generalize to sparse or unseen entities by leveraging semantic properties of entities that participate in identical interaction types.

### Mechanism 2: Frequency-Inverse Weighted Sampling
Training examples involving low-frequency entities are prioritized through inverse frequency weighting, directly countering the natural bias toward high-frequency "head" entities. A two-phase sampling mix ($\alpha$ weighted, $1-\alpha$ uniform) prevents overfitting to noise in rare samples while maintaining global graph structure awareness.

### Mechanism 3: Temporal Recency Weighting
When aggregating features from similar entities, contributions are weighted by temporal proximity to the query time using exponential decay. This ensures the enhanced representation reflects current behavioral patterns rather than outdated historical interactions.

## Foundational Learning

- **Concept: Inductive vs. Transductive Learning**
  - Why needed: The paper explicitly targets "unseen entities" (inductive setting). You must understand that standard TKG models (transductive) fail when test entities have no pre-trained embedding.
  - Quick check: Can the model generate an embedding for an entity ID that was not present in the initial training graph snapshot?

- **Concept: Long-Tail Distribution**
  - Why needed: The core motivation is that standard training overfits to "head" entities. The weighted sampling mechanism is a direct counter-measure to this distribution skew.
  - Quick check: Does the loss function treat a single occurrence of a rare entity equally to a frequent one, or is re-balancing required?

- **Concept: Message Passing in GNNs**
  - Why needed: The paper critiques standard GNNs for relying on "local neighborhood proximity." To implement the enhancement layer, you need to distinguish between aggregating neighbors (standard GNN) and aggregating globally similar entities (this paper's approach).
  - Quick check: If an entity has degree 0 (no local neighbors), can a standard GNN generate a meaningful embedding? (Answer: No, hence the need for global similarity).

## Architecture Onboarding

- **Component map:** Base Model (Titer) -> Enhancement Layer -> Frequency Registry -> Sampler -> Scorer
- **Critical path:**
  1. Sampler selects a batch of quadruples, favoring low-freq entities
  2. For query entity $s$, retrieve set $S_t(r)$ of entities that historically used relation $r$
  3. Compute $e_s = \lambda f(s) + \phi(d_s)(1-\lambda)g(s)$ where $g(s)$ aggregates embeddings of $S_t(r)$
  4. Pass enhanced $e_s$ to Base Model for loss calculation

- **Design tradeoffs:**
  - $\lambda$: Balances original entity features vs. global similarity features
  - $\alpha$: Balances training stability vs. long-tail focus
  - Computational Overhead: Retrieving $S_t(r)$ adds lookup cost

- **Failure signatures:**
  - Degraded Head Performance: MRR drops for frequent entities (α too high)
  - Inductive Failure: Zero performance on unseen entities (enhancement layer not triggered)
  - Stagnant Loss: Loss oscillates (global similarity signal too noisy)

- **First 3 experiments:**
  1. Ablation on Components: Base vs. Base+Sampling vs. Base+Enhancement vs. Full Model
  2. Hyperparameter Sensitivity: Sweep λ (0.3-0.7) and μ (0.1-0.5) on validation set
  3. Inductive Generalization Test: Evaluate specifically on test quadruples with entities not in training

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be further adapted to effectively represent entities with extremely sparse neighborhoods that lack sufficient historical interactions for global similarity aggregation? The current enhancement layer relies on aggregating information from similar entities based on past interactions; if an entity has almost no history, the aggregation mechanism has insufficient signal to construct a meaningful representation.

### Open Question 2
Can the integration of rich textual features or continuous fact extraction from Large Language Models (LLMs) be utilized to refine entity representations within this incremental framework? The current framework relies solely on structural graph topology and does not leverage unstructured text or pre-trained knowledge that could bridge the gap for unseen entities.

### Open Question 3
Is the proposed weighted sampling strategy robust against overfitting to noise in long-tail entities during later incremental stages, where training data might be unbalanced? While the paper demonstrates improved average performance, it does not deeply analyze the trade-off between emphasizing rare entities and potentially amplifying noise in the training signal over time.

## Limitations
- Performance degrades for entities with extremely sparse neighborhoods where insufficient historical interactions exist for meaningful global similarity aggregation
- Framework relies solely on structural graph data without leveraging external knowledge sources or textual features
- Weighted sampling strategy may amplify noise from rare entities without sufficient regularization analysis

## Confidence

| Claim | Confidence |
|-------|------------|
| 10% MRR improvement on ICEWS14 | High |
| Model-agnostic framework design | High |
| Global similarity enhancement mechanism | Medium |
| Frequency-inverse weighted sampling effectiveness | Medium |
| Temporal recency weighting contribution | Low |

## Next Checks
1. Verify the enhancement layer is properly triggered by checking if $S_t(r)$ is non-empty for test queries
2. Run ablation study to isolate contribution of sampling strategy vs. architectural enhancement
3. Test hyperparameter sensitivity of λ and μ on validation set to find optimal balance between local and global features