---
ver: rpa2
title: Steering MoE LLMs via Expert (De)Activation
arxiv_id: '2509.09660'
source_url: https://arxiv.org/abs/2509.09660
tags:
- experts
- wang
- steering
- expert
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SteerMoE is a framework for steering MoE LLMs by detecting and
  controlling behavior-linked experts. It identifies experts with distinct activation
  patterns across paired inputs exhibiting contrasting behaviors (e.g., safe vs.
---

# Steering MoE LLMs via Expert (De)Activation

## Quick Facts
- arXiv ID: 2509.09660
- Source URL: https://arxiv.org/abs/2509.09660
- Reference count: 40
- Primary result: SteerMoE improves faithfulness by up to +27% and safety by up to +20% via expert (de)activation in MoE LLMs

## Executive Summary
SteerMoE is a framework for steering MoE LLMs by detecting and controlling behavior-linked experts. It identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors (e.g., safe vs. unsafe) and selectively (de)activates them during inference to control behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, SteerMoE improves faithfulness by up to +27% and safety by up to +20%. In adversarial attack mode, it reduces safety by -41% alone and -100% when combined with existing jailbreak methods, bypassing all safety guardrails. This reveals that MoE routing exposes alignment vulnerabilities, as unsafe experts can be exploited even in aligned models.

## Method Summary
SteerMoE operates in two phases: detection and steering. During detection, it calculates Risk Difference (RD) scores for each expert by comparing activation frequencies between contrasting behavior pairs (e.g., safe vs. unsafe responses). This identifies experts statistically linked to specific behaviors. During steering, it modifies router logits at inference time using soft logit manipulation - adjusting log-softmax scores to favor or disfavor specific experts without changing model weights. The framework targets mid-layer experts where high-level behaviors are computed, applying activation to enhance desired behaviors or deactivation to suppress undesired ones.

## Key Results
- Improves faithfulness by up to +27% on benchmarks like FaithEval-Counterfactual
- Improves safety by up to +20% on benchmarks like AdvBench and StrongREJECT
- In adversarial mode, reduces safety by -41% alone and -100% when combined with jailbreak methods, completely bypassing safety guardrails

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Routing Difference
Specific experts are causally linked to behaviors if their activation frequencies differ significantly between contrastive prompt pairs. SteerMoE calculates Risk Difference (RD) scores to isolate experts active during target behavior while dormant in contrasting ones, assuming this difference reflects functional specialization rather than random noise.

### Mechanism 2: Soft Logit Manipulation
Modifying router logits to strictly favor or disfavor specific experts alters the model's computation path without modifying FFN weights. SteerMoE adjusts log-softmax scores with small additive margins to ensure target expert selection while maintaining the weighted mixture nature of MoE layers.

### Mechanism 3: Mid-Layer Behavioral Localization
High-level behavioral traits are predominantly computed in middle layers of MoE architecture. Steering is applied to these layers to modify the "intent" of computation, as early layers handle syntax/vocabulary while mid-layers show highest RD scores for behavioral experts.

## Foundational Learning

- **Top-K Gating in MoE**: SteerMoE manipulates the Top-K selection process where only a subset of experts are active per token. Understanding this is critical because the logit manipulation logic depends on controlling which experts are selected. *Quick check: If k=2 and you subtract ε from the top expert's logit, what happens to the expert ranked 3rd?*

- **Behavioral Contrastive Analysis**: The method relies on comparing activation patterns between contrasting behaviors. Understanding how subtraction of activation rates isolates the "delta" responsible for behavior difference is essential. *Quick check: Why is Risk Difference (p₁ - p₂) preferred over raw activation counts for identifying behavior-linked experts?*

- **Inference-Time Intervention**: This technique modifies the forward pass without retraining. Understanding the distinction between modifying weights (training) versus activations/logits (inference steering) is critical. *Quick check: Does SteerMoE require backpropagation or gradient updates during the steering phase?*

## Architecture Onboarding

- **Component map**: Pair Constructor (SQuAD/BeaverTails) → Activation Logger (Hooks) → RD Calculator (Identify Expert Indices) → Input Prompt → Forward Pass → Router Hook (Modify Logits via Eq 7/8) → Expert FFNs → Output

- **Critical path**: The accuracy of the Offline Phase (Risk Difference calculation). If the paired dataset does not cleanly isolate the target behavior, the identified experts will be wrong and steering will fail.

- **Design tradeoffs**: 
  - Activation vs. Deactivation: Deactivation of "unsafe" experts is preferred over activation of "safe" ones to preserve fluency
  - Scope: Steering all layers vs. mid-layers only - targeting mid-layers balances effectiveness against interference with syntactic processing

- **Failure signatures**:
  - Fluency Collapse: Grammatically broken outputs from over-constraining the router
  - Over-Refusal: Model refuses benign prompts from over-steering "safe" experts
  - Jailbreak Success: Safety drops to 0% when combined with jailbreak methods

- **First 3 experiments**:
  1. Implement logit manipulation hook and deactivate random 10 experts to verify fluency preservation
  2. Use BeaverTails-derived expert list to deactivate top 20 "unsafe" experts on red-team benchmark and measure safety change
  3. Attempt to activate "unsafe" experts on benign prompt to verify spontaneous toxic output generation

## Open Questions the Paper Calls Out

- Can SteerMoE be extended to perform dynamic, token-aware steering rather than applying static expert sets? The paper explicitly lists this as a primary future direction.

- How can alignment training be adapted to ensure safety across all experts and routing paths? The paper calls for developing methods that make all experts and routes safe and reliable.

- Is there a principled, automated method to determine optimal number of experts to manipulate without degrading capabilities? The paper notes the current manual tuning approach and inherent trade-off with general performance.

## Limitations
- The correlation between expert activation and behavior could be confounded by prompt artifacts rather than true semantic alignment
- Steering effectiveness varies significantly between models, suggesting potential lack of uniform generalization across architectures
- Complete safety bypass results lack sufficient explanation of underlying mechanisms when combined with jailbreak methods

## Confidence
- **High Confidence**: Contrastive Risk Difference calculation mechanism is mathematically sound and reproducible; logit manipulation approach is technically feasible
- **Medium Confidence**: Steering effectiveness claims are supported by benchmarks but lack ablation studies on expert selection validity
- **Low Confidence**: Adversarial attack results claiming complete safety bypass are concerning but insufficiently characterized

## Next Checks
1. Design experiment where prompt pairs differ only in superficial features (length, formatting) to validate that SteerMoE requires genuine behavioral contrast for effectiveness
2. Perform ablation study using randomly selected experts with similar activation counts versus Risk Difference-selected experts to validate the selection method
3. Apply SteerMoE to an MoE model with different routing mechanisms (conditional MoE or switch transformer) to test robustness of Top-K gating assumptions across architectures