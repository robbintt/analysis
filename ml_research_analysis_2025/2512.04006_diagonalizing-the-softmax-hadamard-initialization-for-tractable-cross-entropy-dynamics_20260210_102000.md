---
ver: rpa2
title: 'Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy
  Dynamics'
arxiv_id: '2512.04006'
source_url: https://arxiv.org/abs/2512.04006
tags:
- singular
- dynamics
- values
- hadamard
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first rigorous proof of convergence to
  neural collapse under multi-class cross-entropy training in a non-convex setting.
  The key innovation is showing that Hadamard initialization diagonalizes the softmax
  operator, freezing the singular vectors of the weight matrices and reducing the
  dynamics entirely to their singular values.
---

# Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics
## Quick Facts
- arXiv ID: 2512.04006
- Source URL: https://arxiv.org/abs/2512.04006
- Reference count: 40
- First rigorous proof of neural collapse convergence under multi-class cross-entropy training in non-convex setting

## Executive Summary
This paper establishes the first rigorous convergence proof for neural collapse under unconstrained features model with cross-entropy loss. The key innovation is using Hadamard initialization to diagonalize the softmax operator, which freezes singular vectors and reduces dynamics to singular values only. This enables extending exact dynamics analysis from MSE to CE loss, despite the non-convex landscape and presence of spurious critical points.

The authors construct a KL divergence-based Lyapunov function that guarantees global convergence to neural collapse. Surprisingly, they discover non-monotonic convergence behavior for K≥8 classes, where natural distance metrics can initially increase before decreasing. The paper also demonstrates exponential slowdown as logit norms grow, and validates results through numerical experiments showing both Hadamard and standard random initialization converge to neural collapse.

## Method Summary
The paper introduces Hadamard initialization as a technique to diagonalize the softmax operator, transforming the non-convex cross-entropy optimization problem into a tractable dynamical system. By carefully constructing initial weight matrices using Hadamard matrices, the authors ensure that the softmax's Jacobian becomes diagonal, effectively decoupling the dynamics of singular values from singular vectors. This reduction enables the construction of an explicit Lyapunov function based on KL divergence that proves global convergence to neural collapse. The analysis extends previous MSE dynamics results to the CE setting by leveraging this diagonalization property.

## Key Results
- First proof of neural collapse convergence under multi-class CE loss in non-convex unconstrained features model
- Hadamard initialization diagonalizes softmax, freezing singular vectors and reducing dynamics to singular values
- Non-monotonic convergence discovered for K≥8 classes - distance metrics can initially increase before decreasing
- Exponential slowdown in dynamics as logit norms grow during training

## Why This Works (Mechanism)
The Hadamard initialization technique works by exploiting the orthogonal structure of Hadamard matrices to diagonalize the softmax operator. When weight matrices are initialized with Hadamard matrices, the softmax Jacobian becomes diagonal because the Hadamard basis aligns with the eigenvector structure of the softmax. This diagonalization decouples the dynamics: singular vectors become fixed (frozen) while only singular values evolve. This reduction transforms the complex non-convex CE optimization into a tractable system where singular value dynamics can be exactly analyzed. The frozen singular vectors eliminate rotational degrees of freedom, making the KL divergence-based Lyapunov function construction possible.

## Foundational Learning
- **Neural Collapse Phenomenon**: Occurs when features from same class collapse to single point while class means form simplex ETF structure. Needed to understand the target geometry. Quick check: Verify class features converge to single points in feature space.
- **Unconstrained Features Model**: Feature extractor fixed, only classifier trained. Needed for tractable analysis. Quick check: Confirm feature extractor remains frozen during training.
- **Hadamard Matrices**: Orthogonal matrices with entries ±1. Needed for diagonalizing softmax. Quick check: Verify Hadamard matrix properties (orthogonal, ±1 entries).
- **Lyapunov Functions**: Scalar functions proving convergence by decreasing along trajectories. Needed for global convergence proof. Quick check: Confirm KL divergence decreases monotonically.
- **Softmax Operator Diagonalization**: When softmax Jacobian becomes diagonal. Needed to simplify dynamics. Quick check: Verify softmax Jacobian is diagonal under Hadamard initialization.

## Architecture Onboarding
**Component Map**: Hadamard Initialization -> Diagonalized Softmax -> Frozen Singular Vectors -> KL Lyapunov Function -> Global Convergence

**Critical Path**: The critical path is Hadamard initialization → diagonalized softmax → frozen singular vectors → reduced dynamics → KL divergence construction → convergence proof. Each step depends on the previous one; failure at any stage breaks the entire analysis.

**Design Tradeoffs**: Hadamard initialization provides theoretical tractability but may not be optimal for practical performance. Standard random initialization biases toward simplex ETF naturally but lacks theoretical guarantees. The choice involves trading mathematical rigor for practical applicability.

**Failure Signatures**: If Hadamard diagonalization fails (finite precision, batch effects), singular vectors rotate and dynamics become coupled, breaking the Lyapunov function construction. If KL divergence doesn't decrease, convergence proof fails. If logit norms grow too fast, exponential slowdown prevents practical convergence.

**First Experiments**:
1. Implement Hadamard initialization and verify softmax Jacobian is diagonal within numerical tolerance
2. Track singular vector evolution to confirm they remain frozen during training
3. Measure KL divergence along optimization trajectory to verify monotonic decrease

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Finite-precision effects may break exact Hadamard diagonalization, limiting practical applicability
- Exponential slowdown as logit norms grow could prevent convergence in deep architectures
- Practical validation on real-world deep learning scenarios is limited to synthetic settings

## Confidence
High: Mathematical framework for Hadamard diagonalization and softmax dynamics is rigorously proven and well-established.
Medium: Extension from MSE to CE loss relies heavily on Hadamard initialization assumption; non-monotonic behavior may be sensitive to implementation details.
Low: Practical implications for real-world deep learning systems not fully explored; interaction with architectural components remains unclear.

## Next Checks
1. Implement finite-precision experiments to quantify how floating-point arithmetic affects Hadamard diagonalization and convergence rates
2. Test initialization schemes on practical deep learning architectures with real-world datasets to validate theoretical benefits
3. Conduct ablation studies varying K and feature dimensions to map boundary conditions for non-monotonic convergence emergence