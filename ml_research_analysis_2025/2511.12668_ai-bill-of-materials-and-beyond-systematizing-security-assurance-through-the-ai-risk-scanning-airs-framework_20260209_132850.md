---
ver: rpa2
title: 'AI Bill of Materials and Beyond: Systematizing Security Assurance through
  the AI Risk Scanning (AIRS) Framework'
arxiv_id: '2511.12668'
source_url: https://arxiv.org/abs/2511.12668
tags:
- airs
- framework
- data
- assurance
- runtime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the AI Risk Scanning (AIRS) Framework, which
  addresses the gap between descriptive AI documentation and verifiable security evidence.
  Through three iterative pilot studies, the authors evolved from a descriptive AI
  Bill of Materials to a threat-model-based methodology that automatically generates
  machine-readable assurance artifacts.
---

# AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework

## Quick Facts
- arXiv ID: 2511.12668
- Source URL: https://arxiv.org/abs/2511.12668
- Authors: Samuel Nathanson; Alexander Lee; Catherine Chen Kieffer; Jared Junkin; Jessica Ye; Amir Saeed; Melanie Lockhart; Russ Fink; Elisha Peterson; Lanier Watkins
- Reference count: 40
- Primary result: AIRS Framework automatically generates machine-readable security evidence from threat models, achieving AUC 0.646 for contamination detection

## Executive Summary
The AI Risk Scanning (AIRS) Framework bridges the critical gap between descriptive AI documentation and verifiable security evidence by transforming MITRE ATLAS threat categories into actionable assurance artifacts. Through three iterative pilot studies, the authors evolved from basic AI Bill of Materials to a sophisticated threat-model-based methodology that produces machine-readable evidence for model integrity, packaging safety, structural adapters, and runtime behaviors. The framework's proof-of-concept on a GPT-OSS-20B model demonstrated enforcement of safe loader policies with per-shard hash verification, while runtime probes showed reasonable detection performance for contamination analysis.

## Method Summary
The AIRS Framework employs an iterative development approach that begins with threat modeling and progresses through automated scanning and runtime probing to generate verifiable security evidence. The methodology involves implementing validators for packaging safety (hash verification, safe loader policies), structural adapters (subgraph isolation, dependency checking), and runtime probes (prompt injection detection, contamination analysis). Evidence is automatically generated through tools like TorchInspection and Trivy, producing machine-readable artifacts that map directly to MITRE ATLAS threat categories. The framework uses requirement levels (Must/Should/May) to support linting and quality checks while maintaining flexibility for different deployment scenarios.

## Key Results
- Runtime contamination probes achieved AUC score of 0.646 and TPR@5%FPR of 0.500 for distinguishing contaminated from clean inputs
- Safe loader policies enforced per-shard hash verification on GPT-OSS-20B model with evidence generation
- Comparative analysis revealed strong alignment with SPDX 3.0 and CycloneDX 1.6 standards but identified critical gaps in representing AI-specific assurance fields

## Why This Works (Mechanism)
The framework succeeds by operationalizing abstract threat categories into concrete, verifiable evidence through automated tools and runtime monitoring. By systematically mapping MITRE ATLAS threats to specific validators and probes, AIRS creates a direct link between security concerns and actionable evidence generation. The requirement-based approach (Must/Should/May) enables flexible quality assurance while maintaining rigorous standards, and the machine-readable output ensures interoperability with existing security tooling and compliance frameworks.

## Foundational Learning
- **Threat modeling for AI systems**: Understanding how to translate abstract security threats into concrete verification requirements
  - Why needed: AI systems have unique threat surfaces requiring specialized security approaches
  - Quick check: Can you map common AI attack vectors to specific assurance requirements?
- **Automated evidence generation**: Creating machine-readable artifacts that prove security properties
  - Why needed: Manual verification doesn't scale across complex AI supply chains
  - Quick check: Are generated artifacts consumable by standard security tooling?
- **Runtime monitoring for model behavior**: Detecting malicious inputs and contamination in deployed models
  - Why needed: Many AI threats manifest during inference rather than in static analysis
  - Quick check: Can the system distinguish between benign and malicious inputs with acceptable accuracy?

## Architecture Onboarding

Component map: Threat Model -> AIRS Schema -> Validators/Probes -> Evidence Generation -> Machine-readable Output

Critical path: Model ingestion → Threat mapping → Automated scanning → Runtime monitoring → Evidence compilation

Design tradeoffs: Static vs. dynamic analysis (comprehensive vs. performant), general vs. task-specific assurance profiles, detailed vs. actionable scoring

Failure signatures: False positives in contamination detection, incomplete threat coverage, integration complexity with existing toolchains

First experiments:
1. Run TorchInspection on a simple PyTorch model to generate packaging evidence
2. Implement a basic runtime probe for prompt injection detection
3. Generate a minimal AIRS profile for a pre-trained model and validate against the schema

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should task- and environment-specific assurance profiles be designed and calibrated to weight AIRS categories for actionable risk interpretation?
- Basis in paper: [explicit] "Designing and calibrating such profiles is future work; AIRS is structured to support it."
- Why unresolved: Requirement levels (Must/Should/May) support linting but not decisions; single global scores are misleading across use cases.
- What evidence would resolve it: Empirical validation of profiled interpretations with transparent weighting across deployment scenarios.

### Open Question 2
- Question: Can runtime contamination probes achieve detection performance sufficient for acceptance testing rather than triage?
- Basis in paper: [explicit] "Our contamination probes...are informative but not definitive. Small-sample studies and overlapping distributions highlight the need for stronger methods and careful calibration."
- Why unresolved: Current AUC of 0.646 and TPR@5%FPR of 0.500 show weak separability between contaminated and clean inputs.
- What evidence would resolve it: Probe methods achieving consistently high AUC (>0.85) with validated thresholds across diverse model families.

### Open Question 3
- Question: How can AIRS be extended to cover system-level threats including tool-calling, RAG integration, and application-layer abuses while preserving auditability?
- Basis in paper: [explicit] "We will leverage insights and lessons learned to expand the threat model and add system-level profiles that incorporate application-layer catalogs with evidence hooks."
- Why unresolved: Current scope is deliberately restricted to model-level artifacts for tractability; orchestration and tooling are deferred.
- What evidence would resolve it: Evidence-bound schema extensions for tool-call allowlists, taint tracking, and output sanitizers.

### Open Question 4
- Question: What adaptations are required to generalize AIRS categories, validators, and runtime probes from text-only LLMs to multimodal models?
- Basis in paper: [explicit] "Extending to other modalities will require an expanded threat model with additional fields while maintaining similar principles of verifiability."
- Why unresolved: Runtime probes and packaging checks are calibrated for text; image/audio modalities introduce distinct attack surfaces.
- What evidence would resolve it: Modality-specific threat mappings and validated probe methodologies for vision and audio models.

## Limitations
- Current evaluation limited to single open-source model (GPT-OSS-20B), restricting generalizability
- Runtime contamination detection probes show room for improvement (AUC 0.646, TPR@5%FPR 0.500)
- Framework effectiveness in production environments with complex supply chains remains unverified

## Confidence

**High Confidence**: The methodology for transforming threat categories into verifiable security evidence through automated scanning and runtime probes is technically sound and well-documented

**Medium Confidence**: The framework's alignment with SPDX 3.0 and CycloneDX standards is well-analyzed, though real-world adoption barriers and integration challenges are not fully explored

**Medium Confidence**: The proof-of-concept results demonstrate feasibility but require validation across broader model families and threat scenarios

## Next Checks

1. Conduct comprehensive testing across multiple model architectures (LLMs, diffusion models, multimodal systems) to validate framework scalability and adaptability
2. Implement controlled adversarial testing to evaluate probe effectiveness against sophisticated contamination techniques and evasion strategies
3. Perform field trials with diverse stakeholders (developers, auditors, compliance officers) to assess usability, integration complexity, and practical adoption barriers