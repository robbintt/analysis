---
ver: rpa2
title: 'AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference Serving
  for Diverse Applications'
arxiv_id: '2503.13737'
source_url: https://arxiv.org/abs/2503.13737
tags:
- requests
- accel
- time
- utilization
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AccelGen is a high-throughput LLM inference serving system designed
  to handle diverse applications with both short and long prompts while meeting heterogeneous
  SLOs for iteration time. It introduces four core components: SLO-guaranteed dynamic
  chunking, which dynamically adjusts chunk sizes to maximize GPU compute utilization
  while meeting iteration-level SLOs; Iteration-level SLO-based task prioritization,
  which prioritizes tight-SLO requests and batches requests with similar SLOs; Multi-resource-aware
  batching, which selects queued requests to maximize the utilizations of both GPU
  compute resource and key-value cache (KVC).'
---

# AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference Serving for Diverse Applications

## Quick Facts
- arXiv ID: 2503.13737
- Source URL: https://arxiv.org/abs/2503.13737
- Reference count: 40
- Primary result: 1.42-11.21X higher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment

## Executive Summary
AccelGen is a high-throughput LLM inference serving system designed to handle diverse applications with both short and long prompts while meeting heterogeneous SLOs for iteration time. It introduces four core components: SLO-guaranteed dynamic chunking, which dynamically adjusts chunk sizes to maximize GPU compute utilization while meeting iteration-level SLOs; Iteration-level SLO-based task prioritization, which prioritizes tight-SLO requests and batches requests with similar SLOs; Multi-resource-aware batching, which selects queued requests to maximize the utilizations of both GPU compute resource and key-value cache (KVC). Trace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X higher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment, and 1.61-12.22X lower response latency compared to the state-of-the-art approaches. It achieves performance near the Oracle, which optimally maximizes goodput.

## Method Summary
AccelGen is built on vLLM and implements three key mechanisms for heterogeneous SLO-guaranteed inference serving. First, it dynamically chunks long prompts using a token budget formula (S_b = S_{pf} \cdot \frac{SLO_{min}}{\bar{T}_{pf}}) to maximize GPU utilization while preserving latency guarantees. Second, it prioritizes requests by remaining time to deadline (T_r = SLO - T_w - T_e) and groups similar-SLO requests to form efficient batches. Third, it implements multi-resource-aware batching that jointly considers GPU compute and KVC availability using Euclidean distance minimization. The system uses OPT-13B and OPT-175B models, profiling pivot forward sizes (768 and 1280 tokens respectively) to determine optimal batch sizes for GPU saturation.

## Key Results
- Achieves 1.42-11.21X higher throughput compared to state-of-the-art approaches
- Delivers 1.43-13.71X higher goodput (tokens/s within 1s window)
- Attains 37-90% higher SLO satisfaction rates across heterogeneous workloads
- Reduces response latency by 1.61-12.22X for diverse request mixes

## Why This Works (Mechanism)

### Mechanism 1: SLO-guaranteed Dynamic Chunking
- Claim: Dynamically adjusting chunk sizes based on iteration-level SLO constraints maximizes GPU utilization while preserving latency guarantees.
- Mechanism: The system calculates a token budget $S_b = S_{pf} \cdot \frac{SLO_{min}}{\bar{T}_{pf}}$, where $S_{pf}$ is the pivot forward size that saturates GPU capacity. Long prompts are chunked on-the-fly to exactly fill this budget per iteration, avoiding both underutilization (static chunking) and SLO violations (oversized batches). Additionally, Exclusive Request Allocation (ERA) limits concurrent long-prompt requests to accelerate KVC release.
- Core assumption: Iteration time scales predictably with forward size, and profiling-derived pivot forward sizes transfer across workloads with similar GPU architectures.
- Evidence anchors: [abstract] "SLO-guaranteed dynamic chunking, which dynamically adjusts chunk sizes to maximize GPU compute utilization while meeting iteration-level SLOs"; [Section 4.2] Equation (6) and ERA strategy description; Figure 6 shows ERA reduces JCT by limiting concurrent KVC occupancy.

### Mechanism 2: Iteration-level SLO-based Task Prioritization
- Claim: Ordering requests by remaining time to deadline and batching similar-SLO requests together improves both SLO attainment and throughput.
- Mechanism: Requests are sorted by $T_r = SLO - T_w - T_e$ (remaining time = SLO minus waiting time minus estimated execution time). Urgent requests ($T_r \approx T_{max}$) are guaranteed batch inclusion. This grouping prevents loose-SLO requests from constraining batch size when mixed with tight-SLO requests.
- Core assumption: Remaining execution time can be accurately estimated from average chunk count and maximum batch execution time; heterogeneous SLO distributions have sufficient request density within SLO bands to form effective batches.
- Evidence anchors: [abstract] "Iteration-level SLO-based task prioritization, which prioritizes tight-SLO requests and batches requests with similar SLOs"; [Section 4.3] Figure 7 shows ordering achieves 37%/34% higher GPU/KVC utilization and 16% higher SLO attainment vs FIFO.

### Mechanism 3: Multi-resource-aware Batching
- Claim: Selecting requests that jointly minimize the distance to available GPU and KVC resources improves dual-resource utilization compared to FCFS or GPU-only optimization.
- Mechanism: After urgent requests are allocated, the system selects non-urgent requests minimizing $\sqrt{(A_c - D_i^c)^2 + (A_m - D_i^m)^2}$ where $A_c, A_m$ are available GPU tokens and KVC space, and $D_i^c, D_i^m$ are request demands. This prevents scenarios where one resource blocks allocation of the other.
- Core assumption: KVC demand per request can be accurately estimated from sequence length and block allocation patterns; the Euclidean distance heuristic effectively balances both resources across diverse request mixes.
- Evidence anchors: [abstract] "Multi-resource-aware batching, which selects queued requests to maximize the utilizations of both GPU compute resource and key-value cache (KVC)"; [Section 4.4, Algorithm 2] Lines 7-12 implement the distance-based selection; Figure 11e shows 31-63% higher GPU utilization and 7.4-13.2% lower unallocated KVC vs baselines.

## Foundational Learning

- Concept: **KV Cache (KVC) allocation and lifecycle**
  - Why needed here: Multi-resource-aware batching requires understanding KVC block allocation (Equation 5), why long prompts increase KVC pressure, and how preemption/ERA affect release timing.
  - Quick check question: Given block size $b=128$, what is the KVC demand for a request with 500 prompt tokens and 300 generated tokens? (Answer: $\lceil 800/128 \rceil \times 128 = 896$ tokens)

- Concept: **Transformer iteration structure (prefill vs. decode)**
  - Why needed here: SLO-guaranteed chunking splits prefill into chunks; understanding that decode depends on cached KV values explains why chunking doesn't require recomputation.
  - Quick check question: Why can a long prompt be chunked across iterations without recomputing attention for earlier chunks? (Answer: KV values are cached and reused; only the final chunk's last token needs logit computation for generation)

- Concept: **Forward size and GPU saturation**
  - Why needed here: Token budget determination relies on pivot forward size $S_{pf}$; understanding GPU compute scaling with $S_f$ (Equation 4) is essential for profiling.
  - Quick check question: For short sequences ($S_f \ll 6H$), which term dominates total operations in Equation (4)? (Answer: The FCL term $24 S_f H^2$; attention scales quadratically but is negligible for short $S_f$)

## Architecture Onboarding

- Component map: Request arrival -> SLO specification -> $T_r$ calculation -> Queue ordering -> Iteration completion -> $S_b$ recalculation -> Urgent request allocation -> Resource-aware request/chunk selection (Algorithm 2) -> Batch dispatch to vLLM -> KVC allocation -> Execution

- Critical path:
  Request arrival → SLO specification (TTFT/TBT or JCT) → $T_r$ calculation → Queue ordering → Iteration completion → $S_b$ recalculation → Urgent request allocation → Resource-aware request/chunk selection (Algorithm 2) → Batch dispatch to vLLM → KVC allocation → Execution

- Design tradeoffs:
  1. **Chunk granularity vs. overhead**: Smaller chunks improve utilization but increase scheduling overhead (Figure 12f shows 0.24-2.4ms overhead)
  2. **SLO strictness vs. throughput**: Tight $SLO_{min}$ reduces token budget and batch size; loose SLOs improve throughput but risk user experience
  3. **ERA concurrency limit vs. long-prompt latency**: Limiting concurrent long prompts improves throughput but may delay individual long requests

- Failure signatures:
  1. **KVC overflow with high long-prompt ratio**: If long prompts exceed configured KVC capacity despite ERA, preemption cascades occur; monitor unallocated KVC metric (Figure 11e)
  2. **SLO miss clustering**: If $SLO_{min}$ is consistently violated, check if profiling $T_{pf}$ is stale or GPU contention exists
  3. **Scheduler overhead spike**: If batch formation exceeds 1-2ms, request queue may be too large; cap candidate set in Algorithm 2 Line 3

- First 3 experiments:
  1. **Baseline profiling**: Run OPT-13B with uniform 512-token prompts at increasing batch sizes to measure $S_{pf}$ and validate Equation (4) scaling; confirm GPU utilization saturates near 768 tokens as stated
  2. **SLO attainment stress test**: Submit mixed workload (35% BookCorpus, 65% Alpaca/ShareGPT) with heterogeneous TBT SLOs (0.05s, 0.1875s, 0.5s); measure per-SLO-band attainment and compare vs. vLLM FCFS
  3. **Dual-resource utilization audit**: At each iteration, log $A_c$, $A_m$, selected requests' $D_i^c$, $D_i^m$; verify Euclidean distance minimization correlates with <15% unallocated KVC (Figure 11e baseline)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating an accurate output token prediction mechanism enable AccelGen to achieve optimal scheduling performance comparable to the Oracle baseline?
- **Basis in paper:** [explicit] Section 7 (Limitations and Discussion) states, "If the number of generated tokens can be accurately predicted, AccelGen can be further optimized... to approach the performance of Oracle. For instance, it can avoid preempting jobs that are nearing completion... We will study this topic."
- **Why unresolved:** The current design relies on estimation or unknown output lengths, leading to sub-optimal Key-Value Cache (KVC) eviction decisions (e.g., preempting a job close to completion) compared to the Oracle which possesses clairvoyant knowledge of request lengths.
- **What evidence would resolve it:** A modified AccelGen implementation utilizing a high-accuracy token prediction model that demonstrates significantly higher goodput and lower JCT, narrowing the performance gap with the Oracle baseline.

### Open Question 2
- **Question:** How can an adaptive chunking size mechanism prevent processing delays for long prompts without sacrificing GPU utilization or violating SLOs?
- **Basis in paper:** [explicit] Section 7 notes, "Chunking long prompts has the potential to introduce delays in their processing. We aim to explore adaptive methods for determining the chunking size, ensuring that SLOs for long prompts are met."
- **Why unresolved:** The current dynamic chunking method focuses on meeting iteration-level SLOs and filling token budgets, but does not explicitly account for the cumulative delay specific chunking strategies might impose on the total completion time of very long prompts.
- **What evidence would resolve it:** Experiments showing a new adaptive algorithm reducing the tail latency of long-prompt completion times while maintaining the throughput advantages of the dynamic chunking approach.

### Open Question 3
- **Question:** What is the optimal preemption strategy for heterogeneous SLOs that minimizes Head-of-Line (HOL) blocking and memory conflicts?
- **Basis in paper:** [explicit] Section 7 states, "Exploring the impact of preempting long prompts and designing a strategy to select requests for preemption based on factors like sequence length and SLO is an important aspect that has not been thoroughly studied in AccelGen."
- **Why unresolved:** AccelGen currently relies on vLLM's default preemption logic for KVC overflow, which does not consider the specific iteration-level SLOs or sequence lengths in its selection of victim requests.
- **What evidence would resolve it:** A comparative analysis of preemption policies (e.g., preempting by longest remaining time vs. loosest SLO) demonstrating improved SLO attainment under memory pressure.

### Open Question 4
- **Question:** How does AccelGen's performance scale across different model architectures (e.g., Mixture-of-Experts) and diverse hardware configurations?
- **Basis in paper:** [explicit] The Conclusion states, "Currently, AccelGen is only tested for the mentioned models [OPT, Llama] and the GPU settings. We will test it on more different models and machine settings."
- **Why unresolved:** The evaluation is limited to dense transformer models (OPT, Llama) on specific AWS p4d instances. Generalizability to sparsely activated models or different memory hierarchies is unproven.
- **What evidence would resolve it:** Benchmark results from AccelGen deployed on Mixture-of-Experts (MoE) models or consumer-grade GPU clusters showing similar throughput and SLO attainment improvements over baselines.

## Limitations
- The paper relies heavily on accurate iteration time profiling (T̄pf) and pivot forward size (S_{pf}) determination, which are not fully specified in methodology and may not transfer across different GPU architectures or model sizes without re-profiling
- The KVC fragmentation problem is acknowledged but not directly addressed; the greedy distance minimization in multi-resource-aware batching may fail under severe fragmentation conditions
- JCT SLO handling depends on accurate Sg prediction, which the paper defers as "beyond scope," suggesting potential approximation errors in real deployments

## Confidence
- **High confidence**: Throughput and goodput improvements (1.42-11.21X, 1.43-13.71X) - these are direct measurements from controlled experiments with clear baselines
- **Medium confidence**: SLO attainment improvements (37-90%) - depends on accurate T̄pf profiling and iteration time predictability assumptions
- **Medium confidence**: Response latency reduction (1.61-12.22X) - combines multiple mechanisms; individual contribution of each mechanism not isolated

## Next Checks
1. **Cross-architecture profiling validation**: Profile S_{pf} and T̄pf for the same OPT-13B model on different GPU types (A100 vs H100 vs RTX 4090) and verify if the same chunking formula produces optimal throughput, or if architecture-specific coefficients are needed

2. **Extreme KVC fragmentation test**: Generate a workload with predominantly 3K-5K token prompts arriving at high rate to deliberately fragment KVC blocks, then measure if multi-resource-aware batching maintains its 31-63% GPU utilization improvement or degrades to baseline performance

3. **Sg prediction error impact**: Implement three different Sg prediction strategies (perfect oracle, average length, and random) and measure how JCT SLO attainment varies across prediction accuracy levels to quantify the impact of the "beyond scope" prediction problem