---
ver: rpa2
title: 'Investigating Language Model Capabilities to Represent and Process Formal
  Knowledge: A Preliminary Study to Assist Ontology Engineering'
arxiv_id: '2509.10249'
source_url: https://arxiv.org/abs/2509.10249
tags:
- reasoning
- language
- grammar
- logical
- clif
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research investigates whether formal knowledge representations
  can improve small language models' (SLMs) performance on first-order logic reasoning
  tasks. A methodology combining Syllogistic Evaluation Framework (SEF) for task classification
  and Common Logic Grammar Construction (CLGC) pipeline for transforming FOL representations
  was developed.
---

# Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering

## Quick Facts
- arXiv ID: 2509.10249
- Source URL: https://arxiv.org/abs/2509.10249
- Reference count: 39
- Primary result: Compact formal representations (CLIF) achieve logical reasoning accuracy comparable to natural language while using significantly smaller vocabularies

## Executive Summary
This preliminary study investigates whether formal knowledge representations can improve small language models' performance on first-order logic reasoning tasks. A methodology combining Syllogistic Evaluation Framework (SEF) for task classification and Common Logic Grammar Construction (CLGC) pipeline for transforming FOL representations was developed. Multiple formal languages (CLIF, CGIF, TFL, TFL+, MINIFOL) were evaluated against natural language baselines using the FOLIO dataset. Results demonstrate that compact formal representations like CLIF achieve performance comparable to natural language (~54% accuracy) while using significantly smaller vocabularies, supporting the hypothesis that more compact formal representations can maintain strong performance on reasoning tasks.

## Method Summary
The study combines a Syllogistic Evaluation Framework (SEF) for task classification with a Common Logic Grammar Construction (CLGC) pipeline that transforms first-order logic representations into target formal languages using Python's Lark parser. The methodology evaluates six representations (FOL, CLIF, CGIF, TFL, TFL+, MINIFOL) against natural language baselines on the FOLIO dataset. Multiple training configurations are tested including supervised fine-tuning (5-10 epochs with PEFT-LoRA for resource-constrained setups), zero-shot, and 8-shot prompting. The CLGC pipeline parses FOL input, applies input-output grammar transformations, and validates the output for each target language. Performance is measured using accuracy, precision, recall, and F1 metrics across different model sizes (Flan-T5-small/base/large, GPT-2, Phi-3.5-mini-instruct, Gemma-2-2b-it).

## Key Results
- Flan-T5-large achieves ~66% accuracy on natural language and ~62% on CLIF using supervised fine-tuning, demonstrating that compact formal representations can match natural language performance
- In-context learning approaches (zero-shot, few-shot) significantly underperform supervised fine-tuning across all representations, with peak zero-shot accuracy of 49.75% versus SFT's 66%
- Tokenizer retraining shows promise only for smaller models on compact formal languages, improving Flan-T5-small TFL+ accuracy from 35.96% to 45.32% while degrading larger model performance
- Compact formal representations like CLIF use dramatically smaller vocabularies while maintaining logical reasoning capabilities comparable to natural language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compact formal language representations (CLIF, TFL+) enable SLMs to perform first-order logic reasoning at accuracy levels comparable to natural language while using significantly smaller vocabularies
- Mechanism: Formal languages reduce syntactic complexity and lexical variance while preserving logical structure, allowing models to allocate capacity toward reasoning patterns rather than parsing verbose or ambiguous natural language expressions
- Core assumption: Vocabulary compactness and reduced syntactic diversity directly improve learning efficiency for logical reasoning tasks, rather than correlating with some other confounding factor
- Evidence anchors:
  - [abstract] "Results show that compact formal representations like CLIF achieve performance comparable to NL (accuracy ~54%) while using significantly smaller vocabularies"
  - [section 4.2] Table 6 shows Flan-T5-large achieving 66.0% accuracy on NL vs 61.57% on CLIF under full supervised fine-tuning; Table 5 shows Flan-T5-small matching NL baseline (43.84%) with CLIF representation
  - [corpus] Limited direct corpus validation for vocabulary compactness benefits in formal reasoning; related SLM reasoning work (Fine-Tuned Thoughts, Knowledge Distillation papers) focuses on chain-of-thought and distillation methods rather than input representation

### Mechanism 2
- Claim: Supervised fine-tuning outperforms in-context learning approaches (zero-shot, few-shot prompting) for logical reasoning with formal representations
- Mechanism: Parameter updates during SFT enable models to internalize grammar-specific patterns and logical inference rules; in-context approaches rely on attention over examples but lack the parameter-level integration needed for consistent formal reasoning
- Core assumption: The paper assumes that performance differences stem from the training methodology itself rather than from prompt engineering quality or hyperparameter choices for prompting conditions
- Evidence anchors:
  - [abstract] "The best results came from supervised fine-tuning, with Flan-T5-large achieving ~66% accuracy on NL and ~62% on CLIF"
  - [section 4.2] Table 5-6 (SFT) show peak accuracies of 43.84-66.0%; Table 7 (zero-shot) shows peak 49.75%; Table 9 (8-shot) shows peak 49.26%
  - [corpus] "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring" confirms SFT effectiveness for SLMs in specialized domains but does not isolate input representation as a variable

### Mechanism 3
- Claim: Tokenizer retraining improves performance for small models on compact formal languages but degrades performance for larger models
- Mechanism: Smaller models benefit from vocabulary alignment that reduces token sequence length and focuses embedding capacity on domain-specific constructs; larger models have sufficient baseline capacity and vocabulary coverage, so retraining introduces distribution shift without proportional benefit and may cause overfitting
- Core assumption: Assumption: Observed gains stem from vocabulary alignment rather than from implicit regularization effects of reduced vocabulary size or random initialization differences
- Evidence anchors:
  - [section 3.3] "In the Tokenizer Re-Training setup, the SLM tokenizer is retrained on the vocabulary of the grammar by passing all train, test and validation data in the specified language to the model"
  - [section 4.2] Table 12 shows Flan-T5-small accuracy on TFL+ improved from 35.96% (default tokenizer, 32128 vocab) to 45.32% (retrained, 180 vocab); Flan-T5-large dropped from 54.18% to 40.39%
  - [corpus] No corpus papers directly validate tokenizer adaptation mechanisms for formal languages; related work on SLM optimization focuses on distillation and chain-of-thought rather than tokenization strategies

## Foundational Learning

- Concept: First-Order Logic and Formal Language Variants
  - Why needed here: The paper evaluates six different formal representations (FOL, CLIF, CGIF, TFL, TFL+, MINIFOL) against natural language; understanding their syntactic differences is required to interpret why CLIF outperformed CGIF and why TFL failed on very small models
  - Quick check question: Given Table 3's transformation examples, can you explain why CGIF's bracket-heavy syntax might be harder for SLMs to learn than CLIF's Lisp-like syntax?

- Concept: Supervised Fine-Tuning vs. In-Context Learning Tradeoffs
  - Why needed here: The paper demonstrates consistent SFT superiority across all representations; practitioners need to understand when the computational cost of SFT is justified versus faster prompting approaches
  - Quick check question: Why does Table 8 show that BNF grammar prompting helps zero-shot performance (Gemma-2-2b-it CLIF improved from 44.33% to 45.32%) while Table 11 shows grammar context-passing hurts SFT performance?

- Concept: Tokenization and Vocabulary Distribution
  - Why needed here: Tokenizer retraining showed model-size-dependent effects that contradict naive intuition; understanding why requires grasping how tokenizer vocabulary size interacts with model embedding capacity
  - Quick check question: Table 12 shows Flan-T5-small with a 180-token vocabulary outperforms the 32128-token baseline on TFL+ (45.32% vs 35.96%), yet the same approach fails for Flan-T5-large. What does this suggest about the relationship between model capacity and vocabulary specialization?

## Architecture Onboarding

- Component map:
  - CLGC Pipeline (Section 3.3, Algorithm 1, Figure 1): FOL input → Lark parser (Python library) → parse tree → Input-Output Grammar Transformation → target parse tree → formatting → formal language output
  - SEF Framework (Section 3.1, Table 1-2): Classifies premises-conclusions pairs into Disjunctive (∨/⊕), Hypothetical (⟹), Categorical (exactly 2 premises + 1 conclusion), Complex (default); used for error analysis in Table 13-14
  - Training Configurations (Section 4.1): SFT (5 epochs standard, 10 epochs with PEFT-LoRA for resource constraints), Zero-Shot/Few-Shot prompting (8-shot), Grammar Context-Passing, Tokenizer Re-Training
  - Evaluation: FOLIO dataset splits (800 train / 203 validation / 201 test), accuracy/precision/recall/F1 metrics

- Critical path:
  1. Implement CLGC pipeline transformations for target formal language (use Table 3 transformations and Algorithm 1 as reference)
  2. Validate grammar parse correctness on FOLIO train set before training (pipeline Step 4 returns errors for malformed grammars)
  3. Select model: Flan-T5-large recommended for best performance (Table 6: 66% NL, 61.57% CLIF); use PEFT-LoRA if only L4/T4 GPUs available
  4. Use SFT without grammar context-passing (Table 11 shows ICGP degrades performance)
  5. Reserve tokenizer retraining only for Flan-T5-small experiments with TFL+ or similar ultra-compact languages (Table 12)
  6. Apply SEF analysis to interpret failure modes across syllogism types (Table 13-14)

- Design tradeoffs:
  - CLIF vs. NL: CLIF achieves ~4 percentage points lower accuracy than NL on Flan-T5-large SFT (Table 6: 61.57% vs 66.0%) but with dramatically smaller vocabulary; choose CLIF for vocabulary-constrained deployments or when ontology integration is required
  - Model size vs. resource constraints: Flan-T5-large requires A100 GPU for full SFT; Flan-T5-small + PEFT-LoRA works on L4/T4 but accuracy drops to ~44% (Table 5)
  - Tokenizer retraining vs. generalization: Table 12 shows retraining improves Flan-T5-small on TFL+ (45.32% vs 35.96%) but authors note potential overfitting risk; do not use for production without separate generalization tests

- Failure signatures:
  - Zero accuracy on specific grammar/model combinations (Table 7: Flan-T5-small with TFL or MINIFOL): indicates model capacity insufficient for formalism; switch to larger model or simpler grammar (CLIF)
  - Performance drop with ICGP (Table 11: Flan-T5-small CLIF 43.84% → 36.94%): BNF grammar context interferes with learned patterns; remove grammar context from SFT inputs
  - Tokenizer retraining degradation on larger models (Table 12: Flan-T5-large TFL+ 54.18% → 40.39%): mechanism breaks at scale; use default tokenizer for models ≥base size
  - Inconsistent predictions across syllogism types (Table 14: Categorical syllogisms show identical errors across NL/CLIF but TFL+ differs): indicates representation-specific reasoning patterns; use SEF analysis to diagnose

- First 3 experiments:
  1. Establish NL baseline: Train Flan-T5-large with SFT (5 epochs, no PEFT-LoRA) on FOLIO natural language data; expect ~66% accuracy per Table 6
  2. Compare CLIF representation: Apply CLGC pipeline to convert FOLIO to CLIF; train identical Flan-T5-large configuration; expect ~62% accuracy with significantly smaller vocabulary (Table 6)
  3. Validate tokenizer retraining boundary: On Flan-T5-small, compare default tokenizer vs. retrained tokenizer (vocab size ~180) for TFL+; expect improvement from ~36% to ~45% accuracy (Table 12). Then replicate on Flan-T5-base to confirm degradation, validating the small-model-only benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does tokenizer re-training on compact formal grammars improve performance in smaller models (e.g., Flan-T5-small) but degrade performance in larger models (e.g., Flan-T5-base/large)?
- Basis in paper: [explicit] Section 4.2 notes that while tokenizer re-training showed promise for smaller models, "this method is not scalable and breaks down when tokenizer re-training is done" for larger models. Section 5 explicitly calls for future work to "understand the cases where re-training would help or fail."
- Why unresolved: The authors observed the phenomenon (erratic performance and potential overfitting in larger models) but did not isolate the underlying mechanism causing the performance drop at scale.
- What evidence would resolve it: A comparative ablation study analyzing token distribution and attention patterns in larger models before and after re-training to determine if the vocabulary restriction causes loss of semantic nuance required by larger architectures.

### Open Question 2
- Question: Can mixed-input representations (e.g., combining Natural Language and CLIF) improve reasoning performance by leveraging the expressiveness of NL and the structure of formal grammars simultaneously?
- Basis in paper: [explicit] Section 5.1 states: "This PhD research will explore mixed-input representations (e.g., NL + CLIF) to assess whether combining expressiveness with the structure of a formal grammar improves reasoning."
- Why unresolved: The current experiments evaluated formal languages and natural language as independent, competing inputs. The potential synergy or interference effects of providing both representations concurrently to the model remain untested.
- What evidence would resolve it: Experimental results from the FOLIO dataset using concatenated NL and CLIF inputs compared against the standalone NL and standalone CLIF baselines established in the paper.

### Open Question 3
- Question: Do compact formal representations like CLIF maintain their competitive advantage over Natural Language on logical reasoning datasets other than FOLIO, such as ProofWriter or RuleTaker?
- Basis in paper: [explicit] Section 5.2 lists the "extension of the SEF-CLGC evaluation to datasets such as ProofWriter, RuleTaker, the Logical Entailment Dataset and SynLogic" as a related future work direction.
- Why unresolved: The conclusion that CLIF is a viable alternative to NL is based solely on the FOLIO dataset, which has a specific and imbalanced syllogistic distribution (heavily Disjunctive/Hypothetical).
- What evidence would resolve it: Replicating the CLGC pipeline methodology on the ProofWriter or RuleTaker benchmarks to verify if the ~62% accuracy and vocabulary efficiency of CLIF transfers to different logical problem distributions.

## Limitations
- BNF grammar definitions for target formal languages are not fully specified, requiring reconstruction from examples
- Performance comparisons rely heavily on a single FOLIO dataset with limited size and scope
- Tokenizer retraining approach shows promising but unstable results that degrade with larger models, suggesting potential overfitting concerns
- Lacks ablation studies isolating the effects of vocabulary compactness from other confounding factors like syntactic simplicity or semantic density

## Confidence
- **High Confidence**: Claims about SFT superiority over prompting approaches (zero-shot/few-shot) are well-supported by consistent performance gaps across all model sizes and representations
- **Medium Confidence**: Claims about CLIF's balanced performance-accuracy tradeoff are supported by Flan-T5-large results but not validated across diverse model families or larger scales
- **Low Confidence**: Claims about tokenizer retraining benefits are based on limited experiments showing contradictory results between model sizes, with no exploration of overfitting mechanisms or generalization tests

## Next Checks
1. Test the CLIF representation on additional logical reasoning datasets beyond FOLIO to verify that performance gains are not dataset-specific artifacts
2. Systematically vary syntactic complexity within formal representations to isolate whether vocabulary compactness or syntactic simplicity drives performance improvements
3. Evaluate tokenizer-retrained models on out-of-distribution logical reasoning tasks to quantify overfitting risks and establish boundaries for when vocabulary specialization helps versus harms performance