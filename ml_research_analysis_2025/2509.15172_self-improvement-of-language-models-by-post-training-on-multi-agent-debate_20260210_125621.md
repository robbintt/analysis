---
ver: rpa2
title: Self-Improvement of Language Models by Post-Training on Multi-Agent Debate
arxiv_id: '2509.15172'
source_url: https://arxiv.org/abs/2509.15172
tags:
- reasoning
- debate
- arxiv
- majority
- consensus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of self-improvement in language
  models, which struggle to improve without external supervision due to inconsistent
  reasoning. The core difficulty is sourcing a training signal stronger than what
  the model itself can currently produce.
---

# Self-Improvement of Language Models by Post-Training on Multi-Agent Debate

## Quick Facts
- arXiv ID: 2509.15172
- Source URL: https://arxiv.org/abs/2509.15172
- Reference count: 40
- Models improve self-consistency (+27.6% on GSM8K) and single-agent accuracy (+21.51% on MathQA) through debate-derived preference learning

## Executive Summary
This work introduces Multi-Agent Consensus Alignment (MACA), a self-improvement framework where language models are post-trained on debate-derived consensus signals from multi-agent discussions. The key insight is that preference learning over full reasoning traces, learning to differentiate between majority and minority reasoning, is more effective than binary consensus rewards or supervised fine-tuning. MACA produces three key improvements: models are better at utilizing multi-agent debate (+26.87% on MATH), individually more accurate (+21.51% on MathQA), and more self-consistent (+27.6% on GSM8K).

## Method Summary
MACA uses M=3 identical model clones to generate multi-round debates (R=2 rounds) on reasoning tasks, with each agent conditioning on peers' previous outputs. Final responses are partitioned by majority consensus into G+ (majority-aligned) and G- (minority) trajectories. The method trains on this signal using preference learning variants: MV-DPO and MV-KTO optimize log-probability gaps between G+ and G- traces, while MV-GRPO uses binary consensus rewards and MV-SFT imitates G+ directly. The approach uses 4-bit QLoRA with LoRA ranks 64-128, 256-token limits, and learning rate 1e-5.

## Key Results
- MV-DPO/MV-KTO provide largest and most consistent gains across all model sizes and datasets
- Self-consistency improvements correlate strongly (r > 0.86) with accuracy gains
- MACA generalizes to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA)
- MV-GRPO can degrade performance on some configurations

## Why This Works (Mechanism)

### Mechanism 1: Debate-Derived Consensus as Richer Training Signal
Multi-round deliberative debate produces higher-quality training signals than single-round majority voting. Multiple agent clones exchange reasoning over R rounds, generating final responses partitioned by consensus alignment (G+/G−). This deliberative process surfaces stable reasoning patterns that survive peer scrutiny versus those that don't, creating a self-generated preference signal. The method fails when base model accuracy is near-zero on target domain, as consensus formation requires at least some correct responses to aggregate.

### Mechanism 2: Full-Trace Preference Learning vs. Binary Rewards
Optimizing log-probability gaps between entire reasoning trajectories outperforms scalar binary rewards or supervised fine-tuning. MV-DPO/MV-KTO contrast consensus-supporting traces (G+) against dissenting traces (G−). Each token contributes to the preference signal via log-probability ratios, enabling credit assignment across long chains rather than sparse final-answer supervision. This approach assumes majority consensus correlates with correctness, supported by 39.8 percentage-point gap between majority/minority accuracy.

### Mechanism 3: Self-Consistency as Targetable Property
Self-consistency (probability mass on modal answer) is trainable and correlates strongly with accuracy improvements. Preference learning shifts sampling distribution toward consensus trajectories, increasing S+θ,τ(x). This creates a positive feedback loop: improved consistency → better debate → stronger training signals. The coupling between consistency and accuracy is strong (r > 0.86), though the causal direction remains unclear.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: MACA's MV-DPO variant builds directly on DPO's formulation; understanding log-probability ratios and the reference model is essential.
  - Quick check question: Can you explain why DPO avoids training a separate reward model?

- **Concept: Self-Consistency / Majority Voting**
  - Why needed here: The paper formalizes S+θ,τ(x) and demonstrates its trainability; this is the core signal being optimized.
  - Quick check question: Why does higher temperature sampling increase answer diversity but reduce consistency?

- **Concept: Multi-Agent Debate Protocols**
  - Why needed here: MACA uses M=3 agents, R=2 rounds as default; understanding debate dynamics is prerequisite for debugging consensus formation.
  - Quick check question: What happens if all agents produce the same incorrect answer—does consensus still form?

## Architecture Onboarding

- **Component map:**
  Debate Generator → Response Parser → Consensus Partitioner (G+/G−) → Preference Dataset Builder → Post-Trainer (DPO/KTO/GRPO/SFT)

- **Critical path:**
  1. Debate quality determines signal quality—validate base model produces parseable, diverse responses before training
  2. Token limits (256 default) constrain reasoning; verify truncation rates before interpreting results
  3. Preference pair balance—G+ typically dominates G−; KTO handles imbalance better than DPO for small models

- **Design tradeoffs:**
  - More agents/rounds → stronger consensus signal but quadratic compute cost
  - Higher temperature → better exploration but weaker base consensus
  - DPO vs. KTO: DPO requires paired examples; KTO handles unbalanced classes (preferred for ≤3B models per Table 1)

- **Failure signatures:**
  - Near-zero G+ size: base model too weak on domain
  - High truncation rate: inefficient reasoning or too few tokens
  - Training loss diverges: check β regularization (0.1 default) and learning rate (1e-5)
  - Agreement stuck at 1/3: models ignoring peer context during debate

- **First 3 experiments:**
  1. **Baseline debate sanity check:** Run M=3, R=2 debate with base model on held-out set. Measure initial/final accuracy, agreement distribution, parseable response rate. Target: >50% parseable, some 2/3 or 3/3 agreement.
  2. **Single-dataset MV-DPO run:** Train on GSM8K (1500 samples) with 256 tokens, QLoRA 4-bit. Evaluate on same-domain test + one OOD benchmark (e.g., SVAMP). Compare against base and MV-SFT baselines.
  3. **Ablate peer context:** Train two models—with and without debate context conditioning. Compare final-round consensus accuracy. Expect context training to show larger gains on final-round metrics per Table 6.

## Open Questions the Paper Calls Out

### Open Question 1
How does the MACA framework perform when applied to heterogeneous agents with distinct architectures, compared to the homogeneous clones used in this study? The experiments strictly use homogeneous clones to isolate the effects of consensus learning.

### Open Question 2
To what extent does the consensus reinforcement mechanism in MACA amplify pre-existing social or cognitive biases present in the base models? The evaluation focuses solely on reasoning accuracy benchmarks rather than bias metrics.

### Open Question 3
Can incorporating minority (dissenting) reasoning traces as positive contrastive signals, rather than just negative preference data, enhance reasoning robustness? The current method treats dissent strictly as a negative signal.

### Open Question 4
What is the minimum base model capability required for multi-agent debate to generate a consensus signal strong enough for effective self-improvement? While the paper shows success in 2B–8B models, the failure mode for models with very low baseline accuracy is noted but not fully quantified as a threshold.

## Limitations
- Consensus signal reliability depends on majority consensus correlating with correctness, which hasn't been validated across diverse reasoning domains
- Trace-based preference learning may not generalize to tasks with multiple valid solutions or subjective answers
- Computational efficiency claims relative to supervised fine-tuning lack absolute timing measurements

## Confidence

**High Confidence**
- MV-DPO and MV-KTO significantly outperform MV-GRPO and MV-SFT across all tested model sizes and datasets
- Self-consistency improvements correlate strongly (r > 0.86) with accuracy gains
- Preference learning over full traces provides larger gains than binary consensus rewards

**Medium Confidence**
- Debate-derived consensus signals are richer than single-round majority voting
- Multi-agent debate improves both debate utilization and single-agent accuracy
- MACA generalizes to unseen benchmarks with consistent improvements

**Low Confidence**
- Majority consensus reliably indicates correctness across diverse reasoning domains
- The preference learning framework generalizes to tasks without clear ground truth
- Computational efficiency claims relative to supervised fine-tuning

## Next Checks

1. **Consensus-Correctness Validation**: Systematically evaluate on datasets where consensus answers are known to be incorrect (e.g., trick questions, subjective reasoning tasks). Measure the false-positive rate where majority consensus reinforces wrong answers, and compare against confidence-based selection methods.

2. **Token Budget Sensitivity Analysis**: Conduct controlled experiments varying the 256-token debate limit across different reasoning task complexities. Measure how truncation rates, consensus formation rates, and final performance scale with token budget.

3. **Cross-Domain Generalization Test**: Train MACA on one reasoning domain (e.g., MATH) and evaluate on structurally similar but semantically distinct domains (e.g., physics problems, logical puzzles). Compare against standard fine-tuning to quantify whether the preference learning mechanism captures transferable reasoning patterns or task-specific heuristics.