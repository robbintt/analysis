---
ver: rpa2
title: 'JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation
  Version 1'
arxiv_id: '2507.20987'
source_url: https://arxiv.org/abs/2507.20987
tags:
- arxiv
- video
- generation
- wang
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JWB-DH-V1, a large-scale benchmark dataset
  for joint whole-body talking avatar and speech generation. It includes 10,000 unique
  identities across 2 million video samples, with 20,000 evaluation samples, and provides
  comprehensive annotations linking body postures, hand gestures, leg stance, and
  spoken language with audio.
---

# JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1

## Quick Facts
- arXiv ID: 2507.20987
- Source URL: https://arxiv.org/abs/2507.20987
- Reference count: 40
- Primary result: Large-scale benchmark with 10K identities, 2M samples, 20K evaluation samples; reveals significant performance disparities between face/hand-centric and whole-body generation models.

## Executive Summary
This paper introduces JWB-DH-V1, a comprehensive benchmark for joint whole-body talking avatar and speech generation. The dataset contains 10,000 unique identities across 2 million video samples with detailed multi-modal annotations linking body postures, hand gestures, leg stance, and spoken language. The authors evaluate eight state-of-the-art models using 12 metrics across three regions (full body, face, hands), revealing consistent performance disparities between face/hand-centric and whole-body generation. The benchmark exposes architectural limitations in current models and provides evaluation tools publicly available at the GitHub repository.

## Method Summary
JWB-DH-V1 provides a large-scale benchmark for evaluating joint whole-body talking avatar and speech generation. The dataset includes 10,000 unique identities across 2 million video samples, with 20,000 held out for evaluation. Each sample contains comprehensive annotations including body segmentation, landmarks, bounding boxes, motion text describing pose semantics, speech transcription with word boundaries and motion flags, and audio. The evaluation framework uses 12 metrics across three regions (whole body, face, hands): six video metrics (Subject Consistency, Background Consistency, Motion Smoothness, Dynamic Degree, Aesthetic Quality, Imaging Quality) and six co-speech metrics (FID, FVD, SSIM, PSNR, E-FID, CSIM). Audio quality is assessed via WER and LALM win-rate using Gemini 2.5 Pro.

## Key Results
- Whole-body subject consistency ranges from 7.08% to 97.12% across evaluated models, significantly lower than face-specific performance in several cases
- Face-centric models show 8.29-66.51% subject consistency compared to ground truth, while top whole-body models achieve 96.83-97.12%
- The benchmark reveals consistent performance disparities between face/hand-centric and whole-body generation across all evaluated models
- Generative models achieve 7.08% (whole body) to 8.29% (face) subject consistency compared to ground truth

## Why This Works (Mechanism)

### Mechanism 1
Region-specific evaluation decomposition exposes performance disparities that aggregate metrics mask. The benchmark evaluates face, hands, and whole body independently using 12 metrics, revealing that models optimized for face/hand-centric tasks degrade significantly on whole-body generation. Table 2 shows subject consistency dropping from 96.83-97.12% (top whole-body models) to 7.08% (worst) and face-specific SC ranging 8.29-66.51% across the same models.

### Mechanism 2
Multi-modal temporal annotations enable fine-grained alignment verification between motion semantics and speech. Each sample includes body segmentation, landmarks, bounding boxes, motion text describing pose semantics, speech transcription with motion flags, and word boundary timestamps. This permits evaluating whether generated gestures temporally align with spoken content at the word level.

### Mechanism 3
LALM-based audio evaluation captures prosodic and expressive qualities that WER misses. The speech protocol uses Gemini 2.5 Pro as a Large Audio-Language Model judge alongside WER, computing win rates against a baseline. Table 1 shows gpt-4o-mini-audio-preview achieving 37.63-60.68% win rates despite higher WER on some tasks.

## Foundational Learning

- Concept: Diffusion-based video generation
  - Why needed here: All evaluated video generation models (Step, HunyuanVideo, Wan, Open-Sora) use diffusion; understanding latent-space denoising, conditioning, and temporal consistency is prerequisite to interpreting results.
  - Quick check question: Can you explain why temporal consistency is harder in video diffusion than image diffusion?

- Concept: Fréchet Distance metrics (FID/FVD)
  - Why needed here: Table 2 reports FID (0-572) and FVD (0-4366) as primary quality metrics; understanding feature distribution comparison is essential for debugging model improvements.
  - Quick check question: What does a lower FVD score indicate about generated video distribution relative to ground truth?

- Concept: Audio-visual synchronization in talking avatars
  - Why needed here: The benchmark explicitly addresses "joint audio-video generation" and excludes Veo-3 due to "instability in generating synchronized outputs from a single frame."
  - Quick check question: Why is single-frame-initiated synchronization harder than video continuation?

## Architecture Onboarding

- Component map: Input frame + text prompt (video models) → Diffusion-based generation with region-specific attention → Three parallel evaluation protocols (video quality, co-speech fidelity, audio quality) → Output metrics
- Critical path: Load evaluation subset → Run inference on candidate model → Extract regions using provided bounding boxes → Compute 12 video metrics per region → Compute audio metrics if applicable → Compare against baseline models
- Design tradeoffs: Reference-free metrics (SC, BC, MS, DD, AQ, IQ) vs. reference-based (FID, FVD, SSIM, PSNR)—former enables generalization, latter enables direct comparison; region decomposition increases granularity 3× but requires accurate segmentation; LALM judging adds semantic evaluation but introduces model dependency
- Failure signatures: Low SC + high FVD: identity drift across frames; High face SC + low whole-body SC: model overfits to facial features; Low DD + high MS: insufficient motion dynamics; High WER + high win-rate: intelligible but unnatural prosody
- First 3 experiments: Replicate Table 2 results on Wan or Open-Sora to validate pipeline; Ablate region-specific evaluation: compute whole-body metrics only, compare diagnostic signal loss; Test a new model with proposed improvements and measure which region's SC improves most

## Open Questions the Paper Calls Out

### Open Question 1
How can joint audio-video generation models be stabilized to produce synchronized outputs from a single input frame? The authors excluded Veo-3 due to instability in generating synchronized outputs from a single frame, and note that achieving stable joint audio-video generation from a single input frame remains experimental.

### Open Question 2
What architectural or training innovations are required to close the performance gap between face/hand-centric generation and full whole-body avatar generation? Results reveal consistent performance disparities between face/hand-centric and whole-body performance, with whole-body SC scores (7.08%–97.12%) lagging behind face-specific performance in several models.

### Open Question 3
How can multi-modal consistency between body postures, hand gestures, leg stance, and spoken language be quantitatively measured and improved during joint generation? Current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech, and existing methods do not achieve stable and joint audio-video generation from a single frame.

### Open Question 4
What pose guidance or conditioning signals are essential for preventing catastrophic degradation in whole-body avatar generation? Table 2 shows dramatic differences between models with pose guidance (Ha3/w: SC 20.20%) versus without (Ha3/wo: SC 10.74%), suggesting current audio-only conditioning is insufficient for whole-body coherence.

## Limitations
- LALM-based audio evaluation lacks human validation studies to confirm correlation with perceived quality
- Benchmark focuses on 10,000 identities across controlled scenarios, limiting generalizability to diverse demographics
- Current metrics (FID, FVD, SSIM, CSIM) assess visual fidelity but don't directly capture semantic alignment between specific body regions and speech content

## Confidence
- **High confidence**: Dataset construction methodology, region-specific evaluation decomposition mechanism, baseline model performance results
- **Medium confidence**: LALM audio evaluation claims, diagnostic value of region-specific disparities
- **Low confidence**: Generalizability to real-world applications, sufficiency of current metrics for holistic human perception

## Next Checks
1. Conduct human validation study comparing LALM audio judgments against human ratings on naturalness, prosody, and expressiveness
2. Evaluate models on diverse, real-world talking avatar scenarios to measure performance degradation relative to benchmark results
3. Test whether region-specific performance patterns persist when models are evaluated on other talking avatar datasets