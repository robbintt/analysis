---
ver: rpa2
title: Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management
  Under Budget Constraints
arxiv_id: '2507.19458'
source_url: https://arxiv.org/abs/2507.19458
tags:
- budget
- maintenance
- learning
- asset
- hdrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses multi-year infrastructure asset management
  under budget constraints, where optimal maintenance scheduling is challenging due
  to combinatorial action spaces, deteriorating assets, and strict budget limits.
  It proposes a Hierarchical Deep Reinforcement Learning (HDRL) framework that decomposes
  the problem into two levels: a Budget Planner allocating annual budgets within feasibility
  bounds, and a Maintenance Planner prioritizing assets within the allocated budget.'
---

# Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints

## Quick Facts
- arXiv ID: 2507.19458
- Source URL: https://arxiv.org/abs/2507.19458
- Authors: Amir Fard; Arnold X. -X. Yuan
- Reference count: 12
- The paper proposes a hierarchical DRL framework for infrastructure asset management that achieves 15% better solution quality than baselines while scaling to larger networks.

## Executive Summary
This paper addresses the challenge of multi-year infrastructure asset management under strict budget constraints, where optimal maintenance scheduling is complicated by deteriorating assets and exponential action spaces. The authors propose a Hierarchical Deep Reinforcement Learning (HDRL) framework that decomposes the problem into a Budget Planner allocating annual budgets and a Maintenance Planner prioritizing assets within those budgets. By integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently handles exponential action space growth while ensuring rigorous budget compliance.

The framework is evaluated on sewer networks of varying sizes (10, 15, and 20 sewersheds), demonstrating faster convergence, better scalability, and near-optimal solutions compared to Deep Q-Learning and enhanced genetic algorithms. The approach maintains feasibility guarantees through LP projection while achieving solution quality improvements of up to 15% over baselines as network size increases.

## Method Summary
The HDRL framework uses a two-level hierarchical architecture where Actor 1 (Budget Planner) outputs a scalar budget fraction that determines annual spending, and Actor 2 (Maintenance Planner) outputs priority scores for each asset conditioned on the budget allocation. These priorities are then processed through a linear programming projection layer that solves a knapsack problem to select the optimal subset of maintenance actions under budget constraints. The framework uses twin Soft Actor-Critic critics to evaluate the joint actions, with entropy regularization encouraging exploration. The MDP formulation tracks asset conditions, remaining budget, and time, with rewards based on the remaining life-of-service after maintenance decisions.

## Key Results
- HDRL converges faster and scales better than Deep Q-Learning and enhanced genetic algorithms for multi-year asset management
- Solution quality improves by up to 15% over baselines as network size increases from 10 to 20 sewersheds
- The framework maintains exact budget constraint satisfaction through LP projection while reducing action space complexity from exponential O(2^n) to linear O(1+n)
- Training time increases modestly from ~176s (DQL) to ~366s (HDRL) for 20-sewershed case while delivering superior solutions

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Action Decomposition
- **Claim:** Decomposing the annual decision into a scalar budget fraction and an n-dimensional priority vector reduces action space complexity from exponential O(2^n) to linear O(1+n).
- **Mechanism:** Actor 1 outputs a single continuous value (budget fraction), which Actor 2 conditions on when producing asset-level priority scores. This factorization separates *when to spend* from *what to spend on*, allowing each policy head to learn at appropriate temporal granularity.
- **Core assumption:** The optimal multi-year policy can be approximated by a temporally-local budget decision followed by a knapsack-style asset selection, without requiring explicit coordination across years in the action representation.
- **Evidence anchors:**
  - [Section 4.1] "By factorizing the problem into a scalar budget action plus an n-dimensional maintenance action... HDRL substantially reduces the complexity compared to a monolithic RL agent... with a combinatorial action space of size 2^n per step."
  - [Table 3] DQL output layer grows from 20 → 364 → 7,448 across 10/15/20 sewersheds; HDRL grows from 11 → 16 → 21.
  - [Corpus] Neighbor paper "Multi-Year Maintenance Planning for Large-Scale Infrastructure Systems" uses monolithic DQL and faces scalability challenges—consistent with this decomposition rationale.
- **Break condition:** If future-year budget constraints require *anticipatory* action coordination beyond what the scalar fraction captures (e.g., reserving capacity for a known expensive intervention in year 4), the decomposition may lose optimality.

### Mechanism 2: LP Projection for Exact Constraint Satisfaction
- **Claim:** Embedding a knapsack-style linear program as a projection layer guarantees that selected maintenance actions satisfy annual and cumulative budget constraints exactly, without relying on soft penalties.
- **Mechanism:** Actor 2 produces priority coefficients a_i,t; these weight the immediate condition gain from each asset. The LP solver (Equation 14) then selects the optimal subset of assets under budget b_t. Because the LP enforces constraints as hard bounds, feasibility is guaranteed by construction.
- **Core assumption:** The priority scores learned by Actor 2 can encode sufficient multi-year value information to make the *greedy LP selection* approximately optimal for the full horizon objective.
- **Evidence anchors:**
  - [Section 4.1] "A local optimization step ensures selected maintenance actions are moving toward maximizing the performance of the system while the costs remain within allocated budgets."
  - [Section 2.3] Reviews four constraint-handling families; notes penalty-based methods "offer no feasibility guarantee."
  - [Corpus] Weak direct corpus evidence on LP projection in DRL for infrastructure; most neighbor papers use penalty-based or Lagrangian approaches.
- **Break condition:** If the LP objective (linear in priority weights) poorly approximates the true multi-year value function—e.g., due to strong inter-temporal substitution effects—the projection may systematically select suboptimal asset subsets.

### Mechanism 3: Entropy-Regularized Off-Policy Learning (SAC)
- **Claim:** Using Soft Actor-Critic with twin Q-networks and entropy regularization stabilizes learning in the hierarchical setting, particularly given the non-stationarity introduced by having two dependent actors.
- **Mechanism:** SAC's entropy bonus encourages sustained exploration early in training, preventing premature convergence to deterministic budget fractions. Twin critics reduce overestimation bias, and off-policy replay improves sample efficiency—critical when each episode simulates a multi-year planning horizon.
- **Core assumption:** The combined entropy term from both actors (Equation 21) adequately balances exploration across the joint action space despite the sequential dependency between a^(1)_t and a^(2)_t.
- **Evidence anchors:**
  - [Section 4.3] "SAC framework is especially well-suited for this hierarchical structure because it supports stable, sample-efficient off-policy learning and facilitates continuous exploration through entropy maximization."
  - [Figure 3a] Shows entropy coefficient α decaying from ~0.1 toward 0 over 5,000 episodes, indicating controlled exploration reduction.
  - [Corpus] SAMP-HDRL paper uses similar hierarchical SAC for portfolio management, suggesting cross-domain applicability of this mechanism.
- **Break condition:** If the two actors' entropy terms scale poorly with network size (n), exploration may become uneven—over-exploring budget allocation while under-exploring asset prioritization for large n.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - **Why needed here:** The paper casts multi-year maintenance as a sequential decision problem where state s_t includes asset conditions, time, and remaining budget. Understanding the MDP structure (states, actions, transitions, rewards) is prerequisite to grasping why RL applies.
  - **Quick check question:** Can you write the Bellman equation for the Q-function in this context? If not, review Section 3's equations 4–7 before proceeding.

- **Concept: Actor-Critic Methods**
  - **Why needed here:** HDRL uses two actors (Budget Planner, Maintenance Planner) and twin critics. The actor-critic paradigm—where a policy (actor) is updated using value estimates from a critic—is the algorithmic backbone.
  - **Quick check question:** Explain why the critic takes (s_t, a^(1)_t, a^(2)_t) as input rather than just s_t. Hint: What does the critic need to evaluate?

- **Concept: Constrained Optimization via Knapsack LP**
  - **Why needed here:** The LP projection layer (Equation 14) is a binary knapsack problem with budget constraint. Without understanding LP feasibility and the knapsack structure, the "exact constraint satisfaction" claim is opaque.
  - **Quick check question:** Given 5 assets with costs [30, 20, 50, 10, 40] and budget 80, which subsets are feasible? How would priority weights [1.0, 0.5, 2.0, 0.3, 1.5] affect selection?

## Architecture Onboarding

- **Component map:** State s_t → Actor 1 → budget fraction a^(1)_t → Actor 2 → priority vector a^(2)_t → LP projection → maintenance actions x_t → Environment → reward r_t and next state s_{t+1}

- **Critical path:**
  1. Observe state s_t
  2. Actor 1 samples budget fraction a^(1)_t
  3. Actor 2 samples priority vector a^(2)_t conditioned on a^(1)_t
  4. LP projection selects maintenance actions x_t
  5. Environment executes x_t, returns reward r_t and next state s_{t+1}
  6. Store transition in replay buffer
  7. Periodically sample mini-batch; update critics, actors, entropy coefficient

- **Design tradeoffs:**
  - **LP solver vs. learned constraint satisfaction:** LP guarantees feasibility but adds computational overhead per step. Penalty-based methods are faster but risk infeasibility.
  - **Hierarchical vs. monolithic action:** Hierarchy improves scalability but assumes budget-prioritization separation is approximately optimal. Monolithic DQL may find better solutions for small n but fails to scale.
  - **Shared vs. separate critics:** Paper uses shared critic receiving both actions. Separate critics could reduce coupling but would require additional coordination mechanisms.

- **Failure signatures:**
  - **Budget constraint violations:** Indicates LP projection not being called or Equation 13 mapping incorrect. Check that b_t is passed correctly to LP.
  - **Divergent actor losses:** If Actor 1 loss plateaus while Actor 2 oscillates, learning rates may be unbalanced (paper uses 8×10^-4 for Actor 1 vs. 8×10^-6 for Actor 2).
  - **No improvement over random baseline:** Check entropy coefficient α—if it decays too fast, exploration is insufficient; if it stays high, policy remains stochastic.
  - **Scalability breakdown:** If training time explodes beyond 20 sewersheds, LP solver may be the bottleneck; consider approximate knapsack heuristics.

- **First 3 experiments:**
  1. **Reproduce 10-sewershed case:** Train HDRL and DQL for 5,000 episodes each. Compare final average objective (HDRL ~1.472, DQL ~1.474 per Table 3) and training curves against Figure 4. Verify HDRL finds near-optimal solutions in cost-condition space.
  2. **Ablate LP projection:** Replace LP with top-k selection based on priority scores (with penalty for budget violation). Measure feasibility rate and solution quality degradation to quantify LP contribution.
  3. **Scale to 15 sewersheds:** Confirm DQL output layer grows to 364 and HDRL to 16. Compare runtimes (DQL ~176s, HDRL ~366s per Table 3) and solution quality gap (~8% improvement for HDRL). Document where DQL training becomes unstable.

## Open Questions the Paper Calls Out
None

## Limitations
- **Generalization beyond sewer networks**: The evaluation is limited to synthetic sewer network models. Real-world applicability to diverse infrastructure types (bridges, roads, water systems) remains untested.
- **LP solver scalability**: While the framework scales better than monolithic DQL, the LP projection becomes computationally intensive as asset count grows. The study caps at 20 sewersheds without testing larger systems.
- **Multi-year budget interdependencies**: The hierarchical decomposition assumes annual budget decisions can be made independently. In cases where future asset conditions strongly influence current spending, this assumption may break down.

## Confidence
- **High confidence** in scalability claims: Supported by systematic comparison across 10/15/20 sewersheds showing consistent improvement over DQL.
- **Medium confidence** in near-optimality: The LP projection guarantees feasibility but may not achieve global optimality for complex multi-year trade-offs.
- **Medium confidence** in mechanism claims: The hierarchical decomposition and entropy regularization are well-supported, but the exact contribution of each component to performance gains is not isolated experimentally.

## Next Checks
1. **Cross-infrastructure validation**: Apply HDRL to bridge or road network datasets and compare performance against domain-specific baselines.
2. **Ablation of LP component**: Replace LP projection with learned constraint satisfaction and measure feasibility rate degradation and solution quality changes.
3. **Stress test budget coupling**: Design scenarios with known future high-cost interventions and evaluate whether the scalar budget fraction captures necessary anticipatory allocation.