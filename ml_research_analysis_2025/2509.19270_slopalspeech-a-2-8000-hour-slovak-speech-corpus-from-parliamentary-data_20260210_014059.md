---
ver: rpa2
title: 'SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data'
arxiv_id: '2509.19270'
source_url: https://arxiv.org/abs/2509.19270
tags:
- whisper
- slovak
- dataset
- speech
- transcripts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of large-scale Slovak speech data
  for ASR by constructing SloPalSpeech, a 2.8k-hour dataset from parliamentary recordings,
  and fine-tuning Whisper models on it. The core method involves aligning long-form
  audio with transcripts using anchor-based timestamping, segmenting into 30-second
  clips, and fine-tuning models from Whisper-small to large-v3.
---

# SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data

## Quick Facts
- **arXiv ID:** 2509.19270
- **Source URL:** https://arxiv.org/abs/2509.19270
- **Reference count:** 0
- **Primary result:** 2.8k-hour Slovak ASR dataset from parliamentary recordings, fine-tuning Whisper models achieves 70% WER reduction

## Executive Summary
This work addresses the lack of large-scale Slovak speech data for ASR by constructing SloPalSpeech, a 2.8k-hour dataset from parliamentary recordings, and fine-tuning Whisper models on it. The core method involves aligning long-form audio with transcripts using anchor-based timestamping, segmenting into 30-second clips, and fine-tuning models from Whisper-small to large-v3. The fine-tuned models show substantial WER reductions on Slovak benchmarks: Whisper-small improves by 70% (CV21 58.4%→25.7%, FLEURS 36.1%→10.6%), while larger models also achieve notable gains, with Whisper-large-v3-Turbo offering the best performance-to-size trade-off. All resources, including the dataset, segmented transcripts (60M words), and models, are publicly released.

## Method Summary
The method constructs a 2,806-hour Slovak ASR dataset from 1,136 parliamentary recordings by aligning audio with DOCX transcripts using anchor-based timestamping with WhisperX reference transcripts, segmenting into 30-second clips, and filtering with 40% WER threshold. Whisper models (small, medium, large-v3, large-v3-turbo) are fine-tuned using conservative learning rates (40× smaller than pretraining) with FSDP for large-v3, achieving substantial WER reductions on Slovak benchmarks while releasing all resources publicly.

## Key Results
- Whisper-small: 70% WER reduction on Slovak benchmarks (CV21: 58.4%→25.7%, FLEURS: 36.1%→10.6%)
- Whisper-large-v3-Turbo: Best performance-to-size trade-off, ~1% WER difference from large-v3 with 730M fewer parameters
- Dataset: 402,966 segments (≤30s), 2,806 hours total, 60M word segmented transcripts
- All resources (dataset, models, scripts) publicly released

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anchor-based timestamping enables alignment of long-form audio with transcripts when forced alignment is infeasible.
- Mechanism: Generate a reference transcript with word-level timestamps using WhisperX, then identify "anchor" words appearing in both reference and ground truth. Anchors are matched via Levenshtein distance ≤1 with forward-only constraint and context scoring (4 words before/after). This produces a strictly increasing timestamp sequence.
- Core assumption: Sufficient words are correctly transcribed in the reference to create reliable anchor points.
- Evidence anchors:
  - [section 4.2] "Each reference word was matched to candidate ground truth words within a Levenshtein distance of 1, provided the match did not jump backward relative to the last aligned word and stayed within a 50-word forward window."
  - [corpus] ParCzech4Speech (arXiv:2509.06675) uses similar WhisperX-based alignment for Czech parliamentary data, suggesting cross-language validity.
- Break condition: If transcription quality is too low (WER >~30-40%), anchor density may be insufficient for reliable alignment.

### Mechanism 2
- Claim: Re-transcription WER filtering removes misaligned segments while preserving majority of data.
- Mechanism: After segmentation, re-apply Whisper to each audio chunk, compute WER against ground truth transcript, and discard segments exceeding threshold (empirically set at 40%).
- Core assumption: High WER on re-transcription correlates with alignment/segmentation errors rather than model limitations.
- Evidence anchors:
  - [section 4.4] "We analyzed different WER intervals and found that lower rates typically indicated well-aligned and correctly segmented audio, whereas higher WERs were often associated with missegmentations or transcription errors."
  - [corpus] Limited direct corpus evidence; this heuristic requires validation on other domains.
- Break condition: Domain mismatch between filtering model and target use case may reject valid segments.

### Mechanism 3
- Claim: Learning rate 40× smaller than pretraining rate enables domain adaptation without catastrophic forgetting.
- Mechanism: Pretraining uses large LR; fine-tuning with LR = 2.5×10⁻⁷ (vs. common 1×10⁻⁵) allows weight updates that adapt to Slovak parliamentary speech while preserving general capabilities. Weight decay (0.01) required for large-v3 to prevent overfitting.
- Core assumption: The pretrained model has transferable representations that benefit from gentle refinement.
- Evidence anchors:
  - [section 5.2] "We followed a recommendation by (Lodagala, 2023) of using a learning rate 40× smaller than that used during pretraining. In our experiments, this adjustment led to more stable convergence."
  - [section 5.3.2] "Introducing a weight decay of 0.01 (previously 0) enabled stable convergence in stages that had previously overfitted."
  - [corpus] "Whispering in Amharic" (arXiv:2503.18485) reports similar Whisper fine-tuning for low-resource languages.
- Break condition: Observed multilingual degradation—English transcription performance decreased after full fine-tuning (section 8).

## Foundational Learning

- Concept: **Whisper's 30-second input constraint**
  - Why needed here: All segmentation and architecture choices derive from this limit; understanding it explains the alignment pipeline design.
  - Quick check question: Why can't you feed a 3-hour parliamentary session directly to Whisper for training?

- Concept: **Catastrophic forgetting**
  - Why needed here: Motivates conservative learning rates and multi-dataset evaluation; explains why they monitored FLEURS/CV during training.
  - Quick check question: What would happen if you fine-tuned with a large learning rate for 10+ epochs on parliamentary data only?

- Concept: **Forced alignment vs. anchor-based alignment**
  - Why needed here: Understanding why traditional forced alignment failed (long duration + transcript-audio mismatch) clarifies the novel method's necessity.
  - Quick check question: Why wouldn't a standard forced aligner work on 4-hour recordings with imperfect transcripts?

## Architecture Onboarding

- Component map: Raw Data (MediaPortál + Digital Library) -> Parsing (Apache Tika → XHTML → speaker segmentation heuristics) -> Alignment (WhisperX reference → anchor matching → timestamp assignment) -> Segmentation (28s window + 2s buffer → audio slicing) -> Quality Filter (re-transcribe → WER < 40%) -> Training Dataset (402,966 segments, 2,806 hours) -> Fine-tuning (FSDP for large-v3; single-GPU for smaller)

- Critical path: Anchor-based alignment is the bottleneck—insufficient anchors produce sparse or invalid segments. Start by validating anchor density on a sample session before scaling.

- Design tradeoffs:
  - 40% WER threshold: Higher preserves more data but includes noisier segments; lower is cleaner but reduces scale.
  - Full fine-tuning vs. LoRA: Full updates gave better Slovak performance but degraded multilingual capability (trade-off noted in section 8).
  - Large-v3-Turbo vs. Large-v3: ~1% WER difference with 730M fewer parameters—Turbo recommended for inference.

- Failure signatures:
  - WER > 1.0 on validation → transcript-audio mismatch (e.g., afternoon sessions included in transcript but not audio).
  - WER increases after warmup → overfitting (increase weight decay, reduce LR).
  - FSDP evaluation crashes during training → use separate GPU for inference checkpoints (documented workaround in section 5.3.1).

- First 3 experiments:
  1. Validate alignment on 5 random sessions: Run anchor detection, check anchor density (>1 per 10 seconds indicates healthy alignment), manually inspect 10 random segments.
  2. Small-scale fine-tuning sanity check: Fine-tune Whisper-small on 100-hour subset for 1 epoch, evaluate on held-out parliamentary + FLEURS subset to confirm WER reduction without catastrophic forgetting.
  3. Threshold sensitivity analysis: Compare dataset quality/quantity at 30%, 40%, 50% WER thresholds by spot-checking 50 segments from each tier.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the described anchor-based alignment pipeline be effectively scaled to other low-resource European languages that possess similar parliamentary transcript archives?
  - Basis in paper: [explicit] Section 9 "Future Work" states: "Parliamentary proceedings are transcribed in many European countries... our alignment method could be scaled to other languages to boost ASR resources beyond Slovak."
  - Why unresolved: The current study validates the pipeline only for Slovak; its robustness across languages with different morphology, orthography, or transcript formatting standards remains untested.
  - What evidence would resolve it: Successful replication of the pipeline for another language (e.g., Croatian or Lithuanian) resulting in a high-quality dataset and subsequent ASR performance improvements.

- **Open Question 2:** How can the significant degradation of non-target language capabilities (catastrophic forgetting) be mitigated when fully fine-tuning models on monolingual domain-specific data?
  - Basis in paper: [inferred] Section 8 "Limitations" notes "degraded English transcription performance, suggesting a loss of multilingual capability" due to updating all model parameters.
  - Why unresolved: While the authors quantify Slovak improvements, they only observe the loss of English capability qualitatively and do not test methods to preserve it (e.g., mixed-language batches or adapter layers).
  - What evidence would resolve it: A comparison of fine-tuning strategies showing that the model can retain Slovak WER gains while maintaining English transcription accuracy within a negligible margin of the baseline.

- **Open Question 3:** To what extent do inference-time strategies like compression ratio checks resolve domain-specific hallucinations where the model biases toward parliamentary terms?
  - Basis in paper: [explicit] Section 8 "Limitations" identifies hallucinations of terms like "Pán poslanec" and explicitly "recommend[s] adopting an approach similar to Faster Whisper... to mitigate hallucinations."
  - Why unresolved: The authors identify the specific failure mode (hallucinating parliamentary terms even in non-parliamentary contexts) and suggest a solution, but they do not implement or evaluate it.
  - What evidence would resolve it: A quantitative evaluation of hallucination rates on out-of-domain test sets (like Common Voice) comparing standard inference against the recommended compression ratio triggering approach.

## Limitations

- The anchor-based alignment method's quality depends heavily on reference transcript accuracy, creating potential data-dependent failure modes
- The 40% WER threshold heuristic may not generalize across domains or languages
- Full fine-tuning causes multilingual capability degradation, particularly for English transcription

## Confidence

**High Confidence:**
- The methodology for constructing the dataset (alignment, segmentation, filtering) is clearly described and reproducible
- The WER improvements on Slovak benchmarks are substantial and statistically significant
- The trade-off between large-v3 and large-v3-Turbo performance is well-quantified

**Medium Confidence:**
- The anchor-based alignment method will generalize to other parliamentary corpora or domains with similar characteristics
- The 40% WER threshold represents an optimal balance between data quantity and quality
- The learning rate recommendations (40× pretraining rate) will generalize to other low-resource language fine-tuning tasks

**Low Confidence:**
- The extent to which the parliamentary domain limitation affects real-world deployment scenarios
- Whether the multilingual degradation can be mitigated through alternative fine-tuning strategies (e.g., LoRA, adapter layers)
- The long-term stability of the fine-tuned models across evolving parliamentary speech patterns

## Next Checks

1. **Anchor Density Validation:** Select 20 random parliamentary sessions and measure anchor point density (anchors per minute). Compare this against manual inspection of alignment quality to establish a minimum acceptable threshold for reliable segmentation.

2. **Threshold Sensitivity Analysis:** Create three parallel datasets using 30%, 40%, and 50% WER thresholds. Fine-tune Whisper-small on each subset and evaluate on a held-out parliamentary test set plus FLEURS to measure the trade-off between WER improvement and dataset size reduction.

3. **Multilingual Capability Assessment:** Fine-tune Whisper-large-v3 using three different strategies: (a) full fine-tuning as described, (b) LoRA adapters with 10%, 25%, and 50% trainable parameters, and (c) continued pretraining with weighted loss between Slovak and multilingual data. Evaluate all models on Slovak benchmarks plus FLEURS English to quantify the degradation and identify strategies to preserve multilingual capabilities.