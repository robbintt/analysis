---
ver: rpa2
title: 'MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models'
arxiv_id: '2512.24693'
source_url: https://arxiv.org/abs/2512.24693
tags:
- multi-turn
- arxiv
- music
- instruction
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training reward models (RMs)
  for multi-turn conversations, which is critical for developing capable Large Language
  Models (LLMs). Standard preference datasets used to train RMs typically only contrast
  responses based on the final conversational turn, providing insufficient signal
  to capture the nuances of multi-turn interactions like coherence and consistency
  across turns.
---

# MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models

## Quick Facts
- **arXiv ID:** 2512.24693
- **Source URL:** https://arxiv.org/abs/2512.24693
- **Reference count:** 13
- **Key outcome:** MUSIC achieves +3.8% to +5.1% improvement in multi-turn Best-of-N evaluation and +3.9% improvement in single-turn reasoning without degrading single-turn performance.

## Executive Summary
This paper addresses the challenge of training reward models (RMs) for multi-turn conversations, which is critical for developing capable Large Language Models (LLMs). Standard preference datasets used to train RMs typically only contrast responses based on the final conversational turn, providing insufficient signal to capture the nuances of multi-turn interactions like coherence and consistency across turns. To address this limitation, the authors propose MUlti-Step Instruction Contrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs with meaningful quality differences distributed across multiple turns. MUSIC works by using LLM-based user and assistant simulators to generate paired conversations from seed contexts. At each turn, a contrastive instruction prompt guides the assistant simulator to produce a lower-quality response for one conversation in the pair, creating a deliberate quality difference across multiple turns. This synthesized data is then combined with existing preference datasets to train a multi-turn RM. The authors demonstrate the efficacy of MUSIC by training a multi-turn RM based on the Gemma-2-9B-Instruct model on a MUSIC-augmented version of the Skywork preference dataset. They evaluate the MUSIC-augmented RM against a baseline RM (trained without MUSIC) on two tasks: (1) a multi-turn Best-of-N inference task, where the RM guides an LLM assistant to generate high-quality multi-turn conversations, and (2) RewardBench, a standard single-turn RM benchmark. Results show that the MUSIC-augmented RM achieves higher agreement with judgments from the advanced Gemini 1.5 Pro model when assessing the quality of multi-turn conversations generated via Best-of-N inference, across different values of N. Importantly, this improvement in multi-turn evaluation capability is achieved without compromising performance on the single-turn RewardBench, and in fact shows a slight improvement (+3.9%) in the Reasoning category. These findings validate MUSIC as an effective strategy for training more robust multi-turn RMs.

## Method Summary
MUSIC augments multi-turn preference datasets by synthesizing contrastive conversation pairs with quality differences distributed across multiple turns. It uses LLM-based user and assistant simulators to generate paired conversations from seed contexts. At each turn, a contrastive instruction prompt guides the assistant simulator to produce a lower-quality response for one conversation in the pair, creating deliberate quality differences across turns. The synthesized data is combined with existing preference datasets to train a multi-turn RM. The authors train a Gemma-2-9B-Instruct-based RM on a MUSIC-augmented version of the Skywork preference dataset and evaluate it on multi-turn Best-of-N inference and single-turn RewardBench tasks.

## Key Results
- MUSIC-augmented RM achieves +3.8% to +5.1% higher win rates in multi-turn Best-of-N evaluation compared to baseline RM
- MUSIC-augmented RM shows +3.9% improvement in single-turn reasoning evaluation on RewardBench
- No degradation in single-turn performance on RewardBench, maintaining or slightly improving performance across categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributing quality differences across multiple turns provides stronger training signal for multi-turn evaluation than single-turn contrasts.
- Mechanism: Standard preference datasets localize differences to the final turn, forcing RMs to learn from sparse signal. MUSIC creates contrastive pairs where quality distinctions (coherence, consistency, instruction-following) accumulate across T turns, providing denser supervision for holistic conversation assessment.
- Core assumption: Reward models trained on multi-turn-distributed contrasts will generalize better to unseen multi-turn conversations than those trained on final-turn-only contrasts.
- Evidence anchors:
  - [abstract] "incorporating contrasts spanning multiple turns is critical for building robust multi-turn RMs"
  - [section 1] "standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal"
  - [corpus] Related work (MARS-Bench) confirms multi-turn evaluation remains challenging; no direct validation of this specific mechanism in corpus.
- Break condition: If RM generalization fails on multi-turn benchmarks despite training with MUSIC, the assumption that distributed contrasts transfer better may not hold.

### Mechanism 2
- Claim: Controlled instruction perturbation generates reliably inferior rejected responses without manual annotation.
- Mechanism: The `Contrast(·)` function transforms user instruction u_t into a modified instruction that is semantically adjacent but requires a response misaligned with the original intent. The assistant generates a coherent response to the modified instruction, which becomes locally coherent but globally inferior—exhibiting issues like inconsistency with prior turns or missed constraints.
- Core assumption: The instruction contrast prompt produces responses that are consistently and recognizably worse along dimensions relevant to multi-turn quality.
- Evidence anchors:
  - [section 3.3] "the user's utterance is first transformed by Contrast(·) into a modified instruction, which prompts M_a to generate a response that is intentionally suboptimal"
  - [appendix A.3] The prompt explicitly instructs: "generate a high-quality answer which is a good response to the modified instruction but not a good response to the original user question"
  - [corpus] No corpus papers validate this perturbation strategy; mechanism remains unverified externally.
- Break condition: If Gemini 1.5 Pro judge does not consistently prefer chosen over rejected trajectories, the contrast mechanism may not produce meaningful quality differences.

### Mechanism 3
- Claim: Multi-turn conversational training transfers positively to single-turn reasoning evaluation.
- Mechanism: Exposure to coherent, logically structured multi-turn dialogues may strengthen the RM's ability to recognize logical consistency and stepwise quality, even when applied to single-turn reasoning tasks. This is an unintended but observed effect.
- Core assumption: Reasoning quality in single turns shares underlying features with multi-turn coherence assessment.
- Evidence anchors:
  - [section 4.3] "We observe a notable improvement (+3.9%) in the Reasoning category for the MUSIC-Augmented RM"
  - [section 4.3] "exposure to coherent, logically structured multi-turn dialogues during training may implicitly enhance the RM's ability to assess reasoning steps"
  - [corpus] Weak support; related work on RMs for reasoning (Enhancing LLM Reasoning with RMs) does not address multi-turn transfer.
- Break condition: If reasoning improvements do not replicate across datasets or base models, this may be an artifact of the specific Skywork/Gemma configuration.

## Foundational Learning

- Concept: **Bradley-Terry Preference Modeling**
  - Why needed here: MUSIC augments preference data, and RM training optimizes the BT log-likelihood to learn scalar rewards from pairwise comparisons.
  - Quick check question: Given two conversation scores R_θ(C_chosen) = 2.1 and R_θ(C_rejected) = 1.4, what is the predicted probability that C_chosen is preferred?

- Concept: **Best-of-N (BoN) Inference**
  - Why needed here: The primary evaluation uses BoN to measure RM effectiveness in guiding multi-turn generation by selecting highest-scored responses.
  - Quick check question: In a 3-turn conversation with N=4 candidates per turn, how many total completions does the RM score?

- Concept: **Outcome Reward Models (ORMs) vs. Process Reward Models (PRMs)**
  - Why needed here: MUSIC targets ORMs for multi-turn conversations where "steps" are turns and quality is implicit, distinct from PRMs' step-level verification.
  - Quick check question: Why might an ORM struggle more than a PRM with long reasoning chains, and how does MUSIC attempt to address this for conversational turns?

## Architecture Onboarding

- Component map:
  Seed Sampler -> User Simulator (Gemini 1.5 Pro) -> Contrast Function -> Assistant Simulator (Gemini 1.5 Pro) -> RM (Gemma-2-9B-Instruct + linear head)

- Critical path:
  1. Sample seed context C_prefix from Skywork
  2. Run Algorithm 1 for T=5 turns to generate (C_chosen, C_rejected)
  3. Combine D_MUSIC with original D (31k + 73k pairs)
  4. Fine-tune Gemma-2-9B-Instruct with BT loss for 2500 steps

- Design tradeoffs:
  - **Simulation turns (T)**: Paper uses T=5 due to context limits; longer conversations may reveal different multi-turn dynamics
  - **Base model choice**: Using same model (Gemini 1.5 Pro) for both simulators may introduce systematic biases; decoupling could improve diversity
  - **Augmentation ratio**: 31k MUSIC / 73k original (~43%); ablation of this ratio is not reported

- Failure signatures:
  - Low win rate in BoN evaluation suggests contrast signal is too weak or noisy
  - Degraded RewardBench performance indicates MUSIC data may have introduced confounding patterns
  - Rejected responses being preferred over chosen responses by judges would indicate `Contrast(·)` is not producing inferior outputs

- First 3 experiments:
  1. **Sanity check**: Manually inspect 20-30 MUSIC-generated pairs to verify chosen responses are qualitatively better than rejected responses across turns
  2. **Ablation on T**: Train RMs with MUSIC rollouts of T ∈ {2, 3, 5, 7} to measure sensitivity to conversation length
  3. **Cross-model evaluation**: Test MUSIC-augmented RM with assistant models other than Gemma-2-9B-Instruct to assess generalization of learned multi-turn preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the alignment between MUSIC-trained RMs and LLM judges (e.g., Gemini 1.5 Pro) correlate with human preferences on multi-turn conversations?
- Basis in paper: [explicit] "While practical and scalable, these models may introduce their own biases or fail to capture the full spectrum of human conversational nuances and preferences. Future work could explore incorporating real human interactions or judgments..."
- Why unresolved: The entire evaluation pipeline uses LLMs as both simulators and judges, creating potential for systematic biases that diverge from human judgment.
- What evidence would resolve it: Human evaluation of conversations selected by MUSIC-trained RMs versus baseline RMs, or correlation analysis between LLM judge scores and human ratings on a held-out multi-turn benchmark.

### Open Question 2
- Question: Does MUSIC's effectiveness scale to significantly longer conversations (e.g., 10+ or 20+ turns)?
- Basis in paper: [explicit] "Our experiments were constrained by computational resources and model context windows, limiting MUSIC rollouts to T=5 turns and BoN evaluation to H=3 turns. The effectiveness of MUSIC for significantly longer conversations remains to be explored."
- Why unresolved: Multi-turn phenomena like coherence and consistency may manifest differently across short versus extended dialogues; the current experiments only validate short-horizon settings.
- What evidence would resolve it: Experiments applying MUSIC to datasets with longer conversation horizons, measuring RM performance degradation or improvement as turn count increases.

### Open Question 3
- Question: What mechanism drives the unexpected improvement (+3.9%) in single-turn reasoning evaluation (RewardBench Reasoning category)?
- Basis in paper: [inferred] "While MUSIC synthesizes multi-turn conversational data and is not explicitly designed for single-turn reasoning tasks, this suggests a potential positive transfer. We hypothesize that exposure to coherent, logically structured multi-turn dialogues during training may implicitly enhance the RM's ability to assess reasoning steps..."
- Why unresolved: The improvement is observed but only a hypothesis is offered; no ablation or analysis validates whether multi-turn coherence training transfers to single-turn reasoning assessment.
- What evidence would resolve it: Ablation studies isolating specific aspects of MUSIC data (e.g., only consistency-focused contrasts vs. instruction-following contrasts) and their impact on reasoning benchmarks, or analysis of learned representations.

## Limitations
- The effectiveness of the instruction contrast prompt in generating consistently inferior responses is assumed but not directly validated; no ablation or manual inspection is reported to confirm this critical assumption.
- The transfer benefit to single-turn reasoning remains weakly supported, with only a modest 3.9% gain on one benchmark category and no deeper mechanistic explanation.
- The use of a single model (Gemini 1.5 Pro) for both user and assistant simulators may introduce systematic biases, limiting the diversity of generated conversations.

## Confidence
- **High confidence:** The improvement in multi-turn BoN win rates (+3.8% to +5.1%) and the preservation of single-turn RewardBench performance are well-supported by the experimental results and clearly demonstrate MUSIC's efficacy in the intended domain.
- **Medium confidence:** The reasoning improvement (+3.9% on RewardBench) is reported but lacks a strong theoretical or empirical justification; the mechanism is plausible but not deeply validated.
- **Low confidence:** The effectiveness of the instruction contrast prompt in generating consistently inferior responses is assumed but not directly validated; no ablation or manual inspection is reported to confirm this critical assumption.

## Next Checks
1. **Manual inspection of contrast quality:** Sample and manually evaluate 20-30 MUSIC-generated conversation pairs to verify that chosen trajectories are consistently rated higher than rejected ones across turns by human or LLM judges.
2. **Ablation on simulation turns:** Train RMs with MUSIC rollouts of varying lengths (e.g., T ∈ {2, 3, 5, 7}) to measure the impact of conversation depth on multi-turn evaluation capability and identify the optimal T.
3. **Cross-model generalization test:** Evaluate the MUSIC-augmented RM with assistant models other than Gemma-2-9B-Instruct to assess whether the learned multi-turn preferences transfer beyond the training base model.