---
ver: rpa2
title: 'Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven
  Diabetes Risk Prediction'
arxiv_id: '2601.13388'
source_url: https://arxiv.org/abs/2601.13388
tags:
- diabetes
- patient
- interview
- data
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the use of large language models (LLMs)
  to extract structured social determinants of health (SDOH) data from unstructured
  patient narratives and assess their predictive value for diabetes control. Using
  unstructured interviews from 65 older T2D patients, including a high proportion
  of Asian Americans, LLMs converted qualitative narratives into structured SDOH ratings
  and qualitative summaries.
---

# Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction

## Quick Facts
- arXiv ID: 2601.13388
- Source URL: https://arxiv.org/abs/2601.13388
- Reference count: 28
- LLMs achieved 60% accuracy predicting diabetes control from interview text

## Executive Summary
This study demonstrates the use of large language models (LLMs) to extract structured social determinants of health (SDOH) data from unstructured patient narratives and assess their predictive value for diabetes control. Using unstructured interviews from 65 older T2D patients, including a high proportion of Asian Americans, LLMs converted qualitative narratives into structured SDOH ratings and qualitative summaries. The SDOH factors, extracted via retrieval-augmented generation, included income, housing, social support, diet, and healthcare access. These ratings were combined with traditional lab biomarkers and used as inputs for linear and tree-based machine learning models. LLMs also directly predicted diabetes control levels from interview text. The SDOH-only models showed limited predictive power (R² up to 0.046), while lab data remained the strongest predictors. LLMs achieved 60% accuracy in predicting diabetes control levels directly from interview text, highlighting their potential for generating actionable insights from unstructured data, though further validation and larger sample sizes are needed for clinical integration.

## Method Summary
The study used 65 unstructured patient interviews (236–7,366 words) from older T2D patients, combined with EHR lab values (triglycerides, HDL, LDL, glucose, creatinine, A1C). LLMs with retrieval-augmented generation (RAG) extracted 15 SDOH subtopic ratings (1–5 scale) from interview text using OpenAI text-embedding-3-large for semantic retrieval and GPT-4o for structured rating generation with supporting quotes. Missing values were imputed via KNN, scaled 0–1, and used as features in Ridge, Lasso, Random Forest, and XGBoost models to predict A1C levels. Separately, LLMs (GPT-4o, o1, o1-mini, DeepSeek r1) predicted diabetes control classes (low <6.0, medium 6.0–7.5, high >7.5) directly from A1C-redacted interview transcripts.

## Key Results
- SDOH-only models showed limited predictive power (R² up to 0.046)
- Lab data remained the strongest predictors, with glucose having highest feature importance (0.216)
- LLMs achieved 60% accuracy predicting diabetes control levels directly from interview text
- Cross-validated R² values were near zero or negative, suggesting overfitting from small sample size relative to feature count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation (RAG) reduces hallucinations when extracting structured SDOH ratings from unstructured narratives.
- Mechanism: The workflow generates embeddings for each interview using text-embedding-3-large, retrieves the most relevant excerpts for each risk factor, then passes excerpts to GPT-4o for structured rating generation with supporting quotes.
- Core assumption: Relevant excerpts contain sufficient signal for the LLM to assign accurate quantitative ratings on a 1-5 scale.
- Evidence anchors:
  - [abstract] "These narratives were analyzed using LLMs with retrieval-augmented generation to produce... structured quantitative SDOH ratings"
  - [section II.A] "We used retrieval-augmented generation (RAG) as our approach, leveraging its proven ability to reduce hallucinations and improve factual accuracy in responses"
  - [corpus] Weak direct evidence—neighbor papers focus on SDOH extraction but don't validate RAG-specific accuracy gains for rating tasks
- Break condition: Interviews where SDOH topics are distributed across many brief mentions rather than concentrated passages; excerpts may lack context for accurate rating.

### Mechanism 2
- Claim: LLMs can predict diabetes control levels directly from interview text by synthesizing behavioral and social cues, even with A1C values redacted.
- Mechanism: GPT-4o processes full interview transcripts and generates A1C predictions with justifications, relying on lifestyle indicators (diet, exercise, medication adherence) rather than explicit clinical markers.
- Core assumption: The language patterns describing daily diabetes management correlate with actual glycemic control in learnable ways.
- Evidence anchors:
  - [abstract] "LLMs achieved 60% accuracy in predicting diabetes control levels from interview text"
  - [section III.C] "Every justification included commentary on the person's lifestyle, whereas fewer mention strictly clinical markers"
  - [corpus] Neighbor paper "Integration of Large Language Models and Traditional Deep Learning for SDOH Prediction" shows LLMs can encode SDOH signals, but direct outcome prediction from narratives is underexplored
- Break condition: Patients who describe lifestyle improvements but have discordant A1C (e.g., Patient 3 with low A1C but 20 years of insulin therapy and hospitalizations)—temporal anchoring failures.

### Mechanism 3
- Claim: Extracted SDOH ratings can augment traditional biomarkers in machine learning risk models, though small sample sizes cause overfitting.
- Mechanism: Structured SDOH ratings (5 topics × 3 subtopics = 15 features) are combined with lab values (glucose, HDL, LDL, triglycerides, creatinine) as inputs to Ridge, Lasso, Random Forest, and XGBoost models.
- Core assumption: SDOH ratings capture orthogonal predictive signal beyond biomarkers.
- Evidence anchors:
  - [abstract] "SDOH-only models showed limited predictive power (R² up to 0.046), while lab data remained the strongest predictors"
  - [section III.B] "The low R² values, along with the disparity between the training and cross-validated R² values, suggest that the models are overfitting to the training data, likely due to the small sample size relative to the number of predictors"
  - [corpus] "Mining SDOH for Heart Failure Readmission via LLM" similarly combines extracted SDOH with structured EHR data but notes integration challenges
- Break condition: High-dimensional feature spaces with n=65 observations; KNN imputation for ~17% missing SDOH values introduces noise.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Extracts relevant passages from long interviews before LLM processing, grounding responses in actual patient statements
  - Quick check question: Can you explain why passing only retrieved excerpts (rather than full text) might improve rating accuracy but risk missing context?

- Concept: Cross-validated R² as overfitting diagnostic
  - Why needed here: Distinguishes models that generalize from models that memorize training data
  - Quick check question: If training R² is 0.70 but cross-validated R² is -0.05, what does this indicate about your model?

- Concept: Feature importance vs. predictive power
  - Why needed here: Glucose had highest feature importance (0.216) but combined model still had near-zero R²—importance rankings don't guarantee useful predictions
  - Quick check question: A feature has high Gini importance in a Random Forest but the model has negative R² on test data. Is this feature "predictive"?

## Architecture Onboarding

- Component map: Interview text -> embedding -> RAG retrieval -> LLM rating -> imputation -> feature scaling -> ML model training -> cross-validation. The direct prediction path bypasses structured extraction entirely.

- Critical path: Interview text → embedding → RAG retrieval → LLM rating → imputation → feature scaling → ML model training → cross-validation. The direct prediction path bypasses structured extraction entirely.

- Design tradeoffs:
  - More subtopics = richer signal but more missing values and higher dimensionality
  - Requiring quotes reduces hallucination risk but increases token costs and may miss implicit signals
  - Linear models (Ridge/Lasso) are more interpretable and handle small samples better than tree-based methods, but can't capture non-linear interactions

- Failure signatures:
  - All predictions clustering around "medium" control (class imbalance + model conservatism)
  - Training R² >> cross-validated R² (overfitting from high p/n ratio)
  - Duplicate information across similar subtopics (e.g., Diet Type vs. Dietary Restrictions)
  - Missing topic handling: rating of -1 may not be properly excluded from imputation

- First 3 experiments:
  1. Dimensionality reduction: Apply PCA or feature selection to reduce 15 SDOH features to 3-5 components before ML modeling; compare cross-validated R²
  2. Sample size scaling test: Bootstrap synthetic datasets at n=200, 500, 1000 from existing data distribution to estimate minimum viable sample size for stable R²
  3. Temporal validation: Compare LLM predictions against most recent A1C vs. longitudinal A1C trends (e.g., 12-month average) to assess whether narratives better capture trajectory than single snapshots

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (n=65) constrains model generalizability and likely drives overfitting in regression tasks
- SDOH-only models showed minimal predictive power (R² up to 0.046), suggesting extracted ratings may not capture sufficient signal beyond biomarkers
- High proportion of Asian American participants (54.8%) limits ethnic generalizability
- Cross-sectional design cannot assess temporal relationships between SDOH factors and glycemic control

## Confidence
- **High confidence**: LLMs can generate structured SDOH ratings from unstructured narratives when provided with RAG-extracted excerpts and required to include supporting quotes
- **Medium confidence**: Direct LLM prediction of diabetes control classes achieves 60% accuracy, though this is based on a small sample and may reflect dataset-specific patterns rather than generalizable predictive capability
- **Low confidence**: The claim that SDOH ratings augment biomarker-based models—the negative or near-zero R² values and large training-to-cross-validation gaps indicate models are not learning generalizable patterns from the extracted SDOH features

## Next Checks
1. **Temporal validation study**: Replicate the direct prediction task using longitudinal A1C trajectories (e.g., 12-month averages) rather than single-point values to assess whether narratives better capture glycemic trends versus snapshots
2. **Feature importance stability test**: Apply bootstrapping to assess whether the observed feature importance rankings (glucose, healthcare access, social support, housing) remain consistent across resampled datasets, indicating robust predictors versus artifacts
3. **External validation cohort**: Test the RAG+LLM extraction pipeline on a separate, demographically distinct patient population to evaluate whether the structured ratings generalize beyond the original Asian American cohort