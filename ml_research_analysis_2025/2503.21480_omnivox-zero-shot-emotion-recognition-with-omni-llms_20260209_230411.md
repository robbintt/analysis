---
ver: rpa2
title: 'OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs'
arxiv_id: '2503.21480'
source_url: https://arxiv.org/abs/2503.21480
tags:
- audio
- context
- acoustic
- iemocap
- meld
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces OmniVox, the first systematic evaluation\
  \ of four omni-LLMs on zero-shot emotion recognition from audio. The method uses\
  \ acoustic prompting\u2014explicit instructions for the models to analyze vocal\
  \ features like pitch, tone, and rhythm before predicting emotion."
---

# OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs

## Quick Facts
- **arXiv ID:** 2503.21480
- **Source URL:** https://arxiv.org/abs/2503.21480
- **Reference count:** 12
- **Key outcome:** Zero-shot omni-LLMs outperform fine-tuned audio-only models on IEMOCAP (55.9% W-F1) and MELD (51.3% W-F1), demonstrating strong emergent audio emotion recognition capabilities.

## Executive Summary
This paper introduces OmniVox, the first systematic evaluation of four omni-LLMs on zero-shot emotion recognition from audio. The method uses acoustic prompting—explicit instructions for models to analyze vocal features like pitch, tone, and rhythm before predicting emotion. Experiments on IEMOCAP and MELD show that zero-shot omni-LLMs outperform or match fine-tuned audio models, achieving up to 7.6% better accuracy on IEMOCAP and competitive performance on MELD. Including conversational context and acoustic analysis improves results, especially on IEMOCAP. Error analysis reveals that emotion confusions often align with mismatches in acoustic feature perception, highlighting areas for improvement. Overall, OmniVox demonstrates the strong potential of omni-LLMs for zero-shot emotion recognition from audio alone.

## Method Summary
OmniVox evaluates zero-shot emotion recognition using four omni-LLMs (GPT-4o-Audio, Gemini-2.0-Flash, Gemini-Lite, Phi-4-Multimodal) on IEMOCAP and MELD datasets. The framework employs three prompting strategies: Minimal (direct classification), Acoustic (intermediate feature description), and CoT (acoustic + step-by-step reasoning). Models process audio files with optional conversational context (0-12 previous turns) and output emotion labels. Performance is measured using weighted-F1, with results averaged over two runs at temperature=0.0. The acoustic prompting strategy explicitly guides models to analyze paralinguistic features before making predictions.

## Key Results
- Omni-LLMs achieve 55.9% W-F1 on IEMOCAP (7.6% better than fine-tuned audio-only baseline)
- Omni-LLMs reach 51.3% W-F1 on MELD (competitive with fine-tuned audio baselines at 48.1-52.3%)
- Acoustic prompting improves performance across all models (e.g., Gemini: 45.0%→48.1%, Phi-4: 24.4%→42.6%)
- Context benefits are corpus-dependent: consistent gains on IEMOCAP, mixed results on MELD

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit acoustic analysis prompting improves zero-shot emotion recognition accuracy compared to minimal instruction prompting.
- **Mechanism:** The prompt instructs the model to generate intermediate acoustic descriptions (pitch, volume, speaking rate, tone) before outputting the emotion label. This structured decomposition may guide the model's attention to paralinguistic cues rather than relying solely on semantic content.
- **Core assumption:** Omni-LLMs possess latent audio understanding capabilities that can be surfaced through explicit reasoning instructions; the intermediate acoustic descriptions meaningfully reflect the model's perception of the audio.
- **Evidence anchors:**
  - [abstract]: "We present acoustic prompting—an audio-specific prompting strategy for omni-LLMs which focuses on acoustic feature analysis, conversation context analysis, and step-by-step reasoning."
  - [section 4.2, Table 2]: Gemini improved from 45.0% (minimal, no context) to 48.1% (acoustic, no context); Phi-4-Multimodal improved from 24.4% to 42.6% (with context). CoT prompting further enhanced Gemini-Lite and GPT-4o-Audio with context, reaching 51.5% and 51.8% respectively.
  - [corpus]: Neighbor paper "Plug-and-Play Emotion Graphs for Compositional Prompting" (FMR=0.55) similarly finds compositional chain-of-thought prompting improves LALM emotion reasoning, suggesting cross-architectural consistency for prompting-based approaches.
- **Break condition:** If models cannot reliably perceive acoustic features (e.g., pitch, volume variation), acoustic prompting may produce hallucinated descriptions that mislead rather than guide prediction. Error analysis (RQ2) shows F1 of only 0.27-0.33 for acoustic variation features, suggesting this may already be occurring for fine-grained cues.

### Mechanism 2
- **Claim:** Conversational context improves audio-based emotion recognition for dyadic interactions (IEMOCAP) but provides inconsistent benefits for multi-party, noisy dialogues (MELD).
- **Mechanism:** Contextual audio from preceding utterances provides speaker state dynamics and emotional trajectory information. The model builds a representation of conversational flow, potentially disambiguating emotions that appear similar acoustically but differ in discourse context.
- **Core assumption:** Omni-LLMs can integrate multiple sequential audio inputs into a coherent representation; longer context windows capture meaningful emotional arcs without introducing noise.
- **Evidence anchors:**
  - [section 4.3, Table 3]: IEMOCAP shows consistent gains—GPT-4o-Audio from 46.0% (c=0) to 55.9% (c=12); Gemini-Lite peaks at 54.1% (c=12). MELD shows mixed results—Gemini improves to 48.8% (c=3) but GPT-4o-Audio degrades from 51.3% (c=0) to 47.1% (c=12).
  - [section 4.3]: Authors hypothesize MELD's multi-party TV dialogues with background noise, audience laughter, and higher SNR variability (56% higher std dev than IEMOCAP) may explain inconsistent context benefits.
  - [corpus]: "Cross-Space Synergy" neighbor paper (FMR=0.53) addresses gradient conflicts in deeper multimodal architectures for MERC, suggesting context integration challenges extend beyond omni-LLMs.
- **Break condition:** Context benefits reverse when additional audio contains conflicting speakers, background noise, or emotional shifts that the model cannot attribute to specific speakers. Context window size optimization is corpus-specific, not universal.

### Mechanism 3
- **Claim:** Zero-shot omni-LLMs can match or exceed fine-tuned audio-only models on emotion recognition, but underperform fine-tuned text and multimodal models.
- **Mechanism:** Omni-LLMs leverage pre-training on diverse audio-text corpora, transferring general audio understanding to emotion tasks without task-specific fine-tuning. The emergent capability stems from scale and multimodal exposure rather than supervised emotion data.
- **Core assumption:** The audio understanding capabilities observed are emergent from general pre-training rather than contamination from emotion datasets; the omni-LLMs have not been explicitly trained on IEMOCAP or MELD.
- **Evidence anchors:**
  - [section 4.4, Table 4]: OmniVox audio-only achieves 55.9% on IEMOCAP (vs. Shou et al. fine-tuned audio: 58.8%) and 51.3% on MELD (vs. Shou et al.: 52.0%), surpassing Yun et al. (48.1%) and Wu et al. (52.3%) fine-tuned audio baselines.
  - [section 1]: Authors note Qwen-2-Audio's multi-task pre-training included emotion recognition data, but evaluate different models (Gemini, GPT-4o, Phi-4) to assess emergent capabilities.
  - [corpus]: Weak corpus evidence—neighbor papers focus on fine-tuned approaches. "Quality-Controlled Multimodal Emotion Recognition" (FMR=0.51) emphasizes transfer learning but requires supervised stages. No direct zero-shot audio-only comparisons available.
- **Break condition:** Performance gains may not generalize to corpora with different acoustic properties, languages, or emotion label schemes. The zero-shot advantage diminishes when fine-tuned models have access to more modalities or larger training data.

## Foundational Learning

- **Concept: Zero-shot inference with omni-modal models**
  - **Why needed here:** The entire OmniVox framework assumes understanding how models perform tasks without task-specific training. The distinction between emergent capabilities (from scale) and transfer learning (from related pre-training tasks) is critical for interpreting results.
  - **Quick check question:** Can you explain why an omni-LLM might recognize emotions from audio it was never explicitly trained to classify, and what evidence would suggest this is true emergence vs. data contamination?

- **Concept: Acoustic feature extraction and paralinguistic communication**
  - **Why needed here:** The acoustic prompting mechanism requires understanding what features (pitch, volume, speaking rate, tone) carry emotional information, and why their variations (not just averages) matter for distinguishing emotions like excitement vs. happiness.
  - **Quick check question:** Why might an omni-LLM accurately predict average pitch (F1=0.62) but struggle with pitch variation (F1=0.27), and how would this affect its ability to distinguish frustrated from neutral speech?

- **Concept: Chain-of-thought prompting**
  - **Why needed here:** The CoT prompting strategy (acoustic analysis → reasoning → label) is central to OmniVox's approach. Understanding when intermediate reasoning helps vs. hurts is essential for deploying this framework.
  - **Quick check question:** In Table 2, why might CoT improve Gemini-Lite (41.2%→51.5% with context) but degrade Gemini (51.9%→49.9%) on IEMOCAP? What does this suggest about model-specific prompt sensitivity?

## Architecture Onboarding

- **Component map:**
  Audio file + optional context audio (0-12 turns) + text prompt (Minimal/Acoustic/CoT) + optional transcript -> Omni-LLM (GPT-4o-Audio, Gemini-2.0-Flash, Gemini-Lite, Phi-4-Multimodal) -> Extract emotion label from structured output -> Compute W-F1 on corpus-specific labels

- **Critical path:**
  1. Load audio utterance and optional context (preprocessing: ensure compatible format/sample rate)
  2. Construct prompt based on strategy (Minimal/Acoustic/CoT) and context window size
  3. Call omni-LLM API with audio + text prompt (set temperature=0 for reproducibility)
  4. Parse output to extract emotion label (handle malformed outputs)
  5. Aggregate predictions for weighted-F1 evaluation

- **Design tradeoffs:**
  - **Context window:** Longer context (c=12) helps IEMOCAP but hurts GPT-4o on MELD. Recommend c=3 as default, c=12 for clean dyadic audio.
  - **Prompting strategy:** CoT helps smaller models (Phi-4, Gemini-Lite) but may slightly hurt larger models. Recommend Acoustic as robust default.
  - **Modality:** Audio-only competitive for IEMOCAP; text-only better for MELD. If transcripts available, evaluate both and ensemble based on corpus characteristics.
  - **Model selection:** GPT-4o-Audio achieves highest peak performance but at higher API cost. Gemini-Lite offers best cost-performance ratio. Phi-4 is open-source but significantly underperforms.

- **Failure signatures:**
  - **Neutral defaulting:** Models over-predict "neutral" when uncertain—MELD shows 41-64% error rates for emotions misclassified as neutral (section 5.1). Mitigate by adjusting prompt to discourage neutral unless acoustically justified.
  - **Acoustic misperception:** Error analysis (RQ3) shows systematic over/underestimation of features—for angry→frustrated confusions, model overestimates volume/pitch/rate ~50% of time. This suggests acoustic grounding failure, not just label confusion.
  - **Context interference:** GPT-4o-Audio performance drops on MELD when context added (51.3%→47.7%). Signal: check if target corpus has multi-party speakers, background noise, or high SNR variability before enabling extended context.
  - **Similar emotion confusion:** Happy↔excited (IEMOCAP) and joy→neutral (MELD) are persistent error patterns aligned with acoustic feature overlap. These may require multimodal inputs or fine-tuning to resolve.

- **First 3 experiments:**
  1. **Establish baseline:** Run Minimal prompt with c=0 context on both IEMOCAP and MELD using Gemini-Lite (lowest cost). Document W-F1, confusion matrices, and inference latency. This provides a reference point for measuring prompting and context improvements.
  2. **Ablate prompting strategies:** Compare Minimal vs. Acoustic vs. CoT on a 100-utterance subset of IEMOCAP with c=3 context. Manually inspect 10 outputs per strategy to verify acoustic descriptions align with ground truth features (use SNR/pitch extraction tools). This validates whether improved scores reflect better reasoning or better calibration to prompt format.
  3. **Context window sweep on target domain:** If deploying on a specific corpus (e.g., customer service calls), run context window sweep (c=0,1,3,5) using Acoustic prompt. Measure both W-F1 and failure rate (neutral defaulting percentage). Identify optimal context size for your acoustic conditions before full deployment.

## Open Questions the Paper Calls Out

- **Can the acoustic prompting strategies developed for emotion recognition be effectively transferred to other cognitive state tasks such as sentiment analysis, belief detection, or intent recognition?**
  - The authors state: "We hypothesize that our work may transfer to other cognitive state tasks... In future work, we will use the insights... to apply to other cognitive state tasks."
  - The current study is restricted exclusively to the emotion recognition task (ERC) and does not test the framework on other cognitive state benchmarks.

- **Does the high performance of zero-shot omni-LLMs on scripted and acted datasets (IEMOCAP, MELD) persist when applied to spontaneous, real-world conversational audio?**
  - The authors note in the Limitations section that relying on IEMOCAP and MELD means the models "may not generalize to real-world emotion detection and classification."
  - The evaluated corpora consist of acted dyadic sessions and TV show dialogue, which possess distinct acoustic properties compared to naturalistic, "in-the-wild" speech.

- **Can improving an omni-LLM's ability to perceive dynamic acoustic variations (e.g., pitch/volume fluctuations) reduce the specific emotion confusion patterns observed between similar states?**
  - The error analysis (Section 5.2-5.3) reveals that models predict average acoustic levels well but fail on variations (F1 < 0.33), which correlates with systematic confusions like "angry → frustrated."
  - The paper identifies the "perception gap" regarding dynamic features and its correlation with errors, but does not propose or validate a method to bridge this gap.

## Limitations

- **Code availability:** No code released (paper states "We will release all code in the final version"), making exact reproduction difficult.
- **Data generalizability:** Performance evaluated only on scripted/acted datasets (IEMOCAP, MELD), which may not transfer to spontaneous real-world conversations.
- **Acoustic perception gaps:** Models show F1 scores of only 0.27-0.33 for fine-grained acoustic variation features, suggesting potential hallucination rather than reliable perception.

## Confidence

- **High confidence:** Omni-LLMs outperform fine-tuned audio-only models on IEMOCAP (55.9% vs 58.8%) and MELD (51.3% vs 48.1%), demonstrating strong zero-shot capabilities. Acoustic prompting consistently improves performance across models, and error patterns align with acoustic feature confusions.
- **Medium confidence:** Context benefits are corpus-dependent—helpful for IEMOCAP but potentially harmful for MELD. The finding that CoT helps smaller models (Phi-4, Gemini-Lite) but may slightly hurt larger models (Gemini) suggests prompt sensitivity varies by model size.
- **Low confidence:** Claims about emergent capabilities versus contamination from pre-training data are weakly supported, as the paper only notes that Qwen-2-Audio included emotion data while evaluating different models. The absence of direct zero-shot audio-only comparisons in related literature limits generalizability claims.

## Next Checks

1. Verify acoustic feature descriptions by extracting ground truth pitch/volume variation from 50 utterances and comparing to model-generated descriptions, checking if low F1 scores reflect hallucination rather than perceptual limitations.
2. Test context window sensitivity on a small subset of MELD to confirm whether GPT-4o-Audio's degradation with context (51.3%→47.7%) is reproducible before scaling to full corpus.
3. Implement minimal reproduction using open-source Phi-4-Multimodal to assess whether the significant performance gap (15-35% W-F1) reflects model architecture differences or implementation choices.