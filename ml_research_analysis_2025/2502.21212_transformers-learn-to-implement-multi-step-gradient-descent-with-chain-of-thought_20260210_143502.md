---
ver: rpa2
title: Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought
arxiv_id: '2502.21212'
source_url: https://arxiv.org/abs/2502.21212
tags:
- lemma
- transformer
- loss
- gradient
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought

## Quick Facts
- arXiv ID: 2502.21212
- Source URL: https://arxiv.org/abs/2502.21212
- Authors: Jianhao Huang; Zixuan Wang; Jason D. Lee
- Reference count: 40
- Primary result: CoT prompting enables one-layer transformers to implement iterative optimization, achieving near-exact recovery

## Executive Summary
This paper proves that Chain of Thought (CoT) prompting can transform a shallow transformer into a deep reasoning engine. By appending predicted weight vectors back into the input sequence, the model implements multi-step gradient descent (GD) iteratively. Without CoT, a one-layer transformer is fundamentally limited to a single GD step, resulting in high error on complex tasks. The theoretical analysis is complemented by empirical validation using both linear and softmax attention, demonstrating that the CoT mechanism enables near-exact recovery of ground-truth weights in synthetic linear regression tasks.

## Method Summary
The paper investigates whether transformers can learn iterative optimization through CoT prompting. It uses a one-layer linear self-attention transformer with residual connections to predict weight vectors for linear regression tasks. The model is trained on synthetic data with ground-truth intermediate reasoning steps (computed via GD). The training objective is the CoT loss, which sums MSE over predicted intermediate steps. Empirical validation is performed using both linear and standard softmax attention, with convergence analysis showing that gradient flow on the CoT objective converges to parameters implementing the multi-step GD algorithm.

## Key Results
- Theorem 3.2 proves existence of parameterizations that implement multi-step GD via CoT
- Theorem 3.1 shows one-layer transformers without CoT are limited to single-step GD
- Theorem 4.1 proves gradient flow on CoT objective converges to GD-implementing parameters
- Empirical validation shows standard softmax transformers can learn CoT behavior

## Why This Works (Mechanism)

### Mechanism 1
Chain of Thought (CoT) prompting enables a single-layer linear transformer to implement iterative optimization (multi-step Gradient Descent) autoregressively. By appending the predicted weight vector $\hat{w}_i$ back to the input sequence at each generation step, the transformer creates a feedback loop. The paper proves that under specific parameterization, each forward pass computes a gradient update term $-\eta \nabla L$, effectively stepping closer to the optimal solution. This transforms a shallow architecture into a deep computational chain dynamically.

### Mechanism 2
Without CoT, a one-layer transformer is fundamentally limited to performing a single step of Gradient Descent, leading to high error on complex tasks. The global minimizer of the evaluation loss without intermediate tokens mathematically reduces to a single update rule. While this is optimal for a one-layer architecture, it is insufficient for tasks requiring iterative refinement, particularly when sample counts $n$ are not infinite.

### Mechanism 3
Gradient flow on the CoT objective converges to parameters that implement the multi-step GD algorithm, implying the solution is learnable, not just existent. The training dynamics undergo a stage-wise process. Initially, dynamics along different eigenspaces of the attention matrices evolve nearly independently, driving parameters near the optimal solution. Subsequently, local convergence fine-tunes the solution to implement the exact GD steps with small error.

## Foundational Learning

- **Concept: Linear Self-Attention**
  - Why needed here: The paper replaces standard softmax attention with a linear approximation ($Z + V Z Z^\top W Z$) to make the training dynamics analytically tractable.
  - Quick check question: Can you write the update equation for linear attention without the softmax normalization?

- **Concept: Gradient Descent (GD) on Linear Regression**
  - Why needed here: The "reasoning" mechanism is explicitly mapped to the iterative update rule of GD on a Mean Squared Error (MSE) loss.
  - Quick check question: If $w_{t+1} = w_t - \eta \nabla L(w_t)$, what happens if $\eta$ is too large?

- **Concept: Autoregressive Token Generation**
  - Why needed here: The CoT mechanism functions by appending the output of step $t$ as the input token for step $t+1$.
  - Quick check question: How does the sequence length $N$ change as the model generates $k$ reasoning steps?

## Architecture Onboarding

- **Component map:** Input Sequence ($Z$) -> Linear Self-Attention Layer ($f_{LSA}$) -> Residual Connection -> CoT Loop
- **Critical path:**
  1. Initialize $Z_0$ with context data and $w_0=0$
  2. Pass $Z_0$ through $f_{LSA}$
  3. Extract $\hat{w}_1$ from the last column
  4. Form $Z_1$ by appending $\hat{w}_1$ to $Z_0$
  5. Repeat for $k$ steps
- **Design tradeoffs:**
  - Linear vs. Softmax: The paper uses linear attention to prove convergence (Theorem 4.1). Real-world softmax models are empirically validated but lack the same theoretical guarantees.
  - CoT vs. Looping: The paper notes looped transformers (reusing layers) perform similarly to CoT (generating tokens), but CoT allows explicit supervision of intermediate steps.
- **Failure signatures:**
  - Plateau without CoT: Loss stabilizes at $\Theta(d^2/n)$ (Corollary 3.1) instead of converging to near-zero
  - Initialization Sensitivity: Convergence to the GD solution requires specific spectral initialization (Assumption 4.1); random initialization might delay convergence or get stuck in local minima
- **First 3 experiments:**
  1. Expressiveness Gap Verification: Train the linear transformer on linear regression without CoT tokens. Confirm that the final loss matches the theoretical lower bound for single-step GD ($\Theta(d^2/n)$)
  2. CoT Convergence Test: Train with CoT tokens (ground-truth intermediate steps). Verify that the parameters $V, W$ converge to the specific construction in Theorem 3.2 and loss drops to near-zero
  3. OOD Generalization Check: Train on data with identity covariance $\Sigma = I$, then evaluate on data with a conditioned covariance $\Sigma \neq I$. Check if the model maintains low loss as per Theorem 4.2

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical convergence guarantees be extended to finite sample settings or online Stochastic Gradient Descent (SGD)?
- Basis in paper: Section 6 (Conclusion) and Appendix A.4 explicitly ask, "Can we move beyond population loss... and show a sample complexity guarantee?" and suggest generalizing to "train the model with online SGD."
- Why unresolved: The current analysis simplifies the optimization landscape by assuming population loss (infinite data) to facilitate closed-form gradient computation and dynamic analysis without noise.
- What evidence would resolve it: A proof establishing a sample complexity bound for the CoT objective, or a theoretical guarantee showing convergence rates for online SGD under this framework.

### Open Question 2
Does the framework apply to compositional reasoning tasks, or is it limited to iterative algorithmic tasks like gradient descent?
- Basis in paper: Section 6 explicitly asks, "Can CoT empower the transformer to acquire compositional reasoning capability instead of doing the same iterative steps?"
- Why unresolved: The paper successfully characterizes training dynamics for iterative refinement (multi-step GD), but compositional reasoning requires combining distinct sub-tasks, which may not fit the current dynamic model of repeated identical steps.
- What evidence would resolve it: A theoretical extension showing that transformers can learn to route intermediate steps to different computational sub-modules, or a proof of convergence on a task requiring multi-hop reasoning rather than iterative optimization.

### Open Question 3
Do the training dynamics and convergence results hold for standard softmax attention with random initialization?
- Basis in paper: Appendix A.4 states that the use of linear self-attention and specific reparameterization "deviates from the practical softmax attention," and calls for "analyzing the dynamics using more practical architectures."
- Why unresolved: The proofs rely on linear attention to derive closed-form expressions for gradients and eigenvalue dynamics; the non-linearity of softmax introduces complex interactions (and potential vanishing/exploding gradients) not covered by the current theory.
- What evidence would resolve it: A theoretical analysis deriving similar convergence guarantees for single-layer transformers utilizing softmax attention and standard random initialization.

## Limitations
- Initialization Sensitivity: The theoretical convergence proofs rely on specific spectral properties of the initialization, creating uncertainty about whether observed learning is due to the mechanism or favorable initialization.
- Generalization Gap: While theory proves the model can implement multi-step GD, it doesn't fully address how well this generalizes beyond the training distribution.
- Softmax vs Linear Attention: The theoretical results are proven for linear self-attention, but empirical validation uses standard softmax attention, creating a gap between what is proven and observed.

## Confidence
- **High Confidence**: The theoretical existence of parameterizations that implement multi-step GD (Theorem 3.2) and the characterization of the single-step limitation (Theorem 3.1).
- **Medium Confidence**: The empirical demonstration that standard softmax transformers can learn the CoT behavior, and the convergence analysis (Theorem 4.1).
- **Low Confidence**: The practical applicability of this mechanism to real-world reasoning tasks, as the paper focuses on a synthetic linear regression task.

## Next Checks
1. **Initialization Sensitivity Test**: Reproduce experiments with different initialization schemesâ€”standard random initialization, theoretically-specified spectral initialization, and random initialization with poor spectral properties. Compare convergence rates and final performance.
2. **Scaling Experiment**: Increase problem dimensions (d and n) and test whether the CoT mechanism maintains effectiveness. Monitor attention matrix conditioning and gradient norms to identify at what scale the linear attention approximation breaks down.
3. **Alternative Task Validation**: Apply the methodology to a non-linear regression task (e.g., polynomial regression) or simple classification problem. Test whether CoT can learn iterative refinement for more complex optimization landscapes.