---
ver: rpa2
title: 'DeskVision: Large Scale Desktop Region Captioning for Advanced GUI Agents'
arxiv_id: '2503.11170'
source_url: https://arxiv.org/abs/2503.11170
tags:
- data
- desktop
- arxiv
- deskvision
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of large-scale, diverse desktop GUI
  data by introducing AutoCaptioner, an automated data generation pipeline that combines
  UI detection models with large vision language models to produce high-quality, richly
  annotated desktop screenshots. Using AutoCaptioner, the authors create DeskVision,
  a novel dataset of 54,855 desktop images with 303,622 annotations balanced across
  Windows, macOS, and Linux, featuring detailed region captions for UI elements.
---

# DeskVision: Large Scale Desktop Region Captioning for Advanced GUI Agents

## Quick Facts
- **arXiv ID:** 2503.11170
- **Source URL:** https://arxiv.org/abs/2503.11170
- **Reference count:** 40
- **Primary result:** Introduces DeskVision, a large-scale desktop GUI dataset with 54,855 images and 303,622 annotations, and achieves state-of-the-art performance on desktop GUI understanding benchmarks.

## Executive Summary
This paper addresses the critical shortage of large-scale, diverse desktop GUI data by introducing AutoCaptioner, an automated pipeline that combines UI detection models with large vision language models to generate high-quality region captions. Using this pipeline, the authors create DeskVision, a novel dataset of 54,855 desktop screenshots with 303,622 annotations balanced across Windows, macOS, and Linux. They also introduce DeskVision-Eval, a 5,000-sample benchmark that is 25 times larger than existing desktop benchmarks. The authors train GUIExplorer, a GUI understanding model using DeskVision, which achieves state-of-the-art performance on multiple benchmarks including ScreenSpot, GUI-Env, and DeskVision-Eval.

## Method Summary
The authors developed AutoCaptioner, a pipeline that first filters raw screenshots using a YOLOv5-based classifier trained iteratively to remove unrelated and invalid images, then employs a hierarchical detection strategy combining OmniParser (icons) and PaddleOCR (text) with IoU-based fusion to localize UI elements. A Large Vision-Language Model (LVLM) then generates detailed region captions for each detected element. This pipeline was used to create DeskVision from 54,855 desktop images, and the GUIExplorer model was trained using SigLIP visual encoder, Qwen-2 LLM, and a 2-layer MLP connector, fine-tuned first on general grounding data then on GUI-specific datasets including DeskVision.

## Key Results
- DeskVision achieves state-of-the-art performance on desktop GUI benchmarks, improving LVLM performance on GUI tasks by 22.1% for Qwen2-VL and 26.1% for LLaVA-OneVision
- DeskVision-Eval is 25 times larger than existing desktop benchmarks with 5,000 samples
- GUIExplorer demonstrates superior grounding accuracy across Windows, macOS, and Linux operating systems
- The dataset shows broad applicability, improving performance not only on desktop tasks but also on mobile and web GUI understanding

## Why This Works (Mechanism)

### Mechanism 1
High-quality training data can be generated from noisy web sources via iterative filtering and hierarchical annotation. The AutoCaptioner pipeline uses a 3-stage YOLOv5 classifier (>95% accuracy) to filter valid screens, then fuses OmniParser and PaddleOCR detections with IoU thresholds before invoking an LVLM for detailed captions. Core assumption: visual features are sufficiently accurate for LVLM association. Break condition: if invalid data with pre-existing annotations isn't rigorously filtered, the model may learn from visual artifacts rather than UI elements themselves.

### Mechanism 2
Detailed "Region Captions" describing text, type, and attributes significantly improve GUI agent grounding compared to simple labels. By fine-tuning LVLMs to generate descriptive captions (e.g., "Blue button with Wi-Fi icon" instead of just "icon"), the training data enforces deeper visual-semantic associations that help disambiguate similar elements. Core assumption: additional tokens provide discriminative signal rather than noise. Break condition: if captions become too verbose or hallucinated, they may mislead grounding for small UI elements with limited context.

### Mechanism 3
Balancing dataset distribution across operating systems improves GUI agent generalizability compared to Windows-heavy datasets. By sourcing data from Windows, macOS, and Linux, the model prevents overfitting to Windows-specific patterns and learns OS-agnostic features of interactive elements. Core assumption: interactive elements share learnable common structures across OS styling. Break condition: if model capacity is insufficient, learning three distinct UI styles might lead to underfitting on all of them compared to specialized single-OS models.

## Foundational Learning

- **Visual Grounding (Referring Expression Comprehension)**: The core task of GUIExplorer is locating specific UI elements based on queries. Without understanding grounding, one cannot evaluate the "Element Accuracy" metric central to the paper. Quick check: Can you distinguish between an OCR task (reading text) and a grounding task (finding the location of the text "Submit" inside a specific button)?

- **Instruction Tuning / SFT (Supervised Fine-Tuning)**: The authors improve general LVLMs by fine-tuning them on DeskVision. Understanding how general pre-training differs from specific domain adaptation is key. Quick check: Why would a general LVLM struggle to identify a specific proprietary Linux widget despite having seen millions of generic internet images?

- **Data Ablation Studies**: The paper validates its dataset by showing performance deltas when added to other training regimes. Quick check: If a model trained on Dataset A + B performs worse than a model trained on Dataset A alone, what does that imply about Dataset B?

## Architecture Onboarding

- **Component map**: Raw screenshots -> YOLOv5 Classifier (Valid/Invalid) -> OmniParser+PaddleOCR fusion (IoU filtering) -> Qwen2.5-VL Captioner -> DeskVision dataset
- **Critical path**: The reliability of the UI Detector (filtering step) is the bottleneck. If the detector misses an icon, the Captioner cannot describe it, creating "false negative" gaps where interactive elements are invisible to the agent.
- **Design tradeoffs**: Chose "Real" web-scraped data over "Synthetic" A11y-tree generated data to capture realistic user scenarios, accepting noisier inputs requiring complex filtering. Uses high IoU thresholds (0.7) and strict filtering to prioritize high-quality annotations over capturing every element.
- **Failure signatures**: Text-icon confusion if IoU filtering isn't tuned correctly; small object hallucination where Captioner invents attributes for visually indistinct icons (e.g., 10x10 px).
- **First 3 experiments**: 1) Run AutoCaptioner on 100 screenshots from your target domain and manually verify the Valid/Invalid classifier. 2) Fine-tune a baseline LVLM using only OS-Atlas desktop data vs. OS-Atlas + DeskVision to reproduce performance gains. 3) Evaluate GUIExplorer specifically on Linux and macOS subsets of DeskVision-Eval to verify balanced dataset claims.

## Open Questions the Paper Calls Out
1. How can the current static dataset be effectively expanded to include trajectory-based data to facilitate the development of multi-step GUI agents? (The conclusion states plans to expand DeskVision to include trajectory-based GUI desktop data for multi-step GUI agents.)

2. What are the specific cross-domain transfer mechanisms that allow a model trained on DeskVision (desktop data) to improve performance on mobile and web tasks? (Table 5 shows DeskVision improves Mobile and Web accuracy, but the paper doesn't analyze why desktop data helps distinct interfaces.)

3. Can the observed performance trade-off between Icon/Widget grounding and Text grounding be resolved without integrating specialized OCR modules? (GUIExplorer achieves SOTA on icons/widgets but has decreased text performance compared to models with dedicated OCR modules.)

## Limitations
- Dataset representativeness may exhibit biases toward certain application types or user demographics despite 54,855 samples across three OSes
- Generalizability of captioning model may degrade on non-standard UI designs or accessibility-focused interfaces that deviate from typical patterns
- Evaluation benchmark size remains relatively small at 5,000 samples for statistical significance across all UI element types and OS variations

## Confidence
- **High confidence**: The architectural design of GUIExplorer (SigLIP + Qwen-2 + MLP projector) is well-specified and follows established LVLM fine-tuning practices
- **Medium confidence**: The claimed 22.1% and 26.1% performance improvements from DeskVision are supported by ablation studies but rely on internal comparisons without external replication
- **Medium confidence**: The claim of OS-agnostic generalization is supported by dataset balancing but requires further validation across more diverse application categories

## Next Checks
1. Apply the trained GUIExplorer model to a held-out set of 100 desktop screenshots from applications not represented in the training data to verify cross-domain generalization claims.

2. Manually evaluate 200 region captions from the DeskVision dataset to measure hallucination rates and semantic accuracy, particularly for small UI elements under 32x32 pixels.

3. Conduct detailed breakdown of Element Accuracy by operating system and UI element type to identify if certain OSes or element categories remain under-generalized despite the balanced dataset design.