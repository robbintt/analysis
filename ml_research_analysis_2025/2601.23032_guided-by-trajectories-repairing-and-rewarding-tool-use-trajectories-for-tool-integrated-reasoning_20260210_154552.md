---
ver: rpa2
title: 'Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for
  Tool-Integrated Reasoning'
arxiv_id: '2601.23032'
source_url: https://arxiv.org/abs/2601.23032
tags:
- reasoning
- trajectory
- answer
- trajectories
- result
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AutoTraj improves tool-integrated reasoning by automatically repairing\
  \ low-quality tool-use trajectories and training a trajectory reward model to provide\
  \ fine-grained supervision. It follows a two-stage framework: in the SFT stage,\
  \ it synthesizes multiple candidate trajectories, repairs low-quality ones, and\
  \ trains on the resulting high-quality set; in the RL stage, it trains a trajectory\
  \ reward model on positive\u2013negative trajectory pairs and optimizes with outcome,\
  \ format, and trajectory rewards."
---

# Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning

## Quick Facts
- arXiv ID: 2601.23032
- Source URL: https://arxiv.org/abs/2601.23032
- Reference count: 40
- Primary result: AutoTraj achieves 34.38% average accuracy across reasoning benchmarks while reducing trajectory length from ~1,000 to ~250 tokens

## Executive Summary
AutoTraj addresses the challenge of low-quality tool-use trajectories in tool-integrated reasoning by introducing a two-stage framework that automatically repairs trajectories and trains a reward model to provide fine-grained supervision. The approach first synthesizes multiple candidate trajectories, repairs low-quality ones, and trains on the resulting high-quality set in the SFT stage. In the RL stage, it trains a trajectory reward model on positive-negative trajectory pairs and optimizes with outcome, format, and trajectory rewards. The method demonstrates significant improvements in both accuracy (34.38% average) and efficiency (reduced trajectory length from ~1,000 to ~250 tokens) compared to existing SFT-RL TIR methods.

## Method Summary
AutoTraj operates through a two-stage framework for improving tool-integrated reasoning. In the SFT stage, the system synthesizes multiple candidate trajectories for each query, identifies and repairs low-quality trajectories, then trains on the resulting high-quality trajectory set. The RL stage involves training a trajectory reward model using positive-negative trajectory pairs, followed by reinforcement learning optimization that incorporates outcome rewards, format rewards, and trajectory rewards. This approach provides more granular supervision than traditional outcome-only reward signals by capturing the quality of intermediate reasoning steps.

## Key Results
- Achieves 34.38% average accuracy across mathematical and knowledge-intensive reasoning benchmarks
- Outperforms existing SFT-RL TIR methods on tested datasets
- Reduces average trajectory length from ~1,000 to ~250 tokens, indicating higher reasoning efficiency

## Why This Works (Mechanism)
The method works by addressing the core problem that tool-use trajectories in reasoning tasks are often low-quality due to errors in tool selection, execution, or reasoning flow. By automatically repairing these trajectories and training a reward model that captures fine-grained quality signals across the entire reasoning process (not just final outcomes), AutoTraj provides more effective supervision for learning tool-integrated reasoning. The two-stage approach ensures that the model first learns from high-quality examples before being fine-tuned with reward-based optimization that reinforces both correct outcomes and efficient reasoning patterns.

## Foundational Learning
- Tool-integrated reasoning: Combining language models with external tools to enhance reasoning capabilities
  - Why needed: Language models alone often struggle with complex reasoning requiring external knowledge or computation
  - Quick check: Verify the system can correctly call appropriate tools and integrate their outputs into reasoning

- Trajectory repair: Identifying and fixing errors in intermediate reasoning steps
  - Why needed: Tool-use trajectories often contain errors that propagate and degrade final performance
  - Quick check: Assess whether repaired trajectories lead to improved final outcomes compared to original trajectories

- Reward modeling for trajectories: Training models to evaluate the quality of entire reasoning paths
  - Why needed: Traditional outcome-only rewards miss important quality signals in intermediate reasoning steps
  - Quick check: Validate that the reward model can distinguish between high-quality and low-quality trajectories

## Architecture Onboarding
**Component Map:** Query -> Trajectory Synthesizer -> Trajectory Repair -> SFT Training -> Reward Model Training -> RL Optimization -> Improved Model

**Critical Path:** The critical path flows from query input through trajectory synthesis, repair, and both training stages (SFT then RL) to produce the final improved model. Each stage depends on the successful completion of the previous one.

**Design Tradeoffs:** The approach trades computational complexity (generating and repairing multiple trajectories per query) for improved reasoning quality. The two-stage training allows for initial learning from clean examples before reinforcement learning fine-tuning.

**Failure Signatures:** 
- Poor trajectory synthesis leading to inadequate training data
- Ineffective repair mechanisms that fail to fix critical errors
- Reward model that cannot capture nuanced quality differences
- RL optimization that overfits to specific trajectory patterns

**First Experiments:**
1. Test trajectory synthesis quality on a small set of queries to ensure diversity and relevance
2. Validate trajectory repair effectiveness by comparing repaired vs. original trajectories on a held-out set
3. Evaluate reward model discrimination ability on known good vs. bad trajectory pairs

## Open Questions the Paper Calls Out
None

## Limitations
- The SFT stage's synthetic trajectory generation depends heavily on the teacher model's ability to generate diverse, high-quality examples, which may not generalize well to out-of-distribution queries
- The reward model is trained on relatively small trajectory datasets, potentially limiting its ability to capture nuanced quality distinctions across diverse reasoning patterns
- The evaluation focuses primarily on accuracy improvements without comprehensive ablations showing the relative contributions of each reward component or the trajectory repair mechanism

## Confidence
- **High Confidence:** The core methodology of trajectory repair + reward modeling + RL fine-tuning is technically sound and well-described. The reported accuracy improvements over baselines are consistent with the expected benefits of this approach.
- **Medium Confidence:** The efficiency gains (reduced trajectory length) are plausible given the optimization objective, but the trade-off between brevity and reasoning quality requires deeper investigation.
- **Low Confidence:** Claims about AutoTraj's superiority in knowledge-intensive tasks are based on limited benchmarks without evaluation on broader knowledge domains or real-world applications.

## Next Checks
1. Conduct ablation studies to isolate the impact of trajectory repair vs. reward modeling vs. RL optimization on final performance
2. Test generalization to out-of-domain reasoning tasks not represented in the training trajectories
3. Evaluate whether the shorter trajectories maintain equivalent or superior reasoning quality through human evaluation or detailed error analysis