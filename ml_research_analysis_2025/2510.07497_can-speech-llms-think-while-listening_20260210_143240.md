---
ver: rpa2
title: Can Speech LLMs Think while Listening?
arxiv_id: '2510.07497'
source_url: https://arxiv.org/abs/2510.07497
tags:
- reasoning
- question
- speech
- answer
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We explore the effect of chain-of-thought (CoT) prompting on multi-stream
  speech large language models (speech LLMs) for reasoning tasks. Our text-based CoT
  fine-tuning approach improves accuracy by 2.4x on average across spoken reasoning
  benchmarks.
---

# Can Speech LLMs Think while Listening?

## Quick Facts
- **arXiv ID:** 2510.07497
- **Source URL:** https://arxiv.org/abs/2510.07497
- **Reference count:** 40
- **Primary result:** 2.4x accuracy improvement on reasoning tasks with "thinking while listening" paradigm

## Executive Summary
This paper investigates the application of chain-of-thought (CoT) prompting to multi-stream speech large language models (speech LLMs) for reasoning tasks. The authors propose a novel approach that combines text-based CoT fine-tuning with a "thinking while listening" paradigm, where reasoning begins based on a Question Completeness metric. This method aims to optimize the trade-off between accuracy and latency in speech-based reasoning tasks.

## Method Summary
The authors develop a text-based CoT fine-tuning approach for speech LLMs, enabling them to perform reasoning tasks more effectively. They introduce a Question Completeness metric to determine the optimal point for reasoning onset during speech processing. Additionally, they apply Direct Preference Optimization (DPO) using preference data created via rejection sampling to further improve the accuracy-latency Pareto frontier. The combination of these techniques allows the model to start reasoning while still processing speech input, potentially reducing overall latency.

## Key Results
- 2.4x average accuracy improvement on spoken reasoning benchmarks through text-based CoT fine-tuning
- 70% latency reduction achieved without accuracy loss using the proposed "thinking while listening" paradigm
- Improved accuracy-latency Pareto frontier through the combination of CoT and DPO techniques

## Why This Works (Mechanism)
The proposed method works by leveraging the ability of speech LLMs to process multiple streams of information simultaneously. By starting the reasoning process early based on question completeness, the model can begin forming logical connections while still receiving additional speech input. This parallel processing approach, combined with CoT fine-tuning and DPO, allows for more efficient use of computational resources and improved performance on reasoning tasks.

## Foundational Learning
- **Chain-of-thought (CoT) prompting:** A technique where language models are guided to reason step-by-step through a problem. Why needed: Enables complex reasoning tasks by breaking them down into manageable steps. Quick check: Verify that the model can correctly solve multi-step reasoning problems.
- **Multi-stream speech processing:** The ability of speech models to handle multiple audio streams simultaneously. Why needed: Allows for parallel processing of speech input and reasoning tasks. Quick check: Confirm that the model can process multiple audio inputs without significant degradation in quality.
- **Question Completeness metric:** A novel metric to determine when sufficient information is available to begin reasoning. Why needed: Enables early initiation of reasoning while minimizing the risk of incomplete information. Quick check: Validate that the metric accurately identifies complete questions across diverse speech patterns.
- **Direct Preference Optimization (DPO):** A method for fine-tuning language models based on human preferences. Why needed: Improves the quality of model outputs by aligning them with human preferences. Quick check: Ensure that DPO results in more desirable outputs according to human evaluators.
- **Rejection sampling for preference data:** A technique to create preference data by sampling and rejecting low-quality examples. Why needed: Generates high-quality preference data for DPO training. Quick check: Verify that the rejection sampling process effectively filters out low-quality examples.

## Architecture Onboarding
- **Component map:** Speech input -> Multi-stream processing -> Question Completeness assessment -> Early reasoning initiation -> CoT fine-tuning -> DPO refinement -> Final output
- **Critical path:** The critical path involves the Question Completeness assessment, which determines when to initiate early reasoning. This decision point is crucial for balancing accuracy and latency.
- **Design tradeoffs:** The main tradeoff is between accuracy and latency. Starting reasoning too early may lead to incomplete information, while waiting too long increases latency. The Question Completeness metric aims to optimize this tradeoff.
- **Failure signatures:** Potential failures include premature reasoning initiation (leading to incomplete or incorrect answers) and delayed reasoning (resulting in increased latency without accuracy gains).
- **First experiments:**
  1. Test the Question Completeness metric on a diverse set of speech inputs to evaluate its robustness.
  2. Compare the accuracy-latency tradeoff with and without early reasoning initiation across various reasoning tasks.
  3. Assess the impact of CoT fine-tuning and DPO on model performance using human evaluations.

## Open Questions the Paper Calls Out
None

## Limitations
- The accuracy improvements and latency reductions may not generalize to all types of reasoning tasks or speech patterns.
- The Question Completeness metric requires further validation across diverse acoustic environments and speaker characteristics.
- The effectiveness of the rejection sampling approach for preference data creation may vary depending on the specific speech LLM and task domain.

## Confidence
- High confidence: The general framework for combining CoT with speech LLMs is methodologically sound
- Medium confidence: The specific accuracy improvements and latency reductions, as these depend heavily on implementation details
- Low confidence: The generalizability of the Question Completeness metric across diverse acoustic environments and speaker characteristics

## Next Checks
1. Test the Question Completeness metric across multiple languages and speaker demographics to assess robustness.
2. Validate the latency-accuracy trade-off on real-time streaming applications with varying network conditions.
3. Compare the rejection sampling approach for preference data creation against alternative methods to confirm the claimed 70% latency reduction is optimal.