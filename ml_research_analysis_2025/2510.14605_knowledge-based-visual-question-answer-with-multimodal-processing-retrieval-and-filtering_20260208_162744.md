---
ver: rpa2
title: Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval
  and Filtering
arxiv_id: '2510.14605'
source_url: https://arxiv.org/abs/2510.14605
tags:
- question
- answer
- information
- tool
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a three-stage multimodal retrieval method (Processing,
  Retrieval, Filtering) for knowledge-based visual question answering. The approach
  uses visual tools to generate precise retrieval queries, integrates visual and text
  features for retrieval, and filters irrelevant content.
---

# Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering

## Quick Facts
- arXiv ID: 2510.14605
- Source URL: https://arxiv.org/abs/2510.14605
- Authors: Yuyang Hong; Jiaqi Gu; Qi Yang; Lubin Fan; Yue Wu; Ying Wang; Kun Ding; Shiming Xiang; Jieping Ye
- Reference count: 40
- Primary result: SOTA on E-VQA (36.0) and InfoSeek (42.8)

## Executive Summary
This paper proposes a three-stage multimodal retrieval method (Processing, Retrieval, Filtering) for knowledge-based visual question answering. The approach uses visual tools to generate precise retrieval queries, integrates visual and text features for retrieval, and filters irrelevant content. Trained with reinforcement learning using answer accuracy and format consistency as rewards, the method improves reasoning and tool invocation. Experiments show state-of-the-art performance on E-VQA (36.0) and InfoSeek (42.8).

## Method Summary
The paper introduces a three-stage multimodal retrieval method (Processing→Retrieval→Filtering) for knowledge-based visual question answering. The processing stage dynamically invokes visual tools (captioning, grounding, flipping) to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to search a knowledge base of Wikipedia articles and images. The filtering stage performs relevance filtering and concentration on retrieval results. The system is trained with reinforcement learning using answer accuracy and format consistency as reward signals. Experiments show state-of-the-art performance on E-VQA (36.0) and InfoSeek (42.8).

## Key Results
- Achieves SOTA accuracy of 36.0 on E-VQA and 42.8 on InfoSeek benchmarks
- RL training improves answer accuracy from 41.8% to 46.3% on InfoSeek compared to supervised fine-tuning
- Only 4K training samples needed versus 934K full dataset, demonstrating sample efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Tool-based preprocessing improves retrieval precision by extracting question-relevant visual information before search.
- **Mechanism**: The VLM-PRF model analyzes the question, then autonomously invokes visual tools (captioning for semantic context, grounding for region-of-interest extraction, flipping for orientation normalization). Each tool generates a specialized query: captioning produces text descriptions for semantic retrieval; grounding crops the image to the relevant object for visual similarity search; flipping mitigates retrieval bias from image orientation.
- **Core assumption**: The base VLM (Qwen2.5-VL) has sufficient grounding and captioning capability that RL can shape into strategic tool selection without supervised tool-use examples.
- **Evidence anchors**:
  - [abstract]: "The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval."
  - [section 4.2, Table 2]: Retrieval recall improves from 45.56% (raw image) to 54.89% (VLM-PRF-7B with tools).
  - [corpus]: mKG-RAG and ReAG papers similarly show structured preprocessing benefits for KB-VQA, though they use different approaches (knowledge graphs vs. tool invocation).
- **Break condition**: If grounding tool produces incorrect bounding boxes or captioning generates irrelevant descriptions, retrieval degrades. Table 8 shows grounding alone (+0.98%) underperforms captioning (+1.94%), suggesting captioning is more reliable.

### Mechanism 2
- **Claim**: Reinforcement learning with outcome-based rewards enables learning of implicit reasoning strategies that supervised fine-tuning cannot capture from QA pairs alone.
- **Mechanism**: GRPO optimization uses answer accuracy (exact match with ground truth) and format compliance (proper use of ılar, <tool>, <answer> tags) as reward signals. The model explores different tool combinations and filtering strategies across 8 sampled responses per question, reinforcing behaviors that maximize answer correctness. This avoids the need for annotated reasoning chains.
- **Core assumption**: The reward landscape is smooth enough that policy gradient methods can discover effective tool-use and filtering strategies from sparse success signals.
- **Evidence anchors**:
  - [abstract]: "trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner"
  - [section 4.2, Table 6]: RL training achieves 46.3% accuracy vs. SFT's 41.8% on InfoSeek, despite SFT having access to ground-truth reasoning.
  - [corpus]: No directly comparable RL-for-multimodal-RAG papers found; the paper claims novelty here. Visual-RFT uses RL for detection but not retrieval.
- **Break condition**: If answer reward alone is insufficient (sparse signal), format rewards guide structure but may produce compliant yet incorrect outputs. Table 13 shows 1:1 ratio of answer:format rewards is optimal, suggesting balance is critical.

### Mechanism 3
- **Claim**: Question-conditioned filtering extracts task-relevant knowledge from noisy retrieval results more effectively than passage-level reranking.
- **Mechanism**: After retrieving top-k articles and their sections, VLM-PRF reasons over combined results in ılar tags, then outputs a condensed knowledge representation in <answer> tags. This filtering is trained via RL to maximize answer accuracy, implicitly learning to identify which retrieved sentences contain answer-critical information.
- **Core assumption**: The model can learn to distinguish relevant from irrelevant information through RL reward signals without explicit relevance annotations.
- **Evidence anchors**:
  - [abstract]: "The filtering stage performs relevance filtering and concentration on retrieval results."
  - [section 4.2, Table 5]: With oracle retrieval (perfect article provided), VLM-PRF-7B achieves 65.8% accuracy vs. 57.6% for ReflectiVA, demonstrating superior knowledge extraction.
  - [corpus]: QKVQA paper proposes question-focused filtering as independently valuable, converging on similar intuition.
- **Break condition**: If retrieval brings completely irrelevant articles, even optimal filtering cannot recover. Table 7 shows performance degrades with larger knowledge bases (100k: 42.8% vs. 10k: 60.3%), indicating retrieval quality remains a bottleneck.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG) for multimodal inputs
  - **Why needed here**: The entire Wiki-PRF framework is a RAG system. Understanding how retrieval and generation interact—particularly how retrieved context conditions the generator—is essential.
  - **Quick check question**: Given an image of an unfamiliar plant and the question "What is its conservation status?", would a text-only RAG system succeed? Why or why not?

- **Concept**: Policy gradient methods (specifically GRPO/PPO-style optimization)
  - **Why needed here**: The training methodology uses GRPO to optimize tool selection and filtering. Understanding the objective function (Eq. 10-11) and how advantage estimates Ȃi,t guide policy updates is necessary to modify the reward structure or training dynamics.
  - **Quick check question**: If format rewards were removed, leaving only answer accuracy, would the model still learn to use the correct output tags? What failure mode might emerge?

- **Concept**: Tool-use in language models (function calling)
  - **Why needed here**: The processing stage requires the VLM to output structured tool invocations (<tool>1. Caption: ...</tool>) that are parsed and executed. Understanding the tool-calling interface and how to extend it (e.g., adding new tools) is necessary for system modifications.
  - **Quick check question**: If you wanted to add a "zoom" tool that magnifies a specific image region, what changes would be needed to the tool schema and execution pipeline?

## Architecture Onboarding

- **Component map**:
  ```
  [Input: Image I, Question Q]
           ↓
  [Processing Stage: VLM-PRF] → Tool selection in <tool> tags
           ↓
  [Tool Execution: VLM-base] → Captioning (C_query), Grounding (I_grounding), Flipping (I_flip)
           ↓
  [Retrieval Stage] → EVA-CLIP embeddings → Faiss cosine similarity search
           ↓           ↘
     [Image retrieval]  [Tool-query retrieval]
           ↓           ↙
        [Combined results: Top-1 article D + Top-k sections S_search]
           ↓
  [Filtering Stage: VLM-PRF] → Reasoning in ılar tags → Filtered knowledge F in <answer> tags
           ↓
  [Answer Generation: VLM] → Final answer A
  ```

- **Critical path**: Processing stage (tool invocation quality) → Retrieval stage (recall@k) → Filtering stage (precision of extracted knowledge). Failure at any stage propagates; retrieval errors cannot be fully recovered by filtering (see Table 7 degradation).

- **Design tradeoffs**:
  - **Training sample size vs. generalization**: Only 4K samples needed (Table 9) vs. 934K full dataset. RL's sample efficiency enables rapid iteration but requires careful reward design.
  - **Number of retrieved articles**: Top-5 is optimal (Table 10). More articles increase noise without proportional gains.
  - **Tool combinations**: RL increases diversity (Table 3 shows 53 combinations for VLM-PRF-3B vs. 34 for Qwen2.5-VL-3B), but inference time increases (Table 11: 6.23s total for 3B model).

- **Failure signatures**:
  - **Grounding errors**: If bounding box is wrong, cropped image retrieval fails. Check: Is the grounded object visually salient? Does the question specify a small region?
  - **Retrieval drift on large KB**: Performance drops 17-18 points from 10k to 100k KB (Table 7). Check: Is the correct entity in the top-k? If not, retrieval embeddings may be misaligned.
  - **Format reward overfitting**: Model produces well-structured outputs with empty or generic content. Check: Monitor answer reward vs. format reward during training (Fig. 5).

- **First 3 experiments**:
  1. **Reproduce retrieval recall baseline**: Run direct image retrieval (no tools) on InfoSeek validation subset (1K samples). Target: ~45% recall. If significantly lower, check EVA-CLIP embedding quality or Faiss index configuration.
  2. **Ablate single tools**: Run Wiki-PRF with only captioning, only grounding, and only flipping on 500 samples. Compare to Table 8 results. Expected: Captioning > Grounding > Flipping in contribution.
  3. **Validate RL training convergence**: Train VLM-PRF-3B on 1K samples for 200 steps. Plot answer reward and format reward (as in Fig. 5). Both should trend upward. If answer reward plateaus early, check reward sparsity or increase sampling temperature.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the integration of a broader set of visual tools (beyond captioning, grounding, and flipping) affect the framework's ability to handle diverse query types in KB-VQA?
- **Basis in paper**: [explicit] The conclusion states: "While limited to three retrieval tools in this study, future work may explore expanded tool integration to further advance capabilities."
- **Why unresolved**: The current implementation restricts the toolset to three specific functions (captioning, grounding, flipping), potentially limiting reasoning for tasks requiring other visual capabilities like OCR or depth estimation.
- **What evidence would resolve it**: Experiments integrating additional tools (e.g., text recognition, object segmentation) on the E-VQA and InfoSeek benchmarks to measure changes in accuracy and retrieval recall.

### Open Question 2
- **Question**: Can the retrieval and filtering mechanisms maintain high recall and precision when scaling the knowledge base to millions or billions of entries?
- **Basis in paper**: [inferred] Table 7 shows that as the knowledge base size increases from 10k to 100k, accuracy degrades (e.g., VLM-PRF-7B drops from 60.3 to 42.8). The paper notes that "larger knowledge bases introduce additional noise, increasing retrieval difficulty."
- **Why unresolved**: The experiments are capped at 100k entries, leaving the performance characteristics in larger, noisier real-world knowledge environments unknown.
- **What evidence would resolve it**: Evaluation of Wiki-PRF on a full-scale Wikipedia dump or a billion-scale web corpus to analyze the rate of performance degradation and filtering effectiveness.

### Open Question 3
- **Question**: Can the high inference latency caused by sequential tool invocation and long-text filtering be reduced to support real-time applications?
- **Basis in paper**: [inferred] Table 11 reports the total inference time for the 7B model is 8.77s per sample, with "Processing & Retrieval" and "Filtering" stages consuming the vast majority of this time.
- **Why unresolved**: The paper identifies the time cost but does not propose or test methods for optimization (e.g., parallel tool calling or token pruning), which is critical for deployment.
- **What evidence would resolve it**: A study measuring latency after applying optimization techniques like quantization, parallel tool execution, or earlier exit strategies for the filtering stage.

### Open Question 4
- **Question**: Is the "flipping" tool strictly necessary for the observed performance gains, given its low invocation frequency?
- **Basis in paper**: [inferred] Table 3 indicates the "Flipping" tool has a very low mean usage (0.22–0.26) compared to Captioning (2.43). However, Table 8 ablates "Multi-Tools" but does not isolate the performance contribution of the flipping tool alone.
- **Why unresolved**: It is unclear if the flipping tool provides a unique geometric invariance benefit or if its contribution is negligible compared to the captioning and grounding tools.
- **What evidence would resolve it**: An ablation study comparing the full model against a version with the flipping tool explicitly disabled.

## Limitations

- Performance degrades significantly (17-18 points) when scaling from 10k to 100k knowledge base size, indicating retrieval remains the bottleneck
- RL training assumes reward signals are sufficient for learning effective tool-use strategies without demonstrating generalization beyond training distribution
- Tool invocation mechanism relies on base VLM's ability to generate semantically meaningful captions and accurate bounding boxes, with no analysis of failure rates

## Confidence

- **High Confidence**: The three-stage architecture design and experimental results showing SOTA performance on E-VQA (36.0) and InfoSeek (42.8) are well-supported by the reported metrics and ablation studies.
- **Medium Confidence**: The effectiveness of RL training for learning implicit reasoning strategies is supported by the improvement from SFT (41.8%) to RL (46.3%) on InfoSeek, but the mechanism could benefit from more detailed analysis of learned tool-use patterns.
- **Low Confidence**: The scalability claims for knowledge base size are questionable given the sharp performance drop with larger KB, suggesting the method may not generalize well to production-scale knowledge bases.

## Next Checks

1. **Retrieval Robustness Test**: Systematically evaluate retrieval recall across KB scales (1k, 10k, 100k, 1M) on a fixed validation set to quantify the scaling degradation and identify at what point retrieval becomes the primary bottleneck.

2. **Tool Reliability Analysis**: Measure the accuracy of individual tools (captioning semantic correctness, grounding bounding box precision, flipping necessity) on a subset of questions to determine which components contribute most to failure cases.

3. **RL Generalization Probe**: Test the RL-trained model on out-of-distribution questions (e.g., different domains, question types, or image styles not present in training) to assess whether the learned tool-use strategies transfer beyond the training distribution.