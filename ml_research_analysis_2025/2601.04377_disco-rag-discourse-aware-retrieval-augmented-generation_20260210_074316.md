---
ver: rpa2
title: 'Disco-RAG: Discourse-Aware Retrieval-Augmented Generation'
arxiv_id: '2601.04377'
source_url: https://arxiv.org/abs/2601.04377
tags:
- chunk
- discourse
- disco-rag
- generation
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Disco-RAG introduces discourse-aware modeling into retrieval-augmented
  generation by parsing retrieved passages into intra-chunk RST trees and inter-chunk
  rhetorical graphs. These structures, combined with a discourse-driven planning module,
  enable the model to reason over evidence hierarchies and rhetorical connections,
  rather than concatenating flat text.
---

# Disco-RAG: Discourse-Aware Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2601.04377
- **Source URL:** https://arxiv.org/abs/2601.04377
- **Reference count:** 40
- **Primary result:** Improves RAG by modeling discourse structures, achieving SOTA on Loong, ASQA, and SciNews without fine-tuning

## Executive Summary
Disco-RAG introduces discourse-aware modeling into retrieval-augmented generation by parsing retrieved passages into intra-chunk RST trees and inter-chunk rhetorical graphs. These structures, combined with a discourse-driven planning module, enable the model to reason over evidence hierarchies and rhetorical connections, rather than concatenating flat text. Evaluated on Loong, ASQA, and SciNews benchmarks, Disco-RAG consistently outperforms standard RAG and prior state-of-the-art methods without fine-tuning. On Loong, it improves LLM Score by up to 10.0 points; on ASQA, it raises Exact Match by 3.1 points and DR Score by 9.2 points; on SciNews, it achieves new SOTA results across RL, BERTScore, SARI, and SummaC. Ablation and perturbation studies confirm the complementary contributions of discourse structures to coherence and factual consistency.

## Method Summary
Disco-RAG is an inference-time, training-free framework that integrates discourse structures into RAG. It uses Qwen3-Embedding-8B for retrieval (Top-10 chunks of 256 tokens), LLM-based parsing to create intra-chunk RST trees (offline), listwise inference to build inter-chunk rhetorical graphs (online), and discourse-aware planning to generate a blueprint for final generation. The generator (Llama-3.3-70B) conditions on query, chunks, RST trees, rhetorical graph, and plan to produce the final answer using beam search (width=3).

## Key Results
- Improves LLM Score by up to 10.0 points on Loong benchmark
- Raises ASQA Exact Match by 3.1 points and DR Score by 9.2 points
- Achieves SOTA on SciNews across RL, BERTScore, SARI, and SummaC metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly modeling intra-chunk rhetorical structure improves local coherence and reduces overgeneralization from conditional claims.
- Mechanism: RST tree parsing segments chunks into Elementary Discourse Units (EDUs), identifies nucleus (core) vs. satellite (support) roles, and assigns rhetorical relations (e.g., Condition, Contrast). This hierarchical structure helps the model distinguish qualifying conditions from main claims during synthesis.
- Core assumption: LLMs can leverage structured discourse annotations as "knowledge-level priors" when they are provided explicitly in the prompt, improving reasoning over flat text.
- Evidence anchors: [abstract] "constructs intra-chunk discourse trees to capture local hierarchies"; [section 3] "P(t_i | c_i; θ_A) = ∏ P(e_ij | c_i) · ∏ P(r_{u,v} | e_{iu}, e_{iv})"
- Break condition: If the RST parser produces noisy or incorrect tree structures, benefits may degrade; perturbation analysis (Figure 4a) shows performance drops when relations or nucleus-satellite roles are shuffled.

### Mechanism 2
- Claim: Inter-chunk rhetorical graphs enable reasoning across conflicting or complementary evidence from multiple retrieved passages.
- Mechanism: A directed graph is constructed over all retrieved chunks, with edges labeled with discourse relations (e.g., SUPPORTS, CONTRADICTS, ELABORATES) or UNRELATED. This provides a "global discourse scaffold" that reveals cross-passage coherence and logical flow.
- Core assumption: Joint (listwise) inference over all chunks captures inter-chunk relations more effectively than pairwise processing; LLMs can interpret these graphs to guide synthesis.
- Evidence anchors: [abstract] "builds inter-chunk rhetorical graphs to model cross-passage coherence"; [section 5] Ablation shows removing rhetorical graph drops LLM Score from 62.07 to 57.10 (Table 4).
- Break condition: If the number of retrieved chunks is very large (>50), listwise inference may become noisy or expensive; Figure 3b shows some degradation at Top-50, though Disco-RAG remains more robust than standard RAG.

### Mechanism 3
- Claim: Discourse-aware planning provides a reasoning-level blueprint that orchestrates argumentative flow before generation.
- Mechanism: A planner LLM takes the query, chunks, RST trees, and rhetorical graph as input and outputs a natural-language plan describing how to structure the answer (e.g., select salient content, resolve conflicts, order evidence). This plan conditions the final generator.
- Core assumption: Planning separates content selection/organization from surface realization, reducing the burden on the generator and improving coherence.
- Evidence anchors: [abstract] "jointly integrated into a planning blueprint that conditions the generation"; [section 5] Ablation shows removing planning drops LLM Score from 62.07 to 59.75; perturbation (Figure 4c) shows shuffling or removing plan steps hurts performance.
- Break condition: If the plan is generic or poorly aligned with the query, benefits diminish; the paper includes generic planning baselines (retrieve-and-plan, plan-and-retrieve) that underperform Disco-RAG, suggesting discourse structures are critical for effective planning.

## Foundational Learning

- **Concept:** Rhetorical Structure Theory (RST)
  - Why needed here: Disco-RAG's core representation; you must understand EDUs, nucleus-satellite relations, and common relation types (e.g., Elaboration, Contrast, Condition) to interpret the trees/graphs.
  - Quick check question: In the vitamin D example (Figure 1), why does identifying "12% lower incidence" as a satellite with a Condition relation help prevent overgeneralization?

- **Concept:** Retrieval-Augmented Generation (RAG) pipelines
  - Why needed here: Disco-RAG modifies the standard RAG pipeline by injecting discourse structures after retrieval; you should know the baseline (retrieve → concatenate → generate) and its limitations.
  - Quick check question: What are the two structural limitations of standard RAG identified in the introduction (intra-chunk blindness, inter-chunk coherence gaps)?

- **Concept:** LLM-based discourse parsing
  - Why needed here: Disco-RAG uses an LLM parser for RST tree and rhetorical graph construction; understand the tradeoffs between zero-shot parsers and fine-tuned parsers.
  - Quick check question: According to Appendix D (Table 7), how does the zero-shot parser compare to a supervised parser on span F1 and nuclearity F1?

## Architecture Onboarding

- **Component map:** Retriever -> RST Parser (offline) -> Rhetorical Graph Builder -> Discourse-aware Planner -> Generator
- **Critical path:**
  1. Offline: Pre-parse RST trees for corpus chunks (once, reusable)
  2. At inference: Retrieve Top-k chunks → (parallel) fetch pre-computed RST trees
  3. Construct inter-chunk rhetorical graph (listwise LLM call)
  4. Generate discourse-aware plan (LLM call)
  5. Generate final answer conditioned on all structures
- **Design tradeoffs:**
  - Token cost/latency vs. quality: Disco-RAG uses ~2.2× tokens and adds ~2s latency vs. standard RAG (Table 8)
  - Parser accuracy vs. cost: Zero-shot parser is cheaper but less accurate than supervised; can plug in better parsers if available
  - Retrieval granularity: Disco-RAG is robust to chunk size variations (Figure 3a), but standard RAG is not; chunk size 256 is recommended default
- **Failure signatures:**
  - Incorrect plan: If the plan misinterprets the query or evidence, the answer may be off-topic or incomplete
  - Noisy rhetorical graph: If inter-chunk relations are mispredicted (e.g., missing CONTRADICTS edge), conflicting evidence may be merged incorrectly
  - Parser errors: If RST trees misassign nucleus/satellite roles, the model may overemphasize supporting details and underemphasize core claims
- **First 3 experiments:**
  1. Replicate the Loong benchmark comparison (Table 1) with Llama-3.3-70B, standard RAG vs. Disco-RAG, to verify gains and familiarize with the pipeline
  2. Run ablation on a small subset (Table 4) to measure the impact of removing each component (RST tree, rhetorical graph, planner) and understand their relative contributions
  3. Stress-test with retrieval noise (Figure 3c): inject 20–40% irrelevant chunks and compare Disco-RAG vs. standard RAG to evaluate robustness to retrieval errors

## Open Questions the Paper Calls Out
- **Cross-lingual and cross-formalism generalization:** How does Disco-RAG perform across different languages and with alternative discourse formalisms beyond Rhetorical Structure Theory? The paper notes this is a direction for future work, having only tested English datasets with RST.
- **Computational efficiency:** Can the computational overhead of discourse parsing and planning be reduced while maintaining performance gains? The paper proposes caching, batching, and distillation as future optimizations but doesn't implement them.
- **Interactive dialogue systems:** How does Disco-RAG generalize to interactive dialogue systems and conversational RAG settings? The paper explicitly identifies this gap, having only evaluated single-turn QA and summarization.

## Limitations
- Performance gains come with 2× token cost and ~2s latency overhead, limiting real-time applicability
- Zero-shot parser introduces uncertainty about generalization across domains and potential hallucination risks
- Inter-chunk rhetorical graph construction with listwise inference could become computationally expensive or noisy with larger retrieval sets (>50 chunks)

## Confidence
- **High confidence:** Performance improvements over standard RAG and prior methods on all three benchmarks (Loong, ASQA, SciNews). The consistent SOTA results across multiple metrics are well-supported by ablation and perturbation studies.
- **Medium confidence:** The claim that discourse structures specifically improve reasoning over evidence hierarchies rather than just adding more context. While ablation shows component importance, the exact mechanism remains partially explained.
- **Medium confidence:** The zero-shot parser's effectiveness compared to supervised alternatives. The paper mentions performance comparisons but doesn't fully explore the tradeoff between parser accuracy and cost across different domains.

## Next Checks
1. **Parser robustness validation:** Run the zero-shot RST parser on a held-out test set with gold-standard discourse annotations to quantify hallucination rates and structural accuracy across different document types, particularly for scientific texts in the SciNews benchmark.
2. **Scaling behavior analysis:** Systematically test Disco-RAG with varying retrieval set sizes (Top-5, Top-10, Top-20, Top-50) to identify the exact point where listwise inter-chunk graph construction becomes computationally prohibitive or degrades in quality, and evaluate whether pairwise approximations could maintain performance with better efficiency.
3. **Cross-domain generalization test:** Apply Disco-RAG to a domain not represented in the training corpus (e.g., legal documents, medical literature, or news articles) to assess whether the discourse-aware framework's benefits transfer beyond the evaluated benchmarks, particularly focusing on whether the zero-shot parser can handle domain-specific rhetorical structures.