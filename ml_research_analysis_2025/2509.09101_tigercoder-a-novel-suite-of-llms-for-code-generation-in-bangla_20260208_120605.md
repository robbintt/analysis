---
ver: rpa2
title: 'TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla'
arxiv_id: '2509.09101'
source_url: https://arxiv.org/abs/2509.09101
tags:
- bangla
- code
- generation
- english
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TigerCoder introduces the first Bangla-specific code-generation
  LLMs, addressing the severe underrepresentation of Bangla in code-generation benchmarks.
  The authors curated three large instruction-code datasets (totaling 300K examples)
  and introduced MBPP-Bangla, a 974-problem benchmark across five programming languages.
---

# TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla

## Quick Facts
- arXiv ID: 2509.09101
- Source URL: https://arxiv.org/abs/2509.09101
- Authors: Nishat Raihan; Antonios Anastasopoulos; Marcos Zampieri
- Reference count: 18
- First Bangla-specific code-generation LLMs outperform larger multilingual models through curated instruction tuning

## Executive Summary
TigerCoder introduces the first Bangla-specific code-generation LLMs, addressing the severe underrepresentation of Bangla in code-generation benchmarks. The authors curated three large instruction-code datasets (totaling 300K examples) and introduced MBPP-Bangla, a 974-problem benchmark across five programming languages. Fine-tuning smaller Bangla models (1B/9B) on these datasets yields Pass@1 improvements of 11-18% over larger multilingual models. Notably, the 1B model surpasses 27× larger proprietary models, demonstrating that high-quality, task-specific data can compensate for scale limitations in low-resource languages. Machine translation of prompts to English fails to improve results, reinforcing the need for native-language training data. TigerCoder sets a new standard for Bangla code generation and establishes a replicable blueprint for efficient LLM development in underrepresented languages.

## Method Summary
The authors fine-tuned TigerLLM (1B/9B) on a curated dataset of 300K Bangla instruction-code pairs using AdamW optimizer, cosine learning rate scheduler, and BF16 precision. The data was generated through three pipelines: Self-Instruct (100K pairs from 5K expert seeds via GPT-4o), Synthetic (100K diverse pairs via GPT-4o/Claude-3.5), and Translated (100K pairs from Evol-Instruct via NLLB-200). All code snippets underwent AST parsing and execution validation (10s timeout, 16GB memory) before inclusion. The models were evaluated on MBPP-Bangla (974 problems) and mHumanEval-Bangla using Pass@K metrics.

## Key Results
- TigerCoder-1B achieves Pass@1 of 0.69 on MBPP-Bangla, surpassing GPT-4o-mini and Gemma-3-4B
- TigerCoder-9B achieves Pass@1 of 0.75 on MBPP-Bangla, outperforming models 27× larger
- Heterogeneous dataset composition (Self-Instruct + Synthetic + Translated) yields best results with clear synergistic effects

## Why This Works (Mechanism)

### Mechanism 1
Targeted, high-quality instruction tuning appears to offset model scale limitations for low-resource code generation. By fine-tuning small parameter models (1B/9B) on a curated dataset of 300K Bangla instruction-code pairs, the model overcomes the "sparse data allocation" issue common in large multilingual models. The specific input-output mapping (Bangla prompt → Code) is learned explicitly rather than hoped for as a side-effect of massive English pre-training.

### Mechanism 2
Machine translation (MT) of prompts fails as a bridge for low-resource code generation due to semantic drift in technical keywords. Translating code prompts via standard MT models (like NLLB) often mistranslates specific programming keywords or logical constraints into generic natural language, obscuring the task requirements.

### Mechanism 3
Heterogeneous dataset composition (Synthetic + Translated + Self-Instruct) creates a synergy that single-source data cannot match. Combining "Self-Instruct" (human-seeded), "Synthetic" (diverse LLM-generated), and "Translated" (high-resource logic transfer) datasets covers different failure modes: naturalness, diversity, and logical complexity.

## Foundational Learning

- **Concept: Instruction Tuning (Self-Instruct)**
  - Why needed here: The paper relies on generating 100K examples from only 5,000 human seeds. Understanding this loop is critical to reproducing the data pipeline.
  - Quick check question: How does the paper ensure that the LLM generating the instruction pairs doesn't simply repeat the same 5,000 prompts in different words? (Answer: Filtering via Cosine Similarity < 0.95).

- **Concept: Pass@K Metric**
  - Why needed here: Evaluation relies on the probability of success within K samples.
  - Quick check question: Why does the paper report Pass@1, Pass@10, and Pass@100? (Answer: To measure single-shot utility vs. upper-bound potential).

- **Concept: Low-Resource Language Constraints**
  - Why needed here: The paper argues scale alone doesn't work for Bangla due to data scarcity.
  - Quick check question: Why does a 27B parameter model sometimes fail where a 1B model succeeds in this specific context? (Answer: The 1B model is fine-tuned on specific Bangla-Code distributions, while the 27B model suffers from sparse Bangla token allocation during pre-training).

## Architecture Onboarding

- **Component map:**
  - TigerLLM (1B/9B) base models → Three datasets (Bangla-Code-Instruct-SI, -Syn, -TE) → AdamW fine-tuning → MBPP-Bangla/mHumanEval-Bangla evaluation

- **Critical path:**
  1. Seed Generation: 5,000 expert prompts
  2. Evolution: Use GPT-4o to expand to 300K pairs
  3. Validation: Execute all code snippets; discard failures
  4. Fine-Tuning: AdamW optimizer, Cosine LR, BF16 precision (96 hours on A100)
  5. Eval: Compare Pass@1 against GPT-4o-mini and Gemma

- **Design tradeoffs:**
  - Syn vs. TE: Synthetic data (Syn) offers diversity but risks hallucinated Bangla; Translated data (TE) offers logic correctness but risks translation artifacts. The paper chooses to use both to mitigate individual weaknesses.
  - Model Size: The 1B model is cheaper to serve but relies heavily on the quality of the fine-tuning data; the 9B model generalizes better but requires more compute.

- **Failure signatures:**
  - MT-Drift: Prompt says "non-empty string", MT outputs "string without content", code fails edge case
  - Syntax vs. Logic: Code passes AST parse (syntax valid) but times out or fails hidden test cases (logic error)

- **First 3 experiments:**
  1. Baseline Sanity Check: Run the base TigerLLM (without TigerCoder weights) on MBPP-Bangla to quantify the exact gain from the instruction tuning.
  2. Translation Ablation: Evaluate TigerCoder on the "Bangla → English-MT" variant of the benchmark to verify if the model has learned to handle "translation-ese" or strictly native Bangla.
  3. Data Ablation: Train three smaller models—one on SI only, one on Syn only, one on TE only—to isolate which dataset contributes most to Pass@1 scores.

## Open Questions the Paper Calls Out

- Does the high-quality, curated data advantage persist for architectures significantly larger than 9B parameters?
- Can the TigerCoder methodology be generalized to complex, repository-level software engineering tasks?
- Can terminology-aware or constrained machine translation methods mitigate the keyword distortion issues identified in simple translation pipelines?

## Limitations

- Data dependency risk: Results depend critically on quality of curated datasets generated using GPT-4o and Claude-3.5
- Generalization beyond Bangla: Scalability to other low-resource languages remains untested
- Evaluation scope: MBPP-Bangla covers only 974 problems across five programming languages

## Confidence

**High Confidence**
- Targeted, high-quality instruction tuning can offset model scale limitations for low-resource code generation
- Machine translation fails as a bridge for low-resource code generation due to semantic drift in technical keywords
- Heterogeneous dataset composition creates synergistic effects

**Medium Confidence**
- The 1B model surpasses 27× larger proprietary models
- High-quality, task-specific data can compensate for scale limitations across different low-resource languages
- Specific numerical improvements (11-18% over larger multilingual models)

**Low Confidence**
- Scalability of this approach to other low-resource languages beyond Bangla
- Long-term robustness of models trained on synthetic data without continuous validation
- Practical deployment implications without testing on real-world, open-ended code generation tasks

## Next Checks

1. **Base Model Ablation**: Evaluate the base TigerLLM (without TigerCoder fine-tuning) on MBPP-Bangla to establish the exact performance gain attributable to instruction tuning, controlling for any baseline capabilities.

2. **Cross-Lingual Transfer**: Test whether models trained on Bangla-Code pairs can successfully handle code generation tasks when prompts are provided in English or other high-resource languages, measuring cross-lingual transfer capability.

3. **Long-Term Robustness**: Conduct a longitudinal study evaluating TigerCoder's performance on a held-out test set after 3, 6, and 12 months to assess whether synthetic data hallucinations or concept drift affect model reliability over time.