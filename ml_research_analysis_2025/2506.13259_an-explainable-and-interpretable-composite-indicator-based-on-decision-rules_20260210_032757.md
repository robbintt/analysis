---
ver: rpa2
title: An Explainable and Interpretable Composite Indicator Based on Decision Rules
arxiv_id: '2506.13259'
source_url: https://arxiv.org/abs/2506.13259
tags:
- rules
- class
- units
- maximal
- assigned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for constructing explainable and interpretable
  composite indicators using "if..., then..." decision rules. The approach addresses
  the need for transparency in composite indicators by generating decision rules that
  explain the relationships between elementary indicators and the composite score.
---

# An Explainable and Interpretable Composite Indicator Based on Decision Rules

## Quick Facts
- arXiv ID: 2506.13259
- Source URL: https://arxiv.org/abs/2506.13259
- Authors: Salvatore Corrente; Salvatore Greco; Roman Słowiński; Silvano Zappalà
- Reference count: 40
- This paper proposes a method for constructing explainable composite indicators using "if..., then..." decision rules based on the Dominance-based Rough Set Approach.

## Executive Summary
This paper introduces a novel method for constructing composite indicators that are both explainable and interpretable through decision rules. The approach uses the Dominance-based Rough Set Approach (DRSA) to induce human-readable rules from classified reference units, avoiding traditional weighted aggregation methods. The resulting rules relate class assignments to threshold conditions on elementary indicators, providing transparency in how composite scores are determined. The method is illustrated through four scenarios including explaining existing indicators, constructing new ones from preferences, and explaining MCDA scoring methods.

## Method Summary
The method applies DRSA to induce at-least and at-most decision rules from classified reference units, where classifications can be exact or interval-based. Rules are generated using threshold conditions on criteria values that consistently correspond to class assignments. The approach ensures non-contradictory classification through a three-stage MILP process that selects minimal covering rule sets while preserving as many original classifications as possible. The method can handle new units by applying the rule set and resolving any contradictions through optimization.

## Key Results
- DRSA generates interpretable "if..., then..." rules explaining composite indicator relationships without requiring weights
- Combining at-least and at-most rules through intersection provides non-contradictory classifications
- MILP optimization selects minimal, non-contradictory rule sets and resolves classification conflicts
- The approach successfully explains existing composite indicators like HDI and provides transparent decision support

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRSA induces human-readable decision rules that explain composite indicator outputs without requiring explicit weights or aggregation functions.
- Mechanism: The Dominance-based Rough Set Approach identifies patterns in classified reference units where specific threshold conditions on criteria values consistently correspond to class assignments. It generates two rule types: "at-least" rules (if conditions met, then unit is at least class t) and "at-most" rules (if conditions met, then unit is at most class t). These rules are expressed as conditional statements with interpretable thresholds drawn directly from observed data.
- Core assumption: The dominance principle holds—units with better or equal evaluations on all criteria should not be assigned to worse classes than units they dominate.
- Evidence anchors:
  - [abstract]: "To induce the rules from scored or classified units, we apply the Dominance-based Rough Set Approach. The resulting decision rules relate the class assignment or unit's score to threshold conditions on values of selected indicators in an intelligible way."
  - [section 3, p.11]: "DRSA yields at-least and at-most decision rules explaining scores or classifications provided by the DM on a set of reference units."
  - [corpus]: Related work on interpretable indicators (e.g., "Transforming User Defined Criteria into Explainable Indicators") addresses explainability but typically requires explicit weight specification, unlike the rule-based approach here.
- Break condition: If reference classifications violate the dominance principle (e.g., a unit dominating another is assigned a worse class), DRSA cannot generate consistent rules. The paper requires revision of such classifications with the decision maker.

### Mechanism 2
- Claim: Combining at-least and at-most rules through intersection provides non-contradictory classifications for new units.
- Mechanism: For any unit a, the minimum class assignment s⁻(a) is computed as the maximum class recommended by any matching at-least rule. The maximum class assignment s⁺(a) is computed as the minimum class recommended by any matching at-most rule. A non-contradictory classification requires s⁻(a) ≤ s⁺(a), yielding an interval [s⁻(a), s⁺(a)] or exact class when equal.
- Core assumption: The rule set derived from reference units generalizes to classify new units consistently.
- Evidence anchors:
  - [section 3, p.12]: Definition 3.1 formalizes non-contradictory classification as s⁻(a) ≤ s⁺(a) for all units.
  - [section 2.3, p.9]: Example shows A25 classified as [1,2] (at least class 1, at most class 2) and A35 as contradictory ([3,1]) when rules conflict.
  - [corpus]: Weak direct evidence; neighboring papers on decision support focus on optimization rather than logical consistency of rule-based classification.
- Break condition: If no at-least rules match a unit, s⁻(a) defaults to 1 (worst class). If no at-most rules match, s⁺(a) defaults to p (best class). Contradiction arises when s⁻(a) > s⁺(a), requiring rule pruning or reclassification.

### Mechanism 3
- Claim: Mixed-Integer Linear Programming (MILP) selects minimal, non-contradictory rule sets and resolves classification conflicts for new units.
- Mechanism: A three-stage MILP process: (1) Problem (5) identifies which existing classifications must be modified to accommodate new units without contradiction; (2) Problem (6) maximizes the number of retained rules while respecting constraints; (3) Problem (7) finds the minimum rule subset that covers all units. Binary variables indicate rule selection; constraints ensure every unit is "covered" by at least one matching rule for its assigned class.
- Core assumption: A parsimonious rule set (fewer rules) is preferable for interpretability while maintaining full coverage of classification decisions.
- Evidence anchors:
  - [section 4, p.16]: MILP problem (4) minimizes rule count subject to coverage constraints for all classified units.
  - [section 5, p.19-22]: Problems (5)-(7) handle new unit classification, contradiction detection, and minimal rule selection.
  - [corpus]: "WANDER" framework uses optimization for decision support but does not employ MILP for rule selection; the DRSA-MILP combination appears novel in this domain.
- Break condition: If η* > 0 in Problem (5), some original classifications must change to accommodate new units without contradiction. The optimization minimizes such changes but cannot guarantee preservation of all original assignments.

## Foundational Learning

- Concept: Dominance-based Rough Set Approach (DRSA)
  - Why needed here: DRSA is the core induction engine that generates decision rules from classified examples. Unlike classical rough sets, DRSA handles ordered decision classes and monotonic relationships between criteria and outcomes.
  - Quick check question: Given units A (score=3, criterion X=10) and B (score=2, criterion X=8), does the dominance principle allow A to dominate B? (Answer: Only if A has X≥8 and all other criteria are at least as good; the score alone doesn't determine dominance.)

- Concept: Composite indicators and aggregation
  - Why needed here: The paper positions itself against traditional weighted-sum aggregation. Understanding compensation (trade-offs between criteria) versus non-compensatory approaches clarifies why decision rules avoid weighting controversies.
  - Quick check question: In a weighted-sum composite indicator with weights [0.5, 0.5] on two criteria, if a unit improves by 2 points on criterion 1 but declines by 1 point on criterion 2, what is the net effect? (Answer: +0.5 points—this is compensatory aggregation, which rules avoid.)

- Concept: Rule support and confirmation measures
  - Why needed here: Rules are prioritized by relative support (fraction of relevant units matching the rule) and Bayesian confirmation measures S and N. These determine rule quality and ordering for the MILP selection process.
  - Quick check question: An at-least rule for class 2 matches 10 units, and 15 units are in classes ≥2. What is its relative support? (Answer: 10/15 ≈ 0.667)

## Architecture Onboarding

- Component map: Input Layer -> DRSA Induction Engine -> Rule Ranking Module -> Consistency Filter -> MILP Optimizer -> Classification Output

- Critical path:
  1. Collect reference unit classifications from domain expert
  2. Verify dominance principle compliance (revise if violated)
  3. Run DRSA to generate R≥ and R≤ rule sets
  4. Apply Algorithm 1 to obtain minimal non-contradictory rules
  5. For new units: apply rules per equation (3), detect contradictions
  6. If contradictions exist, solve MILP sequence (5)→(6)→(7)

- Design tradeoffs:
  - **Parsimony vs. coverage**: MILP minimizes rule count; adding constraints for minimum support may increase interpretability but reduce coverage
  - **Stability vs. adaptability**: New units may force reclassification of existing units (η* > 0); the system favors stability by minimizing modifications
  - **Precision vs. ambiguity**: Non-exact classifications (intervals like [2,3]) are honest about uncertainty but less actionable

- Failure signatures:
  - **Contradictory classifications** (s⁻ > s⁺): Indicates rule set conflict or out-of-distribution new unit
  - **Empty rule matches**: New unit unlike any reference; defaults to [1, p] (completely uncertain)
  - **Reference reclassification**: Algorithm modifies original DM classifications (η > 0); signals overfitting or insufficient reference diversity

- First 3 experiments:
  1. **Replicate GCS example** (Section 2.1): Implement DRSA on the 21-patient table with 3 criteria. Verify that induced rules match the paper's minimal set (3 at-least, 4 at-most). Classify patient a22 and confirm "Moderate" assignment.
  2. **Test contradiction handling** (Section 2.3): Use the stock portfolio data with reference units from Table 4a. Intentionally add a new unit that triggers s⁻ > s⁺. Implement MILP problem (5) to observe which reference classification changes (should be A35 → [1,2]).
  3. **Sensitivity to reference set size**: On the HDI dataset (193 countries), randomly sample 10%, 25%, 50% as reference units. Measure: (a) number of rules induced, (b) classification accuracy on remaining units, (c) rate of contradictions. Expect rules to stabilize as reference set grows.

## Open Questions the Paper Calls Out

- **Question:** How does the performance and stakeholder acceptance of the decision-rule-based composite indicator compare to traditional aggregation methods in diverse, real-world applications?
  - **Basis in paper:** [explicit] The conclusion explicitly states that "further development is needed, particularly through real-world applications to validate the advantages of this approach, and through adaptation to the specific needs of different domains."
  - **Why unresolved:** The paper relies on illustrative case studies (e.g., HDI, medical scales, stock portfolios) to demonstrate feasibility but does not test the method in live, complex environments where stakeholder trust and interpretability are critical factors.
  - **What evidence would resolve it:** Field studies or controlled experiments applying the method to active policy-making or investment scenarios, measuring user comprehension and decision quality against standard weighted additive models.

- **Question:** How does the computational complexity of the Mixed Integer Linear Programming (MILP) formulation scale with an increasing number of units and criteria?
  - **Basis in paper:** [inferred] The methodology requires solving MILP problems (equations 4, 5, 6, 7) to select minimal rule sets. While applied to a set of 193 countries and 50 stocks, the paper does not discuss the algorithmic time complexity or limits regarding dataset size.
  - **Why unresolved:** MILP problems are often NP-hard. Without complexity analysis or heuristic approximations, it is unclear if the method is viable for datasets involving thousands of units or high-dimensional criteria spaces.
  - **What evidence would resolve it:** Computational benchmarks showing execution time growth relative to the number of units (N) and criteria (M), or the proposal of a heuristic algorithm to handle large-scale instances efficiently.

- **Question:** How robust is the rule induction process to noise or small inconsistencies in the Decision Maker's (DM) reference classifications?
  - **Basis in paper:** [inferred] The method assumes the DM's classification of reference units respects the dominance principle. If violated, the paper suggests the assignments "should be revised together with the DM," implying a sensitivity to input errors that is not quantitatively analyzed.
  - **Why unresolved:** In real-world elicitation, DMs often provide inconsistent or noisy examples. The paper does not demonstrate how much noise the DRSA induction can tolerate before the resulting rules become unstable or contradictory.
  - **What evidence would resolve it:** A sensitivity analysis measuring the stability of the generated rule sets (e.g., Jaccard similarity) when random classification errors are injected into the reference unit data.

## Limitations
- DRSA Implementation Dependency: Key algorithmic choices for threshold selection are not fully specified, creating potential reproducibility gaps.
- Reference Classification Precision: Impact of classification imprecision on rule quality and consistency is not quantified.
- Scalability of MILP: Computational tractability for large-scale composite indicators is untested.

## Confidence
- **Explainability via Rules**: High - The mechanism of generating "if..., then..." statements from threshold conditions is well-defined and demonstrated.
- **Non-contradictory Classification**: Medium - The theoretical framework is sound, but practical handling of contradictions relies on MILP, which is sensitive to solver parameters.
- **Interpretability Over Aggregation**: High - The paper convincingly argues that decision rules avoid the opacity of weighted sums, though this is more of a methodological assertion than an empirical claim.

## Next Checks
1. **DRSA Algorithm Verification**: Reproduce the GCS example (Section 2.1) and confirm that the induced rules match the paper's minimal set (3 at-least, 4 at-most). Validate the classification of patient a22 as "Moderate."

2. **Contradiction Resolution Testing**: Use the stock portfolio data (Table 4a) to test MILP problems (5)-(7). Intentionally trigger a contradiction (s⁻ > s⁺) and verify that the solver correctly identifies which reference classification must change (A35 → [1,2]).

3. **Sensitivity to Reference Set Size**: On the HDI dataset (193 countries), sample varying proportions (10%, 25%, 50%) as reference units. Measure rule set stability, classification accuracy, and contradiction rates to assess robustness.