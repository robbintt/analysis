---
ver: rpa2
title: Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage
  Identification from Pathology Reports
arxiv_id: '2511.01052'
source_url: https://arxiv.org/abs/2511.01052
tags:
- reports
- kewltm
- cancer
- kewrag
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of extracting cancer staging
  information from unstructured pathology reports by proposing two knowledge elicitation
  methods for large language models (LLMs). KEwLTM induces interpretable staging rules
  from a small set of unannotated reports through an iterative, label-free process,
  while KEwRAG retrieves guidelines, synthesizes explicit rules in a single pass,
  and applies them for staging classification.
---

# Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports

## Quick Facts
- arXiv ID: 2511.01052
- Source URL: https://arxiv.org/abs/2511.01052
- Reference count: 29
- Primary result: Knowledge elicitation methods (KEwLTM, KEwRAG) extract interpretable staging rules from pathology reports without large labeled datasets.

## Executive Summary
This study addresses the challenge of extracting cancer staging information from unstructured pathology reports by proposing two knowledge elicitation methods for large language models (LLMs). KEwLTM induces interpretable staging rules from a small set of unannotated reports through an iterative, label-free process, while KEwRAG retrieves guidelines, synthesizes explicit rules in a single pass, and applies them for staging classification. Experiments on breast cancer pathology reports from TCGA show that KEwLTM achieves macro-averaged F1 scores of 0.822 (T) and 0.847 (N), outperforming KEwRAG when the base LLM performs well in zero-shot chain-of-thought reasoning. KEwRAG achieves macro-averaged F1 scores of 0.746 (T) and 0.810 (N), excelling when zero-shot reasoning is less effective. Both methods offer transparent, interpretable rule sets, reducing reliance on large labeled datasets and enhancing interpretability for clinical use. Error analysis highlights persistent numerical reasoning challenges, suggesting future integration of external tools for improved accuracy.

## Method Summary
The study proposes two knowledge elicitation methods for interpretable cancer staging from pathology reports. KEwLTM uses an iterative, label-free process where an LLM extracts staging rules from unannotated reports and updates a persistent memory, accepting changes only if they closely match existing memory (measured by edit distance). KEwRAG retrieves relevant guideline chunks from AJCC manuals, synthesizes explicit rules in a single pass, and applies these static rules for classification. Both methods avoid the need for large labeled datasets and produce interpretable rule sets. The methods are evaluated on TCGA breast cancer pathology reports, comparing macro-averaged F1 scores for T and N staging against zero-shot chain-of-thought reasoning baselines.

## Key Results
- KEwLTM achieves macro-averaged F1 scores of 0.822 (T) and 0.847 (N) on TCGA breast cancer reports
- KEwRAG achieves macro-averaged F1 scores of 0.746 (T) and 0.810 (N)
- KEwLTM outperforms KEwRAG when base LLM performs well in zero-shot chain-of-thought reasoning
- Both methods produce interpretable rule sets that reduce reliance on large labeled datasets

## Why This Works (Mechanism)

### Mechanism 1: Label-Free Iterative Rule Induction (KEwLTM)
- **Claim:** LLMs can derive specialized staging rules from unannotated pathology reports if guided through an iterative memory-update loop, but this effectiveness is contingent on the model's inherent reasoning strength.
- **Mechanism:** The system prompts the LLM to extract rules (m) from a report and update a persistent "Long-Term Memory" (M). An edit distance threshold (Levenshtein) acts as a filter, accepting updates only if they closely match existing memory, preventing erratic drift.
- **Core assumption:** The base LLM possesses sufficient pre-trained medical knowledge to generate valid initial rules via Zero-Shot Chain-of-Thought (ZSCOT).
- **Evidence anchors:**
  - [Abstract] "KEwLTM induces interpretable staging rules from a small set of unannotated reports through an iterative, label-free process."
  - [Section 3.2] "Updates to M are accepted only if the newly generated rules (m) closely match the existing long-term memory... measured using edit distance."
  - [Corpus] Neighbor papers (e.g., *Cancer Type, Stage and Prognosis Assessment*) confirm LLMs can process unstructured pathology text, validating the input capability, though the specific iterative memory mechanism is unique to this paper.
- **Break condition:** If the base model's ZSCOT performance is poor, the induced rules will likely be noisy or hallucinated, causing KEwLTM to underperform compared to retrieval-based methods.

### Mechanism 2: Static Rule Synthesis via RAG (KEwRAG)
- **Claim:** Synthesizing a static set of rules from guidelines once (pre-inference) improves efficiency and auditability compared to per-query retrieval, provided the retrieval step successfully captures the correct definitions.
- **Mechanism:** Unlike standard RAG which retrieves chunks per query, KEwRAG retrieves relevant guideline chunks *once*, prompts the LLM to condense them into a structured rule set (K), and uses this static set for all subsequent classifications.
- **Core assumption:** The external guideline text (e.g., AJCC manual) covers the necessary logic for the target reports, and the LLM can correctly parse/summarize it without hallucinating constraints.
- **Evidence anchors:**
  - [Abstract] "KEwRAG retrieves guidelines, synthesizes explicit rules in a single pass, and applies them for staging classification."
  - [Section 3.3] "This set K is then used as a stable knowledge context for all subsequent inferences... avoiding repeated retrieval overhead."
  - [Corpus] *Enhancing Pancreatic Cancer Staging with LLMs* supports the general efficacy of RAG in oncology staging, serving as a conceptual anchor for using external guidelines.
- **Break condition:** If the guidelines differ significantly from the reporting style in the pathology notes (e.g., institutional shorthand not in the manual), the rigid rule set may fail to generalize.

### Mechanism 3: Numerical Reasoning Limitation
- **Claim:** LLM performance in cancer staging is fundamentally bounded by "numerical incompetence," where valid logic fails due to inability to compare numbers (e.g., thresholds).
- **Mechanism:** The LLM correctly identifies tumor size (e.g., "5.2 cm") and the rule ("T3 if >5cm") but fails the internal arithmetic comparison ("5.2 is not > 5").
- **Core assumption:** This is presented as a persistent failure mode rather than a mechanism of success, but it explains the performance ceiling.
- **Evidence anchors:**
  - [Section 4.2] "Numerical Incompetence (NI) was a prevalent issue... misapplying a simple numeric comparison."
  - [Section 5.3.2] "LLM's current difficulty in handling numerical tasks... occasionally undermines performance."
  - [Corpus] (No direct corpus evidence found for this specific failure mode in neighbors; this is an internal finding of the paper).
- **Break condition:** Any attempt to deploy this without external tool integration (e.g., a Python function for comparisons) will likely see persistent errors at category boundaries.

## Foundational Learning

- **Concept: Zero-Shot Chain-of-Thought (ZSCOT)**
  - **Why needed here:** ZSCOT is the baseline capability that KEwLTM attempts to "bake in" to memory. If a model cannot do ZSCOT, KEwLTM fails.
  - **Quick check question:** Can the model accurately answer "What is 12 * 12?" if asked to show its steps, without examples?

- **Concept: Edit Distance (Levenshtein)**
  - **Why needed here:** This is the stability mechanism for KEwLTM. You must understand how string similarity prevents the memory from changing drastically with every new report.
  - **Quick check question:** Would "T1: Tumor < 2cm" have a low or high edit distance compared to "T1: Tumor is less than 2cm"?

- **Concept: AJCC TNM Staging**
  - **Why needed here:** The domain context. You cannot debug the "Incorrect Knowledge" errors without understanding that T=size/tumor, N=nodes, M=metastasis.
  - **Quick check question:** If a report says "Tumor 4cm, 2 positive nodes," is the T-stage or N-stage determined by the node count?

## Architecture Onboarding

- **Component map:**
  - Inference Engine: Open-source LLM (e.g., Mixtral-8x7B, Med42-70B)
  - Knowledge Stores:
    - *Long-Term Memory (M):* Dynamic text file storing rules (for KEwLTM)
    - *Synthesized Knowledge (K):* Static text file storing rules (for KEwRAG)
  - Retrieval (KEwRAG only): Embedding model (NV-Embed-v2) + Vector Store (AJCC Manual chunks)
  - Controller: Script running the loops (Algorithm 1 & 2), managing edit distance checks

- **Critical path:**
  1.  **KEwLTM Path:** Load $D_{train}$ (100 reports) $\to$ Loop: Predict + Update $M$ (if Edit Dist $\le \delta$) $\to$ Save $M$ $\to$ Load $D_{test}$ + $M$ $\to$ Predict.
  2.  **KEwRAG Path:** Query AJCC Manual $\to$ Retrieve Top-5 Chunks $\to$ LLM Synthesizes Rules $\to$ Save $K$ $\to$ Load $D_{test}$ + $K$ $\to$ Predict.

- **Design tradeoffs:**
  - **KEwLTM vs. KEwRAG:** KEwLTM adapts to the specific data distribution (unannotated) but risks hallucinating rules. KEwRAG is grounded in ground-truth guidelines but requires a relevant external corpus and may miss institutional shorthand.
  - **Edit Threshold ($\delta$):** High threshold = stable memory but slow adaptation. Low threshold = fast adaptation but unstable/noisy memory.

- **Failure signatures:**
  - **Oscillating Memory (KEwLTM):** If $M$ length fluctuates wildly, the edit distance threshold is too low.
  - **Numerical Hallucination:** Model outputs "5.2 cm is less than 5 cm" in reasoning trace.
  - **Rule Summarization Error (KEwRAG):** Model creates a rule "T3 requires both >5cm AND skin invasion" (AND vs OR logic error) based on the example in Section 4.2.

- **First 3 experiments:**
  1.  **Baseline ZSCOT Benchmark:** Run standard ZSCOT on 50 reports to establish if the model is "strong" (Mixtral-like) or "weak" (Med42-like in specific contexts) to decide between KEwLTM or KEwRAG.
  2.  **Sensitivity Analysis (KEwLTM):** Run the induction loop on a tiny set (10 reports) with Edit Distance Threshold=0 vs 80. Inspect the resulting memory text $M$ manually for coherence.
  3.  **Rule Validation (KEwRAG):** Execute the RAG synthesis step *only*. Manually compare the generated rule set $K$ against the raw AJCC PDF to verify the LLM didn't hallucinate constraints during synthesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent can integrating external computational tools mitigate "numerical incompetence" in LLMs for cancer staging?
- **Basis in paper:** [explicit] The authors identify "Numerical Incompetence" as a prevalent error cause and explicitly suggest "delegating arithmetic... to an external, specialized function" in Section 5.3.2.
- **Why unresolved:** The current study relies solely on the LLM's internal reasoning for numerical comparisons, which proved error-prone (e.g., misclassifying 5.2 cm as "not more than 5 cm"). The proposed solution remains a theoretical direction.
- **What evidence would resolve it:** An experiment comparing the current model against a "LLM + Tools" framework where deterministic functions handle numeric thresholds, showing a statistically significant reduction in staging errors.

### Open Question 2
- **Question:** Can KEwLTM and KEwRAG maintain high performance when applied to diverse cancer types and inter-institutional pathology reports?
- **Basis in paper:** [explicit] The authors state in Section 5.2 that evaluation was limited to TCGA breast cancer reports, restricting generalizability. They explicitly list "evaluating... across a diverse range of cancer types" as a future step in Section 5.3.3.
- **Why unresolved:** The current study did not test the methods on other malignancies with different reporting styles or distinct staging criteria, leaving robustness across domains unproven.
- **What evidence would resolve it:** Results from applying the proposed methods to non-breast cancer datasets (e.g., lung, colon) and external institutional data, demonstrating comparable macro-F1 scores.

### Open Question 3
- **Question:** How does the performance of KEwRAG change with systematic optimization of retrieval parameters like the number of chunks (`k`) and query formulation?
- **Basis in paper:** [explicit] In Section 5.3.4, the authors note they used heuristics (k=5) and general queries, explicitly calling for "systematically evaluating the impact of the number of retrieved chunks" and "experimenting with more sophisticated query formulations."
- **Why unresolved:** The current KEwRAG pipeline uses a fixed, heuristic configuration without ablation studies on retrieval components.
- **What evidence would resolve it:** An ablation study varying `k` and query structures to measure their correlation with the accuracy of the synthesized rule set and final staging classification.

## Limitations
- Core methodology relies on undisclosed prompt templates and threshold parameters, creating a fundamental barrier to exact reproduction
- Performance heavily depends on base LLM's inherent zero-shot reasoning capability
- Critical numerical incompetence failure mode suggests approach cannot handle boundary conditions without external tool integration
- Study limited to single cancer type (breast) and specific guideline version (AJCC 7th), limiting generalizability claims

## Confidence
- **High Confidence:** The general framework of knowledge elicitation through iterative memory updates and static rule synthesis via RAG is technically sound and well-described
- **Medium Confidence:** The comparative performance results (F1 scores) are reproducible in principle, though exact values depend on unspecified prompt engineering
- **Low Confidence:** Claims about KEwLTM outperforming KEwRAG "when the base LLM performs well in zero-shot reasoning" are conditional on undocumented baseline performance metrics

## Next Checks
1. **Baseline ZSCOT Benchmark:** Implement the exact same prompt templates on 50 TCGA-BRCA reports to establish whether the chosen model (Mixtral-8x7B) exhibits strong or weak zero-shot reasoning before deciding which method to prioritize
2. **Edit Distance Calibration:** Run KEwLTM induction on 10 reports with varying thresholds (0, 20, 40, 80) and manually inspect memory coherenceâ€”this will reveal if the fixed threshold of 20 is appropriate for the specific rule syntax generated
3. **Numerical Boundary Test:** Create synthetic pathology reports with explicit size thresholds (e.g., "5.0 cm tumor," "4.9 cm tumor") and evaluate whether the model consistently misclassifies values exactly at category boundaries, confirming the numerical incompetence failure mode