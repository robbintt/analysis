---
ver: rpa2
title: Training Report of TeleChat3-MoE
arxiv_id: '2512.24157'
source_url: https://arxiv.org/abs/2512.24157
tags:
- training
- parallelism
- arxiv
- pipeline
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report details the training infrastructure for TeleChat3-MoE,
  a series of large-scale Mixture-of-Experts language models trained on Huawei Ascend
  NPU clusters. The work addresses the challenge of efficiently scaling MoE models
  to hundreds of billions to over one trillion parameters.
---

# Training Report of TeleChat3-MoE

## Quick Facts
- arXiv ID: 2512.24157
- Source URL: https://arxiv.org/abs/2512.24157
- Authors: Xinzhang Liu, Chao Wang, Zhihao Yang, Zhuo Jiang, Xuncheng Zhao, Haoran Wang, Lei Li, Dongdong He, Luobin Liu, Kaizhe Yuan, Han Gao, Zihan Wang, Yitong Yao, Sishi Xiong, Wenmin Deng, Haowei He, Kaidong Yu, Yu Zhao, Ruiyu Fang, Yuhao Jiang, Yingyan Li, Xiaohui Hu, Xi Yu, Jingqi Li, Yanwei Liu, Qingli Li, Xinyu Shi, Junhao Niu, Chengnuo Huang, Yao Xiao, Ruiwen Wang, Fengkai Li, Luwen Pu, Kaipeng Jia, Fubei Yao, Yuyao Huang, Xuewei He, Zhuoru Jiang, Ruiting Song, Rui Xue, Qiyi Xie, Jie Zhang, Zilu Huang, Zhaoxi Zhang, Zhilong Lu, Yanhan Zhang, Yin Zhang, Yanlei Xue, Zhu Yuan, Teng Su, Xin Jiang, Shuangyong Song, Yongxiang Li, Xuelong Li
- Reference count: 12
- Key outcome: Systematic training infrastructure for 105B-1T+ parameter MoE models with 10-15% performance gains from interleaved pipeline scheduling and hierarchical EP communication

## Executive Summary
This technical report details the training infrastructure for TeleChat3-MoE, a series of large-scale Mixture-of-Experts language models trained on Huawei Ascend NPU clusters. The work addresses the challenge of efficiently scaling MoE models to hundreds of billions to over one trillion parameters. Key contributions include systematic operator-level and end-to-end accuracy verification methods, performance optimizations such as interleaved pipeline scheduling, attention-aware data scheduling for long sequences, hierarchical communication merging, and DVM-based operator fusion. A systematic parallelization framework leveraging analytical estimation and integer linear programming optimizes multi-dimensional parallelism configurations, reducing tuning time from days to hours. Cluster-level optimizations address host- and device-bound bottlenecks. These advancements yield high Model Flops Utilization (MFU), near-linear scaling on thousands of devices, and reproducible training outcomes, providing a robust foundation for large-scale MoE model development.

## Method Summary
The method centers on a comprehensive training infrastructure for scaling MoE models (105B-1T+ parameters) on Ascend NPU clusters using MindSpore. Core components include systematic accuracy verification through operator-level tolerance testing and cross-hardware alignment, performance optimizations like interleaved 1F1B pipeline scheduling (yielding ~10% throughput improvement), hierarchical expert parallelism communication (delivering ~15% gain at EP=16), and DVM-based operator fusion. A parallelization framework parses model configurations, analytically estimates memory/performance, and uses integer linear programming to jointly optimize pipeline stages, VPP interleaving, and recomputation under memory constraints—reducing tuning time from ~7 days to ~0.5 days. Cluster-level optimizations mitigate host-bound contention (CPU isolation) and device-bound idle-mode triggering (firmware threshold adjustment).

## Key Results
- 10% end-to-end training throughput improvement from interleaved pipeline scheduling with 1F1B overlap
- ~15% higher training throughput at EP degree 16 using hierarchical communication merging vs global All-to-All
- 7 days → 0.5 day reduction in multi-dimensional parallelism tuning time via ILP-based framework
- 438B model on 4096 devices achieves ~40s step time with near-linear scaling
- High MFU maintained across scale-up to thousands of devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaved pipeline scheduling with 1F1B overlap improves end-to-end training throughput by approximately 10% compared to baseline pipeline strategies.
- Mechanism: Distributes non-contiguous model layers across pipeline stages rather than contiguous blocks, then schedules one-forward-one-backward passes from different micro-batches to overlap computation with communication during the steady-state phase.
- Core assumption: The increased communication volume from interleaved stage assignment is outweighed by reduced pipeline bubble overhead and successful computation-communication hiding.
- Evidence anchors:
  - [abstract]: "interleaved pipeline scheduling" listed among key performance optimizations
  - [section 4.1]: "Empirical results show that the combination of interleaved pipeline scheduling and 1F1B overlap yields an end-to-end training performance improvement of approximately 10% compared to baseline pipeline strategies."
  - [corpus]: OptPipe paper confirms pipeline optimization remains active research area with memory/scheduling tradeoffs
- Break condition: If pipeline bubble ratio cannot be reduced below communication overhead increase, or if micro-batch count is too small to fill pipeline stages.

### Mechanism 2
- Claim: Hierarchical communication merging for Expert Parallelism delivers ~15% higher training throughput at EP degree 16 versus global All-to-All baseline.
- Mechanism: Replaces single global All-to-All with topology-aware three-step sequence: (1) inter-node AllGather to collect complete EP data per machine, (2) local filtering retaining only required expert copies, (3) intra-node All-to-All for final redistribution—aligning communication with physical cluster topology.
- Core assumption: Intra-node bandwidth (HCCS) significantly exceeds inter-node bandwidth (RoCE), and local filtering removes sufficient redundancy to justify extra coordination step.
- Evidence anchors:
  - [abstract]: "hierarchical communication merging" listed as performance optimization
  - [section 4.3]: "Under an EP degree of 16, it delivers approximately 15% higher training throughput compared to the baseline global All-to-All approach."
  - [corpus]: Weak direct corpus evidence on hierarchical EP specifically; related work (Mist) addresses memory-parallelism co-optimization but not this exact technique
- Break condition: If EP degree is small or cluster topology has uniform bandwidth, hierarchical overhead may exceed savings.

### Mechanism 3
- Claim: ILP-based parallelization framework reduces multi-dimensional strategy tuning time from ~7 days to ~0.5 days while achieving comparable or superior throughput.
- Mechanism: Parses model YAML configuration, generates candidate strategies, analytically estimates memory and performance to prune search space, then applies integer linear programming to jointly optimize pipeline stage assignment, VPP interleaving, and recomputation under memory constraints—replacing expert trial-and-error with systematic search.
- Core assumption: Analytical cost models sufficiently approximate real execution to enable effective pruning; ILP solver can find near-optimal solutions within tractable time.
- Evidence anchors:
  - [abstract]: "reducing tuning time from days to hours"
  - [section 5]: Table 4 shows 7 days → 0.5 day reduction; Table 5 shows tool-generated strategy achieves 39,969ms vs 40,076ms and 40,147ms expert baselines
  - [corpus]: Mist paper confirms automatic distributed training configuration as active research direction
- Break condition: If analytical models diverge significantly from actual execution (e.g., unexpected communication patterns or operator fusion effects), pruning may eliminate optimal strategies.

## Foundational Learning

- Concept: **Pipeline Parallelism and Bubbles**
  - Why needed here: Interleaved scheduling and 1F1B overlap directly target pipeline bubble reduction; understanding bubble sources (data dependencies between stages, warm-up/drain phases) is prerequisite for evaluating tradeoffs.
  - Quick check question: Given 4 pipeline stages and 8 micro-batches, how many forward passes occur before the first backward pass can start in a standard 1F1B schedule?

- Concept: **Mixture-of-Experts Routing and Expert Parallelism**
  - Why needed here: MoE architecture with hundreds of routed experts requires expert parallelism; hierarchical communication and overlapping optimizations depend on understanding token-to-expert routing patterns and All-to-All communication structure.
  - Quick check question: In a top-8 routing scheme with 256 routed experts across 16 EP devices, how many experts does each device store, and what communication pattern distributes tokens to correct experts?

- Concept: **Multi-dimensional Parallelism Interaction**
  - Why needed here: The parallelization tool co-optimizes DP, TP, PP, EP, VPP, SP, OP simultaneously; memory and communication constraints couple across dimensions (e.g., increasing TP reduces per-device memory but adds AllReduce overhead).
  - Quick check question: If tensor parallelism degree doubles while keeping total devices constant, how does this affect pipeline bubble ratio and inter-device communication volume?

## Architecture Onboarding

- Component map:
  - Model Layer: Multi-Latent Attention (compressed KV), shallow-and-wide topology (45-61 layers), high-sparsity MoE (top-4 to top-8 routing, 192-384 routed experts + 1 shared)
  - Accuracy Verification: Operator-level tolerance testing (accumulation-aware), cross-hardware alignment workflow, parallelism strategy equivalence validation
  - Training Framework: Interleaved pipeline + 1F1B scheduler, attention-aware data scheduler, hierarchical EP communication, EP overlapping via multi-stream partitioning, DVM cross-class operator fusion
  - Parallelization Tool: YAML parser → strategy generator → analytical estimator → ILP optimizer → dry-run validator
  - Cluster Layer: Host-bound mitigation (CPU affinity, isolation domains), device-bound fixes (firmware idle-threshold adjustment, IOMMU passthrough)

- Critical path:
  1. Define model configuration (layers, experts, hidden size) → generate parallelism strategy candidates
  2. Validate numerical accuracy via operator tests and single-device forward/backward alignment
  3. Run parallelization tool to select optimal DP/TP/PP/EP/VPP configuration under memory constraints
  4. Deploy with framework optimizations enabled (interleaved pipeline, hierarchical EP, DVM fusion)
  5. Monitor and tune cluster-level bottlenecks (host contention, device idle-mode triggering)

- Design tradeoffs:
  - **Interleaving depth vs communication**: More VPP chunks reduce bubbles but increase inter-stage communication frequency
  - **EP degree vs load balance**: Higher EP distributes experts across more devices but increases All-to-All volume; hierarchical scheme mitigates at cost of complexity
  - **Recomputation vs memory**: Activating flash-attention or MM recomputation saves activation memory but adds compute overhead
  - **Micro-batch count vs pipeline efficiency**: More micro-batches fill pipeline better but increase per-step latency and memory

- Failure signatures:
  - **Loss divergence after hardware/parallelism change**: Indicates numerical precision mismatch; use layer-wise tensor dumping to localize
  - **MFU degradation on scale-up**: Check pipeline bubble ratio (PP imbalance), EP communication bottlenecks (All-to-All hotspots), or host-bound contention
  - **Inconsistent step times across nodes**: Profile for device idle-mode triggering (short operators) or monitoring-induced IOMMU contention
  - **Gradient norm mismatch on first backward pass**: Check weight decay application, optimizer implementation, or communication operator scaling

- First 3 experiments:
  1. **Single-device forward pass alignment**: Load identical weights and data on reference CPU/GPU vs target NPU; verify initial loss matches within tolerance before enabling any parallelism
  2. **Parallelism strategy dry run**: Use tool-generated configuration on small cluster (e.g., 64 devices); validate memory usage stays under limit and step time matches analytical estimate within 10%
  3. **EP communication profiling at scale**: On 512+ devices with EP enabled, measure All-to-All latency with and without hierarchical merging and overlapping; confirm EP communication time drops from ~30% to ~5% of total communication as claimed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the systematic parallelization framework's ILP-based optimization generalize effectively to non-MoE dense transformer architectures, or is it specialized for MoE-specific constraints like expert parallelism?
- Basis in paper: [inferred] The framework (Section 5) is evaluated only on the TeleChat3-MoE 438B model, with search dimensions including EP-specific parameters. The paper does not demonstrate applicability to dense models.
- Why unresolved: No experiments or theoretical discussion addresses whether the analytical memory estimation and ILP formulation transfer to architectures without expert routing.
- What evidence would resolve it: Application of the tool to train dense transformer models (e.g., TeleChat3 dense variants) with comparison to expert-tuned baselines.

### Open Question 2
- Question: What are the energy consumption tradeoffs of disabling the NPU idle-mode power-saving mechanism, which yielded 25-30% throughput gains on 4096 devices?
- Basis in paper: [inferred] Section 6 describes lowering the "operator execution time threshold required to exit the idle mode" to prevent frequency downscaling, but only reports throughput improvements without measuring energy impact.
- Why unresolved: The optimization trades power efficiency for training speed; the paper provides no energy measurements or cost analysis for large-scale deployments.
- What evidence would resolve it: Comparative energy-per-token or total power consumption metrics between default and modified firmware settings across sustained training runs.

### Open Question 3
- Question: Do the accuracy tolerance thresholds in Table 2 (derived from "practical experience") represent theoretically optimal bounds, or could tighter tolerances further improve end-to-end model convergence?
- Basis in paper: [explicit] Section 3.1 states the tolerance values are "based on our practical experience" and are "reference tolerance values"—acknowledging they are empirically derived rather than formally derived.
- Why unresolved: The paper does not analyze whether accumulated operator-level errors within these tolerances affect final model quality or training stability at trillion-parameter scales.
- What evidence would resolve it: Ablation studies training models with progressively tighter/looser tolerance thresholds and measuring downstream task performance and convergence behavior.

### Open Question 4
- Question: How does the hierarchical EP communication scheme scale with expert parallelism degrees beyond 16, particularly for models with 384+ experts?
- Basis in paper: [inferred] Section 4.3 reports ~15% throughput improvement at EP degree 16, but the 1119B model uses 384 routed experts. The scalability characteristics for higher EP degrees are not characterized.
- Why unresolved: Higher EP degrees fundamentally alter communication patterns and inter-node traffic; the three-step hierarchical scheme may face different bottlenecks.
- What evidence would resolve it: Benchmarks of hierarchical vs. global All-to-All communication across EP degrees of 32, 64, 128, and higher on comparable cluster configurations.

## Limitations
- Training data specifics (corpus composition, token count, preprocessing pipeline) are not provided
- Exact training hyperparameters (learning rate schedule, batch size progression, optimizer details) are unspecified
- DVM-based operator fusion performance claims lack quantitative results
- Flash-attention and MM recomputation effectiveness not evaluated in reported experiments

## Confidence
- **High Confidence**: Operator-level and end-to-end accuracy verification methodology; ILP-based parallelization framework performance claims; Cluster-level optimization impact
- **Medium Confidence**: Interleaved pipeline scheduling improvement (~10%); Hierarchical EP communication efficiency (~15% at EP=16); Attention-aware data scheduling for long sequences
- **Low Confidence**: DVM-based operator fusion performance claims; Flash-attention and MM recomputation effectiveness

## Next Checks
1. **Accuracy Preservation Verification**: Implement the operator-level tolerance testing protocol on a small model (e.g., 1.3B parameters) across different hardware (CPU, GPU, NPU). Verify that forward/backward alignment holds within the specified tolerance thresholds before scaling up.
2. **Parallelization Strategy Validation**: Using the ILP-based tool, generate and test configurations for a mid-scale model (e.g., 10B parameters) on a 64-device cluster. Measure actual memory consumption versus analytical estimates and compare step time against expert-tuned baselines.
3. **EP Communication Profiling**: On a 512+ device cluster with MoE enabled, profile All-to-All communication time with and without hierarchical merging and overlapping. Verify that communication time drops from approximately 30% to 5% of total communication time as claimed.