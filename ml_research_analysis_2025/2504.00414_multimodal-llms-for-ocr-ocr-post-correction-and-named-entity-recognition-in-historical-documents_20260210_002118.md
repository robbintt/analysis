---
ver: rpa2
title: Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition
  in Historical Documents
arxiv_id: '2504.00414'
source_url: https://arxiv.org/abs/2504.00414
tags:
- historical
- recognition
- transcription
- mllms
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates multimodal Large Language Models (mLLMs)\
  \ for Optical Character Recognition (OCR), OCR Post-Correction, and Named Entity\
  \ Recognition (NER) on German historical documents from 1754\u20131870. The best-performing\
  \ mLLM, Gemini 2.0 Flash, achieved a normalized Character Error Rate (CER) of 1.27%\
  \ for OCR without preprocessing or fine-tuning, outperforming conventional OCR models."
---

# Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition in Historical Documents

## Quick Facts
- arXiv ID: 2504.00414
- Source URL: https://arxiv.org/abs/2504.00414
- Reference count: 40
- Key outcome: Gemini 2.0 Flash achieves 1.27% normalized CER for OCR on German historical documents without preprocessing or fine-tuning, outperforming conventional OCR models.

## Executive Summary
This study evaluates multimodal Large Language Models (mLLMs) for Optical Character Recognition (OCR), OCR Post-Correction, and Named Entity Recognition (NER) on German historical documents from 1754–1870. The best-performing mLLM, Gemini 2.0 Flash, achieved a normalized Character Error Rate (CER) of 1.27% for OCR without preprocessing or fine-tuning, outperforming conventional OCR models. A novel mLLM-based OCR Post-Correction method reduced CER to 0.84%. For NER, mLLMs achieved over 90% fuzzy match rates directly from images for some directories. These findings demonstrate mLLMs' potential to revolutionize historical document transcription and data extraction with minimal preprocessing.

## Method Summary
The study evaluates zero-shot multimodal LLM performance on 30 pages from 10 German historical city directories (1754–1870). Using Gemini 2.0 Flash and GPT-4o via API, the researchers performed OCR transcription, OCR post-correction, and direct Named Entity Recognition from images without any preprocessing or fine-tuning. The OCR pipeline uses strict "commanding" prompts with negative instructions to prevent transcription of marginalia. Post-correction involves feeding noisy OCR text back to the mLLM alongside the original image. NER attempts both traditional OCR-then-extract and direct image-to-structured-data approaches, evaluated using normalized CER, WER, and fuzzy matching (Jaro-Winkler) against ground truth datasets.

## Key Results
- Gemini 2.0 Flash achieved 1.27% normalized CER for OCR without preprocessing, outperforming Transkribus baseline models.
- mLLM-based OCR Post-Correction reduced CER from 1.27% to 0.84% by leveraging image-text concurrent inputs.
- Direct NER from images achieved over 90% fuzzy match rates for some directories, with Gemini 2.0 Flash reaching up to 98.21% for Aachen-1839.

## Why This Works (Mechanism)

### Mechanism 1
Multimodal OCR Post-Correction (using image + text) yields higher accuracy than text-only correction or image-only OCR. The mLLM takes a noisy transcription and the original document image as concurrent inputs, aligning the text against visual evidence to correct recognition errors without hallucinating context, effectively grounding the correction in pixel data. Performance degrades if image resolution is too low to distinguish specific glyphs or if noisy input text is too fragmented to provide coherent structure.

### Mechanism 2
Aggressive "negative prompting" can replace traditional image preprocessing (cropping/binarization). By using strict prompts that threaten "TOTAL SYSTEM FAILURE" for including marginalia or partial text, the model is coerced into performing implicit layout analysis and segmentation, ignoring edge noise that would otherwise require manual cropping. Complex layouts where "noise" visually resembles primary text columns may confuse the model, leading to false negatives.

### Mechanism 3
Direct entity extraction (PNG-to-CSV) is viable without intermediate transcription, provided visual-textual alignment is strong. The mLLM processes the image to recognize text and immediately parses it into a structured JSON schema based on semantic understanding of document type, merging OCR and NER steps into a single inference pass. Accuracy drops for documents with "sub-heading" based structures where spatial relationships between entities and attributes are non-standard or vertical.

## Foundational Learning

- **Character Error Rate (CER) Normalization**: Required to fairly compare mLLMs against traditional OCR by isolating recognition capability from formatting differences (archaic spellings, punctuation). If a model transcribes "Schulmeister" as "Schulmeister." (with a period), does strict matching count this as a complete failure for that word?

- **Context Window vs. Image Tokenization**: Essential for understanding costs and latency when processing full pages, as images consume fixed tokens from the model's context window. Why might feeding a high-resolution PNG plus a full page of text into a smaller context window model fail compared to Gemini 2.0 Flash?

- **Zero-shot vs. Fine-tuned OCR**: Shifts learning prerequisite from "how to train a model" to "how to prompt a model" by demonstrating off-the-shelf mLLMs can rival fine-tuned CNN-BiLSTM models. What is the tradeoff between using a generic Gemini model with a complex prompt versus a specialized, fine-tuned Transkribus model?

## Architecture Onboarding

- **Component map**: Input (PDF/PNG) -> Orchestrator (Python API wrapper) -> Core Processor (Gemini 2.0 Flash/GPT-4o) -> Output (TXT for transcription, JSON for NER/CSV)

- **Critical path**: Prompt engineering phase is the bottleneck, using iterative, mLLM-assisted loops to refine prompts. If the prompt does not explicitly forbid transcribing adjacent page content, the pipeline fails.

- **Design tradeoffs**: Speed vs. Reasoning - Gemini 2.0 Flash chosen for speed/cost ($0 vs paid models) and sufficient accuracy, whereas "Pro" models might offer better reasoning but higher latency/cost. Pipeline Separation - "Image -> OCR -> Post-Correct -> NER" is currently more accurate than "Image -> Direct NER," trading architectural simplicity for accuracy.

- **Failure signatures**: Layout Confusion - models struggle with "sub-heading" directories, failing to propagate sub-heading context to subsequent entries. Hallucinated Marginalia - without strict negative prompting, models transcribe partial text from facing pages or archive stamps.

- **First 3 experiments**:
  1. Baseline Benchmark - Run Tesseract/Transkribus on your specific document set to establish a CER baseline.
  2. Prompt Sensitivity Test - Pass a single "messy" page to Gemini 2.0 Flash using the paper's "TOTAL SYSTEM FAILURE" prompt vs. a polite prompt to measure the delta in layout compliance.
  3. Post-Correction Validation - Take output of your current best OCR model, append it to the image, and prompt the mLLM for correction to calculate CER reduction.

## Open Questions the Paper Calls Out

- To what extent do mLLMs generalize to historical documents with non-Latin scripts or complex layouts compared to the Latin-alphabet Fraktur/Antiqua texts tested? The study deliberately restricted its scope to Latin-alphabet prints, leaving performance on diverse global scripts untested.

- How does Named Entity Recognition (NER) performance of off-the-shelf mLLMs compare to corpus-fine-tuned conventional models on historical texts? The authors state that mLLMs require a more in-depth comparison to conventional fine-tuned NER approaches than provided in this paper.

- What specific image properties (e.g., resolution, text density, background contrast) most significantly impact mLLM transcription accuracy? The study evaluated off-the-shelf performance but did not conduct ablation studies to isolate which visual degradations most hinder mLLM performance.

## Limitations
- Evaluation limited to German historical city directories with relatively clean layouts, leaving generalization to other document types untested.
- No systematic ablation study on prompt complexity versus performance, leaving optimal balance between strict negative prompting and model flexibility unclear.
- Some directories excluded from NER evaluation due to row mismatches, suggesting the direct PNG-to-CSV method may not be universally reliable.

## Confidence
- **High Confidence**: Core finding that Gemini 2.0 Flash achieves 1.27% CER without preprocessing is well-supported by direct comparisons to Transkribus baselines.
- **Medium Confidence**: Claim that negative prompting can fully replace image preprocessing is plausible but under-supported, as evidence doesn't systematically explore edge cases.
- **Medium Confidence**: Direct NER results showing >90% fuzzy match rates are impressive, but exclusion of several directories from evaluation suggests method may not be universally reliable.

## Next Checks
1. **Layout Generalization Test**: Apply the same mLLM pipeline to a different historical document type (e.g., medieval charters or scientific tables) with complex layouts to validate whether the negative prompting strategy generalizes beyond city directories.

2. **Prompt Ablation Study**: Systematically test variations in prompt strictness on a subset of pages to quantify the tradeoff between layout compliance and transcription accuracy.

3. **Cost-Benefit Analysis**: Calculate total API token consumption and associated costs for processing an entire historical collection using this mLLM pipeline versus traditional OCR+post-correction workflows, including iterative prompt engineering overhead.