---
ver: rpa2
title: 'Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive
  Jailbreaking'
arxiv_id: '2504.05838'
source_url: https://arxiv.org/abs/2504.05838
tags:
- image
- nsfw
- clip
- benign
- nudity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the hijacking attack, a novel jailbreaking
  method enabled by the Image Prompt Adapter (IP-Adapter) in text-to-image diffusion
  models. The attack allows adversaries to upload imperceptible adversarial examples
  (AEs) to the web, which are then unknowingly downloaded and used by benign users
  to trigger NSFW outputs from image generation services, causing reputational harm
  to service providers.
---

# Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking

## Quick Facts
- **arXiv ID**: 2504.05838
- **Source URL**: https://arxiv.org/abs/2504.05838
- **Reference count**: 40
- **Primary result**: AEO achieves >77% NSFW rate and >57% nudity rate on twelve T2I-IP-DMs across three tasks.

## Executive Summary
This paper introduces the hijacking attack, a novel jailbreaking method enabled by the Image Prompt Adapter (IP-Adapter) in text-to-image diffusion models. The attack allows adversaries to upload imperceptible adversarial examples (AEs) to the web, which are then unknowingly downloaded and used by benign users to trigger NSFW outputs from image generation services, causing reputational harm to service providers. The authors propose the Attack Encoder Only (AEO) method, which aligns AEs with NSFW prompts in the image encoder's feature space. Extensive experiments on twelve T2I-IP-DMs across three tasks show that AEO can achieve over 77% NSFW rate and 57% nudity rate. Existing defenses are found to be ineffective against this attack, and the authors explore adversarial training to mitigate the threat. The study highlights the need for more robust defenses and evaluation frameworks for T2I-IP-DMs.

## Method Summary
The authors propose the Attack Encoder Only (AEO) method to jailbreak T2I-IP-DMs by crafting imperceptible adversarial examples (AEs). AEO optimizes a benign image $x_b$ into an AE $x_{adv}$ by minimizing the distance between the features of $x_{adv}$ and a target NSFW image $x_{nsfw}$ in the image encoder's feature space. This alignment causes the denoiser to behave as if conditioned on $x_{nsfw}$, resulting in NSFW outputs even when the image prompt appears benign. The attack exploits the IP-Adapter's workflow, where image features are extracted by an encoder and injected into the denoiser via cross-attention. The authors evaluate AEO across three tasks (text-to-image, image inpainting, virtual try-on) on twelve T2I-IP-DMs, using PGD optimization with $\epsilon=8/255$ and 500 iterations. Defenses like FARE (robust training) are explored, but existing methods are found ineffective.

## Key Results
- AEO achieves >77% NSFW rate and >57% nudity rate on twelve T2I-IP-DMs across three tasks.
- The attack's effectiveness is positively correlated with the IP-Adapter's weight factor ($\lambda$), with higher $\lambda$ yielding stronger NSFW outputs.
- Existing defenses (e.g., output filters, diffusion-based purification) are ineffective against AEO, and adversarial training is proposed as a mitigation strategy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning adversarial examples (AEs) with NSFW image prompts in the image encoder's feature space can cause T2I-IP-DMs to generate NSFW outputs, even when conditioned on seemingly benign images.
- Mechanism: The AEO method optimizes a benign image $x_b$ into an adversarial example $x_{adv}$ by minimizing a distance metric (MSE or Cosine Similarity) between $f(x_{adv})$ and $f(x_{nsfw})$ in the encoder $f(\cdot)$'s feature space. This makes the denoiser behave as if it were conditioned on $x_{nsfw}$.
- Core assumption: The IP-Adapter is trained to generate images faithful to the feature extracted by the image encoder. If two images have similar features, the model should produce similar outputs.
- Evidence anchors:
  - [abstract] "The authors propose the Attack Encoder Only (AEO) method, which aligns AEs with NSFW prompts in the image encoder’s feature space."
  - [section 3.2] "one intuitive approach... is to align x_adv with x_nsfw in the feature space, such that the denoiser conditioned on x_adv is approximately conditioned on x_nsfw."
  - [corpus] Related work on "adversarial illusions in multi-modal embeddings" (Zhang et al., 2024) explores similar cross-modal alignment attacks, but the corpus evidence for this specific IP-Adapter mechanism is weak.
- Break condition: The mechanism fails if the T2I-IP-DM's downstream modules do not faithfully reconstruct images based on encoder features, or if the encoder itself is robust against feature-space alignment attacks.

### Mechanism 2
- Claim: The hijacking attack's scalability and deceptiveness are enabled by the visual stealthiness of image-based AEs and the social dynamic of benign users unknowingly distributing them.
- Mechanism: Unlike adversarial text, image-space AEs can be constrained by a small $L_p$-norm bound ($\epsilon$), making them imperceptible to humans. An adversary uploads these AEs to the web, where benign users download and use them as image prompts, unwittingly triggering NSFW outputs. The users then blame the service provider, not the benign-looking prompt.
- Core assumption: Benign users will download image prompts from the web and cannot reliably distinguish between a clean image and an AE.
- Evidence anchors:
  - [abstract] "The attack allows adversaries to upload imperceptible adversarial examples (AEs) to the web, which are then unknowingly downloaded and used by benign users..."
  - [section 2.1] "The hijacking attack exploits a realistic scenario wherein benign users will query the IGS with prompts downloaded from the internet..."
  - [corpus] The corpus does not provide direct evidence for this specific social-engineering component.
- Break condition: The attack's scalability breaks if image-sharing platforms deploy effective filters for AEs, or if user behavior patterns shift away from downloading images for prompts.

### Mechanism 3
- Claim: The attack's effectiveness is positively correlated with the IP-Adapter's weight factor ($\lambda$), which controls the influence of the image prompt over the text prompt.
- Mechanism: The IP-Adapter injects image features into the denoiser via decoupled cross-attention. A higher $\lambda$ amplifies the contribution from the image prompt's features, attenuating the text prompt's influence. Therefore, an AE's malicious signal is more dominant at higher $\lambda$.
- Core assumption: The text prompt is benign or non-NSFW. The NSFW content is driven almost entirely by the malicious image prompt.
- Evidence anchors:
  - [section 4.2] "increasing the weight factor boosts AEO’s performance... When the weight factor is 0.25, AEO achieves at most 20.7% Nudity rate... When the weight factor increases to 1.0, the NSFW rate is promoted to at least 77.0%."
  - [abstract] Mentions evaluating across different tasks but does not explicitly state the weight factor mechanism.
  - [corpus] No relevant corpus evidence.
- Break condition: The correlation weakens or inverts if the text prompt is adversarial or explicitly contradicts the NSFW content, or if the model's architecture fundamentally changes the role of the weight factor.

## Foundational Learning

- Concept: **Adversarial Examples (AEs) in Computer Vision**
  - Why needed here: The entire attack hinges on crafting AEs—images with human-imperceptible perturbations that cause model misbehavior. Understanding how gradients can be used to optimize an image toward a malicious goal in feature space is prerequisite.
  - Quick check question: Given a classifier and a benign image, how would you formulate a loss function to create an AE that is misclassified as a specific target class while remaining visually similar?

- Concept: **Diffusion Model Conditioning & Cross-Attention**
  - Why needed here: The IP-Adapter works by injecting image features into a diffusion model's denoiser via cross-attention layers. Understanding this conditioning mechanism is essential to see why aligning features in the encoder space affects the final output.
  - Quick check question: In a latent diffusion model, what is the role of the cross-attention layer in the denoiser U-Net, and how does it integrate the conditioning signal (e.g., from text or image prompts)?

- Concept: **IP-Adapter Architecture (Extraction & Injection Stages)**
  - Why needed here: The paper's attack strategy (AEO) is explicitly designed around the IP-Adapter's two-stage workflow. Knowing that the encoder feature influences all downstream modules via a projection network and decoupled cross-attention explains why attacking the encoder is sufficient.
  - Quick check question: Describe the two stages of the IP-Adapter workflow. Why does the "global-type" vs. "grid-type" distinction matter for an attacker trying to align features?

## Architecture Onboarding

- Component map:
  1. **Adversary's Pipeline:** Benign Image ($x_b$) + NSFW Target ($x_{nsfw}$) → **AE Optimizer (AEO)** → Adversarial Example ($x_{adv}$) → Upload to Web.
  2. **Victim's Pipeline (T2I-IP-DM):** User downloads $x_{adv}$ → **Image Encoder (f)** → Feature $f(x_{adv})$ → **Projection Network (proj)** → **Decoupled Cross-Attention** in Denoiser → NSFW Image Output.
  3. **Defensive Layer:** Output Filter (e.g., NudeNet, SDSC) - *typically bypassed or too late*.
- Critical path: The attack's success flows through: **AE Craftsmanship (AEO)** → **User Trust (Imperceptibility)** → **Model Fidelity (Feature Alignment)**. The most critical technical component is the AEO optimizer, which must solve `min dist(f(x_adv), f(x_nsfw))` effectively.
- Design tradeoffs: The paper highlights a key tradeoff between **attack efficacy and stealthiness**, controlled by the perturbation budget $\epsilon$. A larger $\epsilon$ increases the NSFW rate but makes the AE more perceptible. Another tradeoff is in defense: **FARE** improves robustness but may degrade fidelity on benign prompts.
- Failure signatures: A failing attack may exhibit: (1) Low NSFW/Nudity rates despite high $\lambda$; (2) High perceptual difference between $x_{adv}$ and $x_b$; (3) Poor transferability when the surrogate encoder differs from the target encoder without transfer-enhancing techniques.
- First 3 experiments:
  1. **Baseline AEO Implementation:** Implement AEO (MSE and COS variants) with PGD on a small dataset. Replicate the $\epsilon=8/255$, 500-step setup from Section 4 on one T2I-IP-DM (e.g., SD-v1-5-Global) to verify NSFW rate claims.
  2. **Weight Factor Ablation:** Systematically vary the IP-Adapter weight factor ($\lambda \in \{0.25, 0.5, 0.75, 1.0\}$) and measure the resulting NSFW/Nudity rates for fixed AEs. This validates the core sensitivity mechanism.
  3. **Defense Evaluation Pilot:** Test the most effective AEs from Experiment 1 against a simple output filter (like the Stable Diffusion Safety Checker). This provides a baseline for the "ineffectiveness of existing defenses" claim before exploring advanced defenses like FARE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adversarial training (AT) be improved to maintain robustness against unseen threat models when securing T2I-IP-DMs?
- Basis in paper: [explicit] The authors state in Section 5.2 that "improving AT’s robustness under unseen threat models, which is still an open problem, is necessary for deploying AT-secured T2I-IP-DMs in real scenes."
- Why unresolved: Current adversarial training methods, like FARE, are typically validated against specific threat models and may fail or be bypassed by novel attack strategies not encountered during fine-tuning.
- What evidence would resolve it: A defense method that demonstrates high resistance (low NSFW rate) against AEs crafted using surrogate models or attack algorithms excluded from the training phase.

### Open Question 2
- Question: How can the robustness of diffusion-based purification (DBP) defenses against AEO be accurately evaluated without falling prey to false security signals caused by stochastic gradients?
- Basis in paper: [explicit] Appendix K.2 notes that "How to accurately evaluate the robustness of DBP is still an open problem" because DBP induces stochastic gradients, potentially leading to overestimated security.
- Why unresolved: Standard evaluation methods (e.g., EOT=1) suggest high security, but more rigorous evaluation (e.g., EOT=16) reveals vulnerabilities, making the true security level difficult to assess.
- What evidence would resolve it: A standardized evaluation metric or framework that effectively neutralizes gradient obfuscation effects in DBP to reveal the model's true robustness against AEO.

### Open Question 3
- Question: Can state-of-the-art (SOTA) adversarial attacks be integrated with AEO to further improve the efficacy-stealthiness balance against T2I-IP-DMs?
- Basis in paper: [explicit] Appendix K.3 states, "We believe that incorporating such attacks will further fuel the threat we have uncovered, and we leave the corresponding discussion to future works."
- Why unresolved: The current study relied on PGD to verify feasibility; it did not explore whether more advanced attacks could generate higher NSFW rates or better stealthiness.
- What evidence would resolve it: Experiments demonstrating that advanced attacks (e.g., C&B, AdvDiff) achieve higher success rates or lower perturbation budgets than PGD while maintaining the AEO alignment strategy.

## Limitations

- The hijacking attack's real-world scalability depends critically on benign users downloading and using images from the web, a behavioral assumption not empirically validated.
- The AEO mechanism's effectiveness assumes the IP-Adapter's encoder features are fully leveraged by downstream modules, but the paper does not test the impact of architectural changes or stronger image encoders.
- The defense evaluation is limited: only a few methods are tested, and the proposed FARE defense's training procedure is not detailed, making its practical deployment unclear.

## Confidence

- **High Confidence**: The core mechanism of AEO—optimizing AEs in the encoder feature space to trigger NSFW outputs—is well-supported by the paper's ablation studies and quantitative results (e.g., 77% NSFW rate at $\lambda=1.0$).
- **Medium Confidence**: The claim that existing defenses are ineffective is based on limited tests and may not generalize to all defense types or real-world deployment scenarios.
- **Low Confidence**: The scalability and deceptiveness of the attack in practice, due to the reliance on user behavior and web distribution, is not empirically demonstrated and remains speculative.

## Next Checks

1. **Behavioral Validation**: Conduct a user study or simulate benign user behavior to assess the likelihood of downloading and using web-based image prompts, and whether users can detect adversarial perturbations at $\epsilon=8/255$.
2. **Architectural Robustness**: Test the AEO attack against T2I-IP-DMs with different encoder architectures (e.g., CLIP-ViT-L/14 vs. ViT-H/14) or with stronger feature-space defenses (e.g., adversarial training on the encoder) to identify potential failure modes.
3. **Defense Generalization**: Evaluate the effectiveness of a broader set of defenses (e.g., input sanitization, feature-space denoising, or output filtering) on the crafted AEs, and assess the trade-off between robustness and output fidelity for benign prompts.