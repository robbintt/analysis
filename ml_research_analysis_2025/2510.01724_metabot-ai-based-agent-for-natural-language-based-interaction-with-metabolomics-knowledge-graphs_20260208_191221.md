---
ver: rpa2
title: 'MetaboT: AI-based agent for natural language-based interaction with metabolomics
  knowledge graphs'
arxiv_id: '2510.01724'
source_url: https://arxiv.org/abs/2510.01724
tags:
- query
- knowledge
- agent
- sparql
- metabot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MetaboT is an AI-driven multi-agent framework that translates natural-language
  questions into SPARQL queries for metabolomics knowledge graphs, enabling researchers
  to query complex mass spectrometry data without programming expertise. The system
  uses specialized agents for entity resolution, query validation, and execution,
  achieving 83.67% accuracy on 50 test queries compared to 8.16% for a standard LLM.
---

# MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs

## Quick Facts
- arXiv ID: 2510.01724
- Source URL: https://arxiv.org/abs/2510.01724
- Reference count: 40
- Primary result: 83.67% SPARQL generation accuracy on 50 test queries

## Executive Summary
MetaboT is an AI-driven multi-agent framework that translates natural-language questions into SPARQL queries for metabolomics knowledge graphs, enabling researchers to query complex mass spectrometry data without programming expertise. The system uses specialized agents for entity resolution, query validation, and execution, achieving 83.67% accuracy on 50 test queries compared to 8.16% for a standard LLM. By integrating external databases and iterative query refinement, MetaboT reduces hallucinations and enhances precision, bridging the gap between complex semantic technologies and user-friendly interaction. It supports advanced data mining and biological interpretation while maintaining scalability for future extensions.

## Method Summary
MetaboT employs a six-agent workflow orchestrated via LangGraph, where queries flow through Entry→Validator→Supervisor→KG Agent→SPARQL Query Runner→Interpreter agents. The system resolves entities using external APIs (Wikidata, ChEMBL) and RAG retrieval, then generates SPARQL using GraphSparqlQAChain with one-shot refinement from a FAISS query database. Evaluation used GPT-4o against 50 curated questions with reference SPARQL queries, measuring accuracy, cost (~$0.06/query), and runtime (~80s/query). The architecture leverages 11 specialized tools for entity resolution and query execution against the ENPKG knowledge graph.

## Key Results
- 83.67% SPARQL generation accuracy on 50 test queries versus 8.16% for single LLM
- Average query execution time of 80.45 seconds with approximately $0.06 per query cost
- Successfully handles complex metabolite queries without requiring programming expertise

## Why This Works (Mechanism)

### Mechanism 1: Specialized Agent Decomposition with Role-Based Routing
Decomposing query processing into specialized agents with explicit role boundaries improves SPARQL generation accuracy compared to single-LLM approaches. Each agent receives a tailored prompt defining its scope, reducing the reasoning burden per agent. The Supervisor Agent acts as a router, delegating entity resolution tasks only when needed. This modular separation allows the Validator Agent to reject out-of-scope questions early, while the KG Agent focuses exclusively on identifier retrieval. Core assumption: LLMs perform better on narrow, well-defined subtasks than on end-to-end reasoning chains.

### Mechanism 2: Grounded Entity Resolution via External Tool Calls
Resolving entity identifiers through external API calls and RAG retrieval reduces hallucinations compared to relying on LLM parametric knowledge. The KG Agent invokes deterministic tools—TaxonResolver queries Wikidata SPARQL endpoint, TargetResolver queries ChEMBL API, ChemicalResolver uses FAISS retrieval on a reference file. These tools return verified IRIs that are injected into the SPARQL generation prompt, constraining the LLM to use correct identifiers. Core assumption: External databases contain canonical identifiers and are queryable with high reliability.

### Mechanism 3: Iterative Query Refinement with Error/Data Absence Distinction
A single retry loop for failed queries improves robustness by distinguishing query construction errors from genuine data absence. When GraphSparqlQAChain returns no results, the system retrieves a similar query from a FAISS database of past queries and regenerates with schema-aligned adjustments. If the retry also fails, the system reports data absence rather than infinite retry loops. Core assumption: A single regeneration attempt is sufficient to correct most construction errors without excessive token costs.

## Foundational Learning

- **Concept**: SPARQL query structure (SELECT, WHERE, FILTER, OPTIONAL clauses)
  - Why needed here: MetaboT generates SPARQL against the ENPKG endpoint; understanding basic syntax helps debug generated queries.
  - Quick check question: Can you identify the subject-predicate-object pattern in `SELECT ?compound WHERE { ?compound wdt:P703 ?taxon }`?

- **Concept**: Knowledge graph IRIs and ontology schemas (RDF, Turtle format)
  - Why needed here: The GraphSparqlQAChain tool requires schema information in Turtle format; entity resolution produces IRIs that must match the graph's namespace.
  - Quick check question: What is the difference between a URI and a literal in RDF?

- **Concept**: Multi-agent orchestration with stateful graphs (LangGraph)
  - Why needed here: MetaboT uses LangGraph's directed graph structure; agents pass messages along edges with evolving state.
  - Quick check question: How does a stateful graph differ from a linear chain in LangChain?

## Architecture Onboarding

- **Component map**: Entry Agent → Validator Agent → Supervisor Agent → (KG Agent if entities need resolution) → SPARQL Query Runner Agent → (Interpreter Agent if requested)
- **Critical path**: Entry → Validator → Supervisor → (KG Agent if entities need resolution) → SPARQL Query Runner → (Interpreter if requested). Average 5-8 API calls per query.
- **Design tradeoffs**: Accuracy vs. cost: GPT-4o required for complex queries (78.95% on high-complexity vs. GPT-4o mini's lower performance); average $0.06/question. Single-graph constraint: No federated query support; external data must be pre-integrated into ENPKG. One-retry limit: Balances cost against recovery from construction errors.
- **Failure signatures**: Type 1: Validator rejects valid questions (likely with GPT-4o mini). Type 2: SPARQL syntax errors or property mismatches. Type 3: Supervisor fails to invoke KG Agent for entity resolution. Type 4: KG Agent invokes wrong resolver tool.
- **First 3 experiments**: 1) Run the 50-question evaluation dataset locally with GPT-4o and compare per-question accuracy against the reported 83.67% baseline. 2) Ablate the KG Agent by bypassing entity resolution—pass user question directly to SPARQL Query Runner—to measure hallucination rate increase. 3) Test a single high-complexity query through LangSmith tracing to inspect token usage per agent and identify bottlenecks.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the MetaboT architecture be extended to execute federated SPARQL queries across distributed knowledge graphs?
  - Basis in paper: [explicit] The paper states that a limitation is the restriction to single-graph querying and notes that "developing true federated query capabilities requires solving fundamental challenges" in query generation and optimization.
  - Why unresolved: The current system lacks the necessary indexing systems and query planning algorithms to retrieve information from multiple knowledge graphs simultaneously.
  - Evidence: Successful implementation of a module capable of executing SERVICE clauses to bridge ENPKG with external resources like LOTUS or Wikidata.

- **Open Question 2**: Would a hybrid approach combining multiple LLMs with varying capabilities improve MetaboT's cost-effectiveness and efficiency?
  - Basis in paper: [explicit] The authors "postulate that overall efficiency and cost-effectiveness could be improved by implementing a hybrid approach that combines multiple models with varying capabilities alongside automated prompt optimisation strategies."
  - Why unresolved: The current system relies on single high-performance models (GPT-4o) to achieve high accuracy, incurring high computational costs.
  - Evidence: A comparative benchmark measuring accuracy and operational cost between the current single-model setup and a multi-model orchestration.

- **Open Question 3**: How can the system definitively distinguish between query construction errors and genuine data absence when iterative query refinement fails?
  - Basis in paper: [explicit] The paper notes that if a regenerated query still returns no results, "this may either indicate that the knowledge graph truly lacks data on the topic or that errors persist in the query formulation."
  - Why unresolved: The current refinement workflow performs only one retry, leaving an ambiguity where a lack of results could be a false negative caused by persistent logic errors.
  - Evidence: Analysis of false negative rates in the evaluation dataset to determine if "no results" outputs correlate with existing data.

## Limitations
- Small evaluation dataset (50 questions) potentially overfit to ENPKG schema
- Single-graph constraint with no federated query support across multiple knowledge graphs
- External API dependencies introduce runtime fragility and potential rate limiting issues

## Confidence
- **High**: The modular agent decomposition mechanism is well-supported by the 83.67% vs 8.16% accuracy comparison and aligns with established multi-agent KGQA patterns.
- **Medium**: The grounded entity resolution claim is plausible given external API integration, but hallucination reduction is not quantitatively validated against a no-tools baseline.
- **Medium**: The iterative refinement mechanism is described, but its sufficiency (single retry) is questioned by corpus evidence (e.g., RL-based multi-step approaches).

## Next Checks
1. Reproduce the 50-question evaluation with GPT-4o and compare per-question accuracy against the reported 83.67% baseline, inspecting error type distributions.
2. Perform an ablation study: bypass KG Agent entity resolution and pass raw user questions to SPARQL Query Runner to measure hallucination rate increase.
3. Instrument a single high-complexity query with LangSmith tracing to capture token usage per agent and identify bottlenecks or routing failures.