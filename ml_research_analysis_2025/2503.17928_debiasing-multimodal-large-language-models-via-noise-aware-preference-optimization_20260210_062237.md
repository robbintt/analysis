---
ver: rpa2
title: Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization
arxiv_id: '2503.17928'
source_url: https://arxiv.org/abs/2503.17928
tags:
- data
- bias
- arxiv
- language
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles modality bias in multimodal large language models,
  where models over-rely on a single input modality (e.g., language or vision), leading
  to incorrect responses or irrelevant details. To address this, the authors propose
  a method that uses preference optimization with a custom dataset (RLAIF-V-Bias)
  and a noise-aware loss (NaPO).
---

# Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization

## Quick Facts
- arXiv ID: 2503.17928
- Source URL: https://arxiv.org/abs/2503.17928
- Authors: Zefeng Zhang; Hengzhu Tang; Jiawei Sheng; Zhenyu Zhang; Yiming Ren; Zhenyang Li; Dawei Yin; Duohe Ma; Tingwen Liu
- Reference count: 40
- Key outcome: Method reduces modality bias by 19.5% and improves hallucination benchmark performance through preference optimization with noise-aware loss

## Executive Summary
This paper addresses the critical issue of modality bias in multimodal large language models, where models over-rely on either language or vision, leading to incorrect responses. The authors propose a comprehensive approach combining a novel dataset (RLAIF-V-Bias) with a noise-aware preference optimization method (NaPO). The dataset is constructed by systematically perturbing modalities to create biased responses, while NaPO dynamically adjusts its sensitivity to noise during training by combining binary cross-entropy with noise-robust mean absolute error. Experiments demonstrate significant improvements in reducing modality bias and enhancing performance on hallucination benchmarks compared to standard preference optimization approaches.

## Method Summary
The authors tackle modality bias through a two-pronged approach. First, they construct the RLAIF-V-Bias dataset by starting with preference data from RLAIF-V, then generating biased responses through modality perturbation - randomly masking language or visual tokens and using an LLM to fill in the masked content. This creates pairs of biased responses (over-relying on one modality) and a reference response (balanced). Second, they introduce Noise-aware Preference Optimization (NaPO), a loss function that combines standard binary cross-entropy with a noise-robust mean absolute error term. NaPO dynamically adjusts a noise sensitivity coefficient $q$ during training based on log-probability margins, allowing it to better handle the noisy labels inherent in the biased dataset. The model is fine-tuned using this approach, showing improved balance between modalities.

## Key Results
- 19.5% reduction in modality bias compared to baseline models
- Improved performance on hallucination benchmarks, outperforming standard preference optimization methods
- Ablation studies demonstrate the effectiveness of both the RLAIF-V-Bias dataset and NaPO loss function

## Why This Works (Mechanism)
The method works by explicitly training the model to recognize and correct for its tendency to over-rely on specific modalities. By creating a dataset where responses systematically over-emphasize either language or vision, the model learns to identify when it's falling into these biased patterns. The NaPO loss function is crucial because it's designed to be robust to the noisy labels that arise when comparing biased responses - some comparisons may not be clearly better or worse, just differently biased. By dynamically adjusting its sensitivity to noise based on the training data's characteristics, NaPO can effectively learn from this challenging dataset without being misled by ambiguous comparisons.

## Foundational Learning
- **Multimodal alignment**: Why needed - MLLMs must integrate information from different modalities coherently; Quick check - Can the model answer questions requiring both visual and textual information?
- **Preference optimization**: Why needed - Standard supervised learning doesn't capture human preferences for balanced responses; Quick check - Does the model prefer balanced over biased responses in pairwise comparisons?
- **Noise-robust learning**: Why needed - Dataset contains ambiguous comparisons that could mislead standard loss functions; Quick check - Does the model maintain performance when trained on noisy labels?
- **Modality perturbation**: Why needed - Creating synthetic bias requires controlled manipulation of input representations; Quick check - Can the model recover correct information after targeted modality masking?
- **Dynamic weighting**: Why needed - Different training examples have varying levels of label noise; Quick check - Does the model adapt its learning rate based on confidence in comparisons?
- **Cross-entropy vs MAE**: Why needed - Standard cross-entropy is sensitive to label noise in preference data; Quick check - Does combining both losses improve robustness?

## Architecture Onboarding

**Component Map**
Input Image + Text -> Multimodal Encoder -> Cross-Attention Layers -> Language Decoder -> Output Text
↑
RLAIF-V-Bias Dataset (perturbed responses)
↑
NaPO Loss Function (dynamic weighting)

**Critical Path**
1. Multimodal encoder processes both image and text inputs
2. Cross-attention layers integrate visual and textual features
3. Language decoder generates responses
4. NaPO loss compares generated responses against biased dataset
5. Dynamic weight adjustment based on log-probability margins

**Design Tradeoffs**
- Dataset size vs quality: Larger datasets may capture more bias patterns but introduce more noise
- Static vs dynamic weighting: Fixed weights are simpler but may underperform on heterogeneous noise
- Model capacity vs efficiency: Larger models can capture more complex modality interactions but require more resources
- Synthetic vs real bias: Synthetic bias is controllable but may not reflect all real-world scenarios

**Failure Signatures**
- Model collapse when dynamic weights are applied to non-biased data
- Over-correction leading to under-reliance on one modality
- Sensitivity to hyperparameter choices in NaPO loss
- Performance degradation on tasks requiring strong modality specialization

**3 First Experiments**
1. Ablation study: Replace NaPO with standard DPO while keeping RLAIF-V-Bias dataset
2. Cross-dataset evaluation: Test debiased model on datasets not seen during training
3. Modality-specific testing: Evaluate performance on tasks requiring strong reliance on either vision or language

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Is modality bias universally harmful to MLLM performance, or does it offer latent benefits in specific contexts?
- Basis in paper: The conclusion explicitly lists "whether bias is always harmful" as a lingering question, noting the authors focused on mitigation without exploring potential utility.
- Why unresolved: The paper's methodology is designed to minimize bias, treating it strictly as an error source rather than investigating scenarios where relying on language priors might ensure robustness against corrupted visual data.
- What evidence would resolve it: Experiments comparing debiased models against biased baselines in adversarial or low-information visual environments to identify if bias ever improves factual accuracy or safety.

### Open Question 2
- Question: Can NaPO effectively generalize to handle noise in LLM-synthesized preference data derived from methods other than the specific perturbation strategy used in RLAIF-V-Bias?
- Basis in paper: The conclusion explicitly asks "whether NaPO can robustly handle noise originating from LLM-synthesized data in broader contexts."
- Why unresolved: The noise robustness coefficient ($q$) and dynamic weighting are tuned based on log-probability margin observations specific to the authors' perturbation-based negative sampling method.
- What evidence would resolve it: Benchmarking NaPO on external, diverse preference datasets (e.g., standard RLHF corpora) that contain different types of distributional noise unrelated to modality masking.

### Open Question 3
- Question: Under what specific data conditions does the dynamic weight balancing mechanism ($\gamma_i$) cause training instability or model collapse?
- Basis in paper: Appendix A.2 notes that replacing DPO with NaPO while using dynamic weights ($\gamma_i$) caused the model to collapse, leading the authors to state this mechanism "may not be suitable for all scenarios."
- Why unresolved: The paper leaves the "detail discussion of these issues for future work" without explaining why the dynamic weighting fails on the original RLAIF-V data but succeeds on the Bias-augmented data.
- What evidence would resolve it: A theoretical or empirical analysis of how the dynamic weighting interacts with datasets lacking the distinct margin distributions found in biased data, potentially causing gradient conflicts.

## Limitations
- Narrow scope of evaluation, focusing only on specific hallucination benchmarks rather than diverse multimodal tasks
- Synthetic dataset construction may not fully capture complexity of real-world modality conflicts
- Additional hyperparameters in NaPO loss function may affect reproducibility and scalability
- Claims about real-world effectiveness not directly supported by presented experiments

## Confidence
High confidence: Experimental methodology and results on reported benchmarks appear sound, with clear quantitative improvements over baseline methods supported by ablation studies.

Medium confidence: The 19.5% reduction in modality bias is specific to the evaluation setup and may not generalize across different model architectures or tasks. Hallucination detection improvements need validation in broader scenarios.

Low confidence: Claims about real-world application effectiveness are not directly supported by the controlled experiments with synthetic data.

## Next Checks
1. Evaluate the method across diverse multimodal tasks beyond hallucination benchmarks, including visual question answering, image captioning, and cross-modal retrieval to assess generalizability.

2. Conduct human evaluation studies to verify that reduced modality bias translates to more accurate and useful responses in practical applications, comparing human preferences between debiased and baseline models.

3. Test the method's performance with different multimodal model architectures and sizes to determine if the approach scales effectively and remains robust across various model configurations.