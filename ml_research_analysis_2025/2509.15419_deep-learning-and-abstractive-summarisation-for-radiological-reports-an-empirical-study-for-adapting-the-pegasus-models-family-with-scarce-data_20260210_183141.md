---
ver: rpa2
title: 'Deep learning and abstractive summarisation for radiological reports: an empirical
  study for adapting the PEGASUS models'' family with scarce data'
arxiv_id: '2509.15419'
source_url: https://arxiv.org/abs/2509.15419
tags:
- training
- available
- https
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated fine-tuning abstractive summarization models
  for radiological reports, focusing on PEGASUS and PEGASUS-X architectures. The research
  used a medium-sized public dataset of X-ray reports, fine-tuning models with varying
  training set sizes (10%, 50%, 100%) and evaluating performance using multiple lexical
  and semantic metrics.
---

# Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data

## Quick Facts
- arXiv ID: 2509.15419
- Source URL: https://arxiv.org/abs/2509.15419
- Reference count: 40
- Fine-tuned PEGASUS models achieved best ROUGE-1 score of 0.6323 for xsum variant on chest X-ray report summarization

## Executive Summary
This study fine-tuned PEGASUS and PEGASUS-X models for abstractive summarization of radiological reports, using a medium-sized public dataset of X-ray findings mapped to impressions. The research investigated how training set size (10%, 50%, 100%) and model checkpoint choice affect performance, while monitoring validation metrics throughout training. Key findings include peak-drop-recovery behavior during training, where models initially overfit to simple patterns before recovering better generalization, and the observation that larger checkpoints can underperform smaller ones when fine-tuning data is scarce. The study emphasizes the importance of monitoring full training history to identify optimal checkpoints and warns against early stopping based on initial metric peaks.

## Method Summary
The research used the Indiana University Chest X-ray collection, preprocessing text pairs of "Findings" and "Impressions" while filtering out outliers based on token length Mahalanobis distance. Models were fine-tuned with full-weight updates for 300 epochs using AdamW (learning rate 2e-5, weight decay 0.01) and evaluated every epoch. Four checkpoints were tested: PEGASUS-large, PEGASUS-xsum, PEGASUS-X-base, and PEGASUS-X-large, with batch sizes adjusted for each. Truncation/padding lengths were set at 1.33× the max token length of the filtered dataset. Validation metrics included lexical measures (ROUGE-1/2, BLEU, METEOR) and semantic measures (BERTScore-recall with BioClinical-ModernBERT-base), with the goal of identifying the recovery phase after peak-drop behavior.

## Key Results
- PEGASUS-xsum achieved the best ROUGE-1 score of 0.6323 with 100% training data
- PEGASUS-X-base achieved ROUGE-1 score of 0.6505 with 100% training data
- PEGASUS-X-large underperformed smaller checkpoints due to overfitting and pretraining misalignment
- Models exhibited peak-drop-recovery behavior: early peak (simple patterns), drop (forgetting), then recovery (better generalization)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning exhibits epoch-wise double descent with peak-drop-recovery behavior under data scarcity
- Mechanism: With limited training variance, models first learn dominant patterns (negated diagnoses → early peak), then overfit to training noise (degradation → jibberish outputs), and eventually recover as optimization finds better generalizing solutions with continued training beyond the interpolation threshold.
- Core assumption: The three-phase pattern generalizes beyond encoder-decoder architectures; the paper notes similar phenomena across transformer families but this specific dataset is medium-sized and single-domain.
- Evidence anchors:
  - [abstract] "PEGASUS exhibited different phases, which can be related to epoch-wise double-descent, or peak-drop-recovery behaviour."
  - [section 4.1] "During the early-peaks, the models predicted either negated diagnoses... or empty strings. Then, during the forgetting-phase, the models produced either negated diagnoses or jibberish... both checkpoints manifested a third, new, stationary phase... wherein the metric scores surpassed those during the early peak."
  - [corpus] Weak/no direct corpus support; one neighbor mentions BigBird-PEGASUS token limits but not double descent.
- Break condition: Early stopping based on first peak will miss recovery phase; aggressive early stopping is explicitly warned against.

### Mechanism 2
- Claim: Larger checkpoints can hurt performance when fine-tuning data is scarce and pretraining objectives misalign
- Mechanism: PEGASUS-X-large (512M params) underperformed PEGASUS-X-base (257M params) despite more capacity because higher expressivity + limited fine-tuning variance + mismatched pretraining (long documents vs. short radiology sentences) led to unstable optimization and mixed-domain outputs ("Midfielder of Hungary's Ferencvarous" + "right upper lobe pneumonia").
- Core assumption: The detriment is primarily from parameter count and pretraining mismatch; other architectural differences between PEGASUS and PEGASUS-X (e.g., long-input attention mechanisms) may contribute but were not ablated.
- Evidence anchors:
  - [abstract] "For PEGASUS-X, we found that using a larger checkpoint led to a performance detriment."
  - [section 4.2] "The large checkpoint exhibited... jagged [training history], which is a typical symptom of overfitting... the model was unable to understand the task, and the outputs were combinations of the pre-training datasets and the fine-tuning data."
  - [corpus] Weak/no direct corpus support for this specific finding.
- Break condition: If your fine-tuning data is short and pretraining was for long documents, prefer smaller checkpoints or verify pretraining alignment before scaling.

### Mechanism 3
- Claim: Monitoring validation metrics across the full training history is necessary to identify optimal checkpoints under data scarcity
- Mechanism: Because ROUGE-1, BERTScore-recall, METEOR, and BLEU exhibit non-monotonic trajectories with multiple local optima, selecting checkpoints based on single-metric early peaks yields deceptive performance; full-history monitoring reveals when models transition from spurious pattern matching to genuine generalization.
- Core assumption: Validation set is representative and fixed-size (80/10/10 split used); metric trajectories correlate with qualitative output improvements.
- Evidence anchors:
  - [abstract] "monitored the models' performances with lexical and semantic metrics during the training history on the fixed-size validation set."
  - [section 4.1 Table 2] Binary classification analysis shows early-peak precision/recall (0.62/0.94) is misleading compared to generalization phase (0.75–0.81/0.80–0.86).
  - [corpus] Neighbor papers emphasize benchmarking and domain adaptation but not training-history monitoring.
- Break condition: If validation distribution diverges from test or production, checkpoint selection may not transfer.

## Foundational Learning

- Concept: Encoder-decoder information bottleneck
  - Why needed here: PEGASUS/PEGASUS-X use this architecture for seq2seq summarization; the bottleneck enforces compression from findings to impressions, making them more compute-efficient than decoder-only LLMs for this task.
  - Quick check question: Can you explain why encoder-decoder architectures are preferred over decoder-only models for fixed-length summarization tasks?

- Concept: Double descent in overparameterized models
  - Why needed here: Explains why test performance can improve after worsening, challenging classical bias-variance tradeoff assumptions and justifying extended training despite apparent overfitting.
  - Quick check question: What is the interpolation threshold, and why does crossing it sometimes improve generalization?

- Concept: Catastrophic forgetting
  - Why needed here: Domain adaptation can overwrite pre-trained knowledge; the paper explicitly connects peak-drop behavior to forgetting, where models temporarily lose generalization capacity.
  - Quick check question: When fine-tuning a pre-trained model on a small domain-specific dataset, what signals indicate that forgetting is occurring?

## Architecture Onboarding

- Component map: PEGASUS encoder-decoder transformer with SentencePiece tokenization (BPE/unigram) → fine-tuned on findings→impressions pairs → evaluated via lexical (ROUGE, BLEU, METEOR) and semantic (BERTScore-recall with BioClinical-ModernBERT-base) metrics.
- Critical path: (1) Choose checkpoint aligned with summary conciseness (xsum favored short summaries over large in this study), (2) Set truncation/padding lengths at 1.33× word-tokenized max to handle subword expansion, (3) Train with full-weight fine-tuning while logging validation metrics every epoch, (4) Identify recovery-phase checkpoint rather than early-peak checkpoint.
- Design tradeoffs: PEGASUS-X handles longer inputs (up to 16384 tokens) but misaligns with short radiology texts; larger checkpoints (568M/512M params) increase expressivity but risk overfitting under data scarcity vs. smaller checkpoints (272M/257M params).
- Failure signatures: (1) Early-peak outputs are only negated diagnoses or empty strings, (2) Forgetting-phase outputs are repetitive jibberish ("Arrested Arrested..."), (3) Overfitted large-checkpoint outputs blend pretraining and fine-tuning domains ("Everton defenders are grossly clear of left lung effusion").
- First 3 experiments:
  1. Replicate PEGASUS-xsum fine-tuning on 100% training data with per-epoch validation logging; plot ROUGE-1 trajectory to confirm three-phase pattern and identify recovery onset epoch (32–65 in paper).
  2. Ablate training-set size (10%/50%/100%) to observe whether smaller sets exhibit only peak-drop without recovery, establishing data-scarcity thresholds for your domain.
  3. Compare PEGASUS-X-base vs. PEGASUS-X-large on the same split to verify whether larger checkpoints degrade under your data constraints and pretraining-objective alignment.

## Open Questions the Paper Calls Out

- **Question:** How do the best fine-tuned models perform on the test split when evaluated by clinical experts?
  - Basis in paper: [explicit] The authors state they "defer to future work to compare, with the help of trained clinicians, the most representative fine-tuned models on the test split."
  - Why unresolved: The current study relies solely on automatic metrics (ROUGE, BERTScore) and validation set monitoring.
  - What evidence would resolve it: A human evaluation study using clinicians to assess the clinical accuracy of the generated summaries.

- **Question:** Does the observed "peak-drop-recovery" behavior generalize to other transformer architectures and larger datasets?
  - Basis in paper: [explicit] The authors acknowledge the limitation that the "investigation focused solely on one specific model family" and a dataset of "limited size."
  - Why unresolved: It is unclear if the double-descent phenomenon is specific to PEGASUS/PEGASUS-X or the small data scale.
  - What evidence would resolve it: Replicating fine-tuning experiments on varied architectures (e.g., BART, T5) with larger medical corpora.

- **Question:** How do neural metrics like COMET and BLEURT evolve during the training phases?
  - Basis in paper: [explicit] The paper explicitly defers "the study of the evolution of other metrics, such as COMET [49] and BLEURT [50], to future studies."
  - Why unresolved: The study tracked lexical and embedding metrics but omitted these modern neural evaluation metrics.
  - What evidence would resolve it: Tracking COMET and BLEURT scores alongside ROUGE throughout the training history.

## Limitations

- Single dataset constraint: Study limited to Indiana University Chest X-ray collection, may not generalize to other medical domains or languages
- Single architecture focus: Investigation focused solely on PEGASUS/PEGASUS-X family without comparison to other transformer architectures
- Computational expense: 300-epoch training duration required to observe recovery phases is resource-intensive and may not be practical

## Confidence

**High Confidence:** The observation that PEGASUS-xsum achieved the best ROUGE-1 score (0.6323) and that PEGASUS-X-large underperformed PEGASUS-X-base due to overfitting and pretraining misalignment. These claims are directly supported by training history plots and metric tables with minimal interpretive distance.

**Medium Confidence:** The mechanism explaining peak-drop-recovery as epoch-wise double descent. While the three-phase pattern is clearly documented, the paper does not explicitly test whether this constitutes formal double descent or prove that the recovery phase represents true generalization rather than memorization of a different subset of training patterns.

**Low Confidence:** The generalizability of these findings across medical domains and languages. The study focuses exclusively on English chest X-ray reports from a single institution, and the corpus analysis reveals minimal related work for comparative validation.

## Next Checks

1. **Cross-domain validation:** Replicate the full training history monitoring approach on a different medical domain (e.g., pathology reports or clinical notes) to test whether peak-drop-recovery behavior generalizes beyond chest X-ray findings→impressions summarization.

2. **Ablation of architectural components:** Conduct controlled experiments comparing PEGASUS and PEGASUS-X variants that differ only in parameter count (holding all other architectural features constant) to isolate whether the performance degradation is truly due to expressivity or other architectural mismatches.

3. **Early-stopping optimization validation:** Systematically test whether the recovery-phase checkpoint consistently outperforms early-peak checkpoints across multiple random seeds and dataset splits, quantifying the trade-off between computational cost (300 epochs) and performance gains from waiting for recovery.