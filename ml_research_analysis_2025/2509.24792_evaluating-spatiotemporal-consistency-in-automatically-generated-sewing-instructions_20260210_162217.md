---
ver: rpa2
title: Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions
arxiv_id: '2509.24792'
source_url: https://arxiv.org/abs/2509.24792
tags:
- instructions
- each
- pieces
- evaluation
- piece
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel tree-based evaluation metric designed
  to better capture spatiotemporal correctness in automatically generated step-by-step
  assembly instructions, specifically focusing on sewing instructions. The method
  constructs assembly trees from the generated instructions and compares them against
  manually-annotated gold trees using F1 score, providing a measure of the instructions'
  spatial and temporal consistency.
---

# Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions

## Quick Facts
- **arXiv ID**: 2509.24792
- **Source URL**: https://arxiv.org/abs/2509.24792
- **Reference count**: 9
- **Primary result**: Tree-based evaluation metric achieves r = -0.599 correlation with manual error counts, significantly outperforming traditional text similarity metrics for assembly instruction quality

## Executive Summary
This paper introduces a novel tree-based evaluation metric designed to capture spatiotemporal correctness in automatically generated step-by-step assembly instructions, with a focus on sewing instructions. The method constructs assembly trees from generated instructions and compares them against manually-annotated gold trees using F1 score, providing a measure of spatial and temporal consistency that traditional metrics like BLEU, ROUGE-L, and BERT-Score cannot capture. The tree-based metric demonstrates significantly stronger correlation with manual error annotations (-0.599) and superior sensitivity to instruction quality compared to existing evaluation approaches.

## Method Summary
The approach converts procedural sewing instructions into assembly trees where leaf nodes represent original pattern pieces, intermediate nodes represent partial assemblies, and the root represents the completed garment. Each assembly step creates a parent node with edges to its daughter components, encoding both spatial connections and temporal order. The method uses Context-Free Grammar rules to generate all valid assembly sequences for each pattern, allowing flexible evaluation without penalizing legitimate variation. Evaluation computes subtree-level F1 scores between the predicted tree and all valid gold trees, returning the maximum score to accommodate multiple valid assembly orders.

## Key Results
- Tree-based metric achieves r = -0.599 correlation with manual error counts (p < .001), compared to near-zero or negative correlations for BLEU, ROUGE-L, and BERT-Score
- When instruction steps are randomly permuted, tree score drops 50% while traditional metrics fail to detect meaningful differences
- Human evaluation shows tree metric better aligns with perceived instruction quality, particularly for step coherence (r = 0.23), compared to text similarity metrics

## Why This Works (Mechanism)

### Mechanism 1
Converting procedural instructions into assembly trees enables evaluation of spatiotemporal correctness that surface-level text similarity metrics cannot capture. The system parses each instruction step to extract referenced piece labels, then constructs a tree where leaf nodes represent original pattern pieces, intermediate nodes represent partial assemblies, and the root represents the completed garment. Each assembly operation creates a parent node with edges to its daughter components. The tree explicitly encodes both which pieces connect (spatial) and the order of connections (temporal).

### Mechanism 2
Accommodating multiple valid assembly orders through CFG-generated gold tree sets enables flexible evaluation without penalizing legitimate variation. Annotators encode valid assembly sequences as Context-Free Grammar rules rather than enumerating all possible trees. The CFG generates the complete set of valid assembly trees for each pattern. During evaluation, the extracted tree is compared against all gold trees, and the maximum F1 score is retained. This allows the metric to reward any valid assembly strategy rather than requiring exact match to a single reference.

### Mechanism 3
Subtree-level F1 comparison provides granular error detection that correlates with manual error annotation and human quality perception. Rather than requiring exact tree match, the metric computes F1 over local subtrees—each non-leaf node and its immediate daughters forms a comparison unit. This captures whether individual assembly steps are correct even if the overall sequence has errors. The F1 formulation rewards partial credit: correct subtrees increase recall, while spurious subtrees reduce precision.

## Foundational Learning

- **Context-Free Grammars (CFGs) for sequence generation**: Understanding how gold tree sets are generated and why they capture all valid assembly orders. Quick check: Given production rules S → AB | BA and A → a, B → b, what strings does this CFG generate?
- **Tree data structures and subtree isomorphism**: The metric compares trees at the subtree level; understanding what constitutes a subtree match is essential. Quick check: For a tree with root R and children A, B where A has children a1, a2—what are all the subtrees considered for F1 computation?
- **Precision, Recall, and F1 for structured outputs**: The scoring mechanism uses F1 over subtree sets rather than text tokens. Quick check: If a predicted tree has 10 subtrees, the gold tree has 12 subtrees, and 7 match exactly, what are precision, recall, and F1?

## Architecture Onboarding

- **Component map**: Pattern annotation → CFG rules → gold tree generation → stored as G(P) for each pattern → instruction parsing → piece extraction → tree construction → T(I) output → scoring engine → computes subtree sets from T(I) and each g∈G(P) → returns max F1
- **Critical path**: 1) Pattern annotation (one-time): Manual CFG rule creation → gold tree generation; 2) Per-instruction evaluation: Generated text → piece extraction → tree construction → F1 computation against all gold trees → return maximum
- **Design tradeoffs**: Binary/unary branching constraint simplifies tree comparison but cannot represent multi-piece attachments in single step; node naming inheritance loses detailed self-attachment history but simplifies comparison; maximum F1 selection handles multi-valid-order flexibility but increases computation; corpus evidence gap shows no validation beyond sewing domain
- **Failure signatures**: Extraction failure (LLM fails to identify piece labels → empty or malformed subtrees → very low scores); reference ambiguity (instructions say "attach the front piece" when front has already been assembled → creates incorrect subtree without proper intermediate resolution); forest output (missing assembly steps → T(I) is disconnected forest rather than tree); early error cascades (mistake in step 2 propagates through all subsequent node names → single early error causes multiple subtree mismatches)
- **First 3 experiments**: 1) Validate extraction pipeline: Manually inspect piece extraction accuracy on 10 generated instruction sets; if extraction accuracy < 90%, tune piece-extraction prompt before proceeding; 2) Correlation sanity check: Generate instructions using baseline and few-shot prompts (n=20 each), compute all four metrics, verify tree score shows stronger negative correlation with manual error counts than BLEU/ROUGE/BERT-Score; 3) Permutation robustness test: Take baseline-generated instructions, randomly permute steps, confirm tree score drops >40% while traditional metrics show <10% change

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed tree-based evaluation framework be generalized to other spatiotemporal instruction tasks, such as furniture assembly or puzzle solving? The authors explicitly state in the "Limitations" section that the ability to generalize the CFG-based approach to other domains like Minecraft or furniture building "remains to be explored in future work." The current study restricts its validation to the domain of sewing instructions, and it is unclear if the tree extraction and comparison logic transfer directly to objects with different physical constraints.

### Open Question 2
Can training language models on the structured assembly tree representations generated by this metric improve their spatiotemporal reasoning capabilities? The authors suggest in the "Limitations" section that "future work could investigate how training on these representation might improve the performance of language models." The paper focuses on the metric as an evaluation tool; it does not experiment with using the generated trees as a supervision signal or intermediate representation for model training.

### Open Question 3
How can the tree-based evaluation framework be expanded to accommodate complex garments that require multiple copies of pieces or varied assembly strategies? The authors note in the "Limitations" section under "Garment Complexity" that the current framework is limited to simple garments and "must be expanded" to handle more complex patterns. The current node naming conventions and binary branching constraints break down when handling patterns with multiple left/right copies or complex sub-assemblies.

### Open Question 4
Can the evaluation metric be refined to differentiate between early catastrophic errors and correct subsequent steps? The authors identify a limitation in "Node Naming" where errors propagating up the tree affect node names, causing the metric to "assign a similar score to instructions with frequent early mistakes, regardless of whether the following steps are correct." Because the tree structure relies on the cumulative state of the assembly, a mistake in step 1 theoretically invalidates the logic of all following steps, making it difficult to credit the model for correct local reasoning after an initial error.

## Limitations
- Manual CFG annotation for each new pattern type creates scalability bottleneck for expanding to new garment types
- Binary/unary branching constraint may not capture all valid assembly strategies, particularly for multi-piece attachments
- Subtree-level F1 comparison may miss semantic errors where instructions describe physically impossible operations
- No validation of generalizability beyond sewing domain to furniture assembly, Minecraft, or puzzle solving

## Confidence
- **High confidence**: Tree score demonstrates superior correlation with manual error annotations compared to traditional metrics (r = -0.599 vs near-zero for BLEU/ROUGE/BERT-Score)
- **Medium confidence**: Tree metric correctly identifies nonsensical permuted instructions (50% score drop) while traditional metrics fail to detect meaningful differences
- **Medium confidence**: Human evaluation shows tree metric better aligns with perceived instruction quality, particularly for step coherence (r = 0.23)

## Next Checks
1. Apply the tree-based metric to non-sewing assembly tasks (furniture assembly instructions, Minecraft construction guides, puzzle assembly) to test generalizability beyond garment construction
2. Conduct ablation studies to determine whether the metric's high correlation stems primarily from spatial errors, temporal errors, or both by systematically introducing different error types in generated instructions
3. Measure time and expertise required to create CFG rules for new patterns, and explore automated or semi-automated annotation approaches to reduce manual effort