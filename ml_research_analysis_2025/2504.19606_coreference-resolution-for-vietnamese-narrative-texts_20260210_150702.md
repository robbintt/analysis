---
ver: rpa2
title: Coreference Resolution for Vietnamese Narrative Texts
arxiv_id: '2504.19606'
source_url: https://arxiv.org/abs/2504.19606
tags:
- coreference
- resolution
- vietnamese
- llms
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses coreference resolution for Vietnamese, a low-resource
  language with limited annotated datasets. The authors constructed a dataset from
  VnExpress narrative texts, established detailed annotation guidelines, and evaluated
  large language models (GPT-3.5-Turbo and GPT-4) using few-shot prompting.
---

# Coreference Resolution for Vietnamese Narrative Texts

## Quick Facts
- arXiv ID: 2504.19606
- Source URL: https://arxiv.org/abs/2504.19606
- Reference count: 5
- Primary result: GPT-4 achieved 0.735 CoNLL F1 for Vietnamese coreference resolution, significantly outperforming GPT-3.5-Turbo's 0.478

## Executive Summary
This paper addresses coreference resolution for Vietnamese, a low-resource language with limited annotated datasets. The authors constructed a dataset from VnExpress narrative texts, established detailed annotation guidelines, and evaluated large language models (GPT-3.5-Turbo and GPT-4) using few-shot prompting. Results showed GPT-4 significantly outperformed GPT-3.5-Turbo with a CoNLL F1 score of 0.735 versus 0.478. GPT-4 also achieved higher MUC (0.858 vs. 0.640), B-Cubed (0.723 vs. 0.474), and CEAF ϕ (0.625 vs. 0.321) F1 scores. The study demonstrates GPT-4's superior consistency and reliability for Vietnamese coreference resolution, with fewer post-processing needs compared to GPT-3.5-Turbo. Future work includes expanding the dataset, domain adaptation, and integrating coreference with other NLP tasks.

## Method Summary
The authors built a Vietnamese coreference resolution dataset from VnExpress narrative texts, annotating 266 documents with detailed guidelines focusing on human entities only. They used SACR annotation tool and established rules for entity extraction (excluding adjectives, handling nested mentions with "của" possessive). For evaluation, they employed few-shot prompting with 3 annotated examples, formatting inputs with indexed mentions and outputs as cluster tuples. Both GPT-3.5-Turbo and GPT-4 were evaluated using standard coreference metrics (MUC, B-Cubed, CEAF ϕ), with results averaged for CoNLL F1 score.

## Key Results
- GPT-4 achieved CoNLL F1 score of 0.735, significantly outperforming GPT-3.5-Turbo's 0.478
- GPT-4 demonstrated superior performance across all metrics: MUC (0.858 vs. 0.640), B-Cubed (0.723 vs. 0.474), and CEAF ϕ (0.625 vs. 0.321)
- GPT-3.5-Turbo frequently returned annotated full text instead of tuple format and merged distinct entities, requiring more post-processing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot prompting enables LLMs to perform coreference resolution on low-resource languages without task-specific training.
- **Mechanism:** By providing 3 annotated examples covering key annotation rules (people mentions, groups, nested possessives), the prompt establishes both the task schema and expected output format. The LLM leverages prior linguistic knowledge to generalize from these examples to unseen Vietnamese narrative texts.
- **Core assumption:** The LLM has sufficient multilingual understanding to transfer coreference patterns from the examples to new texts, even with limited Vietnamese-specific pretraining.
- **Evidence anchors:**
  - [abstract] "evaluated the performance of large language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset"
  - [Section 4, Results] "GPT-4 achieved a CoNLL F1 score of 0.735, showing a significant improvement over GPT-3.5-Turbo, which scored 0.478"
  - [corpus] Related work (BOOKCOREF, LegalCore) confirms coreference resolution effectiveness varies substantially by document scale and domain, but limited direct evidence for few-shot mechanisms in Vietnamese specifically.
- **Break condition:** When annotation rules are insufficiently covered in examples (e.g., complex noun phrases, ambiguous pronouns), performance degrades.

### Mechanism 2
- **Claim:** Tuple-based cluster output format improves evaluation efficiency and reduces LLM output errors.
- **Mechanism:** Instead of returning full annotated text, the LLM outputs compact tuples like `[(1, 5), (2, 3, 6)]` representing entity clusters by tag indices. This reduces token generation, speeds inference, and enables direct comparison with gold_clusters without parsing overhead.
- **Core assumption:** LLMs can reliably map indexed mentions to cluster tuples without introducing formatting errors or hallucinated indices.
- **Evidence anchors:**
  - [Section 3] "By formatting the output as tuples representing tag_indices, we minimize the number of tokens the LLM needs to generate"
  - [Section 4.1] "GPT-3.5-Turbo sometimes returned an unnecessarily annotated full text. In contrast, this problem occurred much less frequently with GPT-4"
  - [corpus] No direct corpus evidence on tuple formatting efficacy; this appears to be a design choice by the authors.
- **Break condition:** If the LLM produces malformed tuples or references non-existent indices, post-processing or rejection is required.

### Mechanism 3
- **Claim:** Advanced LLM architectures (GPT-4 vs. GPT-3.5-Turbo) yield higher coreference accuracy through improved contextual reasoning.
- **Mechanism:** GPT-4's larger capacity and training data enable better differentiation of entities, particularly in cases requiring semantic understanding (e.g., distinguishing speaker from husband in pronoun-heavy text). Case studies show GPT-4 correctly separates clusters that GPT-3.5-Turbo merges.
- **Core assumption:** Performance gains are attributable to architectural improvements rather than prompt sensitivity or random variation.
- **Evidence anchors:**
  - [Section 4.3, Case Study] "GPT-3.5-Turbo: Merged references to the speaker with unrelated entities such as the husband, resulting in a single, overly broad cluster"
  - [Section 4.2] "CEAF ϕ metric...showed that GPT-4 performed exceptionally well with an F1 score of 0.858, compared to 0.640 for GPT-3.5-Turbo"
  - [corpus] Neighbor papers (e.g., "Efficient Seq2seq Coreference Resolution") suggest model architecture significantly impacts coreference quality, supporting this inference.
- **Break condition:** Assumption: gains may not generalize to other Vietnamese genres (legal, medical) or extremely long documents beyond the ~450 token average in this dataset.

## Foundational Learning

- **Concept:** Coreference Resolution
  - **Why needed here:** Understanding that the task involves grouping all mentions referring to the same entity (e.g., "My father," "he," "Dad" → one cluster) is foundational to interpreting results.
  - **Quick check question:** Given "[Anna]#1 called [her mother]#2. [She]#3 was happy," which indices should cluster together?

- **Concept:** Few-shot Prompting
  - **Why needed here:** The paper relies on providing 3 examples to guide LLM behavior without fine-tuning.
  - **Quick check question:** How does few-shot differ from zero-shot and fine-tuning approaches?

- **Concept:** CoNLL F1 / MUC / B-Cubed / CEAFϕ Metrics
  - **Why needed here:** Evaluation uses composite CoNLL F1; understanding what each metric captures (link-level vs. mention-level vs. entity-alignment) helps diagnose model weaknesses.
  - **Quick check question:** If a model correctly links mentions but creates extra spurious clusters, which metric would penalize this most?

## Architecture Onboarding

- **Component map:** VnExpress narrative texts → Manual annotation (SACR tool, entity guidelines) → Indexed text formatting → Few-shot prompt construction (3 examples) → LLM API calls (GPT-3.5/GPT-4) → Output parsing → Metric computation (MUC, B-Cubed, CEAFϕ, CoNLL F1)

- **Critical path:** 1. Annotation guideline quality (determines gold standard reliability) 2. Prompt design (coverage of entity types and nested structures) 3. Output format enforcement (tuple consistency)

- **Design tradeoffs:**
  - Few-shot example count (3) vs. API token limits—more examples could improve accuracy but risk truncation
  - Human-only entities vs. full coreference—narrower scope improves annotation consistency but limits downstream utility
  - GPT-4 accuracy vs. cost and latency

- **Failure signatures:**
  - GPT-3.5-Turbo: Returns annotated full text instead of tuples, includes extraneous content, merges distinct entities
  - GPT-4: Rare format violations; residual errors on complex noun phrase extraction (noted in Discussion)
  - Both: Struggle with ambiguous pronoun resolution and implied entity references

- **First 3 experiments:**
  1. Reproduce the evaluation on a 20-document subset to validate your pipeline matches reported CoNLL F1 within ±0.02.
  2. Ablate one few-shot example at a time to measure sensitivity to prompt coverage; expect degradation when nested mention or group examples are removed.
  3. Test domain shift by applying the same prompt to a different Vietnamese text genre (e.g., news headlines) and document CoNLL F1 drop—hypothesis: performance declines on non-narrative structure.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset availability and reproducibility: The 266 Vietnamese narrative texts from VnExpress and their manual annotations are not publicly accessible, creating barriers to exact replication.
- Generalization beyond narrative domain: Results are specific to Vietnamese narrative texts from VnExpress; performance on other Vietnamese genres (legal, news headlines, medical) or longer documents is untested.
- Few-shot example dependency: The paper uses only 3 examples to guide both models, with no ablation studies on example count or content to isolate model architecture effects.

## Confidence
- **High confidence:** GPT-4 significantly outperforms GPT-3.5-Turbo on Vietnamese coreference resolution (CoNLL F1: 0.735 vs. 0.478). This is directly supported by reported metrics and case study analysis.
- **Medium confidence:** Few-shot prompting enables effective zero-shot coreference resolution for low-resource languages. The mechanism is plausible, but direct evidence for Vietnamese is limited; results may depend heavily on example selection and prompt design.
- **Low confidence:** Tuple-based output format consistently reduces LLM errors and improves evaluation efficiency. This is inferred from observed GPT-4 behavior and design rationale, but no ablation or comparison to alternative formats is provided.

## Next Checks
1. Reproduce coreference resolution pipeline: Obtain or construct a comparable Vietnamese narrative dataset, manually annotate 3-5 documents following the paper's entity guidelines, and run the few-shot prompt on GPT-3.5-Turbo and GPT-4. Validate that your CoNLL F1 scores match the reported values (within ±0.02) for a 20-document subset.
2. Test domain shift impact: Apply the same few-shot prompt and evaluation pipeline to a different Vietnamese text genre (e.g., news headlines or legal documents). Measure CoNLL F1 drop to quantify generalization limits; expect performance decline on non-narrative structure.
3. Ablate few-shot examples: Systematically remove one of the three few-shot examples and rerun evaluation. Track CoNLL F1 change to assess sensitivity to prompt coverage, especially for nested mentions and group entities.