---
ver: rpa2
title: OpenAI's Approach to External Red Teaming for AI Models and Systems
arxiv_id: '2503.16431'
source_url: https://arxiv.org/abs/2503.16431
tags:
- teaming
- testing
- systems
- risk
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper outlines OpenAI's approach to external red teaming for
  assessing AI model risks. The authors detail a methodology involving selecting diverse
  domain experts, providing controlled model access, and documenting findings to identify
  novel risks, stress test mitigations, and inform automated evaluations.
---

# OpenAI's Approach to External Red Teaming for AI Models and Systems

## Quick Facts
- arXiv ID: 2503.16431
- Source URL: https://arxiv.org/abs/2503.16431
- Reference count: 40
- Primary result: External red teaming with domain experts helps discover novel AI risks, stress test mitigations, and seed automated evaluations

## Executive Summary
This paper outlines OpenAI's methodology for external red teaming to assess risks in AI models and systems. The approach involves selecting diverse domain experts, providing controlled model access, and documenting findings to identify novel risks, stress test mitigations, and inform automated evaluations. Key design considerations include cohort composition, access levels, and structured documentation. The methodology has successfully identified risks like unauthorized voice generation and informed the development of automated classifiers for harmful content detection.

## Method Summary
OpenAI's external red teaming follows a 4-step campaign design: select red team composition based on prioritized domains through threat modeling, determine appropriate model access levels (early vs. post-mitigation), provide interfaces with specific documentation instructions, and synthesize data from prompt-output pairs to create automated evaluations. The approach leverages domain experts to bring specialized threat models and contextual knowledge unavailable to internal safety teams, generating adversarial prompts that test realistic misuse scenarios.

## Key Results
- External red teaming surfaces risks that internal evaluations miss through domain expert specialization
- Red teaming findings seed the creation of automated evaluations through human-to-AI feedback loops
- Access level timing (early vs. post-mitigation) determines the utility of red teaming efforts for different testing goals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** External red teaming surfaces risks that internal evaluations miss because domain experts bring specialized threat models and contextual knowledge unavailable to generalist safety teams.
- **Mechanism:** External experts with domain-specific expertise apply threat models relevant to their field, generating adversarial prompts that test realistic misuse scenarios. Their independent perspective reduces blind spots from organizational groupthink.
- **Core assumption:** Domain experts can translate their expertise into effective adversarial testing strategies without extensive AI safety training.
- **Evidence anchors:**
  - "collaborating with domain experts to evaluate AI models across various risk areas such as natural sciences, cybersecurity, privacy, and bias"
  - "External red teamers bring valuable context, such as knowledge of regional politics, cultural contexts, or technical fields like law and medicine"
  - Scientists helped evaluate the capabilities and limitations of the o1 family of models in developing biological experiment protocols

### Mechanism 2
- **Claim:** Red teaming data seed-creates automated evaluations through a human-to-AI feedback loop that scales safety testing beyond resource-intensive manual campaigns.
- **Mechanism:** Human red teamers generate prompt-output pairs documenting failures. These become seed datasets. AI models then synthesize variations, creating larger evaluation sets. Classifiers trained on red team data automate future testing.
- **Core assumption:** Patterns in human-discovered failures generalize to unseen cases that AI can synthesize.
- **Evidence anchors:**
  - "describes how red teaming findings can be used to create automated evaluations"
  - "In the case of DALL-E 3, open-ended red teaming uncovered gaps... OpenAI created automated evaluations for selected subdomains with red teamer prompts as seeds. GPT-4 was used to create synthetic data using red team prompts"
  - Related work (ARMs, RedCodeAgent) focuses on automated attacks rather than evaluation synthesis from human data

### Mechanism 3
- **Claim:** Access level timing determines red teaming utility—early access enables capability discovery, while post-mitigation access tests robustness of safety measures.
- **Mechanism:** Unmitigated model access reveals base capabilities and novel risks. Post-mitigation access reveals residual risks and jailbreak vectors. Different testing goals require different access timing decisions.
- **Core assumption:** Organizations can act on early findings before deployment timelines compress decision-making.
- **Evidence anchors:**
  - "Early in the model development process, it may be useful to learn about the model's capabilities prior to any added safety mitigations... Once safety mitigations have been enabled, red teaming efforts may focus on identifying unaddressed residual risks"
  - Figure 2 shows explicit pros/cons matrix for early development, pre-mitigation, post-mitigation, pre-deployment, and post-deployment access
  - "When Search Goes Wrong: Red-Teaming Web-Augmented Large Language Models" confirms timing matters—post-deployment web augmentation introduced new attack surfaces

## Foundational Learning

- **Concept: Threat Modeling**
  - **Why needed here:** The paper explicitly states "Domain prioritization for testing can be informed by threat modeling carried out prior to the red teaming exercise." Without threat modeling, red teaming lacks focus and wastes resources on low-priority areas.
  - **Quick check question:** Can you articulate: "What specific harm, to whom, by what adversary, using what model capability?" for a given risk area?

- **Concept: Adversarial Prompting**
  - **Why needed here:** Red teamers must craft inputs that trigger harmful outputs. Understanding prompt engineering, jailbreak techniques, and circumvention strategies is prerequisite to effective testing.
  - **Quick check question:** Can you describe three distinct techniques for bypassing content filters (e.g., role-playing, encoding, prompt injection)?

- **Concept: Safety Evaluation Design**
  - **Why needed here:** The paper describes converting red team findings into automated evaluations. Understanding classifier design, metric selection, and evaluation validity is necessary for this translation.
  - **Quick check question:** Given a harmful output category, can you specify: what signals to detect, what threshold constitutes failure, and what false positive rate is acceptable?

## Architecture Onboarding

- **Component map:** Model/system version + threat model + access level decision -> Domain expertise mapping -> API access, product UI, or specialized feedback platform -> Prompt-output pairs, risk categorization, severity rating, context notes -> Policy alignment check -> evaluation dataset creation -> automated classifier training -> Risk assessment input, System Card content, automated evaluations

- **Critical path:**
  1. Define threat model and motivating questions (Table 1, Appendix A)
  2. Select red team composition matching prioritized domains
  3. Determine access level based on testing goals (Figure 2 tradeoffs)
  4. Provide interface + instructions + documentation templates
  5. Collect findings → classify by policy relevance
  6. Convert high-value findings into automated evaluation datasets

- **Design tradeoffs:**
  - **Access timing:** Early access = novel risk discovery but findings may obsolete; post-mitigation = tests real deployment but misses base capability assessment
  - **Red team scope:** Open-ended = discovers unknown unknowns but unfocused; structured = covers known risks but misses novel threats
  - **Interface type:** API = programmatic scale but abstracted from user experience; product UI = realistic testing but harder to scale
  - **Disclosure:** Public System Cards build trust but may reveal information hazards

- **Failure signatures:**
  - Findings don't generalize to production because tested wrong model version
  - Red team lacks domain expertise to evaluate sophisticated outputs (paper: "as models become more capable, higher threshold for knowledge humans need")
  - Documentation format too unstructured to convert into evaluations
  - Participant harm from exposure to disturbing content (paper: "required to think like adversaries and interact with harmful content")
  - Information hazard from disclosing jailbreaks before fixes deployed

- **First 3 experiments:**
  1. **Structured domain test:** Select one risk area (e.g., cybersecurity), recruit 2-3 domain experts, provide post-mitigation access via product UI, collect structured documentation. Goal: validate documentation format produces actionable findings.
  2. **Access level comparison:** Run parallel red teaming on same model—cohort A gets pre-mitigation access, cohort B gets post-mitigation. Compare: what risks does each surface? How do findings differ? Goal: validate access timing hypothesis for your deployment context.
  3. **Evaluation synthesis pilot:** Take 50 high-severity findings from experiment 1, use them as seeds for GPT-4 to generate 500 synthetic variations. Train classifier on refusal detection. Test: does classifier catch novel jailbreak attempts? Goal: validate human-to-automated evaluation pipeline before investing at scale.

## Open Questions the Paper Calls Out

- **How can external red teaming processes effectively solicit and incorporate public perspectives on ideal model behavior?**
  - The authors conclude that "additional work is needed to solicit and incorporate public perspectives on ideal model behavior, policies, and other associated decision making processes."
  - Current methodologies prioritize domain experts, potentially missing broader societal consensus on acceptable risks and behaviors.
  - Development and validation of new frameworks that systematically integrate public deliberation into the red teaming and policy-setting lifecycle would resolve this.

- **What constitutes effective, externally specified thresholds and practices for the accountability of risks discovered during red teaming?**
  - The paper states that red teaming "needs to be paired with externally specified thresholds and practices for accountability of discovered risks."
  - While the paper outlines how to discover risks, it acknowledges a gap in standardized governance structures that dictate what must happen once risks are identified.
  - The establishment of industry-wide standards or regulatory frameworks that mandate specific remediation actions when findings exceed defined severity thresholds would resolve this.

- **How does the increasing sophistication of frontier models affect the validity of human red teaming given the rising knowledge threshold required to judge risks?**
  - The "Limitations" section notes that "there will be a higher threshold for knowledge humans need to possess to correctly judge the potential level of risk of outputs" as models improve.
  - If models outpace the expertise of human red teamers, or if the effort required to "jailbreak" them becomes too high, manual testing may lose its effectiveness.
  - Comparative studies measuring the success rate of human experts versus automated systems in identifying risks in highly capable reasoning models would resolve this.

## Limitations
- Limited empirical validation—lacks quantitative data on red teaming effectiveness and false positive rates of synthesized evaluations
- Organizational specificity—approach assumes OpenAI's unique position with access to frontier models and internal safety teams
- Participant well-being—acknowledges potential harm to red teamers but provides minimal detail on mitigation strategies

## Confidence

- **High confidence:** The core mechanism that domain experts can identify novel risks through specialized threat models is well-supported by concrete examples (voice generation, biological protocols)
- **Medium confidence:** The human-to-automated evaluation pipeline is conceptually sound but lacks empirical validation of generalization—only one case study (DALL-E 3) is provided
- **Low confidence:** The scalability of this approach for organizations without OpenAI's resources or model access is questionable, given the "resource intensive" nature and specific infrastructure requirements

## Next Checks
1. **Generalizability test:** Apply the methodology to a non-frontier model (e.g., open-source LLM) and measure whether the same pattern of novel risk discovery holds without proprietary access advantages
2. **Evaluation robustness measurement:** Take red team-derived evaluation datasets and test their persistence across three model versions—does classifier performance degrade predictably or catastrophically?
3. **Participant impact study:** Conduct structured interviews with red teamers post-campaign to quantify psychological effects and identify which aspects of the methodology (documentation burden, content exposure) correlate with negative outcomes