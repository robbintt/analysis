---
ver: rpa2
title: Do We Need Transformers to Play FPS Video Games?
arxiv_id: '2504.17891'
source_url: https://arxiv.org/abs/2504.17891
tags:
- learning
- transformer
- reinforcement
- deep
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates transformer-based architectures for reinforcement
  learning in the VizDoom FPS environment, comparing Deep Transformer Q-Networks (DTQN)
  for online learning and Decision Transformers (DT) for offline learning against
  traditional methods. DTQN, which replaces LSTM layers with transformer decoders,
  and DT, which uses sequence modeling for offline RL, were both found to underperform
  their traditional counterparts.
---

# Do We Need Transformers to Play FPS Video Games?
## Quick Facts
- arXiv ID: 2504.17891
- Source URL: https://arxiv.org/abs/2504.17891
- Reference count: 14
- Primary result: Transformers underperform traditional methods in VizDoom FPS reinforcement learning

## Executive Summary
This study investigates whether transformer architectures provide advantages for reinforcement learning in FPS video games using the VizDoom environment. The researchers compared Deep Transformer Q-Networks (DTQN) for online learning and Decision Transformers (DT) for offline learning against traditional RL methods. Both transformer approaches were found to underperform their traditional counterparts in kill-death ratios and episode rewards. The results suggest that while transformers excel at sequence modeling, they struggle in memory-intensive, partially observable environments requiring strategic decision-making, indicating traditional methods remain more effective for FPS games like Doom.

## Method Summary
The study implemented two transformer-based approaches: DTQN, which replaces LSTM layers in Deep Recurrent Q-Networks with transformer decoders, and Decision Transformers, which use sequence modeling for offline RL. These were compared against traditional DQN-DRQN combinations for online learning and PPO for offline learning in the VizDoom environment. Experiments used a 64x64x1 visual observation space with single-frame inputs. Performance metrics included kill-death ratios for online learning and episode rewards for offline learning. The researchers conducted systematic comparisons across identical environmental conditions to evaluate whether transformers provide benefits for FPS gameplay.

## Key Results
- DQN-DRQN combination achieved significantly better kill-death ratios than DTQN in online learning
- PPO outperformed Decision Transformer in episode rewards for offline RL
- Both transformer approaches (DTQN and DT) consistently underperformed their traditional counterparts in the VizDoom FPS environment

## Why This Works (Mechanism)
Transformers struggle in partially observable environments requiring long-term memory and strategic planning because their attention mechanisms, while powerful for sequence modeling, cannot effectively capture the hierarchical temporal dependencies and memory requirements needed for FPS gameplay. The VizDoom environment presents challenges including partial observability, long-term credit assignment, and complex state-action relationships that traditional RL methods like DQN-DRQN and PPO handle more effectively through their established memory mechanisms.

## Foundational Learning
- Reinforcement Learning Fundamentals: Why needed - understanding the core RL framework and reward structures; Quick check - can you explain the difference between value-based and policy-based methods?
- Transformer Architecture Basics: Why needed - grasping how self-attention works in transformers; Quick check - can you describe the role of positional encodings in transformers?
- Partially Observable Markov Decision Processes (POMDPs): Why needed - FPS games inherently involve partial observability; Quick check - can you explain why memory mechanisms are crucial in POMDPs?
- Deep Q-Networks and Policy Optimization: Why needed - understanding the traditional baselines used in the comparison; Quick check - can you differentiate between DQN and PPO approaches?
- VizDoom Environment: Why needed - familiarity with the specific FPS environment used; Quick check - can you name the key challenges VizDoom presents for RL agents?

## Architecture Onboarding
**Component Map:** Environment (VizDoom) -> Observation Processor -> Transformer/RL Module -> Action Selector -> Reward Signal -> Training Loop
**Critical Path:** Visual observations → Feature extraction → Decision-making module → Action execution → Reward feedback → Policy update
**Design Tradeoffs:** Transformers offer parallel processing and global context awareness but require more parameters and computational resources, while traditional RL methods use sequential processing with established memory mechanisms that may be more suitable for partial observability
**Failure Signatures:** Transformer-based methods show poor performance in long-term strategy tasks, struggle with credit assignment over extended sequences, and exhibit unstable learning curves compared to traditional approaches
**First Experiments:** 1) Test transformer performance with increased observation history, 2) Compare attention mechanisms with different positional encoding schemes, 3) Evaluate hybrid architectures combining transformers with LSTM layers

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to single FPS environment (VizDoom) with fixed observation space and control scheme
- Limited architectural variations and hyperparameter tuning for transformer models
- Results may not generalize to other FPS environments, higher resolution inputs, or different state spaces
- Single-frame input processing may not fully leverage transformers' sequence modeling capabilities

## Confidence
- Core claim (transformers underperform traditional methods in VizDoom FPS RL): High
- Broader claims about transformers' general applicability to FPS games: Medium

## Next Checks
1. Test transformer-based methods on multiple FPS environments with varying observation spaces and action complexities
2. Conduct systematic hyperparameter optimization for transformer architectures, including different positional encoding schemes and attention mechanisms
3. Evaluate whether hybrid architectures combining transformers with traditional RL components improve performance in partially observable settings