---
ver: rpa2
title: The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms
arxiv_id: '2511.04217'
source_url: https://arxiv.org/abs/2511.04217
tags:
- source
- target
- approximation
- theorem
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first theoretical proof of the strong
  lottery ticket hypothesis (SLTH) for multi-head attention mechanisms and transformers.
  The core method involves reinterpreting the query-key inner product in attention
  as a two-layer neural network, enabling application of the two-layers-for-one approximation
  technique.
---

# The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms

## Quick Facts
- **arXiv ID:** 2511.04217
- **Source URL:** https://arxiv.org/abs/2511.04217
- **Reference count:** 40
- **Primary result:** First theoretical proof that randomly initialized multi-head attention contains strong lottery tickets (SLTs) that approximate any target attention mechanism without training

## Executive Summary
This paper establishes the first theoretical proof of the Strong Lottery Ticket Hypothesis (SLTH) for multi-head attention mechanisms and transformers. The authors prove that randomly initialized attention networks with logarithmically large hidden dimensions contain subnetworks (strong lottery tickets) that can approximate any target attention mechanism without any weight training. The key insight is reinterpreting the query-key inner product in attention as a two-layer neural network, enabling application of the two-layers-for-one approximation technique. The theory shows approximation error decreases exponentially with hidden dimensions and remains independent of input sequence length. The authors also propose a new weight initialization scheme for query and key projections that improves SLT performance in practical settings.

## Method Summary
The method involves initializing a "source" multi-head attention mechanism with random weights and finding binary pruning masks that approximate a "target" attention mechanism. The core technique is merging the query and key projections into a single linear layer, then applying subset-sum approximation to find masks that reconstruct the target weights from the random source. The theory proves that hidden dimensions scaling logarithmically with input dimension are sufficient for high-probability approximation. For practical implementation, the authors propose scaling query and key weights by n^(1/4) and use edge-popup algorithms to find masks efficiently. The approach is validated both theoretically and empirically on synthetic tasks and language modeling with GPT-2 architectures.

## Key Results
- Theoretical proof that SLTs exist in randomly initialized attention with hidden dimensions O(d log(d^(3/2)/ε))
- Approximation error decreases exponentially with hidden dimensions and is independent of sequence length T
- New weight initialization (scaling by n^(1/4)) improves SLT performance, approaching fully trained models
- Empirical validation shows SLTs achieve competitive performance on WikiText-103 language modeling

## Why This Works (Mechanism)

### Mechanism 1: Merged Projection Approximation
The query-key (QK) inner product in attention can be theoretically treated as a single linear layer, allowing standard pruning proofs to apply. The target QK product x_i W_Q (W_K)^T X^T can be rewritten as x_i W_{QK} X^T, where W_{QK} is a merged weight matrix. A "source" network with random weights can similarly form a product, and by applying binary pruning masks M to these random matrices, one can approximate the target merged matrix using subset-sum approximation without adding extra layers.

### Mechanism 2: Logarithmic Overparameterization
A randomly initialized network requires a hidden dimension logarithmic in the input dimension to guarantee SLT existence. The proof relies on probabilistic density of random sums - to approximate a target weight to precision ε, the random source must have sufficient "capacity" in its weights. The authors prove that a hidden dimension n = O(d log(d^(3/2)/ε)) is sufficient to approximate any target MHA with high probability.

### Mechanism 3: Sequence-Length Independence
The approximation error of the SLT does not diverge as input sequence length T increases. Standard Lipschitz bounds for softmax scale with T, leading to loose error bounds. The authors derive a tighter bound by analyzing the softmax output and input matrix X simultaneously, showing the error depends on the norm of input vectors (α) rather than count T.

## Foundational Learning

- **Strong Lottery Ticket Hypothesis (SLTH)**: Framework positing that randomly weighted networks contain subnetworks that perform well without any training. Needed as the core theoretical framework. Quick check: What is the fundamental difference between networks found in LTH versus SLTH? (Answer: LTH subnetworks require training; SLTH subnetworks require only pruning/masking).

- **Subset-Sum Approximation**: Mathematical technique that explains how summing a subset of random values can approximate any target value with logarithmic samples. Needed as the engine of the proof. Quick check: Why is the "logarithmic" requirement crucial for efficiency of SLTH? (Answer: It means the source network doesn't need to be exponentially larger than the target).

- **Multi-Head Attention (MHA) Projections**: Understanding roles of W_Q, W_K, W_V, W_O is necessary to grasp the "merging" mechanism (W_Q W_K^T) central to this paper's contribution. Quick check: In the context of this paper, which two projections are "merged" to allow the two-layers-for-one approximation? (Answer: Query and Key).

## Architecture Onboarding

- **Component map:** Target MHA -> Source MHA (with random weights and binary masks) -> Merged projection approximation -> Subset-sum pruning
- **Critical path:**
  1. Initialization: Initialize Source MHA with scaled uniform weights (W_Q, W_K ~ U[-n^(1/4), n^(1/4)])
  2. Merging: Conceptually treat product of Query and Key weights as single linear layer
  3. Pruning: Solve subset-sum problem to identify binary masks M that align source product with target product
- **Design tradeoffs:**
  - Theory vs. Practice: Theoretical proof requires specific uniform distributions, practical experiments use standard normal initializations with n^(1/4) scaling
  - Normalization: Current theory excludes normalization layers (LayerNorm), which are standard in Transformers
- **Failure signatures:**
  - Error Divergence: If hidden dimension not logarithmically large enough, approximation error won't converge exponentially
  - T-Scaling: If implementation relies solely on loose Lipschitz bounds for softmax, error bounds will appear to scale with sequence length T
- **First 3 experiments:**
  1. Dimension Scaling: Plot approximation error vs. hidden dimension n_K to verify exponential decay predicted by Theorem 3
  2. Sequence Length Independence: Fix hidden dimension and vary input length T to confirm error bounds remain constant
  3. Initialization Scaling: Train GPT-2 model using n^(1/4) weight scaling factor on Q/K projections and compare validation loss against standard initialization

## Open Questions the Paper Calls Out

- **Does SLTH hold for transformers with normalization layers?** The authors explicitly state they extend SLTH to "transformers without normalization layers," treating inclusion of normalization as an open problem. The current proof technique relies on merging weight matrices and analyzing error propagation linearly, but normalization layers introduce input-dependent, non-linear scaling that disrupts these strategies.

- **Can theoretical guarantees extend to standard initialization schemes?** The theoretical proof requires specific uniform distributions, while practical experiments use normal distributions. The core of the proof relies on subset-sum approximation which leverages uniform distribution properties - it's unproven whether logarithmic overparameterization bounds hold for Gaussian weights.

- **Can efficient search algorithms provably locate SLTs?** The paper validates theory using synthetic "subset-sum" method but switches to "edge-popup" algorithm for practical language modeling. The disconnect lies between existential proof (existence of masks) and optimization landscape (finding masks) - it's unproven whether gradient-based methods can navigate the non-convex loss landscape to find such masks efficiently.

## Limitations

- Theoretical framework excludes normalization layers (LayerNorm), which are standard in modern transformer architectures
- Proof relies on specific uniform distributions for initialization, creating gap with practical implementations using normal distributions
- Subset-sum approximation becomes computationally intractable for large hidden dimensions in synthetic validation

## Confidence

**High Confidence:** The mechanism of reinterpreting query-key inner product as single linear layer and applying subset-sum approximation is mathematically sound within stated constraints. The logarithmic scaling requirement for hidden dimensions is well-established in related literature.

**Medium Confidence:** The sequence-length independence claim is theoretically justified but depends critically on assumption that pre-softmax approximation error remains bounded. Empirical validation provides support, but edge cases could challenge this.

**Low Confidence:** The practical transferability of n^(1/4) scaling factor from theory to real-world language modeling tasks shows promise but lacks rigorous theoretical backing. Performance improvements may stem from factors beyond theoretical guarantees.

## Next Checks

1. **Solver Implementation Verification:** Implement subset-sum approximation using open-source solver (e.g., CVXPY) and verify binary masks found achieve claimed exponential error decay as hidden dimensions increase.

2. **Normalization Layer Integration:** Extend theoretical framework to include LayerNorm by analyzing how normalization affects two-layers-for-one approximation mechanism and deriving modified error bounds.

3. **Alternative Initialization Testing:** Systematically test n^(1/4) scaling factor across different initialization distributions (normal, uniform, power-law) to determine whether specific uniform distribution is essential or if scaling principle generalizes.