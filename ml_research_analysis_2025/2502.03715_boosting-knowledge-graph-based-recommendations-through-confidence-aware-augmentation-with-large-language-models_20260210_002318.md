---
ver: rpa2
title: Boosting Knowledge Graph-based Recommendations through Confidence-Aware Augmentation
  with Large Language Models
arxiv_id: '2502.03715'
source_url: https://arxiv.org/abs/2502.03715
tags:
- item
- user
- triples
- knowledge
- triplets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CKG-LLMA, a framework that combines knowledge
  graphs with large language models for recommendation tasks. The method uses an LLM-based
  subgraph augmenter to enrich knowledge graphs, a confidence-aware message propagation
  mechanism to filter noisy triplets, and a dual-view contrastive learning method
  to integrate user-item interactions and KG data.
---

# Boosting Knowledge Graph-based Recommendations through Confidence-Aware Augmentation with Large Language Models

## Quick Facts
- arXiv ID: 2502.03715
- Source URL: https://arxiv.org/abs/2502.03715
- Reference count: 18
- Outperforms baselines by up to 5.7% in Recall@10 and 3.4% in NDCG@10

## Executive Summary
This paper introduces CKG-LLMA, a framework that enhances knowledge graph-based recommendation systems by integrating large language models for subgraph augmentation and confidence-aware filtering. The method employs an LLM-based subgraph augmenter to enrich knowledge graphs with high-quality information, a confidence-aware message propagation mechanism to filter noisy triplets, and a dual-view contrastive learning approach to integrate user-item interactions with KG data. Experimental results on three public datasets demonstrate significant improvements over various baseline methods.

## Method Summary
CKG-LLMA combines knowledge graphs with large language models to improve recommendation accuracy. The framework uses an LLM-based subgraph augmenter that employs dual-view prompting (user-view and item-view) to generate high-quality triplet augmentation advice. A confidence-aware MOE message propagation mechanism filters noisy triplets by assigning learnable confidence scores. Dual-view contrastive learning integrates user-item interactions and KG data through cross-view consistency enforcement. The method also includes a confidence-aware explanation generation process to guide LLMs in producing realistic explanations for recommendations.

## Key Results
- Achieves up to 5.7% improvement in Recall@10 compared to baseline methods
- Improves NDCG@10 by up to 3.4% over existing approaches
- Ablation studies show dual-view contrastive learning contributes most significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1: LLM-based Subgraph Augmentation via Dual-View Prompting
- Claim: Extracting focused subgraphs and prompting LLMs from user-view and item-view perspectives yields structured augmentation advice (add/delete triplets) that enriches KG semantics within token constraints.
- Mechanism: User-view prompts target fact verification for item-attribute triplets and reasoning-based pruning for user-item interactions. Item-view prompts leverage item-item correlations to cross-validate item-attribute facts. Outputs populate four pools: P_add_U, P_del_U, P_add_I, P_del_I.
- Core assumption: LLMs can distinguish between relevant and irrelevant KG triplets in recommendation contexts with sufficient context from subgraph structure.
- Evidence anchors:
  - [abstract] "an LLM-based subgraph augmenter for enriching KGs with high-quality information"
  - [section 3.2] "We design 2 kinds of extraction logic (User-view and item-view) with unique prompts to guide LLM in enhancing KG from a multi-view perspective"
  - [corpus] KERL (arXiv:2505.14629) similarly integrates KGs with LLMs for recommendation, supporting the general approach but not this specific dual-view design
- Break condition: If LLM hallucination rate exceeds the confidence mechanism's capacity to filter; if subgraph size N is too small to capture relevant context or too large for token limits.

### Mechanism 2: Confidence-aware MOE Message Propagation for Triplet Filtering
- Claim: Assigning learnable confidence scores to triplets via a Mixture-of-Experts layer enables differentiable noise reduction during knowledge aggregation.
- Mechanism: For each triplet (i, r, a), confidence φ(i,r,a) is computed via MOE layer ξ(·) that takes relation embeddings and concatenated entity features. The gate weights ω_i are computed via softmax over expert outputs. During aggregation (Eq. 5), triplet contributions are weighted by their confidence-derived attention scores.
- Core assumption: Noisy triplets exhibit learnable patterns distinguishable from useful ones through relation-entity interactions.
- Evidence anchors:
  - [abstract] "a confidence-aware message propagation mechanism to filter noisy triplets"
  - [section 3.3] "This approach assigns a confidence score for LLM-enhanced edges, significantly boosting their robustness in handling diverse relational data"
  - [Table 3] Ablation shows performance drop without confidence mechanism (0.1883→0.1773 Recall on AmazonBook)
  - [corpus] "Less is More: Denoising Knowledge Graphs" (arXiv:2510.14271) supports denoising principle but uses different approach
- Break condition: If MOE experts collapse to uniform weights; if relation embeddings fail to capture semantic distinctions; if confidence scores don't correlate with triplet utility for recommendation.

### Mechanism 3: Dual-view Contrastive Learning with Stability-guided IG Augmentation
- Claim: Constructing two augmented graph views (user-view, item-view) and enforcing cross-view consistency via contrastive loss improves representation robustness while item stability scores guide interaction graph augmentation.
- Mechanism: Step 1 samples triplets from LLM pools to create ψ(G_U) and ψ(G_I) with confidence-based dropout via Gumbel-softmax. Step 2 computes item stability s_i via cosine similarity of cross-view representations, converting to keep probabilities for IG node dropout. Contrastive loss (Eq. L_con) maximizes mutual information between same-user/item embeddings across views.
- Core assumption: Items with high cross-view stability are less sensitive to topological noise and should be preserved; useful KG information should yield consistent representations across augmentation views.
- Evidence anchors:
  - [abstract] "a dual-view contrastive learning method to integrate user-item interactions and KG data"
  - [section 3.4] "we tend to drop item nodes in Y with lower stability"
  - [Table 3] Removing contrastive loss drops Recall from 0.1883 to 0.1312 on AmazonBook—largest ablation impact
  - [corpus] KGCL (cited in paper) and VoteGCL (arXiv:2507.21563) support graph contrastive learning for recommendation
- Break condition: If stability scores don't correlate with item importance; if augmentation creates too disparate views (breaks positive pairs); if temperature τ is poorly tuned.

## Foundational Learning

- Concept: **Graph Neural Networks for Recommendation (GNN-based CF)**
  - Why needed here: The framework builds on LightGCN-style message propagation and GAT-style attention for aggregating neighbor information. Without this foundation, the confidence-aware aggregation (Eq. 1, 5) and final propagation (Eq. 12) are opaque.
  - Quick check question: Can you explain how embedding propagation works in LightGCN and why it differs from GCN with non-linear transformations?

- Concept: **Contrastive Learning on Graphs**
  - Why needed here: The dual-view contrastive objective is central to the method. Understanding InfoNCE-style losses, positive/negative pair construction, and temperature scaling is prerequisite to implementing L_con.
  - Quick check question: Given two augmented views of a graph, how would you construct positive and negative pairs for node-level contrastive learning?

- Concept: **Knowledge Graph Embeddings and Relations**
  - Why needed here: The method operates on triplets (h, r, t) and uses relation embeddings in the MOE confidence computation. Understanding TransE, relation-specific projections, or at least the notion of embedding entities and relations separately is necessary.
  - Quick check question: Why might relation embeddings need separate treatment from entity embeddings in a MOE-based confidence function?

## Architecture Onboarding

- Component map:
  Preprocessing Stage (offline) -> LLM-based Subgraph Augmenter -> generates P_add_U, P_del_U, P_add_I, P_del_I
  Model Input: TKG G (union of UI, IA, II triplets), interaction graph Y, LLM augmentation pools
  Forward Pass: GAT-style aggregation (Eq. 1) -> Confidence-aware MOE aggregation (Eq. 2-5) -> Dual-view KG augmentation (Eq. 6-9) -> Stability computation (Eq. 10) -> IG augmentation (Eq. 11) -> LightGCN propagation (Eq. 12)
  Loss Computation: BPR loss (Eq. 13) + Contrastive loss (L_con) + L2 regularization
  Inference/Explanation: Confidence-aware path extraction -> LLM prompting with confidence scores

- Critical path:
  1. Subgraph extraction quality (N=32-64 depending on dataset) determines LLM augmentation quality
  2. MOE expert count (N_e=8) and gating stability affects confidence score reliability
  3. Add ratio μ_a (0.6-0.7) and delete ratio μ_d (0.04-0.08) control augmentation intensity—incorrect values can degrade performance
  4. Gumbel-softmax temperature τ_g controls differentiability vs. discreteness tradeoff

- Design tradeoffs:
  - **Subgraph size N**: Larger N provides more context but increases LLM API cost and token overflow risk. Paper uses N=32 (AmazonBook), 40 (Steam), 64 (Anime).
  - **Number of experts N_e**: More experts increase expressiveness but risk overfitting and computational overhead. Default is 8.
  - **Contrastive weight λ_c vs. BPR weight**: Paper sets λ_c=0.001, but ablation shows contrastive loss is critical—tuning required per dataset.
  - **LLM choice**: Paper uses GPT-3.5-turbo (cost: $27-$68 per dataset). Open-source LLMs possible but may reduce augmentation quality (unverified in paper).

- Failure signatures:
  - **Confidence collapse**: All triplets receive similar confidence scores -> no effective filtering -> performance approaches baseline. Monitor confidence score variance.
  - **Expert collapse in MOE**: Gate weights converge to uniform -> reduced capacity. Check ω_i distribution during training.
  - **Contrastive loss divergence**: Loss goes to -∞ or stalls -> check temperature τ and positive pair construction.
  - **LLM pool contamination**: P_add pools contain low-quality triplets -> ablation with reduced LLM ratio should show degradation. If not, pools may already be noisy.
  - **Instability in Gumbel-softmax**: If τ_g too low, sampling becomes near-deterministic; if too high, gradient variance explodes.

- First 3 experiments:
  1. **Baseline reproduction**: Implement LightGCN + KGAT on the three datasets (AmazonBook, Steam, Anime) to verify data pipeline and metrics. Target: match or approach reported baseline numbers in Table 2.
  2. **Ablation on confidence mechanism**: Run CKG-LLMA with w/o confidence variant. Expected: ~5-10% Recall drop. If drop is minimal, confidence computation may not be learning meaningful weights—check gradient flow through MOE layer.
  3. **LLM pool quality test**: Train with 20%, 40%, 60%, 80% sampling from LLM pools (following Table 3). Expected: monotonic improvement with more LLM data. If non-monotonic, inspect P_add pools for noise—manual review of 50-100 sampled triplets recommended.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CKG-LLMA's performance and cost-effectiveness scale to significantly larger knowledge graphs (millions of entities) where token limitations become more severe?
- Basis in paper: [inferred] The paper mentions token constraints necessitate subgraph extraction and reports API costs ($27-$68), but experiments only use relatively small KGs (3,423-34,902 entities). Scalability to production-scale graphs remains unexplored.
- Why unresolved: Subgraph extraction strategies may not capture necessary global context in larger graphs, and LLM API costs could become prohibitive.
- What evidence would resolve it: Experiments on large-scale industry datasets with cost-performance analysis and alternative subgraph sampling strategies.

### Open Question 2
- Question: How robust is the framework when deployed with different LLM backbones (e.g., open-source models like Llama or smaller models), particularly regarding hallucination rates and augmentation quality?
- Basis in paper: [explicit] The paper states "CKG-LLMA can also use open-source LLMs" in Section 8, but all experiments exclusively use GPT-3.5-turbo.
- Why unresolved: Different LLMs have varying knowledge capacities and hallucination tendencies, which directly impact the quality of triplet augmentation and the confidence mechanism's effectiveness.
- What evidence would resolve it: Comparative experiments across multiple LLM backbones with analysis of augmentation quality and downstream recommendation performance.

### Open Question 3
- Question: What is the optimal methodology for determining dataset-specific hyperparameters such as subgraph size N, add ratio µa, and delete ratio µd?
- Basis in paper: [inferred] Table 1 shows different subgraph sizes (32, 40, 64) across datasets, and Section 4.4 shows optimal µa and µd vary (e.g., µa=60% for AmazonBook vs. 70% for Steam). No systematic approach for determining these is provided.
- Why unresolved: Manual hyperparameter tuning is resource-intensive and may not transfer to new domains.
- What evidence would resolve it: Development of adaptive or meta-learning approaches for automatic hyperparameter selection, validated across diverse datasets.

### Open Question 4
- Question: How can the framework be extended to handle temporal evolution in knowledge graphs, where entity relationships change over time?
- Basis in paper: [explicit] The Introduction explicitly identifies "the static nature of many KGs limits the ability to revise outdated relations" as a key challenge, yet the proposed method does not address temporal dynamics.
- Why unresolved: The current framework treats KGs as static snapshots; time-aware augmentation and confidence decay mechanisms are needed.
- What evidence would resolve it: Experiments on temporal KG datasets with evaluation of how well updated recommendations reflect evolving user preferences and entity relationships.

## Limitations

- The reliance on commercial LLM APIs (GPT-3.5-turbo) raises concerns about reproducibility and scalability, with reported costs of $27-$68 per dataset that could become prohibitive for larger graphs.
- Critical hyperparameters like batch size, training epochs, and early stopping criteria are not specified, making exact reproduction challenging.
- The MOE expert architecture details are underspecified ("linear layers" only), which could affect the confidence mechanism's effectiveness when implemented differently.

## Confidence

- **High Confidence**: The dual-view contrastive learning mechanism shows strong empirical support (Table 3 ablation: 0.1883→0.1312 Recall drop without it). The mathematical formulation is clear and consistent.
- **Medium Confidence**: The confidence-aware MOE message propagation is theoretically sound but depends heavily on expert design and gating stability. The ablation shows 0.1883→0.1773 drop, suggesting moderate impact.
- **Low Confidence**: The LLM-based subgraph augmentation quality is difficult to verify without access to the exact prompts and GPT-3.5-turbo responses. The paper claims high-quality augmentation but provides limited evidence of triplet relevance.

## Next Checks

1. **LLM Pool Quality Audit**: Manually review 50-100 sampled triplets from P_add_U and P_add_I pools across all datasets. Calculate precision@K (relevant triplets) and compare against original KG quality metrics. This validates whether augmentation truly enriches rather than corrupts the KG.

2. **Confidence Score Calibration**: Train the model with w/o confidence variant and analyze the confidence score distribution. Plot confidence vs. triplet utility (measured by contribution to recommendation accuracy). Verify that confidence scores correlate with actual triplet quality rather than being uniform.

3. **Cross-Dataset Hyperparameter Transfer**: Implement the same hyperparameter set (μa=0.60, μd=0.04) across all three datasets without dataset-specific tuning. Compare performance degradation to the tuned versions in Table 2. This tests whether the method's success depends on extensive dataset-specific optimization.