---
ver: rpa2
title: Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language
  Model Agents
arxiv_id: '2511.15074'
source_url: https://arxiv.org/abs/2511.15074
tags:
- agent
- features
- feature
- rogue
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rogue One introduces a novel multi-agent LLM framework for automated
  feature extraction that addresses limitations of existing methods by incorporating
  qualitative feedback and external domain knowledge. The system uses three specialized
  agents (Scientist, Extractor, and Tester) that collaborate iteratively, employing
  a "flooding-pruning" strategy to dynamically balance exploration and exploitation
  of the feature space.
---

# Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents

## Quick Facts
- **arXiv ID**: 2511.15074
- **Source URL**: https://arxiv.org/abs/2511.15074
- **Reference count**: 40
- **Primary result**: MRR of 0.76 for classification, 0.91 for regression on 28 benchmark datasets

## Executive Summary
Rogue One introduces a novel multi-agent LLM framework for automated feature extraction that addresses limitations of existing methods by incorporating qualitative feedback and external domain knowledge. The system uses three specialized agents (Scientist, Extractor, and Tester) that collaborate iteratively, employing a "flooding-pruning" strategy to dynamically balance exploration and exploitation of the feature space. By integrating retrieval-augmented generation for external knowledge and moving beyond simple accuracy metrics, Rogue One generates semantically meaningful and interpretable features. Experiments on 19 classification and 9 regression datasets demonstrate significant performance improvements over state-of-the-art methods, with a mean reciprocal rank of 0.76 for classification tasks. The framework also surfaces novel hypotheses, such as identifying a potential new biomarker in the myocardial dataset, showcasing its utility for scientific discovery beyond mere prediction optimization.

## Method Summary
The method employs a three-agent iterative loop where the Scientist Agent analyzes results to define Focus Areas, the Extractor Agent generates new features via Python code using Pandas/Numpy, and the Tester Agent evaluates features with 5-fold XGBoost validation while producing qualitative assessments. The "flooding-pruning" strategy generates approximately 17 features per iteration before pruning redundant or low-impact ones based on correlation thresholds (>0.95) and feature importance scores. External knowledge integration uses RAG with Qwen3-Embedding-4B for domain literature, while the Scientist Agent accesses web search for broader context. The framework processes 19 classification and 9 regression datasets from OpenML/UEA using GPT-OSS-120B (or substitute code-capable model) without fine-tuning, iterating 10 times per dataset with input prompts specifying task type, global goals, and attribute descriptions.

## Key Results
- Achieved mean reciprocal rank of 0.76 for classification tasks and 0.91 for regression tasks across 28 benchmark datasets
- Outperformed state-of-the-art AutoFE methods by generating semantically meaningful features beyond raw transformations
- Identified a potentially novel biomarker (wbc_roe_ratio) in the myocardial dataset, demonstrating capability for scientific discovery
- Showed systematic performance degradation on datasets with high attribute counts, suggesting iteration limits need scaling

## Why This Works (Mechanism)
The framework works by combining human-like collaborative reasoning with automated execution, where each agent specializes in a distinct cognitive role that mirrors scientific methodology. The Scientist Agent provides strategic direction by analyzing patterns and formulating hypotheses about which feature transformations might be most promising, preventing random exploration of the vast transformation space. The Extractor Agent translates these strategic insights into concrete computational operations while leveraging external knowledge to inform transformation choices beyond what's statistically obvious. The Tester Agent provides critical evaluation that goes beyond accuracy metrics to assess feature interpretability, redundancy, and robustness, creating a feedback loop that gradually refines the feature set toward both predictive power and scientific meaning.

## Foundational Learning

**Feature Pool Management**: A shared data structure storing all generated features with their evaluation metrics and metadata. *Why needed*: Enables iterative refinement and prevents duplicate work across iterations. *Quick check*: Verify features persist correctly between iterations and can be retrieved by attribute name.

**Focus Area Definition**: Natural language descriptions of promising transformation directions derived from Test Pool analysis. *Why needed*: Guides the Extractor Agent toward semantically meaningful transformations rather than random exploration. *Quick check*: Focus Areas should reference specific attributes and suggest clear transformation strategies.

**RAG Integration for Knowledge**: Vector database search using embeddings to retrieve relevant domain literature for feature generation. *Why needed*: Provides contextual understanding that pure statistical analysis might miss, especially for scientific discovery. *Quick check*: Search results should include relevant papers when querying domain-specific terms.

**Qualitative Feature Assessment**: Markdown reports evaluating predictive power, redundancy, and robustness of features. *Why needed*: Moves beyond accuracy metrics to ensure generated features are interpretable and scientifically meaningful. *Quick check*: Assessment should identify at least one actionable insight about feature relationships per iteration.

## Architecture Onboarding

**Component Map**: Scientist -> Focus Area -> Extractor -> Feature Code -> Tester -> Feature Assessment -> Feature Pool (with RAG integration at Extractor/Tester, web search at Scientist)

**Critical Path**: Iteration loop: Scientist analyzes Test Pool → generates Focus Area → Extractor reads Focus Area → generates Python code → executes transformations → Tester validates with 5-fold XGBoost → produces assessment → prunes features → updates Feature Pool → repeat

**Design Tradeoffs**: Flooding with ~17 features per iteration maximizes exploration but requires aggressive pruning to prevent dimensionality explosion; qualitative feedback provides interpretability but introduces subjectivity in pruning decisions; RAG knowledge enhances scientific discovery but may introduce noise if sources are unreliable.

**Failure Signatures**: Extractor generates invalid Python code (syntax errors or missing dependencies); Feature Pool grows uncontrollably without effective pruning; Scientist produces vague Focus Areas that don't reference specific attributes; RAG returns irrelevant or noisy information that degrades feature quality.

**Three First Experiments**:
1. Run single iteration with synthetic dataset to verify agent communication and code execution pipeline
2. Test RAG integration with known domain literature to validate knowledge retrieval quality
3. Implement basic pruning logic and verify it reduces Feature Pool size while maintaining top-performing features

## Open Questions the Paper Calls Out

**Open Question 1**: Does increasing the iteration count effectively mitigate performance degradation on datasets with a high attribute count (p)?
*Basis in paper*: Section 5.1 hypothesizes that extending iterations would allow more thorough exploration to counter combinatorial expansion. *Why unresolved*: Experiments used fixed 10 iterations, insufficient for "wide" datasets. *What evidence would resolve it*: Empirical results from experiments where iteration limits scale with attribute count.

**Open Question 2**: Is the ratio of white blood cell count to erythrocyte sedimentation rate (wbc_roe_ratio) a clinically valid prognostic biomarker for Chronic Heart Failure (CHF)?
*Basis in paper*: Section 5.3 identifies this as a "potentially novel biomarker" and "testable hypothesis." *Why unresolved*: While statistically predictive, biological mechanism and clinical utility haven't been validated by domain experts. *What evidence would resolve it*: Clinical studies verifying the ratio's correlation with CHF outcomes independently.

**Open Question 3**: To what extent does the quality of external knowledge source impact feature reliability?
*Basis in paper*: Section 5.2 acknowledges web search can introduce "noisy and biased information," suggesting unverified knowledge may degrade performance. *Why unresolved*: Paper proposes curated databases but doesn't quantify performance trade-off. *What evidence would resolve it*: Ablation studies comparing feature quality when using verified literature vs. general web results.

## Limitations
- GPT-OSS-120B model availability is uncertain, requiring substitution with potentially different-performing models
- Qualitative feedback mechanism lacks precise implementation details for handling feature tradeoffs
- External knowledge integration details are incomplete, making it difficult to validate claimed performance improvements

## Confidence
**High Confidence**: Core three-agent architecture and iterative framework are clearly specified and reproducible
**Medium Confidence**: Experimental results showing performance improvements over baselines are credible given clear evaluation protocol
**Low Confidence**: Claims about novel scientific discovery and specific contribution of external knowledge integration are difficult to verify

## Next Checks
1. **Model Substitution Validation**: Implement framework using GPT-4o instead of unavailable GPT-OSS-120B, then systematically compare feature quality, generation success rates, and iteration efficiency
2. **Knowledge Integration Ablation**: Run controlled experiments comparing performance with and without RAG knowledge integration on same datasets, measuring marginal contribution to feature quality
3. **Pruning Strategy Robustness**: Implement multiple concrete pruning heuristics and evaluate their impact on final model performance and feature interpretability