---
ver: rpa2
title: Progressive Image Restoration via Text-Conditioned Video Generation
arxiv_id: '2512.02273'
source_url: https://arxiv.org/abs/2512.02273
tags:
- restoration
- cogvideo
- enhancement
- image
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework that repurposes the text-to-video
  generation model CogVideo for image restoration tasks by fine-tuning it to generate
  progressive restoration trajectories. The key insight is that image restoration
  (super-resolution, deblurring, and low-light enhancement) can be modeled as a temporal
  video generation problem, where a sequence of frames gradually evolves from degraded
  to clean states.
---

# Progressive Image Restoration via Text-Conditioned Video Generation

## Quick Facts
- arXiv ID: 2512.02273
- Source URL: https://arxiv.org/abs/2512.02273
- Reference count: 20
- Key outcome: Fine-tuned CogVideo generates progressive restoration sequences with improved PSNR, SSIM, and LPIPS metrics across frames, demonstrating zero-shot generalization to real-world motion blur.

## Executive Summary
This paper proposes a novel framework that repurposes the text-to-video generation model CogVideo for image restoration tasks by fine-tuning it to generate progressive restoration trajectories. The key insight is that image restoration (super-resolution, deblurring, and low-light enhancement) can be modeled as a temporal video generation problem, where a sequence of frames gradually evolves from degraded to clean states. Three synthetic datasets are constructed to train the model on these progressive transformations. Two prompting strategies are evaluated: uniform text prompts and scene-adaptive prompts generated via LLaVA and refined with ChatGPT. The fine-tuned CogVideo successfully generates temporally coherent restoration sequences, with quantitative improvements in PSNR, SSIM, and LPIPS metrics across frames. Notably, the model demonstrates strong zero-shot generalization to real-world motion blur restoration on the ReLoBlur dataset without additional training, highlighting its potential as an interpretable and extensible paradigm for unified visual restoration tasks.

## Method Summary
The method fine-tunes the pre-trained CogVideoX-5B-I2V model using LoRA on synthetic datasets created from DIV2K images with controlled degradation schedules. For each degraded image, nine-frame videos are generated where degradation progressively decreases across frames. Two prompt strategies are used: uniform prompts shared across all samples and scene-adaptive prompts generated via LLaVA and refined with ChatGPT. The model learns to associate temporal progression with restoration quality, producing sequences where PSNR, SSIM, and LPIPS metrics improve across frames. At inference, the final frame of the generated sequence serves as the restored output. The approach is evaluated on synthetic super-resolution, deblurring, and low-light enhancement tasks, as well as real-world motion blur from the ReLoBlur dataset.

## Key Results
- Fine-tuned CogVideo generates temporally coherent restoration sequences with metrics improving across frames
- Scene-adaptive prompting via LLaVA+ChatGPT consistently outperforms uniform prompts across all tasks
- Strong zero-shot generalization to real-world ReLoBlur motion blur without additional training
- Progressive restoration framework unifies super-resolution, deblurring, and low-light enhancement under one paradigm

## Why This Works (Mechanism)

### Mechanism 1: Temporal Progression Encoding
Fine-tuning a video diffusion model on synthetic degradation-to-clean sequences causes it to learn temporal progression as a proxy for restoration quality. The model receives 9-frame sequences where early frames are heavily degraded and later frames are progressively cleaner. Through diffusion training, it learns to predict noise that corresponds to smooth quality transitions rather than natural motion.

### Mechanism 2: Semantic Text-Conditioned Alignment
Scene-specific prompts generated via LLaVA and refined by ChatGPT improve restoration quality compared to uniform prompts by providing richer semantic grounding. Multi-modal LLM descriptions capture scene content (e.g., "A night street gradually brightens under lamplight"), which conditions the diffusion process to attend on semantically relevant features during restoration.

### Mechanism 3: Zero-Shot Transfer via Learned Degradation Priors
Synthetic training on parametric degradations (blur, downsampling, low-light) yields representations that generalize to real-world degradation patterns without additional fine-tuning. The model learns abstract "degradation-to-clean" transformations rather than memorizing specific artifact patterns, enabling application to unseen real blur in ReLoBlur.

## Foundational Learning

- **Concept**: Latent Diffusion Models
  - Why needed here: CogVideo operates in a compressed latent space rather than pixel space; understanding how denoising unfolds in latents is essential for debugging restoration quality.
  - Quick check question: Can you explain why latent diffusion is more efficient than pixel-space diffusion for video generation?

- **Concept**: LoRA (Low-Rank Adaptation)
  - Why needed here: The authors use LoRA fine-tuning rather than full model training; understanding how rank constraints affect what can be learned is critical for capacity planning.
  - Quick check question: What types of transformations might LoRA fail to capture compared to full fine-tuning?

- **Concept**: Temporal Consistency in Video Diffusion
  - Why needed here: The core hypothesis depends on temporal coherence mechanisms transferring from natural motion to restoration progression.
  - Quick check question: How do video diffusion models typically enforce frame-to-frame consistency, and what might break this?

## Architecture Onboarding

- **Component map**: CogVideoX-5B-I2V -> LoRA adapter -> Prompt encoder -> Synthetic dataset pipeline -> Inference sampler

- **Critical path**:
  1. Synthetic video generation with controlled degradation schedules (determines training signal quality)
  2. Prompt generation (LLaVA → ChatGPT refinement) for scene-adaptive conditioning
  3. LoRA fine-tuning on progression videos
  4. Frame extraction at inference (final frame Iₜ used as restored output)

- **Design tradeoffs**:
  - Uniform vs. scene-specific prompts: Uniform is simpler but lower performance; scene-specific requires additional LLM inference overhead
  - Number of frames (T=9): More frames provide smoother progression but increase training/inference cost
  - LoRA rank: Higher rank captures more complex transformations but risks overfitting to synthetic patterns
  - Video resolution (1360×768): Higher resolution improves detail but requires more GPU memory

- **Failure signatures**:
  - Flickering or inconsistent frames in output progression (temporal coherence breakdown)
  - Final frame not reaching clean target quality (insufficient training or capacity)
  - Hallucinated details not present in original scene (over-generation from diffusion prior)
  - Poor generalization to real degradations despite synthetic training (distribution shift)

- **First 3 experiments**:
  1. Validate synthetic data quality: Generate progression videos, manually inspect frame-wise degradation schedules, and verify metric progression (PSNR/SSIM should increase monotonically).
  2. Ablate prompt strategy: Train two LoRA adapters—one with uniform prompts, one with scene-specific—and compare frame-wise metrics on held-out validation set.
  3. Zero-shot transfer test: Apply fine-tuned model to 5 samples from ReLoBlur without adaptation, report LPIPS trajectory across frames to confirm generalization claim.

## Open Questions the Paper Calls Out
None

## Limitations
- LoRA hyperparameters (rank, learning rate, batch size, epochs) are not specified, making exact reproduction impossible
- Prompt templates for LLaVA+ChatGPT scene-adaptive generation are not provided
- Computational requirements beyond using one H100 GPU are not detailed

## Confidence
- **High confidence**: The core hypothesis that video diffusion models can be repurposed for progressive image restoration through synthetic training is well-supported
- **Medium confidence**: The scene-adaptive prompting strategy's superiority over uniform prompts is demonstrated but comes with unquantified computational overhead
- **Medium confidence**: Zero-shot generalization to ReLoBlur is promising but lacks ablation studies on different degradation types

## Next Checks
1. Train multiple LoRA adapters with different ranks (e.g., 8, 16, 32) and measure their impact on restoration quality and training stability
2. Systematically vary prompt detail level and semantic content to quantify the relationship between prompt specificity and restoration quality
3. Apply the fine-tuned model to compound degradations (e.g., blur + low-light + compression) not present in training to assess generalization limits