---
ver: rpa2
title: Segment-Level Attribution for Selective Learning of Long Reasoning Traces
arxiv_id: '2602.00425'
source_url: https://arxiv.org/abs/2602.00425
tags:
- segments
- segment
- reasoning
- arxiv
- important
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of redundancy in long reasoning
  traces generated by large reasoning models, where only a small fraction meaningfully
  contributes to answer prediction while the majority contains repetitive or truncated
  content. The authors propose a segment-level selective learning framework that uses
  integrated gradient attribution to identify important segments based on two metrics:
  attribution strength (overall influence on prediction) and direction consistency
  (uniformity of contribution).'
---

# Segment-Level Attribution for Selective Learning of Long Reasoning Traces

## Quick Facts
- arXiv ID: 2602.00425
- Source URL: https://arxiv.org/abs/2602.00425
- Reference count: 16
- Key outcome: Selective learning framework improves reasoning accuracy by up to 4.7% and reduces output length by up to 18% by focusing on important segments identified via integrated gradient attribution

## Executive Summary
This paper addresses the problem of redundancy in long reasoning traces generated by large reasoning models, where only a small fraction meaningfully contributes to answer prediction while the majority contains repetitive or truncated content. The authors propose a segment-level selective learning framework that uses integrated gradient attribution to identify important segments based on two metrics: attribution strength (overall influence on prediction) and direction consistency (uniformity of contribution). Important segments are defined as those with high attribution strength but moderate consistency, indicating reflective rather than shallow reasoning. The framework applies selective supervised fine-tuning only on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that this approach improves reasoning accuracy by up to 4.7% and reduces output length by up to 18% compared to full chain-of-thought fine-tuning, while maintaining or improving performance compared to various baseline methods.

## Method Summary
The framework segments long CoT traces using transition keywords, computes token-level integrated gradients to quantify each token's influence on the final answer, aggregates these to segment-level metrics (Attribution Strength and Direction Consistency), identifies important segments through a two-step filtering process (high cumulative strength and moderate consistency), and applies selective supervised fine-tuning where loss is computed only on tokens within important segments while masking loss for others. The method uses R1-Distill-Qwen2.5-7B to compute attributions with J=50 integration steps, and trains on models including R1-Distill-Qwen2.5-1.5B/7B and Qwen2.5-7B-Instruct with specified hyperparameters.

## Key Results
- Selective learning improves reasoning accuracy by up to 4.7% compared to full SFT on long CoT traces
- Output length is reduced by up to 18% while maintaining or improving accuracy
- The framework outperforms various baselines including random segment selection, token-level selection, and ablation of the direction consistency filter
- The approach is effective across multiple mathematical reasoning benchmarks (MATH500, AMC23, AIME24, GPQA-Diamond, OlympiadBench)

## Why This Works (Mechanism)

### Mechanism 1: Attribution-Based Redundancy Filtration
Standard SFT on long reasoning traces degrades performance because models waste capacity imitating "verbose but uninformative patterns"; masking the loss for these segments acts as a regularizer. The framework uses Integrated Gradients to quantify token influence on the final answer. By aggregating this into Attribution Strength and masking gradients for segments below a cumulative threshold τ, the optimizer updates weights only using tokens that causally influence the prediction. This prevents the model from overfitting to repetitive or truncated content.

### Mechanism 2: Direction Consistency as a Proxy for Cognitive Depth
High attribution strength alone is insufficient; Direction Consistency distinguishes "shallow" reasoning (uniformly positive/negative attributions) from "reflective" reasoning (mixed attributions). The paper argues that segments with uniformly positive or negative IG values (consistency ≈ 1) often represent superficial clarifications or rote calculations. Conversely, segments with mixed positive/negative IGs (moderate consistency) indicate "reflective reasoning"—where the model explores alternatives or corrects itself—which is more valuable to learn.

### Mechanism 3: Semantic Integrity via Segment-Level Granularity
Applying selectivity at the segment level (rather than token level) preserves semantic coherence, which is crucial for long-context reasoning. Instead of picking scattered tokens, the method groups tokens by transition keywords, preserving the "thought unit." This ensures that when the loss is calculated, the model sees complete logical steps, maintaining the flow of the Chain-of-Thought.

## Foundational Learning

- **Concept: Integrated Gradients (IG)** - This is the core "sensor" used to detect importance. You must understand that IG assigns credit to input features by integrating gradients along a path from a baseline (padding) to the actual input. Quick check: If a token has a negative IG value, does that mean it is unimportant? (Answer: No, it means it pushes the prediction away from the correct class, but the paper uses absolute IG for strength).

- **Concept: Selective Supervised Fine-Tuning (SFT)** - This is the training paradigm shift. Instead of minimizing loss on all tokens, you minimize loss only on a subset. Quick check: How does selective SFT differ from simply deleting (pruning) the unimportant text before training? (Answer: Pruning removes context, potentially breaking flow; selective SFT keeps the text in the context window but computes loss only on specific parts).

- **Concept: Attribution Direction Consistency** - This is the novel metric introduced to filter "blind" reasoning. It measures the ratio of the sum of signed IGs to the sum of absolute IGs. Quick check: A segment has Consistency = 1.0. What does this imply about the nature of the reasoning according to the authors? (Answer: It implies "shallow" or "skewed" reasoning, like a superficial clarification).

## Architecture Onboarding

- **Component map:** Segmentation Module -> Attribution Engine -> Metric Aggregator -> Mask Generator -> Selective Trainer
- **Critical path:** The IG Computation. The paper notes this is computationally intensive (~7 GPU-hours for a 7B model on the dataset, comparable to the SFT itself). Optimization here is key to scalability.
- **Design tradeoffs:** Precision vs. Cost (J=50 integration steps); Threshold Selection (τ=0.7, β=0.8 found via greedy search).
- **Failure signatures:** Fragmented Output (if segmentation keywords misaligned with reasoning structure); Performance Drop vs. Full SFT (if τ too aggressive or β too strict).
- **First 3 experiments:** 1) Validate "Reflective" Hypothesis (replicate Figure 2, measure Δ in Correct Answer Confidence); 2) Hyperparameter Sensitivity (run ablations on τ ∈ {0.6, 0.7, 0.8, 0.9}); 3) Granularity Ablation (compare "Top 45% Tokens" vs. "Top Segments" on small model).

## Open Questions the Paper Calls Out
The paper's conclusion explicitly states the framework "offers broader potential for emphasizing policy updates on targeted content in reinforcement learning," suggesting future work could integrate this approach with reinforcement learning paradigms like GRPO or PPO.

## Limitations
- Computational Cost and Scalability: IG computation is computationally intensive (~7 GPU-hours), raising concerns about practicality for larger models or datasets.
- Threshold Generalization: Hyperparameters τ=0.7 and β=0.8 were found via greedy search but may not generalize across different model families or reasoning styles.
- Attribution Method Validity: The paper uses Integrated Gradients but does not validate whether IG is optimal compared to other attribution methods or justify the choice of J=50 integration steps.

## Confidence
- High Confidence: Core claim that selective learning on important segments outperforms full SFT is well-supported by experimental results across multiple benchmarks.
- Medium Confidence: Distinction between "reflective" (moderate consistency) and "shallow" (high consistency) reasoning is supported but theoretical foundation linking attribution direction consistency to cognitive depth remains heuristic.
- Low Confidence: Claim that direction consistency is a reliable proxy for reasoning quality across diverse domains is most speculative, with no evidence beyond mathematical reasoning benchmarks.

## Next Checks
1. **Attribution Method Ablation**: Replicate main experiments using alternative attribution methods (attention-based, Shapley values) instead of IG to validate framework robustness.
2. **Threshold Sensitivity Analysis**: Systematically vary τ ∈ {0.5, 0.6, 0.7, 0.8, 0.9} and β ∈ {0.6, 0.7, 0.8, 0.9} across all models and datasets to identify robust parameter choices.
3. **Cross-Domain Generalization**: Apply selective learning framework to non-mathematical reasoning datasets (e.g., strategy game reasoning, commonsense reasoning) to evaluate effectiveness in qualitatively different reasoning domains.