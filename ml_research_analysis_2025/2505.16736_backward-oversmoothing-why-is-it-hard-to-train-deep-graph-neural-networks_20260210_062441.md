---
ver: rpa2
title: 'Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?'
arxiv_id: '2505.16736'
source_url: https://arxiv.org/abs/2505.16736
tags:
- oversmoothing
- gnns
- backward
- graph
- stationary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes a fundamental optimization challenge in deep
  Graph Neural Networks (GNNs) called backward oversmoothing. While forward oversmoothing
  (where node representations become indistinguishable as layers increase) has been
  extensively studied, the authors examine how backpropagated errors are also subject
  to oversmoothing from output to input layers.
---

# Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?

## Quick Facts
- arXiv ID: 2505.16736
- Source URL: https://arxiv.org/abs/2505.16736
- Reference count: 40
- Primary result: Backward errors in deep GNNs undergo oversmoothing, creating spurious stationary points where all layers have near-zero gradients despite high loss

## Executive Summary
This paper reveals a fundamental optimization challenge in deep Graph Neural Networks (GNNs) called backward oversmoothing. While forward oversmoothing (where node representations become indistinguishable as layers increase) has been extensively studied, this work examines how backpropagated errors are also subject to oversmoothing from output to input layers. The core insight is that when forward signals are oversmoothed, the backpropagation equations become approximately linear, allowing explicit characterization of the constant oversmoothed limit of backward errors. This backward oversmoothing creates many spurious stationary points: as soon as the output layer is trained, the entire network reaches a near-stationary point with near-zero gradients despite high loss. The main theoretical result proves that for sufficiently deep GNNs, every stationary point in the output layer is also a global stationary point across all layers.

## Method Summary
The paper analyzes vanilla GNNs following the architecture X^(k+1) = ρ(PX^(k)W^(k)), where P is a symmetric bi-stochastic propagation matrix and ρ is a smooth activation function. The analysis focuses on the forward signal X(k), forward post-message-passing signal F(k) = PX(k), and backward error signal B(k) = ∂L/∂H(k). Theoretical bounds are derived for pairwise differences E(X) and E(B(k)) using spectral properties of P and weight matrices. The study uses synthetic Contextual Stochastic Block Model (CSBM) data with 1-λ ≈ 0.16 and compares deep (L=40) versus shallow (L=5) GNNs against MLP baselines, tracking per-layer gradient norms and loss over training epochs.

## Key Results
- Backward errors undergo oversmoothing through P^⊤ with rate λ < 1, becoming approximately linear when forward signals are also oversmoothed
- Every stationary point at the output layer becomes a global stationary point across all layers for sufficiently deep networks
- This phenomenon is specific to GNNs and does not occur in regular MLPs (Proposition 2)
- Networks can get stuck in flat regions with high loss while exhibiting near-zero gradients throughout all layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backward errors undergo oversmoothing from output to input layers, creating small pairwise differences at middle layers.
- Mechanism: The backward signal B(k) = ρ'(H(k)) ⊙ (P^⊤B(k+1)(W(k+1))^⊤) propagates through P^⊤ = P, shrinking non-constant directions by eigenvalue λ < 1. When both k and L-k are large (middle layers), forward oversmoothing makes ρ'(H(k)) approximately constant, reducing the recursion to linear smoothing.
- Core assumption: Weight spectral norm s ≤ λ^{-α} with expansion rate α sufficiently small (α < 1 - √(1 - 1/q) for q = 1 (classification) or q = 2 (regression)).
- Evidence anchors: [abstract] "backpropagated errors used to compute gradients are also subject to oversmoothing from output to input"; [section 3.2, Thm. 2] E(B(k)) ≤ (D_X D_L s^{L+1} + D'_L)/n × (D_ρ s^{L+1}/(1 - λ²s) λ^{k+1} + (λs)^{L-k}); [corpus] Related work on gradient oversmoothing exists (Park & Kim 2024) but only for linear GNNs; this extends to non-linear ρ.
- Break condition: If s > 1/λ (weights sufficiently expanding), oversmoothing may not occur. Also, very shallow networks (small L) avoid the middle-layer trap.

### Mechanism 2
- Claim: When forward signals are oversmoothed, the backward recursion becomes approximately linear, allowing explicit characterization of the oversmoothed limit.
- Mechanism: Near output layers, ρ'(H(k)) ≈ c (constant) because H(k) itself is nearly constant across nodes. The backward recursion simplifies to B(k) ≈ c·P^⊤B(k+1)(W(k+1))^⊤, which is linear and converges to B(k) ∝ 1_n · (1_n^⊤B(L)/n) · (W_L...W_{k+1})^⊤ — the average output error scaled by weight transposes.
- Core assumption: Forward oversmoothing has occurred (k sufficiently large); activation derivative ρ' is Lipschitz (Assumption 4).
- Evidence anchors: [abstract] "the average of the output error plays a key role"; [section 4, sketch of proof] "when the forward signal is also oversmoothed, the recursive iterations on the backward signal are almost linear"; [corpus] No corpus papers characterize this linear limit explicitly for non-linear GNNs.
- Break condition: Non-constant ρ'(H(k)) at early layers (low k) breaks the linear approximation; skip connections maintaining feature diversity would also break this.

### Mechanism 3
- Claim: Stationary points at the output layer become global stationary points, creating flat regions with near-zero gradients but high loss.
- Mechanism: Gradient ∂L/∂W(k) = F(k)^⊤B(k). At late layers, F(k) ≈ 1_n v^⊤ (constant), so gradient ∝ 1_n^⊤B(k) ∝ average of B(k), which relates to average of B(L). When the output layer reaches stationarity, this average vanishes, forcing all layers to near-stationarity. Theorem 4 gives δ ≲ λ^{ξ_q(α)L} + λ^{-αL} ||1_n^⊤B(L)||₂.
- Core assumption: Sufficiently deep network (L ≳ log(1/δ)/(ξ_q(α) log(1/λ)); output forward signal bounded away from zero or labels centered/balanced.
- Evidence anchors: [abstract] "as soon as the last layer is trained, the whole GNN is at a stationary point"; [section 4, Thm. 3] Explicit conditions: L ≳ log(1/(D_F δ))/(ξ_q(α) log(1/λ)) and δ̄ ≲ λ^{αL}D_F δ; [corpus] Corollary 2 in paper shows loss can remain at σ²_y (label variance) at these spurious points.
- Break condition: Proposition 2 shows MLPs (P = Id) do not exhibit this: output stationarity does not imply global stationarity.

## Foundational Learning

- Concept: **Spectral properties of stochastic matrices**
  - Why needed here: Understanding why P shrinks all directions except the constant eigenvector by λ < 1 is essential to grasping both forward and backward oversmoothing rates.
  - Quick check question: For a symmetric bi-stochastic matrix on a connected graph, what is the eigenvalue associated with the constant eigenvector, and why must all other eigenvalues have magnitude < 1?

- Concept: **Weight spectral norm and expansion rate α**
  - Why needed here: The condition s ≤ λ^{-α} determines whether oversmoothing occurs; α quantifies "how much" weights must expand to counteract smoothing.
  - Quick check question: If λ = 0.9 and s = 1.5, does oversmoothing occur? What about s = 1.2?

- Concept: **Stationary points vs. vanishing gradients**
  - Why needed here: The paper distinguishes these concepts—vanishing gradients implies different magnitudes across layers, while stationarity means all gradients are small simultaneously, which can be either successful convergence or a spurious trap.
  - Quick check question: If ||∂L/∂W(k)||_F ≤ 0.001 for all k but loss = 5.0, is this a vanishing gradient problem or a spurious stationary point?

## Architecture Onboarding

- Component map:
  - Forward signal X(k) → Forward signal F(k) = PX(k) → Loss
  - Backward signal B(L) → Backward signal B(k) → Gradient ∂L/∂W(k) = F(k)^⊤B(k)

- Critical path:
  1. Input X(0) → [P × W(0) × ρ] → ... → X(L) → H(L) → Loss
  2. ∂L/∂H(L) = B(L) → [P × W^⊤ × ρ'] → ... → B(0)
  3. Gradient at layer k requires both F(k) (forward) and B(k) (backward)

- Design tradeoffs:
  - **Depth vs. trainability**: Deeper GNNs can reach more distant nodes (addressing under-reaching) but suffer backward oversmoothing
  - **Weight expansion vs. stability**: Larger weights (s > 1) can counteract oversmoothing but risk feature/gradients explosion
  - **Standard initialization vs. counter-oversmoothing**: Standard init keeps s ≈ 1, which is insufficient to escape the oversmoothing regime

- Failure signatures:
  - Gradients at all layers drop to near-zero within first few epochs while loss remains high
  - Last layer trains quickly, then entire network stalls
  - Does NOT occur in equivalent-depth MLPs on same task

- First 3 experiments:
  1. **Replicate Figure 1**: Train deep GNN (L=40) vs. shallow (L=5) vs. MLP on CSBM node classification; plot gradient norms per layer and loss curves to observe the stationary point trap.
  2. **Ablate expansion rate α**: Initialize weights with varying spectral norms (s = 1.0, 1.1, 1.2, ..., 1/λ) and measure whether networks escape oversmoothing; verify if s > 1/λ allows learning.
  3. **Test on real graphs**: Run on Cora/Citeseer (where 1-λ ≈ 10^{-5}, very small) vs. synthetic CSBM (1-λ ≈ 0.16); observe whether graphs with smaller spectral gap trap faster due to stronger smoothing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do skip connections and normalization layers mitigate backward oversmoothing and its associated spurious stationary points?
- Basis in paper: [explicit] The authors state: "An important limitation of our results is that they only apply to vanilla GNNs, without the common fixes to oversmoothing: skip connections, normalization, and so on. A direct extension would be to examine to which extent these fixes also act on the optimization side."
- Why unresolved: The theoretical analysis assumes vanilla GNN propagation without architectural modifications commonly used in practice.
- What evidence would resolve it: Theoretical bounds on backward oversmoothing rates in GNNs with skip connections (e.g., GCNII, JK-nets) or normalization layers (PairNorm, BatchNorm), combined with empirical verification of whether spurious stationary points persist.

### Open Question 2
- Question: Can optimization algorithms be designed to escape spurious flat regions caused by backward oversmoothing without modifying the GNN architecture?
- Basis in paper: [explicit] The paper suggests: "Our results may help design better optimization algorithms for GNNs, that directly compensate for spurious flat regions without necessarily changing the underlying architecture by adding skip connections, normalization, and so on."
- Why unresolved: The paper characterizes the problem theoretically but does not propose algorithmic solutions.
- What evidence would resolve it: A modified gradient descent or second-order optimization method that provably maintains non-zero gradients at inner layers when the output layer reaches stationarity, with empirical demonstrations on deep vanilla GNNs.

### Open Question 3
- Question: Does backward oversmoothing occur in attention-based GNNs (e.g., Graph Attention Networks) where the propagation matrix P is input-dependent rather than fixed?
- Basis in paper: [inferred] The theoretical results assume a fixed bi-stochastic propagation matrix P. The paper notes most GNNs "follow some flavors of this simple architecture" but acknowledges "some that cannot be written as fixed matrix multiplication."
- Why unresolved: The proofs rely on spectral properties of a fixed P; attention mechanisms make P dependent on node features at each layer.
- What evidence would resolve it: Extension of Theorem 2 and 3 to input-dependent propagation matrices, or empirical measurements of E(B(k)) in attention-based GNNs showing whether backward oversmoothing rates differ.

## Limitations

- The theoretical results only apply to vanilla GNNs without architectural modifications like skip connections, normalization layers, or attention mechanisms that are commonly used in practice.
- The expansion rate requirement s ≤ λ^{-α} with α < 1 - √(1 - 1/q) represents a stringent condition that may not hold in standard GNN training regimes.
- The requirement that networks be "sufficiently deep" (L ≳ log(1/δ)/(ξ_q(α) log(1/λ))) is vague and may not apply to many practical architectures.

## Confidence

**High Confidence** in the core mechanism showing backward errors undergo oversmoothing through P^⊤ and become approximately linear when forward signals are oversmoothed.

**Medium Confidence** in the claim that stationary points at the output layer become global stationary points, though practical conditions may be difficult to verify.

**Medium Confidence** in the assertion that this phenomenon is specific to GNNs and does not occur in MLPs, based on theoretical justification.

## Next Checks

1. **Experiment with weight initialization**: Systematically vary weight spectral norms s across multiple orders of magnitude (from s < 1/λ to s > 1/λ) to empirically verify the threshold where backward oversmoothing transitions to manageable training dynamics.

2. **Test on real-world graphs**: Apply the theoretical framework to real citation networks (Cora, Citeseer) and social networks, measuring whether the predicted backward oversmoothing rates and stationary point traps manifest in practice, particularly focusing on how 1-λ values correlate with training difficulty.

3. **Validate linear approximation regime**: Design experiments that track ρ'(H(k)) across layers to empirically verify when it becomes approximately constant, confirming the theoretical prediction that backward recursion becomes linear in the oversmoothed regime.