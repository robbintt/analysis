---
ver: rpa2
title: 'Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR
  Causal Bayes Nets'
arxiv_id: '2512.11909'
source_url: https://arxiv.org/abs/2512.11909
tags:
- causal
- humans
- llms
- tasks
- low-r
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates causal reasoning in large language models
  (LLMs) by comparing them with humans on semantically meaningful causal inference
  tasks. The study uses a collider graph structure and leaky noisy-OR causal Bayes
  net models to evaluate reasoning patterns, including explaining-away and Markov
  violations.
---

# Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets

## Quick Facts
- arXiv ID: 2512.11909
- Source URL: https://arxiv.org/abs/2512.11909
- Reference count: 4
- Key outcome: This paper investigates causal reasoning in large language models (LLMs) by comparing them with humans on semantically meaningful causal inference tasks.

## Executive Summary
This paper investigates causal reasoning in large language models (LLMs) by comparing them with humans on semantically meaningful causal inference tasks. The study uses a collider graph structure and leaky noisy-OR causal Bayes net models to evaluate reasoning patterns, including explaining-away and Markov violations. Results show that while top-performing LLMs already align well with human reasoning under direct prompting, Chain-of-Thought prompting significantly improves alignment for weaker models. LLMs demonstrate explaining-away behavior more strongly than humans, though the effect varies by model and prompting strategy. Human reasoning is consistently strong across tasks, with top LLMs approaching human-level consistency. The analysis reveals distinct parameter signatures distinguishing strong versus weak causal reasoners, with implications for understanding and improving LLM reasoning capabilities.

## Method Summary
The study evaluates 20+ LLMs on 11 semantically meaningful causal inference tasks using collider graph structures (C₁→E←C₂). Models are prompted with either Direct (single probability response) or Chain-of-Thought (reason-then-respond) strategies. Responses are modeled using leaky noisy-OR causal Bayes nets with parameters θ=(b,m₁,m₂,p(C)) where b is leak (unobserved causes), m₁ and m₂ are causal strengths, and p(C) is prior. Explaining-away (EA) is detected when Pr(C₁=1|E=1,C₂=1) < Pr(C₁=1|E=1,C₂=0), while Markov violations (MV) occur when causes are not independent. Alignment with human baselines is measured using Spearman correlation ρ, and task-level consistency is assessed via LOOCV R². AIC model selection determines whether symmetric (m₁=m₂) or asymmetric parameter variants best fit the data.

## Key Results
- Top-performing LLMs (e.g., gemini-2.5-pro) achieve Spearman ρ≈0.85 alignment with human reasoning under Direct prompting
- Chain-of-Thought prompting improves alignment for weaker models by up to 0.503 (gemini-2.5-flash-lite: +0.503→ρ=0.845) but has minimal effect on SOTA models
- Most LLMs (27/30) exhibit explaining-away behavior, with 24/30 exceeding human EA levels (EA_human=0.09)
- Human reasoning shows weak explaining-away and occasional Markov violations, serving as a consistent baseline across all tasks
- Distinct parameter signatures emerge: high-EA/no-MV agents have low leak (b: 0-0.1) and strong causal strengths (m: 0.75-0.99), while weak reasoners show higher leak (b: 0.15-0.62) and weaker strengths (m: 0.25-0.82)

## Why This Works (Mechanism)

### Mechanism 1: Leaky Noisy-OR CBN Parameterization for Reasoning Interpretability
- Claim: Modeling agent probability judgments with a parameterized noisy-OR CBN exposes interpretable reasoning signatures that distinguish strong from weak causal reasoners.
- Mechanism: The model maps judgments to four parameters—leak (b, unobserved causes), causal strengths (m₁, m₂), and prior p(C). High-EA/no-MV agents show low leak (0-0.1) and strong causal strengths (0.75-0.99); weak reasoners show higher leak (0.15-0.62) and weaker strengths (0.25-0.82).
- Core assumption: Agent probability judgments can be meaningfully captured by a parametric CBN; deviations from normative parameter regimes reflect reasoning quality rather than task-appropriate uncertainty.
- Evidence anchors:
  - [abstract] "Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters θ=(b,m₁,m₂,p(C))∈[0,1] include a shared prior p(C)"
  - [section] "High-EA–no-MV agents have low leakage b (0-0.1), strong causal strength m₁,m₂ (0.75-0.99), and midrange priors, while agents with MV or weak EA show higher b (0.15-0.62) and weaker mᵢ (0.25-0.82)"
  - [corpus] Weak direct corpus support; no neighboring papers replicate this specific parameter-profiling approach.
- Break condition: If judgments are not probability-valued or if task structure deviates significantly from collider graphs, parameter interpretability degrades.

### Mechanism 2: CoT as Alignment Equalizer for Weaker Models
- Claim: Chain-of-Thought prompting disproportionately improves human-LLM alignment for weaker/smaller models, narrowing the gap to SOTA ceiling performance.
- Mechanism: CoT reduces reasoning variance and improves LOOCV R² consistency, particularly for agents with poor Direct-prompting performance (e.g., gemini-2.5-flash-lite: +0.503→ρ=0.845). SOTA models show minimal CoT gains because they already approach ceiling under Direct prompting.
- Core assumption: CoT elicits more deliberate reasoning rather than merely providing format cues; alignment gains reflect improved causal computation, not prompt memorization.
- Evidence anchors:
  - [abstract] "Chain-of-Thought prompting significantly improves alignment for weaker models"
  - [section] "the lower tail rises (minimum R²: 0.277→0.692) and the spread tightens markedly (IQR 0.116→0.060)"
  - [corpus] CogMath (arxiv:2506.04481) supports the general claim that assessment granularity reveals capability differences masked by coarse accuracy metrics.
- Break condition: If CoT responses introduce confabulation or if task requires rapid intuition rather than deliberation, alignment gains may reverse (observed in some agents: CoT slightly reduced EA in strong models like gpt-4o).

### Mechanism 3: Explaining-Away as Diagnostic of Causal Structure Sensitivity
- Claim: Most LLMs exhibit explaining-away (EA) behavior exceeding human baselines, suggesting sensitivity to collider structure semantics.
- Mechanism: In collider C₁→E←C₂, observing E=1 and C₂=1 reduces P(C₁=1) relative to observing E=1, C₂=0. EA is operationalized as VIII > VI (positive slope in Fig. 1b). 27/30 LLMs show EA>0; 24/30 exceed human EA=0.09.
- Core assumption: EA reflects genuine structural reasoning rather than response bias; higher EA is not inherently superior (humans show "weak" EA which may be adaptive in uncertain environments).
- Evidence anchors:
  - [abstract] "LLMs demonstrate explaining-away behavior more strongly than humans"
  - [section] "Most LLMs (27/30) show explaining-away (EA>0), and 24/30 exceed human EA levels (EA_human=0.09)"
  - [corpus] Related work (arxiv:2502.10215, "Do LLMs Reason Causally Like Us?") finds LLMs sometimes outperform humans on causal tasks, consistent with strong EA signatures.
- Break condition: If tasks involve non-collider structures or if semantic content primes alternative schemas, EA diagnostics lose validity.

## Foundational Learning

- Concept: **Collider Graph Structure (C₁→E←C₂)**
  - Why needed here: All eleven tasks use this structure; understanding that causes are marginally independent but become dependent when conditioning on the effect is essential for interpreting EA and MV diagnostics.
  - Quick check question: Given C₁ and C₂ independently cause E, what happens to the relationship between C₁ and C₂ when you observe E=1?

- Concept: **Noisy-OR Parameterization**
  - Why needed here: The leaky noisy-OR model assumes each cause independently contributes to the effect with some probability; the leak parameter captures unmodeled causes. Interpreting (b, m₁, m₂) requires understanding this generative assumption.
  - Quick check question: In a leaky noisy-OR with m₁=0.8, m₂=0.6, b=0.1, what is P(E=1|C₁=1, C₂=0)?

- Concept: **Markov Violations vs. Explaining-Away**
  - Why needed here: MV (causes should be independent but aren't) and EA (rational dependency upon observing effect) are distinct diagnostics; conflating them leads to misinterpretation of reasoning quality.
  - Quick check question: If P(C₁=1|C₂=1) ≠ P(C₁=1|C₂=0) without conditioning on E, is this an EA effect or an MV?

## Architecture Onboarding

- Component map:
  Task stimuli -> Prompting module (Direct/CoT) -> Response extraction -> CBN fitting module -> Diagnostic layer (EA/MV) -> Alignment layer (ρ, R²)

- Critical path:
  1. Initialize agent with task prompt (Direct or CoT)
  2. Extract probability judgment
  3. Fit leaky noisy-OR CBN parameters across all task conditions
  4. Compute EA and MV diagnostics from fitted parameters
  5. Compare alignment (ρ) and consistency (R²) to human baseline

- Design tradeoffs:
  - **Symmetric vs. asymmetric model**: 3-param reduces overfitting but assumes equal causal strengths; 4-param captures asymmetry at cost of statistical power. AIC selection balances these.
  - **Leak parameter interpretation**: Low b suggests normative reasoning but may miss legitimate unobserved causes; prompts do not instruct models about unmentioned causes.
  - **CoT verbosity/reasoning effort**: Paper uses varied reasoning-effort settings (gpt-5 family); higher effort does not guarantee better alignment.

- Failure signatures:
  - **No EA (EA≤0)**: Suggests collider structure not recognized; seen in claude-3-haiku, gemini-2.5-flash-lite under Direct prompting
  - **Markov violations (|IV-V|>0.05)**: Indicates failure to maintain cause independence; seen in humans (weak MV) and some weaker LLMs
  - **High leak (b>0.3)**: May indicate hedging or uncertainty about unobserved causes rather than structural misunderstanding
  - **CoT degradation**: Some strong models (gpt-4o, o3-mini) show slightly reduced EA under CoT—possible overthinking or confabulation

- First 3 experiments:
  1. **Baseline alignment sweep**: Run Direct and CoT prompts across all 11 tasks for target model; compute ρ vs. human baseline and LOOCV R². Compare to SOTA ceiling (gemini-2.5-pro: ρ≈0.85, R²≈0.99).
  2. **Parameter signature profiling**: Fit noisy-OR parameters; classify model as high-EA/no-MV (expected: b<0.15, m>0.75) or weak reasoner (b>0.2, m<0.6). Diagnose whether gaps stem from leak, strength, or prior.
  3. **CoT ablation**: For models with poor Direct alignment, test CoT with controlled reasoning-effort levels. Monitor whether R² improvement comes from reduced variance (IQR tightening) or systematic bias reduction. Check for CoT-induced degradation in EA.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do LLMs maintain consistent causal reasoning capabilities when applied to structures other than the collider graph (C₁ → E ← C₂)?
- **Basis in paper:** [explicit] The authors state in the Outlook that next steps include "extending this framework to... other causal structures beyond colliders to probe reasoning robustness."
- **Why unresolved:** The current study exclusively evaluates collider structures to study explaining-away, leaving performance on chains or common causes unknown.
- **What evidence would resolve it:** Applying the leaky noisy-OR CBN methodology to LLM responses on tasks involving diverse causal topologies (e.g., chains, common effects).

### Open Question 2
- **Question:** How does removing semantic context affect LLM causal reasoning consistency and the emergence of explaining-away effects?
- **Basis in paper:** [explicit] The paper identifies "extending this framework to semantically meaningless tasks" as a necessary future step to validate reasoning.
- **Why unresolved:** It is unclear if current high alignment with humans is due to structural reasoning or the utilization of semantic priors from training data.
- **What evidence would resolve it:** A comparison of LOOCV R² and parameter signatures (b, m₁, m₂) on abstract prompts versus the semantically meaningful ones used here.

### Open Question 3
- **Question:** How do explicit instructions regarding unobserved causes modify the "leak" parameter (b) in LLM reasoning?
- **Basis in paper:** [inferred] The authors note their "prompts do not control this dimension" and that optimal leak depends on the user-setting, yet they do not test the models' responsiveness to such instructions.
- **Why unresolved:** It is unknown if LLMs can flexibly adjust their "leakiness" based on context, or if this is a fixed artifact of their training.
- **What evidence would resolve it:** An ablation study measuring the fitted leak parameter (b) under conditions where models are explicitly instructed to ignore vs. consider unmentioned causes.

## Limitations
- The study assumes human baseline judgments represent normative causal reasoning, though humans show "weak" EA and occasional Markov violations that may reflect adaptive uncertainty handling
- The leaky noisy-OR model assumes independent causal contributions and a single shared prior p(C), which may not capture richer dependency structures in real human reasoning
- High leak parameters may reflect appropriate hedging about unmentioned causes rather than structural misunderstanding, complicating interpretation of b as a reasoning quality signal

## Confidence
- High confidence in: CoT prompting improves alignment for weaker models; EA is stronger in LLMs than humans; SOTA models approach human consistency under Direct prompting
- Medium confidence in: Parameter profiles (b, m₁, m₂) cleanly separate strong from weak reasoners; low leak is always desirable
- Low confidence in: Causal strength interpretation across model families; whether CoT improvements reflect deeper reasoning or prompt format effects

## Next Checks
1. Test parameter stability by re-fitting CBN models with perturbed human baseline judgments to assess sensitivity of EA/MV diagnostics
2. Run controlled experiments varying the leak instruction in prompts to see if explicit mention of unmentioned causes changes b estimates
3. Compare EA/MV patterns on non-collider causal structures to determine if current diagnostics generalize beyond the 11 tasks used